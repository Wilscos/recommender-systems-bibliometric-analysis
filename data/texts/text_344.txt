Today, intelligent voice assistant (VA) software like Amazon’s Alexa, Goo gle’s Voice Assistant (GVA) and Apple’s Siri have millions of users. These VAs often collect an d analyze huge user data for improving their functionality. However, this collected data may contain sensitive information (e.g., personal voice recordings) that users might not feel comfortable sharing with others and might cause signiﬁcant privacy concerns. To counter such concerns, service providers like Google present their users with a p ersonal data dashboard (called ‘My Activity Dashboard’), allowing them to manage all voice assistant collected data. However, a realworld GVA-data driven understanding of user perceptions and preferences regarding this data (a nd data dashboards) remained relatively unexplo red in p rior research. To th at end, in this work we focused on Google Voice Assistant (GVA) users and investigated the perceptions and preferences of GVA users regarding data and dashboard while grounding them in real GVA-collected user data. Speciﬁcally, we conducted an 80-participant survey-based user study to collect both generic perceptions regarding GVA usage as well as desired privacy preferences f or a stratiﬁed sample of their GVA data. We show that most participants had superﬁcial k nowledge about the type of data collected by GVA. Worryingly, we found that participants felt uncomfortable sharing a non-trivial 17.7% of GVA-collected data eleme nts with Google. The curr e nt My Activity dashboard, although useful, did not help long-time GVA users effectively mana ge their data privacy. Our real-data-d riven study found that showing users even one sensitive data element can signiﬁcantly improve the usability of data d a shboards. To that end, we built a classiﬁer that can detect sensitive data for data dashboard recommendations with a 95% F1-score and shows 76% improvement over baseline models. Voice assistants like Google’s voice assistant (GVA), Amazon’s Alexa, Microsoft’s Cortana , or Apple’s Siri are extremely popular today as they are well equipped to perform multiple tasks on users’ voice requests (e.g., searching the internet, callin g a friend, or playing music). However, these voice assistants also collect and a nalyze a lot of user da ta (e.g., timestamps, audio recordings, transcripts, etc.) to improve their infra structure across multiple devices (e.g., in both smart speaker and smartphone). Unfortunately, this data can lead to a huge possible privacy nightmare since the voice assistant might be used in private situations. E.g., GVA collects three types of potentially sensitive data—audio clips of conversations, transcripts of conversations, and th e ambient location of use. We refer to individual records of these three data types as data elements in this paper. In this study, we take Google voice assistant (GVA) as our experimental testbed. Previous studies on understanding user perceptions and preferences for data collection by voice assistants (such as [38, 51]) have ma inly focused their attention on smart speaker users. However, recent reports [30, 53] have highlighted the signiﬁcantly greater p opularity of sm a rtphone-based voice assistants over their smart speaker counterparts. Intuitively, smartphones are easier to use in mo re contexts than smar t speakers, multiplying potential privacy problems. To that en d, the exact same GVA software runs in both Google smart speakers and Android smartphone s, effectively a ggregating data from both. So, we focus on GVA u sers and conduct a real user-data driven study to uncover user perceptions regard ing GVA-collected data. Speciﬁcally, to counter this problem of sensitive data collection, service providers like Google of ten provide a dashboard to the users showcasing their GVA collected data (Google’s My Activity dashboard). We noted th at the dashboard includes data from both smart speakers and smartphone s withou t differentiating markers. However, the efﬁcacy of these data dashboards for controlling privacy in the GVA context is not well-understood. To that end, we unpack user perception s and pref erences regarding data collection by GVA as well as data dashboards through a two-part survey-based user study. Our overall goal is to assess the usefulness/efﬁcacy of data dashboa rds. We specially focus on the context of data collected by voice assistants (in smart devices) and investigate the efﬁcacy of these dashboards to enable the privacy goals of users in that context. Our research questions (RQs), as stated below, are designed to unravel (i) whether data dashboards can indeed facilitate a better und erstanding of what (possibly sensitive) data VAs collect, and (ii) the particular helpful (or not so helpful) aspects of data dashboards from a user-centric view. Our RQs also investigate how to improve the usability of these data dashboards. In this study, we p articularly contextualize our RQs with our focus on GVA. We selected GVA primarily because of the huge user base (boosted by the inclusion o f GVA in all Android smartp hones). Even though our choice of GVA poses some limitations, (e. g., GVA userbase and dashboar d might not necessarily represent all VA users or dashboards), our app roach is still useful—ﬁndings from our study answer broader questions about he lpfulness of data dashboards in general and take a step forward towards improving their usability. RQ1- How frequently do Android users leverage GVA? What is their G VA usage context? Most (72.5%) of our particip ants had been using GVA for around two years or more. 73.75% of participants used GVA frequently, at least a couple of times a week in home , ofﬁce and car. For the median participant, GVA collected 83 7.5 data elements. The context of using GVA ranged from getting information to entertainment. RQ2- Wha t are the user perceptions regardin g the data collection and storage practices of GVA? What is their ideal access control preference for Google relative to their social relations for accessing GVA data? Although the majority (78.75%) of participants were aware that Google collects and stores some form of data, around 40% of users were unclear about the type of data (e.g., audio clips) being collected, signifying superﬁcial knowledge. Interestingly, statistical analysis shows that r e la tive to various social relations (proxemic zones [28]), pa rticipants considered Google mainly as a public entity. RQ3- Do users desire to restrict access of Google to GVA collected spe c iﬁc data elements? Do these access preferences correlate w ith the data element class or the medium of the data collection? Participants wanted to restrict Google’s access for 121 (18.08%) out of 669 audio clips and 61 (17.03%) ou t o f 358 transcripts presented in our survey, a non-trivial fraction of all collected data. They had similar preferences for data collected by sma rtphones and smart speakers but felt significantly less comf ortable viewing data elements where they did not know or could not reca ll the device through which it was collected. There were no signiﬁcant differences in user privacy preferences for data elements from different control and possibly sensitive classes (prepared fro m previous work [9,35, 38]), suggesting the inherent complexity of ﬁnding sensitive data. RQ4- Do data dashboard s help users to control the privacy of their GVA data? Can we further assist users by automated means to improve their privacy-preservin g behavior by improving the data dashboard? How? 50% of our participants did not know about the Googleprovided My Activity data dashboard . Most participants found the dashboard easy to use; however, more long-time GVA users expressed a need for assistance in using the da shboard, suggesting a lack of e ffectiveness for larger amounts of data. Showing users even one sensitive data elemen t collected by GVA using our simple class-based sensitive content detection system made them highly (80%) likely to control their collected data. This suggests that assisting dashboard users thro ugh automated means might imp rove their privacypreserving behavior. We took the ﬁrst step in this directio n by exploring an Machine learning (ML) -assisted hum an-in-theloop (HITL) based design for data dashboards. We show that it is possible to create Machine learning- based systems to recommend sensitive content with more than 95% F1 -score showing a concrete, feasible direction to improve data dashboards. We note that, although we used GVA as our experimental testbed, our ﬁnding s regarding the efﬁcacy of data dashboards as w ell on improving data dashboards could be extended to contexts c oncerning other VAs. For example, our results show that d ashboards are indeed useful for tracking VA-collected data. However, there also is a need for automated assistance in using the dashboards, notably for lon gterm users to control the privacy of large amounts of accumulated data. These ﬁndings are potentially useful for designing improved data dashboards fo r any VAs. The rest of the paper is organized as follows- The background and related work in Section2. Our methodology is explained in Section 3. We describe the data analysis in Section4. The survey results are presen ted in Section 5. In Section 6, we explore M L as a possible improvement to recommend sensitive data elements in data dashboards. Finally, we conclud e the paper in Section7. GVA capabilities and usage: GVA is an intelligent voiceactivated assistant software that Google introduced in May 2016 [36]. It allows users to perform a variety of actions such as getting local informa tion, playing media, performing a Google search, managing tasks, and more [49] through simple voice-based com mands. GVA supports cross-device functionality and is available on a wide range of devices such as smartphones, tablets, smartwatches, TVs, headphones, and Figure 1: GVA interface on a n Android sma rtphone. more [49]. As of 2020, GVA is available on more than 1 billion devices, spre ad across 90 countries and su pports over 30 langua ges. It has more than 500 million monthly active users [12], reﬂecting its immense popularity. Figure 1 shows a visual of the GVA interface and som e functionalities. While most users of VAs use them on sm artphones, tablets, and smart speakers [15], recent privacy studies have bee n paying increased attention to concerns surrounding the use of smart speakers [45], primarily bec a use they are always listening devices. However,severa l works point out the popularity of VA residing in smartp hones whic h can capture more diverse contexts and potentially private data. This work focuses on GVA, which runs on both smart speakers and sm a rtphone s. Privacy concerns with VAs: Privacy concerns surrounding voice assistants have been studied extensively. Several researchers have p roposed different approaches to laun ch privacy attac ks against voice assistants [5, 14, 58, 59]. Schon herr et al. explored the accidental triggering of voice assistants [50], and Edu et al. conducted a detailed literature review of Smart Home Personal Assistants (SPA) from a security and privacy perspective [22]. Th ey highlighted several key issues such as weak authentication, weak a uthorization, proﬁling, etc. Edu et al. also studied various attacks on SPAs, suggested c ounterme a sures, and discussed open challenges in this area. Cou rtney further summarized var ious privacy concerns associated with voice assistants [19]. In recent ye a rs, there have been multiple instances of data leaks associated with voice assistants manag e d by prominent technology com panies [41, 44]. Such data leaks can be a huge cause of privacy concern since VA collected data can include sensitive data such as audio recordin gs, location data, etc. Speciﬁcally, one interaction with GVA c an lead to multiple data e lements—audio clip, transcrip t, andlocation— depending on the con trols set in Google-wide settin gs (e.g., Web & App Activity contr ol, which is turned on by default and enables Google to store transcripts, location, and other metadata for all interactions). For instance, Kröge r et al. discussed the threat of un expected inferences (such as obtaining the speaker’s identity, personality, age, emotions, etc.) from audio recordings stored by mic rophone-equipped devices thro ugh voice and speech a nalysis [31]. T he two ma jor classes of entities that can cause privacy violations with collected VA da ta are (i) the technology companies who own the voice assistants and store data on their servers (e.g., Google, Amazon, etc.), and (ii) external third parties with access to collected VA da ta, upon which tech nology companies might rely to review collected data. These two entities comprise our threat model. Managing privacy of VA-collected data: A possible alternative to prevent privacy vio lations is limiting an d c ontrolling the data collected by voice assistants. Over the years, researchers have proposed several techniques for this purpose, involving both hardware and software. Champio n et al. developed a device called the Smar t2Speaker Blocker to address the privacy and secur ity concerns associated with smart speaker use [16]. The device fu nctioned by ﬁltering and blocking sensitive information from reaching the sma rt speaker’s mic rophone(s). However, such an intervention cannot be used in the case of smartphone-based voice assistants since smartphones are portable devices. Vaidya e t al. proposed another technique to limit privacy concerns (such as voice spooﬁng) by removin g certain features from a user’s voice in put locally o n the device [52]. Q ian et al. additionally presented VoiceMask, a robust voice sanitization and anonymization application th at acts as an intermediate between users and the c loud [47]. Earlier work also developed a user-conﬁgurable, priva cy-aware framework to defend against inference attacks with speech data. While these techniques may be effective in checking privacy c oncerns, a signiﬁcant downside is that they modify the collected data, rendering it unusable by developers. This defeats the purpose of collecting data in the ﬁrst place since voice assistant developers need user data to train better ML models and improve their services. Tabassum et al. presented this as a privacy-utility trade-off,suggesting the development of privacy frameworks that allow users to control the amount of data co llected by the voice a ssistant (in exchange for possibly limited servic es) [51]. The survey conducted by Malkin et al. on understanding the privacy attitudes of smart speaker users also highlighted a demand for effectively ﬁltering accidental recordings and sensitive topics [38]. We take a ﬁrst step in this direction by exploring a human-in- the-loop de sign to identify and recommend sensitive data to GVA u sers. Privacy dashboards: Following up on the recommendations made by the Abramatic et al. for better user privacy control [3], Irion et al. advocated the use of privacy dashboards as a practical so lution to enhance user control for data collected throughout the online and mobile ecosystem, including platforms such as GVA [29]. They also highlighted the potential of AI techniques and m ethods to users manage and enforce their privacy settings. In this area, Raschke et al. presented the design and implemen ta tion of a GDPR-compliant and usable privacy dashboard [48]. In fact, to that end, Feth et al. proposed generic require ment and quality models to assist companies with developing privacy dashboards for different domains [26]. Our research is motivated by this prior work on the importance of privacy da shboards—we aim to uncover the efﬁcacy and possible improvement of today’s privacy dashboards by speciﬁcally focusing on a deployed system in our real-world data-driven study. Google My Activity data dashboard: The Google My Activity dash board is the primary data privacy control provid e d by Google for all its products and services. It is a hub wh ere users can see and modify all o f the key information that Google has been collecting abou t the m over the years [46]. Figure 13 (Appen dix) shows the user interface of the Google My Activity dashboard. Since GVA’s launch in 2016, not much work has been d one on studying user perceptions an d the utility o f such data dashboards to manage data privacy. A rec ent and closely related study [25] investigated user perceptions and reactions to the Google My Activity dashboard. Through a survey, this study showed that viewing the My Activity dashboard signiﬁcantly decreases concern about Google’s data collection practice s. However, the authors were unsure if the dashboard actually provided valuable assistance in reviewing the collected data and enforcing user privacy. The ﬁrst part of our study partially revisits this work. However, we answer several additional que stions about the perceptions an d preferen ces of GVA users. We also answer some questions raised by this study, such as the effectiveness of the Google My Activity dashboard to enforce user privacy. We then pre sent a possible solution to improve data dashboards—through recommending sensitive data elements to users. We also demonstrate a highly accurate proof -ofconcept human-in-the- loop-based machine learning model for the same. We conducted a two-part survey-based user study to unpack perceptions and privacy prefere nces regarding GVA. We deployed our study in the crowdsourcing platform Proliﬁc Acad e mic [2] during September 202 0. We recruited 18+ years old US nationals with >95% approval rating on Proliﬁc. Additionally, we required that our participants primarily used an Android smartphone, used GVA more than once per month in the past year, and were willing to install our browser extension to sh are their GVA-collected data. We took the Figure 2: Three key sectio ns of our study—generic Survey 1, ethical data collection and pe rsonalized Survey 2. help of a short screening survey (AppendixA.1) which too k less than a minute with Proliﬁc-suggested compensation of $0.11 to identify poten tial participa nts. U ltimately, we invited 249 participants (who satisﬁed our inclu sion criteria) for particip ating in our actual two-part study. Survey 1 and Survey 2 of our study (seven days apart) took a total of 52 minutes and 42 minu te s on average, respectively. We compensated participants who completed both the parts with $12 ($5 for part 1 and $7 for part 2). In total, 80 participa nts completed both surveys (out of 119 who responded to our initial invitation). This drop in the number of participan ts was potentially due to the task description and eligibility in the recruitment text. We c onsider the data from only these 80 participants in this paper as we wanted to combine data from both sur veys (and, in effect, connect self-reported general per c eption from Survey 1 with the user feedback on realworld GVA-collected data in Survey 2). Figure2 summarizes our institutional ethics committeeapproved stud y procedure. The stu dy consisted of three main sections- (i) gener ic Survey 1, (ii) Ethical GVA data c ollection and (iii) personalized Survey 2. First, participants were explained the study desig n as well as the exact data they needed to share. The par ticipants who gave us informed consent ﬁrst took the generic Survey 1 . This survey contained generic questions (instrument in AppendixA.2) regarding user knowledge and usage of Android smartphones as well as GVA. Afte r completing Survey 1, participants installed a browser extension developed by us for ethical GVA data collection. Our extension worked entirely on the client-side and helped users create an archive of GVA data and upload it to their own Google account. Then, the pa rticipants manually shar e d a link to the o nline archive with us. Next, we leveraged an end-to-end fully automated pipeline to fetch participants’ shared GVA data and processed the data in a secure computer. No researcher ever manually saw o r analyzed the raw data. This processing phase identiﬁed possibly sensitive data elemen ts collected by GVA. Then, with in seven days of comple ting Sur vey 1 and sharing d a ta , we invited the participants to return for a personalized Survey 2 (instrument in AppendixA.3). In Survey 2, we elicited user perceptions of a stratiﬁed sam ple of these possibly sensitive data elements. Sinc e Survey 2 was generated programmatically for each participant using elements in their own GVAcollected data, we refer to it as a “personalized survey”. We also showed each participant all of their possibly sensitive data elements identiﬁed by our data processing pipeline in a personalized Google Drive folder (with named ﬁles and subfolders for categories) created by us. Finally, we asked the participants about the utility of automatically detecting sensitive GVA-collected data elements. Next, we explain ea ch of the three sections of our study. Our participants provided their online in formed consent before starting Survey 1. In the consent form, we highlighted the purpose of our study, the spe ciﬁc data we would ask to share, an d our privacy-preserving data collection and pro cessing approa ch. Then, in Survey 1, we ﬁrst asked participants some general questions to uncover the ir usage of Android smartphones and GVA. Next, drawing from e a rlier studies on privacy concerns surrounding voice assistants, we designed a set of GVA usage scenarios to ground the user and uncover experiences with sensitive and even privacyviolating data collected by GVA [9, 35, 38]. These scen arios r anged from “Using inappropriate language” an d “Using GVA in places with audible background sounds” to “Accidental activation of GVA” (complete list in Table11 of Append ix D). Then, our participants self-repo rted wheth e r they rec alled using GVA in these scenarios and their comfort in such c ontextual G VA usage. After this, participants re sponded to questions abou t their perceptions regarding GVA data collection (in genera l an d under different transmission principles [6]), storage, and access, including a few qu estions speciﬁcally abou t Google My Activity dashboard. Finally, we concluded Survey 1 by a sking qu estions related to general privacy attitudes. Give n the sensitive nature of the GVA-collected data elements, we wanted to collect it in the most ethical manner possible, as we will describe next. Our data collection protocol and analysis plan were thoroug hly evaluated and approved by our Institutional Ethics Comm ittee (equivalent to an IRB). Participants were briefed about the data collection process through the co nsent form at the beginn ing of the study. Deciding on an ethical data collection protocol: We explored several options to collect GVA data ethically from users along with their downsides—a client-side data-analysis approa c h was in feasible due to the scale of data and com putation, a Google password sharing approach en c ouraged oversharing private data, and approaching Go ogle to analyze user data and performing our study could potentially be perceived as diminishing user agency. We ﬁnally asked our participants to use Google Takeou t, create an archive of only GVA data by selecting speciﬁc options in the Google Takeout interface, and share the archive ﬁle with us after reviewing the data. We created a Firefox browser extension to facilitate data collection—(i) The extension automatically selected the right options in the Google takeout interface (in client browser) to create an a rchive with only GVA data by choosing the right options in the Google Takeout inte rface. This approach diminished the chances of oversharing (e.g ., chances of accidentally adding all their em ails). (ii) The extension automatically selected the option provided in Google Takeout to create an archive in a user’s own Go ogle clou d storage(associated with G oogle ac count). A participant shared their uniq ue link with us to allow pr ocessing of their archive ﬁle. Ensuring privacy of our collected data: In our protocol, participant GVA data could only be accessed using unique individual links Moreover, we informed th e participants that they could r evoke access anytime, All GVA-collected data was anonymous since it did not include any email or names of users. On receiving a link, an automated pipeline ch e cked the validity of the data (using data type and folder structure of the shared data) and invited only participants with valid data for Survey 2. All data processing was automated (no manual exploration of raw GVA-data) and was done in passwordprotected compu ters accessible only to the researchers. Our personalized Survey 2 primarily involved eliciting user reactions regarding spec iﬁc data elements co llec te d and stored by GVA . We start with ou r data processing pipeline to select data elements for Survey 2. Creating a classiﬁer to categorize data elements: We identiﬁed (and leveraged in Su rvey 1) a set of privacy-violating scenarios w here according to earlier work, p otentially sensitive data might be collected by GVA [9,35,38]. We analyzed these scenarios to c reate twelve c la sses that encompass potentially sensitive GVA-collected d a ta elements. These da ta elements were broadly of two types—GVA-collected audio clips and transcripts of the commands given to GVA. Aside from these twelve classes, we considered three additional classes—a separate class “location” for location data, and two separate classes (“audio-rand” and “transcript-rand ”) Table 1: Description of ﬁfteen classes from Survey 2 fo r c la ssifying GVA collecte d audio c lips, transcript and location data. The last column signiﬁes the median number of data elements per user for our p a rticipants. which identify audio clips and transcripts not belonging to any of the twelve classes and ac t as a baseline for data elements. These total ﬁfteen classes are presented in Ta ble 1. We then created automated classiﬁers to categorize data elements in each of these classes for each user. These classiﬁers primarily relied on off-the-shelf signal processing (e.g., measuring Signal to Noise Ratio or de te c ting the number and gender of speakers) and NLP techniques (ﬁnding a gra mmatical error, no n-English word or negative sentiment). We created one binary classiﬁer for e ach of th e a bove-mentioned twelve classes in Table1 (aside f rom “location”, “audiorand” and “transcript-rand” classes). Th ese classiﬁers categorized GVA-collected audio clips and transcripts into one or more of th ese classes. The motivation and detailed description of each c la ssiﬁer is in AppendixD. Selecting individual data elements for Survey 2: Once w e classiﬁed each data element into one or mo re categories (with the help o f classiﬁers) from Table1, we used a stratiﬁed sampling approach. In short, we randomly selected one data element from each category (without replacement) and used them to create the Survey 2 questionnair e. We also created a personalized Google Drive folder for each participant to review in Survey 2. The folder con tained all possibly sensitive data elements found in their GVA data, arrange d in thirteen respectively named ﬁles and subfolders (excluding “audio-rand” and “transcript-rand” categories). Note that our pipeline handled all of the above tasks automatically. Once personalized Survey 2 was generated, one researcher manually invited the corresp onding participant (within seven days of data upload) to participate in Survey 2 via messaging on Proliﬁc. Overview of Survey 2: We created a personalized Survey 2 (instrument in Appendix A.3) for each participant using at most ﬁfteen selected data elem e nts, depending on the presence/absence o f a particular class. During the survey, we ﬁrst showed these data ele ments randomly to the participants and correspo ndingly asked some related questions, e.g., what are the contents of the data element and how comfortable is the participant in sharing it with people in different proxemic zones [28] as well as Goo gle. Note that participants were not provided with any clue about the possibly sensitive nature of these data elements at this stage of Survey 2. Next, we gave participants a brief explanation of the respective classes from which the data elements for their p ersonalized survey were selected. The particip ants also rated the accuracy of those explan a tions. Then, to demonstrate the possible output of automa te d techn iques to uncover sensitive data elements, we asked participan ts to review a pe rsonalized Google Drive folder with named ﬁles and subfolders co ntaining categorized possibly sensitive G VA data. Then we asked questions to measure user awareness about GVA after seeing this folder. We concluded b y asking a bout the utility of an automated system for detecting sensitive GVA-collected data. In the end, we gave instructions to uninstall the browser extension. First, our study is limited in recruitme nt since we recruited only US Proliﬁc users who primarily use And roid smartphone s and are familiar with GVA. In other words, we mig ht have chosen primarily English speaking users who are also more frequent Android and GVA users than avera ge. However, US-based users are still an important portion of the GVA user base and any privacy issue uncovered by exploring experienced GVA users possibly also affects lesser experienced users. Second, we focused on GVA users who also used an An droid device as their primary smartpho ne. Since GVA is also available in iOS and third- party IoT devices, we might have missed those users. However, this is expected since, in this study, we aimed to investigate the most prominent users of GVA—Android users (GVA is installed by default in Andro id, unlike iOS). Consequently, som e o f our survey participants’ perceptions about data collection might not be representative of data collected by other voice assistants, which might be used in a different context ( e .g., a voice assistant integrated into a children’s toy Third, a few of our participants might consider some of our questions as probing based on both language of the question and their prior experience—intr oducing bias in some of our self-reported data-based r esults. Lastly, our results might have underestimated privacy needs as very privacy-sensitive ind ividuals would be un likely to participate in a study that aimed to investigate their GVA data. Not covering such privacy-sensitive individuals is a common concern with user studies related to privacy [40]. However, we strongly feel that this work is still valuable since we unpack common privacy perceptions of GVA users regardin g their data an d identify possible avenues to improve data da shboards an d simplify privacy controls for this data. We performed both quantitative and qualitative analyses of participants’ survey responses. In this section, we brieﬂy elaborate on our da ta analysis p rocess. Qualitative open coding: We p erformed qu a litative open coding to analyze free-text responses [33]. First, an author analyzed the responses to each question and created a code book. Next, two researchers independently coded the responses using this shared codeb ook. Across all questions, Cohen’s kappa (inter-rater agreement) ranged from 0.769 to 1.0 signifying near-perfect agreement. At last, the coders me t to resolve disagreements and ﬁnalized a c ode for each response. Quantitative statistical analysis: To gain more insight into the collected quantitative data, we perf ormed several statistical te sts [21, 23]. When the independent variable was c ategorical, and the dependent variable was numerical, we found all distributions were non-normal (using the Shapiro Wilkes test) and nearly all indepe ndent variables with more than two levels. Therefore, we decided to opt for the Kruskal Wallis test for comparing distributions in such cases. When both independent and depe ndent variables were categorical, we used either the χtest or Fisher’s exact test (when individual cell values in the contingency ta ble were < 5) to ﬁnd signiﬁcant correlations. We also u sed difference in proportio ns as a mea sure of effect size in our analysis. Apart from statistical tests, we used standard evaluation metrics such as acc uracy, p recision, and recall to test our prediction model [4, 11]. In this section, we present results from our stud y on understanding u ser perceptions and privacy preferences regarding GVA data. Unless otherwise speciﬁed, results in this section will correspond to self-reported data and not actual u sage data. In spec iﬁc analyses (e .g., sharing comfortability ) involving audio clips and tra nscripts from Survey 2, we sometimes discounted very few elements due to lack of user feedback. A total of 80 participants completed both Survey 1 and Survey 2. We start by chec king the basic demographics of those participants in this section. Basic demographics: Our participant pool had a slight gender bias—6 8.8% self-identiﬁed as male, 30% as female, and 1.2% as non-binary. In terms of age—30% were 18-24 years old, 31.3% were 25-3 4 years old, and 26.3% of the participants were between 35 and 44 years. Our participants self-identiﬁed themselves with several ethnicities—66.3% reported themselves as White, 13.8% as Asian or Paciﬁc Islander, 8.8% as Black or African Amer ic an and 6.3% as Hispanic or Latino. The rest had mixed ethnicity. The majority of our participants were employed—47.5% employed full-time and only 20% identiﬁed as students. In our sample, 53.75% of participants had a bachelor’s degree or higher, and only 30% were associate d with computer science or a related ﬁeld. Overall, our pa rticipants came from a wide demographic spread. Usage of Android smartphones: Even though we did not speciﬁcally attempt to recruit long-time Android users, 91.3% of our participants rep orted using an Android smartphone for three years or more. Furtherm ore, 90% of participants also mentioned using their current Google Account on Android smartphones for three years or more. We had an active And roid-user sample— 61.3% of participants used the ir smartphones daily for 2 to 6 hours and 26.3% for 6 to 10 hours and 6.3 % daily for more than 10 hours. The participants used different smartph one applications—54.5% participants had more than 50 apps on their phones at the time of the study. The majority of our participants were familiar with advanced Android feature s such as rooting, developer options, and launchers (over 70% participants for each). In our sample, 87.5% o f participants owned devices run ning recent Android versions (9 or 10) manufactured by nine different manufacturers. Overall, our participants were long term Android users, well aware of advanced features, and had moderate to high daily usage. In this subsection, we present results on general usage patterns of GVA as well as the context for such usage. General usage: 72.5% of our participants were long-time GVA users, with 43.8% participants using GVA for three years or more, and 28.8% using it for two yea rs and 17.5% for a year. In terms of usage frequency, 43.8% o f participants used GVA at least once a day, 30% used it a couple of times per week, and the remaining 26.2% of participants used it once a week to a couple of time s per month. Participants used different methods to activate GVA (with some using multiple methods)—76.3% of participants used a hotword ( e .g., “OK Google”), and 56.3% activated GVA by touching, pressing, or holding buttons on their device. Interestingly, 97.5% of participa nts used GVA in three br oad zones: home, ofﬁce, and car encompassing both professional and personal lives. Additionally, 38.8% of participants also reported using Table 2: Overview of participants’ GVA data. Google smart speakers. Using actual usage data collecte d in part 1 of the study (as described in Section3.4), we found that interac tions with GVA resulted in 138,874 data elemen ts stored in Google ’s servers. The median participant had 837.5 data elements, signifying non-negligible usage of GVA. An overview of participants’ GVA data is in Table2 a nd yearwise statistics are in Table 10 (Ap pendix C). Understanding context of GVA usage: To understand the context for using GVA, we analyz ed participant responses to the que stion-For what purposes do you use Google Assistant on your An droid smartph one? from Sur vey 1. The common reasons for using GVA were getting local infor mation (50), communicating with others (29), resolving a query ( 28), playing audio and video ﬁles (27), navigating through devices (25), controlling other devices (24), entertainment such as games, jo kes, etc. (16), and plann ing their day (14). Thus, participants used GVA for a wide number of purposes. Usage of GVA in smartphones: For eac h of the 1,027 data elements (audio and transcr ipt) presented in Survey 2, we asked particip a nts to choose the device that, according to them, collected each data element (GVA can run in multiple devices). Participants rep orted that 494 (73.9%) out of 668 audio clips were collected by GVA installed on sma rtphone s, whereas smart speakers collected only 92 clip s, indicating a bias towards smartphones for GVA usage. For a non-tr ivial 81 clips, participants either did not recall or even did not know. T he results are similar for transcripts where 229 (63.78%) out of 359 transcripts were collected by GVA on smartphones, and smart speakers collected 57 transcripts; the par ticipants could not recall or didn’t k now the source for the rest. Thus, most data elements presented in Survey 2 were collected by GVA on sm artphone s. We note that this bias towards GVA use on Android smartp hones could be b e cause of our inclusion criteria since we recruited users of Android, which has GVA pre-built into it. Still, our ﬁnding hints at an important domain of heavy data collection by GVA on smartphone s in a wide variety of contexts. Summary: 73.8% of our participants used GVA frequently (at least a couple of times per week or more). The majority of GVA usage happened in smartphones in multiple c ontexts, and our median participant contributed a total of 8 37.5 data elements. The median age of GVA data was 3 years. Next, we check whether participants understood how Google handled any GVA-related data. Speciﬁcally, we investigate user perceptions regarding GVA data collection and storage using data from Su rvey 1. Perceptions of overall data collection: First, we identiﬁed (using the Google account of the author s) that there are seven different types of da ta collected (at most) by GVA. We veriﬁed these types in our automated data collected phase too. Table3 shows th e seven different types of data collected (at most) by GVA. The top three are the most obviously sensitive data types (audio, tra nscript, loca tion), whereas the rest can be considered metadata. Recall that we referred to these three obviously sensitive data types as data elemen ts in this work. To und erstand the awareness about GVA data collection, we asked participants-Do you think that Google Assistant on your Android smartphone collects any kind of data while you are using it? in Survey 1. 7 8.8% of our participants responded afﬁrmatively with “Yes”, and an additional 20% re sponded “Maybe”, signifying the participants are well-aware of possible da ta collection by GVA. Perceptions of speciﬁc data collection: However, then we dug de e per and asked -What pieces of data do you think are collected when you use Google Assistant on your Android smartphone?, showing participants the list given in Table3. Most participants expressed that GVA collected data such as the date (89.1%) and time (85.7%) of conversations as well as the activation method (81.5%). Interestingly, comparatively fewer participants were aware that GVA c ollected sensitive data types such as the transcripts of conversations (73.9%) and the ambient location (70.58%). Just 61.3% of participants believed that GVA collecte d audio clips of conversations, implying that a non-trivial 38.7% of users w ere unaware about the collection of audio clips by GVA. Correlation between awareness of data co llection and comfort in sharing data with Google: Next, we asked pa rticipants to indicate how comfortable they would feel if GVA collected data from each of the seven data types. The top three data types where most participants felt most uncomfortable to share with Google were audio clips of c onversations (58.8%), transcripts of conversations (45.4%), and ambient Figure 3: Participant awa reness of collected da ta types and comfort in sharing them with Google. Data typ e s perceived to be collected/not-collected correlated with participant comfort in sharing with Google (Fisher’s exact, p < 0.000 1). location (45.4%). The top three data types where most participants felt comfortable with data collection were activation method (66. 4%), notiﬁcations (56.3%), and time of conversations (48. 7%). Next, to check if the data types that most participants felt uncomfortable with being collected were also the ones participants were least awar e of, we performed a correlation analysis. Figure3 presents the analy sis result— participant awareness about the collection of each data type and their com fort level in sharing the data type positively correlated. These results signify that the data types for which people were less aware of collection (e. g., audio clips), people were also less comfortable with them bein g collected. This result indicates a superﬁcial understanding of GVA data collection. We sur mise that this shortcoming might cause a decreasing interest in GVA users to delete the GVA collected data via existing privacy controls—e.g., deleting or even browsing their stored data thro ugh the data dashboar d. Perceptions of data storage: We asked our participants Where d o you think the data, if collected by Google Assistant on your Android smartpho ne ( and voice-enabled Google smart speakers) is stored? 86.3% of our participants correctly responded that the data is stored on Google data storage facilities (servers). However, 10% of participants responded that the data is stored only on the respective d evice, whereas 3.7% of participants responded th a t the data is stored completely or partially in both places. So, the majority of participants had a clear idea of about data stora ge practices of GVA. Summary: Most (78.8%) participants thought that Google collected some data using GVA, and the majo rity were aware of where this d ata is stored. However, their awar e ness about the type of data stored was lack ing—a non-trivial fraction was unaware of the collection of sensitive data types. In fact, the particip ants were uncomfortable sharin g the data elements they were not aware GVA was collecting (e.g., audio clips). Most participants were aware that Go ogle collects and stor es some data using GVA in their servers. Thus, we investigated the desired access contro l rules for Google in the context of speciﬁc classes of GVA data elements. We analyzed participant responses to this question in Survey 2 for speciﬁc data elements—After going thro ugh the audio clip/Google Assistant command, how comfortable wo uld you feel if someone in your intimate/private/social/public relations/Google heard it/came to know about it?. This question ch e cked the sharing (i.e., access control) preferences for GVA data with people in four proxemic zones–intimate, private, social public [28] as well as Google. Then, we used statistical a nalysis to chec k the proxemic zone closest to Google in terms of these sharing preferences. A Fisher’s exact test found tha t there was a statistically signiﬁcant correlation between desired access rules f or Google and all proxemic zone s (Fischer’s exact p < 0.05) across all classes of data elemen ts from Table1. Then we used difference in proportions as a measure of effect size on 2× 2 contingency tables containin g comfort data ele ments between a proxemic zone and go ogle (one table for each class of data element) [23]. For each class, the zone(s) with the smallest effect size had the closest sharing preference with Google . The average effect size for each proxemic zone across all classes revealed th a t participants associated Google most closely with the pub lic zone (average effect size 0.81) and farthest from the private zone (average effect size 0.92). Table9 (Appendix C) contains all the effect sizes. Summary: Across all speciﬁc data elem ents, our participants expressed that the access control rules for Google should be similar to a public entity. To understand how this observation translates to actual user behaviour, we now analyze user privacy preferences for sharing speciﬁc ﬁles with Google. For speciﬁc data elements across different classes presented in Survey 2, we asked participants if they are comfortable sharing speciﬁc data elements with Google today. Users want to restrict acc ess of Google for speciﬁc GVA data: Our participants were uncomfortable shar ing 121 (18.1%) out of 669 au dio clips and 61 (17%) out of 358 tran- Figure 4: User p references for sharing audio clips collected by different devices with Google. Preferences for transcripts followed a similar trend. scripts (pre sented to them in Survey 2) with Google. These numbers indic ate that participants felt uncomfortable shar ing a non-trivial f raction of their GVA-collected data. Next, we will check the correlation between this preferenc e with the medium of data collection and class of data. Correlation with medium of data collection: We checked the correlatio n between the device of data collection with user comfort to share data. Figure4 presents this result. Our statistical test did not reveal any signiﬁcant difference in comfort across data collected via GVA on phones or smart speakers. However, we did ﬁnd a signiﬁcant correlation (p = 0.00) between user knowledge of the medium of data collection and user comfort in sharing the data element (both audio and transcript) with Google. Participants felt signiﬁcantly more comfortable sharing data elements where they knew about or could recall the origin of the collected data element. Interestingly, today, Google My Activity Dashboard only shows whether a data element was collected by a Google smart speaker, completely ignoring smartphone devices. Future dashboard designs could add these missing details to make users more comforta ble while viewing their data. Correlation with auto- detected classes of data e le ment: Participants felt most uncomfortable sharing audio clips containing regret word s ( 24.4% of a ll elements from that c la ss), followed by aud io clips having multiple speakers (21.1%) and transcripts containing regret words (20.8%). However, a Kruskal Wallis test revealed no signiﬁcant differences between pr ivacy preferences for data elements belonging to different classes. This result implies that perhaps these simple classes (often based on word matching) were insufﬁcient to accurately identify the GVA-collected data elements where participants wanted to restrict access. Summary: Participants want to restrict access for a nontrivial fraction of their GVA-collected data, which underlines a need to simplify data dashb oards for identifying such data elements. Interestingly, showing users the origin of their collected data made them more comfortable sharing their data. Our simple NLP and signal processing b a sed classes were unable to capture sensitive data elements. Thus, we need mor e complex tools to identify such sensitive data elements that users are uncomfortable sharing. As highlighted in the previous subsection, GVA users wanted to restrict access to 17.7% of GVA-collected data. Today the My Activity Dashboard is the only Google-provided way (aside from legal interventions) for the users to delete this data (albeit post-facto) and control privacy. Therefore, we sought to un derstand user perception s regarding the utility of this dashboar d. Perceptions of data accessibility: First, w e chec ked whether participants were aware that they could access the data collected by GVA. 61.3% participants believed th at th e data could b e accessed, while 32.5% re sponded that it might be accessed, indicating the majority are at least aware of the possibility of a tool like the data dashboard. Popularity and usage of data dashboard: Now, we check if participants knew about the My Ac tivity dashboard. 40 (50%) participan ts responded tha t they had heard of it, 10% were not sure , whereas a surprisingly high 40 % of participants had never heard of the data dashboard. Among the 40 participants who had heard of the My Activity dashboa rd, 4 participants had never visited it, and 33 of the remaining 36 participants visited their dashboards less than once per month. In effect, only 3 (3.8%) out of 80 participants visited their dashboards more than once a month. According to the 36 participants who had visited their respective dashboards before our study, the top reasons for visiting it were–(i) To simply check it out (30), (ii) To view collected data (18), (iii) To change activity settings (11), and (iv) To delete some data (9). We asked 50 participants w ho were unsure/unaware to visit the data dashboa rd to chec k their GVA-collected data before continuing with the study. Unpacking perceptions of data dashboard: Since all participants had explored their dashboards at least once by this point in the study, we asked them how comfortable they felt while viewing the data on their dashboard and why. On a scale of 1 to 5 (1 being very un comfortable), the average comfortability rating was 3.2 6 (σ = 1.06, median = 3), indicating that most participants felt n either comfortable nor uncomfortable viewing the data. In fact, our qu a litative analysis revealed that participants had mixed reactions to GVA data presented in the dashbo a rd. 37.5% of our participants were either bothered or sur prised by the information collected. For instance, P3 1 said, “I know that google is collecting information, but I am not 100% comfortable to see the amount they collect. There is really no privacy.” 15% of particip ants were glad that the data was available, and 6.25 % of participants were un sure of their choice. The remaining 41.25% of participants were neither bothered nor surprised. For example, P12 told , “I already know Google was c ollecting all of the information I saw.” Using a Kr uskal Wallis test (p = 0.029), we found that pa rticipants who had heard of the Google My Activity dashb oard before the study were more comfortable (N = 4 0, µ = 3. 525) in viewing collected data, as compared to participants who had never heard of the dashb oard (N = 32, µ = 2.8 75). Therefore, participants grew more comfor ta ble with the dashboard as they became more familiar with it. Our results stro ngly support the need for data dashboards since participants feel mo re in control (and thus comfortable) when they can see and manage their data. Understanding usability of data dashboard: To check the usability of the data dash board, we asked our participants the question-How easy was it to reach and ﬁnd what you were looking for?, u sing a 5-point scale (1 being very difﬁcult and 5 being very easy). The average rating was 4.025 (σ = 0.899, median = 4). Thus, most participants found My Activity Dashboard ea sy to reach. To get a better idea of any difﬁculties faced b y par ticipants during navigation, we then asked them the question-Did you face any difﬁculties or problems in navigating through yo ur Google My Activity Dashboard? Qualitative ana lysis sh owed that 8 (10%) out of 80 participants found it hard to navigate through the dashboard to ﬁnd their data. For example, P62 said, “There is so much data it is a little overwhelming.” 2 participants did not che c k their dashboards,and 1 participant faced some pr oblems with navigation but did not elaborate on it. The remaining 69 participants did not report any difﬁculties with navigation. 16.25% of participants said they would like some assistance in using the dashboard, and another 10% told th a t they might want some assistance. The remaining 73.75% of participants indicated that they would not like any assistance. We found a positive co rrelation (Fisher’s exact test, p = 0.048) between the duration of using GVA ( le ss or more than around 2 years) with the need for assistance in using the dashboard, highlighting a possible cognitive overload for long-time GVA users. Summary: We found an interesting knowledge gap within our participants—93.8% of pa rticipants thought their GVAcollected data can or may be accessed. However, only 50% of the participants were aware of the data dashboard, showing a lack of actionable knowledge. E ven the people who knew about data dashboards, just 3.7 5% visited it regularly. In fact, more than one-third participa nts (3 7.5%) felt bothered or surprised while viewing the collected data. Quite assuringly, most participants found the dashboard easy to use; however, 10% of participants also found it difﬁcult to access their data. We obser ved that more long-time GVA users expressed a need for assistance in using the dashboard , suggesting the more they become familiar with the dashboard, the more overwhelmed they might get with the huge data collected by GVA over time. Currently, Google’s My Activity Dashboa rd provides two ways to delete collected data- (i) users can e ither inspect and delete each data element individually, or (ii) delete all data elements stored within a speciﬁed date-time range . The former is particularly not useful from a privacy perspective because inspecting a large number of collected data elements (most of which are non-sensitive) is quite time-consum ing and laborious to be practically feasible, as seen in the previous subsection. On the other hand, the latter can help enforce privacy but is not a good option for users who might want to retain some/all of the ir collected data for future reference, assisting product d evelopment, etc. To assist users with ﬁnding their possibly sensitive data collected by GVA, we did a simple proof-of-concept test—around the end o f Survey 2, we showed them a persona lized Google drive with data elements divided into subfolders according to th eir autodetected classes from Table 1 (with classes as subfolder names) and asked if a system which can show such classiﬁcations will be usef ul. Recall tha t these classes were constructed with privacy-violating scenarios in mind, and hence some of the d ata elements were expected to be sensitive. Recommending elements in data dashboards: After participating in the study, 56.3% of participants reported that they were very likely to delete some of their data collected by Google. 65% of participants said that our classiﬁer provided valuable assistance in ﬁnding sensitive data on Google servers, and 72.5% told that they would recommend others to try it out if made publicly available. This pe rcentage was 50% higher than the 2 7.5% of par ticipants who expressed a need for assistance in ﬁnding sensitive data on Google servers in Survey 1, indicating a strong demand for such a recommendation sy stem in data dashbo a rds. The efﬁca cy and challenge in providing recommendations: The primary challenge that we faced while developing our sensitive content detection system was related to the accuracy of the system in assign ing classes to the data elements. On a 5-point Likert scale (1 is very inaccurate and 5 is very accurate), the average rating provided by participants to our classiﬁer was just 2.67 (σ = 0.96, median = 3), suggesting that most participants did not ﬁnd it highly ac c urate. Additionally, 20 participan ts provided qualitative fe edback regarding the stu dy. 9 out of these 20 participants pointed out that the system accuracy could be improved. For example, P60 stated: “Overall I found the sensitive content system not to be very accurate. It some cases it was correct, but in more cases it wa s rather incorrect.” Despite th e low perceived accuracy of the classiﬁcation, we found it surprising that 65% of participants found it useful to ﬁn d sensitive content. P71 further explained the connection between classiﬁcation accuracy and helpfulness of our recommendation system: “Perhaps try improving the ac curacy. I noticed that while it did get some things right, it’d periodic ally get things wrong. I’m not expecting the system to be perfect though but if you can improve the ac curacy at all that’d be great.” To better understand this resu lt, we looked at participant accuracy scores and sharing preferences for individual data elements presented in Survey 2. Out of 52 (6 5%) participants who believed th a t our system provide d valuable assistance in ﬁnding sensitive data (i.e. who liked our classiﬁcation presentation potentially irrespec tive of accuracy), we focused on 36 participants who rated at least one encountered data element >= 3 for acc uracy and also fe lt neutral or uncomfortable sharing it with Google. We observed that 29 (80.6%) of these 36 par ticipants found our system to be helpful (i.e. they found our classiﬁcation accurate). So, our results hinted that even when our classiﬁer was able to detect at least one possibly sensitive ﬁle, most participants found it useful, signifying a need for such recommendations. Summary: Accu rate recommendations of possibly sensitive data elements help users restrict access and protect the privacy of their VA-collected data . Enc ouragingly, r ecommending users to revisit even one accurately sensitive data element collected by GVA made them highly (80.6%) likely to control their collected data, highlighting th e demand for acc urate, usable dashboards. This ﬁn ding strongly underlines the efﬁcacy of a highly accurate sensitive-element recommendation system to im prove the utility of data dashboards. In the next section, we present the feasibility of building such an automated, highly accurate, sensitive content detection system. Earlier, we saw that voice assistants like GVA collect and store large amounts of u ser data. While deleting all collected data in bulk can help avoid privacy violations, in Survey 2, participants men tioned not deleting the shown data elements for 64.8% (674 out of 1040 data elements shown) of cases. Our open coding of their expla nations revealed interesting themes—the prominent reasons for not immediately deleting these 674 data elements was that these data elements were non-sen sitive (24.0% of data elements), improving Google Assistant or Google services in general (8.2%). For eg., P48 said, “I don’t mind Google having access to clips like this to improve their services.” Other reasons for not deleting collected data included having the choice to view or delete previously collected data at will (1.4%) and personalize d recommendations from Goog le (0.4%). In the case of better personalized recommendations, P32 explained, “I don’t mind if Google knows h at music I listen to, especially if it improves it’s music suggestion service.” For 17.7% of data elements, participants did not me ntion any spe ciﬁc reason, but they wanted Google to ca rry out their processing and delete this data within a time frame (e.g., after 3 months). Even though compa nies provide data dashboards for users to ac cess this data (e .g., My Activity Dashboard by Google), current dashbo ard designs do not offer mechanisms for users to efﬁciently sift through and restrict access to speciﬁc data elements. To that end, our results (section5.7) hint at a need for an improved human- in-the-loop (HITL) GVA data dashboard design—we envision an interface that can prioritize potentially sensitive content in the dashboard interface and assist users in controlling the privacy of their GVA-collected data. However, auto-detecting and restricting access to sensitive data elements to help users is also challenging as sensitivity can depend on external factors (e.g., user’s age, frequency of use, other personal reasons, etc.) aside from the content of data elements. To that end, we explored the feasibility of recommending sensitive d ata e le ments in data dashboards in a HITL scenario where the recommendation is only to help users ﬁnd su c h data elements and not to take away their control. Compan ie s could leverage such recommenda tions to improve their data dashboards by presenting possibly sensitive data to their users for review. We envision that such data elements c an be presented either by creating a separate review section in a dashboard or chan ging the default ranking of shown content. Our prediction task involved predicting whether a user will perceive a pa rticular data element collected by GVA as sensitive. For classiﬁcation, our training dataset consisted of tuples (X, Y), where Xrepresents the fea ture vector, and Y represents the sensitivity label of a data element i. The p rediction task involved binar y classiﬁcation, where Y= 1 correspond ed to the ‘Yes’ label (sensitive class) and Y= 0 correspond ed to the ‘ N o’ label (not sensitive cla ss). The feature vector Xincluded audio-based featu res, text-based features, and user-based features, all of which were captured through user survey responses and shared GVA data. The audio-based features that we used were M el-Frequency Cepstral Coefﬁcients (MFCC) [57], spectral co ntrast, tempo, and SoundNet-based features [8]. The text-based features included LIWC-based features, sentence embedding, pre sence of swear words, presence of regret words, sentiment-b a sed features, emotion-based features, and presence of top 100 unigrams and bigrams. The user-based feature s consisted of age range and gender of users, age of Google Account, frequency and span of GVA usage, and associa tion with c omputer science or a related ﬁeld. The survey responses were included either a s one-hot encoding or binary indicators for multiplechoice answers. A detailed description of all features is in Append ix E. To perform this classiﬁcation, we explored several e stablished supervised ML algorithms such as Support Ve ctor Machines (SVMs), Logistic Regression (LR), Random Forest (RF), Multi-layer Perceptron (MLP), each from the scikitlearn library [42], along with XGBoost (XGB) [18]. We compared the p e rformance of these classiﬁers with two baselines. The ﬁrst one was a random classiﬁer that randomly assigned a label to each data element, where prediction probabilities for labels were chosen based on their prevalence in our dataset. For our second baseline, we used the preliminary categorization (Table1) o f each data element as the input feature to train another XGBoost model called XGB-Class. All model hype rparameters were optimized using grid search with 10-fold cross-validation. We found the XGB model to perform the best and thus use it to repor t our ﬁnal performance metrics. Our dataset consisted of 542 audio clips and 412 transcripts. Each data element was associated with one of three sensitivity labels by the users during Survey 2 - ‘Yes’ (sensitive class), ‘No’ (not sensitive class), and ‘I am not sure’ (ambiguous). Since data sensitivity is subjective, we conside red these user-assigned labels as accurate ground truth for our predictions. The distribution of labels in the dataset was as follows- 18.34% ‘Yes,’ 63.94% ‘No,’ and 1 7.72% ‘I am not sure.’ We were speciﬁcally interested in sensitive data elements (labeled ‘Yes’) for ou r prediction task. Since the n e utral label, ‘I am not sure,’ constituted a non-trivial fraction of our dataset, we performed four different experiments, treating it as a separate label each time. In each experime nt, we trained our best performing XGB classiﬁer using the ‘I am not sure’ label as a proxy for one of the se four labels- ‘Yes’ (sensitive class), ‘No’ (not sen sitive class), ‘Removed’ (do not consider in training process), and ‘I am not sure’ (treat as a separate class). The ﬁrst three experiments wer e binary classiﬁcation problems, wherea s the last one was a threeclass classiﬁcation problem. The best results were obtained associating the ‘I am not sure’ label with the not sensitive class (’No’ label), which semantically also implies a conservative prediction—recommending only data element where the classiﬁer is cer ta in that its sensitive. Hence, we report the results of only this experiment in the paper. The rest of the results are in AppendixB. We performed 10-fold cross-validation and reported macroaveraged precision, recall, and F1 scores fo r each classiﬁer. Here, precision is deﬁned as the ratio, TP/(TP+FP), where TP refers to the number of true positive predictions, and FP refers to the number of false positive predictions. Similarly, recall is deﬁned as TP/(TP+FN), where FN refers to the number of false n egative predictions. Since our dataset was highly skewed away from the class of our interest, we used the Synthetic Minority Oversampling TEchniqu e (SMOTE) [17] to balance our dataset before training the models. SMOTE aims to balance imbalanced datasets by oversampling or randomly replicating samples from the minority class. We used the implementation of SMOTE provided by the imbalancedlearn [34] Py thon libr ary. Next, we plotted precision-recall (PR) cur ves (averaged over 10-fold s) for each classiﬁer to analyze the trade- off between showing a larger number of sensitive data elements to the users and the accuracy in ﬁnding such elements, reﬂected by recall and precision values, respectively. Maximizing both precision and reca ll is often not mathematically possible. In practice, a classiﬁer with high precision and low recall returns fewer but relevant results, whereas a classiﬁer with high recall and low precision returns many but irrelevant results. Therefore, achieving a balance betwe en pr ecision and recall is crucial for such classiﬁers to rec ommend as many sensitive data elements as possible to the user. A valuable heuristic to capture th is trade-off between precision and recall is precision-recall area under curve (PR-AUC). A higher value of PR-AUC for a classiﬁer shows its ability to achieve both good precision and recall. We ca lculated the PR-AUC values for different classiﬁers from their PR curves and used it a s a metric to quantify their overall perf ormance. Finally, we used precision@k to assess d ifferent classiﬁers from a recommendation system’s perspective . In a practical scenario, it is unlikely that a user will go over all suggestions of sensitive data elements presented on their data dashboard. In such cases, a classiﬁer must sort their r ecommendations and minimize the number of false positives within top k recommendations. We report this value using precisio n@k. Precision@k is simply the pro portion of correct classiﬁcations within top k rec ommend a tions. A higher value of precision@k signiﬁes good quality of re c ommend a tions. Our ML models tried to predict whether a particular data ele ment will be perceived by the user as sensitive or not. Table4 shows the macro-averaged precision, recall, and F1 scores for all models. Across all models, XGB offered the best performance with an F1 score of 0.95, followed by LR and MLP, each of which achieved an F1 score of 0.91. The best performing baseline mode l was XGB-Class which ac hieved an F1 score o f 0.54. Our XGB model outperformed the bestperforming b aseline model by approximately 76%. Figure5 shows the PR c urves generated for all models. Once again, the XGB model performed the best, achieving a near-perfect PR-AUC value of 0.9894, followed by LR that achieved a PR-AUC value of 0.9283. The XGB model showed a sign iﬁca nt improvement over the XGB-Class baseline model (PR-AUC = 0.5452), outperforming it by approximately 81%. Figure 6 shows the precision@k curves generated fo r a ll models. Lo oking at the top 3 0 predictions, the XGB and RF Table 4: Macro-averaged Precision, Recall, F1-score for all models. T he highest values in each column are bo ldfaced. Figure 5: PR curves while classifying data elements (SVM , LR, RF, MLP, and XGB are evaluated ML models, whereas Random and XGB-Class are baseline models) models performed the best, achieving a perfect precision@30 value of 1. They were followed by MLP, w hich achieved a precision@30 value of 0.9 . Other models such as SVM and LR had relatively poor precision@30 values comparable to the precision@30 value of 0 .57 for the XGB-Class baseline model. To distinguish between the performance of XGB and RF models, we looked at their p recision@k values for large values of k. We observed a slight drop in performance while varying k from 1 to 500 for the RF model (precision@500 = 0.954), whereas the XGB model retained its performance (precision@500 = 1) even for larger values of k, hig hlighting the stability of XGB model (Figure 15 of Appendix C). Despite achieving a lower F1 score and PR-AUC value than models such as LR, the RF model offered better performance in the scenario where o nly a few data eleme nts should be presented to users. Although o ur be st-performing XGB model achieved a high F1-score of 95%, reso urceful organisations like Google might be able to further improve accuracy in real-world deployments with ad ditional labeled data. Figure 6: Precision@k curves while classifying data elements (SVM, LR, RF, MLP, and XGB are evaluated ML models, whereas Random and XGB-Class are baseline models) Table 5: Top 10 features as decided by the XGB model in decreasing order of importance Finally we analyzed the features that play e d the most important role in our p rediction task. Many of the important features are user-based: Table5 shows the top ten feature s identiﬁed b y our best perfo rming XGB classiﬁer, in decreasing order of importance. Three out of the top ﬁve features were user-based, w hich highlights tha t user details are crucial in predicting the perceived sensitivity of data elements. Five out of the top ten features were text-based, implying that the text content of data elements is also ce ntral to the p rediction task. This is in contrast with the result in Section5.5, where we d id not ﬁnd signiﬁcant differences in user p rivacy prefere nces across simple lexiconbased classes. We b elieve this contrast is be cause of using more involved textual features (e.g., LIWC, sentence embedding, sentiment). In this work, we present the ﬁrst study on understanding users’ privacy attitudes and preferenc es regarding data collection by GVA. Speciﬁcally, using a real-world data -driven approa c h, we unpacked users’ knowledge of the data collection pr actices of GVA. Previous work [25] has looked into general user perceptions and reactions towards the Google My Activity data dashboard. However, we, in contrast, focused on using real-world GVA-collected data elements to elicit speciﬁc user responses. We seek to understand whether such data dashboards actually provide utility in controlling data privacy through an 80-participant user study g rounded into ac tual GVA-collected da ta . Recent studies have paid increased attention to voice assistants on smart ho me speaker devices. Given the pervasiveness o f smartphones, ou r results show that smartphone voice assistants can collect data in a variety of scenarios different from stationary smart speaker devices. Thus, our work sheds light on the data-centric ecosystem of voice assistants, with GVA as our test case. Furthe rmore, in spite of using GVA data dashboards as our test case, many of our ﬁndings on assessing the efﬁcacy of data dashboards and improving their usability are generalizable to dashboards o f other voice assistants. A new direction towards usable data dashboards: Our results e stablish a deﬁnite need for better data dashboards while a cknowledging the utility of the curren t one. As a ﬁrst step, our results hint at the fact that users have just super ﬁcia l knowledge about the data collec tion and storage practices of GVA. Although data dashboards help to raise awareness about the total collected data by GVA, the huge amoun t of data does not help. Long-term users would need automated assistance to review more sensitive data elements. Thus, our results underline a need to make these data dashboards more usable by helping users uncover sensitive data elem ents. Our user feedback and accurate classiﬁcation results identify that machine-le arning based human-in-the-loop systems might signiﬁcantly help the cause. To that e nd, we identiﬁed the top ten most important features for this prediction task. In addition to text-ba sed and aud io-based features already available to the VA platform, our results highlight that user-based features can also play an importa nt role in identify ing sensitive content. We believe that a handful of these user-based features unavailable to the VA platform could be collected by directly ask ing the u sers a s pa rt of an initial setup process (while mentioning this will assist the users in ﬁnding their sensitive data elements). In fact, our second survey, which aimed to r a ise awareness about different types of data collected by GVA and stored by Google increased user awareness about GVA collected data. However, there is much left to explore in this direction, including the p resentation of these recommendations to the users and checking the efﬁcacy of interface nudges u sin g these recommendations. For instance, our HITL design to improve usability focused on assisting users in uncovering potentially sensitive elements. A potential future work is creating and evalua ting a query system in parallel to this recommender system. Such a system could assist users in sifting through the GVA-collected data efﬁciently and further improve th e usability of data dashboards. Thus, we strongly feel our work paves the way to build more usable data dashboards for better assisting users and takes a step forward to bringin g transp a rency to the data ecosy stem of voice assistants. We thank the anonymous reviewer s and our shepherd Camille Cobb for their valuable fee dback. We also thank Shalmoli Ghosh for her help with an earlier iteration of this work and Niloy Ganguly for th e discussion early in the pr oject. The experiments in this work were funded by Huawei Technologies India Private Limited via the ADUL project.