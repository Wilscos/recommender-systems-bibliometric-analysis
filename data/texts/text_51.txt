Modern deep neural networks (DNNs) have greatly facilitated the development of sequential recommender systems by achieving state-of-the-art recommendation performance on various sequential recommendation tasks. Given a sequence of interacted items, existing DNN-based sequential recommenders commonly embed each item into a unique vector to support subsequent computations of the user interest. However, due to the potentially large number of items, the over-parameterised item embedding matrix of a sequential recommender has become a memory bottleneck for ecient deployment in resource-constrained environments, e.g., smartphones and other edge devices. Furthermore, we observe that the widely-used multi-head self-attention, though being eective in modelling sequential dependencies among items, heavily relies on redundant attention units to fully capture both global and local item-item transition patterns within a sequence. In this paper, we introduce a novel lightweight self-attentive network (LSAN) for sequential recommendation. To aggressively compress the original embedding matrix, LSAN leverages the notion of compositional embeddings, where each item embedding is composed by merging a group of selected base embedding vectors derived from substantially smaller embedding matrices. Meanwhile, to account for the intrinsic dynamics of each item, we further propose a temporal context-aware embedding composition scheme. Besides, we develop an innovative twin-attention network that alleviates the redundancy of the traditional multi-head self-attention while retaining full capacity for capturing long- and short-term (i.e., global and local) item dependencies. Comprehensive experiments demonstrate that LSAN signicantly advances the accuracy and memory eciency of existing sequential recommenders. ACM Reference Format: Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight Self-Attentive Sequential Recommendation. In Proce edings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3459637.3482448 Modelling sequential user behaviours has received great attention in contemporary applications, such as e-commerce, online services, and smart transport [49]. Among these applications, sequential recommender systems (SRSs) have become a prominent solution to information overload on the web. The main goal of SRSs is to make a proactive recommendation on the next item a user may be interested in by mining the user’s recent preferences from the sequence of her/his interacted items. Early SRSs incorporate Markov chain-based models [7,10,33] to capture high-order sequential patterns based on Markov chain (MC), which essentially factorises a user-specic item-item transition tensor by considering rst-order Markov chain. However, these methods primarily learn the transition patterns based on the most recent item interactions, neglecting long-term (i.e., global) user preferences. With the revolution of deep neural networks (DNNs), various deep methods have been proposed for the sequential recommendation [21,28–31], especially recurrent neural network-based [3,4,6,22,53] sequential recommenders. Notably, the majority of state-of-the-art SRSs are latent factor models, where each item is mapped into a unique vector representation (a.k.a. embedding), and the item embedding is then used to calculate the sequential preferences of the target user. With the fast pace of digitisation and hardware revolution, there has been a recent surge of moving data analytics from cloud servers to edge devices [35] to ensure timeliness and privacy. As sequential recommendation involves frequent updates on a user’s behaviour records, developing lightweight SRSs appears to be an ongoing trend, because such on-device computation capability can prevent potential latency caused by communications with the cloud and eectively retains users’ personal data on their own devices. Due to the sheer volume of dierent items (e.g., Alibaba’s billion-scale item set [41]), the item embeddings in latent SRSs are the main source of memory consumption [42] rather than other parameters like weights and biases of DNNs. In this regard, recent studies on lightweight recommenders [5,23,24,34] are predominantly focused on compressing the originally large item embedding matrix to improve the memory eciency of recommenders. Their core idea of such compression is compositional embeddings, where a recommender consists of a small number (substantially smaller than the number of all items) of base embeddings, such that an item can be represented as a distinct combination of selected base embeddings. However, most compositional embedding-based recommenders are designed for static recommendation settings, where the recommendation results for each user are purely conditioned on the static user-item anity instead of her/his interest dynamics. As a common practice, the aforementioned lightweight recommenders usually combine compositional embeddings with o-theshelf deep recommendation modules (e.g., the DLRS in [34] and the DeepFM in [25]). Though a similar solution can be sought for lightweight SRSs by straightforwardly feeding compositional item embeddings into sequential DNNs (e.g., recurrent neural networks), an over-parameterised network structure will in fact defeat the purpose of a memory-ecient model. Also, the excessive computations may impede the timeliness of a model’s on-device inference. Hence, in addition to a lightly parameterised item embedding scheme, a lightweight SRS should also be able to thoroughly discover the temporal signals from all interacted items with a carefully designed, compact, yet eective sequence mining paradigm. To this end, we proposelightweightself-attentivenetwork (LSAN), a novel solution to memory-ecient sequential recommendation that simultaneously addresses those two key challenges. Specically, LSAN aggressively replaces the item embedding matrix with 𝑁base embedding matrices, each of which contains substantially fewer embedding vectors (i.e., base embeddings) than the total amount of items, i.e.,𝑁 ≪ |V |for item setV. Then, compositional item embeddings are generated by fusing𝑁base embeddings respectively selected from each base embedding matrix. To ensure the uniqueness of each composed item embedding, we design a context-aware temporal compositional embedding scheme, where base embeddings are located via a quotient-remainder [34] operation. Unlike traditional compositional item embeddings that stay xed regardless of any temporal information in a sequence, we propose to dynamically alter the generated item embedding according to sequence-specic contexts by attentively merging the base embeddings for each item. The rationale is that in a sequence, every item’s relevance is sensitive to factors like seasonal changes and adjacent items [20,43], hence we let LSAN account for such information when generating compositional item embeddings. In LSAN, we resort to self-attention for modelling the temporal patterns among the interacted items. However, though the self-attention [17,27,55] is widely acknowledged as a lightlyparameterised and eective approach for capturing sequential information in SRSs compared with the popular alternative – RNNs, our observation is that recommenders using self-attention can still incur parameter redundancy. For sequential recommendation tasks, the attention module should be able to capture both long- and short-term (i.e., global and local) preferences of a user. Unfortunately, recent studies [44,46] point out that self-attention tends to over-emphasise local relationships between adjacent items, making it dicult to learn the correlations between items that are far from each other in a sequence. Hence, existing recommenders commonly employ multi-head self-attention, so as to enhance the modelling capacity and acquire sucient global information. Such redundancy can be tolerated and may benet the recommendation accuracy when the computing resource allows, but it fails to meet the highly constrained deployment environment in the context of memoryecient SRSs. In light of this, we introduce a novel twin-attention paradigm in LSAN, where the global and local preference signals are separately captured via two specialised modules instead of a group of general attention units. As such, coupled with the dynamic compositional item embedding scheme, LSAN eectively learns local and global user preference signals for accurate sequential recommendation without the need for an excessively complex and large model. With the proposed LSAN, our main contributions to lightweight sequential recommendation are three-fold: •We devise a dynamic context-aware compositional embedding scheme, which largely decreases the memory footprint of item embedding matrix – the major consumer of memory space of SRSs – and ensures the uniqueness and dynamics of generated item embeddings at the same time. •We propose a novel twin-attention sequential framework, which specialises the learning of long- and short-term user preference signals via a dedicated self-attention and convolution operation, respectively. This facilitates explicit modelling of both global and local patterns while avoiding the redundancy of multi-head self-attention modules. •Extensive experiments are conducted on three benchmark datasets. The results demonstrate the advantageous eectiveness and memory eciency of LSAN against state-of-the-art baseline methods. LetV,Ube the sets of items and users, respectively. We useS= {𝑣, 𝑣, ..., 𝑣}to denote a sequence of𝑇chronologically ordered items that user𝑢 ∈ Uhas interacted with. Each item𝑣∈ Sis assigned an order index𝑖 = 1, 2, ...,𝑇which reects the position of an item in the sequence. To locate𝑣among the set of|V|items, we dene a functionindex(𝑣)that maps𝑣to a unique and xed global index1, 2, ..., |V|. Then, given a sequence of interactionsS, our goal is to compute a ranking list consisting of top𝐾items that 𝑢 is most likely to visit at the next time step 𝑇 + 1. In this section, we introduce our proposed memory-ecient sequential recommender, namely LSAN. LSAN consists of two main components: (1) a dynamic context-aware compositional embedding layer that enables a lightweight yet highly expressive item embedding paradigm; and (2) a twin-attention network that eectively learns global and local user preferences without the need for redundant multi-head self-attention modules. In what follows, we present the design of LSAN in detail. Recall that in a typical latent factor-based recommender system, each item𝑣is associated with a unique𝐷-dimensional embedding vector, which corresponds to the𝑣)-th row in an embedding table 𝑬 ∈ R. The memory complexity of maintaining such an embedding table isO(|V|𝐷), where the memory cost will become impractical for edge devices whenVis large-scale. To reduce the size of𝑬for better memory eciency, we replace𝑬with a set of𝑁base embedding tables denoted as {e𝑬,e𝑬, ...,e𝑬}, wheree𝑬∈ R for𝑛 = 1, 2, ..., 𝑁. Here,𝑚indicates the number of base embeddings in the𝑛-th base embedding tablee𝑬and𝑚≪ |V |. For each item, its compositional embedding is produced by rst selecting one base embedding vector from eache𝑬, and then attentively fusing all selected base embeddings into a single vector. It can be concluded that there areΠ𝑚dierent combinations of base embeddings. Thus, we only need to makeΠ𝑚⩾ |V|to ensure the uniqueness of constructed embeddings for each item. To guarantee that each item receives a distinct combination of base embeddings, we resort to the quotient-remainder trick [34] that does not introduce any additional learnable parameters. Particularly, taking the rst base embedding tablee𝑬as an example, the corresponding base embedding index𝑞(i.e., the row index ine𝑬) for item𝑣∈ Scan be computed by a remainder function over the base embedding table size 𝑚: Then, the rst base embeddinge𝒆for𝑣can be retrieved by a lookup operation one𝑬w.r.t. row index𝑞. Mathematically, let𝒇∈ R be the one-hot encoding of𝑣, then a hash matrix𝑹∈ R fore𝑬can be computed element-wise via: Therefore, the look-up operation of𝑣’s rst base embeddinge𝒆 can be mathematically formulated as: Analogously, for𝑛 = 2, 3, ..., 𝑁,𝑣’s hash matrix𝑹for the𝑛-th base embedding table can be generalised as: 𝑹=1 if 𝑗 mod 𝑚= index(𝑣) \ Π𝑚0 otherwise where the index in the𝑛-th base embedding table for item𝑣is determined by the resulting quotient from the prior base embedding tables, i.e.,index(𝑣)\Π𝑚. Then, we obtain the base embedding e𝒆from 𝑹via: Through the quotient-remainder trick, we now have acquired a set of base embeddings{e𝒆,e𝒆, ...,e𝒆}for item𝑣. Intuitively, a unied item embedding can be easily formed by an ensemble operation, such as element-wise addition/multiplication. However, such constructed embeddings are xed, and are insucient in capturing the intrinsic dynamics of an item’s properties. As pointed out by [20,43], learning context-aware temporal item embeddings is benecial for mining a user’s preferences from her/his interaction sequences. In order to bring such temporal contexts into the generated compositional item embeddings, we propose to attentively assign dierent weights to the selected base embeddings conditioned on the context around the target item. Specically, for each item𝑣∈ S(𝑖 = 1, 2, ...,𝑇), we have a context𝑟representing this item’s situation within the sequence. The construction of𝑟can be highly exible, where in our work, based on the side information shared by all of our experimental datasets (see Section 4), we dene 𝑟= (𝑐, 𝑐, time(𝑖))as a triplet of the categories of the previous and current items and the discrete time slot (i.e., every hour of a day). We denote𝑅 = {𝑟, 𝑟, 𝑟}be the set of unique context tuples. Each tuple𝑟 ∈ 𝑅is assigned with a one-hot vector. Then, we can map the one-hot encoding vector𝑟into a dense context embedding𝒓∈ R. Note that a padding label for the category information is adopted when𝑡 = 1. Under a given context𝒓, we can calculate an attention weight for each base embedding: whereSiLU(𝑥) = 𝑥 · sigmoid(𝑥)is an activation function that is an alternative to ReLU providing non-linearity to the model with faster convergence speed [9]. The attention weight is then used to compute the compositional embedding 𝒉for item 𝑣: Finally, to facilitate side information modelling, we inject the context embedding of the item𝑖(i.e.,𝒓) into the above computed compositional embedding ℎvia a non-linear operation: where[; ]is the concatenation operation andMLP(·) : 2𝐷 → 𝐷 denotes a multi-layer perceptron. For a sequence of𝑇interacted items, we can obtain an embedding matrix𝑯 = [˜𝒉;˜𝒉; ...;˜𝒉]∈ Rby sequentially stacking all compositional item embeddings. Self-attention has been a predominant approach in recent SRSs owing to its simplicity and capability of learning sequential dependencies among items. As discussed in Section 1, existing self-attentive sequential recommenders mostly deploy multiple attention heads in parallel to capture both global and local user preference signals from the sequence, resulting in unnecessary redundancy in both the network structure and parameter size. To alleviate such problem, we propose a twin-attention neural network to better capture the sequential information while maintaining the lightweight nature of LSAN. As depicted in Figure 1, it has two branches: a self-attention branch and a convolution branch specialised for global and local preference modelling, respectively. 3.2.1 Convolution Branch for Local Paerns. Dierent from the self-attention branch which attends to all items in a sequence, convolution operations have shown success in extracting local features for image recognition and text classication. Their strong capacity in extracting regional features makes them an ideal component for capturing the short-term preferences among items that co-occur in a short time period. As such, with the matrix𝑯 ∈ Rcarrying all𝑇item embeddings in the sequence, we perform 1D convolution over the embedding matrix. Assuming the sliding window size is𝐿 and the output size is𝐷, the standard convolution operation needs 𝐿𝐷trainable parameters. To decrease the parameter size, we resort to a lightweight version of convolution [45]. In particular, a depth-wise convolution operation is introduced, which applies a shared kernel of size𝐿for each channel (i.e., each item embedding dimension). This reduces the number of required parameters from O(𝐿𝐷)toO(𝐿𝐷). Mathematically, the𝑑-th element (𝑑 = 1, 2, ..., 𝐷) of𝑖-th embedding in the resulted output matrix𝑯∈ Rcan be formulated as: where𝑾∈ Ris the kernel, and⌈·⌉denotes the ceiling operation. It is worth noting that, each row in the resulted matrix𝑯 encodes𝑖-th item’s interaction with items closely surrounding it within the𝐿-sized sliding window, hence is a representation of all the local dependencies within the item sequence. 3.2.2 Self-aention Branch for Global Paerns. The rationale of coupling self-attention with convolution is that, by having a convolution branch dedicated to extracting local sequential patterns, the self-attention branch can now better specialise in learning global patterns, thus reducing the need for using an excessive amount of self-attention units for optimal performance. As for self-attention, it has become one of the most prevalent means in various natural language processing (NLP) and sequential tasks as it can eectively capture relationships among items regardless of their distances (i.e., multi-hop). However, the plain self-attention fails to preserve the inherent orders of items in the sequence, impeding its ecacy for sequential recommendation tasks [17]. In this sense, we make the self-attention branch order-aware. Firstly, we dene𝑇learnable position embeddings𝒑, 𝒑, ..., 𝒑∈ R, which are stacked into a matrix𝑷 ∈ R. Then, we fuse the positional information into the original item embeddings: where each item is essentially paired with its corresponding positional context in the sequence. After that, a scaled dot-product selfattention is applied to compute item representations𝑯∈ R by mining the long-range dependencies: where𝑸 = 𝑾e𝑯,𝑲 = 𝑾e𝑯and𝑽 = 𝑾e𝑯are transformed item representations that are projected into query, key and value spaces, respectively. 3.2.3 Enhancing Expressiveness with Parallelism. Similar to pure self-attention-based methods, one can employ more than one attention heads for both branches in the twin-attention. For simplicity, we assume the convolution and self-attention modules each have 𝐻heads in parallel. Then, the nal output of the twin-attention can be obtained by concatenating2𝐻learned representation matrices followed by : 𝑯= [𝑯; ...; 𝑯; 𝑯; ...; 𝑯 where𝑯∈ Ris the nal output. Note that in LSAN, it is not strictly necessary to set𝐻 > 1as the design of twin-attention can already facilitate comprehensively learning both global and local user preferences. One benet of such parallelism over the traditional multi-head attention is that, with the same total amount of2𝐻attention heads, twin-attention consumes fewer parameters (i.e.,O(𝐻 (𝐿𝐷 + 3𝐷))in twin attention versusO(6𝐻𝐷)in selfattention,𝐿 ≪ 𝐷) and is able to yield stronger performance, as will be illustrated in Section 4. 3.3.1 Point-wise Feed-forward Network. To further enhance the representation capacity of LSAN, we incorporate non-linearity into the output of the twin-attention. Specically, we employ a pointwise feed-forward network (FFN) as follows: where𝑾∈ R, 𝑾∈ Rare weight matrices, 𝒃∈ R, 𝒃∈ Rare bias vectors, andb𝑯∈ 𝑠𝑅is the output of the point-wise FFN. Meanwhile,GeLU(·)denotes the Gaussian error linear unit [8,12] that we use for non-linearity. 3.3.2 Generating Rankings. With the nal representationb𝑯 that encodes both the user’s long- and short-term interests, we generate the rankings for all items to facilitate top-𝐾recommendation. This is achieved by estimating the likelihood of having user𝑢interact with each item, which is formulated as learning a |V|-dimensional probability distribution by: where𝑾∈ Rand𝒃∈ Rare the learnable weight matrix and bias vector, respectively. By sorting each𝑣according to its corresponding probability scoreb𝑦∈ byin a descending order, we will be able to truncate𝐾items from the top of the list as our recommendation results. With the estimated probability vectorby, we then employ crossentropy loss function to quantify the error of predicting the next item for LSAN: where𝑠 ≤ 𝑆is the index of training samples,yis the one-hot vector representing the ground truth of the next item, andΨis the set of all trainable parameters under the𝐿2regularisation term with coecient 𝜆. In this section, we evaluate the recommendation eectiveness and memory eciency of our LSAN model for sequential recommendation. Specically, we rst analyze the performance of LSAN by comparing it with state-of-the-art sequential recommenders from both accuracy and model size perspectives. After that, we further investigate the impact of the key components and hyperparameters in LSAN. Table 1: Statistics of experimental datasets. Avg. Int. per Item 16.4038 14.0554 16.1430 14.7235 4.1.1 Dataset. We conduct experiments on four commonly-used benchmark datasets. The statistical details of all datasets after preprocessing are reported in Table 1, including the number of users, items, interactions, categories, average interactions per user (Avg. Int./User) and average interactions per item (Avg. Int./Item). All the experimental datasets are highly sparse. We briey introduce their properties below. •Beauty, Sports and Toys: These three datasets are provided by [11], which are collected from Amazon and contain product reviews and abundant metadata. •Yelp: The dataset contains user check-in data provided by Yelp, where businesses are viewed as items. The data we use for our experiments span across 2019. For each dataset, we group interactions by user IDs, and then generate one chronological item sequence for each user. The inactive users and unpopular items with less than 5 interactions are discarded. We adopt the leave-one-out evaluation approach, i.e., for each user interaction sequence, we use the last item as the test instance, the second last item as a validation sample, and the remaining items for training. We choose Hit Ratio at Rank𝐾(HR@𝐾) and Normalised Discounted Cumulative Gain at Rank𝐾(nDCG@𝐾) on top-𝐾ranked items, which are widely used in recommender systems [3,17] for top-𝐾performance and overall ranking performance evaluation. We report the performance results on HR@{5, 10, 20} and nDCG@{5, 10, 20}, respectively. As suggested by [19], to eliminate potential biases, we rank each ground truth item along with the whole item set (i.e.,V) to compute all metrics, and report the average scores over all users. We compare LSAN with the most representative, state-of-the-art sequential recommendation methods below: •FPMC [33]: It is a combination of matrix factorisation with Markov chain, which can simultaneously capture sequential information and long-term user preferences. •GRU4Rec [13]: It is an RNN-based sequential recommender with session-wise mini-batch training strategy. The model is optimised by a pair-wise ranking loss. •Caser [39]: This is a CNN-based method that models highorder Markov-chain probability by performing convolutional operations on the item embedding matrix. •SASRec [17]: It is a next-item sequential recommendation method based on the Transformer architecture, which employs multi-head self-attention mechanism to explore implicit user interactions. •BERT4Rec [37]: It is an improvement of SASRec, which contains an additional Cloze objective and bidirectional selfattention structure. LSAN is implemented using PyTorch with Nvidia GTX 2080 Ti. In LSAN, we set the dimension size𝐷to 128, CNN kernel size𝐿to 5, the number of attention heads𝐻to 2 for each branch, and the number of stacked twin-attention layers to 1 on all datasets. All the trainable parameters in our model are optimised using Adam optimiser [18] with the batch size of 256, learning rate of 0.001 and 𝐿2regularisation strength𝜆of1𝑒 − 5. For a fair comparison on accuracy and model size, we apply the same dimension size𝐷for all methods’ embeddings. Note that in LSAN, altering either𝑁or𝑚 can lead to dierent compression rates on the original embedding table. Hence, we x𝑁 = 2and vary𝑚(𝑚=) for the ease of hyper-parameter tuning. In Section 4.5, we will rst test LSAN’s performance with𝑚= 2, while we will further discuss how LSAN performs when we compress the model size more aggressively with a larger 𝑚in Section 4.6. We summarise the results of all models on four benchmark datasets in Table 2. From the table, we can draw the following observations: Among all sequential baseline methods, FPMC receives the worst results over all evaluation metrics. This is mainly because FPMC only exploits rst-order dependencies where the higher-order relationships among items are neglected. In comparison, Caser utilises convolutional kernels to extract k-hop adjacent item dependencies, thus, obtaining better performance results than FPMC. However, since the sliding window size of Caser could only cover a small number of items, which lacks the ability on handling sparse datasets with long sequences, e.g., Yelp. As a result, it can be observed that the RNN-based method, GRU4Rec, has better performance than Caser on those datasets. SASRec and BERT4Rec are state-of-the-art self-attentive approaches, which have clear margins with the other baseline methods. This shows the superiority of the self-attention architecture in sequential behaviour modelling. It is worth noting that BERT4Rec does not outperform SASRec under our evaluation settings (i.e., full test sample set), which is dierent from their original paper. We think this maybe because there may exist improper biases in the naive negative sampled test sets according to their original implementation details. We present the performance results and relative improvements over the best baseline of two versions of our proposed LSAN in Table 2, i.e., LSANand LSAN. The only dierence between them is that LSAN is our proposed model, while LSANis trained using a full-sized embedding table. From the results, using either full-sized embedding table or our proposed dynamic context-aware compositional embedding can surpass the best baseline method SASRec with signicant margins on most evaluation metrics. This proves the eectiveness of our twin-attention structured model on handling long-term and short-term user preferences. Moreover, though a large number of parameters are reduced via our compositional embeddings, LSAN performs even much better than LSANon all datasets. We believe that by incorporating seasonal and categorical factors (i.e., temporal dynamics) within the model can largely enhance the expressiveness of LSAN with fewer trainable parameters. In addition, an obvious model size reduction can be observed from both LSAN and LSAN. We provide a more detailed discussion on memory usage in the following section. In this section, we study how the compression rate aects the model performance. Intuitively, the model will have worse performance when the compression rate𝑚increases. We compare the model size and next-item recommendation performance of LSAN at dierent compression rates with the best baseline method, SASRec. From the results shown in Table 3, we can observe that when𝑚= 2, our LSAN model surpasses SASRec over most metrics with only around 60% of parameters in SASRec on all datasets. When𝑚increases to 3 (i.e., around 45% of SASRec parameters), LSAN still produce comparable recommendation results on Beauty, Toys and Yelp datasets, which further approves the outstanding memory eciency and recommendation eectiveness of our model. However, when𝑚 goes up greater than 3, we can see a clear performance drop. It is understandable that when the model contains a very small number of parameters, the model is under-parameterised resulting in the failure of capturing meaningful information from items. Comparing with the existing self-attentive methods, LSAN mainly contains two novel components: dynamic context-aware compositional embeddings and twin-attention layers. To verify the effectiveness of each component, we conduct ablation studies on all benchmark datasets. Table 4 shows the performance of our default model and its two degraded variants (𝐷 = 128). We give a brief description and detailed analysis of each variant in the following: Specically, we compare LSAN with the following degraded variants: •LSAN: This variant removes the dynamic compositional embedding component. The embedding part becomes exactly the same as the QR embedding introduced in [34]. More concretely, we create this variant by modifying Eq.Í (7) to𝒉=e𝒆. We can see a signicant performance drop on most datasets when the temporal information is removed from the composited embeddings. This suggests that respecting temporal dynamics is of great importance in user preference modelling. •LSAN: This variant replaces the twin-attention layers with self-attention layers. There is a clear performance drop on three datasets when only self-attention is applied. This reveals that the self-attention does not have sucient capability to uncover both long-term and short-term user preferences with limited attention heads. We further examine the impact of four various hyper-parameters, including dimension size𝐷, partition size𝑚, number of attention heads𝐻, and number of stacked twin-attention layers. For each test, we vary the value of one hyper-parameter, while keep the others be the optimal settings. The results are demonstrated in Figure 2. 4.8.1 Impact of Dimension Size. the dimension of The value of dimension size is examined from 16 to 256. We can observe that a small dimension size (i.e., 16) cannot preserve sucient latent information of items for user sequential behaviour modelling. The model performance increases steadily when the dimension size Table 2: Comparison on sequential recommendation accuracy and model sizes. In each row, the best and second best results are highlighted in boldface and underlined, respectively. The parameter size of each model is obtained when 𝐷 = 128. Yelp grows up on all datasets. However, we also nd that a larger hidden dimensionality (e.g., 256) may not contribute to better performance, which may be mainly caused by the over-tting problem especially when the data is extremely sparse. This also proved by the performance results on Sports dataset that has highest sparsity among all datasets: LSAN becomes sensitive to the choice of appropriate dimensionality, i.e.,𝐷 = 64ts the model best. On the other datasets, LSAN reaches the best performance when 𝐷 = 128. 4.8.2 Impact of Aention Head Number. The results in the second graph in Figure 2 show that more attention heads contribute better to the model performance. It is worth noting that the actual number of our twin-attention heads are doubled. Thus, LSAN model reaches best performance on most datasets, when it is equipped with 2 selfattention heads and 2 convolution heads. Table 3: A comparison of performance results and number of model parameters using dierent embedding compression rate 𝑚on four datasets. Datasets Metrics SASRec LSAN(2x) LSAN(3x) LSAN(4x) LSAN(5x) Table 4: Ablation study of dierent variants on four datasets. 4.8.3 Impact of Twin-aention Layer Number. LSAN receives the best performance with 1-layer architecture. Dierent from SASRec and BERT4Rec, which usually require 2 or 3 layers to fully capture various-order item dependencies. With the help of our proposed twin-attention structure, our model is capable of capturing various sequential information with only one layer. We also observe that the model performance drops obviously when more twin-attention layers are stacked on all dataset. This may be because of the over-tting problem when LSAN is launched on extremely sparse datasets. Recall that in Section 1, we argue that self-attention-based models put too much emphasise on local patterns, which is the motivation of our design on twin-attention architecture. To more intuitively demonstrate how twin-attention performs eective user behaviour modelling, we examine three randomly selected user interaction sequence samples and calculate the averaged attention weights on the last 10 items from each sample over all attention heads. The heat maps of the normalised attention weights from LASN and SASRec are illustrated in Figure 3. From the gure, it can be easily distinguished that the attention module in LSAN focus on item Figure 3: Heat maps of average attention weights on Yelp dataset. (a-c) are from SASRec, while (d-f) are from LSAN’s self-attention branch. global patterns (i.e., no diagonal pattern shown in the gure), thus leaving local pattern modelling to the convolution branch in our twin attention. In comparison, the heat maps of SASRec show a clear concentration on local patterns, which fails to model global patterns eectively. Early work on sequential recommendation is mainly based on Markov chains. Rendle et al. [33] propose FPMC that combines the power of matrix factorisation and Markov-chain to learn an item-toitem transition probability matrix, which is then used to make next item prediction based on the user’s latest interaction. After that, several models built upon high-order Markov chains are introduced [10,11]. The advances in recurrent neural networks (RNNs) have brought signicant performance boost in sequential recommendation. Hidasi et al. [13] propose a sequential recommender based on RNNs, which employs gated recurrent units (GRUs) to extract the high-order sequential information from the user’s interaction history. Subsequent RNN-based approaches leverage attention networks [38], memory networks [14,15], copy mechanism [32], or reinforcement learning scheme [47], to improve the eectiveness of sequential user interest modelling. Another line of work [39] treats a sequence of item embeddings as a feature map of an image, and performs convolution operation upon embeddings to capture local dependencies among items. Owing to the promising capability in sequential data modelling, attention mechanism has become popular and widely studied in various domains, such as text classication [48] and machine translation [1]. However, these approaches treat attention mechanism as an additional module upon the RNN backbone, resulting in higher computational cost. To solve this issue, a new attention architecture, transformer, is proposed in [8,40]. Its main building block is multi-head self-attention, which allows faster parallel computation and achieves state-of-the-art performance in a wide range of sequence modelling tasks. In light of self-attention, Kang et al. [17] propose a self-attentive framework named SASRec, which adopts a multi-head self-attention layer to capture the user’s sequential behaviours and achieves state-of-the-art performance on various sequential datasets. Later on, Sun et al. [37] encode sequence data in bidirectional manner by introducing BERT4Rec together with a masked training scheme. However, all aforementioned sequential recommendation methods suer from high memory cost in two aspects, which are infeasible for on-device applications. First, the large item embedding table brings high memory complexity. Second, as discussed in [46], the multi-head self-attention architecture tends to pay more attention on local dependencies, resulting in weak global preference modelling. In contrast, our proposed LSAN largely reduces the memory cost from the embedding table by a dynamic compositional embedding scheme. Besides, LSAN eectively learns global and local dependencies by a novel twinattention, where the heavy redundancy in traditional self-attentive recommenders are resolved by two specialised branches for longand short-range pattern mining. DNN-based methods have demonstrated strong capability in various recommendation tasks. However, with the rapid development of edge devices, there has been an increasing demand on adopting DNN-based models on mobile phones and even smaller edge devices for stability and reliability. In this line of research, the methodologies can be roughly categorised into four types: pruning, quantisation, knowledge distillation, and compositional embedding. Network pruning approaches manage to reduce the overtting parameters by discarding unnecessary ones from the neural model. Zhou et al. [54] introduce a group-sparse regularisation upon CNN kernel to produce a compact version without losing accuracy. However, most pruning methods require more iterations to reach convergence leading to extreme time cost. The second line of work aim to create one or more codebooks for a group of similar item representations. Jégou et al. [16] propose to decompose the item representation space into multiple subspaces, and then a codebook of each subspace can be obtained by clustering items in each subspace. The recent work, LightRec [23], develops a recurrent composite embedding encoder to learn diversied codebooks in a recursive manner. However, the learning of codebooks is independent of learning the item representations. Thus, the model cannot be trained end-to-end. Recently, knowledge distillation has gained popularity due to its high adaptivity to various complex models. This line of work primarily trains a large complex teacher network at the rst place, and subsequently utilises soft labels obtained from the teacher model to train a lightweight student model. Wang et al. [42] devise a tensor-train decomposed lightweight RNN model, and train the model using a well-tuned state-of-the-art teacher model via knowledge distillation for next POI recommendation. With the observation that the embedding matrices in recommender systems are the major source of memory consumption, some recent studies resort to embedding compression. A number of studies [2,26,36,50–52] introduce the idea of converting a continuousvalued embedding vector to a discrete code, where each bit refers to the learned index of a base embedding table. However, this still requires the model to store extra discrete code for each item. To solve the limitation, Shi et al. [34] propose a quotient-remainder indexing technique, which is able to obtain a unique set of base embeddings without allocating extra embedding space. Nevertheless, all the above-mentioned embedding compression work is designed for static recommendation scenario, which neglects the dynamics of user interests in sequential recommendation. To address this issue, we design a context-aware temporal compositional embedding scheme that incorporates temporal information by attentively merging base embeddings for each item. As such, our proposed LSAN is capable of preserving the temporal dynamics and optimising memory eciency simultaneously. In this paper, we introduce a lightweight twin-attention sequential recommender named LSAN, where two parallel branches are respectively specialised for short-term and long-term user preference modelling. To overcome the common bottleneck of large memory cost in existing DNN-based sequential recommender, we introduce temporal context-aware compositional embedding scheme, which largely reduces the memory cost and preserves intrinsic temporal dynamics of sequential data. Extensive experiments conducted on four real-world datasets clearly demonstrate the eectiveness and eciency of our proposed model. This work is partially supported by the Australian Research Council under the streams of Discovery Project (No. DP190101985), Future Fellowship (No. FT210100624), Centre of Excellence (No. CE200100025), and Industry Transformation Training Centre (No. IC200100022).