User interest exploration is an important and challenging topic in recommender systems, which alleviates the closed-loop eects between recommendation models and user-item interactions. Contextual bandit (CB) algorithms strive to make a good trade-o between exploration and exploitation so that users’ potential interests have chances to expose. However, classical CB algorithms can only be applied to a small, sampled item set (usually hundreds), which forces the typical applications in recommender systems limited to candidate post-ranking, homepage top item ranking, ad creative selection, or online model selection (A/B test). In this paper, we introduce two simple but eective hierarchical CB algorithms to make a classical CB model (such as LinUCB and Thompson Sampling) capable to explore users’ interest in the entire item space without limiting to a small item set. We rst construct a hierarchy item tree via a bottom-up clustering algorithm to organize items in a coarse-to-ne manner. Then we propose a hierarchical CB (HCB) algorithm to explore users’ interest on the hierarchy tree. HCB takes the exploration problem as a series of decisionmaking processes, where the goal is to nd a path from the root to a leaf node, and the feedback will be back-propagated to all the nodes in the path. We further propose a progressive hierarchical CB (pHCB) algorithm, which progressively extends visible nodes which reach a condence level for exploration, to avoid misleading actions on upper-level nodes in the sequential decision-making process. Extensive experiments on two public recommendation datasets demonstrate the eectiveness and exibility of our methods. • Information systems → Recommender systems;• Computing methodologies → Sequential decision making. ACM Reference Format: Yu Song, Jianxun Lian, Shuai Sun, Hong Huang, Yu Li, Hai Jin, and Xing Xie. 2018. Show Me the Whole World: Towards Entire Item Space Exploration for Interactive Personalized Recommendations. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY . ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445.1122456 Recommender systems help users to easily nd their favorite items from massive candidates. Typically, recommender models, such as collaborative ltering [15] and DeepFM [9], exploit users’ historical behaviors to learn users’ preference for future recommendations. Recommender systems with only exploitation models usually suer from closed-loop eects [11]: users mostly only interact with the items recommended by the system; the system further consolidates users’ proles with their interacted items recommended by the deployed model. Therefore, as time goes on, the system will be biased to a small, exposed set of interests for each user and keep recommending a limited range of items to a same user. Contextual multi-armed bandit algorithms, such as LinUCB [16], are classical methods that leverage side information to provide a good trade-o between exploration and exploitation, so that the closed-loop eect can be alleviated. Items are treated as arms and the recommender model is treated as an agent. Basically, at each round, the agent chooses one arm which has the biggest potential from𝐾arm candidates, then receives a corresponding reward based on user-item interaction. The goal is to maximize the cumulative reward over𝑇rounds. However, these algorithms hold a premise that𝐾is small, so enumerating all arm candidates’ scores and pick up the best one is feasible. The premise is true for a few scenarios where the candidates are naturally small, for example, homepage breaking news ranking, ads creative ranking and online model selection. In the scenario of general recommender systems, to fully explore users’ potential interests and truly alleviate the closed-loop eect, the arms candidates are the entire item repository, which usually contains millions or even billions of items. Classical bandit methods become infeasible due to the high computation cost of enumerating every one of the arms. To address the challenge, we rst propose a generic hierarchical contextual bandit (HCB) algorithm to eciently explore the interests of users for large-scale recommendation scenarios. Tree structure are widely employed to partition the search space to reduce the computational cost [12,26,40,41]. HCB uses a tree structure as the index for coarse-to-ne retrieval. For example, in e-commerce scenario, “Apparel > Clothing > Women’s Clothing > Dresses” is a path from general apparel to women’s dresses. Instead of using the category taxonomy of items, we utilize a bottom-up clustering method on item embeddings to organize items as a hierarchy tree, on which each node contains a group of semantically similar items. As a result, the number of items associated to each node on the hierarchy tree can be balanced, and users’ collaborative behaviors (such as co-click relations) can be encoded to form the hierarchy tree. HCB leverages the hierarchical information and turns the interest exploration problem into a series of decision-making problems. Starting from the root of the tree, on each non-leaf node, HCB performs a bandit algorithm among the children arms to choose a child node until a leaf node is reached. Afterward, another bandit algorithm is responsible for recommending an item from the leaf node to the user and collect her feedback as reward. The reward will be back-propagated along the path to adjust the estimation of users’ interest towards the hierarchy tree. The process of HCB is like a depth-rst search (DFS) idea. However, selecting a path in this DFS manner may cause new uncertainties, especially for a deep tree. First, if the selection of the parent node is misleading, all the subsequent choices will be impacted, which we call the error propagation. Second, since user interests are usually diverse, it is possible that the user is interested in many child nodes located in dierent parts of the tree. Therefore, we further propose a progressive HCB (pHCB) algorithm to reduce uncertainties and enhance the capacity of recommendation. Like the process of breadth-rst search (BFS), pHCB explores items in an adaptive top-down manner. On the one hand, it gradually maintains a limited number of nodes as a receptive eld. If one node has been explored multiple times and the user’s interest on this node has been veried, the node’s children nodes will be included to the receptive led while the current node will be removed. On the other hand, pHCB learns user interests of dierent aspects by performing a bandit algorithm with visible nodes in the receptive eld as arms. Consequently, the pHCB avoids greedily selecting only one node at each level to improve the HCB. To summarize, we make the following contributions: •We highlight the importance of exploring users’ interests in the entire item space to truly alleviate the closed-loop eect in personalized recommender systems. To the best of our knowledge, it is the rst attempt to implement CB models on millions of items. •Two simple yet eective algorithms, i.e. HCB and pHCB, are proposed to explore potential interests of users eciently through a hierarchy item tree. •We conduct experiments on two large-scale recommendation datasets. Results show the superiority of HCB and pHCB over baselines, as well as the exibility to integrate with dierent exploration methods such as LinUCB, Thompson Sampling and𝜖greedy. In addition, we design an experiment to verify that thanks to the exploring mechanism, both HCB and pHCB can eectively alleviate the closed-loop eects in recommender systems and learn better user proles in the long term. It is the rst work to study entire space user interest exploration. Our work is relevant to two lines of research, and we will review them separately. Contextual bandit algorithms aims to seek a balance between exploration and exploitation, which have been used in several applications, such as recommender systems [21], dynamic pricing [22], quantitative nance [30] and so on. [4] reviews the existing practical applications of contextual bandit algorithms. By assuming the payo model is linear, LinUCB [16] and Thompson Sampling [2] and two representative methods for solving contextual bandit problems. Beyond them, a variety of algorithms have been proposed to optimize the performance or learning speed. For example, ConUCB [39] introduces conversations between the agent and users to ask whether the user is interested in a certain topic occasionally. HATCH [38] considers the resource consumption of exploration and proposes a strategy to conduct bandit exploration with budget limitation. SMAB [6] considers two aspects, one is to maximize the cumulative rewards and the other is to decide how many arms to be pulled so as to reduce the exploration cost. GRC [36] develops a graph regularized cross model to leverage the non-linearity of neural networks for better estimating the rewards. Dierent from them, our work commits to eciently explore user interests in the entire space, rather than from a small subset of items. In the past few years, cluster-of-bandit algorithms have attracted the attention of some scholars. Generally, cluster-of-bandit algorithms aim to model the dependency since the items or users are always related to each other. As a result, cluster-of-bandit algorithms achieve better cumulative rewards than traditional contextual bandit algorithms due to knowledge sharing. For example, CLUB [8], DYNUCB [23], CAB [7] and COFIBA [17] assign users with similar interests into a same subset to make decisions together, thus it make contributions to accelerate the learning speed. Dierent from them, this paper focus on modeling the item dependency. ICTRTS and ICTRUCB [34] explicitly model the item dependencies via clustering 𝐶ℎ (𝑛) The set of child nodes of node 𝑛 𝑃𝑎 (𝑛) The parent node of node 𝑛 𝑿The (static) embedding features of arm 𝑎, 𝑿∈ R 𝑖(𝑡) The selected arm by policy 𝜋 at the 𝑡-th round 𝑟(𝑡) Reward of policy 𝜋 at the 𝑡-th round 𝜽, 𝜽Learnable parameter of user𝑢. A superscript indicates of arms, but they are only designed for context-free bandits. Similarly, [25] uses a taxonomy structure to exploit arm dependencies with context-free bandits. Considering that context-free bandits cannot utilize the abundant side information for making decisions, their exploration ability has yet to be improved. HMAB [33] leverages a tree-structured hierarchy constructed by domain experts to design a hierarchical multi-armed bandit algorithm for online IT ticket automation recommendation. However, domain knowledge is hard to collect and HMAB can not be applied to large-scale recommender systems because it needs to traverse all the paths in the tree. Moreover, HMAB aims to learn latent parameters for the nodes in the hierarchy tree, which is totally dierent from our goal of exploring users’ latent interests. Distributed bandit algorithms, such as DCCB [14] and DistCLUB [20], aim to speed up the computation by distributing the workloads in parallel. However, these methods do not address the issue of searching from tremendous items, the computational cost is still too expensive for responding users’ requests in an online manner (for example, how to response 100 users’ concurrent requests within 10 milliseconds in a scenario involving one million items). In summary, compared with existing cluster-of-bandit algorithms, our HCB and pHCB algorithms leverage a bottom-up clustering method to build a hierarchical tree of items, then explore users’ potential interests in the entire space of items based on the item hierarchy. We start by introducing the multi-armed bandit algorithms and the motivations of this paper. For better readability, we summarize most of the notations used throughout the paper in Table 1. The recommender system is regarded as an agent, where there are 𝑀users and𝑁items. At each round𝑡 =1,2, ·· · ,𝑇of interactions, given a user𝑢, the agent recommends an item𝑖(𝑡)to the user according a policy𝜋. Then the agent receives a feedback𝑟(𝑡)from the user, for example, if the user clicks on the item𝑖(𝑡),𝑟(𝑡)is 1 and otherwise it is 0. The optimal policy is denoted by𝜋. The goal is to learn a good policy𝜋, so that the cumulative regret over 𝑇 rounds, which is dened as below, is minimized: In practice, due to the absence of the optimal policy𝜋, we maximizeÍ the cumulative reward𝑬 [𝑟(𝑡)]instead, because maximizing cumulative reward equals to minimizing cumulative regret [17,33, 37]. At the core of bandit algorithms is to nd an optimal trade-o between exploitation (to recommend fully based on user proles learned from user interaction history) and exploration (nd out the new items which user may potentially love better), so that users diverse new interests have a certain chance to expose, meanwhile the system won’t waste too many resources on items that users are not interested in. Let’s consider the (user-centric) LinUCB [16] algorithm. Each item is regarded as an arm. At𝑡-th round, when receiving a user visit request, the agent selects an arm 𝑎(𝑡) by: The policy𝜋of LinUCB is a linear function between the feature vector𝒙and user hidden parameter𝜃, where the estimated reward is𝑅(𝑡) = 𝜽𝒙+𝜂,𝜂is a Gaussian random variable representing environmental noise, whose mean is zero and variance is𝜎≤1, The upper bound𝐶(𝑡)measures the uncertainty of the reward estimation. The key point lies in how to determine the parameter 𝜽and the upper bound 𝐶(𝑡). With LinUCB, we have: where𝑫∈ Ris the matrix of interacted arms’ features up to time𝑡,𝛼is a hyper-parameter to control the probability that the bound𝐶(𝑡)holds,𝒓∈ Ris the user response vector up to time𝑡. However, as revealed in Eq.(2), LinUCB needs to enumerate and calculate the score for every arm and then select the best one. In a modern recommender system, the number of items is usually very large (millions or even billions), which makes it impossible to calculate scores for all item. Thus, in the research community, a typical setting for existing literature is to randomly sample a small number𝐾(such as 50) arms from the entire𝑁arms at time 𝑡, and perform LinUCB on this small arm setA; in industry, the bandit algorithms can only be applied to scenarios whose candidate pool is small, such as post-ranking stage of a recommender system, homepage most popular item ranking, ad creative ranking, etc. We argue that in order to fully explore users’ potential interest, it is better to place the bandit module in the item retrieval stage (aka the recall stage) of a recommender system, where the candidate pool is the entire item set. Otherwise, in the post-ranking stage of a recommender system, the candidates are actually proposed by recommendation models and are strongly related to users’ past behaviors. Thus, it is less meaningful to explore users’ interest in the latter stages of a recommender system. To fully alleviate the closed-loop eect, in this paper, we advocate to explore users’ interest in the entire space of item repository. However, to the best of our knowledge, there is no work studying how to make the bandit algorithm like LinUCB t for a large candidate set. To address the challenge, we propose to use a tree structure to partition the entire item space into multiple sub-spaces and build the hierarchical dependencies among items, to accelerate the exploration. Formally, we dene the Framework 1: Framework1.Tree-based ExplorationThe entire item set can be organized as a hierarchical tree structureH, where nodes are linked to a subset of items that share some common topics or user interests, and nodes moving from top to bottom reects the topics/interests partition being coarse-to-ne. During the tree-based exploration, we will rst select a node according to some mechanism, then select an item from the candidates linked to this node. The user fee dback on the selected item will not only update the item-wise user preference estimation, but also update the node-wise user preference estimation along the hierarchical path. The tree structure plays a signicant role in designing hierarchical bandit algorithms. Item category taxonomy can serve as the hierarchy tree. However, due to the imbalanced number of items under dierent category and lack of leveraging of users’ collective behaviors, simply using the category taxonomy may lead to suboptimal performance, which is veried in Section 5. In view of this, we rst learn item embeddings based on item content and user co-click behaviors, then design a bottom-up clustering method based on K-Means clustering algorithm [18] to form a hierarchy tree for modeling dependencies among items. Specically, to construct a tree structure with𝐿levels, at rst,𝑁 items are clustered into𝑘dierent subsets based on the similarity of item embeddings. We treat each subset as a new node on the tree, with an embedding vector being the average of all item embeddings belonging to this node. Afterward, these𝑘nodes will be further clustered into𝑘dierent subsets using K-Means and each subset will be treated as a new node on the tree, forming a parent-children relation. This step will be repeated several times until the depth of the tree structure researches𝐿. As a result, the constructed tree structure, denoted byH, contains{𝑘, 𝑘, 𝑘, ·· · , 𝑘}nodes at each level, where𝑘=1 because only a root node appears at the rst level. Intuitively, items within the same node are more similar to each other, thus the clustering results reect the dependencies among items. InH, only the root node does not have parent node, and leaf nodes have no children nodes. In this section, we introduce the proposed hierarchical contextual bandit (HCB) algorithm, which empowers a base bandit model to explore over the entire space of item repository. Our algorithm can be generalized to dierent bandit models, without loss of generality, we take LinUCB as the base model to explain the algorithm for clarity. There are two types of arms: nodes on the hierarchy treeH and items mounted to the leaf nodes. Each node onHrepresents a certain group of items. The feature vector of a leaf node is the average pooling of items mounted to it, and a non-leaf node’s feature vector is the average pooling of its children nodes’ feature vectors. The HCB algorithm makes decisions sequentially, starting from the root node to a leaf node. At any non-leaf node𝑛(𝑡)at𝑙-th level, the policy𝜋selects one of the child nodes from𝐶ℎ(𝑛(𝑡)) by assuming the expected reward of an arm is linear in its feature vector, which is𝜽𝑿, and𝜽is the latent parameters of a given user𝑢towards the nodes at level𝑙,𝑫∈Ris the matrix comprised of interacted items at𝑙-th level, each row of𝑫 represents an item’s feature vector. Applying ridge regression to the training samples to estimate the coecients, we have: Where𝑰is an identity matrix and𝒓is the vector of historical rewards at node level𝑙. LinUCB also considers condence interval to better estimate the arm payo. Let𝑨= 𝑫𝑫+𝑰. According to [32], with probability 1 − 𝛿, the upper bound is: for any𝛿 >0 and𝛼 =1+ln(2/𝛿)/2. In this way, the LinUCB algorithm tends to select an arm with: If policy𝜋recommends𝑖(𝑡)to a given user and receives the reward𝑟(𝑡), similar as [40], then each node on𝑃𝑎𝑡ℎ(𝑟𝑜𝑜𝑡 → 𝑛(𝑡))also receives the same reward𝑟(𝑡). Therefore, the rewards of all selected nodes can be obtained, we can update the learnable parameters{𝜽,𝜽,𝜽,···,𝜽}at each level (where𝜽 means the parameter towards item arms, the other𝜽means parameters towards node arms), which can be formulated as: Where𝑨and𝒃are initialized as𝑑-dimensional identity matrix and zero vector respectively.𝑿is the contextual embedding of the selected node at 𝑙-th level. The pseudo-code of HCB is provided in Algorithm 1. To illustrate HCB, we oer a toy example shown in Figure 1. It has three layers in the hierarchy tree. The agent makes three decisions sequentially, and nally select the path {𝐴,𝐶, 𝐼, 𝑃}. Then the agent will launch another bandit selection among the items mounted to the leaf node 𝑃. The reward on the selected item will impact the parameter estimation on the hierarchy tree{𝜽,𝜽,𝜽,𝜽}, by updating the reward history 𝒓and interaction history 𝑫. Output: The policy 𝜋. node to user 𝑢 with Eq. (2); Figure 1: An illustration of HCB. The policy selects a path { A, C, I, P } from root to a certain leaf node. The HCB learns the interests of each user via a sequential decisionmaking processes and always select the item from the arriving leaf node, which may lead to two problems: (1) the decisions made in upper levels severely impact the scope of lower-level nodes. Once the policy makes a bad decision at a certain level, the rest selections are all sub-optimal. The issue is especially true when the tree hierarchy is deeper. We call this phenomenon error propagation; (2) Users may be interested in more than one child node, thus the greedy selection may fail to capture the comprehensive interests of users. Therefore, we further propose a progressive hierarchical contextual bandit (pHCB) algorithm for exploration in another manner on the tree. The main idea is that the policy continuously expands the receptive eld from top to bottom according to the feedback obtained from historical exploration. We rst give a denition of receptive eld as follows. Definition1.Receptive eldis a personalized set of nodes representing the current potential interests for each user to explore. At the rst round, the receptive eld only consists of the root node (or is set with prior knowledge). With the exploration process progressing, the receptive eld will be expanded (and reduced) when predetermined conditions are met in an adaptive top-down manner. The nodes in the receptive eld are called visible nodes. In HCB, only the leaf node is associated with a set of items. In contrast, in pHCB we allow the policy to select a non-leaf node and then recommend an item from the item set associated with the non-leaf node. Hence, we have the Denition. 2 to dene the item set of each non-leaf node. Definition2. Given a non-leaf node𝑛and the set of child nodes 𝐶ℎ(𝑛), the item set of node𝑛will be the union of item sets of the nodes in𝐶ℎ(𝑛), that is𝐼 (𝑛) = 𝐼 (𝑛) ∪ 𝐼 (𝑛) ∪ ··· ∪ 𝐼 (𝑛)and 𝐶ℎ(𝑛) = {𝑛, 𝑛, ·· · , 𝑛}. At𝑡-th round, the agent faces a user𝑢whose receptive eld is denoted asV(𝑡). pHCB algorithm treats each node inV(𝑡)as an arm, and selects the arm (denoted as𝑛(𝑡)) with highest estimated reward according to Eq.(2). Then another LinUCB algorithm is used to select one item𝑖(𝑡)from the selected𝐼 (𝑛(𝑡))and collect the feedback from the user. pHCB directly selects an arm from the receptive eld without performing sequential decision-making processes, which avoids the aforementioned concerns of HCB. Here we oer an example in Figure 2 for illustrating the expanding process. Assuming at round𝑇, the receptive eld of the user𝑢consists of three nodes:𝐵,𝐶and𝐷. In the next several rounds, if node𝐶is selected multiple times and received several positive rewards, making it meet the conditions of expansion, its children nodes𝐺, 𝐻, 𝐼will then be added into the receptive eld to replace𝐶. As a result, at round𝑇, the receptive eld includes nodes𝐵, 𝐷, 𝐺, 𝐻, 𝐼. In this way, pHCB expands the receptive eld from coarse to ne and gradually discovers the interests of users. A critical mechanism of pHCB is how to expand the receptive eld. Since the tree nodes are organized in dierent granularity, the nodes at top levels represent the coarse interests of users while the nodes at bottom levels depict specic interests of users. We want the receptive eld be able to quickly converge to the leaf nodes, thus we set the expansion conditions as follows: for a non-leaf node 𝑛at the𝑙-th level ofH, if (1) it has been selected at least⌊𝑞¤log𝑙⌋ times, and meanwhile (2) the average reward on this node is larger than𝑝¤log𝑙(0≤ 𝑝 ≤1), then we expand the receptive led by replacing it with its children.𝑞and𝑝are hyper-parameters. The log 𝑙means that the nodes at a top-level are easier to be expanded than those at a low level. One can also design more exible rules for expansion according to the actual application scenario. Overall, the pseudo-code of the pHCB is available in Algorithm 2. As dened in Eq.(1), the regret is dened as the dierence between the expected reward under hindsight knowledge and the actual reward under the algorithm. The regret bound we would like to obtain is established on a premise that the clustering structure is known to the algorithm ahead of time, which is consistent with the scenario of this paper. In this case, each cluster is viewed as an independent arm, according to [1,3,5], the regret bound is up to logarithmic termsln(𝑇 ),ln(𝑁 ), andln(1/𝛿). By hiding the Output: The policy 𝜋. Figure 2: An illustration of pHCB. At round 𝑇, the receptive eld consists of nodes B, C and D; After several trials, at round 𝑇, node C meets the conditions of expansion, so the receptive eld changes to nodes B, D, G, H and I logarithmic factors with notationeO, the cumulative regret over𝑇 rounds is bounded with probability 1 −𝛿 as: In Eq.(9), we shall assume that||𝑿|| =1 for all clusters.√ Then as proven by [8], one can replace𝑇of each arm by a term formulated as√𝑇 (+), where|𝑉|is the size of𝑘-th cluster and𝑁is the total number of items. As a result, the regret bound becomes: In Eq.(10), according to [8], the worst case occurs when each cluster has the same size, leading to the regret bound: Here we rst discuss the regret bound for HCB. For simplicity, we assume that the number of clusters are reduced𝑚times that of the previous level. At the beginning, each item is treated as a cluster, i.e. the number of clusters should be𝑁. In this case, the regret√√ bound holdseO(𝜎𝑑 +𝑑) log(𝑁 )𝑚𝑇. For pHCB, the receptive eld expands in a progressive manner, assuming the nal size of√√ receptive eld is𝑟, the regret bound is at mosteO(𝜎𝑑 +𝑑)𝑟𝑇. As proven in [5], if the arm set is xed over time and contains𝑁 arms, the regret bound of the contextual bandits with linear payo√ functions is up toO(𝑇𝑑ln(𝑁𝑇 ln(𝑇 )/𝛿))). It is signicantly larger than the regret bound of HCB and pHCB due to the higher order of logarithmic terms and𝑑 ≪ 𝑁,𝑚 ≪ 𝑁and𝑟 ≪ 𝑁in most instances. Therefore, our proposed algorithms can improve the exploration eciency substantially by reducing the cumulative regret. Moreover, the exploration time complexity is reduced from O(𝑁 ) to O(log(𝑁 )) with the help of hierarchy. 5.1.1Datasets. We perform experiments on two public recommendation benchmark datasets, basic statistics are shown in Table 2. • MIND[35]: The MIND dataset is the largest public benchmark dataset for news recommendations so far, which is constructed from the click logs of Microsoft News. We use Sentence BERT [27] to train news embeddings from their contents, and adopt a GRU [24] as the user model to ne-tune news embeddings from the sequence of click logs. • Taobao: The Taobao dataset is constructed from user behaviors of Taobao for E-commerce recommendations. Similar to MIND dataset, we also utilize GRU to learn item embeddings from the sequence of user behavior logs. 5.1.2Baselines. We compare the proposed algorithms against the following related and competitive bandit algorithms: • LinUCB[16] is a classical contextual bandit algorithm. It only works with item-level recommendation. • HMAB[33] organizes arms into hierarchy tree purely by domain knowledge. It utilizes category information to model the dependencies among items. Then the algorithm selects a path from root node to a leaf node, and the leaf node is an item. • ICTRUCB[34] formulates the item dependencies as the clusters on arms. Dierent from our methods, it does not consider the hierarchy. • ConUCB[39] utilizes key-terms to organize items into dierent subsets to represent dependencies among items. The algorithm occasional conversation with users and leverages conversational feedbacks on key-terms from users to accelerate the speed of bandit learning. 5.1.3Our Methods and Variants. Our goal is to propose a generic algorithm that can empower dierent bandit models to be more eective on large-scale item set exploration. Therefore, our main experiments contain two groups: rst, with LinUCB as the base bandit model, to compare our algorithms (i.e., HUCB and pHUCB) with the aforementioned baselines (because most of the listed baselines are based on LinUCB); second, with three dierent base bandit models, including LinUCB, Thompson Sampling (TS) [2], and𝜖-greedy, to verify whether our algorithms are eective under dierent settings. In the second group of experiments, we also compare two variants of our models: • CB-Category: It borrows the idea from HMAB [33] by using the prior knowledge, i.e. the category taxonomy, to assign items into dierent subsets. Each subset will be treated as an arm, the policy rst selects a subset and then recommends an item from the subset to users with a base bandit algorithm. • CB-Leaf: It is a variant of ICTRUCB [34]. Since the ICTRUCB is designed for context-free bandits, to keep a fair comparison, we also utilize K-Means clustering on item embeddings to assign items into dierent subsets. Here, we treat the leaf nodes ofH as the clustering results. Each leaf node will be treated as an arm, the policy rst selects a leaf node and then recommends an item from the node to users with a base bandit algorithm. Here, CB- can take a value from { LinUCB, Thompson Sampling (TS), and𝜖-greedy }, our experiments are separated into three groups. For example, if the CB- model is LinUCB, then the involved variants are LinUCB-Category, LinUCB-Leaf, and our nal models are HUCB and pHUCB. For all models, including our proposed ones and baselines, we set the maximum times of score-computing per round to 50 for a fair comparison. For example, in LinUCB, if the number of arm candidates is 1000, then originally we need to compute 1000 scores per round to select the best one in estimation, which exceeds our budget, so we will randomly sample 50 arms from the 1000 arms and then perform the LinUCB on the small set. For hierarchical CB methods, the budget is evenly distributed to each level, e.g., for pHCB, we have two levels of bandit, then each level will get a budget of 25. 5.1.4Evaluation. We evaluate the performance of dierent bandit algorithms with o-policy user simulator evaluation. To reduce the biases of the simulation, we utilize the IPS estimator [10,28,29], which is a standard method used for o-policy evaluation of bandit algorithms. IPS learns to re-weigh the training samples by the propensity score to learn an unbiased simulator. The simulator is trained on the whole data to make the best use of information. This evaluation enables us to compare the performance of candidate hypothetical policies without expensive online A/B tests. Specically, the simulator learns the unbiased embeddings of users and items, and the reward of a user𝑢towards an item𝑖is derived from the inner product of their embeddings. 5.1.5Reproducibility. For MIND dataset, each item is represented by a 64-dimensional embedding vector. The dataset has been split for training/validation/test, only the click logs of the training set is used for learning item embeddings and building tree structures. The users without history logs or impression logs are removed. The click is treated as positive feedback and non-click is treated as negative feedback. The hierarchy tree structure is set to { 1, 100, 10000 }, which means there is 10000 leaf nodes, and only one layer of non-leaf nodes. For Taobao dataset, each item is represented by a 32-dimensional embedding vector. As the same method does in [40], we remove the users who have less than 10 behaviors or the items appearing less than 10 times. We randomly select 10000 users for testing and 1000 users for validation, and the behavior logs of the rest users are used for learning item embeddings. The click is treated as positive feedback and the negative feedback is generated via negativing sampling. The hierarchy tree structure is set to { 1, 50, 5000, 50000 }, so that number of items mounted to a leaf node is less than 100. For the LinUCB-based algorithms, all learnable parameters are initialized as all zero matrices or vectors, and the hyper-parameter 𝛼is set as 0.5. The Gaussian prior is used to design Thompson Sampling-based algorithms for contextual bandit. For𝜖-greedybased algorithms, the𝜖is set as 0.05. With the help of validation set, the hyper-parameters𝑞and𝑝of pHCB and its variants are set as 10 and 0.1 respectively. Follow the setting in [39], we consider a general conguration in which at each round, the computational cost is limited as 50. Note here the arms can be nodes or items depending on dierent bandit algorithms. The learning rate for training GRU is 0.001, the hidden size is the same as the embedding size, and optimizing with Adam optimizer [13]. All algorithms are implemented with Python and PyTorch, and repeated 10 times to report the average performance. The code and processed datasets will be released after acceptance of the paper for easy reproducibility. ICTRUCB 5.60 300.83 709.05 10.98 135.99 290.33 5.2.1Comparison with Baselines. Since all baseline algorithms are on the basis of LinUCB, we also choose LinUCB as the base algorithm for HCB and pHCB to keep a fair comparison. Note that HCB and pHCB can work with dierent base algorithms, and we discuss their generality in sec. 5.2.2. We compare the cumulative rewards over 100/1000/2000 roundsof dierent algorithms Figure 3: Cumulative rewards of our algorithms and variants based on LinUCB, Thompson Sampling and 𝜖-greedy, on the MIND dataset and Taobao dataset, respectively. in Table 3, and the best results are presented in bold. As can be seen, our proposed algorithms, HUCB and pHUCB, outperform all baselines across dierent datasets consistently at dierent rounds, and pHUCB is generally better than HCB. For example, on the largest TaoBao dataset, At 100/1000/2000 rounds, the performance of pHUCB was improved by 54.3%, 268.7%, and 280% compared with LinUCB, respectively. Although HMAB, ICTRUCB, and ConUCB achieve higher cumulative rewards over LinUCB, there is still a considerable gap between these algorithms and ours. The superiority of pHUCB over HUCB further veries that by expanding the receptive eld in a progressive manner, the pHCB algorithm is able to better discover the comprehensive interests of users. 5.2.2Flexibility and Variants Study. Next, we report the cumulative reward of our algorithms and their variants, in Figure 3, based on three dierent base bandit algorithms. From the experimental results, we have the following observations. •Constructing item dependencies in the form of clusters indeed helps a lot in accelerating the exploration. This can be veried from that both our algorihtms (HCB and pHCB) and their variants (CB-category and CB-Leaf) outperform the corresponding base bandit model. •HCB and pHCB achieve the highest cumulative rewards on both two datasets in most of the cases, indicating that the proposed hierarchical algorithms are eective. As we can see, the performance of baseline algorithms has a noticeable gap between our proposed algorithms. This result is reasonable since our proposed methods introduce the hierarchy knowledge to take the item dependencies into consideration, which greatly improves the eciency of exploration. Although the two variants, such as CB-Category and CB-Leaf, also organize items into dierent clusters, they fail to model the coarse-to-ne item dependencies as tree structures. Apart from that, the pHCB algorithm beats other methods, which veries that the progressive exploration can adaptively discover the diverse interests of users with a receptive eld. •Our algorithms have strong exibility. We have tested the performance with base models varying in { LinUCB, Thompson Sampling,𝜖-greedy }, the conclusions are consistent, which proves our proposed frameworks can well generalize to various bandit algorithms. 5.2.3Parameter Sensitivity. In this subsection, we study the impacts of hyper-parameters. Since the pHCB performs best in most of the cases, we particularly study the key hyper-parameter of it, i.e.,𝑞and𝑝, which control the expansion conditions:𝑞determines the number of trails at one arm and𝑝indicates the threshold of average reward for expanding child nodes. To study their impacts, we take pHUCB as an example and vary𝑞from{0,5,10,15,20,25} (at round 1000) will be aected. As shown in Figure 4, dierent hyper-parameters have a noticeable inuence on the cumulative reward of pHUCB algorithm over 1000 rounds. For MIND dataset, if𝑞is too small or𝑝is too large, the cumulative rewards become worse since the former makes the expansion conditions unreliable and the latter makes the receptive eld dicult to expand. As for the Taobao dataset, the trend of impacts caused by𝑞is similar to the MIND dataset. Meanwhile, the model is less sensitive with parameter𝑝. Overall, from Figure 4 we learn that a suggested conguration is 𝑝 = 0.1 and 𝑞 = 10. 5.2.4Alleviate Closed-Loop Eects. Typically, recommender models trained from historical logs are designed for exploitation purposes, here we called them exploitation models. Such recommendation systems often suer from closed-loop eects [11] because they only learn users’ interests from historical logs, but they do not have the ability to explore new interests of users. In contrast, the contextual bandit algorithms are more eective to break the information cocoons with exploration strategies. In order to verify that our model is able to explore the potential interests of users thus Figure 4: Ee ct of hyper-parameters of pHUCB. Transformer 1.3770.6950.6830.546 alleviate the closed-loop eects, we design an additional experiment as follows. We select three exploitation models, i.e., Linear model, GRU, and Transformers [31] as baselines. These exploitation models are rst pre-trained by the historical logs of existing users to get the deployed models. Then, we use exploitation models as well as our proposed models as "deployed models" to serve users. Specically, in this stage, for each new user (whose logs are not used in the rst pre-trained stage), we randomly sample only three clicked items as visible historical logs for generating her initial user embedding with a deployed model. Then we can recommend two hundred items to the user with a deployed model and collect the user’s feedback. Note here the user embedding will be refreshed once the model receives positive feedback. This stage is performed for every deployed model respectively. The third stage is about evaluating the quality of impression logs produced by the deployed models. We utilize the collected historical logs together with all the rest historical logs of existing users (which are used in the rst stage) as training samples to train an evaluating model (here we use the matrix factorization (MF) model as the evaluating model, each user and item will be mapped to an embedding vector. MF-based collaborative ltering method is one of the most popular models for personalized recommendations) and evaluate the trained model on the same test samples for a fair comparison. In order to prove the advantages of our bandit algorithms in exploring user interests, we select two hundred of test users with the most diversied interests as new users: we calculate the Gini impurity [19] of the historical items clicked by the user according to the category of items. Obviously, the larger the Gini impurity, the more diverse the interests of the user. The users of the training set are treated as existing users. We report the test log loss (LogLoss) and area under curve (AUC) score in Table 4. We can observe that both our methods, HUCB and pHUCB, achieve much higher performance than exploitation models including the Linear model, GRU, and Transformers, which demonstrates that our proposed models can eectively help to alleviate the closed-loop eects in recommender systems. In this paper, we propose a general hierarchical bandit framework for entire space user interest exploration. Specically, we design two algorithms, i.e., HCB and pHCB. The HCB algorithm makes a sequence of decision-making tasks to nd a path from the root to a leaf node, while the pHCB progressively expands the receptive eld in a top-down manner to explore the user interests, which is more exible and also achieves more satisfactory results. Extensive experiments are conducted to demonstrate the eectiveness of the proposed framework on two real-world datasets with three dierent base bandit algorithms. In the future, we plan to combine our methods with the start-of-the-art deep learning methods to estimate reward for making more reasonable decisions. Moreover, we assume the items are static in this paper by xing the tree structure unchanged. It would be interesting to extend the proposed frameworks to the non-static setting, which has not been well studied yet.