University of Technology SydneyUniversity of Technology Sydney Yicong.Li@student.uts.edu.auHongxu.Chen@uts.edu.au zhenchao.sun@mail.sdu.edu.cncathylilin@whut.edu.cn Hypergraphs have been becoming a popular choice to model complex, non-pairwise, and higher-order interactions for recommender system. However, compared with traditional graph-based methods, the constructed hypergraphs are usually much sparser, which leads to a dilemma when balancing the benets of hypergraphs and the modelling diculty. Moreover, existing sequential hypergraph recommendation overlooks the temporal modelling among user relationships, which neglects rich social signal from the recommendation data. To tackle the above shortcomings of the existing hypergraph-based sequential recommendations, we propose a novel architecture named Hyperbolic Hypergraph representation learning method forSequentialRecommendation (HSeqRec) with pre-training phase. Specically, we design three self-supervised tasks to obtain the pre-training item embeddings to feed or fuse into the following recommendation architecture (with two ways to use the pre-trained embeddings). In the recommendation phase, we learn multi-scale item embeddings via a hierarchical structure to capture multiple time-span information. To alleviate the negative impact of sparse hypergraphs, we utilize a hyperbolic space-based hypergraph convolutional neural network to learn the dynamic item embeddings. Also, we design an item enhancement module to capture dynamic social information at each timestamp to improve eectiveness. Extensive experiments are conducted on two realworld datasets to prove the eectiveness and high performance of the model. Sequential Recommendation, Hypergraph, Hyperbolic Space, Selfsupervised Learning ACM Reference Format: Yicong Li, Hongxu Chen, Xiangguo Sun, Zhenchao Sun, Lin Li, Lizhen Cui, Philip S. Yu, and Guandong Xu. 2021. Hyperbolic Hypergraphs for Sequential Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482351 Graph-based approaches have been widely used and achieved great improvement for next-item recommender systems. However, most existing literature [2,12,15,18,23,29,32,35,45] treat the dynamic time-dependent user-item interactions as a temporal bipartite graph and learn their latent representations for action. Though the graphbased graph modelling can capture the benecial rst-order (i.e., user-item interactions) and second-order (i.e., co-purchasing) interactions for recommendation, the higher-order signals among users and items are usually neglected by existing works due to the limitations of traditional graph modelling. With noting the shortcoming, recent works [4,24,25,29,30,36,43] resorted hypergraphs to make up and developed hypergraph-based modelling approaches for sequential recommender systems. The basic idea is illustrated in Figure 1, when using traditional graphs to model user-item dynamic relationships evolution, the learned information from Figure 1.(a) to 1.(b) is monotonous due to the simple pair-wise data structure. In contrast, hypergraphs are able to capture high-order dynamic relationships (buy items at the same time, similar user groups, etc.) thanks to the non-pairwise data structure. That is, unlike traditional graphs, an edge in the hypergraph (a.k.a hyperedge) is allowed to connect more than two nodes, promising to capture multi-scale signals for the recommendation. Nevertheless, all these existing works ignored a critical issue of such hypergraph-based approaches, which isthe sparsity issue is becoming more severe in hypergraph based recommender systems. As analysed in [42], the recommendation benchmark Figure 1: A comparison of sequential graph construction and hyp ergraph construction. Single-colored area in (c) and (d) denote hyperedges. Amazon dataset is sparse and exhibits long-tailed distribution, in which many users have limited interactions to the items, while only a small number of users interact with many items. In such a case, the constructed hypergraphs from the original dataset will be much sparser, resulting in insucient training samples for action. To be specic, if we construct hypergraphs based on the original simple user-item bipartite graph, the number of items (nodes) does not change, but the number of hyperedges is dramatically shrunken compared to the number of original links between users and items, which is obvious in Figure 1. Therefore, when constructing hypergraphs, the challenging sparsity issue and long-tailed data distribution are becoming even more severe. The second limitation of existing hypergraph-based sequential recommendation lies inlacking exploitation of hidden hyperedges among users, which we believe will be benecial to understand the hidden but insightful behaviours among users (e.g., common interests groups, co-purchasing groups, users who have similar buying patterns, etc.). Taking the group recommendation task [31,41] as an example, it focuses on a group of users’ preference, which means users in the same group may tend to have a similar preference, or at least they have more common interest compared to the rest of the world. Inspired from the idea, we are curious about the possibility of exploring and leveraging hypergraphs constructed from users side to improve the overall recommendation performance. For example, in Figure 1(c) and (d), from the timestamp𝑡to𝑡, they are the evolution of sequential hypergraph constructions. It is obvious that in the timestamp𝑡, the user𝑢is of co-purchasing relationship with𝑢, and the hyperedges of𝑢 and𝑢are connected. In this case, the items𝑖,𝑖,𝑖and𝑖are likely to be more similar to each other than to𝑖and𝑖. The same is also in the timestamp𝑡. Therefore, the aim is to capture the dynamic items diversication through dynamic hypergraphs constructions according to users’ relationships to enhance the model. Intuitively, like traditional graph-based methods, this try may gain improvement on recommendation. Figure 2: An example of yearly purchase behaviour. To holistically solve the above issues, we propose a novel architecture namedHyperbolicHypergraph representation learning forSequentialRecommendation (HSeqRec). Specically, we propose to rstly pre-train the model with self-supervised learning on three well-sophisticated tasks. It is worth noting that compared with the prior works paying more attention to chronologically model each user’s buying history, one of our contributions is to introduce a hyperedge prediction task, which explicitly models the users’ historical records as hyperedges at each timestamp and explores the potential relationships between linked hyperedges (users who have similar buying history) [37,48]. Besides, we also investigate hyperbolic embedding spaces [46] and manage to map the sparse data points to the hyperboloid manifold directly. The rationale is that hyperbolic space has a stronger ability than Euclidean space to accommodate networks with long-tailed distributions and sparse structures [1,3,13,14,21], which is also veried in our experiments. In addition, to exploit hidden but useful user-side information in sequential recommendation settings, we propose to construct an induced hypergraph for each user to model his behaviour pattern in group-level. Intuitively, a user’s behaviour pattern is not only reected in his historical records but can also be excavated from other users who have similar behaviour pattern. This is particularly helpful to remit the "cold start" problem. Specically, for the target user𝑢, we rst use a hyperedge to collect historical items taken by 𝑢at the timestamp𝑡. Then we nd the other hyperedges from other users who share the overlapping items with𝑢at the timestamp𝑡. All these hyperedges construct an induced hypergraph to describe the user𝑢’s behaviour pattern from the group view at the timestamp𝑡. Although constructing hypergraphs from original simple useritem bipartite graphs seems a better choice, it is dicult to decide the time granularity to which the length of item sequence for each hyperedge is split. After analysing the items in the dataset, we nd the purchase history is not only in a chronological sequence way, but with a periodic regularity, like seasonal evolution. As shown in Figure 2, the user bought a Christmas tree and some decorations in both 2018 and 2019, and she also bought two dresses in the summer of 2018 and 2019. Therefore, rather than model each user’s items in a chronological way, we design three views for hypergraph construction, namely yearly, quarterly and monthly views, and manage to hierarchically learn latent item representations. Moreover, we also nd that when constructing quarterly and yearly hypergraphs, the hypergraphs are no longer as sparse as original single-time grained hypergraphs. In this way, the hierarchical time-span method for hypergraph construction also alleviates the above mentioned sparse problem. In summary, the contributions of this work are as follows. •We propose three self-supervised learning tasks as the pretraining phase. To our best knowledge, we are the rst to Figure 3: Overall architecture HSeqRec contains three modules, multi-scale embeddings via dynamic hierarchical hyperbolic hypergraphs, item enhancement via similar user groups, and learning user preference for sequential recommendation. The pre-training module could be as initial embedding for HSeqRec (HSeqRec-init); and it also could fuse into HSeqRec (HSeqRec-fuse). propose a hypergraph link prediction as a pre-training task to do data augmentation. •We explore hidden insightful behaviours among users by constructing hypergraphs related to users at each timestamp to enhance the recommendation model. •Instead of traditional chronological sequential modelling of data, we argue periodical regularity and model users’ interest as a hierarchical structure for improving recommendation. •We propose a novelHyperbolicHypergraph representation learning method for Sequential Recommendation (HSeqRec) to well-model long-tail data in sequential recommendation task. •Extensive experiments are conducted on benchmark datasets. Our model outperforms the SOTA sequential recommendations, which shows the eectiveness of our model. The section presents the denition and the problem formulation. Definition 1.Hypergraph.AssumingG= (V, E)denotes a hypergraph with a nodes setVand a hyperedges setE, a hyperedge 𝑒∈ Econnects multiple𝑛nodes (𝑛 ≥ 2). The nodes set conne cted by a hyperedge 𝑒is a hypernode 𝑣⊂ V. Problem 1.Sequential Recommendation.In the sequential recommendation task, the user’s related items are in chronological order. Given the user setU = {𝑢, 𝑢, ...,𝑢}and the item setI = {𝑖, 𝑖, ..., 𝑖}, each user has sequential itemsS= {𝑠, 𝑠, ..., 𝑠}, where𝑠∈ Idenotes the𝑡historical item of user𝑢. The sequential recommendation problem is to predict the next𝑞items {𝑠, ..., 𝑠}associated to each user𝑢, according to sequential historical itemsS= {S, S, ..., S}. In this paper,𝑞 = 1. For each timestamp𝑡, a hypergraphGis constructed according to the current user setUand the item setI. Each user’s historical items within the timestamp 𝑡 are connected by a hyperedge. Figure 3 is an overview of our proposed sequential recommendation system. Technically, we rst generate yearly, quarterly, and monthly hypergraph snapshots as views from users’ historical records. Here each hyperedge in the hypergraph refers to one user’s historical items. With these hypergraphs, we design a novel hypergraph convolutional network in the hyperbolic space to learn multi-scale item embeddings. To enhance the item embedding, we further explore user’s behaviour pattern via similar users’ behaviours. In the end, we leverage the Transformer to learn users’ preference and predict the next item via multi-layer perception (MLP). Moreover, before theHSeqRec, we design three self-supervised tasks, as shown in Figure 4, to pre-train item representations. The pretrained item representations could be as initial embeddings for HSeqRec, namedHSeqRec-init; and they could also be fused into HSeqRec, named HSeqRec-fuse. Before we start, we need the initial item features to support the downstream model. Inspired by the self-supervised learning framework [48], which can learn item representations without annotations, we propose three novel pre-training tasks (as shown in Figure 4) to learn the pre-trained item representations: •The rst task is to predict whether a masked item sequence is derived from a user’s historical records. The rationale is that in the real world, one item usually cannot change a user’s long-term preference. Therefore, we randomly mask dierent items for a target user’s historical sequence and then let these sequences’ representations as close as possible. •The second task is to predict whether a sub-sequence is consistent with a user’s short-term interest. The motivation is that the consecutive items taken in a short time usually contain a user’s short-term interest, and these items might share similar functions. If we mask part of them, the user’s short-term interest should be still observable from the rest items. Therefore, we rst mask a sub-sequence with length 2 and then wish the masked sequence be close to the complete sequence. •The third task is to predict whether two users have the same behaviour patterns. Intuitively, users’ behaviour patterns can be reected in their historical records. If two users share many historical items, they are more likely to have the same preference, hobbies or even tastes. To this end, we use a hyperedge to connect one user’s historical items and then let a pair of hyperedges be as close as possible if they share the overlapping items. Lethandhbeing embeddings of two sequences, we adopt contrastive loss [37] for each task: here𝑆is the sampled negative sequences of the user𝑢.his the embedding of the negative sequence,𝑠𝑖𝑚(h, h)is dened Figure 5: Multi-scale Embeddings via Dynamic Hierarchical Hyperbolic Hypergraphs. as:where𝜏is the hyper-parameter. The sequence embeddingshandhare both calculated by Transformer network with two multi-head attention and feed forward blocks. We combine the above three tasks together with optimized weights {0.1,1,1}. With the above initial features, we then design a dynamic hypergraph neural network for learning the multi-scale item embeddings in hyperbolic space, as shown in Figure 5. 3.2.1 Hierarchical Time-span Hypergraph Construction. To model the complex dependencies among the sequential items, we propose a hierarchical architecture to learn the monthly, quarterly and yearly relationship among sequential items. The motivation is that a user’s behaviour patterns are not just suggested in anteroposterior items of the item sequences but also reected via the seasonal and periodical variance. For example, users prefer to buy Christmasrelated products before Christmas, and purchase T-shirts in summer and coat in winter. Inspired by [29], we utilize hypergraphs instead of traditional graphs to model dierent sub-sequences of each user and we also use multi-scale time-spans views (monthly, quarterly, and yearly) to learn the semantics of items. To tackle the sparsity and the long-tail distribution of these hypergraphs, we learn the graph representations in hyperbolic space because it is perfectly suitable for long-tail structures. Specically, when we deal with the monthly item representations, we rst construct a monthly hypergraph where each hyperedge connects user’s items within one month. Then we use hyperbolic space-based hypergraph graph neural network (introduced in 3.2.2) to learn the dynamic item embeddings in each month. Following this approach, we can also learn quarterly and yearly item representations. 3.2.2 Hyperbolic Space-based Hypergraph Convolutional Network. Compared with traditional graphs that mainly rely on pairwise useritem interactions, hypergraphs can model much higher relations in user-item interactions and promise to fuse item context to remit the sparsity problem. For example, if user𝑢bought ower and wedding dress and𝑢bought ower and a vase, we can use a hyperedge to connect the ower and the wedding dress, and use another hyperedge to connect the ower and the vase. In this way, the ower’s semantic for𝑢is about the wedding but for𝑢is about decoration. However, if we use a traditional graph, the graph is hard to reveal dierent item semantics directly. In addition, the user-item record usually follows the long-tail distribution and is sparse. The traditional Euclidean space usually cannot capture this structure leading to representation distortion, while hyperbolic space is benecial to deal with the issue. [1] Based on the above motivations, we propose a hypergraph neural network to model the items evolution in hyperbolic space. Specifically, we rst transform the initial item features from Euclidean space to hyperbolic spaceH, and then we feed the initial hyperbolic item embeddings to learn item embeddings. For the hyper-√ bolic space, we seto := {𝑐, 0, 0, ..., 0} ∈ Has the north pole in H, where−1/𝑐is the negative curvature of hyperbolic model. As analyzed in [1], the initial item features in hyperbolic space can be deduced from Euclidean space as follows: h= exp0, h wherehandhare the initial hyperbolic embedding and the initial Euclidean embedding, respectively. With the initial features in hyperbolic space, we further transform the features via a linear function in hyperbolic space: wherexis the hyperbolic hidden embedding of item𝑖in the 𝐿-th layer after transformation,𝑊andbare the weight and bias, respectively.his item𝑖’s hyperbolic embedding in the last layer. When𝐿 = 1,h= h. Then, the item embeddings can be aggregated from the neighbouring nodes via the following convolutional operation in hyperbolic space: whereyis the hyperbolic hidden embedding of item𝑖in the𝐿-th layer after aggregation,N(.)denotes the neighbors soN(𝑖)is node 𝑖’s neighbors andN(𝑖)is node𝑖’s neighbors where𝑖is on the same hyperedge as𝑖. The node𝑗’s hyperbolic embedding is transformed to Euclidean embedding via𝑙𝑜𝑔(.), so the Euclidean-based sum and add operations are available.𝑒𝑥𝑝(.)aims to transform the Euclidean-based embedding to hyperbolic embedding. According to [1], choosingxas the north pole is the best Euclidean approximation at this step.𝑀is the projecting weight dened as follows: 𝑀(𝑀𝐿𝑃 (𝑙𝑜𝑔(x)||𝑙𝑜𝑔(x))) With the above node features’ aggregation, we then use an activation function to generate the hidden embedding on Layer𝐿: wherehis the node𝑖’s hyperbolic embedding at𝐿-th layer, 𝑅𝑒𝐿𝑈is the𝑅𝑒𝐿𝑈operation in the hyperbolic space. Through 𝑙𝑜𝑔(.), we transform the hyperbolic item embeddinghto Euclidean-space based embedding h, and feed into the next module. 3.2.3 Mix Layer. With item embeddings for each month, quarter and year, we then design a mix layer to fuse them together. Specically, to fuse monthly embeddings and yearly embeddings, we use a two-layer neural network to calculate their correlations as follows: whereℎrepresents the mixture of monthly and yearly item embeddings at timestamp𝑡.ℎandℎmean the monthly and yearly item embeddings respectively.𝑡and𝑡denote the monthly and yearly timestamp, respectively.𝑊andbare weight matrix and bias.⊙means element-wise product. Similarly, the mixed embeddings with the month and the quarter can be dened as ℎ, which is calculated by: ℎ= ℎ= 𝑅𝑒𝐿𝑈 (𝑊ℎ+ b) ⊙ ℎ(8) whereℎis the dynamic item embeddings at timestamp𝑡 learned by this hierarchical architecture. Most existing hypergraph-based sequential recommender systems [29] ignore the hidden relationships among users, like social relationships, co-purchasing relationship, etc. However, these relationships are highly informative to capture the hidden users’ behaviours. For example, a user’s shopping behaviour may be aected by his friend circle, making the user tend to buy similar products with his friends. In light of this, we utilize user groups to enhance the former learned dynamic item embeddings. Specically, we rst nd similar users for each user at each timestamp𝑡. The similar users are those who take the same products at the same timestamp𝑡. Using the current user’s shopping records at𝑡and his similar users’ records at𝑡, we build an induced hypergraph to model his behaviour pattern in group-level. Each hyperedge connects each user’s items. Then we leverage our proposed hyperbolic space-based hypergraph convolutional network to learn item embeddings in group-level. To enhance the item representations, we mix the above embeddingshwith previous embeddingshin Section 3.2.1. The mix operation is dened as follows: where his the item 𝑖’s embedding for user 𝑢 at timestamp 𝑡. In this section, we leverage learned item embeddings to nd users’ preference and then present the sequential recommendation model. 3.4.1 User Preference Learning. For each user𝑢, we use the Transformer [26] to model the dynamic item embeddings during the whole timestamps, mathematically, h= 𝑇𝑟𝑎𝑛𝑠 𝑓 𝑜𝑟𝑚𝑒𝑟 (h, h, ..., h), 𝑖 ∈ S(10) wherehis the embedding of user𝑢’s item sequence, which can describe𝑢’s preference. The input of Transformer are the dynamic item embeddings learned in Equation 9. In this way, the user’s sequential items contain both the dynamic hierarchical information and the user’s potential group information. 3.4.2 The Complete Model. With user preference representation h, we use a two-layer Multilayer Perceptron (MLP) to calculate the rating score of the next item, mathematically, where𝑟is the rating score of user𝑢and item𝑖at timestamp 𝑡,his the embedding of item𝑖at timestamp𝑡. We train the recommendation by Bayesian Pairwise Loss [20], which aims to maximize the dierence between the rating scores of the positive item 𝑖 and negative sample 𝑗: where(𝑢, 𝑖, 𝑡, 𝑗) ∈ Ddenotes the positive pair (u,i,t) and the negative pair (u,j,t) from the training setD.𝜎is the sigmoid function,𝛼 is the weight of L2 regularization term ||𝛿||. 4.1.1 Dataset. We evaluate our method on two real-world datasets, named AMT and Goodreads. The statistics are shown in Table 1. • AMT.It is the subset of the public Amazon dataset [17], which contains consumers’ buying record and reviews in 29 categories. In our experiment, we choose three categories, Automotive, Musical Instruments and Toys and Games, to form the dataset and remove users bought less than 5 items. The time of AMT dataset is range from 2014 to 2018. The rst two items in 2018 are considered as valid data and test data, respectively. Generally, the distribution of the items among users is long-tail distribution [42]. • Goodreads.It [27,28] is collected from goodreads website, which is an online book community website. We choose user-book (item) interactions between 2013 and 2015 as the dataset. Last two interactions are valid and test data, respectively. From Table 1, it is evident that Goodreads is much denser than AMT dataset. 4.1.2 Baselines. • GRU4Rec.[5,6] It is a well-known sequential-based recommendation model that utilizes GRU to sequentially model users’ interactions to achieve top-N recommendation. • SASRec.[12] It is a self-attention based sequential approach for next item recommendation, which could capture each user’s both long-term sequential relations through PointWise Feed-Forward Network and short-term interactions with items through an attention mechanism. • BERT4Rec.[23] This method employs deep bidirectional self-attention to sequentially model the user behaviours in two directions through Cloze task. • SRGNN.[35] The method utilizes graph neural networks to model the session sequences and obtain the complex item transitions for each session. • HGN.[15] It proposes a hierarchical gating network with the Bayesian Personalized Ranking in order to capture user’s both long- and short-term interest. • HyperRec.[29] It uses sequential hypergraphs to model dynamic item embedding sequentially and fuses with static item embeddings as item representations. For each user, item representations are fed into Transformer network to obtain the next item recommendation. 4.1.3 Evaluation. Our proposed method focuses on recommending next item, and therefore we use Top𝐾Hit Ratio (HR@𝐾) and Top𝐾Normalized Discounted Cumulative Gain (NDCG@𝐾) as our evaluation metrics. We choose𝐾 = {1, 5, 10, 20}in the baseline comparison experiment. Our baseline HyperRec [29] randomly selects 100 negative samples for each positive user-item pair. However, we think it is insucient to reect our model’s eect accurately. Moreover, the baseline Bert4Rec and SRGNN’s evaluation speed is too slow to test all the data. Therefore, in our experiment, we randomly choose{100, 500}negative samples and rank{101, 501} items to calculate the HR@𝐾 and NDCG@𝐾 scores. 4.1.4 Parameter Details. We implement GRU4Rec, SASRec, BERT4Rec and SRGNN from the RecBole python package [47]. For other methods, we use the public code provided by each paper. For all the methods, the feature dimension size is 100. For AMT dataset, all baselines’ training batch size is set 512. We use the early stop function in RecBole whose condition is not updating NDCG@10 for 10 epochs. The hidden size is 100, and the dropout probability is 0.5, as the same as our proposed methods. Moreover, we choose BPR as the loss of GRU4Rec, SASRec, BERT4Rec and SRGNN to keep identical with our methods. For those baselines’ code authors provided, we employ the given default parameters, but the learning rate is 0.001 for all baselines. For Goodreads dataset, some baselines’ training time is so long with training batch size 512, so we change it to 4096 if the baseline does not run out of CUDA memory. Other parameters are the same as training AMT dataset. To evaluate our proposed model, we design three strategies to initialize it: 1)HSeqRec, one-hot item embeddings as input without pre-training tasks, 2)HSeqRec-init, pre-trained item embeddings as input and 3)HSeqRec-fuse, a combination of the rst two, one-hot item embeddings as input, and pre-training item embeddings fused with dynamic item embeddings and group-level item embeddings. For our proposed methods HSeqRec, HSeqRec-init and HSeqRec-fuse, the parameter settings are the same. The layer of hyperbolic hypergraph convolutional network is 2, and we choose the hyperboloid model as hyperbolic geometry with the negative curvature -1. Moreover, since the Goodreads dataset is too dense, we sample1/100users to nd similar user groups. To simplify our proposed methods, we fuse the item embeddings learned via similar users’ group with the dynamic item embeddings within each batch rather than for each user. In Transformer, we set the number of heads and blocks, 2 and 1, respectively. The epoch of hyperbolic hypergraph convolutional network is 300. The maximum sequence is set to 50 for all the above methods. Other parameters are the same as the baselines’. 4.2.1 Overall Performance Analysis Between Dierent Baselines. As shown in Table 2, we evaluate our proposed model with 6 state-ofthe-art sequential recommendation baselines. Our proposed model HSeqRec,HSeqRec-init andHSeqRec-fuse could outperform all of them in both HR@𝐾and NDCG@𝐾in 100 and 500 negative sampling experiments. For AMT dataset, our best model improves the best baseline 31.61% and 16.94% at NDCG@1 for negative sampling 100 and 500, respectively. In terms of Goodreads dataset, our best model outperforms the best baseline 10.43% and 13.71% at NDCG@1 for negative sampling 100 and 500, respectively. From the last column (Improvement), the advantages also could see in other evaluation metrics, especially in top 1 and 5 ranking. In recommendation task, it is signicant to have better recommendation in top ranking, because proper recommendation in higher ranking means more eectiveness of the recommendation model. 4.2.2 Eectiveness on Pre-train Features. In Table 2, for AMT dataset, the proposedHSeqRec-fuse achieves the best performance and improves 5.41% and 5.33% than the NDCG@1 ofHSeqRecwith 100 and 500 negative samples. While for the Goodreads dataset, HSeqRecachieves the best performance on NDCG@1 and better thanHSeqRec-fuse by 2.74% and 7.74% in negative sampling of 100 and 500, respectively. From the results, we could get the conclusion that when training sparser dataset, to do data augmentation as pre-training could help improve model. However, in terms of denser dataset, it is useless or even worse to do pre-training while our proposed model without pre-training could still achieve quite well performance. Therefore, unlike traditional recommendation models which blindly pursue pre-training to improve the model, AMT100 Goodreads100 Table 2: Comparison with baselines. The last column is the improvement of the best proposed method than the best baseline. HSeqRec(Hie Table 3: Impact of Dierent Modules (NDCG@K). ¬ is removing the following module and keeping the others. The last column means the decrease rate of each row compared with HSeqRec on NDCG@10. our experiment shows that sometimes pre-training cannot really help improvement. Moreover, from the columnsHSeqRec-init andHSeqRecfuse of Table 2, we nd that the results are of small dierences in two datasets. It means that no matter how to use the pre-training embedding in the model, it is almost the same. Therefore, the conclusion is that pre-training does play a part in the model. We perform ablation test on AMT dataset to study dierent modules’ eects. The evaluation metric is NDCG@{1,5,10,20}, and the number of negative test sampling is 100. In Table 3, the¬represents removing the following module and remaining other modules. For example,HSeqRec ¬U means the proposed model without Item Enhancement via Similar User Groups module in Section 3.3. In the next row, Hie means the hierarchical module, so removing the hierarchical module represents deleting the quarterly and yearly dynamic item embeddings and only feeding monthly dynamic ones into the recommendation. HB means hyperbolic space, and therefore that row meansHSeqRecabandons hyperbolic space and train in Euclidean space.HSeqRec(Hie) changes the order of fusion time-span in hierarchical architecture, that is, it fuses quarterly dynamic item embeddings rstly into monthly dynamic item embeddings, and then yearly item embeddings fuse into them. As shown in the Table 3, the proposed modelHSeqRecachieves the best performance. The Item Enhancement via Similar User Groups module plays the least important role in the whole model, because of the sparsity of AMT dataset. Moreover, using the hierarchical module to learn dynamic item embeddings and hyperbolic space-based item embeddings learning both could improve the model. However, if we change the hierarchical time-span fusion order, the results will be a little bit worse. That may be because if yearly dynamic item embeddings fuse in the last step in hierarchical architecture, the nal fused item embeddings would contain more yearly information and less quarterly information. However, yearly information may be too large as a time interval, and the quarterly one may be apposite. Therefore, that is why we choose to fuse yearly information rst and then quarterly, and the results also prove our rationality. In Table 4, we evaluate dierent pre-training tasks’ inuence on the performance of the AMT dataset with 100 negative samples. Minus afterHSeqRec-fuse denotes only considering the following pre-training tasks.𝑀means the masking random items task,𝑆 represents the masking subsequence task, and𝐻is the hyperedge link prediction task. HSeqRec-fuse-SH andHSeqRec-fuse-MH obtain better performance, compared withHSeqRec-fuse-MS, which proves the eectiveness of our proposed hyperedge prediction task. We can also get this conclusion by the last row,HSeqRec-fuse-H shows the highest performance compared withHSeqRec-fuse-M and HSeqRec-fuse-S. Therefore, this experiment shows our proposed hyperedge link prediction task’s eectiveness, which is more helpful than the existing pre-training methods merely model each user’s buying history in a chronological way. Table 4: Impact of dierent pre-training tasks (NDCG@K). − denotes only considering the following pre-training tasks. In the Figure 6, we analysis dierent dimensions of items inuence on the proposed model and the baselines, we randomly choose three baselines to compare with our proposed models. The negative test sampling is 100, and the evaluation metric is NDCG@1. As shown in Figure 6a, our proposed modelsHSeqRecand HSeqRec-fuse could always outperform GRU4Rec, SASRec and HyperRec when the dimension is {20, 40, 60, 100}. Moreover, when the number of dimensions is larger in the gure, the pre-training phase is more useful to the recommendation performance. It is because the pre-training data augmentation can learn more information about the sparse dataset when the number of dimensions is larger. In Figure 6b, we compare the NDCG@1 on models training. GRU4Rec is early stopped because of not updating NDCG@1 for 10 epochs, so the line is not complete. In general, our proposed two methods always achieve the best performance, especially the HSeqRec-fuse, whose NDCG@1 is still updating after 25 epochs. Our model contains four parts, item features extraction via selfsupervised learning tasks, learning multi-scale embeddings via dynamic hierarchical hyperbolic hypergraphs, item embeddings enhancement via similar users groups and learning user preference for sequential recommendation. Therefore, we will analyse the time complexity of four separate parts, respectively. For the self-supervised learning phase, we have three tasks. The rst task, for each user, we should randomly select two dierent items to mask, so the time complexity is𝑂 (𝑈 ×𝐼×(𝐼−1)), where 𝑈is the number of users, and𝐼is each user’s related items. Since 𝐼≪ 𝑈, the time complexity could approximately be𝑂 (𝑈 ). The second task is that we randomly mask a subsequence for each user, and the length is 2, so the time complexity is about𝑂 (𝑈 × 𝐼) ≈ 𝑂 (𝑈 ). For the last task, we should nd each user’s neighbours and two-hop neighbours, and then build the hyperedge connecting each user’s items within a timestamp. Therefore, if there are𝑇 timestamps, the time complexity is𝑂 (𝑈 ×𝑇×N×N), where Nis the average numbers of each user’s neighbours andNis the average number of each user’s two-hop neighbours. Since𝑇≪ 𝑈 , N≪ 𝑈 , N≪ 𝑈, the complexity is approximately𝑂 (𝑈 ). The pre-training tasks’ time complexity can be added together, and could approximately ignore the smaller magnitude terms, so the overall time complexity of pre-training is𝑂 (𝑈 ). In practice, it is unnecessary to do the pre-training phase every time. The learned pre-training features could be learned oine and stored. Even if new data comes, it can implement incremental training. Therefore, the time complexity is acceptable. For the multi-scale dynamic hierarchical hyperbolic hypergraphs learning, we have three time-spans views including month, quarter, and year. For monthly hyperbolic hypergraph neural network learning, we consider each user’s items within a month connected by a hyperedge. We perform hyperbolic-based hypergraph convolutional neural network on it, so the time complexity is𝑂 (𝑇× 𝐼 × N×𝐿 × 𝐷), whereNis the average number of neighbours for each hyperedge connected to the item, because of the aggregation function in the model. 𝐿 is the number of layers, and 𝐷 is the embedding’s dimension. The time complexity is the same as the quarterly and yearly model, but the number of quarter𝑇and year 𝑇is less than𝑇. To sum up, this module’s time complexity is 𝑂 ((𝑇+𝑇+𝑇) ×𝐼 ×N×𝐿 ×𝐷) ≈ 𝑂 (𝐼), because𝑇,𝑇,𝑇, 𝐿 are countable, the average number of item’s neighbours isN≪ 𝐼 and the dimension is set as 100 in our model. What’s more, this module can also learn oine due to our step-by-step learning style. In practice, this will signicantly save the time of the recommendation. The learning strategy is the same as the pre-training phase. If there are new data, this module can also do incremental training. Therefore, the time complexity of this module seems good. For the item embeddings enhancement via similar users groups module, the training method and the hypergraph construction strategy is the same as the last module. However, this module focuses on the relationship between users and will construct multiple hypergraphs for each user. Therefore, the time complexity is 𝑂 (𝑇×𝑈 × N× N×𝐿 × 𝐷), where𝐼means each user’s related items,Nis the average number of neighbours of each user’s related items andNis our dened similar user’s related items in section 3.3. Similarly, the approximate time complexity is 𝑂 (𝑈 )becauseN≪ 𝑈,N≪ 𝑈. This module can also train oine in advance, thanks to our step-by-step training strategy. In practice, if there are new users fed into the model, it is convenient to train new users’ item embeddings. If some existing users are fed into the model again, it is convenient to train incrementally based on the previous version of item embeddings. In the last module, learning user preference for sequential recommendation module, we use the Transformer to obtain each user’s preference embedding. For each user, we feed user’s sequence into the Transformer (self-attention layer), and therefore the time complexity is𝑂 (𝑈 × 𝐼× 𝐷) ≈ 𝑂 (𝑈 )because of𝐼≪ 𝑈as claimed before. To train the recommendation, we sample 1 negative training item to maximize the dierence between the positive user-item pair score and the negative one. The recommendation part’s time complexity is𝑂 (𝐼× (𝑁 𝐸𝐺 + 1)). Since the user preference embedding learning part and the recommendation part are learned end to end, the whole time complexity of the last module is 𝑂 (𝑈 × 𝐼× 𝐷 × 𝐼× (𝑁 𝐸𝐺 + 1)) ≈ 𝑂 (𝑈 )because of𝐼≪ 𝑈and 𝑁 𝐸𝐺 ≪ 𝑈 . To sum up, the nal time complexity of the proposed model is the maximum time complexity of the above modules, which is 𝑚𝑎𝑥 (𝑂 (𝑈 ), 𝑂 (𝐼 )), so it is acceptable. Early neural recommendations building on typical deep neural networks mostly use recurrent neural networks (RNN) or convolutional neural networks (CNN) to capture the temporal patterns from users’ historical records. Specically, Quadrana et al. [19] design a multi-level RNN structure for learning temporal patterns in a sequence. Although RNN-based methods have their advantages in sequential learning, user-item interactions usually contain noisy information. Only learning these relations is far from achieving more reliable recommendation system. Unlike RNN-based method, Yuan et al. [44] use CNN to learn from user-item sequences because CNN-based methods can not only capture long-term dependencies but also have the character of translation invariance, making the model more stable for various sequential orders. Later, attention is introduced in recommendation models [16,22,23,34]. In particular, Kang et al. [12] treat the user-item sequence as a sentence and model the temporal relations via a transformer model [26]. The transformer takes a self-attention unit to translate sequences as entity embeddings and position embeddings, which can be used to the downstream recommendation system. In the real world, however, users and items contain more nonlinear relations with dierent topological structures. To this end, graph-based sequential modelling approaches become more and more popular to model the recommendation data. By modelling graph structure data, graph neural networks (GNN) are introduced recently. As a conceptional extension of linear CNNs, GNN-based method [7,35,39] take the directed graph as input, and capture the interdependence directly on the graph. For example, Wu et al. [35] treat each user or item as a node in the graph, and transform the user-item sequence as a path. With GNN-based model, they can learn both users and items embeddings over the whole graph. Later, more advanced technologies have been introduced such as, self-supervised learning [11,33,37,38,40], group awareness [8,10,31] and so on. In particular, Hwang et al. [9] propose selfsupervised auxiliary learning tasks to predict meta-paths to capture rich-information of a heterogeneous graph, and thereby improve the primary task to do link prediction. Recently, instead of traditional graph, constructing hypergraphs to learn the data structure to do recommendation become a popular approach. Yu et al. [43] design a multi-channel hypergraph convolutional network to enhance social recommendation by exploiting high-order user relations, which shows great improvement. However, it ignores the sequential information for users. Xia et al. [36] model session data as a hypergraph and propose a dual channel hypergraph convolutional network for session-based recommendation. Wang et al. [30] also construct hypergraphs for each session to model the item correlations, and they also introduce a hypergraph attention layer to exibly aggregate correlated items in the session and infer next interesting item. Although the above two methods both consider some temporal information, it is within a specic session, not a whole sequence. Wang et al. [29] propose a novel next-item recommendation framework empowered by sequential hypergraphs to incorporate the short-term item correlations while modeling the dynamics over time and across users. The advantage is to consider sequential information, but all the above three hypergraph-based methods ignore the severe sparsity problem of hypergraphs. In this work, we focus on the sparse problem of most existing hypergraph-based sequential recommendation and on the lacking exploitation of hidden hyperedges among users problem, and proposeHyperbolicHypergraph representation learning method for SequentialRecommendation (HSeqRec) with pre-training phase. Experiments show that our proposed model outperforms the stateof-the-art sequential recommendations and each of the components contributes to the whole architecture. However, on a sparse dataset, the data augmentation improves the recommendation, while on a dense dataset, it is useless and even worse, which proves dense datasets do not need data augmentation to achieve better performance. In future work, we will explore multiple sequential ways to model the sequential recommendation data on graphs. This work is supported by the Australian Research Council (ARC) under Grant No. DP200101374 and LP170100891, and NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.