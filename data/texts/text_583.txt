Keywords: Tensor estimation, latent permutation, diverging dimensionality, phase transition, statisticalcomputational eﬃciency. Higher-order tensor datasets are rising ubiquitously in modern data science applications, for instance, recommendation systems (Baltrunas et al., 2011; Bi et al., 2018), social networks (Bickel and Chen, 2009), genomics (Hore et al., 2016), and neuroimaging (Zhou et al., 2013). Tensor provides eﬀective representation of data structure that classical vector- and matrix-based methods fail to capture. One example is music recommendation system (Baltrunas et al., 2011) that records ratings of songs from users on various contexts. This three-way tensor of user × song × context allows us to investigate interactions of users and songs in a context-speciﬁc manner. Another example is network dataset that records the connections among a set of nodes. Pairwise interactions are often insuﬃcient to capture the complex relationships, whereas multi-way interactions improve the understanding of networks in molecular system (Young et al., 2018) and social networks (Han et al., 2020). In both examples, higher-order tensors represent multi-way interactions in an eﬃcient way. Tensor estimation problem cannot be solved without imposing structures. An appropriate reordering of tensor entries often provides eﬀective representation of the hidden salient structure. In the music recommendation example, suppose that we have certain criteria available (such as, similar- We consider the problem of structured tensor denoising in the presence of unknown permutations. Such data problems arise commonly in recommendation system, neuroimaging, community detection, and multiway comparison applications. Here, we develop a general family of smooth tensor models up to arbitrary index permutations; the model incorporates the popular tensor block models and Lipschitz hypergraphon models as special cases. We show that a constrained least-squares estimator in the block-wise polynomial family achieves the minimax error bound. A phase transition phenomenon is revealed with respect to the smoothness threshold needed for optimal recovery. In particular, we ﬁnd that a polynomial of degree up to (m − 2)(m + 1)/2 is suﬃcient for accurate recovery of order-m tensors, whereas higher degree exhibits no further beneﬁts. This phenomenon reveals the intrinsic distinction for smooth tensor estimation problems with and without unknown permutations. Furthermore, we provide an eﬃcient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. The eﬃcacy of our procedure is demonstrated through both simulations and Chicago crime data analysis. Figure 1: (a): Illustration of order-m d-dimensional permuted smooth tensor models with m = 2. (b): Phase transition of mean squared error (MSE) (on −log α and tensor order m. Bold dots correspond to the critical smoothness level above which higher smoothness exhibits no further beneﬁts to tensor estimation. ities of music genres, ages of users, and importance of contexts) to reorder the songs, users, and contexts. Then, the sorted tensor will exhibit smooth structure, because entries from similar groups tend to have similar values. Similar observation applies to network examples. An m-uniform hypergraph network can be represented by an order-m adjacency tensor, with entries indicating the presence and absence of m-way interactions among a set of nodes. Suppose the characteristics of individual nodes are available so that one can rearrange nodes based on their similarities. Then, the sorted adjacency tensor will exhibit smooth structure by the same reason. In this article, we develop a permuted smooth tensor model based on the aforementioned motivation. We study a class of structured tensors, called permuted smooth tensor model, of the following form: where π : [d] → [d] is an unknown latent permutation, Θ is an unknown order-m d-dimensional signal tensor, and f is an unknown multivariate function with certain notion of smoothness, and Θ ◦ π denotes the permuted tensor after reordering the indices along each of the m modes. Figure 1(a) shows an example of this generative model for the matrix case m = 2. Our primary goal is to estimate the permuted smooth signal tensor Θ ◦π from the noisy tensor observation Y of arbitrary order m. We develops a suite of statistical theory, eﬃcient algorithms, and related applications for permuted smooth tensor models (1). Our contributions are summarized below. First, we develop a general permuted α-smooth tensor model of arbitrary smoothness level α ≥ 0. We establish the statistically optimal error rate and its dependence on model complexity, including tensor order, tensor dimension, smoothness level, signal-to-noise ratio, and unknown permutations. Table 1 summarizes the comparison of our work with previous results. Our framework substantially generalizes earlier works which focus on only matrices with m = 2 (Gao et al., 2015; Klopp et al., 2017) or Lipschitz function with α = 1 (Balasubramanian, 2021; Li et al., 2019). The generalization Table 1: Comparison of our results with previous work. rate (omitting the log term) for ∞-smooth tensors. Our results allow general tensors of arbitrary smoothness level α ≥ 0; See Theorems 1-3 in Sections 4-5. enables us to obtain results previously impossible: i) As tensor order m increases, we demonstrate the failure of pervious clustering-based algorithms (Balasubramanian, 2021; Gao et al., 2015), and we develop a new block-wise polynomial algorithm for tensors of order m ≥ 3; ii) As smoothness α increases, we demonstrate that the error rate converges to a fast rate O(d the conjectured lower bound O(d results showcase the accuracy gain of our new approach, as well as the intrinsic distinction between matrices and higher-order tensors. Second, we discover a phase transition phenomenon with respect to the smoothness needed for optimal recovery in model (1). Figure 1(b) plots the dependence of estimation error in terms of smoothness level α for tensors of order m. We characterize two distinct error behaviors determined by a critical smoothness threshold; see Theorems 1-2 in Section 4. Speciﬁcally, the accuracy improves with α in the regime α ≤ m(m−1)/2, but then it becomes a constant of α in the regime α > m(m−1)/2. The results imply a polynomial of degree (m−2)(m+1)/2 = [m(m−1)/2−1] is suﬃcient for accurate recovery of order-m tensors of arbitrary smoothness in model (1), whereas higher degree brings no further beneﬁts. The phenomenon is distinctive from matrix problems (Klopp et al., 2017; Gao et al., 2015) and classical non-permuted smooth function estimation (Tsybakov, 2009), thereby highlighting the fundamental challenges in our new setting. These statistical contributions, to our best knowledge, are new to the literature of general permuted smooth tensor problems. Third, we propose two estimation algorithms with accuracy guarantees: the least-squares estimation and Borda count estimation. The least-squares estimation, although being computationally hard, reveals the fundamental model complexity in the problem. The result serves as the benchmark and a useful guide to the algorithm design. Furthermore, we develop an eﬃcient polynomial-time Borda count algorithm that provably achieves optimal rate under monotonicity assumptions. The algorithm handles a broad range of data types, including continuous and binary observations. Lastly, we illustrate the eﬃcacy of our method through both simulations and data applications. A range of practical settings are investigated in simulations, and we show the outperformance of our method compared to alternative approaches. Application to Chicago crime data is presented to showcase the usefulness of our method. We identify the key global pattern and pinpoint local smooth structure in the denoised tensor. Our method will help practitioners eﬃciently analyze tensor datasets in various areas. Toward this end, the package and all data used are released at CRAN. Our work is closely related to but also clearly distinctive from several lines of existing research. We review related literature for comparison. Structure learning with latent permutations. The estimation problem of (1) falls into the general category of structured learning with latent permutation. Models involving latent permutations have recently received a surge of interest, include graphons (Chan and Airoldi, 2014; Klopp et al., 2017), stochastic transitivity models (Chatterjee, 2015; Shah et al., 2019), statistical seriation (Flammarion et al., 2019; H¨utter et al., 2020), and graph matching (Ding et al., 2021; Livi and Rizzi, 2013). These methods, however, are developed for matrices; the tensor counterparts are far less well understood. Table 1 summarizes the most related works to ours. Pananjady and Samworth (2021) studied the permuted tensor estimation under isotonic constraints. We ﬁnd that our smooth model results in a much faster rate O(d models. The works (Balasubramanian, 2021; Li et al., 2019) studied similar smooth models as ours, but we gain substantial improvement in both statistics and computations. Balasubramanian (2021) developed a (non-polynomial-time) clustering-based algorithm with a rate O(d Li et al. (2019) developed a (polynomial-time) nearest neighbor estimation with a rate O(d Neither approach investigates the minimax optimality. By contrast, we develop a polynomial-time algorithm with a fast rate O(d safeguarded by matching a minimax lower bound. Low-rank tensor models. There is a huge literature on structured tensor estimation under low-rank models, including CP models (Kolda and Bader, 2009; Sun et al., 2017), Tucker models (Zhang and Xia, 2018), and block models (Wang and Zeng, 2019). These models belong to parametric approaches, because they aim to explain the data with a ﬁnite number of parameters (i.e., decomposed factors). Our permuted smooth tensor model utilizes a diﬀerent measure of model complexity than the usual low-rankness. We use inﬁnite number of parameters (i.e., smooth functions) to allow growing model complexity. In this sense, our method belongs to nonparametric approaches. The comparison and beneﬁts of nonparametric methods over parametric ones were discussed previously (Pananjady and Samworth, 2021; Li et al., 2019; Gao et al., 2015; Bickel and Chen, 2009; Shah et al., 2019). Nonparametric regression. Our model is also related to nonparametric regression (Tsybakov, 2009). One may view the problem (1) as a nonparametric regression, where the goal is to learn the function f based on scalar response Y(i see Figure 1(a). However, the unknown permutation π signiﬁcantly inﬂuences the statistical and computational hardness of the problem. This latent π leads to a phase transition behavior in the estimation error; see Figure 1(b) and Sections 4. We reveal two components of error for the problem, one for nonparametric error and the other for permutation error. The impact of unknown permutation hinges on tensor order and smoothness in an intriguing way (see Theorems 1-3). This is clearly contrary to classical nonparametric regression. Graphon and hypergraphon. Our work is also connected to graphons and hypergraphons. Graphon is a measurable function representing the limit of a sequence of exchangeable random graphs (matrices) (Klopp et al., 2017; Gao et al., 2015; Chan and Airoldi, 2014). Similarly, hypergraphon (Zhao, 2015; Lov´asz, 2012) is introduced as a limiting function of m-uniform hypergraphs, i.e., a generalization of graphs in which edges can join m vertices with m ≥ 3. While both our model (1) and hypergraphon focus on function representations, there are two remarkable diﬀerences. First, unlike the matrix case where graphon is represented as bivariate functions (Lov´asz, 2012), hypergraphons for m-uniform hypergraphs should be represented as (2 see Zhao (2015, Section 1.2). Our framework (1) represents the function using m coordinates only, and in this sense, the model shares the common ground as simple hypergraphons (Balasubramanian, 2021). We compare our method to earlier work in theory (Table 1 and Sections 4-5) and in numerical studies (Section 6). Second, unlike typical simple hypergraphons where the design points are random, our generative model uses deterministic design points. These two choices lead to diﬀerent analysis in the same spirit as random- vs. ﬁxed-designs in nonparametric regression (Wasserman, 2006; Tsybakov, 2009). We use N d-set with d ∈ N For two positive sequences {a c > 0, and a the ﬂoor function bac is the largest integer no greater than a, and the ceiling function dae is the smallest integer no less than a. We use O(·) to denote big-O notation hiding logarithmic factors, and ◦ the function composition. Let Θ ∈ R Θ(i, . . . , i such that (Θ◦π)(i shorthand notation Θ(ω) for tensor entries with indices ω = (i valued tensor if its entries take value on {0, 1}-labels, and a continues-valued tensor if its entries take values on a continuous scale. We deﬁne the Frobounis norm kΘk and the ∞-norm kxk to denote all permutations on [d], while Π(d, k) = {π : [d] → [k]} the collection of all onto mappings from [d] to [k]. An event A is said to occur with high probability if P(A) tends to 1 as the tensor dimension d → ∞. The rest of the paper is organized as follows. Section 2 presents the permuted smooth tensor model and its connection to smooth function representation. In Section 3, we establish the approximation error based on block-wise polynomial approximation. Then, we develop two estimation algorithms with accuracy guarantees: the least-squares estimation and Borda count estimation. Section 4 presents a statistically optimal but computationally challenging least-squares estimator. Section 5 presents a polynomial-time Borda count algorithm with a provably same optimal rate under monotonicity assumptions. Simulations and data analyses are presented in Section 6. The proofs for the main theorems are provided in Section 7. We conclude the paper with a discussion in Section 8. Extra simulation results and technical lemmas are deferred to Appendix. Suppose we observe an order-m d-dimensional data tensor from the following model, where π : [d] → [d] is an unknown latent permutation, Θ ∈ R under certain smoothness (to be speciﬁed in next paragraph), and E is a noise tensor consisting of zero-mean, independent sub-Gaussian entries with variance bounded by σ and non-identically distributed entries in noise E. For instance, we allow binary tensor problem where entries in Y are {0, 1}-labels from Bernoulli distribution, in which case, the noise variance to denote the set of positive integers, and N= N∪{0}. We use [d] = {1, . . . , d} for ) the tensor entry indexed by (i, . . . , i). We use Θ ◦π to denote the permuted tensor depends on the mean. Our model (2) is applicable to a wide range of data types including continuous and binary tensors. We now describe the smooth model on the signal Θ. Suppose that there exists a multivariate function f : [0, 1] Assume the generative function f is in the α-H¨older smooth family (Wasserman, 2006; Tsybakov, 2009). Deﬁnition 1 (α-H¨older smooth). Let α ≥ 0. A function f : [0, 1] denoted as f ∈ H(α, L), if there exists a polynomial function Poly that for all x, x H¨older smooth function class is one of the most popular function classes considered in the nonparametric regression literature (Klopp et al., 2017; Gao et al., 2015). In addition to the function class H(α, L), we also deﬁne the smooth tensor class based on discretization (3), P(α, L) = Combining (2) and (3) yields our proposed permuted smooth tensor model. The unknown parameters are the smooth tensor Θ ∈ P(α, L) and latent permutation π ∈ Π(d, d). The generative model is visualized in Figure 1(a) for the case m = 2 (matrices). For ease of presentation, we mainly consider the tensor model of equal dimension and same permutations along m modes. The results for non-symmetric tensors with m distinct permutations are similar but require extra notations; we assess this general case in Section 6. We give two examples to show the applicability of our permuted smooth tensor model. Example 1 (Four-player game tensors). Consider the tournament of a four-player board game. Suppose there are in total d players, among which all combinations of four have played with each other. The tournament results are summarized as an order-4 (non-symmetric) tensor, with entries encoding the winner out of the four. Our model is then given by In this setting, we can interpret the permutation π as the unknown ranking among d players, and the function f as the unknown four-player interaction. Players with similar ranking may have similar performance reﬂected by the smoothness of f. For example, a variant of popular Plackett-Luce model (Chen et al., 2021) considers the parametric form f(x exp(βx the function from the data in a nonparametric approach. ∈ [0, 1]and a universal constant L > 0. )/exp(βx). By contrast, our model leaves the form of f unspeciﬁed, and we learn Example 2 (Co-authorship networks). Consider a co-authorship network consisting of d nodes (authors) in total. We say there exists a hyperedge of size m between nodes (i authors i represented as an order-m (symmetric) adjacency tensor. Our model is then expressed as In this setting, we can interpret the permutation π as the aﬃnity measures of authors, and the function f represents the m-way interaction among authors. The parametric model (Wang and Li, 2020) imposes logistic function f(x nonparametric model allows unknown f and learns the function directly from data. Our general strategy for estimating the permuted smooth tensor is based on the block-wise tensor approximation. In this section, we ﬁrst introduce the tensor block model (Wang and Zeng, 2019; Han et al., 2020). Then, we extend this model to block-wise polynomial approximation. Tensor block models describe a checkerboard pattern in a signal tensor. The block model provides a meta structure to many popular models including the low-rankness (Young et al., 2018), latent space models (Wang and Li, 2020), and isotonic tensors (Pananjady and Samworth, 2021). Here, we use tensor block models as a building block for estimating permutated smooth models. Speciﬁcally, suppose that there are k clusters among d entities, and the cluster assignment is represented by a clustering function z : [d] → [k]. Then, the tensor block model assumes that the entries of signal tensor Θ ∈ R clustering function z; that is, Here, the core tensor S collects the entry values of m-way blocks; the core tensor S and clustering function z ∈ Π(d, k) are parameters of interest. A tensor Θ satisfying (5) is called a block-k tensor, where k is often assumed much smaller than d. Tensor block models allow various data types, as shown below. Example 3 (Gaussian tensor block model). Let Y be a continuous-valued tensor. The Gaussian tensor block model draws independent entries according to Y(i The mean model belongs to (5), and the noise has subGaussian parameter σ block model has served as the statistical foundation for many tensor clustering algorithms (Wang and Zeng, 2019; Han et al., 2020). Example 4 (Stochastic tensor block model). Let Y be a binary-valued tensor. The stochastic tensor block model draws independent Bernoulli entries according to P(Y(i S(z(i eter σ bounded by 1/4. The stochastic tensor block model is useful for community detection in multi-relational networks (Bickel and Chen, 2009; Gao et al., 2015). , . . . , ihave co-authored at least one paper. The resulting m-uniform hypergraph is ), . . . , z(i)). The mean model also belongs to (5), and the noise has subGaussianity param- Tensor block models have shown great success in discovering hidden group structures for many applications (Wang and Zeng, 2019; Han et al., 2020). Despite the popularity, the constant block assumption is insuﬃcient to capture delicate structure when the signal tensor is complicated. This parametric model aims to explain data with a ﬁnite number of blocks; such an approach is useful when the sample outsizes the parameters. Our nonparametric model (3), by contrast, uses inﬁnite number of parameters (i.e., smooth functions) to allow growing model complexity. Our next section will shift the goal of tensor block model from discovering hidden group structures to approximating the generative function f in (3). In our setting, the number of blocks k should be interpreted as a resolution parameter (i.e., a bandwidth) of the approximation, similar to the notion of number of bins in histogram and polynomial regression (Wasserman, 2006). The block tensor (5) can be viewed as a discrete version of piece-wise constant function with α = 0 in (4). This connection motivates us to use block-wise polynomial tensors to approximate α-H¨older functions. Now we extend (5) to block-wise polynomial models. We introduce some additional notations. For a given block number k, we use z ∈ Π(d, k) to denote the canonical clustering function that partitions [d] into k equally-sized clusters such that z(i) = dki/de, for all i ∈ [d]. The collection of inverse images {z into k disjoint and equal-sized subsets. We use E of k Let ∆ ∈ E degree-` polynomial tensors within each block ∆ ∈ E block-k degree-` polynomial tensors, where Poly block ∆; that is, a constant function Poly hβ, ωi + β polynomial function. Note that the degree-0 polynomial block tensor reduces to the constant block model (5). We generalize the constant block model to degree-` polynomial block tensor (6), in a way that is analogous to the generalization from k-bin histogram to k-piece-wise polynomial regression in nonparametric statistics (Wasserman, 2006). Smoothness of the function f in (3) plays an important role in the block-wise polynomial approximation. The following lemma explains the role of smoothness in the approximation. Lemma 1 (Block-wise polynomial tensor approximation). Suppose Θ ∈ P(α, L). Then, for every block number k ≤ d, and degree ` ∈ N Lemma 1 implies that we can always ﬁnd a block-wise polynomial tensor close to the signal tensor generated from α-H¨older smooth function f. The approximation error decays with block number k and degree min(α, ` + 1). disjoint and equal-sized subsets in [d], such that denote the element in E. We propose to approximate the signal tensor Θ in (3) by B(k, `) =B ∈ R: B(ω) =Poly(ω)1{ω ∈ ∆} for all ω ∈ [d] We develop two estimation methods based on the block-wise polynomial approximation. We ﬁrst introduce a statistically optimal but computationally ineﬃcient least-squares estimator. The leastsquares estimation serves as a statistical benchmark because of its minimax optimality. In Section 5, we will present a polynomial-time algorithm with a provably same optimal rate under monotonicity assumptions. We propose the least-squares estimation for model (2) by minimizing the Frobounis loss over the block-k degree-` polynomial tensor family B(k, `) up to permutations, The least-squares estimator ( k and the polynomial degree `. The optimal choice (k Our next Theorem 1 establishes the error bound for the least-squares estimator (7). Note that Θ and π are in general not separably identiﬁable; for example, when the true signal is a constant tensor, then every permutation π ∈ Π(d, d) gives equally good ﬁt in statistics. We assess the estimation error on the composition Θ ◦ π to avoid this identiﬁability issue. For two order-m d-dimensional tensors Θ Theorem 1 (Least-squares estimation error). Consider the order-m (m ≥ 2) permuted smooth tensor model (2) with Θ ∈ P(α, L). Let ( a given (k, `). Then, for every k ≤ d and degree ` ∈ N with high probability. In particular, setting ` yields the optimized error rate Here, the constants c tensor dimension d. The closed-form expressions are provided in Section 7.2. We discuss the asymptotic error rate as d → ∞ while treating other model conﬁgurations ﬁxed. The ﬁnal least-squares estimation rate (9) has two sources of error: the nonparametric error d and the permutation error d view each tensor entry as a data point, so sample size is the total number of entries, d unknown permutation results in log(d!) ≈ d log d complexity, whereas the unknown generative function results in d estimating the function f becomes relatively easier compared to estimating the permutation π. This intuition coincides with the fact that the permutation error dominates the nonparametric error when α ≥ m(m − 1)/2. We now compare our results with existing work in the literature. , Θ, deﬁne the mean squared error (MSE) by MSE(Θ, Θ) = dkΘ− Θk. Remark 1 (Comparison to non-parametric regression). In the vector case with m = 1, our model reduces to the one-dimensional regression problem such that where θ rate under the choice of ` metric minimax rate for α-H¨older smooth functions (Tsybakov, 2009) with known permuted design points {π(i)} permutation rate log d in (10). Remark 2 (Breaking previous limits on matrices/tensors). In the matrix case with m = 2, Theorem 1 implies that the best rate is obtained under ` imation. This result is consistent with existing literature on smooth graphons (Bickel and Chen, 2009; Gao et al., 2015; Klopp et al., 2017), where constant block model (see Section 3.1) has been develop for accurate estimation. Earlier work (Balasubramanian, 2021) suggests that constant block approximation (` minimax optimal for tensors of order m ≥ 3. Our Theorem 1 disproves this conjecture, and we reveal a much faster rate d ramanian, 2021) for suﬃciently smooth tensors. We demonstrate that a polynomial up to degree (m − 2)(m + 1)/2 is suﬃcient (and necessary; see Theorem 2 below) for accurate estimation of order-m permuted smooth tensors. For example, permuted α-smooth tensors of order-3 require quadratic approximation (` diﬀerence from matrices and highlight the challenges with tensors. We now show that the rate in (9) cannot be improved. The lower bound is obtained via informationtheoretical analysis and thus applies to all estimators including, but not limited to, the least-squares estimator (7) and Borda count estimator introduced in next section. Theorem 2 (Minimax lower bound). For any given α ≥ 0, the estimation problem based on model (1) obeys the minimax lower bound where c The lower bound in (11) matches the upper bound in (9), demonstrating the statistical optimality of least-squares estimator (7). The two-component error reveals the intrinsic model complexity: the permutation error d tensors. This is clearly contrary to classical nonparametric regression. Remark 3 (Phase transition). We conclude this section by presenting an interesting phase transition phenomenon. Figure 1(b) plots the convergence rate of estimation error based on Theorems 1-2. We ﬁnd that the impact of unknown permutation hinges on the tensor order and smoothness. The accuracy improves with smoothness in the regime α ≤ m(m − 1)/2, but then it becomes a constant of smoothness in the regime α > m(m − 1)/2. The result implies a polynomial of degree = f(i/d) and unknown π ∈ Π(d, d). A similar analysis of our Theorem 1 shows the error infsupPMSE(ˆΘ ◦ ˆπ, Θ ◦ π) & cd+ cdlog d , c> 0 are the same constants as in Theorem 1. ≈ (m − 2)(m + 1)/2 is suﬃcient for accurate recovery of order-m tensors, whereas higher degree brings no further beneﬁts. This full picture of error dependence, to our best knowledge, is new to the literature of permuted smooth tensors. At this point, we should point out that computing the least-squares optimizer ( generally computationally hard, even in the simple matrix case (Gao et al., 2015). In this section, we propose an eﬃcient polynomial-time Borda count algorithm. We show that Borda count estimator provably achieves the same convergence rate as the minimax lower bound (11) under monotonicity assumptions. We introduce a notion of β-monotonicity for the generative functions. Deﬁnition 2 (Weakly β-monotonicity). A function f : [0, 1] denoted as f ∈ M(β), if there exists a small tolerance δ . d where we deﬁne score function g(i) = d Our β-monotonicity condition extends the strictly monotonic degree condition in the graphon literature (Chan and Airoldi, 2014); the latter is a special case of our deﬁnition with β = 1, m = 2 and δ = 0. A large value of β in (12) implies the steepness of g. The introduction of tolerance δ relaxes the condition by allowing for small ﬂuctuations. Our β-monotonicity condition is also related to isotonic functions (Pananjady and Samworth, 2021) which consider more restricted coordinatewise monotonicity; i.e., f(x The β-monotonicity condition allows us to eﬃciently estimate the permutation π. Before presenting the theoretical guarantees, we provide the intuition here. The exponent β measures the diﬃculty for estimating the permutation π. Consider the noisy observation Y from model (1). We deﬁne the empirical score function τ : [d] → R as The permuted score function τ ◦ π Therefore, a good estimate ˆπ should make the permuted score function τ ◦ ˆπ increasing. Notice that the estimated permutation ˆπ could be diﬀerent from the oracle permutation π due to the noise. We ﬁnd that a larger β guarantees a faster consistency rate of ˆπ. A large β implies large gaps of |g(i)−g(j)| for i 6= j. Therefore, we obtain similar orderings of {τ(i)} and after the addition of noise. This intuition is well represented by the following lemma. Lemma 2 (Permutation error). Consider the permuted smooth tensor model with f ∈ M(β). Let ˆπ be the permutation such that the permuted empirical score function τ ◦ ˆπ increasing. Then, with high probability, Figure 2: Illustration of Borda count estimation. We ﬁrst sort tensor entries using the proposed procedure, and then estimate the signal by block-wise polynomial approximation. Now we introduce the Borda count estimation that consists of two stages. The full estimation procedure is illustrated in Figure 2. 1. Sorting stage: The purpose of the sorting is to rearrange the observed tensor Y so that the score function τ of sorted tensor is monotonically increasing. We deﬁne a permutation ˆπ that 2. Block-wise polynomial approximation stage: Given sorted observation signal tensor by block-wise polynomial tensor based on the following optimization, where B(k, `) denotes the block-k degree-` tensor class in (6). An example of this procedure is shown in Figure 2(c). The estimator k and polynomial degree `. The optimal choice of (k least-squares estimator in (7) requires a combinatoric search with exponential-time complexity for estimating the permutation. By contrast, the estimator (14) requires only the estimation of degree` polynomial within k canonical blocks. Therefore, the Borda count estimator is polynomial-time eﬃcient. Our algorithm implementation is available in CRAN. In this section, we show that the Borda count estimation achieves both computational eﬃciency and satanical accuracy. The computational complexity of Borda count estimation is polynomial in tensor dimension d. In the sorting stage, computing the empirical score function τ requires O(d sorting the {τ(i)} stage, we compute k requires O((d/k) Combining the two steps yields the total complexity at most O(d comparable with existing eﬃcient tensor estimation algorithms in other settings (Li et al., 2019; Zhang and Xia, 2018). The following theorem ensures the statistical accuracy of the Borda count estimator. Theorem 3 (Estimation error for Borda count algorithm). Consider the permuted smooth tensor model with f ∈ H(α, L) ∩ M(β). Let ( given (k, `). Then, for every k ≤ d and degree ` ∈ N with high probability. Furthermore, denote a constant c(α, β, m) := Then, setting ` where c Remark 4 (Suﬃciently smooth tensors). When the generative function is inﬁnitely smooth (α = ∞) with Lipschitz monotonic score (β = 1), our estimation error (16) becomes under the choice of degree and block number Now, we compare the rate (17) with the classical low-rank estimation (Wang and Li, 2020; Zhang and Xia, 2018; Kolda and Bader, 2009). The low-rank tensor model with a constant rank is known to have MSE rate O(d the nearly same rate up to the negligible log term. Compared to low-rank models, we utilize a diﬀerent measure of model complexity. When the underlying signal is precisely low-rank, then rank might be a reasonable measure for model complexity. However, if the underlying signal is high rank but has certain shape structure, then our nonparametric approach may better capture the intrinsic model complexity. Remark 5 (Comparison with least-squares estimation). The three terms in the estimation bound (15) correspond to approximation error (Lemma 1), nonparametric error (Theorem 1), and permutation error (Lemma 2), respectively. We ﬁnd that the Borda count estimator achieves the same minimax-optimal rate as the least-squares estimator for suﬃciently smooth tensors under Lipschitz score condition β = 1. The least-squares estimator requires a combinatoric search with exponentialtime complexity. By contrast, the Borda count estimator is polynomial-time solvable. Therefore, Borda count algorithm enjoys both statistical accuracy and computational eﬃciency. Hyperparameter tuning. Our algorithm has two tuning parameters (k, `). The theoretically optimal choices of (k, `) are given in Theorems 1 and 3. In practice, since model conﬁguration is unknown, we search (k, `) via cross-validation. Based on our theorems, a polynomial of degree `= (m −2)(m + 1)/2 is suﬃcient for accurate recovery of order-m tensors, whereas higher degree brings no further beneﬁt. The practical impacts of hyperparameter tuning are investigated in Section 6. , c, c> 0 are the same constants as in Theorem 1. We simulate order-3 d-dimensional tensors based on the permuted smooth tensor model (3). Both symmetric and non-symmetric tensors are investigated. The symmetric tensors are generated based on functions f in Table 2, and the non-symmetric set-up is described in Appendix A.2. The Table 2: Smooth functions in simulation. We deﬁne the numerical CP/Tucker rank as the minimal rank r for which the relative approximation error is below 10 estimated from a 100 × 100 × 100 signal tensor generated by (3). generative functions involve compositions of operations such as polynomial, logarithm, exponential, square roots, etc. Notice that considered functions cover a reasonable range of model complexities from low rank to high rank. Two types of noise are considered: Gaussian noise and Bernoulli noise. For the Gaussian model, we simulate continuous-valued tensors with i.i.d. noises drawn from N(0, 0.5 tensor Θ ◦ π. The permutation π is randomly chosen. For space consideration, only results for Models 1, 3, and 5 are presented in the main paper. The rest is presented in Appendix A.1. We ﬁrst examine impacts of model complexity to estimation accuracy. We then compare Borda count estimation with alternative methods under a range of scenarios. Extra simulation results and extensions are deferred to Appendix. Impacts of the number of blocks, tensor dimension, and polynomial degree. The ﬁrst experiment examines the impact of the block number k and degree of polynomial ` for the approximation. We ﬁx the tensor dimension d = 100, and vary the number of blocks k ∈ {1, . . . , 15} and polynomial degree ` ∈ {0, 1, 2, 3}. Figure 3 demonstrates the trade-oﬀ in accuracy determined by the number of groups for each polynomial degree. The results conﬁrm our bias-variance analysis in Theorem 1. While a large block number k provides less biased approximation, this large k renders the signal tensor estimation diﬃcult within each block due to small sample size. In addition, we ﬁnd that degree-2 polynomial approximation with the optimal k gives the smallest MSE among all considered polynomial approximations. These observations are consistent with our theoretical results that the optimal number of blocks and polynomial degree are (k The second experiment investigates the impact of the tensor dimension d for various polynomial degrees. We vary the tensor dimension d ∈ {10, . . . , 100} and polynomial degree ` ∈ {0, 1, 2, 3} in each model conﬁguration. We set optimal number of blocks as the one that gives the best accuracy. Figure S1 compares the estimation errors among diﬀerent polynomial approximations. The result veriﬁes that the degree-2 polynomial approximation performs the best under the suﬃcient tensor dimension, which is consistent with our theoretical results. We emphasize that this phenomenon is diﬀerent from the matrix case where the degree-0 polynomial approximation gives the best results (Gao et al., 2015; Klopp et al., 2017). ). For the Bernoulli model, we generate binary tensors Y using the success probability Figure 3: MSE versus the number of blocks based on diﬀerent polynomial approximations. Columns 1-3 consider the Models 1, 3, and 5 respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors. Comparison with alternative methods. We compare our method (Borda Count) with several popular alternative methods. • Spectral method (Spectral) (Xu, 2018) that performs universal singular value thresholding (Chatterjee, 2015) on the unfolded tensor. • Least-squares estimation (LSE) (Gao et al., 2015) which solves the optimization problem (7) with constant block approximation (` = 0) based on spectral k-means. We extend the matrixbased biclustering algorithm to higher-order tensors (Han et al., 2020). • Least-squares estimation (BAL) (Balasubramanian, 2021) which solves the optimization problem (7) with constant block approximation (` = 0). This tensor-based algorithm is only available for binary observations because it uses count-based statistics. Therefore, we only use this algorithm for the Bernoulli model. We choose degree-2 polynomial approximation as our theorems suggested, and vary tensor dimension d ∈ {10, . . . , 100} under each model conﬁguration. For Borda Count and LSE, we choose the block numbers that achieve the best performance in the corresponding outputs. For Spectral method, we set the hyperparameter (singular-value threshold) that gives the best performance. Figure 4 shows that our algorithm Borda Count achieves the best performance in all scenarios as the tensor dimension increases. The poor performance of Spectral can be explained by the loss of multilinear structure in the tensor unfolding procedure. The sub-optimality of LSE is possibly due to its limits in both statistics and computations. Statistically, our theorems have shown that Figure 4: MSE versus the tensor dimension based on diﬀerent estimation methods. Columns 1-3 consider the Models 1, 3, and 5 in Table 2 respectively. Panel (a) is for continuous tensors, whereas (b) is for the binary tensors. constant block approximation results in sub-optimal rates compared to polynomial approximation. Computationally, the least-squares optimization (7) is highly non-convex and computationally unstable. Figure 5 displays true signal tensors of three models and corresponding observed tensors of dimension d = 80 with Gaussian noise. We use oracle permutation π to obtain the estimated signal tensor from the estimated permuted signal tensor As shown in the ﬁgure, we see clearly that our method achieves the best signal recovery, thereby supporting the numerical results in Figure 4. The outperformance of Borda count demonstrates the eﬃcacy of our method. Investigation of non-symmetric tensors. Our models and techniques easily extend to nonsymmetric tensors. We use non-symmetric functions to generate order-3 signal tensors; see detailed setup in Appendix A.2. We choose hyperparameters that give the best accuracy for each method (see Table S2). Table 3 compares the MSEs from repeated simulations based on diﬀerent methods under Models 1-5 (see Table S1). We ﬁnd that Borda count estimation outperforms all alternative methods for non-symmetric tensors. The results demonstrate the applicability of our method to general tensors. Chicago crime dataset consists of crime counts reported in the city of Chicago, ranging from January 1st, 2001 to December 11th, 2017. The observed tensor is an order-3 tensor with entries representing the log counts of crimes from 24 hours, 77 community areas, and 32 crime types. We apply our Figure 5: Performance comparison among diﬀerent methods. The observed data tensors, true signal tensors, and estimated signal tensors are plotted for Models 1, 3 and 5 in Table 2 with ﬁxed dimension d = 80. Numbers in parenthesis indicate the mean squared error. Borda count method to Chicago crime dataset. Because the data tensor is non-symmetric, we allow diﬀerent number of blocks across the three modes. Cross validation result suggests the types, respectively. We ﬁrst investigate the four community areas obtained from our Borda count algorithm. Figure 6(b) shows the four areas overlaid on the Chicago map. Interestingly, we ﬁnd that the clusters are consistent with actual locations, even though our algorithm did not take any geographic information such as longitude or latitude as inputs. In addition, we compare the cluster patterns with benchmark maps based on homicides and shooting incidents in Chicago shown in Figure 6(a). We ﬁnd that our clusters share similar geographical patterns with Figure 6(a). The results demonstrate the power of our approach in detecting meaningful pattern from tensor data. Then, we examine the denoised signal tensor obtained from our method and analyze the trends between crime types and crime hours by the four community areas in Figure 6(b). Figure 7 shows the averaged log counts of crimes according to crime types and crime hours by four areas. We ﬁnd that the major diﬀerence among four areas is the crime rates. Area 4 has the highest crime rates, and the crime rates monotonically decrease from Area 4 to Area 1. The variation in crime rates across hour and type, nevertheless, exhibits similarity among the four areas. For example, Figure 7 ObservationModel 1Borda CountSpectralLSEObservationModel 1Borda CountSpectralLSE ObservationModel 3Borda CountSpectralLSEObservationModel 3Borda CountSpectralLSE , k) = (6, 4, 10), representing the block number for crime hours, community areas, and crime Table 3: MSEs from 20 repeated simulations based on diﬀerent methods. All numbers are displayed on the scales 10 Figure 6: Chicago crime maps. Figure(a) is the benchmark map based on homicides and shooting incidents in community areas in Chicago (Jeremy, Jeremy). Figure(b) shows the four clustered areas learned from 32 crime types using our method. shows that the number of crimes increases hourly from 8 p.m., peaks at night hours, and then drops to the lowest at 6 p.m. The identiﬁed similarities and diﬀerences among the four community areas highlight the interpretability of our method in real data. Table 4: Performance comparison in Chicago data analysis. Reported MSEs are averaged over ﬁve runs of cross-validation, with 20% entries for testing and 80% for training, with standard errors in parentheses. Block number is set to achieve the best prediction performance. Finally, we compare the prediction performance based on constant block model and our permuted smooth tensor model. Notice that constant block model uses ` = 0 approximation, whereas our permuted smooth tensor model uses ` = 2 approximation. Table 4 shows the mean squared error over ﬁve runs of cross-validation, with 20% entries for testing and 80% for training. We ﬁnd that Borda count 0.57 (0.01) 0.51 (0.02) 0.87 (0.02) 1.02 (0.02) 2.56 (0.21) Figure 7: Averaged log counts of crimes according to crime types, hours, and the four areas estimated by our Borda count algorithm. We plot the estimated signal tensor entries averaged within four areas in the heatmap. the permuted smooth tensor model substantially outperforms the classical constant block models. We emphasize that our method does not necessarily assume the block structure. The comparison supports our premises that permuted smooth tensor model with polynomial approximation performs better than common constant block models in this application. Here, we provide proofs of the theoretical results presented in Sections 3-5. Proof. Recall that we denote E where z : [d] → [k] is the canonical clustering function such that z(i) = dki/de, for all i ∈ [d], and we use the shorthand for all (i on the partition E where Poly for all (i H¨older smoothness of the generative function f. Based on the construction of block-wise degree-` polynomial tensor B, we have where the ﬁrst inequality uses (19) and the second inequality is from (18). Proof. By Lemma 1, there exists a block-wise polynomial tensor B ∈ B(k, `) such that z(j) ∈ E, ﬁx any index (i, . . . , i) ∈z(j). Then, we have , . . . , i) ∈z(j). We deﬁne the block-wise degree-` polynomial tensor B based B(i, . . . , i) = Polyi− id, . . .i− id, . . . , i) ∈z(j), , . . . , i) ∈z(j). Notice that we can always ﬁnd such polynomial function by αkΘ − Bk By the triangle inequality, Now we bound inner product term. For ﬁxed π, π responding to permutations π and π polynomial tensors, vec(B) and vec(B vec(B) = Xβ and vec(B m-multivaraite degree-` polynomial basis over grid design (1/d, . . . , d/d), β and β are corresponding coeﬃcient vectors. Notice that the number of coeﬃcients for m-multivariate polynomial of degree-` is simplicity. Therefore, we rewrite the inner product where we deﬁne A := Lemma 5, we have where e ∈ R By the union bound of Gaussian maxima over countable set {π, π kˆΘ◦ ˆπ− Θ ◦ πk≤ 2kˆΘ◦ ˆπ− B ◦ πk+ 2 kB ◦ π − Θ ◦ πk ◦ ˆπ− B ◦ πk≤Θ◦ ˆπ− B ◦ πˆ, E + (Θ ◦ π − B ◦ π) where the second inequality is from (22) and the last inequality is from Lemma 6. Setting t =p with high probability. Combining the inequalities (20), (21) and (24) yields the desired conclusion Finally, optimizing (25) with respect to (k, l) gives that under the choice Proof. By the deﬁnition of the tensor space, we seek the minimax rate ε On one hand, if we ﬁx a permutation π ∈ Π(d, d), the problem can be viewed as a classical mdimensional α-smooth nonparametric regression with d is known to be ε become a new type of convergence rate due to the unknown permutation. We refer to the resulting error as the permutation rate, and we will prove that ε the sum of the two rates, it suﬃce to prove the two diﬀerent rates separately. In the following arguments, we will proceed by this strategy. Nonparametric rate. The nonparametric rate for α-smooth function is readily available in the literature; see Gy¨orﬁ et al. (2002, Section 3.2) and Stone (1982, Section 2). We state the results here for self-completeness. Lemma 3 (Minimax rate for α-smooth function estimation). Consider a sample of N data points, is the scalar response. Consider the observation model k(` + m)+ d log d in (24) for suﬃciently large C > 0 gives ), . . . , (x, Y), where x= (, . . . ,) ∈ [0, 1]is the m-dimensional predictor and Y∈ R Assume f is in the α-Holder smooth function class, denoted by H(α, L). Then, Our desired nonparametric rate readily follows from Lemma 3 by taking sample size N = d function norm kf − Permutation rate. Since nonparametric rate dominates permutation rate when α ≤ 1, it is suﬃcient to prove the permutation rate lower bound for α ≥ 1. We ﬁrst show the minimax permutation rate for k-block degree-0 tensor family B(k, 0), and then construct a smooth f ∈ H(α, L) to mimic the constant block tensors. Let Π(d, k) denote the collection of all possible onto mappings from [d] to [k]. Lemma 4 shows the permutation rate over k-block degree-0 tensor family B(k, 0) is σ Lemma 4 (Permutation error for tensor block model). Consider the problem of estimating ddimensional, block-k signal tensors from sub-Gaussian tensor block models. For every given integer k ∈ [d], there exists a core tensor S ∈ R inf The proof of Lemma 4 is constructive and deferred to Appendix B. We ﬁx a core tensor S ∈ R satisfying (27), and use it to construct the smooth tensors. Now we construct a function f ∈ H(α, L) that mimics the core tensor S in block tensor family B(k, 0). Deﬁne k = d K(x) that is inﬁnitely diﬀerentiable, where C The smooth cutoﬀ function has support [−1/2, 1/2] and takes value 1 on the interval [−1/4, 1/4]. For a given core tensor S satisfying Lemma 4, we deﬁne α-smooth function supP1dˆΘ(i, . . . , i) − S (z(i), . . . , z(i))&σlog kd > 0 satisﬁesK(x)dx = 1. Then, we deﬁne a smooth cutoﬀ function as One can verify that f ∈ H(α, L) as long as we choose suﬃciently small δ depending on α and L. Notice that for any (a From this observation, we deﬁne a sub-domain I ⊂ [d] such that Then, {f(i Deﬁne a subset of permutations Π collects permutations on I while ﬁxing indices on [d] \ I. Then we have where (1) absorbs the estimate ˆπ into the estimate and the permutation collections Π such that z(i) = dkπ(i)/de for all i ∈ I. Then, we have Finally, combining (29), (30), and Lemma 4 yields where k is replaced by n Combining two rates. Now, we combine (26) and (31) to get the desired lower bound. For any Θ generated as in (3) with f ∈ H(α, L), by union bound, we have Taking sup on both sides with the property yields the desired rate (11). f(x, . . . , x) = S(a, . . . , a, . . . , x) ∈a− 3/4k,a− 1/4k. /d, . . . , i/d): i, . . . , i∈ I} forms the block structure with the core tensor S ∈ R. infsupP1dkˆΘ ◦ ˆπ − Θ ◦ πk≥ ε ≥ infsupP1dˆΘ(i, . . . , i) − f (π(i)/d, . . . , π(i)/d)≥ ε ≥ infsupP1dˆΘ(i, . . . , i) − f(π(i)/d, . . . , π(i)/d)≥ ε Proof. Without loss of generality, assume that π is the identity permutation. Notice that g(i)−τ(i) is the sample average of roughly (excluding repetitions from symmetricity) d zero sub-Gaussian random variables with the variance proxy σ. Based on the independence of sub-Gaussian random variables, we have with probability 1 − By the weakly β-monotonicity of the function g, we have where δ . d For any given index i, we examine the error |i − ˆπ(i)|. By (33) and (34), we have where |· | denotes the cardinality of the set. We claim that the sets I and II diﬀer only in at most 1. Long-distance indices in {j : |j − i| ≥ C C > 0. In this case, the ordering of (i, j) remains the same in (33) and (34), i.e., We only prove the right side direction in (35) here. The other direction can be similarly proved. Suppose that g(i) < g(j). Then we have where the second inequality is from (32) with probability at least (1−2/d uses weakly β-monotonousity of g(·), the tolerance condition δ . d |j − i| ≥ C case, we conclude that none of long-distance indices belongs to I∆II. 2. Short-distance indices in {j : |j − i| < yield diﬀerent ordering of (i, j). Finally, we have with high probability. elements. To prove this, we partition the indices in [d] in two cases. σdlogd. Therefore we show that g(i) < g(j) implies τ(i) < τ(j). In this Proof. By Lemma 1, there exists a block-wise polynomial tensor B ∈ B(k, `) satisfying (20). By the triangle inequality, we decompose estimation error into three terms, Permutation error. For any (i where the ﬁrst inequality is from the α-H¨older smoothness of Θ, and the last inequality is from Lemma 2. Therefore, we obtain the upper bound of the permutation error Nonparametric error. Recall that Borda count estimation is deﬁned by Θk, where Now we bound the inner product term. By the same argument in the proof of Theorem 1, the space embedding B(k, `) ⊂ R ≤ kˆΘ◦ ˆπ− B ◦ ˆπk+ kB ◦ ˆπ− Θ ◦ ˆπk+ kΘ ◦ ˆπ− Θ ◦ πk = kΘ ◦ ˆπ− Θ ◦ πk+ kˆΘ− Bk+ kB − Θk. ˜Y = Y ◦ (ˆπ). By the optimality of least-square estimator, we have . Therefore, we have where e ∈ R Combining (38) and Lemma 6 yields Setting t = Cσ with high probability. Finally, combining all sources of error from Lemma 1 and inequalities (37), (39), (36) yields Finally, optimizing (40) with respect to (k, l) gives that under the choice where c(α, β, m) := We have presented a suite of statistical theory, estimation methods, and data applications for permuted smooth tensor models. Two estimation algorithms are proposed with accuracy guarantees: the (statistically optimal) least-squares estimation and the (computationally tractable) Borda count estimation. In particular, we establish an interesting phase transition phenomenon with respect to the critical smoothness level. We demonstrate that a block-wise polynomial of order (m − 2)(m + 1)/2 is suﬃcient and necessary for accurate recovery of order-m tensors, in contrast to earlier beliefs on constant block approximation. Experiments demonstrate the eﬀectiveness of both theoretical ﬁndings and algorithms. There are several possible extensions from our work. The theory in this paper assumes symmetry on the signal tensor Θ for simplicity of exposition. In fact, all our results naturally extend to non-symmetric signal tensors. A non-symmetric tensor Θ ∈ R Θ(i, . . . , i mode ` ∈ [m], and the function f is a smooth but non-symmetric function. Under the condition ) = f, . . . ,, where π: [d] → [d] is the latent permutation for each that d accuracy guarantees still hold true. Our framework of block-wise polynomial approximation can be extended to allow other nonparametric techniques, including B splines, smoothing splines, kernel regression, wavelets, etc. We choose to use polynomial basis because of its simplicity. The parsimony allows us to establish the insights on critical smoothness level (m − 2)(m + 1)/2. For example, our result suggests that quadratic splines are enough for accurate estimation of order-3 tensors. One can combine our approach with the modern trend ﬁltering techniques (Tibshirani, 2014; Ortelli and van de Geer, 2021), where k∇ reduces to the total variation smoothing in graphons (Chan and Airoldi, 2014). For general order-m tensors, our theory provides a principle of guidance for the order of smoothness needed. Exploiting the beneﬁts and properties of various nonparametric ﬁtting techniques for general tensor models warrants future research. Finally, our current approach assumes no randomness in the signal tensor Θ. One can also extend the generative model to allow random designs, where the signal tensor is represented by Θ(i, . . . , i lar techniques have been developed for graphons and hypergraphons (Chan and Airoldi, 2014; Gao et al., 2015; Klopp et al., 2017; Balasubramanian, 2021). The two choices of designs lead to diﬀerent analysis in the same spirit as random- vs. ﬁxed-designs in nonparametric regression. Extending our theory to random design is an interesting question for future research. This research is supported in part by NSF grants DMS-1915978, DMS-2023239, and funding from the Wisconsin Alumni Research foundation. , . . . , dare asymptotically of the same order, similar estimation algorithms and theoretical ˆf = arg minXY(ˆπ(i), ˆπ(i), ˆπ(i)) − fi,i,i+ λ k∇fk, fk represents total variation of second order diﬀerence. The case of m = 2 (matrix) ) = f(x, . . . , x) with (x)i.i.d. randomly drawn from certain distribution. Simi-