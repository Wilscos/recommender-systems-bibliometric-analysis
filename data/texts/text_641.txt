<title>A SURVEY OF GENERALISATION IN DEEP REINFORCEMENT LEARNING</title> <title>ABSTRACT</title> The study of generalisation in deep Reinforcement Learning (RL) aims to produce RL algorithms whose policies generalise well to novel unseen situations at deployment time, avoiding overﬁtting to their training environments. Tackling this is vital if we are to deploy reinforcement learning algorithms in real world scenarios, where the environment will be diverse, dynamic and unpredictable. This survey is an overview of this nascent ﬁeld. We provide a unifying formalism and terminology for discussing different generalisation problems, building upon previous works. We go on to categorise existing benchmarks for generalisation, as well as current methods for tackling the generalisation problem. Finally, we provide a critical discussion of the current state of the ﬁeld, including recommendations for future work. Among other conclusions, we argue that taking a purely procedural content generation approach to benchmark design is not conducive to progress in generalisation, we suggest fast online adaptation and tackling RL-speciﬁc problems as some areas for future work on methods for generalisation, and we recommend building benchmarks in underexplored problem settings such as ofﬂine RL generalisation and reward-function variation. <title>1 Introduction</title> Reinforcement Learning (RL) could be used in a range of applications such as autonomous vehicles [ ], algorithm control [ ], robotics [ ], but to fulﬁll this potential we need RL algorithms that can be used in the real world. Reality is dynamic, open-ended and always changing, and RL algorithms will need to be robust to variation in their environments, and have the capability to transfer and adapt to unseen (but similar) environments during their deployment. <title>arXiv:2111.09794v4  [cs.LG]  7 Jan 2022</title> resulted in policies that perform badly on even slightly adjusted environment instances (speciﬁc levels or tasks within an environment) and often fail on unseen random seeds used for initialisation [8, 9, 10, 11]. Many researchers have taken these criticisms seriously, and now focus on improving generalisation in RL (as can be seen from the content of this survey). This research is focused on producing algorithms whose policies have the desired robustness, transfer and adaptation properties, challenging the basic assumption that train and test will be identical (Fig. 1 middle and right columns). While this research is valuable, currently it often lacks clarity or coherence between papers. We argue that this is partly because generalisation (especially in RL) is a class of problems, rather than a speciﬁc problem. Improving “generalisation” without specifying the kind of generalisation that is desired is underspeciﬁed; it is unlikely that we can generically improve generalisation, given this class of problems is so broad that some analogy of the No Free Lunch theorem [ 12 ] applies: improving generalisation in some settings could harm generalisation in others. Two broad categories of generalisation problem are shown in Fig. 1 in the centre and right columns. specifying a generalisation problem. This is then grounded in choices made by speciﬁc benchmarks, and assumptions made to justify speciﬁc methods, which we discuss next. Finally, we propose several settings within generalisation which are underexplored but still vital for various real-world applications of RL, as well as many avenues for future work on methods that can solve different generalisation problems. We aim to make the ﬁeld more legible to researchers and practitioners both in and out of the ﬁeld, and make discussing new research directions easier. This new clarity can improve the ﬁeld, and enable robust progress towards more general RL methods. Scope. We focus on a problem setting called zero-shot policy transfer. Here, a policy is evaluated zero-shot on a collection of environments different to those it was trained on. We discuss this setting and its restrictions more in Section 3.7, Motivating Zero-Shot Policy Transfer, but note here that this setting disallows any additional training in or data from the test environments, meaning methods such as domain adaptation and many meta RL approaches are not applicable. We only cover single agent RL in this work. There are generalisation problems within multi-agent reinforcement learning (MARL), such as being general enough to defeat multiple different opponent strategies [ 18 19 ] and generalising to new team-mates in cooperative games [20, 21], but we do not cover any work in this area here. Relatedly, there is work on using multiple agents in a single-agent setting to increase the diversity of the environment and hence the generality of the policy [22], which we do cover. We do not cover theoretical work on generalisation in RL. While there is recent work in this area [ 23 24 ], it is often quite disconnected from the current empirical results and practices in deep RL, and hence is not as applicable. Overview of the Survey. The structure of the survey is as follows. We ﬁrst brieﬂy describe related work such as other surveys and overviews in Section 2. We introduce the formalism and terminology for generalisation in RL in Section 3, including the relevant background. We then proceed to use this formalism to describe current benchmarks for generalisation in RL in Section 4, discussing both environments (Section 4.1) and evaluation protocols (Section 4.2). We categorise and describe work producing methods for tackling generalisation in Section 5. Finally, we present a critical discussion of the current ﬁeld, including recommendations for future work in both methods and benchmarks, in Section 6, and conclude with a summary of the key takeaways from the survey in Section 7. Contributions. To summarise, our key contributions are: We present a formalism and terminology for discussing the broad class of generalisation problems, building on formalisms and terminology presented in multiple previous works [ 13 14 15 16 17 ]. Our contribution here is the uniﬁcation of these prior works into a clear formal description of the class of problems referred to as generalisation in RL. We propose a taxonomy of existing benchmarks that can be used to test for generalisation, splitting the discussion into categorising environments and evaluation protocols. Our formalism allows us to cleanly describe weaknesses of the purely PCG approach to generalisation benchmarking and environment design: having a completely PCG environment limits the precision of the research that can be done on that environment. We recommend that future environments should use a combination of PCG and controllable factors of variation. We propose a categorisation of existing methods to tackle various generalisation problems, motivated by a desire to make it easy both for practitioners to choose methods given a concrete problem and for researchers to understand the landscape of methods and where novel and useful contributions could be made. We point to many under-explored avenues for further research, including fast online adaptation, tackling RL-speciﬁc generalisation issues, novel architectures, model-based RL and environment generation. We critically discuss the current state of generalisation in RL research, recommending future research directions. In particular, we point that building benchmarks would enable progress ofﬂine RL generalisation and reward-function variation, both of which are important settings. Further, we point to several different settings and evaluation metrics that are worth exploring: investigating context-efﬁciency and working in a continual RL setting are both areas where future work is necessary. <title>2 Related Work: Surveys In Reinforcement Learning Subﬁelds</title> While there have been previous surveys of related subﬁelds in RL, none have covered generalisation in RL explicitly. Khetarpal et al. [25] motivate and survey continual reinforcement learning (CRL), which is closely related to generalisation in RL as both settings require adaptation to unseen tasks or environments; however, they explicitly do not discuss zero-shot generalisation which is the concern of this paper (for more discussion of CRL see Section 6.1). Chen and Li [26] give a brief overview of Robust RL (RRL) [ 27 ], a ﬁeld aimed at tackling a speciﬁc form of environment model misspeciﬁcation through worst-case optimisation. This is a sub-problem within the class of generalisation problems we discuss here, and [26] only brieﬂy survey the ﬁeld. Zhao et al. [28] surveys methods for sim-to-real transfer for deep RL in robotics. Sim-to-real is a concrete instantiation of the generalisation problem, and hence there is some overlap between our work and [ 28 ], but our work covers a much broader subject area, and some methods for sim-to-real transfer rely on data from the testing environment (reality), which we do not assume here. Müller-Brockhausen et al. [29] and Zhu et al. [30] survey methods for transfer learning in RL (TRL). TRL is related to generalisation in that both topics assume a policy is trained in a different setting to its deployment, but TRL generally assumes some form of extra training in the deployment or target environment, whereas we are focused on zero-shot generalisation. Finally, surveys on less related topics include Vithayathil Varghese and Mahmoud [31] on multi-task deep RL, Amin et al. [32] on exploration in RL, and Narvekar et al. [33] on curriculum learning in RL. None of these surveys focuses on the zero-shot generalisation setting that is the focus of this work, and there is still a need for a formalism for the class of generalisation problems which will enable research in this ﬁeld to discuss the differences between different problems. <title>3 Formalising Generalisation In Reinforcement Learning</title> In this section we present a formalism for understanding and discussing the class of generalisation problems in RL. We ﬁrst review relevant background in supervised learning and RL before motivating the formalism itself. Formalising the generalisation in this way shows that generalisation refers to a class of problems, rather than a speciﬁc problem, and hence research on generalisation needs to specify which group of generalisation problems it is tackling. Having laid out this class of problems in Section 3.4, we discuss additional assumptions of structure that could make generalisation more tractable in Section 3.6; this is effectively specifying sub-problems of the wider generalisation problem. This gap is used as a measure of generalisation speciﬁcally: a smaller gap means a model generalises better. One speciﬁc type of generalisation examined frequently in supervised learning which is relevant to RL is compositional generalisation [ 34 35 ]. We explore a categorisation of compositional generalisation here from [ 34 ]. While this was designed for generalisation in language, many of those forms are relevant for RL. The ﬁve forms of compositional generalisation deﬁned are: 1. systematicity: generalisation via systematically recombining known parts and rules, 2. productivity: the ability to extend predictions beyond the length seen in training data, 3. substitutivity: generalisation via the ability to replace components with synonyms, 4. localism: if model composition operations are local vs. global, 5. overgeneralisation: if models pay attention to or are robust to exceptions. For intuition we will explore examples of some of these different types of compositional generalisation in a blockstacking environment. An example of systematicity is the ability to stack blocks in new conﬁgurations once the basics of block-stacking are mastered. Similarly, productivity can be measured by how many blocks the agent can generalise to, and the complexity of the stacking conﬁgurations. Substitutivity can be evaluated by the agent’s ability to generalise to blocks of new colours, understanding that the new colour does not affect the physics of the block. In Section 4.3, Compositional Generalisation in Contextual MDPs we discuss how assumptions of compositional structure in the collection of RL environments can enable us to test these forms of generalisation. In Section 4.2 we will discuss how some of these forms of generalisation can be evaluated in current RL benchmarks. The standard formalism in RL is the Markov Decision Process (MDP). An MDP consists of a tuple M = (S, A, R, T, p) where is the state space; is the action space; R : S × A × S → R is the scalar reward function; T (s |s, a) is the possibly stochastic markovian transition function; and p(s is the initial state distribution. We also consider partially observable MDPs (POMDPs). A POMDP consists of a tuple M = (S, A, O, R, T, φ, p) , where S, A, R, T and are as above, is the observation space, and φ : S → O is the emission or observation function. In POMDPs, the policy only observes the observation of the state produced by φ. The standard problem in an MDP is to learn a policy π(a|s) which produces a distribution over actions given a state, such that the cumulative reward of the policy in the MDP is maximised: = argmax [R(s)] , This is the total expected reward gained by the policy from a state . The goal in a POMDP is the same, but with the policy taking observations rather than states as input. This sum may not to exist if the MDP does not have a ﬁxed horizon, so we normally use one of two other forms of the return, either assuming a ﬁxed number of steps per episode (a horizon H) or an exponential discounting of future rewards by a discount factor γ. To discuss generalisation, we need a way of talking about a collection of tasks, environments or levels: the need for generalisation emerges from the fact we train and test the policy on different collections of tasks. Consider for example OpenAI Procgen [ 36 ]: in this benchmark suite, each game is a collection of procedurally generated levels. Which level is generated is completely determined by a level seed, and the standard protocol is to train a policy on a ﬁxed set of 200 levels, and then evaluate performance on the full distribution of levels. Almost all other benchmarks share this structure: they have a collection of levels or tasks, which are speciﬁed by some seed, ID or parameter vector, and generalisation is measured by training and testing on different distributions over the collection of levels or tasks. To give a different example, in the Distracting Control Suite [ 37 ], the parameter vector determines a range of possible visual distractions applied to the observation of a continuous control task, from changing the colours of objects to controlling the camera angle. While this set of parameter vectors has more structure than the set of seeds in Procgen, both can be understood within the framework we propose. See Section 4.2 for discussion of the differences between these styles of environments. To formalise the notion of a collection of tasks, we start with the Contextual Markov Decision Process (CMDP), as originally formalised in [ 14 ], but using the alternative formalism from [ 13 ]. Our formalism also builds on that presented in [ 38 ]. A contextual MDP (CMDP) is an MDP where the state can be decomposed into a tuple s = (c, s ) ∈ S Here, ∈ S is the underlying state, and we refer to c ∈ C as the context. This context takes the role of the seed, ID or parameter vector which determines the level. Hence, it does not change within an episode, only between episodes. The CMDP is the entire collection of tasks or environments; in Procgen, each game is a separate CMDP. p(s) = p((c, s )) := p(c)p(s |c), and we call p(c) the context distribution. This distribution is what is used to determine the training and testing collections of levels, tasks or environments; in Procgen this distribution is uniform over the ﬁxed 200 seeds at training time, and uniform over all seeds at testing time. We often assume the context is not observed by the agent, making the CMDP a POMDP with observation space and an emission function which discards the context information: φ((c, s )) := s . An analogous deﬁnition can be given for a contextual partially observable MDP (CPOMDP), where the emission function, as well as discarding the context, also transforms the state conditioned (on that context). We will generally use “MDP” to refer to environments that are either MDPs or POMDPs. As the reward function, transition function, initial state distribution and emission function all take the context as input, the choice of context determines everything about the MDP apart from the action space, which we assume is ﬁxed. Given a context , we call the MDP resulting in the restriction of the CMDP to the single context a context-MDP . This is a speciﬁc task or environment, for example a single level of a game in Procgen, as speciﬁed by a single random seed that is the context. Some MDPs have stochastic transition or reward functions. When these MDPs are simulated, researchers often have control of this stochasticity through the choice of a random seed. In theory, these stochastic MDPs could be considered deterministic contextual MDPs, where the context is the random seed. We do not consider stochastic MDPs as automatically contextual in this way and assume that the random seed is always chosen randomly, rather than being modelled as a context. This more closely maps to real-world scenarios with stochastic dynamics where we cannot control the stochasticity. We now describe the class of generalisation problems we focus on, using the CMDP formalism. As mentioned, the need for generalisation emerges from a difference between the training and testing environments, and so we want to specify both a set of training context-MDPs and a testing set. We specify these sets of context-MDPs by their contexts, as the context uniquely determines the MDP. we can choose a subset of the context set ⊆ C , and then produce a new CMDP M| by setting the context set to , and adjusting the context distribution to be uniform over . This allows us to split the total collection of tasks into smaller subsets, as determined by the contexts. For example, in Procgen any possible subset of the set of all seeds can be used to deﬁne a different version of the game with a limited set of levels. R(π, M) := E [R(π, M| )], where R is the expected return of a policy in an MDP and p(c) is the context distribution as before. To formally specify a generalisation problem, we choose a training context set and a testing context set . We train a policy using the training context-set CMDP M| for some number of training steps (possibly inﬁnite), and the resulting policy is then evaluated on the testing context set CMDP M| . The objective is for the expected return on the testing context set to be as high as possible: For example, in Procgen we want to achieve the highest return possible on the testing distribution (which is the full distribution over levels) after training for 25 million steps on the training distribution of levels (which is a ﬁxed set of 200 levels). We call this zero-shot policy transfer [15, 16]. As in supervised learning, we can consider the gap between training and testing performance as a measure of generalisation. We deﬁne this analogously to in supervised learning (Eq. (1)), swapping the ordering between training and testing (as we maximise reward, rather than minimise loss): This formalism deﬁnes a class of generalisation problems, each determined by a choice of CMDP, training and testing context sets. This means that we do not make any assumptions about shared structure within the CMDP between context-MDPs: for any speciﬁc problem some assumption of this kind will be required (Section 3.6), but we do not believe there is a unifying assumption behind all generalisation problems apart from those stated here. We chose this formalism as it is simple to understand, captures all the problems we are interested in, and is based on prior work. To further justify why this formalism is useful, and to give intuition about how it can be used in a variety of settings, we give several examples of applying it to existing benchmarks and hypothetical real-world scenarios: As described throughout this section, OpenAI Procgen [ 36 ] is a popular benchmark for generalisation in RL. It is a suite of 16 procedurally generated arcade games, with variation in the state space and observation function: the games consist of different levels with different layouts or enemy numbers, as well as different visual styles which do not impact the dynamics or reward function. In this environment, the context is a single random seed that acts as input to the level generation. We can understand the context as mostly changing the initial state distribution to select the initial state for a given level, and then also conditioning the observation function. For some games (such as starpilot ) enemies appear throughout the level, so the context conditions the transition function to spawn enemies at the speciﬁed time. Sim-to-real is a classic problem of generalisation, and one which can be captured in this framework. The context set will be split into those contexts which correspond to simulation, and those that correspond to reality. The context conditions the dynamics, observation function and state space. The CMDP can often be understood as effectively a union of two CMDPs, one for reality and one for simulation, with shared action space and observation space type. Domain randomisation approaches are motivated by the idea that producing a wide range of possible contexts in simulation will make it more likely that the testing distribution of contexts is closer to the expanded training distribution. Healthcare is a promising domain for deploying future RL methods, as there are many sequential decision-making problems. For example, the task of diagnosing and treating individual patients can be understood as a CMDP where the patient effectively speciﬁes the context: patients will react differently to tests and treatments (dynamics variation) and may provide different measurements (state variation). The context can always condition the relevant MDP function to control the variation. Generalising to treating new patients is then exactly generalising to novel contexts. In this setting, we may be able to assume some part of the context (or some information about the context) is observable, as we will have access to patients’ medical history and personal information. Autonomous vehicles are another area where RL methods could be applied. These vehicles will be goal-conditioned in some sense, such that they can perform different journeys (reward variation). Driving in different locations (state space variation), under different weather and lighting conditions due to the time of day (observation function variation) and on different road surfaces (dynamics variation) are all problems that need to be tackled by these systems. We can understand this in the CMDP framework, where the context contains information about the weather, time of day, location and goal, as well as information about the state of the current vehicle. Some of this context will be observed directly, and some may be inferred from observation. In this setting we may only be able to train in certain contexts (i.e. certain cities, or restricted weather conditions), but we require the policy to generalise to the unseen contexts well. In choosing the CMDP formalism we opted to formalise generalisation in a way that captures the full class of problems we are concerned with, but this means that it is almost certainly impossible to prove any formal theoretical guarantees on learning performance using solely the CMDP structural assumptions. While we do not prove this, it is easy to see how one could design pathological CMDPs where generalisation to new contexts is entirely impossible without strong domain knowledge of the new contexts. To have any chance of solving a speciﬁc generalisation problem then, further assumptions (either explicit or implicit) have to be made. These could be assumptions on the type of variation, the distributions from which the training and testing context sets are drawn, or additional underlying structure in the context set. We describe several popular or promising assumptions here, and note that the taxonomy in Section 4 also acts as a set of possible additional assumptions to make when tackling a generalisation problem. Assumptions on the Training and Testing Context Set Distributions. One assumption which is often made is that while the training and testing context sets are not identical, the elements of the two sets have been drawn from the same underlying distribution, analogously to the iid data assumption in supervised learning. For example, this is the setup of OpenAI Procgen [ 36 ], where the training context set is a set of 200 seeds sampled uniformly at random from the full distribution of seeds, and the full distribution is used as the testing context set. However, many works on generalisation in RL do not assume that the train and test environments are drawn from the same distribution. This is often called Domain Generalisation, where we refer to the training and testing environments as different domains that may be similar but are not from the same underlying generative distribution. Concrete examples occur in robotics such as the sim-to-real problem. Further Formal Assumptions of Structure. Another kind of assumption that can be made is on the structure of the CMDP itself, e.g. the context space or transition function. There are several families of MDPs with additional structure which could enable generalisation. However, these assumptions are often not explicitly made when designing benchmarks and methods, which can make understanding why and how generalisation occurs difﬁcult. A detailed discussion and formal deﬁnitions for these structures can be found in Appendix A, but we provide a high-level overview here, focusing on assumptions that have been used in practice, and those that hold particular promise for generalisation. An example of a structured MDP that has been used to improve generalisation is the block MDP [ 39 ]. It assumes a block structure in the mapping from a latent state space to the given observation space, or that there exists another MDP described by a smaller state space with the same behaviour as the given MDP. This assumption is relevant in settings where we only have access to high-dimensional, unstructured inputs, but know that there exists a lowerdimensional state space that gives rise to an equivalent MDP. Du et al. [39] use this assumption for improved bounds on exploration that relies on the size of the latent state space rather than the given observation space. Zhang et al. [40] develop a representation learning method that disentangles relevant from irrelevant features, improving generalisation to environments where only the irrelevant features change, a simple form of systematicity (Section 3.1 [34]). This is a rare example of a method explicitly utilising additional assumptions of structure to improve generalisation. Factored MDPs [ 41 42 ] can be used to describe object-oriented environments or multi-agent settings where the state space can be broken up into independent factors, i.e. with sparse relationships over the one-step dynamics. This can be leveraged to learn dynamics models that explicitly ignore irrelevant factors in prediction or to compute improved sample complexity bounds for policy learning [ 43 ], and seems particularly relevant for generalisation as additional structure in the context set could map onto the factored structure in the transition and reward functions. An initial example of using a similar formalism to a factored MDP in a multi-domain RL setting is [ 44 ], although it does not target the zero-shot policy transfer setting directly. We hope to see more work applying these kinds of structural assumptions to generalisation problems. On Metrics for Generalisation. There are two obvious ways of evaluating generalisation performance of models. One is to only look at absolute performance on evaluation tasks (Eq. (2)), and the other is to look at the generalisation gap (Eq. (3)). In supervised learning, generalisation capabilities of different algorithms are usually evaluated via ﬁnal performance on an evaluation task. When the tasks used to evaluate a model are close to (or the same as) the tasks that the model will eventually be deployed on, it is clear that ﬁnal performance is a good metric to evaluate on. However, in RL the benchmark tasks we use are often very dissimilar to the eventual real world tasks we want to apply these algorithms to. Further, RL algorithms are currently still quite brittle and performance can vary greatly depending on hyperparameter tuning and the speciﬁc task being used [ 45 ]. In this setting, we may care more about the generalisation potential of algorithms by decoupling generalisation from training performance and evaluating using generalisation gap instead. However, using generalisation gap as a metric assumes that there exists a general measure of progress towards tackling generalisation in RL, but given how broad the current set of assumptions is, this seems unlikely: across such a broad class, objectives may even be conﬂicting [ 12 ]. Therefore, our recommendation is to focus on problem-speciﬁc benchmarks and revert to the SL standard of using overall performance in speciﬁc settings (e.g. visual distractors, stochastic dynamics, sparse reward, hard exploration). The generalisation performance of various RL algorithms is likely contingent on the type of environment they are deployed on and therefore careful categorisation of the type of challenges present at deployment is needed to properly evaluate generalisation capability (for further discussion see Section 4.3, The Downsides of Procedural Content Generation for Generalisation and Section 6.2). Angles to Tackle the Generalisation Problem. While we aim to improve test-time performance Eq. (2), we often do not have access to that performance metric directly (or will not when applying our methods in the real world). To improve the test-time performance, we can either (1) increase the train-time performance while keeping the generalisation gap constant, (2) decrease the generalisation gap while keeping the train-time reward constant, or (3) do a mixture of the two approaches. Work in RL not concerned with generalisation tends to (implicitly) take the ﬁrst approach, assuming that the generalisation gap will not change. Work on generalisation in RL instead normally aims at (2) reducing the generalisation gap explicitly, which may reduce train-time performance but increase test-time performance. Some work also aims at improving train-time performance in a way that is likely to keep the generalisation gap constant. Motivating Zero-Shot Policy Transfer. In this work, we focus on zero-shot policy transfer [ 15 16 ]: a policy is transferred from the training CMDP to the testing CMDP, and is not allowed any further training in the test contextMDPs (hence zero-shot). While we do not cover methods that relax the zero-shot assumption, we believe that in a real-world scenario it will likely be possible to do so. However, zero-shot policy transfer is still a useful problem to tackle, as solutions are likely to help with a wide range of settings resulting from different relaxations of the assumptions made here: zero shot policy transfer algorithms will be used as a base which is then built upon with domain-speciﬁc knowledge and data. Note that while “training” and “learning” are difﬁcult to deﬁne precisely, the fact that the evaluation is based on the expected testing return within a single episode means that online learning or adaptation is unlikely to be a tractable solution unless the adaptation happens within a single episode. Several methods do take this approach, as described in Section 5.2, Adapting Online. To be clear, we are grounding ourselves in this speciﬁc objective, and not placing any restrictions on the properties of methods. <title>4 Benchmarks For Generalisation In Reinforcement Learning</title> In this section we give a taxonomy of benchmarks for generalisation in RL. A key split in the factors of variation for a benchmark is those factors concerned with the environment and those concerned with the evaluation protocol. A benchmark task is a combination of a choice of environment (a CMDP, covered in Section 4.1) and suitable evaluation protocol (a train and test context set, covered in Section 4.2). This means that all environments support multiple possible evaluation protocols, as determined by their context sets. the range of difﬁculty among generalisation problems (Section 4.3, What Generalisation Can We Expect?). More discussion of future work on benchmarks for generalisation can be found in Sections 6.1, 6.2 and 6.4. In Table 1, we list the available environments for testing generalisation in RL, as well as summarising each environment’s key properties. These environments all provide a non-singleton context set that can be used to create a variety of evaluation protocols. Choosing a speciﬁc evaluation protocol then produces a benchmark. We describe the meaning of the columns in Table 1 here. Style. This gives a rough high-level description of the kind of environment. Contexts. This describes the context set. In the literature there are two approaches to designing a context set, and the key difference between these approaches is whether the context-MDP creation is accessible and visible to the researcher. The ﬁrst, which we refer to as Procedural Content Generation (PCG), relies on a single random seed to determine multiple choices during the context-MDP generation. Here the context set is the set of all supported random seeds. This is a black-box process in which the researcher only chooses a seed. The second approach provides more direct control over the factors of variation between context-MDPs, and we call these Controllable environments. The context set is generally a product of multiple factor spaces, some of which may be discrete (i.e. a choice between several colour schemes) and some continuous (i.e. a friction coefﬁcient in a physical simulation). Borrowing from [ 82 ], a distinction between discrete factors of variation is whether they are cardinal (i.e. the choices are just a set with no additional structure) or ordinal (i.e. the set has additional structure through an ordering). Examples of cardinal factors include different game modes or visual distractions, and ordinal factors are commonly the number of entities of a certain type within the context-MDP. All continuous factors are effectively also ordinal factors, as continuity implies an ordering. Previous literature has deﬁned PCG as any process by which an algorithm produces MDPs given some input [ 92 ], which applies to both kinds of context sets we have described. Throughout the rest of this survey we use “PCG” to refer to black-box PCG, which uses a seed as input, and “controllable” to refer to environments where the context set directly changes the parameters of interest in the context-MDPs, which could also be seen as “white-box PCG”. We can understand (black-box) PCG settings as combinations of discrete and continuous factor spaces (i.e. controllable environments) where the choice of the value in each space is determined by the random generation process. However, only some environments make this more informative parametrisation of the context-MDPs available. In this table we describe environments where this information is not easily controllable as PCG environments. See Section 4.3, The Downsides of Procedural Content Generation for Generalisation for discussion of the downsides of purely PCG approaches. Variation. This describes what varies within the set of context MDPs. This could be state-space variation (the initial state distribution and hence implicitly the state space), dynamics variation (the transition function), visual variation (the observation function) or reward function variation. Where the reward varies, the policy often needs to be given some indication of the goal or reward, so that the set of contexts is solvable by a single policy [93]. There are several trends and patterns shown in Table 1, which we draw the reader’s attention to here. We describe 52 environments in total, and have aimed to be fairly exhaustive. There are a range of different Style s that these environments have, which is beneﬁcial as generalisation methods should themselves be generally applicable across styles if possible. While numerically there is a focus on gridworlds (13, 25%) and continuous control (13, 25%) there are well established benchmarks for arcade styles [ 36 ] and 3D environments 81 ]. Looking at Context sets, we see that PCG is heavily used in generalisation environments, featuring in 20 (38%) environments. Many environments combine PCG components with controllable variation [ 48 87 81 74 22 91 61 56, 53]. Most environments have several different kinds of factors of variation within their context set. There is a lot of differences between environments when looking at the Variation they use. Numerically, state variation is most common (39, 75%) followed by observation (27, 52%), and then dynamics (19, 37%) and reward (19, 37%). dynamics variation (RWRL [ 86 ]), and none with solely reward variation. State and Observation variations are often the easiest to engineer, especially with the aid of PCG. This is because changing the rendering effects of a simulator, or designing multiple ways the objects in a simulator could be arranged, is generally easier than designing a simulator engine that is parametrisable (for dynamics variation). Creating an environment for reward variation requires further design choices about how to specify the reward function or goal such that the environment satisﬁes the Principle Of Unchanged Optimality [ 93 ]. PCG is often the only good way of generating a large diversity in state variation, and as such is often necessary to create highly varied environments. Only CausalWorld [ 52 ] enables easily testing all forms of variation at once. There are several clusters that can be pointed out in the collection of benchmarks: There are several PCG state-varying gridworld environments (MiniGrid, BabyAI, Crafter, Rogue-gym, MarsExplorer, NLE, MiniHack [ 75 48 54 84 70 78 76 ]), non-PCG observation-varying continuous control environments (RoboSuite, DMC-Remastered, DMC-GB, DCS, KitchenShift, NaturalEnvs MuJoCo [ 50 60 59 37 94 77 ]), and multi-task continuous control benchmarks which could be adapted to zero-shot generalisation (CausalWorld, RLBench, Meta-world [52, 83, 73]). As discussed, a benchmark is the combination of an environment and an evaluation protocol. Each environment supports a range of evaluating protocols determined by the context set, and often there are protocols recommended by the environment creators. In this section we discuss the protocols, and the differences between them. An evaluation protocol speciﬁes the training and testing context sets, any restrictions on sampling from the training set during training, and the number of samples allowed from the training environment. An important ﬁrst attribute that varies between evaluation protocols is context-efﬁciency. This is analogous to sample efﬁciency, where only a certain number of samples are allowed during training, but instead we place restrictions on the number of contexts. This ranges from a single context, to a small number of contexts, to the entire context set. approximation of the performance on the full distribution. This limitation of PCG environments is discussed further below (Section 4.3, The Downsides of Procedural Content Generation for Generalisation). This gives three classes of evaluation protocol for PCG environments, as determined by their training context set: A single context, a small set of contexts, or the full context set. These are visualised in Fig. 2 and respectively. There are not any examples of protocol (for purely PCG environments), likely due to the high difﬁculty of such a challenge. For protocol , while “a small set of contexts” is imprecise, the relevant point is that this set is meaningfully different from the full context set: it is possible to overﬁt to this set without getting good performance on the testing set. Examples of this protocol include both modes of OpenAI Procgen [ 36 ], RogueGym [ 84 ], some uses of JumpingFromPixels [ 67 and MarsExplorer [70]. Protocol is commonly used among PCG environments that are not explicitly targeted at generalisation (MiniGrid, NLE, MiniHack, Alchemy [ 75 78 76 46 ]). The testing context set consists of seeds held out from the training set, and otherwise during training the full context set is used. This protocol effectively tests for more robust RL optimisation improvements, but does not test for generalisation beyond avoiding memorising. While this protocol only tests for generalisation in a very weak sense, it still matches a wider variety of real-world scenarios than the previous standard in RL of testing on the training set, and so we believe it should be the standard evaluation protocol in RL (not just in generalisation), and the previous standard should be considered a special case. Controllable Environment Evaluation Protocols. Many environments do not use only PCG, and have factors of variation that can be controlled by the user of the environment. In these controllable environments, there is a much wider range of possible evaluation protocols. The choice in PCG protocols – between a single context, a small set, or the entire range of contexts – transfers to the choice for each factor of variation in a controllable environment. For each factor we can choose one of these options for the training context set, and then choose to sample either within or outside this range for the testing context set. The range of options is visualised in Fig. 3. interpolation and extrapolation). The number of factors chosen to be extrapolating contributes to the difﬁculty of the evaluation protocol. However, if we create correlations or links between values of factors during training, we can get a non-convex training context set within the full context set (Fig. 3 ). Each possible testing context can either be within the training context set (fully interpolation), within the set formed by taking the convex hull of the non-convex training context set (combinatorial interpolation), or fully outside the convex hull (extrapolation). Combinatorial interpolation tests the ability of an agent to exhibit systematicity, a form of compositional generalisation discussed in Section 3.1. For ordinal factors, we can also choose disjoint ranges, which allows us to test interpolation along a single axis (i.e. taking values between the two ranges). Note that when discussing convex hulls, this only applies to factors of variation that are continuous or discrete-ordinal; for cardinal factors of variation, the convex hull just includes those values sampled during training. For example, consider a policy trained in a CMDP where the context set consists of values for friction and gravity strength. During training, the environments have either a friction coefﬁcient between 0.5 and 1 but gravity ﬁxed at 1, or gravity strength ranging between 0.5 and 1 but friction ﬁxed at 1 (the light blue line in Fig. 3 ). Testing contexts which take friction and gravity values within the training distribution are full interpolation (e.g. (f = 0.5, g = 1), (f = 1, g = 1) ), contexts which take values for friction and gravity which have been seen independently but not in combination are combinatorial interpolation (e.g. (f = 0.5, g = 0.5), (f = 0.5, g = 0.9) , the yellow area), and contexts which take values for friction and gravity which are outside the seen ranges during training are full extrapolation (e.g. (f = 0.2, g = 0.5), (f = 1.1, g = 1.5), either the dark blue or green areas). We can still consider the number of contexts within the training context set, which controls the density of the training context set, given its shape. When testing for extrapolation we can also vary the “width” of the training context set on the axis of variation along which extrapolation is being tested (Fig. 3 vs ). These tests evaluate the agent’s ability to exhibit productivity (Section 3.1). Of course, generalisation will be easier if there is a wide diversity of values for this factor at training time, even if the values at test time are still outside this set. For example, if we are testing whether a policy can generalise to novel amounts of previously seen objects, then we should expect the policy to perform better if it has seen different amounts during training, as opposed to only having seen a single amount of the object during training. To expand on the friction and gravity example, during training the policy never sees gravity and friction varying together, which makes it much more difﬁcult to generalise to the testing contexts. If gravity and friction did vary together during training then this would make generalisation easier. A notable point in this space is that of a single training context, and a wide variety of testing contexts. This protocol tests for a strong form of generalisation, where the policy must be able to extrapolate to unseen contexts at test time. Because of the difﬁculty of this problem, benchmarks with this evaluation protocol focus on visual variation: the policy needs to be robust to different observation functions on the same underlying MDP [ 60 37 94 ]. The protocol is often motivated by the sim-to-real problem, where we expect an agent trained in a single simulation to be robust to multiple visually different real-world settings at deployment time. Beyond this single point, it is challenging to draw any more meaningful categorisation from the current array of evaluation protocols. Generally, each one is motivated by a speciﬁc problem setting or characteristic of human reasoning which we believe RL agents should be able to solve or have respectively. There are several comments, insights and conclusions that can be gained from surveying the breadth of generalisation benchmarks, which we raise here. Non-visual Generalisation. If testing of non-visual types of generalisation, then visually simple domains such as MiniHack [ 76 ] and NLE [ 78 ] should be used. These environments contain enough complexity to test for many types and strengths of non-visual generalisation but save on computation due to the lack of complex visual processing required. There are many real-world problem settings where no visual processing is required, such as systems control and recommender systems. Representation learning is still a problem in these non-visual domains, as many of them have a large variety of entities and scenarios which representations and hence policies need to generalise across. Control Suite [ 37 ] is the most fully-featured variant in this space, as it features the broadest set of variations, the hardest combinations of which are unsolvable by current methods. Unintentional Generalisation Benchmarks. Some environments listed in Table 1 were not originally intended as generalisation benchmarks. For example, Tosch et al. [88] presents three highly parametrisable versions of Atari games, and uses them to perform post hoc analysis of agents trained on a single variant. Some environments are not targeted at zero-shot policy transfer (CausalWorld, RWRL, RLBench, Alchemy, Meta-world [ 52 86 83 46 73 ]), but could be adapted to such a scenario with a different evaluation protocol. More generally, all environments provide a context set, and many then propose speciﬁc evaluation protocols, but other protocols could be used as long as they were well-justiﬁed. This ﬂexibility has downsides, as different methods can be evaluated on subtly different evaluation protocols which may favour one over another. We recommend being explicit when using these benchmarks in exactly which protocol is being used and comparing with evaluations of previous methods. Using a standard protocol aids reproducibility. The Downsides of Procedural Content Generation for Generalisation. Many environments make use of procedural content generation (PCG) for creating a variety of context-MDPs. In these environments the context set is the set of random seeds used for the PCG and has no additional structure with which to control the variation between context-MDPs. This means that while PCG is a useful tool for creating a large set of context-MDPs, there is a downside to purely PCG-based environments: the range of evaluation protocols supported by these environments is limited to different sizes of the training context set. Measuring generalisation along speciﬁc factors of variation is impossible without signiﬁcant effort either labelling generated levels or unravelling the PCG to expose the underlying parametrisation which captures these factors. Often more effort is required to enable setting these factors to speciﬁc values, as opposed to just revealing their values for generated levels. Hence, PCG benchmarks are testing for a “general” form of generalisation and RL optimisation, but do not enable more targeted evaluation of speciﬁc types of generalisation. This means making research progress on speciﬁc problems is difﬁcult, as focusing on the speciﬁc bottleneck in isolation is hard. An interesting compromise, which is struck by several environments, is to have some low-level portion of the environment procedurally generated, but still have many factors of variation under the control of the researcher. For example, Obstacle Tower [ 81 ] has procedurally generated level layouts, but the visual features (and to some extent the layout complexity) can be controlled. Another example is MiniHack [ 76 ], where entire MDPs can be speciﬁed from scratch in a rich description language, and PCG can ﬁll in any components if required. These both enable more targeted types of experimentation. We believe this kind of combined PCG and controllable environment is the best approach for designing future environments; some usage of PCG will be necessary to generate sufﬁcient variety in the environments (especially the state space), and if the control is ﬁne-grained enough to enable precise scientiﬁc experimentation, then the environment will still be useful for disentangling progress in generalisation. Compositional Generalisation in Contextual MDPs. Compositional generalisation is a key point of interest for many researchers (see Section 3.1). In controllable environments, different evaluation protocols enable us to test for some of the forms of compositional generalisation introduced in Section 3.1 [ 34 ]: (1) Systematicity can be evaluated using a multidimensional context set, and testing on novel combinations of the context dimensions not seen at training time (combinatorial interpolation in Fig. 3). (2) Productivity can be evaluated with ordinal or continuous factors, measuring the ability to perform well in environments with context values beyond those seen at training time (either type of extrapolation in Fig. 3). If dealing with CMDPs where the context space is partially language, as in languageconditioned RL [96], then the evaluations discussed in [34] can be directly applied to the language space. Crucially, a controllable environment with a structured context space is required to test these forms of compositional generalisation, and to ensure that the agent is seeing truly novel combinations at test time; this is difﬁculty to validate in PCG environments like OpenAI Procgen [36] or NLE [78]. The other forms of compositional generalisation in [ 34 ] require additional structure not captured by the choices of evaluation protocol, and we describe what testing these forms could entail here: (3) substitutivity through the use of synonyms (in language) or equivalent objects and tools; (4) locality through comparing the interpretation of an agent given command A and command B separately vs. the combination of A + B and if those interpretations are different; and (5) overgeneralisation through how the agent responds to exceptions in language or rules of the environment. out-of-distribution combinations of entities and level layouts. Each evaluation protocol measures a different kind of generalisation strength, and they hence form a kind of partial ordering where “easier” evaluation protocols come before “harder” protocols. We outline this ordering here: Increasing the number of samples can make an evaluation protocol easier, but often only to a point: more samples are unlikely to bring greater variety which is needed for generalisation. Increasing the number of contexts (while keeping the shape of the context set the same) also makes an evaluation protocol easier. Even a small amount of additional variety can improve performance. The number of factors of variation which are extrapolating or combinatorially interpolating in the testing context set can also be varied. The more there are, the more difﬁcult the evaluation protocol. Further, the width of the range of values that the extrapolating factors take at training time can vary. This is linked to the number of contexts but is also related to the variety available during training time along these axes of variation. Following [ 82 ], we consider the difﬁculty of interpolating and extrapolating along different types of factors of variation. Interpolation along ordinal axes is likely the easiest, followed by cardinal axes interpolation (which happens through unseen combinations of seen values for a cardinal axis combined with any other axis), and then extrapolation along ordinal axes. Finally, extrapolation along cardinal axes is the most difﬁcult. As the difﬁculty of an evaluation protocol increases, it becomes less likely that standard RL approaches will get good performance. In more difﬁcult protocols which involve extrapolation of some form, generalisation is unlikely to occur at all with standard RL methods, as there is no reason to expect a policy to generalise correctly to entirely unseen values. That does not mean that this type of generalisation is impossible: it just makes clear the fact that to achieve it, more than standard RL methods will be needed. That is, methods incorporating prior knowledge such as transfer from related environments [ 30 ]; strong inductive biases [ 97 ] or assumptions about the variation; or utilising online adaptation [98, 99] will be necessary to produce policies that generalise in this way. <title>5 Methods For Generalisation In Reinforcement Learning</title> We now classify methods that tackle generalisation in RL. The problem of generalisation occurs when the training and the testing context sets are not identical. These context sets and their similarities and differences are what how to improve generalisation. There are many types of generalisation problems (as described in more detail in Section 4), and hence there are many different styles of methods. We categorise the methods into those that try and increase the similarity between training and testing data and objective (Section 5.1), those that explicitly aim to handle differences between training and testing environments (Section 5.2), and those that target RL-speciﬁc issues or optimisation improvements which aid generalisation performance (Section 5.3). See Fig. 4 for a diagram of this categorisation, and Table 2 for a table classifying methods by their approach, the environment variation they were evaluated on, and whether they mostly change the environment, loss function or architecture. Performing this comprehensive classiﬁcation enables us to see the under-explored areas within generalisation research, and we discuss future work for methods in Section 6.6. All else being equal, the more similar the training and testing environments are, the smaller the generalisation gap and the higher the test time performance. This similarity can be increased by designing the training environment to be as close to the testing environment as possible. Assuming this has been done, in this section we cover methods that make the data and objective being used to learn the policy during training closer to that which would be used if we were optimising on the testing environment. Data Augmentation and Domain Randomisation. Two natural ways to make training and testing data more similar is data augmentation [ 100 ] and domain randomisation [ 101 102 103 ]. This is especially effective when the variation between the training and testing environments is known, as then a data augmentation or domain randomisation can be used which captures this variation. In practice there is only so far this approach can go, as stronger types of variation often cannot be captured by this simple method. the model to have the same output (or the same internal representations) for different augmented data points. In this view, DA is more with encoding inductive biases, which we cover in Section 5.2, Encoding Inductive Biases. We include all DA work in this section for clarity. There are many examples of using DA in RL, although not all of them are targeted at generalisation performance. Raileanu et al. [104 , UCB-DrAC adapts the DA technique DrQ [ 105 ] to an actor-critic setting [ 106 , PPO], and introduces a method for automatically picking the best augmentation during training. Wang et al. [107 , Mixreg adapts mixup [ 108 ] to the RL setting, which encourages the policy to be linear in its outputs with respect to mixtures of possible inputs. Zhang and Guo [109 , PAADA use adversarial DA combined with mixup. Lee et al. [110 , RandFM uses a randomised convolutional layer at the start of the network to improve robustness to a wide variety of visual inputs. 111 , MixStyle] mixes style statistics across spatial dimensions in CNNs for increased data diversity. All these methods 104 107 109 110 111 ] show improved performance on CoinRun [ 112 ] or OpenAI Procgen [ 36 ] by improving both training and testing performance, and some also show gains on other benchmarks such as visually distracting DeepMind Control (DMC) variants. Hansen and Wang [59 , SODA uses similar augmentations as before but only to learn a more robust image encoder, while the policy is trained on non-augmented data, demonstrating good performance on DMC-GB [ 59 ]. James et al. [113 , RCAN use DA to learn a visual mapping from any different observation back to a canonical observation of the same state, and then train a policy on this canonical observation. They show improved sim-to-real performance on a robotic grasping task. Ko and Ok [114 , InDA,ExDA show that the time at which the augmentations are applied is important for the performance: some augmentations help during training, whereas others only need to be applied to regularise the ﬁnal policy. Fan et al. [50 , SECANT introduce a method for combining DA with policy distillation. As training on strong augmentations can hinder performance, they ﬁrst train on weak augmentations to get an expert policy, which they then distil into a student policy trained with strong augmentations. Hansen et al. [115 , SVEA argues that DA when naively applied increases the variance of Q-value targets, making learning less stable and efﬁcient. They introduce adjustments to the standard data-augmentation protocol by only applying augmentations to speciﬁc components at speciﬁc points during the calculation of the loss function, evaluating performance on DMC-GB [ 59 ]. These works show that in RL settings the choice of when to apply augmentations and what type of augmentations to apply is non-trivial, as the performance of the model during training impacts the ﬁnal performance through changing the data the policy learns from. is just the creation of a non-singleton training context set, and then randomly sampling from this set. Tobin et al. [101] , Sadeghi and Levine [102] and Peng et al. [103] introduced this idea in the setting of sim-to-real transfer in robotics. Much work has been done on different types of DR, and so we cover just sample here. OpenAI et al. [3] describes Automatic Domain Randomisation: instead of sampling possible environment parametrisations uniformly at random, this method dynamically adjusts the distribution in response to the agent’s current performance. Ren et al. [116 , Minimax DSAC use adversarial training to learn the DR for improved robustness. Zhao and Hospedales [117 P2PDRL improve DR though peer-to-peer distillation. Wellmer and Kwok [118 , DDL learns a world model in which to train a policy and then applies dropout to the recurrent network within the world model, effectively performing DR in imagination. Finally, as procedural content generation [ 92 ] is a method for generating a non-zero context set, it can be seen as a form of DR. Works on DR generally leverage the possibility of using a non-uniform context distribution, which possible varies during training. In both DA and DR approaches, as the training environments are increasingly augmented or randomised, optimisation becomes increasingly difﬁcult, which often makes these methods much less sample-efﬁcient. This is the motivation behind [ 113 59 50 ] and other works, all of which train an RL policy on a non-randomised or only weakly randomised environment while using other techniques such as supervised or self-supervised learning to train a robust visual encoder. Finally, note that most of the DA techniques are focused on visual variation in the context set, as that is the easiest variation to produce useful augmentations for. Some DR work focuses on dynamics variation as a way of tackling the sim-to-real problem, where it is assumed that the dynamics will change between training (simulation) and testing (reality). Environment Generation. While DR and PCG produce context-MDPs within a pre-determined context set, it is normally assumed that all the context-MDPs are solvable. However, in some settings it is unknown how to sample from the set of all solvable context-MDPs. For example, consider a simple gridworld maze environment, where the context set consists of all possible block layouts on the grid; some conﬁguration of block placements will result in unsolvable mazes. Further, many block conﬁgurations are not useful for training: they may produce trivially easy mazes. To solve these problems we can learn to generate new levels (sample new contexts) on which to train the agent, such that we can be sure these context-MDPs are solvable and useful training instances. We want a distribution over context-MDPs which is closer to the testing context set, which likely has only solvable context-MDPs. This is known as environment generation. Wang et al. [119] introduces Paired Open-Ended Trailblazer (POET), a method for jointly evolving context-MDPs and policies which solve those MDPs, aiming for a policy that can solve a wide variety of context-MDPs. They produce policies that solve unseen level instances reliably and perform better than training from scratch or a naive curriculum. Wang et al. [120] builds on POET, introducing improvements to the open-ended algorithm including a measure of how novel generated context-MDPs are and a generic measure of how much a system exhibits open-ended innovation. These additions improve the diversity and complexity of context-MDPs generated. Dennis et al. [121] introduces the framework of Unsupervised Environment Design (UED), similar to POET, in which the task is to generate context-MDPs in an unsupervised manner, which are then used to train a policy. The aim is to improve zero-shot generalisation to unseen tasks either within or outside the environment’s context-MDP space. Their method PAIRED outperforms standard DR and a method analogous to POET in the grid-world setting described above, as measured by the zero-shot generalisation performance to unseen levels. Jiang et al. [122] extends the formal framework of UED, combining it with Prioritized Level Replay (PLR) [ 123 ], and motivates understanding PLR as an environment generation algorithm. This combined method shows improved performance in terms of generalisation to unseen out-of-distribution tasks in both gridworld mazes and 2D car-racing tracks. We summarise PLR in the next section. Environment generation and DR are both methods for adjusting the context distribution over some context set provided by the environment. Environment generation tends to learn this sampling procedure, and is targeted at environments where the context set is unstructured such that not all context-MDPs are solvable or useful for training, whereas DR work often uses hard-coded heuristics or non-parametric learning approaches to adjust the context distribution, and focuses on settings where the domains are all solvable but possibly have different difﬁculties or learning potentials. Both can often also be seen as a form of automatic curriculum learning [ 124 ], especially if the context distribution is changing during training and adapting to the agent’s performance. This area is very new and we expect there to be more research soon. However, it does require access to an environment where context-MDPs can be generated at a fairly ﬁne level of detail. Environment generation methods can target generalisation over any kind of variation, as long as that kind of variation is present in the output space of the contextMDP generator. Current methods focus on state-space variation, as that is the most intuitive to formulate as a context set within which the generator can produce speciﬁc MDPs. Optimisation Objectives. It is sometimes possible to change our optimisation objective (explicitly or implicitly) to one which better aligns with testing performance. Changing the distribution over which the training objective is calculated can be seen as implicitly changing the optimisation objective. An initial example in this area applied to improving generalisation is PLR [ 123 ], in which the sampling distribution over levels is changed to increase the learning efﬁciency and generalisation performance of the trained policy. They show both increased training and testing performance on OpenAI Procgen, and the method effectively forms a rough curriculum over levels, enabling more sample-efﬁcient learning while also ensuring that no context-MDP’s performance is too low. Methods for Robust RL (RRL) are also targeted at the zero-shot generalisation problem, and work by changing the optimisation objective of the RL problem. These methods take a worst-case optimisation approach, maximising the minimum performance over a set of possible environment perturbations (which can be understood as different context-MDPs), and are focused on improving generalisation to unseen dynamics. Chen and Li [26] gives an overview and introduction to the ﬁeld. Abdullah et al. [125 , WR optimises the worst-case performance using a Wasserstein ball around the transition function to deﬁne the perturbation set. Mankowitz et al. [126 , SRE-MPO incorporates RLL into MPO [ 127 ] and shows improved performance on the RWRL benchmark [ 86 ]. Pinto et al. [128 , RARL also (implicitly) optimises a robust RL objective through the use of an adversary which is trained to pick the worst perturbations to the transition function. This can also be seen as an adversarial domain randomisation technique. One way of conceptualising why policies do not transfer perfectly at test time is due to the differences between the two environments: the trained model will learn to rely on features during training that change in the testing environment, and generalisation performance then suffers. In this section we review methods that try and explicitly handle the possible differences between the features of the training and testing environments. Encoding Inductive Biases. If we know how features change between the training and testing context-MDPs, we can use inductive biases to encourage or ensure the model does not rely on features that we expect to change: The policy should only rely on features which will behave similarly in both the training and testing environments. For example if we know colour varies between training and testing, and colour is irrelevant for the task, then we can remove colour from the visual input before processing. Simple changes like this tend not to be worthy of separate papers, but they are still important to consider in real-world problem scenarios. IDAAC [ 129 ] adds an adversarial regularisation term which encourages the internal representations of the policy not to be predictive of time within an episode. This invariance is useful for OpenAI Procgen [ 36 ], as timestep is irrelevant for the optimal policy but could be used to overﬁt to the training set of levels. Higgins et al. [16 , DARLA uses -VAEs [ 130 ] to encode the inductive bias of disentanglement into the representations of the policy, improving zero-shot performance on various visual variations. Vlastelica et al. [131 , NAP incorporates a black-box shortest-path solver to improve generalisation performance in hard navigation problems. Zambaldi et al. [132 97] incorporate a relational inductive bias into the model architecture which aids in generalising along ordinal axes of variation, including extrapolation performance. Kansky et al. [133 , SchemaNetworks use an object-oriented and entity-focused architecture, combined with structure-learning methods, to learn logical schema which can be used for backwards-chaining-based planning. These schemas generalise zero-shot to novel state spaces as long as the dynamics are consistent. Wang et al. [134 , VAI use unsupervised visual attention and keypoint detection methods to enforce a visual encoder to only encode information relevant to the foreground of the visual image, encoding the inductive bias that the foreground is the only part of the visual input that is important. Tang et al. [57] introduce AttentionAgent, which uses neuroevolution to optimise an architecture with a hard attention bottleneck, resulting in a network that only receives a fraction of the visual input. The key inductive bias here is that selective attention is beneﬁcial for optimisation and generalisation. Their method generalises zero-shot to unseen backgrounds in CarRacing [ ] and VizDoom [ 135 58 ]. Tang and Ha [136 , SensoryNeuron build on AttentionAgent, adding an inductive bias of permutation invariance in the input space. They argue this is useful for improving generalisation, for a similar reason as before: the attention mechanism used for the permutation-invariant architecture encourages the agent to ignore parts of the input space that are irrelevant. François-Lavet et al. [137 , CRAR use a modular architecture combining dynamics learning and value estimation both in a low-dimensional latent space, and show improved generalisation on a simple maze task. Hill et al. [138 , SHIFTT and Lynch and Sermanet [139 , TransferLanfLfP both use large pretrained models [ 140 141 to encode natural language instructions for instruction following tasks, tackling reward function variation. They both show improved generalisation to novel instructions, leveraging the generalisation power of the large pretrained model. This can be seen as utilising domain knowledge to improve generalisation to novel goal speciﬁcations by incorporating the inductive bias that all the goal speciﬁcations will be natural language. While methods in this area appear dissimilar, they all share the motivation of incorporating speciﬁc inductive biases into the RL algorithm. There are several ways of incorporating domain knowledge as an inductive bias. The architecture of the model can be changed to process the variation correctly. If the variation is one to which the policy should be invariant, it can either be removed entirely, or adversarial regularisation can be used to ensure the policy’s representations are invariant. More broadly, regularisation or auxiliary losses which encourages the policy to handle this variation correctly can be used. A recommendation to make this body of work more systematic is to use the additional types of structural assumptions discussed in Section 3.6 as a starting point for developing algorithms that leverage those assumptions – many of the works discussed here rely on assumptions that can be classiﬁed in those introduced in Section 3.6. For a deeper discussion of inductive biases see [ 142 ], and for the original ideas surrounding inductive biases and No Free Lunch theorems see [12]. Regularisation and Simplicity. When we cannot encode a speciﬁc inductive bias, standard regularisation can be used. This is generally motivated by a paraphrased Occam’s razor: the simplest model will generalise the best. Task-agnostic regularisation encourages simpler models that rely on fewer features or less complex combinations of features. For example, L2 weight decay biases the network towards less complex features, dropout ensures the network cannot rely on speciﬁc combinations of features, and the information bottleneck ensures that only the most informative features are used. Cobbe et al. [112] introduced CoinRun, and evaluated standard supervised learning regularisation techniques on the benchmark. They investigate data augmentation (a modiﬁed form of Cutout [ 143 ]), dropout, batch norm, L2 weight decay, policy entropy, and a combination of all techniques. All the techniques separately improve performance, and the combination improves performance further, although the gains of the combination is minimal over the individual methods, implying that many of these methods address similar causes of worse generalisation. Early stopping can be seen as a form of regularisation, and Ada et al. [144] shows that considering training iteration as a hyperparameter (effectively a form of early stopping) improves generalisation performance on some benchmarks. Almost all methods report performance at the end of training, as often early stopping would not be beneﬁcial (as can be seen from the training and testing reward curves), but this is likely an attribute of the speciﬁc benchmarks being used, and in the future we should keep early stopping in mind. Several methods utilise information-theoretic regularisation techniques, building on the information bottleneck [ 145 ]. Igl et al. [146 , IBAC-SNI and Lu et al. [147 , IB-annealing concurrently introduce methods that rely on the information bottleneck, along with other techniques to improve performance, demonstrating improved performance on OpenAI Procgen, and a variety of random mazes and continuous control tasks, respectively. Eysenbach et al. [148 , RPC extend the motivation of the information bottleneck to the RL setting speciﬁcally, learning a dynamics model and policy which jointly minimises the information taken from the environment by using information from previous states to predict future states. This results in policies using much less information than previously, which has beneﬁts for robustness and generalisation, although this method was not compared on standard generalisation benchmarks to other methods. Chen [149 , SMIRL use surprise minimisation to improve the generalisation of the trained policy, although more rigorous benchmarking is needed to know whether this method has a positive effect. Song et al. [17] show that larger model sizes can lead to implicit regularisation: larger models, especially those with residual connections, generalise better, even when trained on the same number of training steps. This is also shown in [112, 36]. Learning Invariances. Sometimes we cannot rely on a speciﬁc inductive bias or standard regularisation. This is a very challenging setting for RL (and machine learning in general) to tackle, as there is a fundamental limit to performance due to a kind of “no free lunch” analogy: we cannot expect a policy to generalise to arbitrary contexts. However, several techniques can help, centred around the idea of using multiple training contexts to learn the invariances necessary to generalise to the testing contexts. If the factors of variation within the training contexts are the same as the factors of variation between the training and testing contexts, and the values which those factors take in testing are not far from those in training, then you can use that to learn these factors of variation and how to adapt or be invariant to them. representation space and the use of intrinsic rewards and regularisation. Agarwal et al. [152 , PSM suggest limitations of bisimulation metrics, and instead propose a policy similarity metric, where states are similar if the optimal policy has similar behaviour in that and future states. They use this metric, combined with a contrastive learning approach, to learn policies invariant to observation variation. Several approaches use multiple contexts to learn an invariant representation, which is then assumed to generalise well to testing contexts. Sonar et al. [153, IPO] apply ideas from Invariant Risk Minimization [154] to policy optimisation, learning a representation which enables jointly optimal action prediction across all domains, and show improved performance over PPO on several visual and dynamics variation environments. Bertrán et al. [155 , IAPE introduce the Instance MDP, an alternative formalism for the generalisation problem, and then motivate theoretically an approach to learn a collection of policies on subsets of the training domains, such that the aggregate policy is invariant to any individual context-speciﬁc features which would not generalise. They show improved performance on the CoinRun benchmark [ 112 ] compared to standard regularisation techniques. Ghosh et al. [13 , LEEP also produce an invariant policy across a collection of subsets of the training contexts but motivate this with Bayesian RL. Their approach learns separate policies on each subset and then optimistically aggregates them at test time, as opposed to [ 155 ] which uses the aggregate policy during training for data collection and learns the collection of policies off-policy. While these approaches are similar there has been no direct comparison between them. Other approaches try to learn behaviourally similar representations with less theoretical motivation. Liu et al. [156 CSSC use behavioural similarity (similarity of short future action sequences) to ﬁnd positive and negative pairs for a contrastive learning objective. This auxiliary loss aids in generalisation and sample efﬁciency in several OpenAI Procgen games. Mazoure et al. [157 , CTRL uses clustering methods and self-supervised learning to deﬁne an auxiliary task for representation learning based on behavioural similarity, showing improved performance on OpenAI Procgen. Li et al. [158 , DARL uses adversarial learning to enforce the representations of different domains to be indistinguishable, improving performance on visually diverse testing contexts, even when only trained on simple training contexts. Fan and Li [159 , DRIBO uses contrastive learning combined with an information-theoretic objective to learn representations that only contain task-relevant information while being predictive of the future. They show improved performance on both visually diverse domains in DeepMind Control, and in OpenAI Procgen. Adapting Online. A ﬁnal way to handle differences between training and testing contexts is to adapt online to the testing contexts. This adaption has to happen within a single episode and be rapid enough to be useful for improved performance within that episode. Most work on Meta RL, which is traditionally concerned with fast adaptation, assumes access to multiple training episodes in the testing CMDP, violating the zero-shot assumption. However, there are works which can adapt zero-shot. While not being exhaustive, we give a brief overview of this body of work here, focusing on key examples where zero-shot generalisation is evaluated. Many methods learn a context encoder or inference network, which then conditions either a policy or dynamics model for improved generalisation. The inference of this context at test-time can be seen as online adaptation. Yu et al. [160 UP-OSI uses Online System Identiﬁcation to infer the context, which then conditions a policy. Yen-Chen et al. [161 EVF learns a context inference network end-to-end with a dynamics model and uses this to adapt to novel objects. Kumar et al. [162 , RMA tackles the sim-to-real problem by training an agent using domain randomisation in simulation, and training a context inference model to condition the policy, similar to [ 160 ] but with learned context inference. Ball et al. [163 , AugWM take a similar idea but work in the ofﬂine RL setting, so ﬁrst train a world model from ofﬂine data. They then perform domain randomisation of a speciﬁc form in the world model, and then use a hard-coded update rule (enabled by the speciﬁc form of domain randomisation) to determine the context to condition the policy, enabling it to adapt zero-shot to downstream tasks. These methods all tackle dynamics variation in continuous control or visual tasks. Often, these methods make use of domain randomisation approaches but aim to have a context-conditioned adaptive policy or model, rather than a policy invariant to all possible contexts. becomes more difﬁcult to draw the line between learned and hard-coded update rules: The gradient update itself is hard-coded, but the initialisation is learned, and in the recurrent case the update is fully learned. Finally, several methods meta-learn an adaptation function during training. RL 169 170 ] is a meta RL method where a recurrent network is used, the hidden state of which is not reset at episode boundaries, allowing it to learn and adapt within the recurrent state over multiple episodes. While often compared to methods that require multiple training episodes, this method can often adapt and perform well within a single episode, due to the optimisation approach and architecture. Zintgraf et al. [99] introduce an extension to RL called VariBAD based on bayesian RL, where the recurrent network learns to produce latent representations that are predictive of future rewards and previous transitions. This latent representation is used by the policy. Dorfman et al. [171 , BOReL adjust VariBAD to be usable in an ofﬂine setting, improving zero-shot exploration using ofﬂine data. Zintgraf et al. [172 , HyperX improves VariBAD with additional exploration bonuses to improve meta-exploration. Mishra et al. [173 , SNAIL models fast adaptation as a sequence-to-sequence problem, and learns an attention-based architecture which encodes sequences of experience to condition the policy. These methods all show improved performance on zero-shot generalisation tasks, even thought their main focus is on adaptation over longer time horizons. The motivations in the previous two sections are mostly equally applicable to supervised learning. However, on top of the problems of generalisation from supervised learning, RL has additional problems which inhibit generalisation performance. In this section we discuss methods targeted at these problems, and also discuss methods that improve generalisation purely through more effective optimisation on the training set in a way that does not overﬁt (at least empirically). RL-speciﬁc Problems. Optimisation in RL has additional issues on top of supervised learning, such as the nonstationarity of the data distribution, and the need to explore. These issues likely interact with generalisation in a non-trivial way. Igl et al. [174 , ITER shows that the non-stationarity of RL training means that policies learn features that do not generalise well, even if they achieve the same training performance. To address this, they introduce a method to iteratively distil the current policy network into a new policy network with reinitialised weights. This reduces the impact of non-stationarity on the new network, as it is being trained on a more stationary distribution. Other RL-speciﬁc optimisation issues likely interact with generalisation either positively or negatively, and this area deserves further attention if we are to go beyond techniques copied and adjusted from supervised learning. Better Optimisation without Overﬁtting. Several works improve generalisation through improving the training performance without overﬁtting. Cobbe et al. [175] introduce Phasic Policy Gradient (PPG), which adjusts the training regime and architecture of PPO such that the policy and value functions use entirely separate networks (rather than just separate heads), which allows the value head to be optimised for longer while not causing the policy to overﬁt. To recover the beneﬁts of a joint representation, the value network is distilled into an auxiliary value head on the policy network. Raileanu and Fergus [129] build on PPG and introduce Decoupled Advantage Actor Critic (DAAC). They distil an advantage function calculated with GAE into the policy instead of a value function, which further ensures that the policy does not overﬁt to details that may be predictive of value function but not optimal action selection. They both show improved performance on OpenAI Procgen, demonstrating that value functions can be optimised more strongly than policies. Singh and Zheng [176 , Sparse DVE adjusts the architecture of the value function to allow for a multi-modal output, more closely modelling the true value function given just a visual input. This novel architecture combined with sparsity losses to ensure the value function has the desired properties reduces the variance of value function prediction and improves performance in terms of both return and navigation efﬁciency in OpenAI Procgen. Another angle on better optimisation is the use of model-based RL (MBRL). Very little work has applied MBRL to generalisation benchmarks, with [ 177 ] being an initial example. Anand et al. [177] apply MuZero Reanalyse [ 178 179 ], a SOTA MBRL method, to OpenAI Procgen [ 36 ], showing much-improved performance over SOTA model-free methods at much lower sample complexity. This shows the potential of using MBRL to improve generalisation to varying state and observations. The authors also apply MuZero to the meta-learning tasks in Meta-World [ 73 ], although the performance there is not as impressive, showing that generalising to new rewards (as is necessary in Meta-World) is not currently as amenable to MBRL approaches. While the methods described above do not target generalisation speciﬁcally, they improve test-time performance on generalisation benchmarks and so are included here. We hope the ﬁeld will move towards benchmarks like Procgen being the standard for RL (and not just generalisation), such that in time this work is considered standard RL, rather the generalisation speciﬁcally. Having described existing methods for generalisation in RL, and categorised them in Table 2, we now draw some broader conclusions about the ﬁeld, as well as discuss possible alternative classiﬁcations of methods. Alternative Classiﬁcations. We have presented one possible classiﬁcation of RL methods, but there are of course others. One alternative is to classify methods based on whether they change the architecture, environment or objective of the standard RL approach. This is useful from a low-level implementation perspective of what the differences are between approaches. This approach is not as useful for future researchers or practitioners who hope to choose a generalisation method for a concrete problem they are facing, as there is not a clear mapping between implementation details and whether a method will be effective for a speciﬁc problem. We do apply this classiﬁcation through the colours in Table 2, to emphasise the current focus on adjusting the loss or objective function in current methods. Another approach would be to classify methods based on which benchmarks they attempt to solve, or what speciﬁc problem motivated their design. This goes too far in the other direction, grounding methods in exactly the benchmarks they tackle. While practitioners or researchers could try and see which benchmark is most similar to their problem, they might not understand which differences between benchmarks are most important, and hence choose a method that is not likely to succeed. This classiﬁcation is also less useful in pointing out areas where there is less research being done. Our approach strikes a balance between these two approaches, describing the problem motivations and solution approaches at a high level which is useful for both practitioners and researchers in choosing methods and investigating future research directions. Strong Generalisation Requires Inductive Biases. As described in Section 4.3, What Generalisation Can We Expect?, there are hard generalisation problems involving combinatorial interpolation or extrapolation. We want to tackle these problems, as they will occur in real-world scenarios when we have limited contexts to train on, or we know the type of variation but cannot create context-MDPs in the deployment context-MDP set (e.g. due to limited simulators). To tackle these problems, we need stronger inductive biases targeted towards speciﬁc types of extrapolation, as there is unlikely to be a general-purpose algorithm that can handle all types of extrapolation [ 12 ]. When doing research tackling extrapolative generalisation, researchers should be clear that they are introducing an inductive bias to help extrapolate in a speciﬁc way, and be rigorous in analysing how this inductive bias helps. This involves also discussing in which situations the bias may hinder performance, for example in a different setting where extrapolation requires something else. Going Beyond Supervised Learning as Inspiration. Methods for improving generalisation from supervised learning have been a source of inspiration for many methods, particularly for visual variation. This is exactly the variation that happens in computer vision, and hence many methods from that ﬁeld are applicable. However, non-visual forms of generalisation (i.e. dynamics, state and reward), while equally important are less studied. These challenges will be speciﬁc to RL and interact with other problems unique to RL such as the exploration-exploitation trade-off and the non-stationarity of the underlying data distribution. We hope to see more work in the area of non-visual generalisation, particularly when other hard RL problems are present. <title>6 Discussion And Future Work</title> In this section we highlight further points for discussion, building on those in Section 4.3 and Section 5.4, including directions for future research on new methods, benchmarks, evaluation protocols and understanding. This survey focuses on zero-shot policy transfer. We believe this problem setting is a reasonable one that captures many challenges relevant for deploying RL systems. However, there are many important scenarios where zero-shot generalisation is impossible, or the assumptions can be relaxed. We will want to move beyond zero-shot policy transfer if we are to use RL effectively in a wider variety of scenarios. setting can be conceptualised as one in which the domain that tasks are within changes over time. This is in contrast to benchmarks that evaluate generalisation to levels sampled from the same distribution as during training, such as OpenAI Procgen [ 36 ]. Hence, we recommend work on building new CRL benchmarks, to make progress on CRL and zero-shot generalisation together. Coupled with the idea of CRL as a more realistic setting for generalisation in RL, we can take inspiration from how humans generalise and what they transfer when generalising, to go beyond transferring a single policy. While humans may not always be able to achieve good results zero-shot on a new task, if the task is related to previously seen tasks, they can reuse previous knowledge or skills to learn the new task faster. This broader notion of generalisation of objects other than a complete policy (e.g. skills or environment knowledge) will become more relevant when we start to build more powerful RL systems. Hierarchical and multi-task RL are related ﬁelds, and methods in these settings often learn subcomponents, modules or skills on source tasks (possibly in an unsupervised manner) which they can then use to increase learning speed and performance when transferred to novel tasks [ 181 182 ]. It is likely the capability to transfer components other than a single policy will be useful for future systems, and it would hence be beneﬁcial to have benchmarks that enable us to test these kinds of capabilities. However, this is a challenging request to meet, as deﬁning what these components are, and deciding on a performance metric for these subcomponent transfer benchmarks, are both difﬁcult conceptual problems. We hope to see work on this area in the future. A ﬁnal assumption which is almost untouched in RL is that of a ﬁxed action space between training and testing. Recently, Jain et al. [183] introduced a novel problem setting and framework centered around how to generalise to new actions in RL. They introduce several benchmarks for testing methods which generalise to new actions, and a method based on learning an action representation combined with an action-ranking network which acts as the policy at test time. There is very little work in this area, and we do not cover it in this survey, but it presents an interesting future direction for generalisation research. In Dulac-Arnold et al. [86] , the authors propose 9 properties that are characteristic of real-world RL. In thinking about the current set of generalisation benchmarks, these properties are relevant in two ways. First, when applying our methods to the real world, we will have to tackle these problems. Hence, it would be beneﬁcial if generalisation benchmarks had these properties, such that we can ensure that our generalisation methods work in real-world environments. Second, several properties are particularly relevant for generalisation and the design of new benchmarks: (1) high cost of new samples, (2) training from ofﬂine data and (3) underspeciﬁed or multi-objective reward functions. We explain each of these and their relation to generalisation below. Context Efﬁciency. Addressing (1), it is likely that the high cost of new samples will also mean a high cost of new environment tasks or contexts. This means we want methods that are context efﬁcient as well as sample efﬁcient, and hence we require benchmarks and evaluation protocols in which only a few contexts are allowed during training, rather than several 1000s. It is also worth investigating if there is an optimal trade-off (for different costs per sample and per context) between new training samples and new contexts. This line of work would revolve around different possible evaluation metrics based on how many contexts are needed to reach certain levels of generalisation performance. Further, there may be ways of actively selecting new contexts to maximise the generalisation performance while minimising the number of new contexts used, effectively a form of active learning. Evaluating context efﬁciency will be more computationally expensive, as models will need to be repeatedly trained on different numbers of training contexts, so work which ﬁgures out how to more efﬁciently evaluate this property is also beneﬁcial. Sim-to-Real and Ofﬂine RL. To tackle (2), two options emerge. The ﬁrst is relying on good simulations, and then tackling the sim-to-real problem, and the second is tackling the ofﬂine RL problem directly [ 184 ]. These approaches might be more or less relevant or applicable depending on the scenario: for example in many robotics applications a simulator is available, whereas in healthcare settings it is likely learning from ofﬂine data is the only approach possible. Sim-to-real is a problem of domain generalisation. If this direction is most relevant, it implies we should focus on building environments that test for good domain generalisation. Existing work on sim-to-real does address this to some extent, but it would be beneﬁcial to have a fully simulated benchmark for testing sim-to-real methods, as that enables faster research progress than requiring real robots. This is a difﬁcult task and is prone to the possibility of overﬁtting to the simulation of the sim-to-real problem, but it would be useful as an initial environment for testing sim-to-real transfer. Ofﬂine RL is also a problem of generalisation: a key issue here is generalising to state-action pairs unseen in the training data, and most current methods tackle this by conservatively avoiding such pairs [ 185 ]. If we had methods to reliably extrapolate to such pairs we could improve ofﬂine RL performance. As well as generalisation improving ofﬂine RL, it is likely that future RL deployment scenarios will need to tackle the combination of ofﬂine RL and generalisation: training policies ofﬂine that then generalise to new contexts unseen in the ofﬂine dataset. Current ofﬂine RL benchmarks [ 186 187 ] do not measure generalisation in this way, but we believe they should enable us to tackle this combined problem: for example, training on ofﬂine data from 200 levels in OpenAI Procgen, and evaluating on the full distribution of levels. If tackling this is infeasible with current methods (as both ofﬂine RL and generalisation are hard problems), then a good compromise is to ﬁrst work on the ofﬂine-online setting, where ofﬂine RL is used for pretraining, followed by online ﬁne-tuning. Some work has been done in this area [ 188 ], but this does not tackle the generalisation problem speciﬁcally. Creating benchmarks for evaluating these approaches, where the emphasis is on reducing the length of the online ﬁne-tuning stage and evaluating generalisation after ﬁne-tuning, would move us towards truly ofﬂine RL generalisation while still being tractable with current methods. Reward Function Variation. It is likely future RL systems will be goal- or task-conditioned, as it will be more efﬁcient to train a general system to do several related tasks than to train a different system for each task. Here, as well as generalising to new dynamics and observations, the trained policies will need to generalise to unseen goals or tasks. The policy will need to be able to solve novel problem formulations, and hence have a more generic problem-solving ability. Benchmarks which address this capability are hence necessary for progress towards more general RL systems (see Section 6.4). Related to challenges of generalisation surrounding reward functions, for many real-world problems designing good reward functions is very difﬁcult. A promising approach is using inverse RL [ 189 190 , IRL] to learn a reward function from human demonstrations [ 191 192 193 194 ], rather than hand-crafting reward functions for each task. This is often more time-efﬁcient, as demonstrating a task is easier than specifying a reward function for it. There are two generalisation problems here: ensuring the learned reward function generalises to unseen context-MDPs during policy training, and ensuring the trained policy generalises to unseen context-MDPs at test time. The ﬁrst is an IRL generalisation problem, and the second is the standard problem of generalisation we have considered here. Solving both will be important for ensuring that this approach to training agents is effective. Work building benchmarks and methods to solve these problems would be valuable future work. Some of these directions could be addressed by combining new evaluation protocols with pre-existing environments to create new benchmarks, rather than requiring entirely new environments to be designed and created. Generalisation performance is usually reported using a single scalar value of test-time performance. However, this does not give us much information with which to compare and choose between methods. While better performance on a benchmark, all else being equal, probably means that a method is more useful, it is usually not clear to what extent the ordering of methods on a benchmark’s leaderboard is representative of the hypothetical ordering of those methods on a real-world problem scenario for which we have to choose a method. To alleviate this, performance should be reported on multiple different testing context sets which evaluate different types of generalisation, and radar plots (such as those in [ 195 ]) can be used to compare methods. This will be more useful for comparing methods in a more realistic way, as well as for practitioners choosing between methods. Very few environments have the context set required for this type of evaluation, and even those that do would require additional work to create the speciﬁc testing context sets. Hence, we recommend that future benchmarks for generalisation are designed to enable this type of evaluation. This requires building environments with controllable context sets as well as PCG components (Section 4.3, The Downsides of Procedural Content Generation for Generalisation), as well as careful thought to create the variety of testing context sets, ensuring they match important types of generalisation. A ﬁrst way of splitting up testing context sets might be by the type of variation between training and testing, as well as whether interpolation or extrapolation are required to generalise to that context set. There are likely many other ways, which may be domain-speciﬁc or general across many domains. Many of the methods for generalisation tackle observation function or state space variation. Both of these (or at least the practical implementations of them used in the corresponding benchmarks) tend to produce environment families in which it is much simpler to verify (at least intuitively) The Principle Of Unchanged Optimality, meaning that tackling the generalisation problem is tractable; generalisation problems with this style of variation tend to be easier to solve. These two types of variation will appear in real-world scenarios, but the other types of variation are equally important and often harder to tackle. Work on dynamics is mostly focused on two speciﬁc settings: sim-to-real transfer, and multi-agent environments. In sim-to-real transfer [ 28 ], there is always dynamics variation between the simulator and reality, and much work has focused on how to train policies in this setting. More generally, work on robotics and continuous control tends to address some forms of dynamics variation either in how the robot itself is controlled (e.g. due to degrading parts) or in the environment (e.g. different terrain). In multi-agent environments, if the other agents are considered part of the environment (for example in a single-agent training setting), then varying the other agents varies the dynamics of the environment [ 22 ]. These both occur in the real world, but there are inevitably other forms of dynamics variation which are less well studied. Investigating what these other forms of dynamics variation are and whether studying them would be useful is promising future work. Tackling reward-function variation will be required to train general-purpose policies that can perform a variety of tasks, and generalise to unseen tasks without further training data (as discussed in Section 6.2, Reward Function Variation). This variation is more difﬁcult to tackle, and it is often difﬁcult or impossible to verify The Principle Of Unchanged Optimality [ 93 ]. However, we must do work on research to tackle these problems, as otherwise RL approaches will be limited to less-ambitious problems or less-general applications. Further work building more benchmarks that enable testing for reward function variation, especially beyond simple styles of goal speciﬁcation such as a target state, would be beneﬁcial. Special attention needs to be paid to The Principle Of Unchanged Optimality [ 93 ] while building these benchmarks: current work tends to handle this by conditioning the policy on a goal or reward speciﬁcation. Research on what approaches to goal speciﬁcation are both tractable for policy optimisation and useful for real-world scenarios would be beneﬁcial, as there is likely a trade-off between these two desirable attributes. Hill et al. [138] , Lynch and Sermanet [139] are good examples of investigating natural language as goal speciﬁcation, utilising pretrained models to improve generalisation, and we look forward to seeing more work in this area. While beyond the scope of this survey, several works try to understand the problems underlying generalisation in RL. Works in this area include [ 17 ], which describes the notion of observational overﬁtting as one cause of the generalisation gap in RL; [ 196 ], which analyses the relationship between gradient interference and generalisation in supervised and RL showing that temporal difference methods tend to have lower-interference training, which correlates with worse generalisation; [ 174 ] which studies transient non-stationarity in RL and shows that it negatively impacts RL generalisation; and [ 197 ], which investigates what environmental factors affect generalisation in an instruction-following task, ﬁnding for example that an egocentric viewpoint improves generalisation, as does a richer observation space. This research is barely scratching the surface of understanding why generalisation in RL in particular is a challenge, and there is much future work to be done. This will enable us to build better methods, and understand any theoretical limits to the diversity of tasks that an RL agent can solve given a limited number of training contexts. The precise empirical experimentation required for this kind of research is exactly that which is enabled by having tight control over the factors of variation in the environments being used, which reinforces the conclusion made in Section 4.3, The Downsides of Procedural Content Generation for Generalisation that purely PCG environments are unsuited for a study of generalisation in RL. In this subsection we summarise directions for future work on methods for generalisation, informed by Section 5. As described in Section 6.5, there are many RL-speciﬁc factors that interact with generalisation performance, often likely in a negative way. Examples of these factors include the non-stationarity of the data distribution used for training; bootstrapping and TD learning in general; and the need for exploration. Work to understand these factors and then build methods to tackle them as discussed in Section 5.3, RL-speciﬁc Problems is a fruitful direction for future work. We often have a context space that is unstructured or contains many unsolvable context-MDPs. Methods that enable more effective sampling from these context spaces can alleviate this. Several methods were covered in Section 5.1, Environment Generation but more work in this area, tackling more challenging and realistic environments with different types of variation, would be beneﬁcial. be necessary in these scenarios. Initial work in this area is described in Section 5.2, Adapting Online, but much more research should be done. There are several under-explored approaches that cut across the categorisation in this work. As shown in Table 2, most methods focus on changing the loss function or algorithmic approach. Architectural changes informed by inductive biases are less well studied, with notable examples being [ 175 176 57 131 97 ]. More work can be done on investigating different architectures, either taking inspiration from supervised learning or creating RL-speciﬁc architectures. These architectures could encode inductive biases in ways that are difﬁcult to encode through the use of auxiliary losses or regularisation. A second under-explored area is model-based reinforcement learning (MBRL) for generalisation. Most methods surveyed here are model-free, with notable exceptions being [ 163 133 164 177 ]. Learning a world model and combining it with planning methods can enable stronger forms of generalisation, especially to novel reward functions (if the reward function is available during planning). As long as the model generalises well, it could also enable generalisation to novel state and observation functions. World models which can adapt to changing dynamics will be more challenging, but Seo et al. [164] give an initial example. Anand et al. [177] is the ﬁrst example investigating how well standard MBRL approaches generalise, and we look forward to seeing more work in this area. <title>7 Conclusion</title> The study of generalisation in RL is still new but is of vital importance if we want to develop applicable and usable RL solutions to real-world problems. In this survey we have aimed to clarify the terminology and formalism concerning generalisation in RL, bringing together disparate threads of research together in a uniﬁed framework. We presented a categorisation of benchmarks for generalisation, splitting the taxonomy into environments and evaluation protocols, and we categorised existing methods for tackling the wide variety of generalisation problems. Here we summarise the key takeaways of this survey (with pointers to the more in-depth discussion of the takeaway). The ﬁrst two takeaways are concerned with the problem setting as a whole. The next four are focused on evaluating generalisation through benchmarks and metrics, and future work in these areas. The last two are focused on methods for tackling the generalisation problem. Zero-shot policy transfer is useful to study, even if in speciﬁc settings we may be able to relax the zero-shot assumption, as it provides base algorithms upon which domain-speciﬁc solutions can be built (Section 3.7, Motivating Zero-Shot Policy Transfer). However, more work should be done to look beyond zero-shot policy transfer, particularly at continual reinforcement learning, as a way to get around the restriction of the principle of unchanged optimality (Section 6.1). Purely black-box PCG environments are not useful for testing speciﬁc forms of generalisation and are most useful for ensuring robust improvements in standard RL algorithms. Combining PCG and controllable factors of variation is our recommended way to design new environments, having the best trade-off between high variety and the possibility of scientiﬁc experimentation (Section 4.3, The Downsides of Procedural Content Generation for Generalisation). This also enables a more multidimensional approach to evaluating generalisation performance (Section 6.3), and speciﬁc experimentation aimed at improving our understanding of generalisation in RL (Section 6.5). For real-world scenarios, we have to consider both sample-efﬁciency and context efﬁciency. Evaluating the performance of methods on different sizes of training context sets is a useful evaluation metric which gives us more information to choose between different methods (Section 6.2, Context Efﬁciency). Work on generalisation problems associated with ofﬂine RL is under-explored and would ensure that ofﬂine RL approaches are able to generalise effectively (Section 6.2, Sim-to-Real and Ofﬂine RL). While observation-function and state-space variation are commonly studied, dynamics variation is only tackled in limited settings and reward-function variation is very under-studied. These stronger forms of variation are still likely to appear in real-world scenarios, and hence should be the focus of future research (Section 6.4). For stronger forms of generalisation, stronger inductive biases are necessary, and research should be up-front about what the inductive bias they are introducing is, how it tackles the speciﬁc benchmark they are tackling, and how general they expect that inductive bias to be (Section 5.4, Strong Generalisation Requires Inductive Biases). There is much underexplored future work in developing new methods for improved generalisation, such as modelbased RL, new architectures, fast online adaptation, solving RL-speciﬁc generalisation problems, and environment generation (Section 6.6). We hope that this survey will help clarify and unify work tackling the problem of generalisation in RL, spurring further research in this area, and survey as a touch-point and reference for researchers and practitioners both inside and outside the ﬁeld. <title>Acknowledgements</title> We thank (in alphabetical order) Flo Dorner, Jack Parker-Holder, Katja Hofmann, Laura Ruis, Maximilian Igl, Mikayel Samvelyan, Minqi Jiang, Nicklas Hansen, Roberta Raileanu, Yingchen Xi and Zhengyao Jiang for discussion and comments on drafts of this work. We also thank (alphabetically) André Biedenkapp, Chelsea Finn, Eliot Xing, Jessica Hamrick, Pablo Samuel Castro, Sirui Xie, Steve Hansen, Theresa Eimer, Vincent François-Lavet and Zhou Kaiyang for pointing out missing references or work for an updated version of this survey. <title>Author Contributions</title> Robert Kirk led the work, developed the formalism, benchmarks categorisation, methods categorisation, and discussion and future work, wrote the full manuscript of the survey, and wrote successive drafts with comments and feedback from the other authors. Amy Zhang wrote parts of Sections 3.1, 3.6, 3.7 and 4.3 and Appendix A, as well as providing improvements on the entire work through discussion and editing. Tim Rocktäschel and Edward Grefenstette advised Robert Kirk, providing discussion and feedback in developing the ideas behind the survey, and provided feedback and comments on the manuscript. <title>References</title> [1] Angelos Filos, Panagiotis Tigkas, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, and Yarin Gal. Can autonomous vehicles identify, recover from, and adapt to distribution shifts? In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3145–3153. PMLR, 2020. URL http://proceedings. mlr.press/v119/filos20a.html. [2] Andr&#233 Biedenkapp, H. Furkan Bozkurt, Theresa Eimer, Frank Hutter, and Marius Lindauer. Dynamic Algorithm Conﬁguration: Foundation of a New Meta-Algorithmic Framework. ECAI 2020, pages 427–434, 2020. doi:10.3233/FAIA200122. URL https://ebooks.iospress.nl/doi/10.3233/FAIA200122. [3] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang. Solving Rubik’s Cube with a Robot Hand. arXiv:1910.07113 [cs, stat], 2019. URL http://arxiv.org/abs/1910.07113. [4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artiﬁcial Intelligence Research, 47:253–279, 2013. ISSN 1076-9757. doi:10.1613/jair.3912. URL https://www.jair.org/index.php/jair/article/view/10819. [5] E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026–5033, 7. ISBN 21530866. doi:10.1109/IROS.2012.6386109. [6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv:1606.01540 [cs], 2016. URL http://arxiv.org/abs/1606.01540. [7] Shimon Whiteson, Brian Tanner, Matthew E. Taylor, and Peter Stone. Protecting against evaluation overﬁtting in empirical reinforcement learning. In 2011 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 120–127, 2011. doi:10.1109/ADPRL.2011.5967363. [8] Amy Zhang, Nicolas Ballas, and Joelle Pineau. A Dissection of Overﬁtting and Generalization in Continuous Reinforcement Learning. arXiv:1806.07937 [cs, stat], 2018. URL http://arxiv.org/abs/1806.07937. [9] Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A Study on Overﬁtting in Deep Reinforcement Learning. arXiv:1804.06893 [cs, stat], 2018. URL http://arxiv.org/abs/1804.06893. [10] Jesse Farebrother, Marlos C. Machado, and Michael Bowling. Generalization and Regularization in DQN. arXiv:1810.00123 [cs, stat], 2020. URL http://arxiv.org/abs/1810.00123. [11] Shani Gamrian and Yoav Goldberg. Transfer learning for related reinforcement learning tasks via imageto-image translation. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 2063–2072. PMLR, 2019. URL http: //proceedings.mlr.press/v97/gamrian19a.html. [12] D.H. Wolpert and W.G. Macready. No free lunch theorems for optimization. IEEE Transactions on Evolutionary Computation, 1(1):67–82, 1997. ISSN 1941-0026. doi:10.1109/4235.585893. [13] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P. Adams, and Sergey Levine. Why Generalization in RL is Difﬁcult: Epistemic POMDPs and Implicit Partial Observability. arXiv:2107.06277 [cs, stat], 2021. URL http://arxiv.org/abs/2107.06277. [14] Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual Markov Decision Processes. arXiv:1502.02259 [cs, stat], 2015. URL http://arxiv.org/abs/1502.02259. [15] James Harrison, Animesh Garg, Boris Ivanovic, Yuke Zhu, Silvio Savarese, Li Fei-Fei, and Marco Pavone. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems. arXiv:1707.04674 [cs], 2017. URL http://arxiv.org/abs/1707.04674. [16] Irina Higgins, Arka Pal, Andrei A. Rusu, Loïc Matthey, Christopher Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. DARLA: improving zero-shot transfer in reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1480–1490. PMLR, 2017. URL http://proceedings.mlr.press/v70/ higgins17a.html. [17] Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, and Behnam Neyshabur. Observational overﬁtting in reinforcement learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= HJli2hNKDH. [18] OpenAI, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with Large Scale Deep Reinforcement Learning. arXiv:1912.06680 [cs, stat], 2019. URL http://arxiv.org/abs/ 1912.06680. [19] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja Huang, Laurent Sifre, Trevor Cai, John P. Agapiou, Max Jaderberg, Alexander S. Vezhnevets, Rémi Leblond, Tobias Pohlen, Valentin Dalibard, David Budden, Yury Sulsky, James Molloy, Tom L. Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani Yogatama, Dario Wünsch, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and David Silver. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575 (7782):350–354, 2019. ISSN 1476-4687. doi:10.1038/s41586-019-1724-z. URL https://www.nature.com/ articles/s41586-019-1724-z. [20] Hengyuan Hu, Adam Lerer, Brandon Cui, David Wu, Luis Pineda, Noam Brown, and Jakob Foerster. Off-Belief Learning. arXiv:2103.04000 [cs], 2021. URL http://arxiv.org/abs/2103.04000. [21] Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob N. Foerster. "other-play" for zero-shot coordination. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 4399–4410. PMLR, 2020. URL http://proceedings.mlr.press/v119/hu20a.html. [22] Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, Nat McAleese, Nathalie Bradley-Schmieg, Nathaniel Wong, Nicolas Porcel, Roberta Raileanu, Steph Hughes-Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-Ended Learning Leads to Generally Capable Agents. arXiv:2107.12808 [cs], 2021. URL http://arxiv.org/abs/2107.12808. [23] Simon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation sufﬁcient for sample efﬁcient reinforcement learning? In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum? id=r1genAVKPB. [24] Dhruv Malik, Yuanzhi Li, and Pradeep Ravikumar. When Is Generalizable Reinforcement Learning Tractable? arXiv:2101.00300 [cs, stat], 2021. URL http://arxiv.org/abs/2101.00300. [25] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards Continual Reinforcement Learning: A Review and Perspectives. arXiv:2012.13490 [cs], 2020. URL http://arxiv.org/abs/2012.13490. [26] Shiyu Chen and Yanjie Li. An Overview of Robust Reinforcement Learning. In 2020 IEEE International Conference on Networking, Sensing and Control (ICNSC), pages 1–6, 2020. doi:10.1109/ICNSC48988.2020.9238129. [27] Jun Morimoto and Kenji Doya. Robust reinforcement learning. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems (NIPS) 2000, Denver, CO, USA, pages 1061–1067. MIT Press, 2000. URL https: //proceedings.neurips.cc/paper/2000/hash/e8dfff4676a47048d6f0c4ef899593dd-Abstract. html. [28] Wenshuai Zhao, Jorge Peña Queralta, and Tomi Westerlund. Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: A Survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pages 737–744, 2020. doi:10.1109/SSCI47803.2020.9308468. [29] Matthias Müller-Brockhausen, Mike Preuss, and Aske Plaat. Procedural Content Generation: Better Benchmarks for Transfer Reinforcement Learning. arXiv:2105.14780 [cs], 2021. URL http://arxiv.org/abs/2105. 14780. [30] Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer Learning in Deep Reinforcement Learning: A Survey. arXiv:2009.07888 [cs, stat], 2021. URL http://arxiv.org/abs/2009.07888. [31] Nelson Vithayathil Varghese and Qusay H. Mahmoud. A survey of multi-task deep reinforcement learning. Electronics, 9(9), 2020. ISSN 2079-9292. doi:10.3390/electronics9091363. URL https://www.mdpi.com/ 2079-9292/9/9/1363. [32] Susan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A Survey of Exploration Methods in Reinforcement Learning. arXiv:2109.00157 [cs], 2021. URL http://arxiv.org/abs/2109. 00157. [33] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone. Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey. arXiv:2003.04960 [cs, stat], 2020. URL http://arxiv.org/abs/2003.04960. [34] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How do neural networks generalise? arXiv:1908.08351 [cs, stat], 2020. URL http://arxiv.org/abs/1908.08351. [35] Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SygcCnNKwr. [36] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 2048–2056. PMLR, 2020. URL http://proceedings.mlr.press/v119/cobbe20a.html. [37] Austin Stone, Oscar Ramirez, Kurt Konolige, and Rico Jonschkowski. The Distracting Control Suite – A Challenging Benchmark for Reinforcement Learning from Pixels. arXiv:2101.02722 [cs], 2021. URL http: //arxiv.org/abs/2101.02722. [38] Finale Doshi-Velez and George Dimitri Konidaris. Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations. In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 1432–1440. IJCAI/AAAI Press, 2016. URL http: //www.ijcai.org/Abstract/16/206. [39] Simon S. Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudík, and John Langford. Provably efﬁcient RL with rich observations via latent state decoding. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1665–1674. PMLR, 2019. URL http://proceedings.mlr.press/v97/du19b.html. [40] Amy Zhang, Clare Lyle, Shagun Sodhani, Angelos Filos, Marta Kwiatkowska, Joelle Pineau, Yarin Gal, and Doina Precup. Invariant causal prediction for block mdps. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 11214–11224. PMLR, 2020. URL http://proceedings.mlr.press/v119/zhang20t. html. [41] Craig Boutilier, Richard Dearden, and Moisés Goldszmidt. Stochastic dynamic programming with factored representations. Artiﬁcial Intelligence, 121(1):49–107, 2000. ISSN 0004-3702. doi:10.1016/S0004-3702(00)00033-3. URL https://www.sciencedirect.com/science/article/pii/S0004370200000333. [42] Alexander Strehl, Carlos Diuk, and Michael Littman. Efﬁcient Structure Learning in Factored-State MDPs. pages 645–650, 2007. [43] Botao Hao, Tor Lattimore, Csaba Szepesvári, and Mengdi Wang. Online sparse reinforcement learning. In Arindam Banerjee and Kenji Fukumizu, editors, The 24th International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 316–324. PMLR, 2021. URL http://proceedings.mlr.press/v130/hao21a.html. [44] Biwei Huang, Fan Feng, Chaochao Lu, Sara Magliacane, and Kun Zhang. AdaRL: What, Where, and How to Adapt in Transfer Reinforcement Learning. arXiv:2107.02729 [cs, stat], 2021. URL http://arxiv.org/abs/ 2107.02729. [45] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 3207–3214. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16669. [46] Jane X. Wang, Michael King, Nicolas Porcel, Zeb Kurth-Nelson, Tina Zhu, Charlie Deck, Peter Choy, Mary Cassin, Malcolm Reynolds, Francis Song, Gavin Buttimore, David P. Reichert, Neil Rabinowitz, Loic Matthey, Demis Hassabis, Alexander Lerchner, and Matthew Botvinick. Alchemy: A structured task distribution for meta-reinforcement learning. arXiv:2102.02926 [cs], 2021. URL http://arxiv.org/abs/2102.02926. [47] Marlos C. Machado, Marc G. Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents. arXiv:1709.06009 [cs], 2017. URL http://arxiv.org/abs/1709.06009. [48] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efﬁciency of grounded language learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJeXCo0cYX. [49] Carolin Benjamins, Theresa Eimer, Frederik Schubert, André Biedenkapp, Bodo Rosenhahn, Frank Hutter, and Marius Lindauer. CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning. 2021. URL https://openreview.net/forum?id=6D45bYP5MRP. [50] Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, and Anima Anandkumar. SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies. arXiv:2106.09678 [cs], 2021. URL http://arxiv.org/abs/2106.09678. [51] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Martín-Martín. Robosuite: A Modular Simulation Framework and Benchmark for Robot Learning. arXiv:2009.12293 [cs], 2020. URL http://arxiv.org/abs/ 2009.12293. [52] Ossama Ahmed, Frederik Träuble, Anirudh Goyal, Alexander Neitz, Yoshua Bengio, Bernhard Schölkopf, Manuel Wüthrich, and Stefan Bauer. CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. arXiv:2010.04296 [cs, stat], 2020. URL http://arxiv.org/abs/2010.04296. [53] Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L. Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, and Jessica B. Hamrick. Structured agents for physical construction. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 464–474. PMLR, 2019. URL http://proceedings.mlr.press/v97/bapst19a.html. [54] Danijar Hafner. Benchmarking the Spectrum of Agent Capabilities. arXiv:2109.06780 [cs], 2021. URL http://arxiv.org/abs/2109.06780. [55] Valerie Chen, Abhinav Gupta, and Kenneth Marino. Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning. arXiv:2011.00517 [cs], 2021. URL http://arxiv.org/abs/ 2011.00517. [56] Theresa Eimer, André Biedenkapp, Maximilian Reimer, Steven Adriaensen, Frank Hutter, and Marius Lindauer. DACBench: A Benchmark Library for Dynamic Algorithm Conﬁguration. arXiv:2105.08541 [cs], 2021. URL http://arxiv.org/abs/2105.08541. [57] Yujin Tang, Duong Nguyen, and David Ha. Neuroevolution of Self-Interpretable Agents. Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pages 414–424, 2020. doi:10.1145/3377930.3389847. URL http://arxiv.org/abs/2003.08165. [58] OpenAI Gym: The DoomTakeCover-v0 environment. URL https://gym.openai.com/envs/ DoomTakeCover-v0. [59] Nicklas Hansen and Xiaolong Wang. Generalization in Reinforcement Learning by Soft Data Augmentation. arXiv:2011.13389 [cs], 2021. URL http://arxiv.org/abs/2011.13389. [60] Jake Grigsby and Yanjun Qi. Measuring Visual Generalization in Continuous Control from Pixels. arXiv:2010.06740 [cs], 2020. URL http://arxiv.org/abs/2010.06740. [61] Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adrià Puigdomènech Badia, Gavin Buttimore, Charlie Deck, Joel Z. Leibo, and Charles Blundell. Generalization of reinforcement learners with working and episodic memory. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 12448–12457, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ 02ed812220b0705fabb868ddbf17ea20-Abstract.html. [62] Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song. Assessing Generalization in Deep Reinforcement Learning. arXiv:1810.12282 [cs, stat], 2019. URL http://arxiv.org/ abs/1810.12282. [63] Diego Perez-Liebana, Jialin Liu, Ahmed Khalifa, Raluca D. Gaina, Julian Togelius, and Simon M. Lucas. General Video Game AI: A Multi-Track Framework for Evaluating Agents, Games and Content Generation Algorithms. arXiv:1802.10363 [cs], 2019. URL http://arxiv.org/abs/1802.10363. [64] Sirui Xie, Xiaojian Ma, Peiyu Yu, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. HALMA: Humanlike Abstraction Learning Meets Affordance in Rapid Problem Solving. arXiv:2102.11344 [cs], 2021. URL http://arxiv.org/abs/2102.11344. [65] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-D’Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. iGibson 1.0: A Simulation Environment for Interactive Tasks in Large Realistic Scenes. arXiv:2012.02924 [cs], 2021. URL http://arxiv.org/abs/2012.02924. [66] Matthew J. Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive ﬁction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903–7910. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/article/ view/6297. [67] Remi Tachet, Philip Bachman, and Harm van Seijen. Learning Invariances for Policy Generalization. arXiv:1809.02591 [cs, stat], 2020. URL http://arxiv.org/abs/1809.02591. [68] Eliot Xing, Abhinav Gupta, Sam Powers, and Victoria Dean. KitchenShift: Evaluating Zero-Shot Generalization of Imitation-Based Policy Learning Under Domain Shifts. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications, 2021. URL https://openreview.net/forum?id=DdglKo8hBq0. [69] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artiﬁcial intelligence experimentation. In Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016, pages 4246–4247. IJCAI/AAAI Press, 2016. URL http://www.ijcai.org/Abstract/16/643. [70] Dimitrios I. Koutras, Athanasios Ch Kapoutsis, Angelos A. Amanatiadis, and Elias B. Kosmatopoulos. MarsExplorer: Exploration of Unknown Terrains via Deep Reinforcement Learning and Procedurally Generated Environments. arXiv:2107.09996 [cs], 2021. URL http://arxiv.org/abs/2107.09996. [71] Luke Harries, Sebastian Lee, Jaroslaw Rzepecki, Katja Hofmann, and Sam Devlin. MazeExplorer: A Customisable 3D Benchmark for Assessing Generalisation in Reinforcement Learning. In 2019 IEEE Conference on Games (CoG), pages 1–4, 2019. doi:10.1109/CIG.2019.8848048. [72] Raghu Rajan, Jessica Lizeth Borja Diaz, Suresh Guttikonda, Fabio Ferreira, André Biedenkapp, Jan Ole von Hartz, and Frank Hutter. MDP Playground: A Design and Debug Testbed for Reinforcement Learning. arXiv:1909.07750 [cs, stat], 2021. URL http://arxiv.org/abs/1909.07750. [73] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. arXiv preprint arXiv:1910.10897, 2019. [74] Quanyi Li, Zhenghao Peng, Zhenghai Xue, Qihang Zhang, and Bolei Zhou. MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning. 2021. URL https://openreview.net/forum? id=9YyNOthj5Ex. [75] Maxime Chevalier-Boisvert. Minimalistic Gridworld Environment (MiniGrid), 2021. URL https://github. com/maximecb/gym-minigrid. [76] Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang, Eric Hambro, Fabio Petroni, Heinrich Kuttler, Edward Grefenstette, and Tim Rocktäschel. MiniHack the Planet: A Sandbox for OpenEnded Reinforcement Learning Research. In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id= skFwlyefkWJ. [77] Amy Zhang, Yuxin Wu, and Joelle Pineau. Natural Environment Benchmarks for Reinforcement Learning. arXiv:1811.06032 [cs, stat], 2018. URL http://arxiv.org/abs/1811.06032. [78] Heinrich Küttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The nethack learning environment. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 569ff987c643b4bedf504efda8f786c2-Abstract.html. [79] Chenyang Zhao, Olivier Sigaud, Freek Stulp, and Timothy M. Hospedales. Investigating Generalisation in Continuous Deep Reinforcement Learning. arXiv:1902.07015 [cs, stat], 2019. URL http://arxiv.org/abs/ 1902.07015. [80] Shivam Goel, Gyan Tatiya, Matthias Scheutz, and Jivko Sinapov. NovelGridworlds: A benchmark environment for detecting and adapting to novelties in open worlds. 2021. [81] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in vision, control, and planning. In Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 2684–2691. ijcai.org, 2019. doi:10.24963/ijcai.2019/373. URL https://doi.org/10.24963/ijcai.2019/373. [82] Nan Rosemary Ke, Ishita Dasgupta, Silvia Chiappa, Anirudh Goyal, Theophane Weber, Jovana Mitrovic, Felix Hill, Stephanie C. Y. Chan, Michael Curtis Mozer, Danilo Jimenez Rezende, and Pushmeet Kohli. Parametric Generalization for Benchmarking Reinforcement Learning Algorithms. 2021. URL https://openreview. net/forum?id=zx36-ng_9U_. [83] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. RLBench: The Robot Learning Benchmark & Learning Environment. arXiv:1909.12271 [cs], 2019. URL http://arxiv.org/abs/1909. 12271. [84] Yuji Kanagawa and Tomoyuki Kaneko. Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning. arXiv:1904.08129 [cs, stat], 2019. URL http://arxiv.org/abs/1904.08129. [85] Victor Zhong, Tim Rocktäschel, and Edward Grefenstette. RTFM: Generalising to Novel Environment Dynamics via Reading. arXiv:1910.08210 [cs], 2021. URL http://arxiv.org/abs/1910.08210. [86] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learning. arXiv:2003.11881 [cs], 2021. URL http://arxiv.org/abs/2003.11881. [87] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. TextWorld: A Learning Environment for Text-based Games. arXiv:1806.11532 [cs, stat], 2019. URL http://arxiv.org/ abs/1806.11532. [88] Emma Tosch, Kaleigh Clary, John Foley, and David Jensen. Toybox: A Suite of Environments for Experimental Evaluation of Deep Reinforcement Learning. arXiv:1905.02825 [cs, stat], 2019. URL http://arxiv.org/ abs/1905.02825. [89] Sam Wenke, Dan Saunders, Mike Qiu, and Jim Fleming. Reasoning and Generalization in RL: A Tool Use Perspective. arXiv:1907.02050 [cs], 2019. URL http://arxiv.org/abs/1907.02050. [90] Minqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip H.S. Torr, Shimon Whiteson, and Tim Rocktäschel. WordCraft: An environment for benchmarking commonsense agents. In Workshop on Language in Reinforcement Learning (LaRel), 2020. URL https://github.com/minqi/wordcraft. [91] Cheng Xue, Vimukthini Pinto, Chathura Gamage, Ekaterina Nikonova, Peng Zhang, and Jochen Renz. Phy-Q: A Benchmark for Physical Reasoning. arXiv:2108.13696 [cs], 2021. URL http://arxiv.org/abs/2108. 13696. [92] Sebastian Risi and Julian Togelius. Increasing generality in machine learning through procedural content generation. Nat Mach Intell, 2(8):428–436, 2020. ISSN 2522-5839. doi:10.1038/s42256-020-0208-z. URL https://www.nature.com/articles/s42256-020-0208-z. [93] Alex Irpan and Xingyou Song. The Principle of Unchanged Optimality in Reinforcement Learning Generalization. arXiv:1906.00336 [cs, stat], 2019. URL http://arxiv.org/abs/1906.00336. [94] Eliot Xing, Abhinav Gupta, Sam Powers, and Victoria Dean. Evaluating Generalization of Policy Learning Under Domain Shifts. 2021. URL https://openreview.net/forum?id=11yRzZJsMwA. [95] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. DeepMind Control Suite. arXiv:1801.00690 [cs], 2018. URL http://arxiv.org/abs/1801.00690. [96] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob N. Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, and Tim Rocktäschel. A survey of reinforcement learning informed by natural language. In Sarit Kraus, editor, Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 6309–6317. ijcai.org, 2019. doi:10.24963/ijcai.2019/880. URL https://doi.org/10.24963/ijcai.2019/880. [97] Vinícius Flores Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David P. Reichert, Timothy P. Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter W. Battaglia. Deep reinforcement learning with relational inductive biases. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=HkxaFoC9KQ. [98] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Alenyà, Pieter Abbeel, Alexei A. Efros, Lerrel Pinto, and Xiaolong Wang. Self-Supervised Policy Adaptation during Deployment. arXiv:2007.04309 [cs, stat], 2021. URL http://arxiv.org/abs/2007.04309. [99] Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep RL via meta-learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=Hkl9JlBYvr. [100] Connor Shorten and Taghi M. Khoshgoftaar. A survey on Image Data Augmentation for Deep Learning. J Big Data, 6(1):60, 2019. ISSN 2196-1115. doi:10.1186/s40537-019-0197-0. URL https://doi.org/10.1186/ s40537-019-0197-0. [101] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. arXiv:1703.06907 [cs], 2017. URL http://arxiv.org/abs/1703.06907. [102] Fereshteh Sadeghi and Sergey Levine. CAD2RL: Real Single-Image Flight without a Single Real Image. arXiv:1611.04201 [cs], 2017. URL http://arxiv.org/abs/1611.04201. [103] Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Sim-to-Real Transfer of Robotic Control with Dynamics Randomization. 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 3803–3810, 2018. doi:10.1109/ICRA.2018.8460528. URL http://arxiv.org/abs/1710. 06537. [104] Roberta Raileanu, Max Goldstein, Denis Yarats, Ilya Kostrikov, and Rob Fergus. Automatic Data Augmentation for Generalization in Deep Reinforcement Learning. arXiv:2006.12862 [cs], 2021. URL http://arxiv.org/ abs/2006.12862. [105] Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels. arXiv:2004.13649 [cs, eess, stat], 2021. URL http://arxiv.org/abs/ 2004.13649. [106] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs], 2017. URL http://arxiv.org/abs/1707.06347. [108] Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview. net/forum?id=r1Ddp1-Rb. [109] Hanping Zhang and Yuhong Guo. Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation. arXiv:2106.15587 [cs], 2021. URL http://arxiv.org/abs/2106.15587. [110] Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee. Network randomization: A simple technique for generalization in deep reinforcement learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=HJgcvJBFvB. [111] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain Generalization with MixStyle. arXiv:2104.02008 [cs], 2021. URL http://arxiv.org/abs/2104.02008. [112] Karl Cobbe, Oleg Klimov, Christopher Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 1282–1289. PMLR, 2019. URL http: //proceedings.mlr.press/v97/cobbe19a.html. [113] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efﬁcient robotic grasping via randomized-to-canonical adaptation networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 12627–12637. Computer Vision Foundation / IEEE, 2019. doi:10.1109/CVPR.2019.01291. URL http://openaccess.thecvf. com/content_CVPR_2019/html/James_Sim-To-Real_via_Sim-To-Sim_Data-Efficient_Robotic_ Grasping_via_Randomized-To-Canonical_Adaptation_Networks_CVPR_2019_paper.html. [114] Byungchan Ko and Jungseul Ok. Time Matters in Using Data Augmentation for Vision-based Deep Reinforcement Learning. arXiv:2102.08581 [cs], 2021. URL http://arxiv.org/abs/2102.08581. [115] Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation. arXiv:2107.00644 [cs], 2021. URL http://arxiv.org/abs/2107. 00644. [116] Yangang Ren, Jingliang Duan, Shengbo Eben Li, Yang Guan, and Qi Sun. Improving Generalization of Reinforcement Learning with Minimax Distributional Soft Actor-Critic. arXiv:2002.05502 [cs, stat], 2020. URL http://arxiv.org/abs/2002.05502. [117] Chenyang Zhao and Timothy Hospedales. Robust Domain Randomised Reinforcement Learning through Peer-to-Peer Distillation. arXiv:2012.04839 [cs], 2020. URL http://arxiv.org/abs/2012.04839. [118] Zac Wellmer and James T. Kwok. Dropout’s Dream Land: Generalization from Learned Simulators to Reality. In Nuria Oliver, Fernando Pérez-Cruz, Stefan Kramer, Jesse Read, and Jose A. Lozano, editors, Machine Learning and Knowledge Discovery in Databases. Research Track, Lecture Notes in Computer Science, pages 255–270, Cham, 2021. Springer International Publishing. ISBN 978-3-030-86486-6. doi:10.1007/978-3-030-86486-6_16. [119] Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions. arXiv:1901.01753 [cs], 2019. URL http://arxiv.org/abs/1901.01753. [120] Rui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth O. Stanley. Enhanced POET: open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9940–9951. PMLR, 2020. URL http://proceedings.mlr.press/v119/wang20l.html. [121] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/ 2020/hash/985e9a46e10005356bbaf194249f6856-Abstract.html. [122] Minqi Jiang, Michael Dennis, Jack Parker-Holder, Jakob Foerster, Edward Grefenstette, and Tim Rocktäschel. Replay-Guided Adversarial Environment Design. arXiv:2110.02439 [cs], 2021. URL http://arxiv.org/ abs/2110.02439. [123] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized Level Replay. arXiv:2010.03934 [cs], 2021. URL http://arxiv.org/abs/2010.03934. [124] Rémy Portelas, Cédric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Automatic curriculum learning for deep RL: A short survey. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2020, pages 4819–4825. ijcai.org, 2020. doi:10.24963/ijcai.2020/671. URL https://doi.org/10.24963/ijcai.2020/671. [125] Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo, Mingtian Zhang, and Jun Wang. Wasserstein Robust Reinforcement Learning. arXiv:1907.13196 [cs, stat], 2019. URL http://arxiv.org/abs/1907.13196. [126] Daniel J. Mankowitz, Nir Levine, Rae Jeong, Abbas Abdolmaleki, Jost Tobias Springenberg, Yuanyuan Shi, Jackie Kay, Todd Hester, Timothy A. Mann, and Martin A. Riedmiller. Robust reinforcement learning for continuous control with model misspeciﬁcation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview. net/forum?id=HJgC60EtwB. [127] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Rémi Munos, Nicolas Heess, and Martin A. Riedmiller. Maximum a posteriori policy optimisation. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=S1ANxQW0b. [128] Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 2817–2826. PMLR, 2017. URL http://proceedings.mlr.press/v70/pinto17a.html. [129] Roberta Raileanu and Rob Fergus. Decoupling Value and Policy for Generalization in Reinforcement Learning. arXiv:2102.10330 [cs], 2021. URL http://arxiv.org/abs/2102.10330. [131] Marin Vlastelica, Michal Rolínek, and Georg Martius. Neuro-algorithmic Policies enable Fast Combinatorial Generalization. arXiv:2102.07456 [cs], 2021. URL http://arxiv.org/abs/2102.07456. [132] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan Pascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. Relational Deep Reinforcement Learning. arXiv:1806.01830 [cs, stat], 2018. URL http://arxiv.org/abs/1806.01830. [133] Ken Kansky, Tom Silver, David A. Mély, Mohamed Eldawy, Miguel Lázaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, D. Scott Phoenix, and Dileep George. Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1809–1818. PMLR, 2017. URL http://proceedings.mlr.press/v70/kansky17a.html. [134] Xudong Wang, Long Lian, and Stella X. Yu. Unsupervised Visual Attention and Invariance for Reinforcement Learning. arXiv:2104.02921 [cs], 2021. URL http://arxiv.org/abs/2104.02921. [135] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja skowski. ViZDoom: A Doombased AI research platform for visual reinforcement learning. In IEEE Conference on Computational Intelligence and Games, pages 341–348, Santorini, Greece, 2016. IEEE. URL http://arxiv.org/abs/1605.02097. [136] Yujin Tang and David Ha. The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning. arXiv:2109.02869 [cs], 2021. URL http://arxiv.org/abs/2109.02869. [137] Vincent François-Lavet, Yoshua Bengio, Doina Precup, and Joelle Pineau. Combined reinforcement learning via abstract representations. In The Thirty-Third AAAI Conference on Artiﬁcial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 3582–3589. AAAI Press, 2019. doi:10.1609/aaai.v33i01.33013582. URL https://doi.org/10.1609/aaai.v33i01.33013582. [138] Felix Hill, Sona Mokra, Nathaniel Wong, and Tim Harley. Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text. arXiv:2005.09382 [cs], 2020. URL http://arxiv.org/abs/ 2005.09382. [139] Corey Lynch and Pierre Sermanet. Language Conditioned Imitation Learning over Unstructured Data. arXiv:2005.07648 [cs], 2021. URL http://arxiv.org/abs/2005.07648. [140] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi:10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423. [141] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-hsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual universal sentence encoder for semantic retrieval. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 87–94, Online, 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.acl-demos.12. URL https://www.aclweb.org/anthology/2020.acl-demos.12. [142] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261 [cs, stat], 2018. URL http://arxiv.org/abs/1806.01261. [143] Terrance DeVries and Graham W. Taylor. Improved Regularization of Convolutional Neural Networks with Cutout. arXiv:1708.04552 [cs], 2017. URL http://arxiv.org/abs/1708.04552. [144] Suzan Ece Ada, Emre Ugur, and H. Levent Akin. Generalization in Transfer Learning. arXiv:1909.01331 [cs, stat], 2021. URL http://arxiv.org/abs/1909.01331. [145] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pages 1–5, 2015. doi:10.1109/ITW.2015.7133169. [146] Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and information bottleneck. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13956–13968, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/ e2ccf95a7f2e1878fcafc8376649b6e8-Abstract.html. [147] Xingyu Lu, Kimin Lee, Pieter Abbeel, and Stas Tiomkin. Dynamics Generalization via Information Bottleneck in Deep Reinforcement Learning. arXiv:2008.00614 [cs, stat], 2020. URL http://arxiv.org/abs/2008. 00614. [148] Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Robust Predictable Control. arXiv:2109.03214 [cs], 2021. URL http://arxiv.org/abs/2109.03214. [149] Jerry Zikun Chen. Reinforcement Learning Generalization with Surprise Minimization. arXiv:2004.12399 [cs], 2020. URL http://arxiv.org/abs/2004.12399. [150] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning Invariant Representations for Reinforcement Learning without Reconstruction. arXiv:2006.10742 [cs, stat], 2021. URL http://arxiv.org/abs/2006.10742. [151] Mete Kemertas and Tristan Aumentado-Armstrong. Towards Robust Bisimulation Metric Learning. arXiv:2110.14096 [cs], 2021. URL http://arxiv.org/abs/2110.14096. [152] Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G. Bellemare. Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning. arXiv:2101.05265 [cs, stat], 2021. URL http://arxiv.org/abs/2101.05265. [153] Anoopkumar Sonar, Vincent Pacelli, and Anirudha Majumdar. Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning. arXiv:2006.01096 [cs, stat], 2020. URL http://arxiv.org/abs/ 2006.01096. [154] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization. arXiv:1907.02893 [cs, stat], 2020. URL http://arxiv.org/abs/1907.02893. [155] Martín Bertrán, Natalia Martínez, Mariano Phielipp, and Guillermo Sapiro. Instance-based generalization in reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 82674fc29bc0d9895cee346548c2cb5c-Abstract.html. [156] Guan Ting Liu, Pu-Jen Cheng, and GuanYu Lin. Cross-State Self-Constraint for Feature Generalization in Deep Reinforcement Learning. 2020. URL https://openreview.net/forum?id=JiNvAGORcMW. [157] Bogdan Mazoure, Ahmed M. Ahmed, Patrick MacAlpine, R. Devon Hjelm, and Andrey Kolobov. CrossTrajectory Representation Learning for Zero-Shot Generalization in RL. arXiv:2106.02193 [cs], 2021. URL http://arxiv.org/abs/2106.02193. [158] Bonnie Li, Vincent François-Lavet, Thang Doan, and Joelle Pineau. Domain Adversarial Reinforcement Learning. arXiv:2102.07097 [cs], 2021. URL http://arxiv.org/abs/2102.07097. [159] Jiameng Fan and Wenchao Li. DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck. arXiv:2102.13268 [cs], 2021. URL http://arxiv.org/abs/2102.13268. [160] Wenhao Yu, Jie Tan, C. Karen Liu, and Greg Turk. Preparing for the Unknown: Learning a Universal Policy with Online System Identiﬁcation. arXiv:1702.02453 [cs], 2017. URL http://arxiv.org/abs/1702.02453. [161] Lin Yen-Chen, Maria Bauza, and Phillip Isola. Experience-Embedded Visual Foresight. arXiv:1911.05071 [cs], 2019. URL http://arxiv.org/abs/1911.05071. [162] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. RMA: Rapid Motor Adaptation for Legged Robots. arXiv:2107.04034 [cs], 2021. URL http://arxiv.org/abs/2107.04034. [163] Philip J. Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts. Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Ofﬂine Environment. arXiv:2104.05632 [cs], 2021. URL http://arxiv.org/abs/2104.05632. [164] Younggyo Seo, Kimin Lee, Ignasi Clavera, Thanard Kurutach, Jinwoo Shin, and Pieter Abbeel. Trajectory-wise Multiple Choice Learning for Dynamics Generalization in Reinforcement Learning. arXiv:2010.13303 [cs], 2020. URL http://arxiv.org/abs/2010.13303. [165] Anusha Nagabandi, Chelsea Finn, and Sergey Levine. Deep online learning via meta-learning: Continual adaptation for model-based RL. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= HyxAfnA5tm. [166] Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, and Davide Maltoni. Continual Reinforcement Learning in 3D Non-stationary Environments. arXiv:1905.10112 [cs, stat], 2020. URL http://arxiv.org/abs/1905. 10112. [167] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=HyztsoC5Y7. [168] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 1126–1135. PMLR, 2017. URL http://proceedings.mlr.press/v70/ finn17a.html. [169] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl $ 2$: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. [170] Jane X. Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763 [cs, stat], 2017. URL http://arxiv.org/abs/1611.05763. [171] Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Ofﬂine Meta Learning of Exploration. arXiv:2008.02598 [cs, stat], 2021. URL http://arxiv.org/abs/2008.02598. [172] Luisa Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon Whiteson. Exploration in Approximate Hyper-State Space for Meta Reinforcement Learning. arXiv:2010.01062 [cs, stat], 2021. URL http://arxiv.org/abs/2010.01062. [173] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id= B1DmUzWAW. [174] Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, and Shimon Whiteson. Transient Non-Stationarity and Generalisation in Deep Reinforcement Learning. arXiv:2006.05826 [cs, stat], 2021. URL http://arxiv.org/abs/2006.05826. [175] Karl Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic Policy Gradient. arXiv:2009.04416 [cs, stat], 2020. URL http://arxiv.org/abs/2009.04416. [176] Jaskirat Singh and Liang Zheng. Sparse Attention Guided Dynamic Value Estimation for Single-Task Multi-Scene Reinforcement Learning. arXiv:2102.07266 [cs, stat], 2021. URL http://arxiv.org/abs/2102.07266. [177] Ankesh Anand, Jacob Walker, Yazhe Li, Eszter Vértes, Julian Schrittwieser, Sherjil Ozair, Théophane Weber, and Jessica B. Hamrick. Procedural Generalization by Planning with Self-Supervised World Models. arXiv:2111.01587 [cs], 2021. URL http://arxiv.org/abs/2111.01587. [178] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and David Silver. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020. ISSN 1476-4687. doi:10.1038/s41586-020-03051-4. URL https://www.nature.com/articles/s41586-020-03051-4. [179] Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and Ofﬂine Reinforcement Learning by Planning with a Learned Model. arXiv:2104.06294 [cs], 2021. URL http://arxiv.org/abs/2104.06294. [180] Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, and Abhinav Gupta. CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents. arXiv:2110.10067 [cs], 2021. URL http://arxiv.org/abs/2110.10067. [181] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= SJx63jRqFm. [182] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-Task Reinforcement Learning with Soft Modularization. arXiv:2003.13661 [cs, stat], 2020. URL http://arxiv.org/abs/2003.13661. [183] Ayush Jain, Andrew Szot, and Joseph J. Lim. Generalization to new actions in reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 4661–4672. PMLR, 2020. URL http://proceedings.mlr.press/v119/jain20b.html. [184] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems. arXiv:2005.01643 [cs, stat], 2020. URL http://arxiv.org/abs/2005. 01643. [185] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for ofﬂine reinforcement learning. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings. neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html. [186] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for Deep Data-Driven Reinforcement Learning. arXiv:2004.07219 [cs, stat], 2021. URL http://arxiv.org/abs/2004.07219. [187] Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gomez Colmenarejo, Konrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Oﬁr Nachum, George Tucker, Nicolas Heess, and Nando de Freitas. RL Unplugged: A Suite of Benchmarks for Ofﬂine Reinforcement Learning. arXiv:2006.13888 [cs, stat], 2021. URL http: //arxiv.org/abs/2006.13888. [188] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. AWAC: Accelerating Online Reinforcement Learning with Ofﬂine Datasets. arXiv:2006.09359 [cs, stat], 2021. URL http://arxiv.org/abs/2006. 09359. [189] Saurabh Arora and Prashant Doshi. A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress. arXiv:1806.06877 [cs, stat], 2020. URL http://arxiv.org/abs/1806.06877. [190] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In Pat Langley, editor, Proceedings of the Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 - July 2, 2000, pages 663–670. Morgan Kaufmann, 2000. [191] Annie S. Chen, Suraj Nair, and Chelsea Finn. Learning Generalizable Robotic Reward Functions from "In-TheWild" Human Videos. arXiv:2103.16817 [cs], 2021. URL http://arxiv.org/abs/2103.16817. [192] Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea Finn. Reinforcement Learning with Videos: Combining Ofﬂine Observations with Interaction. arXiv:2011.06507 [cs], 2020. URL http: //arxiv.org/abs/2011.06507. [193] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. Time-Contrastive Networks: Self-Supervised Learning from Video. arXiv:1704.06888 [cs], 2018. URL http://arxiv.org/abs/1704.06888. [194] Alessandro Sestini, Alexander Kuhnle, and Andrew D. Bagdanov. Demonstration-efﬁcient Inverse Reinforcement Learning in Procedurally Generated Environments. arXiv:2012.02527 [cs], 2020. URL http://arxiv.org/ abs/2012.02527. [195] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvári, Satinder Singh, Benjamin Van Roy, Richard S. Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=rygf-kSYwH. [196] Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal difference learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 767–777. PMLR, 2020. URL http://proceedings.mlr.press/v119/bengio20a.html. [197] Felix Hill, Andrew K. Lampinen, Rosalia Schneider, Stephen Clark, Matthew Botvinick, James L. McClelland, and Adam Santoro. Environmental drivers of systematicity and generalization in a situated agent. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SklGryBtwr. [198] Ian Osband and Benjamin Van Roy. Near-optimal reinforcement learning in factored mdps. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 604–612, 2014. URL https://proceedings.neurips. cc/paper/2014/hash/0deb1c54814305ca9ad266f53bc82511-Abstract.html. [199] Bernhard Schölkopf. Causality for Machine Learning. arXiv:1911.10500 [cs, stat], 2019. URL http: //arxiv.org/abs/1911.10500. [200] Daniel S Weld. Solving Relational MDPs with First-Order Machine Learning. page 8. [201] Carlos Diuk, Andre Cohen, and Michael L. Littman. An object-oriented representation for efﬁcient reinforcement learning. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, volume 307 of ACM International Conference Proceeding Series, pages 240–247. ACM, 2008. doi:10.1145/1390156.1390187. URL https://doi.org/10.1145/1390156.1390187. <title>A Other Structural Assumptions on MDPs</title> Other forms of structured MDPs have been deﬁned beyond the contextual MDP [ 14 ] and leveraged to develop algorithms that exploit those structural assumptions. One type that holds promise for generalisation is the factored MDP. A factored MDP assumes a state space described by a set of discrete variables, denoted S := {S , S , . . . , S 41 42 ]. We follow the notation and deﬁnitions used by [198]. The transition function T has the following property: where P (s |s, a) is the probability distribution for each factor S conditioned on the previous state and action. where the probability of each factor only depends on its parent factors PA(s from the previous time step. Ideally, these parents are only a subset of all factors so this representation results in a reduction in size from the original MDP. Further, this enforces that there are no synchronous edges between factors in the same time step. Critically, the rewards can also be factored in the following way: One can think of this factored MDP framework as an additional assumption for a single context-MDP, or the combination of one or more of these factors as the context, with the space of all possible combinations of factors being the context set. In the latter case, this formulation explicitly encodes how generalisation can be achieved to new contexts via systematicity (Section 3.1, [ 34 ]): the policy will be trained on contexts taking some values within the set of possible factors, and then be tested on unseen combinations of seen factors. A more restricted form of structured MDP is the relational MDP 200 ]. A relational MDP is described by tuple hC, F, A, D, T, Ri is the set of object types, is the set of ﬂuent schemata that are arguments that modify each object type. is the set of action schemata that acts on objects, is the set of domain objects, each associated with a single type from , and ﬁnally is the transition function and the reward function. An additional assumption is that objects that are not acted upon do not change in a transition. Note that the relational MDP can be expanded into a factored MDP that does not assume the additional structure of the form of object types with invariant relations. While this form of MDP is a Planning Domain Deﬁnition Language (PDDL) and therefore lends itself well to planning algorithms, it is overly complex for learning algorithms. Object-oriented MDPs 201 ] are a simpler form of relational MDPs that are less constrained, and therefore hold more promise for learning methods. Objects, ﬂuents, and actions are deﬁned in the same away as in relational MDPs, but all transition dynamics are determined by a set of Boolean transition terms which consist of a set of pre-deﬁned relation terms between objects and object attributes. In spite of this simpliﬁcation, it is still signiﬁcantly constrained compared to CMDPs, and can be difﬁcult to use when describing complex systems. A ﬁnal example of a structured MDP is the Block MDP . Block MDPs [ 39 ] are described by a tuple hS, A, X , p, q, Ri with a ﬁnite, unobservable state space and possibly inﬁnite, but observable space denotes the latent transition distribution, is the (possibly stochastic) emission function and the reward function. This structured MDP is useful in rich observation environments where the given observation space is large, but a much smaller state space can be found that yields an equivalent MDP. This allows for improved exploration and sample complexity bounds that rely on the size of that latent state space rather than the given observation space.