<title>Will You Ever Become Popular? Learning to Predict Virality of Dance Clips</title> Music-centric short videos, such as dance challenge, short lip-sync and spoofs, are booming on social media. Take TikTok (a.k.a. Douyin in China) as an example, it is recorded to have 689 million monthly active users worldwide by early 2021 [ 36 ]. For the reason of limited network bandwidth and fragmented reading behavior, the duration of video is usually set to less than one minute, <title>arXiv:2111.03819v1  [cs.CV]  6 Nov 2021</title> typically 3-15 seconds. Since such short videos are purposely created and highly topic-driven, it results in a huge number of well-formated, richly annotated videos and creates very specic demands of video content analysis. Dance challenge is one of the hottest topics of the short video community, in which people perform dance with the same background music and compete with each other for higher popularity. Once a challenge becomes popular, thousands of challengers will upload their performance. However, most of the uploads are parodies and only a fraction of them can go viral. Therefore, automatically predicting the potential of virality for a new upload becomes an important issue. For short video platforms, the resulting implications can be used for content recommendation, trac pool management, video retrieval and advertising. For end users, prompts and feedback can be acquired to raise their popularity. Over the past few years, plenty of works have been done to predict the popularity of online videos, in which samples from YouTube [ 45 52 ] and Facebook [ ] with miscellaneous content have been investigated. Researchers utilize multi-modal data including the early evolution pattern and social media, visual, acoustic, textual features [ 10 45 52 ] to make popularity prediction. Meanwhile, in the domain of human-centric performance assessment, models are developed to predict performance scores of Olympic events [ 39 42 43 46 60 ], surgical skill [ 21 67 ] and patient rehabilitation [ 41 50 ]. These works usually take single modality data (e.g. human skeleton, video frames) as the assessment basis. In this paper, we focus on predicting virality of dance clips with visual cues. Unlike existing video popularity prediction works that deal with universal videos with miscellaneous content, we only pay attention to single-person dance clips within exclusive dance challenges. Our virality score is based on several popularity indicators and reects the public appreciation. As illustrated in Fig. 1, the virality of a dance clip can be inuenced by various factors, such as body movements, facial expressions, backgrounds, etc. To comprehensively predict the virality, we propose a multimodal prediction framework that consists of a skeleton-based stream and three appearance-based streams, accounting for body movements and appearance cues of multiple aspects, respectively. The overview of our framework is presented in Fig. 2. For skeleton-based virality prediction, we introduce a pyramidal skeleton graph convolutional network (PSGCN), which hierarchically exploits human skeleton dynamics with spatial-temporal graph convolutions. In order to capture robust virality information across dierent dance challenges, we design a graph down-sampling module to gradually rene the spatio-temporal graphs, forming a pyramidal network architecture. For appearance-based virality prediction, we employ state-ofthe-art deep architectures [ 57 64 66 ] to extract multi-modal features with holistic, facial and scenic information. An eective and ecient relational temporal convolutional network (RTCN) is proposed to extract appearance dynamics from frame-level features. By inserting the devised relational pooling blocks between temporal convolutions, we explore non-local temporal relations via multi-head self-attentions. To aggregate predictions from dierent modalities, we adopt a late fusion [ 24 57 ] scheme facilitated by a data-dependent modality attention mechanism, which employs a modality attention module to adaptively learn attention weights for dierent streams according to the input video. Finally, to validate our approach, we present a large-scale viral dance video (VDV) dataset, which contains 4,292 dance clips sampled from eight representative challenges with a total length of 17.1 hours. To facilitate the development of multi-modal solutions, we also release the data in skeletal, facial and scenic modalities. Extensive experiments on VDV dataset demonstrate the superiority of the proposed approach compared with state-of-the-art methods. Moreover, we show that applications for both short video platforms and end users, such as multi-dimensional video recommendation and action feedback, can be derived from our model. Extension experiments on UNLV-Sports [ 43 ] dataset indicate that our multi-modal framework is benecial to other human-centric performance assessment tasks as well. In summary, our main contributions are three-fold: We rst study virality prediction from dance challenges using visual cues, which has great commercial value. To facilitate the research, we release VDV dataset, a large-scale multi-modal dance virality prediction benchmark. A multi-modal framework modeling both body movements and appearance dynamics is developed. For skeleton-based prediction, we devise a pyramidal skeleton graph convolutional network (PSGCN). For appearance-based prediction, relational temporal convolutional networks (RTCN) are proposed. An attentive modality fusion approach is introduced to aggregate predictions from multiple streams. Our model achieves the state-of-the-art performance on VDV dataset. Real-world short video applications like multi-dimensional video recommendation and action feedback can be derived from our model. Extension experiments indicate that our multi-modal framework has potential in other assessment tasks like sports rating. The rest of this paper is structured as follows. Section 2 reviews related works and their relations with this paper. Section 3 introduces the proposed VDV dataset. Section 4 presents details of our multi-modal prediction framework. Section 5 shows experimental evaluations of our approach and the competitor methods. Finally, Section 6 concludes the paper. Online video popularity prediction. Researchers from multimedia and data mining communities have done a considerable number of works to predict the popularity of online videos. Most of these works intend to predict the popularity of videos from online video or social media websites, such as YouTube [ 45 52 59 ], Vine [ 10 37 48 61 ], Facebook [ ], Kuaishou [ 65 ], etc. Multi-modal data including popularity evolution pattern and social media, visual, acoustic, textual, geography features are exploited for prediction. In [ 45 ], view counts of YouTube videos at early stages are used to predict those in the future. Chen et al. [ 10 ] propose to predict the popularity of microvideos from Vine using a transductive model, in which social, visual, acoustic and textual features are taken as the input. Following this work, methods like variational encoder-decoder [ 61 ] and feature-discrimination transductive model [ 48 ] are explored to better capture popularity-related information from the same features. Visual cues are combined with early evolution patterns in [ 52 to train an support vector regressor (SVR) for popularity prediction. In [ ], a spatial-attentive network is proposed to explore the relationship between visual appearance and video virality. A graph convolution based model is introduced in [ 65 ] to predict video popularity from geography information of users. In comparison, our work focuses on single-person dance clips in short video apps and aims to predict the virality with multi-modal visual cues. As later introduced in Section 3, the videos in VDV dataset are carefully selected to ensure that the virality mainly depends on the video content. Unlike most of the video popularity prediction works that utilize non-visual data (e.g. early evolution pattern, social media data, etc.) for prediction, we only adopt visual features since we concentrate on the virality of dance performance itself, rather than other irrelevant information. Human-centric performance assessment. Human-centric performance assessment, which aims to predict the performance score of human individuals in certain kinds of activities, is attracting increasing attention due to its broad applications in multiple areas. Some researchers focus on assessing the performance of athletes in Olympic events, such as diving, gure skating and gymnastic vault [ 39 42 43 46 49 60 ], some target on skill determination in daily activities [ 15 16 ], others address medical issues like surgical skill [ 20 21 67 ] and patient rehabilitation [ 41 50 ]. In comparison, little attention is paid to entertainment videos like viral dance clips. In terms of data modality, most works only utilize single modality data for assessment, such as human skeleton [ 41 46 55 ], video frames [ 40 42 43 60 ]. Pirsiavash et al. [ 46 ] use the discrete cosine transform (DCT) to represent human skeleton features and train SVR models to predict performance scores of Olympic events. Later works also explore other machine learning methods including the hidden Markov model [ 50 ] and boosted decision trees [ 41 ] to directly predict the performance score from human skeleton features. Due to the diculty in estimating atypical body postures [ 43 ], recent works tend to utilize visual features directly extracted from video frames instead. In [ 40 43 ], C3D models [ 51 ] are used to extract spatio-temporal features from input videos. Models like the long short-term memory (LSTM) and SVR are used to regress the performance score. A segment-based pseudo-3D residual network (P3D) [ 47 ] is proposed in [ 60 ] to extract features at segment-level for better performance. Later works like [ 20 49 ] employ I3D [ ] model for segmental feature extraction. In [ 49 ], an uncertainty-aware score distribution learning (USDL) approach is proposed to handle the score ambiguity caused by multiple judges in sports events. The performance scores in Olympic events [12] and surgical skills [35] are given by the professionals. Actions of individual performers are usually carried out rigorously in similar environments. Only body movements are of major concern in these tasks. In comparison, viral dance is a more improvisational kind of activity. Although the music is the same in each challenge, the expression can be very dissimilar among dancers. Additionally, every dancer has unique appearance of stature, clothing, backgrounds, etc. It hence requires analysis from multiple features for comprehensive virality prediction. In this work, we introduce a multi-modal framework to capture virality information from both skeletal and appearance modalities. We demonstrate that the proposed PSGCN and RTCN models are more eective in dance virality prediction compared with state-of-the-art 3D-CNN based methods. Furthermore, the scale of our VDV dataset is much larger than existing human-centric performance assessment datasets [ 16 20 21 40 43 46 ], which makes it more challenging in terms of model robustness and generalization ability. Our viral dance video (VDV) dataset is collected from the most popular short video app, TikTok. To begin with, we select eight representative viral dance challenges with exclusive background music. As presented in Fig. 3, these challenges have characteristic body movements and diverse dance styles. More importantly, as shown in Table 1, every challenge has millions of view counts, which makes them among the most popular challenges at the time. For a fair virality comparison solely by the video content, we lter the uploaders with extremely high popularity (i.e. with over 1,000 followers). Although video clips on TikTok are limited to 3-15 seconds, we set the minimum video length to 5 seconds to avoid uninformative videos. Over 24,000 videos are collected initially. The videos are then manually screened to meet the requirement of virality prediction. Since we dene dance virality prediction as a multi-modal human-centric performance assessment problem, the screening criteria are formalized as follows: • Only one single dancer is allowed to appear in the video. The dancer is performing the same dance of the hashtagged TikTok challenge with the same background music. At least the joints above the waist of the dancer, i.e. head top, upper neck, shoulders, elbows, hips as dened in the MPII [1] human pose dataset, are visible in the video. • The camera motion shall not disrupt the continuity of dance performance. While gathering videos, associated view counts are collected as well. We adopt a similar computation method of the ground-truth virality score as in [ 10 ]. Our virality score is based on three popularity-related indicators, namely, like counts, comment counts and repost counts. We denote these indicators as and . It is observed that the popularity of a short video accumulates rapidly on a daily basis in a period of time (usually about one week) [ 30 ]. So we normalize the virality score with clipped existing time (in days), which is denoted as . A log transform is nally applied as in [ ] to handle the large variation and skewness of the original score distribution. We dene the raw virality score 𝑠 as: where 𝑐𝑙𝑖𝑝 denotes the clipping operation with min-max thresholds. The additional increment in the numerator prevents computing logarithm of zero. Since dierent dance challenge has inherent dierence in popularity, for an accurate virality comparison across all challenges, we further normalize 𝑠 with min-max normalization to get the nal virality score 𝑠, which is formulated as: −𝑚𝑖𝑛(𝑠 𝑠 = . (2) 𝑚𝑎𝑥 (𝑠 ) −𝑚𝑖𝑛(𝑠 We denote the minimum and maximum raw score values of each challenge as 𝑚𝑖𝑛(𝑠 and 𝑚𝑎𝑥 (𝑠 This min-max normalization makes the nal virality score a value between 0 and 1, which is compatible for both intra- and inter-challenge virality comparison. We present general statistics of VDV dataset in Table 1. VDV contains 4,292 viral dance clips of eight challenges ( ∼𝐶 ). The total length of video data is 17 1 hours with over 1 6 million video frames. Moreover, as later introduced in Section 4.3, we utilize state-of-the-art deep learning models [ 17 57 66 ] to extract multi-modal (i.e. skeletal, holistic, facial and scenic) features from RGB videos and release them as a part of our dataset. The numbers of extracted frames and skeletal, facial, scenic features in each challenge are listed in Table 1. With abundant ready-to-use features in each modality, multi-modal solutions can be adequately evaluated on our VDV benchmark. Table 2 shows the comparison of VDV dataset and other publicly available human-centric performance assessment datasets. These datasets are from various domains including sports [ 20 40 43 46 ], surgical [ 21 ] and daily activities [ 16 ]. However, online short videos with great commercial value like viral dance challenges have not been explored before VDV dataset. Datasets like JIGSAWS [ 21 and BEST [ 16 ] are hand-centric rather than focusing on the entire human body, which limits their applicability in common scenarios. In terms of data modality, JIGSAWS dataset provides kinematic and video data captured by a surgical system, while other datasets only provide video data except VDV. In terms of dataset scale, to our best knowledge, VDV is currently the largest human-centric performance assessment benchmark. To explore human body movements, we directly utilize skeleton features, i.e. the 2D coordinates of body joints estimated by the state-of-the-art pose estimator AlphaPose [ 17 ]. As human skeleton sequence is naturally non-Euclidean, we employ graph convolutions [ 26 31 38 62 ] and propose a pyramidal skeleton graph convolutional network (PSGCN) for skeleton-based virality prediction. Spatial-temporal graph convolution. Yan et al. [ 62 ] rst propose to use a spatial temporal graph convolutional network (ST-GCN) to model skeleton data for action recognition problem. As shown in Fig. 4 (a), we adopt a similar graph structure as their work, where joints in the same frame are spatially connected with spatial edges following the denition in MPII dataset [ ]. For the temporal dimension, the same joints in two consecutive frames are connected with temporal edges. The skeleton features are then expressed as a tensor 𝑓 ∈ R , where denote temporal length, number of vertexes and number of channels, respectively. We dene the graph convolution on such a spatial-temporal graph as: where is the vertex of the -th joint in frame and is the graph labeling function [ 38 ] that helps to gather corresponding weights from the convolution kernel denotes the neighboring area of each vertex. As Fig. 4 (a) shows, is partitioned into 𝐾 = 3 subsets following the spatial partitioning strategy dened in [ 62 ], which divides neighboring joints according to their distance to the center joint. The cardinality of each subset, denoted as , is employed as a normalization term. We adopt the same implementation of graph convolution as in [ 26 ]. For the spatial dimension, the graph convolution is computed as: ∗𝑤 , (4) where ∈ R is the adjacency matrix of the -th subset. (𝐴 is utilized to normalize the adjacency matrix. ∈ R is the convolution kernel. The input feature map is convolved with by the convolution operation . For the temporal dimension, we directly perform a 1 convolution on the output feature map ∈ R with temporal kernel size 𝐾 , because only the same joints in two consecutive frames are connected. Pyramidal skeleton graph convolutional network. With the spatial-temporal graph convolution dened above, we devise the pyramidal skeleton graph convolutional network (PSGCN) which explicitly renes skeleton graphs to capture virality information. It is demonstrated that reducing the resolution of feature maps with pooling layers improves the performance and robustness of CNN models [ 27 63 ]. However, it is nontrivial to extend the pooling operation to graph convolutional networks as the graph nodes have no locality information. As shown in Fig. 4 (b), we propose a graph down-sampling module for skeleton graphs to address this problem. Let ∈ R and 𝐴 ∈ R denote the input feature map and normalized adjacency matrix, respectively. The graph down-sampling operation is performed parallelly on 𝐾 subsets as: where we rene with the 𝑠𝑎𝑚𝑝𝑙𝑒 operation and employ a learnable convolution kernel with a stride of two. As formulated in Eqn. (6) , the 𝑠𝑎𝑚𝑝𝑙𝑒 operation produces downsampled adjacency matrices by quadratically interpolating between neighboring vertexes. Finally, outputs of subsets are fused by element-wise summation, resulting in the output feature map ∈ R Our PSGCN is composed of 9 graph convolution blocks as illustrated in Fig. 5, where parameters of the network architecture are given as (temporal stride, spatial stride) block number. Within each convolution block, we employ the residual learning mechanism [ 22 ], in which an optional 1 convolutional layer is utilized when input and output channels are not identical. To increase the exibility of our model, we introduce learnable edge weighting into PSGCN. The adjacency matrix in Eqn. (5) is actually formed as 𝐴 = 𝐴 +𝐴 , where is the original normalized adjacency matrix and is the learnable connectivity matrix parameterized for each graph convolution block. With this learnable weighting mechanism, our model can partially adjust the graph structure according to the semantic information in dierent graph convolution blocks. As Fig. 5 shows, the skeleton graphs in PSGCN are hierarchically rened, forming a pyramidal network architecture. We demonstrate the advantage of this pyramidal skeleton architecture later in Section 5.2. Besides virality prediction, our PSGCN is also capable of generating action feedback for dancers on how to improve their performance. Although teaching videos of viral dance challenges are uploaded by professional dancers in short video communities sometimes, making a high quality teaching video is usually laborious and requires extensive experience. Also, it is dicult for starters to obtain personalized advice on body movements in these videos. Therefore, automatic action feedback for dance challenges is a very promising application. In this section, we present the generation of two kinds of practical action feedback, i.e. action guidance and dance tips. Action guidance. As PSGCN takes human joint coordinates as input to predict the virality score, action guidance leading to higher score can be acquired by dierentiating the predicted virality score w.r.t. the joint coordinates. Concretely, let denote the predicted score for a dance clip and denote the graph vertex containing 2D coordinate of the -th joint in frame , we derive the gradient of predicted score w.r.t. each joint position as: We dene as the action guidance vector of the -th joint in frame . These vectors indicate the direction toward which the dancers shall adjust their body to improve the score. Compared with the man-made teaching videos, our action guidance is generated on-the-y and personalized for individual users. Dance tips. It is benecial for most challengers to be aware of dance tips on crucial body parts before performing the dance. Here we derive two kinds of dance tips from our model, i.e. temporal attention and overall attention. The rst one shows the temporal variation of body-part importance in each challenge while the second one indicates the overall body-part inuence across challenges. Let denote the action guidance vector (dened in Eqn. (7) ) of the -th video in challenge , we compute the temporal attention as: where is the video number of challenge . Consequently, is mathematically the averaged feedback magnitude of the -th joint at frame in challenge , where the magnitude is represented by the norm of feedback vectors. Temporal attention provides dancers with specic tips by telling them which body part should be paid more attention at dierent moment. Let denote the frame number of the 𝑖-th video in 𝑐, the overall attention is computed as: Overall attention gives dancers general tips on which body part deserves more attention when participating in dierent challenges. Examples of above feedback results are presented in Section 5.3. In terms of dance virality prediction, visual appearance is also a strong reference and usually complementary to skeleton features. Therefore, we incorporate the appearance-based prediction streams in our framework. As shown in Fig. 2, three streams of appearance features, i.e. holistic, facial and scenic streams, are utilized for comprehensive appearance-based virality prediction. Holistic features. Video frames extracted directly from dance clips contain the holistic appearance information of the whole picture. We use a TSN [ 57 ] model pre-trained on the Kinetics [ ] dataset to extract holistic appearance features from video frames at 25-fps. Each frame is represented by a 2,048 dimensional vector. Facial features. As shown in Fig. 1, dance clips tend to be more attractive when the dancer has a gorgeous face or attractive expressions. Therefore, we take facial features into consideration in virality prediction. We use MTCNN [ 64 ] to detect human face at 3-fps for each video and then extract facial features with a ResNet-50 [ 22 ] model pre-trained on VGGFace2 [ ] dataset. Finally, every face image is represented by a 2,048 dimensional vector. Scenic features. We also observe that a good background scenery helps to increase the virality of videos. To capture scenic features, we rst perform background extraction with the ViBe algorithm [ 53 ] performed in a sliding window of 120 frames. After that, a ResNet-18 [ 22 ] model pre-trained on the scene recognition dataset Places [ 66 ] is used to extract the scenic features. In the end, we obtain a 512 dimensional feature vector for each background image. Relational temporal convolutional network. As the appearance features are extracted at framelevel, it is crucial to capture temporal dynamics from these features. For example, the variations in facial expressions can only be exploited by modeling a sequence of consecutive facial images. Recent studies have demonstrated that temporal convolutional networks (TCN) are of great competitiveness in plenty of sequence modeling tasks including action segmentation [ 18 29 56 ], natural language processing [ 13 ] and so on. Compared with recurrent neural network (RNN) architectures, TCN has lower memory requirement and less training diculty [ ]. However, conventional TCN models are subject to the locality of convolution operations, making them weak at perceiving non-local frame-to-frame relations in video understanding tasks. We believe that such non-local relations are important in dance virality prediction as well. For example, in some dance challenges, there exist corresponding relations between the starting and ending actions of the dancer. These distant relations is hard to be captured with conventional TCNs. Therefore, we propose a relational temporal convolutional network (RTCN), which excels in capturing non-local relations between distant temporal positions while retaining the eciency of TCN architectures. As illustrated in Fig. 6 (a), the basic components of our RTCN are temporal convolution blocks and relational pooling blocks. The temporal convolution block is designed under the paradigm of residual convolution blocks [ 22 ], which has two consecutive sequences of dilated temporal convolution, batch normalization, ReLU and dropout layers. The input of each block is processed by an optional 1 1 convolutional layer and added to the output. Fig. 6 (b) shows a simplied example of neural connections within the structure of Fig. 6 (a). The convolution kernel size is 3 and the dilation rates are exponentially increased. In practice, we set the dilation rates to 2 , where is the number of the corresponding convolution block. To explicitly capture non-local relations upon temporal convolution features, we devise the relational pooling block as shown in Fig. 6 (a). The relational pooling block is composed of a max pooling layer and a multi-head self-attention layer. The max pooling layer improves feature robustness by reducing the temporal length. These features are then fed into a multi-head self-attention layer, where holistic temporal relations are explored. As presented in Fig. 6 (b), each temporal position is adaptively connected to all positions from previous features under the self-attention mechanism, which is introduced later. In summary, our RTCN is composed of six convolution blocks with channel numbers of 256 256 512 512 1024 1024 . We insert three relational pooling blocks after the second, forth and sixth convolution blocks, respectively. Multi-head self-attention. Self-attention mechanism [ 54 ] is demonstrated to be a powerful tool for non-local feature aggregation in many research elds including machine translation [ 14 54 ], action recognition [ 58 ] and object detection [ ]. In this work, we introduce a multi-head selfattention layer in the relational pooling block to adaptively extract non-local features. As is shown in Fig. 6 (c), let 𝐼 ∈ R denote the 𝑖𝑛𝑝𝑢𝑡 tensor of the self-attention layer, it is rst parallelly projected into sets of 𝑞𝑢𝑒𝑟𝑦 𝑘𝑒𝑦 𝑣𝑎𝑙𝑢𝑒 tensors. We denote each of these triplets as 𝑄, 𝐾, 𝑉 ∈ R where 𝐶 = 𝐶/𝐻. The non-local features are computed with scaled dot-product attention [54] as: Here the compatibility of and is computed by matrix dot product and scaled by to avoid exploding gradients. The 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 function is used to obtain normalized attention weights, which are used to compute a weighted sum of . Outputs of attention heads are then concatenated and once again projected. After a residual connection followed by layer normalization [ ], we obtain the nal output 𝑂 ∈ R . This multi-head self-attention process is formulated as: 𝑂 = 𝐿𝑎𝑦𝑒𝑟 𝑁𝑜𝑟𝑚(𝐶𝑜𝑛𝑐𝑎𝑡 (𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛(𝐼𝑊 , 𝐼𝑊 , 𝐼𝑊 ))𝑊 +𝐼 ), (11) where ,𝑊 ,𝑊 ∈ R are the multi-head linear projection matrices and ∈ R is the output projection matrix. The concatenation of tensors are denoted as 𝐶𝑜𝑛𝑐𝑎𝑡 . We utilize 𝐻 = 4 attention heads and visualize the results in Section 5.2. A dance clip can go viral for a number of reasons. For example, some videos are popular for exaggerations in body movements while others are popular for the attractive appearance of dancers. Furthermore, in terms of dierent dance challenge, the importance of dierent visual aspect varies as well. As Fig. 2 shows, we adopt a late fusion (score fusion) strategy facilitated by a modality attention mechanism. There are two major advantages of adopting this late fusion strategy. Firstly, it avoids feature degradation in feature-level fusion schemes, as dimensionalities of multi-modal features have signicant dierence (up to 8x as shown in Fig 2). Secondly, it ensures each modality stream makes prediction independently, facilitating practical applications like multi-dimensional video recommendation. Concretely, we concatenate video-level features of dierent modalities and feed it into the modality attention module, which consists of two fully-connected (FC) layers followed by a 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 function. The modality attention weight vector 𝐴 is derived as: where denote attention weights for the skeletal, holistic, facial and scenic modalities, respectively. 𝐼 ∈ R is the input dimensional concatenated feature vector. ∈ R and ∈ R are the parameter matrices of the hidden and output FC layers. Let 𝑆 = [𝑠 , 𝑠 , 𝑠 , 𝑠 denote the predicted scores from corresponding modality streams in , the nal virality score prediction is computed as: 𝑠 = 𝐴𝑆 . To stabilize the training of dierent modality streams, we impose ranking constrains on each of them as later introduced in Section 5.1. Evaluation protocol. To comprehensively evaluate the performance and generalization ability of dierent models, we set up single-challenge and all-challenge tracks of the VDV benchmark. In the single-challenge tracks, we separately split training and test sets for each challenge. Models are trained and tested on eight challenges respectively, evaluating the cross-challenge robustness of models and their performance with limited single-challenge data. In the all-challenge track, videos from eight challenges are shued and split altogether in order to evaluate models on the complete dataset. This track is more challenging as it evaluates the generalization ability of models and their performance under class-agnostic scenarios. The dataset partitions of all tracks are xed with 80% training data and 20% testing data. As in many other score prediction tasks [ 43 46 ], we use Spearman’s rank correlation coecient (SRC) as the performance metric, where a value of 1 represents the identical ranking order as the ground-truth and -1 represents the opposite. Loss functions. Our loss function consists of a mean square error (MSE) term and margin ranking loss [6, 16] terms 𝐿 , which are formulated as: 𝐿 = 𝛼𝐿 + 𝛽𝐿 +𝛾 , (14) where and denote the predicted and ground-truth virality scores for sample in a training batch of size 𝑠𝑖𝑔𝑛 is the sign taking operation and denotes the constant margin. and are applied on the nal prediction as introduced in Section 4.4. Additionally, we add a ranking loss term to each of the modality streams to encourage right ranking order without constraining the score values. and stand for the weights of corresponding loss terms, which are automatically adjusted with the Gradnorm [11] algorithm. Implementation and competitor methods. Our framework is implemented with PyTorch [ 44 ]. The temporal kernel size is set to 7 for both PSGCN and RTCN models. 1 1 convolution layers are added at the beginning of RTCN models for dimensionality reduction. Dropout probabilities are set to 0.5. For other competitor methods, we utilize the ocially released implementations unless otherwise specied. We thoroughly evaluate applicable models [ 43 46 49 60 ] in video virality prediction and human-centric performance assessment domains on VDV benchmark. For the C3D+SVR [ 43 model, we utilize a C3D network pre-trained on Kinetics dataset [ ] and perform SVR with sklearn. We further evaluate more recent 3D-CNN models including I3D [ ] and SlowFast [ 19 ] under this classic paradigm. Moreover, state-of-the-art graph convolution models [ 32 62 ] for human action understanding are evaluated in comparison with PSGCN. DMGNN [ 32 ] utilizes multi-scale skeleton graphs in the encoding stage of human motion prediction, we take the encoder part of it to regress virality scores. Lastly, sequential models of LSTM [ 23 ] and TCN [ ] are taken as baselines in comparison with RTCN. The LSTM model has two recurrent layers with the hidden size of 512. The layer number and dimensionalities of TCN model consist with our RTCN. In terms of model training, we use ADAM [ 25 ] optimizer with a batch size of 8. The learning rate is set to 0.0001 initially and decays every 10 epochs by the rate of 0.5. Action guidance. As introduced in Section 4.2, our framework generates instructive action guidance by calculating joint coordinate gradients. Fig. 10 presents examples of generated action guidance from (left) and (right), in each of which three critical frames are selected to be visualized. We illustrate feedback vectors with red arrows, whose length shows the feedback magnitude. By learning through massive dance data, our PSGCN perceives important factors of dance virality, thus deriving personalized action guidance. For example, in the rst frame of , dancers should move right arms close to their bodies and adjust knees leftward. In the second frame, dancers should move knees inward with left feet spread out. In the third frame, it is suggested to extend right arms down while bending right legs inward. For challenge , in the rst frame, dancers should swing both arms upper right. In the second frame, it is suggested to raise left wrists high while keeping right legs straight. In the third frame, dancers should put left hands on their hips and move both elbows outward, while nodding heads leftward. As the gradient calculation can be inferred on-the-y, dancers can obtain real-time action guidance in this way, which greatly facilitates dance learning. Dance tips. Fig. 11 (a) presents the temporal attention (dened in Eqn. (8) ) for challenge . Three fragments from an example clip is shown at the top. R and L stand for right and left, respectively. As the dancer performs dierent actions, the joint feedback magnitude varies as well. In the rst fragment, where the dancer performs a high left leg kicking, the feedback of left hip, left knee and left ankle is signicantly higher. In the second fragment, the active feedback of left elbow and left wrist corresponds to a shooting action performed by the left arm. In the third fragment, the dancer bends the right leg, lifts up hips and pus out the chest, which explains the strong feedback of right ankle, right knee, hips, pelvis and thorax. By referring to temporal attention like this, dancers can learn when and which joint should be noticed through a viral dance. Fig. 11 (b) shows the overall attention for every challenge as dened in Eqn. (9) . It can be observed that for challenge and , the feedback for lower body joints, i.e. hips, knees and ankles, is signicantly stronger. These challenges have rhythmic lower body movements like stepping forth and swaying hips. For challenge and , feedback of upper body joints, i.e. shoulders, elbows and wrists, is more signicant. This is because that upper body motions are more intense in these challenges, like shrugging shoulders and waving arms. We also nd that the neck position is inuential in all challenges, especially for those with active actions of both upper and lower body, e.g. and . This is probably because that neck is the central joint which connects limbs and head. With these valuable insights, dancers can notice which body part should get more attention when participating in dierent challenges. To validate the applicability of the proposed multi-modal framework in other human-centric performance assessment tasks, we extend it to sports videos and evaluate on UNLV-Sports [ 43 dataset. UNLV-Sports consists of 717 videos from three Olympic events, i.e. gure skating, gymnastic vault and platform diving. The performance scores in UNLV-Sports are given by expert judges according to the scoring criteria of corresponding Olympic events [ 43 ]. For example, a dive score is determined by the product of execution (quality score given by judges) and diculty (xed value based on dive type). Since only skeletal and holistic features are of major concern in sports rating, we adopt these two modality streams in our multi-modal framework. Features are extracted in the same way as in VDV dataset. Table 4 presents the comparison results on UNLV-Sports. For skeleton-based prediction, due to the strenuous movements in Olympic events, there are many atypical body postures in this dataset, which increases the error in estimated skeletal features. Owing to the graph down-sampling modules, PSGCN is more robust to noisy inputs, thus outperforming Pose+DCT and ST-GCN by a large margin. Also, in comparison with DMGNN, the compact architecture of PSGCN shows better scalability and excels in all three events. For appearance-based prediction, RTCN performs the best in gure skating. In vault and diving events, RTCN achieves competitive results compared with recent state-of-the-arts like S3D and USDL. Finally, by integrating skeletal and holistic streams with attentive modality fusion, we observe distinct improvements in all three events. This demonstrates that our multi-modal approach is also benecial to other human-centric performance assessment tasks like sports rating. Furthermore, applications like action feedback can also be implemented in sports videos. Fig. 12 presents examples of action guidance generated for gure skating videos. Two technical actions (i.e. basic-camel, cannonball ) from three athletes are shown in this gure with a demonstration image on the left. In the basic-camel posture, athletes should stretch left leg upward and move left arm close to the body. Their right arm should be dropped down straightly. In the cannonball posture, athletes should contract the skating leg downward and extend the free leg outward. The chest should be close to thighs with hands holding the ankle of the free leg. Our PSGCN learns essentials of these two actions and generates instructive action guidance. We conduct ablative experiments to validate the eectiveness of critical components in our approach. For skeleton-based prediction, we ablate graph down-sampling modules in PSGCN, resulting in a network without pyramidal graph structures. For appearance-based prediction, relational pooling blocks are removed from the RTCN trained with holistic features. For multi-modal prediction, we replace the attentive modality fusion with simple average weighting. Each of the appearance modality streams in integrated model is ablated to validate their ecacy. The eectiveness of MSE and ranking loss terms are also studied. Results are shown in Table 5. Noticeable performance degradation is observed in both skeleton-based and appearance-based models, demonstrating the eectiveness of PSGCN and RTCN architectures. For the integrated model, the removal of modality attention module causes performance drops in most single-challenge tracks. The all-challenge result has a even more signicant drop of 15% (0.34 vs. 0.29), indicating that learning inter-class modality inuence is important in class-agnostic scenarios. In terms of dierent modality streams, the holistic one yields the most improvement, followed by the facial and scenic streams. Although there are some exceptions in single-challenge tracks, facial and scenic features contribute in most cases. In the all-challenge track, the three modalities (i.e. holistic, facial and scenic) improve performance by 21%, 12%, 9%, demonstrating the eectiveness of each component in our integrated model. The ablation of MSE and ranking loss terms also results in apparent performance decline. Removing the ranking loss term causes more degradation on the whole, which shows the importance of imposing ranking order constraints. In this paper, we introduce the problem of virality prediction from dance challenges, which is of both academical and commercial value. To expedite related research, we release the VDV dataset, a TikTok based large-scale multi-modal virality prediction benchmark. We then propose a multi-modal prediction framework modeling human skeleton and appearance information with independent modality streams. For skeleton-based prediction, we devise PSGCN to hierarchically rene human skeleton graphs. For appearance-based prediction, RTCN is introduced to capture non-local temporal relations. Predictions from multi-modal streams are fused by the proposed attentive modality fusion method. Extensive experiments on VDV benchmark demonstrate the superiority of our approach. We demonstrate that practical applications like multi-dimensional video recommendation and action feedback can be facilitated by our model. Further experiments on UNLV-Sports dataset show that our multi-modal framework is benecial to other human-centric performance assessment tasks like sports rating. As the potential of VDV dataset has not been fully explored yet, we will leave other promising directions like video comment generation and action retargeting to our future work. We believe that our study will pave the way for the short video community and beyond. This work was supported by National Natural Science Foundation of China (Grant No. U20B2069), Foundation for Innovative Research Groups through the National Natural Science Foundation of China (Grant No. 61421003) and CCF-Tencent Rhino-Bird Research Fund.