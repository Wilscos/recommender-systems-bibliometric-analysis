The NLP pipeline has evolved dramatically in the last few years. The ﬁrst step in the pipeline is to ﬁnd suitable annotated datasets to evaluate the tasks we are trying to solve. Unfortunately, most of the published datasets lack metadata annotations that describe their attributes. Not to mention, the absence of a public catalogue that indexes all the publicly available datasets related to speciﬁc regions or languages. When we consider low-resource dialectical languages, for example, this issue becomes more prominent. In this paper we create Masader, the largest public catalogue for Arabic NLP datasets, which consists of 200 datasets annotated with 25 attributes. Furthermore, We develop a metadata annotation strategy that could be extended to other languages. We also make remarks and highlight some issues about the current status of Arabic NLP datasets and suggest recommendations to address them. The emergence of deep learning and its applications in many ﬁelds had a great impact on the development of various natural language processing (NLP) and speech techniques that were adapted to many languages. Many might correlate that to the availability of data especially with the existence of social media and the manufacturing of hardware devices that fostered research in the ﬁeld, namely GPUs. Typically, we are referring to the era of deep learning which started roughly after 2010. Following that, many public Arabic NLP and speech datasets have been published in conjunction with the recent advances in deep learning (Zaghouani, 2017). Currently, there is no online centralized catalogue for Arabic NLP and speech datasets. It is unclear how many online datasets there are as well as the metadata describing the datasets’ characteristics, such as diversity, demographic distribution, ethical considerations, quality, and so on. This study attempts to identify the publicly available Arabic NLP datasets and to provide a catalogue of Arabic datasets to researchers. The catalogue will increase the discoverability and provide some key metadata that will help researchers identify the most suitable dataset for their research questions. We highlight our contributions as the following: •We create the largest catalogue with 25 attributes for 200 Arabic NLP and speech datasets. •We design a metadata schema for annotating the datasets. NLP and speech datasets, discover issues and recommend solutions. The paper is structured as follows. Section 2 looks into previous work in the literature. Section 3 summarizes our approach to develop the catalogue. It discusses the research methodology, metadata design, and the annotation process. Section 4 outlines our ﬁndings. The results are then inspected, and issues and recommendations are highlighted in sections 5 and 6 respectively. Surveying the literature to derive analysis about a speciﬁc research ﬁeld or topic is a standard practice. It helps to provide an overview on the directions and trends on subject of interest. A prominent example of such effort is Ammar et al. (2018). They collected a large corpus of 280 million nodes. These nodes are diverse entities representing authors, papers, etc. An application of their work is the connected papers project Eitan et al. (2020). It aims to construct a graph of related literature based on a given query. Radev et al. (2016) extends the analysis by accounting for the citation count and extensive manual annotation on the collected literature. They curated their dataset from ACL Anthology papers. Their analysis covers various various attributes including authors impact factor, h-index and collaboration. For massive analysis reports on the ﬁeld of NLP, Mohammad (2020) surveyed the literature with 1.1 million paper information dataset collected from Google Scholar. Additionally, Sharma et al. (2021) proposed DRIFT, a data analysis tool that presents an overview of the landscape of a queried topic. They constructed their dataset from arXiv papers’ abstracts. As the dataset collection research is vastly growing with the signiﬁcant emergence of data on the web, the need to mandate such process becomes a necessity. This is, in fact, an active branch of research happening across various domains and disciplines. Consider, for instance, the systematic literature review protocol developed by Kitchenham (2004) that governs the data collection process for surveys. Another example is the guidelines reported by Mbuagbaw et al. (2017) on clinical trials. There are also studies that propose standardizing the documentation of datasets. Gebru et al. (2018) proposed datasheets for datasets. Their aim is to accompany datasets with a descriptive datasheet schema describing diverse attributes about the dataset. Such attributes include operating characteristics, recommended uses, motivation, collection process and test results. Similarly, Bender and Friedman (2018) propose data statement, a similar standardization approach that overlaps to datasheets in some of its attributes. However, while datasheets aims to document more general information about the dataset, data statement is more speciﬁc to linguistics and NLP. In the ﬁeld of Arabic datasets, there are many studies that attempted to survey the available data resources. Shoufan and Alameri (2015) reviewed the NLP literature for dialectical Arabic. Their work can be considered as a quick reference to locate important contributions for certain Arabic dialects that address speciﬁc NLP features. However, this study reviewed limited literature as it was concentrated only on Arabic dialects and the research on Arabic corpora was still infant. A comprehensive approach was implemented by Zaghouani (2017) where they collected a list of around 80 freely available Arabic datasets including datasets that are not related to NLP. The also provide links to the datasets but some of them do not work anymore. There are also some efforts to survey speciﬁc dialects. For example, Younes et al. (2020) provided a review of various kinds of constructed language resources (LRs) of Maghrebi Arabic dialects (MADs). They reviewed MAD raw corpora and divided it into speech corpora, speech transaction, web, and social media corpora. Recently, Guellil et al. (2021) presented and classiﬁed 90 studies that covered classical Arabic, Modern Standard Arabic, and Arabic Dialects. Also, they provided links to around 52 NLP datasets. Another survey paper was published by Darwish et al. (2021) to review the available tools and resources for Arabic. However, they provided links to a few datasets. The research method behind this survey follows the keyword-base literature review process Rowley and Slack (2004) followed by an annotation process to enrich the ﬁltered results with metadata. Our methodology follows ﬁve steps: (i) searching resources, (ii) ﬁltering using a selection criteria, (iii) annotating resources with metadata, (iv) validating the resources, and (v) analysing the results. Based on a pre-selected keywords, the retrieved Arabic language resources are added to our preliminary list of data sources. Next, all the resources collected are ﬁltered according to a set of inclusion criteria. The datasets that passed the ﬁltering criteria are ported to our ﬁnal list of datasets, and then are annotated with a set of metadata, both manually and automatically. Those which do not pass the criteria are discarded. Following that, a veriﬁcation step is performed to ensure the accuracy of the metadata. Finally, the ﬁnal set of resources are analysed according to the metadata and presented in this study. The initial search and the ﬁltering for all sources were done between July and August 2021, and the annotation process took place progressively after the data was collected and concluded on September 2021. The following subsections describe each step in more detail. The targeted search is performed using Google Search Engine to identity Arabic NLP dataset directly. To search speciﬁc dataset, we also conduct the search against the well-known data repositories and indexing websites using a set of keywords. The selected repositories are GitHub, Paperswithcode, Huggingface, LREC, Google Scholar, and LDC. The search combined terms related to NLP and Arabic language, such as "NLP", "Natural Language Processing" and all variations of Arabic dialects, as well as terms such as "database", "dataset", "resource", and "corpus". This step generates our preliminary list of data sources which consists of around 299 resources. Our search was additionally supplemented by manually screening retrieved articles and datasets, which we perform using a set of inclusion criteria, which are as follows: • the dataset is speciﬁc for NLP. •there is a publication associated with the language resource. • the resource is created after 2010. •the resource, in its raw form or annotated format, is suitable for language modelling and text generation tasks. •the resource does not serve as NLP tool such as spelling-checking and stop-words. From the preliminary list, we ended up with 207 papers. Our inclusion criteria removed 93 papers initially. By applying the inclusion criteria, the datasets search pool is reduced to the ﬁnal set of considered resources. At this stage, an annotation process is applied to manually annotate them with a set of pre-agreed metadata. When applied to this process, we consider three main goals: 1) designing metadata speciﬁc for Arabic resources, 3) deciding on the annotation format, 2) setting up an annotation workﬂow, and ﬁnally 3) deﬁning an annotation task. Metadata SelectionThe main motivation behind designing metadata for Arabic NLP resources is to increase the discoverability and reusability of such resources. The metadata are chosen to represent different aspects of the language resource. Park et al. (2021)’s work serves as an example for identifying the appropriate metadata for our Arabic NLP use case. Following several revisions, the ﬁnal agreed-upon metadata is represented by a taxonomy in Figure 1. It consists of ﬁve subcategories as follows: •Publication: This subcategory concerns about metadata relating to author, publisher and other publication details for the dataset referenced publication. It includes attributes such as the title, the link, the year of publication for the referenced paper, and the venue title, name, and type. •Content: This subcategory is concerned with the content of the dataset in terms of the size, the representation and the quality. The size tag indicates the quantity of the dataset by specifying the unit of measurement (tokens, sentences, documents, MB, GB, TB, hours, others), as well as the number of units using the volume tag. The representation dimension describes the contextual information about the dataset. For example, the tokenized ﬂag speciﬁes whether the dataset is tokenized. This is useful since various tokenizers project different behaviour, and when this is not speciﬁed, it impacts downstream tasks. The form tag, on the other hand, deﬁnes the form of the content, being written, or spoken language, while the script tag describes the writing system used in the dataset (Arab,Latn,Arab-Latn,Other). The third dimension, quality, describes elements related to the data collection. It covers, for example, the collection style used for building the dataset (e.g crawling, translation, etc), the ethical risk associated with utilizing the data set (low, medium, and high), and the domain of the dataset (social media, etc ). •Accessibility: This subcategory concerns about the timeliness and the reliability of access to the data. Its associated metadata includes: the name of the data provider, the name of the data host, the link to download the data from the host, the licence and the cost to obtain the data. •Diversity: This metadata subclass is used to capture the linguistic and culture diversity within Arabic language. It covers the language tag to represent the language of the dataset, either being Arabic (ar or multilingual to denote a dataset that contains several languages), as well as the subsets tag to denote the sub-datasets that are contained inside this dataset. The last tag in this class is dialect. To capture the linguistic variety of Arabic, we adapted ﬁve high-level categories of dialect variations, resulting in a total of 29 dialect categories. These categories are as follows: i) MSA for Modern Standard Arabic, ii) CLS for Classic Arabic and Qura’anic text, ii) Regional dialects for the four regions (GLF, LEV, EGY, NOR), iii) country-based dialects which cover the 22 dialects spoken in Arabic-speaking countries, and ﬁnally iv) the other which includes mixed dialects and code-switched script. •Evaluation: The metadata within this subcategory describes the process of using the dataset in relation to the evaluation phase of the NLP pipeline.The ﬁrst tag is the test split, which is deployed as a boolean ﬂag to signal if the dataset is prepared for evaluation task by having a distinct split between the training and the test sets. The tasks tag deﬁnes the list of tasks to which the dataset is applied, whilst the related datasets attribute, lists what dataset(s) intersect with the current dataset. Annotation FormatsBased on the chosen metadata, we ﬁgured out that different annotation procedures can be applied to insert the metadata. Hence, two formats of annotation are adapted in this work; (i) manual curation, and (ii) automatic annotation of the metadata. The manual curation is performed by the human annotators via manually inspecting the dataset link, and it’s referenced paper. We basically use this format to extract metadata that is hard to automate or not implicitly mentioned. The second format is auto-annotation. For this format, we rely on APIs from academic publishers, such as Semantic Scholar API (Python Library). As such, most of the metadata is manually annotated, except for the publication information which was retrieved using the API. Annotation WorkﬂowWe used Google Sheets to set up our annotation workﬂow, where the metadata is speciﬁed as Google Sheet columns. The datasets were annotated with a set of metadata by the four authors, who are ﬂuent Arabic speakers and researchers in the ﬁeld of natural language processing. We deﬁne the manual annotation task as follows. For each link of Arabic language resource in our ﬁltered pool of resources, the main goal of the task is to annotate the ﬁltered datasets against the metadata. The annotators were instructed to follow the following set of guidelines: 1. Examine the resource link. 2. Examine the referenced paper. 3. Fill the metadata entries on Google Sheets. 4.If a particular attribute of the dataset is not mentioned, log it in the notes’ column. 5. If a conﬂict is observed between the reported metadata from the resource link and the actual published paper, mark this entry in the sheet. 3.4 Step 4: Veriﬁcation of metadata Following the completion of the annotation, a veriﬁcation step is performed to conﬁrm the accuracy of the information given. This step was deemed necessary in order to explain the notes from the prior stage. It is done manually and involved active communication amongst the annotators. At this stage, we removed 7 extra papers, 2 of them were duplicated datasets and 5 had wrong annotations for the year attribute (before 2010). In Figure 2, we show an example of the metadata annotations of a chosen dataset. 3.5 Step 5: Analysis After verifying the ﬁnal collection of annotated datasets, we conduct the analysis on various metadata. The ﬁndings of the analysis will be described in the next sections. In this section we describe our ﬁndings in collecting all the data resources related to Arabic NLP published between 2010 and September 2021. We describe driven statistics of the datasets in addition to how we represent our data in a user friendly format. 4.1 Data Statistics The total number of datasets included in this catalogue is 200. More than 90 % of the datasets’ written format is text while the remaining is speech data. Table 1 summarizes the overall statistics of the catalogue in terms of volume. We mainly used the reported numbers in the paper and validated the numbers by downloading the dataset. If the size is different we report the numbers from the downloaded dataset. If the number can’t be validated, because of the size of the dataset for example, we report the numbers from the paper. Mostly, we use tokens to represent datasets that tackle tokenbased tasks like named entity recognition (NER), sentences to represent datasets that are related to sentence-based tasks like sentiment analysis and documents if the size of the dataset is too large. 4.2 Masader Interface To easily navigate the sources we created a website that is connected directly to the Google Sheets, allowing any updates in the sheets to be reﬂected immediately on the website. The website’s primary Table 1: Summary of the 200 Arabic NLP datasets in the Masader project in terms of volume. interface only displays nine attributes. These are deliberately chosen for trail, and testing their relevance for academic search. The interface supports discoverability by including the following features: 1) a clickable association between the dataset and its published paper, 2) a direct link to the most recent hosted version of the dataset, 3) a clickable link on the dataset name, which leads to dataset card displaying the remaining metadata of a dataset, and ﬁnally 4) ﬁltering and sorting based on each attribute. This section provides an analysis on the surveyed datasets. We mainly focus on discussing the current trend of publishing Arabic resources and drawing some remarks about the overall status of the landscape. Figure 3 depicts the evolution of Arabic NLP in the light of publicly available data resources from various venues. The graph demonstrates a general growth in the number of published resources, with a particular increase in even years. This can be attributed to the large number of datasets published at the bi-annual LREC conference. We also anticipate a signiﬁcant increase, particularly in 2020, with the emergence of pretrained language models language models namely AraBERT (Antoun et al., 2020a), Multi-dialect BERT (Talafha et al., 2020) and Araelectra (Antoun et al., 2020b). We can also observe that most of the datasets are published in conferences and workshops. Figure 2: Example demonstrates the annotation of the metadata on the Shami dataset (Abu Kwaik et al., 2018). The subsets tag represents the dialects and each subset (For example, Jordanian) inherits all the metadata from the superset Shami, except the volume. 5.2 Data Accessibility Data accessibility is an important aspect of fostering open research. Making the data source available extends its lifespan and allows it to be utilized in the way the dataset authors intended. In our initial data sources collection, we observed that more than half of the 93 of the discarded datasets had no online presence, nor an explicit means of accessing any version of the data. Based on the examination of the data sources, there is general trend of making the data available through open source repositories such as GitHub, GitLab and Mendeley Data. In the last three years, more than 80 % of the data can be accessed freely on different data hosters. This trend is very promising as it shows an increased interest in making the datasets available online. In the recent years, there is a small portion of datasets that needs authentication to access either through email or registration forms. We also observed some papers that suggested contacting the corresponding author to access the data privately. We didn’t include such papers on our ﬁnal list. 5.3 Data Providers and Licensing Data providers are important to collect, annotate, distribute, and perhaps host the datasets. Another responsibility of the data provider is to select the appropriate licence for the datasets. In fact, having a proper licence is a key component of any dataset for both data providers and researchers. In terms of data providers, our ﬁndings show that the majority of the data sources we collected were created in virtue of collaboration of multiple institutions. Institutions such as QCRI, Qatar university, NYU Abu Dhabi, and Nile University are the top four providers of Arabic NLP datasets. While datasets from these prominent providers are typically accompanied by clear licensing. Unfortunately, about 50 % of the datasets lack licences. Among those with explicit licence, there is a wide range of used licences. Some examples include several variations of Common Creative licences, Apache, MIT, GPL and BSD. As we can see from Figure 4, there were more than 20 entries out of the 200 with annotations of the dialects. These datasets are primarily intended for dialect identiﬁcation tasks. The scope of the datasets we collected across the Middle East and North Africa is depicted in Figure. The Egyptian dialect is the most prevalent, followed by Algerian, Moroccan, and Saudi dialects. Somali, Djibouti, and Mauritanian dialects are underrepresented in the surveyed datasets, with only three resources for each. Figure 4: Dialects representation across datasets. Figure 5 illustrates the distribution of tasks that appeared in more than one dataset. The graph shows that machine translation and sentiment analysis are the most popular tasks within Arabic NLP community. Machine translation has received an increasing attention in the literature across many languages, particularly in multilingual datasets, which explains the high frequency of publications in that area. Sentiment analysis, on the other hand, is heavily researched for a variety of reasons. Partly because the datasets for this task are primarily derived from social media sites with minimum effort, and partly because it serves as a suitable representation of everyday language that displays more sentiments. Other tasks that have presence within the community include dialect identiﬁcation, topic classiﬁcation, named entity recognition and speech recognition. There are also several low resource tasks that appeared at most once but are not displayed in the ﬁgure, such as poetry classiﬁcation, word disambiguation, grammar checking, to name a few. Each of these sources only contains a single dataset addressing a distinct task. In contrast, datasets like KALIMAT (El-Haj and Koulali, 2013), contains annotations for multiple tasks, or evaluation suites such as ALUE which is an aggregation of multiple datasets(Seelawi et al., 2021). This section highlights some issues of Arabic NLP data related to their legitimization, the haphazard collection, annotation, and the documentation practices. Data Availability.It is encouraging that our review identiﬁed 200 datasets that were publicly available, yet discoverability seems, by all accounts, to be an issue. While a few datasets are well recognized in the ﬁeld, many are not, which might potentially lead to missed research opportunities and might result in bias because of an overuse of a few potentially non-representative datasets. Further considerations in this regard, arising from our survey, include the sustainability (persistence) of the dataset URLs. As there is not a dedicated platform to host Arabic NLP datasets, some datasets’ links appear to be inaccessible due to URLs invalidity (orphan datasets). We identiﬁed one obvious/clear cause of orphaned datasets, which is the termination of academic afﬁliation and so the broken dataset link when the dataset is published as part of the researcher’s academic webpage. One possible solution to address this issue is to host the datasets on public repositories like GitHub, Gitlab, Mendeley Data, SourceForge, to name a few. Data Documentation.Data documentation refers to the process that describes the collected data and aims at facilitating cataloguing and discoverability of the data. One key form of data documentation is metadata, which are characteristics describing the data. For a dataset to be truly reusable, adequate documentation is important to offer the necessary insights into the potential usage of the dataset. For researchers, providing such insights saves time and resources, and it suggests a reliable dataset for reuse (Perrier et al., 2020). In this work, we analyse the documentation of Arabic NLP datasets in relations to the proposed metadata in Section 3. When we examine these datasets, we recognize that a few of them are accompanied by documentation. The majority appears to report inadequate metadata that is insufﬁcient to make a decision on the dataset reuse. More precisely, there appears to be a pattern in which some researchers are satisﬁed with only publishing the direct URL link to download the dataset, or accompanying the downloadable dataset with a READMEﬁle stating the size of the dataset, and a reference to the publishing paper on the dataset host page. Within the quality metadata, we observed very few instances reporting the data collection style. Another consideration includes the absence of clarity around the terms of access and use from some dataset providers. In most cases, we noticed that datasets are not accompanied by sufﬁcient information regarding their provenance, and hence it is not possible for researchers to know if there is an appropriate ethical and governance framework underpinning the provision of these datasets. As a result of this research, we conclude that governance information, such as licencing, is a crucial part of the documentation and that, if not speciﬁed, the dataset’s potential reuse may be limited. Regardless, as noted earlier, ease of access and good documentation is an important driver for researchers. Therefore, deploying a framework for documenting NLP data, such as those proposed by Bender and Friedman (2018) and Gebru et al. (2018) is considered as a good step towards promoting data sharing. Data Sharing.Data sharing is positively seen in the NLP community, with even top conferences are recognizing researchers who have shown a desire to share datasets. This process is usually volunteered, unless it is enforced internally by institutions and corporates measures. Within the Arab NLP community, we observed a high intention to support the research by sharing datasets. While some datasets are poorly documented and hardly accessible, others are well-prepared with clear documentation. We identiﬁed some datasets that are never published, or they are inaccessible, even where there was an intention to make them available, as declared in the formal publications. In terms of openness, we also observed a pattern in which some providers require a form of registration prior to sharing the datasets. Regarding sharing dataset links, as it was detailed in the data documentation, the unsustainability of dataset link poses a challenge, and hence data repository such as Github and Gitlab are usually adopted as a platform for sustainable data sharing. Evaluation.NLP models are usually evaluated by training on speciﬁc tasks. In the literature, the test split is provided as an approach to evaluate models after training on the training split. In our metadata collection process, we observed that more than 60 % of the datasets do not have predeﬁned test splits. To mitigate that, researchers replicate the experiments by evaluating the old models again on a chosen random split of the data. As a result, a dataset’s results will be incomparable across different NLP models. Data Collection or Curation.Having stated annotation protocols and clear justiﬁcation behind inter-rater agreement increases the reliability of the data. In the surveyed datasets, we have the following observations about the collection style. First, some crawling driven datasets lack any consideration of ethics and legal frameworks imposed by the platform from which the data is scraped, and the country of the data subjects. It also imposes an ethical risk by stating personal identiﬁed information. Secondly, when using machine translation to drive an Arabic version of a non-Arabic dataset, we observed missing information such as the translation models, veriﬁcation process by native speakers, the reported errors, to name a few. Given the current quality of machine translation models, this approach in creating Arabic datasets opens many questions about the quality of the dataset and its potential usage. While this approach can be used to help create datasets for some tasks, it is a risky approach if used to drive benchmark datasets for tasks such as common sense. Thirdly, as it was highlighted in previous points, not having an indication of the ethical risk of using datasets is a weak point in the Arabic NLP datasets. While each Arabic-speaking country has its own legislations and data protection acts (Abu-Ghazaleh, 2000), it is important to ﬂag any potential risk of using the datasets for future usage. Ethical Concerns and Privacy.Social media data, such as Twitter data, composed the greatest proportion of Arabic NLP datasets, particularly for dialect representations. Typically, such data is associated with the risk of exploiting personal information. In fact, this raises the concerns of considering data subject’s right and the ethics behind using such data. Likewise, datasets acquired from publications and human-produced literature pose concerns regarding the incorporation of copyright consideration in the derived Arabic NLP datasets. In either type of the datasets, we found no explicit risk indications at any point of the NLP pipeline: collection, modelling, evaluation, or deployment. In this context, we encourage data providers to state information about the ethical risks associated with their released datasets, as well as the appropriate approach to mitigate them, in order to enrich the Arabic NLP landscape. We recognize some limitations to our study. Firstly, given the nature of our search strategy, only datasets that are probably indexed with metadata, and whose publication contains one of our search key-phrases are likely to have been retrieved. Secondly, there exists some additional data resources available that are either with open access or regulated access (e.g LDC), but they were not explored in this study since they do not conform to our inclusion criteria. As a future work, we plan to keep the catalogue updated by adding new datasets and also support community-based contributions where authors can submit the metadata of their datasets to our online catalogue. In this research, we created an online catalogue of 200 Arabic NLP datasets with metadata annotations. We analyzed our ﬁndings, discovered some issues and suggested some resolutions. Mainly, we recognise that the NLP ﬁeld is rapidly evolving, and that both Arabic NLP researchers and practitioners recognise the value of incorporating Arabic into language technologies, particularly beyond Modern Standard Arabic. As a result, while this research provides a comprehensive analysis, it is only a snapshot in time and extra efforts are required to drive the ﬁeld more in that direction. This research was conducted under the BigScience initiative for open research, a one-year-long research initiative targeting the study of large models and datasets. It was conducted as part of the data sourcing group for collecting datasets for different languages. We would like to thank the members of the data sourcing group for the insightful discussions.