Trip recommender system, which targets at recommending a trip consisting of several ordered Points of Interest (POIs), has long been treated as an important application for many location-based services. Currently, most prior arts generate trips following pre-deﬁned objectives based on constraint programming, which may fail to reﬂect the complex latent patterns hidden in the human mobility data. And most of these methods are usually difﬁcult to respond in real time when the number of POIs is large. To that end, we propose an Adversarial Neural Trip Recommendation (ANT) framework to tackle the above challenges. First of all, we devise a novel attention-based encoder-decoder trip generator that can learn the correlations among POIs and generate well-designed trips under given constraints. Another novelty of ANT relies on an adversarial learning strategy integrating with reinforcement learning to guide the trip generator to produce high-quality trips. For this purpose, we introduce a discriminator, which distinguishes the generated trips from real-life trips taken by users, to provide reward signals to optimize the generator. Moreover, we devise a novel pre-train schema based on learning from demonstration, which speeds up the convergence to achieve a sufﬁcient-and-efﬁcient training process. Extensive experiments on four real-world datasets validate the effectiveness and efﬁciency of our proposed ANT framework, which demonstrates that ANT could remarkably outperform the state-of-the-art baselines with short response time. Trip recommendation (or trip planning) aims to recommend a trip consisting of several ordered Points of Interest (POIs) for a user to maximize the user experience. This problem has been extensively investigated over the past years (Lim et al. 2015; Chen, Ong, and Xie 2016; He, Qi, and Ramamohanarao 2019). Most existing studies tackle the problem by a two-stage process. First, they majorly exploit POI popularity, user preferences, or POI co-occurrence to score POIs and design various objective functions respectively. Then, they model the trip recommendation problem as a combinatorial problem: Orienteering problem (Golden, Levy, and Vohra 1987), and generate trips by maximizing the pre-deﬁned objective with the help of constraint programming (CP). This work was done when the ﬁrst author was an intern in Baidu Research under the supervision of the second author. This is a preprint. Though using this CP-based paradigm to solve such a combinatorial problem is very popular over the past years, its drawbacks are still obvious. First, the recommended trips by such methods are optimized by the pre-deﬁned objective function, which may not follow the latent patterns hidden in the human mobility data generated by users. For instance, according to the statistics from a real-life trip dataset from Beijing (see Experiment section), after watching a ﬁlm, 26% users choose to go to a restaurant while only less than 1% users choose to go to a Karaoke bar. However, the predeﬁned objective may not be capable of capturing such mobility sequential preferences and generate unusual trips like (cinema → Karaoke bar). Second, the time complexity of such CP-based methods is usually too high to handle hundreds of POIs in a city in real time. As shown in our experiment section, the response time of such methods with 100 POIs can be more than 1 minute. Such a weakness is very disruptive to the user experience. To this end, we propose an Adversarial Neural Trip Recommendation (ANT) framework to solve the challenges mentioned above. At ﬁrst, we propose an encoder-decoder based trip generator that can generate the trip under given constraints in an end-to-end fashion. Concretely, the encoder takes advantage of multi-head self-attention to capture correlations among POIs. Afterwards, the decoder subsequently selects POI into a trip with mask mechanism to meet the given constraints while maintaining a novel context embedding to represent the contextual environment when choosing POIs. Second, we devise an adversarial learning strategy into the specially designed reinforcement learning paradigm to train the generator. Speciﬁcally, we introduce a discriminator to distinguish the real-life trips taken by users from the trips generated by the trip generator for better learning the latent human mobility patterns. During the training process, once trips are produced by the generator, they will be evaluated by the discriminator while the feedback from the discriminator can be regarded as reward signals to optimize the generator. Therefore, the generator will push itself to generate high-quality trips to obtain high rewards from the discriminator. Finally, a signiﬁcant distinction of our framework from existing trip planning methods is that we do not adopt the traditional constraint programming methodology. Considering the excellent performance for inference(prediction) of the deep-learning (DL) based models, the efﬁciency of our method is much better than such CPbased methods. To sum up, the contributions of this paper can be summarized as follows: • To the best of our knowledge, we are the ﬁrst to propose an end-to-end DL-based framework to study the trip recommendation problem. • We devise a novel encoder-decoder model to generate trips under given constraints. Furthermore, we propose an adversarial learning strategy integrating with reinforcement learning to guide the trip generator to produce trips that follow the latent human mobility patterns. • We conduct extensive experiments on four large-scale real-world datasets. The results demonstrate that ANT remarkably outperforms the state-of-the-art techniques from both effectiveness and efﬁciency perspectives. Our study is related with POI recommendation and trip recommendation problems which are brieﬂy discussed in this section respectively. 2.1 POI Recommendation POI recommendation usually takes the user’s historical check-ins as input and aims to predict the POIs that the user is interested in. This problem has been extensively investigated in the past years. For example, Yang et al. (2017) proposed to jointly learn user embeddings and POI embeddings simultaneously to fully comprehend user-POI interactions and predict user preference on POIs under various contexts. Ma et al. (2018) investigated to utilize attention mechanism to seek what factors of POIs users are concerned about, integrating with geographical inﬂuence. Luo et al. (2020) studied to build a multi-level POI recommendation model with considering the POIs in different spatial granularity levels. However, such methods target on recommending an individual POI not a sequence of POI, and do not consider the dependence and correlations among POIs. In addition, these methods do not take time budget into consideration while it is vital to recommend trips under the time budget constraint. 2.2 Trip Recommendation Trip recommendation aims to recommend a sequence of POIs (i.e. trip) to maximize user experience under given constraints. Lim et al. (2015) focused on user interest based on visit duration and personalize the POI visit duration for different users. Chen, Ong, and Xie (2016) modeled the POI transit probabilities, integrating with some manually designed features to suggest trips. Another study modeled POIs and users in a uniﬁed latent space by integrating the co-occurrences of POIs, user preferences and POI popularity(He, Qi, and Ramamohanarao 2019). These methods above share similar constraints: a start POI, an end POI and a time budget or trip length constraint, and they all maximize respective pre-deﬁned objectives by adopting constraint programming. However, such pre-deﬁned objectives may fail to generate trips that follow the latent human mobility patterns among POIs. Different from these methods, Gu et al. (2020) focused on the attractiveness of the routes between POIs to recommend trips and generate trips by using greedy algorithm. However, only modeling users and POIs in category space may not be capable of learning the complex human mobility patterns. The prediction performance based on greedy strategy is also not satisﬁed enough. In this section, we ﬁrst introduce the basic concepts and notations, and then we give a formal deﬁnition of the trip recommendation problem. A POI l is a unique location with geographical coordinates (α, β) and a category c, i.e. l =< (α, β), c >. A checkin is a record that indicates a user u arrives in a POI l at timestamp tand leaves at timestamp t, which can be represented as r = (u, l, t, t). We denote all check-ins as R and the check-ins on a speciﬁc location l as R. Since we have the check-ins generated by users, we can estimate the user duration time on POIs. Given a POI l and corresponding check-in data R, the expected duration time of a user spends on the POI is denoted by T(l), which is the average duration time of all check-ins on location l: We denote the transit time from a POI lto another POI l as T(l, l). The time cost along one trip can be calculated by summing all the duration time of each POI and all the time cost on the transit between POIs. In our experiment, the transit time is estimated by the distance between POIs and the walking speed of the user (e.g. 2m/s). A trip is an ordered sequence of POIs S = l→ l→ ··· → l. Given a query user u, a time budget Tand a start POI l, we aim to plan a trip S = l→ l→ ··· → lfor the user. We name the query user, the start POI and the time budget a trip query, denoted as a triple q = (u, l, T). Now we deﬁne the trip recommendation problem formally. Given a trip query q = (u, l, T), we aim to recommend a well-designed trip that does not exceed the time budget and maximize the likelihood that the user will follow the planned trip. For convenience, we denote the sum of transit time from current POI to the next POI and duration time on the next POI as T(S, S) = T(S) + T(S, S), Sis the i-th POI in trip S. So the time cost on the planned trip denoted as T(S) can be calculated by T (S) = T(S)+ PT(S, S). Overall, the problem can be formulated as follows: Figure 1: An overview of the proposed framework. The overall framework of ANT is shown in Figure 1. We ﬁrst selectively retrieve hundreds of POIs to construct a candidate set. Next, we use a well-devised novel time-aware trip generator G to generate the well-planned trip for users with incorporating the time budget and the POI correlation. The trip generation process can be considered as a sequential decision process, that is to say, at each step there is a smart agent to select the best POI which can ﬁnally form an optimal trip. Thus, we model the trip generation procedure as a Markov Decision Process(MDP)(Bellman 1957), where we regard selecting POI as action, the probability distribution on POIs to be selected as a stochastic policy, and contextual environment(e.g. available time, selected POIs) when selecting POIs as state. Therefore, our goal is to learn an optimal policy, which guarantees that the agent can always take the best action, i.e. the POI with the highest probability is the most promising option. To train the policy, we construct a discriminator D (following the Generative Adversarial Networks(GAN) structure (Goodfellow et al. 2014)) to provide feedback compared with the real-life trips taken by users. Therefore, the generator can be trained through policy gradient by reinforcement learning to draw the generated trips and the real-life trips closer. 4.1 Candidate Construction As for a trip query, it is usually not necessary to take all the POIs into consideration to plan a reasonable trip. For instance, those POIs that are too far away from the start POI are impossible to be part of the trip. Here we propose a rulebased retrieval procedure to pick up a small amount of POI candidates from the large POI corpus, named candidate set, which incorporates the impact of connection among trips and geographic inﬂuence. Drawing Lessons from Other Trips If a user requests a trip at the start location l, former trips that are associated with lare promising to provide a reference. Inspired by this, we could assume that given the start POI lof a trip query, those POIs that once co-occurred with lin the same trip could be potential options for the trip query, which can be named drawing lessons from other trips. Hypergraph provides a natural way to gather POIs belonging to different trips and also to glimpse other trips via hyperedges. Figure 2: An instance of hypergraph construction. Deﬁnition 1 (Trip Hypergraph). Let G = (L, E) denote a hypergraph, where L is the vertex set and E is the hyperedge set. Each vertex represents a POI land each hyperedge e ∈ E connects two or more vertices, representing a trip. Speciﬁcally, we use trips in the training set to build the trip hypergraph. On one hand, all the POIs in the same trip are linked by a hyperedge, which preserves the matching information between POIs and trips. On the other hand, a POI may exist in arbitrary hyperedges, connecting different trips via hyperedges. Given a trip query (u, l, T), POIs that are connected with lvia hyperedges are promising to be visited for the upcoming trip request, so we directly add them into the candidate set. Figure 2 is a simple example of trip hypergraph retrieval. If the start POI of the upcoming trip query is l, POIs {l, l, l, l, l, l, l} will be added into the candidate set for the corresponding trip query. Spatial Retrieval Distance between users and POIs is a crucial factor affecting user’s decisions in location-relative recommendation. It is typical that a user’s check-ins are mostly centralized on several areas(Hao et al. 2016, 2020), which is the famous geographical clustering phenomenon and is adopted by earlier work to enhance the performance of location recommendation (Ma et al. 2018; Lian et al. 2014; Li et al. 2015). Therefore, except for the candidates generated by hypergraph, we also add POIs into candidate set from near to far. In our framework, we generate ﬁxed-length candidate sets for every trip query, denoted as Afor the corresponding trip query q. We ﬁrst use the hypergraph to generate candidates and then use the spatial retrieval. In other words, if the numbers of candidates generated by the hypergraph retrieval for different trip queries are smaller than the pre-deﬁned number |A|, we pad the candidate set with the sorted POIs in order of distance to a ﬁxed length. As shown in Figure 3, the generator consists of two main components: 1) a POI correlation encoding module (i.e., the encoder), which outputs the representation of all POIs in the candidate set; 2) a trip generation module (i.e., the decoder), which selects location sequentially by maintaining a special context embedding, and keeps the time budget constraint satisﬁed by masking mechanism. Joint Embedding Given the trip query (u, l, T) and the corresponding selected candidate set A, we use a simple Figure 3: Illustration of time-aware trip generator. linear transform to combine the user u and the POI lin A with its category c for embedding: where x, x, xare POI embedding, category embedding and user embedding (which are all trainable embedding), [a; b; c] means concatenation of vectors a, b, c, and W, b are trainable parameters. Thus, we get the matrix presentation of the candidates H∈ R, each row of His the representation of a POI in the candidate set. POI Correlation Encoding We apply a self-attention encoder to produce the representation of locations. The reasons to use the self-attention encoder can be justiﬁed from two perspectives. The ﬁrst reason is due to the permutation invariance for sets. For a candidate set, the order of POIs in this set is invariant to the ﬁnal result, i.e. any permutation of the inputs is supposed to produce the same output representation. Thus, we do not adopt the classical RNN-based encoder architecture because it focuses on the sequential information of the inputs, which is not suitable for our problem. Second, a reasonable generated trip is supposed to consider the relationship between POIs. For instance, after staying at a restaurant for a while a person is more interested in other kinds of POIs but not another restaurant. So it is helpful to produce a POI representation with attention to other POIs. The encoder we apply is similar to the encoder used in the Transformer architecture (Vaswani et al. 2017) while we remove the position encoding, which is not suitable for our problem. We stack multiple attention layers and each layer has the same sublayers: a multi-head attention(MHA), and a point-wise feed-forward network(FFN). The initial input of the ﬁrst attention layer is Hand we apply the scaled dot-product attention for each head in layer l as: where 1 ≤ i ≤ M, W, W, W∈ R, d= d/M, M is the number of heads and dis the dimension for each head. The scaled dot-product attention computes as: where the softmax is row-wise. M attention heads are able to capture different aspects of attention information and the results from each head are concatenated followed by a linear projection to get the ﬁnal output of the MHA. We compute the output of MHA sublayer as: where W∈ R. We endow the encoder with nonlinearity by adding interactions between dimensions by using the FFN sublayer. The FFN we apply is a two-layer feedforward network, whose output is computed as: where W∈ R, W∈ R. Note that all the parameters for each attention layer is unique. Besides, to stabilize and speed up converging, the multihead attention and feed-forward network are both followed by skip connection and batch normalization (Vaswani et al. 2017). To sum up, by considering the interactions and inner relationship among POIs, the encoder transforms the original embeddings of POIs into informative representations. Trip Generation It is of great importance to consider the contextual environment when planning the trip so we design a novel context embedding integrating candidate information, time budget and selected POIs. Self-Attention Context Embedding. By aggregating the location embeddings, we apply a mean pooling of ﬁnal location embedding¯h=Phas candidate embedding. During the process of decoding, the decoder selects a POI from the candidate set once at a time based on selected POIs S, t< t and the available time left . We keep track of the remaining available time Tat time step t. Initially T= T− T(S), and Tis updated as: where S= l. Following existing methods to represent the contextual environment in the procedure of decoding (Bello et al. 2017; Kool, van Hoof, and Welling 2019), we employ a novel context embedding hconditioned on candidate set and remaining time, which will change along the decoding proceeds. The context embedding his deﬁned as: where h∈ R. Before deciding which POI to add into the trip at time step t, it is important to look back the information about candidates and remind ourselves which POIs are optional and which POIs should not be considered because they break the given constraints. Therefore, we ﬁrst glimpse the candidates that are optional, i.e. are never selected before and do not exceed the time budget, and then integrate the information with attention to the output from the encoder: where W∈ R, W, W∈ R, his the i-th row of the location embedding matrix H, and Θ(·) is a Heaviside step function, which plays a crucial role as the time-aware mask operator. Thus, the reﬁned context embedding¯his computed as:X We omit the multi-head due to the page limit. Self-Attention Prediction. After getting the reﬁned context embedding, we apply a ﬁnal attention layer with a single attention head with mask mechanism.( u=otherwise.(13) Finally, the softmax is applied to get the probability distribution: The decoding proceeds until there is no enough time left and then we get the entire trip generated by the decoder S. To sum up, by maintaining a context embedding and using the representation of location from the encoder, the decoder constructs a trip with attention mechanism and meets the constraints by mask mechanism. 4.3 Policy Optimization by Adversarial Learning The next problem is how to train the encoder-decoder framework for trip generation. We devise a mobility discriminator to distinguish real-life trips taken by users between generated trips, which provides feedback to guide the optimization of the trip generator. After the evaluation between generated trips and real-life trips, the output of the discriminator can be regarded as reward signals to improve the generator. By the adversarial procedure, the generator pushes itself to generate high-quality trips to obtain high rewards from the discriminator. Mobility Discriminator The task for the discriminator essentially is binary classiﬁcation. Here we apply a simple but effective one-layer Gated Recurrent Unit (GRU) (Cho et al. 2014), followed by a two-layer feed-forward network to accomplish this task. We denote the mobility discriminator as Dand the trip generator as G, where θ and φ represent the parameters of the generator and discriminator respectively. We denote all the real-life trips as P. As a binary classiﬁcation task, we train the discriminator Das follows: maxE[log D(ˆS)]+E[log(1−D(S))] (15) Adversarial Learning with Policy Gradient We adopt the reinforcement learning technique to train the generator. The standard training algorithm for GAN does not apply to our framework: the discrete output of the trip generator blocks the gradient back-propagation, making it unable to optimize the generator (Yu et al. 2017). As described previously, the trip generation process is a sequential decision problem, leading us to tackle the problem by adopting reinforcement learning techniques. With modeling the trip generation procedure as an MDP, an important setting is to regard the score from the discriminator as reward. Thus, we deﬁne the loss as: L(S) = E[D(S)], which represents the expected score for the generated trip S given trip query q. Following REINFORCE (Williams 1992) algorithm, we optimize the loss by gradient ascent: Learning from Demonstration In order to accelerate the training process and further improve the performance, we propose a novel pre-train schema based on learning from demonstration (Silver, Bagnell, and Stentz 2010), which not only fully utilizes the data of real-life trips but also obtains a decent trip generator before adversarial learning. Learning directly from rewards is sample-inefﬁcient and hard to achieve the promising performance (Yu et al. 2017), which is also our reason to introduce the pre-train schema. During pre-training, we use real-life trips taken as ground-truth, regard choosing POI at each time step as a multi-classiﬁcation problem and optimize by softmax loss function. Nevertheless, during inference, the trip generator needs the preceding POI to select the next POI while we have no access to the true preceding POI in training, which may lead to cumulative poor decisions (Bengio et al. 2015). To bridge such a gap between training and inference, we select POI by sampling with the probability distribution (deﬁned in Equation 14) during training. Finally, the loss can be computed as: where S is the actual generated trip during training andˆS is the corresponding real-life trip. Teacher Forcing The training process is usually unstable by optimizing the generator with Equation 16 (Li et al. 2017). The reason behind this is that once the generator deteriorates in some training batches and the discriminator will recognize the unreasonable trips soon, then the generator will be lost. The generator knows the generated trips are not good based on the received rewards from the discriminator, but it does not know how to improve the quality of generated trips. To alleviate this issue and give the generator more access to real-life trips, after we update the generator with adversarial loss, we also feed the generator real-life trips and update it with supervised loss(Equation 17) again. To sum up, we ﬁrst pre-train the trip generator by leveraging demonstration data. Afterwards, we alternately update the discriminator and the generator with the respective objective. During updating the generator, we also feed real-life Dataset trips to the generator, regulating the generator from deviation from the demonstration data. 5.1 Experimental Setups Dataset We use four real-world POI check-in datasets and Table 1 summarizes the statistics of the four datasets. Foursquare(Yang et al. 2015) This real-world check-in dataset includes check-in data in New York City and Tokyo collected from Foursquare. We sort the check-ins of a user by timestamp and split them into non-overlapping trips. If the time interval between two successive check-ins is more than ﬁve hours, we split them into two trips. Map This dataset collects real-world check-ins in Beijing and Chengdu from 1 July 2019 to 30 September 2019 from an online map service provider in China. We consider the check-ins in one day as a trip for a user. We remove the trips of which length is less than 3 and we remove the POIs visited by fewer than 5 users as they are outliers in the dataset. We split the datasets in chronological order, where the former 80 % for training, the medium 10 % for validation, and the last 10 % for testing. Baselines We compare the performance of our proposed method with three state-of-the-art baselines that are designed for trip recommendation: TRAR (Gu et al. 2020) proposes the concept of attractive routes and enhances trip recommendation with attractive routes. PERSTOUR (Lim et al. 2015) personalizes the duration time for each user based on their preferences and generates trips to maximize user preference. C-ILP (He, Qi, and Ramamohanarao 2019) learns a context-aware POI embedding by integrating POI co-occurrences, user preferences and POI popularity, and transforms the problem to an integer linear programming problem. For C-ILP and PERSTOUR, we ﬁrst generate 100 candidates by using our proposed retrieval procedure because larger candidates can not be solved in a tolerable time. C-ILP and PERSTOUR both utilize lpsolve (Berkelaar et al. 2004), a linear programming package, to generate trips among the candidates, which follows their implementation. To fully validate the effectiveness of our proposed method, we introduce some baselines designed for POI recommendation. These baselines share the same trip generation procedure: they repeatedly choose the POI with the highest score among all the unvisited POIs until the time budget exhausts. The scores of POIs are produced by the corresponding model in SAE-NAD and GRU4Rec while the scores are popularity(visit frequency) of POIs in POP. POP is a naive method that measures the popularity of POIs by counting the visit frequency of POIs. SAE-NAD (Ma et al. 2018) applies a self-attentive encoder for presenting POIs and a neighbor-aware decoder for exploiting the geographical inﬂuence. GRU4Rec (Hidasi et al. 2016) models the sequential information by GRU. The implementation and hyper-parameters will be reported in the appendix. Evaluation Metrics A trip is determined by the POIs that compose the trip and the order of POIs in the trip. We evaluate these two aspects by Hit Ratio and Order-aware Precision (Huang et al. 2019) respectively. These two metrics are popularly used for trip recommendation (and planning) in previous studies. Hit Ratio. Hit Ratio (HR) is a recall-based metric, which measures how many POIs in the real trip are covered in the planned trip except the start POI: HR =. Order-aware Sequence Precision (Huang et al. 2019). Order-aware Sequence Precision (OSP) measures the order precision of overlapped part between the real trip and the generated trip except the start POI, which is deﬁned as: OSP = M/B where B is the number of all POI pairs in the overlapped part and M is the number of the pairs that contain the correct order. We give an example in the appendix. Effectiveness Table 2 shows the performance under HR and OSP metrics on the four datasets with respect to different methods. It can be observed that our proposed method consistently outperforms all the baselines with a signiﬁcant margin on all the four datasets, especially on OSP, which demonstrates that our method can recommend high-quality trips. PERSTOUR and CILP are both based on integer linear programming, which restricts them to respond in real time when the number of locations is large and affects their performance. TRAR is ill-behaved because modeling users and POIs only in category space are not enough to extract informative features to recommend reasonable trips. SAE-NAD is a strong baseline with good performance on HR while it performs poorly on OSP, which validates that conventional POI recommendation methods are not capable of being extended to support trip recommendation directly. Due to the page limit, we omit the results on Beijing and Chengdu if without speciﬁcation in the following analysis and the conclusions are similar on these two datasets, which can be found in the appendix. Efﬁciency Besides the high prediction accuracy, another advantage of our framework is its good efﬁciency that is investigated in this section. We compare the running time of ANT with trip recommendation baselines, i.e. TRAR, C-ILP and PERSTOUR. Even though ANT can be parallelized, for fair comparison we make ANT generate trips serially and we run all four methods on the same CPU device (Intel 6258R). The average running time of C-ILP and PERSTOUR on an instance both exceed one minute, while the average running time of ANT is less than 45 ms, which demonstrates the superiority of our model in efﬁciency compared to traditional CP-based models. Even though TRAR is faster than ANT Figure 4: Running time compared with baselines and running time on different numbers of candidates. Figure 5: Ablation study of each component. with the help of the greedy algorithm but TRAR’s performance is much worse than ANT, even worse than PERSTOUR and C-ILP. To further validate ANT’s availability to scale up to various numbers of candidates, we run ANT conditioned on varying numbers of candidates and show the result in Figure 4, which shows that the inference time of ANT is relatively stable with different numbers of candidates. Ablation Study To analyze the effect of each component of the ANT framework, we conduct an experimental evaluation on four variants of ANT: ANT-E, ANT-D, ANT-A, ANT-P. ANT-E means to remove the POI correlation module, ANT-D means the trip generation module is replaced with Pointer Networks (Vinyals, Fortunato, and Jaitly 2015), ANT-A means that we train the whole model only using learning from demonstration and ANT-P means we train the model only using the adversarial learning. Due to the page limit, we omit the performance under OSP, which can be found in the appendix. As can be observed in Figure 5, each component makes contributions to the ﬁnal performance. Training our model without pre-training leads to huge performance degradation, indicating the effectiveness of pre-training on stabilizing and speeding up the training process. Adversarial learning makes the model further improved based on pre-training. Also, removing the POI correlation module leads to a performance Figure 6: The impact of the number of candidates. drop, indicating the necessity of multi-head self-attention to capture the POI correlation. And compared to Pointer Networks, the well-designed context embedding for trip recommendation also shows its superiority. Impact of Candidates Here we evaluate the impact of candidates on the performance of ANT. Intuitively, when increasing the number of candidates, the target POIs have a high probability to be included in the candidate set for trip recommendation, but it also raises the difﬁculty to plan the correct trips. As we can see from Figure 6, when the number of candidates is larger than 200, the prediction performance of ANT (under HR and OSP) on the NYC dataset becomes relatively stable with the number of candidates increasing. The same phenomenon can be found on the Tokyo dataset when the number of candidates is larger than 250. Therefore, the performance is not sensitive to the number of candidates if the number is relatively large enough. In this experimental evaluation, we set the number of candidates to 200, which can be also adjusted according to different characteristics of different cities. In this paper, we investigated the trip recommendation problem by an end-to-end deep learning model. Along this line, we devised an encoder-decoder based trip generator to learn a well-formed policy to select the optimal POI at each time step by integrating POI correlation and contextual environment. Especially, we proposed a novel adversarial learning strategy integrating with reinforcement learning to train the trip generator. The extensive results on four large-scale realworld datasets demonstrate our framework could remarkably outperform the state-of-the-art baselines both on effectiveness and efﬁciency.