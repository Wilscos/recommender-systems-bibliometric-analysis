In a contextual bandit problem, a learner takes sequential actions to interact with an environment to maximize its received cumulative reward. As a natural and important variant, linear stochastic bandits (Auer, 2002; Li et al., 2010; Abbasi-yadkori et al., 2011) assume the expected reward of an arm bandit algorithm thus adaptively improves its estimation of pulled arms. Thanks to their sound theoretical guarantees and promising empirical performance, linear stochastic bandits have become a reference solution to many real-world problems, such as content recommendation and online advertisement (Li et al., 2010; Chapelle and Li, 2011; Durand et al., 2018). are susceptible to adversarial attacks, especially the data poisoning attacks. Under such an attack, a malicious adversary observes the pulled arm and its reward feedback, and then modiﬁes the reward to misguide the bandit algorithm to pull a target arm, which is of the adversary’s interest. Due to the wide applicability of bandit algorithms in practice, understanding the robustness of such algorithms under data poisoning attacks becomes increasingly important (Jun et al., 2018; Liu and Shroﬀ, 2019; Garcelon et al., 2020). et al. (2018) and Liu and Shroﬀ (2019) showed that in a context-free stochastic multi-armed bandit We study adversarial attacks on linear stochastic bandits, a sequential decision making problem with many important applications in recommender systems, online advertising, medical treatment, and etc. By manipulating the rewards, an adversary aims to control the behaviour of the bandit algorithm. Perhaps surprisingly, we ﬁrst show that some attack goals can never be achieved. This is in sharp contrast to context-free stochastic bandits, and is intrinsically due to the correlation among arms in linear stochastic bandits. Motivated by this observation, this paper studies the attackability of ak-armed linear bandit environment. We ﬁrst provide a full necessity and suﬃciency characterization of attackability based on the geometry of the context vectors. We then propose a two-stage attack method against LinUCB and Robust Phase Elimination. The method ﬁrst asserts whether the current environment is attackable, and if Yes, modiﬁes the rewards to force the algorithm to pull a target arm linear times using only a sublinear cost. Numerical experiments further validate the eﬀectiveness and cost-eﬃciency of the proposed method. ais a linear function of its feature vectorxand an unknown bandit parameterθ. A linear Since bandit algorithms adapt their behavior according to its received feedback, such algorithms Most existing studies on adversarial attacks in bandits focused on context-free settings. Jun environment, an adversary can force any bandit algorithm to pull a target arm linear times only using a logarithmic cost. Garcelon et al. (2020) studied poisoning attacks on bandits and showed its vulnerability. Linear stochastic bandits are related to context-free stochastic bandits and linear contextual bandits. But surprisingly, there is no known result about attacks on linear stochastic bandit to our best knowledge; in fact, even whether such a learning environment is attackable is unknown. It turns out that there is a reason for this gap — as we will elaborate later, the analysis of attacks to linear stochastic bandits turns out to be more challenging due to correlation among arms. stochastic bandits, where an adversary modiﬁes the reward using a sublinear budget to misguide the bandit algorithm to pull a target arm environment is not always eﬃciently attackable of ﬁnding a parameter vector reward of target arm speciﬁed by of non-target arms in the null space of feasibility of the resulting quadratically constrained linear program is both suﬃcient and necessary for attacking a linear stochastic bandit environment. stochastic bandit algorithms and demonstrate its application against LinUCB (Li et al., 2010) and Robust Phase Elimination (Bogunovic et al., 2021): the former is one of the most widely used linear contextual bandit algorithms, and the latter is a robust version designed for settings with adversarial corruptions. In the ﬁrst stage, our method collects a carefully calibrated amount of rewards on the target arm to assess whether the given environment is attackable. The decision is based on an “empirical” version of our feasibility characterization. If attackable, i.e., there exists a feasible solution non-target arms based on optimal. In this paper, we ﬁll the aforementioned gap by studying data poisoning attacks onk-armed linear Inspired by our attackability analysis, we propose a two-stage attack framework against linear ˜θ; in the second stage the method depresses the rewards the bandit algorithm receives on Our main contributions can be summarized as follows: •We characterize the suﬃcient and necessary conditions about when a linear bandit environment is attackable as the feasibility of a set of linear constraints. En route to proving the suﬃciency, we also provide an oracle attack method that can attack any no-regret learning algorithm given the knowledge of ground-truth bandit parameterθ. A direct corollary of our characterization is that context-free stochastic MAB is always attackable, recovering the ﬁndings in (Jun et al., 2018; Liu and Shroﬀ, 2019). •We propose a two-stage attack method that works without the knowledge of ground-truth bandit parameter. In the ﬁrst stage, the algorithm detects the attackability of the environment and estimates the model parameter. In the second stage, it solves for a working solution˜θ and attacks accordingly. Our theoretical analysis shows this attack method is eﬀective against LinUCB (Li et al., 2010) and Robust Phase Elimination (Bogunovic et al., 2021), i.e., pulling the target arm T −o(T ) times using o(T ) budget when the environment is attackable. Linear stochastic bandit. bandit, where a bandit algorithm sequentially interacts with an environment for round from the environment. Each arm and we assume both context feature and unknown bandit parameter assume where pseudo-regret, which is deﬁned as manipulating the reward of an arm will also change the reward estimation of other correlated arms. This is diﬀerent from the 2020), where each arm has its own bandit parameter and the reward estimation is independent among arms. Thus the reward manipulation of an arm will not aﬀect other arms. linear stochastic bandit. It estimates a bandit model parameter We use estimation on arm a sublinear upper regret bound (Chu et al., 2011; Abbasi-yadkori et al., 2011), i.e., ignoring the logarithmic term. pull the target arm 2018; Liu and Shroﬀ, 2019; Garcelon et al., 2020; Zhang et al., 2020), we consider the data poisoning attack on the rewards: after arm original reward the manipulated reward with minimum attack cost achieves a sublinear cost, i.e., is taken with respect to only the sub-Gaussian noise in the rewards. We study the attackability of a linear stochastic bandit environment. At a ﬁrst glance, one might wonder whether attackability is the property of a bandit algorithm rather than a property of the environment, since if an algorithm can be attacked, we should have “blamed” the algorithm for not being robust, rather than blaming the environment. A key ﬁnding of this work is that attackability is also a property of the linear stochastic bandit environment; and in other words, not all environments are attackable. Deﬁnition 1 stochastic bandit environment t, the algorithm pulls an armafrom a setA={x}withkarms, and receives rewardr kθk≤1. After pulling arma, the algorithm observes reward feedbackr=xθ+η, ηis anR-sub-Gaussian noise term. The performance of a bandit algorithm is evaluated by itsP , i.e.,x=arg max[xθ]. Due to the possible correlation among the context vectors, LinUCB. LinUCB (Li et al., 2010; Abbasi-yadkori et al., 2011) is a classical algorithm for Axr, whereA=xx+λIandλis the coeﬃcient of L2-regularization.√ kxk=xAxto denote the matrix norm of vectorx. Conﬁdence bound about reward −ˆθk. In each roundt, LinUCB pulls an arm with the highest upper conﬁdence bound, i.e., arg max[xˆθ+CB(x)] to balance the explore-exploit trade-oﬀ. LinUCB algorithm achieves√ Threat model. The goal of an adversary is to fool the linear stochastic bandit algorithm to ˜x ∈ A budget but fools the algorithm to pull arm ˜x at least T − o(T) times for any T large enough. about the bandit environment algorithms. Second, if an attack method can only fool a bandit algorithm to pull the target arm under (only) a few diﬀerent time horizons attack has to succeed for inﬁnitely many time horizons. Third, by reversing the order of quantiﬁers, we obtain an assertion that a bandit environment is not attackable w.r.t. the target arm exists some no-regret learning algorithm such that no attack method can use to fool the algorithm to pull arm simple yet insightful example illustrates that there are indeed linear stochastic bandit environment setups where some no-regret learning algorithm cannot be attacked. Example 1 parameter best arm in this environment. We give an intuitive argument here that this environment with target arm argue that LinUCB cannot be attacked in the above environment (our argument shall generalize to any linear-regression based no-regret algorithms). Suppose, for the sake of contradiction, that there exists an eﬃcient attack which fools LinUCB to pull θin the feedback in that the estimated parameter implying that either x’s estimated reward) for any which causes a contradiction. Therefore, we can safely conclude that this environment cannot be attacked given o(T ) budget. feasible attack strategy is to perturb reward according to null space of reward of (and therefore attackable. After all, only when we face an unattackable environment, we can ensure the existence of certain no-regret algorithms that would be immune to some particular type of adversarial attacks. if for any no-regret learning algorithm, there exists an attack method that useso(T) attacking It is worthwhile to further digest the above deﬁnition of attackability. First, this deﬁnition is all = (0,1), x= (1,2), x= (−1,2)}. Suppose the target arm˜x=xand the ground-truth bandit θ= (1,1). The expected true rewards of the arms arer= 1, r= 3, r= 1 andxis the ˜xis not attackable, while its formal proof is an instantiation of our Theorem 1. Speciﬁcally, we x’s direction almost accurately asTbecomes large, since the Ω(T) amount of true reward xdirection will ultimately dominate theo(T) adversarial contamination. This suggests Note that whenA={x, x}, the environment becomes attackable: as shown in the ﬁgure, a xfrom the environment. The presence of armxprevents the existence of such a˜θ The above example motivates us to study when a linear stochastic bandit environment is attackable. As Example 1 shows, the attackability of a bandit algorithm depends on the arm set in this section, we shall always assume that the adversary knows exactly the ground-truth bandit parameter in previous works (Jun et al., 2018; Liu and Shroﬀ, 2019; Rakhsha et al., 2020). However, in the next section, we will show that this assumption is not needed: when the bandit environment is indeed attackable, we can design provably successful attacks even if the adversary does not know the underlying bandit parameter θ denote the projection of ground-truth bandit parameter depends on the target arm environment. The following theorem provides a clean characterization of attackability. Theorem 1 attackable if and only if the optimal objective program (QCLP) satisﬁes  where  ∈ R and attacks in bandit algorithms, we would like to elaborate on its implications. Speciﬁcally, our characterization of the attackability is a property of the bandit environment but not a property of some bandit algorithms. We, for the ﬁrst time, point out some learning environment is just intrinsically vulnerable such that any no regret learning algorithm can be attacked (as later shown in our oracle attack). The signiﬁcance is that only when an environment is not attackable, i.e., QCLP (2) has optimal objective  environment. And only in this situation, if the regret is still large we can then “blame” the algorithm for not being robust enough, since by the deﬁnition of (un)attackability there does exist robust no-regret algorithms. Notably, almost all previous works have focused on the vulnerability of bandit algorithms, e.g., by designing attacks against UCB, et al., 2020), as it just so happens that they were already studied under an attackable environment (see our Corollary 1). Additionally, as we will show next, our proof techniques are also signiﬁcantly diﬀerent from existing ones, since what’s central to our proof is to argue that when QCLP not satisﬁed, then there will exist a robust algorithm that cannot be eﬃciently attacked by any adversary. This can be viewed as analyzing the robustness properties of certain bandit algorithms when  following deﬁnition for reference convenience. We now proceed to give a complete characterization about what bandit environments are {x}, the target arm˜x, and the underlying bandit parameterθ. For clarity of presentation, θand thus the true expected reward of each arm. This is also called the oracle attack We need the following convenient notation to state our results. Let Since the conceptual message of Theorem 1 signiﬁcantly diﬀers from previous studies on adversarial ≤ 0 in QCLP (2). Since QCLP(2)and its solutions will show up very often in our later discussions, we provide the Deﬁnition 2 the attackability index and the optimal solution when designing attack algorithms without the knowledge of determine how diﬃcult it is to attack the environment. Proof of Theorem 1. Proof of suﬃciency. eﬃcient attack strategy if QCLP (2) holds. the where misleads the algorithm to believe that θgenerate the same expected reward on less “suspicious”, a sub-Gaussian noise The key idea is that the attacker does not need to perturb the reward of target arm because the original reward is the same as perturbed reward in expectation. Instead, the attacker only perturbs the reward in the null space of arm Any no-regret bandit algorithm will only pull the non-target arms Notably, both the indexand certiﬁcate˜θare intrinsic to the bandit environmenthA= , θ, ˜xi, and are irrelevant to any bandit algorithms used. As we will see in the next section Suppose the attackability index>0 and let˜θbe a certiﬁcate. In Algorithm 1, we design oracle null space attackbased on the knowledge of bandit parameterθ. Let˜θ=θ+˜θ θis deﬁned in Eq(1). The adversary perturbs the reward of any non-target armx6=˜xby =x˜θ+˜η, whereas the reward of the target arm˜xis not touched. In other words, the adversary Since the perturbed rewards observed by the bandit algorithm are generated by˜θ, the target ˜xis the optimal arm in this environment due to the attackability indexbeing strictly positive. T − o the attacker only perturbs rewards on the non-target arms for each attack is bounded by a constant (because of the ﬁnite norm of x Proof of necessity. We shall prove that if (un)attackability, we will need to identify at least one no-regret bandit algorithm such that no attack strategy can succeed in attacking it. In particular, we will show that LinUCB is robust to any attack strategy with o(T ) budget when  LinUCB with the choice of L2-regularization parameter satisfy the last constraint in QCLP Suppose, for the sake of contradiction, LinUCB is attackable when 1, the target arm that the following inequalities hold when with probability 1): we have we have the following conclusion: This implies that there must exist a (2) contradicts our assumption under such an environment. such that: 1) it is orthogonal to is attackable if and only if such a gap (i.e., the attackability index) is strictly larger than 0, i.e., when the geometry of arm context vectors allows the adversary to lower non-target arms’ rewards by attacking in the null space of the same scale as the unattacked rewards. Our analysis characterizes the attackability based on the geometry of arm features: when the geometry forbids an adversary from lowering the rewards of non-target arms in the null space of the target arm, the environment is unattackable. arm set bandit environment is always attackable under Deﬁnition 1. This ﬁnding turns out to be a corollary of Theorem 1. (T) times. Thus the attack is successful. Moreover, the cost of oracle attack iso(T) because LinUCB maintains a model estimateˆθat roundtusing the attacked rewards{˜r}. We consider By attackability, we know that the above inequality will hold for inﬁnitely manyt’s. Ast → ∞, CB(˜x)→0, andCB(x) is strictly positive. Moreover, the estimation ofˆθwill converge since˜xwill be pulled fort−o(t) times. By lettingt → ∞in both sides of the above inequalities, strictly above 0. Therefore, its optimal objectivemust also be strictly positive. This however We now provide an intuitive explanation about Theorem 1. QCLP(2)is to ﬁnd a vector˜θ and the largestx(θ+˜θ) among allx6=˜x. Theorem 1 states that the bandit environment Recent works have shown that any no-regret algorithm for the context-freek-armed setting (where Ais orthonormal) can be attacked (Liu and Shroﬀ, 2019), i.e., a context-free stochastic }, θ, ˜xi is attackable for any target arm ˜x. Proof. objective value objective value is positive gives us a feasible and an adversary can lower the rewards of non-target arms to make the target arm optimal. This is also the attack strategy in (Jun et al., 2018; Liu and Shroﬀ, 2019). Garcelon et al. (Garcelon et al., 2020) showed that similar idea works for associated with an unknown bandit parameter and the reward estimations are independent among diﬀerent arms. Arguably, our setting is more challenging since arms are correlated and the simple attack idea may not be successful as shown in our Example 1. In the previous section, we characterized the attackability of a linear stochastic bandit environment by assuming the knowledge of ground-truth bandit parameter knowledge is actually not needed when designing executable attacks. Towards this end, we propose provably eﬀective attacks against two representative bandit algorithms, the most well-known LinUCB (Abbasi-yadkori et al., 2011) and a state-of-the-art robust linear stochastic bandit algorithm based on robust phase elimination (Bogunovic et al., 2021). Their diﬀerent levels of robustness lead to diﬀerent amount of required attack cost, which further illustrates that the attack analysis often goes hand-in-hand with robustness analysis. Two-stage Null Space Attack. adversary spends the ﬁrst bandit environment bandit algorithm. This stage is for the adversary to observe the rewards for which helps it estimate the parameter ˜and certiﬁcate second stage of attack. From like the oracle attack but using the estimated ˜xfor the ﬁrst time in the second stage, the adversary will compensate its reward as shown in line 19. collected in the ﬁrst stage to follow Tdepends on certain “robustness” property of the bandit algorithm in use. Consequently, it also leads to diﬀerent amount of attack cost for diﬀerent algorithms. For example, as we will show below, the attack to Robust Phase Elimination will be more costly than the attack to LinUCB. estimation error in correct with high probability (see below). We note that an important step in our attack is that the adversary manipulates the rewards for both the targeted arm and other arms in the second stage, as shown in line 21 of Algorithm 2. This is diﬀerent from the oracle attack where only the rewards of non-target arms are perturbed. This diﬀerence is crucial because it guarantees that the rewards are Since arms are orthogonal to each other, it is easy to see that˜θ=−C[x] achieves The intuition behind this corollary is that arms in context-free stochastic bandits are independent, , to perform the “attackability test” by solving QCLP(2)using˜θto obtain an estimated index ) is the number of times target arm is pulled beforeT. The goal is to correct the rewards on˜x to improve the estimate ofθ; but it also means a larger attacking cost. The optimal choice of Our attackability test might make both false positive and false negative assertions due to the (almost) always generated by does not perturb the rewards of the target arm to the bandit algorithm, these rewards could be viewed as “corrupted” — the corruption comes from the diﬀerence between rounds’ pulling of the target arm. This discrepancy may harm our attempts on lowering the bandit algorithm’s estimated rewards of non-target arms due to its correlation with the feature vectorss of other arms. Remark 1. be detected, e.g., using some homogeneity test. We acknowledge that a bandit player could realize the attack if equipped with some change detector as part of its procedure. However, attacking such a cautious bandit algorithm is beyond the scope of this paper. Moreover, it is very diﬃcult (if not impossible) to attack linear stochastic bandits with a stationary reward distribution or undetectable perturbation (using a ﬁxed target parameter parameter ˜θis limited to a few choices and it is almost impossible to directly start the attack with a valid where needs to immediately attack close to knowing detector or showing the inability to attack such cautious algorithms is an important future work of ours. Attack against LinUCB. Theorem 2. attackability with probability at least 1 probability at least 1 − 2δ the attack strategy will fool LinUCB to pull non-target arms at most rounds and the adversary’s cost is at most where the last term is due to the manipulation whenever a non-target arm is pulled at the second stage. Speciﬁcally, when Proof Sketch. estimated algorithm will pull the target arm According to Hoeﬀding’s inequality, the estimation error enough and we can correctly assert attackability with positive estimated index attackable with index  of LinUCB under the reward discrepancy between the two stages, i.e., corrupted rewards in the ﬁrst stage. Our proof crucially hinges on the following robustness property of LinUCB. Lemma 1 stochastic bandits under poisoning attack. For any where quently, the regret of LinUCB can be bounded by O(d( The total corruption is in the ﬁrst stage (the rewards of target arm are compensated in line 19). So the non-target arms are ˜θwithout knowingθ. For example, if we change the third arm in Example 1 tox= (−1 +,0) is a small constant, we can see that the valid parameters are in a small range around −1− ,1). Therefore, in order to attack with a stationary reward distribution, the adversary θ. Overall, we think designing an attack strategy for a bandit algorithm with reward change ), and the non-target arms are pulled at most˜O(T) rounds. ˜θis close to the true parameterθ. We ﬁrst note that in the ﬁrst stage, the bandit T, the error of˜x’s reward estimation is smaller than. Thus solving QCLP(2)with˜θ To prove the success and the cost of the attack, the main challenge lies at analyzing the behavior S=|∆|is the total corruption until timet, andα=d log+λ. Conse- Based on this lemma, we can derive the regretR(T) of LinUCB with˜θas the true parameter.√ pulled at most bound. the target arm in the second stage; and 3) attacks on non-target arms in the second stage. The second term is in the order of pulled by LinUCB. By setting T Remark 2. linear contextual bandit algorithm based on phase elimination in (Bogunovic et al., 2021) (which we examine next). However, the regret term et al., 2021). Attack against Robust Phase Elimination. (RobustPhE) can also be attacked by Algorithm 2. Comparing to attacking LinUCB, robustness of this algorithm brings challenge to the ﬁrst stage as attack cost is more sensitive to the length of this stage. Corollary 2. attackability with high probability. Moreover, when the environment is attackable, with probability at least 1 − δ the attack strategy will fool RobustPhE to pull non-target arms at most rounds and the adversary spends cost at most where the last term is due to the manipulation whenever a non-target arm is pulled at the second stage. Speciﬁcally, are pulled at most Sis unknown to the bandit algorithm). If we view the second stage attack model environment bandit model, rewards generated by Tamount of rewards from the ﬁrst stage means term in the cost and the number of non-target arm pulls compared with Theorem 2. Hence, the adversary can only run fewer iterations in the ﬁrst stage but spends more budget there. On the other hand, this also favors the design of attack such that line 18-19 in Algorithm 2 is not necessary: the corruption in the ﬁrst stage can be handled by the robustness of bandit algorithm. Our success of attacking RobustPhE does not violate the robustness claim in the original paper (Bogunovic et al., 2021): RobustPhE could tolerate O( The attack cost has three sources: 1) attacks in the ﬁrst stage bounded by 2T; 2) attacks on (1/T). The third term has the same order as the number of rounds non-target arms are o(T). This tolerance ofo(T) attack turns out to be the same as the recently proposed robust o(T) regime compared to theO(S) regret dependence of the robust algorithm in (Bogunovic Robust Phase Elimination has an additional regret termO(S) for total corruptionS(assuming We use simulation-based experiments to validate the eﬀectiveness and cost-eﬃciency of our proposed attack methods. In our simulations, we generate a sizeassociated with a context vector Gaussian distributions with variances sampled from a uniform distribution normalized to pool size re-sample the environment hA, θ attack, against LinUCB (Li et al., 2010) and Robust Phase Elimination (RobustPhE) (Bogunovic et al., 2021). We report average results of 10 runs where in each run we sample a random attackable environment. Both oracle attack and two-stage attack can eﬀectively fool the two bandit algorithms to pull the target arm linear times and we report this result in appendix. Figure 2 shows the total cost of the attack. We observe that both attack methods are cost-eﬃcient with sublinear total cost, while two-stage attack requires more attack budget. Speciﬁcally, we notice that the adversary spends almost linear budget in the ﬁrst stage. This is because in the ﬁrst stage the adversary attacks according to parameter stage, the cost is much smaller: the adversary only spends arm. This also corresponds to our theoretical analysis that total cost of two-stage attack is against LinUCB and cost of two-stage attack is larger than oracle attack. The key reason is that when pulling target arm, the oracle attack does not perturb the reward. We see that cost does not increase in oracle attack against LinUCB in the later stage, but the curve of two-stage attack against LinUCB keeps increase over time. We also notice that for the same attack method, attacking RobustPhE requires more budget and the target arm pull is also smaller comparing with attacking LinUCB, due to the robustness of the algorithm. and without the ground-truth bandit parameter of the experiment are described in Experiments section. We observe that both oracle attack and two-stage attack can eﬀectively fool the two bandit algorithms to pull the target arm linear times. RobustPhE will pull the target arm less often than LinUCB because of its robustness to reward k= 1. We setdto 10, the standard derivationσof Gaussian noiseηto 0.1, and the arm kto 30 in our simulations. We run the experiment forT= 10,000 iterations. We will We compare the two proposed attack methods, oracle null space attack and two-stage null space In Figure 3(a) we report the number of target arm pulls under the two attack methods (i.e., with manipulation. i.e., how often the adversary mistakenly asserts that an attackable environment is not attackable. As we explained in Proof of Theorem 1, the wrong assertion is because of using estimated of the ground-truth bandit parameter. In this experiment, we consider an attackable three-arm environment with solving QCLP test two-stage null space attack against LinUCB with attackability after the ﬁrst iterations is suﬃcient for attackability test. We report averaged results of 100 runs. We also vary the standard derivation false negative rate is almost zero when adversary only needs around 10 rounds to make a correct assertion. We also notice the false negative rate becomes higher under a larger noise scale. As suggested in Lemma 2, the error in is larger if noise scale is larger or the number of target arm’s rewards n(˜x) is smaller, which highly depends on negative assertion. However, T Adversarial attacks to bandit algorithms was ﬁrst studied in the stochastic multi-armed bandit setting (Jun et al., 2018; Liu and Shroﬀ, 2019) and recently in linear contextual bandits (Garcelon et al., 2020). These works share a similar attack idea: lowering the rewards of non-target arms while not modifying the reward of target arm. However, as our attackability analysis revealed, this idea may fail for attacking linear stochastic bandit environment since one cannot lower the rewards of non-target arms without modifying the reward of target arm due to their correlation. This insight is a key reason that gives rise to unattackable environments. Ma et al. (2018) also considered the attackability issue of linear bandits, but under the setting of oﬄine data poisoning attack where the adversary has the power to modify the rewards in history. There are also several recent works on In another experiment, we study the false negative rate of the attackability test in Algorithm 2, poisoning attacks against reinforcement learning (Yu and Sra, 2019; Zhang et al., 2020; Rakhsha et al., 2021, 2020). (2018) proposed a robust MAB algorithm and Gupta et al. (2019) further improved the solution with additive regret dependency on attack budget. They assumed a weaker oblivious adversary who determines the manipulation before the bandit algorithm pulls an arm. Hajiesmaili et al. (2020) studied robust adversarial bandit algorithm. Bogunovic et al. (2021) proposed robust phase elimination algorithm for linear stochastic bandits under a stronger adversary (same as ours), which could tolerate that our two-stage null space attack could eﬀectively attack this algorithm with In this paper, we studied the problem of data poisoning attacks in with the goal of forcing the bandit algorithm to pull the target arm linear times using a sublinear budget. Diﬀerent from context-free stochastic bandits and the environment is always attackable, we showed that some linear stochastic bandit environment is not attackable due to the correlation among arms. We characterized the attackability condition as the feasibility of a linear program based on the geometry of the arm features. Our key insight is that giving the ground-truth parameter the reward of non-target arms in the null space of target arm’s feature proposed a two-stage null space attack without the knowledge of and Robust Phase Elimination. We showed that the proposed attack methods are eﬀective and cost-eﬃcient, both theoretically and empirically. As future work, it is interesting to study the lower bound of attack cost in linear stochastic bandits and also design cost-optimal attack method with a matching upper bound. This work was supported in part by National Science Foundation Grant IIS-2128019, IIS-1618948, IIS-1553568, Bloomberg Data Science Ph.D. Fellowship, and a Google Faculty Research Award. A parallel line of works focused on improving the robustness of bandit algorithms. Lykouris et al.