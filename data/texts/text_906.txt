Noname manuscript No. (will be inserted by the editor) Abstract Scientiﬁc writing builds upon already published papers. Manual identiﬁcation of publications to read, cite or consider as related papers relies on a researcher’s ability to identify ﬁtting keywords or initial papers from which a literature search can be started. The rapidly increasing amount of papers has called for automatic measures to ﬁnd the desired relevant publications, so-called paper recommendation systems. As the number of publications increases so does the amount of paper recommendation systems. Former literature reviews focused on discussing the general landscape of approaches throughout the years and highlight the main directions. We refrain from this perspective, instead we only consider a comparatively small time frame but analyse it fully. In this literature review we discuss used methods, datasets, evaluations and open challenges encountered in all works ﬁrst released between January 2019 and October 2021. The goal of this survey is to provide a comprehensive and complete overview of current paper recommendation systems. Keywords Paper Recommendation System · Publication Suggestion · Literature Review The rapidly increasing number of publications leads to a large quantity of possibly relevant papers [6] for more speciﬁc tasks such as ﬁnding related papers [26], ﬁnding ones to read [104] or literature search in general to inspire new directions and understand the state-of-theart approaches [43]. Overall researchers typically spend a large amount of time on searching for relevant related work [7]. Keyword based search options are insufﬁcient to ﬁnd relevant papers [9,49,104], they require some form of initial knowledge about a ﬁeld. Oftentimes, users’ information needs are not explicitly speciﬁed [53] which impedes this task further. To close this gap, a plethora of paper recommendation systems have been proposed recently [34,36,83,99, 112]. These systems should fulﬁl diﬀerent functions: for junior researchers systems should recommend a broad variety of papers, for senior ones the recommendations should align more with their already established interests [9] or help them discover relevant interdisciplinary research [95]. In general paper recommendation approaches positively aﬀect researchers’ professional lives as they enable ﬁnding relevant literature more likely and faster [47]. As there are many diﬀerent approaches, their objectives and assumptions are also diverse. A simple problem deﬁnition of a paper recommendation system could be the following: given one paper recommend a list of papers ﬁtting the source paper [64]. This deﬁnition would not ﬁt all approaches as some speciﬁcally do not require any initial paper to be speciﬁed but instead observe a user as input [34]. Some systems recommend sets of publications ﬁtting the queried terms only if they are observed together [57,58], most of the approaches suggest a number of single publications as their result [34, 36,83,112]. Most approaches assume that all required data to run a system to be present already [34,112] but some works [36,83] explicitly crawl general publication information or even abstracts and keywords from the web. In this literature review we observe papers recently published in the area of scientiﬁc paper recommendation between and including January 2019 and October 2021. We strive to give comprehensive overviews on their utilised methods as well as their datasets, evaluation measures and open challenges of current approaches. Our contribution is 4-fold: – We propose a novel multidimensional characterisation of current paper recommendation approaches. – We compile a list of recently used datasets in evaluations of paper recommendation approaches. – We compile a list of recently used evaluation measures for paper recommendation. – We analyse existing open challenges and identify current novel problems in paper recommendation which could be speciﬁcally helpful for future approaches to address. In the following Section 2 we describe the general problem statement for paper recommendation systems before we dive into the literature review in Section 3. Section 4 gives insight into datasets used in current work. In the following Section 5 diﬀerent deﬁnitions of relevance, relevance assessment as well as evaluation measures are analysed. Open challenges and objectives are discussed in detail in Section 6. Lastly Section 7 concludes this literature review. Over the years diﬀerent formulations for a problem statement of a paper recommendation system have emerged. In general they should specify the input for the recommendation system, the type of recommendation results, the point in time when the recommendation will be made and which speciﬁc goal an approach tries to achieve. Additionally, the target audience should be speciﬁed. As input we can either specify an initial paper [26], keywords [112], a user [34], a user and a paper [5] or more complex information such as user-constructed knowledge graphs [104]. Users can be modelled as a combination of features of papers they interacted with [17, 19], e.g. their clicked [24] or authored publications [20]. Papers can for example be represented by their textual content [83]. As types of recommendation we could either specify single (independent) papers [34] or a set of paper which is to be observed in its full form [58]. A study by Beierle et al. [16] found that existing digital libraries recommend between three and ten single papers, in their case the optimal number of suggestions to display users was ﬁve to six. As for the point in time, most work focuses on immediate recommendation of papers. Only few approaches also consider delayed suggestion via newsletter for example [53]. In general, recommended papers should be relevant in one way or another to achieve certain goals. They could e.g. be related to an initial paper [26] or publications which should be read [104]. Diﬀerent target audiences for example junior or senior researcher have diﬀerent demands from paper recommendation systems [9]. Usually paper recommendation approaches target single users but there are also works which strive to recommend papers for sets of users [105,106]. In this chapter we ﬁrst clearly deﬁne the scope of of our literature review (see Sect. 3.1) before we conduct a meta analysis on the observed papers (see Sect. 3.2). Afterwards our categorisation or lack thereof is discussed in depth (see Sect. 3.3), before we give short overviews of all paper recommendation systems we found (see Sect. 3.4) and some other relevant related work (see Sect. 3.5). To the best of our knowledge the literature reviews by Bai et al. [9], Li and Zou [55] and Shahid et al. [87] are the most recent ones targeting the domain of scientiﬁc paper recommendation systems. They were accepted for publication or published in 2019 so they only consider paper recommendation systems up until 2019 at most. We want to bridge the gap between papers published after their surveys were ﬁnalised and current work so we only focus on the discussion of publications which appeared between January 2019 and October 2021 when this literature search was conducted. We conducted our literature search on the following digital libraries: ACM, dblp, GoogleScholarand Springer. Titles of considered publications had to contain either paper, article or publication as well as some form of recommend. Papers had to be written in English to be observed. We judged relevance of retrieved publications by observing titles and abstracts if the title alone did not suﬃce to assess their topical relevance. In addition to these papers found by systematically searching digital libraries, we also considered their referenced publications if they were from the speciﬁed time period and of topical ﬁt. For all papers their date of ﬁrst publication determines their publication year. E.g. for journal articles we consider the point in time when they were ﬁrst published online instead of the data on which they were published in an issue, for conference articles we consider the date of the conference instead a later date when they were published online. We refrain from including works in our study which do not identify as scientiﬁc paper recommendation systems such as Wikipedia article recommendation [66, 74,80] or general news article recommendation [29,40, 98]. Citation recommendation systems [68,85,118] are also out of scope of this literature review. Even though citation and paper recommendation can be regarded analogously [42] we argue the diﬀering functions of citations [31] and tasks of these recommendation systems [63] should not be mixed with the problem of paper recommendation. We also consciously refrain from discussing the plethora of more area-independent recommender systems which could be adopted to the domain of scientiﬁc paper recommendation. Our literature research resulted in 76 relevant papers. We found 12 manuscripts which do not present paper recommendation systems but are relevant works for the area nonetheless, they are discussed in Section 3.5. This left 64 publications describing paper recommendation systems for us to analyse in the following. For papers within our scope we consider their publication year as stated in the citation information. Of the 64 relevant system papers, 21 were published in 2019, 23 were published in 2020 and 20 were published in 2021. On average each paper has 4.0625 authors (std. dev.=1.7036) and 12.5781 pages (std. dev.=9.2192). 34 of the papers appeared as conference papers, 27 papers were published in journals and there were two preprints which have not yet been published otherwise. There has been one master’s thesis within scope. The most common venues for publications were the ones depicted in Table 1. Some papers [70,71,72,88,89] described the same approach without modiﬁcation or extension of the actual paper recommendation methodology e.g. by providing evaluations. This left us with 61 diﬀerent paper recommendation systems to discuss. The already mentioned three most recent [9,55,87] and one older but highly inﬂuential [14] literature reviews in scientiﬁc paper recommendation utilise diﬀerent categorisations to group approaches. Beel et al. [14] categorise observed papers by their underlying recommendation principle in stereotyping, content-based ﬁltering, collaborative ﬁltering, co-occurrence, graph based, global relevance and hybrid models. Bai et al. [9] only utilise the classes content-based ﬁltering, collaborative ﬁltering, graph-based methods, hybrid methods and other models. Li and Zou [55] use the categories contentbased recommendation, hybrid recommendation, graphbased recommendation and recommendation based on deep learning. Shahid et al. [87] label approaches by the criterion they identify relevant papers with: content, metadata, collaborative ﬁltering and citations. The four predominant categories thus are contentbased ﬁltering, collaborative ﬁltering, graph-based and hybrid systems. Most of these categories are deﬁned sharply but graph-based approaches are not always characterised concisely: Content-based ﬁltering (CBF) methods are said to be ones where user interest is inferred by observing their historic interactions with papers [9, 14,55]. Recommendations are composed by observing features of papers and users [5]. In collaborative ﬁltering (CF) systems the preferences of users similar to a current one are observed to identify likely relevant publications [9,14,55]. Current users’ past interactions need to be similar to similar users’ past interactions [9, 14]. Hybrid approaches are ones which combine multiple types of recommendations [9,14,55]. Graph-based methods can be characterised in multiple ways. A very narrow deﬁnition only encompasses ones which observe the recommendation task as a link prediction problem or utilise random walk [5]. Another less strict deﬁnition identiﬁes these systems as ones which construct networks of papers and authors and then apply some graph algorithm to estimate relevance [9]. Another deﬁnition speciﬁes this class as one using graph metrics such as random walk with restart, bibliographic coupling or co-citation inverse document frequency [101]. Li and Zhou [55] abstain from clearly characterising this type of systems directly but give examples which hint that in their understanding of graph-based methods somewhere in the recommendation process, some type of graph information e.g. bibliographic coupling or co-citation strength, should be used. Beel et al. [14] as well as Bai et al. [9] follow a similar line, they characterise graph-based methods broadly as ones which build upon the existing connections in a scientiﬁc context to construct a graph network. When trying to classify approaches by their recommendation type, we encountered some problems: 1. We have to refrain from only utilising the labels the works give themselves (see Table 2 for an overview of self-labels of works which do classify themselves). Works do not necessarily (clearly) state, which category they belong to [26,46,57]. Another problem with self-labelling is authors’ individual deﬁnitions of categories while disregarding all possible ones (as e.g. seen with Afsar et al. [1] or Ali et al. [5]). Misdeﬁnition or omitting of categories could lead to an incorrect classiﬁcation. 2. When considering the broadest deﬁnition of graphbases methods many recent paper recommendation systems tend to belong to the class of hybrid methods. Most of the approaches [5,43,45,46,54,83,100, 112] utilise some type of graph structure information as part of the approach which would classify them as graph-based but as they also utilise historic user-interaction data or descriptions of paper features (see e.g. Li et al. [54] who describe their approach as network-based while using a graph structure, textual components and user proﬁles) which would render them as either CF or CBF also. Thus we argue the former categories do not suﬃce to classify the particularities of current approaches in a meaningful way. So instead, we introduce more dimensions by which systems could be grouped. Recent paper recommendation systems can be categorised in 20 diﬀerent dimensions by data and methods, which are part of the approach: – Personalisation (Personal.): The approach produces personalised recommendations. – Input: The approach requires some form of input, either a paper (p), keywords (k), user (u) or something else, e.g. an advanced type of input (o). Hybrid forms are also possible. In some cases the input is not clearly speciﬁed throughout the paper so it is unknown (?). – Title: The approach utilises titles of papers. – Abstract: The approach utilises abstracts of papers. – Keyword: The approach utilises keywords of papers. – Text: The approach utilises some type of text of papers which is not clearly speciﬁed as titles, abstracts or keywords. In the evaluation this approach might utilise speciﬁed text fragments of publications. – Citation: The approach utilises citation information, e.g. numbers of citations or co-references. – Historic interaction (interaction): The approach uses some sort of historic user-interaction data, e.g. previously authored, cited or liked publications. An approach can only include historic user-interaction data if it also somehow contains user proﬁles. – User proﬁle (user): The approach constructs some sort of user proﬁle or utilises proﬁle information. – Popularity: The approach utilises some sort of popularity indication, e.g. CORE rank, numbers of citations or number of likes. – Key phrase: The approach utilises key phrases. – Embedding: The approach utilises some sort of text or graph embedding technique, e.g. BERT or Doc2Vec. – Topic model (TM): The approach utilises some sort of topic model, e.g. LDA. – Knowledge graph (KG): The approach utilises or builds some sort of knowledge graph. – Graph: The approach actively builds or directly uses a graph structure, e.g. a knowledge graph or scientiﬁc heterogeneous network. Utilisation of a neural network is not considered in this dimension. – Meta-path (Path): The approach utilises meta-paths. – Random Walk (with Restart) (RW(WR)): The approach utilises Random Walk or Random Walk with Restart. – Advanced machine learning (AML): The approach utilises some sort of advanced machine learning component in its core such as a neural network. Utilisation of established embedding methods which themselves use neural networks (e.g. BERT) are not considered in this dimension. – Crawling: The approach conducts some sort of web crawling step. – Cosine similarity (cosine): The approach utilises cosine similarity at some point. Of the observed paper recommendation systems, six were general systems or methods which were only applied on the domain of paper recommendation [3,4,22, 57,113,115]. Two were targeting explicit set-based recommendation of publications where only all papers in the set together satisfy users’ information needs [57, 58], two recommend multiple papers [39,67] (e.g. on a path [39]), all the other approaches focused on recommendation of k single papers. Only two approaches focus on recommendation of papers to user groups instead of single users [105,106]. Only one paper [53] supports subscription-based recommendation of papers, all other approaches solely regarded a scenario in which papers were suggested straight away. Table 3 classiﬁes the observed approaches according to the afore discussed dimensions. The 64 relevant works identiﬁed in our literature search are described in this section. We deliberately refrain from trying to structure the section by classifying papers by an arbitrary dimension and instead point to Table 3 to identify those dimensions in which a reader is interested to navigate the following short descriptions. The works are ordered by the surname of the ﬁrst author and ascending publication year. An exception to this rule are papers presenting extensions of previous approaches with diﬀerent ﬁrst authors. These papers are ordered to their preceding approaches. Afsar et al. [1] propose KERS, a multi armed bandit approach for patients to help with medical treatment decision making. It consists of two phases: ﬁrst an exploration phase identiﬁes categories users are implicitly interested in. This is supported by an expertbuilt knowledge base. Afterwards an exploitation phase takes place where articles from these categories are recommended until a user’s focus changes and another exploitation phase is initiated. The authors strive to minimise the exploration eﬀorts while maximising users’ satisfaction. Ahmedi et al. [3] propose a personalised approach which can also be applied to more general recommendation scenarios which include user proﬁles. They utilise Collaborative Topic Regression to mine association rules from historic user interaction data. Alfarhood and Cheng [4] introduce Collaborative Attentive Autoencoder, a deep learning-based model for general recommendation targeting the data sparsity problem. They apply probabilistic matrix factorisation while also utilising textual information to train a model which identiﬁes latent factors in users and papers. Ali et al. [5] construct PR-HNE, a personalised probabilistic paper recommendation model based on a joint representation of authors and publications. They utilise graph information such as citations as well as co-authorships, venue information and topical relevance to suggest papers. They apply SBERT and LDA to represent author embeddings and topic embeddings respectively. Bereczki [17] models users and papers in a bipartite graph. Papers are represented by their contents’ Word2Vec or BERT embeddings, users’ vectors consist of representations of papers they interacted with. These vectors are then aggregated with simple graph convolution. Bulut et al. [20] focus on current user interest in their approach which utilises k-Means and KNN. Users’ proﬁles are constructed from their authored papers. Recommended papers are the highest cited ones from the cluster most similar to a user. In a subsequent work they extended their research group to again work in the same domain. Bulut et al. [19] again focus on users’ features. They represent users as the sum of features of their papers. These representations are then compared with all papers’ vector representations to ﬁnd the most similar ones. Papers can be represented by TF-IDF, Word2Vec or Doc2Vec vectors. Chaudhuri et al. [23] use indirect features derived from direct features of papers in addition to direct ones in their paper recommendation approach: keyword diversiﬁcation, text complexity and citation analysis. In an extended group Chaudhuri et al. [24] later propose usage of more indirect features such as quality in paper recommendation. Users’ proﬁles are composed of their clicked papers. Subsequently they again worked on an approach in the same area but in a slightly smaller group. Chaudhuri et al. [22] propose the general Hybrid Topic Model and apply it on paper recommendation. It learns users’ preferences and intentions by combining LDA and Word2Vec. They compute user’s interest from probability distributions of words of clicked papers and dominant topics in publications. Chen and Ban [25] introduce CPM, a recommendation model based on topically clustered user interests mined from their published papers. They derive user need models from these clusters by using LDA and pattern equivalence class mining. Candidate papers are then ranked against the user need models to identify the best-ﬁtting suggestions. Collins and Beel [26] propose the usage of their paper recommendation system Mr. DLib as a recommender as-a-service. They compare representing papers via Doc2Vec with a key phrase-based recommender and TF-IDF vectors. Du et al. [27] introduce HNPR, a heterogeneous network method using two diﬀerent graphs. The approach incorporates citation information, co-author relations and research areas of publications. They apply random walk on the networks to generate vector representations of papers. Du et al. [28] propose Polar++, a personalised active one-shot learning-based paper recommendation system where new users are presented articles to vote on before they obtain recommendations. The model trains a neural network by incorporating a matching score between a query article and the recommended articles as well as a personalisation score dependant on the user. Guo et al. [34] recommend publications based on papers initially liked by a user. They learn semantics between titles and abstracts of papers on word- and sentence-level, e.g. with Word2Vec and LSTMs to represent user preferences. Habib and Afzal [35] crawl full texts of papers from CiteSeer. They then apply bibliographic coupling between input papers and a clusters of candidate papers to identify the most relevant recommendations. In a subsequent work Afzal again used a similar technique. Ahmad and Afzal [2] crawled papers from CiteSeerX. Cosine similarity of TF-IDF representations of key terms from titles and abstracts is combined with co-citation strength of paper pairs. This combined score then ranks the most relevant papers the highest. Haruna et al. [36] incorporate paper-citation relations combined with contents of titles and abstracts of papers to recommend the most ﬁtting publications for an input query corresponding to a paper. Hu et al. [38] present ADRCR, a paper recommendation approach incorporating author-author and authorpaper citation relationships as well as authors’ and papers’ authoritativeness. A network is built which uses citation information as weights. Matrix decomposition helps learning the model. Hua et al. [39] propose PAPR which recommends relevant paper sets as an ordered path. They strive to overcome recommendation merely based on similarity by observing topics in papers changing over time. They combine similarities of TF-IDF paper representations with random-walk on diﬀerent scientiﬁc networks. Jing and Yu [41] build a three-layer graph model which they traverse with random-walk with restart in an algorithm named PAFRWR. The graph model consists of one layer with citations between papers’ textual content represented via Word2Vec vectors, another layer modelling co-authorships between authors and the third layer encodes relationships between papers and topics contained in them. Kanakia et al. [42] build their approach upon the MAG dataset and strive to overcome the common problems of scalability and cold-start. They combine TFIDF and Word2Vec representations of the content with co-citations of papers to compute recommendations. Speedup is achieved by comparing papers to clusters of papers instead of all other single papers. Kang et al. [43] crawl full texts of papers from CiteSeer and construct citation graphs to determine candidate papers. Then they compute a combination of section-based citation and key phrase similarity to rank recommendations. Kong et al. [45] present VOPRec, a model combining textual components in form of Doc2vec and Paper2Vec paper representations with citation network information in form of Struc2vec. Those networks of papers connect the most similar publications based on text and structure. Random walk on these graphs contributes to the goal of learning vector representations. L et al. [46] base their recommendation on lately accessed papers of users as they assume future accessed papers are similar to recently seen ones. They utilise a sliding window to generate sequences of papers, on those they construct a GNN to aggregate neighbouring papers to identify users’ interests. Li et al. [53] introduce a subscription-based approach which learns a mapping between users’ browsing history and their clicks in the recommendation mails. They learn a re-ranking of paper recommendations by using its metadata, recency, word representations and entity representations by knowledge graphs as input for a neural network. Their deﬁned target audience are new users. Li et al. [52] present HNTA a paper recommendation method utilising heterogeneous networks and changing user interests. Paper similarities are calculated with Word2Vec representations of words recommended for each paper. Changing user interest is modelled with help of an exponential time decay function on word vectors. Li et al. [54] utilise user proﬁles with a history of preferences to construct heterogeneous networks where they apply random walks on meta-paths to learn personalised weights. They strive to discover user preference patterns and model preferences of users as their recently cited papers. Lin et al. [56] utilise authors’ citations and years they have been publishing papers in their recommendation approach. All candidate publications are matched against user-entered keywords, the two factors of authors of these candidate publications are combined to identify the overall top recommendations. Liu et al. [57] explicitly do not require all recommended publications to ﬁt the query of a user perfectly. Instead they state the set of recommended papers fulﬁls the information need only in the complete form. Here they treat paper recommendation as a link prediction problem incorporating publishing time, keywords and author inﬂuence. In a subsequent work, part of the previous research group again observes the same problem. In this work Liu et al. [58] propose an approach utilising numbers of citations (author popularity) and relationships between publications in an undirected citation graph. They compute Steiner trees to identify the sets of papers to recommend. Lu et al. [59] propose TGMF-FMLP, a paper recommendation approach focusing on the changing preferences of users and novelty of papers. They combine category attributes (such as paper type, publisher or journal), a time-decay function, Doc2Vec representations of the papers’ content and a specialised matrix factorisation to compute recommendations. Ma et al. [61] introduce HIPRec, a paper recommendation approach on heterogeneous networks of authors, papers, venues and topics specialised on new publications. They use the most interesting meta-paths to construct signiﬁcant meta-paths. With these paths and features from these paths they train a model to identify new papers ﬁtting users. Together with another researcher Ma further pursued this research direction. Ma and Wang [60] propose HGRec, a heterogeneous graph representation learning-based model working on the same network. They use meta-path-based features and Doc2Vec paper embeddings to learn the node embeddings in the network. Manju et al. [30] attempt to solve the cold-start problem with their paper recommendation approach coding social interactions as well as topical relevance into a heterogeneous graph. They incorporate believe propagation into the network and compute recommendations by applying random walk. Mohamed Hassan et al. [65] adopt an existing tag prediction model which relies on a hierarchical attention network to capture semantics of papers. Matrix factorisation then identiﬁes the publications to recommend. Nair et al. [67] propose C-SAR, a paper recommendation approach using a neural network. They input GloVe embeddings of paper titles into their Gated Recurrent Union model to compute probabilities of similarities of papers. The resulting adjacency matrix is input to an association rule mining a priori algorithm which generates the set of recommendations. Nishioka et al. [70,71] state serendipity of recommendations as their main objective. They incorporate users’ tweets to construct proﬁles in hopes to model recent interests and developments which did not yet manifest in users’ papers. They strive to diversity the list of recommended papers. In more recent work Nishioka et al. [72] explained their evaluation more in depth. Rahdari and Brusilovsky [79] observe paper recommendation for participants of scientiﬁc conferences. Users’ proﬁles are composed of their past publications. Users control the impact of features such as publication similarity, popularity of papers and its authors to inﬂuence the ordering of their suggestions. Renuka et al. [81] propose a paper recommendation approach utilising TF-IDF representations of automatically extracted keywords and key phrases. They then either use cosine similarity between vectors or a clustering method to identify the most similar papers for an input paper. Sakib et al. [84] present a paper recommendation approach utilising second-level citation information and citation context. They strive to not rely on user proﬁles in the paper recommendation process. Instead they measure similarity of candidate papers to an input paper based on co-occurred or co-occurring papers. In a follow-up work with a bigger research group Sakib et al. [83] combine contents of titles, keywords and abstracts with their previously mentioned collaborative ﬁltering approach. They again utilise second-level citation relationships between papers to ﬁnd correlated publications. Shahid et al. [89] utilise in-text citation frequencies and assume a reference is more important to a referencing paper the more often it occurs in the text. They crawl papers from CiteSeerX to retrieve the top 500 citing papers. In a follow-up work with a partially diﬀerent research group Shahid et al. [88] evaluate the previously presented approach with a user study. Sharma et al. [90] propose IBM PARSe, a paper recommendation system for the medical domain to reduce the number of papers to review for keeping an existing knowledge graph up-to-date. Classiﬁers identify new papers from target domains, named entity recognition ﬁnds relevant medical concepts before papers’ TF-IDF vectors are compared to ones in the knowledge graph. New publications most similar to already relevant ones with matching entities are recommended to be included in the knowledge base. Subathra and Kumar [93] constructed an paper recommendation system which applies LDA on Wikipedia articles twice. Top related words are computed using pointwise mutual information before papers are recommended for these top words. Tang et al. [99] introduce CGPrec, a content-based and knowledge graph-based paper recommendation system. They focus on users’ sparse interaction history with papers and strive to predict papers on which users are likely to click. They utilise Word2Vec and a Double Convolutional Neural Network to emulate users’ preferences directly from paper content as well as indirectly by using knowledge graphs. Tanner et al. [101] consider relevance and strength of citation relations to weigh the citation network. They fetch citation information from the parsed full texts of papers. On the weighted citation networks they run either weighted co-citation inverse document frequency, weighted bibliographic coupling or random walk with restart to identify the highest scoring papers. Tao et al. [102] use embeddings and topic modelling to compute paper recommendations. They combine LDA and Word2Vec to obtain topic embeddings. Then they calculate most similar topics for all papers using Doc2Vec vector representations and afterwards identify the most similar papers. With PageRank on the citation network they re-rank these candidate papers. Waheed et al. [103] propose CNRN, a recommendation approach using a multilevel citation and authorship network to identify recommendation candidates. From these candidate papers ones to recommend are chosen by combining centrality measures and authors’ popularity. Highly correlated but unrelated Shi et al. [91] present AMHG, an approach utilising a multilayer perceptron. They also construct a multilevel citation network as described before with added author relations. Here they additionally utilise vector representations of publications and recency. Wang et al. [108] introduce a knowledge-aware path recurrent network model. An LSTM mines path information from the knowledge graphs incorporating papers and users. Users are represented by their downloaded, collected and browsed papers, papers are represented by TF-IDF representations of their keywords. Wang et al. [104] require users to construct knowledge graphs to specify the domain(s) and enter keywords for which recommended papers are suggested. From the keywords they compute initially selected papers. They apply Doc2Vec and emotion-weighted similarity between papers to identify recommendations. Wang et al. [105] regard paper recommendation targeting a group of people instead of single users and introduce GPRAH ER. They employ a two-step process which ﬁrst individually predicts papers for users in the group before recommended papers are aggregated. Here users in the group are not considered equal, diﬀerent importance and reliability weights are assigned such that important persons’ preferences are more decisive of the recommended papers. Together with a diﬀerent research group two authors again pursued this deﬁnition of the paper recommendation problem. Wang et al. [106] recommend papers for groups of users in an approach called GPMF ER. As with the previous approach they compute TF-IDF vectors of keywords of papers to calculate most similar publications for each user. Probabilistic matrix factorisation is used to integrate these similarities in a model such that predictive ratings of all users and papers can be obtained. In the aggregation phase the number of papers read by a user is determined to replace the importance component. Xie et al. [111] propose JTIE, an approach incorporating contents, authors and venues of papers to learn paper embeddings. Further, directed citation relations are included into the model. Based on users’ authored and referenced papers personalised recommendations are computed. They consider explainability of recommendations. In a subsequent work part of the researchers again work on this topic. Xie et al. [110] specify on recommendation of papers from diﬀerent areas for userprovided keywords or papers. They use hierarchical LDA to model evolving concepts of papers and citations as evidence of correlation in their approach. Yang et al. [112] incorporate the age of papers and impact factors of venues as weights in their citation network-based approach named PubTeller. Papers are clustered by topic, the most popular ones from the clusters most similar to the query terms are recommendation candidates. In this approach, LDA and TF-IDF are used to represent publications. Yu et al. [113] propose ICMN, a general collaborative memory network approach.User and item embeddings are composed by incorporating papers’ neighbourhoods and users’ implicit preferences. Zhang et al. [115] propose W-Rank, a general approach weighting edges in a heterogeneous author, paper and venue graph by incorporating citation relevance and author contribution. They apply their method on paper recommendation. Network- (via citations) and semantic-based (via AWD) similarity between papers is combined for weighting edges between papers, harmonic counting deﬁnes weights of edges between authors and papers. A HITS-inspired algorithm computes the ﬁnal authority scores. In a subsequent work in a slightly smaller group they focus on a specialised approach for paper recommendation. Here Zhang et al. [116] strive to emulate a human expert recommending papers. They construct a heterogeneous network with authors, papers, venues and citations. Citation weights are determined by semantic- and network-level similarity of papers. Lastly, recommendation candidates are re-ranked while combining the weighted heterogeneous network and recency of papers. Zhao et al. [117] present a personalised approach focusing on diversity of results which consists of three parts. First LFM extracts latent factor vectors of papers and users from the users’ interactions history with papers. Then BERT vectors are constructed for each word of the papers, with those vectors as input and the latent factor vectors as label a BiGRU model is trained. Lastly, diversity and a user’s rating weights determine the ranking of recommended publications for the speciﬁc user. We now brieﬂy discuss some papers which did not present novel paper recommendation approaches but are relevant in the scope of this literature review nonetheless. Here we present two works which could be classiﬁed as ones to use on top of or in combination with existing paper recommendation systems: Lee et al. [48] introduce LIMEADE, a general approach for opaque recommendation systems which can for example be applied on any paper recommendation system. They produce explanations for recommendations as a list of weighted interpretable features such as inﬂuential paper terms. Beierle et al. [16] use the recommendation-as-a-service provider Mr. DLib to analyse choice overload in user evaluations. They report several click-based measures and discuss eﬀects of diﬀerent study parameters on engagement of users. The following three works can be grouped as ones which provide (r)evaluations of already existing approaches. Their results could be useful for the construction of novel systems: Ostendorﬀ [73] suggests considering the context of paper similarity in background, methodology and ﬁndings sections instead of undiﬀerentiated textual similarity for scientiﬁc paper recommendation. Mohamed Hassan et al. [64] compare diﬀerent text embedding methods such as BERT, ELMo, USE and InferSent to express semantics of papers. They perform paper recommendation and re-ranking of recommendation candidates based on cosine similarity of titles. Le et al. [47] evaluate the already existing paper recommendation system Mendeley Suggest, which provides recommendations with diﬀerent collaborative or content-based approaches. They observe diﬀerent usage behaviours and state utilisation of paper recommendation systems does positively eﬀect users’ professional lives. Living labs help researchers conduct meaningful evaluations by providing an environment, in which recommendations produced by experimental systems are shown to real users in realistic scenarios [12]. We found three relevant works for the area of scientiﬁc paper recommendation: Beel et al. [12] proposed a living lab for scholarly recommendation built on top of Mr. DLib, their recommender-as-a-service system. They log users’ actions such as clicks, downloads and purchases for related recommended papers. Additionally, they plan to extend their living lab to also incorporate research grant or research collaborator recommendation. Gingstad et al. [33] propose ArXivDigest, an online living lab for explainable and personalised paper recommendations from arXiv. Users can either be suggested papers while browsing their website or via email as a subscription-type service. Diﬀerent approaches can be hooked into ArXivDigest, the recommendations generated by them can then be evaluated by users. A simple text-based baseline compares user-input topics with articles. Target values of evaluations are users’ clicked and saved papers. Schaer et al. [86] held the Living Labs for Academic Search (LiLAS) where they hosted two shared tasks: dataset recommendation for scientiﬁc papers and adhoc multi-lingual retrieval of most relevant publications regarding speciﬁc queries. To overcome the gap between real-world and lab-based evaluations they allowed integrating participants’ systems into real-world academic search systems, namely LIVIO and GESIS Search. The previous survey by Li and Zhou [55] identiﬁes crosslanguage paper recommendation as a future research direction. The following two works could be useful for this aspect: Keller and Munz [44] present their results of participating on the CLEF LiLAS challenge where they tackled recommendation of multilingual papers based on queries. They utilised a pre-computed ranking approach, Solr and pseudo-relevance feedback to extend queries and identify ﬁtting papers. Safaryan et al. [82] compare diﬀerent already existing techniques for cross-language recommendation of publications. They compare word by word translation, linear projection from a Russian to an English vector representation, VecMap alignment and MUSE word embeddings. Some recommendation approaches are slightly out of scope of pure paper recommendation systems but could still provide inspiration or relevant results: Ng [69] proposes CBRec, a children’s book recommendation system utilising matrix factorisation. His goal is to encourage good reading habits of children. The approach combines readability levels of users and books with TF-IDF representations of books to ﬁnd ones which are similar to ones which a child may have already liked. Patra et al. [75] recommend publications relevant for datasets to increase reusability. Those papers could describe the dataset, use it or be related literature. The authors represent datasets and articles as vectors and use cosine similarity to identify the best ﬁtting papers. Re-ranking them with usage of Word2Vec embeddings results in the ﬁnal recommendation. As the discussed paper recommendation systems utilise diﬀerent inputs or components of scientiﬁc publications and pursue slightly diﬀerent objectives, datasets to experiment on are also of diverse nature. We do not consider datasets of approaches which do not evaluate [57] or do not evaluate the actual paper recommendation [2, 23,35,79,81]. We also do not discuss datasets where only the data sources are mentioned but no remarks are made regarding the size or composition of the dataset [19, 99] or ones where we were not able to identify actual numbers [30]. Table 4 gives an overview of datasets used in the evaluation of the considered discussed methods. Many of the datasets are unavailable only few years after publication of the approach. Most approaches utilise their own modiﬁed version of a public dataset which makes exact replication of experiments hard. In the following the main underlying data sources and publicly available datasets are discussed. Non-publicly available datasets are brieﬂy described in Table 5. The dblp computer science bibliography (dblp) is a digital library oﬀering metadata on authors, papers and venues from the area of computer science and adjacent ﬁelds [51]. They provide publicly available shorttime stored daily and longer-time stored monthly data dumps. The dblp + Citations v1 dataset [100] builds upon a dblp version from 2010 mapped on AMiner. It contains 1,632,442 publications with 2,327,450 citations. The dblp + Citations v11 datasetbuilds upon dblp. It contains 4,107,340 papers, 245,204 authors, 16,209 venues and 36,624,464 citations Descriptions of non-public datasets based on dblp (dblp + IEEE + ACM + Pubmed, DBLP paths, DBLPCitation-network f. AMiner, dblp, DBLP + Citations v8, DBLP-REC, dblp + AMiner KG, dblp + AMiner + venue) can be found in Table 5. The Scholarly Paper Recommendation Dataset (SPRD) was constructed by collecting publications written by 50 researchers of diﬀerent seniority from the area of computer science which are contained in dblp from 2000 to 2006 [96,97,55]. The dataset contains 100,351 candidate papers extracted from the ACM Digital Library as well as citations and references for papers. Relevance assessments of papers relevant to their current interests of the 50 researchers are also included. A subset of SPRD, SPRD Senior, which contains only the data of senior researchers can also be constructed [94]. CiteULike [18] was a social bookmarking site for scientiﬁc papers. It contained papers and their metadata. Users were able to include priorities, tags or comments for papers on their reading list. There were daily data dumps available from which datasets could be constructed. Citeulike-a [107]contains 5,551 users, 16,980 papers with titles and abstracts from 2004 to 2006 and their 204,986 interactions between users and papers. Papers are represented by their title and abstract. Citeulike-t [107]contains 7,947 users, 25,975 papers and 134,860 user-paper interactions. Papers are represented by their pre-processed title and abstract. The description of a non-public dataset based on CiteULike (Citeulike huge, Citeulike medium, Citeulike tiny) can be found in Table 5. The ACM Digital Library (ACM) is a semi-open digital library oﬀering information on scientiﬁc authors, papers, citations and venues from the area of computer science. They oﬀer an API to query for information. Descriptions of non-public datasets based on ACM (ACM paths, ACM citation network V8 ) can be found in Table 5. Scopus is a semi-open digital library containing metadata on authors, papers and aﬃliations in diﬀerent scientiﬁc areas. They oﬀer an API to query for data. Descriptions of non-public datasets based on Scopus (Scopus tiny, ScienceDirect + Scopus, Scopus, Scopus + venue) can be found in Table 5. ArnetMiner (AMiner) [100] is an open academic search system modelling the academic network consisting of authors, papers and venues from all areas. They provide an API to query for information. Descriptions of non-public datasets based on AMiner (AMiner, AMiner + Wanfang, AMiner tiny, AMinerhuge, ACM C-D) can be found in Table 5. The ACL Anthology Network (AAN) [76,77,78] is a networked database containing papers, authors and citations from the area of computational linguistics. It consists of three networks representing paper-citation relations, author-collaboration relations and the authorcitation relations. The original dataset contains 24,766 papers and 124,857 citations [67]. Descriptions of non-public datasets based on AAN (AAN modiﬁed, AAN tiny) can be found in Table 5. Sowiport was an open digital library containing information on publications from the social sciences and adjacent ﬁelds [13,37]. It contained author names, keywords and venue titles by which the constructed social network could be traversed by users. Sowiport cooperated with the recommendation-as-a-service system Mr. DLib [26]. Descriptions of non-public datasets based on Sowiport (Sowiport, RARD tiny) can be found in Table 5. CiteSeerX [32,109] is a digital library focused on metadata and full-texts of open access literature. It is the overhauled form of the former digital library CiteSeer. Descriptions of non-public datasets based on CiteSeerX (CiteSeer, CiteSeer tiny, CiteSeer medium) can be found in Table 5. The Patents dataset provides information on patents and trademarks granted by the United States Patent and Trademark Oﬃce. Descriptions of non-public datasets based on Patents (Patents tiny, Patents, ACM H-I ) can be found in Table 5. The original unaltered Hep-TH [50] datasetstems from the area of high energy physics theory. It contains papers in a graph which were published between 1993 and 2003. It was released as part of KDD Cup 2003. Descriptions of non-public datasets based on HepTH (Hep-TH graph, arXiv Hep-TH ) can be found in Table 5. The Microsoft Academic Graph (MAG) [92] was an open scientiﬁc network containing metadata on academic communication activities. Their heterogeneous graph consists of nodes representing ﬁelds of study, authors, aﬃliations, papers and venues. Descriptions of non-public datasets based on MAG (MSA, MAG 2017, MAG 2018 ) can be found in Table 5. The following datasets have no common underlying data source: The BBCdataset contains 2,225 BBC news articles which stem from 5 topics. PRSDatasetcontains 2,453 users, 21,940 items and 35,969 pairs of users and items. Descriptions of all other non-public datasets can be found in Table 5. Due to the vast diﬀerences in approaches and datasets used to apply the methods, there is also a spectrum of used evaluation measures and objectives. In this section ﬁrst we observe diﬀerent notions of relevance of recommended papers and individual assessment strategies for relevance. Afterwards we analyse commonly used evaluation measures and list ones which are only rarely encountered in evaluation of paper recommendation systems. Lastly we shed light on the diﬀerent types of evaluation which authors conducted. In this discussion we again only consider paper recommendation systems which also evaluate their actual approach. We disregard approaches which do evaluate other properties [2,23,35,79,81,116] or contain no evaluation [57]. Thus we observe 54 diﬀerent approaches in this analysis. Relevance of recommended publications can be evaluated against multiple target values: clicked papers [22, 53,99], references [41,110], references of recently authored papers [54], papers an author interacted with in the past [46], degree-of-relevancy which is determined by citation strength [89], a ranking based on future citation numbers [115] as well as papers accepted [24] or deemed relevant by authors [36,83]. Assessing the relevance of recommendations can also be conducted in diﬀerent ways: the top n papers recommended by a system can be judged by either a referee team [104] or single persons [24,70,71]. Other options for relevance assessment are the usage of a dataset with user ratings [36,83] or emulation of users and their interests [1,54]. Table 6 holds information on utilised relevance indicators and target values which indicate relevance for the 54 discussed approaches. Relevancy describes the method that deﬁnes which of the recommended papers are relevant: – Human rating: The approach is evaluated using assessments of real users of results speciﬁc to the approach. – Dataset: The approach is evaluated using some type of assessment of a target value which is not speciﬁc to the approach but from a dataset. The assessment was either conducted for another approach and reused or it was collected independent of an approach. – Papers: The approach is evaluated by some type of assessment of a target value which is directly generated from the papers contained in the dataset such as citations or their keywords. The target values in Table 6 describe the entities which the approach tried to approximate: – Clicked: The approximated target value is derived from users’ clicks on papers. – Read: The approximated target value is derived from users’ read papers. – Cited: The approximated target value is derived from cited papers. – Liked: The approximated target value is derived from users’ liked papers. – Relevancy: The approximated target value is derived from users’ relevance assessment of papers. – Other: The approximated target value is derived from other entities, e.g. papers with identical references or interest. Only three approaches evaluate against multiple target values [19,28,99]. Six approaches (11.11%) utilise clicks of users, only one approach (1.85%) uses read papers as target value. Even though cited papers are not the main objective of paper recommendation systems but rather citation recommendation systems, this target was approximated by 13 (24.07%) of the observed systems. Ten approaches (18.52%) evaluated against liked papers, 15 (27.78%) against relevant papers and 13 (24.07%) against some other target value. We diﬀerentiate between commonly used and rarely used evaluation measures for the task of scientiﬁc paper recommendation. They are described in the following sections. Table 6 holds indications of utilised evaluation measures for the 54 discussed approaches. Measures are the methods used to evaluate the approach’s ability to approximate the target value which can be of type precision, recall, f1 measure, nDCG, MMR, MAP or another one. Out of the observed systems, twelve approaches [1, 26,28,46,56,61,65,67,70,71,72,102,111,110] (22.22%) only report one single measure, all others report at least two diﬀerent ones. Bai et al [9] identify precision (P), recall (R), F1, nDCG, MMR and MAP as evaluation features which have been used regularly in the area of paper recommendation systems. Table 7 gives usage percentages of each of these measures in observed related work. Alfarhood and Cheng [4] argue against the use of precision when utilising implicit feedback. If a user gives no feedback for a paper it could either mean disinterest or that a user does not know of the existence of the speciﬁc publication. We found a plethora of rarer used evaluation measures which have either been utilised only by the work they were introduced in or to evaluate few approaches. Our analysis in this aspect might be highly inﬂuenced by the narrow time frame we observe. Novel measures might require more time to be adopted by a broader audience. Thus we diﬀerentiate between novel rarely used evaluation measures and ones where authors do not explicitly claim they are novel. A list of rare but already deﬁned evaluation measures can be found in Table 8. In total 25 approaches (46.3%) did use an evaluation measure not considered common. Novel rarely used Evaluation Measures. In our considered approaches we only encountered three novel evaluation measures: Recommendation quality as deﬁned by Chaudhuri et al. [24] is the acceptance of recommendations by users rated on a Likert scale from 1 to 10. TotNP EU is a measure deﬁned by Manju et al. [30] speciﬁcally introduced for measuring performance of approaches regarding the cold start problem. It indicates the number of new publications suggested to users with a prediction value above a certain threshold. TotNP AVG is another measure deﬁned by Manju et al. [30] for measuring performance of approaches regarding the cold start problem. It indicates the average number of new publications suggested to users with a prediction value above a certain threshold. Evaluations can be classiﬁed into diﬀerent categories. We follow the notion of Beel and Langer [15] who diﬀerentiate between user studies, online evaluations and ofﬂine evaluations. They deﬁne user studies as ones where users’ satisfaction with recommendation results is measured by collecting explicit ratings. Online evaluations are ones where users do not explicitly rate the recommendation results; relevancy is derived from e.g. clicks. In oﬄine evaluations a ground truth is used to evaluate the approach. From the 54 observed approaches we found four using multiple evaluation types [27,43,87,89,104]. Twelve (22.22%) were conducting user studies which describe the size and composition of the participant group. Only two approaches [26,30] (3.7%) in the observed papers were evaluated with an online evaluation. We found 44 approaches (81.48%) providing an oﬄine evaluation. Oﬄine evaluations being the most common form of evaluation is unsurprising as this tendency has also been observed in an evaluation of general scientiﬁc recommender systems [21]. Oﬄine evaluations are fast and do not require users [21]. Nevertheless the margin by which this form of evaluation is conducted could be rather surprising. A distinction in lab-based vs. real world user studies can be conducted [14,15]. User studies where participants rate recommendations according to some criteria and are aware of the study are lab-based, all others are considered real world studies. Living labs [33,12,86] for example enable real world user studies. On average the lab-based user studies were conducted with 17.83 users. Table 9 holds information on the number of participants for all studies as well as the composition of groups in terms of seniority. For oﬄine evaluation, they can either be ones with an explicit ground truth given by a dataset containing user rankings, implicit ones by deriving user interactions such as liked or cited papers or expert ones with manually collected expert ratings [15]. We found 22 explicit oﬄine evaluations (40.74%) corresponding to ones using datasets to estimate relevance (see Table 6) and 21 implicit oﬄine evaluations (38.89%) corresponding to ones using paper information to identify relevant recommendations (see Table 6). We did not ﬁnd any expert oﬄine evaluations. All paper recommendation approaches which were considered in this survey could have been improved in some way or another. Some papers did not conduct evaluations which would satisfy a critical reader, others could be more convincing if they compared their methods to appropriate competitors. The possible problems we encountered within the papers can be summarised in different open challenges, which papers should strive to overcome. We separate our analysis and discussion of open challenges in those which have already been described by previous literature reviews (see Section 6.1) and ones we identify as new or emerging problems (see Section 6.2). Lastly we brieﬂy discuss the presented challenges (see Section 6.3). In the following we will explain possible shortcomings which were already explicitly discussed in previous literature reviews [9,14,87]. We regard these challenges in light of current paper recommendation systems to identify problems which are nowadays still encountered. Neglect of user modelling has been described by Beel et al. [14] as identiﬁcation of target audiences’ information needs. They describe the trade-oﬀ between specifying keywords which brings recommendation systems closer to search engines and utilising user proﬁles as input. Currently only some approaches consider users of systems to inﬂuence the recommendation outcome, as seen with Table 3 users are not always part of the input to systems. Instead many paper recommendation systems assume that users do not state their information needs explicitly but only enter keywords or a paper. With paper recommendation systems where users are not considered, the problem of neglecting user modelling still holds. Focus on accuracy as a problem is described by Beel et al. [14]. They state putting users’ satisfaction with recommendations on a level with accuracy of approaches does not depict reality. More factors should be considered. Only over one fourth of current approaches do not only report precision or accuracy but also observe more diversity focused measures such as MMR. We also found usage of less widespread measures to capture diﬀerent aspects such as popularity, serendipity or click-throughrate. The missing translation of research into practice is described by Beel et al. [14]. They mention the small percentage of approaches which are available as prototype as well as the discrepancy between real world systems and methods described in scientiﬁc papers. Only four of our observed approaches deﬁnitively must have been available online at any point in time [26, 42,30,79]. We did not encounter any of the more complex approaches being used in widespread paper recommendation systems. Beel et al. [14] describe the lack of persistence and authority in the ﬁeld of paper recommendation systems as one of the main reasons why research is not adapted in practice. The analysis of this possible shortcoming of current work could be highly aﬀected by the short time period from which we observed works. We found several groups publishing multiple papers as seen in Table 10 which corresponds to 29.69% of approaches. The most papers a group published was three so this amount still cannot fully mark a research group as authority in the area. Problems with cooperation are described by Beel et al. [14]. They state even though approaches have been proposed by multiple authors building upon prior work is rare. Corporations between diﬀerent research groups are also only encountered sporadically. Here again we want to point to the fact that our observed time frame of less than three years might be too short to make substantive claims regarding this aspect. Table 11 holds information on the diﬀerent numbers of authors for papers and the percentage of papers out of the 64 observed ones which are authored by groups of this size. We only encountered little cooperation between diﬀerent co-author groups (see Haruna et al. [36] and Sakib et al. [83] for an exception). There were several groups not extending their previous work [115,116]. We refrain from analysing citations of related previous approaches as our considered period of less than three years is too short for all publications to have been able to be recognised by the wider scientiﬁc community. Information scarcity is described by Beel et al. [14] as researchers’ tendency to only provide insuﬃcient detail to re-implement their approaches. This leads to problems with reproducibility. Many of the approaches we encountered did not provide suﬃcient information to make a re-implementation possible: with Afsar et al. [1] it is unclear how the knowledge graph and categories were formed, Collins and Beel [26] do not describe their Doc2Vec enough, Liu et al. [58] do not specify the extraction of keywords for papers in the graph and Tang et al. [99] do not clearly describe their utilisation of Word2Vec. In general oftentimes details are missing [3,4,57,112]. Exceptions to these observations are e.g. found with Bereczki [17], Nishioka et al. [70,71,72] and Sakib et al. [83]. We did not ﬁnd a single papers’ code e.g. provided as a link to GitHub. Pure collaborative ﬁltering systems encounter the cold start problem as described by Bai et al. [9] and Shahid et al. [87]. If new users are considered, no historical data is available, they cannot be compared to other users to ﬁnd relevant recommendations. While this problem still persists, most current approaches are no pure collaborative ﬁltering based recommendation systems (see Section 3.3.1). Systems using deep learning could overcome this issue [55]. There are approaches speciﬁcally targeting this problem [56, 91], some [56] also introduced speciﬁc evaluation measures (totNPEU and avgNP EU) to quantify systems’ ability to overcome the cold start problem. Bai et al. [9] state the user-paper-matrix being sparse for collaborative ﬁltering based approaches. Shahid et al. [87] also mention this problem as the reduce coverage problem. This trait makes it hard for approaches to learn relevancy of infrequently rated papers. Again, while this problem is still encountered, current approaches mostly are no longer pure collaborative ﬁltering based systems but instead utilise more information (see Section 3.3.1). Using deep learning in the recommendation process might reduce the impact of this problem [55]. The problem of scalability was described by Bai et al. [9]. They state paper recommendation systems should be able to work in huge, ever expanding environments where new users and papers are added regularly. A few approaches [35,43,83,104] contain a web crawling step which directly tackles challenges related to outdated or missing data. Some approaches [24,58] evaluate the time it takes to compute paper recommendations which also indicates their focus on this general problem. But most times scalability is not explicitly mentioned by current paper recommendation systems. There are several works [39,42,91,103,111] evaluating on bigger datasets with over 1 million papers and which thus are able to handle big amounts of data. Sizes of current relevant real-world data collections exceed this threshold many times over (see e.g. PubMed with over 33 million papersor SemanticScholar with over 203 million papers). Kanakia et al. [42] explicitly state scalability as a problem their approach is able to overcome. Instead of comparing each paper to all other papers they utilise clustering to reduce the number of required computations. They present the only approach running on several hundred million publications. Nair et al. [67] mention scalability issues they encountered even when only considering around 25,000 publications and their citation relations. The problem of privacy in personalised paper recommendation is described by Bai et al. [9]. Shahid et al. [87] also mention this as a problem occurring in collaborative ﬁltering approaches. An issue is encountered when sensitive information such as habits or weaknesses that users might not want to disclose is used by a system. This leads to users’ having negative impressions of systems. Keeping sensitive information private should therefore be a main goal. In the current approaches, we did not ﬁnd a discussion of privacy concerns. Some approach even explicitly utilise likes [79] or association rules [3] of other users while failing to mention privacy altogether. In approaches not incorporating any user data, this issue does not arise at all. Serendipity is described by Bai et al. [9] as an attribute often encountered in collaborative ﬁltering [14]. Usually paper recommender systems focus on identiﬁcation of relevant papers even though also including not obviously relevant ones might enhance the overall recommendation. Junior researchers could proﬁt from stray recommendations to broaden their horizon, senior researchers might be able to gain knowledge to enhance their research. The ratio between clearly relevant and serendipitous papers is crucial to prevent users from losing trust in the recommender system. A main objective of the works of Nishioka et al. [70, 71,72] is serendipity. Other approaches do not mention this aspect. Diﬀerent data formats of data collections is mentioned as a problem by Bai et al. [9]. They mention digital libraries containing relevant information which needs to be uniﬁed in order to use the data in a paper recommendation system. Additionally the combination of datasets could also lead to problems. Many of the approaches we observe do not consider data collection or preparation as part of the approach, they often only mention the combination of diﬀerent datasets as part of the evaluation (see e.g. Du et al. [27], Li et al. [53] or Xie et al. [110]). An exception to this general rule are systems which contain a web crawling step for data (see e.g. Ahmad and Afzal [2] or Sakib et al. [83]). Even with this type of approaches the combination of datasets and their diverse data formats is not identiﬁed as a problem. Shahid et al. [87] describe the problem of synonymy encountered in collaborative ﬁltering approaches. They deﬁne this problem as diﬀerent words having the same meaning. Even though there are still approaches (not necessarily CF ones) utilising basic TF-IDF representations of papers [2,39,81,90], nowadays this problem can be bypassed by using a text embedding method such as Doc2Vec or BERT. Gray sheep is a problem described by Shahid et al. [87] as an issue encountered in collaborative ﬁltering approaches. They describe it as some users not consistently (dis)agreeing with any reference group. We did not ﬁnd any current approach mentioning this problem. Black sheep is a problem described by Shahid et al. [87] as an issue encountered in collaborative ﬁltering approaches. They describe it as some users not (dis)agreeing with any reference group. We did not ﬁnd any current approach mentioning this problem. Shilling attacks are described by Shahid et al. [87] as a problem encountered in collaborative ﬁltering approaches. They deﬁne this problem as users being able to manually enhance visibility of their own research by rating authored papers as relevant while negatively rating any other recommendations. Although we did not ﬁnd any current approach mentioning this problem we assume maybe it is no longer highly relevant as most approaches are no longer pure collaborative ﬁltering ones. Additionally from the considered collaborative ﬁltering approaches no one explicitly stated to feed relevance ratings back into the system. In addition to the open challenges discussed in former literature reviews by Bai et al. [9], Beel et al. [14] and Shahid et al. [87] we identiﬁed the following problems and derive desirable goals for future approaches from them. Paper recommendation is always targeted at human users. But oftentimes an evaluation with real users to quantify users’ satisfaction with recommended publications is simply not conducted [79]. Conducting huge user studies is not feasible [35]. So sometimes user data to evaluate with is fetched from the presented datasets [36, 83] or user behaviour is artiﬁcially emulated [1, 17,54]. Noteworthy counter-examplesare the studies by Bulut et al. [20] who emailed 50 researchers to rate relevancy of recommended articles or Chaudhuri et al. [24] who asked 45 participants to rate their acceptance of recommended publications. Another option to overcome this issue is utilisation of living labs as seen with ArXivDigest [33], Mr. DLib’s living lab [12] or LiLAS for the related tasks of dataset recommendation for scientiﬁc publications and multi-lingual document retrieval [86]. Desirable goal. Paper recommendation systems targeted at users should always contain a user evaluation with a description of the composition of participants. Current works mostly fail to clearly characterise the intended users of a system altogether and the varying interests of diﬀerent types of users are not examined in their evaluations. There are some noteworthy counter-examples: Afsar et al. [1] mention cancer patients and their close relatives as intended target audience. Bereczki [17] identiﬁes new users as a special group they want to recommend papers to. Hua et al. [39] consider users which start diving into a topic which they have not yet researched before. Sharma et al. [90] name subject matter experts incorporating articles into a medical knowledge base as their target audience. Shi et al. [91] clearly state use cases for their approach which always target users which are unaware of a topic but already have one interesting paper from the area. They strive to recommend more papers similar to the ﬁrst one. User characteristics such as registration status of users are already mentioned by Beel et al. [14] as a factor which is disregarded in evaluations. We want to extend on this point and highlight the oftentimes missing or inadequate descriptions of intended users of paper recommendation systems. Traits of users and their information needs are not only important for experiments but should also be regarded in the construction of an approach. The targeted audience of a paper recommendation system should inﬂuence its suggestions. Bai et al. [9] highlight diﬀerent needs of junior researchers which should be recommended a broad variety of papers as they still have to ﬁgure out their direction. They state recommendations for senior researchers should be more in line with their already established interests. Sugiyama and Kan [95] describe the need to help discover interdisciplinary research for this experienced user group. Most works do not recognise possible diﬀerent functions of paper recommendation systems for users depending on their level of seniority. If papers include an evaluation with real persons, they e.g. mix Master’s students with professors but do not address their diﬀerent goals or expectations from paper recommendation [70]. Chaudhuri et al. [24] have junior, experienced and expert users as participants of their study and give individual ratings but do not calculate evaluation scores per user group. In some studies the exact composition of test users is not even mentioned (see Table 9). Desirable goal. Deﬁnition and consideration of a speciﬁc target audience for an approach and evaluation with members of this audience. If there is no speciﬁc person group a system should suit best, this should be discussed, executed and evaluated accordingly. Suggested papers from an approach should either be ones to read [104,41], to cite or fulﬁl another speciﬁed information need such as help patients in cancer treatment decision making [1]. Most work does not clearly state which is the case. Instead recommended papers are only said to be related [4,26], relevant [4,5,24,25,35, 39,42,45,53,54,100,110,112], satisfactory [39,58], suitable [19], appropriate and useful [20,83] or a description which scenario is tackled is skipped altogether [3,34,36, 79]. In rare cases if the recommendation scenario is mentioned there is the possibility of it not perfectly ﬁtting the evaluated scenario. This can e.g. be seen in the work of Jing and Yu [41] where they propose paper recommendation for papers to read but evaluate papers which were cited. Cited papers should always be ones which have been read beforehand but the decision to cite papers can be inﬂuenced by multiple aspects [31]. Desirable goal. The clear description of the recommendation scenario is important for comparability of approaches as well as the validity of the evaluation. Anand et al [8] deﬁne fairness as the balance between relevance and diversity of recommendation results. Only focusing on ﬁt between the user or input paper and suggestions would lead to highly similar results which might not be vastly diﬀerent from each other. Having diverse recommendation results can help cover multiple aspects of a user query instead of only satisfying the most prominent feature of the query [8]. In general more diverse recommendations provide greater utility for users [72]. Most of the current paper recommendation systems do not consider fairness but some approaches speciﬁcally mention diversity [24,70,71,72] while striving to recommend relevant publications. Thus these systems consider fairness. Over one fourth of considered approaches with an evaluation report MMR as a measure of their system’s quality. This at least seems to show researchers’ awareness of the general problem of diverse recommendation results. Desirable Goal. Diversiﬁcation of suggested papers to ensure fairness of the approach. Paper recommendation systems tend to become more complex, convoluted or composed of multiple parts. We observed this trend by regarding the classiﬁcation of current systems compared to previous literature reviews (see Section 3.3.1). While systems’ complexity increases, users’ interaction with the systems should not become more complex. If an approach requires user interaction at all, it should be as simple as possible. Users should not be required to construct sophisticated knowledge graphs [104] or enter multiple rounds of keywords for an approach to learn their user proﬁle [22]. Desirable Goal. Maintain simplicity of usage even if approaches become more complex. Conﬁdence in the recommendation system has already been mentioned by Beel et al. [14] as an example of what could enhance users’ satisfaction but what is overlooked in approaches in favour of accuracy. This aspect should be considered with more vigour as the general research area of explainable recommendation has gained immense traction [114]. Gingstad et al. [33] regard explainability as a core component of paper recommendation systems. Xie et al. [111] mention explainability as a key feature of their approach but do not state how they achieve it or if their explanations satisfy users. Suggestions of recommendation systems should be explainable to enhance their trustworthiness and make them more engaging [62]. Here, diﬀerent explanation goals such as eﬀectiveness, eﬃciency, transparency or trust and their inﬂuence on each other should be considered [10]. If an approach uses neural networks [22,34,46,53] it is oftentimes impossible to explain why the system learned, that a speciﬁc suggested paper might be relevant. Lee et al. [48] introduce a general approach which could be applied to any paper recommendation system to generate explanations for recommendations. Even though this option seems to help solve the described problem it is not clear how valuable post-hoc explanations are compared to systems which construct them directly. Desirable Goal. The conceptualisation of recommendation systems which comprehensibly explain their users why a speciﬁc paper is suggested. Current approaches utilise many diﬀerent datasets (see Table 4). A large portion of them are built by the authors such that they are not publicly available for others to use as well [1,28,106]. Part of the approaches already use open datasets in their evaluation but a large portion still does not seem to regard this as a priority (see Table 5). Utilisation of already public data sources or construction of datasets which are also published and remain available thus should be a priority in order to support reproducibility of approaches. Desirable Goal Utilisation of publicly available datasets in the evaluation of paper recommendation systems. From the approaches we observed many identiﬁed themselves as paper recommendation ones but only evaluated against systems, which are more general recommendation systems or ones utilising some same methodologies but not from the sub-domain of paper recommendation (seen with e.g. Guo et al [34], Tanner et al. [101] or Yang et al. [112]). While some of the works might claim to only be applied on paper recommendation and be of more general applicability (see e.g. the works by Ahmedi et al. [3] or Alfarhood and Cheng [4]) we state that they should still be compared to ones, which mainly identify as paper recommendation systems as seen in the work of Chaudhuri et al. [22]. Only if a more general approach is compared to a paper recommendation approach, its usefulness for the area of paper recommendation can be fully assessed. Several times, the baselines to evaluate against are not even other works but artiﬁcially constructed ones [2, 35] or no other approach at all [20]. Desirable Goal. Evaluation of paper recommendation approaches, even those which are applicable in a wider context, should always be against at least one paper recommendation system to clearly report relevance of the proposed method in the claimed context. From the already existing problems, several of them are still encountered in current paper recommendation approaches. Users are not always part of the approaches so users are not always modelled but this also prevents privacy issues. Accuracy seems to still be the main focus of recommendation systems. Novel techniques proposed in papers are not available online or applied by existing paper recommendation systems. Approaches do not provide enough details to enable re-implementation. Other problems mainly encountered in pure collaborative ﬁltering systems such as the cold start problem, sparsity, synonymy, gray sheep, black sheep and shilling attacks do not seem to be as relevant anymore. We observed a trend towards hybrid models, this recommendation system type can overcome these issues. These hybrid models should also be able to produce serendipitous recommendations. Unifying data sources is conducted often but nowadays it does not seem to be regarded as a problem. With scalability we encountered the same. Approaches are oftentimes able to handle millions of papers, here they do not speciﬁcally mention scalability as a problem they overcome but they also mostly do not consider huge datasets with several hundreds of millions of publications. Due to the limited scope of our survey we are not able to derive substantive claims regarding cooperation and persistence. We found around 30% of approaches published by groups which authored multiple papers and very few collaborations between diﬀerent author groups. As for the newly introduced problems part of the observed approaches conducted evaluations with users, on publicly available datasets and against other paper recommendation systems. Many works considered a low complexity for users. Target audiences in general were rarely deﬁned, the recommendation scenario was mostly not described. Diversity was considered by few. Overall the explainability of recommendations was dismissed. To conclude, there are many challenges which are not constantly considered by current approaches. They deﬁne the requirements for future works in the area of paper recommendation systems. This literature review of publications targeting paper recommendation between January 2019 and October 2021 provided comprehensive overviews of their methods, datasets and evaluation measures. We showed the need for a richer multi-dimensional characterisation of paper recommendation as former ones no longer seem suﬃcient in classifying the increasingly complex approaches. We also revisited known open challenges in the current time frame and highlighted possibly underobserved problems which future works could focus on. Eﬀorts should be made to standardise or better differentiate between the varying notions of relevancy and recommendation scenarios when it comes to paper recommendation. Future work could try revaluate already existing methods with real humans and against other paper recommendation systems. This could for example be realised in an extendable paper recommendation benchmarking system similar to the in a living lab environments ArXivDigest [33], Mr. DLib’s living lab [12] or LiLAS [86] but with the additional property that it also provides build-in oﬄine evaluations. As fairness and explainability of current paper recommendation systems have not been tackled widely, those aspects should be further explored. Another direction could be the comparison of multiple rare evaluation measures on the same system to help identify those which should be focused on in the future. As we observed a vast variety in datasets utilised for evaluation of the approaches (see Table 4), construction of publicly available and widely reusable ones would be worthwhile.