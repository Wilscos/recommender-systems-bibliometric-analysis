Luis M. Vaquero PhD, KidsLoop Ltd, Funding information three primary axes listed here as a framework for categorising and comparing experiments: 1. iterativity, 2. scale, and 3. technological readiness. These axes are shown visually in Figure 1, and the set of nodes results in eight distinct experimental types. The most foundational and common node is the traditional static classroom that are often void of direct experimentation, but will follow recommended practices for teaching. Each axis leaving this root node ‘adds’ new capabilities to the learning environment, and the remaining nodes in the cube are the cases where one, two or three new capabilities. of daily teaching practise. This experimentation process can be distinguished from the ‘static classroom’ by moving along the ‘iterative’ axis (edge number (1) in Figure 1). It is possible to augment this testing environment further by introducing technology that can, for example, capture data and coordinate several teachers/classes in the same year or even several schools in a district. This shifts the experimental categorisation down edge the ‘technology enablement’ axis (edge (2) in Figure 1). In both cases it is important that best practices for experimental are followed so that they do not incur potentially harmful and costly initiatives on children [1]. Using best practice guides from the medical literature as an example, experiments are expected to be double-blind and eﬀect and sample size calculations should be conducted a priori. This is particularly critical when experimenting may lead to the dissemination of public guidance for best education practice. However, it is easy to see the challenges in guaranteeing these requirements in several cases in the cube: teacher-led experimentation cannot be double-blind, and sample size is often determined by the class size and cannot be negotiated in small experiments. In other words, although these are valuable experiments, they will likely need to be validated by a larger-scale experiment before wide-scale adoption can be considered. ing across disciplines. Owing to their long and successful legacy in many contexts (particularly medical), they have garnered deep trust, oﬀering many safeguards against poor treatments and outcomes, and statistical guarantees that Don’t give up! I believe in you all. A person’s a person, no matter how small! And you very small persons will not have to die. If you make yourselves heard! So come on, now, and TRY! The fundamental topic of consideration in this paper is experimentation in educational settings. We consider Concretely, it is well-known that most educational experiments occur in small and localised environment as part Randomised Controlled Trials (RCTs) are the widely accepted standard of experimentation and hypotheses test- F I G U R E 1 Classroom Experimentation Cube: starting from the top left corner of the cube one can explore where the addition of iterative experimentation (such as the one happening in the classroom), large number of participants (scale - like in large RCTs) and advanced technology support would lead to in terms of type of experiment being produced. successes will be detected. It is little surprise, therefore, that RCTs are the de facto model for educational experimentation. The majority of recent successes in educational experimentation have aligned with this. However, even when rigorous evaluations are performed using randomised experiments, they often lack the iterative nature of low scale classroom led experimentation (see “large static RCT” and “big data RCT” in Figure 1). Also, the resulting eﬀect sizes are much smaller than previous research in the same substantive area [2]. learners. Achieving the required rigour has traditionally been linked to slow, costly processes that require enormous infrastructure and funding schemes (see “expensive RCT” in Figure 1). Government funding and coordination is often required to ensure the implementation of educational RCTs [3]. To establish good RCT practice in the U.S., the Institute of Education Sciences (IES) was introduced by the Education Science Reform Act of 2002. The mission of the IES is to build a body of rigorous evidence to inform education policy and practice [4] (http://ies.ed.gov). Similar governance processes have been instigated in Europe, with the Education Endowment Foundation (EEF) in the U.K. overseeing over 100 education RCTs in a very short period of time in [5]. RCTs have several weaknesses, however, including high expense, recruitment challenges, sample size calculation, implementation and testing. These randomised online controlled experiments, often called A/B tests, are used to drive decisions about how to improve their products. A/B tests and RCTs both rely on the same family of experimental methods, and have been tried and tested millions of times for over a century. Although RCTs and A/B tests have many similarities (particularly on their theoretical foundations), there are signiﬁcant diﬀerences on how they are applied in general. RCTs tend to be expensive, have longer follow up times, and be extra rigorous in terms of planning the analysis, resulting in a very slow and bureaucratic process overall. On the other hand, A/B tests are inexpensive, fast, and encourage quick tests to prune down non productive interventions early. experiments. In many online trials, up to 70% of the promising ideas fail to improve the core outcome metrics, and these rates approach 80–90% for well-optimised systems [6]. A priori, therefore, it is expected that approximately 9 out of 10 experiments will fail, similar to the teacher-led experiments in the classroom. Despite these poor odds, This means lots of resources are wasted and key decisions result in irrelevant impact on the education of the On the other hand, cheap and reliable experiments are routinely conducted by nearly every technology company. A key distinction between the two that is worth mentioning explicitly here is the consequence and cost of failed online testing repeatedly and consistently deliver enormous value because they are designed to be resilient to failure. Resilience is achieved with rapid experimental pivots, testing small incremental changes fast and continuously, technology has enabled methodologically consistent, statistically rigorous, cost eﬀective RCTs at an incredible scale. A/B tests are delivered through/with the assistance of online devices such as phones, tablets or computers. improve education quickly and at scale is at hand. Adopting the techniques from technology companies can bring the potential to dramatically alter the scale and pace of how trials are conducted in education. Rather than seeing educational interventions as single-shot experiments seeking large changes, embracing technology-like A/B test means adopting a long chain of (potentially) small eﬀects that compound to signiﬁcant long-term beneﬁts. The combination of the rapid experimentation of teacher-led small scale experiments with the rigour of large scale RCTs has been enabled by the availability of new technologies at scale (see “large scale A/B test” in Figure 1). By following the best experimental practice and performing experiments at scale, highly credible results should be acquired. In turn, these may be appraised by a committee of educational experts, augment statistical power through conducting meta-analyses and become a key component for making public recommendations of best education practice, ultimately aﬀecting all nodes in the experimentation cube. common in an A/B testing context. We start by presenting the main current challenges in using RCTs in education: sample size (Section 2), recruitment (Section 3), cost (Section 4), readiness (Section 5), and rigour (Sections 6 and 7). All of these sections present technical and cultural changes induced by the potential introduction of A/B tests that can help address/mitigate current limitations. Finally, Section 8 discusses how a hybrid between RCTs and A/B tests can solve most of these challenges so that the education community may consider whether and how similar designs can be applied in a focused fashion or at massive scale. As platforms scale up, more experiments are required and experimenters need to spot incrementally smaller eﬀects after the ﬁrst initial big wins in not-optimised platforms are realised. Given that the relationship between the detectable eﬀect and the required umber of users for a ﬁxed statistical power is quadratic [7], four times more users are required to detect eﬀects half as large. months) studies [5]. Indeed, a major diﬀerence between A/B testing in technology and educational RCTs that dramatically aﬀects sizing is their time horizons: learning takes time and progress and impact can take years to realise. to the expression in Equation 1) tends to decrease (smaller error). This assumes the coeﬃcient of variation (CV), the ratio of the standard deviation to the mean, does not change for the duration of an experiment. However, empirical data shows that CV does change over time especially for long-lasting experiments. With the migration of many aspects of education to digital devices, the opportunity to learn, experiment, and Figure 2 shows how education RCTs could improve by embracing some of the techniques and elements that are This limitation has led to relatively small (75% including fewer than 1000 learners) or excessively long (many As time goes by in a study, the sample size also increases and the 95% conﬁdence interval (which is proportional F I G U R E 2 This ﬁgure illustrates the main challenges for education RCTs that are covered by this paper and lists the relevant sections where we show how A/B testing can dramatically improve the situation. . An Overall Evaluation Criterion (OEC) is a (usually composite) quantitative measure of the experiment’s objective, when a single key performance indicator is deemed insuﬃcient for the evaluation of the outcome of an online Controlled-experiment Using Pre-Existing Data (CUPED) is a variance reduction technique that uses previous data to control for natural variation in an experiment’s main metric. By removing natural variation, we can run statistical tests that require a smaller sample size. Temporal horizons represent the duration of the treatment and the follow up controlled experiment and using individual separate evaluation of metrics is not desirable for some reason. F I G U R E 3 Experiment campaign compounds a large eﬀect as the result of a long succession of short experiments with a modest individual eﬀect each of them. more users will be needed [8]: in practice, experimenters will need to expose each user to multiple experiments at the same time. Tech companies design campaigns of experiments that are shorter in duration where the immutability of CV can be guaranteed. for students. Let’s assume that experimental design speciﬁes that 3,000 students are required to understand signiﬁcance of an experiment, and let us further assume that we wish to run 50 experiments simultaneously. If this experimental campaign assigns students to only one experiment, then 150,000 unique students must be recruited. On the other hand, if students can simultaneously be in 5 experiments on average (this may be reasonable if each experiment targets diﬀerent aspects of the product), then we reduce our recruitment burden dramatically by a factor of 5. As shown in Section 6 classroom settings impose some additional considerations when determining the right test to estimate required size/power. Recommendation 1 Education RCTs may need to be to be realised as a planned campaign of shorter experiments aiming to reveal smaller eﬀect sizes over large populations that can be subject to many experiments in parallel. The timeframe for compounding eﬀects is large and aligned with learning as a long life activity. of time to a massive population, resulting in potentially big impacts for the learners and society. serve the robustness and rigour of the experiments, they had to develop mechanisms to prevent interactions (e.g., by declaring constraints or parameters being modiﬁed, the system will guarantee disjoint users to those experiments) and overnight tests on all pairs of experiments to detect interactions. tests is exerting noise on a subgroup enrolled in a diﬀerent test. Despite the theoretical risk of experiment cross-talk, practical results have shown that interactions in online platforms tend to be relatively rare and more often represent bugs than true statistical interactions [6]. Small multi-variate tests that evaluate the impact of multiple variables that could interact can be run to check for suspected cross-talks between experiments. Recommendation 2 Online education platforms would have to preserve as much contextual information as possible and elaborate tests to For this assumption to hold, experiments need to be shortened and, in order to keep statistical power high, many Consider the following example that illustrates population requirements in single vs. multi-experiment enrolment Figure 3 shows how a campaign of short-lived experiments delivering small eﬀects can build up over long periods There are many examples of large tech companies using overlapped/concurrent experiments [6, 9, 10]. To pre- Cross-talk tests would have to look at eﬀects across several variables such as country, language, gender to see if a check things such as agonistic/antagonistic eﬀects between subjects that the student is taking. Recruiting schools can be a challenge as RCTs normally contribute with extra load to their already busy days. [2] suggest that timing can be critical: recruiting schools in the summer (national assessments, school trips) tends to result in poor engagement and higher attrition rates. Attrition may aﬀect the statistical tests if some students drop out of the control/treatment group. Schools often welcome being part of the intervention group and the attrition rate in the control group tends to be much higher [2]. recommends recruiting periods of at least 3 months or, ideally, 5-6 months [2], which is a signiﬁcant fraction of the work considering that 70% of the educational studies last less than one term [5]. Online platforms enable dynamic recruitment of users into experiments. They check the conditions for inclusion/exclusion are met and potential confounders suggested to the experimenter. and they remain agnostic of the fact that they may be part of one or more experiments. principals). Recommendation 3 A/B testing platforms enable a seamless recruitment that can be done dynamically (as enrolment conditions are met) and requires little (if any) training/eﬀort for the teachers as “treatment“ is delivered in a blind manner for students and teachers alike, with frequent (live) results available in an aggregated form. In addition to the sizing/timing and enrollment practicalities of RCTs, they also come with notorious ﬁnancial considerations. The cost of enrolling students/schools, obtaining consent, setting up the experiment, and analysing the results is minimised through the usage of online experimentation. quantify the cost as a function of the cost to measure an expected size. Interventions that cost less than £80 per pupil per 0.1 shift in the standard deviation of the eﬀect are considered very good value for money, less than £170 per pupil per 0.1 standard deviations is good value for money (all of these are 2018 British Sterling Pound) [12, 2]. The marginal cost or running a new RCT in an online platform tends to null once it is built and the cost is amortised across the millions of learners in the platform and the built-in automation of tests and process such as recruitment results in negligible monetary impact. In addition, teachers need to be trained and supported to ensure appropriate conditions are met. The UK’s EEF This dynamic enrollment ability means the user is automatically and inadvertently enrolled in a double-blind test Another element to preserve engagement will be delivering insight about progress to key stakeholders (e.g. school A cost-eﬃcient RCT in 1970 would be $10000 (the equivalent of $65000 in 2021’s money) [11]. Other authors recruit and randomise schools or individual children in a double-blind manner. Also, an online experimentation platform includes guidance/automation of data processing to ensure statistical rigour in the obtained results (see Section 6 below). This aﬀordability makes some of the classic considerations about size of the eﬀects and cost less relevant [2]. Recommendation 4 Near zero cost per new user means new elements to be taken into consideration, such as the need to constantly (in near real-time) check for key metrics ensuring progress and protection from harmful eﬀects for the learners. According to [2], some of the interventions funded by the EEF were simply not ready for trial. The responsibility for this problem is shared by: the developers, for seeking funding for something that has not previously been adequately tested for evidence that it could work. platform is no longer limiting the number of experiments, organisations embrace a ‘test everything with controlled experiments’ mentality. The limiting factor to innovation now becomes the ability to generate ideas and develop the code for them adding automated precautions not to make harmful interventions for the learners. experiment with the ﬁrst read on the experiment impact in minutes for critical metrics. Recommendation 5 Calculating hundreds or thousands of metrics automatically for every experiment in near real-time means that if certain key metrics degrade beyond acceptable limits, the experiment is terminated automatically. If after several hours no key metric degrades, the experiment auto-ramps up to a higher percentage of users and at multiple locations. To avoid this [13] recommend: • Changes need to be not only statistically signiﬁcant, but also large enough in absolute magnitude to have mean- • Corrections for multiple testing. The O’Brien and Fleming procedure suggests using lower p-values early on and • Diﬀerent magnitudes of changes for diﬀerent metrics are categorised in speciﬁc severity levels. The most severe As the experimentation platform matures, running and analysing experiments becomes self-service. This means that users without coding skills can design and setup an experiment with assistance to automatically Lack of readiness in education trials resulted in time and money waste. However, when the experimentation In A/B tests, experiments are set and deﬁned using a web browser with 1000s of metrics computed for each Also, fast turn around of experiment results can help to abort obviously bad experiments by triggering alerts. Alerting on any statistically signiﬁcant negative metric changes will lead to an unacceptable number of false alerts. ingful user, data quality, or business impact. to increase them over time, so that an experiment that if the results are extreme we gain the beneﬁt of stopping early. changes result in automatic shutdown of an experiment but less severe changes will result in emails sent to the owner of the experiment and a central experimentation team. The availability of a cost eﬃcient platform to run experiments resulted in release cycles going from 6 months to monthly, to weekly, to daily, to multiple times a day [8]. they were designed to improve (even worse for well-optimised systems, where failures were in the range of 80–90% [6]). tised a priori, but are easy to code and evaluate. There is no strong correlation between the eﬀort to code an idea and its value [8]. For example, small changes to Google’s colour scheme, were worth over $200M annually. Recommendation 6 Having an online experimentation platform that supports the amalgamation and archival of diﬀerent forms of evidence (e.g. causal inference methods) and be useful to stage RCTs and help design RCT campaigns better [14]. A key assumption for A/B tests to deliver good results is the: “Stable Unit Treatment Value Assumption”, which states that the behavior of each sample in the experiment depends only on their own treatment and not on the treatments of others [15]. It is easy to see how a user who is served better search results is more likely to click, and that behavior is entirely independent of others using the same search engine. highly likely to be inﬂuenced by her neighbourhood. For example, if a video lesson is confusing, learners will share that confusion leading to lower usage and/or more discussion about it for learners in both, treatment and control groups. propagate over to his/her neighbourhood, regardless whether his/her neighbors are in treatment or control. This is similar to social networks such as Facebook, Twitter and LinkedIn in which many features are likely to have network eﬀects [8]. control by: 1) partitioning the learners into clusters and 2) treating each cluster as a unit for randomisation so all users in one cluster have the same experiment assignment [16, 8]. One big drawback with this clustering approach is its vulnerability to carryover eﬀects, where the same users who were impacted by the ﬁrst experiment are being used for the follow-on experiment although simple A/A tests can be run to check for carryover eﬀects triggering cluster membership randomisation at the expense of a temporary loss of experiment capacity [13]. This is much more important in education where interventions are expected to have a positive long lasting eﬀect. Recommendation 7 Online platforms make it easy to deliver experiments where treatment and control users receive identical experience (known as A/A tests). In this setting, any statistically signiﬁcant diﬀerences for each metric should happen at about 5% (when using a p value cutoﬀ of 0.05) can be attributed to chance or bias [8] such as hidden dependencies or carry-over (residual) eﬀects with previous experiments aﬀecting subsequent experiments on a subgroup of users. Indeed, testing ideas before implementation resulted 70% of the promising ideas failing to improve the metrics Cheap, reproducible, controlled experiments encourages more exploration of ideas that may not be highly priori- However, such assumption does not always hold in experiments run on education where a learner’s behaviour is In an A/B experiment, this implies that if the treatment has a signiﬁcant impact on a user, the eﬀect would Thus, a learner’s network connections need to be taken into account when sampling them into treatment and can make some experimental designs non-standard [17]. These authors highlight the need to take cluster-level and individual level into account to answer the “under what conditions” and “for whom” questions, making calculations more complicated than for standard RCTs or A/B tests. (not due to the Hawthorne eﬀect or a willingness to be treated). Stratiﬁcation can be expensive to implement efﬁciently during the sampling phase, so many online platforms start by dynamically checking the equilibrium of key metrics using historical data. If they are unbalanced, there is a re-randomisation process that uses a diﬀerent hash ID. Seeds for the hash function can be used to determine which one leads to a diﬀerence that is not statistically signiﬁcant [13], using post-stratiﬁcation or CUPED [18]. Recommendation 8 Education RCTs delivered as A/B tests will require rigorous testing for: • cross talk when users can be enrolled in multiple experiments • individual metric variation and composite OEC eﬀect • stratiﬁcation eﬀects and re-randomisation to balance diﬀerent strata • ability to re-run tests for reproducibility Appropriate testing means the company builds trust in the process of incrementally gathering evidence about what works and does not work. As a result, organisational goals tend to shift from ‘deliver feature X by date Y’ to ‘improve the Overall Evaluation Criterion (OEC) by x% over the next year’. After a few initial easy wins, most ideas are not as good as we believe [19] and many interventions end up impacting on secondary outcomes, without major long term progress on the main OEC. This makes the need to iterate on small incremental gains that build up in the same way compound interest does. Recommendation 9 Rather than focusing just on the average treatment eﬀect, having a score card of key metrics helps to unveil key segments of users where the treatment eﬀect is diﬀerent than the average. This enables hypotheses to develop and experiments start to target segments of users. In contrast to typically underpowered subgroup analyses in education trials, these are highly powered with a surplus of users that make the segments big enough for reliable statistical analyses. With so many experiments running in parallel for every learner, education practitioners will be concerned about lack of trustworthiness and false positive results. Online platforms perform multiple tests to identify scenarios that would indicate a problem [20]. of users in an experiment likely indicate a problem and the experiment needs to be tagged as invalid. There are simple Moreover, education RCTs tend to rely on clusters (schools or classrooms) as a basic randomisation unit, which Randomisation may not work all the time like when users are keener in treatment than in control just by chance For instance, if the experiment design is for equally sized control and treatment then deviations in the actual ratio tests that help us identify numerous issues in experiments, many of which invoke Twyman’s law (“Any ﬁgure that looks interesting or diﬀerent is usually wrong”) [21]. the short interventions often evaluated within RCTs, as post-assessments either have to be made long after the intervention ended or pre-assessments are not current enough [3]. Recommendation 10 Online platforms enable a nearly continuous evaluation of the results of the assessments. An OEC is a (usually composite) quantitative measure of goal of an experiment (also known as Response or Dependent Variable, Outcome Variable, Evaluation metric, Performance metric). OECs are used when a single key performance indicator (KPI) is not suﬃcient for the evaluation of the outcome of an online controlled experiment and a weighted combination of KPIs is used instead. time. The challenge is then coming up with a small set of key metrics, ideally a single OEC, to help make trade oﬀ decisions. long-term objectives (e.g. predicted lifetime value, compound acceleration of learning rate, content engagement, and repeat visits), but must be based on metrics that are measurable in short-term experiments. proﬁt, expected lifetime value, or some weighted combination of these): if users are coming more often, it is usually a strong sign that the treatment is useful. This metric may not be ideal in education as increased screen time is not always welcome by parents and teachers. The deﬁnition of appropriate metrics including not just engagement, but also how productive a learning session was seems much more relevant for learning applications. Recommendation 11 In online experimentation platforms, team performance is measured on the sum of treatment eﬀects of a single key metric. All experiments with a positive result are rerun (hence replicability is key), so that the replication run determines the actual credit the team gets. The replication eﬀect is unbiased, while the ﬁrst run may have found an exaggerated eﬀect [22]. to be a problem in education science [23]. Technology is evolving, and unless we can overcome the inertia of today’s education process many opportunities will be missed. very diﬀerent setting in which RCTs and A/B tests are applied. RCTs tend to be applied in academic environments where process and rigour are the most important elements, event at the cost of signiﬁcantly higher additional expenses and timelines. Hence, RCT participants are very carefully selected and are aware they are part of an RCT. They can only Long spacing between assessments (like in Norway and Sweden) makes assessment results less well suited for Agreement on the OEC becomes critically important, with hundreds to thousands of metrics changing at the same A good OEC should not be short term (e.g. clicks, time to completion) as it should capture the organisational A classic example in online technological companies is sessions per user metric (also units purchased, revenue, This technical element also helps mitigate the replication crisis that has plagued other scientiﬁc ﬁelds and is likely Table 1 shows the main diﬀerences between RCTs and A/B tests. These diﬀerences are mainly derived from the Iteration and rectiﬁcation be involved in that RCT where a handful of metrics and mediators are measured over a long period of time (months to years). A/B tests diminish the cost and accelerate iteration speed and do not directly feed recommendations back to the public domain. Hybrid testing that incorporates the strengths of A/B tests and RCTs should deliver rapid, trustworthy and continuous experimentation in the educational domain and should lead to actionable recommendations for the public education. The current generation of RCTs in education fall short of these ambitions principally due to prohibitively slow turnaround time. We believe that a paradigm shift of experimentation process is overdue, and that the next generation of experimentation in education will be heralded by incorporating modern experimentation practices. of short campaigns with smaller eﬀects (and large sample sizes) that compound over time. This is compatible with current thinking in the domain that argues that small size eﬀects requiring large sample sizes that are pedagogically sound and easy to implement, have an important impact [2]. Failing fast and moving on to the next idea seems to be better [13]. The speed of analysis required by A/B testing is calling for a change in how evaluation is performed. Assessments tend to be formative (aimed at informing the teacher to steer the class and target the materials) or summative (more formal student knowledge tests). The former are frequent and often done at the discretion of each teacher and not captured by online platforms; the latter are scarce and often mandated by an authority. Online platforms should allow for a more frequent of assessing the learners without them knowing they are actually being assessed. ate performance. Other metrics, such as productive engagement (minimising time on an online platform but maximising learning) or enjoyable focused time will have to be deﬁned to help educational A/B tests delver their value into the education domain. Interventions targeting hard-to-reach groups or school structures remain diﬃcult in education [2]. Online platforms can mitigate this challenge by enabling the experimentation on subsets of the population that would be diﬃcult to identify or pool together otherwise. A/B testing platforms enable the dynamic enrollment of pupils upon meeting some conditions (e.g. struggling with a piece of content of a given kind for more than X weeks in a row). While this may remain a challenge, scaling content utilisation to millions of learners and makes the chances of generalisation beyond the online platform the tests were performed on more likely. A/B testing and the ability to create an archive of experiments that can be used for later meta analysis can be helpful. Indeed, comparing the “causal web“ between any two experiments may prove a useful tool for generalisation [14]. level in observational studies [24, 25]. A/B testing in online educational platforms may enable the application of epidemiology-like techniques in education. Rather than designing for large size long lived experiments, education RCTs would have to shift to a sequence Many schools and organisations tend to focus on the assessment of learning outcomes as a key element to evalu- Hedges [23] highlights the generalisability of education research beyond the context where it was tested on. The possibility of determining information and knowledge propagation has been explored at a single course RCTs are known to present issues when adapting to increasingly complex interventions [23]. The same is true for A/B tests: the ﬁrst tests tend to result huge successes, but with each fruitful intervention the percentage of improvement becomes smaller. However, a succession of cheap, rapid and simple interventions, albeit small, cause a two-fold compound eﬀect: 1) over time on each single learner and 2) over the whole population when applied at scale. RCTs have been criticised for just explaining the why and not the how, making it hard to ﬁnd mechanistic explanations on how these interventions cause their eﬀects [23]. A/B tests are no diﬀerent. This situation is also similar to how medical research happens: an RCT discovers an unexpected eﬀect and this triggers a wave of more basic research operating at a molecular level trying to dissect the explanation for the observed evidence. This is the basis of inductive thinking. theory in deductive thinking scenarios. but less than policy makers [14, p. 21] is diluted by the utilisation of an experimentation platform that reduces technical and operational barriers and can be used by all of them alike, ensuring comparable rigour and standards. RCTs are not suited to answering all kinds of questions, and there are some things to which schools are just not willing to be randomised, such as ideological questions, ﬁnancial incentives for teachers that not all schools could follow, school timetables, etc. practice, therefore reducing the impact of the Hawthorne eﬀect and helping to make evidence more robust. [14] (p. 21) highlights how educational studies should be asking “under what circumstances does this work“. The wide access to millions of learners and teachers and iterative nature of A/B tests can help determine speciﬁc conditions for certain interventions to work. Moreover, it helps organisations build a historical memory of things that have not worked in their speciﬁc context. A/B tests platforms could learn about how to do meta-analyses to extract insight from A/B tests performed in diﬀerent campaigns or settings. and causal inference), which can help with how RCTs are prepared and screened and enable triangulation of data from many directions [14]. We’d like to thank Sandy Suh, Rath Bala, Mona Nainie, and PJ Williams for their useful feedback to prior versions of this manuscript. In addition, well thought campaigns of A/B tests can be used as tools to gather the evidence to prove or refute a The classic questioning about whether researchers know more about what constitutes evidence than teachers, A/B tests tend to be imperceptible by teachers and learners, blind to them and are embedded into common A/B testing platforms tend to include tools and support for forms of evidence other than RCTs (e.g. observational The authors are with an EdTech company running A/B tests to make content adapt to the needs of the learner.