As AI systems demonstrate increasingly strong predictive performance, their adoption has grown in numerous domains. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging eld of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research eorts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI models and AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the eld, and make a list of recommendations for future research. Our survey highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientic knowledge. We also hope this survey will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making. 1 INTRODUCTION Thanks to recent advances, AI has become a ubiquitous technology and been introduced into high-stakes domains such as healthcare, nance, criminal justice, hiring, and more [ of failures, complete automation is often not desirable in such domains. Instead, AI systems are often introduced to augment or assist human decision makers—by providing a prediction or recommendation for a decision task, which humans can choose to follow or ignore and make their own decision. Besides predictions, current AI technologies can provide a range of other capabilities to help humans gauge the model predictions and make better nal decisions, such as providing performance metrics of the model, or uncertainty and explanation for the prediction. In this paper, we refer to these capabilities as dierent AI assistance elements that an AI system can choose to provide. We refer to this general paradigm as human-AI decision making, though in relevant literature we found a multitude of generic terms used such as human-AI interaction, human-AI collaboration, and human-AI teaming. Figure 1 shows that the number of papers on this topic has been growing dramatically in the past ve years. Many of these papers made technical contributions for AI to better support human decisions, such as developing new algorithms to generate explanations for model predictions. Meanwhile, the research community is increasingly recognizing the importance of empirical studies of human-AI decision making by involving human subjects and performing decision tasks. These studies are not only necessary to evaluate the eectiveness of AI technologies in Fig. 1. The number of papers based on Google Scholar for two queries, human-AI interaction and human-AI decision making, over the past years. assisting decision making, but also to form a foundational understanding of how people interact with AI to make decisions. This understanding can serve multiple grounds, including: (1) to inform new AI techniques that provide more eective assistance or are more human compatible; (2) to guide practitioners in making technical and design choices for more eective decision-support AI systems; (3) to provide input for human-centric policy, infrastructure, and practices around AI for decision making, such as regulatory requirements on when AI can or cannot be used to augment certain human decisions [132]. However, empirical human-subject studies on human-AI decision making are distributed in multiple research communities, asking diverse research questions, and adopting various methodologies. Currently there is a lack of coherent overview of this area, let alone coherent practices in designing and conducting these studies, hindering concerted research eort and emergence of scientic knowledge. We recognize several challenges to coherence. First, empirical studies of human-AI decision making are conducted in various domains with dierent decision tasks. Without investigating the scope of these tasks and their impact, we may not be able to generalize from individual ndings. Second, human interactions with AI are enabled and augmented by the aordances of chosen AI assistance elements. Individual empirical studies tend to focus on a small set of AI assistance elements. There is a lack of common frameworks to understand how results for these AI assistance elements generalize and, therefore, their eect on human interactions with AI. Lastly, design of human-subject studies are inherently complex, varying depending on the research questions, disciplinary practices, accessible subjects and other resources, etc. This challenge is further exacerbated by the fact that methodologies, including evaluation metrics, to study human-AI interaction are still in an early development stage. To facilitate coherence and develop a rigorous science of human-AI decision making, we provide an overview of the current state of the eld — focusing on empirical human-subject studies of human-AI decision making — through this survey. We focus on studies with the goal of evaluating, understanding and/or improving human performance and experience for a decision making task, rather than improving the model. This scope dierentiates from prior surveys on empirical studies of human-AI interactions that either deviate from the scope of decision making or focus on only one aspect of it, like trust [63,89,126,127,141,146]. With the above-mentioned challenges in mind, our survey focuses on analyzing three aspects of study design choices made in these surveyed papers: the decision tasks, the types of AI and AI assistance elements, and the evaluation metrics. For each aspect we summarize current trends, identify potential gaps, and provide recommendations for future work. The remainder of this survey paper is structured as follows. We rst discuss the scope of papers surveyed, and methodology for paper inclusion and coding. We then present our analysis for each of the three areas mentioned above, with summary tables provided. We conclude the survey with a call to actions for developing common frameworks for the design and research spaces of human-AI decision making, and mutual shaping of empirical science and computational technologies in this emerging area. To allow for easy access to the papers that we cite, they are available at https://haidecisionmaking.github.io. 2 METHODOLOGY In this section, we dene the scope of our survey in detail and describe how we selected papers to include. Survey scope and paper inclusion criteria. AI decision making, where the goal is to evaluate, understand and/or improve human performance and experience for a decision making task, rather than to improve the model. As such, we specify the following inclusion criteria: •The paper must include evaluative human-subject studies. We thus exclude purely formative studies that focus on exploring user needs to inform design of AI systems, often qualitatively. •The paper must target a decision making task, thus we exclude tasks of other purposes (e.g., debugging and other forms of improving the model, co-creation, gaming). •The task must involve and focus on studying human decision makers, thus we exclude papers on AI automation or other AI stakeholders (model developers, ML practitioners). However, we do not limit our studies to those that implement complete decision making processes, but also include studies that claim to evaluate some aspects of decision makers’ perceptions, such as their understanding, satisfaction, and perceived fairness of the AI. Search strategy. conferences where AI and human-computer interaction (HCI) works are published from 2018 to 2021, to identify papers that t the criteria mentioned above. Specically, conferences we searched include ACM CHI Conference on Human Factors in Computing Systems, ACM Conference on Computer-supported Cooperative Work and Social Computing, ACM Conference on Fairness, Accountability, and Transparency, ACM Conference on Intelligent User Interfaces, Conference on Empirical Methods in Natural Language Processing, Conference of the Association for Computational Linguistics, and Conference of the North American Chapter of the Association for Computational Linguistics. We focused on NLP conferences in AI because (1) the fraction of papers with empirical studies on human subjects is low in AI (including NLP) conferences such as AAAI and NeurIPS; (2) the expertise of the authors enables regular examinations of all papers in NLP conferences. We further expand our considered papers by choosing relevant references from each paper and examining the papers that cite these papers. An initial search process yielded in over 130 papers. All the authors then looked through the list and discussed with each other to exclude out-of-scope papers, resulting in over 80 in-scope papers. We collated them in a spreadsheet then started coding these papers by the three study design choices they make: decision tasks, types of AI and AI assistance elements, and evaluation metrics. We started the coding by having one author extracting relevant information from the paper, such as what kind of decision tasks, AI models and assistance elements, and evaluation metrics were used in the study. A second-round In addition to papers that we were already aware of, we looked through proceedings of premier coding was then performed, focusing on merging similar codes and grouping related codes into areas. For example, decision tasks were grouped by domains, as shown in Table 3. For the second-round coding, all authors were assigned to look through the initial codes for one of the three study design choices. All authors met regularly to discuss the grouping and ambiguous cases to ensure the integrity of the process, until consensus on the codes and grouping, as presented in the summary tables, were reached. We provide summary tables for each study design choice and these tables can provide a quick overview of the literature space. 3 DECISION TASKS We start by reviewing decision tasks that researchers have used to conduct empirical studies of human-AI decision making. We group decision tasks used in prior studies based on their application domains (e.g., healthcare, education, etc.). To facilitate consideration on how the choices of decision tasks may impact the generalizability of study results, we highlight four dimensions that dier in these tasks—risk, required expertise, subjectivity, and AI for emulation vs. discovery—to help interpret results in these studies. 3.1 Tasks Grouped by Domains We group the decision tasks used in the surveyed papers by their application domains, as summarized in Table 1: Law & Civic, Medicine & Healthcare, Finance & Business, Education, Leisure, Professional, Articial, Generic, and others. Law & Civic. This domain includes tasks in the justice system and for civil purposes. The most commonly used task is recidivism prediction, which has attracted a lot of interest since the ProPublica article on biases in recidivism prediction algorithms used by the criminal justice system in the United States [6]. Popular datasets for recidivism prediction and the discussed variants include COMPAS [6], rcdv [121], and ICPSR [138]. Given these datasets, how these studies dened recidivism prediction varied. A common formulation is to predict whether a person with a particular prole will recidivate, or be rearrested within 2 years of their most recent crime [94,139,143] or reoend before bail [58]. Slight variations of this denition include: (1) predicting if a rearrest is for a violent crime within two years [54,55] or (2) predicting if the defendant will reoend or not, without the two year time limit [39,115]. In comparison, Green and Chen[54,55]dene the recidivism prediction task not as a binary prediction, but as assessing the likelihood that the defendant will commit a crime or fail to appear in court if they are released before trial, ranging from a scale from 0% to 100% in intervals of 10%. This more ned-grained question could potentially make it harder for human subjects to assess and use model output. In a slightly dierent set-up, Anik and Bunt[7], Harrison et al. [61], Lakkaraju et al. [82]dene the decision tasks as either to predict if a defendant is released on bail or predict one out of four given bail outcomes: (1) the defendant is not arrested while out on bail and appears for further court dates (No Risk); (2) the defendant fails to appear for further court dates (FTA); (3) the defendant commits a non-violent crime (NCA) and (4) the defendant commits a violent crime (NVCA) when released on bail. Prior works also explore how AI assistance can be applied to civil activities in the public sector. For example, DeArteaga et al. [35]examine the child maltreatment hotline and develop a model that assists call workers in identifying potential high-risk cases. Medicine & Healthcare.This domain includes tasks related to clinical decision making, ranging from medical diagnosis to sub tasks such as medical image search. The general formulation of medical diagnosis is to predict whether a patient has a disease given a list of symptoms or other information about the patient. Researchers have studied AI assistance for a range of medical diagnosis tasks, include general disease diagnosis [82], COVID-19 diagnosis [137] and balance Others disorder diagnosis [ during medical diagnoses. For example, Cai et al when diagnosing prostate cancer. Other imaging tasks include interpreting chest x-rays [ [86,87]investigate a support system for stroke rehabilitation assessment, which assists physical therapists in assessing patients’ progress. Due to the diculty in understanding and predicting biological processes in healthcare, these tasks can be dicult, even for medical experts (i.e., pathologists, radiologists). Lastly, unlike previously mentioned studies where the focus is on medical diagnosis, Levy et al. [90] investigates the eects on annotating medical notes with the help of AI assistance. Finance & Business. used in income and credit prediction include the Adult Income dataset in UCI Machine Learning Repository [ Table 1. Types of decision tasks grouped by application domains in human-AI decision making. 22]. Another popular area is imaging-related assistance to help medical sta make better decisions Lending Club [129]. As a result, an income task is to predict whether a particular person’s prole earns more than $50K annually [62,115,144,155], driven by the Adult Income dataset [113]. Note that the number, $50K, is outdated and somewhat arbitrary given ination. Other similar tasks include loan approval (e.g., assessing the likelihood of an applicant defaulting on a loan) [15, 55, 139], loan risk prediction [29], and freezing of bank account prediction [15]. In addition to income and credit prediction in the nancial domain, prior work that explores how AI assistance can help make other business-related decisions. Some classication tasks include sales forecasting prediction where Dietvorst et al. [36]dene a sales forecasting task to predict the rank (1 to 50) of individual U.S. states in terms of the number of airline passengers that departed from that state in 2011, marketing email prediction [60] where the task is to predict the better email to send given customers reactions, and predicting overbooked airline ights [15], On the other hand, regression tasks include forecasting monthly sales of Ahold Delhaize’s stores [97], property price prediction [1,111], apartment rent prediction [101], and car insurance prediction [15]. Lastly, Biran and McKeown[16] asks participants to decide if they would buy a stock given various AI assistance. Education.This domain includes decisions performed within the education system. Most tasks are broadly about forecasting student performance. Dierent variations exist: (1) predicting how well students perform in a given program [36]; (2) predicting if a student will not graduate on time or drop out [82] (3) predicting students’ grades in tests such as math exams [37,144]; and (4) making admission decisions [7,28]. Bansal et al. [13]also considered the test questions in Law School Admission Test (LSAT). Leisure.This domain includes tasks serving entertainment purposes. Prior works have explored AI assistance for a range of leisure activities (e.g., to recommend music [76,77], to recommend movies [78], to predict songs’ chart ranking (i.e. song popularity) [95]), and to reorder your Facebook news feed [112]. Other works used games or gamied tasks such as to predict if a person would date a person given a prole [96,152], classify types of leaves [147], and distribute goods fairly [88]. Games are also used, such as Quizbowl [43], draw-and-guess [23], word-guessing [48], and chess playing [32]. Professional.This domain includes tasks related to employment and professional progress. Binns et al. [15]dene a task to predict whether a person’s prole would receive a promotion. Liu et al. [94]dene a task to predict a person’s occupation given a short biography. Other tasks not related to jobs include classifying emails’ topic [125], AI-assisted meeting scheduling [74], military planning via monitoring and unmanned vehicles [130], and cybersecurity monitoring [42]. Articial.This domain includes tasks that are articial or ctional, usually made up to explore specic research questions. Lage et al. [79]and Narayanan et al. [104]created two ctional tasks to evaluate the eect of providing model explanations: (1) predicting aliens food preferences in various settings and (2) personalized treatment strategies for various ctional symptoms. Other tasks include predicting the number of jellybeans in an image [109], predicting water pipe failure [10], predicting news reading time [134], predicting broken glass [153], predicting defective object in a pipeline [11,12], and answering math questions [45]. Articial tasks have the advantage of being easily accessible to lay people, and allowing researchers to control for confounding factors. The ip side is that results obtained from articial tasks may not generalize to real applications. Generic.Generic tasks are ones without specied applications and can be applied to dierent domains. These include AI benchmarks where crowdsourced datasets are used to test how well AI models can emulate human intelligence such as object recognition in images (e.g., horses, trains) [3] and [27,52]. Another popular generic task is review sentiment analysis, which are performed with various contents such as movie reviews [ reviews [13]. Others.Finally, we list decision tasks that do not t in any of the domains above: attractiveness estimation [ activity recognition (e.g., exercise [ a person’s weight based on an image [ prediction [ of text; forest cover prediction, predicting if an area is covered by spruce-r forest [143]. 3.2 Task Characteristics Given the wide variety of decision tasks that have been studied, it is important to understand how ndings generalize across tasks. Although domain can serve as a thematic umbrella, it is not useful for evaluating generalizability because each domain includes tasks with drastically dierent properties (e.g., medicine & healthcare includes both diagnosing cancer and annotating medical notes). Here we seek to identify meaningful task characteristics. Characteristics of the decision task can determine whether a task is appropriate for the claims in a study as well as their generalizability. For example, a low-stakes decision task may not create an ideal condition with vulnerability to study trust. A task that is more challenging for human to perform may induce higher baseline reliance on the AI, so the results may not generalize to settings where the human outperforms the AI, and vice versa. However, existing literature often does not provide explicit justication on the choices of decision task, nor indicate the scope of generalizability of the results. To facilitate future research to make such considerations, we look across the surveyed papers and highlight four dimensions that vary in the chosen design tasks: (1) task risk (e.g., high, low, or articial stakes), (2) required expertise in the task, and (3) decision subjectivity, and (4) AI for emulation vs. discovery. We do not claim these four as an exhaustive list, but hope to illuminate the challenges in interpreting and generalizing from results in studies that adopt dierent decision tasks, and encourage future studies to justify the choice of tasks and report their characteristics. Risk.The risk, including its stakes and potential societal impact, of a task (whether high, low, or articial) is an important characteristic that could impact decision behaviors, particularly for user trust and reliance. In fact, Jacovi et al. [65] Tasks in Law & Civic, Medicine & Healthcare, Education and Finance & Business, are mostly considered high stakes. In comparison, leisure and articial are mostly of relatively low stakes. Tasks in Professional can have varying stakes, for example, human resource related decisions are high stakes, while email topic classication is low stakes. Generic is a category driven by the creation of AI benchmark datasets. It is unclear how to interpret their stakes or societal relevance, and their stakes are contingent on the contexts they are adopted. Researchers should carefully consider design choices with respect to risk: it is also critical to be cognizant of potential ethical concerns when using AI assistance for high-stakes decisions, such as recidivism prediction [ unwarranted trust can be highly problematic in high-stakes decisions [ by risk as well; for example, ndings related to AI assistance eectiveness in the context of low-risk scenarios, especially on reliance, may not generalize to high-risk scenarios without further research. Required expertise. For some tasks, limited to no domain expertise is required (e.g., articial tasks), whereas others require signicant ]; toxicity classication [26], predicting external consensus of whether a comment is toxic or not; predicting 114], predicting whether text is about Christianity or atheism; emotion analysis [128], predicting emotion argue that trust can only be manipulated and evaluated in high-stakes scenarios where vulnerability is at play. expertise (e.g., cancer image classication). In the ideal situation, participants in user studies for tasks that require domain expertise should be experts (i.e., judges, radiologists) or target users, however due to pragmatic reasons, some of the studies are performed by laypeople on crowdsourcing platforms. For instance, it is unclear whether results derived from crowdworkers would generalize to judges or doctors. Expertise required is often correlated with risk. High-risk tasks in Law & Civic, Medicine & Healthcare, Education and Finance & Business usually require expertise, while low-risk tasks in leisure and articial do not. Similarly, tasks in Professional can have varying requirements in expertise, for example, human resource related decisions may require more expertise than email topic classication. Generic usually do not require expertise. While many works in human-AI decision making categorize decision makers as either “domain experts” or “lay users”, AI literacy is also an important consideration, especially as it relates to one’s ability to interpret AI assistance elements. A framework proposed by Suresh et al. [133]suggests decomposing stakeholder expertise into both context (domain vs. machine learning) and knowledge for that context (e.g., formal, personal, etc.). For example, decision making performance with AI-enabled medical decision support tools [24], is aected by both formal, instrumental, and personal domain expertise as well as instrumental machine learning expertise (i.e., familiarity with ML toolkits). As such, systems should be evaluated with targeted expertise, or even varied levels of expertise to investigate the generalizability of results. Studies should also carefully report on participants’ expertise to allow appropriate interpretation and usage of the results. Subjectivity.Many decision tasks are framed as supervised prediction problems in machine learning, where there exists groundtruth,𝑦. This choice often implicitly assumes that this is an objective prediction task (at least in hindsight), e.g., whether a person has a balance disorder or not [22] or whether a person will pay back a loan [15,55]. Only in these tasks, quantitative measures of human performance are appropriate. In comparison, personal decision making can be subjective, for example, whether a music recommendation is good is subjective for the person receiving it [76, 77]; similarly, whether or not language is perceived as “toxic” depends on the person assessing it and their determination is hard for others to refute [26]. Subjective decision tasks typically have high variability (low agreement) on what is the correct model output. Yet, AI assistance is still valuable to help people make subjective decisions. However, human performance might not be a good measure for evaluating these subjective decision making tasks, or non-trivial assumptions are required to convert such decisions to objective tasks (e.g., predicting which movie has the highest box oce proceeds). As a major focus so far in human-AI decision making is to improve the performance of human-AI teams, most of the tasks in our surveyed papers are objective tasks. AI for emulation vs. discovery.We highlight a nal dimension that aects how one should interpret results from a study but is often overlooked in the choice of tasks. Within objective tasks, we can further distinguish tasks based on the source of groundtruth. In many high-stakes decisions, groundtruth can come from (social and biological) processes that are external to human judgments (e.g., the labels in recidivism prediction are based on observing the behavior of the defendants after bailing rather than judges’ decisions). In these tasks, machine learning models can be used to discover patterns that humans may not recognize, and can be useful for tasks such as recidivism prediction [39,54,55,58,94,115,139,143], deception detection [80,81,94], and income prediction [62,115,139,144,155]. We refer to such tasks as AI for discovery tasks.These tasks are usually more challenging to humans, because they require humans to reason about external (social and biological) processes that are not innate to human intelligence. In fact, human performance in some of these tasks, such as deception detection tasks [ guessing (manual annotation is thus inappropriate for getting groundtruth). Humans decisions are also prone to biases in these challenging reasoning tasks such as recidivism prediction. AI can improve decision ecacy and alleviate potential biases not only by providing predictions, but also by elucidating the embedded patterns in these decision tasks, such as by providing explanations. However, a challenge lies in the diculty for humans to determine whether counterintuitive or inconspicuous patterns are genuinely valid or are driven by spurious correlations. In comparison, a typical narrative of AI is to emulate human intelligence. For example, humans perform well at simple recognition tasks, such as determining whether images include people or whether documents discuss sports, and we build AI to emulate this ability. That is, machine learning models are designed to emulate the human intelligence for these tasks, and human performance is considered as the upper bound. In these tasks, the groundtruth comes from human decision makers. We refer to these tasks as emulation tasks. As such, these tasks are designed for automation purposes, and are not preferred choices for studying human-AI decision making because humans are less likely to benet from AI assistance. However, there are still a handful of experimental studies investigating human-AI decision making in emulation tasks [ might be interpreted as reducing the mistakes of crowdworkers (possibly due to the lack of attention). It is unclear whether results would generalize to discovery tasks where humans reason about external processes and models may identify counterintuitive patterns, and future research should explicitly consider the boundary between the two. 3.3 Summary & Takeaways We summarize current trends in the choices of decision tasks, discuss gaps we see in current practices of the eld, and make recommendations towards a more rigorous science of human-AI decision making. We follow this organization when summarizing each of the remaining sections. Current trends. (1) Variety: domains. This variety demonstrates the potential of human-AI decision making and also leads to challenges in generalization of results and developing scientic knowledge across studies. (2) Task characteristics: & healthcare, nance, and education; while articial and generic tasks are still used by some. Although many decision tasks require domain expertise, experts are seldom the subjects of study. Finally, most existing studies focus on “AI for discovery” tasks because humans typically need or can benet from AI’s assistance in these tasks more than “AI for emulation” tasks. However, studies often do not explicitly justify using decision tasks with these characteristics nor discuss their implications for other study design choices (e.g., subjects) and generalizability of results. Gaps in current practices. (1) Choice of tasks are driven by datasets availability. ICPSR [ civic domain. In comparison, despite the public discourse on the potential harm of AI in other domains like hiring [ Existing studies on human-AI decision making cover a wide variety of tasks in many application 138] datasets, many studies used recidivism predictions as the decision task and focused on the law & 33], there is relatively little research on AI assistance in human resources due to lack of available datasets. We suspect that this is also the reason that emulation tasks are used in some studies (e.g., prevalence of AI benchmarks such as visual question answering). (2) Lack of frameworks for generalizable knowledge.A key question for the research community is how to develop scientic knowledge by validating and comparing results from studies across many dierent domains and types of decision task. For example, when an articial task is used, how much can the results generalize to other domains? How to interpret dierences in the results in a medical diagnosis task versus a movie recommendation task? Do results on medical diagnosis generalize to bailing decisions? We believe a rst step is to identify dierent underlying characteristics of decision tasks such as risk and required expertise, in order to make meaningful comparisons across studies and reconcile dierences in empirical results. (3) Misalignment with application reality.The focuses and study design choices of current studies may not align with how AI is or will be used in real-world decision-support applications. For instance, the overwhelming focus on high-stake domains is worrisome if the study designs (subjects, consequence, context) do not align with the reality. Tasks dened based on easily available datasets may deviate from realistic decision making scenarios. For example, experiments based on generic tasks such as visual question answering can be quite dierent from real-world imaging related tasks such as for medical diagnosis. This misalignment is analogous to the discrepancies between the recent burst of COVID-related machine learning papers and clinical practices [116]. Recommendations for future work. (1) Develop frameworks to characterize decision tasks.To allow scientic understanding across studies, there is an urgent need for the eld to have frameworks that can characterize the space of human-AI decision tasks. As a starting point we suggest the following dimensions in Section 3.2: risk, required expertise, subjectivity and AI for emulation v.s. discovery. We encourage future work to further develop such frameworks. We further encourage specication, such as including meta-data of task characteristics whenever a new decision task or dataset is introduced to study human-AI decision making. (2) Justify choices of de cision task.We encourage researchers to articulate the rationale behind the choice of decision task, including its suitability to answer the research questions. Researchers should also consider whether other study design choices such as system design, subject recruitment and evaluation methods align with the characteristics of the task. Such practices can help interpret and consolidate results across studies and identify important and new dimensions of decision task characteristics. (3) Expand datasets availability.A bottleneck hindering the community from studying broader and more realistic decision tasks is the availability of datasets. Popular datasets are often introduced for AI algorithmic research and may not reect what is needed for realistic AI decision-support tasks. The eld should motivate dataset creation by what decision tasks are needed to better understand human-AI decision making, which may require rst better understanding decision-makers’ needs for AI support. 4 AI MODELS AND AI ASSISTANCE ELEMENTS To use AI to accomplish decision tasks, people not only rely on the model’s predictions, but can also leverage other information provided by the system to make informed decisions, including gauging if the model predictions are reliable. For example, with the recent surge of the eld explainable AI (XAI), many have contended that AI explanations could provide additional insights to assist decision making [40,81,93]. Therefore, we take a broad view on “AI assistance Table 2. Dierent AI model types used in human-AI decision making, grouped into 3 categories: (1) deep learning models, (2), “shallow” models, and (3) Wizard of Oz. We exclude logistic regression and linear regression from GAMS. elements” and review the system features studied in prior work that could impact people’s decision making. We start with an overview of the types of model and data used in the surveyed studies and then unpack AI assistance elements. 4.1 AI Models and Data An important driving factor for the recent surge of interest in human-AI decision making is the growing capacity of AI models to aid decisions. This subsection provides a summary of the dierent types of models used in surveyed studies, as listed in Table 2. Deep learning models. demonstrate strong performance in a wide variety of tasks and can even outperform humans. Deep learning models are based on neural networks, which usually consist of more than two layers. Deep learning models have been included in many recent studies on human-AI decision making [ 110,152]. Some papers specied their deep learning models, e.g., convolution neural networks [ networks [ settings, deep learning models typically provide greater predictive power than traditional “shallow” models but with the expense of added system complexity. Deep learning models are commonly considered not directly interpretable and thus raise concerns of user trust. To tackle this challenge, many “post-hoc” explanation techniques [ developed to approximate the complex model’s logic, which also raise concerns about explanation delity [ discuss some examples of post-hoc explanation techniques studied in Section 4.2. “Shallow” mo dels. traditional, “shallow” models, which are often easier to train and debug. These models include generalized additive models (e.g., logistic regression and linear regression) [ trees/random forests [ shallow neural networks [ factorization [ accuracy to deep learning models [ interpretable. For example, coecients in linear models as feature importance and shallow decision trees are relatively 26,60], BERT [80], and RoBERTa [13], a hybrid LSTM and CNN model for VQA task [115]. In average training 78]. In prediction tasks with a small number of features, “shallow” models are able to achieve competitive intuitive to comprehend. It is worth noting that more papers used shallow models instead of deep learning models to conduct empirical studies on human-AI decision making. Wizard of Oz.Finally, many experiments did not use an actual model, but instead having researchers manually creating and simulating the model output, a common method called “Wizard of Oz” (WoZ) in HCI research [71]. Researchers have used WoZ method with ctional cases of model predictions and explanation styles [7,15,20–22,79,95,96,104,109,139]. WoZ is not only convenient for conducting user studies without investing in technical development, but also gives researchers full control over the interested model behaviors. For instance, utilizing this approach allowed researchers to adjust the algorithm accuracy [109], control error types [21], and test dierent explanation styles [7,15,20,22]. However, it can be challenging to design realistic WoZ studies given the complexity of model behaviors. Failing to do so could impair the validity and generalizability of study results. Data typesBesides the models, it is also important to distinguish dierent types of data used: text, imagery, audio, video, and tabular (or structured data). The surveyed papers used a number of data types, including plain text (e.g., LSAT questions [13], hotel reviews [80,81], etc.), imagery (potentially cancerous images [25], meal images [21], etc.), video (stroke patient rehabilitation videos [87], kitchen activity videos [107], etc.), and tabular (or structured) data (e.g., company nancial data [16], personal nancial data [29], etc.). For some tasks, combinations of data types are used. For example, in music recommendations, humans might review structured data (e.g., song title, artist, and genre) and listen to audio when determining whether to listen to a recommended song [76]. Video question answering requires assessing both images as well as textual questions about them [27]. The data type inuences what ML models can be applied and how well they perform. For example, shallow models may perform relatively better on structured (or tabular data) than unstructured text compared to deep learning approaches, because of the high dimensionality of text data. More importantly, the data type can determine the nature of the decision task and the aordances of AI assistance for the decision. For example, a common form of AI assistance is explanation of the model outputs, such as the models’ attention. For example, prior work found that attention explanations have limited utility for explaining image classications [3]. This might be because such explanations—where important areas of the image are painted—can be noisy and confusing to humans compared to attention explanations for text data where important words are highlighted. Data type can also inuence the experience with the decision tasks in many ways: video can take longer to review than images or short text. 4.2 AI Assistance Elements A central consideration in studies of human-AI decision making is what kind of assistance is eective in improving decision outcomes. At the minimum, models can assist decision makers by providing predictions, for example, music or movie recommendations, or generating a health risk scores. It is often desirable to provide information about model predictions to help users judge whether they should follow them, especially in cases of disagreement or in high-stakes domains. It is also common to provide information about the model to help users gain an overall understanding of how the model works or the data it was trained on, which can inuence their perceptions of and interactions with the model in decision making tasks. Figure 2 illustrates our taxonomy of AI assistance elements, which includes predictions, information about predictions, information about models, and other AI system elements that govern the use of the system (e.g., workows, user control, and varied model quality). Based on this conceptualization, we categorize the AI assistance elements studied in the survey paper into these four groups, as listed in Table 3. It is interesting to note that many of the studies reviewed focused on providing information Fig. 2. A diagram of AI assistance elements. Systems might simply provide a prediction for which the decision-maker can choose to follow or ignore, or they might provide additional assistance in the form of information about that prediction, information about the model (or training data), or other AI system elements, such as user agency or workflows. about model predictions or the model in the form of model explanations, which are hypothesized to help humans understand model’s predictions, detect errors, as well as gain additional information or knowledge about the decision task at hand. 4.2.1 Model Predictions. Providing a model’s prediction is the most natural form of assistance for decision making. For example, in medical decision support tasks, models can predict medical diagnoses based on patient information; here, a doctor can decide whether to follow or ignore the prediction from the model in favor of a dierent diagnosis. We can dierentiate AI assistants based on the types of predictions they provide, including binary predictions (e.g., positive or negative sentiment or binary recidivism prediction) [ recommendation) [ (e.g., house prices) [ predictions or perform multi-step decision making [ system of Buçinca et al replacement alternatives for it. And, while the majority of systems show only the top prediction, some provide prediction alternates as additional options for users to review [10, 13, 42, 43, 60, 130]. It is worth noting that some papers did not study AI assistance with predictions [ based on other information, without implementing the complete decision making process. For example, Buçinca et al. [20] the explanations. Anik and Bunt explanations. Another reason for not revealing model predictions is to retain as much human agency as possible in the decision making process [81]. 4.2.2 Information about Model Predictions. Information about predictions can include the model’s uncertainty for the prediction and other prediction-specic explanation (or local explanations), which help users understand why the prediction was made. Such local explanations include local feature importance, rule-based explanations, example-based ,137,143,152,153,155], multi-class, or categorical predictions (e.g., object type in an image or song ,112,139,144]. One reason is that they chose to focus on evaluating people’s perceptions of the model , Chandrasekaran et al. [27]ask humans to simulate the prediction or predict its correctness based on Model uncertaintyClassication condence (or probability) [10,13,21,22,43,60,68,73,86,87, Example-based methodsNearest neighbor or similar training instances [15,20,23,24,39,62,77,78,81, Counterfactual explanationsContrastive or sensitive features [39,97], counterfactual examples [15,45,92, Global feature importanceCoecients [39], permutation-based [143], shape function of GAMs [1], Wiz- Presentation of simple modelsDecision sets [82], decision trees [45], linear regression [111], logistic regres- Global example-based explanationsModel tutorial [80], prototypes [23, 43, 74] Model documentationOverview of the model or algorithm [74,76,77,88,112], model prediction Information about training dataInput features or information the model considers [61,77,111,155], aggregate Interventions or workows aect-User makes prediction before model [21,58,96,111,143,152,155], vary system ing cognitive processresponse times [21,109], outcome feedback to user [12,13,27,58,147,153], Levels of user agencyAllowing user feedback or personalization for model [43,76,125], outcome Table 3. AI assistance elements explored in prior work on human-AI decision making, grouped into four categories: (1) model predictions, (2) information about model predictions, (3) information about models (and training data), and (4) other AI system elements aecting user agency or experience. explanations, counterfactual explanations, and natural language explanations. For our review, we primarily focus on the forms of explanation and how they are presented to humans instead of the underlying algorithm/computation. Model uncertainty. more or less sure, so long as uncertainty estimates are reliable [ not over-rely on the prediction and resort to their own judgment or other resources. The most common form of uncertainty information is a condence score or prediction probability for a classication model. Here, uncertainty is usually calculated as the probability, a numeric value between 0 to 1, associated with the predicted label (opposed to alternative labels) given by the underlying ML model. Many prior systems expose classication condence scores to human decision makers [ studies, Buçinca et al the classication model to generate probabilities and typically present these scores alongside the prediction. Other classication systems expose uncertainty with labeled categories. For example, Levy et al as low condence in one version of their clinical notes annotation system. In contrast to classication, uncertainty information of regression models is currently under-explored in human-AI decision making, with one recent exception [ long been studied outside the context of AI assistance, e.g. [70]). Uncertainty for regression models can take the form of uncertainty distribution—how the possible values are distributed (often centered around the given prediction)—or prediction interval—the range of possible values. Currently there is also a lack of discussion on the reliability, or sometimes referred to as calibration [ estimates, and whether decision makers can make sense of uncertainty estimates properly. For example, in some deep learning models, prediction probabilities are prone to overcondence [ probabilistic models to give more reliable uncertainty estimations, including Bayesian neural networks [ deep neural networks that integrate dropout [ reliability of uncertainty information aects decision making, or how to communicate the reliability or a lack thereof, remain open questions in the context of human-AI decision making. Local feature importance. is made to assist humans’ judgment of the prediction and inform their nal decision. A common local explanation type is local feature importance, which, given a single instance, quanties the contribution (or importance) of each of its features to the model’s prediction of it. For example, when predicting property values, certain features are more important to the prediction (e.g., lot size and number of rooms) while others might be less important (e.g., distance to a school). Tsai et al recommendation [because]: 1) you are feeling ill, 2) you have COVID-19 related contact history...). We enumerate the common ways to derive local feature importance grouped by built-in, post-hoc, and Wizard of Oz techniques: • Built-in methods directly from the models. Models that can generate feature importance explanations from their coecients are often considered as “directly interpretable”. For instance, the coecient for GAMs is a direct measure of feature importance [ mechanism is a common component in deep learning models, especially in the context of NLP. Attention as explanations shows a subset of input features that the deep learning model should pay more attention to for a . [137]use feature importance to explain the diagnosis for a COVID-19 chatbot (e.g., “I [provided] this 120], when each feature is scaled to have the standard deviation. Many papers [16,28,39,43,49,55, 81,86,87,94,106,111,125,128,143] adopted coecients as the feature importance scores. Second, attention particular prediction. Carton et al. [26], Chandrasekaran et al. [27], Lai et al. [80]adopt attention mechanism to model the local feature importance. • Post-hoc methodslearn to generate explanations separately for a trained model, often a non-interpretable model such as deep neural networks. These methods can be grouped into: gradient-based [27,73,106], propagationbased (LRP [3]), and perturbation-based (e.g., LIME [3,13,62,106], SHAP [29,144,155]). First, gradient-based methods compute the gradient of the prediction with respect to the input features. Examples using gradient-based methods include classication activation map (CAM) [73] and Grad-CAM [27]. Second, propagation-based methods, especially Layer-wise Relevance Propagation (LRP), uses a forward pass and then a backward pass to calculate the relevance among input features. Alqaraawi et al. [3]adopted LRP in their experiments. Third, perturbation-based methods, such as SHAP and LIME, manipulates parts of the inputs to generate explanations. LIME [114] uses a sparse linear model to approximate the behavior of a machine learning model locally. The coecients of this sparse linear model can then serve as explanations. Alqaraawi et al. [3], Bansal et al. [13], Hase and Bansal [62], Nguyen[106]use explanations from LIME as AI assistance. SHAP (SHapley Additive exPlanations) provides the marginal contribution of each feature for a particular prediction, averaged over all possible permutations, which is rst proposed by Lundberg and Lee[98]. Shapley values assign each feature an importance value for a particular prediction. In the context of human-AI decision making, Weerts et al. [144], Zhang et al. [155] use SHAP for local feature importance. In addition, deep learning models to generate video-specic feature captions [107] are also being used. • Wizard of Ozallows manual creation of local feature importance. Researchers often choose to adopt Wizardof-Oz to simulate both the AI model predictions and their explanations. Wizard-of-Oz is often chosen to save technical investment and have more control over studying interested AI model behaviors and explanation styles, as mentioned in Binns et al. [15], Buçinca et al. [20,21], Bussone et al. [22]. However, they can deviate from the actual output of existing explanation methods, and researchers should carefully justify the design choices and consider the limitations. Rule-based explanations.Rule-based explanations are constructed with a combination of rules, where a rule can be a simple ‘if-then’ statement. Both built-in approaches (e.g., decision sets and decision trees) and post-hoc approaches (e.g., anchors) have been explored for generating rule-based explanations in the context of human-AI decision making. • Decision sets: Lakkaraju et al. [82]generate interpretable decision sets, which are sets of if-then rules to explain model decisions. In their study, participants are asked to describe the characteristics of certain classes (e.g., depression) based on the learned decision set for that class. Lage et al. [79], Narayanan et al. [104]also used decision sets as local explanations in their studies. • Tree-based explanations: For decision tree-based models, local explanations can be generated directly by the decision-tree path, as rules that the model followed, to reach the given decision. Tree-based explanations were used by Kulesza et al. [77]in the context of a music recommendation system, which employed a content-based decision tree approach for selecting songs. Lim et al. [92]used the underlying decision tree model to generate multiple types of explanations (why, why not, how to, and what if ), such as by the decision-tree paths to reach an alternative decision. • Anchors features) that guarantee the given input to have the prediction, such that changes to the rest of the features will not change the prediction. Hase and Bansal in their experiments. Example-based methods. outcomes) to support case-based reasoning. A common formulation is to nd instances from training dataset that are similar to the given input. The explanations should include their labels in the ground truth to help users make sense of reasons behind the current prediction. For example, we might explain the predicted price for a given home by showing similar homes with their actual prices. A common and simple way to generate similar instances is to nd the nearest neighbors in the embedding (latent representation) space. This method is used by many papers to explain predictions for human-AI decision making [15, 20, 23, 24, 39, 62, 77, 78, 81, 137, 143]. Counterfactual Explanations. change to get an alternative prediction, answering “why not” (a dierent prediction) or “how to be a dierent prediction” instead of a “why” question. Counterfactual explanations can also be provided based on either features or examples. Feature-based ones are often called contrastive feature or sensitive feature methods—highlighting features that if changed, often implying minimum change, will alter the prediction to the alternative class. For example, a counterfactual explanation for loan prediction task could be “you would have received the loan if your income was higher by $10,000.” Example-based are often called counterfactual examples, by providing examples in the training data or hypothetical examples with the alternative class label but minimum dierences from the current input. A small but growing number of prior works studying human-AI decision making have utilized counterfactual explanations, such as contrastive and sensitive features [ explanations to help people discover unfair decisions (by changing just the race feature, the model would have a dierent prediction). And, Lucic et al ranges that lead to reasonable predictions Similarly, other work uses counterfactual examples include [ For example, Wang and Yin [45] asked users to answer ‘what if’ question given a perturbed input. Natural language explanations. tions, are a form of “why” explanations that provide the reasoning or rationale behind a particular decision. For example, Tsai et al. [137] diagnostic questions to the user. These explanation types are sometimes referred to as “justications” [ language explanations can be dierentiated by how they are generated, either model/algorithm generated [ where these explanations are produced by the system—or human experts generated [ algorithm developers) provided rationales behind types of predictions to be shown to users. Partial decision boundary. latent space around a specic input, in order to show how the model behaves as the input changes. The methods were initially proposed in the computer vision domain [ method for text and tabular data. 4.2.3 Information about models (including training data). Users of AI systems are often interested in better understanding the underlying model to form an appropriate mental model that can help them interact more eectively. “Global” , proposed by Ribeiro et al. [115], learns if-then rules representing “sucient” conditions (important study rationale-based explanations for their COVID-19 chatbot, such as why the chatbot asks particular information about the model can include the model’s overall performance, global explanations (e.g., how the model weighs dierent features, visualizing the whole model processes for simple models), input and output spaces, information about the training data, provenance, and more. Recently, there are growing interests in providing documentation or ‘About Me” page to present such global information (e.g., Model cards [102], FactSheets [9]). In this section, we discuss what types of global information about models have been studied in surveyed papers of human-AI decision making. Model performance.Model performance describes how well a model works in general. In studies of human-AI decision making, model performance has been mainly presented in the form of accuracy (i.e., percentage of correctly predicted instances) [61,80,81,147,152]. These works typically explore how observing model accuracy aects people’s perception of and decision making with the model. For example, Lai and Tan[81]investigate whether human subjects’ awareness of the ML model’s accuracy improves their performance in decision making tasks. And, Yin et al. [152] studies the eect of accuracy on human’s trust in ML models. Model performance has also been described by false positive rates, or how frequently the system mislabels an input as a particular class. For example, Harrison et al. [61]showed the presenting false positive rates in addition to accuracy in their experiments helped people gauge fairness of the model for recidivism prediction tasks. It is useful to note that accuracy information is usually estimated on a held-out dataset, and the model’s actual performance in deployment can shift, especially when the actual decision inputs or their distribution dier from that of the training data. This gap between communicated accuracy and experienced accuracy has been studied in Yin et al. [152]. Future work should also explore the eects of other types of performance metrics, such as precision and recall. Global feature importance.Dierent from local feature importance that quanties each feature’s importance to a specic prediction, global feature importance quanties each feature’s overall importance to the model’s decisions for a given task. Here we enumerate methods used in surveyed papers for computing global feature importance, grouped into built-in methods, post-hoc methods, and Wizard of Oz as follows: • Built-in methodsderive the global feature importance score directly from the trained model. Two such examples are coecients and shape function of GAMs. Coecients derived from a logistic regression model encode the relative importance of each feature. For example, Dodge et al. [39]use coecients to present the global inuence of features by showing their positive or negative eect on the decision boundary. The shape function of GAMs is used to inspect the global feature importance for GAMs. Mathematically, GAM is modeled as 𝑦 = 𝛽+ 𝑓(𝑥) + 𝑓(𝑥) + .... + 𝑓(𝑥), where𝑦denotes the prediction and𝑥denotes the input feature. The shape function𝑓describes the global importance of feature𝑥. Abdul et al. [1]visualizes the shape function via charts that serve as global explanations of their system. • Post-hoc methodsgenerate global feature importance in a post-hoc manner for a trained model, often for a complex, non-interpretable model. One such method is permutation importance, also known as variable importance (VI), which measures the increase in model prediction error when an input feature is permuted. Early work used permutation importance to compute global feature importance for random forests Breiman[18] followed by a rich line of research on the topic [4,34,56,57,131,156]. More recently, Fisher et al. [44]proposed a model-agnostic version of permutation importance. Wang and Yin[143]adopt this method Fisher et al. [44] to compute global feature importance in their paper, exploring whether such explanations are helpful during decision making tasks. • Wizard of Ozmanually constructs the global feature importance. For example, Binns et al. [15]created scenarios of recidivism prediction with hypothetical global feature importance to explore people’s fairness perception. Presentation of simple models. to humans to give them a detailed view of how the model makes decisions. These simple—often referred to as inherently transparent or intrinsically interpretable—models can be presented to humans in the form of decision tree, rule sets, graphs, or other visualizations. For this reason, such models are often preferred over more complicated architectures (e.g., neural nets) when interpretability is desired. In the context of human-AI decision making, researchers have explored presentations of simple models, including decision sets [ For example, Lakkaraju et al behavior of blackbox models in certain parts of feature space. Friedler et al decision tree as a node-link diagram and both a logistic regression and a one-layer MLP as math worksheets that are intended to “walk the users through the calculations without any previous training in using the model.” Global example-based explanations. the prediction or provide insights to the data which would help humans make task decisions. Lai et al examples with features that provide great coverage from training set as tutorial to the task using the SP-LIME algorithm [114]. They also proposed the Spaced Repetition algorithm that creates a set of examples which exposes humans to important features repeatedly. Another common approach is to pinpoint one or a set of training samples that are representative of prototypical instance (with the outcome of a given prediction class) [ instance is called a prototype. For linear models, it is natural to nd the important training example based on the distance in the representation space [ are proposed. Kocielnik et al class respectively to help user understand how the AI component operates. Note that prototypical examples can also be used to explain a prediction locally, by presenting the prototype in its proximity, such as the explanations used in Cai et al. [23], Feng and Boyd-Graber [43]. Model documentation. an overview of the model characteristics but also how it is developed and intended to be used, are discussed in recent literature as critical to AI transparency and governance [ explored using relevant features in human-AI decision making [ assistant of Kocielnik et al Scheduling Assistant examines each sentence separately and looks for meeting related phrases to make a decision.” Some work on model documentation argued for the importance of providing an overview of model’s input and output spaces, such as the output distribution. van Berkel et al participant’s demographic information (e.g., a male participant rst sees the data of male loan applicants). Information about training data. models by providing information about the data on which they were trained, such as the inputs features used or data distribution [ aware of whether or not the model considers “marital status” as a feature. Some studies present demographic-based or aggregated statistics of the training data [ training data explanation, including how the data was collected. They also describes demographics, recommended usage, potential issues, and so on. 82] and trees [45], linear [111] and logistic regressions [45], and one-layer multilayer perceptron (MLP)s [45]. 39,61,77,111,155]. For example, in the income prediction system of Zhang et al. [155], humans are made 4.2.4 Other AI System Elements Aecting User Agency or Experience. Besides providing information about the AI to assist decision making, prior research also studied additional system elements that can aect user experience, mainly around interventions that aect users’ cognitive processes of decision making, or users’ agency over the system. Interventions or workows aecting cognitive processes.Besides providing information about the model and predictions, how people process such information to form perceptions of AI and make decisions can be impacted by interventions that change their cognitive processes One area of interventions is concerned with how to design the workow, such as when users make their own decisions versus seeing the model’s predictions. The typical paradigm of human-AI decision making is to have models providing predictions, then users can choose to follow or ignore. Some studies explored having users making their own predictions before being shown the model output [21,58,96,111,143,152,155]. Such designs force people to engage more deliberately with their own decision making rather than relying on the model predictions. Prior work also explored the impact of dierent workow design on users’ mental models of how AI assistants make decisions. Some systems include a training phase prior to the task, during which users review model outputs and explanations of how the system works [27,76,80,111,155]. In some real-world scenarios, decision makers can see the actual outcomes of decisions. Studies have also explored how receiving outcome feedback on either their or the models’ decision correctness [12, 13, 27, 58, 147, 153] impact people’s perception of the models and performance of the tasks. Another type of intervention studied is system response time, or how long the system takes to provide a decision [21, 109]. For example, Buçinca et al. [21]compare the eect of cognitive forcing functions, where participants get suggestions immediately as opposed to having to wait 30 seconds for the machine’s prediction, on over-reliance and subjective perception. The models’ actual performance (as opposed to communicated performance metrics described in Section 4.2.3) can govern the usefulness of the decision support. Prior work also explored how varying model performance or prediction quality impacts human-AI decision making [42,74,90,107,109,125,152,153]. For example, Smith-Renner et al. [125] explored whether describing model errors, which enables users to gauge model performance, without the opportunity to make xes yielded user frustration for both low and high quality models. Similarly, Kocielnik et al. [74]studied the dierence between high precision and high recall models (without explicitly showing this information to users) on user perceptions. Lastly, some studies looked at how the source of assistance, whether from an AI versus from a human, aect decision making [36,78,95]. For example, Kunkel et al. [78]explore how machine generated versus human generated explanation impact the acceptance and trust of a system for movie recommendations. Levels of user agency.Typical decision-support AI systems work in a closed loop without the possibilities for guidance from end users. This kind of set-up limits the agency users can have for controlling or improving the decision assistance from the AI. Some studies have explored improving the user agency, such as allowing and incorporating user feedback on predictions or personalization of the model [43,76,125]. For example, in the music recommendation system of Kulesza et al. [76], participants can provide feedback about the recommended songs and guidelines to the model to improve future recommendations. Another studied form of user agency is the ability to guide the prediction (or outcome) either before [74] or after the model’s decisions [37,88,155]. For example, Kocielnik et al. [74]explore user experience with a system for detecting meeting requests from emails. They compare whether providing users a slider to control whether the system tends towards false positive (high recall) or false negatives (high precision) improves experience. This control occurs before the system makes predictions, but can be updated as needed. Lee et al. [88]study whether allowing participants to override the outputs of a system on how to split food between grad students promotes fairness perception. More recently, interactive explanations have been investigated, which allow humans to have better control on what kind of explanations they can get from the model [ interactive renement tool to extract similar images for pathologists in medical domain. In their tool, users are not provided with explicit explanations, but instead can interact with the system and test it under dierent hypotheses. Cheng et al decisions. They nd that an interactive explanation approach is more eective at improving comprehension compared to static explanations, but at the expense of taking more time. Another form of user control in human-AI decision making is to allow users to choose input data to get model predictions. Prior work explores cases where users choose the input dataa (or the underlying features) for the model to consider [ points of the initial image for the system to attend to when looking for similar images. Finally, researchers compare dierent levels of machine agency. For example, Levy et al distinct clinical note annotation systems: one only suggests annotation labels after users choose text spans to be labeled, and another performs both span and label suggestions. Similarly, Buçinca et al on demand. 4.3 Summary and Takeaways We summarize current trends and gaps in how AI models and assistance elements are used and studied, then make recommendations for future work. Current trends. (1) Limited uses of deep learning models. of empirical studies still adopted traditional shallow models, even Wizard of Oz, possibly due to their ease to develop or access. Further, shallow models are typically less complicated to explain, making them an easier choice for studying human-AI decision making assisted by information about the prediction or model. (2) Assistance beyond predictions. a wide range of elements providing information about the prediction and the model on improving decision performance. By summarizing these elements studied, we hope to also inform the design space of AI decision support systems. (3) A focus on AI explanations. both local explanations for a prediction and global explanations for the model. This is necessary information for people to better understand the AI to interact, but also partly due to a recent surge in the eld of explainable AI (XAI), which produces increasing technical availability to generate explanations. (4) Beyond the model. that aect user agency and action spaces, including workow and user control. Gaps in current practices. (1) A fragmented understanding and limited design space of AI assistance elements. subjects studies often focus on one or a small set of AI assistance elements. We have very limited understanding . [28]compare dierent explanation interfaces for understanding an algorithm’s university admission 24,73,90]. For example, in the similar image search system of Cai et al. [24], participants denote the important on the interplay between dierent assistance elements, and thus limited knowledge in how to choose between, or combine, them when designing AI systems. More problematically, studies are often driven by technical availability such as new explanation techniques. This practice may risk losing sight on what users need to better accomplish decision tasks or what are the necessary elements of the design space of human-AI decision making, which is especially important knowledge for practitioners to develop eective AI systems. For example, only a small number of studies explored non-model-centric system elements that can aect users’ action space and cognitive processes, and showed that they are critical for user experience and also their interaction with model assistance features [21, 43, 125]. (2) Focus on decision trials only instead of the holistic experience with AIExisting work commonly experimented with participants performing discrete decision trial tasks—seeing an instance and making a decision with AI’s assistance. However, in reality, when people use a decision-support AI system, many other steps and aspects can aect their experience and interaction with the AI, such as system on-boarding experience, workow, contexts where the decision happens, and repeated experiences with the system. Their eects are currently under-explored for human-AI decision making. This narrow use of experimental tasks could have led to certain gaps or biases of assistance elements studied. For example, studies tended to focus on decision-specic assistance but less on model-wide information. (3) Gaps in models used.Our analysis revealed a current bias in model types used in the studies—more using traditional shallow models than deep learning models. It is necessary to elucidate how model types and their associated properties aect the experiment setup and generalizability of results, which can guide future studies to make appropriate choices. For example, some deep learning models not only tend to perform better in average (but not all) training settings, but also are likely less interpretable and prone to over-condence. While wizard-of-oz approaches have a long tradition in HCI, applying them to studying AI models face many new challenges, such as how to simulate model errors and explanations in a realistic way. We caution against them without justifying the design as suciently approximating the interested model behaviors and stating the limitations. Another gap in models used is limited studies of regression models. In addition to the prediction forms, some assistance elements take distinct forms for regression v.s. classication (e.g., uncertainty information), and their eects are under-explored for regression. Recommendations for future work. (1) Human-centered analysis to dene the design space of AI assistance elements. Complementary to current practices of studying fragmented AI assistance elements, the eld can benet from having top-down frameworks that dene the design space of AI assistance elements needed for better human-AI decision making, which requires analysis centering on what decision-makers need rather than technical availability. Having this kind of framework can guide researchers to identify gaps in the literature and formulate research questions, and ultimately produce unied knowledge that can better help practitioners make appropriate design choices when developing AI decision support systems. We hope our analysis can inform such eorts. (2) Extend the design space and studies beyond decision trials.To center the research eorts on real user needs also means we should look beyond the discrete decision trials used by current studies, which not only lack may ecological validity but also fail to account for many temporal, contextual, and individual factors that can shape how people perceive and interact with AI, such as on-boarding experience, time constraints, workload, prior experience, and individual dierences. Future work should explore these factors, and conduct eld and longitudinal studies of human-AI decision making. (3) Task-driven studies to complement feature-driven studies. standing the eect of certain assistance elements or design features. Then a decision task is chosen in an ad-hoc fashion or even arbitrarily in some cases. To inform the design space of AI assistance elements and actionable design guidelines for dierent types of AI system, we believe it is useful to complement current practices with task-driven studies, which may require conducting formative studies to understand user needs and behaviors for a given decision task. 5 EVALUATION OF HUMAN-AI DECISION MAKING Deciding on the evaluation metrics is one of the most critical research design choices. This decision often involves choosing the construct — what to evaluate, then choosing the specic formulation or content of the metrics — how to evaluate the target construct. Our survey reveals a wide range of constructs evaluated in studies of human-AI decision making, likely due to broad research questions asked by the community and a lack of standardized evaluation methods. As mentioned in the Methodology section, our survey focuses on quantitative evaluation metrics, although some studies used qualitative analysis to gain a further understanding of user perceptions and behaviors. At a high-level, we group the evaluation metrics into two categories: (1) evaluation with respect to the decision making task and (2) evaluation with respect to the AI. Under each, we group them into areas of evaluation such as task ecacy versus eciency. Then we further classify them as either objective or subjective measurements. Later in this section we discuss that subjective and objective measurements may in fact target dierent constructs (perception or attitude versus behavioral outcomes guided by the attitude). Here we classify them based on what the studies claim they are measuring. Note our analysis stays at the granularity of measurement construct instead of detailed dierences in the content or formulation (e.g., what specic survey items are used). It is worth noting that many papers did not provide access to the survey scales or questionnaires. As a result, it can be dicult to interpret some of these ndings or for future research to replicate them. 5.1 Evaluation with respect to the decision task Decision making performance—for which in the AI assistance is designed to support—is intuitively the most important outcome measurement for human-AI decision making. Evaluation of the decision tasks mainly belongs to two categories: (1) ecacy (i.e., the quality of the decisions); and (2) eciency (i.e., the speed in making these decisions). In addition, we include a category on measuring people’s task satisfaction. Table 4 gives a summary of these measures and how they are collected, both objectively and subjectively in the surveyed papers. Ecacy. measured as the percentage of correctly predicted instances (or equivalently, error rate, the percentage of incorrectly predicted instances) [ of the joint outcome of human-AI teams, compared against the baseline accuracy of humans without AI assistance or of AI alone. As the evaluation is essentially comparing decision labels against ground-truth labels, used to evaluate AI performance can also be used to evaluate the performance of human-AI decision making. These We start with objective metrics of decision task performance. The most commonly used metric is accuracy, EcacySubjectiveSelf-rated error/accuracy [37,81,137], perceived performance improvement[32], condence in the decisions [54,55,60,95], soundness of participants’ EciencyObjectivetime taken on the task (response time, average time for a game round, speed)[1,10,26,28,45,48,52,74,79,82,90,92,104,125,130,144,147], total number Task satisfactionSubjectiveSatisfaction with the process [37], condence in the process [37], frustra& mental demandtion/annoyance [77,125], mental demand/eort [20,21,77,144], workload metrics include F1 [16,104], precision [16], recall [16,90], AUC-ROC [41], which are commonly adopted for imbalanced datasets in the machine learning literature. For cases where the cost of mistakes varies signicantly for the positive class and the negative class, false positives rate [26,41,54,99] or false negatives rate [26,41,99], true positive rate [41], and true negative rate [41] have been used. In regression tasks, such as asking people to predict the likelihood of recidivism Green and Chen[54,55], prior work similarly adopts continuous counterparts of accuracy, including mean prediction error [111] and brier score (1 − (prediction − outcome)) [54, 55]. In gamied tasks, researchers also use win rate [32,48], cumulative award [52], customized return [12], and human percentile rank [32] to capture the performance of human-AI teams. Finally, in cases where groundtruth labels are not available, agreement between labels (inter-annotator agreement) has also been used [87]. In addition to objective metrics, subjective metrics can help understand human perception of the task performance. A natural extension to the objective metrics for performance is perceived accuracy (i.e., self-rated error/accuracy) [37,81,137] and perceived performance improvement [32]. Another common metric is to ask humans about their condence in the decisions [54,55,60,95]. These perceived condence measurements are usually based on Likert scales. Finally, Kulesza et al. [76]introduce a unique metric that combines subjective metrics and objective metrics toÍ measure mental model soundness as,(correctness× condence), where𝑖is the index of questions. While this metric was originally used for comprehension questions of how a recommendation system works, Kulesza et al. [76]show it can be adapted to measure the soundness of mental models on test instances. Eciency.In addition to ecacy—how accurately participants make decisions, another important dimension to consider is eciency—how quickly they can make them. The main motivation is to gauge if the AI assistance can help humans make decisions faster. The most common objective metric is time taken on the task [1,10,26,28,45,48, 52,74,79,82,90,92,104,125,130,144,147]. Alternatively, the total number of labels (or task output) can be used to measure eciency as in Levy et al. [90], most appropriate when task time is held constant. Notably, subjective metrics of eciency are not seen in the papers we reviewed, although self-reported task eciency is a common metric used in usability testing [46]. Task-level satisfaction and mental demand. whether AI assistance improves human’s satisfaction or enjoyment with the decision task. We consider both direct measurement of task satisfaction or the counter-measurement of task mental demand in this category. Most metrics in this category are subjective, typically solicited through survey questions at the exit survey or questionnaire. The only exception in papers we surveyed is Lakkaraju et al user feedback to gauge user satisfaction. Researchers have asked participants about their subjective satisfaction with the process [ workload [24, 87, 128, 130], and task diculty [10]. 5.2 Evaluation with respect to AI In addition to evaluating the decision task, works on human-AI decision making also focus on evaluating users’ perception and response to the AI system itself, including understanding of AI, trust in AI, fairness perception, AI system satisfaction, and others. Table 5 summarizes these measures. Understanding. making assistance, users’ understanding of the AI is a commonly used measurement. Subjective metrics of understanding typically ask participants to directly rate their understanding of the AI [ variations of it such as condence in understanding [ Other metrics ask participants to rate on perceived intuitiveness [134] or transparency [112, 137] of the AI system. Objective metrics often test how well people understand the system compared to ground-truth facts about its outputs or how it works. The most commonly used metric is forward simulation [ 153], by asking participants to simulate a model’s predictions on unseen instances. Some researchers have also used counterfactual simulation [ metrics measures the correctness of people’s assessment of model performance [ identication of important features [ 112]. Other studies also design comprehension quizzes to evaluate human understanding [ these objective measures, researchers aim to evaluate humans’ mental model of the AI systems’ innerworkings and how they make predictions. It is important to note that objective and subjective understanding do not always align due to the phenomena of illusory condence with which one did [29]. Trust and reliance. direct self-reported trust is often used [ of it such as acceptance or condence in the model [ or perceived accuracy of the AI [ framework [ capability, benevolence and integrity [ Objective metrics of trust often focus on reliance as a direct outcome of trusting (i.e., how much people’s decisions rely on or are inuenced by the AI’s), such as acceptance of model suggestions [ 37], condence in the process [37], frustration/annoyance [77,125], mental demand/eort [20,21,77,144], Since a signicant proportion of empirical studies focus on AI explanations as a form of decision 100] or a subset of it, which measures the subjective trust belief (i.e., perceived trustworthiness) as perceived ,155], likelihood to switch [58,96,101,109,152,155], weight of model advice [95,111], choice to use the UnderstandingSubjectiveSelf-reported understanding [7,15,20,23,28,97,125,143,147], condencein understanding [76], condence in simulation [3,106], ease of understand- Trust and relianceSubjectiveSelf-reported trust [1,3,20,28,29,36,45,48,55,76,111,111,115,125,128,137],model condence/acceptance [3,29,77,125,134,143], self-reported agree- FairnessSubjectivePerceived fairness [7,39,54,61,139], individual fairness [88], group fair-ness [88], process fairness [15], deserved outcome [15], feature fairness [15, SystemSubjectiveSatisfaction [16,37,74,79,97,104,137], helpfulness/support [16,20,24,42, satisfaction and74,147], usefulness [60,86], eectiveness [137], quality [137], appropriateusabilityness [20], preference/likability [76,86,115,147], system aect [128], system model [11,36,37,114], model inuence (dierence between conditions) [54,55],as well as disagreement or deviation from the model’s recommendations [111]. Some researchers have also looked at more ne-grained reliance such as over-reliance (relying on the model when it is wrong) [21,22,143,147], under-reliance (not relying on the model when it is right) [22,143,147], and appropriate reliance [52,111,143,147]. These objective trust metrics often inuence the joint decision-outcomes and thus correlate with the task ecacy metrics we reviewed above. It is worth noting that although trust as an attitude guides the behavior of reliance, the two are in fact two dierent constructs. Social science and human factors literature have suggested that, besides trust, other factors can inuence reliance, such as workload, time constraints, eorts to engage, perceived risk, and self-condence (see a review in Lee and See [85]) Fairness. area. These studies primarily rely on subjective metrics, from general perceived fairness [ of more ne-grained types of fairness such as individual fairness [ outcome [ and they can control the outputs the system produces) [ (e.g., the action to follow model’s recommendations despite their lack of fairness) [ perceived fairness. System satisfaction and usability. eectiveness [ usability related metrics such as usability [ information richness [ we surveyed, only Cai et al reect users’ interest and satisfaction. Others.Other measures focus on evaluating a specic feature of the AI. For example, AI explanations are frequently studied in the context of human-AI decision making, and subjective metrics have been used to measure p eople’s perceived explanation quality [ workload [ self [23], desire to provide feedback [125], and expected improvement of the AI system over time [125]. Qualitative analysis. to supplement with qualitative analysis to further gauge the target measure (e.g., coded participants’ statements about how the system work to measure understanding [ example, some studies asked open-ended “why” questions following survey scales, others conducted exit interviews or asked participants to think aloud while using the AI system [ analysis is then typically used to analyze these qualitative data, which allows researchers to extract main themes from the bulk of information and serve as insightful knowledge. Other qualitative analysis performed includes grounded theory [91, 103] and anity diagramming [14, 64, 148, 149]. 5.3 Summary & Takeaways We summarize current trends and gaps in how surveyed studies evaluate human-AI decision making, and make recommendations for future work. Current trends. (1) Diverse evaluation focuses studies focused on dierent evaluation constructs. Our analysis reveals a framework that dierentiates between Studying how people perceive the fairness of AI and what design impacts the perception is an active research 15], feature fairness [15,139], and accountability (i.e., the extent to which participants think the system is fair 74,79,97,104,137] or related constructs such as perceived helpfulness [16,20,24,42,74,147], usefulness [60,86], 137], quality [137], appropriateness [20], likeability [76,86,115,147], etc. Some studies leverage system 1], and agreement with the explanation [144]. Other metrics include users’ outcome attribution to AI versus dimensions evaluating the human-AI decisions and dimensions evaluating human perception and interactions with the AI, each with subjective and objective measurements. (2) A focus on ecacy when evaluating decision tasks, but eciency and subjective satisfaction are also useful indicators. (3) Focuses on understanding, trust, system satisfaction and fairness with regard to AI.We note that some of these focuses could be a result of the eld’s focus on explanation features and fair machine learning. (4) A lack of common measurements.Within a given measurement area, there exists signicant variations in the choices of evaluation construct, content, and formulation. For example, trust has been measured by a single item, multi-items, by trustworthiness dimensions, trust intention, and objective reliance, among others; similarly, there are many nuanced constructs to measure satisfaction. Gaps in current practices. (1) A focus on decision ecacy (i.e., performance), and less emphasis on eciency and user satisfaction. The three are commonly used constructs for usability measures [46]. This reects a deep value of the eld [17], which prioritizes optimizing decision outcomes rather than the experience of human decision makers. That being said, we acknowledge that not all decision tasks require high eciency (also the eciency of AI alone is trivially better than human-AI teams). An open question for the eld is to better understand the role of eciency in tasks where it is necessary for human and AI to collaborate. (2) Use of subjective versus objective measurements need to be better understood and regulated.It is important to recognize that the results from subjective and objective metrics do not always align, and the two may be in fact measuring dierent constructs, despite some studies make mixed claims. For example, participants can express high subjective understanding without objectively understanding the model behaviors. In some cases, objective metrics are measuring behavioral outcomes that are guided by user attitudes that are evaluated by subjective measures, but often in a non-linear way. One example that has long been studied in the human factors literature is trust as an attitude (by subjective measurements) versus reliance as a behavior (by objective measures). Despite some studies claim using reliance behaviors to reect trust, many other factors besides trust can inuence reliance, such as required eorts, perceived risk, self-condence, and time constraints [85]. We do not claim the superiority of either. Studies may choose to focus on objective versus subjective measurements for many reasons. For example, the research questions may deal with user attitude versus behavioral outcomes, or it is easier or only feasible for the system gather data for one type of measure in practice. However, this choice is often not explicitly justied or disentangled in terms of the actual constructs being measured. (3) Home-grown measurements, especially subje ctive survey items, are often used.There is a lack of practices to validate, re-use (and enabling re-use of) measurements, and leverage existing psychometrics or survey scales developed in HCI. Especially for subjective measurements, it is also not common practices to publish the survey scales used in the experiments. As a result, it can be dicult to replicate a study or compare dierent studies. (4) Variance on the coverage of measurements.Some studies measure only task ecacy or ask about user trust, other studies cover many aspects. While the choice should be driven by research questions, it might reect a lack of common framework or awareness for researchers to make choices in a principled manner. Recommendations for future work. (1) Make choices of evaluation metrics by research questions/hypotheses and targeted constructs. important to articulate what constructs, whether it is subjective perception or attitude, objective behavioral outcomes, with regard to the AI or with regard to the decision tasks, should be measured for the research questions or hypotheses. In general, researchers should pay attention to the concepts of measurement validity established in statistics and social sciences, including construct validity (does the test measure the concept that it is intended to measure) [ Meanwhile, thinking through dierent areas of evaluation metrics (i.e., what can a given design/assistance element impact?) can help formulate more comprehensive and insightful hypotheses. (2) Work towards common metrics and a shared understanding on the meanings, strengths and weaknesses of dierent evaluation methods. We must also recognize that human-AI decision making is a nascent area where new metrics may need to be developed. Studies should not be limited to focusing on areas reviewed in this paper or using existing metrics. (3) Keep reecting on common evaluation metrics as value-laden choices. accepted and used, can profoundly shape the outcomes of a eld. At a collective level, we should keep questioning whether the evaluation measures we use capture what matters for stakeholders and the society, and what the potential long-term outcome could be if we prioritize one set of measures over the other. This will also help the eld expand the measurements and ultimately lead to more principled and responsible AI for decision making. 6 SUMMARY: TOWARDS A SCIENCE OF HUMAN-AI DECISION MAKING By summarizing decision choices made in more than 100 papers on empirical studies of human-AI decision making, specically around the decision tasks, AI assistance elements studies, and evaluation metrics, we reect on the barriers for the eld to produce scientic knowledge and eectively advance human-AI decision making. A few core recommendations for future work emerged in our analysis, as we summarize below. Building on each other’s work. from other experimental sciences such as psychology to practice replication, meta-analysis across studies, rigorous methodology and metrics development, and theory development that helps consolidate (sometimes contradicting) empirical results. To build on each other’s work also means that researchers should prioritize enabling others to re-use and reproduce when publishing results, by articulating rationales behind design choices and reecting on them to build shared knowledge, and making study materials accessible. The eld should also strive to establish common practices or infrastructure that make knowledge sharing easier. For example, in the context of evaluation for human-centered machine learning, Sperrle et al principled choices in study design, and a reporting template that, besides results, covers many aspects of study design such as hypotheses, procedure, tasks, data, participants, and analysis. Developing common frameworks for human-AI decision making. unied scientic knowledge is to develop frameworks that account for the research spaces for human-AI decision making. In this paper, we discuss the needs for the eld to develop frameworks that characterize dierent decision tasks, lay out the design space for AI assistance elements, and areas of evaluation metrics. Such frameworks can help shape research eorts in several ways. First, they can provide researchers a shared understanding to identify important research problems and articulate research questions in a common language. For example, with a framework on the design space for AI assistance elements, researchers can identify under-explored areas. Second, frameworks make explicit otherwise latent or disregarded factors that can help interpret and consolidate results across studies, and ultimately lead to more robust knowledge and theories. For example, a framework on task characteristics can help dierentiate between the setups of two studies, and a framework on evaluation metrics can help dierentiate their coverage of measurements. Last but not least, developing principled frameworks is also a critical and reective practice—reecting on the limitations and gaps in current research, and questioning the missing perspectives. For example, we urge the eld to consider the design space of AI assistance beyond supporting discrete decision trials by paying attention to the entire decision process, the holistic experience with an AI system, as well as contextual and individual factors. We also encourage research eorts that systematically examine what should be measured for human-AI decision making, considering what matters for dierent stakeholders instead of just the decision-makers (e.g., people whose life will be impacted by the decision), and what should be the ethical principles of decision-support AI. Bridging AI and HCI communities to mutually shape human-AI decision making.To advance human-AI decision making requires both a foundational understanding of human needs and behaviors, and based on them developing more eective and human-compatible AI to support decision-makers. In the present time, the research eorts are somewhat one-directional. The HCI community typically work as the receiver of new AI techniques, then build systems or design evaluative studies. How can the two communities work better together? How can HCI research drive AI technical development? Such questions have been long contemplated on in other interdisciplinary areas such as interactive machine learning [5] and human-robot interaction [118]. We believe one aspect is to reconsider the priorities of HCI research contributions. Rather than focusing on conducting evaluative studies, developing theories and principled frameworks based on empirical studies and engagement with user needs can help guide AI research eorts. For example, a framework of AI assistance elements can inform what kinds of AI technique are needed to better support human-AI decision making; and a framework of evaluation metrics can guide the technical optimization eorts. Meanwhile, the AI communities should prioritize technical work that is informed by human needs and behaviors, and actively seek to distill insights from empirical studies as well as psychological and behavioral theories into computational work. As always, cross-disciplinary collaboration will require change of culture and translative research to bridge dierent perspectives. We hope the common goal of improving human-AI decision making can unite researchers from the two communities, and this survey as a bridge for joint research eorts.