 for statistical inference. Over the past two decades, a wide range of algorithms have been proposed for learning parameters in computationally feasible ways, often under the heading of approximate Bayesian computation or likelihood-free inference. There is, however, no consensus on how to rigorously evaluate the performance of these algorithms. Here, we argue for scoring algorithms by the mean squared error in estimating expectations of functions with respect to the posterior. We show that score implies common alternatives, including the acceptance rate and eﬀective sample size, as limiting special cases. We then derive asymptotically optimal distributions for choosing or sampling discrete or continuous simulation parameters, respectively. Our recommendations diﬀer signiﬁcantly from guidelines based on alternative scores outside of their region of validity. As an application, we show sequential Monte Carlo in this context can be made more accurate with no new samples by accepting particles from all rounds. A key challenge in modern Bayesian inference is developing algorithms to handle increasingly complex scientiﬁc data and models. Standard approaches rely on explicit likelihood functions for either analytic calculation or, more commonly, Markov chain Monte Carlo (MCMC). However, for many important models in contexts from cosmology [1, 22] to psychology [36] and neuroscience [11] computing the likelihood itself is intractable. In such settings, inference must be done using simulations that sample from the unavailable likelihood. aden.forrow@maths.ox.ac.uk Complex scientiﬁc models where the likelihood cannot be evaluated present a challenge such likelihood-free inference [3, 6, 7, 10, 11, 16, 20, 23, 24, 27, 29, 34], each aiming to perform accurate and eﬃcient inference when the likelihood is only implicitly deﬁned. Thus far, however, there is no rigorously justiﬁed consensus on how to measure either accuracy or eﬃciency. Proposed algorithms are accompanied by diverse methods for evaluating performance, which we refer to as scores to avoid confusion with the technical meanings of metric and measure. Examples include the acceptance rate [6, 7, 12, 18, 29, 32, 31, 34, 35], eﬀective sample size [10, 28, 29, 30], and precision in recovering known simulation parameters [11, 21, 24, 29]. This diversity of scores makes it diﬃcult to compare algorithms, as the relationship between good performance on diﬀerent scores is often unclear. mean squared error (MSE) in approximating posterior expectations. We begin by laying out the general likelihood-free inference problem in Section 1.1. In Section 1.2, we present the MSE score and the independent motivation for using it. We then review commonly used alternative scores together with algorithms designed to optimize them, showing that each score either can be derived as an approximate special case of a function expectation or fails to account for important features of the match between true and approximate posteriors. gorithms. For many practical applications of likelihood-free inference, the dominant computational cost comes from performing expensive simulations from the model. We will ignore the complexity of other stages of an algorithm, though for suﬃciently simple models they may be relevant. The goal, then, is to optimize an appropriate score while minimizing the required number of simulations. For this paper, we focus on optimizing over one aspect of likelihood-free inference, the choice of parameters with which to simulate the model. lations to run that optimizes the asymptotic MSE for a given function. Our recommendations diﬀer signiﬁcantly from strategies proposed in the literature based on other scores. Next, in Section 3, we demonstrate that although continuous parameters present new challenges that make rigorous analysis diﬃcult, the discrete results qualitatively translate to that setting. Here again, optimizing an algorithm for an inappropriate score can cause ineﬃciency. In likelihood-free inference, we aim to use observed data x θ ∈ Θ governing a scientiﬁc model. We approach the problem from a Bayesian perspective: we have a known prior distribution p(θ) encoding our initial knowledge about θ and seek to Ongoing research over the past two decades has led to a wide range of algorithms for This paper argues for evaluating the accuracy of likelihood-free inference methods by the A key use for a reliable score is to compare the computational eﬃciency of diﬀerent al- We begin in section 2 with the case of discrete parameters, where we derive the set of simuinfer a posterior distribution p(θ|x Bayes’ Theorem: up to a normalizing constant. Such intractable likelihoods often occur when a model includes many unobserved latent parameters. In a stochastic epidemic model, for example, explicitly calculating the likelihood of a certain number of patients testing positive for the disease may require summing over all possible conﬁgurations of asymptomatic or unconﬁrmed cases. To avoid that impossibly expensive computation, we learn about the likelihood by running simulations to sample from p(x|θ). must implement two steps: 1. Choose {θ 2. Estimate ˆp(θ|x The steps may not be separable from each other: many algorithms [3, 12, 25, 32] use an intermediate posterior approximation to guide subsequent choices of simulation parameters. In addition, the form of ˆp(θ|x a set of samples approximately from p(θ|x θ to p(θ|x approximate models are possible [27, 28]. whose most basic form approximates the posterior with the empirical distribution of {(θ weighted by a kernel K (∆(x ABC involves several important complications we will not analyze here, including the choice of ∆, often involving summary statistics, and the setting of . For our theoretical results in Sections 2 and 3, we assume that the probability of observing x is discrete, that assumption is trivial; if the raw data y observation that y is within an -ball of y A ﬁrst step in evaluating an algorithm’s performance is to choose a quantitative discrepancy function d(ˆp(θ|x such function, we can take an expectation over the randomness in computing ˆp(θ|x Unlike in traditional Bayesian inference, we do not assume p(x|θ) can be computed, even Our goal is to create an approximation ˆp(θ|x) to the posterior. Any algorithm to do so ). We assume throughout that xis drawn from p(x|θ), though alternatives with A popular strain of likelihood-free inference is Approximate Bayesian computation (ABC), Integral probability metrics [33] Table 2: Summary of scores proposed in the literature. The third column is a not-exhaustive list of references that use or recommend optimizing for each score. Number of samples (θ, x) accepted, if applicable a score for the algorithm, which we aim to minimize. The function d should target features we care about in the match between ˆp(θ|x the expectation of θ; in a model selection problem, the posterior probability of model M is the expectation of an indicator function for M; I is a posterior credible interval covering probability P if P is the posterior expectation of the indicator (θ ∈ I). Estimation of both the mean and second moment enables estimation of the posterior variance, which is a common goal [11, 21]. set of functions collected into a vector, and setting where we choose the squared error for later analytical convenience. The expectations in Eq. 3 are taken with respect to ˆq(θ) and q(θ), i.e. the approximate and true posteriors, in contrast to Eq. 2 where the expectation is over the distribution of approximate posteriors produced by an algorithm. The corresponding score S al. [2] to evaluate rates of convergence of rejection sampling, Li and Fearnhead [17] to argue sampling simulation parameters from either prior or posterior is asymptotically ineﬃcient, and Prescott and Baker [27] to evaluate the eﬃciency of multiﬁdelity ABC. function d and a supremum over functions in a suﬃciently rich class F of bounded, measurable, realvalued functions [33]: Such metrics, called integral probability metrics (IPMs), include many standard metrics on probability distributions. For example, choosing F = {f : sup variation (TV) distance [33]; choosing the class of functions with Lipschitz constant at most 1 yields the Wasserstein distance [33]; and choosing the unit ball in a reproducing kernel Hilbert space yields the maximum mean discrepancy (MMD) [14]. d(ˆq, q) will be small if and only if d to a proper metric as a motivation for using the MSE score S Many relevant features are encoded in expectations of functions. The posterior mean is We can assess the accuracy of estimation of such features by choosing a function f(θ), or While d(ˆq, q) = 0 for an individual function f does not alone imply ˆq = q, the discrepancy can be extended to a metric on probability distributions by taking a square root The core of an IPM is the error in expectations of individual functions. By deﬁnition, for three reasons. First, dealing with the supremum in Eq. 4 may be analytically diﬃcult. Second, if F is too rich then a small IPM distance d goal. For example, if we approximate a continuous posterior in D ≥ 2 dimensions with the empirical distribution of N samples, the total variation distance between ˆp(θ|x is always 1 and the Wasserstein distance decays as N us to pay attention to which expectations we estimate well or poorly, which enables improved eﬃciency for speciﬁc functions of interest. stage of our argument for using it is to show that several commonly used alternatives are, at least approximately, equivalent to choosing a speciﬁc function f (θ) and, in some cases, a speciﬁc inference algorithm. We ﬁrst discuss the acceptance rate and eﬀective sample size, in the process introducing implementations of likelihood-free inference where those scores are often considered. We then show how both φ-divergences [33] and the MSE in p(θ|x particular targets f (θ). Finally, we conclude our section on scores by mentioning the remaining approaches we have seen for evaluating posterior approximations, which may complement the MSE score without replacing it as a goal. A traditional score for ABC is the acceptance rate, which comes from the simplest ABC variant, rejection sampling (Algorithm 1). Simulation parameters {θ from the prior and accepted if the simulation output x data. The posterior is approximated as the empirical distribution of accepted parameters, or, equivalently, by giving each accepted parameter weight w weight w “suﬃciently close” will be equality x is a set of N nonzero probability to avoid complications from either setting a threshold  on kx making assumptions about the structure of p(x|θ). pirical average Algorithm 1 are independent, of f (θ). For rejection sampling, then, the acceptance rate N score S convergence rate for any particular f(θ). Finally, considering each f individually forces The previous paragraphs demonstrate that Sis a plausible candidate score. The next = 0. For the purposes of this paper, except when discussing sequential Monte Carlo, We can compute an approximate posterior expectation of any function f(θ) as the emwith any function. Clearly, too small an acceptance rate makes inference diﬃcult. If the expected acceptance Algorithm 1 ABC rejection sampling for i = 1 : N do θ∼ p(θ) x∼ p(x|θ if x else end if end for return {θ rate is not suﬃciently large compared to 1/N, there is a risk that no samples will be accepted and no information gained about the posterior. For realistic problems, the acceptance rate of Algorithm 1 may be vanishingly small if the prior covers large regions of parameter space where the likelihood of generating data similar to x too many samples are rejected [6, 18, 24, 31, 32, 34]. Implicitly, that argument suggests that algorithms with higher acceptance rates should systematically outperform algorithms with lower acceptance rates. In that spirit, the acceptance rate is included as an algorithm quality score in a wide range of papers [6, 7, 12, 29, 31, 32, 35], in some cases together with other evaluations. to improve the acceptance rate. Rather than sampling from the prior, {θ importance distribution q(θ) and, if accepted, given a weight w weighted empirical distribution ˆp(θ|x the posterior. Expectations can again be approximated with empirical averages: This estimate is biased due to the ratio, though ideally not signiﬁcantly biased. In some importance sampling contexts [9] the expectation of the denominator can be explicitly calculated to give an unbiased estimator; here, that is not possible because it depends on the unknown likelihood: Monte Carlo (SMC) [7, 12, 32, 34]. ABC-SMC methods do multiple rounds of importance : w= 1} This issue leads to a standard argument against rejection sampling: it is ineﬃcient because A long strain of research [7, 12, 32, 35] combines Algorithm 1 with importance sampling The importance sampling variant leads naturally to a combination of ABC with sequential sampling from an adaptively-constructed approximation to the posterior. In the ﬁrst round, the θ x. In each subsequent round k, parameters, called particles, are perturbed samples from the previous round’s accepted particles and the tolerance  output is the weighted set of particles from the ﬁnal round, though including particles from all rounds may signiﬁcantly improve accuracy (Supplement Section S7). The goal of running multiple rounds is to get as close as possible to sampling parameters from the posterior, which has been claimed to give “the maximum possible eﬃciency of ABC samplers” [32]. eﬃciency. The change from sampling parameters from p(θ) to sampling parameters from the importance distribution q(θ) broke the direct connection between acceptance rate and estimator variance: the variance of general, outside of the regime where tiny acceptance rates make inference infeasible, the relationship between the acceptance rate and other accuracy scores is not well understood. Truly maximizing the acceptance rate would be undesirable if it were possible, as all simulations would be at the maximum likelihood parameter values and give no information about the rest of the posterior. weights. The eﬀective sample size is a heuristic designed to account for that variability. It is deﬁned for a set of N samples {θ The eﬀective sample size variance of an importance sampling estimate of E f(θ) with θ ∼ p(θ|x refers directly to the ratio of the variance of an estimator to the variance of a single posterior sample, which may be estimated diﬀerently [38]. Here we will only consider the estimate in Eq. 7, where the hat emphasizes that we do not have the true relative variance. from 1 to N and is larger when the weights are more uniform. For rejection sampling, where w= 1 for accepted samples and zero for rejected samples, the eﬀective sample size is always the number of accepted samples, N [ESS is commonly used to measure sample quality. Fearnhead and Prangle [10] proposed an importance distribution designed to maximize are sampled from the prior and accepted if xis within a relatively large tolerance of However, despite its intuitive appeal, sampling {θ} from the posterior may not improve A key factor ignored in focusing solely on the acceptance rate is variability in the sample Despite criticism [9] in the context of importance sampling, where it was derived [19], observed many authors [23, 28, 29, 30] use it to evaluate their methods. The clearest advantage of Eq. 7 is that it is easy to compute without information about the function of interest f (θ) or the true posterior. purpose scores is that they are not deﬁned for all algorithms. We could not use example, to compare an algorithm using Eq. 5 to one that integrated a kernel regression estimate (Supplement Section S6) based on the same set of weighted samples {θ lar function. The derivation in [19], done for pure importance sampling rather than likelihoodfree inference, leads to where all expectations are taken with respect to the importance sampling target. The error is straightforwardly zero in two cases: rejection sampling, because w − E[w] is zero everywhere; and target functions where kf (θ) − E[f(θ)]k transformations of indicator functions for 50% credible intervals. We are not aware of any arguments for why it should be small in general. does not incorporate f(θ), once we have moved away from Algorithm 1 the degree to which it is an accurate approximation of the estimator variance does depend on f (θ). Moreover, the derivation of sampling ([16] and Section 2), or with stratiﬁed sampling (Supplement Section S5.2) changes the variance. Optimizing variance, as we will see in Sections 2 and 3 after we complete our survey of possible scores. sider the value of the posterior itself. We can either pick one θ or integrate to acceptance rate rather than the acceptance threshold  on kx calculated the asymptotic contributions of bias and variance to S estimate ˆp(θ [ESS to control the rate at which target distributions approach the posterior; and One diﬃculty with either the acceptance rate or the eﬀective sample size as general- More importantly, the approximations used to derive Eq. 7 may not apply for any particu- The independence of[ESS from the target f (θ) is therefore an illusion. While Eq. 7 If we are strongly opposed to choosing target functions, a tempting alternative is to con- Biau et al. [4] computed the rate of convergence of Sfor ABC algorithms where the Rather than being independent of f, however, both options are special cases of the MSE score. S respectively, where the latter is considered as a function of θ by J¨arvenp¨a¨a et al. [16]. This simpliﬁcation is computationally convenient, as dealing with normalization complicates calculations. It is, however, a simpliﬁcation, and ignores the eﬀect of error in estimating the normalizing constant on estimating p(θ|x in Section 2 where that error dominates. by a convex function φ with φ(1) = 0 as This includes the Kullback-Leibler (KL) divergence KL(ˆq|q), with φ(t) = t log t, as well as the TV distance, with φ(t) = the KL divergence between the joint distributions of particles of consecutive iterations and independent copies of the posterior. Other authors [18, 24] use KL divergences for ﬁnal evaluations. for comparing the true and approximate posteriors. However, in the relevant limiting case of an accurate algorithm φ-divergences may be approximated by a diﬀerence of function expectations, as we now show. If φ is twice continuously diﬀerentiable at 1, the integrand of Eq. 10 can be expanded as The ﬁrst term is zero by deﬁnition; the second term integrates to zero when ˆq and q are both normalized. Suppose ˆq is a close enough approximation to q that the last term is negligible, which for likelihood-free inference means that we have used enough computational eﬀort to get a good approximation to the posterior. Then which is equal to the expectation squared error d θ). A similar argument leads to the same result for the leading order term in d (Supplement Section S1). A minor variant of Sis to target the unnormalized posterior p(x|θ)p(θ), as done Our ﬁnal class of scores to connect to the MSE score are φ-divergences [33]. Each is deﬁned With its connections to information theory, the KL divergence is a compelling candidate ˆq(θ)q(θ) = φ(1)q(θ)+φ(1)q(θ)ˆq(θ)− 1+1φ(1)q(θ)ˆq(θ)− 1+Oˆq(θ)− 1 q(θ)q(θ)2q(θ)q(θ) In the context of a consistent likelihood-free inference algorithm, therefore, the KL divergence between ˆp(θ|x the MSE of a data-dependent function emphasizing parameters with low posterior probability. We validate the approximation for a simple example in Fig. 1. The only commonly used φ divergence which is not smooth, and so not covered by this argument, is the TV distance which can be deﬁned independently in terms of function expectations. complications with discrete approximations to continuous posteriors. Both KL (ˆp(θ|x and d this problem is not insurmountable, as an algorithm that returned posterior samples could be supplemented with a kernel regression estimate or equivalent procedure, choosing the MSE score S tions of interest and estimating S if achievable, likely isn’t necessary for many applications. Meanwhile, other commonly used quantitative scores imply a speciﬁc target function, a speciﬁc algorithm structure, or both. None of them are truly independent of the target. Using a diﬀerent score without explicitly accounting for f(θ) merely hides the dependence. Alternative ways to compare ˆp(θ|x exist. For completeness, we ﬁrst mention two less commonly used options. Van Opheusden et al. [37] propose inverse binomial sampling (IBS) as a way to get an unbiased estimate of the log-likelihood of the model, rather than the likelihood itself. Estimating log-likelihoods is more diﬃcult than estimating likelihoods: small errors in small likelihoods are magniﬁed because log (p(x most computational eﬀort on low-likelihood regions of parameter space to distinguish between small and very small likelihoods. Often, however, the diﬀerence between small and very small does not matter for conclusions that will be drawn from ˆp(θ|x particular low-probability event of interest, it can be targeted with a speciﬁc f(θ). one of their true and approximate posterior pairs were the same. Such an approach combines the controversial aspects of null hypothesis signiﬁcance testing [13, 40] with technical diﬃculties like choosing the sample size in the K-S test. quantitative. These aim only to check whether ˆp(θ|x For these last scores Sand S, as for TV and Wasserstein distances, we run into (ˆp(θ|x), p(θ|x)) are inﬁnite if ˆp(θ|x) is an empirical distribution of samples. While avoids it. For quantitative evaluation of the accuracy of ˆp(θ|x), then, we recommend choosing func- Separately, Turner and Sederberg [36] used a Kolmogorov-Smirnov (K-S) test for whether The alternative evaluation methods we ﬁnd more promising are qualitative rather than approach is to plot ˆp(θ|x is known. This is ubiquitous in the literature [21, 23, 24, 26, 29, 32, 36], for good reason. Plotting is easy to implement and makes the most important diﬀerences between ˆp(θ|x p(θ|x care about. posterior with the ground truth parameters of synthetic data. Such a comparison does not require the true posterior to be tractable. A good estimate by this criterion concentrates on the true value, or assigns high probability to the true value [25], or has mean near the true value [18]. Like plotting the full posterior, comparing to a known ground truth is easy to do and can diagnose signiﬁcant algorithm failures. It does not require knowing the posterior and can be supplemented with calibration tests [25] or cross-validation [29]. Alone, however, ground truth comparison provides limited information. A distribution may have high density at the ground truth parameters while still being a poor approximation to the true posterior, either by underestimating uncertainty or because the data is atypical for the true parameters. parameters are unknown. In that case, it is still possible to sample parameters from the approximate posterior, simulate from them, and compare the simulation results to the data. Like posterior plots, such checks can diagnose signiﬁcant algorithm failures where the posterior predictive distribution is far from the data. Good performance on a posterior predictive check, however, does not imply that the approximate posterior has the right level of uncertainty. An approximate posterior that puts too little weight on plausible alternative parameter values would still perform well. all give plausible results. We would like to be able to evaluate the accuracy and computational eﬃciency of the many proposed likelihood-free inference algorithms whose approximate posteriors for standard examples are not obviously unreasonable. Of the quantitative scores we considered, S ready to compare algorithms’ eﬃciency. Once we have a satisfactory score, a natural followup is to ask what algorithm design optimizes accuracy for a ﬁxed computational cost. To highlight the eﬀect of the choice of score on the answer to that question, we ﬁrst consider the simpliﬁed case where the set of parameters is ) visually obvious, without requiring the user to decide beforehand what features they Rather than plotting the full distribution, it is also common [11, 21, 24] to compare the Neither of the previous approaches is possible when both the true posterior and the true In general, qualitative methods are useful but insuﬃcient for comparing algorithms that discrete. This could be, for example, a model selection problem where each model has no internal parameters to be estimated. As we outlined in Section 1.1, likelihood-free inference algorithms have two parts: choosing parameters for model simulations, and estimating the posterior based on the simulation results. With discrete parameters, the second part will be trivial, allowing us to focus on how parameters should be chosen. to inﬁnity while k, the number of possible parameter values, remains ﬁxed and ﬁnite. For each i, we choose a number n We assume n maximum likelihood estimate ˆp the prior on parameters. Importantly, n θ, not the probability of sampling θ variance, as discussed in the supplement (Section S5). The numerator and denominator are separately unbiased estimates of µ p(x in comparison to the variance (Supplement Section S2). Each term in Eq. 14 can be calculated explicitly: This leads, after some algebra, to an expression for the asymptotic variance: ulation budget N = Let Θ = {θ} for i = 1, 2, . . . , k. We will let N , the number of simulations performed, go Given a function f(θ), the resulting estimate of the true posterior expectation¯f is ) respectively; their ratio has bias of order 1/n, which will be asymptotically negligible Using the delta method (Supplement Section S2), the variance can be approximated as The only choice in our algorithm is how to select the nwith a constrained total sim- This involves three factors. First, π our prior knowledge. Second, (p hood: we should spend more eﬀort on regions where the likelihood is harder to learn. Third, trate on regions where a changed estimate of the likelihood would have a greater eﬀect on the estimate of The optimal simulation strategy for one function may be far from optimal for another. where in the last line we removed all factors that are shared between the expressions for n and n on neither. Moreover, both remaining factors imply that increasing p n. To minimize estimation error for this maximally simpliﬁed problem, you should always spend more computational eﬀort on the parameter value the data support less. p(x in Eq. 18 (solid green line) has the same scale and qualitative shape as empirical results, with some quantitative diﬀerences particularly for n samples with x average KL divergence E [KL (ˆp(θ|x expected, our asymptotic results are more accurate for higher N. With N = 1000, both bias and higher order contributions to the variance are negligible and the approximation of the KL divergence from Eq. 12 is excellent. for a larger discrete space and several target functions in the supplement (Section S4.1). In addition, the factor p can compete with the factor |f(θ ) −¯f, the diﬀerence between f(θ) and the true posterior mean,¯f: we should concen- ¯f. As in the case of importance sampling [9], the dependence on f is unavoidable. In the case k = 2, Equation 19 can be instructively simpliﬁed: . Both the prior and the function f have disappeared. The optimal algorithm depends Figure 1 illustrates the accuracy of the delta method calculation for an example with |θ) = 0.3, and p(x|θ) = 0.05. With N = 100 simulations, the variance approximation The behavior for k = 2 is not generic. For k > 2, Equation 19 depends on f, as we illustrate Figure 1: The variance estimate in Eq. 18 is qualitatively correct for small samples and precise for large samples, based on simulations of a model selection task with k = 2, p(x and p(x f(θ) = (θ = θ Eq. 12 (red diamonds), both averaged over 1000 trials, to Eq. 18 (green line). The horizontal axis is the single algorithm parameter n far from the posterior n percentiles over trials, which do not always cover the mean. On the far right of (a), often no samples from θ MSE for that estimate. in regions with high posterior probability. If, for example, f(θ) = (θ ∈ S), then |f (θ either p(θ ∈ S), if θ should be sampled correspondingly more. The choice of score is critical in the derivation of the optimal sampling proportions in Eq. 19. Both acceptance rate and eﬀective sample size are always improved by sampling more from parameters with higher likelihood, while error in the unnormalized posterior is minimized by sampling more from the parameter with likelihood closest to 0.5 where the Bernoulli variance asymptotically, samples from parameters in reverse order of likelihood. and summarize their recommendations in Table 3. For ease of comparison to the acceptance rate and eﬀective sample size, where higher is better, we plot the inverse MSE, or precision, rather than the MSE itself. Optimizing n |θ) = 0.05. Each plot compares the mean squared error for the indicator function are accepted, so ˆp(θ|x) = 1; the error bar quantiles are the KL divergence or ) is greater. In the opposite direction, inverse binomial sampling [37], where n∝ 1/p We illustrate the diﬀerent scores for the same two-hypothesis example as Fig. 1 in Fig. 2 Figure 2: In the example of Fig. 1, the acceptance rate (blue, dotted), eﬀective sample size (red, dash and two dots), or precision in estimating the unnormalized posterior (brown, dot-dashed) poorly match the precision in estimating the normalized posterior (green, solid). The horizontal axis is the proportion of simulations performed with θ score divided by the same score for rejection sampling (n tion S3.3), for example, increases the expected sampling while decreasing the accuracy of the posterior estimate by a factor of 0.62. IBS is the only strategy for choosing simulation parameters we found in the literature that outperforms rejection sampling in this example. In general discrete spaces with k > 2, targeting an alternate score may or may not improve on rejection sampling in MSE depending on the likelihood and function of interest (Supplement Section S4.1). in line with the derivations in Section 1.2. First, if f(θ) is an indicator function corresponding Table 3: Distributions of simulations that optimize various scores. Maximizing the acceptance rate implies never simulating with some parameters, which does not give a consistent estimator. When f(θ) is chosen appropriately, Eq. 19 optimizes other scores in addition to the MSE, to a 50% credible interval, so that |f (θ) − which optimizes the error in the unnormalized posterior (Supplement Section S3). If we also take the limit p(x is the proposal distribution that maximizes to either prior or posterior is (4N p(x argued in a slightly diﬀerent setting and way in [17]. For the prior, (4Np(x The acceptance rate does appear as a relevant score: a low acceptance rate suggest higher variance for a given dataset. This justiﬁes the intuition behind the standard argument against rejection sampling. However, there is an important distinction between “for a ﬁxed sampling scheme, does a higher acceptance rate for dataset A over dataset B mean lower error for dataset A?” and “for a ﬁxed dataset, does a higher acceptance rate for algorithm X over algorithm Y mean lower error for algorithm X?”. Low acceptance rates indicate a diﬃcult problem but not necessarily an ineﬃcient algorithm. pends on the unknown likelihood and posterior mean. Given a target function f (θ), however, the optimal distribution of simulation parameters is straightforward to estimate adaptively. Following that distribution, we can compute both an estimate of variance and an estimate of the variance itself (Supplement Section S4.2). For brevity, we postpone that discussion to the supplement and end our treatment of the discrete case here. We next turn to continuous parameter spaces, where despite new analytical challenges we will see qualitatively similar behavior. Analysis like that of Section 2 is signiﬁcantly more complicated when the parameter space Θ is continuous, for three main reasons. 1. It is no longer possible to simulate n In that limit, the variance approximation in Eq. 18 with simulations distributed according Optimal performance by the MSE score is not immediately available, because Eq. 19 dewe must either deterministically choose or randomly sample {θ} ⊂ Θ. If using an importance-weighted average of accepted samples (Eq. 5) the latter adds variance from the selection of {θ}, which we calculate and optimize in the supplement (Section S5), while the former option introduces bias. sults. One could use Eq. 5, apply multilevel Monte Carlo for variance reduction [39], or numerically integrate an approximate posterior based on a kernel regression esti- 3. It may be diﬃcult to choose or sample simulation parameters from the optimal distributhat results for the discrete case qualitatively translate. First, we show that for estimating the posterior mean of a simple one-dimensional parameter the optimal distribution for independently sampled θ outperforms sampling from the prior or posterior or maximizing Second, as an illustration of the additional complexity of continuous parameters, we present an example where changing from Eq. 5 to numerically integrating a kernel regression estimate (Supplement Section S6) reduces MSE similarly to optimizing the choice of simulation parameters. Finally, we apply the optimized independent sampling distribution in a model selection problem combining discrete and continuous parameters. case (Supplement Section S5) leads to an optimal importance distribution In our ﬁrst example (Fig. 3), we let θ ∈ R with prior uniform on [−40, 60] and likelihood sampling θ p(θ) nonzero acceptances; beyond that threshold, it performs as well as or better than sampling from the posterior (Fig. 3). Optimizing for the eﬀective sample size is better than either, and including the factor |f(θ) − (Fig. 3b). possibilities for improvement. First, estimating be optimal. Instead, we could train a model to explicitly compute the likelihood or posterior and integrate the result. Much of the recent research eﬀort in likelihood-free inference has been devoted to developing methods to do that. mate [4, 5], Gaussian process [15, 16], or mixture density network [21, 24]. As we will see, this choice can have a signiﬁcant eﬀect on the quality of the estimate. tion. In the discrete case, the minimum-MSE nfrom Eq. 19 can be explicitly computed and normalized given a posterior approximation. In the continuous case, sampling likely requires either a simpliﬁcation of the optimal distribution or MCMC with additional technical considerations and computational overhead. We do not fully address these obstacles. Instead, we present three examples demonstrating For independently sampled parameters, a delta method calculation similar to the discrete . We evaluate the mean squared error in E[θ] with four sampling strategies: prior ∼ p(θ), posterior sampling θ∼ p(θ|x), optimization of[ESS with θ∼ q(θ) ∝ pp(x|θ), and optimized independent sampling with θ∼ q(θ). Sampling from the prior requires at least p(x)≈ 100 samples to consistently have Although the targeted distribution outperformed alternatives in Fig. 3, there are two Figure 3: Sampling from a distribution targeting f(θ) = θ yields lower error than generic strategies. Here we consider a one dimensional parameter θ ∈ R with prior uniform on [−40, 60] and likelihood p(x|θ) = e (red, dot-dashed), qp ¯f)p(θ)p(x Section S5.1). We ran 1000 trials and computed the empirical mean eﬀective sample size (b) and squared error in estimating E error when using q size ∼ 3× higher than sampling from the prior (blue diamonds) but ∼ 4× lower than the maximum (green triangles). Despite the lower than for the squares) by either metric gets relatively worse with larger N , likely because more of our trials include accepted simulations with low likelihood and high weight that decrease variance. Points are omitted for prior sampling with N < 200 because there is a signiﬁcant chance no simulations are accepted and no posterior estimate can be made. Error bars in (b) show empirical standard errors. Error bars in (d) show empirical standard errors ignoring variance of the denominator; judging by the variation across N, they likely underestimate variability. The horizontal position of points in (b) and (d) is shifted slightly so the error bars do not overlap. . (a) Our candidate sampling distributions are the prior (blue, dashed), posteriorp |θ) (purple, solid) to optimize the MSE with independent sampling (Supplement [ESS-optimizing distribution. The observed performance of posterior sampling (red of θ. For example, Bayesian synthetic likelihood [29] ﬁts a multivariate normal distribution to observed summary statistics for each θ considered, while J¨arvenp¨a¨a et al. [16] use a Gaussian process prior on ∆. The resulting model then gives an estimate of the likelihood of the observed data p(x to approximate either p(x θ). The likelihood estimate is then combined with the prior to form a posterior, which can also be targeted directly [24]. Alternatively, kernel regression estimates ([4, 5], Supplement Section S6) convert an empirical distribution of samples to a continuous posterior estimate. For ABC-SMC, accuracy can be improved by including accepted samples from all rounds in the ﬁnal output (Supplement Section S7). Each of these algorithms is a proposal for step 2 of likelihood-free inference and can be combined with any strategy for choosing the simulation parameters {θ ample via stratiﬁcation (Supplement Section S5.2). This can reduce the part of the variance in Eq. 5 that comes purely from sampling parameters. Thoroughly investigating either of these choices goes beyond the scope of this paper. Instead, we show one example where both stratiﬁcation and changing from estimating as the empirical weighted mean of accepted samples (Eq. 5) to numerical integration of a kernel regression estimate give similar improvements in accuracy. pling. The parameter θ has a uniform prior on [0, 1], p(x the posterior probability that θ < 0.5. Stratiﬁed sampling following the continuous equivalent to Eq. 19 (Supplement Section S5.2) performs better than any other sampling strategy, but alternative distributions have nearly the same MSE if a kernel regression estimate is used. Full details of our kernel approach are given in the supplement (Section S6). This both is consistent with the good results seen with recent methods [1, 8, 11, 16, 21, 20, 39] that change how the posterior is estimated from simulations and suggests there is room for improvement in selection of simulation parameters. a model selection problem [34, 35], where there is at least one discrete parameter M (representing the choice of model) as well as model-speciﬁc parameters that may be continuous. Our results can be straightforwardly applied to such cases. As an illustration, we consider a problem with two models of equal prior probability. The likelihood for model 1 (M = 1) One approach models the distribution of the data x or discrepancy ∆(x, x) as a function Other proposals [1, 8, 11, 21, 20, 25] use density estimators based on neural networks The second way of improving estimates is to sample {θ} with variance reduction, for ex- Fig. 4 presents a case where optimizing for[ESS performs no better than rejection sam- Our ﬁnal example is a combination of the discrete and continuous cases. This represents Figure 4: The optimal distribution of simulation parameters depends on whether parameters are sampled independently and how posterior expectations are estimated. Here we consider estimating the posterior probability that θ < 0.5 (via f(θ) = [θ < 0.5]) with θ ∈ [0, 1] and p(x Panel (a) shows distributions targeted at f(θ) designed for stratiﬁed (solid blue line, Eq. S45) or independent (dashed red line, Eq. 23) sampling of {θ sampling strategies: (blue) sampling from the posterior, (red) sampling independently from the distribution designed for stratiﬁed sampling, (green) sampling from the prior, (purple) sampling from p(θ) pling distribution (Eq. 23). For each sampling distribution, the left box plot shows the error using Eq. 5 while the right box plot shows the error from numerically integrating a kernel regression estimate (Supplement Section S6), both with N = 1000 simulations. Crosses mark the mean of each distribution. While stratiﬁed sampling following Eq. S45 performs the best, other sampling strategies have similar error distributions when using the kernel regression estimate. [f(θ)] −¯f)for stratiﬁed sampling using Eq. S45 (cyan) to ﬁve diﬀerent independent p(x|θ) to maximizeESS, and (brown) sampling from the optimal independent sam- date sampling distributions are the prior (blue, dashed), posterior (red, dot-dashed), qp p(θ)p(x solid) with f(θ) = [M = 1] to optimize the MSE with independent sampling (Supplement Section S5.1). For each, we plot the importance distribution evaluated at M = 1, θ of θ. We ran 1000 trials and computed the empirical mean eﬀective sample size (b) and squared error in estimating p(M = 1|x using q(θ) in (d). The targeted distribution (purple circles) has an intermediate eﬀective sample size ∼ 20% lower than the optimum (green triangles) but substantially higher than sampling from the prior (blue diamonds). Despite the lower lower than for the (red squares) by either metric gets relatively worse with larger N, likely because more of our trials include accepted simulations with low likelihood and high weight that decrease variance. Error bars in (b) show empirical standard errors. Error bars in (d) show empirical standard errors ignoring variance of the denominator; judging by the variation across N, they likely underestimate variability. The horizontal position of points in (b) and (d) is shifted slightly so the error bars do not overlap. |θ) to maximize[ESS (green, dotted), and q(θ) ∝ (f(θ) −¯f)p(θ)p(x|θ) (purple, is e each θ three parameters together. As in Fig. 3, we compare four sampling strategies: θ θ∼ p(θ|x choose f (θ) = [M = 1] to target the posterior probability of model 1. prior without a better maximum likelihood. For the true posterior E proportion of simulations assigned to Model 1 is 93% for maximizing targeted independent sampling (Eq. 23). Again, incorporating the target function f yields higher precision (Fig. 5c-d) and lower eﬀective sample size (Fig. 5b). Properly evaluating the many important recent advances in likelihood-free inference requires clear quantitative scores for algorithm performance. We argue for scoring by the mean squared error of estimated function expectations, on the grounds that that score can be usefully applied to any algorithm yielding an approximate posterior, captures key features of interest like the mean, variance, and credible intervals, and includes the most promising other alternatives as limiting special cases. Common alternatives, like the eﬀective sample size possibly unacknowledged. Other scores answer diﬀerent questions: the acceptance rate, for example, is a better measure of the diﬃculty of a problem than the eﬃciency of an algorithm. Our empirical results show that using distributions targeted at speciﬁc functions, preferably with stratiﬁed rather than independent sampling, leads to lower MSE than optimization for generic scores. more sophisticated algorithms, however, have often used heuristics like the acceptance rate or eﬀective sample size for evaluation [6, 7, 12, 23, 28, 29, 32]. We are not aware, for example, of a treatment of the variance of ABC-SMC estimators comparable to Barber et al’s analysis of ABC rejection [2]. This paper begins to bridge that gap by optimizing the distribution of simulation parameters directly for the MSE score. parameters from the posterior, as is targeted in a wide range of algorithms [3, 7, 12, 21, 24, 25, 35, 34], is optimal for ABC, or even consistently better than sampling from the prior. ; the likelihood for model 2 (M = 2) is e. For each model, the prior on is uniform between −10 and 15. Without a subscript, θ = (M, θ, θ) refers to all ), θ∼ q(θ) ∝ p(θ)pp(x|θ), and θ∼ q(θ) ∝ |f(θ) −¯f|p(θ)pp(x|θ). We The evidence for model 1 is high: model 2 has an extra parameter and more diﬀuse This MSE score explicitly depends on the target f(θ), which we see as an advantage. Past work [2, 5] has analyzed the MSE score for rejection sampling. Papers introducing Carefully measuring accuracy allows us to disprove the claim [32] that sampling simulation This complements earlier work arguing against sampling from either prior or posterior by comparing to maximum likelihood (ML) estimation in an informative-data limit where ML works well [17]. The intuition underlying Eq. 19, that simulations should be concentrated where (1) they are required to learn the likelihood and (2) learning the likelihood matters, should be generally relevant for all state-of-the-art methods. is in the limit of large sample sizes. With a small number of simulations, higher order terms ignored with the delta method as well as the diﬀerence between a φ-divergence and expectation MSE may be signiﬁcant. Future work may also consider the eﬀect of the supremum over a class of functions in an integral probability metric. MMD [14] in particular is a promising candidate for further analysis, as unlike TV and Wasserstein it converges with rate n empirical samples. would more analysis of continuous or high-dimensional data where summary statistics are commonly used [10]. There are open questions both in how to choose and sample from an optimized distribution of parameters and in how to estimate the likelihood and posterior given simulation results; as we see in Fig. 4, each step matters. tional cost of likelihood-free inference is in simulating from the model and ignore algorithmic overhead. That assumption could be investigated more carefully; one particular risk is that an overly complex algorithm may lose the easy parallelization available with rejection sampling. For suﬃciently large N and cheap simulations, it will also be important to choose an algorithm with O(N) complexity [7] rather than the O(N x. In some applications, the same simulations may be used to analyze multiple datasets separately [11]. Optimal simulation distributions for such global learning [1, 20] are not yet known, though sampling from the prior [24] is intuitively plausible. Future work should pay careful attention to appropriately measuring accuracy and eﬃciency. Julia code to reproduce our examples is available at https://github.com/aforr/LFI_accuracy. Our results provide a base for further research in many directions. First, all of our analysis A rigorous treatment of the case with continuous parameters would be worthwhile, as Following much of the ABC literature [2, 28, 39], we assume that the primary computa- Finally, we assumed the goal was to learn the posterior for a single observed sample For all of these research directions, the choice of performance score will continue to matter.