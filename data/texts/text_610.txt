<title>Study of keyword extraction techniques for Electric Double Layer Capacitor domain using text similarity indexes: An experimental analysis</title> <title>M. Saef Ullah Miah , Junaida Sulaiman , Talha Bin Sarwar , Kamal Z. Zamli and Rajan Jose</title> <title>Abstract</title> <title>arXiv:2111.07068v1  [cs.IR]  13 Nov 2021</title> recommender systems to select more suitable keyword extraction and similarity index calculation techniques. <title>1 Introduction</title> Keywords are signiﬁcant for automated document processing. Keywords are the concise representation of the contents of a document [1]. From keywords, the context of the documents can be easily understood. When there is a need to process lots of documents or classify any document for any purpose, it is tedious to go through the whole document one by one and classify them. Instead, going through the keywords makes this process faster, even for a human. However, it is also a time-consuming process to go through the keywords for many documents by a human. This task can be automated by employing machines to look for the keywords and classify the documents. Since the process of keyword extraction is being automated, it should also be assured that extracted keywords represent the actual context of the document; else automated extraction will be a complete loss of time and resources. This assurance can be done by comparing the extracted keywords with human or expert assigned keywords. Therefore, this paper introduces an experimental study to measure the similarity score between expert-provided keywords and keyword extraction algorithms generated keywords to observe how similar the machinegenerated keywords’ values are to the expert provided keywords. In other words, this experiment can guide if the machine-generated keywords are feasible to utilize instead of expert-provided keywords for any speciﬁc domain. There are several different keyword extraction algorithms available at present [2, 3]. These algorithms are employed in different scenarios, such as recommender systems, trend analysis, similar document identiﬁcation, relevant document selection [4, 5, 6]. All these algorithms are divided into three primary categories based on their extraction technique: supervised, unsupervised, and semi-supervised technique [7]. This study compares the similarity scores for supervised and unsupervised techniques with three prominent similarity indexes, namely, Jaccard similarity index [8], Cosine similarity index [9, 10] and Cosine with Word vector similarity [11]. The key contributions of this work are, • Recommending a keyword extraction technique that provides more similar machinegenerated keywords to the expert or human provided keywords. • Recommending type of texts (positive texts only or whole text of a document) that provides more similar keywords. • Recommending a better similarity index for measuring similarity score between documents. • Finding the feasibility of utilizing machine-generated keywords instead of expertcurated keywords. The rest of the paper is organized as follows. Employed keyword extraction techniques and relevant works are presented in Section 2 with their known shortcomings and strengths. Employed methodologies for the experiment are mentioned in Section 3. Then, the result analysis of the experiment is discussed in Section 4, and concluding remarks in Section 5. <title>2 Background Study</title> In this paper, some notable and well-known similarity index calculation algorithms and keyword extraction algorithms are employed. All the text-similarity and keyword extraction algorithms with shortcomings and strengths are discussed in this section. <title>2.1 Keyword Extraction</title> Keyword extraction from text is an analysis technique that automatically extracts the most used and most important words or phrases from text based on different parameters [12]. In some techniques, these parameters can be deﬁned externally, and some techniques do not support external deﬁnition [7]. Mainly there are three classes of keyword extraction techniques. Among them, supervised and unsupervised techniques are employed in this study. Four unsupervised keyword extraction techniques are employed in this paper. Unsupervised techniques are prone to poor accuracy and require a larger corpus input, and do not extrapolate well [13]. However, unsupervised techniques are utilized widely compared to supervised techniques, as all sorts of domain-speciﬁc training labeled data are not always available for all the domains. YAKE YAKE was proposed by Campos et al., [14]. It is a lightweight unsupervised keyword extraction technique based on TF-IDF. YAKE extracts keywords by calculating ﬁve features, namely, Word Casing (WC), Word position (WP), Word Frequency (WF), Word Relatedness to Context (WRC), and Word DifSentence (WF). The relation between ﬁve features can be expressed through the following Equation 1, where S(w) is the measure for each word. After calculating the measure for each word, the ﬁnal keyword is calculated utilizing a 3-gram model [15]. W R ∗WP S(w) = (1) WC + TopicRank Bougouin et al. proposed topicRank [16] in 2013, which is a clusteringbased model. It divides the document into multiple topics employing the hierarchical agglomerative clustering [17]. Then utilizing the PageRank [18], it scores each topic and selects each top-ranked candidate keyword from each topic. After that, it selects all the top candidate words as ﬁnal keywords. MultipartiteRank MultipartiteRank is a topic-based keyword extraction model. It encodes topical information of a document in a multipartite graph structure. This technique represents candidate keywords and topics of a document in a single graph, and utilizing the mutually reinforcing relationship of the candidate keywords and topics improves candidate ranking. This method has two steps of selecting candidate words as keywords, i) Representing the whole document in a graph and ii) Assigning relevance score to each word. Between these two steps, position information is captured utilizing edge weights adjustment. As a result, most of the time, it outperforms different other key-phrase extraction techniques [19]. KPMiner El-Beltagy et al. proposed the KP-miner [20] in 2009. This method also utilizes TF-IDF to calculate words as keywords. This calculation is done in three steps, i) Selecting candidate words from the document utilizing least allowable seen frequency (lasf) factor and CutOff factor, ii) Calculating candidate word’s score, and iii) Selecting the candidate word with the highest score utilizing the candidate word position and TFIDF score as the ﬁnal keyword. While unsupervised algorithms do not need a large amount of labeled training data, supervised algorithms need a large amount of that data and perform poorly except in the training domain. However, for any speciﬁc domain, supervised techniques are preferred over unsupervised techniques [15]. In this paper, two supervised techniques are employed, KEA and WINGNUS. KEA KEA is a supervised keyword extraction algorithm proposed by Witten et al. in 1999 [21]. KEA classiﬁes a candidate keyword utilizing word frequency and position of the word in the document. After that, it predicts which candidate words are qualiﬁed as keywords utilizing the Naive Bayes machine learning algorithm. The machine learning model builds a predictive model initially. Then, keywords are extracted utilizing this predictive model [22]. WINGNUS This supervised keyword extraction technique is developed focusing on keyword extraction from scientiﬁc documents [23]. It utilizes inferred document logical structure [24] in the candidate word identiﬁcation process to limit the phrase number in the candidate word list. This method utilizes regular expression rules to extract candidate words, and instead of whole document text, it utilizes input text in different levels like title and headers or abstract and introduction. Like KEA, it also utilizes the Naive Bayes machine learning algorithm to select candidate words. <title>2.2 Text Similarity Index</title> Determining how similar two pieces of text are to each other is the simple idea of text similarity index or text similarity calculation. In this study, keywords from different documents extracted by keyword extraction algorithms and expert-provided keywords’ similarity are measured. In two ways, this similarity can be measured, one is lexical similarity, and another is semantic similarity [25, 26, 27, 28, 29, 30]. This paper implemented both the similarity measures utilizing Jaccard, Cosine, and Cosine with word vector similarity indexes and presented the outcome for EDLC based scientiﬁc articles. Jaccard similarity index is a lexical similarity index method, which calculates the similarity index at the word level. As lexical similarity is unaware of the word’s actual meaning or the entire phrase, Jaccard similarity takes two sets of text and calculates the similarity between all pairs of sets. Jaccard provides a similarity score with a range of 0% to 100%. This algorithm is very sensitive to sample size and may provide unexpected results for a small sample size. Conversely, for larger sample sizes, it is computationally costly [31, 32]. Jaccard similarity index is calculated utilizing the Equation 2, where A and B are two different sets of text or documents. |A B| J(A, B) = (2) |A| + |B| − |A B| Word vectors are a type of word embedding, where similar meaningful words are arranged in a similar representation, mostly with vectors. Each word is mapped to a vector in a predeﬁned vector space [33]. It is different from Jaccard similarity in the way that Jaccard measures lexical similarity, but in word vector, it is measured for semantic similarity. Utilizing word vectors, similar meaningful words can be measured rather than the exact word, enabling better scores for similarity measures. In this study, as a word vector model, Wod2vec [11] proposed by Mikolov et el., is utilized. Word2vec is different from the traditional tf-idf measure, where tf-idf sets one number per word, but Word2vec sets one vector per word. <title>3 Methodology</title> This study diverges into three major components, i) Data collection, ii) Data Processing, and iii) Similarity score calculation. In the data collection component, ground truth data and test data are collected from respective sources. Collected data is cleaned and processed for the similarity calculation component is done in the data processing component. In the similarity score calculation component, similarity scores for collected data are calculated with different similarity indexes employing different keyword extraction techniques. The conceptual overview of the employed methodology can be found in Figure 1. <title>3.1 Data Collection</title> In this study, the Electric Double Layer Capacitor (EDLC) domain is considered as the experiment’s use case. Hence, from the domain experts, a set of 32 keywords of the EDLC domain has been collected as ground truth keywords, and ten scientiﬁc documents are collected from the same domain, which satisﬁes the keywords and is suggested as the relevant document to the domain. The experiment is based on the quest that, from these ten documents, keywords are extracted through different keyword extraction techniques, and then extracted keywords are compared for the similarity score with the domain expert provided keywords. First column from the left of Table 1 contains the domain expert provided keywords for the EDLC domain. All the scientiﬁc documents are collected in portable document format (pdf), and keywords are collected in plain text. <title>3.2 Data Processing</title> In the data processing stage, collected pdf ﬁles are initially converted to plain text format. To convert the ﬁles, grobid [34] tool is utilized, which primarily converts the pdf ﬁles to tei xml format and then with a custom tei xml parser xml contents are converted to a plain text ﬁle. The custom xml parser is developed by the authors utilizing the python programming language. After the conversion, text contents are cleaned to remove extra Figure 1: Overview of the employed methodology Table 1: Domain expert curated keywords for EDLC domain with lemmatised and stemmed version. From left, Keywords column contains the original keywords provided by the domain experts. Lemmatised keyword and Stemmed keyword columns contain lemmatised and stemmed version of the original keywords. Figure 2: Positive and Negative sentence distribution of the dataset utilized in this study. <title>3.3 Similarity Calculation</title> With two sets of text obtained from the data processing component, all keyword extraction algorithms are employed to extract keywords from each set of each document. Firstly texts are passed into all the keyword extraction techniques, namely YAKE, TopicRank, MultipartiteRank, KPMiner, KEA, and WINGNUS. All techniques return the extracted keywords of the provided texts of a document. Then those keywords and expert-provided keywords are passed to the similarity index calculator to calculate the similarity score between them. Three similarity indexes are utilized to calculate the similarity score, namely, Jaccard, Cosine, and Cosine with word vector similarity index. This whole process is executed for all the documents with positive and all texts of each document. After processing each document, scores are stored with appropriate labels to analyze the result. The similarity calculation component for the scenario described above can be expressed through the Algorithm 1 provided below. Algorithm 1: Similarity Score Calculation Input : Whole text String A_string Input : Positive sentence String P_string Input : Domain expert curated keywords list’s string KW _string Output: String containing ﬁlename, algorithm and score algo_name algorithm from text_content Sim_algo "wingnus"] <title>3.4 Experimental Setup</title> All experiment-related codes are developed utilizing Python programming language version 3.7.3 [39] for this study. Jaccard and Cosine similarity algorithms are developed following the equation described in their original papers [8, 40]. Cosine similarity with word vector algorithm is implemented utilizing Spacy Python library [41]. All keyword extraction algorithms are implemented utilizing pke [42] Python package. The experiment is done in a MacBook with macOS Big Sur operating system version 11.5 with a 1.2 GHz dual-core Intel Core m5 processor and 8 gigabytes of RAM. <title>4 Results and Discussion</title> To begin with the result analysis, Table 2 and Table 3 are generated from the experiment. Both tables contain the similarity scores of ten standard documents generated by different keyword extraction techniques and similarity index algorithms. Table 2 contains the results obtained from the unsupervised keyword extraction techniques, and Table 3 contains the results generated by the supervised keyword extraction techniques. For unsupervised techniques, the MultipartiteRank algorithm performs better in all three similarity indexes than other implemented keyword extraction techniques. Furthermore, it gives the best result of 92% similarity score for positive sentences and 91% for all sentences of the documents while employed with the Cosine with word vector similarity index. The lowest performing similarity index algorithm is the Jaccard similarity index for the same keyword extraction technique with a score of 14% similarity score for both positive and all sentences of the documents. It is also observed from the experimental result that, Cosine with word vector similarity index is consistently performing better than Jaccard and cosine similarity index for all the unsupervised keyword extraction techniques. This analysis can easily be understood from Figure 3a. This ﬁgure presents the distribution of all the similarity scores of all the unsupervised techniques employed in this study for Jaccard, Cosine, and Cosine with word vector similarity indexes. Table 2: Similarity scores calculated for different unsupervised keyword extraction techniques. Table 3: Similarity scores calculated for different supervised keyword extraction techniques. Figure 3: Distribution of similarity scores of supervised and unsupervised keyword extraction techniques employed in positive and all sentences for Jaccard, Cosine and Cosine with Word Vector similarity indexes. On the other hand, for the supervised techniques, the KEA keyword extraction algorithm performs the best with 91% of similarity score while calculating with the Cosine with word vector similarity index for both positive and all sentences of the documents. However, the WINGNUS supervised keyword extraction technique provides better similarity scores for Cosine and Jaccard similarity indexes only for positive sentences, which are 22% and 12% similarity scores. Nevertheless, KEA is performing better for all sentences while measured with Jaccard and Cosine similarity indexes. However, KEA holds the best similarity score utilizing the Cosine with word vector similarity index, which is around 70% more than those measured with Jaccard and Cosine similarity index. This analysis can be more clear with a visual representation. Figure 3b represents the distribution of all the similarity scores for all the supervised keyword extraction techniques with all three similarity indexes. Among supervised and unsupervised keyword extraction techniques, the unsupervised technique, namely, MultipartiteRank, exhibits better performance in achieving a higher similarity score for positive sentences while measured with Cosine with word vector similarity index. Furthermore, for all sentences, unsupervised technique, MultipartiteRank, and supervised technique, KEA produces the same score of 91% in Cosine with word vector similarity index. Similarity score comparisons for both supervised and unsupervised methods are projected in Figure 4. Figure 4: Similarity scores of different supervised and unsupervised keyword extraction techniques for Jaccard, Cosine and Cosine with Word vector similarity indexes. positive sentences and the dataset with all sentences, as shown in the experimental result. The similarity values between the positive sentences and all sentences vary from 1% to 4%. For example, in the MultipartiteRank algorithm, the Jaccard and Cosine similarity values are the same for both texts, 14% and 25%, respectively. However, for the cosine with word vector similarity index, the text of the positive sentence achieves 92% similarity, and the text of all sentences achieves 91% similarity, which is a minimal difference of 1%. On the other hand, in the algorithm KEA, the similarity value of cosine with word vector is the same for both text data, i.e., 91% of similarity value. The maximum difference of 4% in similarity score is observed for the YAKE algorithm in similarity index Cosine with Word Vector. Hence, it can be said that positive sentences and all sentences have a similar effect on the similarity index with very little difference from 1% to 4%. Although the positive sentences have a negligible effect on the similarity computation, they have a more signiﬁcant impact on the running time of the similarity computation process. From the experiment results, the unsupervised algorithms MultipartiteRank and the supervised algorithms KEA perform better than the other algorithms used in terms of similarity index. Therefore, a runtime comparison is performed for both algorithms to study the runtime for both positive and all text sets for computing all similarity indices. Table 4 presents the runtime comparison result for the two better-performing keyword extraction techniques MultipartiteRank and KEA for Jaccard, Cosine, and Cosine similarity with word vector indices. The runtimes reported in the table 4 are the average of 5 runtimes of the experiment, which includes only the similarity computation. From the runtime table, it can be seen that positive texts have a great impact on the duration of the similarity calculation. When computing the similarity of the texts with the keywords given by the experts, the positive sentences take signiﬁcantly less time than computing the similarity of all sentences. For example, in the unsupervised MultipartiteRank algorithm, the computation of all sentences takes 232.4, 225.1, and 230.2 seconds for the Jaccard, Cosine, and Cosine with word vector similarity indices, respectively. On the other hand, the computation of positive sentences takes only 143.6, 140.86, and 142.7 seconds for Jaccard, Cosine, and Cosine with word vector similarity indices, respectively, which is 88.8, 84.24, and 87.5 seconds less for the aforementioned similarity indices. A similar pattern is also observed for the supervised KEA algorithm, i.e., computing the similarity of positive sentences takes less time than computing all sentences. Figure 5 shows the comparison results in a more understandable form. Table 4: Run-time comparison in seconds(s) of positive and all sentences texts for MultipartiteRank and KEA keyword extraction algorithms in terms of Jaccard, Cosine and Cosine with Word Vector similarity indexes. Figure 5: Comparative scores of similarity calculation run-times for positive and all sentences employing MultipartiteRank and KEA keyword extraction algorithms. all the documents. From the word clouds of top-performing two methods, it is also visible that there are similar keywords of the same scores among all machine-generated and expert provided keywords. The study of the experimental results suggests that for extracting keywords and checking the similarity of the extracted keywords from scientiﬁc documents, especially for the EDLC related documents, the unsupervised keyword extraction technique MultipartiteRank algorithm can be considered in addition to the expert curated keywords. Although this algorithm requires slightly more computation time than the supervised keyword extraction technique KEA, it gives better results than KEA. If computation time is considered or required over better similarity score, then it is recommended to employ the supervised keyword extraction technique KEA for 1% of similarity score drop over MultipartiteRank algorithm. When choosing between the positive and the whole article text content, it is recommended to choose the positive text as it has a very small impact on the similarity score but a larger impact on the computation time. Positive texts have no or very little impact on the similarity scores, but require less computation time than all the texts of the scientiﬁc articles. <title>5 Conclusion</title> The aim of this study is to ﬁnd out which keyword extraction technique provides more similar keywords to the expert provided keywords, which text types have more similarity, which similarity index provides more similarity scores and whether the use of machine generated keywords is feasible with respect to the expert provided keywords. The experiment shows that the unsupervised keyword extraction technique MultipartiteRank provides 92% similarity with the expert provided keywords in cosine with Word Vector similarity index for positive sentences of the documents from EDLC domain. This study can be further extended with keywords for other domains with a larger dataset in other environments, including author-supplied keywords. Figure 6: Word cloud representation of the keywords extracted by the top performing keyword extraction techniques achieved with Cosine with word vector similarity index. <title>Data Availability</title> <title>Conﬂicts of Interest</title> The authors declare no conﬂicts of interest. <title>Acknowledgment</title> Authors would like to express their gratitude to the domain experts for their support and knowledge. <title>References</title> [1] S. Rose, D. Engel, N. Cramer, and W. Cowley, “Automatic keyword extraction from individual documents,” Text mining: applications and theory, vol. 1, pp. 1–20, 2010. [2] K. S. Hasan and V. Ng, “Automatic keyphrase extraction: A survey of the state of the art,” in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1262–1273, 2014. [3] M. S. U. Miah, M. S. Tahsin, S. Azad, G. Rabby, M. S. Islam, S. Uddin, and M. Masuduzzaman, “A geofencing-based recent trends identiﬁcation from twitter data,” IOP Conference Series: Materials Science and Engineering, vol. 769, p. 012008, jun 2020. [4] T. B. Sarwar and N. M. Noor, “An experimental comparison of unsupervised keyphrase extraction techniques for extracting signiﬁcant information from scientiﬁc research articles,” in 2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM), pp. 130–135, IEEE, 2021. [5] M. S. U. Miah, M. S. Tahsin, S. Azad, G. Rabby, M. S. Islam, S. Uddin, and M. Masuduzzaman, “A geofencing-based recent trends identiﬁcation from twitter data,” in IOP Conference Series: Materials Science and Engineering, vol. 769, p. 012008, IOP Publishing, 2020. [6] M. S. U. Miah, J. Sulaiman, S. Azad, K. Z. Zamli, and R. Jose, “Comparison of document similarity algorithms in extracting document keywords from an academic paper,” in 2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM), pp. 631–636, IEEE, 2021. [7] S. Beliga, “Keyword extraction: a review of methods and approaches,” University of Rijeka, Department of Informatics, Rijeka, pp. 1–9, 2014. [8] P. Jaccard, “THE DISTRIBUTION OF THE FLORA IN THE ALPINE ZONE.,” New Phytologist, vol. 11, pp. 37–50, feb 1912. [9] “Cosine Similarity - Understanding the math and how it works? (with python).” [10] “9.5.2. The Cosine Similarity algorithm - 9.5. Similarity algorithms.” [11] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013. [12] N. Firoozeh, A. Nazarenko, F. Alizon, and B. Daille, “Keyword extraction: Issues and methods,” Natural Language Engineering, vol. 26, pp. 259–291, may 2020. [13] K. Bennani-Smires, C. Musat, A. Hossmann, M. Baeriswyl, and M. Jaggi, “Simple unsupervised keyphrase extraction using sentence embeddings,” in Proceedings of the 22nd Conference on Computational Natural Language Learning, (Brussels, Belgium), pp. 221–229, Association for Computational Linguistics, Oct. 2018. [14] R. Campos, V. Mangaravite, A. Pasquali, A. Jorge, C. Nunes, and A. Jatowt, “YAKE! Keyword extraction from single documents using multiple local features,” Information Sciences, vol. 509, pp. 257–289, jan 2020. [15] C. Sun, L. Hu, S. Li, T. Li, H. Li, and L. Chi, “A review of unsupervised keyphrase extraction methods using within-collection resources,” Symmetry, vol. 12, no. 11, pp. 1–20, 2020. [16] A. Bougouin, F. Boudin, and B. Daille, “Topicrank: Graph-based topic ranking for keyphrase extraction,” in International joint conference on natural language processing (IJCNLP), pp. 543–551, 2013. [17] O. Medelyan, E. Frank, and I. H. Witten, “Human-competitive tagging using automatic keyphrase extraction,” in Proceedings of the 2009 conference on empirical methods in natural language processing, pp. 1318–1327, 2009. [18] L. Page, “Method for node ranking in a linked database,” Google,“Patents,”http://www. google. com/patents/US6285999, 1997. [19] F. Boudin, “Unsupervised keyphrase extraction with multipartite graphs,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), (New Orleans, Louisiana), pp. 667–672, Association for Computational Linguistics, June 2018. [20] S. R. El-Beltagy and A. Rafea, “Kp-miner: A keyphrase extraction system for english and arabic documents,” Information systems, vol. 34, no. 1, pp. 132–144, 2009. [21] I. Witten, G. Paynter, E. Frank, and C. Gutwin, “C., nevillmanning,“kea: Practical automatic keyphrase extraction,”,” in Proceedings of the fourth ACM conference on Digital, libraries, pp. 254–255, 1999. [22] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, and C. G. Nevill-Manning, “Kea: Practical automated keyphrase extraction,” in Design and Usability of Digital Libraries: Case Studies in the Asia Paciﬁc, pp. 129–152, IGI global, 2005. [23] T. D. Nguyen and M.-T. Luong, “Wingnus: Keyphrase extraction utilizing document logical structure,” in Proceedings of the 5th international workshop on semantic evaluation, pp. 166–169, 2010. [24] S. Mao, A. Rosenfeld, and T. Kanungo, “Document structure analysis algorithms: a literature survey,” in Document Recognition and Retrieval X, vol. 5010, pp. 197– 207, International Society for Optics and Photonics, 2003. [25] G. Maheshwari, P. Trivedi, H. Sahijwani, K. Jha, S. Dasgupta, and J. Lehmann, “Simdoc: Topic sequence alignment based document similarity framework,” in Proceedings of the Knowledge Capture Conference, pp. 1–8, 2017. [26] C. Yang, B. He, and Y. Ran, “Utilizing embeddings for ad-hoc retrieval by document-to-document similarity,” arXiv preprint arXiv:1708.03181, 2017. [27] S. Aryal, K. M. Ting, T. Washio, and G. Haffari, “A new simple and effective measure for bag-of-word inter-document similarity measurement,” arXiv preprint arXiv:1902.03402, 2019. [28] P. Sitikhu, K. Pahi, P. Thapa, and S. Shakya, “A Comparison of Semantic Similarity Methods for Maximum Human Interpretability,” in International Conference on Artiﬁcial Intelligence for Transforming Business and Society, AITB 2019, Institute of Electrical and Electronics Engineers Inc., nov 2019. [29] V. Thada and V. Jaglan, “Comparison of jaccard, dice, cosine similarity coefﬁcient to ﬁnd best ﬁtness value for web retrieved documents using genetic algorithm,” International Journal of Innovations in Engineering and Technology, vol. 2, no. 4, pp. 202–205, 2013. [30] R. Steinberger, B. Pouliquen, and J. Hagman, “Cross-lingual document similarity calculation using the multilingual thesaurus EUROVOC,” in Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), vol. 2276, pp. 415–424, Springer Verlag, 2002. [31] I. Neo4j, “9.5.1. The Jaccard Similarity algorithm - 9.5. Similarity algorithms.” [32] S. Glen, “"Jaccard Index / Similarity Coefﬁcient" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/jaccardindex/.” [33] J. Brownlee, “What Are Word Embeddings for Text?.” [34] “Grobid.” https://github.com/kermitt2/grobid, 2008–2021. [35] R. Sproat, A. W. Black, S. Chen, S. Kumar, M. Ostendorf, and C. Richards, “Normalization of non-standard words,” Computer speech & language, vol. 15, no. 3, pp. 287–333, 2001. [36] Grammarly, “Negatives and Negation–Grammar Rules | Grammarly,” 2021. [37] J. Col, “Negative Vocabulary Word List - Enchanted Learning,” 1998. [38] Y. HaCohen-Kerner and H. Badash, “Positive and Negative Sentiment Words in a Blog Corpus Written in Hebrew,” in Procedia Computer Science, vol. 96, pp. 733– 743, Elsevier B.V., jan 2016. [39] G. Van Rossum and F. L. Drake, Python 3 Reference Manual. Scotts Valley, CA: CreateSpace, 2009. [40] G. Salton and C. Buckley, “Term-weighting approaches in automatic text retrieval,” Information Processing and Management, vol. 24, pp. 513–523, jan 1988. [41] M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd, “spaCy: Industrialstrength Natural Language Processing in Python,” 2020. [42] F. Boudin, “pke: an open source python-based keyphrase extraction toolkit,” in Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, (Osaka, Japan), pp. 69–73, December 2016.