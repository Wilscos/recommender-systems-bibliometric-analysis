Abundant real-world data can be naturally represented by large-scale networks, which demands eﬃcient and eﬀective learning algorithms. At the same time, labels may only be available for some networks, which demands these algorithms to be able to adapt to unlabeled networks. Domain-adaptive hash learning has enjoyed considerable success in the computer vision community in many practical tasks due to its lower cost in both retrieval time and storage footprint. However, it has not been applied to multiple-domain networks. In this work, we bridge this gap by developing an unsupervised domain-adaptive hash learning method for networks, dubbed UDAH. Specifically, we develop four task-speciﬁc yet correlated components: (1) network structure preservation via a hard groupwise contrastive loss, (2) relaxationfree supervised hashing, (3) cross-domain intersected discriminators, and (4) semantic center alignment. We conduct a wide range of experiments to eval- Tao He, Lianli Gao, Jingkuan Song, Yuan-Fang Li uate the eﬀectiveness and eﬃciency of our method on a range of tasks including link prediction, node classiﬁcation, and neighbor recommendation. Our evaluation results demonstrate that our model achieves better performance than the state-of-the-art conventional discrete embedding methods over all the tasks. Keywords: Domain-adaptive Learning, Network Embedding, Cross-domain Discriminator, Center Alignment 1. Introduction discriminative and compact binary codes from high-dimensional features or attributes. With the explosive increase in the amount of available digital data, hash learning has become a prominent approach to eﬀective data compression, as it enables eﬃcient storage (in terms of space) and fast retrieval (in terms of time). Networks can naturally represent data in diverse real-world applications. As a result, representation learning (i.e., embedding) methods [2], especially those based on neural networks, have become an active research problem in the machine learning and deep learning communities. tations of nodes into semantical binary codes, taking into account neighborhood proximity. Although hashing techniques [3, 4, 5] have been explored for network embedding, most of them focus on single-domain networks, that is, these hash functions only work well on the source domain but would per- Hash learning, or learning to hash [1], aims at learning low-dimensional, For networks, hashing is useful for converting high-dimensional represenform badly on a target (new) dataset with a large distributional shift from the source. In other words, these methods do not adequately address the domain adaptation problem in network hashing. domain so that it can handle a target domain. However, the problem of ﬁnetuning lies in the fact that retraining the model on new datasets requires availability of human annotations on the target domain, which is expensive to obtain and thus may not be available. Therefore, it is critical to learn a domain-adaptive hash function that is able to handle multiple domains without supervised training on the unlabeled target domain. Recently, unsupervised domain adaptive learning [7, 8] has attracted signiﬁcant attention. The aim of this task is to transfer knowledge learned in the supervised setting on the source domain to the target domain, which is unlabeled. However, the distribution discrepancy between the source and target domains becomes a main obstacle to the knowledge transfer. Speciﬁcally, that disparity could result in undesirable non-alignment of the two domains’ embeddings on the common space, which heavily inﬂuences the prediction on the target domain. Hence, how to eﬀectively align the two domains is a central challenge in domain adaptive learning. Generative Adversarial Networks (GANs), and train a discriminator network aiming at judging whether the features comes from the source domain or the target domain. At the same time, a feature learning component tries to fool A natural solution is to ﬁnetune [6] the hash function learned on a source To alleviate the disparity issue, a suite of techiques [9, 10, 11, 12] employ the discriminator so that it cannot distinguish the origin of the features. When the discriminator cannot determine the origin of the features, domain disparity has been suppressed to a relatively low level. However, the discriminator in [9, 11] is only aware of which domain the feature comes from, but unaware of the speciﬁc semantics of the feature, ultimately leading to coarse alignment of tow domains, on their common space. Additionally, though the discriminators can distinguish the distribution of continuous features, they are not designed to judge the distribution of discrete codes [13], which limits the application of GANs in domain adaptive hash learning. diﬀerentiable nature of hashing function [14, 15], which is caused by the widely-used, non-diﬀerentiable sign function. This problem has been mitigated by replacing it with the diﬀerentiable and continuous tanh function. However, the tanh function could produce undesirable relaxation errors [10] and degrade the quality of learned hash codes. Although some works [3, 16] have proposed to leverage an alternating algorithm to optimize hash codes, it is hard to integrate it into a deep neural network in an end-to-end fashion. Hashing method for networks that eﬀectively transfers knowledge learned on the source domain to the target domain. UDAH address the three issues discussed above: (1) how to enable the learned knowledge on source domain to transfer to the target domain; (2) how to eﬀectively align two domains on the commone space; and (3) how to alleviate the issue of vanishing gradient Another major challenging facing domain adaptive hashing is the non- In this work, we propose UDAH, an Unsupervised Domain-Adaptive produced by the relaxed hash function tanh. To address the ﬁrst issue, we devise cross-domain intersected discriminators with a knowledge distillation loss. For the second, we explore a semantic centers alignment component to ensure that semantic space are explicitly aligned. Last, we adopt a reparameterization trick, i.e., Gumbel-Softmax [17], which has enjoyed great success in other discretization tasks such as product quantisation [18]. GumbelSoftmax can eﬀectively reduce gradient vanishing and enable our model to be trained in an end-to-end manner. 1. We propose an unsupervised domain adaptive hashing method for net- 2. We develop two components, cross-domain discriminators and semantic 3. To allow the model to eﬀectively preserve neighborhood structure, we 4. We evaluate UDAH on three domain-adaptive networks. Our results In summary, our main contributions are fourfold. works, dubbed UDAH, which can be trained on the source domain in a supervised fashion, and transfered to the unlabeled target domain. To the best of our knowledge, we are the ﬁrst to propose a technique that is dedicated to learning domain-adaptive hash functions for networks. centers alignment, to reduce domain distribution disparity. devise a hash groupwise contrastive loss, which shows superiority to the conventional pairwise constraint. strongly demonstrate that UDAH outperforms the other state-of-theart methods on three tasks: link prediction, node classiﬁcation, and node recommendation. Furthermore, we theoretically analyze the fea- The preliminary version proposed a domain-adaptive hashing method for images, which leverages generative adversarial networks (GANs) to alleviate the distribution discrepancy. In this manuscript, we have made the following major extensions: 1. We ﬁrst propose the task, domain-adaptive hash for networks, aiming to 2. We further adopt a knowledge distillation strategy to reduce domains 3. We leverage a reparameterization technique to design a diﬀerentiable 4. We propose a hard groupwise contrastive loss for network embedding. 5. We provide a theoretical analysis about the relationship between re- 2. Related Work learning, network embedding, and learning to hash. sibility of reducing domain disparity for UDAH . A short conference version of this paper has appeared in IJCAI 2019 [19]. learn a transferable hash function so that it performs well on multipledomain networks with large distribution disparity. discrepancy of networks. hash function. ducing domain discrepancy and UDAH. We brieﬂy survey relevant literature from three aspects: domain-adaptive 2.1. Domain Adaptation information on the source domain to guide model training on the target domain without ground-truth labels. In the computer vision community, domain adaptation has been applied in segmentation [20] and image retrieval [21]. Unlike traditional training datasets that consist of single-domain data, domain-adaptive learning aims to train a uniﬁed model so that it can handle multiple domains (e.g., digits and handwritten numbers). The main challenge of domain adaptation lies in the distributional discrepancy between diﬀerent domains, also named as domain shift in some other ﬁelds. To this end, many unsupervised strategies has been proposed to diminish the domain distribution semantical mismatch. A fundamental idea is to supervisedly train a classiﬁer on the source domain and then ﬁnetune it on the new domain [22]. Some works focus on how to assign high-conﬁdence labels, also named as pseudo labels, for the target domain. In this work [23], an autoencoder based duplex networks was proposed to bridge the distribution gap between the two domains by equipping two generator networks with the ability to reconstruct both domains’ data from their latent space. In fact, a standard practice of dealing with adaptive learning is to project the source and target domains into a common space and then reduce the domain disparity [24] by a distribution alignment component or loss function. The main purpose of domain adaptation is to manipulate supervised 2.2. Network Embedding heterogeneous network, homogeneous network, attributed network, etc) into a low-dimension vector and simultaneously preserve the network’s information, including structure and semantics as much as possible. core idea of matrix factorization is to construct a high-dimensional graph matrix, namely a Laplacian matrix or a similarity matrix, from high dimensional data features and then use some dimensionality reduction strategies (e.g., SVD and NMF, etc.) to transform the original high dimension vectors to low-dimensional, compact structure-preserving embeddings. Specifically, [26] ﬁrst proposed an objective function based on Graph Laplacian Eigenmaps aiming to enable embedded vectors to approximate their original similarity matrix constructed by Laplacian transform. Due to the fact that the similarity matrix plays an important role in the embedding process, many subsequent works investigated how to construct an information-rich and representative matrix so that the embedded vectors are equipped with more similarity information. in recent years, especially with the boom of graph convolutional networks (GCN) [27]. Autoencoders [28] are a widely used technique in network embedding methods that are based on deep learning. The core goal of autoencoders is to bridge the gap between the input and output by an encoder and a Network embedding [2] aims to map each node or edge in a network (e.g., Matrix Factorization is widely adopted in many previous studies [25]. The Deep Learning is a mainstream technique to conduct network embedding decoder, where the encoder aims to project the input data into a latent space by nonlinear mapping functions, and the decoder inversely reconstructs the original information (e.g., similarity matrix and edge matrix, etc.) from the latent space. When the reconstruction loss achieves a relative low level, we treat the output on the latent space as embedding vectors. 2.3. Learning to Hash for many years due to its time and space eﬃciency [29]. We can generally divide hash methods into two categories: supervised and unsupervised. For the former, many works are dedicated to preserving pointwise similarity signals of raw data points into binary codes by various metrics, such as pairwise [30] and ranking list [31]. As a matter of fact, for all variants of similarity calculation, the main purpose is to force the binary codes to be equipped with consistent semantic information. In comparison, unsupervised hash methods turn to pseudo similarity preservation [32] constructed by side information or the model itself instead of directly using the ground truth. It is no doubt that supervised hash methods is signiﬁcantly superior to the unsupervised ones in terms of the quality of hash codes. Another issue is that most of the existing methods [29, 33] can only perform well on single-domain data, that is, the learned hash function lacks the transferability between various datasets. To alleviate this drawback, cross-modal hash techniques [34, 35] have been proposed to deal with multiple domains, such as images to text or Hashing as a powerful compression method that has been widely studied text to images. 3. Problem Deﬁnition is the label of x goal of domain-adaptive hash learning is to train a shared hash function M supervisedly on G both domains. (1) the deep encoder network, (2) diﬀerentiable supervised hashing, (3) se- Let G= {(x, y)}denote the source domain, where y∈ {1, . . . , N} Figure 1 shows our overall framework, which consists of four modules: mantic centers alignment, and (4) cross-domain intersected discriminators. Speciﬁcally, the encoder aims at transforming input nodes’ attributes into embeddings. The supervised hashing component focuses on learning a hash function able to ﬁt the target domain by supervision of the source domain. the cross-domain intersected discriminators are responsible for transferring knowledge from source domain to target domain. Finally, Semantic centers alignment constrains the semantics, e.g., cluster centers of the two domains, to be aligned. 4.1. Deep Encoder Networks the multi-layer perceptron (MLP) [37] as our encoder network. Note that we use the same encoder network for both domains. First, we illustrate how our MLP-based encoder embeds network data into a latent space. Concretely, our deep encoder network consists of multiple perceptron layers and takes nodes attributes X as input. The encoding process for both domains can be formulated, as: where the MLP consists of three components: Dropout, LayerNorm and ReLU non-linearity; i denotes the i-th layer of the multi-layer perceptron, and superscripts Following a wide range of network embedding techniques [7, 36], we deploy and brepresent the i-th layer’s weight and bias parameters respectively; It is worth noting that when i = 1, h (resp. target) domain node features x learning discriminative representations. Recently, many works [39, 40] have proposed a triplet network based on metric learning to preserve neighborhood proximity into the latent space. Inspired by these work, we propose a hard contrastive loss to equip embeddings with nodes’ neighborhood information. Speciﬁcally, we deﬁne two types of node pairs: positive and negative, and consider two nodes as a positive pair P otherwise as a negative pair P function as: where λ is a constant margin hyperparameter; W the encoder; and φ(·) is a function to measure the distance of two embeddings in the embedding space, for which we choose the Euclidean distance (L norm). However, as the number of negative pairs are orders of magnitudes more than the number of positive pairs, the model is prone to inclining heavily to the negative samples, causing it to poorly preserve network structure. loss on positive and negative groups instead of pairs, i.e., our groupwise objective function aims at minimizing the maximal distance in the positive Mnih et al. [38] has demonstrated that metric learning is eﬀective in To address this issue, we propose further to impose a hard contrastive group whilst maximizing the minimal distance in the negative group. More concretely, the positive group of node i is deﬁned as all of its direct neighbors, denoted as j ∈ P the neighborhood of node i, that is, j ∈ P as the groupwise hard contrastive loss as the below: i, we sample all its positive neighbors as P the large size of |P construct P distance calculation, and it is natural to ask why it could perform better than Eq. (2). We think the main reason is that Eq. (2) is prone to falling into sub-optimality, because Eq. (2) can only select |P each time, but due to the large scale of |P anchor nodes’ positive pairs have smaller distances than its negative ones. By contrast, in Eq. (3), more negative pairs are sampled each time and we only select the negative points with the largest distance to optimize the contrastive loss, which beneﬁts the model to ﬁnd the optimal solution. L=max(0, λ + maxφ(z, p) − minφ(z, p))(3) In our implementation, since |P| is not too large, for a give anchor node Compared with Eq. (2), Eq. (3) only adds the group constraint on the 4.2. Diﬀerentiable Supervised Hashing for Source Domain while it is often assumed that no label is available for the target domain [41, 36]. Therefore, how to make use of the available labels plays a key role in domain-adaptive hashing learning. Intuitively, we adopt the supervised pairwise hash objective function in our preliminary work [19] to preserve label similarity into hash codes, as below: where W by b truth labels of the source domain, and l is the length of the hash code. Speciﬁcally, if two points have at least one same label, their similarity is deﬁned as 1 and otherwise −1. By optimizing Equation (4), our model minimizes the embedding distance of points with a same label but maximizes the distance across diﬀerent labels. diﬀerentiable, so it cannot be minimized directly by backpropagation. To solve this problem, we treat the discretization as a classiﬁcation problem, that is, a hash code in each dimension is the result of a binary classiﬁcation, which is equivalent to adding a binary classiﬁers for each discretization dimension. In the implementation, we add a linear classiﬁer layer after the embeddings In domain-adaptive learning, labels are available for the source domain, denotes parameters of the encoder, b, bare hash codes generated = sign(z), S∈ {−1, 1} is a similarity matrix constructed from ground- Unfortunately, as discussed before, the hash function b= sign(z) is not z, and the classiﬁed score is denoted as u denoting the classiﬁed two options and l is the hash code length. For ease of illustration, we separate z i.e. z has gained great success in selecting categorical variables, we leverage it to estimate the gradient produced by our discretization, as below: where g where U is a uniform distribution and τ ∈ (0, ∞) is a temperature parameter to adjust the approximation [44]. strategies to learn the approximate hash codes, where u stage, we adopt argmax(·) to choose the maximum classiﬁed score’s index as hash codes, that is: = [z, z, . . . , z]. Inspired by Gumbel-Softmax [17, 42, 43] that u=softmax(), softmax(), . . . , softmax()(5) softmax() =P(6) ∈ Ris sampled from a Gumbel Distribution, g= log(−log(U(0, 1)) Consequently, we can rewrite the discrete hash function (4) as: Since Equation (7) is diﬀerentiable, we can directly use gradient descent 4.3. Cross-domain Intersected Discriminators section, we describe how we preserve the semantics into binary hash codes to improve their quality. as D get domain respectively. Note that each discriminator consists of two MLP layers, followed by a classiﬁer that produces a label prediction for each embedding. Due to the fact that the source domain has rich-label information but the target domain is unlabeled, a key challenge is how to transfer the learned patterns under source-domain supervision to the target discriminator. To this end, many previous work [7, 12, 23] used a pseudo-label strategy to generate proxy ground-truth annotations for the unlabeled domain in a self-supervised manner. However, they simply built two independent classiﬁers for each domain without too much mutual interaction except for letting the source classiﬁer predict pseudo labels. We hypothesize that sharing information (knowledge) learned by both classiﬁers, instead of focusing on its own domain, will be more beneﬁcial to the knowledge transfer between the two classiﬁers. Hence, we develop a cross-domain intersected discriminator component, that is, the source domain classiﬁer classiﬁes not only source domain data but also target domain data. The last section solves the discretization problem in hash learning. In this As depicted in Figure 1, our model consists of two discriminators, denoted and D, aiming to classify labels for the source domain and the tar- First, the source discriminator classiﬁes the source data by a cross-entropy loss under supervision: where W probability from D ground-truth label. to generate pseudo labels. To obtain highly precise pseudo labels, we set a threshold T to select the target domain label, as shown below: where y point i’s prediction probability from D labels as the proxy ground-truth and train the target-domain discriminator in a supervised way: where W dicted label by D For the unlabeled target domain, we ﬁrst use the source discriminator denotes the pseudo label of the target domain point i, and ˆyis To enable the supervised signals to transfer to the target-domain discriminator, we further adopt the widely-used knowledge distillation strategy [45]. Speciﬁcally, we view the source domain as a teacher discriminator and the target discriminator as the student, aiming to mimic the prediction of the teacher and thus achieve the goal of reducing domain discrepancy. The knowledge distillation strategy for the student is formulated as follows: where KL(·) denotes the Kullback-Leibler divergence to measure the discrepancy between the two domains. 4.4. Semantic Centers Alignment of the target domain. If both domains are well aligned, the knowledge learned on the source domain, especially the discriminative information learned by the supervised hash component, can readily transfer to the target domain and boost its classiﬁcation performance. Although many approaches [46, 47] have proposed to use a generative adversarial network (GAN) to handle this problem, they do not make sure that the semantic subspaces in the latent space is regionally aligned. the two domains to semantically align in terms of each label’s cluster center, that is, the same class centers of the two domains should be close to each other. It is true that if centers of both domains are highly aligned, the Distributional alignment plays a vitally important role in the classiﬁcation Therefore, we propose a semantic centers alignment component to force discriminative clues learned by supervised signals of the source domain can be readily transferred to the target-domain embeddings. The critical challenge is how to measure the alignment of the two domains, since the target domain is unlabeled. To address this problem, we exploit a semantic centers alignment loss based on clustering algorithms, i.e. the K-means algorithm, to calculate each category’s center and force the same class centroid to be close. The semantic centers alignment loss can be formulated as: where K is the number of classes and m samples in the same cluster in the source (resp. target) domain. ϕ(·, ·) is the function that measures the distance of diﬀerent centers. y label generated by Equation (10). In this work, we leverage the Euclidean distance to deﬁne the distance between centers, i.e., ϕ(z Additionally, due to the high time cost when calculating K-means on all samples, we do not precisely calculate the centers of all nodes in the training stage, but instead calculate the centers of mini-batches to approximate the global centers. It is worth noting that we usually set a large batch size to make sure the K-means clustering covers all categories during training. 5. Theoretical Analysis two domains’ distribution disparity, including cross-domain discriminators and semantic centers alignment. It is important to discern why they are eﬀective to reduce that discrepancy for domain-adaptive hashing. To answer this question, we theoretically analyze the latent correlation between their distribution alignment and objective loss functions. In this work, there are two types information to be embedded: neighborhood structure proximity and node label semantics. In the remainder of this section we take the latter as an example to elucidate the eﬀectiveness. required parameters as ϑ, we could formulate a hash code of a source node xas: F(x code is denoted as F(x expressed as: where v a hamming distance between two hash codes. It is worth noting that v does not exist in our dataset, but can be obtained by our supervised hash component 4.2. In the previous section we have described a suite of techniques to reduce Let our learned domain-adaptive hash function be denoted as F and all denotes the group-truth hash vector of xand H(·, ·) is to calculate Similarly, we could express the loss function on the target domain Gas below: where v Proof. same hash codes, i.e., v domain discriminators 4.3. Hence, we could reorder Eq. (17) so that v for each pair (x has the same number of points. If |G by a resampling strategy [48] to force |G for each category. denotes the learned hash codes on target domain. Then we will have the following hypothesis: If xand xhave a same node label, we view both of them as having the where v Hence, we could obtain the following inequality: distance of the two domain’s hash codes, i.e., reducing the disparity of the two domains, L 6. Learning tions: network structure embedding loss L tion (7)), cross-domain classiﬁcation loss L Then, we could rewrite Eq. (17) in an alignment way as: L− L=H(v, F(x; ϑ)) −H(v, F(x denotes their aligned hash codes. Since the Hamming distance H satisﬁes the triangular inequality: L− L=H(v, F(x; ϑ)) −H(v, F(x; ϑ)) From Eq.20, we could know that when we optimize Land the Hamming In summary, our model consists of the following ﬁve main objective funcsemantic centers alignment loss L function is: where the superscript target domain (t) respectively for simplicity, and α, β, σ and δ are four hyperparameters to balance the structure, hashing, cross-domain classiﬁcation, and alignment loss terms, respectively. Since all parameters W are continuous, we can directly use stochastic gradient descent (SGD) to optimize all the parameters. lation is based on mini-batches, it makes sense that the larger the mini-batch size is, the more accurate cluster centroids can be obtained. Moreover, during the training stage, we need to ensure the batch size is much larger than the number of classes (K). At the same time, the semantic centers C are also learned by the following update strategy: where the superscript batch, C denotes the centers calculated by Equation (22),  (set as 0.3 in our experiments) is the update step size, and Φ(·, ·) is the clustering function K-means to calculate each class’ center in the r-th mini batch. For the centers alignment component, since both domains’ center calcu- 7. Experiments some state-of-the-art models on networks. We begin by describing the benchmark datasets, baseline models and implementation details. 7.1. Datasets works obtained from ArnetMiner [49]. Brief statistics of the three networks are shown in Table 1. We sample subsets from three large citation networks: DBLPv4 (D), ACMv8 (A) and Citationv1 (C). To reduce the overlap between diﬀerent datasets, we extract published papers from diﬀerent periods for these three datasets following UDA [7]. Papers are classiﬁed into eight categories: Engineering, Electronic, Software Engineering, Mathematics, Theory, Applied, Artiﬁcial Intelligence , and Computer Science. For the attributes, we extract the word frequency of each paper’s abstract, which is represented as an 8, 328-dimensional vector. In this section we will evaluate our domain-adaptive hashing model against Following UDA [7], we conduct our experiments on three citation net- 7.2. Baselines work embeddings as our baselines. We choose the following state-of-the-arts discrete hash methods for net- • SH [50] is a classical and widely-applied learning to hash method for the approximate nearest neighbour search task. • Discrete Collaborative Filtering (DCF) [51] is a principled hashing method able to tackle the challenging discrete optimization problem in hash learning and avoid large quantisation errors caused by two-step optimization. • DNE [3] is the ﬁrst work to a discrete representation for networks by preserving Hamming similarity. • NetHash [4] utilises the randomized hashing technique to embed trees in a graph, which can preserve information closer to the root node as much as possible. • Binarized Attributed Network Embedding (BANE) [5] develops a Weisfeiler- Lehman proximity matrix that can preserve the dependence between node attributes and connections via combining the features from neighbouring nodes. • Information Network Hashing (INH) [52] is an embedding compression method based on matrix factorization and able to preserve high-order proximity into binary codes. 7.3. Implementation Details layer outputs 256-dimensional embeddings. The batch size is set to 400, which is much greater than the number of labels, so it is safe to guarantee eﬀectiveness of the K-means algorithm. For a fair comparison with baseline methods, all methods’ hash code length is set to 128. The temperature τ in Equation (6) is set to 1. The learning rate, and the ﬁve hyperparameters α, β, δ, δ and λ are set to 0.005, 1.0, 0.01, 1.0, 0.1 and 5 respectively, and are obtained by grid search on the validation set. The threshold T for pseudo label selection is set to 0.85. nodes’ embeddings and then train a one-vs-rest logistic regression classiﬁer to classify the embeddings, where all methods use the same-dimensional hash codes for training and testing. We measure the mean score of Micro F1 and Macro F1 metrics to evaluate the performance of node classiﬁcation, following DANE [46], and use the area under curve (AUC) score to evaluate the performance of link predication, following Graph2Gauss [37]. For link prediction, we randomly select 5% and 10% edges as the validation and test set respectively, following Graph2Gauss [37]. • Discrete Embedding for Latent Networks (DELN) [16] is an end-to-end discrete network embedding method to learn binary representations. The feature encoder network consists of three MLP modules and the last For the evaluation on the node classiﬁcation task, we ﬁrst generate all All experiments were performed on a workstation with 256 GB memory, 32 Intel(R) Xeon(R) CPUs (E5-2620 v4 @ 2.10GHz) and 8 GeForce GTX 1080Ti GPUs. 7.4. Cross-domain Node Classiﬁcation Results mance of preserving semantics. In this work, we also adopt it to test the discrete hash codes’ capability to learn semantics. compared with the state-of-the-art discrete network embedding methods. For a fair comparison, due to the fact that SH cannot learn representations for each node, we use the feature learned from UDAH to train SH, i.e., the latent embeddings z, while the other models use nodes’ attributes to train. performance over all but one domain-transfer tasks, except for C→A with DELN being 0.33 percentage points higher than ours. On average, UDAH is superior to the baseline methods, surpassing the second best method DELN by 2.42 percentage points. well on single domains, but perform poorly on new domains with relatively large distribution disparity. For example, the majority of discrete embedding methods, including DNE, INH and DELN, leverage a matrix factorization technique to learn the binary codes. They decompose the input attribute matrix into hash codes with the constraint to reconstruct the original at- Node classiﬁcation is a standard task to evaluate the embedding perfor- Table 2 shows the node classiﬁcation results (of discrete embeddings) From Table 2, it can be observed that our method UDAH achieves the best We consider that the main reason is the compared methods can only work tribute matrix. Their common issue lies in the fact that the target network has a large diﬀerent attributes distribution from the source network, which results in large reconstruction errors by the matrix decomposition operation, and ﬁnally leading to poor results on the domain transfer. In contrast, UDAH explores several techniques to mitigate the problem of attribute distribution shift, including the KL divergence loss and semantic centers alignment. In the ablation study described later, we will further test the performance of each component to analyze how much they contribute to domain-adaptive hash learning. It is worth noting that SH gains comparable results with DCF and does not show catastrophic performance degradation as we originally anticipated, possibly because SH is trained by our learned continuous representations, which, to some extent, conﬁrms that our network embedding strategy can learn high-quality embeddings for multiple domains. Therefore, we can conclude that our UDAH handle domain-adaptive hash learning more eﬀectively than the conventional single-domain discrete network embedding methods in terms of semantics preservation. 7.5. Link Prediction Results the original network’s neighbor structure. Following Graph2Gauss [37], the validation/test set consists of 5%/10% of edges randomly sampled from the network respectively, and we randomly selected edges and an equal number of non-edges from the test set. Table 3 shows the link prediction results of discrete embeddings on the six cross-domain tasks. single-domain discrete embedding methods over all domain transfer tasks. In particular, UDAH is 2.86 points better than the second best method INH. Since INH places emphasis to preserving high-order proximity, it shows superiority to other matrix factorization based models such as DNE. It is worth Link prediction evaluates the learned hash codes’ ability to reconstruct From the results we can observe that our UDAH method exceeds other noting that DELN achieves poor performance in this task, although it obtains competitive results in node classiﬁcation. tary networks, they consistently show performance decreases when evaluated in a domain-adaptive setting, in which our method achieves a substantial performance advantage due to techniques speciﬁcally designed to tackle this problem. 7.6. Node Recommendation formance for social and commercial networks, for which discrete embeddings can save much time. Given a query hash code, node recommendation aims to returning a list of nodes, ranked by their structural similarity. Following the settings in INH [52], we sample 90% of neighbours of each node to train the model while the remaining 10% neighbours are reserved for evaluation, and use NDCG@50 as the evaluation metric. domain tasks. From the table, we could observe that our UDAH method outperforms all the baseline methods in terms of the average performance, outperforming the second best method INH by approx. 2 points. INH in turn outperforms he third best DELN by 1.26 points. Although both of them are based on matrix factorization, INH’s advantage comes from its capability to learn high-order proximity. Although UDAH only explores the ﬁrst-order In summary, although single-domain embedding methods can handle uni- Node recommendation is a widely employed task to evaluate retrieval per- Table 4 presents the performance of node recommendation on six crossneighborhood structure preservation, our other techniques aiming to reduce the domain discrepancy play an important role in network structure and semantics preservation. The other methods, such as DCF, NetHash and DNRE, perform much more poorly in this task. 7.7. Ablation Study our model. Speciﬁcally, for the ﬁve components: groupwise contrastive loss, Guambel-softmax strategy, cross-domain classiﬁcation loss, KL divergence loss, and semantic center alignment loss, we ablate them into the following ﬁve model variants: In this section, we aims to study the eﬀectiveness of each component in • −Luses a point-wise contrastive loss Equation (2) instead of our group-wise constraint Equatin (3). • −Luses the conventional hash learning strategy in [19] instead of the Gumbel-Sofmax strategy in Equations (5) and (6). sion, dubbed NoDAH, which has L and link prediction, as shown in Tables 5 and 6, respectively. It is worth noting that all variants are under the same experimental conﬁgurations expect for their corresponding ablated module(s). From the two tables, we could make the following observations: move of all domain-adaptive learning strategies, such as Kullback–Leibler divergence loss, cross-domain discriminator, and semantic centers alignment. Hence, it is reasonable that NoDAH achieves comparable results with other single-domain discrete embedding methods such as DCF. be seen in the variant of −L loss is more eﬀective than the pairwise version. Besides, we could observe that the groupwise contrastive loss plays a more important role in link prediction than in node classiﬁcation, possibly because L structure instead of semantics. • −Lremoves the cross domain classiﬁcation loss, i.e. Equation (9) and (11). • −Ldiscards the KL divergence loss, i.e. Equation (12). • −Lremoves the semantic centers alignment loss, i.e. Equation (13) In addition, we further modify UDAH into a none-domain-adaptive ver- We thoroughly conduct a wide range of experiments on node classiﬁcation (1) NoDAH performs the worst among all the variants, due to the re- (2) From the comparison between −Land UDAN, a slight decrease can Softmax reparameterization trick brings about 2 and 1 points lift on node classiﬁcation and link prediction, respectively. We conjecture the reason is that Gumbel-Softmax completely discards the discretization function sign(·) during training and test by multiple linear classiﬁers to produce hash codes, which bypasses relaxation errors generated by sign(·) or tanh(·) functions. supervisedly leaned on the source domain to the target domain, i.e. crossdomain discriminators Eq. (9) and (11), knowledge distillation loss Eq. (12), and semantic centers alignment Eq. (13), all contribute positively to the ﬁnal performance. Speciﬁcally, among the three strategies, the cross-domain discriminators and center alignment are the more important and have larger impact on performance, while knowledge distillation seems to be not as critical as them. contributions to domain-adaptive hash learning. Moreover, the cross-domain discriminators and center alignment modules play the most important role in UDAH. duced by UDAH by visualizing the learned embeddings under diﬀerent settings. (3) Comparing −Lwith UDAH, we could notice that the Gumbel- (4) Our three proposed techniques that aim at transferring the knowledge In summary, all proposed components can make positive task-speciﬁc In the next section, we will qualitatively evaluate the embeddings pro- 7.8. Embedding Space Visualization learned embeddings. Thus, we use t-SNE to reduce the embeddings to 2dimensional vectors and randomly sample some embeddings. Figure 2 shows the visualization results of NoDAH and UDAH on a graph with 1, 000 nodes randomly selected from ACMv8, where the three models are trained on DBLPv4 and colors represent node classes. UDAH, respectively. Generally, we can make the observation that nodes of the same labels tend to cluster together in Figure 2c where nodes of diﬀerent labels are separated by a large margins. However, the points in Figure 2a and 2b are much more tightly tangled, which conﬁrms that single-domain network embeddings do not perform well in domain-adaptive settings. On the other hand, our model UDAH can eﬀectively learn discriminative embeddings in domain-adaptive transfer learning. 8. Conclusion lem for networks, which is an under-explored but important problem that facilitates network analysis tasks in a time- and space-eﬃcient manner. We develop UDAH, an end-to-end unsupervised domain-adaptive hash method to learn adaptive discrete embeddings. Speciﬁcally, we propose a suite of techniques: groupwise contrastive loss to preserve network structure, cross- Feature visualization is a common way to evaluate the quality of the Figure 2a, 2b and 2c visualizes the embeddings of NoDAH, DELN and In this paper, we address the domain-adaptive learning to hash probdomain discriminators and knowledge distillation modules to transfer knowledge of the source domain to the target domain, and center alignment to reduce the distribution disparity of both domains. Evaluation on three benchmark datasets against a number of state-of-the-art learning to hash methods demonstrate the superiority of UDAH on three tasks: node classiﬁcation, link prediction and node recommendation. In future, we plan to extend our framework to support more diverse adaption scenarios, such as the transfer from social networks to citation networks.