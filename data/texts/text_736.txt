Recently, sequential recommendation systems are important in solving the information overload in many online services. Current methods in sequential recommendation focus on learning a Ô¨Åxed number of representations for each user at any time, with a single representation or multi-interest representations for the user. However, when a user is exploring items on an e-commerce recommendation system, the number of this user‚Äôs interests may change overtime (e.g. increase/reduce one interest), affected by the user‚Äôs evolving self needs. Moreover, different users may have various number of interests. In this paper, we argue that it is meaningful to explore a personalized dynamic number of user interests, and learn a dynamic group of user interest representations accordingly. We propose a Reinforced sequential model with dynamic number of interest representations for recommendation systems (RDRSR). SpeciÔ¨Åcally, RDRSR is composed of a dynamic interest discriminator (DID) module and a dynamic interest allocator (DIA) module. The DID module explores the number of a user‚Äôs interests by learning the overall sequential characteristics with bi-directional self-attention and Gumble-Softmax. The DIA module allocates the historical clicked items into a group of sub-sequences and constructs user‚Äôs dynamic interest representations. We formalize the allocation problem in the form of Markov Decision Process(MDP), and sample an action from policyùúãfor each item to determine which sub-sequence it belongs to. Additionally, experiments on the real-world datasets demonstrates our model‚Äôs effectiveness. ACM Reference Format: Weiqi Shao, Xu Chen, Jiashu Zhao,Long Xia, Dawei Yin. 2022. Gumble Softmax For User Behavior Modeling. In Proceedings of ACM Conference (Conference‚Äô17). ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn With the development of Internet technologies, recommender systems have been widely applied to many online services such as Figure 1: From the click sequences of user A and user B, there are multi conceptually distinct items in a user‚Äôs click behaviors which indicates the change of dynamic number of user‚Äôs interest and different interest number between users by the time. e-commerce, advertising, social media, and etc. Recommender systems serve to alleviate the information overload problem and enhance user experiences. Traditional recommender systems mostly focus on promoting generalized user interests, such as collaborative Ô¨Åltering [22,23]. In recent years, more and more researchers study the sequential recommendation problem to capture the dynamic user behaviors, which assumes that a user‚Äôs information need changes over the time [19]. The existing sequential recommendation solutions represent a user as a Ô¨Åxed number of representations, including a single representation or multiple representations. For the single representation recommendation, only one user embedding representation is generated for the next-item prediction. Early solutions usually adapted the Markov Chain [19] which assumes that the next-item prediction is closely related to the previous item [20]. With the breakthrough of deep learning in many areas (e.g. computer vision and natural language processing) [36], sequential neural networks such as recurrent neural network [9,17]and Transformer [29]have been adopted to the sequential recommendation tasks. These sequential neural networks can characterize the sequential item interactions and learn informative representations for user behaviors [14]. Additional context information can also be considered to enhance the performance of neural sequential recommendation [10,37]. For multi-representation recommendation approaches, a user is assumed to have multiple interests and these interests jointly affect the user‚Äôs next item selection. From the empirical analysis, a user usually interacts with several types of items that are conceptually different over time. For example, zhang [37] identiÔ¨Åes that the items in a user‚Äôs recent behaviors belong to different categories on Taobao dataset. Various approaches have been adopted to model the multiple interests from the user‚Äôs historical behaviors, including Capsule routing network [21] and multi-head self-attention [33]. The temporal information in the sequence can also be considered to enhance the recommendation performance [4]. All the multi-interest modeling approach rely on a pre-given Ô¨Åxed number to generate the corresponding number of representations, which assumes that the numbers of interests for all users are the same and do not change over the time. However, the Ô¨Åxed-number of interest assumption is not necessarily true in the real applications. For example, one user may have very broad interests, and another user have more focused intents. Figure 2 shows two users each with a sequence of interacted (i.e. clicked) items, user A overall has three interests (furniture, electronic products, and sport products), while user B has two interests only ((furniture and electronic products). On the other hand, throughout the user behaviours over the time, a user may have more/less interests. In Figure 2, user A is only interested in furniture at the beginning, then A gradually start to show interest in electronic products and sport products. So we can see that user the number of A‚Äôs interest changes from one to three. Therefore, modeling a Ô¨Åxed number of interests can not fully simulate the real user intents. If a user has more interests than the given Ô¨Åxed number, then the user‚Äôs intent can not be accurately represented. On the other hand, if a user has less interests than the given Ô¨Åxed number, then the user‚Äôs intent will be represented with noise. Therefore, it is important to consider user‚Äôs dynamic interest number in recommendation. In this paper, we propose a promising alternative method to learn a dynamic group of embedding representations for a user‚Äôs behavior sequence, where each embedding representation encodes one aspect of the user‚Äôs intends. Inspired by the above observations, we introduce Learning Reinforced Dynamic Representations for Sequential Recommendation(RDRSR) to learn the a dynamic of group representations. Specifically, we design Dynamic Interest Discriminator(DID) to detect the dynamic number of a user‚Äôs interests using self-attention [29] and Gumble-Softmax [11]. With self-attention, the items with high attention weights are clustered together to form different interests. And then with the informative item representations, Gumble-Softmax determines the interest number with the Gumble distribution as the noise to improve the exploration of the user interest number. Furthermore, we design the Dynamic Interest Allocator(DIA) to allocate the user‚Äôs click sequence into a dynamic group of interest sub-sequences, where DIA formalizes the allocation process in the form of Markov Decision Process(MDP) and sample the action for each item to determine which sub-sequence it belongs to. Here each sub-sequence forms a user‚Äôs interest representation with average-pooling method. As for the next-item prediction, we input the candidate item into the policyùúãto decide which sub-sequence it belongs to and use the corresponding user interest representation to calculate the compatibility between the sub-sequence and candidate item for prediction. To summarize, the main contributions of this paper are: ‚Ä¢To the best of our knowledge, we are the Ô¨Årst to consider a dynamic number of interest in sequential recommendation. The explore of user interest number improves the performance in the sequential recommendation. ‚Ä¢We propose the RDRSR model. The RDRSR model includes DID to learn the user‚Äôs dynamic interest number over the time and leverages the DIA to allocate the click into different sub-sequence to form multi interests through average-pooling method for the next item prediction. ‚Ä¢We conducted experiments on several real datasets with several public benchmarks to verify the effectiveness of the model. We analyze the DID module and DIA module to valid the proposed RDRSR model through ablation study. Before introducing the details of the proposed model, in this section, we introduce the related literature about recommendation systems, including general model, sequential model, multi-interest recommendation systems and attention mechanism we used in the paper. The main methods in traditional recommendation system is extracting users‚Äô general tastes from their historical behaviors to make recommendation. Typical methods include Collaborative Filtering [22, 38], Matrix Factorization [15] and Factorization Machines. Collaborative Filtering method is based on the similarity of users [38] or the similarity of items [22] for recommendation. But it is a non-trivial work to quickly and accurately Ô¨Ånd the similar users or items. Matrix Factorization(MF) [15] as one the most popular technique in recommendation system, map users and items into joint latent space and estimate user-item scores through the inner product between their embedding vectors. Factorization Machines(FM) [19] methods consider all the variable interaction information which not only improve the recommendation results but also achieve good results even when the data is sparse. With the success of deep learning in computer vision and natural language processing [36], more and more efforts has been done to apply deep learning to the recommendation system [34]. He [7,7,8] makes a great success, NCF [7]uses multi-layer perceptions to replace the inner product operation in MF for interaction estimation. [7,8]use deep learning to obtain higher-oeder interactive expressions of interaction with a fast calculation trick. These deep learning based methods achieve good performance. Moreover, several attempts also tried to apply graph neural networks [6, 13, 26]. In relevant literature,many sequential recommendation models have been proposed to leverage user historical records in a sequential manner to capture the user‚Äôs preference for the next item. By integrating the good performance of matrix factorization and the sequential pattern of Markov chains, factorized personalized Markov chains (FPMC) [20] embeds the sequential information between adjacent clicked items into the Ô¨Ånal prediction for recommendation, and later the hierarchical representation model (HRM) [30] simultaneously consider the sequence behaviors and user preferences. Though they make progress in sequential recommendation, these methods only model the local sequential patterns between every two adjacent clicked item [35]. To model longer sequential behaviors, [9] Ô¨Årst adopted recurrent neural network to model the long sequence pattern for recommendation, RNN care too much about the sequence pattern which could be disturbed by the noise in the click sequence while neglect the user‚Äôs main intent, [17,18] not only consider the sequence pattern in the sequence and also explore the user‚Äôs main purpose through the attention mechanism. Later, [14] consider the importance of each item and other items in the click sequence achieve great progress in many real datasets [24] with unsupervised learning to learn the hidden relationships between items and make a difference. The main difference between multi-interest recommendation and single embedding recommendation is that multi interest recommendation uses multi vectors to represent the user while only one vector in other methods. The classic method [3,16] use a capsule routing based method to extract the user‚Äôs multi interest. [33]explore user‚Äôs with multi-head self-attentive, where the multi-head number as the multi-interest number through sum-pooling method. [4]consider the time interval to extract the multi interest and [27] infer a sparse set of concepts for each user from the large concept as its multi interest. Those methods have achieved good performance in recommendation, but non of them consider the different interest number between different users at different time and the dynamic user interest number over time. The originality of attention mechanism is in computer vision [2,25] to make the target object get more weight, but its great success in various Ô¨Åelds in artiÔ¨Åcial intelligence comes only in recent years with the development of deep learning. It Ô¨Årst come to the center of the stage is in machine translation [1,29] and is rather useful and efÔ¨Åcient in real-world application tasks. It is also been successfully applied in recommendation applications [32] which learns the importance of each feature interaction from data via a neural attention network. What‚Äôs more, [14,24] use the different relationships between items in the clicked sequence to capture both the long-term semantics and short-term semantics make a difference. In this section, before going into the details of our proposed model. We Ô¨Årst describe the problem statement in our work. And then we will give an overview of the proposed Learning Reinforced Dynamic Representations for Sequential Recommendation(RDRSR) framework(as shown Figure 2), which consists two main modules DID and DIA for dynamic interest number detector and user behavior allocation. The key claim of sequential recommendation is that the current user preference should be related with the historical behaviors. Formally, suppose we have a user setU={ùë¢,ùë¢, ...,ùë¢}, an item set I={ùëñ,ùëñ, ...,ùëñ}, andùëõandùëöare the numbers of users and items in the sequential recommendation task. Unlike general recommendation, which only captures the correlation between a user and an item without considering the order of the click sequence. We use C={ùë•,ùë•, ...,ùë•,ùë•}to denote a sequence of items in chronological order that a user has interacted, and theùë•‚àà I. The goal of sequential recommendation is to predict the next itemùë•depending on the precious click sequence {ùë•,ùë•, ...,ùë•}. We create an item embedding matrixùê∏‚àà Rand an user embedding matrixùê∏‚àà R, where d is the latent dimension andùëõandùëöare the number of user and item. We retrieve the click item in the click sequenceC={ùë•,ùë•, ...,ùë•}with a latent vector in the item embedding embedding and get the item sequence embedding ùê∏={ùëí,ùëí, ...,ùëí}and the corresponding user embeddingùëí, where t is the click sequence length and we process the datasets like [14] Furthermore, we incorporate a learnable position encoding matrix ùëÉ ‚àà ùëÖto enhance the input representations. In this way, the input representationsùê∏ ‚àà ùëÖfor the generator can be obtained by summing two embedding matrices: ùê∏ = ùê∏+ùëÉ. As mentioned before, the user‚Äôs dynamic interest number is evolving and changing by the time, a new click item would indicate user get one more interest or reduce a interest due to he may get what he want. DID aims to Ô¨Ånd the user dynamic interest number with the user‚Äôs current click sequence. First, we stack multiple bi-directional architecture self-attention [29] block based on the embedding layer. With the bi-directional architecture self-attention block, interest relevant items in the click sequence are clustering more close and get a more informative item representation. Self-AttentionFrom the formula, the attention layer calculates a weighted sum of all values, where the weight between query and value, which could cluster those items belong to the same interest and effectively Ô¨Ånd the dynamic interest number. And the scale factor‚àöùëëis to avoid overly large values of the inner product when the dimension is very high. We take E as input, convert it to three matrices through linear projections, and feed them into an attention layer: where the projections matricesùëäùëäùëä‚àà R. The projections make the model more Ô¨Çexible. Feed Forward NetworkThough the attention calculation is able to aggregate previous items‚Äô embeddings with corresponding weights, it is still a linear model. In order to enforce the model with non-linearity and to get more high-order interaction information, we apply a two-layer feed-forward network to all S. whereùëä,ùëäare d x d matrices andùëè,ùëèare d-dimensional vectors. In order to get the user‚Äôs dynamic interest number, we set an attention mechanism with the F andùëíto get the united user general purpose representation. where f is a k-dimension vector represent the probability of each possible interest number,ùëäis a dxk matrix and k is the max dynamic interest number set in our model. Figure 2: An overview of our model RDRSR. The input of our model is a user behavior sequence and those items are fed into the embedding layer and transformed into the item embeddings. Dynamic Interest Discriminator concentrates on exploring the dynamic interest number with a bi-directional architecture self-attention and Gumble-Softmax. Dynamic Interest Allocator activates the corresponding allocation policy according the learning interest number in DID. Then DIA allocates the click item into the subsequence with activeated policy and form different interests through average-pooling method. Last, we put the target item into the policy ùúã to decide which sub-sequence it belongs to and use the corresponding user interest representation to calculate the rewards between the interest and target item which will be used for prediction. Gumble Softmax SamplingWe employ the Gumble Softmax [11] sampling method to produce the user dynamic interest number. DID(Dynamic Interest Discriminator) draws z from a categorical distribution with class probabilities ùëì = {ùëì,ùëì, ...,ùëì}. where‚Ñéis current generated dynamic interest number and{ùëî,ùëî, ...,ùëî} are sample drawn fromùê∫ùë¢ùëöùëèùëôùëí(0,1) distributions. In practice, we sample theùê∫ùë¢ùëöùëèùëôùëí(0,1) distribution using inverse transform sampling by drawing u from a uniform distribution. What‚Äôs more, those added new Gumble distribution as the noise changes the probability distributions and give other original non-max alternative interest number chance to be chosen, which improve the exploration of the user interest number and make our model more solid. where u is sampling from Uniform(0,1). The argmax operation in Eq. (6) is non-differentiable, but we can resort to the Gumbel Softmax distribution, which adopts softmax as a continuous relaxation to argmax in order to alleviate the nondifferentiable problem used in Eq. (11)(12). ùëß=ùëíùë•ùëù ((log ùëì+ùëî)/T)√çùëì ùëúùëü ùëñ = 1, 2, . . . , ùëò(8) whereTis a temperature parameter to control the discreteness of the output vectorùëß, which is set 10 in our model. Now, we get the probabilityùëßfor each dynamic interest number. During the forward pass, we sample the dynamic interest number‚Ñéusing Eq. (6) for the click item sequence. As for the backward pass, we are able to estimating the gradients of the discrete samples by computing the gradients of the continuous softmax relaxation ùëß in Eq. (8). After the user‚Äôs dynamic interest number‚Ñégenerated in Eq. (6) and more informative representationùêπin Eq. (3) of item vector found in DID, DIA split the click sequence into different sub-sequences, where each sub-sequence represents a user‚Äôs interest and we use average-pooling method to get the user‚Äôs interest representation of those sub-sequence. DIA formalizes the allocation click sequence problem in the form of Markov Decision Process(MDP), and sample action form policyùúãfor each item to determine which sub-sequence it belongs to. Item representationùêπwith the bi-directional architecture self-attentive, our policyùúãcan foresee future sequential information when making a decision, which could offer insightful clues to determine item-level relevance without direct supervision signals. We consider an episodic RL approach to allocate the click sequence C={ùë•,ùë•, ...,ùë•}into h sub-sequenceùëÜ={ùë†ùë¢ùëè,ùë†ùë¢ùëè, ...,ùë†ùë¢ùëè}and each sub-sequence represent a user interest represenation. Episode RLWe see the allocation sequence split as an episode RL approach. At each time T, the process is in some stateùë†‚àà ùëÜ. According to the stateùë†, the agent performs an actionùëémodeled by a policyùúã (ùëé|ùë†). The action space isùëé ‚àà {ùëé,ùëé, ...,ùëé}, where ùëéis that at time T, theùëñùë°ùëíùëöis belongs toùë†ùë¢ùëè ‚àíùë†ùëíùëûùë¢ùëíùëõùëêùëí. The following is the policy ùúã. ùúã (ùëé|ùë†) = ùëÜùëú ùëì ùë°ùëöùëéùë• (ùëÖùëíùêøùëà (ùë†ùëä+ùëè))ùëä+ùëè(9) whereùúã (ùëé|ùë†)is the discrete probability distribution thatùëñùë°ùëíùëö belongs to sub-sequenceùëéandùëäis a d xùëëmatrix andùëäis a ùëëx h matrix. State TransitionWe give each sub-sequence an initial multi interest representation at time 0ùëÉ={ùëù,ùëù, ...,ùëù}where eachùëù is a d-dimension vector and initialize with the corresponding user embeddingùëí. At time T, we put theùë†into the policyùúãto get the actionùëé(the sub-sequenceùëéùëñùë°ùëíùëöbelongs to). We then use our well-designed pooling method to update the corresponding interest representation embedding with the new addedùëñùë°ùëíùëö, where the representation of ùëñùë°ùëíùëöis ùêπ. In reality, there are complex relationships between the user‚Äôs click sequence, like point level,union level with or without skip[28]. For accurately capturing those relationships, we use a well-designed attention mechanism to deÔ¨Åne the state transition, which explore the relationships between the new click item and the generated subsequence with a weighted sumùëù,ùêπin Eq. (3) and dynamic interest number distribution probability informationùëßin Eq. (8) through a Neural Networks to get the ùë† where ùëäis a 2ùëëùë•ùëë matrix and (¬∑) represent the inner product. Even we use a hard allocate, but some information from other other sub-sequence is transitioning into the dynamic interest representation when we deÔ¨Åne the state transition, which makes our model more solid. Reward SettingAfter the allocation process, we get the multi interests representation at time tùëÉ={ùëù,ùëù, ...,ùëù}. With the generated dynamic multi interests, here comes to the question that which interest representation is related to the target item. To conÔ¨Årm the target interest,we use the target itemùëùto get the current state ùë†through formulasùê∏ùëû.(11)(12)and put it into the policy net ùúã (ùëé|ùë†)set in Eq. (9) to sample action for getting the subsequenceùëùthe target item belongs to. Here we leverage a Sampled Softmax technique [5,12] to calculate reward where the relationshipùëùwith the target item and other candidate item will be considered. o is the sample item number in the dataset. ThroughùëÖconsider other items when calculate the reward, it doesn‚Äôt use other generated multi interests which means that only when our target interest selection is correct, the reward is the optimal result. In order to promote the policyùúãto choose the right action, we employ a baseline in the reward function which use the average scores of the all generated multi interests, deÔ¨Åned as: with the baseline reward setting, and the advantage of selected dynamic interest representation reward setting is as below: In order to enforce the learned dynamic multi interests representation orthogonally. SpeciÔ¨Åc, we denote theùëÖas the mean of the absolute value of the inner product between all different generated dynamic interest representations ùëùin ùëÉ. where| ¬∑ |represents the absolute value of inner product between ùëùandùëùinùëÉ. Combine the two reward above, the Ô¨Ånal reward function of our model is: whereùúÜis the trade-off parameter to balance the two rewards, which is set 0.001 in our experiments. We treat the allocation task as a RL problem and apply the classic policy gradient to learn the model parameters. SpeciÔ¨Åcally, the corresponding probability of generatingùëùsub-sequence isP(ùë†ùë¢ùëè) which is calculated as follows: P(ùë†ùë¢ùëè) =ùúã (ùëé|ùë†, ùúÉ ) ‚àó ùëÉ (ùë†|ùë†, ùëé, ùúÉ ) =ùúã (ùëé|ùë†, ùúÉ ) Theùëùis then used for the dynamic interest selection for target itemùúã (ùëé|ùë†)in Reward Setting. Thus, the probability of the generated sample action sequence is as followed: Formally, the objective of the policy network is to maximize the expected reward at the Ô¨Ånal prediction. whereùëÖis deÔ¨Åned in Eq.(17) and its gradient will be detached in the training process, C is the all the generated action sequence of target sub-sequence andùúÉis the parameters of the model including the parameters of DIA and DID. The gradient of the objective function ‚àáJ(ùúÉ) regard to the model parameters ùúÉ can derived as: ‚àáJ(ùúÉ) = ‚àáùëÖ‚àó P(ùë†) Therefore, the optimization of the policy network is calculate with a log trick as follow: Here we use the standard cross-entropy and a Sampled Softmax technique [5, 12] to calculate the classiÔ¨Åcation loss: o is the same as the negative sample number in reward calculation. Finally, we jointly train the allocation task and classiÔ¨Åcation task with a trade-off parameter ùõΩ: ùõΩcontrol the weight of theLloss, which is set 1 in our experiments. When we do the prediction, we Ô¨Årst scan the user click session and select each action with the maximal probability at policyùúãEq. (9) and corresponding state transition Eq. (11)(12) , which can be written as follows: For each candidate item, we put it into policy net to get which subsequence it belongs to and get its corresponding reward. We then rank all candidate items according to their rewards at Eq. (13) and return the top-ùëÅ rewards item as the Ô¨Ånal recommendations. In this section, we conduct experiments on sequential recommendation to evaluate the performance of our proposed method RDRSR on three benchmark datasets compare with several state-of-the-art baselines. We Ô¨Årst brieÔ¨Çy introduce the datasets and the state-of-theart methods, then we conduct experimental analysis on the proposed model and the benchmark models. SpeciÔ¨Åcally, we try to answer the following questions: ‚Ä¢How effective is the proposed method compared to other stateof-the-art baselines? Q1 ‚Ä¢What are the effects of the DIA(Dynamic Interest Allocator) and DID(Dynamic Interest Discriminator) modules through ablation studies? Q2 ‚Ä¢How sensitive are the hyper-parameter the max dynamic interest number ùëò in proposed model RDRSR? Q3 In this section, we introduce the details of the three experiment datasets, evaluation metrics, and comparing baselines in our experiments. DatasetsWe perform experiments on three publicly available dataset, including MovieLens, Lastfm and Foursquare. And the relative statistics information of the three datasets are shown in Table 1. Ml ‚àí 100kis a dataset about user‚Äôs rating score for movies. In experiments, we follow [8] to preprocess the dataset. Foursquareis a location based social networks datasets which contains check-in, tip and tag data of restaurant venues in NYC collected from Foursquare from 24 October 2011 to 20 February 2012. Lastfmrecords the music records of users from Last.fm. In experiments, we only use the click behaviors. For Foursquare and Movielens datasets, we Ô¨Ålter items and users interacted less ten times, and Ô¨Åve times in Lastfm datasets. And all datasets are taken Leave-one-out method in [14] to split the datasets into training, validation and testing sets. SpeciÔ¨Åcally, we split the historical sequence for each user into three parts: (1) the most recent action for testing, (2) the second most recent action for validation, and (3) all remaining actions for training. And if the click sequence length is less than t, we repeatedly add a ‚Äòpadding‚Äô item to the left until the length is t. Note that during testing, the input sequences contain training actions and the validation actions. BaeslinesWe compare our proposed model RDRSR with the following state-of-the-art sequential recommendation baselines, including single representation methods and multi representation methods. Single representation modelsThe most common sequential recommendation methods which generates a single embedding representation for the next-item prediction. ‚Ä¢ GRU4Rec[9] is a pioneering work which Ô¨Årst leverages GRU to model user behavior sequences for prediction. ‚Ä¢ Caser[28] is a recently proposed CNN-based method capturing sequential pattern by applying convolutional operations on the embedding matrix for the most recent items, and achieves state-ofthe-art sequential recommendation performance. ‚Ä¢ BERT4Rec[24] is a recently proposed BERT-based method which achieves state-of-the-art sequential recommendation performance. ‚Ä¢ STAMP[18] is a neural sequential model by incorporating user short-term memories and preferences. Table 2: Overall comparison between the baselines and our models. The best results are highlighted with bold fold. All the numbers in the table are percentage numbers with ‚Äô%‚Äô omitted. Multi representation modelSequential recommendation methods that generates multi representation to model user click behavior for the next-item prediction. ‚Ä¢ MCPRN[31] is a recent representative work for extracting multiple interests which designs a mixture-channel purpose routing networks with a purpose routing network to detect the purposes of each item and assign them into the corresponding channels to form multi presentations. Parameter ConÔ¨Åguration.For a fair comparison, all baseline methods are implemented in Pytorch and optimized with Adam optimizer with a mini-batch size of 2048. The learning rate is tuned in the ranges of [0.01,0.001]. We tuned the parameters of comparing methods according to values suggested in original papers and set the embedding sizeùëëas 64, and sequence length t=10. For our method, it has three crucial hyper-parameters: the trade-off parameterùúÜ,ùúÜ and the max dynamic interest numberùëò. We searchùëòfrom 3, 4, 5, and we setùúÜ,ùúÜ0.001 and 1. In order to keep the policy consistent in Dynamic Interest Allocator, we put the same user traing dataset in a batch to train the model. The conÔ¨Åguration of the other two parameters max dynamic interest number k and neg samples o for three datasets are reported in Table 3. Table 3: The optimal setting of our hyper-parameters for our model. Other parameters like dimension ùëë and learning rate ùõæ are set as 64 and 0.001, respectively. Evaluation Metrics.For each user in the test set, we treat all the items that the user has not interacted with as negative items. We use two commonly used evaluation criteria [7]: Hit Rate (HR) and Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of our model. Table 1 summarizes the performance of RDRSR and baselines including single-representation and multi-presenation methods on three benchmark datasets. Obviously, RDRSR achieves comparable performance to other the baselines on the evaluation metrics in general. In the baselines of single representation methods, we Ô¨Ånd that GRU4Rec obtains good performance over other singlerepresentation methods. What‚Äôs more, compare single representation methods with multi representation methods, it is obvious that recommendation with multiple presentations ( MCPRN, RDRSR) for a user click sequence perform generally better than those with single representation (Caser, GRU4Rec, BERT4Rec ...). Therefore, it is necessary to explore multiple representation to model user‚Äôs diverse intents. Moreover, we can observe that the improvement introduced by capturing user‚Äôs various intentions is more signiÔ¨Åcant for Movielens and Lastfm datasets due to their density. The users in denser datasets like Movielens and Lastfm tend to exhibit more diverse interests in online activity than rating datasets Movielens, which veriÔ¨Åes the necessity of our motivation to model the dynamic interest number in the user behavior and the effectiveness of the DID module in exploring the user‚Äôs dynamic interest number. The improvement of RDRSR over the Ô¨Åxed interest number multi representation method(MCPRN) shows that dynamic interest exploration serves as a better multi-interest extractor than Ô¨Åxed multi interest. Considering the RDRSR and other baselines results, RDRSR consistently outperforms them on three datasets over all evaluation metrics. This can be attributed to two points: 1) The Dynamic Interest Discriminator explores user‚Äôs dynamic interest number which takes the advantage of single representation methods when user‚Äôs intent is one and multi representation methods when user‚Äôs intents are more than one. 2) RMRSR could correctly explore user‚Äôs dynamic interest number and generates corresponding dynamic interest representation for next-item prediction while all other methods could be seen as Ô¨Åxed interest number method which are without enough Ô¨Çexibility. In our model, a major novelty is that we want to explore the user‚Äôs dynamic interest number and form the corresponding dynamic interest representation. To obtain a better understanding why RDRSR performs better than other models, shown in Figure 3, we further construct a case study on Movielens dataset. SpeciÔ¨Åcally, we present a snapshot of the interaction sequence for a sampled user, which contains seven items, and top-one as the recommendatio result. Here we use different colors to represent the different dynamic interest sub-sequences, which is captured by the DID and DIA modules, and the total number of colors is equal to the user‚Äôs dynamic interest number. The Ô¨Årst Ô¨Åve items are user‚Äôs click behavior. In the Ô¨Årst line at time t=6, the new movie doesn‚Äôt not increase the user‚Äôs dynamic interest number and the dynamic interest number is still 2. The second line at time t=6, the new movie with one more color yellow means that the user‚Äôs dynamic interest number is increasing from 2 to 3. Next, the user‚Äôs new interest in sci-Ô¨Å movie is main for the next-item prediction at time t=7. The result shows that our model can correctly explore the user‚Äôs dynamic interest number and makes better recommendation. Figure 3: Case study. The left before t=5 is the user behavior, at t=6 the user clicks two different movies and get different recommendation results. Here we use colors to represent dynamic interest sub-sequences. The picture of each movie is downloaded from https://movie.douban.com. We introduce one variant (RDRSR-F) to validate the effectiveness of the proposed model. SpeciÔ¨Åcally, RDRSR-F shuts down the module Dynamic Interest Discriminator, and the the module Dynamic Interest Allocator set a Ô¨Åxed dynamic interest number. We conduct experiments on all three datasets. Table 4 reports the results in terms of NDCG@10. RDRSR-F3 and MCPRN-3 means that the Ô¨Åxed interest number is 3 and the max dynamic interest number of RDRSR-3 is 3. Obviously, RDRSR-3 signiÔ¨Åcantly outperforms the variant RDRSR-F3 on all datasets. The substantial difference between RDRSR-F3 and RDRSR-3 shows that the learning dynamic user dynamic interest number in DID module is better than those Ô¨Åxed interest number in RDRSR-F3. And it veriÔ¨Åes our motivation to explore the user dynamic interest number in sequential recommendation and the effectiveness of the proposed module DID. What‚Äôs more, the improvement of RDRSR-F3 over MCPRN-3 validates that our DIA module is useful to model user‚Äôs dynamic interest representations for next-item recommendation. Table 4: Ablation study. Performance comparison of RDRSR-3 (max dynamic interest number 3), its variant RDRSR-F3 and MCPRN-3 (Ô¨Åxed interest number 3) over three datasets. And all the numbers in the table are percentage numbers with ‚Äô%‚Äô omitted. We also investigate the sensitivity of the max dynamic interest numberùëòto RDRSR in all three datasets. Figure 4 reports the performance of our model in the metrics of HR and NDCG. In particular, We keep the other parameters in the model consistent with the Q1 settings. From the Ô¨Ågure, we can observe that RDRSR obtains the best performance of HR and NDCG whenùëòequals 3 or 4. With the Ô¨Åxed sequence length t the result increases with the increase of the max dynamic interest numberùëò, which indicates that the user‚Äôs dynamic interest number is multi and bigger max dynamic interest number set in the model may meet the requirements better. RDRSR becomes a single representation method(RDRSR-1) when the max dynamic interest number is 1. The sub-optimal results achieved Figure 4: Hyperparameter study, where the horizontal coordinates is the max dynamic interest number from 1 to 7, and the the vertical coordinates are the metric HR@10 and NDCG@10. by RDRSR-1 gives evidences that single representation is not the best solution for sequential recommendation and the necessity of dynamic interest representations methods. The recommendation performance increases at the beginning, but decreases after reaching a peak due to the complex model structures with bigger max dynamic interest number k, which brings more noise and makes sub-optimal recommendation. In this article, we learning a dynamic group of representations for user to improve the performance of the sequential recommender system. In order to achieve this goal, we design DID and DIA to capture the dynamic interest number and form the corresponding dynamic interest representations. What‚Äôs more, we conducted a ablation study to explore the effectiveness of DID and DIA modules and veriÔ¨Åed the effectiveness of RDRSR on several real datasets with SOTA methods. To the best of our knowledge, we are the Ô¨Årst to consider the personalized dynamic interest number in sequential recommendation. However, the proposed model also exists shortcomings in computing speed, where we formulate the allocation task in DIA module as a MDP problem which is computing cost and unstable in training. In the future we will consider how to allocate the click sequence in a more effective way.