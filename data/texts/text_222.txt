Abstract such as venue search, is typically constrained by the underlying database which restricts the user to specify a predeﬁned set of preferences, or slots, corresponding to the database ﬁelds. We envision a more natural information navigation dialogue interface where a user has ﬂexibility to specify unconstrained preferences that may not match a predeﬁned schema. We propose to use information retrieval from unstructured knowledge to identify entities relevant to a user request. We update the Cambridge restaurants database with unstructured knowledge snippets (reviews and information from the web) for each of the restaurants and annotate a set of query-snippet pairs with a relevance label. We use the annotated dataset to train and evaluate snippet relevance classiﬁers, as a proxy to evaluating recommendation accuracy. We show that with a pretrained transformer model as an encoder, an unsupervised/supervised classiﬁer achieves a weighted F1 of .661/.856. A conversation is a natural user interface for accessing information. In information navigation dialogue, such as search (for restaurants, hotels, or tourist attractions), a user speciﬁes search constraints and navigates over search results using text-based, spoken, or multi-modal interface. Information navigation tasks are typically handled with a schema-driven approach [ A user input to a schema-driven dialogue information navigation system, effectively bootstrapped for a new application using the structure and content of the corresponding database, a user is limited in the range of constraints they may specify. For example, in a restaurant search domain with the schema ﬁelds area, price range and food type, a user may specify any combination of these ﬁelds but not others [ To evaluate a schema-driven information navigation system, the recruited experiment participants are typically given a conversation ‘goal’ based on the database schema, e.g. “you are looking for a cheap Italian place in the center”, and are instructed to retrieve matching venues using spoken or text chat interaction. These instructions guide the user to specify in-domain preferences that can be handled by the dialogue system. An initial user request based on the above ‘goal’ may be: While there is variability in the natural language utterances of a recruited user for a predeﬁned ‘goal’, the preference type speciﬁed in the goal is limited to the schema. ences and formulate their requests to the system without a bias of the instructions. The challenge is that a user request with preferences outside of the domain schema, e.g. ‘Find me a cosy family friendly pub that serves pizza.’ can not be handled by a purely schema-driven system. The majority of user queries (75 out of 105) that we collected without priming the user by a predeﬁned ‘goal’ do not mention any of the domain-speciﬁc schema ﬁelds, indicating that a purely schema-driven system is not sufﬁcient to handle naturally constructed user requests. In contrast to a schema-driven dialogue system, a search interface handles such unconstrained user queries using information retrieval methods from unstructured (text) data. Search interfaces, however, are not interactive and do not handle query changes or follow-up Unlike the recruited subjects, real users come up with personalised search preferquestions. In this work, we aim to information navigation dialogue system. We propose to dialogue system to handle out-of-schema user queries retrieval module in the dialogue system pipeline. information. For this study, we create an annotated dataset for the Cambridge Restaurants domain following the process outlined in Figure 1. We ﬁrst collect a database from the Web, including text snippets composed from restaurant reviews and descriptions along with a set of unconstrained restaurant search queries. Next, unsupervised and transfer learning methods are used to obtain a set of query-snippet pairs which are then annotated using Amazon Mechanical Turk. The resulting annotated dataset is then used to build supervised relevance scoring models and compared with the performance of unsupervised and transfer learning approaches. We show that using a pretrained transformer model as an encoder, an of the snippet relevance to the query achieves a weighted F1 of .661/.856. • A manually annotated dataset with 1.7K query and text snippet pairs. search. In Section 3, we present an extended Cambridge Restaurants 2021 dataset and annotated REStaurant Query Snippet dataset (ResQS). In Section 4, we describe the approaches used to detect text snippets relevant to a query and present experimental results in Section 5. Finally, the conclusions are presented in Section 6. Conversational search aims at providing users with an interactive natural language interface for access to unstructured open-domain information [ oriented information navigation dialogue systems often involve search, e.g. for venues or catalogue items [ scope than closed-domain search in task-oriented dialogue, empirical analysis shows that both tasks result in a similar conversational structure [19]. systems both take unconstrained natural language as input. However, dialogue system users are typically limited in expressing their preferences by the domain schema. In response to an out-of-schema user request, a task-oriented dialogue system may produce an informative help message guiding the user to adapt to its limitations For example, Kim et al. [ An entity retrieval model requires domain-speciﬁc data to extract the requested The contributions of this paper are: A methodology for extending a schema-driven dialogue system to support natural user preferences. Evaluation of supervised, unsupervised, and transfer learning approaches for snippet relevance classiﬁcation. The rest of the paper is organized as follows. In Section 2, we outline related re- Open-domain search interfaces and task-oriented information navigation dialogue 17]. Alternatively, system capabilities may be extended beyond a domain API. in task-oriented dialogue systems. To support pragmatic interpretation, Louis at al. to handle natural preferences, a corpus of natural requests for movie preferences was collected using a novel approach to preference elicitation [11]. from unstructured text. Pretrained transformer models, such as BERT [ shown to be effective in extracting information from text, leading to signiﬁcant improvements on many NLP tasks, including open-domain question answering, FAQ retrieval, and dialogue generation [ both in a supervised and an unsupervised setting [ learning from a general natural language entailment task using publicly available corpora [1]. To develop and evaluate an entity retrieval component for a dialogue system that handles unconstrained user queries, it is necessary to construct a dataset that includes text snippets associated with the entities, collect unconstrained user queries, and identify a set of matching entities for the queries. We collect a new extended dataset of Cambridge restaurants (Section 3.1), a set of unconstrained search queries (Section 3.2), and annotate query-snippet pairs with relevance labels (Section 3.3). In previous work [ restaurants, which did not have review information. In this work, we created an up-todate database with 422 restaurants in Cambridge, UK. previous work, each restaurant is associated with cuisine, price range, location, and description. As in the past systems, the price range is mapped to cheap, moderate, expensive and location to east, west, centre, south. However, cuisine in our database is associated with a list of values rather than a single value for each entity. In addition, the new dataset includes information on meals (breakfast, lunch, dinner), special diets (e.g., vegan, gluten free) and reviews. the unconstrained user’s search preferences (example, cosy family friendly pub). In such regards, we theorise that personalised messages, such as reviews, are an acceptable source to handle such requests. We collect 62.3K reviews with an average and standard deviation of 145(256) per restaurant. The reviews together with text in each data ﬁelds are used as text snippets to retrieve items relevant to the query in the ] explores users’ indirect responses to questions. To extend a task-oriented system Task-oriented dialogue systems require accurate models to extract information A standard restaurant database may not always contain information relating to experiments. Only positive reviews (rating 4 or 5 stars) are used, as we expect user queries to mention desirable properties of the restaurant. To simulate natural unbiased user requests in a restaurant search domain, we created an online form with one question: Please type a sentence describing your restaurant preference to your smart virtual assistant.’ The form was distributed to several university and company mailing lists. Each participant was asked to enter one query. sponding to one of the pre-deﬁned ﬁelds of the database schema (area, cuisine, or price range) that may be used to search for a restaurant in a purely schema-driven system. Although only 14 of these queries contained an entity exactly matching a value in the database and only 7 had no other preferences besides the slot value. For example, ‘I would like to eat in a ﬁne dining establishment, preferably cuisine.’ contains a mention of cuisine as well as a vague preference for a ‘ﬁne dining establishment’. The unbiased queries are highly diverse and most frequently contain a menu item (46), a subjective (31) or an objective preference (25) about a restaurant (see Table 1). We removed 5 queries that mention proximity, e.g. ‘near me’. Such queries would not be handled with the proposed method as it would require additional geographical location information. We use the remaining 100 queries in our experiments. For extracting relevant entities using unstructured data, we need to develop supervised models, which require preferably a balanced training dataset. We create a REStaurant We found that only 30 out of 105 collected queries speciﬁed a constraint corre- Query Snippet (ResQS) dataset of query-snippet pairs, where the snippets include text from reviews and each of the database ﬁelds, labelled with ‘1’ if the snippet is relevant to the query and ‘0’, otherwise. For each query, most of the snippets from our set of 62.3K candidates will be irrelevant. Thus, a random selection of snippets for each query would result in an unbalanced dataset of mostly irrelevant snippets. Instead, unsupervised and transfer learning methods are used to select a set of relevant snippets for each query. We then use manual annotation on the selected set to create query-snippet pairs with annotated relevance information. one transfer learning method (see the ﬁrst three methods shown in Figure 2). Next, we compute a relevance score for each of the 422 restaurants in the dataset and use it to rank them (see Section 4). Finally, we randomly sample one of the ﬁve top-ranked restaurants for the query to compute the relevance score of this restaurant. Manual labels are used for the evaluation of the unsupervised methods as well as for training the supervised models. Using three methods to select ﬁve snippets per query we generate pairs. Additionally, to simulate the hybrid system where both relevance ranking and database match are used to extract a relevant item, we use the 14 queries that specify a value for one of the system slots (area, cuisine, or price range). This subset is then used to select a recommended restaurant for the query. Thus by using three methods to select ﬁve snippets for the 14 queries we extend the dataset by pairs resulting in 1710 examples. Human Intelligence Tasks (HIT) consists of 23 randomized query-snippet pairs and is annotated by three crowd workers. to government pay guidance. workers to select the ﬁnal label. The pairwise agreement between each annotator and the majority vote is very good with Fleiss Kappa k > .7 [4]. In this work, we extend a schema-driven dialogue system to support users’ unconstrained search requests. Our goal is to extract the items most relevant to the users’ query. A dialogue system then presents these items as recommendations to the user and responds to follow-up questions. This work only addresses the extraction of the relevant items, leaving the evaluation of information presentation and handling of interaction to future work. We apply and evaluate our approach using the Cambridge Restaurants search domain. For each of the 100 queries, we ﬁrst score all snippets using two unsupervised and For annotation, we use crowd workers from Amazon Mechanical Turk. Each Algorithm 1 Relevance ranking snippets are scored (line 9) then the top recommended restaurants are obtained by ranking the list of restaurants based on the average score of the top 5 snippets. Cos-BERT, and two supervised approaches: transfer learning from another domain (Trans-BERT), and in-domain training approach (Dom-BERT) illustrated on Figure 2. The input to each of the models are two strings (query and text snippet) and the output is a score where the scores closer to 1 indicate a higher relevance. Algorithm 1 is used to extract an item most relevant to the user’s query. First the For snippets scoring, we explore two unsupervised approaches: Cos-TF-IDF and We expect relevant text snippets to have a higher word overlap and semantic similarity with the corresponding query. For example, for a user query: the two relevant snippets have overlapping vocabulary: a ﬁxed-sized vector using a mapping function similarity score between the user request and each snippet: Cos-TF-IDF: Cosine similarity with TF-IDF Encoding As a baseline, we use Term Frequency-Inverse Document Frequency(TF-IDF), an efﬁcient and simple algorithm for matching words in a query to documents that are relevant to that query, to encode the query and the knowledge snippets [ snippet for each restaurant is taken as a separate document to calculate TF-IDF for each word in the snippet. TF-IDF score for each word in the query is also calculated. Then, the words in the query and the words in the text snippet are replaced by their TF-IDF score to form vocabulary-sized vectors. the vocabulary overlap between Q Cos-BERT: Cosine similarity using BERT encoding We use pretrained Sentence-BERT (SBERT) model to encode the query and the snippets [ vectors of BERT, ﬁne-tuned on the SNLI corpus, to derive a ﬁxed-sized sentence embedding [ and S We build a classiﬁer using a DNN with a single fully-connected linear layer that takes as input encoded representation of a query and a snippet. The input ( query < SEP > snippet encoding of the special symbol ( We map the user query (Q) and each snippet (S) for all of the restaurants into 13]. SBERT uses a pooling operation to obtain the mean of all output 2- or 3-way classiﬁcation. The model is trained to minimize cross-entropy on the training set. For transfer learning we use Stanford Natural Language Inference (SNLI) and DSTC9 datasets [ and hypotheses sentences annotated with the labels contradiction, entailment, and neutral as in the following example: The intuition behind using a model trained on the SNLI dataset is that the relevant snippet for the user’s query would be classiﬁed as entailment while the irrelevant snippets would be classiﬁed as neutral or contradiction. Hence, we train a 3-way classiﬁcation and use the score of entailment class to estimate relevance. questions in a dialogue system. It contains 938 question and answer pairs for the train, hotel, and restaurant domains as shown in following examples. We use the question-answer pairs as the positive examples (relevant) for training the model. The negative examples for training a binary classiﬁer are extracted by randomly sampling answers from different questions following the approaches used by the authors. Dom-BERT: In-domain Training Approach The models that are trained on in-domain data usually achieve better performance. To compare the models trained with the in-domain data with the transfer/unsupervised approaches, we train a supervised binary classiﬁer on ResQS and the combination of ResQS and DSTC9 datasets. A pre-trained BERT encoder was ﬁne-tuned on the indomain classiﬁcation task using the pruning method that removes the task-irrelevant neural connections from the BERT model to better reﬂect the data for the task [5]. Evaluating the overall relevance of a recommended restaurant is challenging, as the information presented to the user would bias their judgement. We assume that the restaurants preferred by the user are associated with the snippets that are relevant to the query. The models are evaluated based on the scores they assign to the snippets. DSTC9 dataset was constructed for the purpose of extracting answers to follow-up information retrieval to extract a recommended restaurant for each query by ranking restaurants based on the relevance score of the snippets (see Section 5.1). The dataset produced with the information retrieval is then manually annotated and used to train supervised snippet relevance classiﬁcation models. To compare snippet relevance classiﬁcation methods, we use a threshold to classify each snippet’s relevance to the query and compute the classiﬁer’s precision, recall, and F1 (see Section 5.2). For each of the queries we apply unsupervised and transfer learning methods to score each snippet and use these scores to rank the restaurants. As a dialogue system can present multiple options in response to a query, we randomly select one of the top ﬁve recommended restaurants and manually annotate the top ﬁve matching snippets with a binary label relevant/not-relevant (see Section 3.3). For the two unsupervised cosine-similarity (Cos-TF-IDF and Cos-BERT) and transfer (Trans-BERT-SNLI) methods we report the snippet relevance (the percent of snippets extracted by the model labelled as relevant) and the overall recommendation quality (see Table 2). We further analyze the relevance scoring performance across three query types that mention a menu item, objective, or subjective information. on exact keyword match, achieves the highest overall snippet relevance of 65%, followed by Cos-BERT with 57%. Transfer learning did not work well, yielding only 16% snippet relevance. We observe that Cos-TF-IDF performance is the highest (67%) on the queries with subjective information that contain adjectives (‘excellent’, ‘great’). However, its performance is below Cos-BERT model on the queries with objective and menu item information (‘ﬁreplace’, ‘dogs’, ’desserts’). For a user query ‘Find me a restaurant with great desserts’, Cos-TF-IDF model extracts a restaurant with generic positive reviews which are not relevant to the query: query that focus on the quality of deserts: We use the two unsupervised and the transfer learning from SNLI models for Cosine similarity approach using TF-IDF encoding (Cos-TF-IDF), which relies However, Cos-BERT model extracts a restaurant with the snippets relevant to this semantics which is especially important for the queries with objective information. rants that would satisfy the user. We approximate the subjective user satisfaction with 1) the percentage of the queries for which at least one of the top ﬁve snippets was labelled as relevant and 2) the average number of the top ﬁve snippets labeled as relevant. Cos-BERT model outperforms the Cos-TF-IDF on the ﬁrst metric (86% vs 83%) and Cos-TF-IDF outperforms Cos-BERT on the second one (3.25 vs 2.86). outperformed the semantic method (Cos-BERT) because it was likely to ﬁnd exact word match for each query. However, for smaller datasets, where vocabulary in a query may not match any of the text snippet, semantic methods may be more beneﬁcial. of 1710 query-snippet (ResQS) pairs which we use to train and evaluate supervised relevance labeling models described in the next section. We compare the snippet relevance classiﬁcation performance of the unsupervised (Cos-TF-IDF, Cos-BERT), transfer (Trans-BERT) and supervised (Dom-BERT) methods. The unsupervised cosine similarity methods use a threshold of 0.5 to determine relevance of a snippet to a query. The Trans-BERT models are trained on the publicly available SNLI and DSTC9 datasets. The model trained on SNLI dataset outputs a 3-way classiﬁcation with the classes contradiction, entailment, and neutral. Using BERT for embedding the query and the snippets appears to capture the The overall recommendation accuracy is the proportion of recommended restau- Our dataset contains 62.3K snippets. Exact word match (TF-IDF) may have The overall relevance scoring accuracy is 48%, resulting in a balanced dataset To apply it on our binary query relevance detection task, we combine contradiction and neutral into the not relevant class and use entailment as the relevant class. The unsupervised and transfer methods are evaluated on the full ResQS corpus as they do not use any of its data for training and the supervised methods are evaluated with 10-fold cross-validation. dataset. Cos-TF-IDF method achieves F1 of 0.365, slightly higher than the baseline that predicts all snippets as relevant (F1=0.322) while Cos-BERT achieves F1 of 0.661. We observe that Cos-TF-IDF which relies on word match has a lower recall than Cos-BERT which captures semantic meaning (0.519 vs 0.672). the always-relevant baseline but lower than the unsupervised Cos-BERT model. The Trans-BERT model trained on DSTC9, a dataset more closely resembling to the target data, achieves F1 of 0.769 outperforming both of the unsupervised methods. The model trained on in-domain ResQS dataset achieves F1 of 0.824 and outperforms all unsupervised and transfer learning models. The best result (F1=0.856) is obtained by training on the combined in-domain ResQS and DSTC9 dataset. Our aim was to collect data that covers most of the aspects of the individual restaurants. In addition to the standard aspects (cuisine, location) provided by the restaurants, we also used reviews which augmented the restaurant information with further non-standard (ﬁreplace, dogs, exotic) aspects thus providing a comprehensive unstructured dataset for handling unconstrained queries. using unstructured data. Cosine similarity with the BERT encoder method resulted in 57% relevant snippets on the information retrieval task and achieved F1 score of .661 on the snippet relevance classiﬁcation task. With this approach 86% of top-5 recommended restaurants had at least one relevant snippet (the estimated recommendation accuracy). Assuming that an item with more relevant snippets is a better match for a user query, we expect to achieve even higher restaurant recommendation accuracy using a more accurate supervised snippet relevance scoring model. The supervised model achieves F1 score of 0.856 on the snippet relevance classiﬁcation task. that they are slower compared to unsupervised methods to extract relevant information from unstructured data is not real-time, it becomes unfeasible for use in a dialogue system. To reduce latency while maintaining accuracy, we could use a combination of models by ﬁrst ﬁltering snippets with an unsupervised method and then applying a supervised model on a smaller dataset. Table 3 shows the average precision, recall, and weighed F1 scores on the ResQS The Transfer-BERT trained on SNLI achieves F1 of 0.368, slightly higher than We then test how accurately an unsupervised method can extract relevant items Although we achieve improvements with the supervised models, it should be noted In this work, we aim to improve naturalness of interaction with an information navigation dialogue system. When a user is not primed by instructions, most user queries in the restaurant search domain include out-of-schema search preferences. Such queries cannot be handled by a purely schema-driven dialogue system, resulting in ineffective and unnatural conversations. To address this problem, we propose an entity retrieval method by incorporating a snippet relevance classiﬁer into the pipeline of a schema-driven dialogue system. systems to use unstructured knowledge and handle out-of-schema user preferences. knowledge snippets. To simulate a naive user, we collect restaurant queries from users without biasing them with speciﬁc instructions In our experimental restaurant search domain, the snippets were obtained from publicly available restaurant reviews and descriptions and annotated by Amazon Mechanical Turk workers. We build a supervised text relevance classiﬁcation model on the annotated data and compare its performance with an unsupervised method. We show that on the annotated dataset an unsupervised/supervised classiﬁer achieves a weighted F1 of .661/.856 work, we will incorporate the proposed approach into the Cambridge restaurant search dialogue system and evaluate it with users. We present an effective methodology for extending schema-driven dialogue We update the Cambridge restaurants database, and extend it with text