The issue of fairness in recommendation is becoming increasingly essential as Recommender Systems (RS) touch and inuence more and more people in their daily lives. In fairness-aware recommendation, most of the existing algorithmic approaches mainly aim at solving a constrained optimization problem by imposing a constraint on the level of fairness while optimizing the main recommendation objective, e.g., click through rate (CTR). While this alleviates the impact of unfair recommendations, the expected return of an approach may signicantly compromise the recommendation accuracy due to the inherent trade-o between fairness and utility. This motivates us to deal with these conicting objectives and explore the optimal trade-o between them in recommendation. One conspicuous approach is to seek a Pareto ecient/optimal solution to guarantee optimal compromises between utility and fairness. Moreover, considering the needs of real-world e-commerce platforms, it would be more desirable if we can generalize the whole Pareto Frontier, so that the decision-makers can specify any preference of one objective over another based on their current business needs. Therefore, in this work, we propose a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL), called MoFIR (pronounced “more fair”), which is able to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Specially, we modify traditional Deep Deterministic Policy Gradient (DDPG) by introducing conditioned network (CN) into it, which conditions the networks directly on these preferences and outputs Q-value-vectors. Experiments on several real-world recommendation datasets verify the superiority of our framework on both fairness metrics and recommendation measures when compared with all other baselines. We also extract the approximate Pareto Frontier on real-world datasets generated by MoFIR and compare to state-of-the-art fairness methods. • Information systems → Recommender systems;• Computing methodologies → Sequential decision making. Etsy Inc. Recommender System; Multi-Objective Reinforcement Learning; Pareto Ecient Fairness; Unbiased Recommendation ACM Reference Format: Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng Hsieh, Yongfeng Zhang. 2022. Toward Pareto Ecient Fairness-Utility Tradeo in Recommendation through Reinforcement Learning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22), February 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3488560.3498487 Personalized recommender systems (RS), which are extensively employed in e-commerce platforms, have been acknowledged for their capacity to deliver high-quality services that bridge the gap between products and customers [7,17,44,51]. Despite these huge advantages, several recent studies also raised concerns that RS may be vulnerable to algorithmic bias in several aspects, which may result in detrimental consequences for underrepresented or disadvantaged groups [19,29,43,59]. For example, the “Matthew Eect” becomes increasingly evident in RS, which creates a huge disparity in the exposure of the producers/products in real-world recommendation systems [16,18,33]. Fortunately, these concerns about algorithmic fairness have resulted in a resurgence of interest to develop fairness-aware recommendation models to ensure such models do not become a source of unfair discrimination in recommendation [13, 15, 26, 28]. In the area of fairness-aware recommendation, the methods can be roughly divided into three categories: pre-processing, inprocessing and post-processing algorithms [14,29]. Pre-processing methods usually aim to remove bias in data, e.g., sampling from data to cover items of all groups or balancing data to increase coverage of minority groups. In-processing methods aim at encoding fairness as part of the objective function, while post-processing methods tend to modify the presentations of the results. Even though all of them could successfully alleviate the impact of unfair recommendations to some extent, the expected return of an approach may signicantly compromise the recommendation accuracy due to the inherent trade-o between fairness and utility, which has been demonstrated by several recent work both empirically and theoretically [22, 23, 32, 55]. In light of the above, one fundamental research questions is asked,RQ1: Can we learn a recommendation model that allows for higher fairness without signicantly compromising recommendation accuracy? And a more challenging one is,RQ2: Can we learn a single recommendation model that is able to produce optimal recommendation policies under dierent levels of fairness-utility trade-o so that it would be more desirable for decision-makers of e-commerce platforms to specify any preference of one objective over another based on their current business needs? To deal withRQ1, one conspicuous approach is to seek a Pareto optimal solution to guarantee optimal compromises between utility and fairness, where a Pareto ecient/optimal solution means no single objective can be further improved without hurting the others. To nd solutions with dierent levels of trade-o between utility and fairness (RQ2), we need to generalize their Pareto frontier in the objective space, where Pareto frontier denotes a set, whose elements are all Pareto optimal. Unfortunately, state-of-the-art approaches of fairness-aware recommendation are limited in understanding the fairness-utility trade-o. Therefore, in this work, we aim to address the above problems and propose a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL) with linear preferences, called MoFIR, which aims to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Technically, we rst formulate the fairnessaware recommendation task as a Multi-Objective Markov Decision Process (MOMDP), with one recommendation objective, e.g., CTR, and one fairness objective, e.g., item exposure fairness (our method is able to generalize to more recommendation objectives as well as more fairness objectives). Second, we modify classic and commonly-used RL algorithm—DDPG [42] by introducing conditioned networks [3] into it, which is a representative method to deal with multi-objective reinforcement learning. Specially, we condition the policy network and the value network directly on the preferences by augmenting them to the feature space. Finally, we utilize the vectorized Q-value functions together with modied loss function to update the parameters. The contributions of this work can be summarized as follows: •We study the problem of Pareto optimal/ecient fairness-utility trade-o in recommendation and extensively explore their Pareto frontier to better satisfy real-world needs; •We formulate the problem into a MOMDP and solve it through a MORL framework, MoFIR, which is optimized over the entire space of preferences in a domain, and allows the trained model to produce the optimal policy for any specied preferences; •Unlike prior methods for fairness-aware recommendation, the proposed framework does not employ any relaxation for objectives in the optimization problem, hence it could achieve stateof-the-art results; •Experiments on several real-world recommendation datasets verify the superiority of our framework on both fairness measures and recommendation performance when compared with all other baselines. There have been growing concerns on fairness in recommendation as recommender systems touch and inuence more and more people in their daily lives. Several recent works have found various types of bias in recommendations, such as gender and race [2,8], item popularity [15,16,59], user feedback [13,25,27] and opinion polarity [54]. There are two primary paradigms adopted in recent studies on algorithmic discrimination: individual fairness and group fairness. Individual fairness requires that each similar individual should be treated similarly, while group fairness requires that the protected groups should be treated similarly to the advantaged group or the populations as a whole. Our work focuses on the item popularity fairness from a group level, yet it can be used to solve multiple types of fairness simultaneously by properly dening and adding them as additional objectives. The relevant methods related to fairness in ranking and recommendation can be roughly divided into three categories: preprocessing, in-processing and post-processing algorithms [14,28, 29]. First of all, pre-processing methods usually aim to minimize the bias in data as bias may arise from the data source. This includes fairness-aware sampling methodologies in the data collection process to cover items of all groups, or balancing methodologies to increase coverage of minority groups, or repairing methodologies to ensure label correctness, remove disparate impact [14]. However, most of the time, we do not have access to the data collection process, but are given the dataset. Secondly, in-processing methods aim at encoding fairness as part of the objective function, typically as a regularizer [1,4]. Finally, post-processing methods tend to modify the presentations of the results, e.g., re-ranking through linear programming [25,43,53] or multi-armed bandit [5]. However, there is no free lunch, imposing fairness constraints to the main learning task introduces a trade-o between these objectives, which have been asserted in several studies [22,23,32,55], e.g., Dutta et al.[12] showed that because of noise on the underrepresented groups the trade-o between accuracy and equality of opportunity exists. Unfortunately, there is very few work of fairness-aware recommendation that can be found to study the fairness-utility trade-o. The closest one to our work is [47], which mainly focused on the trade-o between two-sided fairness in e-commerce recommendation. [47] used a traditional multiple gradient descent algorithm to solve multi-objective optimization problem, meaning that they need to train one network per point on the Pareto frontier, while our MoFIR generates the full Pareto frontier of solutions in a single optimization run. Besides, the authors relaxed all their objectives to get their dierentiable approximations, which, to some extent, hurt its performance, as is shown in the experiment part, Fig. 2. Recommendation with multiple objectives is a signicant but challenging problem, with the core diculty stemming from the potential conicts between objectives. In most real-world recommendation systems, recommendation accuracy (e.g., CTR-oriented objectives) is the dominating factor, while some studies believed that other characteristics, such as usability, protability, usefulness, or diversity should be considered at the same time [20,21,36]. When multiple objectives are concerned, it is expected to get a Pareto optimal/ecient recommendation [31, 39, 50]. The approaches on recommendation with multiple objectives to achieve Pareto eciency can be categorized into two groups: evolutionary algorithm [60] and scalarization [31]. Ribeiro et al.[39, 40] jointly considered multiple trained recommendation algorithms with a Pareto-ecient manner, and conducted an evolutionary algorithm to nd the appropriate parameters for weighted model combination. Besides, Lin et al.[31] optimized GMV and CTR in e-commerce simultaneously based on multiple-gradient descent algorithm, which combines scalarization with Pareto-ecient SGD, and used a relaxed KKT condition. Our proposed method, MoFIR, belongs to scalarization, however, compared with earlier attempts in multi-objective recommendation [31,47], our method learns to adapt a single network for all the trade-o combinations of the inputted preference vectors, therefore it is able to approximate all solutions of the Pareto frontier after a single optimization run. RL-based recommenders have recently become an important and attractive topic, as it is natural to model the recommendation process as a Markov Decision Process (MDP) and use RL agents to capture the dynamics in recommendation scenarios [34,35,41,48,49,58]. Generally speaking, RL-based recommendation systems can be further classied into two categories: policy-based [6,9,11] or valuebased [37,56,58] methods. On one hand, policy-based methods aim to learn strategies that generate actions based on state (such as recommending items). These methods are optimized by policy gradient, which can be deterministic approaches [11,30,42] or stochastic approaches [6,9]. On the other hand, value-based methods aims to model the quality (e.g. Q-value) of actions so that the best action corresponds to the one with the highest Q-value. Apart from using RL in general recommendation task, there also existed several works focusing on using RL in explainable recommendation through knowledge graphs [48, 49]. Currently, there are very few studies using MORL in recommendation. Xie et al.[50] studied multi-objective recommendation to capture users’ objective-level preferences. However, unlike our proposed MoFIR, which learns a single parametric representation for optimal recommendation policies, they conducted a Pareto-oriented RL to generate the personalized objective weights in scalarization for each user, which is a totally dierent problem formulation. In reinforcement learning, agents aim at learning to act in an environment in order to maximize their cumulative reward. A popular model for such problems is Markov Decision Processes (MDP), which is a tuple𝑀 = (S, A, P, R, 𝜇, 𝛾), where𝑆is a set of𝑛states, Ais a set of𝑚actions,P:S × A × S → [0,1]denotes the transition probability function,R:S×A ×S → Ris the reward function, 𝜇:S → [0,1]is the starting state distribution, and𝛾 ∈ [0,1)is the discount factor. We denote the set of all stationary policies byΠ, where a stationary policy𝜋 ∈ Π:S → 𝑃 (A)is a map from states to probability distributions over actions, with𝜋 (𝑎|𝑠)denoting the probability of selecting action𝑎in state𝑠. We aim to learn a policy 𝜋 ∈ Π, able to maximize a performance measure,𝐽 (𝜋), which is typically taken to be the innite horizon discounted total return, where𝜏denotes a trajectory, e.g.,𝜏 = (𝑠, 𝑎, 𝑠, 𝑎, . . . ), and𝜏 ∼ 𝜋indicates that the distribution over trajectories depends on𝜋 :𝑠∼ 𝜇, 𝑎∼ 𝜋(·|𝑠), 𝑠∼ 𝑃(·|𝑠, 𝑎). We denote𝑅(𝜏)as the discounted rewards of a trajectory, the on-policy value function as 𝑉(𝑠) E[𝑅(𝜏)|𝑠= 𝑠], the on-policy action-value function as 𝑄(𝑠, 𝑎) E[𝑅(𝜏)|𝑠= 𝑠, 𝑎= 𝑎], and the advantage function as 𝐴(𝑠, 𝑎)  𝑄(𝑠, 𝑎) − 𝑉(𝑠). Multi-Objective Markov Decision Processes (MOMDP) are MDPs with a vector-valued reward functionr= R(𝑠, 𝑎), where each component ofrcorresponds to one certain objective. A scalarization function f maps the multi-objective value of a policy𝜋to a scalar value. In this work, we consider the commonly-used class of MOMDPs with linear preference functions, e.g.,f(R(𝑠, 𝑎)) = 𝝎 · R(𝑠, 𝑎). It is worth noting that if𝝎is xed to a single value, this MOMDP collapses into a standard MDP. An optimal solution for an MOMDP under linear f is a convex coverage set (CCS), e.g., a set of undominated policies containing at least one optimal policy for any linear scalarization. Abels et al.[3] studied multi-objective reinforcement learning with linear preferences and proposed a novel algorithm for learning a single Q-network that is optimized over the entire space of preferences in a domain. The main idea is called Conditioned Network (CN), in which a Q-Network is augmented to output weight-dependent multi-objective Q-value-vectors, as is shown in the right side of Fig. 1 (Conditioned Critic Network, where action and state representations together with weight vector are inputed to the network). Besides, to promote quick convergence on the new weight vector’s policy and to maintain previously learned policies, the authors updated each experience tuple in a mini-batch with respect to the current weight vector and a random previously encountered weight vector. Specially, given a mini-batch of trajectories, they computed the loss for a given trajectory(𝑠, 𝑎, r, 𝑠)as the sum of the loss on the active weight vector𝝎and on𝝎randomly sampled from the set of encountered weights. y= r+ 𝛾QargmaxQ𝑎, 𝑠; 𝝎· 𝝎, 𝑠; 𝝎(3) whereQ(𝑎, 𝑠; 𝝎)is the network’s Q-value-vector for action𝑎 in state𝑠and with weight vector𝝎. They claimed that training the same sample on two dierent weight vectors has the added advantage of forcing the network to identify that dierent weight vectors can have dierent Q-values for the same state. A more comprehensive review of MOMDPs and CN can be seen in [3]. In the original paper, the authors only proposed an algorithm based on Double DQN with discrete action space, which is not suitable for recommendation scenarios as the action space of recommendation is very large. Therefore, we modify the traditional DDPG [42] by introducing conditioned network into its policy network as well as critic network, and more importantly, we modify the original loss functions for both of them. We choose DDPG as it is a commonly adopted methods in RL, while our modication can be generalized to other reinforcement learning methods, such as trust region ploicy optimization. More details about our modication will be introduced in Section 5. The recommendation agent will take the feature representation of the current user and item candidatesIas input, and generate a list of items𝐿 ∈ Ito recommend, where𝐾 ≥1 after a user sends a request to it at timestamp𝑡 ∈(𝑡,𝑡,𝑡,𝑡,𝑡,. . .). User 𝑢who has received the list of recommended items𝐿will give feedback𝐵via clicking on this set of items, which can be used to measure the recommendation performance. Besides, based on the recommendation results, we will acquire the total number of exposure for each item group𝐺, which can later be used to measure fairness. Thus, the state𝑠can be represented by user features (e.g., user’s recent click history), action𝑎is represented by items in 𝐿, rewardris the immediate reward vector after taking action𝑎, with each component ofrcorresponds to one certain objective (e.g., whether user clicks on an item in𝐿for utility objective or whether an item comes from predened disadvantageous group for fairness objective). The problem formulation is formally presented as follows: • State S:A state𝑠is the representation of user’s most recent positive interaction history𝐻with the recommendation system, together with his/her demographic information (if exists). • Action A:An action𝑎= {𝑎, . . . , 𝑎}is a recommendation list with 𝐾 items to a user 𝑢 at time 𝑡 with current state 𝑠. • Vector Reward Function 𝒓 :A vector-valued reward function r= R(𝑠, 𝑎), where each component ofrcorresponds to one certain objective. In this work, the reward vector includes two elements: utility objective and fairness objective. The details of the denition of our task-specic objectives will be introduced in the following section. • Scalarization function f :In this paper, we consider the class of MOMDPs with linear preferences functionsf, which is a commonly-used scalarization function. Under this setting, each objective is given a weight𝜔, such that the scalarization functionÍ becomes f(R) = 𝝎 · R, where each 𝜔∈ [0, 1] and𝜔= 1. • Discount rate 𝛾: 𝛾 ∈ [0,1]is a discount factor measuring the present value of long-term rewards. We aim to learn a policy𝜋, mapping from states to actions, to generate recommendations that achieve the Pareto ecient tradeo between fairness and utility. The reward vector is designed to measure the recommendation system’s gain regarding utility and fairness. While our method is capable of dealing with multiple objectives simultaneously, for simplicity we deliberately select click through rate and item (group) exposure fairness as our two objectives recommendation utility and item exposure fairness respectively. 4.2.1 Utility Objective. On one hand, given the recommendation based on the action𝑎and the user state𝑠, the user will provide feedback, e.g. click or purchase, etc. The recommender receives immediate reward𝑅(𝑠, 𝑎)according to the user’s positive feedback. We also normalize the reward value by dividing𝐾, which is the length of the recommendation list. 𝑅(𝑠, 𝑎, 𝑠) =𝟙(𝑎gets positive feedback)𝐾(4) 4.2.2 Fairness Objective. On the other hand, based on the recommendation list𝑎, the total number of exposure of each item group will be counted and used to measure exposure fairness. Here, we calculate the ratio of items from sensitive group to the total number of recommended items, and use a hinge loss with margin𝛽to punish the abuse of fairness. Usually, we set𝛽to be the ratio of the number of items in sensitive group to the total number of items. 𝑅(𝑠, 𝑎, 𝑠) = max𝟙(𝑎𝑖𝑠 𝑖𝑛 𝑠𝑒𝑛𝑠𝑖𝑡𝑖𝑣𝑒 𝑔𝑟𝑜𝑢𝑝)𝐾, 𝛽(5) The conditioned actor is almost the same as traditional actor except that we condition the predictions of the policy network to the preference vectors. Practically, we concatenate the state representation𝑠with the vector𝝎and train a neural network on this joint feature space, which is depicted in Fig. 1 (Conditioned Actor Network). The conditioned actor𝜋parameterized by𝜃serves as a stochastic policy that samples an action𝑎∈ Igiven the current state 𝑠∈ Rof a user and the preference vector 𝝎. First of all, we dene𝑠as the concatenation of the user embedding e∈ Rand their recent history embedding h: where the recent history embeddingh= GRU(𝐻)is acquired by encoding𝑁item embeddings via Gated Recurrent Units (GRU) [10], and𝐻= {𝐻, 𝐻, . . . , 𝐻}denotes the most recent𝑁items from user𝑢’s interaction history. We dene the user’s recent history is organized as a queue with xed length, and update it only if the recommended item𝑎∈ 𝑎receives a positive feedback, which ensures that the state can always represent the user’s most recent interests. 𝐻={𝐻, . . . , 𝐻, 𝑎} 𝑎gets positive feedback𝐻Otherwise(7) Secondly, we assume that the probability of actions conditioned on states and preferences follows a continuous high-dimensional Gaussian distribution. We also assume it has mean𝜇 ∈ Rand covariance matrixΣ ∈ R(only elements at diagonal are nonzeros and there are actually𝐾𝑑parameters). In order to achieve better representation ability, we approximate the distribution via a deep neural network, which maps the encoded state𝑠and preferences𝝎to𝜇andΣ. Specically, we adopt a Multi Layer Perceptron (MLP) with tanh(·) as the non-linear activation function, Once received𝜇andΣ, we sample a vector from the acquired Gaussian distributionN (𝜇, Σ)and convert it into a proposal matrix𝑊 ∼ N (𝜇, Σ) ∈ R, whose𝑘-th row, denoted by𝑊∈ R, represents an “ideal” embedding of a virtual item. Finally, the probability matrix𝑃 ∈ Rof selecting the𝑘-th candidate item is given by𝑃= somax(𝑊V), 𝑘 =1, . . . , 𝐾, whereV ∈ Ris the embedding matrix of all candidate items. This is equivalent to using dot product to determine similarity between𝑊and any item. As the result of taking the action at step 𝑡, the actor recommends the 𝑘-th item as follows: where𝑃denotes the probability of taking the𝑖-th item at rank𝑘. The conditioned critic𝜇also diers from the traditional critic in that we concatenate the state representation𝑠with the vector𝝎as well as the embedding of action𝑎, and require the output to be a Qvalue-vector with the size equal to the number of objectives, which is depicted in Fig. 1 (Conditioned Critic Network). The conditioned critic𝜇is parameterized with𝜃and is constructed to approximate the true state-action value vector functionQ(𝑠, 𝑎, 𝝎 )and is used in the optimization of the actor. Following Eq. 2 introduced in conditioned network [3], the conditioned critic network is updated according to temporal-dierence learning that minimizes the following loss function: where y= r+ 𝛾Q(𝑠, 𝑎, 𝝎 ; 𝜃). We present the detailed training procedure of our proposed model, MoFIR, in Algorithm 1 and the model architecture in Fig. 1. As mentioned before, we modify traditional single-objective DDPG into multi-objective DDPG by introducing the conditioned networks to both its actor network and critic network. In each episode, there are two phases — the trajectory generation phase (line 15-20) and model updating phase (line 22-32). In the trajectory generation phase, we sample one linear preference𝝎and x it to generate user-item interaction trajectories. Then in the model updating phase, we sample anotherNpreferences together with𝝎to update the conditioned actor network and the conditioned critic network. Here, we do not follow the original setting in [3], which only uses one more random sampled preference vector, as Yang et al.[52] observed that increasing the number of sampled preference vectors can further improve the coverage ratio of RL agent and diminish the adaptation error in their experiments. In this section, we rst introduce the datasets, the comparison baselines, then discuss and analyse the experimental results. To evaluate the models under dierent data scales, data sparsity and application scenarios, we perform experiments on three realworld datasets. Some basic statistics of the experimental datasets are shown in Table 1. • Movielens:We choose Movielens100K, which includes about one hundred thousand user transactions, respectively (user id, item id, rating, timestamp, etc.). • Ciao:Ciao was collected by Tang et al.[45] from a popular product review site, Epinions, in the month of May, 2011. For each user, they collected user proles, user ratings and user trust relations. For each rating, they collected the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating. • Etsy: We collect a few weeks of user-item interaction data on a famous e-commerce platform, Etsy. For each record, we collect user id, item id and timestamp. Since the original data is sparse, we lter out users and items with fewer than twenty interactions. For each dataset, we rst sort the records of each user based on the timestamp, and then split the records into training and testing sets chronologically by 4:1. The last item in the training set of each user is put into the validation set. Since we focus on item exposure fairness, we need to split items into two groups𝐺 and𝐺based on item popularity. It would be desirable if we have the item impression/listing information and use it to group items, however, since Movielens and Ciao are public dataset and only have Table 1: Basic statistics of the experimental datasets. interaction data, we use the number of interaction to group items in them. Specically, for Movielens and Ciao, the top 20% items in terms of number of interactions belong to the popular group𝐺, and the remaining 80% belong to the long-tail group𝐺, while for Etsy data, we additionally collect the listing impressions per month for each item and group items based on this. Moreover, for RL-based methods, we set the initial state for each user during training as the rst ve clicked items in the training set, and the initial state during testing as the last ve clicked items in the training set. We also set the RL agent recommend ten items to a user each time. 6.2.1Baselines:We compare our proposed method with the following baselines, including both traditional and reinforcement learning based recommendation models. • MF: Collaborative Filtering based on matrix factorization [24] is a representative method for rating prediction. However, since not all datasets contain rating scores, we turn the rating prediction task into ranking prediction. Specically, the user and item interaction vectors are considered as the representation vector for each user and item. • BPR-MF: Bayesian Personalized Ranking [38] is one of the most widely used ranking methods for top-K recommendation, which models recommendation as a pair-wise ranking problem. In the implementation, we conduct balanced negative sampling on nonpurchased items for model learning. • NGCF: Neural Graph Collaborative Filtering [46] is a neural network-based recommendation algorithm, which integrates the user-item interactions into the embedding learning process and exploits the graph structure by propagating embeddings on it to model the high-order connectivity. • LIRD: The original paper for List-wise recommendation based on deep reinforcement learning (LIRD) [57] utilized the concatenation of item embeddings to represent the user state, and the actor will provide a list of K items as an action. We also include two state-of-the-art fairness frameworks to show the fairness performance of our proposed method. • FOE: Fairness of Exposure in Ranking (FOE) [43] is a type of postprocessing algorithm incorporating a standard linear program and the Birkho-von Neumann decomposition. It is originally designed for searching problems, so we follow the same modication method mentioned in [16,47], and use ranking prediction model such as MF, BPR, and NGCF as the base ranker, where the raw utility is given by the predicted probability of user𝑖 clicking item𝑗. In our experiment, we haveMF-FOE,BPR-FOE andNGCF-FOEas our fairness baselines. Since FOE assumes independence of items in the list, it cannot be applied to LIRD, which is a sequential model and the order in its recommendation makes a dierence. • MFR: Multi-FR (MFR) [47] is a generic fairness-aware recommendation framework with multi-objective optimization, which jointly optimizes fairness and utility for two-sided recommendation. In our experiment, we only choose its item popularity fairness. We also modify it as the original fairness considers position bias as well, which is not the same setting as ours. Finally, we haveMF-MFR,BPR-MFRandNGCF-MFR. For same reason as FOE, we do not include LIRD as well. Table 2: Summary of the performance on three datasets. We evaluate for ranking (𝑅𝑒𝑐𝑎𝑙𝑙, 𝐹 % symbol is omitted in the table for clarity) and fairness (𝐾𝐿 𝐷𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒 and 𝑃𝑜𝑝𝑢𝑙𝑎𝑟𝑖𝑡𝑦 𝑅𝑎𝑡𝑒, also in % values), while 𝐾 is the length of recommendation list. Bold scores are used when MoFIR is the best, while underlined scores indicate the strongest baselines. When MoFIR is the best, its improvements against the best baseline are signicant at p < 0.01. We implement MF, BPR-MF, NGCF, MF-FOE, BPR-FOE, NGCFFOE, MF-MFR BPR-MFR and NGCF-MFR using Pytorch with Adam optimizer. For all of them, we consider latent dimensions𝑑from {16, 32, 64, 128, 256}, learning rate𝑙𝑟from {1e-1, 5e-2, 1e-2, . . . , 5e-4, 1e-4}, and the L2 penalty is chosen from {0.01, 0.1, 1}. We tune the hyper-parameters using the validation set and terminate training when the performance on the validation set does not change within 5 epochs. Further, since the FOE-based methods needs to solve a linear programming with size|I| × |I|for each consumer, which brings huge computational costs, we rerank the top-200 items from the base model then select the new top-K (K<100) as the nal recommendation. Similarly, we implementMoFIRwith𝑃𝑦𝑡𝑜𝑟𝑐ℎ. We rst perform basic MF to pretrain 16-dimensional user and item embeddings, and x them through training and test. We set|𝐻| =5, and use two GRU layers to get the state representation𝑠. For the actor network and the critic network, we use two hidden layer MLP with tanh(·) as activation function. Finally, we ne-tune MoFIR’s hyper-parameters on our validation set. In order to examine the trade-o between performance and fairness, we use dierent level of preference vectors in test. Since MoFIR is able to approximate all possible solutions of the Pareto frontier, we simply input dierent Figure 2: Approximate Pareto frontier in three datasets generated by MoFIR and NGCF-MFR, where 𝑥-axis represents the 𝐿𝑜𝑛𝑔𝑡𝑎𝑖𝑙 𝑅𝑎𝑡𝑒@20 (𝐿𝑜𝑛𝑔𝑡𝑎𝑖𝑙 𝑅𝑎𝑡𝑒 equals to one minus 𝑃𝑜𝑝𝑢𝑙𝑎𝑟𝑖𝑡𝑦 𝑅𝑎𝑡𝑒) and 𝑦-axis represents the value of 𝑁 𝐷𝐶𝐺@20. preference vetors𝝎into the trained model to get variants of MoFIR and denote the resulting alternatives asMoFIR-1.0,MoFIR-0.5, andMoFIR-0.1, where the scalar is the weight on the recommendation utility objective. 6.2.2Evaluation Metrics:We select several most commonly used top-K ranking metrics to evaluate each model’s recommendation performance, includingRecall,F1 Score, andNDCG. For fairness evaluation, we denePopularity Rate, which simply refers to the ratio of the number of popular items in the recommendation list to the total number of items in the list. We also employ KL-divergence(KL) to compute the expectation of the dierence between protected group membership at top-K vs. in the over-allÍ population, where𝑑(𝐷||𝐷)=𝐷( 𝑗) lnwith𝐷represents the true group distribution between𝐺and𝐺in top-K recommendation list, and𝐷= [,]represents their ideal distribution of the overall population. The major experimental results are shown in Table 2, besides, we also plot the approximate Pareto frontier between NDCG and Longtail Rate (namely, 1-Popularity Rate) in Fig. 2. We analyze and discuss the results in terms of the following perspectives. 6.3.1 Recommendation Performance. For recommendation performance, we compare MoFIR-1.0 with MF, BPR, NGCF, and LIRD based on𝑅𝑒𝑐𝑎𝑙𝑙@𝑘,𝐹1@𝑘and𝑁 𝐷𝐶𝐺@𝑘and provide the these results of the recommendation performance in Table 2. Among all the baseline models, we can see that all sequential recommendation methods (LIRD, MoFIR-1.0) are much better than the traditional method, which demonstrates the superiority of sequential recommendation on top-K ranking tasks. Specically, LIRD is the strongest baseline in all three datasets on all performance metrics: when averaging across recommendation lengths LIRD achieves 41.28% improvement than MF, 27.08% improvement than BPR-MF, and 8.97% improvement than NGCF. Our MoFIR approach achieves the best top-K recommendation performance against all baselines on all datasets: when averaging across three recommendation lengths on all performance metrics, MoFIR gets 41.40% improvement than the best baseline on Movielens100K; MoFIR gets 46.45% improvement than LIRD on Ciao; and MoFIR gets 18.98% improvement than LIRD on Etsy. These above observations imply that the proposed method does have the ability to capture the dynamic nature in user-item interactions, which results in better recommendation results. Besides, unlike LIRD, which only concatenates user and item embeddings together, MoFIR uses several GRU layers to better capture the sequential information in user history, which benets the model performance. 6.3.2 Fairness Performance. For fairness performance, we compare MoFIRs with FOE-based methods and MFR-based methods based on 𝐾𝐿 𝐷𝑖𝑣𝑒𝑟𝑔𝑒𝑛𝑐𝑒@𝑘and𝑃𝑜𝑝𝑢𝑙𝑎𝑟𝑖𝑡𝑦 𝑅𝑎𝑡𝑒@𝑘, which are also shown in Table 2. It is easy to nd that there does exist a trade-o between the recommendation performance and the fairness performance, which is understandable, as most of the long-tail items have relatively fewer interactions with users. When comparing the baselines, we can easily nd that MFR is able to achieve better trade-o than FOE as it is also a multi-objective optimization method. From Table 2, MoFIR is able to adjust the degree of trade-o between utility and fairness through simply modifying the weight of recommendation utility objective. It is worth noting that MoFIR0.1 can always closely achieve the ideal distribution as its 𝐾𝐿s are close to zero. In Table 2, we can nd that even MoFIR has the similar performance of fairness with other baselines, it can still achieve much better recommendation performance (for example, BPR-FOE and MoFIR-0.5 in Movielens100k or NGCF-FOE and MoFIR-0.5 in Ciao or MF-MFR and MoFIR-0.5 in Etsy), which indicates its capability of nding better trade-o. 6.3.3 Fairness-Utility Trade-o. We only compare MoFIR with MFR, since FOE is a post-processing method, which doesn’t optimize the fairness-utility trade-o. In order to better illustrate the trade-o between utility and fairness, we x the length of the recommendation list at 20 and plot𝑁 𝐷𝐶𝐺@20 against𝐿𝑜𝑛𝑔𝑡𝑎𝑖𝑙 𝑅𝑎𝑡𝑒 in Fig. 2 for all datasets, where𝐿𝑜𝑛𝑔𝑡𝑎𝑖𝑙 𝑅𝑎𝑡𝑒equals to one minus 𝑃𝑜𝑝𝑢𝑙𝑎𝑟𝑖𝑡𝑦 𝑅𝑎𝑡𝑒. Each blue point is generated by simply changing the input weights to the ne-tuned MoFIR, while each orange point is generated by running the entire MFR optimization. The clear margin distance between the blue points’ curve (Approximate Pareto frontier) and the orange points’ curve demonstrates the great eectiveness of MORL compared with traditional multi-objective optimization method in recommendation. In this work, we achieve the approximate Pareto ecient tradeo between fairness and utility in recommendation systems and characterize their Pareto Frontier in the objective space in order to nd solutions with dierent level of trade-o. We accomplish the task by proposing a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL) with linear preferences, called MoFIR, which aims to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Experiments across three dierent datasets demonstrate the eectiveness of our approach in both fairness measures and recommendation performance. We gratefully acknowledge the valuable cooperation of Runzhe Yang from Princeton University and Shuchang Liu from Rutgers University.