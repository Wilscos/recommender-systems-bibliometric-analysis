We propose a novel framework to evaluate the robustness of transformer-based form ﬁeld extraction methods via form attacks. We introduce 14 novel form transformations to evaluate the vulnerability of the state-of-the-art ﬁeld extractors against form attacks from both OCR level and form level, including OCR location/order rearrangement, form background manipulation and form ﬁeld-value augmentation. We conduct robustness evaluation using real invoices and receipts, and perform comprehensive research analysis. Experimental results suggest that the evaluated models are very susceptible to form perturbations such as the variation of ﬁeld-values (∼ 15% drop in F1 score), the disarrangement of input text order(∼ 15% drop in F1 score) and the disruption of the neighboring words of ﬁeldvalues(∼ 10% drop in F1 score). Guided by the analysis, we make recommendations to improve the design of ﬁeld extractors and the process of data collection. Forms such as invoices and receipts are essential in business workﬂows. Extracting target values for ﬁelds of interest from forms (see an example in Fig. 1) is among the most important tasks in document understanding. There are large amounts of forms processed every day, but most current systems still rely on human labor to manually capture ﬁeld-values from massively irrelevant information. Developing a method that automatically extracts ﬁeld-values based on understanding the forms is crucial to reduce human labor, thus improve business efﬁciency. Existing works (Chiticariu et al., 2013; Schuster et al., 2013; Palm et al., 2019; Majumder et al., 2020; Xu et al., 2020) focus on improving the modeling of ﬁeld extractors and have made great progress. However, their evaluation paradigms are Figure 1: A form ﬁeld extraction system may fail due to a slight modiﬁcation to the form. Keys (concrete textexpressions of ﬁelds) are marked in blue boxes. Values are marked in red boxes. limited. First, most of the methods are evaluated using internal datasets. Internal datasets usually have very limited variations and are often biased towards certain data distributions due to the constraints of the data collection process. For example, the forms might be collected from just a few vendors in a relatively short time which leads to similar semantics and layouts across the forms. Second, public datasets lack for diversity in terms of both textual expression and form layouts. Take the most frequently used dataset, SROIE (Huang et al., 2019), as an example. The ﬁelds, company and address, are always on the very top in all receipts. Although the existing models achieve decent performance on these datasets, it is difﬁcult to know whether they can generalize well. This issue can be solved by collecting large-scale diverse forms for evaluation, but it is very challenging since real forms usually contain customers’ private information, thus are not publicly accessible. To tackle this dilemma, we propose a novel framework to evaluate the robustness of form ﬁeld extractors by attacking the models using form transformations. We consider form perturbations from both OCR level and form level, including OCR text location/order rearrangement, form background manipulation, and form ﬁeld-value augmentation. Fourteen form transformations are proposed to impose these attacks. Using the proposed framework, we conduct robustness analysis on two commonly used form types, i.e., invoices and receipts. Experimental results demonstrate that the state-of-theart (SOTA) methods are particularly vulnerable to form perturbations, including the variation of ﬁeldvalues, the disarrangement of input text order, and the disruption of the neighboring words of ﬁeldvalues. Recommendations for model design and data collection/augmentation are made accordingly. Our contributions are summarized as follows. First, we introduce a framework to measure the robustness of form ﬁeld extractors by attacking the models using the proposed form transformations. To the best of our knowledge, this is the ﬁrst work studying form attacks to ﬁeld extraction methods. Second, we identify the susceptibilities of the SOTA methods by comprehensive robustness analysis on two form types using the proposed framework and make insightful recommendations. Information extraction from formsis a widely researched area. Katti et al. (2018) and Denk and Reisswig (2019) encode each page of a form as a two-dimensional grid and extract header and line items from it using fully convolutional networks. DocStruct (Wang et al., 2020) conducts document structure inference by encoding the form structure as a graph-like hierarchy of text fragments. Research works speciﬁcally focusing on form ﬁeld extraction are more related to our work. Earlier methods (Chiticariu et al., 2013; Schuster et al., 2013) relied on pre-registered templates in the system for information extraction. Palm et al. (2019) extract ﬁeld-values of invoices via an Attend, Copy, Parse architecture. Recent methods formulate the ﬁeld extraction problem as ﬁeld-value pairing (Majumder et al., 2020) and ﬁeld tagging (Xu et al., 2020) tasks, where transformer (Vaswani et al., 2017) based structures are used to extract informative form representation via modeling interactions among text tokens. We focus on evaluating transformer-based ﬁeld extraction methods given their great predictive capability for the task. Robustness evaluationof models has received considerable attention. Errudite (Wu et al., 2019) introduces model and task agnostic principles for informative error analysis of NLP models. Ma (2019) propose NLPAug, which contains simple textual augmentations to improve model robustness. Some works aim at robustness of text attacks (Morris et al., 2020; Zeng et al., 2020; Kiela et al., 2021). A recent work, Robustness Gym (Goel et al., 2021), presents a simple and extensible evaluation toolkit that uniﬁes standard evaluation paradigms. There are also recent methods studying robustness of visual models (Santurkar et al., 2020; Salman et al., 2020; Taori et al., 2020). To the best of our knowledge, this work is the ﬁrst one focusing on robustness evaluation of form ﬁeld extraction systems. We are focusing on the robustness evaluation of transformer-based form ﬁeld extractors due to their undisputedly outstanding performance. Before discussing the robustness evaluation, we ﬁrst illustrate the ﬁeld extraction pipeline. In a standard ﬁeld extraction system, an OCR engine is used to extract a set of words, {w, w, ..., w}and their bounding box locations, {b, b, ..., b}, whereNindicates the total number of words. Then, a transformer-based feature backbone is used to model the interactions between text tokens and generate informative token representations,f. Since both semantics and layouts are essential for ﬁeld-value inference, we use LayoutLM (Xu et al., 2020) as the feature backbone. We also experiment with two more transformers, i.e., BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) which only take text as input in the appendix. Finally, a fully connected (FC) layer is used to project the token features to ﬁeld space and generate,s= F C(f), wheres∈ R indicates the predicted ﬁeld score andMdenotes the total number of positive ﬁelds. During training, cross entropy loss betweensand ﬁeld label is utilized for model optimization. During inference, a post-processing method is applied to the predicted ﬁeld scores to get the value for each ﬁeld. We follow the simple criteria to generate ﬁeld-values: (1) we ﬁnd the predicted ﬁeld label for each word by ˆfd = argmax(s), whereccorresponds to ﬁelds (2) by default, for each ﬁeld, we only keep the word as the value if its prediction score is the highest among all the words and larger than a threshold (θ = 0.1). For ﬁelds that often include multi-word values, e.g., address and company, we keep all the Figure 2: An illustration of our evaluation pipeline. An OCR engine is ﬁrst used to extract texts and locations. 14 transformations are applied to the texts and their locations to generate diverse form variants. Then, each transformed set will input to a transformer-based ﬁeld extractor. Robustness evaluation results are ﬁnally generated. words exceeding the threshold and group nearby ones with the same predicted ﬁeld. Evaluation Metric. End-to-end F1 score averaging over ﬁelds is used to evaluate models. We use exact string matching between the predicted values and the ground-truth to count true positives, false positives, and false negatives. Precision, recall and F1 scores are obtained accordingly for each ﬁeld. We propose OCR level and form level transformations to attack ﬁeld extractors. As shown in Fig. 2, our transformations are performed after OCR extraction, and the transformed data is input to a transformer-based ﬁeld extractor. An analysis is conducted via performance comparison between the original set and transformed sets. Each transformation and the principles behind it are introduced as follows. We transform the original data to meaningful variants by slightly altering the OCR locations and the text order arrangement. These transformations simulate scenarios where we may obtain different OCR results before inputting to ﬁeld extractors due to various reasons, e.g., the quality of OCR engines. Center Shift and Box Stretch. To evaluate model robustness to OCR text location jittering, we propose two word-level location transformations. In Center Shift, we keep the box size and randomly shift the center of a box. The shifting is in proportional to the width (horizontally) and height (vertically) of the box, and the ratio is a random number drawn from a normal distribution,N (0, δ). Box Stretch randomly changes the four coordinates of a box in a similar way using N (0, δ). Margin Padding. Scanning forms may introduce white margins, which globally changes text locations. We use Margin Padding to manipulate the locations of all the words in a form. We pad white margins in the left, right, up, and down sides of a form where the margin length is a generated random number between 1 and rof the page size. Global Shufﬂe. We observe that organizing a transformer’s inputs in reading order is particularly beneﬁcial to understanding the form structure. However, the reading order is not always guaranteed by OCR engines. Hence, it is interesting to investigate model robustness to poor reading order quality. We use Global Shufﬂe to shufﬂe the order of words before inputting to transformers. Note that the words and their locations are not changed at all and the only difference is the order of the word sequence input to the transformer. Neighbor Shufﬂe and Non-neighbor Shufﬂe. Intuitively, local neighbors of a value make more contributions to its prediction. So, we propose Neighbor Shufﬂe which shufﬂes the order of each value’s neighbors and keeps the order of the rest. Oppositely, we also have Non-neighbor Shufﬂe. A word,w, is deﬁned as a value’s neighbor if the IoU betweenband the neighbor zone of the value is larger than0.5. The neighbor zone is a box that shares the same center as the value box with expanded width and height (expand rate denoted as r). We also includennearby words from the original reading order as the neighbors. Form background generally affects the model performance in two ways: (1) some background words are strong indicators to improve ﬁeld-values’ recall and (2) accurate prediction of background reduces false positives, thus increase model precision. We propose the following transformations to evaluate model robustness to background perturbations. BG Drop. Background (BG) Drop mimics the scenario that some words are completely missed by OCR detection. This transformation removes background words together with the corresponding boxes at a probability of p. Neighbor BG Dropis similar to Neighbor Shufﬂe, which drops all background words if they are neighbors of a ﬁeld-value. Key Drop. Keys are concrete text-representations of ﬁelds in a form. For example, the ﬁeld invoice_number may be represented as "INV #", "Invoice No." etc. in a form. A key is a very important feature for value localization since the value is often located near the key. We propose Key Drop to see the model performance change if keys are accidentally missed by OCR detection. BG Typo. OCR recognition usually makes errors. BG Typo simulates word-level string typos. We select each background word at a probability of p. For each selected word, we apply one of the error types, including swapping, deleting, adding, and replacing a random or a speciﬁc character. BG Synonyms. Similar semantics may be represented using different word synonyms. BG Synonyms randomly replaces each background word at a probability of pwith their synonyms. BG Adversarial. Some forms contain only one word in the same data type as ﬁeld-values. For example, there might be only one word with the date type of date. It is less challenging for a model to recognize it as the invoice_date. However, this type of easy case is not always guaranteed in real-world applications. BG Adversarial is used to increase the difﬁculty level by adding distraction. Concretely, we select background words at a probability of pand use adversarial words for replacement. For each replacement, we randomly choose a data type and then generate a random value of the corresponding data type. We focus on three data types, i.e., date, number and money. We generate random dates using Faker.For numbers, we ﬁrst generate the number length randomly and then a random number of the length accordingly. For money, we obtain a random amount within lower and upper bounds. Then, we make the amount in money format, where we add a decimal point at the second to the left digit, place a comma at every third digit to the left of the decimal point and randomly insert $at the beginning. To protect strong indicators for values, neighbor words are not replaced. Modifying ﬁeld-values is a more direct way to increase the diversity of the evaluation set. We augment ﬁeld-values in both text and locations. Value Text Augment. Field-values of forms may be biased due to the limitation of the data collection process. For example, the invoice_date may be restricted to the year the form is collected, and the invoice_number may be biased towards the vendor’s numbering system. Value Text Augment transformation targets at augmenting the ﬁeld-values based on their data types. For each ﬁeld-value, we randomly generate a substitute with the same data type following the same value generation procedure as we do in BG Adversarial. Value Location Augment. Form layouts can be very diverse in real-world scenarios. Intuitively, we should be able to infer a ﬁeld-value as long as a key is represented properly no matter where we place the key-value pair in the document. We introduce Value Location Augment to increase layout diversity. To maintain the form format to the most, we keep the background as it is and shufﬂe the keyvalue pair’s locations in the form. For example, for the ﬁeld invoice_number (key: Invoice No., value: 1234) and invoice_date (key: Invoice date, value: 01/01/2021), we swap the box locations of “Invoice No." and “Invoice date", and also the locations of “1234" and “01/01/21". We evaluate the robustness of transformation-based ﬁeld extractors using our framework on two commonly used form types, i.e., invoices and receipts. Our evaluation models are trained using a labeled train set, and the best performing model is picked based on a validation set. We prepare a separate test set to perform the robustness evaluation. To perform the proposed transformations, we annotate both the key and value of each ﬁeld of interest with their bounding box locations in a form. Invoice. The train, valid, and test sets contain 158, 348 and 338 real invoices. They are collected from 111, 222, and 222 vendors, respectively. We sample at most 5 forms from the same vendor and the vendors of train, valid and test sets do not overlap. We consider 7 frequently used ﬁelds including invoice_number, purchase_order, invoice_date, due_date, amount_due, total_amount and total_tax. Receipt. We use the publicly available receipt dataset, SROIE. The annotations of their original test set are not publicly available, so we split the original train set to train, valid, and test sets based on their company names and sample at most 5 forms per company following Majumder et al. (2020). Finally, we get 237 receipts for training, 76 for validation, and 74 for testing. The ﬁelds of interest are company, address, date and total. We add value boxes and annotate keys according to the text-level annotations provided by the original dataset. Our evaluation framework is implemented using Pytorch and the experiments are conducted on a single Tesla V100 GPU. The strength of a transformation is controlled by parameters. We set parameters to make moderate perturbations. In BG Typo, BG Drop, BG Synonyms and BG Adversarial, transformations are only applied to some selected words. We ﬁx the pre-deﬁned probabilities, p, p, p, to 0.1.θis set to 0.5 andθ is 0.1.rin Margin Padding is set to 0.3. When determining the value’s neighbor zone, we set the expand rate as r= 0.02 and n= 2. We generate random values based on data types in BG Adversarial and Value Text Augment. For dates, we randomly pick a date from the year 2001 to 2021 in one of the formats, including mm/dd/yy, yy-mm-dd, dd/month/yy, and dd/mon/yy. For numbers, the number length is randomly generated from 3 to 12. The amount of money is randomly selected from 1 to 10,000,000. We use a commercial OCR enginefor OCR extraction and utilize Tesseractto rank the words in reading order. Our default transformer is LayoutLM (Xu et al., 2020) with text and boxes as inputs. We also evaluate using BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) that take only text tokens as the inputs in Sec. A. All models are ﬁnetuned from the corresponding base models. During training, we set the batch size to 8 and use the Adam optimizer with a learning rate of 5e. Table 1: Evaluation of robustness to OCR location modiﬁcations on invoices. LayoutLM is used as the transformer model. OCR location modiﬁcation. Robustness evaluation of the LayoutLM model to OCR text location jittering is shown in Table 1. We obtain comparable results to the original performance when applying Center Shift and Box Stretch which indicates that slight box jittering is tolerable in our case. Margin Pad shifts all text locations by adding random margins around a form. This transformation also just slightly decrease the model performance. Table 2: Evaluation of robustness to OCR text order on invoices. LayoutLM is used as the transformer model. OCR text orderis essential to transformer-based ﬁeld extractors since the order can serve as an important feature to improve model performance. We show the model performance when applying order shufﬂe to different places in Table 2. The results show that if we shufﬂe all text orders, the performance drops dramatically by 14.1% in F1 score. When only shufﬂe value neighbors, we obtain∼6% lower F1 score. We get∼4% lower F1 score if we shufﬂe non-neighbor words, even though we keep the text order of all values and their neighbors the same. The results demonstrate the importance of the text order. If the model is trained using texts with a good reading order, we may also want to ensure a good reading order during inference. A natural question is, what if we break the reading orders during training. Will this help us save the effort of ensuring test reading order during inference? We re-train ﬁeld extractors using texts with random orders. We obtain 58.7% in F1 score, which is 11.3% lower than our baseline. The comparison result suggests that the text reading order is a very important feature. How to use it without overﬁtting to it is an interesting research topic. Table 3: Evaluation of robustness to BG manipulations on invoices. LayoutLM is used as the transformer model. Background droprelated transformations simulate the scenarios where an OCR detector accidentally misses some background words. BG Drop removes words randomly selected in the background. As shown in Table 3, global BG Drop leads to slight performance decrease. When we apply Neighbor BG Drop, the performance largely drops by∼9% in F1 score. Comparing BG Drop and Neighbor BG Drop, we ﬁnd that neighbor words are indeed more important for value extraction. For a fair comparison, we have adjusted the dropping rate of BG Drop to 0.13, such that the total number of dropped words is roughly equal to the number of neighbor words. Further, Key Drop results in a similar performance decrease as Neighbor BG Drop, although the total number of Keys is less than half of that for neighbor words. Table 4: Evaluation of robustness to background and ﬁeld-value augmentations on invoices. LayoutLM is used as the transformer model. Other background manipulations. Some background words, e.g., keys and other useful indicators, are important features to localize values. Adding typos to the background words may be harmful to these good features, thus BG Typo leads to a 2.4% drop in recall rate. Forms from different vendors may use different words even when represent similar semantics. We attack the models using BG Synonyms. As shown in Table 3, our ﬁeld extractors are quite robust to this transformation with only a negligible drop in F1 score. BG Adversarial is used to add background words (serve as distractions) with similar data types as ﬁeld-values. As shown in Table 3, BG Adversarial leads to 4.7% drop in model precision. Value augmentations. Field-values of forms may be limited in diversity. Value Text Augment transformation augments ﬁeld-values by replacing them with randomly generated values in the same data type. We augment the values of all the ﬁelds, except for total_amount and amount_due, since these two ﬁelds may involve complicated mathematical computations. For total_tax, we randomly select a number between 0 and 15% of the total_amount. The comparison results in Table 4 show that the model performance drop signiﬁcantly by 15.5% F1 score. Value Location Augment changes the spatial arrangements of key-value pairs. In practice, we only shufﬂe the key-value pairs if they have the same number of key words and the same number of value words, resulting in more than 75% key-value pairs relocated. The results in Table 4 demonstrate that Value Location Augment signiﬁcantly reduces F1 scores by 11.2%. Multiple transformations. The proposed transformations can be combined together to generate more diverse sets. We conduct exhaustive combinations of every two and three transformations which result in 91 2-transformation combinations and 364 3-transformation combinations.The top-10 most impactful combinations are shown in Fig. 3. The comparison results suggest the following conclusions. First, generally if an individual transformation drops more performance, it also contributes more drop when combined with other transformations. The most impactful combination is (Value text Aug- Figure 3: Top-10 most impactful 2-transformation and 3-transformation combinations. VTA: Value Text Augment, GS: Global Shift, VLA: Value Location Augment, KD: Key Drop, NBD: Neighbor Background Drop, BA: Background Adversarial, MP: Margin Pad, BT: Background Typo, NS: Neighbor Shufﬂe, NNS: Non-Neighbor Shufﬂe. ment, Global Shufﬂe, Value Location Augment) with a F1 score of 25.7. They are the top-3 impactful transformations suggested in Fig. 4. Second, some individual transformations are less impactful, but they affect more when combined with some speciﬁc transformations. For example, individual Margin Pad ranks low in Fig. 4. However, it leads to more performance drop when combined with Value Text Augment and Global Shufﬂe. Their performance drop ranks 5 out of 364 combinations (F1 is 30.8). This may due to that Margin Pad (changes all words’ locations), Value Text Augment (changes values’ texts) and Global Shufﬂe (changes text input order) are three complementary transformations. When we do Margin Pad alone, the model resorts to the information of value texts and text orders. However, when we do these three transformations together, the model becomes inevitably confused. Third, if the transformations have overlapping effects, their combination has a lower impact. For example, Key Drop, Neighbor BG Drop and Neighbor Shufﬂe all manipulate neighbor words. The performance drop on their combination ranks 246 out of 364 combinations although their individual transformation is impactful (see Fig.4). The F1 score is 58.1 which is very close to an individual Key Drop transformation (59.1). There are two interesting features of the SROIE dataset. First, a signiﬁcant amount of ﬁeld-values have no keys, for example, all values of company and address, and some values of date and total. Consequently, changing the context of values has a minor effect on model performance. Second, the layouts of different receipts are very ﬁxed. For example, company and address are always on the very top of every receipt. So, models could easily overﬁt to ﬁeld-value locations and the text order. As shown in Table 5, Global Shufﬂe leads to signiﬁcant performance drop by 38.7% F1 score. Speciﬁcally, the ﬁelds of address and company become 0% F1 score when the text’s order is completely shufﬂed before inputting to the transformer. The results demonstrate that the model is overﬁtting to the input text order, especially for address and company. Table 5: Robustness evaluation on SROIE dataset. LayoutLM is used as the transformer model. Most of the ﬁelds in SROIE have no keys. To augment receipt layouts, we design a dedicated method that locally moves ﬁeld-value locations. Speciﬁcally, for company and address in SROIE receipts, we move the values to the bottom of the form and shift the rest above to ﬁll the gap as shown in Fig. 5. We refer to this transformation as Value Location Augment*. This transformation changes the location of the values without breaking the text order within each value. We obtain 21.6% and 1.7% F1 score for company and address, respectively, which are around 60% and 68% lower than the original numbers. Besides, we also evaluate models on test set transformed by Value Text Augment. We replace values of company, address and date using substitute randomly generated by Faker. Same as what we do for total_amount and amount_due for invoices, we keep the values of total as they are. To maintain the layout structure, we only replace company and address if we are able to get a randomly generated sample with the same number of words as the original sample. This results in about 69% company values and 31% address values changed, respectively. As shown in Table 5, the Value Text Augment largely decreases the model performance Figure 4: An overview of LayoutLM-based model performance drop due on different transformed dataset. The results are sorted by the performance gap. Figure 5: An illustration of Value Location Augment* transformation on a receipt in SROIE dateset. by 5.5% in F1 score. An overview comparison of all the transformations of invoices is summarized in Fig. 4. As we can see, the top-3 substantial transformations are Value Text Augment, Global Shufﬂe and Value Location Augment. Experiments on receipts also show the effectiveness of these three transformations. We make the following recommendations based on the analysis. For data collection/augmentation, forms with more diverse values are preferable. For example, we may want dates covering a wide range of time periods with more types of formats and numbers being more extensive. Varying forms’ layouts is also beneﬁcial. Especially, we may want to focus on varying the arrangement of ﬁeld-values instead of altering individual word locations locally. For the design of ﬁeld extractors, we suggest making better utilization of the text order. As shown in our experiments, the text order is a very useful feature. How to utilize the text reading order without overﬁtting to it is an interesting topic. Besides, Key Drop and Neighbor BG Drop result in signiﬁcant performance decreases as shown in Fig. 4. This suggests that value’s neighbors, especially the keys, are essential for value extractions. Current state-of-the-art models use transformers to model interactions between all words. We believe paying attention to keys and neighbors in the model design has the potential to improve the existing ﬁeld extraction systems. We proposed a novel framework to evaluate the robustness of transformer-based form ﬁeld extractors via form attacks. We introduced 14 transformations that transform forms in different aspects, including OCR-level location and order, background contexts, and ﬁeld-value text and layouts. We conducted studies on real invoices and receipts with three types of transformer-based models using our proposed framework. Research recommendations were made based on the robustness analysis. Improving ﬁeld extraction from forms using the research analysis generated by the robustness evaluation is a very meaningful research area. The proposed transformations are potentially useful for increasing the diversity of training samples, thus improving model robustness. We will consider this in the future work. This work targets at robustness evaluation of form information extraction systems, so it has positive impacts such as identifying bias of existing information extractors and improving the fairness of model comparison. On the opposite side, our method may have unintended negative consequences in that we have proposed transformations that evaluate various aspects of model robustness, but the metrics we have selected may not be comprehensive. As a result, there is likely some degree of model bias present that has been missed by the proposed framework. However, this negative impact is not speciﬁc to our work and should be considered in general in the ﬁeld of robustness AI. The invoice dataset is for internal use only and does not contain any personally identiﬁable data. The SROIE dataset is a public dataset under MIT license. All forms were annotated by the authors. Consequently, we are conﬁdent that the datasets do not have ethical issues.