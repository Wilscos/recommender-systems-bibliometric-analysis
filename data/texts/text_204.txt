Scaling properties of neural networks have long been an intriguing topic of study [ the practical success of modern neural networks at scale, theoretical understanding of the factors governing the quality and training dynamics of large neural networks has also being developing budget has been identiﬁed as a reliable approach to improve generalization performance on several machine learning tasks. For many of these tasks the scaling behavior of neural networks is highly predictable; model ﬁt or test loss can be described precisely as a function of its number of parameters Preprint. Under review. ghorbani@google.comorhanf@google.com We present an empirical study of scaling properties of encoder-decoder Transformer models used in neural machine translation (NMT). We show that cross-entropy loss as a function of model size follows a certain scaling law. Speciﬁcally (i) We propose a formula which describes the scaling behavior of cross-entropy loss as a bivariate function of encoder and decoder size, and show that it gives accurate predictions under a variety of scaling approaches and languages; we show that the total number of parameters alone is not sufﬁcient for such purposes. (ii) We observe different power law exponents when scaling the decoder vs scaling the encoder, and provide recommendations for optimal allocation of encoder/decoder capacity based on this observation. (iii) We also report that the scaling behavior of the model is acutely inﬂuenced by composition bias of the train/test sets, which we deﬁne as any deviation from naturally generated text (either via machine generated or human translated text). We observe that natural text on the target side enjoys scaling, which manifests as successful reduction of the cross-entropy loss. (iv) Finally, we investigate the relationship between the cross-entropy loss and the quality of the generated translations. We ﬁnd two different behaviors, depending on the nature of the test data. For test sets which were originally translated from target language to source language, both loss and BLEU score improve as model size increases. In contrast, for test sets originally translated from source language to target language, the loss improves, but the BLEU score stops improving after a certain threshold. We release generated text from all models used in this study. ,12,8,20,5,27]. In particular, scaling model sizes, datasets and the total computation 16,17,31]. Neural machine translation (NMT) has long enjoyed the beneﬁts of scaling ], but studies investigating the scaling behavior of NMT models are missing. We present the ﬁrst large-scale systematic study of scaling laws for encoder-decoder Transformer models applied to NMT [36]. We start with highlighting the major differences between decoder-only language models, where the majority of the previous work has focused, and encoder-decoder (conditional) language models applied to NMT. The two differ along a few crucial dimensions. The ﬁrst difference results from the very nature of the separate architectures being used, i.e. decoder-only vs encoder-decoder. The presence of separate architectural components complicates the study of scaling properties due to the increased degree of freedom. Second, contrary to language modeling, the task of machine translation is conditional: the task is predictive rather than fully generative. Furthermore, this prediction task is ambiguous: there is no one right answer for a given source, and translations can vary substantially depending on the translator’s incentives. This manifests itself as different scaling beneﬁts for different test sets. To take an extreme example, a test set translated by someone who writes nearly word-forword translations may beneﬁt less from model scaling than one translated by someone who considers each translation a work of art. In this work, these differences in difﬁculty coincide with the translation direction of the test set; that is, whether the source was translated into the target (source-original) or vice versa (target-original). Source-original data has translated text on the target side, which contains several artifacts of “translationese” that distinguish it from text originally written in that language, often lacking the diversity and complexity of “natural” text [ the prediction of more complex natural text on the target side. Finally, unlike language models, NMT is evaluated on metrics that quantify generation quality against reference translations (for eg. BLEU) [28] instead of evaluating model ﬁt (perplexity) on an evaluation set. In this paper, we aim to provide empirical answers to the following research questions: 1. Does the encoder-decoder architecture for NMT share the same scaling law function 2. How does the naturalness of source/target side data affect scaling behavior? 3. Do scaling improvements in cross-entropy translate into corresponding improvements Our results on multiple language pairs and training/test data compositions validate that scaling predictably improves the cross-entropy on validation data raise several questions regarding the effect of naturalness of training and evaluation text and how cross-entropy eventually relates with generation quality for auto-regressive generative models. Model Architectures and Training varying sizes [ An initial version of this study was submitted to NeurIPS 2021. A few weeks before the publication of this manuscript on Arxiv, [ papers study scaling laws for NMT, our studies focus on different parameter regimes (393K-56M vs 100M-3.5B). as the language models?Contrary to previous work on LM, we show that a univariate law depending on the total number of parameters in the network does not adequately describe the scaling behavior of NMT models. Our scaling laws parameterize the cross entropy loss as a bivariate function of the number of encoder parameters and the number of decoder parameters as separate variables. Our results indicate that the scaling behavior is largely determined by the total capacity of the model, and the capacity allocation between the encoder and the decoder. the effect of naturalness of the source and target text, both for training and evaluation. When evaluating with target side natural text, scaling the model capacity continues improving model quality throughout our range of measurements. On the other hand, improvements on cross-entropy saturate (or reaches the irreducible error region) on source side natural evaluation sets even for moderately-sized models. in generation quality?Finally we study the relationship between generation quality and cross-entropy and how their correlation changes as we: (i) Scale different components of the model (encoder vs decoder) and (ii) Evaluate on source-natural or target-natural evaluation sets. 39]. Models are trained with per-token cross-entropy loss and Adafactor optimizer [35]. All models are trained with a ﬁxed batch-size of feed-forward activations and attention. All models are trained to near convergence for steps. Details of the model hyper-parameters are described in Appendix A. Model Scaling cross-attention and feed-forward layers, each having multiple adjustable hyper-parameters (e.g. model-dimension, number of attention heads, attention projection dimension etc.). Considering the combinatorial expansion of the search space for scaling each one [ to vary only the total number of Transformer Blocks, while keeping the internal hyper-parameters intact across different scales. In other words, we scale the depth of the Transformers while keeping width and other variables ﬁxed. We use GPipe pipeline parallelism for scaling [ ﬂexible API across various depths. In an encoder-decoder Transformer architecture for NMT, depth scaling can naturally be implemented by varying encoder-decoder blocks independently or symmetrically. Hence, we examine the change in the cross-entropy loss as the number of parameters increase with three depth scaling approaches: Encoder Scaling: vary encoder depth (2 to 64) while decoder depth is ﬁxed (6 layers). Decoder Scaling: vary decoder depth (2 to 64) while encoder depth is ﬁxed (6 layers). Symmetric Scaling: increasing decoder and encoder layers together (from For all experiments, conﬁguration of the individual layers is unchanged: the model dimension, width of the feed-forward layer, and number of attention heads are ﬁxed respectively at Each encoder layer adds approximately 20M parameters to the model while each decoder layer adds around 25M parameters. In this section, we train decoder size by approximately a factor of Following the convention in this literature, we do not count the parameters in the embedding and softmax layers towards the model size. Language Pairs using an in-house web-crawled dataset with around billion tokens) for both translation directions. This dataset provides a large enough training set to ensure the dataset size is not a bottleneck in the model performance. Evaluation Sets Domain (ii) News-Domain (iii) Wikipedia (iv) Patents. The news-domain test sets come from the WMT2019 [ internal test sets representing the different domains, ranging from 500 to 5000 sentence pairs. For each domain, we randomly sample sentences in the source language and use professional translators to generate a reference translation in the target language. Throughout the paper, we will refer this type of test sets as source-original as the source sentences have been crawled from the web while the reference translations are added later. For most of the domains, we also have a target-original counterpart which is generated in the opposite direction: Sentences are crawled in the target language and human translated into the source language. Earlier work [ differentiate between the two different kinds of test sets as the style of natural sentences and human (or machine) translations (translationese) is quite different. Cross-entropy loss is evaluated on the different test sets during training. To reduce the variation caused by the parameter ﬂuctuations at the end of the training, we present the median loss over the last Figure 1 shows the empirical evolution of the test loss on the Web-Domain test sets for encoder and decoder scaling for English in the literature for decoder only models [21, 16], we have ﬁtted a power law of the form A complete description of the model architecture is provided in Appendix A Transformer architecture consists of Transformer Blocks: a cascade of self-attention, of Transformer Blocks in the encoder and decoder being equal. 6] evaluation campaign (newstest2019) for all language pairs. The other test sets are to the data. {α, p, L different effects on the test loss compared to scaling the decoder. As such, simple power-law relations similar to Eq. behavior of the model. Figure 1: Evolution of the test loss as a function of the total model parameters for English Scaling the encoder has different effects compared to scaling the decoder. As such, traditional power laws of type expressed as in Eq. (100 ×explained variance Proposed Scaling Law decoder nature of the architecture as well as the bilingual format of the data. Let the number of non-embedding parameters in the encoder and the decoder respectively. Then, our proposed scaling law has the form where{α, p parameters corresponding to the number of encoder / decoder parameters in our baseline encoder-decoder model. compared to the baseline model) that one can hope from scaling, while exponents for encoder and decoder respectively. L Figure 2 presents the ﬁt achieved by the proposed scaling law on Web-Domain test sets. The dashed lines describe the ﬁt of the scaling law given in Eq. data. The plots suggest that our proposed scaling law is able to simultaneously capture both encoder and decoder scaling behaviors. To validate the (out-of-sample) prediction power of these scaling laws, we compare their predictions with empirical loss values achieved by our symmetric scaling models. Figure 3 presents this comparison. The plots suggest that the predictions of the scaling law match the empirical (out-of-sample) results with remarkable accuracy. These results suggest that the predictions of the scaling law are not sensitive to the scaling approach; the scaling law ﬁtted on encoder / decoder scaling data is able to almost perfectly predict the scaling behavior of symmetric models. Notice that the parameter range of symmetric scaling models is much larger than either of the encoder or decoder scaling models. Nevertheless, our ﬁtted scaling laws are able to extrapolate effectively to models with sizes beyond the ones used for ﬁtting them. Figures 2 & 3 suggest that the functional form proposed in Eq. of English Details of the curve ﬁtting procedure are presented in Appendix E. Corresponds to 6-layer encoder - 6-layer decoder. Here,Nis the total number of parameters outside of embedding / softmax layers and }are the ﬁtted parameters of the power law. As Figure 1 suggests, scaling the encoder has (1)that only consider the total number of parameters, fail to capture the correct scaling total variance) and maximum absolute deviation (k · k) are reported for each ﬁt. , p, L}are test set speciﬁc (ﬁtted) parameters.¯Nand¯Nare ﬁxed normalization →German models accurately. To verify the robustness of our proposed scaling law, we Figure 2: Evolution of log-perplexity as a function of the model size for English (2)is jointly ﬁtted to the empirical loss values from encoder scaling and decoder scaling experiments. Our proposed scaling law is able to capture more than some ﬂuctuations around the predicted trend (with estimated standard deviation of the randomness in the training pipeline (see Appendix C). We observe similar results for our other test sets (see Figures 12 & 13 of the appendix). Figure 3: Comparison of the (out-of-sample) predictions of the scaling law with the empirical test loss values from symmetric scaling English / decoder scaling data and then just evaluated on the symmetric scaling model parameters. Our proposed scaling law is able to almost fully capture the variation in the data ( though it has not been ﬁtted on it. To examine the randomness in the results, we have repeated a subset of training runs with similar results for our other test sets (see Figure 14 of the appendix). evaluate it on an additional translation task namely, German corresponding scaling behavior on Web-Domain test set. Similar to the En functional form is able to closely capture the scaling behavior of the models. The above results suggest that scaling law formalized in Eq. Transformer NMT models in multiple language pairs. As such, we can study the ﬁtted coefﬁcients to fully understand the scaling properties of these models. Figure 5 presents the ﬁtted coefﬁcients for all of the test sets under consideration. Several observations are in order: Decoder vs Encoder Scaling: larger than the encoder exponents, it is much more effective to scale the decoder rather than the encoder. This is contrary to the usual Figure 4: Fitted scaling laws for De empirical loss value from encoder scaling and decoder scaling experiments. Similar to En the law is able to describe the empirical scaling behavior of the models with high accuracy. See Figures 15 & 16 in the appendix for the ﬁt on other test sets. practice; due to latency considerations, many practitioners train NMT models with deep encoders and shallow decoders [ reduction. Proposition 1 below leverages Eq. in between the encoder and decoder optimally. The proof is presented in Appendix D. Proposition 1 LetBdenote the budget for total number of parameters. Then, the optimal encoder / decoder sizes (denoted respectively by N In addition, when optimally scaling the model, the scaling law reduces to: Figure 5: Fitted scaling law exponents for all our test sets. Across all the test sets under consideration, we observe p Proposition 1 suggests that when task. Inspection of the functional form of Eq. scales (i.e. the encoder and decoder grow proportionally together), the optimal scaling exponent, examine how signiﬁcant this sub-optimality can be, in Figure 6, we compare the multiplicative constants resulting from proportional scaling of the encoder and decoder with different values of (Optimal Scaling).Assume the loss performance of the model is described by Eq.(2). ), can be achieved, albeit with a potentially sub-optimal multiplicative constant,α. To N/N. The results suggest that as long as the parameter allocation is not extremely far from encoder and decoder layers, which yields scaling scheme described in Proposition 1. In contrast, lopsided models which heavily favor the encoder or the decoder achieve a much worse multiplicative constant. Figure 6: We use our ﬁtted scaling laws to evaluate the effect of encoder / decoder parameter allocation ratio when proportionally scaling the encoder and the decoder. Left: parameter allocation schemes. Right: The predicted additive loss penalty, with 5 × 10 Translation deals with the problem of mapping a sequence in one language into another language. A good translation should not only be adequate and ﬂuent, but should ideally also adopt the style of a sentence naturally written in the target language. This necessitates MT models to make sense of natural looking inputs and generate natural looking outputs. As mentioned in Section 2, the examples used to train or test NMT models carry a critical bias, which we refer to as composition bias. Composition bias is introduced because of the unavailability of source-target examples (pairs) that are both natural in the accessible data generating distribution. For any given naturally generated text in a language, the corresponding text in the other language is either translated by humans, introducing translationese bias or translated by other machine translation systems, introducing MT bias. We consider both biases affecting the problem from a similar angle, hence we bundle them and call it composition bias. While machine translation by design has composition bias in the training/test sets employed [ composition bias in scaling and identify critical factors playing role. We caution the reader to not take the composition bias as a problem speciﬁc to NMT. In fact as most training corpora in NMT are web-crawled, they can contain machine translation output on either the source or target side. Considering the growth of generated content in the web by machine learning models learning models is going to be biased by other models that are continuously generating content. The Effect of Test Set Construction: on the test sets used in this study and then investigate the inﬂuence on the training set. Figure 5 shows the ﬁtted scaling law coefﬁcient for all of our test sets. The coefﬁcients suggests that the scaling powers for source-original test sets are drastically different from those of target-original test sets. This behavior is in direct contrast with language modeling setting [ evaluation on different test sets merely acted as a scaling penalty that only changed the multiplicative constants of the scaling law. To elucidate this phenomenon further, in Figure 7, we compare the scaling trends for different source and target original test sets. To factor out the effect of the data domain, we present one source original https://openai.com/blog/gpt-3-apps/ https://blog.google/products/translate/one-billion-installs/ ), the scaling scheme is approximately optimal. In particular, symmetrically scaling the total (non-embedding) parameters. Each line corresponds to a different test set. 10,30], its effect on model scaling is unknown. In this section we investigate the role of , it is not improbable that a proportion of the content collected and used by machine and one target original test set for each domain. Several observations are in order: Test sets with a similar composition approach (source or target original) have a qualitatively similar scaling behavior. However, scaling behavior is vastly different between the two composition approaches. Reducible loss quickly decays to zero for source original test sets. In fact, we observe that scaling our baseline 6L-6L model by a factor of source original test sets. In contrast, on target original test sets, the loss decays much more slowly with model size. For comparison, to ensure that reducible loss is below sets, we estimate that the baseline model has to be scaled up by a factor of 11. Figure 7: A comparison of scaling behavior across source and target original test sets. We use our ﬁtted scaling laws to estimate the evolution of reducible loss for each test set. All scaling trends correspond to symmetrically scaling the encoder and decoder layers. Because of this behavior, the value of larger models in NMT is closely tied to their evaluation sets: On source original test sets, due to larger scaling exponents, even moderate increases in model size are sufﬁcient for pushing the reducible loss close to zero. Hence, beyond a few hundred million parameters, there is no beneﬁt in increasing the model size. In contrast, for target original test sets, which generally have smaller scaling exponents, large models are needed to push the reducible loss to zero. The Effect of Training Set Construction: construction of the test data plays a key role in the scaling behavior of the model. Now, we brieﬂy examine the role of En→De datasets, that were not used in the previous experiments. One fully target original and another completely source original. To generate the target original dataset, we compile a set of German documents from the web. Documents are screened to ensure the data is not machine generated. We use a Hybrid model (with 380M parameters) [ source original data, we collect human generated English documents and (forward) translate them to German using a hybrid model (with approximately 327M parameters). Both datasets provide us with approximately 2.2 billion training examples. We mimic the experimental setup of Section 2. Note that even though these datasets are not human generated, they reﬂect important aspects of training large NMT models. Many modern NMT datasets are harvested from the web and as a result, are contaminated with machine generated data. Moreover, many popular data augmentation algorithms such as Back Translation [ purposefully add machine generated data into the training pipeline in order to take advantage of monolingual data. Figure 8 describes the scaling behavior for models trained on target-original data. We observe that even though larger models are successful in reducing the training loss, they are unable to improve the test loss after roughly the training data and the test loss starts to deteriorate across all of our test sets. We hypothesize that this size threshold corresponds to the capacity of the original back-translation model. This assertion suggests that in order for back-translation to be beneﬁcial for training large models, it has to be performed with a models with comparable capacity or higher. Although quite intriguing, we leave the veriﬁcation of this hypothesis to future work. Figure 8: Scaling behavior of models trained on back-translated data. Right: Increasing the model size successfully reduces the loss on the training distribution. However, on the test data (left and center) the loss increases after approximately 400M parameters. Figure 9 paints another interesting picture for the models trained on the source-original data only, implying the target side having the composition bias, expected to be simpler, dull and not rich in its content, in short - not natural looking. As experiments suggest, even our smallest models are able to achieve extremely low loss values (roughly the same phenomenon is also related to the "data simpliﬁcation" effect seeked by non-autoregressive models in NMT [40]. Figure 9: Scaling behavior of models trained on forward translated data. Left / center: early stopping test loss on Web-Domain. Right: loss at the end of the training for a subset of the training data. We examine the effects of scaling on the output quality as measured by BLEU score analysis of this section, we focus on output generated via beam search [ we do not attempt to tune the (many) hyper-parameters of beam-search for each model. Instead, we use the conﬁguration optimized for the baseline model (listed in Appendix F) in all the decoding tasks. Figure 10 presents the co-evolution of BLEU score and cross-entropy loss throughout the training for all of our models. Depending on the construction of the test sets, two different empirical behaviors emerge. On target-original test sets, larger models are able to improve (lower) the test loss. These improvements in the loss are accompanied with consistent improvements (increases) in BLEU score. In fact, we observe that a simple power law of the form can capture the relationship between BLEU score and cross-entropy loss for high-quality models. We computed the BLEU scores using an internal reimplementation of Moses scorer: mteval-v13a.pl. We observe certain deviations from this trend for smaller models and for early checkpoints. We document these deviations in Appendix F. In contrast, on source-original test sets, this relationship is absent; larger models consistently achieve better test losses, however, beyond a certain threshold, BLEU scores begin to deteriorate. Figures 21 and 22 exhibit that this phenomenon is not due to over-training; the BLEU score gap between large and small models is persistent throughout the training. To ensure that this observation truly reﬂects the generation quality of the models (as opposed to potential biases of BLEU score), we repeat our analysis with BLEURT score [ presented in Figure 11. As the ﬁgure suggests, BLEURT scores closely mirror the behavior of BLEU scores with respect to model scaling. A careful look at the left-subplots of Figures 10 & 11 brings up another interesting trend. At similar values of the test loss, encoder-scaled models result in better generation quality compared to decoder-scaled models. This ﬁndings agrees with previous work that relied on encoder-scaling when optimizing for BLEU and inference latency [ encoder-scaling and decoder-scaling are caused by insufﬁcient search algorithms, or just different model ﬁts from different architectural priors is left to future work. Figure 10: Log-log plot of the evolution of BLEU score as a function of cross-entropy loss for different models. For each scaling approach, warmer colors represent larger models. Each individual color represents different checkpoints of a single model during training. On target original data (left column), improvements to cross-entropy loss lead to consistent improvements in BLEU score. Dashed lines correspond to ﬁt achieved by Eq. data (right column). More examples of this phenomenon are presented in Appendix F. In this work we have attempted to quantify the evolution of model quality as a function of model capacity for encoder-decoder NMT models. While a univariate scaling law describing the cross-entropy as a function of the total number of parameters in the model is insufﬁcient, a bivariate law treating the number of encoder and decoder parameters as separate variables adequately describes the scaling behavior of these models under various scaling strategies. We validate this behavior on 2 language pairs and on a variety of evaluation sets with different compositions. Whether this behavior is intrinsic to the encoder-decoder architecture, or arising from the nature of the NMT task, requires further study. Next, we demonstrate that this scaling behavior is highly dependent on the composition of the evaluation data, speciﬁcally on whether the source or the target sentences are “original”. Our ﬁndings Figure 11: The evolution of BLEURT score as a function of cross-entropy loss for different models. For each scaling approach, warmer colors represent larger models. Each individual color represents different checkpoints of a single model during training. On target original data (left column), improvements to cross-entropy loss lead to consistent improvements in BLEURT score. The relationship breaks down for source original data (right column). More examples are provided in Appendix F. indicate that target-original evaluation sets continue beneﬁting from model scaling throughout our range of measurements, while the reducible error on source-original evaluation sets quickly saturates to0. This could be an artifact of the lack of diversity in translated text; a simpler target distribution doesn’t require much capacity to model while generating ﬂuent or natural-looking text could beneﬁt much more from scale. We also study how the composition of training data affects the scaling behavior of models. When training on target-original (back-translated) text, model quality keeps improving until a point after which the trend saturates. In our study the capacity where saturation manifests ﬁrst is perilously close to the capacity of the model used for back-translation, indicating that the capacity of the generative model used to generate synthetic text might have a role to play, but this requires further investigation. When training on source-original text, even low-capacity models are sufﬁcient to reach the irreducible loss region, painting a gloomy picture for synthetic data. While we have explored these ideas in the context of machine translation, given the proliferation of generative models this problem will likely be a challenge for future practitioners training on web-scraped monolingual datasets as well. For low-resource languages, the proliferation of machine translated text is already a problem given that a signiﬁcant portion of web text in these languages is machine translated. Finally, we attempt to understand how generation quality evolves with the improvements in crossentropy resulting from model scaling. As with our previous ﬁndings, dataset composition plays a major role in determining the trends. For source-original evaluation sets, the correlation between cross-entropy and generation quality breaks down. On target-original evaluation, we observe an inverse correlation between cross-entropy and BLEU/BLEURT, suggesting that improved model ﬁt results in a corresponding improvement in generation quality. The slope of this relationship is different for encoder-scaling and decoder-scaling, with encoder-scaled models performing better on BLEU/BLEURT than decoder-scaled models, at the same level of cross-entropy loss. Whether this is an artifact of our search strategy (beam search, tuned to a 6L-encoder 6L-decoder model) or the difference in architectural priors is something that requires further investigation. Our ﬁndings suggest that scaling behavior of encoder-decoder NMT models is predictable, but the exact formulation of scaling laws might vary depending on the particular architecture or task being studied. Our empirical ﬁndings also raise concerns regarding the effect of synthetic data on model scaling and evaluation, and how proliferation of machine generated text might hamper the quality of future models trained on web-text. We would like to thank the Google Translate and Google Brain teams for their useful input and discussion, and the entire Lingvo development team for their foundational contributions to this project. We would also like to thank Kyunghyun Cho, Ethan Dyer, Yasaman Bahri and David Luan for their insightful comments.