Noname manuscript No. (will be inserted by the editor) Abstract Mining data streams poses a number of challenges, including the continuous and non-stationary nature of data, the massive volume of information to be processed and constraints put on the computational resources. While there is a number of supervised solutions proposed for this problem in the literature, most of them assume that access to the ground truth (in form of class labels) is unlimited and such information can be instantly utilized when updating the learning system. This is far from being realistic, as one must consider the underlying cost of acquiring labels. Therefore, solutions that can reduce the requirements for ground truth in streaming scenarios are required. In this paper, we propose a novel framework for mining drifting data streams on a budget, by combining information coming from active learning and self-labeling. We introduce several strategies that can take advantage of both intelligent instance selection and semi-supervised procedures, while taking into account the potential presence of concept drift. Such a hybrid approach allows for eﬃcient exploration and exploitation of streaming data structures within realistic labeling budgets. Since our framework works as a wrapper, it may be applied with diﬀerent learning algorithms. Experimental study, carried out on a diverse set of real-world data streams with various types of concept drift, proves the usefulness of the proposed strategies when dealing with highly limited access to class labels. The presented hybrid approach is especially feasible when one cannot increase a budget for labeling or replace an ineﬃcient classiﬁer. We deliver a set of recommendations regarding areas of applicability for our strategies. Keywords Machine learning · Data stream mining · Active learning · Semisupervised learning · Self-labeling · Concept drift. Contemporary data sources generate new information at both tremendous size and speed. Therefore, modern machine learning systems must deal not only with the volume but also velocity issues [46]. Stock exchange, sensor networks, or social media are among the examples of scenarios in which new instances continuously arrive at high speed over time, creating a demand for adaptive, real-time data mining algorithms. This has led to the emergence of data streams notion and the development of a family of dedicated algorithms. New challenges needed to be addressed, such as the potentially unbounded size of the data that may quickly overﬂow computational resources [10], learning schemes that are able to use both new and historic instances, as well as approaches for managing the evolving nature of data [36]. A phenomenon known as concept drift is embedded in the streaming scenario, as characteristics of data may change over time [17]. allowing for eﬃcient classiﬁcation and prediction from non-stationary data [13]. However, the vast majority of research in this area assumes that class labels become available right after the incoming instance was processed. Then a label is obtained and used to update the learning system. While this is true to the overall stream mining principles, it completely neglects the issue of how to actually obtain every single label. If we would have access to a theoretical oracle that would provide us with such information every time we query for it, then what is the purpose of a classiﬁcation system? In reality, a class label should be provided by a domain expert in order to maintain the highest certainty with regard to the used information. However, an expert’s service is related to the cost, both in monetary and time terms [47], and thus cannot be called upon every time a new instance becomes available [4,32]. This holds especially true for massive and high-speed data streams (e.g., Twitter can produce around 350 000 new tweets every 60 seconds). Therefore, methods that would allow for mining data streams on a budget are in high demand [41]. Summary. We propose a novel hybrid framework for low-cost data stream mining. It is based on combining active learning and self-labeling for using available limited labels most eﬀectively. These two strategies work in a complementary fashion, boosting their advantages while minimizing their weak sides. Active learning allows for an informative selection of instances that will be most useful for adjusting the classiﬁer to the current state of the stream. However, each such query reduces the available budget. Self-labeling exploits discovered data structures and improves the competency of a classiﬁer at no cost, yet oﬀers no quality validation. The proposed hybrid approach allows active learning to explore the incoming data stream for new emerging concepts, while self-labeling oﬀers their further exploitation without depleting the budget. We introduce 7 hybrid strategies, divided into two groups: blind and informed. The former one consists of methods that use the classiﬁer output for deciding if a new instance should be used for active learning or selflabeling. The latter one combines the information from both the classiﬁer and drift detector modules to better adapt decisions to the evolving nature of data streams. The analysis of the performance of these proposed hybrid algorithms sheds light on their areas of applicability, as well as allows us to identify shortcomings of state-of-the-art active learning methods. Supervised learning has gained signiﬁcant attention in data stream mining, Main contributions. This work oﬀers the following contributions to the data stream mining domain. – A novel hybrid framework for mining data streams on a budget, using the – Two families of methods based on blind and informed approaches, leading to – Thorough experimental study on real data streams under various labeling bud- – Analysis and recommendations on areas of applicability of the proposed algo- A data stream can be deﬁned as a sequence of ordered instances arriving over time and of potentially unbounded volume. Mining data streams imposes certain speciﬁc requirements on used classiﬁers. Contrary to static scenarios a predeﬁned training set is not available, as instances become available at given time intervals one by one (online case) or in form of chunks (block case). The entire stream cannot be stored in memory due to its unknown and ever-expanding size. Thus only a limited number of most recent instances can be stored, while the old ones are to be discarded to limit the computational resources being used. Characteristics of the stream may evolve over time and this must be accommodated during the continuous learning process. element S each of them being independent and randomly generated according to a probability distribution D then we deal with a stationary stream. However, in most real-life scenarios incoming data is subject to change, leading to the notion of non-stationary streams and concept drift [17]. As concept drift may aﬀect various properties of the stream, therefore we may analyze it from various perspectives. When taking into account the inﬂuence on learned decision boundaries, one may distinguish between real and virtual concept drift. The former has an eﬀect on posterior probabilities and may impact unconditional probability density functions. This forces the learning system to adapt to change in order not to lose competence. The latter drift does not have any eﬀect on posterior probabilities, but only on conditional probability density functions. It may still cause diﬃculties for the learning system, leading to false alarms and unnecessary computational expenses due to rebuilding the classiﬁer. Another view on the concept drift comes from the severity of ongoing changes. Sudden concept drift appears when S are generated by a mixture of D changing. Incremental concept drift is characterized by a smooth and slow transition between distributions, where the diﬀerences between D signiﬁcant. Additionally, we may face recurring concept drift, in which a state combination of active and semi-supervised learning. seven algorithms for empowering active learning with self-labeling. gets, showcasing the advantages of using hybrid solutions when available class label are scarce, especially in extremely small budget cases. rithms. More technically, a data stream is a sequence < S, S, ..., S, ... >, where each is a set of instances (or a single instance in case of online learning), 6= D. Gradual concept drift is a transition phase where examples in S from k-th previous iteration may suddenly reemerge D take place once or periodically. presence of concept drift and have embedded a solution to tackle it [13]. Three general possible approaches include: (i) rebuilding a classiﬁer from scratch whenever new instances become available; (ii) using a mechanism to detect change occurrence and guide the rebuilding process more eﬀectively; and (iii) using an adaptive classiﬁer that will adjust automatically to the current state of the stream. The ﬁrst approach is impractical, due to prohibitive costs connected with continuous model deleting and rebuilding. Therefore, the two remaining directions are commonly utilized in this domain. Concept drift detectors are external tools that allow for monitoring the characteristics of a stream, in order to anticipate and detect the change point [24]. Adaptive classiﬁers are based on either sliding windows [43] or on online learners [40]. Finally, ensemble solutions are popularly used for mining drifting data streams [26]. While supervised learning from drifting data streams has gained signiﬁcant attention in recent years, the vast majority of developed algorithms assume working in test-then-train mode. This means that each new instance (or chunk) is ﬁrst used to test the current classiﬁer and then utilized for the updating procedure. This scheme easily translates into real-life scenarios, when we use the model at hand to predict labels for new data and then try to conduct the learner update. However, many of these algorithms work under the assumption that true class labels become available the moment after we obtain predictions from the model. While such a set-up is convenient for an experimental evaluation of a classiﬁer in the controlled environment, it completely fails to capture the real-life situation of dealing with limited ground truth availability [30]. Therefore, many supervised classiﬁers, while delivering excellent performance on benchmark data, cannot be directly used in practical applications [26]. is far from being a trivial task. If we would have access to a theoretical oracle that would provide us with such information every time a new instance becomes available, then there is no need to have any classiﬁcation procedures. In most real-life applications, a domain expert is required to analyze a given instance and label it. While one may theorize that a company developing a speciﬁc data stream mining system should have such an expert at its disposal, we cannot forget the costs connected with such a procedure. This can be viewed as monetary costs, as an expert would require a payment for sharing his knowledge, as well as a time cost, as an expert needs to spend some time analyzing each instance. Therefore, in a real-life scenario, neither a constant label query is possible (as a given company would quickly use up its budget), nor instant label availability [20]. Even if these factors, for various reasons, play a less important role, the human throughput must also be considered. A given expert cannot work non-stop and will have limited responsiveness per given time unit. Thus, in cases of massive and high-speed data streams assumption of continuous label availability cannot hold. Learning systems designed for data stream mining must take into account the This is caused by the fact that obtaining a true class label for a new instance at no cost. Let us consider stock exchange or weather prediction. Here one may observe the current state of the environment in order to verify the prediction. However, one still cannot assume that the ground truth will become instantly available. Usually, such predictions are being made forward into the future, whether we consider hours, days or months. Therefore, a given amount of time must pass before a ground truth can be observed. Although here we do not have the problem of associated cost, we still need to deal with label latency [33]. class labels. Most popular approaches include active learning solutions that allow selecting only a limited number of instances for labeling [25]. They are usually selected to oﬀer new information to the classiﬁer, instead of reinforcing old concepts [31]. Although there is a good amount of research on active learning for static scenarios [35], there exist but a few solutions that take into account the drifting and evolving nature of streams [22,41]. Another branch of works focuses on semisupervised learning [32], usually using clustering-based solutions [28]. The most common assumption here is that with every time interval a subset of instances arrives as labeled and one may use them to guide the learning process. This is fundamentally diﬀerent from active learning, as it concentrates on how to make use of unlabeled instances, instead of selecting the proper ones for labeling. There also exist algorithms working under the assumption that only initial data is being labeled and no further ground truth access is to be expected [15]. to better exploit the given budget and take advantage of unlabeled instances without paying the cost for querying them are highly attractive for data stream mining [45]. Therefore, we focus our attention on combining active and semi-supervised [39] learning paradigms. Many hybrid frameworks have been proposed for batch mode, oﬄine settings and successfully applied to diﬀerent domains. Combining self-labeling based on uncertainty with active learning strategies is just one possible approach, where the least certain instances are chosen to be queried and the most certain are labeled by a classiﬁer on its own. Graph-based methods build a similarity graph to determine which samples are the most informative and should be queried. The labeled objects are then used to propagate their classes to the closest neighbors in the graph. recent idea and yet weakly explored. As far as we know, while there is a lot of pure active learning [29,38] or semi-supervised learning [34,44] methods for streaming data, very few hybrid solutions have been proposed. One of them uses variance reduction on data chunks as an active learning strategy and the same instance selection method as a self-labeling step [21]. Another solution is based on the incremental graph building algorithm – Adaptive Incremental Neural Gas (AING) – which provides information needed to decide if a sample should be queried or if a classiﬁer can teach itself with it [9]. An interesting framework was proposed in [18]. It is described as a highly scalable, parallelizable and optimized online Bayesian framework using sequential Monte Carlo and so-called gap assumption. In our previous work, we have proposed the ﬁrst study on a combination of active and self-labeling solutions for data streams [23]. However, the algorithm described there used a ﬁxed threshold and therefore was unable to eﬃciently adapt to the There are speciﬁc application areas, where true class labels can be obtained This has led to works on learning from data streams under limited access to As we deal with limited access to class labels, any techniques that may allow us The area of hybrid frameworks for evolving data streams mining is a relatively presence of concept drift. Addressing this limitation is the cornerstone of the study proposed in this manuscript. we investigate a framework that oﬀers a combination of active learning with different self-labeling strategies based on uncertainty thresholding in the presence of concept drifts and under labeling constraints. While most of the presented solutions take for granted ﬁxed thresholds, which are core elements of proper decisions, we focus on changing the self-labeling threshold adaptively to an incoming data stream. 4 Combining active learning with self-labeling for drifting data streams The reduction of the labeling cost can be intensiﬁed using diﬀerent methods concurrently. Both active learning and semi-supervised learning algorithms aim to enhance the learning process when few labeled instances are available and there is a vast number of unlabeled samples. However, they focus on generally disjunctive facets of data utilization. The former group of methods uses limited supervision for selecting only the most informative samples to be labeled, while the latter exploits already gained knowledge to adjust a model more properly in an unsupervised manner. Therefore, active learning can be interpreted as an exploration process, since it asks for the unknown, and semi-supervised learning as an exploitation step – it exploits the known [37]. Fig. 1: The idea of combining active learning and self-labeling. (Top) The original linear model; (middle) the decision boundary updated after utilizing labeled instances selected by the active learning query; (bottom) self-labeling improving the decision boundary at no additional labeling cost, due to smoothness assumptions being fulﬁlled. Due to the lack of comprehensive works on hybrid frameworks for data streams, in which the decision boundary is successively moved away from the green cluster’s center. At ﬁrst, the labeled instances selected by the active learning algorithm delineate a general decision boundary. Since this dataset fulﬁlls the smoothness assumption [12], during the self-labeling step it is more likely that objects will be taken from the dense cluster on the left, so the classiﬁcation will be correct and the border will be moved in the right direction at no additional cost. This can be explained by the fact that active learning itself, especially on low budgets, will not be able to exhaustively sample the properties of each class. Therefore, we may deal with the issue of underﬁtting, where we do not have enough labeled instances to correctly estimate the decision boundary. Self-labeling allows for correcting this problem by using the small number of labeled instances to enrich the training set for each of the classes. We ﬁnd it intuitive to consider some hybrid frameworks which oﬀer a hybridization of both techniques to reduce the overall amount of labeled instances needed for an eﬀective model construction under drifting data streams. In this section, we present our generic online hybrid framework for active and semisupervised hybridization. The framework consists of four ﬂexible components. – Adaptive classiﬁer: a backbone of our framework that can be realized by – Concept drift detector: a module for monitoring the changes in the stream – Active learning strategy: a module for selecting the most valuable instances – Self-labeling strategy: a module allowing for handling the underﬁtting of Adaptive classiﬁer is being updated with incoming data samples. It is important that the algorithm should be able to work in an online setting since the whole framework is designed to do so. Another constraint is that a classiﬁer should return a probabilistic outcome or values which can be approximately treated as the posterior probability. Some of them are Na¨ıve Bayes or k-NN algorithm, to name a few. The state-of-art classiﬁer, which is able to classify data streams eﬃciently and is responsive to a non-stationary distribution, is Adaptive Hoeﬀding Tree (AHT) [8]. It is a modiﬁcation of the incremental decision tree called Very Fast Decision Tree (VFDT) [14] which uses the Hoeﬀding bound to expand its structure. AHT applies adaptive windows on every node to adjust local statistics independently and replace outdated nodes, so it is able to adapt to concept drifts. There are a few other variants of Hoeﬀding Tree - one of them is Randomized Hoeﬀding Tree (RHT) [3] that introduces additional randomization to the learning process. More complex classiﬁers like online ensembles can also be used in our framework. The idea behind such a combination is depicted in Fig. 1. It depicts a situation any learning algorithm capable of incremental/online processing of instances. and informing our framework when it is necessary to take a drifting nature of a distribution into account. for the label query process that allows us to obtain labeled instances under the given budget constraints. current concepts that uses instances acquired by active learning to increase the size of labeled training set at no additional cost imposed on the budget. For example, there is Recurring Concept Drift framework (RCD), which handles drifts by storing a separate base learner for each of distinguished concepts [19], or Accuracy Weighted Ensemble (AWE), which maintains a committee of models that are weighted by their accuracy on incoming data [42]. We ﬁnd our hybrid framework for learning on a budget highly ﬂexible, so it can be adjusted to many real-life data stream mining scenarios. Concept drift detector is responsible for providing indications of changes in a data stream. The information can be used by diﬀerent strategies to adjust their parameters and boost the overall performance. The most popular drift detectors are Drift Detection Method (DDM) [16] and Early Drift Detection Method EDDM [6], which will be described later. Active learning strategy can be realized via any online strategy from the group of stream-based selective sampling methods [37]. In our work, we incorporate an uncertainty sampling strategy [5] which makes its decision based on some uncertainty measures – usually, they use a maximum a posteriori probability or an entropy. The measure is compared with a threshold. If it is low enough, a sample should be queried. For the former measure the inequality is given as follows: where X is a vector of an instance’s features, p(y|X) is the conditional class probability and θ is the threshold. The lower the threshold, the less certain samples are chosen to be queried. Therefore, lower values are preferred. However, one should notice that too strict conditions may favor only those objects which are close to a decision boundary, so changes that occur far from the boundary may never be detected strategies has been comprehensively studied in [41]. We construct our active learning module based on the results from this work. One of the proposed reliable strategies is Variable Uncertainty Strategy with Randomization (RandVar). In Alg. 1, each incoming instance X is forwarded to a classiﬁer L which returns posterior probabilities for the sample. The uncertainty measure is determined through the maximum a posterior rule. There is a threshold θ that is used for decision making. These are usual steps. The strategy introduces two additional operations. Algorithm 1: Variable Uncertainty Strategy with Randomization The issue of determining the appropriate threshold for uncertainty sampling is to lower the value when a concept drift occurs and the classiﬁer returns more uncertain labels more frequently; and to increase it when there is a stable period and the model is more sure about its decisions. This step leads to a more balanced budget spending, so the labeling is more uniformly distributed over time. Secondly, the threshold is randomized by multiplying it by a variable drawn from the Gaussian distribution with mean m = 1 and predeﬁned variance σ. This step ensures that a decision space is sampled more uniformly since the instances which are far from the decision boundary are more likely to be queried. We ﬁnd the strategy universal and according to the presented results reliable, so we choose it as our base active learning strategy. We do not perform any further investigation on the matter. Instead, we focus on selecting the appropriate self-labeling strategy which will cooperate well in the hybrid framework. Self-labeling strategy complements the selected active learning strategy, acting as our semi-supervised learning approach. We assume that unsupervised learning based on the knowledge provided by the queries will lead to more exhaustive and accurate adaptation on the same budget. In addition, we preconceive that the active learning strategy will guide self-labeling properly, thus reducing the risk of error ampliﬁcation. This module should also work in an online setting. Relating to the previous method, we propose diﬀerent strategies for changing the self-labeling threshold respectively to an incoming data stream. We investigate selfreliant approaches, as well as, using a feedback from the active learning strategy and drift detectors. To the best of our knowledge, such research has not been done before. Algorithm 2: The generic hybrid algorithm for combining active learning and self-labeling For each incoming sample, the actual budget spending tentially inﬁnite streams it can be estimated as a ratio of already labeled instances to all received samples. If the value does not exceed a given budget B and a response from the active learning strategy (QueryStrategy) is positive, the sample is queried. After receiving a label, the expenses, drift indications and classiﬁer are Firstly, it modiﬁes the threshold value with an adjustment step s. The idea The proposed generic algorithm is presented in pseudo-code form as Alg. 2. updated. If the query strategy rejects the sample, it still can be subject to the self-labeling approach (SelflabelingStrategy). This depends on the response of the second strategy we add to the classiﬁcation ﬂow. The method may also be optionally supplied with a feedback from the active learning strategy or the drift detector. It is worth saying that the chosen strategies and their measures are, in fact, complementary. Very uncertain data samples will be forwarded to an expert and, obviously, they will not be used for the self-training. On the other hand, the algorithm will discard the objects for which it will be very conﬁdent and will use them for the training without additional supervision. We put the active learning step before the self-labeling, because we assume that the latter requires suﬃcient guidelines from the former, thus it is better to spend the whole available budget. Proposed self-labeling strategies for the hybrid online approach are described in the next sections. The ﬁrst group of strategies are blind approaches [17]. These methods do not use any explicit information about changes that occur in a data stream. They make some assumptions and use heuristics to handle data evolution. Classiﬁers, which are guided by them, adapt gradually to concept drifts, slowly forgetting old concepts and learning new ones. It takes time, as there is no clear and direct indication of a change. Some blind strategies for active learning were proposed in the mentioned work [41]. Another one is, for example, the adaptation method used in VFDT that incorporates a sliding window technique for its statistics [14]. The ﬁrst and the simplest blind strategy we use in our hybrid framework is based on a ﬁxed threshold – we call it Fixed strategy. To determine a conﬁdence measure, which is compared with the self-labeling threshold γ, the maximum a posteriori rule is used, since it returns a value that is chosen by a model to classify an instance: The inequality is opposite to the active learning condition. It prefers those samples to which a classiﬁer displays the highest certainty, so there is a higher possibility that a classiﬁer’s own decision will be correct. This strategy is the most popular one in semi-supervised streaming frameworks discussed before, as the research on the self-labeling module is usually very limited. The main weakness of the previous strategy is the need to predeﬁne the threshold value. During a concept drift reliability of a classiﬁer may signiﬁcantly change, so a distribution of posterior probabilities will also be diﬀerent – there will be more lower values than during a stable period. It is not obvious which values and when can be denoted as high enough or even that they should be actually high. ﬁrst one is Uni. It incorporates the idea of balancing a budget spending over time (Alg. 1). Since there is no budget for self-labeling, we simply aim to ensure that the process will be uniformly frequent, regardless of a stream’s state. Algorithm 3: Uni self-labeling strategy conﬁdent, so we prevent it from being overﬁtted. On the other hand, we decrease the threshold when some changes occur and decisions are less conﬁdent, therefore, the model should be suﬃciently supplied with additional labels and adjust itself to changes faster. RandVar strategy (Alg. 1) with increasing and decreasing the threshold as in the previous algorithm. We assume that the randomization step may assure some helpful diversiﬁcation, especially after abrupt changes, when γ is usually far from a more stable value. The last blind strategy is Inverted Uncertainty (InvUnc). In this approach, we use the active learning threshold θ from RandVar algorithm to control the selflabeling threshold γ using a simple transformation higher during a concept drift and lower for stable periods. It is motivated by an assumption that when a concept is changing and a model is insuﬃciently adapted, there are no formed internal class structures to be exploited, so only the most certain decisions should be made. On the other hand, when a concept is stable these structures are more likely to be present, therefore, we should intensify selflabeling in order to be able to exploit them. In RandVar the threshold is higher during stable periods and lower when drifts occur. The self-labeling condition is then given as follows: where p(ˆy|X) is a maximum a posteriori probability and c is a number of possible classes. 1 − θ factor is self-explanatory. That is why we call the method Inverted Our two following strategies are based on the methods presented in [41]. The We achieve this by increasing the threshold when the classiﬁer L becomes more The second one is Randomized Uniform (RandUni) strategy. In fact, it is Uncertainty. We add 1/c factor, due to the fact that p(ˆy|X) is never lower than it, so regardless of small disturbances introduced by the randomization step for small values of s (which should be used [41]), the threshold θ is in range h We want γ to be in the same range. To improve the responsiveness of the adaptation process dedicated change detectors can be introduced. Those algorithms are directly oriented on indicating changes, thus they provide accurate information about stream evolution. Strategies that use such indicators are called informed [17]. The simplest drift detector can be based on a classiﬁcation error rate. The two most popular algorithms that have been already mentioned are DDM and EDDM. We use them as a base of our next three self-labeling strategies, in order to improve their adaptation capabilities in the presence of concept drift. In this approach, we use DDM indications to create continuous control over the self-labeling threshold. We call the method Continuous DDM (cDDM). The standard DDM is designed to indicate three discrete states of a data stream: stable, warning and change. The core assumption of this method is that an error rate p of a classiﬁer should be approximately constant and low when a concept is stable. When a drift occurs, the error should be signiﬁcantly higher than the value registered for the static distribution, since the classiﬁer has not adapted to a new concept yet. Therefore, changes can be detected by tracking the actual error rate p along with its standard deviation s and comparing it with the registered error for the stable period. The algorithm makes decisions based on the condition: where p a stable concept after at least 30 samples. The α parameter is used to determine thresholds for warning (α = 2, the conﬁdence interval is 95%) and change (α = 3, the conﬁdence interval is 99%) states [16]. Such discrete DDM can be used to reset a classiﬁer and itself when a drift occurs [41]. tively to the DDM indications, we take the whole algorithm as it is, excluding the classiﬁer reset, and we simply extract the tracked, continuous error measure  = p + s. We use this value analogously to the InvUnc idea (Sec. 4.2.3) – the threshold should be higher during a concept drift and lower during a stable period. Decisions are based on the following condition: We add 1/c to additionally penalize a situation when a classiﬁer simply guesses labels for  = 1 − 1/c. For example when  = 0.5 for two classes, then  + which is the maximum possible posterior probability returned by the model. Since Since we want to control the self-labeling threshold continuously and respecthe error can be higher than 1 − 1/c we have to normalize the overall value – we use tanh 2x for this, which is approximately correct, so γ is never higher than 1. At the same time, the lowest threshold γ is always higher than 1/c, therefore, the condition in this strategy is a bit stricter than in the others. It can be justiﬁed since for posterior probabilities close to 1/c a classiﬁer is very uncertain about its decisions. Another drift detector – EDDM – presents a slightly diﬀerent approach. We use it for our second informed strategy called Continuous EDDM (cEDDM). Instead of considering the mean error, the algorithm calculates an average distance p two misclassiﬁed objects and also its standard deviation s stable, the average distance increases due to a model adaptation process. When the concept starts to change, gaps between two mistakes become shorter. The algorithm compares the current mean distance with the maximum registered: where p errors. β is a threshold for a similarity between two error distance distributions. It is empirically recommended to set β = 0.95 for warning and β = 0.9 for change [6]. The standard EDDM is used just like DDM, including resetting. reset step and extract the similarity measure ζ One should notice that ζ ∈ h0.9, 1i, because when a drift is detected for ζ = 0.9 the EDDM is reset and ζ stays unchanged until the next update is possible. The idea of the control is the same as for InvUnc and cDDM. The condition that is used to check if we should use an instance in the self-labeling process is given as follows: where f(ζ) is any decreasing function deﬁned as: In our case we choose a simple linear function which fulﬁlls the above requirements. The last strategy is Windowed Error (WinErr). This method is, indeed, a sliding window for the mean error. Instead of resetting drift indications p and s, like in cDDM, we track the strictly continuous error that changes dynamically within the window. We use the same self-labeling condition as in cDDM. on indications generated by drift detectors. Due to the lack of a suﬃcient amount of error indicators (coming with labels), internal estimators used in the detectors will be inaccurate, impeding the process of drift detection. It may be a serious Again, to control self-labeling in a continuous way, we remove the classiﬁer It is worth noting that a limited labeling budget may have a signiﬁcant inﬂuence problem in the case of discrete (binary) drift detection when one wants to use these indications to reset a model and retrain it from scratch since if there is not enough information the detectors may never be triggered or they may act at random. However, in our work, we do not use drift detectors in such a way. Instead of working in the retraining mode, we continuously update our strategies in an informed manner using the error (DDM) or distance between errors (EDDM) as a continuous input to our self-labeling strategies. By doing this we alleviate the mentioned problem since we do not have to rely on rare, binary and unreliable drift detections. Obviously, we may still be forced to use imperfect estimations, especially for very low budgets, but it is something we have to accept, due to the assumption of strictly limited access to ground truth. There is very little that can be done without more labels. tings and it is possible that some improvements can be done in order to improve their performance while working on a budget. However, since that would require a broader in-depth statistical analysis in various scenarios we ﬁnd it beyond the scope of this work. In this section, we describe our experiments conducted to prove the validity of introduced strategies for active learning and self-labeling hybridization. Firstly, we present data streams that are used in the evaluation process. Next, we brieﬂy delineate the problem of measuring performance in a streaming data environment and describe a chosen set-up. Finally, obtained results with a commentary and conclusions are presented. Data streams. To examine the adaptation capabilities of the presented algorithms we used a set of drifting data streams. We attempted to explore our strategies in the context of a variety of data streams and concept drifts. Therefore, we utilized real streams from diﬀerent domains and with diversiﬁed properties. The real data streams allow for conducting relatively reliable tests of adaptive methods in real-world environments. They are characterized by mixed types of drifts that are coming from the underlying nature of datasets, thus leading to more realistic learning diﬃculties. Details of used real streams are given in Tab. 1. Evaluation methods. Evaluation in streaming data environments enforces a different approach than those which are used in standard batch mode scenarios. Such reliable methods like k-fold cross validation may turn out to be impractical due to its time-consuming nature. Furthermore, other techniques which are dedicated to data streams, like holdout [27], may be inappropriate for streams with concept drifts. Simpler error-estimation procedures that cope with dynamic online settings are possible. One method, which can be successfully used for the considered evaluation, is prequential evaluation p Still, the used drift detectors could be further investigated in the given setexamples [7]: where ω is the window size and L(y, y outputs y and true labels y for testing and later for learning. The chosen accuracy measure is recalculated instance by instance. It provides a relatively good measurement sensitivity, but it highly depends on the optimal window size. We chose this approach with ω = 1000 to estimate an average error within the window for time series measurements and, in addition, to calculate a global average for a whole stream. Examined strategies. In our experiments, we examined the proposed strategies: Fixed, Uni, RandUni, InvUnc, cDDM, cEDDM and WinErr on the presented real data streams, using the chosen evaluation method. As a baseline, we selected a pure active learning strategy RandVar introduced in Sec. 4.1. The variable threshold was set to s = 0.01 and the standard deviation of randomization to σ = 1. We call the strategy ALRV in the experiments. In addition we include results for random selection (ALR) [41] and sampling (ALS) [11]. For the Fixed strategy we empirically found out that very high threshold values are preferable on average, therefore we set γ = 0.95. Parameters of Uni and RandUni were the same like in ALRV. The internal conﬁguration of drift detectors for cDDM and cEDDM was set to default, as stated in [16,6], so a minimum number of registered samples was n = 30 and for errors it was also n on empirical observations that smaller windows are more reactive and accurate when a rate of information is low (a limited labeling budget), which is somehow indicated in the literature [7]. Labeling budgets. We compared eﬀectiveness of these strategies for diﬀerent but generally low and very low budgets B = {1%, 5%, 10%, 20%, 50%}. As base learners, we chose diﬀerent classiﬁers to show that the presented framework is indeed generic. We picked two single classiﬁers and two ensembles that have been already mentioned in Sec. 4.1. These are two diﬀerent Hoeﬀding Trees – AHT, RHT and two ensembles – RCD with Na¨ıve Bayes as base learners and AWE with perceptrons. They were tested with all the presented strategies The results are presented in two forms: tables and performance series. For the former, the overall average accuracy is given. The best results are in bold. In addition, cells with scores higher than the best active learning strategy are in the green color. For the graphs presenting performance series, the accuracy within the sliding window is included. These are the best AL and self-labeling results obtained for each presented case. and 5) clearly show that the proposed strategies were able to enhance the accuracy for a wide range of budgets. The single classiﬁers achieved similar performance on average – for some cases, the AHT algorithm performed better, for example, Cover, Spam or Elec, but for the rest, like Sensor or Gas, the RHT classiﬁer was more eﬃcient. However, it can be easily noticed that the latter was improved much more frequently than the former. Although several improvements can also be seen for both RCD and AWE, there are some cases – the Sensor stream, for instance – in which they were not able to learn concepts properly with or without a self-labeling module, so they perform slightly worse in general than the single classiﬁers. It is especially interesting since the diversity of the ensembles should, theoretically, provide better adaptivity. Although it may be trivial, one should also notice that the overall accuracy rises for all classiﬁers when the budget increases. Fixed or WinErr strategy improved results for almost all budgets from nearly 3% for B = 1% to more than 15% for B = 10%. One should notice that there are over 50 classes for the stream, so the improvement is very signiﬁcant. In Fig. 2 it Table 2: AHT – the average accuracy for real streams given a budget. The included results for single classiﬁers (Tab. 2 and 3) and ensembles (Tab. 4 Both single learners performed well on the Sensor stream. AHT using the Table 3: RHT – the average accuracy for real streams given a budget. Table 4: RCD – the average accuracy for real streams given a budget. can be clearly seen that for AHT with WinErr and budget values equal to 5% or 10% the accuracy within the sliding window is more frequently on a higher level than for ALR. RHT boosted the learning process for almost all strategies on 1% and 5% budget, achieving the best results on average for such settings. Neither of the committees was able to learn the concept properly. Even for the basic approaches without self-labeling they worked practically at random, so there was no chance that a semi-supervised learning step could improve something without a suﬃciently reliable model. case of AHT, the enhancement is deﬁnitely present on low budgets for all strategies, excluding Uni. Among others, the InvUnc strategy gave almost 60% gain in accuracy for B = 1% and more than 30% for B = 5%. cEDDM was able to increase the accuracy by 10% even for B = 10%. The graph for the stream and the cEDDM strategy (Fig. 2) shows that ALRV was not able to learn new concepts quickly enough for both 1% (after the second drift) and 10% (after the ﬁrst change). For B = 50% there is no noticeable gain. RHT provides slightly lower accuracy for low budgets, however, it was able to improve the learning process for the whole range of budgets when the InvUnc, cDDM, cEDDM or WinErr strategy was used. The InvUnc strategy gave almost 30% gain on B = 10%, cDDM gave nearly 20% on B = 20% and more than 12% on B = 50%, to name a few. Out of the two ensembles, AWE performed much better than RCD. The former achieved as good results as the single classiﬁers and even the best result (90.54%) overall, using WinErr on B = 50%. The latter improved adaptivity mainly for the lowest budget. and a rate of improvements, can be noticed for the Cover stream. AHT worked Table 5: AWE – the average accuracy for real streams given a budget. The most impressive improvement was obtained for the Spam stream. In the A very similar relation between AHT and RHT, regarding an average accuracy Fig. 2: Accuracy of AHT with diﬀerent strategies given a budget for the Sensor and Spam data stream. best with self-labeling approaches for B = 1%, while RHT for all budgets. RCD was able to signiﬁcantly boost the learning mainly for high values of the budget. For B = 50% almost all self-labeling strategies provided a gain from 10% to 15%. In Fig. 4 it can be seen that when B = 20% the accuracy was more frequently between 50% and 75% than in the case of AL and on B = 50% above 75% for most of the time. The ALRV strategy remained unaﬀected by the increasing budget, even if ﬁve times more labels were provided. stable for practically all strategies and budgets, providing some improvements at the same time, mainly for the Fixed and WinErr strategy. However, the boost of performance was rather minor, between 1% and 2%. When RHT was used instead of AHT it can be seen that the active learning strategies alone were not capable of maintaining relatively eﬃcient models in the dynamic environment. The accuracy dropped drastically to less than 5% and in Fig. 3 we can observe that the model is completely useless without a self-labeling module. After adding the InvUnc, cDDM, cEDDM or WinErr strategy the average accuracy rose by about 10%. In the given example for InvUnc we can see that the algorithm starts reacting to the changes even for very small B = 1%. In the case of the Power data stream, the hybrid framework using AHT was Fig. 3: Accuracy of RHT with diﬀerent strategies given a budget for the Power and Gas data stream. informed strategies are combined with both single classiﬁers. More signiﬁcant gain is present for RHT and it is another case in which a model using the last four strategies provided signiﬁcant boosts of accuracy – from 2% for InvUnc on B = 10% to almost 15% for cEDDM on B = 50%. In Fig. 3 we can see that the cDDM strategy elevates the accuracy for 10% and 50% budget, especially in the second half of the stream. For B = 1% the learning process is slightly more intensiﬁed, so it looks a bit more dynamic, however, it does not result in better performance on average. The RCD ensemble improved learning only for the lowest budget. In the case of AWE we can see much more improvements, however, the classiﬁer did not learn the concepts as well as all the rest of the models, being about three times worse than them. strategies with AHT and RCD improved the accuracy only for low budgets up to 10%, with RHT they did it for the higher ones, and the Uni, RandUni or cEDDM strategy worked well with AWE for all budgets. In the case of the Poker data stream, only AHT and RCD algorithms with the Fixed strategy were able to convincingly enhance the performance for almost all budgets. The ensemble provided A few improvements can be noticed for the Gas stream, especially when the For the Usenet stream, all algorithms worked well on average. Self-labeling Fig. 4: Accuracy of RCD with diﬀerent strategies given a budget for the Cover and Elec data stream. a notable gain with all self-labeling strategies on B = 50%. For the Elec stream, we can see improvements mainly for committees, however, once again, only RCD performed stably and it can be compared with AHT, which did not cooperate well with self-labeling algorithms, but it achieved the best accuracy on average. On the graph below (Fig. 4) we can observe how the Fixed self-labeling strategy improved learning for all presented budgets, for example when at the very beginning the accuracy was elevated above 75% for B = 10%. exploit new concepts suﬃciently, since there are not enough labeled instances to reinforce a new concept discovery. The hybrid approach that uses a self-labeling step helps with exploiting the concept and creating a more adequate model, without aﬀecting an available budget. This is especially valuable when taking into account that the aim of this approach was to develop methods for learning with very limited access to true class labels. For higher values of the budget, there are a bit fewer improvements, but there are still many of them. Well-modeled concepts may be more easily exploited by self-labeling, since there is a lower risk that an error will be ampliﬁed when class boundaries are more or less correct. One must although remember that very large budgets are unrealistic and prohibitive in most Pure active learning approaches on low budgets are sometimes not able to real-life scenarios, so we cannot increase it as much as it may be necessary. This is why we did not consider budgets higher than 50%. On the other hand, when a lot of labeled instances are available, the active learning strategy may be more likely to dominate the learning process and it may be suﬃciently good in sampling incoming concepts, while a self-labeling strategy may only impede the process. Such scenarios lead to the situation where the feasibility of a hybrid approach on higher budgets is highly dependent on a stream to which it may be applied. Let us summarize the ﬁndings from this manuscript and formulate a set of observations and recommendations regarding the usefulness and applicability areas of proposed hybrid methods for mining drifting data streams on a budget. Two following measures are used for this purpose. The ﬁrst one is the average accuracy (Acc) for all self-labeling strategies over all examined data streams. We calculate it for each classiﬁer and budget separately. The second measure is the fraction (Fh) of cases (each cell for a self-labeling strategy in the result tables is a single case) in which a hybrid approach achieved a better result than any reference method relying only on active learning. The fraction is calculated for each budget and classiﬁer individually. As long as our baseline classiﬁer achieved better than random accuracy, we can assume that improvements are relevant. Therefore, we excluded AWE results for the Sensor and Power stream. The fraction measure tells us if a strategy works well with a given base classiﬁer. The comprehensive summary of all results regarding classiﬁers, strategies and budgets is presented below. Budget matters. For real data streams, we can observe that improvements occur for all budgets, but they are present mainly on lower budgets (Fig. 5). The most frequent enhancements can be seen for RCD (in more than a half of cases on B = 1%) and RHT (from about 40% on B = 1% to almost 55% on B = 20%). It is very encouraging since low budgets are the most realistic and practical ones. The RHT classiﬁer reinforced with self-labeling was able to achieve results comparable with very solid AHT. We distinguish the RHT classiﬁer on low budgets as the best for the proposed framework. The good inﬂuence of the hybrid approach was also observed for the highest considered budget, on which improvements are relatively frequent. The explanation of why self-labeling works for very low and very high budgets has been presented at the end of Sec. 5.2. On the other hand, we can notice a local minimum on B = 10% and B = 20%. It might mean that the hybrid framework is not good for in-between cases, when models are not very reliable and, at the same time, there are fewer chances for improvements. We can also observe that the average accuracy increases with the budget. This is very intuitive, as more labeled instances support more accurate learning. Generic wrapper. We can see that the hybrid framework was able to improve results for many cases and for all of the examined classiﬁers. For most of the considered real scenarios at least one better hybrid solution could be found (see Sec. 5.2). Since the framework has been designed as a wrapper (a classiﬁer is modular) it is very important that we can observe this fact. One can easily apply the solution to any online classiﬁer in order to boost its performance, especially when facing very limited access to class labels. We recommend experimenting with the framework if results are not satisfying and a classiﬁer cannot be changed. Although the framework is able to improve all considered classiﬁers, some diﬀerences can be noticed. As already mentioned, the highest impact was registered for the RHT classiﬁer. Moreover, both Hoeﬀding Trees achieved the highest average accuracy in almost all cases (see Fig. 5). In general, single classiﬁers were able to integrate with self-labeling better than ensembles, which, besides the worse results, are also slower. However, we suppose that committees can be improved in the context of semi-supervised learning. The main reason why they perform worse than single classiﬁers is probably a fact that for low budgets there are not enough labeled samples to generate suﬃciently diversiﬁed ensembles. One can notice that the accuracy of the RCD classiﬁer on higher budgets is closer to RHT than on lower ones (Fig. 5). Informed over blind. The informed self-labeling strategies – cDDM, cEDDM and WinErr – performed generally better for real streams than the blind approaches, regarding both accuracy and the fraction of enhancements (see Fig. 6). They worked well with all classiﬁers, but the most signiﬁcant diﬀerence can be observed for the Fig. 6: Acc and Fh for diﬀerent strategies over the examined data streams. RHT classiﬁer, for which improvements were registered in between 60% and 65% of cases. The most straightforward explanation is that the information from drift detectors accordingly and eﬃciently supported the adjustment of a self-labeling threshold. The only exception is the blind Fixed strategy that uses a very high conﬁdence threshold. It cooperated eﬀectively with AHT and RCD on our examined real-world datasets. The InvUnc strategy was able to signiﬁcantly improve results only for RHT on real data streams. More for free. Last but not least, we want to emphasize the most important fact that our hybrid framework can signiﬁcantly reduce the cost of maintaining online classiﬁers that work with drifting streaming data. As we could notice, for the Spam data stream the AHT classiﬁer using the active learning strategy without self-labeling was able to achieve nearly 90% accuracy only if 50% of objects were labeled. Applying the InvUnc strategy to this case provided a very similar performance with only 1% of data being annotated, while for the same budget all the active learning strategies were correctly classifying less than 30% of samples. Let us consider an illustrative example. It has been estimated that more than 500 million tweets is created every day [1] and according to the CrowdFlower’s oﬀer [2], annotation of 100 000 rows costs 1500 dollars every month. If one wanted to have 50% of tweets labeled he would have to spend 6.75 million dollars monthly for that, while for 1% it is 50 times less, so only 75 000 dollars monthly. This is only a theoretical reduction that self-labeling connected with active learning may provide, but it shows very clearly why the hybrid approach should be considered each time a better accuracy is required and increasing a budget is not feasible. In this paper, we have introduced a novel hybrid framework for learning from drifting data streams on a budget. In real-life scenarios, unlimited access to ground truth cannot be assumed, as the cost is connected with obtaining such information from a domain expert. Therefore, we discussed a set-up of a learning system under scarce access to labels. We have proposed a combination of information coming from active learning and self-labeling, in order to obtain more eﬃcient usage of very few available instances. Active learning allowed for selecting proper ones for label queries, thus leading to the exploration of new concepts emerging from a data stream. These seeds were then utilized by a self-labeling module that oﬀered exploitation of previously discovered structures at no additional costs. We developed two families of algorithms that relied only on classiﬁer outputs or empowered them with additional information from a drift detection module. Seven algorithms were proposed in total, oﬀering a selection between complexity and performance. We recommend applying the hybrid approach if results for all available classiﬁers are insuﬃcient and one cannot increase a budget or when a weak classiﬁer cannot be replaced. proach, especially for realistic scenarios with a highly limited budget. In such cases inclusion of self-labeling allowed to improve the classiﬁer performance by increasing its competence over a discovered concept, while saving the small budget at hand for adapting to changes. We proved that the hybrid framework is a ﬂexible wrapper, so it can work with diﬀerent classiﬁers, including online ensembles. Experimental analysis showed the usefulness of the proposed hybrid-based ap- We also observed that informed strategies are preferable over blind approaches with an exception to the strategy that uses a high ﬁxed threshold for self-labeling. The proposed hybrid solutions displayed excellent performance for real-life data streams. budget from drifting data streams. As a next step, we envision works on another semi-supervised approach which is oversampling. It may be used to enhance the adaptation process by providing more labeled instances without additional cost. We suppose that such an approach may reinforce the construction of diversiﬁed online ensembles. The obtained results encourage us to continue our works on learning on a