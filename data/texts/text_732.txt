User modeling plays a fundamental role in industrial recommender systems, either in the matching stage and the ranking stage, in terms of both the customer experience and business revenue. How to extract users’ multiple interests eectively from their historical behavior sequences to improve the relevance and personalization of the recommend results remains an open problem for user modeling. Most existing deep-learning based approaches exploit item-ids and category-ids but neglect ne-grained features like color and material, which hinders modeling the ne granularity of users’ interests. In the paper, we present Multiple interest and Fine granularity Network (MFN), which tackle users’ multiple and ne-grained interests and construct the model from both the similarity relationship and the combination relationship among the users’ multiple interests. Specically, for modeling the similarity relationship, we leverage two sets of embeddings, where one is the xed embedding from pretrained models (e.g. Glove) to give the attention weights and the other is trainable embedding to be trained with MFN together. For modeling the combination relationship, self-attentive layers are exploited to build the higher order combinations of dierent interest representations. In the construction of network, we design an interest-extract module using attention mechanism to capture multiple interest representations from user historical behavior sequences and leverage an auxiliary loss to boost the distinction of the interest representations. Then a hierarchical network is applied to model the attention relation between the multiple interest vectors of dierent granularities and the target item. We evaluate MFN on both public and industrial datasets. The experimental results demonstrate that the proposed MFN achieves superior performance than other existed representing methods. • Information systems → Recommender systems;• Learning to rank; • Personalization; Multiple interest, ne granularity, user modeling, search ranking ACM Reference Format: Jiaxuan Xie, Jianxiong Wei, Qingsong Hua, Yu Zhang. 2021. Multiple Interest and Fine Granularity Network for User Modeling. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 4 pages. https://doi. org/10.1145/1122445.1122456 In recent years, recommender systems have become increasingly prevalent and gained success in various applications such as news distribution, video watching and e-commerce. Instead of only considering categories or keywords, current recommender approaches attempt to merge more personalized information into modeling with the goal of understanding exactly the intention of users and showing what they are most interested in. The user historical behavior sequences, which have been proved to be of great value for personalization, play a signicant role in user modeling for extracting users’ hidden interests. A popular user modeling strategy is to obtain user interest representations by leveraging the mean pooling or the weighted pooling over the historical behavior sequences. Several methods follow a similar Embedding&MLP paradigm [2,3,14]. They capture the representation of users’ interests by averaging the embedding vectors of user behaviors and transform them into a xed-length vector as users’ interest representation. While attention-based methods like DIN [16], DIEN [15] and DSIN [4], they adaptively extract the insterest vector by considering the relevance of historical behaviors given a target item using attention mechanisms, which consequently assigns higher activated weights to those historical behaviors with higher relevance. DIN and its successors achieve better performance than Embedding&MLP methods, but a single interest vector is still insucient to capture the varying characteristics of users’ interests. To address this problem, MIND [6] and ComiRec [1] leverage capsule routing mechanism for clustering historical behaviors and obtaining users’ multiple interest vectors. However, except item-ids, category-ids and similar id features, both of them lack the modeling of ner grained features like color and materials. [5,8,12] exploit transformers or similar hierarchical attention structure to capture multiple interest representations of users. Yet, their complicated model structures make serving online nearly infeasible without specic optimization for practical usage. In this paper, we propose Multiple Interest and Fine granularity Network (MFN), for user modeling to handle with users’ multiple and ne-grained interests. There are two key components in MFN, one is for modeling the similarity relationship and the other is for the combination relationship among users’ multiple interests. Similaritymeans two interests are similar in terms of physical characteristics,e.g.a customer likes black windbreakers and black coats.Combinationmeans two interests share latent collocation, e.g.a father buys beers and pampers. Specically, for modeling the similarity relationship, we leverage two sets of embeddings, where one is the xed embedding from pretrained models (e.g. Glove) to give the attention weights and the other is trainable embedding to be trained with MFN together. For modeling the combination relationship, self-attentive layers are exploited to build the higher order combinations of dierent interest representations. By modeling the similarity and the combination relationship separately, we are able to build more expressive and eective users’ multiple and ne-grained interest representations. In a nutshell, the main contributions of this paper can be concluded as following: •We propose to study the problem of extracting multiple and ne-grained interest representations from users’ historical behavior sequences for user modeling and present the Multiple interest and Fine granularity Network (MFN). •We propose to model both the similarity relationship and the combination relationship among users’ multiple interests. For the former, we leverage two sets of embeddings, where the xed one is for giving he attention weights and the trainable one is trained for the main task. For the latter, we leverage self-attentive layers to learning high-order interest interactions. •We conduct extensive experiments on both public and industrial datasets. Experimental results demonstrates the proposed MFN achieves superior performance than other representing methods with other important attributes like good explainability and little online response time. In recent decades, user interest modeling has attracted much attention in industrial applications such as recommender systems and online advertising, which concentrates on learning the representation of users’ interests from the historical behavior sequences. DIN [16] leverages an attention mechanism to capture the diverse interests of a user on dierent candidate items. DIEN [15] considers the temporal relationship among the historical behaviors and proposes to model the evolution of users’ interests with an interest extraction layers based on GRU. DSIN [4] highlights that user behaviors are highly homogeneous in each session and heterogeneous cross sessions and designs a self-attention network with bias encoding to get the corresponding interest representation of each session. MIND [6] and ComiRec [1] emphasize that a single interest vector is insucient to capture the varying nature of users’ interests. They exploit capsule network and dynamic routing to obtain the representation of users’ interests as multiple interest vectors. Due to the success of transformers in other areas, [8,11] propose to leverage transformers or similar structures with multiple head attentions to extract users’ multiple interests from the behavior sequences. There are also a series of works that focus on long-term even lifelong user interest modeling. [9] demonstrates that the longer historical behavior sequences in the user interest modeling module can support the CTR model with better performance. MIMN [9] embeds user long-term interest into xed-sized memory network to decrease the burden of the latency and storage of online serving. SIM [10] leverages a general search unit to get a sub user behavior sequence that is relevant to the candidate item and proposes an exact search unit to model the precise relationship between the candidate item and the sub sequence. In this section, we formulate the problem and illustrate the proposed nt Multiple interest and Fine granularity Network (MFN) in detail. Considering the deployment of MFN in practical scenarios, we build MFN with a hierarchical structure with two levels. The rst level is for category and the second level is for ne-grained features like item entities. Suppose we have a set of usersUand a set of itemsI. For simplicity, here we consider a user𝑢 ∈ Uand a candidate item𝑖∈ I. The historical behavior sequence of𝑢is𝐸 =(𝑖, . . . , 𝑖, . . . , 𝑖)∈ R, where𝐸 ⊂ I,𝑡denotes the𝑡-th interaction item𝑑is the feature size,𝑁is the length of the sequence. The feature elds of items include id features ( e.g. iid-item id, cid-category id, sid-shopid, bid-brand id) and ne-grained features (e.g. entity features like color or materials), i.e.𝑖=𝑖; 𝑖, where𝑖⊆ {𝑖, 𝑖, 𝑖, 𝑖}, 𝑖denote the features of coarse id features and ne-grained features, respectively. The feature construction here follows the basic embedding paradigm [2? ]. Similarity means two interests are similar in terms of physical characteristics, e.g. a customer likes black windbreakers and black coats. Hence, we can divide and extract users’ multiple interests according to the similarity of the items in the behavior sequences. Here, to model the similarity relationship among the users’ multiple interests, we construct two sets of embeddings,𝐸andE, 𝐸, E ∈ R. One is provided by pretrained models and is xed in the model to measure the similarity, still denoted as𝐸. For id features like iid, cid, sid, the pretrained embedding can be obtained from a vanilla model under an auxiliary CTR prediction task based on longterm behavior data. If the ne-grained features are from words, we can exploit Golve or Word2Vec to get the pretrained embeddings. And the other set of embeddingEis trainable and is trained under the main task in the proposed MFN model. For clustering multiple interest vector centers, [1,6] leverage random centers as the initial centers. However, Using random centers not only does require more iteration in the following clustering, but also lose the expressive power and the explainability to some degree. In this paper, we propose to pretrain the cluster centers. Suppose one user’s multiple interest cluster centers are𝐶 = {𝐶, . . . , 𝐶. . . , 𝐶} ∈ R, where 𝐾is the number of centers,𝐶denotes the𝑗-th center. Recall the historical behavior sequence𝐸 =(𝑖, . . . , 𝑖, . . . , 𝑖)∈ R, we assume the probability that behavior𝑖belongs to center𝐶is𝑃. On the one hand, we hope that the centers are not redundant,i.e. each cluster is lled with samples and each center can be covered, hence we maximum the entropy of the sum along the rows of 𝑃, On the other hand, one behavior𝑖ought to be assigned to only one centers, hence we minimum the entropy of each 𝑃, From Eqn. (1) and Eqn. (2), the optimization target is to minimize Considering the large-scale situation, we can use the mini-batch version. The algorithm to obtain the multiple interest centers is demonstrated as Alg. 1. Note that here we leverage backward propagation to optimize the trainable centers𝐶, and the stopping criterion can be set to stop at a given number of iterations. Data: Trainable centers 𝐶 ∈ R, 𝑀 users’ historical behavior sequence 𝐸= {𝐸}, 𝐸∈ R, learning rate 𝛼 Result: Trained centers 𝐶. {1, . . . , 𝑀}; And in the next we introduce how to leverage the multiple interest centers in the main task of MFN. After obtaining the multiple interest centers𝐶, using attention mechanism we are able to give the similarity weights,𝑖.𝑒.the probabilities that the behaviorsE can be divided into interest centers 𝐶, And the multiple interest representations of a user 𝑙 is where 𝑅∈ R. Besides the similarity attribute in terms of physical or semantic characteristics, another signicant relationship among the users’ multiple interests is combination. Combination here means two interests share latent collocation, e.g. a father buys beers and pamper at the same time, or a student buys an English book and an electronic dictionary. Here, we adapt the self attentive method [7] for capturing high-order combinations of multiple interests. Recall E ∈ Ris the trainable embedding for behavior sequences, using self-attention mechanism, we get the combinational weight matrix 𝐴 as whereswish(·)denotes theswishactivation,MSA(·)denotes the Multi-head self-attention,WandWare trainable parameters with size 𝑑× 𝑑 and 𝑑× 𝐾, respectively. The combinational interest representations𝑅can be computed where𝑅∈ R,𝐾is the number of interests and𝑑is the dimension of features. In this section, we introduce how to aggregate the interest representations from both the similarity and combination relationships. After extracting the multiple interests for each user from the historical behavior sequences, we have the whole interests R as whereR ∈ R. We can get the interest representationR(𝑖) with regard to the candidate item 𝑖∈ R, R(𝑖) = 𝑓 (R, 𝑖 whererdenotes the𝑗-th interest inR(i.e.the𝑗-th row ofR),𝑎(·) and𝑎(·)is a feed-forward network with output as the weight, andÍÍ Leveraging the interest representationR(𝑖), along with the User Prole 𝑋, theItem prole 𝑋andContext features 𝑋, we train the MFN model by optimizing the cross entropy loss Table 1: Experimental results on industrial datasets whereDis the training set,𝑥is represented by[𝑋, 𝑋, 𝑋]that is the input of the network,𝑦 ∈ {0,1}denotes the labels, where 1 means the user has an inveraction with the candidate item,𝑝 (·)is the nal output of the model which represents the output probability. In this section, we conduct experiments on both public and industrial datasets to evaluate the eectiveness of the proposed MFN compared with other representative methods. We introduce the datasets, competitors, evaluation metrics and then report and analyse the experimental results. In this paper, we evaluate the model on both the public and industrial datasets. We collect the industrial datasets from an e-commerce app and leverage 3 days’ logs with users’ historical sequences for constructing the training dataset and use the next one day’s data for testing. For evaluating the performance on public datasets, the Area Under the Curve (AUC) is exploited and for the industrial datasets we use the RelaImpr [13] to measure the relative improvement, which is dened as 𝑅𝑒𝑙𝑎𝐼𝑚𝑝𝑟 =𝐴𝑈 𝐶 (𝑇 𝑒𝑠𝑡_𝑚𝑜𝑑𝑒𝑙) − 0.5𝐴𝑈 𝐶 (𝐵𝑎𝑠𝑒_𝑚𝑜𝑑𝑒𝑙) − 0.5− 1 We conduct our experiments on Tensorow 1.4 and use the Adam optimizer. The initial learning rate is 1e-4, the epoch is set to 1. Other methods are set as their recommend settings. The results on the industrial datasets is illustrated as Table. 1 In the paper, we present Multiple interest and Fine granularity Network (MFN), which tackle users’ multiple and ne-grained interests and construct the model from both the similarity relationship and the combination relationship among the users’ multiple interests. We evaluate MFN on both public and industrial datasets. The experimental results demonstrate that the proposed MFN achieves superior performance than other existed representing methods.