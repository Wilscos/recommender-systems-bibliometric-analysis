In order to meet the diverse and personalized information needs of users, the personalized recommendation system has emerged [ items. It recommends similar items to users with similar behaviors based on interactive content such as purchases, clicks, and ratings. This method is called collaborative ﬁltering. Collaborative ﬁltering generally uses embedding to represent users and items, and predicts scores based on embedding. In recent years, a large number of collaborative ﬁltering methods have been successfully implemented, such as [5, 6, 7, 8, 9]. Although these collaborative ﬁltering methods are effective, they only consider the explicit interaction between a single user and a single item, and do not encode the collaborative signal explicitly. When the interaction data is very sparse, it is difﬁcult to learn embedded representations with sufﬁcient knowledge. Can not effectively characterize the similarities between users or items [10, 11]. School of Automation Engineering, University of Electronic Science and Technology of China Compared with the traditional collaborative ﬁltering methods, the graph convolution network can explicitly model the interaction between the nodes of the user-item bipartite graph and effectively use higher-order neighbors, which enables the graph neural network to obtain more effective embeddings for recommendation, such as NGCF And LightGCN. However, its representations is very susceptible to the noise of interaction. In response to this problem, SGL explored the self-supervised learning on the user-item graph to improve the robustness of GCN. Although effective, we found that SGL directly applies SimCLR’s comparative learning framework. This framework may not be directly applicable to the scenario of the recommendation system, and does not fully consider the uncertainty of user-item interaction. In this work, we aim to consider the application of contrastive learning in the scenario of the recommendation system adequately, making it more suitable for recommendation task. We propose a supervised contrastive learning framework to pre-train the user-item bipartite graph, and then ﬁne-tune the graph convolutional neural network. Speciﬁcally, we will compare the similarity between users and items during data preprocessing, and then when applying contrastive learning, not only will the augmented views be regarded as the positive samples, but also a certain number of similar samples will be regarded as the positive samples, which is different from SimCLR who treats other samples in a batch as negative samples. We term this learning method as Supervised Contrastive Learning(SCL) and apply it on the most advanced LightGCN. In addition, in order to consider the uncertainty of node interaction, we also propose a new data augment method called node replication. Empirical research and ablation study on a data set prove the effectiveness of SCL and node replication, which improve the accuracy of recommendations and robustness to interactive noise. Supervised contrastive Learning·Graph convolution network·Recommended system·Representational 1,2,3,4]. A recommendation system can help users ﬁnd items of interest in a large amount of candidate Figure 1: The limitation of contrastive learning framework used by SGL. SGL treats other samples in a batch as negative samples, which makes the representation of similar users further in the representation space, and it should be closer to be more effective for subsequent recommendation task. Our proposed SCL introduces supervised information and labels similar users as positive samples, as shown by the red word in the ﬁgure. After contrastive learning, similar users are closer in the representation space, which can obtain representations that are more conducive to recommendation. In order to make use of the user-item bipartite graph high-hop neighbor information fully, more effectively repersent the embedding of users and items, Wang et al. recently proposed graph convolution collaborative ﬁltering, and the best experimental results were obtained on three data sets [ to the ﬁeld of recommendation systems, and regarded the user-item bipartite graph as a isomorphic graph, and carried out feature embedding, information dissemination and feature aggregation on the graph to obtain the ﬁnal representation of users and items. Their method encoded the high-hop neighbors between the user and the item explicitly, so they get more reliable and more informative embedding representations and achieve better results. Based on NGCF, He et al. proposed lightGCN [ from GCN. These operations are not necessarily suitable for collaborative ﬁltering tasks in the recommendation ﬁeld. Therefore, they simplify many operations of NGCF, reduce the difﬁculty of model training, and achieve the current state-of-the-art on multiple data sets. GCN provides a most advanced and efﬁcient solution for solving user-item bipartite graph sparsity problems and interactive modeling problems. However, Wu et al. believe that the representations learned by GCN are easily biased towards high-frequency items or users and are also easily affected by noise of interactions. In order to address the above limitations, they proposed Self-supervised Graph Learning(SGL) for recommendation, which uses unlabeled data space by augmenting the input data, thereby achieving a signiﬁcant improvement in downstream tasks [ contrastive learning of SimCLR and use the framework of simCLR as a paradigm of self-supervised learning [15]. Although SGL has achieved a certain degree of effect improvement, we believe that the contrastive learning framework of SimCLR may not be very suitable for recommendation tasks. Speciﬁcally, the application background of SimCLR is a computer vision(CV) task. It treats two augmenting samples of a input data as the positive samples and treats other samples in the batch as the negative samples. Due to the diversity of samples in CV tasks, this design is reasonable and helps to mine hard negative samples which improve the quality of the learned representations. However, in the recommendation system, the core of the comparison is the users or items node and the ultimate goal is to compare the cosine similarity of the data. This means that there is a high probability that there are similar users or items in a batch. Regarding these samples as the negative samples, similar users or items will become farther away in the representation space, which violates the optimization purpose of the recommendation system, affects the ﬁnal representation learned by GCN, and reduces the performance of the recommendation system. In order to solve the above limitations, we believe that it is necessary to improve the application of contrastive learning. We propose a Supervised Contrastive Learning(SCL) for recommendation. We hope to design a framework of contrastive learning with the recommendation task as the goal, so as to obtain the representations that is more in line with the requirements of the recommendation task and improve the performance of downstream tasks. Speciﬁcally, it consists of two steps: (1) data augmentation, which generates multiple views for each node; (2) contrastive learning, which makes similar samples are closer and different samples are further in the representation space. Since the Bayesian Personalized Figure 2: Motivation for node replication. By replacing part of the data of user 1 with the corresponding data of similar user 2, the augmented sample are more diverse, and the obtained representation can better express the user’s preferences, thereby making the recommendation results more diverse. Ranking(BPR) loss of the recommendation task takes node interaction as input, a batch may contain a large number of similar users or nodes [ learning of representation, which is different from SGL directly taking contrast learning as In the way of auxiliary tasks, SCL use comparative learning as a pre-training task to obtain a preliminary representation, and then use BPR loss for ﬁne-tuning. In addition, in order to be more adaptable to the recommendation task, we will not simply treat other samples in the same batch as negative samples when we perform comparative learning, but treat all similar samples as positive samples, and dissimilar samples as negative samples. This comparative learning method introduces supervised information, which makes similar users or items more inclined to learn similar representations, which is beneﬁcial to downstream recommendation tasks. This supervised and contrasted representation learning method starts from the goal of the recommendation system, and its design is more suitable for the recommendation system and can further improve the performance of the recommendation task. In addition, we found that the BPR loss is simply optimized by constructing interactive and non-interactive triples. However, the user-item bipartite graph may have noisy interaction or similar user item pairs without interaction. From the purpose of the recommendation system, in order to further improve the recommendation performance, we believe that the edge drop and node drop on the user-item bipartite graph does not bring enough diversity information. In order to improve the model’s adaptability to noise interaction and the diversity of the representations, we propose a new data augmentation method called node replication. Speciﬁcally, we replace part of the interaction of the current node with the corresponding interaction of similar nodes according to a certain probability. This data augmentation method can effectively improve recommendation performance and help increase the diversity of recommendations. It is worth noting that our SCL can be applied to any collaborative ﬁltering network based on the user-item bipartite graph. In this work, we choose to implement it on the most advanced LightGCN, and prove the effectiveness of SCL on a benchmark data set, which can signiﬁcantly improve the recommendation performance. To summarize, The main contributions of this paper are summarized as follows: • We veriﬁed the effect of SCL on a benchmark dataset and proved the effectiveness of SCL. We propose a new supervised comparative learning paradigm, which considers the purpose of the recommendation system and provides supervised information for representation learning, so that similar nodes are closer in the representation space, which helps to improve the recommendation performance. In order to improve the representation ability of nodes and obtain more diverse information, we propose a new data augmentation method called node replication, which improves the robustness and diversity of the representations, and makes the recommendation results more diverse and has better performance. The set of user-item interactions can be easily modeled as a bipartite graph set,Vrepresents the item set, and there is an edge between them [ j ∈ Vinto a uniﬁed d-dimensional representation space, and the corresponding representations are expressed by andv ∈ item should be recommended to the user. The core of GCN is to aggregate the domain representations of each node on the bipartite graph representation of current node by considering the information of high-order neighbors, and ﬁnally obtain an effective representation. For user i, its representation is calculated as follows: Wherev average, weighted sum, linear mapping, etc [ of thel-th layer, udenotes the representation of user which are trainble parameters. Speciﬁcally, GCN ﬁrst aggregates the representations of all domain nodes of user its own representation, and then obtains the representation of layer the calculation method of its representation is the same as that on the user side. The obtained representation of the layer corresponds to the information aggregation of the node’s are also different ways to generate the ﬁnal node representation, such as using the last layer only, splicing, averaging, weighted sum, attention and so on [12, 13, 22, 23, 24]. After obtaining the ﬁnal representation of each node, the prediction layer can be used to calculate a certain user preference for item as the prediction layer at present [13]: When optimizing model parameters, GCN generally use BPR loss, and BPR loss chooses a user, an interactive item and a non-interactive item to form a triple, and the predicted value of interaction is expected to be greater than the predicted value of no interaction: Whereλ BPR loss is the key to achieving the ﬁnal recommendation result, and we apply it in the ﬁne-tuning stage. The framework of contrastive learning is shown in the ﬁgure 3. It maximizes the similarity between augmented samples generated by the same data and minimizes the similarity with other samples in the same batch to learn representations. The framework includes four components: data augmentation, encoder, projection head, and contrast loss function [ Speciﬁcally, the data augmentation module generates related views from the original data data augmentation methods, denoted as include node drop, edge drop, and node replication. Different data augmentation methods have a great impact on the performance of comparative learning. The encoder generates a representation vector problem, the encoder is the GCN network. . By calculating the inner product between the representation s of user and the item we can know whether the ∈ N (u)represents the domain node of useri,Adenotes the node aggregation function, which can be δis the activation function of thel-th layer, which can besigmoid,Relu, or None, etc [20,18,21]. is the parameter that controls the intensity of L2 regularization, andMis the total number of interactions. Figure 3: The framework of contrastive learning, it amit to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space. The purpose of the projection head is to project the representation into the contrast loss space. The projection head is generally an MLP with a hidden layer. Many experiments on contrastive learning have proved that the projection head is beneﬁcial to improve the performance of contrastive learning [15, 25, 26, 27]. The contrast loss function which called InfoNCE aims to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space [15, 28, 29, 30]: Wheresim(u, v ) = u function, when k is not equal to i, the value is 1, otherwise it is 0. τ denotes a temperature parameter. We present the proposed paradigm of Supervised Constrastive Learning(SCL) for recommendation system. The learning process is divided into two stages, as shown in the ﬁgure 4. First, we use supervised constrastive learning as the pre-training part to learn preliminary representations, and then apply the BPR loss to ﬁne-tune the model. In this section, we ﬁrst introduce how to perform data augmentation to generate multiple views, and then introduce the design details of SCL. The two-stage training method is adopted because BPR loss uses interactions as input, while contrastive learning uses nodes as input. SCL focuses on recommended task, hoping to make similar nodes similar in the representation space, and address the limitations of the comparative learning framework used in SGL, which makes SCL more suitable for recommendation systems. In SimCLR, the authors discusses various augmentation methods for images [ data structure is different so that the data augmentation methods cannot be directly migrated. In the user-item bipartite graph recommendation task, the input data is a sparse matrix composed of interactions. We regard this data as the graph structure data as shown in the ﬁgure 4. In order to learn the representation of each node fully, we need to design new data augmentation schemes for graph structure data. In SGL, the authors has designed various augmentation methods including node drop and edge drop for graph structure data [14]. In order to further improve the diversity of the recommendation results and ensure that the representation of Figure 4: The framework of contrastive learning, it amit to minimize the distance of the augmented views generated by the same sample in the contrast loss space, while maximizing the distance of the augmented views generated by other samples in the same batch in that space. each node contains the user’s possible interests as much as possible, we proposed a new data augmentation method called node replication. We detail the augmentation methods as follows: 3.1.1 Node Drop(ND) Node drop discards a certain node in the graph and any interactions(i.e. edge in graph) related to it according to probability ρ WhereV (G) indicates that the corresponding node is discarded. nodes set V. When performing data augmentation, we can generate multiple views by applying formula 8 multiple times on the original graph data. This augmentation can reduce the impact of high-frequency nodes on the representation by randomly drop some nodes, so that the representations can learn more from the long tail node. 3.1.2 Edge Drop(ED) Edge drop discards an certain interaction in the graph (i.e. edge in graph) according to the probability ρ WhereM discarded. E represents the set of edges. This augmentation randomly discards some edges, and it is expected that the representation learned by contrastive learning will not be affected by speciﬁc interactions, and the robustness of the representation will be improved. 3.1.3 Node Replication(NR) Node replication will replace part of the interactions of the current node with the corresponding interactions of similar nodes according to probability similar is the interaction history, the more similar are the nodes. The matrix form of similarity calculation is as follows: is the view generated byGas input.Mis the masking vectors which apply on the node set, where 0 is the masking vectors which apply on the edge set, where 0 indicates that the corresponding edge is WhereG select the ﬁrst nodes and item-side nodes are calculated separately. We divide the interaction history of each node into node replication. Node replication is as follows: WhereM node replication. This augmentation randomly replaces part of the interactions, and it is expected that the representation can contain more information, increase the diversity of recommendation results, and improve the performance of the recommendation system. After establishing multiple views of the node, we input them into the GCN respectively, and then perform contrastive learning loss on the generated representations. Different from InfoNCE, which treats the views of the same node as a positive sample pair, and treats the views of any other different nodes as the negative samples, we propose a supervised contrastive learning loss, which treat similar nodes as the same type and all are regarded as positive sample. We call this loss supervised InfoNCE(S-InfoNCE): Where1 denotes that function, when j is similar to i. sim (i, j) and τ is same as infoNCE. Our S-InfoNCE encourages the representations of similar nodes to be close to each other in the representation space to ensure their consistency. In addition, dissimilar samples are taken as negative samples to ensure that the representations of these nodes are signiﬁcantly different. We take the recommendation task as the goal, and believe that similar users should have similar representations to facilitate collaborative ﬁltering. It is worth noting that our SCL learning paradigm will only perform pre-training with contrastive learning on the basis of GCN without introducing any additional parameters. In other words, the learning paradigm we proposed will not bring any parameter burden to the original model, and it will improve the recommendation performance while ensuring its high efﬁciency. To verify the effectiveness of the SCL for recommendation paradigm proposed in this paper, a dataset named MovieLens100k(ML-100K) is adopted for top-K recommendation and the results are compared with other state-of-the-art methods. The speciﬁc statistics of ML-100K are shown in the table ??. is the sparse matrix represents the interaction history of each node. Then for each node we will sortSand Nnodes that can be used for replacement. It is worth noting that when calculating similarity, user-side is the masking vectors which apply on the node set, where 0 indicates that the corresponding node is applied denotes an indicator function, whenkis not similar toi, the value is 1, otherwise it is 0. While1 In top-k recommendation, we follow the strategy described in [ choose MAP (Mean Average Precision), MRR (Mean Reciprocal Rank) and MNDCG (Mean Normalized Discounted Cumulative Gain) to evaluate the recommendation performance. And consider the case where K is equal to 3, 5, 10 respectively. The proposed method is compared with the pure MLP, LSTM, BiLSTM and IDCNN to verify validity and efﬁciency. Since the core innovation of this paper is the proposed time relationship unit and decoupling position embedding unit that can be regarded as a new feature extractor, for fairness, the same classiﬁcation layer is applied when comparing the performance among the those methods. We compare the proposed SCL with the following strong baselined CF models: DeepWalk [ [33], PinSage [ LightGCN, we implement three variants of SCL, namely SCL-ND, SCL-ED, and SCL-NR, which respectively applied the data augmentation method of Node Drop, Edge drop, Node Replication. The above methods are all using the ofﬁcial implementation. For fairness, all methods use the same training and testing data. The embedding size is uniformly ﬁxed as 128 and the learning rate is 0.001. IGMC uses 1-hop subgraphs with an encoder depth of 2. The adam is used as optimizer and the batch size is set to 1024. As can be seen from the table, our proposed SCL learning paradigm is more effective on ML-100K data, and has achieved signiﬁcant performance improvements in all three indicators. In addition, comparing different data augmentation method, it can be seen that NR is signiﬁcantly effective. This data augmentation method can effectively improve the diversity of recommendation results, thereby improving recommendation performance. We have only conducted experiments on small datasets at present, and we will conduct more adequate experiments on large datasets such as gowalla in the future. In addition, we will further conduct ablation experiments on SCL to illustrate its performance. We also hope to further analyze the principle of SCL. 34], GC-MC [35], IGMC [36], NeuMF [6], NGCF [12], LightGCN [15] and SGL [14]. On the basis of