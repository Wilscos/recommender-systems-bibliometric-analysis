When learning a second language (L2), one of the most important but tedious components that often demoralizes students with its ineffectiveness and inefﬁciency is vocabulary acquisition, or more simply put, memorizing words. In light of such, a personalized and educational vocabulary recommendation system that traces a learner’s vocabulary knowledge state would have an immense learning impact as it could resolve both issues. Therefore, in this paper, we propose and release data for a novel task called Pedagogical Word Recommendation (PWR). The main goal of PWR is to predict whether a given learner knows a given word based on other words the learner has already seen. To elaborate, we collect this data via an Intelligent Tutoring System (ITS) called Santa that is serviced to ∼1M L2 learners who study for the standardized English exam, TOEIC. As a feature of this ITS, students can directly indicate words they do not know from the questions they solved to create wordbooks. Finally, we report the evaluation results of a Neural Collaborative Filtering approach along with an exploratory data analysis and discuss the impact and efﬁcacy of this dataset as a baseline for future studies on this task. Memorizing words is undoubtedly one of the most tiresome parts of studying a second language (L2). As discouraging as it is, vocabulary acquisition also plays a crucial role in L2 learning which makes it inevitable for students to struggle with new words every day. Evidently, this problem was in the spotlight in the ﬁeld of second language acquisition for a long time (Laufer and Nation, 1999; Schmitt et al., 2001; Nation, 2006; Nakata, 2011). Recently, there has also been an increasing focus on this topic from the NLP community (Ehara et al., 2010, 2012, 2014; Ehara, 2018; Settles and Meeder, 2016; Settles et al., 2018). Among them, Ehara et al. (2010, 2012, 2014); Ehara (2018) formalizes this task as Vocabulary Prediction and provide datasets for it. However, Ehara et al. (2012) also argues that a) tracking what words learners already know and b) determining the words learners should know (Nation, 2006) are two different things that have to be modeled independently. On the other hand, we believe that these are actually two sides of the same coin, which is to recommend words that learners should know based on what they already know. This is precisely what our proposed task – Pedagogical Word Recommendation (PWR), is about. Furthermore, many works have already shown that vocabulary prediction can be a useful support application for language learning such as TOEIC score prediction (Ehara, 2018), adaptive word difﬁculty (Ehara et al., 2012), or lexical simpliﬁcation (Yeung and Lee, 2018; Lee and Yeung, 2018). On top of this, PWR can become a core module of an ITS – such as personalized ﬂashcard generation. Our Contributions.The contributions of this paper can be summarized into three points: Recommendation that provides a recommendation system perspective to vocabulary prediction. •Based on this formulation, we provide a novel dataset that is both large-scale (∼36M) and self-reported. •We provide detailed statistics and analysis of the dataset, and evaluate standard Collaborative Filtering models to show the efﬁcacy of our work as a benchmark for future studies in this task. Table 1: Comparison among vocabulary prediction datasets. Our proposed dataset is more suitable for a recommendation system than others as it not only is the largest in size, but also has more desirable traits (words come from various context, self-reported, real-service, timestamps). †: Unobserved here indicates the ratio of unobserved user-word pairs. ‡: Both Duolingo datasets are multilingual ones so we calculate statistics only from their English data for fair comparison. a vocabulary prediction dataset by surveying 15 English L2 learners on their knowledge of 11,999 individual words in order to model a user’s vocabulary knowledge along with learner-speciﬁc word difﬁculty. Using this dataset, Ehara et al. (2014) investigated a more practical scenario of sampling a small subset of words for vocabulary prediction. Moreover, Ehara (2018) provides a dataset on vocabulary knowledge of 100 Japanese ESL learners on 100 words obtained from vocabulary exams. However, our work differs in several important aspects from the above works as PWR data 1) is much larger in scale, 2) comes from real-service data with words reported from TOEIC questions, and 3) has timestamps (for temporal models). To continue, real-data from the popular L2 learning ITS Duolingo has also been released by (Settles and Meeder, 2016; Settles et al., 2018). The former (Settles and Meeder, 2016) releases a large dataset with word-recall related features in order to model forgetting behaviors of learners. Meanwhile, the latter (Settles et al., 2018) proposes a new task that aims to predict where the learners often make mistakes during translation and is highly associated with grammatical error detection/correction. The main differences with our work are that PWR data 1) is self-reported which is a crucial aspect for recommendation, 2) has much longer context length for the words, and 3) is more focused on vocabulary recommendation (or prediction). Furthermore, these works are highly related to Knowledge Tracing (Corbett and Anderson, 1994), which is a heavily studied task (Feng et al., 2009; Lindsey et al., 2014; Choi et al., 2020) that aims to predict the learner’s future performance on selected knowledge components (e.g. questions or concepts) given the learner’s historical data. Ehara et al. (2012, 2014); Settles and Meeder (2016); Settles et al. (2018) can be viewed as an instance of Knowledge Tracing, but these works are not positioned for the task we propose. For example, these works may not apply well to more realistic language learning scenarios like reading comprehension. As shown in Table 1, PWR originates from such situations making it more appropriate. We believe that PWR combines the advantages of ITS (Settles and Meeder, 2016; Settles et al., 2018) with the formalizations from Ehara et al. (2012); Ehara (2018). 3.1 Task Deﬁnition The task deﬁnition is illustrated in Figure 1 and is similar to that of Ehara et al. (2010, 2012): given a user i (or learner) and a word j, predict whether he or she knows this word (binary classiﬁcation). Given such, we can see that our proposed task follows the Collaborative Filtering (Su and Khoshgoftaar, 2009) setting. Our task can be considered similar to classic recommendation system settings, but just binary (Verstrepen et al., 2017) instead of ratings. The underlying assumption is that similar users “do not know" similar words. This setting is analogous to that of NeurIPS Education Challenge Task 1 (Wang et al., 2020), if we interchange “questions" with “words". 3.2 Dataset Description We collect our data mainly from an Intelligent Tutoring System (ITS) called Santathat is live serviced to∼1M English L2 learners who are prepar- Figure 1: Collaborative Filtering setting: Given the seen-and-unknown words and seen-but-known words as training set, a model has to predict the values of the held out words as a test set. ing for TOEIC. Users of this web & mobile app are able to study for (mock) TOEIC Reading & Listening Comprehension questions. For each question they have solved, users are able to review the full passages (or transcripts for Listening questions) and mark the words they do not know, and append them to their wordbooks. Basic summary statistics are shown in the top row group of Table 2. From the joint distribution plot in Figure 2, we can see an interesting concentration on 0% and 100% accuracy that correspond to the very low number of words per user. We can also see that the "high" number of unknown words is mostly associated with accuracy between 50-70%. These observations can be explained by actual learning behaviors of L2 learners: 1) students on both extremes either know too much or too few words to create a wordbook, 2) while students in the middle are more active in studying vocabulary. Meanwhile, we also look at the top-20 words that users know and do not know in Table 3. From this table, we can see that the words that are marked words are indeed difﬁcult ones for beginners, and the words that are popularly known are seemingly easier ones. We provide details on how we prepare this dataset along with additional dataset analysis in Appendix B and C. We evaluate our dataset on two well-known modelbased Collaborative Filtering methods: Matrix Factorization (MF) (Zhang et al., 2014) and Neural Collaborative Filtering (NCF) (He et al., 2017). The inputs of these models are given as useru, wordv, and labely. To get the output prediction ˆy, collaborative ﬁltering models follow a general Table 2: Summary statistics (top rows) and distributional statistics (bottom rows). For the bottom rows, the left is the mean and the right is standard deviation of counts. There is high imbalance between positive and negative labels in the training set, but the dev/test sets are balanced for proper evaluation. Table 3: Words that had most number of users. Top row is words that the largest number of users found difﬁcult (# Users per word (+)), and bottom row is words that most users knew (# Users per word (−)). framework that is composed of two components: 1) embedding models for users (U) and words (V), and an arbitrary similarity scoring function (f) that takes in user (u) and word (v) embedding vectors. , whereσ(·)is the Sigmoid function. The training objective is to minimize the binary cross entropy loss betweenyandˆy. For the former model MF,fis simply a dot product operation; hence, Equation 2 becomesσ(u · v). On the other hand, for the latter modelNCF,fis a multi-layer perceptron network that takes[u; v]as input (concatenation). In addition, we initializeVwith pre-trained word2vec embeddings (Mikolov et al., 2013). In this section, we describe how we set up the experiments in order to evaluate the usability of this PWR dataset. We use accuracy, precision, recall, and f1-score as metrics for evaluation. We experiment usingword2vecas initialization and also experiment with different number of layers forNCF. Figure 2: Joint plot on the marginal distributions. The y-axis accuracy is calculated only on the questions that the marked words come from. Other versions (hexbin & scatter plots) are available in Appendix C. Table 4: Experiment results on test set. Each model is run 5 times with different seeds; hence, each result entry is in the form of mean+−standard deviation. Appendix A describes in detail the hyperparameters and other training details. Table 4 summarizes the experiment results on the test set. In general, we can see that adding word2vecto bothMFandNCFhelps the performance – whether the effect is signiﬁcant or minor. Interestingly, the effect ofword2vecis much stronger onMFwhich can be attributed to the smaller model capacity. We can also see that the models are overall making balanced predictions as accuracy and F1-score are similar. Finally, while the comparison betweenNCFis inconclusive, they certainly outperformMF. From these results, we show that PWR can serve as a challenging benchmark dataset. Similar evaluation results on the dev split are shown in Appendix A. In this section, we discuss the implications of PWR to the research community. First of all, vocabulary prediction itself can be a core component to other NLP tasks like lexical simpliﬁcation (Yeung and Lee, 2018; Lee and Yeung, 2018) or automated essay scoring (Taghipour and Ng, 2016). Ehara (2018) has already shown that TOEIC score prediction has a high correlation with vocabulary predictions. Similarly, we can try to improve neural essay scoring (Taghipour and Ng, 2016) with our dataset. Secondly, we believe that our work can fundamentally improve computer-assisted language learning through efﬁcient and effective personalized vocabulary acquisition. For example, as mentioned before, an automated ﬂashcard generator would be able to signiﬁcantly increase both efﬁciency and effectiveness of memorizing words. Finally, PWR can more realistically extend Knowledge Tracing to vocabulary-level. There were also a few works that incorporate question text information for Knowledge Tracing (Liu et al., 2019; Pandey and Srivastava, 2020; Tong et al., 2020), but these works mostly focused on utilizing word-level embeddings for obtaining question embeddings. On the other hand, applying Deep Knowledge Tracing models (Piech et al., 2015; Zhang et al., 2017; Ghosh et al., 2020) to PWR will allow us to have a better understanding of user knowledge states. In this paper, we focus on solving the inefﬁciency and ineffectiveness of vocabulary acquisition during second language (L2) learning. For this purpose, we formalized and introduced a novel task called Pedagogical Word Recommendation that adds a recommendation system perspective to the conventional vocabulary prediction task. We provide a novel dataset that is large-scale (∼36M) for this task and describe how it was created along with detailed statistics and analysis. Finally, we show the efﬁcacy of this dataset not only as a benchmark but for a test-bed of bridging the gap between NLP and education.