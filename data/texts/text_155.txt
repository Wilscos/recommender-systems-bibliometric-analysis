As research and applications of machine intelligence progress, more concerns are arising questioning whether the deployed AI systems fair, explainable, and ethical. Recommender systems are ubiquitous in social networks, streaming services, e-commerce, web marketing and advertising, web-search to provide personalized experience. Their algorithms use the new data from the system to improve their future performance. A positive feedback loop results in that only a small subset of available items is presented to the user. This effect is observed when user interests are being reinforced by previous exposure to speciﬁc items or item categories, thus producing self-induced concept drift. The feedback loop effect is studied in real and model systems as an undesirable phenomenon related to reliable and ethical AI. Some of the consequences of the effect are induced shift in users interests [ in recommendations [ prices prediction [ lacking for many cases [10, 1]. In this paper we extend prior results [ the user behavior. We speciﬁcally consider noise as a random shift in user interests to items and categories the user is exposed to. In Section 3 we theoretically derive existence conditions for two noise models and explore our predictions empirically in sections 4 and 5 with four bandit algorithms in a simulated environment. Multi-armed bandits are commonly used in online recommendation systems and experiment design [ results demonstrate how feedback loops inﬂuence distribution of recommendations and user preferences. In [ show that posterior distribution in Thompson Sampling algorithms is affected by the feedback loop, which worsens regret performance. We explore a hidden feedback loops effect in online recommender systems. Feedback loops result in degradation of online multi-armed bandit (MAB) recommendations to a small subset and loss of coverage and novelty. We study how uncertainty and noise in user interests inﬂuence the existence of feedback loops. First, we show that an unbiased additive random noise in user interests does not prevent a feedback loop. Second, we demonstrate that a non-zero probability of resetting user interests is sufﬁcient to limit the feedback loop and estimate the size of the effect. Our experiments conﬁrm the theoretical ﬁndings in a simulated environment for four bandit algorithms. Keywords hidden feedback loops · recommender systems · multi-armed bandits 8]. Nevertheless, a full description of the feedback loop effect and its existence conditions is still In [6] authors argue that a feedback loop would exist in a stochastic multi-armed bandit recommender system under some mild assumptions unless a set of available items does not grow at least linearly. Whereas noise and sudden shifts in user interest are important phenomena that occur in practice, they are not usually taken into account. An important contribution of this paper is that we show that a feedback loop would not occur even with a bounded set of available items if user interests are affected by speciﬁc kinds of random noise. Let us consider a recommender system with a single user and available items selects different items interest to item interest. After that the user examines the items and responds with proportionally to the user interest in item that has Bernoulli distribution: user interests abides to the monotonicity constraint for each element a updated according to the rule: whereδ itemais a random variable with a uniform distribution when users act independently and recommendations for one user do not affect others. Following Jiang et al. [ when a 2−norm of user interests grows to inﬁnity with step number t: Let us sate an online MAB recommendation problem. There is a set of probability distribution of reward associated with it. The agent can play the levers. The goal of the agent is to maximize the reward or, in other terms, minimize the regret, which is difference between the achieved and the maximum total reward. We correspond levers with available items user responses problem that the agent is trying to solve is Thompson Sampling c= 1probability if a lever is played and initialize them at The posterior distribution conditioned on user responses is then given by parameters α -greedy probability Policy state is updated from the user feedback c Optimal. policy that needs to be updated. The policy is optimal in a sense that it knows the actual user interests and selects levers with the highest expected reward. aat steptis described by a functionµ: M → R. Larger values ofµ(a)correspond to stronger ∼ Uniform[0, 0.01]indicates how much does the user interest change at stept. The initial user interestµto cat stept. User is the environment and the agent executes an item selection policyS. The optimizationPP [13]. With probabilitythe policy decides to explore and selectsllevers at random uniformly. With 1 − it returns topllevers with the highest mean rewardsd/n,dis total accumulated rewards for itema. The policy selectslitems with the highest user interestµat each step. There is no internal state for the Random. any internal state to update. Let us drop the assumption made in Section 3 that user interests rule. Instead we model user interest in item unbiased random noise, E ω whereδ and bounded noise we deﬁne a constant best levers condition when for all t > t Statement 1 Let the recommender system with noise (5) satisfy the constant best levers condition. Then As follows from this statement any bounded unbiased additive noise in a form of [5] does not prevent a feedback loop from occurring. Users may sometimes lose or forget their interest to items. We call such event an interest restart. Restarts may be caused by satisfaction of the interest, change of users agenda outside of the system or introduction of a competing interest, disappearance of the item itself and other reasons. We consider a linear model of the aforementioned effect. When a restart occurs with probability the same uniform initial distribution as µ where ∆ We set up an empirical evaluation to test if our assumptions about system behavior are valid and to check whether the theoretical results hold in practical conditions. RQ 1. A positive feedback loop occurs even when an unbiased additive noise is added to user interests as demonstrated by Statement 1. RQ 2. The system does not need to exhibit exactly the constant best levers regime for results of Statement 1 and Statement 2 to hold. RQ 3. Results of Statement 1 and Statement 2 hold for a range of different lever selection polices. We implement a recommender system [ policies. We include Thomson Sampling (TS) and policies as baselines. Both latter policies do not require learning. The following parameters are varied during the experiment: number of items available for recommendation parameters. We run the experiment for a maximum of get an estimate of the conﬁdence interval for results. For TS policy the priors are set to to reﬂect the uniform distribution. The policy samples rewards from the prior The policy selects each item fromMwith the same probability1/|M|at random. The policy does not have remains the same as in(1)andcnow depends on the noisy user interestµ. We consider a case with uniform = µ− µas deﬁned in (1). Note that now the update rule is not monotonic. Let the recommender system with restarts(7),(1)satisfy the constant best levers condition. Ifµ> 0then Figure 1: Maximum user interest (color) for Thompson Sampling (TS) policy with different and strength of additive noise w at step t = 2000. the highest sampled rewards policies do not require initialization. The Optimal policy returns with new user interests trial starts, with probability The policy is updated with user responses parameter. In Interest restarts model we set a probability of restart q and scale s parameters. The experiment proceeds as follows. A grid of experiment parameters with with is set up before start and random seed is ﬁxed. Then tuples of parameters are retrieved from the grid and a number of trials is run. At each trial an experiment instance is initialized with parameters taken from the tuple. The selected user interests model and policy are initialized with corresponding parameters. Only one of the models and policies are used in each trial. At each step we store user feedback -greedy policy. We show how the total reward changes for the Thompson Sampling (TS) selection policy when the step number 0 ≤ t ≤ 2000 uses a logarithmic scale. The maximum value of user interest estimated over 30 runs. Although the growth rate decreases with Thus for the parameters explored in the experiment we can conﬁrm RQ 1 for TS selection policy. We found that constant best lever assumption holds for Thompson Sampling (TS) selection policies most of the time. While property. At Fig. 2 we compare different selection policies for the additive noise model: Thompson Sampling (TS), Random, -greedy, Optimal. The ﬁgure shows the user interest kµ over and conﬁdence intervals are shown for 30 runs. We can see that all policies exhibit a feedback loop when noise w = 3.0. The interest grows much slower for the Random selection policy but the system still exhibits a feedback loop. At Fig. 3 we show the amplitude of the feedback loop also plot the expected upper bound predicted by 7 on the same ﬁgure. It can be seen that at higher becomes tight. When the restart probability is low, non-optimality of the selection policy and insufﬁcient number of steps T = 5000 limits the growth of user interests. That is, the received reward is also bounded by t · E δ With this we can conﬁrm that Statement 2 even when the constant best levers condition does not strictly hold. We also explore the interest restarts model for different values of available and obtained similar results. Thus, RQ 2 holds. Results for different policies with the interest restarts model are also shown at Fig. 3. Results are averaged over and conﬁdence intervals are shown for 30 runs. As we can see, the predicted upper bound 8 holds in all cases, although the Random policy demonstrates smaller growth in user interest than other policies, as expected. Considering the results we can conﬁrm RQ 3 that Statement 1 and Statement 2 do not depend on the selection policy used. In the experiment we ﬁnd that additive noise does not prevent a feedback loop from occurring for several selection policies, and that interest restarts does limit the feedback loop. An upper bound is given by Statement 2 and it is tested at Fig. 1 with different values of additive noisew ∈ {0.0, 0.3, 1.0, 3.0, 5.0, 10.0}. Note that the ﬁgure kgrows for allwand this does not contradict Statement 1. Results are averaged and conﬁdence intervals are Figure 2: Maximum user interest (color) for Thompson Sampling (ts), Random, Optimal and policies. Axes: y-available items w = 3.0 Figure 3: Maximum user interest averaged over 10 runs (blue dots) with interest restarts model for Thompson Sampling (ts), Random, Optimal and (8)for given scale and restart probability. Axes: y- scale parameter Total number of steps T = 5000. in the experiments. Therefore, if an interest restart is possible in the system then the feedback loop is limited. The probability of restart and scale parameters should be estimated from the actual user behavior, which is a possible future direction of research. We ﬁnd the constant best lever assumption useful for theoretical analysis as it greatly simpliﬁes the proof. Results of the analysis are still valid when the assumption is violated in a feedback loop, as experiments show, because the feedback loop just reinforces the best lever already selected by the policy. Limitations and validity. rewards. Non-stationary selection policies, such as Discounted Thompson Sampling (dTS) [ suitable in case of sudden changes in user interests, especially for the interest restarts model. Our study is limited to theoretical models, which parameters need to be estimated from the actual user behavior. Nevertheless, our results still hold and useful, because we specify how predictions depend on the parameters even when assumptions are relaxed. We state a problem of existence of feedback loops in presence of noise in user interests. We explore unbiased additive noise model and demonstrate that such type of noise does not affect the existence of the feedback loop both theoretically and experimentally. We also develop an interest restart model that models cases when users partly or completely lose interest recommended items. For this model we show that there exists an upper bound on the feedback loop if a restart is possible. We conﬁrm our ﬁndings in the experiment. Further research could focus on parameter estimation and studying the non-stationary multi-armed problem.