overload has become an increasingly crucial challenge. Personalized recommender systems act as an indispensable tool to help users ﬁnd their preferred information from massive irrelevant contents. Nowadays, users are easily accessible to large amounts of online information represented in multiple modalities, including images, texts, videos, etc. For example, the visual appearance and textual descriptions play important roles when users selecting products online; the visual cover and textual tags allow users to ﬁnd interesting items from a large amount of instant videos. Recent years have witnessed growing research interests in multimedia recommendation, which aims to predict whether a user will interact with an item with multimodal contents. It has been successfully applied to many online applications, such as e-commerce, instant video platforms and social media platforms. Collaborative Filtering (CF), as one of the most prevalent techniques in personalized recommendation, have been widely studied. Focusing on exploiting abundant user-item interactions, CF methods group users according to their historical interactions, by encoding users and items into lowdimensional dense vectors and making recommendations based on these embeddings [2,3,4]. Following traditional CF framework, early work on multimedia recommendation like VBPR [5], DeepStyle [6], and ACF [7] incorporates multimodal features as side information in addition to the learned dense vectors of items, so as to group users based on both historical interactions and item contents. However, since these methods only model direct user-item interactions, their expressiveness is conﬁned. Inspired by the recent surge of graph neural networks [8,9], Wang et al.[10]propose to model user-item relationships as bipartite graphs. The ﬁrst-order connectivities in user-item graphs indicate the interaction history. And the second-order connectivities reveal collaborative relations that similar users (or items) who have co-interacted with the same items (or users). These graph-based recommender systems [10,11,12] inject high-order connectivities into the embedding process to learn better representations and achieve great success. Recently, many attempts have been made to integrate multimodal contents into graphbased recommendation systems. MMGCN [13] constructs modality-speciﬁc user-item interaction graphs to model user preferences speciﬁc to each modality. Following MMGCN, GRCN [14] utilizes multimodal features to reﬁne user-item interaction graphs by identifying false-positive feedbacks and prunes the corresponding noisy edges. Despite their effectiveness, previous attempts suffer from two limitations. Firstly, existing work fails to explicitly model item-item relationships, which have been proved to be important in recommender systems [15]. Speciﬁcally, onlycollaborativerelations are implicitly considered through high-order item-user-item co-occurrences. However, semanticrelations, reﬂecting the content information of items which also helps inferring users’ preferences, are not explicitly modeled. Taking Figure 1 as an example, existing methods will recommend the shirt ( ) foruaccording to collaborative relations, since shirts ( ), hats ( ), and pants ( ) all interacted withu. However, previous work may not be able to recommend coats ( ) tou, which are semantically (visually in this example) similar to shirts. Considering that items are associated with rich multimodal content features in multimedia recommendation, there exist a wealth of semantic relations underlying multimodal contents, which would assist the recommender models to comprehensively discover candidate items. Secondly, previous attempts disregard the ﬁne-grained multimodal fusion: early work [5,6,16] only focuses on unimodal information; other work on multimedia recommendation [13,14] conducts multimodal fusion by simple linear combination or concatenation. We argue that ﬁnegrained multimodal fusion is important to learn better item representations by exploiting the correlation and interactions between modalities. Based on the hypothesis that a powerful representation is one that models modality-invariant factors [17] which has exhibited remarkable beneﬁts in many multimodal tasks [18,19,20], we propose to conduct ﬁne-grained multimodal fusion by capturing shared item relationships from multiple modalities, which would facilitate learning better item representations and making more accurate recommendations. In this paper, we propose a novel method to mine latent semantic item-item relationships underlying multimodal features of items, and conduct ﬁne-grained multimodal fusion based on the learned structures to inject shared item-item relationships from multiple modalities into the item representations. As shown in Figure 2, the proposed MICRO consists of four key components. Firstly, we develop a novel modality-aware structure learning layer, which learns modality-aware item structures from content features of each modality. Secondly, we perform graph convolutions on the learned modality-aware latent graphs to explicitly consider item relationships of each modality individually. Thirdly, we devise a novel multimodal contrastive framework and construct self-supervision signals by maximizing the agreement between item representations under individual modalities and the multimodal fused representations, and thus the fused multimodal representations can adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. Finally, the resulting enhanced item representations are infused with item relationships in multiple modalities, which will be added into the output item embeddings of CF models to make recommendations. Our work enjoys two additional beneﬁts. Firstly, MICRO can alleviate the cold-start problem. Previous graph-based multimedia recommendation methods face cold-start problems where long-tailed items are only interacted with few users or even never interacted with users. Since previous methods utilize multimodal content features based on useritem interaction graph, those long-tailed items will become isolated nodes in graph, which will invalidate their usage of multimodal information. Our work, on the contrary, can alleviate the cold-start problem in two ways: ﬁrstly, we mine latent item-item structures and the long-tailed items will get similar user feedbacks from their learned neighbors; secondly, the multimodal contrastive framework serves as a self-supervised auxiliary task. The external self-supervision signals are introduced to learn better item representations involved with relation information, which would further alleviate the cold-start problem. Secondly, MICRO can serve as a ﬂexible play-and-plug module. Unlike previous attempts which utilize multimodal features based on dedicated user-item aggregation strategies, MICRO separates the usage of multimodal features with the usage of user-item interactions and is agnostic to downstream CF methods. In summary, the main contribution of this work is threefold. We highlight the importance of explicitly exploiting item relationships and considering ﬁne-grained multimodal fusion in multimedia recommendation. We propose a novel method to mine latent item relations and conduct ﬁne-grained multimodal fusion based on the mined structures. We perform extensive experiments on three public datasets. Notably, our method outperforms the state-ofthe-art methods by 20% on average in terms of different metrics, validating the effectiveness of our proposed model. To foster reproducible research, our code is made publicly available at https://github.com/CRIPAC-DIG/MICRO. In this section, we ﬁrst formulate the multimedia recommendation problem and introduce our model in detail. As illustrated in Figure 2, there are four main components in our proposed framework: (1) a modality-aware graph structure learning layer that learns item graph structures from content features of each modality, (2) graph convolutional layers that learn the modality-aware item embeddings by injecting item-item afﬁnities based on the learned graph structures, (3) an attentive multimodal fusion framework with contrastive auxiliary task to promote ﬁne-grained multimodal fusion, and (4) downstream CF methods. 2.1 Preliminary LetU,I(|I| = N)denote the set of users and items, respectively. Each useru ∈ Uis associated with a set of itemsIwith positive feedbacks which indicate the preference scorey= 1fori ∈ I.x, x∈ Ris the input ID embedding ofuandi, respectively, wheredis the embedding dimension. Besides user-item interactions, multimodal features are offered as content information of items. We denote the modality features of itemiase∈ R, whereddenotes the dimension of the features,m ∈ Mis the modality, andMis the set of modalities. The purpose of multimedia recommendation is to accurately predict users’ preferences by ranking items for each user according to predicted preference scoresˆy. In this paper, we consider visual and textual modalities denoted byM = {v, t}. Please kindly note that our method is not ﬁxed to the two modalities and multiple modalities can be involved. 2.2 Modality-aware Latent Structure Mining Multimodal features provide rich and meaningful content information of items, while existing methods only utilize Graph ConvolutionsContrastive Fusionembedding multimodal features as side information for each item, ignoring the important semantic relationships of items underlying features. In this section, we introduce how to discover the underlying latent graph structure of item graphs in order to learn better item representations. To be speciﬁc, we ﬁrst construct initialk-NearestNeighbor (kNN) modality-aware item graphseSby utilizing raw multimodal features. After that, we learn the latent graph structureseAfrom transformed multimodal features. Finally, we combine the learned structures with the initial structures by a skip connection. We ﬁrst construct initialkNN modality-aware graphS by using raw features for each modalitym. Based on the hypothesis that similar items are more likely to interact than dissimilar items [21], we quantify the semantic relationship between two items by their similarity. Common options for node similarity measurement include cosine similarity [22], kernel-based functions [23], and attention mechanisms [24]. Our method is agnostic to similarity measurements, and we opt to the simple and parameter-free cosine similarity in this paper. The similarity matrix S∈ Ris computed by Typically, the graph adjacency matrix is supposed to be nonnegative butSranges between[−1, 1]. Thus, we suppress its negative entries to zeros. Moreover, common graph structures are much sparser other than a fully-connected graph, which is computationally demanding and might introduce noisy, unimportant edges [24]. We conductkNN sparsiﬁcation [25] on the dense graph: for each itemi, we only keep edges with the top-k conﬁdence scores: whereSdenotes thei-row ofS, andbSis the resulting sparsiﬁed, directed graph adjacency matrix. To alleviate the exploding or vanishing gradient problem [8], we normalize the adjacency matrix as: whereD∈ Ris the diagonal degree matrix ofbSP 2.2.2 Learning Latent Modality-aware Graphs Although we have obtained the modality-aware initial graph structureseSby utilizing raw multimodal features, they may not be ideal for the recommendation task. This is because the raw multimodal features are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. To this end, we propose to dynamically learn the graph structures by the transformed multimodal features and combine the learned structures with initial ones. Firstly, we transform raw modality features into highlevel featuresee: whereW∈ Randb∈ Rdenote the trainable transformation matrix and the bias vector, respectively. We dynamically infer the graph structures utilizingee, repeat the graph learning process described in Eqs. (1, 2, 3) and obtain the adjacency matrixeA. Although the initial graph could be noisy, it still carries rich and useful information regarding item graph structures. Also, drastic change of adjacency matrix will lead to unstable training. To keep rich information of initial item graph and stabilize the training process, we add a skip connection that combines the learned graph with the initial graph: whereλ ∈ (0, 1)is the coefﬁcient of skip connection that controls the amount of information from the initial structure. The obtainedAis the ﬁnal graph adjacency matrix representing latent structures for modality m. It is worth mentioning that botheSandeAare sparsiﬁed and normalized matrices, thus the ﬁnal adjacency matrixAis also sparsiﬁed and normalized, which is computationally efﬁcient and stabilizes gradients. 2.3 Item Afﬁnities Learning with Graph Convolutions After obtaining the modality-aware latent structures, we perform graph convolution operations to learn better item representations by injecting item-item afﬁnities into the embedding process. Graph convolutions can be treated as message propagation and aggregation. Through propagating the item representations from its neighbors, one item can aggregate information within the ﬁrst-order neighborhood. Furthermore, by stacking multiple graph convolutional layers, the high-order item-item relationships can be captured. Following Wu et al.[26]and He et al.[12], we employ simple message propagation and aggregation without feature transformation and non-linear activations which is effective and computationally efﬁcient. In thel-th layer, the message passing and aggregation could be formulated as: whereH∈ Ris thel-th layer item embedding matrix of modalitym, thei-th row of which denotes the embedding vector of itemi. For all modalitiesm ∈ M, we use the same item ID embedding matrix to initialize the input embedding matrixH. We utilize ID embedding vector of items as input representations rather than multimodal features, since we employ graph convolutions to directly capture itemitem afﬁnities and multimodal features are used to bridge semantic relationships. After stackingLlayers,Hencodes the high-order item-item relationships of modality m. 2.4 Multimodal Fusion with Contrastive Auxiliary Task Multiple modalities convey comprehensive information [27]. Item relations shared between modalities are important to learn better item representations based on the hypothesis that a powerful representation is one that models modalityinvariant factors [17,18]. To this end, we ﬁrst utilize an attention mechanism to fuse item embeddingsHof different modalities, and then devise a self-supervised contrastive loss to promote multimodal fusion. 2.4.1 Aggregating Multiple Modalities We omit the subscript(L)and usehto denote thei-th row ofH, which is the output embedding of graph convolutions corresponding to itemi. The importance of each modality corresponding to itemican be formulated as follows: whereq ∈ Rdenotes attention vector andW ∈ R, b ∈ Rdenote the weight matrix and bias vector, respectively. These parameters are shared for all modalities. After obtaining the importance of different modalities, we normalize them to get the weight coefﬁcients: Then, the multimodal fused embedding of itemican be represented as: 2.4.2 Contrastive Auxiliary Task After obtaining the multimodal fused item embeddings, we devise a novel self-supervised auxiliary task to further force the fused item embeddings to adaptively distill the shared information from multiple modalities. Existing contrastive learning frameworks [28] seek to maximize the agreement among differently augmented views of the same data examples, which has been proven to be effective in multiview representation learning [18,29] and multimodal tasks [30,31]. In this work, since multiple modality-aware graphs are involved, we propose to construct self-supervision signals by maximizing the agreement between item representations under individual modalities and the fused multimodal representations. In this way, the fused multimodal representations can adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. The resulting contrastive loss can be mathematically expressed as: whereI(·, ·)denotes the mutual information which quantiﬁes the agreement between two representations, which is implemented by the InfoNCE estimator [28]. Speciﬁcally, we set(h, h)as positive samples, while all other item embeddings in an individual modality(h, h)and the fused multimodal embeddings(h, h)are considered as negatives: I(h, h) = whereτ ∈ Ris a temperature parameter andθ(·, ·)is the critic function which is implemented by a simple cosine similarity. The proposed objective also conceptually relates to contrastive knowledge distillation [32], where several teacher models (representations under different individual modalities) and one student model (the fused representations) are employed. By forcing the embeddings between several teachers and a student to be the same, these fused representations adaptively collect information from all modality-aware item relations. Additionally, the multimodal contrastive framework serves as a self-supervised auxiliary task, where the external self-supervision signals are introduced to learn better item representations involved with relation information from multiple modalities, which would further alleviate the cold-start problem. Unlike previous attempts which utilize multimodal features based on sophisticated user-item aggregation strategies, MICRO separates the usage of multimodal features with the usage of user-item interactions and is agnostic to downstream CF methods. Speciﬁcally, we learn item representations from mined item relations and then combine them with downstream CF methods that model user-item interactions. It is ﬂexible and could be served as a play-and-plug module for any CF methods. We denote the output user and item embeddings from CF methods asex,ex∈ Rand simply enhance item embeddings by adding normalized multimodal fused item embeddings h: We then compute the user-item preference score by taking inner product of user embeddings and enhanced item embeddings: Additionally, the play-and-plug paradigm separates the usage of multimodal features with user-item interactions, thus alleviating the cold-start problem, where the long-tailed items are only interacted with few users or even never interacted with users. We learn latent structures for items and the tailed items will get similar feedbacks from relevant neighbors through neighborhood aggregation. We adopt the Bayesian Personalized Ranking (BPR) loss [33] to compute the pair-wise ranking, which encourages the prediction of an observed entry to be higher than its unobserved counterparts: whereIindicates the observed items associated with user uand(u, i, j)denotes the pairwise training triples where i ∈ Iis the positive item andj /∈ Iis the negative item sampled from unobserved interactions.σ(·)is the sigmoid function. The overall loss function can be formulated as: whereβ ∈ Ris a hyper-parameter to control the effect of the contrastive auxiliary task. In this section, we conduct experiments on three widely used real-world datasets to answer the following research questions: state-of-the-art multimedia recommendation methods and other CF methods in both warm-start and cold-start settings? learning modules contribute to the model performance? of several key hyper-parameters? 3.1 Experimental Settings 3.1.1 Datasets We conduct experiments on three categories of widely used Amazon datasets introduced by McAuley et al.[34]: (a) Clothing, Shoes and Jewelry, (b) Sports and Outdoors, and (c) Baby, which we refer to asClothing,SportsandBabyin brief. The statistics of these three datasets are summarized in Table 1. The three datasets include both visual and textual modalities. We use the 4,096-dimensional visual features that have been extracted and published. For the textual modality, we extract textual embeddings by concatenating the title, descriptions, categories, and brand of each item and utilize sentence-transformers [35] to obtain 1,024-dimensional sentence embeddings. 3.1.2 Baselines To evaluate the effectiveness of our proposed model, we compare it with several state-of-the-art recommendation models. These baselines fall into two groups: CF methods (i.e., MF, NGCF, LightGCN) and deep content-aware recommendation models (i.e., VBPR, MMGCN, GRCN). Bayesian personalized ranking (BPR) loss, which exploits the user-item direct interactions only as the target value of interaction function. a bipartite graph. By leveraging graph convolutional operations, it allows the embeddings of users and items to interact with each other to harvest the collaborative signals as well as high-order connectivity signals. design of GCNs (i.e., feature transformation and nonlinear activation) for recommendation systems and proposes a light model which only consists of two essential components: light graph convolution and layer combination. visual features and ID embeddings of each item as its representation and feeds them into the Matrix Factorization framework. In our experiments, we concatenate multi-modal features as content information to predict the interactions between users and items. recommendation methods, which constructs modalspeciﬁc graphs and reﬁnes modal-speciﬁc representations for users and items. It aggregates all model-speciﬁc representations to obtain the representations of users or items for prediction. modal recommendation methods. It reﬁnes user-item interaction graph by identifying the false-positive feedback and prunes the corresponding noisy edges in the interaction graph. 3.1.3 Evaluation Protocols We conduct experiments in both warm-start and cold-start settings. Warm-start settings.For each dataset, we select 80% of historical interactions of each user to constitute the training set, 10% for validation set, and the remaining 10% for testing set. For each observed user-item interaction, we treat it as a positive pair, and then conduct the negative sampling strategy to pair them with one negative item that the user does not interact before. Cold-start settings.We remove all user-item interaction pairs associated with a randomly selected 20% item set from the training set. We further divide the half of the items (10%) into the validation set and half (10%) into the testing set. In other words, these items are entirely unseen in the training set. We adopt three widely-used metrics to evaluate the performance of preference ranking: Recall@k, NDCG@k, and Precision@k. By default, we setk = 20and report the averaged metrics for all users in the testing set. 3.1.4 Implementation Details We implemente our method in PyTorch [36] and set the embedding dimensiondﬁxed to 64 for all models to ensure fair comparison. We optimize all models with the Adam [37] optimizer, where the batch size is ﬁxed at 1024. We use the Xavier initializer [38] to initialize the model parameters. The optimal hyper-parameters are determined via grid search on the validation set: the learning rate is set to0.0005, the coefﬁcient of`normalization is set to10. ThekofkNN sparsiﬁcation is set to10, theλof skip connection is set to 0.7, the temperature parameterτis set to0.5, the coefﬁcient βused to control the effect of contrastive auxiliary task is set to0.03. Besides, we stop training if Recall@20 on the validation set does not increase for 10 successive epochs to avoid overﬁtting. 3.2 Performance Comparison (RQ1) We start by comparing the performance of all methods, and then explore how the our method alleviate the coldstart problem. In this subsection, we combine MICRO with LightGCN as a downstream CF method, and will also conduct experiments with different CF methods in Section 3.3. 3.2.1 Overall Performances Table 2 reports the performance comparison results, from which we can observe: Our method MICRO signiﬁcantly outperforms both CF methods and content-aware methods, verifying the effectiveness of our methods. Speciﬁcally, MICRO improves over the strongest baselines in terms of Recall@20 by 24.1%, 18.6%, and 18.3% in Clothing, Sports, and Baby, respectively. This indicates that our proposed method is well-designed for multimedia recommendation by discovering underlying item-item relationships and conducting ﬁne-grained multimodal fusion through the contrastive auxiliary task. Compared with CF methods, content-aware methods yield better performance overall, which indicates that multimodal features provide rich content information about items, and can boost recommendation accuracies. GRCN outperforms other baselines in three datasets since it discovers and prunes false-positive edges in user-item interaction graphs. Additionally, existing content-aware recommendation models are highly dependent on the representativeness of multimodal features and thus obtain ﬂuctuating performances over different datasets. For Clothing dataset where visual features are very important in revealing item attributes [5,6], VBPR, MMGCN, and GRCN outperform all CF methods. For the other two datasets where multimodal features may not directly reveal item attributes, content-aware methods obtain relatively small improvements. The performances of VBPR and MMGCN are even inferior to the CF method LightGCN. Different from existing content-aware methods, we discover the latent item relationships underlying multimodal features instead of directly using them as side information. The latent item relationships are less dependent on the representativeness of multimodal features, and thus we are able to obtain robust performance. 3.2.2 Performances in Cold-start Settings The cold-start problem remains a prominent challenge in recommendation systems [39]. Multimodal features of items provide rich content information, which can be exploited to alleviate the cold-start problem. We conduct cold-start experiments and compare with representative baselines. MICRO w/o. contrastis the simpliﬁed variant of MICRO, which discards the contrastive auxiliary task in Section 2.4.2 and only utilizes the BPR loss in Eq. (14). Figure 3 reports the results of performance comparison, from which we can observe: Both MICRO w/o. contrast and MICRO can alleviate the cold-start problem and outperform all baselines on three datasets. They learn item graphs from multimodal features, along which cold-start items will get similar feedbacks from relevant neighbors through neighborhood aggregation of graph convolutions. Additionally, MICRO outperforms MICRO w/o. contrast on three datasets. In MICRO, the multimodal contrastive framework serves as a self-supervised auxiliary task. The self-supervision signals are constructed by maximizing the agreement between item representations under individual modalities and the multimodal fused representations to learn better item representations which encode item relationships from multiple modalities. In this way, the cold-start problem would be further alleviated. CF methods MF and LightGCN obtain poor performances under cold-start settings in general, primarily because they only leverage users’ feedbacks to predict the interactions between users and items. Although these methods may work well for items with sufﬁcient feedbacks, they cannot help in cold-start settings, since no user-item interaction is available to update the representations of cold-start items. Content-aware model VBPR outperforms CF methods in general, which indicates that the content information provided by multimodal features beneﬁts recommendation for cold-start items. In particular, content information can help bridge the gap between the existing items to cold-start items. However, some graph-based contentaware methods such as GRCN, although perform well in warm-start settings, obtain poor performance in coldstart settings. GRCN utilizes multimodal features on user-item interaction bipartite graphs, which is also heavily dependent on user-item interactions. For coldstart items, they never interact with users and become isolated nodes in the user-item graphs, leading to inferior performance. 3.3 Ablation Studies (RQ2) In this subsection, we combine MICRO with three commonused CF methods, i.e., MF, NGCF, and LightGCN to validate the effectiveness and ﬂexibility of our proposed method. For each CF method, we have three other variants: the ﬁrst one isCF+feats, which does not consider latent itemitem relationships and directly uses transformed multimodal features to replace the item representations learned from item graphs in Eq. (12). The second one is named asMICRO/feats, which uses multimodal features as the input initial item embeddings of graph convolutions instead of ID embeddings. The third isMICRO w/o. contrast, which discards the contrastive auxiliary task in Section 2.4.2 and only utilizes the BPR loss in Eq. (14). Table 3 summarizes the performance and the relative improvements gained byMICROoverCF+feats, from which we have the following observations: MICRO signiﬁcantly and consistently outperforms all original CF methods and CF+feats variants on three datasets, obtaining up to 68.8% improvements over the CF+feats variants, verifying the ﬂexibility of our plug-in paradigm. Even without the contrastive auxiliary task, MICRO w/o. contrast obtains signiﬁcant improvements over CF+feats, indicating the effectiveness of discovering latent item-item relationships from multimodal features. Furthermore, the improvements between MICRO and MICRO w/o. contrast show the importance of ﬁnegrained multimodal fusion, through which we can capture item relationships shared between modalities adaptively. Based on the learned item graph structures, MICRO/feats employs graph convolutions on multimodal features. Our proposed method MICRO utilizes the same learned structures but employ graph convolutions on item ID embeddings, which aims to directly model item afﬁnities. The improvements between them validate the effectiveness of explicitly modeling item afﬁnities where multimodal features are only used to bridge semantic relationships between items. 3.4 Sensitivity Analysis (RQ3) Since the graph structure learning layer and the contrastive auxiliary task play pivotal roles in our method, in this subsection, we conduct sensitivity analysis with different 7.23.5 7.03.27.7 0 5 10 15 20 25 300.0 0.2 0.4 0.6 0.8 1.03.4 9.04.6 8.59.84.5 8.09.7 0 5 10 15 20 25 300.0 0.2 0.4 0.6 0.8 1.0 7.03.5 0 5 10 15 20 25 306.03.00.0 0.2 0.4 0.6 0.8 1.0 hyper-parameters on graph structure learning layers and the contrastive auxiliary task. Firstly, we investigate performance of MICRO-LightGCN with respect to differentkvalue of the k-NN sparsiﬁcation operation sincekis important which determines the number of neighbors of each item, and controls the amount of information propagated from neighbors. Secondly, we discuss how the skip connection coefﬁcient λaffects the performance which controls the amount of information from the initial graph structures. Finally, we explore how the contrastive auxiliary task magnitudeβ affects the performance. 3.4.1 Impact of Varied k Values Figure 4 reports the results of performance comparison. k = 0means no item relationships are included and the model is degenerated to LightGCN. We have the following observations: Our method gains signiﬁcant improvement between k = 0andk = 5, which validates the rationality of item relationships mined from multimodal features. Even if only a small part of the neighbors are included, we can obtain better item representations by aggregating meaningful and important information from the neighbors, which boost the recommendation performance. Furthermore, the performance ﬁrst improves askincreases, which veriﬁes the effectiveness of information aggregation along item-item graphs since more neighbors bring more meaningful information that helps to make more accurate recommendations. The trend, however, declines whenkcontinues to increase, since there may exist many unimportant neighbors that will inevitably introduce noise to information propagation. This demonstrates the necessity of conducting kNN sparsiﬁcation on the learned dense graph. 3.4.2 Impact of Varied Coefﬁcients λ Figure 4 reports the performance comparison.λ = 0means only consider the graph structure learned by the transformed multimodal features, andλ = 1means we only consider the initial structure generated by the raw multimodal features. We have the following observations: When we setλ = 0, the model obtains poor performance. It only learns graph structure from the transformed features, completely updating the adjacency matrix every time, ignoring the rich and useful information of raw features and resulting in ﬂuctuated training process. The performance ﬁrst grows asλbecomes larger, validating the importance of initial structures constructed by raw multimodal features. However, it begins to deteriorate whenλcontinues to increase, since raw features are often noisy due to the inevitably error-prone data measurement or collection process. Learning the graph structures dynamically can reduce noise. Overall, there are no apparent sharp rises and falls, indicating that our method is not that sensitive to the selection ofλ. Notably, all models surpass the baselines (c.f. Table 2), proving the effectiveness of item graphs. 3.4.3 Impact of Varied Coefﬁcients β We investigate how the contrastive auxiliary task magnitude β affects the performance. Figure 5 reports the performance comparison.β = 0denotes MICRO w/o. contrast, which discards the contrastive auxiliary task. We can observe that With the increase ofβ, the performances on all datasets ﬁrst rise and is always better thanβ = 0. The primary recommendation task achieves decent gains when jointly optimized with the self-supervised auxiliary task even with a small β. However, it begins to decline whenβcontinues to increase. A smallβcan promote the primary task, while a larger one would mislead it. The beneﬁts brought by the self-supervised task could be easily neutralized and the recommendation task is sensitive to the magnitude of the self-supervised task, which is also observed in other recommendation works with contrastive learning [40, 41]. 3.5 The Contribution of Each Modality (RQ4) In this subsection, we aim to explore the contribution of each modality. Table 4 reports the performance comparison over different modalities. We observe that the performances of utilizing multiple modalities are better than that of ones within the single modality, demonstrating that incorporating the information from multiple modalities facilitates comprehensive understanding of items. Additionally, textual modality contributes more than visual modality in general. It is reasonable since textual modality provide more ﬁne-grained information which directly reveals the titles, categories and descriptions of items while visual modality only provides coarse-grained visual appearances. 4.1 Multimodal Recommendation Collaborative ﬁltering (CF) has achieved great success in recommendation systems, which leverage users’ feedbacks (such as clicks and purchases) to predict the preference of users and make recommendations. However, CF-based methods suffer from sparse data with limited user-item interactions and rarely accessed items. To address the problem of data sparsity, it is important to exploit other information besides user-item interactions. Multimodal recommendation systems consider massive multimedia content information of items, which have been successfully applied to many applications, such as e-commerce, instant video platforms and social media platforms [34, 42, 43, 44]. For example, VBPR [5] extends matrix factorization by incorporating visual features extracted from product images to improve the performance. DVBPR [45] attempts to jointly train the image representation as well as the parameters in a recommender model. Sherlock [16] incorporates categorical information for recommendation based on visual features. DeepStyle [6] disentangles category information from visual representations for learning style features of items and sensing preferences of users. ACF [7] introduces an itemlevel and component-level attention model for inferring the underlying users’ preferences encoded in the implicit user feedbacks. VECF [46] models users’ various attentions on different image regions and reviews. MV-RNN [47] uses multimodal features for sequential recommendation in a recurrent framework. Recently, Graph Neural Networks (GNNs) have been introduced into recommendation systems [10,11,48] and especially multimodal recommendation systems [13,14,49]. MMGCN [13] constructs modal-speciﬁc graph and conducts graph convolutional operations, to capture the modal-speciﬁc user preference and distills the item representations simultaneously. In this way, the learned user representation can reﬂect the users’ speciﬁc interests on items. Following MMGCN, GRCN [14] focuses on adaptively reﬁning the structure of interaction graph to discover and prune potential false-positive edges. The above methods directly utilize multimodal features as side information of each item and disregard ﬁne-grained multimodal fusion. In our model, we step further by discovering item-item relationships from multimodal features, and conduct ﬁne-grained multimodal fusion to inject complementary item-item relationships from multiple modalities into the item representations. 4.2 Deep Graph Structure Learning GNNs have shown great power on analyzing graphstructured data and have been widely employed for graph analytical tasks across a variety of domains, including node classiﬁcation [8,50], link prediction [51], information retrieval[52,53], etc. However, most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure that are hard to construct in real-world applications [54]. Since GNNs recursively aggregate information from neighborhoods of one node to compute its node embedding, such an iterative mechanism has cascading effects — small noise in a graph will be propagated to neighboring nodes, affecting the embeddings of many others. Additionally, there also exist many realworld applications where initial graph structures are not available. Recently, considerable literature has arisen around the central theme of Graph Structure Learning (GSL), which targets at jointly learning an optimized graph structure and corresponding representations. There are three categories of GSL methods: metric learning [22,23,24], probabilistic modeling [54,55,56], and direct optimization approaches [57, 58, 59]. For example, IDGL [24] casts the graph learning problem into a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph; DGM [60] predicts a probabilistic graph, allowing a discrete graph to be sampled accordingly in order to be used in any graph convolutional operator. NeuralSparse [55] considers the graph sparsiﬁcation task by removing taskirrelevant edges. It utilizes a deep neural network to learn k-neighbor subgraphs by selecting at mostkneighbors for each node in the graph. We kindly refer to [61] for a recent overview of approaches for graph structure learning. In personalized recommendation, although user-item interactions can be formulated as a bipartite graph naturally, item-item relations remain rarely explored. To model item relationships explicitly, we employ metric learning approaches to represent edge weights as a distance measure between two end nodes, which ﬁts for multimedia recommendation since rich content information can be included to measure the semantic relationship between two items. 4.3 Contrastive Learning Self-supervised learning is an emerging technique to learn representations by self-deﬁned supervision signals generated from raw data without relying on annotated labels. Contrastive learning (CL) has become a dominant branch of self-supervised learning, which targets at obtaining robust and discriminative representations by grouping positive samples closer and negative samples far from each other. For visual data, negative samples can be generated using a multiple-stage augmentation pipeline [28,62,63], consisting of color jitter, random ﬂip, cropping, resizing, rotation, color distortion, etc. The latest advances extend self-supervised learning to graph representation learning [64]. Velickovic et al. [65]introduce an objective function measuring the Mutual Information (MI) between global graph embeddings and local node embeddings. GraphCL [66] and GRACE [50] propose a node-level contrastive objective to simplify previous work. Furthermore, Zhu et al.[67]propose a contrastive method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Generally, most CL work differs from each other in terms of the generation of negative samples and contrastive objectives. There also exist several works combining self-supervised learning with session-based recommendation [68,69], social recommendation [40,69] and multimedia recommendation [70,71]. Zhou et al.[69]utilize contrastive learning to learn the correlations among attribute, item, subsequence, and sequence. Yu et al.[40]employ self-supervised learning to regain the connectivity information with hierarchical mutual information maximization. Wei et al.[71]aim to maximize the mutual information between item content and collaborative signals to alleviate the cold-start problem. In this work, since multiple modality-aware graphs are involved, the individual modality-aware item representations and multimodal fused representations are natural positive pairs. We utilize contrastive learning to maximize the agreement between item representations under an individual modality and the multimodal fused representations. In this way, the fused multimodal representations can adaptively capture itemitem relationships shared between multiple modalities in a self-supervised manner. In this paper, we have proposed the latent structure mining method (MICRO) for multimodal recommendation, which leverages graph structure learning to discover latent item relationships underlying multimodal features and devises a novel contrastive framework to fuse multimodal item relationships. In particular, we ﬁrst develop a modality-aware structure learning layer and graph convolutions to inject modality-aware item relationships into item representations. Furthermore, we propose a novel multimodal contrastive framework to adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. Finally, the resulting enhanced item representations are infused with item relationships in multiple modalities, which will be added into the output item embeddings of CF models to make recommendations. Empirical results on three public datasets have demonstrated the effectiveness of our proposed model. This work was supported by National Key Research and Development Program (2018YFB1402600), National Natural Science Foundation of China (61772528), Beijing National Natural Science Foundation (4182066), and Shandong Provincial Key Research and Development Program (2019JZZY010119).