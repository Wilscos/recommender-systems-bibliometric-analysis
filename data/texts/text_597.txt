Graph Neural Networks (GNNs) have shown success in learning from graph-structured data, with applications to fraud detection, recommendation, and knowledge graph reasoning. However, training GNN efﬁciently is challenging because: 1) GPU memory capacity is limited and can be insufﬁcient for large datasets, and 2) the graph-based data structure causes irregular data access patterns. In this work, we provide a method to statistical analyze and identify more frequently accessed data ahead of GNN training. Our data tiering method not only utilizes the structure of input graph, but also an insight gained from actual GNN training process to achieve a higher prediction result. With our data tiering method, we additionally provide a new data placement and access strategy to further minimize the CPU-GPU communication overhead. We also take into account of multi-GPU GNN training as well and we demonstrate the effectiveness of our strategy in a multi-GPU system. The evaluation results show that our work reduces CPU-GPU trafﬁc by 87–95% and improves the training speed of GNN over the existing solutions by 1.6–2.1× on graphs with hundreds of millions of nodes and billions of edges. Graph neural networks (GNNs) have shown promising successes on multiple graph-based machine learning tasks including fraud detection (Liu et al., 2020), recommendation (Fan et al., 2019), search and knowledge graph reasoning (Dettmers et al., 2018). With a rapidly growing need to apply GNNs in various domains, there are several efforts from the community to provide open-source GNN-speciﬁc machine learning frameworks such as PyTorch Geometric (PyG) (Fey & Lenssen, 2019), Deep Graph Library (DGL) (Wang et al., 2019), and Spektral (Grattarola & Alippi, 2020). Those graph-speciﬁc frameworks implement several highly optimized message passing operators and graph-speciﬁc computation layers which were lacking in the previous DNN frameworks. Yet, the challenges of GNN training are not limited to the message passing or the computational layers. Recently, the training of GNNs has been widening to very large graphs. With the successes of using large datasets in machine learning to increase the training accuracy (Russakovsky et al., 2014; Wu et al., 2018), the importance of using larger graphs took a place in GNN training as well (Hu et al., 2021). The number of nodes and the edges of these graphs reach millions to billions (Ugander et al., 2011; Zhu et al., 2019) and the graphs with such scales make the ordinary na¨ıve University of Illinois at Urbana-ChampaignUniversity of BuffaloAWS Shanghai AI LabNVIDIA. Correspondence to: Seungwon Min <min16@illinois.edu>. Jinjun XiongXiang SongWen-mei Hwu software/hardware approaches ineffective. The earlier implementations of GNN were mostly focusing on a small scale graphs (Kipf & Welling, 2016; Veliˇckovi´c et al., 2018) and assumed the whole graph ﬁts into a single GPU memory. Therefore, previously, accessing an arbitrary node’s feature data was merely a process of indexing the GPU’s own memory space. However, for large graphs whose node/edge feature data cannot ﬁt into the GPU memory, at least part of the graph needs to be placed into the CPU memory. One common practice to train GNNs in such scenario is to create a smaller set of problem by performing a mini-batched training. With the mini-batch training, only a subset of nodes are randomly picked along with their neighboring nodes and sent to GPU. This method is very effective when training GNNs on large graphs as it practically reduces the memory footprint of the application. Not only that, several recently introduced GNN models (Shi et al., 2021; Addanki et al., 2021; Daniluk et al., 2021) showed that the mini-batched based approaches are superior in achieving high training accuracy as well. A mini-batch training process that places the all or part of the input graph feature data in the CPU memory needs to frequently transfer mini-batch data from CPU to GPU through a slow PCIe interconnect. Furthermore, the minibatch method ampliﬁes the total amount of data access because the different minibatches can have overlapping nodes. Due to these reasons, training GNN is often throttled by CPU-GPU data transfer time. In many of our measurements, we often ﬁnd the GPU is only about 30-40% utilized during the GNN training when the datasets do not ﬁt in GPU memory. To remedy this problem, in this work, we introduce Data Tiering in GNN, which does not inﬂict any algorithmic changes on the training models, but yet dramatically reduces CPU-GPU data transfer volume. Our data tiering improves GNN training in two ways. First, it provides a statistical method by using reverse pagerank to effectively predict the importance of each node in the input graphs and identiﬁes which nodes should be located in the GPU memory. Second, it introduces a hardware-friendly data placement and access strategy which minimizes the cost of accessing cold data in CPU memory. Our data placement and access strategy is quite comprehensive and it also enables more advanced optimization techniques for the multi-GPU systems with high speed GPU-to-GPU interconnects. We evaluate our work using public frameworks PyTorch and DGL. The demonstration of our work on realistic minibatched training shows that our approach eliminates PCIe trafﬁc by 87–95% in various datasets by loading only 10% of them into GPU memory. With the data transfer time optimization alone from our data tiering strategy, we ﬁnd the training speeds of the existing GNN implementations can be improved by 1.6–2.1×. To demonstrate the scalability of our work, we also train a dataset with 350GB of size in a system with four NVIDIA V100 32GB GPUs. GNNs are a series of multi-layer feedforward neural networks that propagate and transform layer-wise features following a graph structure. Among these models, a graph convolutional network (GCN) (Kipf & Welling, 2016) architecture is widely employed, which relies on the layer-wise message passing scheme. Formally, the(l + 1)-th layer of a GNN is deﬁned as: where the functionf(G, H)is determined by learnable parameterswandσ(·)is an optional activation function. Grepresents a graph composed ofNnodes andEedges. Additionally,Hrepresents the embeddings of the nodes in thel-th layer, andHis initialized with the nodes’ input featuresX. The node input features are stored in aN×D matrix whereNis the total number of nodes of the input graph andDis the dimension of each node feature. The size of node feature varies signiﬁcantly depending on the dataset, but the typical sizes are in between 512B to 4KB. When the node feature size is multiplied with the total number of We omit edge features for simplicity. nodes in a graph, the node feature matrix (or tensor) can reach tens of gigabytes to hundreds of gigabytes. Therefore, storing the node feature tensors of GNN datasets is the most difﬁcult task with the large graphs. Figure 1.GNN with node feature aggregation and label prediction. Whenlis identical to the last layer of GNN, the correspondingHtensor is the output embedding tensorZ. The output embedding tensorZis used to create predicted labels and classify unclassiﬁed nodes. For training purposes, if the nodes already have ground truth labels, then the predicted labels are compared with them to perform a backpropagation and a model update. Similar to the image classiﬁcation task, the qualities of both the trained model and the output embeddings can beneﬁt from using a larger dataset with more expressiveness (Hu et al., 2021). The layer-wise aggregation implementation of GNN provides a promising way of gathering relational information from graphs, but it requires reading all neighboring nodes of each layer. With a large graph, this approach quickly shows a scalability challenge as the number of nodes that we need to read exponentially grows with an increasing number of layers. To alleviate this scalability problem, GraphSAGE (Hamilton et al., 2017) introduces neighborhood sampling and aggregating approach. By sampling a ﬁxed number of neighboring nodes per target node instead of demanding the whole adjacency matrix, the neighborhood sampling reduces the computation and memory footprints. With the predeﬁned numbers of sampling per layer, we can also effectively control the size of each mini-batching in both training and inference. Neighborhood sampling is applied to every neighboring node in every aggregation step. GraphSAGE uses a uniformly random selection process to sample the neighboring nodes to provide an enough randomness to the training process. The commonly used hyperparameters for the neighborhood sampling sizeSare(S, S) = (25, 10)for a 2-layer sampling approach and(S, S, S) = (10, 10, 10) for a 3-layer approach. It is uncommon to go beyond the three layers of sampling because of the need to limit the size of mini-batch. After the sampling, a sub-graph which only contains the sampled nodes is created so the computation kernel knows how to aggregate the node features of interest. Over different iterations of training, a new sampling is done to increase the learning entropy. When the node feature tensor is in CPU memory, the features of the sampled nodes must be transferred to the GPU memory. Due to the slow PCIe interconnect between CPU and GPU, this data transfer process can be quite time consuming. In this paper, we present an optimization technique calleddata tieringthat exploits locality in accessing feature data and minimizes the need for cross-PCIe accesses. By deﬁnition, the neighbor sampling process is random and it is difﬁcult to exactly predict which nodes will be accessed during training. Thus, we must statistically approach the problem of identifying and exploiting locality. The ﬁrst metric we can use is the outdegreeof each node in the input graph. With a high out degree, even if the node is not selected in a speciﬁc run of neighbor sampling, the cumulative chance of the node being selected during the entire training process is higher than the less connected nodes. Considering that we perform quite signiﬁcant number of sampling per training epoch for the large graphs, this prediction is statically reasonable as we empirically prove it in Section 5.2. In case of ogbn-papers100M, we sample about 130 millions of nodes per training epoch. Figure 2.Snapshot of Pagerank vs. Reverse Pagerank. Only a single iteration of algorithms shown. In case of regular pagerank, the score is divided by theout degree, but in case of reverse pagerank, the score is divided by the in degree. The second option is a reverse pagerank (R-Pagerank) (BarYossef & Mashiach, 2008). In the original pagerank, the score of each node is higher if the in degree is higher and the out degree is lower. For the reverse pagerank, it is the opposite. In Figure 2, we depict the difference between the original pagerank and the reverse pagerank further. For simplicity, we only show a case of nodeAwith single iteration, but this is done for all nodes until the score values converge in the real implementation. In the original pagerank, to calculate the score of the source node, we sum the scores of the nodes which are targeting the source node and divide the summed score by the out degree of the source node. Now with the reverse pagerank, we sum the scores of the nodes which are targeted by the source node and divide the summed score by the in degree of the source node. The idea behind this mechanism is that if a certain nodeAhas many outgoing edges, it can potentially get a higher score by summing many nodes’ scores. Therefore, if there is another nodeBwhich is targeting nodeA, nodeBalso gets a higher score by adding the score of node A. In the context of neighbor sampling, the scoring mechanism of the reverse pagerank can be understood by referring to Figure 1. The green nodes are sampled while generating the embedding for the red node, referred to a node A, because they have an outgoing edge to A. The blue nodes are sampled because they have an outgoing edge to the green nodes. Therefore, If a node can reach many nodes directly or indirectly through its outgoing edges, it is likely to be picked during the sampling process. Since the probability of nodeAbeing picked is high, the other nodes which are targeting this node also has a relatively higher probability of being picked when we are sampling multiple layers. However, if the nodeAalso has a high in degree, because now there are so many nodes which can be sampled from node A, the other nodes should not expect the chance of them being selected too high even if nodeAwas selected. Thus, in this case, we divide the score of nodeAby the in degree before propagating it to the other nodes so these nodes receive less increase to their estimated probability of being picked during sampling. The potential advanatge of using the reverse pagerank over the simple degree method is to capture further multi-layer sampling patterns. For the simple degree method, the information we can capture is limited to a single hop of relationship, while the actual neighbor sampling can extend to multiple hops. On the other hand, in reverse pagerank, the score value of each node is propagated to multiple layers of nodes away until the score converges to a certain limit. Therefore, by using the reverse pagerank, it is possible to capture the subtle pattern of multi-layer sampling in the neighbor sampling in a better way. The third option is to further incorporate the labeling status of the nodes into the reverse pagerank method. As we explained in Section 2.1, the goal of GNN training is to create a model which can predict the labels for the unlabeled nodes. To train such models, we must be able to compare the predicted labels with the ground-truth labels. Therefore, during training, the nodes which we can pick to start the neighbor sampling are reduced to the nodes that come with with labels. This means that, if we can devise a method to statistically put further emphasis to those nodes and their surrounding nodes, we can compress the search space. Currently, the most similar existing algorithm used to achieve such goal is the personalized pagerank (PPR). In PPR, instead of calculating the scores of all nodes in general, we select a speciﬁc node of interest and calculate the scores of the rest of the nodes from the selected node’s perspective. In other word, now the score generation process is more customized for the selected node. However, the problem of PPR is that it only works with a single source node but not with multiple source nodes. Indeed, running multiple separate PPR instances for multiple source nodes is algorithmically possible, but each PPR instance has O(n) of space complexity wherenis the total number of nodes in the input graph (Oracle, 2021). Considering that we have millions of labeled nodes for any realistically large datasets, this space complexity is simply not affordable for our case. Therefore, instead of utilizing PPR, we add some tweaks on top of the reverse pagerank algorithm by uniformly applying a weight value to the labeled nodes. The detailed implementation of the weighting process is described in Algorithm 1. First, before we decide how to weight the labeled nodes, we need to decide how much we want to weight them. The assumption behind the weighting is that by knowing the exact starting locations of the neighbor sampling, we can more focus on those nodes and their surroundings. This means, that if there are few starting nodes available in the graph, the sampling tendency will be more biased toward them and their surrounding nodes. In the opposite, if every node can be selected as a starting node, there is no starting bias and simply the nodes with high out degree is likely to be selected during the sampling. As a result, the weighting intensity should be high if there are few labeled nodes, and the weighting intensity should be low if there are many labeled nodes. In our algorithm, we reﬂect this by deﬁning weight = (# of all nodes) / (# of labeled nodes). Next, the actual weighting is done by multiplying the initial scores of the labeled nodes with the weight value we calculated (Algorithm 1, Line #10). In the original pagerank algorithm, the default initial score of all nodes is 1 divided by the total number of nodes in the graph. In general, by running pagerank long enough, the initial impact of the initial scores wear down and the scores start to converge to certain values. To avoid this, we do not run our weighted reverse pagerank until the scores converge, but only ﬁve iterations. The rest of the algorithm is identical to the reverse pagerank algorithm. We call this algorithm as weighted reverse Algorithm 1 Weighted Reverse Pagerank 1: Input: graph g, iteration iter, damp d, train id tid 2: num node = num nodes(g) 3: num train = length(tid) 4: for i = 0 to num node − 1 do 7: end for 8: weight = num node/num train 9: for i = 0 to num train − 1 do 20: end for pagerank (Weighted R-Pagerank). With the score values, now we want to split the node features into a high score group and a low score group. The simplest way to achieve this is to reorder the node features based on the score values and divide them into top X% of portion and bottom 100-X% of portion. However, at the same time, reordering the node feature tensor alone creates a discrepancy between the node IDs in the graph and the row IDs of the node feature tensor. To resolve this, we can consider two methods. First, we reorder the node feature tensor but not the graph nodes. In this case, we need to create a mapping which translates the old graph node IDs to the new node feature row IDs. Second, we reorder the node feature tensor and also the graph nodes. In this case, we can directly use the node IDs in the graph to access the corresponding node feature in the node feature tensor. For our implementation, we decide to use the second method because the full-scale mapping itself takes space, and with hundreds of millions of nodes, the mapping alone will be several GBs. However, unfortunately, the process of reordering the graph nodes is less intuitive than reordering a 2D dense matrix like the node feature tensor because the graphs are often represented by a sparse matrix format. Due to it’s nonstraightforward nature, the current implementation of graph reordering in existing frameworks like DGL has a simple sequential implementation, but this approach is too time consuming when we need to reorder a graph with hundreds of millions of nodes. Therefore, in this work, we implement our own parallel version of algorithm to accelerate the graph feature reordering process. To better understand our implementation, we ﬁrst brieﬂy explain the graph reordering problem in general. In Figure 3, we show the overview of the graph reordering process. In compressed sparse representation (CSR), the graph structure is divided into an edge list and a node list. The edge list is a collection of many neighbor lists where each contains the IDs of nodes connected to a speciﬁc node. To reorder a graph, we need to perform the following three tasks: First, we need to create a new node list which contains the information of new partitioning of the new edge list. Second, we need to relocate the blocks in the edge list based on the new partitioning information. Third, we need to update all node ID values in the edge list. The key to parallelize the workloads is to generate a full mapping of old to new IDs in advance so the ID translation becomes a simple lookup process. The detailed process is described in algorithm 2. We generate thesrt idxmapping list by creating a list of indices which can be used to sort the score values in descending order. If the list of scores generated from Section 3.1 is [0.1, 0.4, 0.2, 0.3], then the resulting sorted list should be [0.4, 0.3, 0.2, 0.1] and the mapping is [3, 0, 2, 1]. Now, we create a new node listndand iteratively ﬁll the list with the sizes of the neighbor lists by using both the old node listndand the previous generated srt idx. Since all the indices insrt idxare unique, we can simply parallelize this iterative process. Next, we perform a preﬁx sum on the ndto create a new node list. Creating a new edge list requires two steps. First, the values in the edge list should be updated and the neighbor lists in the edge list should be relocated. To update the values, we simply index thesrt idxwith the old edge list values and replace them. This process is fully parallelizable as there is no race condition. Next, to relocate the neighbor lists in the edge list, we use the previously createdndand old ndlists. We identify the location of old neighbor list using ndand place to the new location usingnd. This process is also easily parallelizable by distributing the neighbor list copy operation over multiple threads. Depending on Input: graph g, score score srt idx = indices of sorted list(score, descending) nd = node list(g), num node = num nodes(g) edg = edge list(g), num edge = num edges(g) for i = 0 to num node − 1 do neighbor list length = nd[i + 1] − nd[i] nd[srt idx[i] + 1] = neighbor list length end for nd[0] = 0 nd= prefix sum(nd) for i = 0 to num edge − 1 do edg[i] = srt idx[edg[i]] end for for i = 0 to num node − 1 do for j = 0 to nd[i + 1] − nd[i] do edg[nd[i] + j] = edg[nd[i] + j] end for end for return (nd, edg) the variation of each neighbor list’s length, the workload can be unbalanced between different CPU threads for this step, but we ﬁnd the nodes with similar scores have similar neighbor list lengths and the workload distribution also becomes balanced. When we deﬁnenas a number of nodes andeas a number edges, the total time complexity of this algorithm is eitherO(nlogn)due to the node sorting, or O(e)when the number of edges is very large. However, thanks to our fully parallelizable approach, we ﬁnd the endto-end graph reordering takes only about 31 seconds with ogbn-papers100M dataset (Table 1), which has 111M nodes and 3.2B edges. The end goal of data tiering is to locate frequently accessed data in the GPU memory so we can maximize the effective bandwidth. In this section, we describe a system level design to achieve this goal. The overall data placement and access strategy is shown in Figure 4. With the sorted node feature tensor, the hot data portion with a high score is placed in the GPU memory and the rest of cold data portion with a low score is placed in the CPU memory. From the user application perspective, we provide a single monolithic and contiguous fake view of the two tensors so the user application can use the existing data indexing scheme. For the cold data access, it is important to maintain a low end-to-end data transfer overhead since crossing over PCIe Figure 4. Simple data placement and access method overview. is already a huge burden. One the of most common mistakes made by programmers during CPU-GPU data communications is that often the programs spend too much time on simply coordinating the data transfer. Using cross-device data copy engines like DMA is wide spread, but it is only effective when the size of data that we want to transfer is large enough. In the case of node feature sampling and aggregation, the size of each random access is typically between 512B and 4KB, much smaller than the size required to make DMA transfers efﬁcient (Pearson et al., 2019). Not only that, if GPU needs to rely on CPU to initiate the data transfer as is the case of DMA, the synchronization overhead can be completely outweigh the time cost of the transfer itself. Therefore, in this work, we take a GPU-centric approach to accessing data instead of depending on DMA or CPU. At the hardware level, the GPU-centric method is enabled by using the zero-copy access capability of NVIDIA GPUs. The zero-copy accessible CPU memory space is mapped into the GPU page table and it allows us to directly access the CPU memory space from the GPU CUDA kernel code. At the user (application) level, we utilize the DGL’s UniﬁedTensor (Min et al., 2021) implementation to enable this data access mechanism. With our data placement and access strategy, the overall ﬂow of node feature access in GNN training is as follows. First, the neighbor sampling function traverses the input graph and generates a list of node IDs. Next, the node IDs are sent to the GPU(s). Next, the GPU threads start accessing the node features with the node IDs. While looping over the node IDs, the GPU threads check if the ID values are within the hot data boundary set by the users. If the ID values are within the boundary, then the threads use a pre-stored GPU memory pointer and take the advantage of fast local memory access. If the node ID is outside the boundary, then the threads use a pre-stored CPU memory pointer and perform the zero-copy access. One major beneﬁt of our approach is that we only attack the data transfer part of the GNN training. The internal indexing scheme with varying GPU/CPU memory access modes of our design is completely transparent to the GNN model itself. Such attribute makes our approach extremely modular and immediately allows the existing GNN models to be trained on large graphs. Similar to the other neural network training methods, GNN training also extensively utilizes multiple GPUs to further accelerate the training process. With fast GPU-to-GPU interconnects like NVLink, we can create a larger pool of collective GPU memory space (Figure 5) from multi-GPU systems. In Figure 6, we show the complete view of our data tiering strategy in a multi-GPU system. To load the hot data into this collective memory space, instead of using a na¨ıve blocked partition method, we use an interleaved data loading method. Since the node feature tensor is sorted in a descending order of the score, a simple block partitioning scheme can result in unbalanced memory and interconnect bandwidth consumption across GPUs. With the combined GPU memory space, we can hold a larger portion of hot data in a faster tier of memory space. The speciﬁc usage of CUDA APIs to enable our data placement strategy in multi-GPU environment is explained in Appendix A. Figure 5.An example system with four NVIDIA V100 GPUs connected over NVLink. All bandwidth numbers shown are unidirectional. In our experience, the NVLink bandwidth is fast enough to hide most of the data transfer time of GNN training, but in case the data transfer time is still an issue, we can additionally replicate some hot data over multiple GPUs. In this case, the most frequently accessed data will come from the local GPU memory, the next most frequently accessed data from the peer GPU memory, and the least frequently accessed data from the CPU memory. The generation of the combined tensors is fully automated in our implementation. To generate this combined tensor, users simply need to provide the sizes of local GPU tensor and multi-GPU tensor, and the number of GPUs connected over NVLink. If there are no high speed links between GPUs, the mapping simply falls back to Figure 4. The optimal distribution factor of each GPU’s memory capacity between the replicated hot data and the interleaved hot data in the collective memory space may vary depending on the dataset. For our experiments, we simply maximize the multi-GPU tensor and do not utilize the replicated local GPU tensor. Figure 6. Data loading in multi-GPU environment. For the evaluation, we implement data tiering on PyTorch and DGL. Since neither of the frameworks support kernellevel direct peer GPU memory access, we modify their tensor implementations to enable it. Currently, the frameworks can only perform peer-to-peer DMAs. We use GraphSAGE implementation of DGL to explore various neighbor sampling strategies. For the dataset, we use the following three from Open Graph Benchmark (OGB) (Hu et al., 2020): ogbn-papers100M, MAG240M, and WikiKG90M. WikiKG90M is from a different task domain and does not come with the labels needed for node classiﬁcation but due to the lack of public large datasets, we repurpose it as a node classiﬁcation task dataset with synthetic label values. Further details of the datasets are shown in Table 1. To make our experiment realistic, we use the carefully tuned hyper parameters for different datasets which are taken from previous GNN training works with high accuracy models (Shi et al., 2020; Niu, 2021; Shi et al., 2021). Based on the previous works, we use (12,12,12) as neighbor sampling fanout parameter for ogbn-papers100M and (25,15) for MAG240M. For WikiKG90M, we use the identical parameters used in ogbn-papers100M training. Throughout the evaluation, we use a machine with two Intel Xeon Gold 6230 CPUs and four NVIDIA V100 32GB GPUs (Figure 5). All NVIDIA V100 GPUs are connected over NVLink with 50GB/s unidirectional bandwidth per connection. Because each GPU is connected to three other peer GPUs, the aggregated NVLink bandwidth is 3×50GB/s = 150GB/s for each GPU. In this experiment, we try to ﬁnd out if the score functions we discussed in Section 3.1 can correctly predict the data reusability in real GNN training. In Figure 7, we list the nodes of graphs in the X-axis in descending order of scores with the three different scoring functions: node degree, reverse pagerank, and weighted reverse pagerank. In the Y-axis, we show the measured access frequency of each node during GNN training, in a cumulative fashion. In general, we ﬁnd all functions can provide some level of beneﬁts when we try to perform data tiering in GNN training. For example, based on the scores calculated, when we keep top 10% of nodes, we can expect at least 35% of hit during the training regardless of the dataset. If we increase the ratio to 25%, the minimum hit ratio further increases to 56%. Also, in general, we ﬁnd it becomes easier to predict which nodes would have high data access counts if a graph has a more extreme power law distribution. For example, WikiKG90M graph has an extremely unbalanced edge connectivity and 80% of edges in the entire graph are connected to only 1% of nodes. With such extremely concentrated connections, we can observe that simply choosing a few nodes with the highest degrees automatically guarantees a very high hit ratio during the neighbor sampling. In cases of ogbn-papers100M and MAG240M, the edge connectivities of the datasets are more balanced and the ratios are 32% and 46%, respectively. However, even though the simple degree method can be effective for certain graphs, we ﬁnd the weighted reverse pagerank is more preferable in general because it consistently gives the best prediction result. 5.3 GNN Training Time (Single GPU) In this section, we evaluate the actual beneﬁt of our work in GNN training. We compared the performance of our work against the following two existing methods: 1) CPU gathering and 2) zero-copy access. The CPU gathering method relies on CPU to gather node features and then utilize GPU DMA engine to copy the gathered node features into GPU memory. Due to the additional data gathering process, this method not only wastes the CPU memory bandwidth, but also adds a non-negligible amount of data transfer latency Figure 7.Access frequency distribution comparison on different datasets with different score functions. Y-axis is normalized to the total number of access. to GPU. This is the only way currently available in PyTorch to transfer scattered data in CPU memory to GPU memory. The zero-copy access method is a method recently introduced in DGL to overcome the data gathering overhead in the CPU gathering method. With this method, GPU kernels can directly dereference CPU memory pointers and thus we do not need to rely on CPU to gather data for the GPUs. To enable the zero-copy access capability, DGL implements a new class of tensor called UniﬁedTensor which transforms the CPU tensor of PyTorch into a zero-copy accessible tensor. In UniﬁedTensor, the speciﬁc task is done by utilizing cudaHostRegister()API from CUDA on top of existing CPU memory allocation. The further technical detail is identical to the process explained in Appendix A.2. For our work, we ﬁrst score the nodes with the weighted reverse pagerank function like in Figure 7, and then reorder the node feature tensor and the graph nodes in the datasets. Figure 8. Single epoch training time comparison. For this experiment, we load 10% of hot data for ogbnpapers100M and WikiKG90M, but only 5% for MAG240M due to the GPU memory limitation. In Figure 8, we show the overall comparison. From this comparison, we can ﬁrst observe that relying on CPU to gather data results in seriously increasing the overall training time. In this case, we ﬁnd that the GPU is only about 1030% utilized and mostly idling. By adopting the zero-copy access method, the training performance is visibly improved (2.5-4.6×). The zero-copy only method does not leverage any temporal data locality strategies, but simply removing CPU from the data access path signiﬁcantly increases the overall performance. Finally, with our method, the training performance is further improved by 1.6-2.1×on top of the zero-copy only method. Considering that we have run the entire experiment on top of PyTorch and DGL with python, the beneﬁts that we observe are immediately deliverable to the regular users as well. Now, we take ogbn-papers100M as an example for more detailed analysis. First, even though we know that the most of existing GNN works use two to three layers of sampling depths, we still like to know how much increasing the sampling depth can affect the data tiering efﬁciency. To understand the impact, we use different sampling depths during the GNN training and observe how the node access frequency distribution varies. In Figure 9, we show two access frequency charts similar to Figure 7, but now with the varying sampling parameters of (10,10), (10,10,10), (10,10,10,10), and (10,10,10,10,10). For the score functions, we use the weighted reverse pagerank and the degree count. For both cases, we can observe the accesses are now more spread out with the deeper sampling parameters. This is an expected behavior because with a deeper sampling depth, the graph coverage of each minibatch becomes larger and also we start to access secluded nodes more frequently. Figure 9.Access frequency distribution comparison of using different neighbor sampling parameters in ogbn-papers100M dataset. cases, we can still identify a signiﬁcant portion of accesses are made to a few selected nodes. For example, with the (10,10,10,10,10) sampling parameter, top 10% of the highest score nodes of the weighted reverse pagerank and the degree count functions account for 52% and 28% of the entire accesses, respectively. This experiment result shows that the beneﬁt of data tiering is not immediately nulliﬁed with a growing sampling layer depth and it gives some room for the future GNN models which may attempt to sample deeper. For the second analysis, we would like to more closely: 1) verify the hardware-level beneﬁt of data tiering and 2) observe what is the impact of controlling the portion of data loaded to GPU. To better understand these, we sweep the portion of data loaded in GPU during GNN training and measure the volume of PCIe trafﬁc and the training time (Figure 10). For this experiment, we perform data tiering with the weighted reverse pagerank function on ogbn-papers100M. To measure the PCIe trafﬁc, we use the NVIDIA proﬁling tool nvprof. As we can see, our data placement and access strategy effectively reduces the PCIe trafﬁc with more hot data loaded into the GPU memory. When we compare the cases with no data loading and 25% of data loading, we can achieve about 97% of PCIe trafﬁc reduction. At this point, most of the node feature accesses are resolved within GPU and only very few data accesses need to be directed to the CPU memory over slow PCIe. The performance gains in GNN training show a similar trend to the PCIe trafﬁc reduction. With the 5% of data loaded, we can already reduce the training time by 33% and with the 25% of data loaded, we can further reduce the training time by 42%. In general, the GPU memory consumed by the training process itself is proportional to the minibatch size, and the minibatch size is exponentially proportional to Figure 10.PCIe trafﬁc and training time comparison in actual GNN training with increasing hot data portion loaded in GPU. ogbn-papers100M dataset used. the sampling depth (Wu et al., 2019). Therefore, hypothetically, if the sampling depth is very deep, the GPU memory available for hot data can be limited, but the base space complexity of GNN is relatively low. For example, in case of ogbn-papers100M training with 3-layer sampling, we consume only about 400MB of GPU memory and the rest of the space is left unused. Additionally, considering the trend of increasing capacity of GPU memory (e.g., NVIDIA A100 80GB) and the distributed Multi-GPU tensor solution we discussed in Section 4.2, we believe the actual impact from this limitation is negligible. In this section, we show the performance beneﬁt of the multiGPU implementation of our work described in Section 4.2. In this experiment, we use four V100 32GB and therefore we can have toal 128GB of collective GPU memory space. For the training dataset, we use MAG240M which has 350GB of node feature tensor. For data placement, we divide the node feature tensor into two tensors, a multi-GPU tensor and a CPU tensor. We do not allocate any space for the replicated GPU tensor. In Figure 11, we show the training time evaluation of MAG240M with increasing sizes of hot data loaded in the multi-GPU tensor. Before we go into further detail of the GPU sampling results, we ﬁrst focus on the CPU sampling results. Similar to the results from Figure 10, we observe a sharp drop of training time with 5% of node feature loaded into GPU memory. Beyond that, we observe only marginal performance improvements. The overall performance improvements in multi-GPU training is underwhelming because the single GPU training of MAG240M in Figure 8 can reach 23.5 seconds already. This means, with four GPUs, we can reduce only about 3 seconds of training time further. After several proﬁling, we ﬁnd that in the multi-GPU training, the neighbor sampling process itself starts to throttle the whole training process and gives us a poor scalability. As we described in Section 2.2, the sampling step traverses the graph structure and generates the node IDs for a minibatch in preparation for the node feature aggregation step. For the neighbor sampling, we have been using CPU since the graph structure is store in the CPU memory. In a single GPU training, CPU was able to sample neighbors fast enough and provide their IDs to GPU in a reasonable amount of time. However, now in a multiple-GPU training, the number of minibatches that we need to generate is multiplied by the number of GPUs and this starts to affect the overall training time. Just to clarify, the implementation of CPU sampling process is already done in a parallel fashion. In short, the amount of parallelism available from CPU is not enough to quickly traverse the graph structure and sample neighbors for multiple GPUs. This problem can be overcome with the GPU-based sampling method, but only with our multi-GPU data placement strategy. This is because to perform the GPU-based sampling, we now need to consider how to let GPUs to access the graph structure as well. Of course, the simplest way of achieving this is loading the entire graph structure to each GPUs’ memories, but the size of the graph structure of MAG240M alone is 30GB and it is too wasteful to load it into every GPU. To resolve this issue, we expand the idea of multi-GPU node feature data placement strategy to the graph structure as well and distribute the graph structure over multiple GPUs. The beneﬁt of the combination of our data placement strategy and the GPU sampling is shown in Figure 11. When we compare the memory footprints of the CPU sampling method and the GPU sampling method, we can ﬁnd in general the GPU sampling chart has been shifted to right because now the graph structure is consuming some GPU memory space. However, in terms of overall performance, the GPU sampling method removes the CPU sampling bottleneck and notably increases the training speed in the multi-GPU setup. As we can see, even though we initially designed our data placement and access methodology mainly for the node feature tensor, it can be expanded to different types of data structures as well. We believe the other DNNs which have sparse data access patterns like what we observe with the graphs would beneﬁt the most from our design. GNS (Dong et al., 2021) samples a global cache of nodes periodically for all mini-batches and stores them in GPUs. It employs a preferential sampling approach in generating mini-batches, which gives priority to neighbors that exist in the GPU cache. This can greatly reduce data copy between Figure 11.Multi-GPU MAG240M training time comparison while loading different amounts (in %) of node feature tensor into GPU memory. GPU sampling requires the graph structure to be loaded in GPU memory and thus it has a higher memory footprint. CPU and GPU, but it requires the modiﬁcation of native node-wise sampling algorithm, which cannot be easily extended to other sampling methods. On the other hand, the design of our data tiering is orthogonal to these sampling methods, which makes it sampling-agnostic. Furthermore, the method in GNS lacks the capability to leverage multiGPU memory to store the cached data. LazyGCN (Ramezani et al., 2020) periodically samples mega-batches and recycles the sampled nodes within a megabatch to generate mini-batches. This reduces the overhead of data preparation. Though, LazyGCN is sampling-agnostic, it requires a large mega-batch size, regardless of which sampling method is used, to guarantee the model accuracy. With node-wise neighbor sampling, it can easily run out of GPU memory on large graph such as ogbn-papers100M. PaGraph (Lin et al., 2020) and AliGraph (Zhu et al., 2019) provide static node caching scheme in GNN training, but their caching strategies are simply limited to high out degree nodes and their entire cache managements are done by CPU. This approach makes GPUs to always synchronize with CPU to access data. In this work, we presented a data tiering technique for GNN training. In general, we ﬁnd the training time of GNN can be easily improved with well-deﬁned data placement and rearrangement optimizations. Our data tiering strategy is a novel solution that does not affect the algorithm of GNN at all but still maximizes the beneﬁt of multi-tier memory subsystem of modern hardware. We further demonstrate that our approach improves the scalability of multi-GPU training. We demonstrated our work by using existing libraries such as PyTorch and DGL, and our data tiering implementation can be immediately adopted by the end users.