The University of Western AustraliaThe University of Hong Kong Heterogeneous Information Networks (HINs) capture complex relations among entities of various kinds and have been used extensively to improve the eectiveness of various data mining tasks, such as in recommender systems. Many existing HIN-based recommendation algorithms utilize handcrafted meta-paths to extract semantic information from the networks. These algorithms rely on extensive domain knowledge with which the best set of meta-paths can be selected. For applications where the HINs are highly complex with numerous node and link types, the approach of hand-crafting a meta-path set is too tedious and error-prone. To tackle this problem, we propose the Reinforcement learning-based Meta-path Selection (RMS) framework to select eective metapaths and to incorporate them into existing meta-path-based recommenders. To identify high-quality meta-paths, RMS trains a reinforcement learning (RL) based policy network (agent), which gets rewards from the performance on the downstream recommendation tasks. We design a HIN-based recommendation model, HRec, that eectively uses the metapath information. We further integrate HRec with RMS and derive our recommendation solution, RMS-HRec, that automatically utilizes the eective meta-paths. Experiments on real datasets show that our algorithm can signicantly improve the performance of recommendation models by capturing important meta-paths automatically. Keywords:recommender system, reinforcement learning, meta-path, graph neural network. ACM Reference Format: Wentao Ning, Reynold Cheng, Jiajun Shen, Nur Al Hasan Haldar, Ben Kao, Nan Huo, Wai Kit Lam, Tian Li, and Bo Tang. 2022. Reinforced Meta-path Selection for Recommendation on Heterogeneous Information Networks. In Proceddings of The Web Conference 2022 (WWW’22), April 25–29, 2022, Lyon, France. ACM, New York, NY, USA, 11 pages. hps://doi.org/10.1145/1122445.1122456 Nowadays, Heterogeneous Information Networks (HINs) [16] are being extensively used to improve the performance of many novel data mining tasks such as recommender systems [27,35,37], natural language processing [34,46], community detection [7,31]. A HIN consists of multiple types of entities and links, and provides an advanced data structure that can convey diversity in an intuitive manner. It reveals the complex inter-dependency of each entity in a single graph and provides much side information for recommendation tasks. In Figure 1, the toy example of a movie knowledge-graph with four diverse node types and three types of relations represents a heterogeneous information network. A real world network containing both the structural and semantic information (e.g., social networks, knowledge graphs, world wide web, etc.), can be modeled as a HIN. Dierent from the conventional network denition, a HIN can eectively combine various structural information and can contain rich semantics, which benets real-world applications, such as recommendation systems. For example, traditional recommendation algorithms [20,24] usually use user-item interaction history to generate results. However, HIN can analyze more information than the interaction logs, which produces more accurate results. Therefore, it received a lot of attention and has been applied in many recommender systems in recent years. Due to the complex relations on HINs, the concept of metapath [28] is proposed to extract useful information from it. A meta-path consists of a set of node types, which are linked by a set of edge types. In Figure 1,𝑈 𝑠𝑒𝑟 → 𝑀𝑜𝑣𝑖𝑒 → 𝐴𝑐𝑡𝑜𝑟 → 𝑀𝑜𝑣𝑖𝑒is a meta-path, which can be adopted to nd movies that have the same actor as movies watched by specic users. With the popularity of meta-path, plenty of recommendation algorithms [27,41] based on meta-paths are proposed. These methods leverage semantic information in dierent metapaths to enhance the representation of users and items in the HIN. Some studies [9,12] employ an attention mechanism to aggregate the information available in dierent meta-paths. Though the meta-paths are considered eective for recommendation, however, selecting the most eective metapath set has not been well addressed in the existing studies mentioned above. The performance of recommendation is highly sensitive to the choice of meta-paths. According to our experiments, using dierent meta-path sets can result in performance that diers by more than 30%. Current metapath-based algorithms [8,12,40] suppose that meta-paths are chosen by domain experts. However, the diculties are 1) only a few people have prior knowledge on the design of meta-path, 2) the searching space of the meta-path combination is extremely huge. It is impossible to use brute force to select the optimal meta-path set. 3) the selection of meta-paths is highly dependent on datasets and models. The optimal meta-path set cannot be easily extended to other datasets or algorithms. Han et. al. proposed GEMS [10] that uses a genetic algorithm to search useful meta-structures. However, the enormous searching space of meta-structures results in the ineciency of this method and this method also cannot be applied in existing methods. To undertake these challenges, we propose Reinforced Meta-path Selection (RMS), an RL-based method, to gure out meaningful meta-paths for recommendation. We model this problem as a Markov Decision Process (MDP) and use Deep Q-Network [23] (DQN), which is an extensively used reinforcement learning framework, to tackle this complex problem. We train a policy network (agent) to automatically generate a relation to extend current meta-paths to take the place of the human labor of manual design of meta-path set. The current state is based on the existing meta-paths we selected and the reward function considers the performance gain on the recommendation (NDCG@10), which drives the agent to generate better relations to extend the current meta-path set. However, not all the generated metapaths are useful and informative, so we also dene some strategies to guarantee the generated meta-paths are eective for recommendation. We also integrate RMS and propose a meta-path-based recommendation algorithm RMS-HRec, without the need to manually specify the meta-paths. For the core recommendation algorithm part, we borrow the idea from HAN [40] and propose a new recommendation model HRec. HRec needs a set of user meta-paths, which start and end with a user type node, and a set of item meta-paths, which start and end with an item type node. It will apply a node-level attention layer to all meta-path neighbors for each meta-path and fuse the information from them by a meta-path-level attention layer. After we get the embeddings of users and items, we multiply them to get the recommendation scores of users to items. The main contributions of our work are summarized as follows: •We propose a general meta-path selection framework RMS, which can be plugged into meta-path-based recommendation models. To the best of our knowledge, this is the rst framework that has been experimentally proven to be useful for existing meta-path-based recommenders. •Equipped with RMS, we develop a new meta-pathbased recommendation method RMS-HRec and design some strategies during training to fully explore the potential of meta-paths for recommendation tasks. •We conduct extensive experiments to evaluate the performance of RMS and RMS-HRec. Results demonstrate the eectiveness of both models and reveal our method can identify important meta-paths that may be neglected by manual design. Denition 2.1.(Heterogeneous Information Network) A Heterogeneous Information Network (HIN) is a graph, denoted asG = (V, E, N, R), with multiple types of nodes and edges.Vrepresents the node set, which is associated with a node type mapping function𝜙:V → N, whereN is the node type set.Edenotes the edge set, which is also associated with an edge type mapping function𝜓:E → R, where R denotes the relation type set. Example:Figure 1 is an example of HIN, which consists of four types of nodes (i.e.,N= {user, movie, actor, director}) and three types of relations / edges (i.e.,R= {watch/watched, act/acted, direct/directed}). Figure 2.An overview of RMS. The agent is a policy network that aims to investigate high-quality meta-paths. The agent will embed the current meta-path set into a state vector, which is the input of this agent, and it will generate the next relation, which will update the current meta-path set and produce a new set. Denition 2.2.(Meta-path) Given an HING, a meta-path Mis a sequence of node types which are linked by relation types in the form of𝑛−→ 𝑛−→ · · ·−→ 𝑛(abbreviated as𝑛𝑛· · · 𝑛), where𝑛∈ Ndenotes the node type and 𝑟∈ R refers the relation type. Example: 𝑈 𝑠𝑒𝑟−→ 𝑀𝑜𝑣𝑖𝑒−→ 𝐴𝑐𝑡𝑜𝑟−→ 𝑀𝑜𝑣𝑖𝑒(abbreviated as 𝑈 𝑀𝐴𝑀) is a meta-path in Figure 1. Denition 2.3.(Meta-path Instances) Given an HING and a meta-pathM, a meta-path instance𝐼is dened as a node sequence in G following the meta-path M. Example:As shown in Figure 1, suppose meta-pathM = 𝑈 𝑀𝐴𝑀, then one meta-path instance𝐼can be𝑈−→ Denition 2.4.(Meta-path Neighbors) Given an HING and a meta-pathM, the meta-path neighbors of node𝑛are a set of nodes𝑁inGwhich is connected to𝑛via the meta-path M. Example:As shown in Figure 1, suppose meta-pathM = 𝑀𝐴𝑀, then node𝑀’s meta-path neighbors𝑁= {𝑀, 𝑀}. Figure 2 shows the overview of our meta-path selection framework RMS, which uses an agent to generate new metapath sets and gets rewards from the recommendation model to update the agent. For the recommender, it can be any meta-path-based model, and we also propose a model HRec that adopts HAN [40] to embed users and items. We design some training techniques to train HRec better. Integrated with RMS, we propose a new algorithm RMS-HRec, which can avoid manually designing meta-paths. The RL agent and the recommender will be co-trained in RMS-HRec. In this section, we will rst introduce our recommender HRec, which takes advantage of the semantic information of meta-paths for recommendation. Then we will elaborate on how the RL agent works, which is the core part of RMS. Last, we will explain how our framework RMS can be adopted in other meta-path-based methods. 3.1 Meta-path-based Recommender Figure 3 shows the architecture of our recommender. Our recommendation model HRec is a hierarchical model which adopt HAN [40] to embed nodes. It contains a two-level attention mechanism to embed nodes. The rst level is nodelevel attention and the second is meta-path-level attention. Furthermore, the model is trained by a contrastive learning way and enhanced by some strategies we design. Two-level Attention.Since there are many types of nodes on a HIN, dierent types of nodes may have dierent feature spaces. It applies a type-specic projection to project the embeddings of dierent types of nodes into the same embedding space. After that, the information of each node will be propagated to their meta-path neighbors by a nodelevel attention layer, then the embedding of each node will be updated. Since dierent meta-paths may have dierent importance for a recommendation task, it uses a meta-pathlevel attention mechanism to fuse the node embeddings from dierent meta-paths. Since the two-level attention model is not our main contribution, we put the details in Appendix A. Training strategies.Here, we also propose two strategies which can improve the eciency and eectiveness of recommendation. 1.In the real world, the graph is often scale-free, which may contain billions of nodes and edges. Also, for some nodes on a HIN, they may have too many metapath neighbors, which will result in a huge time and memory cost. In most cases, it will encounter an outof-memory error. Therefore, we employ the Neighbor Sampling (NS) technique, which will randomly sample a subset of meta-path neighbors to perform message passing every time. This technique allows us to avoid running out of memory and save a lot of time. 2.Meta-path should be an eective tool to select semanticrelated nodes. However, if a node is connected to too many nodes in the graph via a meta-path, then this meta-path should be ineective because it cannot distinguish which nodes are highly related to a particular node. Therefore, for each meta-path, we will calculate the average number of the meta-path neighbors of each node. If the average number divided by the total number of nodes is larger than a given threshold𝑡(0 <𝑡< 1), we will discard this meta-path and not use it in the recommender. Figure 3.An overview of HRec. The node-level attention is to learn to dierentiate the importance of dierent meta-path neighbors and aggregate the weighted information from them to update the node embeddings. Also, since dierent meta-paths may play a dierent role in a recommendation task, the meta-path-level attention is proposed to learn to assign the weight for dierent meta-paths and fuse the embeddings for each meta-path to get a better representation of nodes. After that, we perform inner product to calculate the recommendation scores and apply a contrastive learning way to train the model. Recommendation.After we get the embeddings for users and items, we will calculate the inner product of them to predict the scores that how much an item matches a user: To train our recommendation model, we adopt a contrastive learning way and use BPR loss [25], which makes the scores of observed user-item iterations larger than the scores of unobserved interactions: whereO ={(𝑢, 𝑖, 𝑗) | (𝑢, 𝑖) ∈ R, (𝑢, 𝑗) ∈ R}denotes the training set.Rmeans the positive (interacted) user-item pairs andRdenotes the negative (un-interacted) user-item pairs, 𝜎(·) represents the sigmoid function. For the training steps, we will adopt the embeddings learned by Matrix Factorization (MF) [25] as the initial embeddings of user and item nodes. Then we optimize the loss Lusing Adam [18], a widely used optimizer that dynamically controls the learning rate. 3.2 RL-based Meta-path Selection To get user/item embeddings in HRec, we need meta-paths that start and end with a user/item type node to make sure that the meta-path neighbors are users/items. We propose an RL-based framework RMS to select these meta-paths for the recommender. This framework can not only be used in our recommendation model but also exiting meta-path-based algorithms. Here, we elaborate on the procedure of RMS. Reinforcement learning follows a Markov Decision Process (MDP) [30] formulation, which contains a set of states S, a set of actionsA, a decision policyPand a reward functionR. The agent learns to take actions based on the current state in the environment derived from a HIN to maximize the cumulative rewards. We design the key components of RMS and formalize it with the quartuple(S, A, P, R). Since our recommendation model needs both meta-paths which start and end with users and items, so we train two RL agents to generate user meta-paths and item meta-paths separately. The procedure is formulated as below: • State (S):To guarantee that the agent can take an optimal action based on the current state, the state needs to store the structural information of current meta-paths. Here, for each meta-path, we use the relation IDs to represent it. To encode a meta-path, we will use a binary vector and assign high bits (1) to the positions which are contained in that meta-path, then assign low bits (0) to other positions. For instance, if there are 6 kinds of relations on a HIN, we will assign the relation IDs to 1...6. if the ID representation of a meta-path𝜙is[2,6,4], then its encoding𝐸will be (0, 1, 0, 1, 0, 1). The state𝑠at step𝑖is represented by the embedding of the meta-path setΦat step𝑖. To encode a meta-path set, we just add up all of the vector representations of the meta-path in that meta-path set and apply a L2 normalization as following: whereΦdenotes the meta-path set at step𝑖,𝐸represents the encoding of meta-path 𝜙. Note that we use{𝑈𝑠𝑒𝑟 − 𝐼𝑡𝑒𝑚 − 𝑈 𝑠𝑒𝑟 }as the initial user meta-path set and{𝐼𝑡𝑒𝑚 − 𝑈 𝑠𝑒𝑟 − 𝐼𝑡𝑒𝑚}as the initial item meta-path set for the initial step. The initial user/item meta-path set contains only one meta-path that starts and ends with a user/item and reects the user-item interactions, which is crucial for recommendation tasks. • Action (A):The action spaceAfor a state𝑠is all the relations(𝑟, 𝑟, ...)that appear on a HIN, with a special action𝑆𝑇𝑂𝑃 (𝑟). At each step, the policy network will predict an action to extend the current meta-path set to yield a higher reward during the long period. Here, if the policy network selects the action 𝑆𝑇𝑂𝑃or the current step exceeds the maximal step limit 𝐼, the meta-paths will not be extended. If the predicted action is a relation in the HIN, we concatenate it with a complementary relation to make it a symmetric meta-path. This is done to ensure that the generated meta-paths are also symmetric and start and end with a user/item. Then we try to extend all the current meta-paths with this relation after autocompletion and also try to insert this relation into the current meta-path set. Note that the previous metapaths will also be copied and preserved in the metapath set and will not be removed. For instance, if the current user meta-path set is{𝑈 − 𝑀 − 𝑈 }and the predicted action (relation) is𝑀 − 𝐴, then we will auto-complete it with the relation𝐴 − 𝑀 and it will become𝑀 − 𝐴 − 𝑀. The meta-path set at the next step will be{𝑈 − 𝑀 −𝑈,𝑈 − 𝑀 − 𝐴 − 𝑀 − 𝑈 }. However,𝑀 − 𝐴 − 𝑀does not start with a user type, so it will not be added to the user meta-path set. • Policy (P):The decision policyPis a state transition function that searches for the optimal action based on the current state. The goal is to train a policy that can try to maximize the discounted cumulative rewardÍ 𝑅 =𝛾𝑟, where𝐼is the maximal step limit and𝛾is a real number between 0 and 1.𝛾is acted as a discount factor to make the far future reward less important than near future reward. In our problem, states are in a continuous space and actions are in a discrete space. Therefore, we use a typical RL algorithm, DQN [23], although it can be substituted with other RL algorithms. The basic idea of DQN is a Q-function which is a neural network and can compute a Q-value which is used to evaluate an action on a state:S × A → R. Suppose that we have a Q-network𝑄, we can directly construct a policy𝜋 that maximizes the reward: And for the update of𝑄, it will obey the Bellman equation [23]: where𝑠is the next state and𝑎is the next action. In our algorithm, we utilize MLP as the Q-network since it can approximate any function. Note that it can also be replaced by any neural network. • Reward (R):Reward is to evaluate the decision made by the policy network, which helps the agent to get better performance. Here, if the agent choose𝑆𝑇𝑂𝑃, the reward will be 0, if the agent choose any relations which will not change the meta-path set, to punish this situation, the reward will be -1. Otherwise, we dene the reward as the improvement of the performance. The formulation can be represented as follow: Here,𝑁 (𝑠, 𝑎)is the performance of recommendation task at step𝑖. Here, we adopt the NDCG@10 as the performance metric, but other metrics could also be used. Note that the reward can be less than 0. After the policy network predicts the action, we extend the metapath set and put them into the recommendation model to perform one-epoch training. To have an ecient training process, we evaluate the model on a randomly selected small test set to get the performance. Training and optimization.The whole procedure of training can be reviewed in Figure 2: i) get the current state based on the current meta-path set. ii) generate the next action (relation) based on the current state. iii) extend the current meta-path set and get the next state. iv) put the new meta-path set into the recommender and get the reward to update the RL agent. When we train the agent, we rst calculate the temporal dierence error 𝛿: In order to minimize this error, we use the Huber loss [17], which is shown as below: whereL(𝛿) =𝛿|𝛿 | ≤ 1|𝛿 | −otherwise,𝐵is randomly sampled history data. 3.3 Apply in other recommenders According to the introduction of RMS, it can not only be used in our recommendation model but also in other metapath-based models. To the best of our knowledge, the metapath-based solutions can be divided into two categories. The rst category needs two kinds of meta-paths, one is the metapaths that start and end with the user type, and the other is the meta-paths that start and end with the item type. The second category uses the meta-paths which start with the user type and end with the item type. Our recommendation algorithm belongs to the rst category and we also integrate HERec [27], which is also a rstcategory method, into our RL framework in our experiments. The results show that our RL method greatly improves the performance of recommendation. For the second category, it is also very easy to adapt to RMS. We just need to use {𝑈𝑠𝑒𝑟 − 𝐼𝑡𝑒𝑚}as the initial meta-path set and the following procedures remain the same. We also equip RMS with MCRec [12], which belongs to the second category, in our experiments. The experiment results show that RMS can also eectively improve its performance. We conduct experiments to answer these research questions. • RQ1:How does the RMS-explored meta-path set perform compared with dierent meta-path sets? • RQ2:How does RMS perform compared to other metapath selection strategies? • RQ3:How does RMS-HRec perform compared to other state-of-the-art recommendation models? 4.1 Experiment Settings Datasets.We conduct our experiments on two wildly used datasets Yelpand Douban Movie. The statistics of them are shown in Table 1 and the schema are shown in Figure 4. The detail descriptions of datasets are in Appendix B.1. Experiments Methods and Metrics.To evaluate the performance of recommendation, we split our dataset into training, validating and testing set with 8:1:1 ratio. We adopt the leave-one-out evaluation. For each user, we will regard the interacted items as positive items and the remaining items as negative items. For each positive item, we randomly select 499 negative items and rank the positive item among 500 items. Then we adopt two common metrics, Hit Ratio Yelp#Compliment (Co): 11# U - Co: 76,875 Douban#Group (G): 2,753#U - U: 4,085 Movie#Actor (A): 6,311#M - A: 33,587 at Rank k (HR@k) and Normalized Discounted Cumulative Gain at Rank k (NDCG@k). HR@k indicates whether the test positive item is ranked at top k and NDCG@k assign higher scores if the ranking position of the test item is higher. Meta-path-based Recommenders.To prove that RMS can be applied in all meta-path-based recommenders. We not only adopt it on RMS-HRec, but also on two state-of-the-art meta-path-based models (HERec [27] and MCRec [12]). Other Baseline Models.In order to show the recommendation performance of RMS-HRec, we also compare six state-of-the-art models besides the above two models. They contain Matrix Factorization (MF)-based models (BPR [25], NCF [11]), regularization-based models (CKE [44], CFKG [1]) and GNN-based models (GEMS [10], KGAT [37]). The detail descriptions of all the baseline models are in Appendix B.2. Baseline Meta-path Selection Strategies.Since this is the rst work to be applied to existing recommendation algorithms, in order to prove the eectiveness of RMS in meta-path-based recommenders, We also design two baseline meta-path selection strategies and compare them with RMS. • Random: We run several experiments until the time limit is reached. For each experiment, we randomly select a set of meta-paths. The number of meta-paths is a random number between 1 and 4. To ensure fairness, we also do one-epoch training and evaluation. After that, we select the best meta-path set to train and test. • Greedy: We rst initialize our meta-path set as the same as RMS and run several experiments until the time limit is reached. For each experiment, we randomly select a set of meta-paths. We also perform one-epoch training and evaluating on the training set Table 2. Eect of meta-path on RMS-HRec for each meta-path, then we expand our meta-path set with the best one. After that, we use the expanded meta-path set to train and test on the test set. To ensure fairness, we make sure that all of the strategies will run within the same time limit. Note that we only perform one-epoch training and evaluation when we train the RL agent. After the RL agent is well trained on the training set, we use the meta-path set generated by the RL agent and train the recommender until the model converges. Then test the recommender on the whole test set. The implementation details of our experiments are in Appendix B.3. 4.2 Meta-path Sensitivity Analysis (RQ1) To illustrate the inuence of meta-paths on recommendation performance and prove the eectiveness of RMS, we run RMS-HRec on both datasets and it returns {UBU, UBUU, BUB, BCiB, BCaB} on Yelp dataset and {UMU, UMDMU, MAM, MDM, MUM, MUMDM, MAMAM, MDMAM} on Douban Movie dataset. To test the meta-path sensitivity of RMSHRec, we adopt leave-one-out principle to see the performance change if we remove one meta-path from it. Similarly, we also try to add one meta-path and test it. Finally, we test the performance of HRec using the initial meta-path set to see how much performance improvement RMS brings. The results are reported in Table 2. Here, ’-’ means remove a meta-path, ’+’ means add a meta-path and ’Init.’ means the initial meta-path set. We have the following ndings: •The meta-path set found by RMS performs better than all of the other meta-paths on both datasets, which Table 3. Eect of dierent meta-path selection strategies proves that our approach is eective and can gure out the optimal ones. •Compared with the performance using the initial metapath, RMS get 33.3%, 20.0% and 14.9% performance improvement in terms of HR@1, HR@3 and NDCG@10 on Yelp dataset. On Douban Movie dataset, RMS gets 24.9%, 20.8% and 19.1% performance gain. This suggests that using meaningful meta-paths can greatly improve the performance of recommendation. •When we removeBUBon Yelp dataset orMDMon Douban Movie dataset, the performance drop will be larger than when we remove other meta-paths. This demonstrates that dierent meta-paths have dierent impacts on recommendation performance. When we add another meta-path into our meta-path set, the performance also drops. It indicates that using as many meta-paths as possible does not necessarily lead to performance gains. This also proves that nding optimal meta-path set by human labor is very dicult. 4.3 Meta-path Selection Method Study (RQ2) To demonstrate the eectiveness of RMS, we design two meta-path selection strategies as our baselines. We set the running time limit of all strategies to be the same and test these strategies on both HRec and existing meta-path-based models. We report the experimental results in Table 3 and have some observations and conclusions as following: RMS constantly outperforms random and greedy strategies on all metrics. This proves that our method can nd eective meta-paths for recommendation in a more ecient way than other baselines. •In HRec algorithm, we nd that RMS outperforms random by 32.5%, 25.7% and 20.1% on HR@1, HR@3 and NDCG@10 on Yelp dataset. Also, it has similar results on Douban Movie dataset. This demonstrates that random meta-paths are commonly not useful. Greedy algorithm outperforms random in most instances, but it is still not good as RMS. •We also nd that in some algorithms such as MCRec, the performance improvement is not as large as in HRec. We argue that the performance gain highly depends on the algorithms. Some models do not leverage meta-paths eectively so that their performance is not highly sensitive to the selected meta-paths. 4.4 Recommendation Eectiveness (RQ3) The main results of recommendation performance comparison are shown in Table 4. Here, we use the best meta-path set found by RMS when we run HERec and MCRec. The major observations are summarized as follows: •Our method outperforms all the baselines over all metrics on both datasets. This result shows that metapaths play an important role in recommendation on HINs and our method can eectively leverage the semantic and structural information of the HINs and perform recommendation. •RMS-HRec outperforms two meta-path-based methods (HERec and MCRec). This demonstrates that our method can maximize the roles of meta-paths in recommendation tasks on HINs. RMS-HRec can better utilize meta-paths to model the entities on HINs. •RMS-HRec outperforms two MF-based methods (BPR, NCF). This is because that HINs have more entities besides users and items, it contains more semantic information. RMS-HRec also has a better performance compared to two GNN-based methods (GEMS, KGAT), this shows that meta-paths can be a better way to explore information from HINs. •GEMS gets a bad performance in our experiments, this indicates it has limited power to leverage metastructures to capture the complex collaborative signals. Furthermore, the search space of GEMS is extremely large and it also leads to ineciency. Recommendation on HINs.HIN-based recommenders can be divided into three categories: 1) Embedding-based methods. These algorithms usually extract information from HINs directly to enrich the embeddings of users and items, such as CKE [44] , CFKG [45], KSR [14], DKN [35]. 2) Path-based methods. The path-based methods usually exploit the connectivity between users and items on a HIN which contains users, items, and other entities. This kind of algorithms [12, 15,27,29,38] can also improve the explainability since they can nd which paths are more important. 3) Graph Neural Network (GNN)-based methods. GNN-based solutions [4,36, 37,39] leverage both semantic information and connectivity information on a HIN for recommendation. Message passing is applied in these methods to propagate information to the neighbors of each node iteratively to update the embeddings. Meta-path-based recommenders.Recently, Meta-paths are wildly used in a large volume of HIN-based algorithms. Metapath2vec [5] is a HIN embedding method that adopts a random work strategy on meta-paths for node embedding. However, this method can only use one meta-path that may result in information loss. HAN [40] improves this drawback and utilizes an attention mechanism to fuse embeddings from dierent meta-paths. PEAGNN [9] generates meta-path subgraphs for each meta-path rst, then perform a GNN layer on each sub-graph and fuse embeddings from them via an attention mechanism. MEIRec [6] is an intent recommendation algorithm that will recommend queries to users. They utilize meta-paths to select related neighbors and design a GNN to obtain the embeddings of users and queries. However, all of these methods suppose that meta-paths are already given, which is not realistic in real applications. Meta-path discovery on HINs.Some work [22,26,33] attempts to discover meaningful meta-paths on HINs. FSPG [22] and MPDRL [33] are the methods in data mining elds for meta-path discovery based on several node pairs. However, they require users to provide node pairs and cannot be applied in existing recommenders. GEMS [10] adopts a genetic algorithm to nd eective meta-structures for recommendation and performs GCN [19] layers to fuse information from the meta-structures. However, these meta-structures can only be used in their recommender due to the path format and also very time-consuming to nd them because of the huge search space. Some graph embedding methods (e.g. GTN [42], HGT [13]) can weight all meta-paths via attention mechanisms. Nevertheless, they do not give a specic meta-path set, so it cannot be used in existing recommendation solutions. Furthermore, dierent algorithms may have dierent requirements in the form of meta-paths. However, the above algorithms cannot nd the specied form of metapaths (e.g. start and end with a user type) so that they cannot be used in existing methods. In this paper, we proposed a reinforcement learning (RL)based meta-path selection framework RMS to automatically select meaningful meta-paths for meta-path-based recommendation models to play to their maximum ability. We designed the state and reward function to enable the RL agent to capture the features and importance of meta-paths. Also, we proposed a recommendation model and design some training strategies to fully explore the potential of meta-paths. Extensive experiment results demonstrate the meta-paths may signicantly inuence the performance of recommendation but our method can eectively nd the meaningful ones for recommendation.