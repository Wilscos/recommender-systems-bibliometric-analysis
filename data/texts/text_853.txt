Conversational recommendation system (CRS) is able to obtain ﬁnegrained and dynamic user preferences based on interactive dialogue. Previous CRS assumes that the user has a clear target item, which often deviates from the real scenario, that is for many users who resort to CRS, they might not have a clear idea about what they really like. Speciﬁcally, the user may have a clear single preference for some attribute types (e.g. color) of items, while for other attribute types (e.g. brand), the user may have multiple preferences or even no clear preferences, which leads to multiple acceptable attribute instances (e.g. black and red) of one attribute type. Therefore, the users could show their preferences over items under multiple combinations of attribute instances rather than a single item with unique combination of all attribute instances. As a result, we ﬁrst propose a more realistic conversational recommendation learning setting, namely Multi-Interest Multi-round Conversational Recommendation (MIMCR), where users may have multiple interests in attribute instance combinations and accept multiple items with partially overlapped combinations of attribute instances. To effectively cope with the new CRS learning setting, in this paper, we propose a novel learning framework namely, Multi-Choice questions based Multi-Interest Policy Learning (MCMIPL). In order to obtain user preferences more efﬁciently, the agent generates multi-choice questions rather than binary yes/no ones on speciﬁc attribute instance. Furthermore, we propose a union set strategy to select candidate items instead of Simon Fraser University existing intersection set strategy in order to overcome over-ﬁltering items during the conversation. Finally, we design a Multi-Interest Policy Learning (MIPL) module, which utilizes captured multiple interests of the user to decide next action, either asking attribute instances or recommending items. Extensive experimental results on four datasets demonstrate the superiority of our method for the proposed MIMCR setting. • Information systems → Users and interactive retrieval;Recommender systems. Conversational Recommendation, Reinforcement Learning, Graph Representation Learning ACM Reference Format: Yiming Zhang, Lingfei Wu, Qi Shen, Yitong Pang, Zhihua Wei, Fangli Xu, Bo Long, and Jian Pei. 2018. Multi-Choice Questions based Multi-Interest Policy Learning for Conversational Recommendation. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445. 1122456 Conversational recommendation system (CRS) aims to obtain ﬁnegrained and dynamic user preferences and make successful recommendations through conversations with users [13,39]. In each conversation turn, CRS can select different actions [12] based on user feedback, either asking attributes or recommending items. Since it is able to explicitly obtain user preferences and has the advantage of conducting explainable recommendation, CRS has become one of the hot topics in current research. Various methods [7,11,18,26,46] have been proposed to improve the performance of CRS based on different problem settings. In this work, we focus on the multi-round conversational recommendation (MCR) setting [8,13,15], which is the most realistic CRS setting so far. The system focuses on whether asking attributes or recommending items in each turn, and adjusts actions ﬂexibly via user feedback to make successful recommendations with fewer turns. Despite the success of MCR in recent years, the assumption of the existing MCR [13], that the user preserves clear preferences towards all the attributes and items, may often deviate from the real scenario. For the user who resorts to CRS, he/she might not have a clear idea about what he/she really likes. Speciﬁcally, the user may have a clear single preference for someattribute types(e.g., color) of items, while for other attribute types (e.g., brand), the user might have multiple preferences or even no clear preferences. With the guidance of CRS, he/she may accept multipleattribute instances (e.g., red and black) of one attribute type. In addition, different combinations of these attribute instances are generally associated with different items. Therefore, the user could show his preferences over items under multiple combinations of attribute instances rather than a single item with unique combination of all attribute instances. To this end, we extend the MCR to a more realistic scenario, namely Multi-Interest Multi-round Conversational Recommendation (MIMCR), in which users may have multiple interests in attribute instance combinations and accept multiple items with partially overlapped combinations of attribute instances. As shown in Figure 1, the user wants a black T-shirt. For the attribute types such as "style" or "brand", he/she can accept one or more instances. He/She shows interest in the combinations of "Nike-brand" and "sports", as well as "solid" and "polo" respectively. The user could accept a "black solid polo" T-shirt or a "black Nike-brand sports" T-shirt. The task will be completed as CRS successfully recommends one of them. Existing works may encounter three signiﬁcant limitations under the MIMCR scenario. First, current CRS frameworks often employ binary questions [13], which is concise but unable to elicit user interests effectively. As shown in the conversation (a) in Figure 1, although the user accepts all of the attribute instances asked by CRS, the combination of them does not point to any target items the user prefers. Moreover, since the CRS agent has asked attribute instance "sports", it will hardly ask "polo" (the user favors). This is the result of the mutual exclusion of attribute instances with the same attribute type in the current CRS system design. On the other hand, enumerating all choices (associated with each attribute instances) [13,23,44] are not practical since there may be too many attribute instances to be shown and answered by the user. Second, as shown in the conversation (b) in Figure 1, CRS can efﬁciently obtain user preferences by using multi-choice questions. However, the existing methods utilize the intersection set strategy to select items that are associated with all accepted attribute instances, which could easily lead to the over-ﬁlter of user preferred candidate items as the conversation progresses. Finally, the existing methods simply model user’s intentions in a uniform manner, while neglecting the diversity of user interests, which will often fail to identify the user’s multiple interests through the combinations of attribute instances. To effectively address the aforementioned challenges, we propose a novel framework named Multi-Choice questions based MultiInterest Policy Learning (MCMIPL) for MIMCR. In order to obtain user preferences more efﬁciently, our method generates attribute type-based multiple choice questions. As the conversation (b) in Figure 1, the user can ﬂexibly select the attribute instances he/she likes or the option "Others" if he/she likes none. To avoid over-ﬁltering items, we propose a union set strategy to select candidate items. In particular, we select the items satisfying at least one of the accepted attribute instances as the candidate items. Moreover, we develop a Multi-Interest Policy Learning (MIPL) module to decide the next action, either asking or recommending items. In details, we construct a current graph based on the conversation state, and a global graph based on the historical user-item interactions and the global itemattribute instance correlations. Based on the representation learned by graph neural network (GNN), we iteratively capture multiple interests of the user. Finally, the next action will be decided based on the policy learning with the multi-interest representations. The contributions of this work are summarized as follows: •We extend existing CRS to a more realistic scenario setting named MIMCR, which comprehensively takes into account the incompleteness and diversity of user’s interests. •For the MIMCR scenario, we propose the MCMIPL framework with more appropriate strategies to generate questions and select candidate items. Furthermore, our method iteratively extracts the user’s multiple interests based on the current state and historical global information, to decide the next action via policy learning. •We adapt four datasets for MIMCR, and extensive experimental results on these datasets show the superiority of our method. Compared to existing sequential or social recommendation systems [21,27,40], Conversational Recommendation System (CRS) is an effective solution for dynamic user preference modeling and explainable recommendation, originated from task-oriented dialogue systems [14]. This emerging task helps the user to ﬁnd his dynamic demand with human-like dialogues. Through the conversations with users, CRS collects the user’s preference and then generates recommendations directly. In recent years, various approaches [4,11,16,22,37,39,42,46] based on deep learning and reinforcement learning (RL) have been proposed for CRS. For instance, Multi-Armed Bandits based methods [7,18,39] and meta-learning based methods [11,46] solve the user cold-start problem and balance the exploration and exploitation trade-offs for CRS. Besides, some methods [7,26,41,45] focus on asking questions about items for collecting more information of users’ preference. In addition, the approaches focusing on the dialogue ability [4,17,42], are more likely to understand user’s preferences and intentions with the input of raw natural language, and automatically generate ﬂuent responses. The most realistic conversational recommendation setting proposed so far is multi-round conversational recommendation (MCR) [8,13,15,38]. In MCR task, the system focuses on whether to ask attributes or make recommendations based on policy learning at each turn, and adjusts action ﬂexibly via users’ feedback, which aims to hit the target item for fewer interaction turns to improve the user experience. In this work, we focus on the MCR problem. For multi-round conversational recommendation, a conversation strategy is essential in the interaction process. The key of the conversation strategy is to dynamically decide when to ask questions, and when to make recommendations. At current stage of research, several reinforcement learning (RL) based frameworks have been adopted into MCR to model the complex conversational interaction environment. For instance, EAR [13] utilizes latent vectors based on available information to capture the current state of MCR, and learns the proper timing to ask questions about attributes or to recommend. Furthermore, SCPR [15] models the MCR task as an interactive path reasoning problem on the knowledge graph (KG). It chooses attributes and items strictly following the paths, and reasons on KG to ﬁnd the candidate attributes or items via user’s feedback. KBQG [23] generates the clarifying questions to collect the user’s preference of attribute types based on knowledge graph. UNICORN [8] proposes a uniﬁed reinforcement learning framework based on dynamic weighted graph for MCR, which uniﬁes three decision-making processes. Moreover, some sophisticated conversational strategies try to lead dialogues [35], which can introduce diverse topics and tasks in MCR [16, 19, 29, 32, 34, 36, 43]. However, these works all ignore a more realistic scenario in which users may accept multiple items with partially overlapped attributes. Therefore, we propose a new scenario named MIMCR to ﬁll this gap. Furthermore, we develop a novel framework namely MCMIPL to tackle the existing challenges. Although the multi-round conversational recommendation (MCR) scenario [8,13,15] is the most realistic CRS setting proposed so far, the assumption proposed by MCR [13], that the user preserves clear preferences towards all the attributes and items, still deviates from real scenario. In this work, we assume the user’s preference for items is incomplete when resorting to CRS. Speciﬁcally, the user has clear single preferences for some attribute types, while for other attribute types, his preference might be various or vague. With the guidance of CRS, he/she may accept multiple attribute instances with the same type, which results in that the user may show interests in over items under different attribute instance combinations. Therefore, we propose a new scenario namedMulti-InterestMultiround Conversational Recommendation (MIMCR). In this scenario, we deﬁne the sets of users and items asUand V, respectively. And we also separately deﬁne the sets of attribute types and instances asCandP. Each𝑣 ∈ Vis associated with a set of attribute instancesP. Each𝑝 ∈ Phas its corresponding attribute type𝑐∈ C. In each episode, there is a setVof items that are acceptable to the user 𝑢 ∈ U. The set is represented as follows: where𝑁is the number of acceptable items,P∩ P∩ · · · ∩ P= P≠ ∅andP≠ P. A conversation session is initialized by user 𝑢specifying an attribute instance𝑝∈ Phe/she clearly prefers. Then, the agent selects to ask questions about attribute instances or to recommend items based on policy learning. The CRS will update the conversational state based on the user feedback. The process will repeat until at least one acceptable item is successfully recommended to the user or the system reaches the maximum number of turn 𝑇 . We propose Multi-Choice questions based Multi-Interest Policy Learning (MCMIPL), a novel framework for MIMCR. The goal of our framework is to learn the policy network𝜋 (𝑠|𝑎)to maximizeÍ the expected cumulative rewards as:𝜋= argmaxE𝑟, where𝑠denotes the current state,𝑎denotes the action taken by the agent and the𝑟is intermediate reward. On the whole, the process of our framework in one turn can be decomposed into three steps: user modeling, consultation and transition. We ﬁrstly encode the state𝑠, which contains all the conversational information of the prior𝑡 −1turns. The current state includes six components:𝑠= {𝑢, P, P, V, P, V}. Previous methods [8,13,15] for MCR only extract the user’s interest from the current state, ignoring the complements of historical interactions to the current user’s preference. To this end, we construct a current graph and a global graph to jointly learn user representations. Moreover, we develop an iterative multi-interest extractor to obtain multiple interests of the user, which will be discussed in subsection 5.1. Once the system ﬁnishes the user modeling step, it will move to the consultation step, with the purpose to decide whether to ask attribute instances or to recommend items. To make the next action more proﬁtable and recommend successfully with the fewer turns, we employ a reinforcement learning (RL) method based on the extracted multiple interests of the user to learn the policy. The action space includes all candidate items and candidate attribute instances. However, in the real world, the number of items and attribute instances is very large, which severely limits the efﬁciency of CRS. To improve the efﬁciency, we sample𝐾items and𝐾attribute instances as action spaceA. We develop a novel dueling Q-network [33] to calculate the Q-value of each action inA. If CRS decides to ask a question, our method will select𝐾attribute instances inAwith the same attribute type to generate attribute type-based multi-choice questions. The user can choose zero (the option "Others" as shown in conversation (b) of Figure 1), one, or more attribute instances with the given attribute type. If CRS decides to recommend items, the system will select𝐾items inAto recommend. We will discuss the details of sampling strategies and policy learning in subsection 5.2. When the user responds to the action of agent, the transition step will be triggered. This step will transition the current state to the next state𝑠. If the user responses the question, attribute instance sets that the user accepts and rejects in this turn can be deﬁned as PandPrespectively. Some components are updated andP= P∪ P. When the user is recommended items, if the setVof recommended items are all rejected, the next state can be updated byV= V∪ V. Otherwise, this conversation session ends. Finally, we need to update the candidate item setVbased on the user’s feedback. Previous works [8,15] update candidate items based the intersection set strategy, that is, only the items satisfying all the accepted attribute instances inP remain, which obviously deviates from the scenario. In fact, the user might not prefer the combination of all attribute instances, but rather part of them. To this end, we propose the attribute instance-based union set strategy to update Vas follows: V= {𝑣 |𝑣 ∈ V− Vand P∩ P≠ ∅ whereVis the item set in which all items are associated to attribute instance𝑝which initializes the conversation session. In this way, we can get the next state, which will be updated as𝑠= {𝑢, P, P, V, P, V}. In this work, ﬁve kinds of rewards are deﬁned following [8,15], namely, (1)𝑟, a strongly positive reward when the recommendation succeeds, (2)𝑟, a strongly negative reward when the recommendation fails, (3)𝑟, a slightly positive reward when the user accepts an asked attribute instance, (4)𝑟, a negative reward when the user rejects an asked attribute instance, (5)𝑟, a strongly negative reward if the session reaches the maximum number of turns. In addition, since our method asks multi-choice questions, we design the reward from the user’s feedback on a question in theÍÍ form of sum as 𝑟=𝑟+𝑟. In this section, we detail the design of Multi-Interest Policy Learning (MIPL) module. As shown in Figure 2, to obtain more comprehensive user representations, we establish a current graph to capture user current preferences, and a global graph to capture long-term preferences. Based on the learned node representations of the two graphs, we propose an iterative multi-interest extractor to model user’s preferences for different combinations of attribute instances. Moreover, we design a new dueling Q-network [33] to decide the next action based on the extracted multiple interests. 5.1.1 GNN-based Representation Fusion. The existing methods [8,13,15] capture user preferences based on the current conversation state, which might cause user preferences to be incomplete due to the limited number of turns. In addition, only the current conversation information is not enough to capture the correlation of attribute instances. Therefore, we construct a current graph based on the conversation state, and a historical global graph based on the historical user-item interactions and the global item-attribute instance correlations. We employ GNNs to learn the node representations of two graphs separately and utilize gating mechanism for fusion. Current Graph Representation. Following [8], we construct a weighted graph based on the𝑡-th turn state of a episode asG= (N, E), whereN= {𝑢} ∪ P∪ P∪ V. For the edge weightE, we consider three cases: (1) The weight of edge between the user and each accepted attribute instance is1; (2) The weight of edge between each attribute instance and the associated item is 1; (3) The weight of edge between the user and each item is 𝑤, which indicates the coarse matching score of the item𝑣to the current state as follows: where𝜎 (·)is the sigmoid function,e,eande∈ Rare the initial embedding of user, item and attribute instance. We employ a𝐿-layer GCN [10] to capture the connectivity between nodes ofGand obtain higher-quality node representations in the current state. We deﬁne the initial embeddingeof node𝑛 ase, andeas the output node embedding of𝑙-th layer. The calculation method of 𝑙 + 1-th layer is as follows: whereNdenotes the set of neighbor nodes of node𝑛in the turn𝑡, W∈ Ris trainable parameters. We deﬁne the output of the last layer eas the ﬁnal embedding eof the node. Global Graph Representation. We use the historical interactions between users and items as well as the correlation between items and attribute instances to establish a heterogeneous global graphG= (N, E), whereN = U ∪ V ∪ PandE = E∪ E. The edge(𝑢, 𝑣, 𝑟) ∈ Edenotes the user𝑢has interacted the item𝑣. And the edge(𝑝, 𝑣, 𝑟) ∈ Edenotes that the item𝑣is associated with the attribute instance 𝑝. Inspired by [5,6,25], we employ a𝐿-layer Global Graph Neural Network (GGNN) to extract long-term historical interests of users, and global correlations of items and attribute instances. The initial input embeddings of the ﬁrst layer ares= e,s= eand s= e. Lets,sandsdenote the output representations of nodes after the propagation of𝑙-th layer. For the𝑙 + 1-th layer of GGNN, we model different edge types separately. For the edge in E, we adopt the calculation method as follow: whereN(𝑛)denotes the neighbor nodes of node𝑛with the edge type𝑟,Wandbare trainable parameters. For the edge in E, we adopt the same method as Equation 5 to get s(𝑛). For the user𝑢and attribute instance𝑝, we utilize ReLU function to activate semantic messages to obtain output node embeddings: s= ReLU(s(𝑢)),s= ReLU(s(𝑝)). Since item𝑣 is connected by both two kinds of edges, we accumulate different messages propagated by different types of edges and update the item node representation as follows: whereaccum(·)denotes an accumulation operation, such assum(·) ormean(·). Similarly, we deﬁne the output of the last layersas the ﬁnal embedding sof the node. We apply the gating mechanism to fuse the embeddings of nodes which belong to both graphs Gand Gas follows: where∥is the concatenate operation,W∈ Ris trainable parameter and 𝜎(·) is the sigmoid function. 5.1.2 Iterative multi-interest Extractor. In CRS scenario, since the user’s interest is diversity, we use multi-attention mechanism to model the user𝑢and attribute instances accepted by𝑢. The multiinterest embeddings of user can be obtained through the combination of attribute instances with different weights. Inspired by [3,24,31], we adopt the iterative update rule to adjust the weights of attribute instances with 𝑀 iterations more precisely. Previous works rarely consider items or attribute instances rejected by users, which can complement the current preferences of the user effectively. Therefore, we ﬁrst fuse the global embeddings of the rejected items and attribute instances with the user’s embedding: whereN= V∪ P, andW∈ Ris trainable parameters. Then, we deﬁne𝐾attention networks for𝐾interests. Based on accepted attribute instance embeddings[v, v, ..., v]and user embeddingˆv, the initial iteration calculation method of each attention network to obtain the interest embedding qis as follows: wherehandWare trainable parameters. Then, the𝑚-th iteration precisely adjusts the weights𝛼based on the𝑚 − 1-th iteration results as follows: wherehandWare parameters shared with the previous iterations. We deﬁne the output{q, q, ..., q}of𝑀-th iteration as the ﬁnal multi-interest embeddings {q, q, ...q}. A large action search space will bring a great negative impact on the efﬁciency of the system. Following [8], we select𝐾items and𝐾 attribute instances as candidate action spaceA. For candidate items to be recommended, we consider how well they match the current state. Therefore, we select top-𝐾items into the action space based on𝑤. For candidate attribute instances, we also select top-𝐾 attribute instances based on 𝑤, which is calculated: Inspired by [8], we design an improved dueling Q-network [33] to determine the next action. Following the standard assumption that delayed rewards are discounted by a factor of𝛾per timestep, we deﬁne the Q-value𝑄 (𝑠, 𝑎)as the expected reward based on the state𝑠and the action𝑎. Based on the obtained𝐾interest representations according to the current state𝑠, we calculate each score between action𝑎and each interest, and take the maximum value as Q-value: 𝑄 (𝑠, 𝑎) = max(𝑓(q) + 𝑓(q, 𝑎)), 𝑘 ∈ {1, ...𝐾}(15) where𝑓(·)and𝑓(·)are two separate multi-layer perceptions (MLP). The optimal Q-function𝑄(𝑠, 𝑎)can achieve the maximum expected reward by the optimal policy𝜋, following the Bellman [1] equation: The CRS ﬁrstly selects the action with the max Q-value. If the selected action points to an item, the system will recommend top-𝐾 items with the highest Q-value to the user. If the selected action points to an attribute instance𝑝, the system will generate attribute type-based multi-choice questions to ask user. To be speciﬁc, the system will decide a attribute type𝑐, and select top-𝐾attribute instances whose corresponding attribute type is𝑐with the highest Q-value. Then the user can choose which of the attribute instances he/she likes or dislikes. We propose two strategies to decide the attribute type𝑐: (1) Top-based strategy. We select the attribute type corresponding to the attribute instance with the highest Q-value. (2) Sum-based strategy. For each attribute type, we sum the Q-values of its corresponding attribute instances to obtain the attribute type level score, and select the attribute type with the highest score. During the experiment, we mainly use Top-based strategy, and the other strategy will be compared in the ablation study. For each turn, the agent will receive the reward𝑟based on the user’s feedback. According to user feedback, we can update the state 𝑠and action spaceA. We deﬁne a replay bufferDfollowing [8], which stores the experience(𝑠, 𝑎, 𝑟, 𝑠, A). To train our model, we sample mini-batch experiences from the replay bufferD and deﬁne a loss function as follows: L = E(𝑦− 𝑄 (𝑠, 𝑎; 𝜃, 𝜃))(17) where𝜃is the set of parameters to capture multi-interest embeddings,𝜃= {𝜃, 𝜃}, and𝑦is the target value, which is based on the optimal Q-function as follows: We update model parameters by minimizing the loss function. To fully demonstrate the superiority of our method, we conduct experiments to verify the following four research questions (RQ): • (RQ1): Compared with the state-of-the-art methods, does our framework achieve better performance? • (RQ2): What are the impacts of key components on framework performance? • (RQ3): How do the settings of hyper-parameters (such as the number of interests 𝐾) affect our framework? • (RQ4): How can our framework effectively extract multiple interests in different attribute instance combinations? To evaluate the proposed method, we adapt two existing MCR benchmark datasets, named Yelp and LastFM. To evaluate our method more comprehensively, we also process two additional datasets. The statistics of these datasets are presented in Table 1. #Triplets2,533,827 • YelpandLastFM[13]: For the Yelp, Lei et al. build a 2-layer taxonomy. We deﬁne the 29 ﬁrst-layer categories as attribute types, and 590 second-layer categories as attribute instances. For the LastFM, we adopt original attributes as attribute instances. We utilize clustering to select 34 categories as attribute types. • Amazon-Book[30]: We select entities and relations in knowledge graph (KG) as attribute instances and types, separately. To ensure data quality, we select entities associated with at least 10 items. • MovieLens: MovieLens-20Mis a widely used recommendation benchmark dataset. We retain the user-item interactions with the rating> 3. Similarly, we select entities in KG as attribute instances and relations as attribute types. For each conversation episode, we sample𝑁items with partially overlapped attribute instances as the acceptable items for the user. 6.2.1 User Simulator. Since MCR is a system based on interaction with users, we design a user simulator to train and evaluate it. Based on the scenario MIMCR, we adjust the user simulator adopted in [13]. We simulate a conversation session for each observed useritem set interaction pair(𝑢, V). We regard each item𝑣∈ Vas the ground-truth target item. The session is initialized by the simulated user specifying an attribute instance𝑝∈ P. Given a conversation, the simulated user’s feedback of each turn follows the rules: (1) when the system asks a question, he/she will accept the attribute instances which are associated with any item inVand reject others; (2) when the system recommends a list of items, he/she will accept it if the list contains at least one item inV; (3) We consider that user’s patience will run out when the maximum number of turn𝑇is reached. The simulated user will exit the system until he/she accepts the recommended item list or his patience runs out. 6.2.2 Baselines. To evaluate model performance, we compare our model with following six representative baselines: • Max Entropy: selects an attribute with the maximum entropy to ask, or recommends the top ranked items based on a certain probability [13] for each turn. • Abs Greedy[7]: only recommends items in each turn and treats rejected items as negative examples to update the model. • CRM[28]: is originally designed for single-round CRS, which utilizes reinforcement learning to select next action. Following [13], we adapt CRM to MCR scenario. • EAR[13]: adopts a three stage solution called Estimation–Action– Reﬂection for MCR, and employs RL strategy to decide actions. • SCPR[15]: proposes a generic framework that models MCR as an interactive path reasoning problem on a graph, and employs the DQN [20] framework to select actions. • UNICORN[8]: proposes a uniﬁed policy learning framework, which develops a dynamic graph based RL to select action for each turn. It is the state-of-the-art (SOTA) method. Since the above methods all use binary questions and intersection set strategy, for a more comprehensive and fair performance comparison, we adapt SCPR and UNICORN as follows: (1) The system employs multi-choice questions to ask the user. When the system decides to ask the user, the agent will generate attribute type-based multichoice questions as described in subsection 5.2. (2) The system selects candidate item set by the attribute instance-based union set strategy described in subsection 4.3. We name the two adapted methods SCPRand UNICORNrespectively. 6.2.3 Parameters Setting. We randomly split each dataset for training, validation and test with the ratio of7 : 1.5 : 1.5. We implement our model based on Pytorch and DGL. The embedding dimension is set as 64, while the batch size as128. We recommend top𝐾 = 10items or ask𝑘= 2attribute instances in each turn. The maximum turn𝑇of conversation is set as15. We employ the Adam optimizer with the learning rate1𝑒 − 4. Discount factor𝛾is set to be 0.999. Following [8], we adopt TransE [2] via OpenKE [9] to pretrain the node embeddings in the constructed KG with the training set. We construct the global graph based on the training set. The numbers of current GNN layers𝐿and global GNN layers𝐿are set to be2 and1, respectively. We extract user’s multiple interests with𝑀 = 2 iterations. For the action space, we select𝐾= 10attribute instances and𝐾= 10items. To maintain a fair comparison, we adopt the same reward settings:𝑟= 1, 𝑟= −0.1, 𝑟= 0.01, 𝑟= 0.1, 𝑟= −0.3. We conduct the online training for10, 000episodes for all methods. We set the maximum number 𝑁of acceptable items as 2 during the experiment. Other settings are explored in the hyper-parameter analysis. 6.2.4 Evaluation Metrics. Following previous studies on MCR [8,13,15], we utilize success rate (SR@𝑇) [28] to measure the cumulative ratio of successful recommendation with the maximum turn𝑇, and average turn (AT) to evaluate the average number of Figure 3: Comparisons at Different Conversation Turns. turns. Besides, we adopt hDCG@(𝑇,𝐾) [8] to additionally evaluate the ranking performance of recommendations. For SR@𝑡and hDCG@(𝑇,𝐾), the higher value indicates better performance. While the lower AT means the overall higher efﬁciency. The comparison experimental results of the baseline models and our models are shown in Table 2. We also intuitively present the performance comparison of success rate at each turn in Figure 3. Relative success rate denotes the difference between each methods and the most competitive baselineUNICORN, where the blue line ofUNICORNis set to𝑦 = 0in the ﬁgures. For clear observation, we only report the result of four competitive baselines and our model. Based on the comparison in the table and ﬁgures, we can summarize our observations as follows: • Our framework outperforms all the comparison methods on four datasets. Compared with baselines, our method extends the form of questions to attribute type-based multi-choice formula, eliciting user’s feedback of multi-acceptable items efﬁciently. Besides, the union set strategy can effectively avoid over-ﬁltering items. Moreover, we extract multiple interests of the user from the accepted attribute instances by combining current preferences with historical interactions, instead of utilizing a mixed single state representation to decide the next action. -w/o multi-interest0.435 12.41 0.145 0.522 10.96 0.204 -w/o global graph0.463 12.31 0.150 0.516 11.03 0.198 -binary questions0.448 12.96 0.151 0.513 11.12 0.192 -intersection set strategy0.414 12.29 0.145 0.438 11.81 0.159 -Sum-based strategy0.467 11.94 0.152 0.529 11.01 0.217 Figure 4: Performance comparisons w.r.t. 𝐾with 𝑁= 2. Figure 5: Performance comparisons w.r.t. 𝐾with 𝑁= 3. •Compared to the original version of SCPR and UNICORN, adaptedSCPRandUNICORNachieve better performance, which indicates the effectiveness of above designs (multi-choice questions and union set strategy) for MIMCR. Nevertheless, our method still outperforms the adapted methods. We infer that the single user preference extracted by these baselines limits the ability to capture ﬁne-grained user interests. •Interestingly, we can ﬁnd that original SCPR and UNICORN outperform adapted versions at the ﬁrst few turns, but they fall quickly as the turn increases. Since original frameworks narrow the candidate item set following the intersection set strategy, and the acceptable items might not be ﬁltered out when the number of accepted attribute instances is small, a smaller candidate item set can increase the probability of successful recommendation. As the number of conversations turn grows, the over-speciﬁc candidate item set over-ﬁlters out the acceptable items, which limits the subsequent improvement of these methods. On the contrary, our method achieves an outstanding performance in the latter stage of the conversation, where there are still comparatively generalized candidate items set and attributes space to avoid over-ﬁltering. In order to verify the effectiveness of some key designs, we conduct a series of ablation experiments on the Yelp and Amazon-Book datasets. The results are shown in Table 3. 6.4.1 Impact of different modules. Firstly, we evaluate the effectiveness of different modules, including Iterative Multi-interest Extractor and Global Graph Representation. Speciﬁcally, we remove Figure 6: Performance comparisons w.r.t. 𝐾. these critical modules of MCMIPL to observe performance changes. As can be seen in Table 3, the model performance decreases signiﬁcantly without the Iterative Multi-interest Extractor, which suggests that multi-interest representation is more appropriate for MIMCR, compared to the mixed single-interest representation. Moreover, we can see that the removal of Global Graph Representation module also leads to poor performance, which indicates that the historical user representation is important for revealing latent user preferences and guiding the extraction of current multiple interests. 6.4.2 Impact of different strategies. We conduct some experiments to study the effectiveness of strategies. Speciﬁcally, we retain the binary question type ("-binary questions"), traditional candidate item ﬁltering strategy ("-intersection set strategy"), separately. Meanwhile, we utilize the Sum-based strategy to decide the attribute type involved in questions. The binary question type version of our model performs worse than default setting, which demonstrates the efﬁciency of multi-choice question types for the conversational interaction. Besides, the intersection set strategy achieves inferior performance. It can be inferred that limitation of item selection strategy based on all accepted attribute instances will over-ﬁlter some user-acceptable items. While for the adjustment of sum-based strategy, the model still keeps competitive performance in all metrics for MIMCR, which indicates that this strategy can select suitable attribute types based on user interests. 6.5.1 Impact of Interests Number. Since interest number𝐾 is closely related to maximum number𝑁of acceptable items. We explore the hyper-parameter𝐾in the case of the maximum number 𝑁of acceptable items is 2 and 3 respectively. As we can see from Figure 4 and Figure 5, with the increase of interest number𝐾, the performance of our methods improves. In addition, when the interest number𝐾exceeds the maximum number of acceptable items, the performance will hardly improve, which indicates that some interests may exist redundancy and point to the same user preferences. 6.5.2 Impact of Asked Attribute Instances Number. When asking users questions, the attribute instances number𝐾included in a question affects model performance. As can be seen in Figure 6, the performance improves as the value of𝐾increases, which indicates that the larger number of asked attribute instances in a turn, the more information the CRS obtains. However, if the value of𝐾is too large, performance improvement is limited. That also indicates the most of extra attribute instances are invalid. Figure 7: A conversation generated by our framework. 𝐼and 𝐼denote two interests of the user, respectively. To show the process of extracting the user’s multiple interests, we present a conversation case generated by our framework from MovieLens dataset in Figure 7. We only show the attribute types and instances that are relevant to the questions. For each interest, we present attribute instances with high contribution rate, where the sum of their attention scores≥ 0.8. As can be seen, based on user’s feedback of each turn as well as historical global information, our model extracts multiple interests in different attribute instances combinations. Finally, our method makes a successful recommendation based on one of the interest representations that perfectly matches user’s preference. In this work, we deﬁne a more realistic CRS scenario named MIMCR, in which the user may accepts one of multiple potential items instead of single target item in a conversation. Based on the scenario, we propose a novel framework MCMIPL, which generates multi-choice questions to collect user preferences, and utilizes union set strategy to select candidate items. In addition, we propose a MIPL module to exact multi-interest of the user to decide the next action. Extensive experimental results on four datasets demonstrate the superiority of our method in the proposed scenario.