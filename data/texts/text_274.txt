Click-through rate (CTR) prediction is one of the the most important tasks in modern search engine, recommendation and advertising systems. Recently, some existing models leverage user’s historical behaviors for multiple interest modeling. However, there remain two main challenges in the prior works: (1) Raw user behavior sequence is noisy and intertwined, making it difﬁcult to extract multiple core interests. (2) The latent correlations between extracted multiple interest vectors are neglected, leading to information loss. In this paper, we propose a novel model named DemiNet (short for DEpendency-Aw are Multi-Interest Network) to address the above two issues. To be speciﬁc, we ﬁrst consider various dependency types between item nodes and perform dependency-aware heterogeneous attention for denoising and obtaining accurate sequence item representations. Secondly, for multiple interests extraction, multi-head attention is conducted on top of the graph embedding. To ﬁlter out noisy inter-item correlations and enhance the robustness of extracted interests, self-supervised interest learning is introduced to the above two steps. Thirdly, to aggregate the multiple interests, interest experts corresponding t o different interest routes give rating scores respectively, while a specialized network assigns the conﬁdence of each score. Experimental results on three real-world datasets demonstrate that the proposed DemiNet signiﬁcantly improves the overall recommendation performance over several state-of-the-art baselines. Further studies verify the efﬁcacy and interpretability beneﬁts brought from the ﬁne-grained user interest modeling. In recommender and advertising systems, the capab ility of click-through rate ( CTR) prediction not only inﬂuences the overall revenue of the whole platform, but also directly affects user experience a nd satisfaction. Recently, most de ep CTR models focus on capturing interaction between feature s from different ﬁelds(Wang et al. 2017; Guo et al. 2017) and the evolution of user interest over time (Zhou et al. 2019; Feng et al. 2019). Designing model to captu re user’s multiple interests can fu rther improve the performance of CTR prediction as well as model’s inter pretability. For diverse user interest mod e ling, Deep Interest Network (DIN) (Zhou et a l. 2018) utilizes attention based Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (ww w.aaai.org). All rights reserved. Figure 1: A user usually has multiple in terests when browsing an e-com merce platform. His curr e nt behavior is highly correlated with shor t-term contextual and long-term similarity dependencies. method to capture relative interests to the target item, and ob tains adaptive interest representation. However, compressing diverse user interests into one single vector may lead to information loss. Recently, in the matching stage, MIND and ComiRec (Li et al. 2019; Cen et al. 2020) utilize dynamic routing method based on capsule network (Sabour, Frosst, and Hinton 2017) to adaptively aggregate user’s historical be haviors into multiple representation vectors, which can reﬂect the various interests of the user. PIMI (Chen et a l. 2021b) additionally considers period icity and graph-struc ture in the item sequence. DMIN (Xiao et al. 2020) introduces self-attention to capture user’s multiple interests in the ranking stage. However, there exist two major challenges in user multiple interest modeling that have not been well-addr essed so far as follows: • User behavior sequences reﬂect implicit and noisy intention signals, increasing the difﬁculty of core interest extraction. The prior methods fully conn e ct all items for b e tter representation learning in the sequence, but actually they introduce a huge amount of noisy interactions. For example, in Figure 1, currently, the user bought an Apple pencil. Since the user has multiple interests, the actual dependency between this behavior is highly correlated with the short-term contextual dependency and long-te rm similarity d e pendency, while its correlation with other historical items is actually weak. • Given ex tracted multiple interest vectors, the correlations between them may provide additional in- formation for aggrega tion, but they were neglected. Prior methods either utilize the interest vectors separately (Chen et al. 2021b) or ju st concatenate all the interest vectors (Xiao et al. 2020), neglecting their underlying r e lations. To address the above two challenges, we propose a heterogeneous graph enhanced network f or robust interest extraction and intr oduce multiple interest experts for interest aggregation in the ranking stage. Spe ciﬁcally, there are three main components: Dependency-Aware Interaction Module serves for denoising and obtaining accurate item representations for the user behavior sequence, in w hich we novelly construct a heterogeneous graph network with four types of dependencies and perform hierarchical attention. Later on, as for th e Multi-Interest Extraction Module, multiple core user interests are extracted through mu lti- head attention mechanism according to the target item. To improve the robustness of the extracted interests, self-supervised interest learning is introduced through performing random edge dropout in the above two modules. In the ﬁnal MultiDependency Interaction Module, multiple intere st experts give pre diction ratings respectively. Meanwhile, a Co nﬁ-Net calculates the weights of each CTR prediction result. The ﬁnal output is the weighted aggregation of all the ratings. To summarize, DemiNet ma kes the following contributions: • For de noising and ob ta ining accurate sequence item representations, we propose a multi-dependency-aware he terogen e ous gr aph attention network to capture the item correlations and then co mbine the various dependency semantics. • We innovatively integrate a self-supervised task into the multi-interest extraction process to ﬁlter out noisy correlations between sequence item s, enhancing the robustness of the multiple interest representations. • We design a novel multiple interests aggregation structure that involves interest experts focusing on corresponding interest routes and a conﬁdence network to aggregate the ratings depending on their interest excitation strengths. • We evaluate our proposed m e thod on three real-world datasets in terms of click-through rate prediction. The results achieve state-of-the-art performance, which veriﬁes the efﬁcacy of DemiNet. User Multiple Interest Extraction. So far, most recent works wh ich explicitly modelin g user’s diverse interests are based o n user behavior sequence. DINbased methods (Zhou et al. 2018, 2019) design a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to the candidate item. Despite its functionality, we argue that a uniﬁed user embedding is difﬁcult to reﬂect the user’s multiple interests. On the other hand, MIND (Li et al. 2019) utilizes a dynamic routing method based on the capsule network (Sabour, Frosst, and Hinton 201 7) to adaptively aggregate user behavior sequence into multiple representation vectors, which can reﬂect the different interests of the user. ComiRec (Cen et al. 2020) ﬁrst clusters the user behaviors and then leverages self-attention mechanism or dynamic routing method for multi-interest extraction following MIND. PIMI (Chen et al. 2 021b) additionally models the periodicity of user’s multiple interests and introduces a graph-struc ture. DMIN (Xiao et al. 2020) introduces selfattention to reﬁne the behavior sequence embeddings. However, these methods have th e following limitations: (1) The interactivity between items in the user behavior sequence is not explored effectively, thus large noisy information is retained in the seque nce before interest extraction. (2) During the inference process, the fusion process of multiple interest vectors is coarse-grained and loses semantic information. Graph neural network in CTR Prediction. Utilizing user-item interaction data, graph neural network(GNN) has achieved great success in collaborative ﬁltering tasks. In sequential-based CTR prediction scenes, the target of graph rep resentation learning is to reﬁne the embedd ing of nodes in the graph constructed by the sequence. For example, SRGNN and its follow-ups (Wu et al. 2019; Xu et al. 2019) model the session sequence as a directed graph and perform attention mechanism for representation learning. LESSR (Chen and Wong 2020) furthe r enhances the graph layers to capture more intra-session correlations and maintain the long-range dependen cies. SURGE (Chang et al. 2021) integrates cluster-aware attention in the graph convolutional propagation process. However, the above methods don’t fully consider the different types of dependencies in the sequence graph, thus the item-wise interactions are imprecise. Heterogeneous graph neural networks (HGNN), on the other hand, can embrace various entities and relationships in a uniﬁed g raph. Recent works have introduced HGNN into recommender systems thanks to its heterogen eity a nd rich semantic info rmation (Chen et al. 2021a; Jin et al. 2020). HAN(Wang et al. 2019) utilizes a novel hierarchical attention mechanism to fully consider the particular importance of nodes and meta-paths. However, the underlying dependency relations betwe e n item nodes in the sequence graph are not considered in the above works. Problem Formalization Assume U, I d enotes the user and item set, respe ctively. Each user u ∈ U has a historical behavior sequence Sin time o rder. Historical data collected by the system is collected for building a CTR pre diction mode l. Speciﬁcally, each user-item pair instance to be predicted is repr esented as a tuple (S,P,F), where Srepresents the user behavior sequence of user u, Pdenotes the basic proﬁles of u ser u (like user age and gender ), Fdenotes the features of target item i (such a s item id and category id). Given S= (i, i, ..., i, i), where n is the number of interactions in the sequence and i∈ I r epresents the r-th item in the sequence. In practice, the goal of CTR prediction is to predict the next item ithat the user might be interested in. Heterogeneous Graph for Behavior Sequence Since the dependency links between sequence items constructed gra ph is quite com prehensive, heterogene ous graph is a powerful tool which can pack the rich relation seman tics into different edge types and graph convolution p rocess. For each user be havior sequence S, w e build its heterogen e ous directed graph , G= (V, E). Each vertex v ∈ Vcorrespo nds to an intera c te d item entity. To model the comprehensive inter-item dependencies, the itemitem h eterogeneous link interactions are denoted as E= {E, E, ..., E}, where E= [e]∈ {0, 1} denotes whether item node vhas intera c tion with item node vunder dep endency relation r, and R is the set of predeﬁned dependency relation types. The input to the heteroge neous graph is a node e mbedding matrix H= {~h,~h, · · · ,~h},~h∈ R, w here n is th e length of the user behavior sequ ence. Through the graph convolutional network, it produces the reﬁned item node embeddings H= Multi-Dependency Interaction Module Heterogeneous Graph Construction Speciﬁcally, we novelly design the following four inter-node dependency relations in the heteroge neous graph: • Contextual-Incoming Dependency Relation r. For each node vin G, the ǫ-prior neighbor set of vis denoted as N(v), which includes the prior ǫ items in the behavior sequence. For each item node in N(v) , we set it as source node and add a directed edge to v. • Contextual-Outgoing De pendency Relation r. For each node vin G, the ǫ-subsequent neighbo r set of v is d enoted as N(v), w hich includes the subsequent ǫ items in the behavior sequence. We set vas source node and add a directed edge to each item node in N(v). • Similarity Dependency Relation r. For every item node pair (v, v) in G, we perform node similarity measurement and the outcome is recorded as M. Without loss of generality, we conduct cosine distance (Wang et al. 2020) as our m e tric function. If the pair’s similarity is larger than the control threshold t, we add bidirectional edges between the pair: • Self-Connection Dependency Relation r. To strengthen the information and uniqueness of itself, we take every item node’s self-conne c tion into consideration Since the scale of each sequence graph Gis quite limited, to avoid over-parameterization and over-smoothing (Chen et al. 2020), we model the semantics between nodes at the gran ularity of the above fo ur dependency r elations, which are all one-hop . Hierarchical Heterogeneous Graph Attention To extract core interests and relieve noise signals in G, we leverage the graph attention mechan ism (Veliˇckovi´c et al. 2017) for inter-node interaction since it can strengthen important signals and weaken noise ones during message propagation. Moreover, we notice that neighbors based on different dependency relations play different roles during node interactions. Th erefore, each node should ﬁrstly fuse m essage fr om neighbor nodes of the same dependency and then combine the dependency-wise correlation information. Speciﬁcally, for each node, we ﬁrst perf orm inter-node a ttention to learn the weights o f dependency-based neighbor nodes, aggregating them to obtain the dependency-speciﬁc representation. After that, inter-dependency attention is performed to obtain the uniﬁed no de embedding, which is the adaptive weighted combination of dependency-speciﬁc representations. To increase the expressive power and stabilize the learning proce ss, the attention process is extended to the multi-head metric (Vaswani et al. 2017). • Inter-Node At tention. Under the dependency-speciﬁc edges E, given a node e mbedding pair o f head φ (~h,~h) with directed connection, the inter-node attention can learn the importance awhich means how important node vis for node v: where Wis a trainable matrix, ⊗ is the concatenation and matrix product operator. Then we normalize the importance to get the weight coefﬁcient αvia softmax function: The node embedding of vunder relation r can be aggregated as follows, in which k is the concatenation opera- • Inter-Dependency Attention. For fusing semantic information of node v, after obtaining all the dependen cyspeciﬁc embeddings, dependency-level attention ﬁrst Figure 2: Illustration of the DemiNet model, which is made up of three modules and a self-supervised interest learning task. learns the importan ce of each dependency em bedding, denoted as a, is shown as follows: where Wis a trainable matrix, bis a trainable scalar, ⊗ is the matrix product operator. T he weight of dependency relation r of node v, denoted as β, can be obtained by normalizing the importa nces of all dependency relations, Then, the uniﬁed embed ding of node vunder dependency relation r can be aggregated as follows, in which k is the concatenation operator:: After o btaining reﬁned item embeddings H∈ R from the prior module, to stre ngthen the sequen c e order relationship, we add it with a trainable positional embedding matrix E∈ R, the ﬁnal node embeddings H∈ R is shown as follows: Multi-Interest Extraction Module Since multi-interest extraction can be viewed as the pro c ess of soft clustering of items, we utilize multi-head attention (Vaswani et al. 2017) to extract multiple interests, in which each head captures a unique interest vecto r. To be speciﬁc, given the candidate item embedding~hand item sequence−→ embedd ing H, the interest embeddingvis generated as follows: where the attention mechanism Attentionis a two-layer feed-forward neural network applying the LeakyReLU nonlinearity, k is th e concatenation operator. Through stacking all the captured interest vector representations from K attention heads, we can obtain the user multiple interests matrix according to the target item i: V=−→−→−→ Multi-Interest Aggregation Module Obtaining Vfrom the prior module, in this place we design K interest experts focu sing on various domains to give prediction scores respectively. Meanwhile, the conﬁdence level of e ach score is calculated according to a speciﬁc Conﬁ-Net which is based on a three-layer neural network. The ﬁnal output of the modu le is the weighted aggregation of the prediction scores. Interest Expert This unit contains the main CTR prediction route and a trainable interest expert prototypical representation p. A s for the main CTR pre diction route, it contains a unique batch normalization layer following by a three-layer neural network. Given V, we ﬂatten it and concatenate them with embeddings of Sand F, denoting this overall embedding vector as E. With Eas the network input, through the m a in route , each interest exper t k produces its output score o. Conﬁ-Net The implementation of Conﬁ-Net is a threelayer MLP. Given the k-th interest expert, we fetch the k-th−→ user interest vectorvfrom Vand concatenate it with Sand F, deno ting this embedding vector as E. Feeding Einto the Con ﬁ-Net, it outputs a combination vector c, which condenses the excitation relatio nship betwe en the candidate item and the k-th interest. We perform dot product to calculate the con ﬁdence between cand the k-th interest expert. The ﬁnal conﬁdence of each interest expert is assigned with a normalized weighted ω: We have scaled the d ot product results by a factor of to help convergence. Then we get ﬁnal output vector of the model ˆy: It should be noticed that in Eq.14, during the softmax process, the model is alr eady forc ed to preserve distinct information in each interest route (Ma et a l. 2020). Therefore, in practice, we do not add regularization terms to encourage disentanglement between the K-routes interest vectors. Self-Supervised Interest Learning The heterogeneous graph network empowers DemiNet to acquire accurate item sequence representations. However, after modeling the four types of dependency relations, we consider that the denseness of inter-item corre la tions in sequence grap h might impede interest extraction process, which would result in a sub-optimal CTR prediction performance. Inspired by the suc c essful practices of selfsupervised learning on sim ple gr aphs (Wu et al. 2021), we novelly integrate it into our model to enhance the robustness of interests extractio n process. We ﬁrst perform random edge drop out on the heterogen eous graph twice, gene rating two different gr aph views. Then by graph attention and interest extraction, we can obtain two views of multiple interests. Throu gh Jensen–Shannon (JS)-Divergence minimization between th e two interest views, the attention operators and interest extractor can capture more robust features and relieve the impact from noisy inter-item correlations. Multi-View Generation For the heter ogeneous graph G, we perform edge dropout with the drop out ratio ρ, creating two different views of G. The two independent edge dropout processes sand sare re presented as: where M, M∈ {0, 1}are two masking vectors on the edge set E. Then we go through the graph attention and interest extraction forward pass on s(G) and s(G) respectively, obtaining two multiple interests matrix V, V from different views. Since in the heterogeneous graph construction process, not all the edges in the dependency relations are valuable, some redundant ed ges are in troduced. Hence, fo r denoising, we couple th ese two interest matrixs together, aiming to capture the intrinsic and core patterns of the dependency relations. Self-Supervised Re gularization Speciﬁcally, we apply softmax alo ng each in te rest vector in the two interests matrixs, obtaining their dimension-level interest distributions. For V, V, the two distributions at interest-route k are denoted as P, P. Then, we conduct regularization by minimizing the JS divergence between these two output distributions. After summing up all the training samples, the regularization term is formulated as: Overall Loss Function The main loss function is deﬁned as the cross-entropy of the pred ic tion and the ground truth. It can be wr itten as follows: where ydenotes the one -hot encodin g vector of ground truth. Considering the self-supervised regularization, the ﬁnal training fu nction of DemiNet is formulated as: where β controls the magnitude of the self-supervised task. Experimental Setup Datasets We conduct experim ents on three publicly available datasets: Taobao, Amazo nand RetailRocket. Taobao dataset is a collec tion of user behaviors from Taobao’s recommender system. It contains user behavior sequences of about one million users. We take the click behaviors for each user and sort them according to time for constructing the behavior sequence. For the Am a zon Dataset, we choose the Books subset. We regard reviews as one kind of interaction behavior, and sort the reviews from one user by time. RetailRocket is collected from a real-world e-commerce website. It contains sequential events of viewing and adding to cart. We treat both of them as clicks. After preprocessing, the statistics of the datasets are shown in Table 1. https://tianchi.aliyun.com/dataset/dataDetail?dataId=649&use rId=1 http://jmcauley.ucsd.edu/data/amazon/ https://www.kaggle.com/retailrocket/ecommerce-dataset Settings and Evaluation Metrics To make sure the strong generalization of our m ethod and avoid da ta leakage, we split the train set and test set rigidly according to the timestamp. Speciﬁcally, we search the 80% tim e splitting point of each dataset, putting behavior samples occurred b efore that splitting point in to train set while latters into the test set. For all the datasets, we ﬁlter out users with less than 5 interactions. For evaluation metrics, we choose the widely used AUC and Log Loss to measure the performan ce. In the CTR prediction task, they reﬂect the pairwise ranking ability and point-wise likelihood, respectively. Baselines To illustrate the effectiveness of our model, w e choose methods in the following two groups for c omparison: • Group 1 (Multi-Interest Extraction) (1) DIN (Zhou et al. 2018) is an early work for user behavior modeling, which proposes to so ft-search user behaviors w.r.t. candidates. (2) DIEN (Zhou et al. 2019) integrates GRU with attention mechanism for capturing the evolution trend of user interests. We omit the trick of auxiliary loss fo r better embedding learning. (3) MIND (Li et al. 2019) extracts multi-interest through dynamic routing . (4) Co miRec (Cen et al. 2020) combines multiinterest extraction and diversity-aware regularization together. There exists two versions: ComiRec-SA and ComiRec-DR, which based on attention mechanism and dynamic routing respectively. (5) DMIN (Xia o et al. 2020) captures u ser’s latent multiple interests through stacking multi-head attention layers in the ranking stage. • Group 2 (Ensemble Learning) (1) Multi-Avg gives the average score of all the interest experts. (2) HardRouting chooses the score from the interest expert with the highest conﬁdence as the ﬁnal result in the inference stage. (3) MoE (Ma et al. 2018) designs a m ixture o f experts, in which every exp ert’s probability is obta ined through a gating network. Parameter Settings All m ethods are learned with th e Adam optimizer (Kingma and Ba 2014). The batch size is set as 256. The learning rate is set as 10. For a fair comp a rison, the embedding size of each feature d is set as 16 for all m odels. For the self-supervised in terest learning part, the edge-dropo ut rate is set as 0.6 on all datasets. For hyperparameters of De miNet, the similarity threshold t is searched between {0.6, 0.7, 0.8}. the num ber of neighbors ε in contextual dependency m odeling is set as 3. We set K = 8 on the Taob ao dataset while K = 4 on the other two datasets according to the gap statistic method (Tibshirani, Walther, and Hastie 2001). To avoid over-smoothing on the dense heterogen eous g raph, we set the depth of GNN layers as 2. Table 2: T he performance of DemiNet with other baseline methods over th ree datasets ∗ denotes signiﬁcance p-value <0.01 versus best baseline. ∗ denotes signiﬁcance p-value <0.01 versus best baseline. Overall Performance Comparison The overall exp e rimental results are shown in Table 2. • Group 1 (Multi-Interest Extraction) From the results, we could ﬁnd the following facts: (i) The performanc e of DemiNet is signiﬁcantly better than the baselines. AUC values are improved by 3. 25%, 1. 66%, and 1.76% on three datasets co mpared to the best baseline, r espectively. (ii) We could ﬁnd that multi-vector interest representation models (DMIN, ComiRecSA) outperform o ther traditional models which only use one uniﬁed user interest embedd ing. This fact shows th at extractin g multiple interests is essential to CTR prediction performance. (iii) DemiNet outperf orms other multiple interest extraction methods signiﬁcantly. MIND & ComiRec neglect the in teractions in item sequences, failing to denoise it. Moreover, the interests are relatively independent in their aggregation part. DMIN on the other hand introduces noisy item interactions. • Group 2 (Ensemble Learning ) In this section, we modify the aggregation method of scores fro m in te rest experts. From the table, we can observe that: (i) DemiNet outperforms all the baselines in three datasets with 0.93%, 0.79%, and 0.34 % improvement rates, respectively. (ii) Multi-Avg and Hard-Routing are two simple yet effective aggregation oper ators. DemiNet outperform s them, manifesting the importance of each interest expert is adaptive accordin g to the user-item pair. ( iii) DemiNet’s improvement over MoE man ifests that obtaining the excitatio n relation between user interest and candidate item before interacting with the interest expert representation is an effective approach, which is a c lear two-step process. Ablation and Hyper-Parameter Study Ablation Study We further investigate that multidependency-aware heterogen e ous attention (DHA), self- Figure 4: Multiple Interests Visualization using t-SNE supervised interest learning and multi-inte rest aggregation module are all essential parts of the CTR predcition task. We conduct the ablation studies by removing the above three components. For the model variant w/o DHA, we remove the heteroge neous graph neural network and extract the multiple interests directly from the initial be havior seque nce. And for the model variant w/o SSL, we remove the self-supervised interest learning task. In w/o IE, we replace multiple interest experts with a single one. We show the experimental results of them on the thr e e datasets in Table 3. According to the experimental results, we have the following observations: • DemiNet performs better than w/o DHA, w/o SSL a nd w/o IE in terms of AUC and Log Loss, which demon strates each component improves the performance effectively. • Among the three main components, DHA brings the most signiﬁcant performance gain through modeling the contextual and similarity item dependencies. Multiple interest experts also enhances th e prediction accuracy greatly through adaptive aggregation. As for selfsupervised inte rest learning, we can draw to the conclusion that m ore robust interest representations can improve the performance by extracting core interests. Effect of Multiple Interest Number K Figure 3a shows the p e rforman ce comparison for the number of interests K on all the three datasets, in which we vary K from 2 to 16. The experimental results demonstrate that as the number of interests increases, the model captures and combines mu ltiple intentions of the user and its pe rforman c e will b e hig her. However, the perfor mance tends to become saturated as K continues to gr ow, possibly due to overﬁtting problem, and the computational cost will also increase. Effect of Graph Convolution Operator We compare the effects of using various graph convolutional operators on th e hete rogeneous graph, including GCMC (Berg, Kipf, and Welling 2017), LGC (He et al. 2020) and vanilla GAT (Veliˇckovi´c et al. 2017) a nd our proposed DHA. The ﬁrst observation is tha t methods based on attention mechanism performs slighter better, which can strengthen important signals and weaken noisy signals. The second observation is that DHA performs the best on all the datasets, which demonstrates the information gain of considering the dependency semantics into graph attentive proce ss. Case Study In-depth analyses of the extracted multiple interests For in -depth analyzing the extracted user multiple interests, we randomly select two users in Amazo n-Book dataset and project their interests vectors according to various ca ndidate items into 2-D space with t-SNE alg orithm (Van der Maaten and Hinton 2008). As shown in Figure 4, dots with the same color refers to the same interest route. From the distribution we can observe that interest vectors from the same ﬁne-g rained interest route tend to cluster with each other. To conclude, the visualization shows DemiNet can effectively distinguish the ﬁne-gr a ined im plicit intentions be hind the interactions between users a nd items, leading to ﬁn er reﬂection of deep motivations behind user behaviors. Adaptive conﬁdence assignment of various items To analyze the conﬁdence assignment variation according to the candidate items, we randomly select a user in Amazon-Book dataset. Figure 5 shows ﬁve recommended items for th is user which belong to different categories. As for items belonging to similar category in semantics (i.e. Math and Biological in this case), DemiNet tends to assign h igher conﬁdence to one same interest expert (i.e. IE-2 in this case), which is skilled in that domain. While for items belonging to categories that are quite distant in semantics, the interest expert assigned with higher conﬁdence is also different. In this paper, we propo se a novel method named DemiNet which models the multiple user interests in CTR prediction tasks explicitly. To alleviate the noisy signals in the behavior sequence, we perform mu lti- dependency-aware heterogen e ous attention and self-superv ised interest learning. To aggregate the multiple interests, route-speciﬁc interest experts and Co nﬁ-Net are introduced. We conduct extensive experiments on three real-world datasets. The results demonstra te the effectiveness of DemiNet. Future work includes multi-hop dependency meta-path modeling and investigating the interpretability of DemiNet.