Nowadays, recommender systems already impact almost every facet of people’s lives. To provide personalized highquality recommendation results, conventional systems usually train pointwise rankers to predict the absolute value of objectives and leverage a distinct shallow tower to estimate and alleviate the impact of position bias. However, with such a training paradigm, the optimization target differs a lot from the ranking metrics valuing the relative order of top-ranked items rather than the prediction precision of each item. Moreover, as the existing system tends to recommend more relevant items at higher positions, it is difﬁcult for the shallow tower based methods to precisely attribute the user feedback to the impact of position or relevance. Therefore, there exists an exciting opportunity for us to get enhanced performance if we manage to solve the aforementioned issues. Unbiased learning-to-rank(LTR) algorithms, which are veriﬁed to model the relative relevance accurately based on noisy feedback, are appealing candidates and have already been applied in many applications with single categorical labels, such as user click signals. Nevertheless, the existing unbiased LTR methods cannot properly handle multiple feedback incorporating both categorical and continuous labels. Accordingly, we design a novel unbiased LTR algorithmto tackle the challenges, which innovatively models position bias in the pairwise fashion and introduces the pairwise trust bias to separate the position bias, trust bias, and user relevance explicitly. Experiment results on public benchmark datasets and internal live trafﬁc show the superior results of the proposed method for both categorical and continuous labels. Recommender systems model user behaviors (e.g., click, purchase, share, and dwell-time) to learn user preference and recommend top-ranked items to the corresponding user. In industrial recommendation applications, pointwise learning is the mainstream preference modeling approach, which aims to learn the absolute probability of user behavior rather than the relative preference between items. Therefore, there *. These authors contributed equally.†. henrybjren@tencent.com ‡. violatang@tencent.com §. siwenzhu@tencent.com 1. The source code of the proposed algorithm is released at https://github.com/phyllist/ULTRA. is a gap between the training objective of pointwise learning and the actual ranking objective in recommendation scenarios. On the contrary, pairwise learning focuses on minimizing the number of inversions to predict relative orders between items, which often performs better in practice and has been widely applied to search engines (Pasumarthi et al. 2019) as predicting relative preference is more consistent with the nature of ranking. However, user behaviors are often biased in recommendation scenarios. For instance, a user might click and watch a video simply because it is ranked high rather than it is the one that the user likes the most, so higher-ranked items often exhibit signiﬁcantly larger CTR than lower-ranked items. Due to the position bias, it is difﬁcult for pairwise learning to learn the accurate relative preference, which results in the rare application of pairwise learning in recommender systems. Thus, to estimate relative user preference accurately and improve ranking performance in recommender systems, it is essential to correct position bias from user behaviors. To deal with position bias in recommender systems, prior works propose to add a shallow tower to model bias directly and alleviate its impact (Zhao et al. 2019; Guo et al. 2019). However, bias features and relevance-related features are coupled together with the shallow tower based models, which makes it rather difﬁcult to eliminate bias effectively (Wang et al. 2018a). Unbiased learning to rank (LTR) methods (Ai et al. 2018; Hu et al. 2019; Agarwal et al. 2019) model relation between the absolute values of unbiased relevance and the biased user feedback to learn the unbiased ranker, which perform well in search engines. Nonetheless, the existing methods can only work for categorical labels instead of continuous labels, as the relationship between relevance and user feedback in continuous tasks does not adhere to their core assumptions shown in Equation 3. Moreover, as continuous feedback (e.g., dwell time and video watch ratio) is essential to model user preference, it is critical to design an unbiased LTR method for continuous labels to learn unbiased relative preference accurately in recommender systems. To achieve the goal, we propose a novel unbiased pairwise LTR method, which models position-based examination bias in the pairwise fashion and introduces the pairwise trust bias to better model the underlying bias and learn the relative preference more accurately. Compared with the state-of-the-art pointwise unbiased LTR methods, the proposed method models the correlation between the unbiased relevance pairs and the biased user feedback pairs directly, which bears no assumption mentioned above on labels and can work for both categorical and continuous labels. Moreover, pairwise debiasing is more consistent with the objective of ranking, which achieves better performance of debiasing and preference learning. Furthermore, the introduction of pairwise trust bias separates the position-dependent examination bias and the position-pair-dependent relative trust bias explicitly for better correction. Experiment results on benchmark datasets and online A/B testing show that the proposed method outperforms not only the SOTA ranking models in recommender systems but also the SOTA unbiased LTR methods commonly used in search engines. The main contributions of the paper are as follows: • We propose a novel unbiased pairwise LTR method for better bias correction and unbiased relative preference learning. To the best of our knowledge, the proposed method is the ﬁrst unbiased LTR method that can work for both categorical labels and continuous labels. • We extend the regression-based EM algorithm (Wang et al. 2018a) to the pairwise setting to estimate the parameters of the proposed method effectively. • We conduct ofﬂine and online experiments to evaluate the performance of our method. Results on benchmark datasets show that our method outperforms both the SOTA unbiased LTR methods and the SOTA rankers in recommendation scenarios. Online A/B test in one of the world’s largest content recommendation platforms shows a signiﬁcant 2.08% improvement of the business metric. In this section, we discuss related works on unbiased learning to rank and debiasing methods in recommender systems. Unbiased Learning to Rank Recently, unbiased LTR has been actively studied as a promising approach to learning from biased feedback. (Joachims, Swaminathan, and Schnabel 2017) ﬁrst present the counterfactual framework to learn the theoretically unbiased ranker via the inverse propensity weighting (IPW) estimator. To estimate the propensity, (Wang et al. 2016) and (Joachims, Swaminathan, and Schnabel 2017) propose the methods of result randomization and intervention. To reduce the negative impact on user experience, some works propose to learn the propensity from biased feedback directly, such as DLA (Ai et al. 2018), regression EM (Wang et al. 2018a) and Unbiased LambdaMART (Hu et al. 2019). The positiondependent click noise (Joachims et al. 2017) is ignored in these methods, which is addressed by (Agarwal et al. 2019). However, all of the above methods can only work for categorical labels because of their core assumption of equation 3. In contrast, our proposed method can work for both categorical and continuous labels. Modeling Biases in Recommender Systems There have been some works on modeling and eliminating biases in recommender systems. (Zhao et al. 2019) and (Guo et al. 2019) both propose to employ a shallow tower to estimate and alleviate the impact of bias by combining the output of the shallow tower and the main model with summation or multiplication. However, due to the coupling of bias features and relevance-related features, shallow tower based methods cannot work effectively. (Wu et al. 2021) apply the idea of (Wang et al. 2018a) to one recommendation scenario and learn unbiased ranker with IPW framework and regression-EM method. However, it still can only work for categorical labels, which is inadequate for typical recommender systems. The proposed method in this paper extends the IPW framework to the pairwise setting to deal with continuous labels in recommender systems, which achieves superior performance of debiasing and preference learning. In this section, we provide the general framework of unbiased learning to rank in recommender systems for both categorical and continuous labels. Suppose there are N positions, for a user request u, item x is displayed at position i∈[1, N], ris the unbiased relevance of user-item pair (u, x). For simplicity, we only consider binary relevance, and one can easily extend it to the multi-level case. Then we can calculate the risk function as follows: R(f) =L(f(u, x), r)dP (r= 1, u, x where f denotes a ranking system, L(f (u, x), r) denotes the loss function based on performance metrics and P (r= 1, u, x) denotes the probability distribution of positive ron x. For notation simplicity, the position information of items is omitted from the loss function. As most performance metrics such as Discounted Cumulative Gain (DCG) (J¨arvelin and Kek¨al¨ainen 2002, 2017) and Average Relevance Position (ARP) (Wang et al. 2018b) only consider relevant items, the risk function here is calculated only based on items with positive relevance label. If the unbiased relevance label is available, we can learn a ranker by minimizing the empirical risk function shown in Equation 2. However, it is infeasible to collect enough true relevance labels in large-scale recommendation applications as the recommendation results are personalized and we cannot afford to ﬁnd enough suitable judges for all users. Thus, abundant biased user actions (e.g., click, purchase, like) are valuable labels if we can ﬁll the gap between user actions and relevance labels. As assumed in Position-Based Model (PBM) (Richardson, Dominowska, and Ragno 2007), the user clicks the item if and only if he examines a relevant item, and the probability of examination only depends on the position and is independent with relevance. Let edenotes whether item xis examined by the user, cdenotes the user action label of x, we can model the relationship Notation Description xFeatures of item displayed in position i. rUnbiased recommendation relevance of x. cBiased synthesized label of x. eExamination of x. θP (e= 1|i) θP (e= 1|i, c= 0) P (c> c|e= 1, e= 1, r> r, i, j) P (c> c|e= 1, e= 1, r<= r, i, j) βP (r> 0|u, x) between cand ras follows: P (c= 1|u, x, i) = P (r= 1|u, x, i)P (e= 1|u, x, i) Then we can derive the risk function and empirical risk function based on user action label: R(f) =L(f(u, x), c)P (e= 1|i)dP (c= 1, u, x, i) As shown in Equation 4, one can see that Rwith the inverse propensity weighting (IPW) (Joachims, Swaminathan, and Schnabel 2017) loss equals to Rin fact. In other words, if we can estimate the examination probability P (e= 1|i) accurately, we can learn an unbiased ranker based on user action labels. Due to the assumption between the probability distribution of relevance labels and user action labels shown in Equation 3, the aforementioned method can only be applied to tasks with categorical labels rather than continuous labels, which tends to exhibit much more complex relationships between the user action labels and the true relevance. However, continuous labels such as dwell time and video watch ratio are important feedback of user satisfaction and are commonly used in industrial recommender systems. To correct position bias for continuous labels, we propose a novel unbiased LTR method which circumvents the previous assumption between the unbiased relevance labels and the biased user action labels. Details of the method would be provided in the following sections. To provide high-quality recommendations, industrial recommender systems often consider multiple live metrics simultaneously (Tang et al. 2020). For example, we would like to maximize the user’s video view count, dwell time, and other satisfaction-related metrics (e.g., liking and sharing) in the video recommendation scenario. To directly model multiple live metrics simultaneously in one single model, we deﬁne a synthesized label based on diverse user actions. As shown in Equation 6, the synthesized label merges multiple user actions (e.g., click, dwell time, and satisfaction) through a combination function Φ, where M is the number of user actions considered in live metrics. The combination function is usually set based on business goals, and its form can be fairly ﬂexible to capture complex relations between live metrics. For instance, weighted summation and weighted multiplication are commonly used in industrial recommender systems. There are two reasons to combine multiple user behaviors to the synthesized label for the learning objective. First, the gap between the training objective and the live metrics can be bridged through learning from the synthesized label, as the synthesized label is closely related to the ultimate business goals. Second, different from pointwise learning, pairwise learning and listwise learning care more about the relative order of the items rather than the absolute scores, which means that the absolute predicted scores of different unbiased LTR models might not be in the same scale. Thus, if we model different user actions through different unbiased LTR models, we could not combine predictions of different user actions in a theoretically sound way. Due to the continuous user feedback, the synthesized label is not categorical, and the aforementioned unbiased LTR method cannot work. To circumvent the limitation of the strong assumption between unbiased relevance labels and biased user action labels, we propose a novel unbiased LTR method from the perspective of pairwise learning, which can work for both categorical and continuous labels. In the pairwise setting, the loss function is deﬁned on item pairs. Let ˆy= f (u, x) denotes the prediction for pair (u, x), L(ˆy, r, ˆy, r) denotes the pairwise loss function and P (r>r, u, x, x) denotes the probability distribution of positive relevance pair on item pair (x, x), the risk function and the empirical risk function are deﬁned in Equation 7 and Equation 8 respectively. Similar to unbiased LTR for categorical label, we also consider positive relevance pairs only as negative and neutral pairs should not be involved to optimize common ranking metrics such as DCG and ARP. R(f) =L(ˆy, r, ˆy, r)dP (r> r, u, x, x) (7) Pairwise PBM based Unbiased LTR. To model the relation between unbiased relevance pair (r, r) and biased synthesized label pair (c, c), we extend the PBM to the pairwise setting. Speciﬁcally, we assume that the examination of item xand xare independent and if xand xare both examined, xexhibits larger synthesized label than x if and only if xis more relevant than x, which is: P (c>c, e= 1, e= 1|u, x, x, i, j)=θθγ(9) Then we can derive the risk function based on the positive synthesized label pairs: =L(ˆy,c, ˆy,c)θθP (e=1|u,i,j,x,x,c>c)dP (c>c,u,x,x,i,j) =L(ˆy,c,ˆy,c)θθdP (c>c,u,x,x, i, j, e= 1, e= 1) =L(ˆy,c,ˆy,c)θθdP (r>r,u,x,x, i, j, e= 1, e= 1) =L(ˆy,r,ˆy,r)θθdP (r>r,u,x,x, i, j)P (e=1|i)P (e=1|j) Based on the justiﬁcation that Ris an unbiased estimator of R, we can learn an unbiased ranker through minimizing the empirical risk function lossshown in Equation 11, where L= L(ˆy,c,ˆy,c). It is worth noting that samples with positive labels and zero labels are separately handled explicitly for better precision in the empirical risk function, as the posterior probability of examination is different for samples with different labels. In detail, the posterior examination probability is deﬁnitely 1 for items with a positive label but is uncertain for items with zero labels. where h= P (e= 1|u, x, x, i, j, c> c, c= 0). Pairwise Trust Bias based Unbiased LTR. The pairwise PBM based unbiased LTR method assumes that if xis more relevant than xand they are both examined, xwill exhibit a larger synthesized label than x. However, according to the user study in (Joachims et al. 2017), users tend toloss trust higher-ranked items more, so higher-ranked items exhibit larger click ratios even if items in different positions are all examined and equally relevant, which is called the trust bias. To estimate the relation between unbiased relevance pair (r, r) and biased synthesized label pair (c, c) more accurately, we incorporate the trust bias explained above and extend it to the pairwise setting in this paper. Since the trust bias only depends on the exposed position, we formulate the pairwise trust bias in Equation 12 and Equation 13 and assume that P (c> c|e= 1, e= 1, r> r, u, x, x, i, j) and P (c> c|e= 1, e= 1, r≤ r, u, x, x, i, j) are independent of (u, x, x). Speciﬁcally, for a positive relevance pair r> r, the probability of exposing a positive synthesized label pair c> cis not 1 but , while a non-positive relevance pair also has the probability to exhibit a positive synthesized label pair. and are only position-dependent and follows the constraint 0 < < P (c>c|e= 1, e= 1, r> r, i, j)= (12) P (c>c|e= 1, e= 1, r≤ r, i, j)= (13) Then we can calculate the probability of a positive synthesized label pair. As shown in Equation 14, there are two cases to expose a positive synthesized label pair c>c. First, if xis examined, the relation between cand cdepends on the relation between rand rand the trust bias. Second, if xis not examined, the positive pair c> creduces to the positive data point c> 0, which only depends on the examination and relevance of xaccording to PBM. P (c> c|u, x, x, i, j) = P (c> c, e= 1, e= 1|u, x, x, i, j) + P (c> 0, e= 1, e= 0|u, x, x, i, j) As we can no longer deduce the state of r> rfrom the condition of c>c, e=1, e=1, Rcould not be derived from Rwith the introduction of pairwise trust bias. To tackle this issue, we calculate the probability of positive relevance pair given the positive synthesized label pair and true examination, which is: P (r>r|u, x, x, i, j, c> c, e= 1, e= 1) =P (c>c|e=1,e=1,r>r,u,x,x,i,j)P (r>r|u,x,x)P (c>c,e= 1,e= 1|u, x, x, i, j) Finally, we deﬁne a Bayes-IPW loss based on loss and m, which compensates and corrects for both the position-based examination bias and trust bias. One can see from Equation 16 and Equation 15 that the Bayes-IPW loss reduces to the lossin the noisy free case of pairwise PBM, i.e., when = 1 and − = 0. where his calculated as follows: h= (e= 1|u, x, x, i, j, c> c, c= 0) =P (e=1|u,x,j,c=0)P (c>c|u,x,x,i,j,e=1,c=0)P (c>c|u, x, x, i, j, c=0) where θdenotes the posterior examination probability when the synthesized label of item xis known to be 0. Direct Optimization for Ranking Metrics. As pairwise loss (Burges et al. 2005) focuses on minimizing the number of pairwise errors and neglecting the relative importance of item pairs with different relevance, which does not match well with common performance metrics such as DCG and ARP. To directly optimize the ranking metrics in the model, we reﬁne the loss function following the practice of LambdaRank (Burges, Ragno, and Le 2006). The main idea of LambdaRank is to incorporate a delta NDCG to directly optimize the evaluation metric NDCG, where delta NDCG denotes the difference between NDCG scores if item xand xare swapped in the ranking list (Burges, Ragno, and Le 2006; Burges 2010). As the unbiased true relevance is not available, we assume that the biased synthesized label is positively correlated to the unbiased relevance, and reﬁne the pairwise loss lossto lossas follows: where ∆Zis the difference between evaluation metrics based on the biased synthesized label if item xand xare swapped in the ranking list. Thus, if we can estimate the parameters of θ, θ,  and , we can learn an unbiased rankerˆfthrough minimizing the loss function. In this paper, we employ a regression-based Expectation-Maximization (EM) method (Wang et al. 2018a) to estimate the parameters. The estimation process would be described in the next section. Expectation-Maximization (EM) (Moon 1996) is an iterative method to ﬁnd maximum likelihood estimates of parameters. In this paper, we extend the previously proposed regression-based EM method (Wang et al. 2018a) to the pairwise setting to estimate parameters. In the procedure of regression-based EM algorithm, parameters are estimated by iterating over the Expectation steps and Maximization steps until convergence. In this section, we illustrate the process of the Expectation step and the Maximization step respectively. In the Expectation step, we need to estimate the distribution of hidden variable e, eand relevance pair (r, r) given parameters θ, and and γ. To achieve the goal, we ﬁrst calculate the joint probability of all hidden variables shown in Figure 1 , where all formulasfollow directly from Bayes rules. For instance, in the ﬁrst equation, we have: P (e= 1, e= 1, r> r|u, x, x, i, j, c> c) =P (c>c|e=1, e=1, r>r)P (e=1, e=1, r>r)P (c>c|u, x, x, i, j) It is worth noting that we estimate hidden variables e, e and true relevance r> 0 in the pointwise fashion, which is shown in last 4 equations in Figure 1, as they only depend on a single item. After calculating the joint probability, we can then calculate the marginal probability of hidden variables, which will be used in the Maximization step. For instance, the marginal probability P(e= 1|c= 0) equals to the sum of P (e= 1, r> 0|c= 0) and P (e= 1, r≤ 0|c= 0). Maximization Step In the maximization step, all parameters are updated to minimize the loss function, given the training samples and the posterior probabilities from the Expectation step. All formulas for parameter updating are presented in Figure 2. In the procedure of standard EM algorithm, γand β are updated following the 3rd equation and 4th equation respectively, which requires that (u, x, x) should repeat and 2. The detailed derivation of formulas in Figure 1 and Figure 2 is presented in the supplementary material. be shown in different position pairs. As samples of useritem-pair (u, x, x) are highly sparse and synthesized label pairs (c, c) are noisy in recommendation scenario, it is extremely difﬁcult to minimize loss through free parameters γand β. To address the problem, we apply the regression-based EM (Wang et al. 2018a) algorithm to estimate parameters of γand βvia learning a regression function. Speciﬁcally, we assume that there are feature vector Xand Xrepresenting the sample of user-item-pair (u, x, x) and user-item (u, x) respectively, we use function g to compute the relevance preference γ= g(X) and function h to compute absolute relevance β= h(X). Thus, we aim to ﬁnd regression functions g(X) and h(X) in maximization step to minimize loss function, given training samples and the distributions of hidden variables calculated in the Expectation step. For instance, we can regress the feature vector Xto the probability P (r> r|u, x, x, i, j, c, c). Similar to (Wang et al. 2018a), we convert such a regression problem to a classiﬁcation problem based on Bernoulli sampling. Speciﬁcally, we sample a binary relevance pair label γ ∈ {0, 1} and a binary relevance label β ∈ {0, 1} according to P (r> r|u, x, x, i, j, c, c) and P (r> 0|u, x, i, c) respectively. Then we can adopt classiﬁcation models to learn g(X) and h(X) based on training set {X, γ} and {X, β}. It is ﬂexible to use any classiﬁcation model for g(X) and h(X) and we choose the commonly used DNN model in this paper. Note that the original EM algorithm updates parameters in the Maximization step through calculation on the whole data set, which imposes challenges for industrial recommender systems with large-scale training data. To address the problem, we employ the mini-batch EM following the idea of online EM (Capp´e and Moulines 2009). Accordingly, we calculate the formulas in Figure 2 based on data in one single mini-batch, and update parameters incrementally in each batch as follows: where α is the scheduled learning rate of ,cis the estimation of in the current batch. In this section, we conduct ofﬂine experiments on benchmark datasets and online A/B testing in one of the world’s largest content recommendation platforms to evaluate the performance of the proposed method. Experiments on Benchmark Dataset To evaluate the effectiveness of the proposed method, we conduct experiments on two of the largest public LTR benchmark datasets, i.e., Yahoo! LETOR set 1and MSLRWeb30K, which both consists of multiple query document pairs represented by feature vectors and 5-level relevance labels. As there is no user action in the datasets, we consider each query as a user request and generate the click and dwell-time labels to simulate key tasks in recommender systems. We follow the same data split of training, validation, and testing in the datasets and conduct experiments on both binary click labels and continuous synthesized labels to evaluate the performance of our method comprehensively. Data Simulation. In the experiment, we ﬁrst sample clicks following the same procedure as (Ai et al. 2018). Then we sample continuous dwell-time labels based on sampled click data as only clicked samples have non-zero dwell-time in practice, which is formulated as: where cis the sampled click label, δis the position bias of dwell-time at position i and ωis the unbiased dwelltime related to the relevance. As dwell-time is continuous, we assume δand ωfollows the following normal distributions: δ∼ N (,), ω∼ N ( + (1 − )y,), where y ∈ [0, 4] is the relevance label, yis the max relevance of 4,  is the noise due to that irrelevant items may have non-zero dwell-time, which is set to 0.1 here. Experiments on Categorical Labels. We ﬁrst perform experiments on the sampled binary click labels to compare our method with the SOTA unbiased LTR methods including DLA (Ai et al. 2018), Regression EM (Wang et al. 2018a) 3. https://webscope.sandbox.yahoo.com/catalog.php?datatype=c 4. https://www.microsoft.com/en-us/research/project/mslr/ Label and Pairwise Debiasing (Hu et al. 2019). For comprehensive comparison, we train models with the unbiased relevance labels and the biased click labels without debiasing as upper bound and lower bound respectively, and adopt both pointwise learning and pairwise learning for lower bound and upper bound. Moreover, we remove the pairwise trust bias correction and loss reﬁnement for direct metrics optimization from our method to explore their effectiveness. For each method, we adopt a DNN model with [512, 256, 128] hidden units and ELU activation and tune hyper-parameters carefully. We train and evaluate each method 10 times and report the mean NDCG in Table 2. It is shown that our method outperforms all SOTA unbiased LTR methods in categorical labels. Furthermore, the pairwise trust bias correction achieves signiﬁcant improvement on Yahoo Dataset and slightly better results on Web30K. Besides, direct optimization for ranking metrics exhibits better performance on Web30K but similar performance on Yahoo dataset as we can only compute ∆Zbased on the noisy click signal. On the whole, our method achieves the best performance for unbiased LTR. Experiments on Continuous Labels. Then we generate the synthesized label following Equation 6 with summation combination function and conduct experiments to evaluate the performance of our method on the setting similar to the typical recommendation scenarios. The lower bound here is trained with the synthesized labels without debiasing. Note that the SOTA industrial recommender systems train multiple pointwise models to predict multiple objectives respectively and employ PAL (Guo et al. 2019) or sum-based shallow tower (Zhao et al. 2019) for debiasing. Moreover, PAL can only work for categorical labels due to its decomposition. So we consider the following baseline methods: • Baseline V1: We train a ranker with the synthesized label and adopt the sum-based shallow tower for debiasing. • Baseline V2: We train the click and dwell-time task respectively and adopt sum-based shallow tower for debiasing on the click task. • Baseline V3: We train the click and dwell-time task respectively and adopt PAL for debiasing on the click task. • Baseline V4: We train the click and dwell-time task respectively and adopt PAL on the click task and sumbased shallow tower on the dwell-time task for debiasing. For each task, we adopt a DNN model with [512, 256, 128] hidden units, ELU activation, and cross-entropy loss for classiﬁcation task and MSE loss for regression task. For Baseline V2, V3 and V4, we sum the predicted click ratio and dwell-time for ranking. According to the mean NDCG over 10 runs shown in Table 2, our method outperforms all baseline methods signiﬁcantly in both datasets. One can see that pairwise learning performs worse than pointwise learning with the biased synthesized label while performs much better on relevance label, which indicates that debiasing is more important for pairwise learning in recommendation scenarios. Moreover, employing shallow tower on dwell-time harms the performance signiﬁcantly, which shows that shallow tower cannot work well for continuous labels. In contrast, our probabilistic graphical based method has a clear separation of bias and relevance and achieves superior performance on debiasing and preference learning for different types of labels and applications. Online A/B Testing We have deployed our method in one of the world’s content recommendation platforms and conducted online A/B testing for 7 days. The live metric and synthesized label are deﬁned as the combination of multiple user actions (e.g. click, dwell-time, liking, and sharing). Comparing with the online baseline method similar to Baseline V3 which models multiple user actions respectively and employs PAL on the click task for debiasing, the proposed method improves the live metric by 2.08%, which demonstrates the effectiveness of our method in debiasing and preference learning in real-world applications. In this paper, we propose a novel unbiased pairwise LTR method to model position-based examination bias and trust bias in the pairwise fashion for better bias correction and preference learning, which is the ﬁrst unbiased LTR method working for both categorical and continuous labels. Ofﬂine experiment results on public benchmark datasets and online A/B testing show signiﬁcant and consistent improvements of the proposed method over SOTA ranking models in realworld recommender systems and SOTA unbiased LTR methods commonly used in search engines. Correcting for other types of bias besides position bias will be the focus of future work.