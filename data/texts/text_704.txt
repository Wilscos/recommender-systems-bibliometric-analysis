{liweibin02, hemingkai, huangzhengjie, wangxianming, fengshikun01, wuzhihua02, suweiyue, sunyu02}@baidu.com In recent years, owing to the outstanding performance in graph representation learning, graph neural network (GNN) techniques have gained considerable interests in many real-world scenarios, such as recommender systems and social networks. In recommender systems, the main challenge is to learn the effective user/item representations from their interactions. However, many recent publications using GNNs for recommender systems cannot be directly compared, due to their difference on datasets and evaluation metrics. Furthermore, many of them only provide a demo to conduct experiments on small datasets, which is far away to be applied in real-world recommender systems. To address this problem, we introduce Graph4Rec, a universal toolkit that uniﬁes the paradigm to train GNN models into the following parts: graphs input, random walk generation, ego graphs generation, pairs generation and GNNs selection. From this training pipeline, one can easily establish his own GNN model with a few conﬁgurations. Besides, we develop a large-scale graph engine and a parameter server to support distributed GNN training. We conduct a systematic and comprehensive experiment to compare the performance of different GNN models on several scenarios in different scale. Extensive experiments are demonstrated to identify the key components of GNNs. We also try to ﬁgure out how the sparse and dense parameters affect the performance of GNNs. Finally, we investigate methods including negative sampling, ego graph construction order, and warm start strategy to ﬁnd a more effective and efﬁcient GNNs practice on recommender systems. Our toolkit is based on PGLand the code is opened source in https://github.com/PaddlePaddle/PGL/tree/main/apps/Graph4Rec. Learning representation from a user-item graph is ubiquitous in modern deep learning-based Recommender Systems (RS), such as news recommendations and product suggestions on e-commerce websites. In these applications, the interactions between users and items construct a large-scale heterogeneous graph. The representation of nodes learned from the graph structure, i.e., the low dimension vectors of users and items can be applied to downstream applications. For example, user-based collaborative ﬁltering methods can https://github.com/PaddlePaddle/PGL predict the likely-to-interact items for a user based on ratings given to that item by the other users with similar tastes, which are measured by the similarity of user embeddings. While for item-based collaborative ﬁltering methods, item embeddings are leveraged for similar products recommendation. Therefore, learning meaningful representations from complex graph structures plays a vital role in the development of recommender systems. Over the past decade, a series of studies have been explored for mining homogeneous or heterogeneous graphs. For example, walk-based algorithms generate node sequence by deﬁned random walk strategies (Perozzi, AlRfou, and Skiena 2014; Dong, Chawla, and Swami 2017; Wang et al. 2018). Then they pull closer the nodes inside the same window and push away those from random sample nodes. Recently, as Graph Neural Networks (GNNs) have shown their capability of modeling complex structural data, many existing works attempt to adopt GNNs for representation learning and recommender systems. GNN models iteratively aggregate neighbors for node representations and utilize the high-order network information for improvements (Kipf and Welling 2016; Veliˇckovi´c et al. 2017; Hamilton, Ying, and Leskovec 2017; Xu et al. 2018). Despite the research on GNNs’ architecture, there are also many studies focusing on developing effective tasks for graph learning such as contrastive learning and self-supervised learning (Qiu et al. 2020; You et al. 2020; Wu et al. 2021). However, many recent works related to GNNs for recommender systems cannot be directly compared, due to their difference in datasets and evaluation metrics. Furthermore, many of them only provide a demo to conduct experiments on small datasets, which is far away from web-scale recommender systems in real world. In recent years, there are several existed graph embedding systems, such as GraphVite (Zhu et al. 2019) and PyTorchBigGraph (Lerer et al. 2019) with PBG for short. GraphVite only performs walk-based models on a single machine with multi-GPUs. Although PBG support distributed training, it cannot deal with heterogeneous GNN models, lacking the capability of modeling complex structural data for recommender systems. Besides, neither of the systems can handle side information of node in a graph to address the cold start problem. In this paper, we introduce Graph4Rec, a universal toolkit that uniﬁes the paradigm to train GNN models into the following parts: graphs input, random walk generation, ego graphs generation, pairs generation and GNNs selection. With a few conﬁgurations, researchers are freely to build their own GNN models. Besides, unlike the traditional walk-based embedding system, the utilization of GNNs makes Graph4Rec more competitive while modeling complex user-item interactions data for recommender systems. The distributed graph engine and parameter server empower Graph4Rec to handle large-scale graph data. Furthermore, we provide systematic and comprehensive experiments to evaluate the performance of various GNNs in different scenarios and attempt to give out some suggestions for developing GNNs in the recommender system. In this section, we introduce the notation of heterogeneous graphs and brieﬂy review some recent research on GNNbased representation learning for recommender system. Generally, the complex interactions between users and items in recommender systems can be simply regarded as a heterogeneous graph denoted as G = (V, E, R), where V denotes the nodes, E represents edges, and R is the edge types. In recommender systems, V can be the set of users and items while R representing different relations between users and items, such as click and purchase. If we only have single type of relation between vertices, the heterogeneous graph will degenerate into homogeneous graph. Recent developments of GNNs show their strong capability for mining graph data (Kipf and Welling 2016; Hamilton, Ying, and Leskovec 2017; Veliˇckovi´c et al. 2017; Xu et al. 2018). The main idea of GNNs is to iteratively update the representation of nodes by aggregating representations of its neighbors. And most of them follow message passing paradigm (Gilmer et al. 2017). After K iterations of aggregation, a node’s representation captures the structural information within its K-hop neighborhood. Formally, the calculation in the k-th layer of a GNN is given as follows: ˆh= AGGREGATEh: u ∈ N where his the representation of node v at the k-th layer. And Nstands for the neighborhood set of v. The AGGREGATE function can be implemented by a number of order-independent operations, such as max, mean, and sum. The COMBINE function is usually a simple neural network layer with feature transformation and non-linear activation. And neighborhood sampling is the most common way to address the exponential growth problem caused by K-hop neighbors (Hamilton, Ying, and Leskovec 2017). The performance of the recommender systems usually depends on the quality of the user and item representation learnt from the interaction data. And the embeddings are often used in measuring the similarity between users and items for user-based or item-based collaborative ﬁltering algorithms. For example, Wang (2018) constructs item-item graph from the user log and learns representation for items. Then it recommends similar items according to user history. Inspired by word2vec (Mikolov et al. 2013), early attempts on graph representations learning are skip-gram based models with pre-deﬁned random walks on graph (Perozzi, AlRfou, and Skiena 2014; Dong, Chawla, and Swami 2017; Wang et al. 2018). Recent approaches leverage high-order information from graph structure with GNNs rather than simple ID embeddings. Although the unsupervised graph representation learning methods are named differently as contrastive learning with augmentation (Qiu et al. 2020; You et al. 2020) or self-supervised learning (Wu et al. 2021), they both follow the same idea that pull closer the positive pairs with similar structure and push away from negatives. The loss function can be formulated as follows: L = − log σ(y) −E[log σ(−y)], (2) where ytakes the inner product hhfrom the ﬁnal node representations, σ(z) =and P (w) is the distribution from which a negative node wis drew from for M times. As shown in Figure 1, the paradigm of GNNs training process for recommender system consists of the following ﬁve components: graphs input, random walk generation, ego graphs generation, pairs generation, and GNNs selection. Multiple conﬁguration settings in each component are provided to better control the graph embedding system ﬂexibly. In the following sections, we describe the framework and the implementation of each component in Graph4Rec in detail. In Graph4Rec, the basic data structure is the heterogeneous graph, in which nodes and edges have multiple types. A heterogeneous graph can be decomposed into several bipartitedirected graphs in which a triple (u, r, v) is denoted as the source node, relation, and destination node. This general design of graph inputs helps us easily handle complex graph data. For example, in recommender systems, there are two types of nodes, i.e., user node and item node, represented by “u” and “i”. And the “click” interaction between “u” and “i” can be denoted as “u2click2i”. For convenience, we use “2” as a delimiter to split the string into a triple, and the reverse relation of “i2click2u” will be automatically added when symmetry is true. If we only have a single type of nodes and edges, the heterogeneous graph will degenerate into a homogeneous graph, in which we can set the node type to “u” and the edge type to “u2u”. Figure 1: Graph4Rec uniﬁes the paradigm to train GNN models into the following parts: graphs input, random walk generation, ego graphs generation, pairs generation and GNNs selection. Distributed Graph Engine Meanwhile, Graph4Rec designs a distributed graph engine to deal with large-scale heterogeneous graph data. Nodes are partitioned uniformly into several machines. And the adjacency list of each node is stored in the corresponding server. To lower the communication cost between processes and machines, we optimize the data generation pipeline which will be discussed later. 3.2 Random Walk Generation Deﬁning proximity using nodes within the same random path is the most essential method for graph representation learning (Perozzi, Al-Rfou, and Skiena 2014; Qiu et al. 2020). As for heterogeneous graph mining, meta path random walk (Dong, Chawla, and Swami 2017) is adopted as a basic operation in Graph4Rec with its completeness satisfying most situations. Metapath can be simply deﬁned as shown in Figure 1 that a sequence of edge types assembled head-to-tail with a hyphen. Inspired by the metapath2vec model, we develop a multi-metapaths random walk strategy that can sample multiple meta-paths from the heterogeneous graph. For example, given a heterogeneous graph described in Section 3.1, we can specify the two metapaths, “u2click2i - i2click2u” and “u2buy2i - i2buy2u”, to generate different types of paths to learn more structure information from the graph. As for the homogeneous graph, we can set the metapath to “u2u - u2u”, which is equal to a random walk. 3.3 Ego Graphs Generation For every node in the training samples, neighborhood sampling is required to reduce the computation cost for the later multi-hop neighbor aggregation in GNNs. In this work, we use an ego graph to represent a training sample of a central node. The deﬁnition of ego graph is that its node is composed of a central node and its neighbors. For a node v, its neighbors of type r are deﬁned as S= {u : d(u, r, v) ≤ K}, where d(u, r, v) is the shortest path distance between u and v of type r. Thus, an ego graph of node v in type r, denoted as G, is the subgraph induced by S. Since there are multiple edge types, we then develop a relation-wise neighbor sampling method to allow relationwise aggregation as described in Section 3.5. Formally, a relation-wise ego graph is denoted as G, where G= {G: r ∈ R}. Therefore, each node inside the path received from random walk generation becomes a central node. And thus nodes in the same paths batch will form their disjoint ego graphs with relation-wise neighborhood sampling, as shown in Figure 1. Besides, ego graphs generation can be skipped if one only wants to train a walk-based model. Pairs inside a random path are usually used to deﬁne proximity for contrastive learning or self-supervised learning on graphs (Qiu et al. 2020; Wu et al. 2021). Besides, items or users in the same path like “u-i-u-i” can be a potential recommendation result. Because the path implies that the users have the same behaviors, and the items have the same user groups. In this component, as shown in 1, win size is used for the user to control the deﬁnition of proximity in a meta path. Then we generate ego graph pairs as positive training samples for the next procedure. After getting pairs of training samples, GNNs are then selected for ego graph encoding. For all the heterogeneous settings, we follow the idea from R-GCN (Schlichtkrull et al. 2018) and provide neighborhood aggregation with distinct weights for each individual relation type. The node representations are calculated via h= GNN(h, {h: u ∈ N}) GNNcan be any graph neural network model that follows the message passing paradigm deﬁned by Eq. 1. For convenience, Graph4Rec has embraced various classical GNNs, such as GCN, GAT, LightGCN, etc. Users can also develop their substitution models. Unlike homogeneous graph with single relation aggregation result, φdenotes the weight for relation-wise aggregation. The simplest way is to keep it as a constant uniform with φ= 1 between relations. We also provide a learnable relation aggregation setting like GATNE (Cen et al. 2019), which adopt a shallow network to provide attention score for each relation calculated by φ= softmax(wtanh(Wh)). Over-smoothing (Li et al. 2019; Chen et al. 2020) is the key drawback of vanilla GNNs. α is the control of the residual connection from the bottom features hto the output to tackle this problem. It can also be regarded as feature propagation with Personal PageRank (Klicpera, Bojchevski, and G¨unnemann 2018), which is a simple but effective strategy. To address the cold start issue, side information such as category, brand, or user proﬁle can be utilized as a basic feature and combined with ID embedding. In Graph4Rec, we support conﬁgurable sparse features with multiple slots. And each slot can have multiple values to support features with a variable length such as text or tags. In the rest of our paper, although we adopt the model’s name in their original paper, the relation-wise operation and tricks for handling over-smoothing problems are applied to all the models for a fair comparison. Parameter Server. With the increasing scale of graph data, the capacity of sparse learnable parameters (IDs or side information) also grow rapidly, limiting the GNN models to be applied in large-scale recommender systems. In this work, we adopt a parameter server, containing a key-value store of embeddings based on PaddlePaddle. At each training step, the embeddings are pulled remotely from the parameter server. Then the calculated gradients are pushed to the server for an asynchronous update. The lazy initialization occurs when the embedding is required for the ﬁrst time, which is memory efﬁcient. Empowered by distributed graph engine and parameter server, Graph4Rec can easily train a GNN model with large-scale graph data for recommender systems. Walk, Sample, Pair: Order Matters. The order of the training sample generation is important to the speed of training GNNs. The most intuitive generation order is that we ﬁrst sample a path, and then for a current vertice inside the path, another vertice is selected within the same window to construct pair. Then we sample the ego graph for each pair to feed into GNN models. However, this kind of method will produce many repetitive nodes and each of them must sample an ego graph, which leads to increased communication for graph engine. To alleviate this problem, we exchange the order of pair generation and ego graph sampling. That is, after generating a walk path, we ﬁrst perform ego graph sampling for each vertice within the path and then construct training pair for GNN models. Formally, suppose we have https://github.com/PaddlePaddle/Paddle a sampled path of length L and window size is w. Then we will have O(wL) ego graph sampling operations when generating pairs before sampling ego graphs. But if we exchange the order, the sampling times can reduce to O(L). Since the diversity of training samples has been reduced, in the later section we will conduct experiment to show the trade-off between speed and performance. In-batch Negative Sampling. The main idea of our framework is to learn the representation of each node, which can pull similar points (positive pairs) together while pushing away dissimilar points (negative pairs). Here, the positive pairs can be the observed interactions while the negative pairs are randomly selected from V. However, random selection of negative samples is time consuming, especially in distributed training mode where nodes and their side information are saved in different machines. Therefore, we implement an improved version that uses in-batch negative sampling. We maximize the scores for linked nodes while minimizing the scores of other nodes in a batch. This method can reduce additional data input, thereby increasing the training speed. Pre-training and Parameters Warm Start. The traditional walk-based models are fast and effective. We ﬁrst pretrain the sparse embeddings from walk-based models. Then we train a GNN-based model and inherit the parameters for fast convergence and performance improvement. In this section, we report the settings and results of our experiments on four public datasets, and study the following research questions: • RQ1: Does our proposed Graph4Rec outperform other existing graph learning systems in both speed and performance? • RQ2: What is the best practice performance of current graph representation learning? • RQ3: How does the modeling of side information affect the performance of GNN models? • RQ4: What is the impact of in-batch negative sampling? • RQ5: How does the order of sampling ego graph inﬂuence the training speed? • RQ6: What is the relationship of convergence between walk-based models and GNN-based models? 4.1 Datasets We conduct experiments on four publicly available heterogeneous datasets, i.e., RetailRocket, Rec15, Tmalland UB. RetailRocket includes more than 4 months of examining, adding-to-carts, and purchasing records in an ecommerce platform. Rec15 is a competition dataset published in RecSys Challenge 2015. Tmall is another dataset https://www.kaggle.com/retailrocket/ecommerce-dataset https://recsys.acm.org/recsys15/challenge/ https://tianchi.aliyun.com/dataset/dataDetail?dataId=42 https://tianchi.aliyun.com/dataset/dataDetail?dataId=649 Table 1: Statistics of the processed datasets used in our experiments. released by IJCAI Competition 2015, which contains four common behaviors in e-commerce websites, including click, purchase, ad-to-cart and add-to-favorite. UB is a user behavior dataset collected by real e-commerce websites. For all datasets, we remove users in which the number of interactions is smaller than 10 (5 for RetailRocket). For the RetailRocket dataset, we drop the later duplicated (user, item, behavior) triple and discard items that are interacted fewer than 5 times. We sort the historical interactions of each user in the order of timestamp, and select the ﬁrst 80% of each sequence as the training set, the next 10% as the valid set, and the rest as the test set. The statistics of the four processed datasets is shown in Table 1. We will release the preprocessed dataset together with our toolkit. 4.2 Recall Strategies and Evaluation Metrics In our experiment, there are three common recall strategies i.e., user-based collaborative ﬁltering (UCF), item-based collaborative ﬁltering (ICF) and U2I that recall items for each user. Speciﬁcally, the ICF strategy recalls the most similar top-N (N=20 for our experiment) items for each interacted item i of user u and recommends the top-K items that appear most frequently in the recalled item set. UCF strategy recalls the most similar top-N (N=20 for our experiment) users ufor each user u, and recommends the top-K frequent items by aggregating the interacted items of u. The U2I strategy directly uses user embedding to retrieve item embedding, and recommend the most similar top-K item. We evaluate the top-K recommendation performance via a common metric, i.e., recall, which measures how many items in our recommendation list actually hit the user’s interest. We use ICF@K, UCF@K and U2I@K to represent the recall indicator of a top-K recommendation list obtained by ICF strategy, UCF strategy and U2I strategy, respectively. Table 2: Features supported by each systems. In order to study whether our Graph4Rec is competitive to the other existing graph learning systems, we select the below systems as baselines for comparison, and report the best performance of the algorithms supported by each system on the four datasets. • PyTorch-BigGraph (PBG) (Lerer et al. 2019): A distributed system for learning graph embeddings for large graphs, particularly big web interaction graphs with up to billions of entities and trillions of edges. • GraphVite (Zhu et al. 2019): A general graph embedding engine, dedicated to high-speed and large-scale embedding learning in various applications. A detailed comparison of the above systems is shown in Table 2. GraphVite only performs walk-based models on a single machine. PBG supports training both on single and distributed environment, but it is particularly designed to learn multi-relation embedding for knowledge graphs and neither of them can handle heterogeneous graphs and side information. In contrast, our Graph4Rec has more comprehensive functions, which can process more complex structure data and models in recommender systems. 4.4 Experimental Results Performance on Different Systems (RQ1). We ﬁrst evaluate our Graph4Rec on the four public recommender datasets compared with other three systems. Since the three existing systems do not support GNN-based models, we then use the DeepWalk (Perozzi, Al-Rfou, and Skiena 2014) model (DistMult (Yang et al. 2015) for PBG) for comparison. For each model, we train it with 2 billion pairs (edges) and report the results. As shown in Table 3, the performance of DeepWalk model implemented by our Graph4Rec is competitive with others, verifying the capability of our Graph4Rec. Moreover, we also present the performance of LightGCN (GNN-based model) (He et al. 2020) and ﬁnd that it can achieve a better performance. Figure 2: The runtime on Rec15 with 2 billion pairs. We further report the training speed of each existing systems on Rec15 dataset as shown in Figure 2. For fair comparison, we train the model on each system with 2 billion pairs (edges) on a 40-core CPU machine. Since PBG do not contain the DeepWalk model, we use the DistMult (Yang et al. 2015) model instead. Figure 2 shows that, compared to GraphVite and PBG, our Graph4Rec can run faster, and even is 2× faster than GraphVite on a single machine. One Table 3: The performance of different models implemented with our Graph4Rec and other existing systems on four datasets. reason for this is that we have done some optimizations for graph sampling, as decribed in Section 3.6. Moreover, our Graph4Rec can linearly speed up the training process with distributed setting using multiple machines. Performance in Graph4Rec (RQ2). We also conduct a series of experiments to ﬁnd out what is the best practice performance of current graph representation learning for recommender systems. As shown in Table 4, metapath2vec, i.e., heterogeneous random walk, achieves better result than DeepWalk, i.e., homogeneous random walk, which means that heterogeneous graph structure plays a signiﬁcant role in representation learning. To further verify the ability of different GNN models in our Graph4Rec toolkit, we implement some GNN models including GraphSAGE, LightGCN, GAT, GIN, NGCF and GATNE. In general, LightGCN can outperform other GNNbased models as shown in Table 4. The results of this experiments verify the description of the paper of LightGCN (He et al. 2020), that is, in large-scale graph representation learning, linear transformation has no positive effect on the effectiveness of collaborative ﬁltering. In addition, we also ﬁnd that using the embedding trained by walk-based models to warm start GNN models can obtain a better performance in less training time compared with pure GNN models. Figure 3 shows that, after warming up from metapath2vec model, most GNN models can reach a better performance. The GNN-based model can aggregate more interaction information from neighbors at one time and generate more informative node representations. With the help of the warm up strategy, the nodes in the graph already have certain structural information, so that the GNN-based model can obtain better aggregated features from the beginning, which helps the training of the GNN model. GNNs with Side Information (RQ3). Side information is some discrete features (bag-of features). It can be regarded as the node feature so that when meeting a new node Table 5: The inﬂuence of side information on various GNN models on Tmall dataset. (user), one can use the node feature embedding to represent it. To train GNN models with side information, we directly sum the features embeddings with the node ID embeddings. From the Table 5, we demonstrate that both walk-based and GNN-based models can beneﬁt from adding side information. This means that the side information is signiﬁcant to improve the ability of node embeddings. From Table 5, We discover an interesting phenomenon. When only node ID embeddings are used, LightGCN can outperform other GNN models similar to the ﬁnding that transformation and non-linear has negative effects on GNNs (He et al. 2020). When appending side information with node ID embeddings, most of the models can take advantage of it. Moreover, GATNE can surpass the effect of LightGCN with additional features. In-batch Negative Sampling (RQ4). Typically, distributed training can easily fall into a communication bottleneck, which is more likely to be occurred in graph represen- Rec15 Table 6: The training speed and performance of metapath2vec model between random and in-batch negative sampling. We recall the most similar top-100 items for Rec15 and top-1k items for Tmall. Table 7: The training speed and performance of LightGCN model. We recall the most similar top-1k items on Tmall dataset. tation learning due to the frequent graph sampling. Therefore, we make some optimizations to the training speed, including in-batch negative sampling and ego-graph construction order exchange. Table 6 shows the impact of in-batch negative sampling. We set batch size to 1000 and the number of negative samples to 5 on this experiment. For the Rec15 dataset, we train the metapath2vec model with 2 billion edges. While for Tmall dataset, we use 3 billion edges. The results show that the training speed of in-batch negative sampling strategy is near 4× faster than the random negative sampling strategy, while it still maintains the competitive performance. Order of Ego Graphs Generation and Pairs Generation (RQ5). As described in Section 3.6, to speed up generating training pairs, we exchange the order of pairs generation and ego graphs generation, which might change the diversity of training samples with fewer neighborhood sampling operations. We set the window size w = 2 and walk length L = 4. Table 7 shows the inﬂuence of the order on LightGCN for Tmall dataset. We run 150 million pairs for each task and ﬁnd that ego graph sampling ﬁrst can nearly double the speed while the performance on three metrics only declines slightly. Thereby, it is worthwhile to do ego graph sampling before generating pairs from the walk path. Figure 4: The comparison of training speed and convergence with respect to metapath2vec and LightGCN model on Tmall dataset. Walk-based vs. GNN-based (RQ6). In our experiments, we ﬁnd that the training speed of GNN-based models is much slower than walk-based models. The reason for this is that when training GNN-based models, we need to process ego graph sampling and neighbor aggregation which slows down the mini-batch data generation. Therefore, we conduct a experiment to see the performance of walk-based models and GNN-based models under the same training time. As shown in Figure 4, the amount of training samples in metapath2vec model is nearly 10× larger than in LightGCN model. However, the performance of LightGCN is still better than metapath2vec model. One possible reason is that GNNbased models can aggregate multiple neighbors at one time, making the convergence faster. In this paper, we introduce Graph4Rec, a universal toolkit with graph neural networks for recommender systems, that uniﬁes the paradigm to train GNN models into ﬁve components. There are three highlights in our Graph4Rec, namely large-scale, abundance and easy-to-use. Extensive experimental results show that our Graph4Rec is competitive with other existing systems. And we also demonstrate some useful experiments which may guide our practice in industrial recommender systems.