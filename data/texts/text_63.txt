The paradigm and recommended practices described here stem from the National Security Commission on Artificial Intelligence’s (NSCAI) “line of effort” on Ethics and Responsible Artificial Intelligence (AI). The development of the content was supported by input and feedback drawn from consultations with representatives from academia, civil society, industry, and federal agencies. These recommended considerations and practices provide the foundation for concepts on the responsible development and fielding of AI technologies appearing throughout the NSCAI report, including the Commission’s recommendations. The NSCAI report was delivered to Congress and the Executive Branch on March 1, 2021. The concepts in this document were instrumental in developing the content appearing in Chapters 7 on “Establishing Justified Confidence in AI Systems,” and Chapter 8 on “Upholding Democratic Values: Privacy, Civil Liberties, and Civil Rights in Uses of AI for National Security.”  An earlier version of this document was published by the NSCAI as an appendix in the “Second Quarter Recommendations” of the Commission in July 2020. An updated version is included as an appendix in the final NSCAI report, entitled, “Key Considerations for the Responsible Development and Fielding of Artificial Intelligence.” The NSCAI final report was approved by all 15 NSCAI Commissioners (Safra Catz, Dr. Steve Chien, Hon. Mignon Clyburn, Chris Darby, Dr. Ken Ford, Dr. José-Marie Griffiths, Andy Jassy, Gilman Louie, Dr. William Mark, Dr. Jason Matheny, Hon. Katharina McFarland, and Dr. Andrew Moore, Dr. Eric Schmidt, Hon. Robert O. Work, in addition to Dr. Eric Horvitz, who chaired the line of effort on ethics and responsible AI). Commissioners Horvitz, Matheny, Clyburn, and Griffiths served on the NSCAI line of effort on Ethics and Responsible AI.  Concerns about the responsible development and fielding of AI technologies span a range of issues. Discussions and debates are ongoing as the technology and its applications rapidly evolve, and the need for norms and best practices becomes more apparent.   Efforts have been undertaken to date to establish ethics guidelines for AI by entities in government, in the private sector, and around the world. Principles on Artificial Intelligence. critical step of adopting a set of high-level principles to guide its development and use of AI, the Office of the Director of National Intelligence’s (ODNI) adoption of AI principles for the Intelligence Community (IC). Government. level concepts into concrete actions. There is often a gap between articulating high-level goals around responsible AI and operationalizing them.  The ideas in this manuscript are based on an assessment of current challenges for responsibly developing and fielding AI systems, and the practices and future directions needed to overcome these challenges. We focus our discussion of these challenges and recommendations within the context of five categories: Aligning AI Systems with Democratic Values and the Rule of Law; Engineering Practices; System Performance; Human-AI Interaction and Teaming; and Accountability and Governance. By assessing key   In 2020, Executive Order 13960 further established Principles for Use of AI in  However, even in cases where principles are offered, it can be difficult to translate the highconsiderations for responsible AI adoption within the context of uses by national security departments and agencies, we explore considerations that are especially imperative in high-stakes deployments, including those of safety-critical systems and those with acute implications for the preservation of life and liberties.  Section I. of this document provides guidance specific to implementing systems that abide by democratic values and the rule of law. The section covers aligning the run-time behavior of systems to the related, more technical encodings of objectives, utilities, and tradeoffs. The four following sections (on II. Engineering Practices, III. System Performance, IV. Human-AI Interaction, and V. Accountability & Governance) serve in support of core democratic values and outline practices needed to develop and field systems that are trustworthy, understandable, reliable, and robust. Recommended practices span multiple phases of the AI lifecycle, from conception and early design, through development and testing, and maintenance and technical refresh. “Development” refers to ‘designing, building, and testing during development and prior to deployment’ and “fielding” to refer to ‘deployment, monitoring, and sustainment.’  Though best practices will evolve, these recommended practices establish a baseline for the responsible development and fielding of AI technologies. They provide a floor, rather than a ceiling, for the responsible development and fielding of AI technologies.    Within each of the five sections of this manuscript, we first provide a conceptual overview of the scope and importance of the topic. We then illustrate an example of a current challenge relevant to national security departments that underscores the need to adopt recommended practices in this area. Then, we provide a list of recommended practices that should be adopted, acknowledging research, industry tools, and exemplary models within government that could support adoption of recommended practices. Finally, in areas where recommended practices do not exist or they are especially challenging to implement, we note the need for future work as a priority; this includes, for example, R&D and standards development. We also identify potential areas in which collaboration with allies and partners would be beneficial for interoperability and trust, and we note that the Key Considerations can inform potential future efforts to discuss military uses of AI with strategic competitors.  Our values guide our decisions and our assessment of their outcomes. Our values shape our policies, our sensitivities, and how we balance tradeoffs among competing interests. Our values, and our commitment to upholding them, are reflected in the U.S. Constitution, and our laws, regulations, programs, and processes.  One of the seven principles we set forth in our 2019 Interim Report is the following:  Values established in the U.S. Constitution, and further operationalized in legislation, include freedoms of speech and assembly, the rights to due process, inclusion, fairness, non-discrimination (including equal protection), and privacy (including protection from unwarranted government interference in one’s private affairs). Beyond the values codified in the U.S. Constitution and the U.S. Code, our values also are expressed via international treaties that the United States has ratified that affirm our commitments to human rights and human dignity, including the International Convention of Civil and Political Rights. America’s national security departments, our commitment to protecting and upholding privacy and civil liberties is further embedded in the policies and programs of the IC, (DHS), the Department of Defense (DoD), values that U.S. citizens would identify as core principles of the United States. However, the paradigm of considerations and recommended practices for AI that we introduce resonate with these highlighted values as they have been acknowledged and elevated as critical by the U.S. government and national security departments and agencies. Further, many of these values are common to America’s like-minded partners who share a commitment to democracy, human dignity, and human rights.  In the military context, core values such as distinction and proportionality are embodied in the nation’s commitment to, and the DoD’s policies to uphold, the Uniform Code of Military Justice and the Law of Armed Conflict (LOAC). Against Torture and Other Cruel, Inhuman or Degrading Treatment or Punishment; Engagement; U.S. values demand that the development and use of AI respect these foundational values, and that they enable human empowerment as well as accountability. They require that the operation of AI systems and components be compliant with our laws and international legal commitments, and with departmental policies. In short, core democratic values must inform the way we develop and field AI systems, and the way our AI systems behave in the world.  To date, AI Principles adopted and endorsed by the U.S. Executive Branch, including by national security department and agencies, have focused on aligning AI with many of the values discussed in this section, including fairness and non-discrimination, DoD Principles as one example, fairness is evoked by the “Equitable” principle that the Department will “take deliberate steps to minimize unintended bias in AI capabilities.” “Responsible” principle that “DoD personnel will exercise appropriate levels of judgment and care while remaining responsible for the development, deployment and use of AI capabilities.” establishing principles reiterates the importance of developing and deploying AI systems in accordance with these values.   and DoD Directive 3000.09.  With respect to the U.S. Government, machine learning techniques can assist DoD agencies with conducting large scale data analyses to support and enhance decision-making about personnel. As an example, the Joint Artificial Intelligence Center (JAIC) Warfighter Health Mission Initiative Integrated Disability Evaluation System model seeks to leverage data analyses to identify service members on the verge of ineligibility due to concerns with their readiness. evaluations, including analyzing various factors that lead to success or failure in promotion. Caution and proven practices are needed, however, to avoid pitfalls in fairness and inclusiveness, several of which have been highlighted in high-profile challenges in such areas as criminal justice, and face recognition. harmful disparate impact. must be carefully considered to avoid inadvertently reinforcing existing biases through ML-assisted decisions.  (3) Recommended Practices the rule of law.  1. Employ technologies and operational policies that align with privacy preservation, fairness, inclusion, human rights, and law of armed conflict. Technologies and policies throughout the AI lifecycle should support achieving the goals that AI systems and uses are consistent with these values—and should mitigate the risk that AI system uses/outcomes will violate these values.  ● An explicit analysis of outcomes that would violate these values should be performed.  ● While not an exhaustive list, we offer the following examples based upon core values fielding systems that align with key values through employing technologies, engineering efforts, and operational policies. Another important practice for aligning AI systems with values is to consider values as (1) embodied in choices about engineering tradeoffs and (2) as explicitly represented in the goals and utility functions of an AI system.  On (1), multiple tradeoffs may be encountered with the engineering of an AI system. With AI, tradeoffs need to be made based on what is most valued (and the benefits and risks to those values) including for high-stakes, high-risk pattern recognition, recommendation, and decision making under uncertainty. Decisions about tradeoffs for AI systems must be made about internal representations, policies of usage and controls, run-time execution monitoring, and thresholds. These include a number of well-known, inescapable engineering tradeoffs when it comes to building and using machine-learning to develop models for prediction, classification, and perception. For example, systems that perform recognition or prediction tasks can be set to work at different operating thresholds or settings (along a well characterized curve) where different settings change the tradeoff between precision and recall or the rates of true positives and false positives. By changing the settings, the ratio of true positives to false positives is changed. Often, one can raise the rate of true positives but will also raise the false negatives. In high-stakes applications, different kinds of inaccuracies (e.g., missing a recognition and falsely recognizing) are associated with different outcomes and costs. For example, in a medical recommendation system, a false negative will lead to a missed or delayed treatment of an illness, while a false positive will lead to a potentially costly, but unnecessary and dangerous treatment for an illness that is not present. An engineer or policy maker can change the likelihood of each of these failures by shifting the threshold for an inferred probability of illness at which a recommendation for treatment is made. Further, investing greater resources in data and modeling will shift failure rates, and thus, frame additional questions about values around the engineering effort invested in systems employed in high-stakes settings. Thus, decisions about thresholds and about engineering investments, and understanding the influences of these decisions on the behavior of a system entail making value judgments. As with all engineering tradeoffs, making choices about tradeoffs explicitly and deliberately provides more transparency, accountability, and confidence in the process than making decisions implicitly and ad hoc as they arise.  On (2), systems may be guided by optimization processes that seek to maximize an objective function. Such objectives can represent the desirability or the pursuit of a combination of independent goals. Various technical approaches (e.g., use of multi-attribute utility functions) may be employed to guide a system’s actions based on an objective that is constructed by weighing several individual factors. In some cases, explicit weights are assigned to capture the asserted importance of each of the different factors. Sets of weightings on factors, and the inclusion versus exclusion of specific factors, can be viewed as embedding different values into a system. Here too, tradeoffs are made either explicitly or implicitly when setting different weights (of importance) to different objectives. For example, there may be structural relationships among desired factors, such as the inverse relationship between the speed and safety at which an autonomous vehicle transports people.  Increasing the weighting of one desired factor (speed of travel) may necessarily reduce the weighting on another (safety of travel). As another example, when tuning a model for fairness, optimizing for one metric of fairness can cause a tradeoff in performance across the second metric. As a result, it is important to acknowledge inherent tradeoffs and the need for setting or encoding values or preferences about tradeoffs, which requires someone or some organization to make a call about the trade.  Recommended Practices for Representing Objectives and Tradeoffs tradeoffs with accuracy are handled; this includes selection of operating thresholds that have implications for performance, such as the precision (positive predictive value) and recall (sensitivity) of predictions or the true positive and false positive rates.  objective functions, especially when assigning weightings that capture the importance of different goals for the system.  important to:  ● Be transparent and keep documentation on assertions about the tradeoffs made, optimization justifications, and acceptable thresholds for false positives and false negatives. ● During system development and testing, consider the potential need for context-specific changes in goals or objectives that would require a revision of parameters on settings or weightings on factors. ● Establish explicit controls in specific use cases and have the capability to change or set controls, potentially by context or by policy, per organization. ● Review documentation and run-time execution tradeoffs, potentially on a recurrent basis, by appropriate experts/authorities. ● Acknowledge that performance characteristics are statistics over multiple cases, and that different settings and workloads have different performance. ● Set logical limits based on disallowed outcomes, where needed, to put additional constraints on allowed performance.  Future R&D is needed to advance capabilities for preserving and ensuring that developed or acquired AI systems will act in accordance with democratic values and the rule of law. For instance, there is a need for R&D to assure that the personal privacy of individuals is protected in the acquisition and use of data for AI system development. including disclosure and consent about data collection and use models (including uses of data to build base models that are later retrained and fine-tuned for specific tasks). R&D should also advance development of anonymity techniques and privacy-preserving technologies including homomorphic encryption and differential privacy techniques and identify optimal approaches for specific use cases. Research should focus upon advancing multi-party compute capabilities (to allow collaboration on the pooling of data from multiple organizations without sharing datasets), and developing a better understanding of the compatibility of the promising privacy preserving approaches with regulatory approaches such as the European Union’s General Data Protection Regulation (GDPR), as both areas are important for allied cooperation.  The government, and its partners (including vendors), should adopt recommended practices for creating and maintaining trustworthy and robust AI systems that are auditable (able to be interrogated and yield information at each stage of the AI lifecycle to determine compliance with policy, standards, or regulations methods applicable to AI capabilities, e.g., with transparent and auditable methodologies, data sources,  ); traceable (to understand the technology, development processes, and operational and design procedure and documentation system output There are no broadly directed best practices or standards (e.g., endorsed by the Secretary of Defense or Director of National Intelligence) in place to define how relevant organizations should build AI systems that are consistent with designated AI principles. But efforts in commercial, scientific, research, and policy communities are generating candidate approaches, minimal standards, and engineering proven practices to ensure the responsible design, development, and deployment of AI systems. While AI refers to a constellation of technologies, including logic-based systems, the rise in capabilities in AI systems over the last decade is largely attributable to capabilities provided by data-centric machine learning (ML) methods. New security and robustness challenges are linked to different phases of ML system construction and operations. associated with weaknesses that make the systems brittle and exploitable in specific ways—and vulnerable to failure modalities not seen in traditional software systems. Such failures can rise inadvertently or as the intended results of malicious attacks and manipulation. Attributes of machine learning training procedures and run-times linked to intentional and unintentional failures include: (1) the critical reliance on data for training, (2) the common use of such algorithmic procedures as differentiation and gradient descent to construct and optimize the performance of models, (3) the ability to probe models with multiple tasks or queries, and (4) the possibility of gaining access to information about models and their parameters.  Given the increasing consequences of failure in AI systems as they are integrated into critical uses, the various failure modes of AI systems have received significant attention. The exploration of AI failure modes  ), and reliable (to perform in the intended manner within the intended domain of use).  has been divided into adversarial attacks pursuit of security and robustness of AI systems requires awareness, attention, and proven practices around intentional and unintentional failure modes. Intentional failures are the result of malicious actors explicitly attacking some aspect of (AI) system training or run-time behavior. Researchers and practitioners in the evolving area of Adversarial Machine Learning (AML) have created taxonomies of malicious attacks on machine learning training procedures and runtimes. Attacks span ML training and testing and each has associated defenses. failures introduced by adversaries include training data poisoning attacks, model inversion, and ML supply chain attacks. of sets of potential vulnerabilities and proven practices for detecting attacks and protecting systems is critical. AI developed for this community must remain current with a rapidly developing understanding of the nature of vulnerabilities to attacks as these attacks grow in sophistication. Advances in new attack methods and vectors must be followed with care and recommended practices implemented around technical and process methods for mitigating vulnerabilities and detecting, alerting, and responding to attacks.  Unintentional failures can be introduced at multiple points in the AI development and deployment lifecycle. In addition to faults that can be inadvertently introduced into any software development effort (e.g., requirements ambiguity, coding errors, inadequate TEVV, flaws in tools used to develop and evaluate the system), distinct additional failure modes can be introduced for machine learning systems. Examples of unintentional AI failures (with particular relevance to deep learning and reinforcement learning) include reward hacking, side-effects, distributional shifts, and natural adversarial examples. failure includes the inadequate specification of values per objectives represented in system utility functions   National security uses of AI are likely targets of sustained adversarial efforts; awareness  (as described in Section 1 above on Representing Objectives and Trade-offs), leading to unexpected and costly behaviors and outcomes, akin to outcomes in the fable of the Sorcerer’s Apprentice. classes of unintentional failures can arise as unexpected and potentially costly behaviors generated via the interactions of multiple distinct AI systems that are each developed and tested in isolation. The explicit or inadvertent composition of sets of AI systems within one’s own services, forces, agencies, and between US systems and those of allies, adversaries, and potential adversaries, can lead to complex multi-agent situations with unexpected and poorly-characterized behaviors. To make high-stakes decisions, and often in safety-critical contexts, Departments and agencies such as DoD and the IC must be able to depend on the integrity and security of the data that is used to train some kinds of ML systems. The challenges of doing so have been echoed by the leadership of the DoD and the Intelligence Community, sensor spoofing, and “enchanting attacks” (when the adversary lures a reinforcement learning agent to a designated target state that benefits the adversary). Engineering Recommended Practices Critical engineering practices needed to operationalize AI principles (such as ‘traceable’ and ‘reliable’ are described in the non-exhaustive list below. These practices span design, development, and deployment of AI systems.  1. Refine design and development requirements, informed by the concept of operations and risk  2. Produce documentation of the AI lifecycle: Whether building and fielding an AI system or “infusing  ● Risk assessment. In conducting stakeholder engagement and hazard analysis, it is important to assess risks and tradeoffs with a diverse interdisciplinary group. This includes an analysis of the system’s potential societal impact and the impacts of the system’s failure modes. Prior to developing or acquiring a system, or conducting AI R&D in a novel area, risk assessment questions should be asked relevant to the national security context in critical areas, including questions about privacy and civil liberties, the law of armed conflict, human rights, system security, and the risks of a new technology being leaked, stolen, or weaponized.  3. Leverage infrastructure to support traceability, including auditability and forensics.  ● If ML is used, the data used for training and testing, including clear and consistent annotation of data, the origin of the data (e.g., why, how, and from whom), provenance, intended uses, and any caveats with re-uses; ● The algorithm(s) used to build models, characteristics about the model (e.g, training), and the intended uses of the AI capabilities separately or as part of another system; ● Connections between and dependencies within systems, and associated potential complications; ● The selected testing methodologies and performance indicators and results for models used in the AI component (e.g., confusion matrix and thresholds for true and false positives and true and false negatives area under the curve (AUC) as metrics for performance/error); this includes how tests were done, and the simulated or real-world data used in the tests—including caveats about the assumptions of the training and testing, per type of scenarios, per the data used in testing and training; ● Required maintenance, including re-testing requirements, and technical refresh. This includes requirements for re-testing, retraining, and tuning when a system is used in a different scenario or setting (including details about definitions of scenarios and settings) or if the AI system is capable of online learning or adaptation.  4. For security and robustness, address intentional and unintentional failures. include various “machine learning attacks,” which may take the form of an attack through supply chain, online access, adversarial training data, or model inference attacks, including through Generative Adversarial Networks (GANS). Agencies should seek latest technologies that demonstrate the ability to detect and notify operators of attacks, and also tolerate attacks. evolution of the field of study of intentional and unintentional ML failures, national security organizations must follow and adapt to the latest knowledge about failures and proven practices for monitoring, detection, and engineering and run-time protections. Related efforts and R&D focus on developing and deploying robust AI methods. includes developing and regularly refining threat models to capture and consolidate the characteristics of various attacks in a way that can shape system development to mitigate vulnerabilities. A matrixed focus for developing and refining threat models is valuable.  5. Conduct red teaming for both intentional and unintentional failure modalities. Bring together  DevSecOps should address ML development, deployment, and when ML systems are under attack. architecture that monitors component performance and handles errors when anomalies are detected; build AI components to be self-protecting and self-checking; and include aggressive stress testing under conditions of intended use. Where technically feasible, ensure that high consequence AI systems have overall system architectures that support robust recovery and repair or fail-fast and fail-over to a reliable degraded mode safe system. and persistent attacks on systems and organizations, and defend against such attacks by employing methods that can make systems more resistant to adversarial attacks, work with adversarial testing tools, and deploy teams dedicated to trying to break systems and push them to violate rules for appropriate behavior. conditions they are expected to operate in. When selecting third-party components, consider the impact that a security vulnerability in them could have to the security of the larger system into which they are integrated. Have an accurate inventory of third-party components and a plan to respond when new vulnerabilities are discovered.  ● For documentation: There is an urgency for documentation strategy best practices. needed to ensure sufficient documentation by all national security departments and agencies, including the precisions noted above in this section. In the meantime, national security departments and agencies should pilot documentation approaches across the AI lifecycle to help inform such a strategy.  ● To improve traceability: While recommended practices exist for audit trails, standards have yet to be developed. departments/agencies and the broader AI community (including industry), to develop audit trail requirements per mission needs for high-stakes AI systems including safety-critical applications.  ● Future R&D is needed to advance capabilities for: should consider establishing broader enterprise-wide communities of AI red teaming capabilities that could be applied to multiple AI developments (e.g., at a DoD service or IC element level, or higher).   Future work is needed by standard setting bodies, alongside national security conditions; advance approaches that enable assessment of types and levels of vulnerability and immunity; and to enable systems to withstand or to degrade gracefully when targeted by a deliberate attack. standard methods and metrics for evaluating degrees of auditability, traceability, interpretability, explainability, and reliability. For interpretability in particular, R&D is also  Fielding AI systems in a responsible manner includes establishing confidence that the technology will perform as intended, especially in high-stakes scenarios. assessed, scenarios or with simulations of realistic contexts, world settings—including adversarial attacks on AI components) during development and in deployment. positives and false negatives on a test set representative of the environment in which a system will be deployed, and test sets can be varied in realistic ways to estimate robustness. Testing protocols and requirements are essential for measuring and reporting on system performance, including reliability, during the test phase (pre-deployment) and in operational settings. (The Commission uses industry terminology ‘testing’ to broadly refer to what the DoD calls “Test, Evaluation, Verification, and Validation” (TEVV) This testing includes both what DoD refers to as Developmental Test and Evaluation and Operational Test and Evaluation.). AI systems present new challenges to established testing protocols and requirements as they increase in complexity, particularly for operational testing. However, there are some existing methods to continuously monitor AI system performance. For example, high-fidelity performance traces and means for sensing shifts, such as distributional shifts in targeted scenarios, permit needed to improve our understanding of the efficacy of interpretability tools and possible interfaces.   including assessing its capabilities and blind spots with data representative of real-world   For example, a system’s performance on recognition tasks can be characterized by its false ongoing monitoring to ensure system performance does not stray outside of acceptable parameters; if inadequate performance is detected, they provide insight needed to improve and update systems. System performance characterization also includes assessing robustness. As noted above, this entails determining how resilient the system is in real-world settings where there may be blocking and handling of attacks and where natural real-world variation exists. system performance must also measure compliance with requirements derived from values such as fairness.  When evaluating system performance, it is especially important to take into account holistic, end-to-end system behavior. Emergence is the principle that entities exhibit properties which are meaningful only when attributed to the whole, not to its parts. Emergent system behavior can be viewed as a consequence of the interactions and relationships among system elements rather than the independent behavior of individual elements. It emerges from a combination of the behavior and properties of the system elements and the system’s structure or allowable interactions between the elements, and may be triggered or influenced by a stimulus from the system’s environment. The System Engineering Community and the National Security Community have focused on system of systems engineering for years, challenges for emergent performance. Given the requirement to establish and preserve justified confidence in the performance of AI systems, attention must be paid to the potential for undesired interactions and emergent performance as AI systems are composed. This composition may include pipelines where the output of one system is part of the input for another in a potentially complex and distributed ad hoc pipeline. developing and deploying AI systems at scale notes, “AI components are more difficult to handle as distinct modules than traditional software components—models may be ‘entangled’ in complex ways.” These challenges are pronounced when the entanglement is the result of system composition and integration.  As America’s AI-intensive systems may increasingly be composed (including through ad hoc opportunities to integrate systems) with allied AI-intensive systems, this becomes a topic for coordination with allies as well. Multi-agent systems are being explored and adopted in multiple domains, and teams of autonomous systems. Having justified confidence in AI systems requires assurances that they will perform as intended, including when interacting with humans and other systems. Testing directed at providing these assurances will increasingly encounter challenges when compared with testing of traditional software systems: ● With respect to the U.S. government, although recent extensions to acquisition processes intended to accommodate rapid iterative development for software intensive systems, there are still challenges in integrating the agile development process typical of machine learning into enterprise processes. The data that shapes supervised learning is as important as code, requiring version control and configuration management for volumes of data as well as code for retest and regression testing. Training a model is processor-intensive and time-consuming, a challenge for rapid build and deploy cycles.  ● There is an ongoing need for common infrastructure for developing and testing AI systems, including common frameworks/architectures, common built-in instrumentation for transparency and interpretability in testing and operation, and common testbeds and test ranges.  To minimize performance problems and unanticipated outcomes, testing is essential. Yet, there is a lack of common metrics to assess trustworthiness that AI systems will perform as intended.  Critical practices for ensuring optimal system performance are described in the following non-exhaustive list:  A. Training and Testing: Procedures should cover key aspects of performance and appropriate performance metrics. metrics and reporting are needed to adequately: a. Achieve consistency across testing and test reporting for critical areas. b. Test for blind spots as a specific failure mode of importance to some ML implementations. c. Test for fairness. When testing for fairness, sustained fairness assessments are needed throughout development and deployment, including assessing a system’s accuracy and errors relative to one or more agreed to statistical definitions of fairness and documenting deliberations made on the appropriate fairness metrics to use. Agencies should also conduct outcome and impact analysis to detect when subtle assumptions in  the system concept of operations and requirements are showing up as unexpected and undesired outcomes in the operational environment. d. Articulate system performance. This includes ways to communicate to the end user the meaning/significance of performance metrics, e.g., through a probability assessment, based on sensitivity and specificity. It also requires clear documentation of system performance (across diverse environments or contexts), including information content of model output. hand. For machine learning models, challenges exist when transferring a model to a context/setting that differs from the one for which it was trained and tested. When using classification and prediction technologies, challenges with representativeness of data used in analyses, and fairness/accuracy of inferences and recommendations made with systems leveraging that data when applied in different populations/contexts, should be considered explicitly and documented. As appropriate, robust and reliable methods can be used to enable model generalization and transfer beyond the training context.  Benchmarks should assist in determining if an AI system’s performance meets or exceeds current best performance. benchmark might be the current best performance of a human operator or the composed performance of the human-machine team. Where humans and machines interact, it is important to measure the aggregate performance of the team rather than the AI system alone. demonstrate impressive performance on average but can fail in ways that are unexpected in any specific instance. The performance potential of an AI system is often roughly determined by experiment and test, rather than by any predictive analytics. AI can have blinds spots and unknown fragilities. Focus on tools and techniques to carefully bound assumptions of robustness of the AI component in the larger system architecture, and provide sustained attention to characterizing the actual performance envelope for nominal and off-nominal conditions throughout development and deployment.For systems of particularly high potential consequences of failure, considerable architecture and design work will have been put into making the overall system fail-safe. Special attention must be paid to confirming that error detection and recovery or fail-over mechanisms in the system are effective (e.g., by design reviews, testing that challenges boundary conditions and assumptions, and instrumentation and monitoring). will be combined in various ways in an enterprise to accomplish broader missions beyond the scope of any single system. For example, pipelines of AI systems will exist where the output of one system serves as the input for another AI system. (The output of a track management and classifier system might be input to a target prioritization system which might in turn provide input to a weapon/target pairing tool.) Multiple relatively independent AI systems can be viewed as distinct agents interacting in the environment of the system of systems, and some of these agents will be humans in and on the loop. Industry has encountered and documented problems in building ‘systems of systems’ out of multiple AI systems. A related problem is poor backward compatibility when the performance of one model in a pipeline is  B. Maintenance and deployment. Given the dynamic nature of AI systems, recommended practices for maintenance are also critically important. These include: enhanced and may result in degrading the overall system of system behavior. These problems in composition illustrate emergent performance, as described in the conceptual overview portion of this section. Unexpected failures that transpire in systems of systems may not be the result of any one component failing but may instead be based in the interactions among the composed systems.  A frequent cause of failures in composed systems is the violation of assumptions that were not previously challenged; therefore, a priority during testing should be to challenge (“stress test”) interfaces and usage patterns with boundary conditions and challenges to assumptions about the operational environment and use. This is focused on both unintended violations of assumptions from system composition and also deliberate challenges to the system by adversarial attacks. performance can degrade over time. traces to determine continuously if a system is going outside of acceptable parameters (including operational performance measures and established constraints for fairness and core values), both during pre-deployment and operation. This includes measuring system performance per acceptable parameters in terms of both reliability and values. It also includes assessing statistical results for performance over time, for example, to detect emergent bias or anomalies.As with any instrumentation and storage or monitoring design (e.g., flight data recorders, system logs), design tradeoffs must be made between the potential value of the data captured and monitored and the costs for bandwidth, computing, and storage imposed. provide characteristics on capabilities might not transfer or generalize to specific settings of usage (for example lighting conditions in some applications may be very different for scene interpretation); thus, testing and validation may need to be done recurrently, and at strategic intervention points, but especially for new deployments and classes of task. composed in ways not anticipated by the developers (e.g., opportunistic integration with an ally’s system). These use cases clearly can’t be adequately addressed at development time; some aspects of confidence in the composition must be shifted to monitoring the actual performance of the composed system and its components. For emergent performance  (4) Recommendations for Future Action concerns when AI systems are composed, there are advances in runtime assurance/verification and feature interaction management that can be adapted.  o Testing, Evaluation, Verification, and Validation (TEVV) of AI systems - to develop a o Multi-agent scenario understanding—to advance the understanding of interacting AI as fairness and explainability. Progress on a common understanding of the concepts and requirements is critical for progress in widely used metrics for performance. assess system performance across attributes for responsible AI according to application/context profiles. (Such attributes, for example, include fairness, interpretability, reliability and robustness.) Future work is needed to develop: (1) definitions, taxonomy, and metrics needed to enable agencies to better assess AI performance and vulnerabilities, and; (2) metrics and benchmarks to assess reliability and intelligibility of produced model explanations. In the near term, guidance is needed on: (1) standards for testing intentional and unintentional failure modes; (2) exemplar datasets for benchmarking and evaluation, including robustness testing and red teaming, and; (3) defining characteristics of AI data quality and training environment fidelity (to support adequate performance and governance). o Align on how to test and verify AI system reliability and performance along shared  Responsible AI development and fielding requires striking the right balance of leveraging human and AI reasoning, recommendation, and decision-making processes. Ultimately, all AI systems will have some degree of human-AI interaction as they will all be developed to support humans. In some settings, the best outcomes will be achieved when AI is designed to augment human intellect, or to support human-AI collaboration more generally. In other settings, however, time-criticality and the nature of tasks may make some aspects of human-AI interaction difficult or suboptimal. time decisions because it is more appropriate, valuable, or designated as such by our values, AI should be intentionally designed to effectively augment and support human understanding, decision making, and intellect. Sustained attention must be focused on optimizing the desired human-machine interaction throughout the AI system lifecycle. It is important to think through the use criteria that are most relevant depending on the model. Models are different for human-assisted AI decision-making, AI-assisted human decision-making, pure AI decision-making, and AI-assisted machine decision-making.  There is an opportunity to develop AI systems to complement and augment human understanding, decision making, and capabilities. Decisions about developing and fielding AI systems aimed at specific domains or scenarios should consider the relative strengths of AI capabilities and human intellect across expected distributions of tasks, considering AI system maturity or capability and how people and machines might coordinate.  Designs and methods for human-AI interaction can be employed to enhance human-AI teaming. Methods in support of effective human-AI interaction can help AI systems to understand when and how to engage humans for assistance, when AI systems should take initiative to assist human operators, and, more generally, how to support the creation of effective human-AI teams. In engaging with end users, it may be important for AI systems to infer and share with end users well-calibrated levels of confidence about their inferences, so as to provide human operators with an ability to weigh the importance of machine output or pause to consider details behind a recommendation more carefully. Methods, representations, and machinery can be employed to provide insight about AI inferences, including the use of interpretable machine learning. at reasoning about human strengths and weaknesses, such as recognizing and responding to the potential for costly human biases of judgment and decision making in specific settings. centers on mechanisms that consider the ideal mix of initiatives, including when and how to rely on human expertise versus on AI inferences. ability to detect the focus of attention, workload, and interruptability of human operators and consider these inferences in decisions about when and how to engage with the operators. include developing mechanisms for identifying the most relevant information or inferences to provide end users of different skills in different settings. bias, including potential biases that may arise because of the configuration and sequencing of rendered data. For example, IC research information is displayed, and this order can consequently impact or sway intel analyst decisions. Careful design and study can help to identify and mitigate such bias.  Critical practices to ensure optimal human-AI interaction are described in the non-exhaustive list below. These recommended practices span the entire AI lifecycle.  A. Identification of functions of human in design, engineering, and fielding of AI  B. Explicit support of human-AI interaction and collaboration accountability and human judgment, define the tasks of humans and the goals and mission of the human-machine team across the AI lifecycle. This entails noting needs for feedback loops, including opportunities for oversight. specific individuals. Functions will vary for each domain and each project within a domain; they should be periodically revisited as model maturity and human expertise evolve over time.  ● Design methodologies. Develop methodologies that improve understanding of human- ● Design guidelines. AI systems designs should take into consideration the defined tasks functions that provide individuals with task-relevant knowledge and understanding need to take into consideration that key factors in an AI system's inferences and actions can be understood differently by various audiences. These audiences span real-time operators who need to understand inferences and recommendations for decision support, engineers and data scientists involved in developing and debugging systems, and other stakeholders including those involved in oversight. Interpretability and explainability exists in degrees; what’s needed in terms of explainability will depend on who is receiving the explanation, what the context is, and the amount of time available to deliver and process this explanation. In this regard, interpretability intersects with traceability, audit, and documentation practices. system has in the results or behaviors of the system. AI system designs should appropriately convey uncertainty and error bounding. For instance, a user interface should convey system self-assessment of confidence alerts when the operational environment is significantly different from the environment the system was trained for, and indicate internal inconsistencies that call for caution. human computer interaction, system interface, and operational design, should define when and how information or tasks should be handed off from a machine to a human operator and vice versa. Include checks to continually evaluate whether distribution of tasks is working. Special attention should be given to the fact that humans may freeze during an unexpected handoff due to the processing time the brain needs, potential distractions, or the condition during which the handoff occurs. The same may be true with an AI system which may not fully understand the human’s intent during the handoff and may consequently make unexpected actions.  processes must capture details about human-AI interactions to retroactively understand where challenges occurred, and why, in order to improve systems and their use in the future and for redress. Infrastructure and instrumentation can also help assess humans, systems, and environments to gauge the impact of AI at all levels of system maturity; and to measure the effectiveness and performance for hybrid human-AI systems in a mission context. including human operators, decision makers, and procurement officers. Training should help the workforce better interact, collaborate with, and be supported by AI systems, including understanding AI tools. Training also should include experiences with use of systems in realistic situations. Beyond training in the specifics of the system and application, operators of systems with AI components, especially systems that perform classification or pattern recognition, should receive education that includes fundamentals of AI and data science, including coverage of key descriptors of performance, including rates of false negatives and false positives, precision and recall, and sensitivity and specificity.  ● Periodic certification and refresh. In addition to initial programs of training, operators  Future R&D is needed to advance capabilities for: o To progress the ability of AI technologies to perceive and understand the meaning of o To improve human-machine teaming. This should include disciplines and o To advance human-AI and AI-AI teaming. R&D is needed to optimize the ability of  National security departments and agencies must specify who will be held accountable for both specific system outcomes and general system maintenance and auditing, in what way, and for what purpose. Government must address the difficulties in preserving human accountability, including for end users, developers, testers, and the organizations employing AI systems. End users and those ultimately affected by the actions of an AI system should be offered the opportunity to appeal an AI system’s determinations. And, finally, accountability and appellate processes must exist not only for AI decisions, but also for AI system inferences, recommendations, and actions.  Overseeing entities must have the technological capacity to understand what in the AI system caused the contentious outcome. For example, if a soldier uses an AI-enabled weapon and the result violates international law of war standards, an investigating body or military tribunal should be able to re-create what happened through auditing trails and other documentation. Without policies requiring such supported by AI systems. In its First Quarter Recommendations, the Commission provided recommendations for such training. o Workforce training. A complementary best practice for Human-AI Interaction is technology and the enforcement of those policies, proper accountability would be elusive if not impossible. Moreover, auditing trails and documentation will prove critical as courts begin to grapple with whether AI system’s determinations reach the requisite standards to be admitted as evidence. traceability infrastructure to permit auditing (as described in the Engineering Practices section) will increase the costs of building AI systems and take significant work—a necessary investment given our commitment to accountability, discoverability, and legal compliance.  Critical accountability and governance practices are identified in the non-exhaustive list below.  1. Appoint full-time responsible AI leads to join senior leadership. Every department and agency critical to national security and each branch of the armed services, at a minimum, should have a dedicated, full-time responsible AI lead who is part of the senior leadership team. Such leads should oversee the implementation of the Key Considerations recommended practices alongside the department/agency’s respective AI principles. This includes: driving responsible AI training; serving as subject matter experts regarding existing and proposed responsible AI policy and best practices; leading interagency best practice sharing for responsible AI; and shaping procurement policy and guidance for product managers to ensure alignment with recommended practices and adopted AI principles. The department’s responsible AI lead should determine the responsible AI governance structure to ensure centralized and consistent policies are applied across the department, and internally coordinate across the department to ensure synergistic implementation of Responsible AI policies and programs.  2. Identify responsible actors. Determine and document who is accountable for a specific AI system or any given part of an AI system and the processes involved with it. This includes identifying who is responsible for the development or procurement; operation (including the system’s inferences, recommendations, and actions during usage) and maintenance of an AI system; as well as the authorization of a system and enforcement of policies for use. Determine and document the  mechanism/structure for holding such actors accountable and to whom should that mechanism/structure be disclosed to ensure proper oversight.  3. Require technology to strengthen accountability processes and goals. Document the chains of custody and command involved in developing and fielding AI systems. Policy should establish clear requirements about information that should be captured about the development process (via traceability) and about system performance and behavior in operation (run-time monitoring) to support reliability and robustness as well as auditing for oversight; these are further tailored by AI policy leads and engineers. These requirements should be tailored for the specific constraints imposed by policy (e.g., governing data retention) and technical issues (e.g., capacity of bandwidth, compute, and storage). This will allow the government to know who was responsible at which point in time. Improving traceability and auditability capabilities will allow agencies to better track a system’s performance and outcomes. 4. Adopt policies to strengthen accountability and governance. Identify or, if lacking, establish policies that allow individuals to raise concerns about irresponsible AI development/fielding, e.g. via an ombudsman. This requires ensuring a governance structure is in place to address grievances and harms if systems fail, which supports feedback loops and oversight to ensure that systems operate as they should. Agencies should institute specific oversight and enforcement practices, including: auditing and reporting requirements; a mechanism that would allow thorough review of the most sensitive/high-risk AI systems to ensure auditability and compliance with responsible use and fielding requirements; an appealable process for those found at fault of developing or using AI irresponsibly; and grievance processes for those affected by the actions of AI systems. Agencies should leverage best practices from academia and industry for conducting internal audits and assessments, also acknowledging the benefits offered by external audits. 5. Support external oversight. Remain responsive and facilitate Congressional oversight through documentation processes and other policy decisions. specifically documentation to audit trails, will allow for external oversight. alone might prove to be inadequate in all scenarios. throughout the AI lifecycle, asking critical questions of agency leadership and those responsible for AI systems.  Currently no external oversight mechanism exists specific to AI in national security. Notwithstanding the important work of Inspectors General in conducting internal oversight, open questions remain as to how to complement current practices and structures.  Acknowledgments Useful feedback on the Key Considerations manuscript was provided by Anne Bowser, Erin Hahn, and David Danks. Special thanks to Caroline Danauy for legal analysis and editorial review. We also thank Lance Lantier for insights on DoD policies and directives, and Nik Marda, Samuel Trotter, and Jaide Tarwid for editorial support. 