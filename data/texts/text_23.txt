Mark Zhao, Niket Agarwal, Aarti Basant, Bugra Gedik Jerry Pan, Tianshu Bao, Haowei Lu, Sundaram Narayanan Abstract—Domain-speciﬁc accelerators (DSAs) are integrated in datacenter-scale clusters across industry to train increasinglycomplex deep learning models over massive datasets. As innovations in DSAs continue to increase training efﬁciency and throughput, the data storage and ingestion (DSI) pipeline, the systems and hardware responsible for storing and preprocessing training data, will dominate and constrain training capacity. Similar innovation in DSI is urgent, demanding an in-depth understanding of DSI systems, infrastructure, and characteristics. To this end, this paper presents Meta’s end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service (DPP) that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across our global ﬂeet, how massive and evolving datasets are stored and read, and how online preprocessing places intense demands on our underlying hardware. We synthesize key takeaways from our characterization and close with a discussion of lessons learned and research opportunities for both industry and academia. Domain-speciﬁc accelerators (DSAs) for deep neural networks (DNNs) have become ubiquitous because of their superior performance-per-watt over traditional general-purpose processors [39]. Industry has rapidly adopted DSAs for both DNN training and inference. In addition to traditional GPUs [28] and FPGA technologies [69], companies, such as Habana [10], Graphcore [44], SambaNova [64], Tenstorrent [15], Tesla [16], AWS [5], and Google [39], among others, are embracing application-speciﬁc integrated circuits (ASICs). These DSAs are deployed in immense scale-out systems to train increasingly-complex and computationally-demanding DNNs using massive datasets. For example, the latest MLPerf Training round (v1.1) [12] contains submissions from Azure and NVIDIA using 2048 and 4320 A100 GPUs, respectively, whereas Google submitted training results using pods containing up to 4096 TPUv4s [11]. At Meta, our recent ZionEX nodes can scale to thousands of GPUs, forming a datacenterscale AI training cluster [56]. These DSAs have been laserfocused on optimizing compute for training, namely matrixheavy computations used during backpropagation. In reality, training machine learning (ML) models in production involves signiﬁcantly more than just backpropagation. Namely, a data storage and ingestion (DSI) pipeline, consisting of ofﬂine data generation, dataset storage, and online preprocessing services, must store and feed exabytes of data , Satadru Pan, Mustafa Ozdal, Rakesh Komuravelli, , Jack Langman, Kevin Wilfong, Harsha Rastogi, to train three production DLRMs, with line drawn at 50%. DLRMs exhibit diverse DSI resource requirements and can consume more power than training. bandwidth across our recommendation models. Storage and bandwidth is in the exabytes and hundreds of Tbps and has grown by over 2x and 4x over the past two years, respectively. to high-performance training nodes (trainers). The design of the DSI pipeline and its infrastructure signiﬁcantly affects the overall DNN training capacity and performance, but has received little consideration compared to model training itself. This paper focuses on understanding DSI requirements, unique workload characteristics, and systems for industryscale, deep learning recommendation model (DLRM) training. We focus on DLRMs because DLRMs a) underpin many of Meta’s personalization and ranking services [22], [35]–[37], [53], b) consume the vast majority of the overall ML training cycles [22] (and DSI capacity) in Meta datacenters, and c) introduce novel DSI challenges not yet captured by current ML benchmarks [52] nor considered by existing systems [40], [46], [48], [55], [58], [79], [80]. Understanding and efﬁciently scaling the DSI pipeline is essential in enabling large-scale training for several reasons. First, inefﬁciencies in the pipeline cripple training throughput [55], underutilizing expensive DSAs. Second, DSI infrastructure competes for valuable power resources with trainers. Figure 1 shows how storage and online preprocessing can already consume more power than the actual GPU trainers themselves in Meta’s datacenters. This directly constrains training capacity due to the ﬁxed datacenter power budgets [26]. Finally, steady innovation in model complexity and DSAs for training are increasing data storage and bandwidth demands. Figure 2 shows how industry-scale dataset sizes and online data ingestion bandwidth requirements have grown by over 2× and 4× over the past two years, respectively. Barring similar innovation for DSI, we expect DSI infrastructures to severely limit training capacity at the datacenter-scale as training DSAs continue to yield higher performance-per-watt. We begin by presenting Meta’s end-to-end DSI pipeline, which enables large-scale ML model training at-scale. Training data is generated by extract-transform-load (ETL) jobs that transform unstructured feature data and event logs collected across the production ﬂeets into structured training samples. Petabyte-scale datasets are held in a centralized data warehouse as Hive tables [68], which are subsequently stored as optimized columnar ﬁles in Tectonic [61], Meta’s appendonly distributed ﬁlesystem. Finally, to handle intense online preprocessing demands, we present a production-deployed disaggregated online preprocessing framework called Data PreProcessing Service (DPP) that iteratively reads and transforms mini-batches of training data, scaling from 10s to 100s of preprocessing nodes for each training job. A key contribution of this paper lies in the deep system performance characterization for Meta’s production-deployed DSI pipeline, supporting large-scale DNN training. We describe Meta’s collaborative feature engineering and model training process for DLRMs, which reveals key system design requirements and optimizations for Meta’s underlying global DSI infrastructure. Features for DLRM training evolve rapidly in our datasets, and samples are constantly generated. This requires us to store and serve massive and dynamically-changing feature sets, representing exabytes of cumulative storage, that are constantly evolving and growing. Each training job requires an online preprocessing step demanding signiﬁcant compute, network, and memory resources to extract, transform, and load samples into materialized tensors for training. Ultimately, we hope that this work can help the architecture community identify and focus on workloads that are representative of industry uses and distill meaningful system and hardware challenges in ML, beyond just DSAs for DNN training. Table I summarizes system characterization results for the production-deployed DSI pipeline and connects each to key takeaways and open research problems for the wider research community, which we explore further in Section VII. In summary, our primary contributions are: equally important, yet vastly understudied, component of datacenter-scale ML training infrastructures. design rationales behind our production-deployed DSI pipeline architecture, tailor-made to meet important requirements of DLRM training at scale. DLRM training workloads (summarized in Table I), including coordinated training, data generation and storage, and online preprocessing, on the production hardware — identifying critical bottlenecks and key insights. questions for systems and computer architects to design and scale the DSI pipeline for large-scale training. Personalized recommendation models are used to suggest new, relevant content to users to provide meaningful interactions. These models are highly potent across a breadth of tasks. They leverage features from a user and a potential recommendation, and output a prediction (e.g., click-through rate) of how likely the user is to interact with the recommendation. For example, a video hosting site may use a user’s set of liked videos and candidate videos’ genres to rank new videos to recommend to the user. Recommendation models are trained using mini-batch stochastic gradient descent (SGD) [65], similar to most vision and natural language processing (NLP) models. SGD generalizes a model to complex distributions by iteratively updating the model’s weights to minimize a loss function, given successive mini-batches of samples. Each training sample is represented as a preprocessed tensor of features and a label. Our production recommendation models are built on our open-source DLRM architecture [60]. Modern DLRMs are massive, consisting of over 12 trillion parameters to train, requiring ≈ 1 zetaFLOPs of total compute [56]. To improve training throughput, we use data [45] and model [32] parallelism by replicating and sharding the model across multiple trainers. We distribute different mini-batches to each trainer. Trainer nodes synchronize embeddings, activations, and gradients with each other using collective communication primitives, iterating until a certain model quality metric (e.g., normalized entropy [38]) is reached. At Meta, we use hundreds of distinct recommendation models in production across our services. We continuously train new versions of each model ofﬂine (Section IV), and subsequently update production models with new data [37]. Each training job relies on a data storage and ingestion (DSI) pipeline to supply each trainer with training data throughout the duration of the job. The DSI pipeline is thus responsible for generating training samples, storing them into datasets, and preprocessing samples into tensors loaded in device memory, e.g., GPU HBMs or CPU DRAMs. III. META’S DISAGGREGATED DATA STORAGE, In this section, we present our DSI requirements and the storage, preprocessing, and training systems that compose Meta’s DSI pipeline as shown in Figure 3. A. Data Generation and Storage Overview and Requirements. Figure 3 shows how fresh training samples are continuously generated by our model serving framework in order to ensure model accuracy and comply with privacy requirements [37]. Each sample is created as services evaluate a user and item using the model serving framework. The framework ﬁrst generates an extensive set of features (e.g., a user’s liked pages) as input to an appropriate model, which outputs a prediction used for recommendation tasks. The requesting service then monitors events representing the outcome of each recommendation (e.g., if a user interacted with a post). These features and events are logged at serving time to avoid data leakage [43] between model serving and training. Subsequent streaming and batch extract-loadtransform (ETL) jobs continuously join and label raw feature and event logs into labeled and schematized samples. Training samples are placed in a storage solution that must meet several key requirements. First, individual tables require tens of thousands of features, with features being constantly added or removed. Training jobs must be able to dynamically and selectively read stored features. Second, developer productivity is paramount. We must allow ranking engineers and the underlying infrastructure to easily work across hundreds of models and tables via a centralized data warehouse using a common schema. Furthermore, ranking engineers frequently run interactive queries using Spark [76] or Presto [66] as a part of feature engineering in addition to training. Finally, datasets are continuously updated with fresh samples. 1) Data Generation: To extract and aggregate billions of features and events across the entire ﬂeet each day, we rely on Scribe [42] — Meta’s global distributed messaging system. Each service responsible for serving models or handling interactions continuously passes raw feature and event logs to a Scribe daemon running on every host. Scribe then groups logs into record-oriented logical streams and stores each stream into LogDevice [51] — a reliable distributed store for append-only, trimmable streams built on top of RocksDB [14]. To update production models, streaming engines ﬁrst join and label raw feature and event logs from Scribe and publish labeled samples into various Scribe streams used to update inproduction models. Traditional batch processing engines, such as Spark [76], further join, label, and ﬁlter samples from Scribe streams on a periodic basis (hourly or daily) to produce ofﬂine datasets used to train new production model versions. Because traditional engines work well to generate training data, require relatively little compute resources, and are not on the critical path of training, we elide in-depth discussion here. 2) Data Storage: We store training samples in a data warehouse as partitioned Hive [68] tables because of Hive’s compatibility with both internal systems and open source engines including Spark and Presto. Samples are represented structured rows, each containing features and labels, with features requiring the vast majority (> 99%) of stored bytes. To ensure interoperability of the DSI pipeline across hundreds of models and tens of thousands of features, we store two types of features, dense and sparse, in map columns. A dense feature column maps a feature ID to a continuous value (e.g., current time). A sparse feature column maps a feature ID to a variable length list of categorical values (e.g., page IDs). Some sparse features are stored in an additional column that further associates each categorical value with a ﬂoating point ”score” used for weighing (e.g., page creation time). We encode Hive tables in a columnar ﬁle format (DWRF), forked from Apache ORC [3]. Like ORC, rows are stored across multiple ﬁles. Each ﬁle contains a set of stripes, representing a number of table rows. Stripes are further divided into compressed and encrypted streams. A key distinction of DWRF is the ability to enable feature ﬁltering at the storage layer by ﬂattening each feature column and storing features as thousands of separate logical columns at the ﬁle layer (see Section VII). Each ﬂattened feature column is subsequently encoded as one or more streams, depending on its schema and encoding. Stripes are periodically ﬂushed and appended to the ﬁle. Files are written in Tectonic [61] — Meta’s exabytescale distributed append-only ﬁlesystem. Tectonic splits ﬁles into durable blocks distributed across HDD storage nodes. B. Online Preprocessing Overview and Requirements. Each training job uses an online (training-time) preprocessing pipeline to continuously transform raw samples in storage into preprocessed tensors in a trainer’s memory. Like ofﬂine data generation, online preprocessing is commonly subdivided into ETL phases. Raw bytes are extracted from storage and decoded into training samples, a process involving ﬁltering, decryption, decompression, reconstruction, and other format transformations. Training samples are next transformed into tensors. Float values may be normalized, and categorical values may be hashed, clipped, or even sorted. New features may even be derived from multiple raw features. Once features are preprocessed, they are batched together into tensors. The tensors are loaded into trainers, usually in device memory (e.g., HBM of GPUs). Online preprocessing has distinct requirements that differ those met by traditional ETL engines. First, transformations for online preprocessing are localized to each mini-batch, not across many rows. Second, online preprocessing is on the critical path of training and must match throughput required by trainers. Right-sizing online preprocessing throughput is critical to avoid either over-provisioning resources or introducing data stalls [55] that will bottleneck and underutilize expensive trainers. Finally, online preprocessing runs concurrently alongside each training job, requiring signiﬁcant cumulative compute and power resources that scales with training capacity, unlike batch ofﬂine data generation. 1) Scalable Preprocessing with DPP: Data PreProcessing Service (DPP) is our disaggregated service that provides online preprocessing for training jobs across the datacenter ﬂeet. DPP is responsible for reading raw training data from storage, preprocessing it into ready-to-load tensors, and supplying the tensors to each training node’s PyTorch [62] runtime. We designed DPP to both scale to right-sized resources and eliminate data stalls across disparate jobs, as well as enable vital productivity mechanisms for developers and ML engineers. We meet these requirements by dividing DPP into a data and control plane, which are designed to enable application throughput and ease-of-use, respectively. The control plane consists of a Master, and the data plane consists of DPP Workers and Clients. As shown in Figure 3, ML Engineers launch training jobs via FBLearner Flow [34], which then launches a DPP Master and at least one DPP Worker on general-purpose compute nodes. DPP Control Plane. At the beginning of each training job, the DPP Master receives a session speciﬁcation (a PyTorch DATASET) that reﬂects the preprocessing workload, containing the dataset table, speciﬁc partitions, required features, and transformation operations for each feature. The DPP Master enables scalable work distribution by breaking down the entire preprocessing workload, across petabytes of data, into independent and self-contained work items for the data plane called splits that represent successive rows of the entire dataset. The Master serves splits to DPP Workers upon request and tracks progress as splits are completed. In addition to work distribution, the DPP Master is responsible for fault tolerance and auto-scaling. The DPP Master periodically creates a checkpoint which can be used to restore reader state on failure. The DPP Master also continuously monitors Worker health, automatically restarting any Workers that have failed without needing a checkpoint restore due to Workers’ stateless design. The DPP Master itself is replicated to avoid being a single point of failure. Finally, the DPP Master implements auto-scaling via a controller. The controller collects utilization (CPU, memory, and network) statistics and the number of buffered tensors from each DPP Worker. It then periodically evaluates scaling decisions, calculating the number of DPP Workers to either drain or launch with the goal of maintaining a non-zero number of buffered tensors (indicating that trainer demand is met) and maximum CPU, network, and memory utilization. In doing so, the DPP Master eliminates data stalls with minimal DPP resource requirement. DPP Data Plane. DPP Workers and Clients are responsible for data plane operations of DPP. Workers are designed to effortlessly scale out to eliminate data stalls. Workers are stateless, precluding any limit to how many Workers can exist in a given DPP session. They only communicate with the DPP Master (to fetch work items) and a limited number of DPP Clients (to serve tensors); all transformations within a mini-batch are performed locally. On startup, each Worker pulls a set of transformations from the Master, represented by a serialized and compiled PyTorch module, that it will use during preprocessing. Workers then continuously query for and process splits from the DPP Master. As shown in Figure 3, each split requires Workers to extract, transform, and (partially) load training data. Speciﬁcally, Workers begin by reading, decompressing, and decrypting raw Tectonic chunks. Sets of raw chunks are then reconstructed into streams and decoded into raw table rows, ﬁltering out unused features if necessary. Next, it applies the speciﬁed transformations to each raw feature using high-performance C++ binaries. Once features are transformed, Workers batch samples together into tensors to be loaded onto GPU trainers. We ensure that transient delays in the pipeline do not introduce data stalls by maintaining a small buffer of tensors in memory. Trainers. DPP Clients form the other half of the data plane. A Client runs on each training node, exposing a hook that the PyTorch runtime can call to obtain preprocessed tensors. These requests are transparently transformed into a simple RPC request which returns a batch of tensors from the Worker buffer. By ofﬂoading computationally expensive operations to DPP Workers and enabling Client multithreading, Clients do not bottleneck the data ingestion pipeline. To ensure that Client and Worker network connections can scale, each Client uses partitioned round robin routing, capping the number of connections that Clients and Workers need to maintain. To enable training larger recommendation models, we have built high-performance training clusters, enabling individual jobs to train on hundreds to thousands of GPUs. Each training job is controlled by a Trainer Master, which manages the overall training session. On each trainer, a PyTorch runtime manages the local training workﬂow, transferring preprocessed tensors between the DPP Client and GPU device memory. Parameter updates between trainers occur over a dedicated backend network and do not impact data ingestion. We defer readers to [56] for details on our scale-out training clusters. Next, we explore how DLRMs are trained and deployed at Meta. We do so because large-scale training deployments require thousands of training jobs over hundreds of models and datasets, all running on a shared global infrastructure. Understanding industrial training jobs highlights important system and infrastructure implications not found in commonly-studied hyperparameter (HP) tuning or isolated training jobs. We focus on the three representative recommendation models (RMs) highlighted in Figure 1, denoted RM, as these models are the most widely-used and training resource-intensive. Hundreds of recommendation models are deployed in production at Meta, each continuously developed by many training jobs. Each training job produces a new model version with the goal of becoming the next production model. Many of our models are supported by large teams of ranking engineers, requiring the need to allow continuous experimentation across engineers while avoiding conﬂicts between model versions and conserving limited training capacity. This need naturally arises as model engineering teams mature [63]. of 82 RMcombo jobs within one model release iteration. jobs over one year, showing peaks corresponding to combo jobs. We thus adopted a regimented release process which occurs over three phases. First, ML engineers explore their ideas (e.g., new features or model architectural improvements) on top of the current production model through hundreds to thousands of small training jobs. Exploratory jobs generally require less compute and use a small fraction (typically < 5%) of its respective table’s total samples. Next, the most promising ideas are periodically combined in various permutations to generate tens to hundreds of training jobs. These combo jobs are large and trained within a short window, demanding immense parallelism and the majority of the table. Finally, the most promising release candidates (RCs) are further trained and evaluated on fresh data, and the most accurate model is deployed in production. While these jobs are large, there are only a few. Counter-intuitively, the model release process can result in more diversity in terms of temporal locality, model architectures, and feature sets than in isolated or HP tuning jobs [40], [46]. This is because training capacity is highly-constrained compared to per-job compute requirements, requiring engineers to combine many architecture and feature proposals into one combo job. Figure 4 illustrates how the model design space is explored given compute constraints. While individual jobs are long-running and can take over 10 days to train, many jobs fail or are killed because their performance is lackluster. Instead of waiting to launch jobs synchronously, engineers will immediately schedule new jobs to maximize the number of explored ideas within the combo time window, resulting in a large temporal skew between jobs. The aforementioned collaborative training jobs, including exploratory, combo, and release candidates, run on the global ﬂeet of training (including DSI) infrastructure, spread across global regions, each with multiple datacenters [37]. Figure 5 shows a historical demand, in terms of normalized compute, split by global region (R1-R5), normalized to model J. NUMBER OF FEATURES CREATED FOR RM of all collaborative training jobs across DLRMs over one calendar year across our entire ﬂeet. We observe distinct peaks in demand, corresponding to periods where many models concurrently train combo jobs. Because these combo jobs are on the critical path of model release, we must explicitly architect our datacenters with sufﬁcient storage, preprocessing, and training capacity to meet the peak demand of combo jobs. Furthermore, as we explore in Section V, each model reads a distinct dataset. At the same time, cross-region (and often cross-datacenter) bandwidth is highly-constrained. This requires systems and datacenter architects to co-locate DSI resources with trainers themselves and provision enough capacity for each. Figure 6 shows a bar chart of the relative compute demand of the ten most commonly-run models, broken down by the region in which they ran. Our global scheduler currently balances training jobs for each model across regions, requiring each region to contain a copy of all models’ datasets. Bin-packing opportunities can reduce storage costs, with care to ensure data availability for each model as its peak compute demand can exceed regional capacity. The sets of features stored to a dataset and read by training jobs can also vary heavily, as features also undergo rapid experimentation and productionization. To understand feature storage variability, Table II shows the total number of new features proposed for RM’s production dataset within a 6month window and the status of the feature 6 months later. Beta features are not actively logged, but may be back-ﬁlled or injected (i.e., dynamically joined) for each exploratory training job. Experimental features are used as a part of combo or release candidate jobs. If the release candidate job becomes the next production model, its used features become active, while some older features may become deprecated following a review process or even reaped to protect user privacy. Experimental, active, and deprecated features are actively written. We observe that features are rapidly changing in production datasets, with hundreds of new features added and deprecated each month. Thus, efﬁcient ML data storage infrastructure must adapt to frequent changes in the feature set. COMPRESSED SIZES OF ALL TABLE PARTITIONS, EACH PARTITION, Training production models needs a collaborative release process across hundreds of engineers. Critically, ideas are periodically amalgamated in a large number of concurrent combo jobs for each model, resulting in large peaks in training and DSI resources across our ﬂeet during this phase. Because combo jobs are on the critical path of model release, we must design datacenters with sufﬁcient capacity across global regions for peak demand corresponding to combo jobs. This capacity required is spread across hundreds of models with varying compute demand, motivating the need for efﬁcient co-location and scheduling of jobs and datasets across regions to reduce inter-region storage and network demands. Finally, we explored how the training jobs are temporally skewed and exhibit diverse model architectures, and datasets are continuously evolving, inhibiting system optimizations that assume highly-synchronized and similar training jobs or static datasets, e.g. [40], [46], [55]. We next explore how datasets are stored in our data warehouse and read by training jobs, highlighting implications to our storage hardware and infrastructure. Benchmark datasets are typically re-read multiple times (epochs) to reach target accuracy [52]. Thus, existing work focuses on randomly modifying [23], [27], [29], [30], [48] or caching [46], [55], [58], [74] data across epochs. Unlike these datasets, production training jobs are not constrained by the amount of data. Instead, model size and compute capacity limits constrain the number of features and samples each job can use, respectively. Production training jobs do not require stochastic preprocessing across multiple epochs, but instead can reach a desired target accuracy with (less than) one epoch containing many samples. Individual training jobs specify a given table as its dataset, along with ﬁlters that select a subset of data within the table along two dimensions: a variable number of partitions (row ﬁlter) and a set of features within each sample to read (column ﬁlter). We begin by analyzing one representative RC training job from R M. Table III shows the (compressed) size characteristics of each model’s respective production training data table. It also shows the size of each partition (by date) in the table and the cumulative size of the partitions used by the training job for each RM . Even our largest training jobs often read less than the entire available dataset and each sample only once. The partitions that are read still FEATURE CHARACTERISTICS OF PRODUCTION MODELS. DATASET CHARACTERISTICS FOR EACH MODEL. require petabytes of data, which is signiﬁcantly larger than the local storage capacity at each trainer node, contrary to prior assumptions [55]. Furthermore, as shown in Figure 2, dataset sizes for production models are continuously growing, driven by multiple factors such as organic user growth, reduced downsampling, and an increase in engineered features. Next, we study how a training job selects data along the feature (column) dimension. Individual training jobs specify a feature projection, consisting of a list of desired features to be read from all rows in the designated partitions. Table IV shows the number of dense, sparse, and derived features required by a representative RC model version for each RM . These model versions require 504 − 1221 and 42 − 306 dense and sparse features, respectively. This is in contrast to Table V, which shows that signiﬁcantly more features are logged in each model’s table. Each training job only needs to read 9 − 11% of stored features. Even when accounting for the number of bytes read, Table V highlights that RMonly read between 21 and 37 percent of stored bytes across used partitions. The relative increase in read bytes is because read features typically exhibit larger coverage and lengths, and thus require more bytes, as these features contribute stronger signals to model quality and are thus favored by ML engineers. While there appears to be room to reduce feature collection, feature experimentation is essential for ML engineers. We prioritize developer productivity and heavily err on the side of keeping features, even at the cost of storage, to ensure that ML engineers have access to the features they need. Selective reading also has further implications for the performance of our storage nodes. Table VI shows the distribution, in bytes, of a representative RMtraining job’s IO sizes from storage. Heavy ﬁltering and columnar storage of features on disk (Section III) results in relatively-small contiguous regions for read features. Further software-hardware co-design is needed to ensure that disk seeks do not cripple storage IOPS. While individual training jobs require extensive ﬁltering, training jobs do collectively reuse data. Inter-job data reuse can occur throughout the model release process because ML engineers do not develop an entirely new model architecture IO SIZES FOR FEATURES READ BY AN RM1 TRAINING JOB. month of each RM’s runs. Popular bytes are reused across runs. and feature set each iteration, but instead largely build upon a common baseline (e.g., the current production model version). Figure 7 shows that, based on training runs for RMover one month, training runs for each model tend to favor speciﬁc bytes. The x-axis shows a CDF of bytes within the model’s used set of table partitions. The y-axis shows the percent of all I/O from storage that the most-popular x percent of stored bytes contribute to. To serve 80% of trafﬁc from storage, we only require the most commonly-used 39, 37, and 18 percent of RM’s, RM’s, and R M’s datasets, respectively. Combined with Table V, Figure 7 also highlights how used features and bytes vary across training jobs. RM3 exhibits little variance in features — Table V and Figure 7 show that individual and collective models read roughly 21% of the stored bytes. Meanwhile, RMand RMshow high variance; individual jobs only read 37% and 34% of the stored bytes, respectively, while jobs collectively read over 60% of the stored bytes. In this section, we explored how features are stored in petabyte-scale datasets that greatly exceed local storage capacities, requiring training jobs to read samples from a centralized data warehouse (Section III-A). Furthermore, each training job requires extensive ﬁltering both in the number of samples (rows) and features extracted from each sample (columns). Column-wise ﬁltering results in small reads from storage because features are stored in columnar ﬁles. Finally, across training jobs for a given model, we observed signiﬁcant reuse in commonly-used features, with 40% of bytes contributing to over 80% of read throughput. Distributed trainers drive strict data ingestion bandwidth requirements. Data stalls result when online preprocessing throughput is less than the aggregate throughput of the trainers themselves, underutilizing GPU resources [55]. Current preprocessing solutions, which perform preprocessing on the CPUs of each training node, are insufﬁcient. To demonstrate this, we ran a training job for RMon a training node, consisting of two 28-core x86 CPU sockets, two 100 Gbps NICs, and a total of 8 NVIDIA V100 GPUs. The RMS DRIVE LARGE AND DIVERSE TRAINER THROUGHPUT. trainer read from distributed storage, preprocessed each minibatch using the production PyTorch [62] stack, and performed training on the same machine. Table VII shows that 56% of GPU cycles were spent stalled waiting for training data. The high CPU utilization shows that the trainer’s CPUs cannot preprocess data fast enough to serve the GPUs. We next seek to understand the preprocessing bottlenecks in detail. To do so, we analyzed the preprocessing throughput demanded by GPUs across RMs and traced through data extraction, transformation, and loading requirements. We measured the online preprocessing throughput required by each R M by running a production training job for each RM on a training node, ensuring that GPUs were not stalled by serving tensors from an in-memory buffer. Table VIII shows the per-trainer node GPU throughput requirements (i.e., tensor ingestion rate) for each representative RM . GPU throughput requirements are not only signiﬁcant, but vary by over 6× across models. The difference in throughput across the models is due to the variations in operational intensity (i.e., compute per sample) across models, as well as synchronization overheads between GPUs during each iteration. Furthermore, we project the online preprocessing throughput requirement to increase by 3.5× within the next two years due to larger training samples, improved hardware accelerators, and software optimizations. We cannot simply over-provision resources for the worst-case model; doing so would waste large amounts of capacity and power across our ﬂeet. The DSI pipeline must scale online preprocessing to meet intense and increasing GPU throughput demands, and adapt to the diverse requirements across models. Section V established that we must load training data from distributed storage. We now show that data loading over the network, even without extraction or transformations, requires signiﬁcant trainer CPU, network, and memory bandwidth. Figure 8 shows the memory bandwidth and CPU utilization on the 2-socket, 8-GPU training node as the data ingestion rate increases. Vertical lines represent the required GPU throughput across each RM, as measured in Table VIII. High training data throughput demands driven by the GPUs directly translate to considerable front-end resource requirements for data loading. Production-scale model training is approaching NIC saturation, even with signiﬁcant reduction in data sizes due ingestion throughput scales using a dummy trainer. DPP WORKER THROUGHPUT ACROSS RMS AND THE RESULTING # NODES REQUIRED TO MEET TRAINER DEMANDS. STORAGE RX to preprocessing (see Section VI-C). Second, even without expensive extraction or transformation operations, production models require up to 40% of CPU cycles (almost a full socket) and 55% of memory bandwidth. This demand is due to network stack and memory management requirements in addition to the necessary ”datacenter tax” [41] operations such as TLS decryption and Thrift deserialization that are required in our production environment. Considering memory bandwidth saturates at ≈ 70% utilization, data loading constrains CPU, memory bandwidth, and network resources. Data extraction and transformation requires strikingly more resources than available on trainers and must be distributed. Table IX shows the maximum data extraction and transformation throughput achieved by each DPP Worker, running on a general purpose server (C-v1, Table X). We need large and highly-variable throughput across models to meet the GPU demands in Table VIII — between 9 and 55 servers per trainer node. Unlike traditional distributed query executors [54], [57], [75], [76], which rely on large clusters to produce a result as fast as possible, online preprocessing requires continuous throughput guided by GPU demands; allocating more preprocessing workers will not improve end-to-end training time. On the other hand, using trainer hosts for preprocessing is also insufﬁcient, as our online preprocessing demands represent considerably more network, compute, and memory bandwidth resources than available locally, especially when factoring in data loading. Not only do models require large and diverse data loading throughput, achievable extract and load throughput, given ﬁxed compute, varies across models, emphasizing the need to right-size preprocessing resources to each model. To understand the implications of these results in more detail, we observe that preprocessing signiﬁcantly reduces data sizes, especially considering storage bytes are compressed in Table IX. This is due to a combination of ﬁltering, over reading features from storage (see Section VII), and size reduction cessing workers across RMs. CPU utilization is broken down into transformation, extraction, and miscellaneous cycles. HARDWARE ACROSS THREE VERSIONS OF COMPUTE SERVERS. during transformations. This has implications on network throughput requirements, as 1.18 to 3.64× more network bandwidth is required to extract raw samples from storage than to load preprocessed tensors. Thus, performing data extraction at the trainers would further amplify the network bandwidth requirements beyond the per-model requirements shown in Figure 8, resulting in data ingestion bottlenecks. Furthermore, network bandwidth is not the only limiting factor. Figure 9 shows DPP CPU and memory bandwidth utilization at saturation. Each model exhibits diverse resource requirements. In fact, only RMis bound on ingress NIC bandwidth as shown in Table IX. RMis bottlenecked on memory bandwidth and CPU utilization. As shown in Figure 9, this is because RMrequires signiﬁcantly more CPU cycles for preprocessing due to its computationally expensive transformations. RMis bound on memory capacity, forcing us to limit the worker thread pool size to avoid OOM exceptions. While models are constrained on various hardware currently, we expect memory bandwidth to become the dominant bottleneck at readers. Table X shows the hardware characteristics of our current (C-v1) and upcoming versions of compute nodes used by DPP. Memory bandwidth grows at a signiﬁcantly slower rate than the number of cores and NIC bandwidth. For example, we ran preprocessing for RMon the C-v2 node in Table X and observed that memory bandwidth, not network, was the bottleneck. We further studied where memory bandwidth is going, and observed that 50.4%, 24.9%, 16.4%, and 4.7% of LLC misses were due to transformations, extraction, network receive, and network send, respectively, highlighting areas for future optimization. Accelerating transforms (e.g., via GPUs [20]) are promising, but requires more research as our transformations can be distinctly different from the matrix-heavy operations used in training and may contend cycles with training. Table XI provides a list of important (but not exhaustive) preprocessing transformations that are needed by our production DLRMs. These operations are distinctly different from the image-centric DESCRIPTION OF COMMON PREPROCESSING TRANSFORMATIONS. operations used in many preprocessing libraries [6], such as crops, resizing, and color augmentations. DLRM transformation operations can be split into three classes: feature generation, sparse feature normalization, and dense feature normalization. Dense feature normalization (Logit, BoxCox, Onehot) and sparse feature normalization (SigridHash, FirstX) normalize features based on dataset statistics. Feature generation operations (commonly-used ones include Bucketize, NGram, and MapId) derive new dense and sparse features from raw dataset features. Feature generation is especially expensive — dense normalization, sparse normalization, and feature generation typically require around 5%, 20%, and 75% of transformation cycles, respectively. We are actively open-sourcing these operations in TorchArrow [18]. DLRM training jobs require online preprocessing that induces data stalls due to limited host CPU resources. We built DPP, a disaggregated online preprocessing service, to completely eliminate data stalls. Nevertheless, trainers must be provisioned with enough host compute, networking, and memory to handle data loading rates driven by the GPUs. At disaggregated DPP Workers, we expect memory bandwidth to become the primary bottleneck, largely due to transformations. Finally, we identiﬁed how DLRM transformation operations differ from image models, and feature generation dominates transformation compute. This section explores key insights we learned while architecting and proﬁling Meta’s DSI pipeline and important research challenges we continue to face. As DSAs for ML continue to improve, the DSI pipeline is becoming an increasingly resource-intensive component of the end-to-end ML training pipeline. We argue that similar attention to DSI is warranted in order to continue scaling datacenter-scale training. Where are the hardware bottlenecks in DSI? In the production storage layer (i.e., Tectonic [61]), we must provision sufﬁcient storage capacity and IOPS bandwidth at the datacenter level. Given industry-scale dataset sizes (Table III), aggregate trainer node throughput (Table VIII) scaled by data volume changes due to preprocessing (Table IX), and IO size characteristics (Table VI) on our HDD storage nodes, the throughput-to-storage gap is over 8x even after accounting for triplicate replication for durability. In other words, we must provision signiﬁcantly more storage capacity per datacenter than is required, in order to meet IOPS demands. At DPP Workers, Figure 9 demonstrated that different RMs strained network, memory, and compute resources. However, Table X shows that network and compute growth is outpacing memory bandwidth in our general-purpose compute nodes. We expect to be heavily constrained on memory bandwidth. Finally, at Trainers, Section VI explains how trainers are constrained on both front-end network and host CPU resources without DPP. However, given that we disaggregated our DSI pipeline, we can ensure that the accelerators themselves are the bottleneck by simply provisioning enough front-end compute and network resources for data loading at design time. For example, our next-generation ZionEX nodes contain 4 CPU sockets, each with a 100 Gbps dedicated front-end NIC, to ensure that all GPUs are fully fed [56]. Opportunities for Heterogeneous Hardware. The bottlenecks above allude to ample opportunity for heterogeneity in DSI. First, we noted how our HDD-based storage nodes presented ≈ 8× throughput-to-storage gap. We can gain power efﬁciency by leveraging heterogeneous storage media with a higher IOPS per watt. For example, our SSD-based storage nodes can provide 326% of IOPS per watt, but trades off storage capacity with only 9% capacity per watt, compared to HDDs within our ﬂeet. However, simply placing training data on SSDs would result in an unfavorable storage-tothroughput gap due to our large datasets. An ideal solution would balance data between heterogeneous storage, such as SSDs for throughput and HDDs for capacity. There are further opportunities in both software, such as a system that places popular features (Figure 7) on an SSD-based cache, and hardware, such as leveraging non-volatile memory. We also foresee further opportunities for acceleration during online preprocessing. For example, recent efforts have been used to accelerate preprocessing on the training GPU [20], but risks degrading training throughput. While we believe preprocessing can be accelerated, there are numerous open questions. Preprocessing can be performed on the training GPU, host CPU, disaggregated CPUs, or disaggregated accelerators — deciding the optimal placement is non-trivial. Furthermore, many preprocessing operations described in Section VI exhibit diverse amenability for acceleration. For example, we observed an 11.9× and 1.3× GPU/CPU performance for SigridHash and Bucketize, respectively, on a V100 and 20 CPU threads. Preprocessing also requires many distinct kernels (e.g., 3-5 operations to derive a single feature); kernel launch overheads are non-negligible. Finally, transformation operations also may diverge from GPU-optimized GEMM operations, warranting further exploration into DSI-speciﬁc DSAs. Finally, Section VI shows how datacenter tax further constrains DPP Worker resources. For example, TLS ampliﬁes memory bandwidth 3×, further constraining a limited resource. Techniques such as TLS NIC ofﬂoading and HW acceleration [67] will be critical to further scale DSI. Datacenter Planning and Global Scheduling. We must intelligently design and provision compute, network, and storage capacity in each datacenter to ensure both high utilization of each DSI system given a ﬁxed power budget and sufﬁcient capacity to meet peak training demands. To do so, we rely on extensive models built by continuously proﬁling DSI workloads on both storage nodes and DPP Workers. Designing a datacenter for ML training thus requires not only understanding compute requirements of the models, but also an accurate benchmark of the datasets and preprocessing requirements (which we discuss next). We must also consider scheduling as an important component of ML training systems. We have multiple datacenters in a region and multiple regions globally. Section IV explored how our training workload is spread across regions, requiring datasets to be replicated and co-located with trainers. We foresee opportunity for a global scheduler to intelligently route and bin-pack training jobs to speciﬁc regions to reduce storage duplication. Furthermore, as our datasets continue to grow beyond DC-scale, other parallelism techniques, such as modelhopper parallelism [59] to move TB-scale models across DCs instead of PB-scale data, will become increasingly important. Importance of Representative Benchmark Datasets. Recent advancements in DSAs have largely been driven and measured by benchmarks. For example, the latest round of MLPerf Training [52] (v1.1) saw results from 14 organizations with up to 2.3× improvement over the previous round. Unfortunately, these benchmarks have largely focused on models as opposed to datasets, leading to rapid innovation in model architectures and training nodes while largely ignoring the DSI pipeline. Current leading benchmark datasets, such as ImageNet [33] and COCO [49], were released in 2010 and 2014 and have been largely unchanged, despite their ubiquity in academia. The Criteo 1TB Click Logs [8], the Recommendation dataset used by MLPerf Training, was released in 2013. Section V describes how our datasets differ greatly from static benchmark datasets. Understanding how representative datasets are generated, stored, and read are critical for further research in DSI for ML training. Speciﬁcally, we characterized that for DLRMs, training samples are a) continuously generated from real-world events, b) stored as structured samples in a common data warehouse or feature store in columnar ﬁles in a distributed ﬁlesystem, and c) require petabytes of storage. Furthermore, when training jobs read the dataset, they d) perform extensive online preprocessing operations, e) only require one epoch, and f) require further per-feature ﬁltering. We are actively working towards more representative benchmark datasets (e.g., [7]), and we hope this paper motivates the importance of further work in this area. Multi-dimensional System Co-design. Efﬁciency gains solely via hardware are slow, as hardware refreshes must be planned years in advance. To continuously improve DSI efﬁciency (and increase training capacity), we spend signiﬁcant effort optimizing our DSI systems because at scale, small efﬁciency gains can translate to MWs of additional trainer capacity. In our experience, DSI system architects must understand and co-design optimizations across two dimensions. Top-to-bottom: DSI systems must leverage characteristics and meet requirements of applications (ML models and engineers) while optimizing for the underlying hardware. End-to-end: Optimizations must be considered across the DSI components; optimizations can trade-off efﬁciency across the entire pipeline. We highlight this through recent examples next. Our map-based schemas, designed to handle dynamic features, initially required reading the entire row. In contrast, training jobs heavily ﬁltered features. We optimized our DWRF ﬁle format with feature ﬂattening, which appended metadata for each feature ID to allow selective feature reading. While DPP Worker throughput doubled (with 12% increase in storage capacity), Table VI showed how ﬁltering led to small IO sizes and thus poor IOPS on our HDD-based storage nodes. We optimized for HDDs by coalescing reads, reading feature streams within 1.25 MiB in one I/O, eliminating storage throughput degradation by amortizing disk seeks. Unfortunately, coalesced reads resulted in over reads of unused features, limiting its effectiveness. We addressed over reads by optimizing how data generation pipelines wrote ﬁles in Tectonic. We reordered popular feature streams together to reduce the amount of unnecessary features in a coalesced read. We also increased the number of rows in each ﬁle stripe to increase the average I/O size of each read. Finally, we also noted how DWRF and tensor formats represent feature values contiguously across rows. We changed how DPP Workers represent samples with in-memory ﬂatmaps to match, reducing format conversions and thus reduce constrained memory bandwidth demands. By enabling model-driven ﬁltering on HDDs (top-to-bottom optimization) and improving how samples are written and stored (end-to-end) with feature ﬂattening, coalesced reads, write-path optimizations, and in-memory ﬂatmaps, we increased DPP and storage throughput by 2.94× and 2.41×, respectively. When weighed by our provisioned DPP and storage power requirements, these co-designed optimizations resulted in a 2.59× reduction in DSI power requirements. Additional co-designed optimization opportunities that aim at improving data extraction and DPP in-memory formats, such as Velox [19] and TorchArrow [18], are promising. We are also exploring optimization techniques, such as caching preprocessed tensors and balancing transformations between ofﬂine and online ETL. Data Storage and Ingestion for ML. We presented Meta’s production-deployed DSI pipeline. ETL pipelines. We discussed how we use traditional ETL engines, such as Spark [76], to generate structured training data from raw logs. A number of query and streaming engines [24], [54], [57], [75]–[77] are used across industry for this task. We characterized how online data preprocessing demands diverge from traditional ETL, requiring deep integration into PyTorch, pipelined compute, localized minibatch transforms, and right-sizing, highlighting a need for a distinct online preprocessing framework that optimizes for power efﬁciency while eliminating data stalls. Data Storage and Warehousing for ML. We characterized how industrial datasets differ from benchmarks, requiring massive and evolving datasets, highly selective ﬁltering, and interoperability and reuse across multiple models and systems. To address these needs, we store datasets as Hive [68] tables on top of Tectonic [61] using an optimized Apache ORC [3] like format. Comparable solutions exist across industry. Feature stores (e.g., Tecton [9]) and data warehouses (e.g., DeltaLake [25] and Snowﬂake [31]) manage datasets. These rely on variety of storage and memory formats [1], [2], [4], [17]. Online Preprocessing for ML. tf.data [58] presents a runtime and API for online preprocessing in TensorFlow [21]. While tf.data Service [13] is an experimental feature to distribute online preprocessing on a user-managed cluster, tf.data focuses on optimizing preprocessing on the host CPU. DPP similarly presents a runtime for online preprocessing fully integrated in PyTorch [62]. DPP is inherently disaggregated and runs as a fully-managed service at Meta, enabling online preprocessing to automatically scale to meet the throughput required by training jobs running on hundreds of GPUs. Other recent works target key DSI components, focusing on benchmark vision and NLP models. CoorDL [55], Quiver [46], and DIESEL [70] are caches that optimize for single-server training, HP tuning jobs, and small ﬁles, respectively. DeepIO [79] and DLFS [80] leverage hardware (RDMA and NVMeOF) to provide randomized minibatches from storage. Revamper [48] randomly augments samples across epochs to reduce online preprocessing costs. Wang et al. [71] and Kumar et al. [47] mitigate data stalls on TPUs. OneAccess [40] motivates sharing online preprocessing for HP tuning jobs. Industrial DSI characteristics are markedly different from benchmarks (Section VII), limiting the impact of such systems in industrial settings. Understanding Large-Scale Training. tf.data [58] characterized online preprocessing at Google, highlighting similar ﬁndings such as prevalent data reuse and demanding compute requirements. Xin et al. [73] characterized how ML models are trained and deployed at Google, focusing on model lifecycle management. To the best of our knowledge, our work represents the ﬁrst characterization of the end-to-end DSI workloads, systems, and infrastructure in a large-scale training deployment, noting key research opportunities. Recommendation Models. Gupta et al. [36] analyzed industry-scale inference models on three different CPU architectures. DeepRecSys [35] and RecSSD [72] optimized inference requests across CPUs and GPUs, and SSDs, respectively. Acun et al. [22] characterized DLRM architectures on GPU trainers, CPR [50] explored checkpointing trainer state, and AIBox [78] optimized training using hierarchical memory for parameters. However, these prior works do not discuss the DSI pipeline, a critical part of ML training. DSI infrastructure will dominate large-scale training resource and power capacity without further innovation and optimization. This paper presented Meta’s end-to-end data storage, ingestion, and training pipeline used to train our production recommendation models. We characterized DSI workloads on our ﬂeet, including coordinated training, dataset storage and reading, and online preprocessing workloads. To spur further research, we synthesized key insights and important research directions gleaned from our characterization and experience.