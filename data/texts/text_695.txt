Contextual bandit problems (see e.g., [LS19; Sli19]) are a special case of reinforcement learning, in which the state (context) at each time step is chosen independently, rather than being dependent on the past history of states and actions. Despite this limitation, contextual bandits are widely used in real-world applications, such as recommender systems [Li+10; Guo+20], advertising [McM+13; Du+21], healthcare [Gre+17; AKR21], etc. The goal is to maximize the sequence of rewards response to each input context or state E [y agent does not get to see the “correct” output, but instead only gets feedback on whether the choice it made was good or bad (in the form of the reward signal). If the agent knew using information about the reward function, before it can “exploit” its model. on the upper conﬁdence bound (UCB) method (see e.g., [Li+10; KCG12]) and the Thompson Sampling (TS) method (see e.g., [AG13; Rus+18]). The key bottleneck in both UBC and TS is eﬃciently computing the posterior This can be done in closed form for linear-Gaussian models, but for nonlinear models, such as deep neural networks (DNNs), it is computationally infeasible. parameter posterior our approach is that we show how to scale the EKF to large neural networks by leveraging recent results that show that deep neural networks often have very few “degrees of freedom” (see e.g., [Li+18; Izm+19; Lar+21]). Thus we can compute a low-dimensional subspace and perform Bayesian ﬁltering in the subspace rather than the original parameter space. We therefore call our method “Bayesian subspace bandits”. In this paper we present a new algorithm for online (sequential) inference in Bayesian neural networks, and show its suitability for tackling contextual bandit problems. The key idea is to combine the extended Kalman ﬁlter (which locally linearizes the likelihood function at each time step) with a (learned or random) low-dimensional aﬃne subspace for the parameters; the use of a subspace enables us to scale our algorithm to models with∼1Mparameters. While most other neural bandit methods need to store the entire past dataset in order to avoid the problem of “catastrophic forgetting”, our approach uses constant memory. This is possible because we represent uncertainty about all the parameters in the model, not just the ﬁnal linear layer. We show good results on the “Deep Bayesian Bandit Showdown” benchmark, as well as MNIST and a recommender system. |s, a, θ]=f(s, a;θ), whereθare the unknown model parameters. Unlike supervised learning, the a=argmaxf(s, a;θ). However, sinceθis unknown, the agent must “explore”, so it can gather In the bandit literature, the two most common solutions to solving the explore-exploit dilemma are based In this paper, we propose to use a version of the extended Kalman ﬁlter to recursively approximate the it has not been done in an online or bandit setting, as far as we know. Since we are using approximate inference, we lose the well-known optimality of Thompson sampling [PAYD19]; we leave proving regret bounds for our method to future work. In this paper, we restrict attention to an empirical comparison. We show that our method works well in practice on various datasets, including the “Deep Bayesian Bandits Showdown” benchmark [RTS18], the MNIST dataset, and a recommender system dataset. In addition, our method uses much less memory and time than most other methods. computation of the posterior. This includes tasks such as life long learning, Bayesian optimization, active learning, reinforcement learning, etc. In this section, we brieﬂy review related work. We divide the prior work into several groups: Bayesian neural networks, neural net subspaces, and neural contextual bandits. approaches include the Laplace approximation [Mac92; Mac95; Dax+21a]; Hamiltonian MCMC [Nea95; Izm+21]; variational inference, such as the “Bayes by backprop” method of [Blu+15], and the “variational online Gauss-Newton” method of [Osa+19]; expectation propagation, such as the “probabilistic backpropagation” method of [HLA15]; and many others. (For more details and references, see e.g., [PS17; Wil20; WI20; Kha20].) propose an online version of the Laplace approximation, [Ngu+18] propose an online version of variational inference, and [GDFY16] propose to use assumed density ﬁltering (an online version of expectation propagation). However, in [RTS18], they showed that these methods do not work very well for bandit problems. In this paper, we build on older work, speciﬁcally [SW89; FNG00], which used the extended Kalman ﬁlter (EKF) to perform approximate online inference for DNNs. We combine this with subspace methods to scale to high dimensions, as we discuss below. simple approach is to use variational inference with a diagonal Gaussian posterior, but this ignores important correlations between the weights. It is also possible to use low-rank factorizations of the posterior covariance matrix. In [Dax+21b], they propose to use a MAP estimate for some parameters and a Laplace approximation for others. However, their computation of the MAP estimate relies on standard oﬄine SGD (stochastic gradient descent), whereas we perform online Bayesian inference without using SGD. In [Izm+19], they compute a linear subspace of dimension [Izm+18]; they then perform slice sampling in this low-dimensional subspace. In this paper, we also leverage subspace inference, but we do so in the online setting, which is necessary when solving bandit problems. which utilizes DNNs to model the reward function, combined with Thompson sampling as the policy for choosing the action. In [RTS18], they evaluated many diﬀerent approximate inference methods for Bayesian neural networks on a set of benchmark contextual bandit problems; they called this the “Deep Bayesian Bandits Showdown”. The best performing method in their showdown is what they call the “neural linear” method, which we discuss in Section 3.3. data to avoid the problem of “catastrophic forgetting” [Rob95; Fre99; Kir+17]. This means that the memory complexity is impractical for applications where the data is high dimensional, and/or the agent is running for a long time. Although Bayesian inference in DNN subspaces has previously been explored (see related work in Section 2), Our algorithm is not speciﬁc to bandits, and can be applied to any situation that requires eﬃcient online Most work on Bayesian inference for neural networks has focused on the oﬄine (batch) setting. Common There are several techniques for online or sequential Bayesian inference for neural networks. [RBB18] There are several techniques for scaling Bayesian inference to neural networks with many parameters. A The literature on contextual bandits is vast (see e.g., [LS19; Sli19]). Here we just discuss recent work Unfortunately the neural linear method is not a fully online algorithm, since it needs to keep all the past In [NZM21], they make an online version of the neural linear method which they call "Lim2", which stands for “Limited Memory Neural-Linear with Likelihood Matching”. We discuss this in more detail in Section 3.3. including neural Thompson sampling [Zha+21] and neural UCB [ZLG20]. We discuss these methods in more detail in Section 3.3. Although Neural-TS and Neural-UCB in principle achieve a regret of there are some disadvantages. First, these algorithms perform multiple gradient steps, based on all the past data, at each step of the algorithm. Thus these are full memory algorithms that take time. Second, it can be shown [AZL19; Gho+20] that NTKs are less data eﬃcient learners than (ﬁnite width) hierarchical DNNs, both in theory and in practice. Indeed we will show that our approach, that uses constant memory and ﬁnite width DNNs, does signiﬁcantly better in practice. In this section, we discuss various methods for tackling bandit problems, including our proposed new method. Algorithm 1: Online-Eval(Agent, Env, T , τ ) (agent), given access to an environment or simulator. In the case of a Thompson sampling agent, the action selection is usually implemented by ﬁrst sampling a parameter vector from the posterior (belief state), posterior predicted mean and variance, and then picking the action with the highest optimistic estimate of reward: where Thompson sampling, but our methods can be extended to UCB in a straightforward way. Consequently we let the agent have a “warmup period”, in which we systematically try each action in a round robin fashion, for a total of More recently, several methods based on neural tangent kernels (NTK) have been developed [JGH18], In Algorithm 1, we give the pseudocode for a way to estimate the expected reward for a bandit policy ˜θ∼ p(θ|D), and then predicting the reward for each action and greedily picking the best,hi argmaxEy|s, a,˜θ. In the case of a UCB agent, the action is chosen by ﬁrst computing the α >0 is a tuning parameter that controls the degree of exploration. In this paper, we focus on Since the prior on the parameters is usually uninformative, the initial actions are eﬀectively random. Figure 1: Illustration of some common MLP architectures used in bandit problems. vector, hidden node in layer is a concatentation of structured vector, where we insert zero. belief state to get an informative prior. If we have a long warmup period, then we will have a better initial estimate, but we may incur high regret during this period, since we are choosing actions “blindly”. Thus we can view of the agent (if T is large, we can more easily amortize the cost of a long warmup period). We will assume a Gaussian bandit setting, in which the observation model for the reward is a Gaussian with a ﬁxed or inferred observation variance: Bernoulli bandit case in Section 5.) features. That is, it has the form by a feature extractor, ﬁnal linear layer, with one output “head” per action. For example, in Figure 1a, we show a 2 layer model where the ﬁrst and second layer weights. (We ignore the bias terms for simplicity.) Thus the feature vector that is passed to the ﬁnal linear layer. If the feature vector is ﬁxed (i.e., is not learned), so φ(s) = s, we get a linear model of the form f(s, a; w) = w to get an input of the form many possible actions; in this case, we can represent arms in terms of their features instead of their indices, just as we represent states in terms of their features. In this formulation, the linear output layer returns the predicted reward for the speciﬁed ( the reward vector for each possible action. the result, to get we insert the state feature vector into the block corresponding to the chosen action (see Figure 1c). This approach is used by recent NTK methods. If we assume assume that the MLP has no hidden layers, then this model becomes equivalent to the linear model, since arepresents the action vector,yrepresents the reward vector (for each possible action), andzis thei’th τas a hyperparameter of the algorithm. The optimal value will depend on the expected lifetimeT Many current bandit algorithms assume the reward function is a linear model applied to a set of learned φ(s;V) =ReLU(VReLU(Vs)) is the feature vector, andV∈ RandV∈ Rare An alternative model structure is to concatentate the state vector,φ(s), with the action vetcor,φ(a) Instead of concatenating the state and action vectors, we can compute their outer product and then ﬂatten (a) is a one-hot encoding, we get the block-structured inputx= (0, ··· , 0, φ(s), 0, ··· , 0), where Table 1: Summary of the methods for Bayesian inference considered in this paper. Notation: by the agent in the environment; N: num. epochs over the training data for each run of SGD; example, (feature) layer; linear; D = D In this section, we brieﬂy describe existing inference methods that we will compare to. More details on all methods can be found in the Supplementary Information. These methods diﬀer in the kind of belief state they use to represent uncertainty about the model parameters, and in their mechanism for updating this belief state. See Table 1 for a summary. Linear method expected reward, we can represent the belief state as a Gaussian, updated online using the recursive least squares algorithm, which is a special case of the Kalman ﬁlter (see Appendix A.1 for details). Neural linear method outperformed many other more sophisticated approaches, such as variational inference, on their bandit showdown benchmark. It assumes that the reward model has the form neural linear method computes a point estimate of V by using SGD, and uses Bayesian linear regression to update the posterior over each w The standard solution to this is to store all the past data, and to re-run (minibatch) SGD on all the data at each step. Thus the belief state is represented as b its gradient). every invoke SGD. memory to the last the time reduces to performance, as we will see. LiM2 with Likelihood Matching”. This is an extension of the neural linear method designed to solve the “catastrophic forgetting” that occurs when using a ﬁxed memory buﬀer. The basic idea is to approximate the covariance of N: num. actions;N: size of input feature vector for state and action.N: num. features in penultimate ;V)∈ Ris the hidden state computed by a feature extractor (see Figure 1a for an illustration). The If we just updateVat each step usingD, we run the risk of “catastrophic forgetting” (see Section 2). The time cost isO(TNC), whereNis the number of epochs (passes over the data) at each step, and is the cost of a single forwards-backwards pass through the network (needed to compute the objective and Tsteps. The total time then becomesO(TT NC), whereT=T/Tis the total number of times we The memory cost isO(D+T N), whereNis the size of each input example,x= (s, a). If we limit the In [NZM21], they propose a method called “LiM2”, which stands for “Limited Memory Neural-Linear the old features in the memory buﬀer before replacing them with the new features, computed after updating the network parameters. This old covariance can be used as a prior during the Bayesian linear regression step. step. In practice, the SDP can be solved using an inner loop of projected gradient descent (PGD), which involves solving an eigendecomposition at each step. This takes the number of PGD steps per SGD step. See Appendix A.3 for details. NTK methods the propose a related method called “neural UCB”. Both methods are based on approximating the MLP with a neural tangent kernel or NTK [JGH18]. Speciﬁcally, the feature vector at time at the most recent parameter estimate. They use a linear Gaussian model on top of these features. The network parameters are re-estimated at each step based on all the past data, and then the method eﬀectively performs Bayesian linear regression on the output layer (see Appendix A.4 for details). A natural alternative to just modeling uncertainty in the ﬁnal layer weights is to “be Bayesian” about all the network parameters. Since our model is nonlinear, we must use approximate Bayesian inference. In this paper we choose to use the Extended Kalman Filter (EKF), which is a popular deterministic inference scheme for nonlinear state-space models based on linearizing the model (see Appendix A.5 for details). It was ﬁrst applied to inferring the parameters of an MLP in [SW89], although it has not been applied to bandit problems, as far as we know. In more detail, we deﬁne the latent variable to be the unknown parameters θ. The (non-stationary) observation model is given by inputs to the model, and the dynamics model for the parameters is given by We can set time. However in practice we use a small non-zero value for τ , for numerical stability. compute. Modern neural networks often have millions of parameters, which makes direct application of the EKF intractable. We can reduce the memory from using a diagonal approximation to important for good performance (as we show empirically in Section 4). We can improve the approximation by using a block structured approximation, with one block per layer of the MLP, but this still ignores correlations between layers. is to exploit the fact that the DNN pararameters are not independent “degrees of freedom”. Indeed, [Li+18] showed empirically that we can replace the original neural network weights version, parameters optimizing in the d  D explanation for why such a threshold exists, based on geometric properties of the high dimensional loss landscape. a warmup period. Similarly, instead of using a random basis matrix SVD to the iterates of SGD during the warmup period, as proposed in [Izm+19; Lar+21]. (If we wish, we can just keep a subset of the iterates, since consecutive samples are correlated.) These two changes reduce the dimensionality of the subspace cross-validation on the data from the warmup phase to ﬁnd a good value for d.) Computing the updated prior covariance requires solving a semi-deﬁnite program (SDP) after each SGD s, a) = (1/N)∇f(s, a)|, whereNis the width of each hidden layer, and the gradient is evaluated τ= 0 to encode the assumption that the parameters of the reward function are constant over The belief state of an EKF has the formb= (µ, Σ). This takesO(D) space andO(T D) time to In this paper, we explore a diﬀerent approach to scaling the EKF to large neural networks. Our key insight z ∈ R, by deﬁning the aﬃne mappingθ(z) =Az+θ, and then optimizing the low-dimensional is a random initial guess of the parameters (which we call an “oﬀset”). In [Li+18], they show that , provided thatd > d, wheredis a critical threshold. In [Lar+21], they provide a theoretical Instead of using a random oﬀsetθ, we can optimize it by performing SGD in the originalθspace during Once we have computed the subspace, we can perform Bayesian inference for the embedded parameters z ∈ R model with a (non-stationary) observation model of the form deterministic transition model of the form model in Figure 2. warmup phase, but results are worse, as we show in Section 4.) The algorithm takes Empirically we ﬁnd that we can reduce models with (or sometimes better) performance, as we show in Section 4. We can further reduce the time to by using a diagonal covariance matrix, with little change to the performance, as we shown in Section 4. The time cost of the warmup phase is dominated by SVD. If we have exact SVD is O(τD log d + (τ + D)d going beyond this may require the use of a sparse random orthogonal matrix to represent leave this to future work. depends on all of the parameters in the model. By contrast, the neural linear and Lim2 methods assume that the model has a linear ﬁnal layer, and they only capture parameter uncertainty in this ﬁnal layer. Thus these methods cannot be combined with the subspace trick. Algorithm 2: Neural Subspace Bandits instead of the original parametersθ ∈ R. We do this by applying the EKF to the a state-space The overall algorithm is summarized in Algorithm 2. (If we use a random subspace, we can skip the The memory cost isO(d+Dd), since we need to store the belief state,b= (µ, Σ), as well as the oﬀset and theD × dbasis matrixA. We have succesfully scaled this to models with∼1Mparameters, but Note that our method can be applied to any kind of DNN, not just MLPs. The low dimensional vectorz Figure 3: Reward for various methods on 3 tabular datasets. The maximum possible reward for each dataset is 5000. In this section, we present empirical results in which we evaluate the performance (reward) and speed (time) of our method compared to other methods on various bandit problems. We also study the eﬀects of various hyper-parameters of our algorithm, such as how we choose the subspace. To compare ourselves to prior works, we consider a subset of the datasets used in the “Deep Bayesian Bandits Showdown” [RTS18]. These are small tabular datasets, where the goal is to predict the class label given the features. 1 if the correct label is predicted, and is 0 otherwise. Thus the cumulative reward is the number of correct classiﬁcations, and the regret is the number of incorrect classiﬁcations. units and use and report the mean reward, together with the standard deviation. EKF in a random subspace (with full or diagonal covariance), EKF in the original parameter space (with full or diagonal covariance), Linear, Neural-Linear (with unlimited or limited memory), LiM2, and Neural-TS. For the 6 EKF methods, we use our own code. authors. own codebase. All the hyperparameters are the same as in the original papers/code (namely [NZM21] for Linear, Neural-Linear and Lim2, and [Zha+21] for Neural-TS). experiments, which we found to work well.) On the Adult dataset, all methods have similar performance, showng that this is an easy problem. On the Covertype dataset, we ﬁnd that the best method is EKF in a learned (SVD) subspace with full covariance (light blue bar). This is the only method to beat the linear baseline (purple). On the Shuttle (Statlog) dataset, we see that all the EKF subspace variants work well, and match the accuracy of Lim2 while being much faster. (We discuss speed in Section 4.5.) We see that EKF in the original parameter space peforms worse, especially when we use a diagonal approximation (red). We also see that limited memory version of neural linear (light orange) is worse than unlimited memory (dark orange). We turn this into a bandit problem by deﬁning the actions to be the class labels, and the reward is Following prior work, we use the multi-headed MLP in Figure 1a, with one hidden layer withN= 50 ReLUactivations. (The Neural-TS results are based on the multi-input model in Figure 1c.) We N= 20 “pulls” per arm during the warmup phase and run forT= 5000 steps. We run 10 random trials We compare the following 11 methods: EKF in a learned (SVD) subspace (with full or diagonal covariance), For Linear and Neural-Linear methods, we reproduced the original code from the authors in our We show the average reward for each method on each dataset in Figure 3. (We used= 200 for all the error bars. We also noticed this with other examples from the Bandit Showdown benchmark (results not shown). We therefore believe this benchmark is too simple to be a reliable way of measuring performance diﬀerences of neural bandit algorithms (despite its popularity in the literature). In the sections below, we consider more challenging benchmarks, where the relative performance diﬀerences are clearer. One of the main applications of bandits is to recommender systems (see e.g., [Li+10; Guo+20]). Unfortunately, evaluating bandit policies in such systems requires running a live experiment, unless we have a simulator or we use oﬀ-policy evaluation methods such as those in [Li+11]. In this section, we build a simple simulator by applying SVD to the MovieLens-100k dataset, following the example in the TF-Agents library. 943 users on 1682 movies. This deﬁnes a sparse 943 entries. We extract a subset of this matrix corresponding to the ﬁrst 20 movies to get a 943 We then compute the SVD of this matrix, user in context and there are also 20 actions (movies). or 2 hidden layers, with 50 hidden units per layer. Since the Lim2 and NeuralTS code was not designed for this environment, we restrict ourselves to the 9 methods we have implemented ourselves. We show the results in Figure 4. On this dataset we see that the EKF subspace methods perform the best (by a large margin), followed by linear, and then neural-linear, and ﬁnally EKF in the original space (diagonal approximation). We also see that the deeper model (MLP2) performs worse than the shallower model (MLP1) when using the neural linear approximation; we attribute this to overﬁtting, due to not being Bayesian about the parameters of the feature extractor. By contrast, our fully Bayesian approach is robust to using overparameterized models, even in the small sample setting. So far we have only considered low dimensional problems. To check the scalability of our method, we applied it to MNIST, which has 784 input features and 10 classes (actions). In addition to a baseline linear model, we consider three diﬀerent kinds of deep neural network: an MLP with 50 hidden units and 10 linear outputs However, we also see that diﬀerences between most methods are often rather small, and are often within In more detail, we start with the MovieLens-100k dataset, which has 100,000 ratings on a scale of 1–5 from USV. (This is a standard approach to matrix imputation, see e.g., [SJ03; BK07]). We treat each ias a context, represented byu, and treat each moviejas an action; the reward for taking actionjin iisX∈ R. We follow the TF-Agents example and useK= 20, so the context has 20 features, Having created this simulator, we can use it to evaluate various bandit algorithms. We use MLPs with 1 Figure 6: Reward vs dimensionality of the subspace on (a) Adult, (b) Covertype. Blue estimates the subspace using SVD, orange uses a random subspace. (MLP1, with (MLP2 with [LeC+98] with D = 61, 706 parameters. Figure 5). Furthermore, for any given model, we see that our EKF-subspace method outperforms the widely used neural-linear method, even though the latter has unlimited memory (and therefore potentially takes O(T this size of subspace, there is not a big diﬀerence between using an SVD subspace and a random subspace. However, using a full covariance in the subspace works better than a diagonal covariance (compare blue bars with the green bars). We see that all subspace methods work better than the neural linear baseline. In the original parameter space, a full covariance is intractable, and EKF with a diagonal approximation (red bar) works very poorly. A critical component of our approach is how we estimating the parameter subspace matrix explained in Section 3.4, we have two diﬀerent approaches for computing this: randomly or based on SVD applied to the parameter iterates computing by gradient descent during the warmup phase. We show the performance vs some tabular datasets. We see two main trends: SVD is usually much better than random, especially in low Not surprisingly, we ﬁnd that the CNN works better than MLP2, which works better than MLP1 (see ) time). For this experiment, we use a subspace dimensionality ofd= 470 (chosen using a validation set). With Figure 8: Running time (CPU seconds) for 5000 steps using various methods on MNIST. Note the vertical axis is logarithmic. dimensions; and performance usually increases with in performance with increasing dimensionality is odd, but is consistent with the results in [Lar+21], who noticed exactly the same eﬀect. We leave investigating the causes of this to future work. One aspect of bandit algorithms that has been overlooked in the literature is their time and space complexity, which is important in many practical applications, like recommender systems or robotic systems, that may run indeﬁnitely (and hence need bounded memory) and need a fast response time. We give the asymptotic complexity of each method in Table 1. In Figure 7, we show the empirical wall clock time for each method when applied to the MovieLens dataset. We see the following trends: Neural-linear methods (orange) are the slowest, with the limited memory version usually being slightly faster than the unlimited memory version, as expected. The EKF subspace methods are the second slowest, with SVD slightly slower than RND, and full covariance (blue) slower than diagonal (green). Finally, the fastest method is diagonal EKF in the original parameter space; however, the performance (expected reward) of this method is poor. It is interesting to note that our subspace models are faster than the linear baseline; this is because we only have to invert a d × d matrix, instead of inverting N The relative performance trends (when viewed on a log scale) are similar to the MovieLens case. However, the linear baseline is much slower than most other methods, since it works with the 784-dimensional input Figure 7: Running time (CPU seconds) for 5000 steps using various methods on MovieLens. In Figure 8, we show the empirical wall clock time for each method when applied to the MNIST dataset. features, whereas the neural methods work with lower dimensional latent features. We also see that the neural linear method is quite slow, especially when applied to CNNs, and even more so in the unlimited memory setting. (We could not apply Lim2 to MNIST since the code is designed for the tabular datasets in the showdown benchmark.) bandit methods store the entire past history of observations, to avoid catastrophic forgetting. If we limit SGD updates of the feature extractor to a window of the last e.g., Figure 3). The Lim2 method attempts to solve this, but is very slow, as we have seen. Our subspace EKF method is both fast and memory eﬃcient. We have shown that we can perform eﬃcient online Bayesian inference for large neural networks by applying the extended Kalman ﬁlter to a low dimensional version of the parameter space. In future work, we would like to apply the method to other sequential decision problems, such as Bayesian optimization and active learning. We also intend to extend it to Bernoulli and other GLM bandits [Fil+10]. Fortunately, we can generalize the EKF (and hence our method) to work with the exponential family, as explained in [Oll18]. more tractable, which could increase their use. We view this as a positive thing, since Bayesian methods can express uncertainty, and may be less prone to making conﬁdent but wrong decisions [Bha+21]. However, we acknowledge that bandit algorithms are often used for recommender systems and online advertising, which can have some unintended harmful societal eﬀects [MTF20]. We would like to thank Luca Rossini, Alex Shestopaloﬀ and Eﬁ Kokiopoulou for helpful comments on an earlier draft of the paper. In addition to time constraints, memory is also a concern for long-running systems. Most online neural Finally, a note on societal impact. Our method makes online Bayesian inference for neural networks In this section, we discuss how to do belief updating for a linear bandit, where the reward model has the form augmenting the input features single arm. In practice, this procedure is repeated separately for each arm, using the contexts and rewards for the time periods where that arm was used. For now, we assume the observation noise where the times), and let on the warmup data by applying Bayes rule to the uninformative prior to get formula for rank one updating to eﬃciently compute the new covariance, without any matrix inversions: To compute the mean, we will assume µ special case of the Kalman ﬁlter (see e.g., [Bor16] for the derivation). The updates are as follows: (Of course, we only update the belief state for the arm that was actually pulled at time t.) Now we consider the case where represent uncertainty in the reward for each action, which will increase the dynamic range of the sampled parameters, leading to more aggressive exploration. We have noticed this gives improved results over ﬁxing ;θ) =ws, whereθ=Ware the parameters. (We ignore the bias term, which can be accomodated by µ=0is the prior mean andΣ= (1/)Iis the prior covariance for some small >0. LetXbe N × Nmatrix of contexts for this arm during the warmup period (soN=Nif we pull each armN After this initial batch update, we can perform incremental updates. We can use the Sherman-Morrison An alternative (but equivalent) approach is to use the recursive least squares (RLS) algorithm, which is a follows, where X is all the contexts for this arm up to t, and y is all the rewards for this arm up to t: It is natural to want to derive a version of these equations which avoids the matrix inversion at each step. We can incrementally update However, computing where V = σ Let λ = 1/V be the observation precision. To start the algorithm, we use the following prior: where step. We assume that the prior belief state at time t − 1 is We will use a conjugate normal inverse Gamma priorNIG(w, σ|µ, Σ, a, b). The batch update is as We can rewrite the above equations in incremental form as follows: To describe this algorithm, let the likelihood at time t be deﬁned as follows: τis the prior mean forσ, andν>0 is the strength of this prior. We now discuss the belief updating where If we marginalize out V , the marginal distribution for z sampling, it is simpler to sample The neural linear model assumes that approximates the posterior over all the parameters by using a point estimate for for each w where catastrophic forgetting, we also need to store all of the previous observations, so the belief state has the form updating W, using the following equations where we deﬁne iwas taken, and Algorithm 3 for the pseudocode. In this section, we describe the LiM2 method of [NZM21]. It is similar to the neural linear method except that the prior ( SGD is only applied to a rolling window of the last bounded. See Algorithm 4 for the pseudocode. avoid catastrophic forgetting. optimize the new covariance. θ= (V, W, a, b) are all the parameters, andδ(u) is a delta function. Furthermore, to avoid D,ˆV, µ, Σ, a, b). The neural network parameters are computed using SGD. After ˆV, we update the parameters of the Normal-Inverse-Gamma distribution for the ﬁnal layer weights See Algorithm 5 for the pseudocode for the step that updates the DNN and the prior on the last layer, to See Algorithm 6 for the projected gradient descent (PGD) step, which solves a semi deﬁnite program to Algorithm 3: Neural Linear. = Environment.GetState(t) ; ∼ InverseGamma(a, b) for all i ; ˜w∼ N(µ, ˜σΣ) for all i; = argmax˜wφ(s; V) ; = Environment.GetReward(s, a) ; = (s, a, y) ; Algorithm 4: LiM2 = Environment.GetState(t) ; ∼ IG(a, b) for all i ; ˜w∼ N(µ, ˜σΣ) for all i; = argmax˜wφ(s; V) ; = Environment.GetReward(s, a) ; = (s, a, y) ; = push(D) ; steps do = wfor each i ; In this section, we discuss the “Neural Thompson Sampling” method of [Zha+21]. We follow the presentation of [LA21], that shows the connection with linear TS. Recall from Appendix A.1.1 that the posterior over the parameters is given by The induced posterior predictive distribution over the reward is given by which is the gradient of the neural net (an MLP with posterior predictive distribution for the reward becomes First consider the linear modelr=xw. We assumeσis ﬁxed,Σ=κI,µ=0, andλ=. Now consider the NTK case. We replace xwith where and we initialize with greedy action is chosen. In this section, we describe the extended Kalman ﬁlter (EKF) formulation in more detail. Consider the following nonlinear Gaussian state space model: where following Jacobian matrices: (These terms are easy to compute using standard libraries such as JAX.) The updates then become (In the case of Bernoulli bandits, we can use the exponential family formulation of the EKF discussed in [Oll18].) approximation is to use a block diagonal approximation. Let us deﬁne the following Jacobian matrices for block i: We then compute the following updates for each block: z∈ Ris the hidden state,y∈ Ris the observation,f:R→ Ris the dynamics model, and :R→ Ris the observation model. The EKF linearizes the model at each step by computing the The cost of the EKF isO(NN), which can be prohibitive for large state spaces. In such cases, a natural the dynamics model f where updates become This is called the “decoupled EKF” [PF91; PF03]. Now we specialize the above equations to the setting of this paper, where the latent state isz=θ, and x= (s, a). We setR=σ, andQ=I, to allow for a small amount of parameter drift. The EKF To match the notation in [PF03], let us deﬁneP=Σ,w=µ,A=S,ˆH=H. (Note that is a N× Nmatrix, so is a scalar if y∈ R.) Then we can rewrite the above as follows: