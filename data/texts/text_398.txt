Reinforcement Learning (RL) is increasingly used across many ﬁelds to create agents that learn via interaction and reward feedback. In many such cases, we rely on experts to perform certain tasks, integrating their knowledge to improve learning efﬁciency and overall performance. Imitation Learning (IL, Hussein et al. [2017]) is concerned with learning via expert demonstrations without access to a reward function. Similarly, RL settings often use expert data to boost performance, eliminating the need to learn from scratch. In this work we consider the IL and RL paradigms in the presence of partially observable expert data. While expert demonstration data is useful, in many realistic settings such data may be prone to hidden confounding [Gottesman et al., 2019], i.e., there may be features used by the expert which are not observed by the learning agent. This can occur due to, e.g., privacy constraints, continually changing features in ongoing production pipelines, or when not all information available to the human expert was recorded. As we show in our work, covariate shift of unobserved factors between the expert data and the real world may lead to signiﬁcant negative impact on performance, frequently rendering the data useless for imitation (see Figure 1 and Theorem 2). In this paper we deﬁne the tasks of imitation and reinforcement learning using expert data with unobserved confounders and possible covariate shift. We focus on a contextual MDP setting [Hallak et al., 2015], where a context is sampled at every episode from some distribution, affecting both the reward and the transition between states. We assume that the agent has access to additional expert data, generated by an optimal policy, for which the sampled context is missing, yet is observed in the online environment. Shie MannorGal ChechikUri Shalit Technion University &Nvidia ResearchTechnion University We consider the problem of using expert data with unobserved confounders for imitation and reinforcement learning. We begin by deﬁning the problem of learning from confounded expert data in a contextual MDP setup. We analyze the limitations of learning from such data with and without external reward, and propose an adjustment of standard imitation learning algorithms to ﬁt this setup. We then discuss the problem of distribution shift between the expert data and the online environment when the data is only partially observable. We prove possibility and impossibility results for imitation learning under arbitrary distribution shift of the missing covariates. When additional external reward is provided, we propose a sampling procedure that addresses the unknown shift and prove convergence to an optimal solution. Finally, we validate our claims empirically on challenging assistive healthcare and recommender system simulation tasks. Figure 1: Failure of using confounded expert data under context distribution mismatch between online environment and expert data. Caregiver does not learn to perform well in a dressing task when covariate shift of hidden confounders is present but not accounted for. We begin by analyzing the imitation-learning problem, (i.e., without access to reward) in Section 3. Under no covariate shift in the unobserved context, we characterize a sufﬁcient and necessary set of optimal policies. In contrast, we prove that in the presence of a covariate shift, if the true reward depends on the context, then the imitation-learning problem is non-identiﬁable and prone to catastrophic errors (see Section 3.2 and Theorem 2). We further analyze the RL setting (i.e., with access to reward and confounded expert data) in Section 4. Figure 1 depicts a possible failure case of using confounded expert data with unknown covariate shift in a dressing task. Unlike the imitation setting, we show that optimality can still be achieved in the RL setting while using confounded expert data with arbitrary covariate shift. We use a corrective data sampling procedure and prove convergence to an optimal policy. Our contributions are as follows. (1) We introduce IL and RL with hidden confounding and prove fundamental characteristics w.r.t. covariate shift and the feasibility of imitation. (2) In the RL setting, under arbitrary covariate shift, we provide a novel algorithm with convergence guarantees which uses a corrective sampling technique to account for the unknown context distribution in the expert data. (3) Finally, we conduct extensive experiments on recommender system [Ie et al., 2019] and assistive-healthcare [Erickson et al., 2020] environments, demonstrating our theoretical results, and suggesting that confounded expert data can be used in a controlled manner to improve the efﬁciency and performance of RL agents. Online Environment. M = (S, X , A, P, r, ρ S × S × A × X 7→ [0, 1] dependent reward function, and ρ: X 7→ [0, 1] and an initial state distribution ν : S × X 7→ [0, 1]. The environment initializes at some context s∈ Sand an agent selects an action transitions to state s We deﬁne a Markovian stationary policy probability. We deﬁne the value of a policy where Edenotes the expectation induced by the policy π. We denote by Π the set of all Markovian policies and Π the set of deterministic Markovian policies. We deﬁne the optimal value and policy by π∈ arg max to denote the set of optimal policies in v(π), respectively. Whenever appropriate, we simplify notation and writev, π. We useΠ policies Π We later use this set to show impossibility of imitation under arbitrary covariate shift and a context-independent transition function. Expert Data with Unobserved Confounders. ing of expert trajectories the dataset were sampled i.i.d. from the marginalized expert distribution (under possible context covariate shift) over contexts. Importantly, Notice that it is assumed that though it is missing in the data. In this work, we consider two settings: In both settings we aim to ﬁnd a context-dependent policy which maximizes the cumulative reward. The confounding factor here is w.r.t. the unobserved context and distribution ρ Marginalized Stationary Distribution. x ∈ Xbyd(s, a|x) = (1−γ) induced byπ. Similarly, given a distribution over contexts, we deﬁne the marginalized stationary distribution of a policy π under the corresponding context distribution by A Causal Perspective. believe it is essential to bridge the gap between these two ﬁelds, and include an interpretation of our model using CI terminology in Appendix D, where we equivalently deﬁne our objective as an intervention over the unknown distribution ρin a speciﬁed Structural Causal Model. In this section, we analyze the problem of confounded imitation learning, namely, learning from expert trajectories with hidden confounders and without reward. Similar to previous work, we consider the task of imitation learning from expert data in the setting where the agent is allowed to interact with the environment [Ho and Ermon, 2016, Fu et al., 2017, Kostrikov et al., 2019, Brantley et al., 2019]. In the ﬁrst part of this section we assume no covariate shift between the online environment and the data is present, i.e., on the covariate shift of the hidden confounders. We ﬁrst consider the scenario in which no covariate shift is present between the ofﬂine data and the online environment, i.e., ρ= ρ. We begin by deﬁning the marginalized ambiguity set, a central component of our work. Deﬁnition 1 marginalized stationary distributions of π by as the set , a, . . . , s) =Pρ(x)ν(s|x)QP (s|s, a, x)π(a|s, x),whereρis some distribution context distribution ρ) as well as real environment (S, X , A, P, ρ, ν, γ), without access to reward. cess to confounded expert data (with context distributionρ) as well as real environment M = (S, X , A, P, r, ρ, ν, γ), with access to reward. (Ambiguity Set).For a policyπ ∈ Π, we deﬁne the set of all deterministic policies that match the Recall that, in general, deterministic policies that cannot be distinguished from shows that for any policy optimal, while the set is the smallest set of candidate optimal policies. Theorem 1. π6= π, then there exists r The above theorem shows that any policy in expert used, no policy in the imitation solution is uniquely deﬁned by the set catastrophic policy. Nevertheless, as we show in the following proposition, acting uniformly w.r.t. the worst policy in the set, i.e., robust to the ambiguity set (see Appendix F for proof). Proposition 1. Then, v(¯π) ≥ α Remark 1. Note that ¯π is generally not the average policy Remark 2.In an episodic setting, episode, and playing it until the environment terminates. We provide a practical algorithm in Appendix B which calculates the ambiguity set with computational guarantees, showing that analyze a more challenging scenario, for which problem. Next, we assume covariate shift exists between the online environment and the expert data, i.e., without further assumptions on the extent of covariate shift, we show two extremes of the problem. In Theorem 2 we prove that whenever the transitions are independent of the context, the data cannot in general be used for imitation. In contrast, in Theorem 3 we prove that, whenever the reward is independent of the context, the imitation problem can be efﬁciently solved. Clearly, ifSupp(ρ We therefore assume throughout that as those that cannot be distinguished from their respective stationary distributions without information on ρ Deﬁnition 2. for all i 6= j. Next, focusing on catastrophic policies (recall Equation (1)), we deﬁne catastrophic expert policies as those which could be either optimal or catastrophic under ρ For a distribution P we denote by Supp(P) the support of P. We deﬁne non-identiﬁability in Deﬁnition 2. We use a similar notion of identiﬁability as in Pearl [2009a] TransitionBounded Confounding(Appendix C)Reward (Theorem 2)(Theorem 3) Figure 2: A spectrum for the difﬁculty of confounded imitation with covariate shift. Υis indiscernible fromΥ, i.e.,Υ= Υ(see Appendix F for proof). In other words,Υ [Sufﬁciency ofΥ] Assumeρ≡ ρ.Letπ∈ Πand letπ∈ Υ. Then,Υ= Υ. Moreover, if Deﬁne the mean policy¯π(a|s, x) =,and denoteα=∈ [0, 1]. v+ (1 − α) minv(π).P ) 6⊆ Supp(ρ)then there existsx ∈ Supp(ρ)for whichπis not identiﬁable from the expert data. We say that{π}are non-identiﬁable policies if there exist{ρ}such thatd(s, a) = d(s, a) We say that{π}are catastrophic expert policies if there exist{r}such that for alli,π∈ Π, Algorithm 1 RL using Expert Data with Unobserved Confounders (Follow the Leader) 1: input: Expert data with missing context D 2: init: Policy π 3: for k = 1, . . . do Using the fact that both ofx, one could ﬁnd two policies which are non-identiﬁable, catastrophic expert policies (see Appendix F for proof). In other words, in the case of context-independent transitions, without further information on useless for imitation. Furthermore, attempting to imitate the policy using the expert data could result in a catastrophic policy. While Theorem 2 shows the impossibility of imitation for context-free transitions, whenever the reward is independent of the context, the imitation problem becomes feasible. In fact, as we show in the following theorem, for context-free rewards, any policy in Υ Theorems 2 and 3 suggest that the hardness of the imitation problem under covariate shift lies on a wide spectrum (as depicted in Figure 2). While dependence of the transition in the expert data, the dependence of the reward problem. Both of these results are concerned with arbitrary confounding. For the interested reader, we further analyze the case of bounded confounding in Appendix C. We also demonstrate the effect of bounded confounding in Section 5. In the following section, we show that, while arbitrary confounding may result in catastrophic results for the imitation learning problem, when coupled with reward, one can still make use of the expert data. In the previous section we showed sufﬁcient conditions under which imitation is possible, with and without covariate shift. When covariate shift is present, but unknown, the imitation learning problem may be hard, or even impossible (see Theorem 2, catastrophic imitation). We ask, had we had access to the reward function, would the expert data be useful under arbitrary covariate shift? In this section we show that expert data with unobserved confounders can be used to converge to an expert policy, even when arbitary covariate shift is present. In our experiments (Section 5) we empirically show that using our method can also improve overall performance. We view the confounded expert data as side information to the RL problem. Speciﬁcally, we assume access to the true reward signal in the online environment and wish to leverage the ofﬂine expert data to aid the agent in converging to an optimal policy. To do this, we deﬁne an optimization problem that maximizes the cumulative reward, while minimizing an f-divergence (e.g., KL-divergence, TV-distance, χ Here,λ > 0and is an optimal policy background on the variational form of by Nachum et al. [2019]: where fis the convex conjugate of f, i.e., f g+ E(FTL Cost Player) [Catastrophic Imitation] Assume|X | ≥ |A|, andP (s|s, a, x) = P (s|s, a, x)for allx, x∈ X. Then , π} are non-identiﬁable, catastrophic expert policies. [Sufﬁciency of Context-Free Reward] AssumeSupp(ρ) ⊆ Supp(ρ)andr(s, a, x) = r(s, a, x)for all Dis thef-divergence, wherefis a convex functionf : (0, ∞) 7→ R. The solution to Problem (P1) π∈ Πas long asρ≡ ρ. RewritingDusing its variational form (see Appendix A for maxminE[r(s, a, x) + λg(s, a)] − λE[f Algorithm 2 RL using Expert Data with Unobserved Confounders (Online Gradient Descent) 1: input: Expert data with missing context, λ, B, N > 0, policy optimization algorithm ALG-RL 2: init: Policy π 3: for k = 1, . . . do Unfortunately, when covariate shift exists (i.e., optimal policy (Theorem 2). Instead, we propose to reformulate Problem (P1b) using a distribution the f-divergence, as follows, Here,B(X )denotes the set of probability measures on the Borel sets of Indeed, whenever Problems (P1) and (P1b), Problem (P2) can achieve an optimal solution to the RL problem which still uses the expert data. Corrective Trajectory Sampling (CTS). d(s, a). Fortunately, expectation over unobserved contexts, as shown by the following proposition (see Appendix F for proof): Proposition 2. Π, g : S × A 7→ R Proposition 2 allows us to estimate the inner minimization problem over Particularly, we uniformly sample k distributions p which can be estimated by using the variational form of Trajectory Sampling (CTS), as it uses complete trajectory samples to account for the unknown context distribution Solving Problem (P2). lem (P2). It uses alternative updates of a cost player (line 5) and policy player (line 6). In line 5 the gradient of w.r.t.dis taken using a Follow the Leader (FTL) cost player to estimate the next bonus iterate. Finally, in line 6, an efﬁcient, approximate policy optimization algorithm theorem, provides convergence guarantees for Algorithm 1 with an approximate best response RL-algorithm (see Appendix F for proof based on Zahavy et al. [2021]). Theorem 4. =. Then, Algorithm 1 will converge to an -optimal solution to Problem (P2) in O Notice that, while Theorem 4 shows Algorithm 1 converges to an optimal policy, it does not determine whether the expert data improves overall learning efﬁciency. We leave this theoretical question for future work. Nevertheless, in the following section we conduct extensive experiments to show that such data can indeed improve overall performance on various tasks. A drawback of Algorithm 1 is that it needs to estimate the stationary distributions. A practical implementation of Algorithm 1 using online gradient descent (OGD) is provided in Algorithm 2 – the algorithm does not require , bonus reward network g according to ∇L(θ) =∇[f(g(s, a)) − g(s, a)] maxminE[r(s, a, x) + λg(s, a)] − λE[f Supp(ρ) ⊆ Supp(ρ), we have that(π, ρ) = (π, ρ)is a solution to Problem (P2). That is, unlike [Trajectory Sampling Equivalence] Letρwhich minimizes Problem (P2) for someπ ∈ (1 − γ)γ1(s, a) = (s, a). LetALG-RLbe an approximate best response player that solves the RL problem in iterationkto accuracy Figure 3: Plots compare training curves of using CTS vs. normal sampling of expert data for small (β = 0.3) and large (β = 0.8) covariate shift bias in four assistive-healthcare tasks. Dashed black lines show expert and RL (without data) scores. Runs were averaged over 5 seeds. Legend is shared across all plots. approximate estimates of the stationary distributions, but rather, only the ability to sample from them. Similar to Algorithm 1, we use CTS (see Equation (2)) to estimate drawn from the current policy as well as samples from neural network representation for OGD. Finally, the policy is updated using ALG-RL and an augmented reward. We tested our proposed approach for using expert data with hidden confounding in recommender-system and assistivehealthcare environments. For all our experiments we used to work best. Comparison to other divergences is provided in Figure 4 (left). We used PPO [Schulman et al., 2017] implemented in RLlib [Liang et al., 2018] for both the imitation as well as RL settings. We include speciﬁc choice of hyperparameters and an exhaustive overview of further implementation details in Appendix E. Assistive Healthcare. set of tasks for assistive-healthcare, simulating autonomous robots as versatile caregivers [Erickson et al., 2020]. Each task has a unique goal, affected by both the physical world as well as the patient speciﬁc preferences and disabilities. We tested our algorithm on four tasks: feeding, dressing, bathing, and drinking. In these, we used the following features to deﬁne user context: gender, mass, radius, height, patient impairment, and patient preferences. The patient’s mass, radius, and height distributions were dependent on gender. The patient’s impairment was given by either limited movement, weakness, or tremor (with sporadic movement). Finally, the patient’s preferences were affected by the velocity and pressure of touch forces applied by the robot. For the context distribution provided by the original environment. To enforce a distribution shift in the expert data, we shifted each distribution randomly with an additive factor Figure 4: Left plot shows comparison of different choices of shift) on the BedBathingPR2 environment. Middle plot depicts execution of imitation with hidden confounding (without reward) for different levels of covariate shift. Right plot compares our CTS correction on the RecSim environment with strong covariate shift bias. All runs were averaged over 5 seeds. distributions (see Appendix E). Here ﬁxed policy trained using a dense reward function. A sparse reward signal was used for executing our experiments with the confounded expert data. For further details, we refer the reader to Appendix E. Figure 3 depicts results for executing Algorithm 2 on four assistive-gym environments with various covariate shift strengths. As evident in most of the enviornments, covariate shift strongly affected overall performance. Particularly in the feeding, drinking, and dressing environments, the success of reaching the goal (i.e., spoon to mouth, cup to mouth, and sleeve to hand) was highly affected by the degree of covariate shift. This is due to the changing distribution of size, movement, and preferences of the patient, and thus of the goal. Nevertheless, in all environments, using the expert data (with and without CTS) was found to help induce better policies than executing the same RL algorithm without expert data. This suggests that expert data can assist in improving overall RL performance, yet correcting for covariate shift may signiﬁcantly improve it in these domains. Recommender Systems. for optimizing user long-term engagement and overall satisfaction [Ie et al., 2019]. Leveraging expert data collected using, e.g., surveys to users, may greatly beneﬁt future solutions. Because features are repeatedly added to these systems, full information in the data is rarely available. Here, we use the recently proposed RecSim Interest Evolution environment [Ie et al., 2019], simulating sequential user interaction with a slate-based recommender system. The environment consists of a document model for sampling documents, a user model for deﬁning a distribution over user context features, and a user choice model, which deﬁnes the intent of the user based on observable document features and the user’s sampled context (e.g., personality, satisfaction, interests, demographics, and other behavioral features such as session length or visit frequency). We used a slate of 10 documents and a user context of dimension 20. To test the severity of the implications of Theorem 2 in the confounded imitation setting, we used a user-model sampled from a Beta-distribution. Speciﬁcally, for the expert data the user context features andα= 1.5 + α= (1 − β) used a uniform distribution to generate user contexts, the Beta-distribution let us analyze severe forms of covariate shifts, testing the limits of our results in Sections 3 and 4. Still, we emphasize that both the original (uniform) distribution and the Beta distribution are not necessarily realistic user distributions. Figure 4(a) depicts the effect of increased covariate shift on imitation in the RecSim environment with a dataset of 100 expert trajectories (generated by an optimal policy that had access to the full context). Without covariate shift ( an optimal score is achieved, and as reached (β = 1 has in fact reached a catastrophic one, as shown by the orange plot. Conversely, Figure 4(b) depicts the beneﬁt of using confounded expert data in the RL setting, i.e. when an online reward signal is available. Though strong confounding is present, the agent is capable of leveraging the data to improve overall learning performance. i. In contrast, the online environment features were sampled from a shifted Beta-distribution with 1.5 +i+ β10 −i, whereβ ∈ [0, 1]deﬁned the shift strength. While the original environment ), a catastrophic policy is reached. While the imitator “believes" to have reached an optimal policy, it Imitation Learning. 1989, Bratko et al., 1995] as well as online setting [Ho and Ermon, 2016, Fu et al., 2018, Kim and Park, 2018, Brantley et al., 2019]. Speciﬁc to our work are GAIL [Ho and Ermon, 2016], AIRL [Fu et al., 2017], and DICE [Kostrikov et al., 2019], which use distribution matching methods. Our work generalizes these settings to imitation with hidden confounders. Reinforcement Learning with Expert Data. ofﬂine RL [Levine et al., 2020] has shown great improvement over regular ofﬂine imitation techniques [Kumar et al., 2020, Kostrikov et al., 2021, Tennenholtz et al., 2021a, Fujimoto and Gu, 2021]. In the online RL setting, the combination of ofﬂine data to improve RL efﬁciency has shown great success [Nair et al., 2020]. KL-regularized techniques [Peng et al., 2019, Siegel et al., 2019] as well as DICE-based algorithms [Nachum et al., 2019] have also shown efﬁcient utilization of ofﬂine data. Our work generalizes the latter to the confounded setting. Intersection of Causal Inference and Imitation Learning. There, the authors suggest a notion of imitability, showing when observational data can help identify a policy under some partially observed structural causal model. Our work provides an alternative perspective on the problem. In contrast to their work, we rely on concurrent imitation approaches (i.e. stationary distribution matching techniques) and importantly, allow access to the online environment. Furthermore, we provide guarantees and practical algorithms for both the imitation and RL settings. We refer the reader to Appendix D for a speciﬁc interpretation of our framework in CI terminology, from a perspective of stochastic interventions in a Structural Causal Model. Another intersection with causal inference discusses the problem of causal confusion in imitation [de Haan et al., 2019]. Causal confusion is concerned with the problem of nuisances in observed confounded data due to an unknown causal structure. These “causal misidentiﬁcations” can lead to spurious correlations and catastrophic failures in generalization. In contrast, our work discusses the orthogonal problem of hidden confounders with possible covariate shift. Intersection of Causal Inference and Reinforcement Learning. control from logged data with unboserved confounders [Lattimore et al., 2016], as well as utilizing (non-expert) confounded data for online interactions [Tennenholtz et al., 2021b]. Much work has revolved around the reinforcement learning setup with access to (non-expert) confounded data [Zhang and Bareinboim, 2019, Wang et al., 2020]. Other work has considered the problem of off-policy evaluation from confounded data [Tennenholtz et al., 2020, Oberst and Sontag, 2019, Kallus and Zhou, 2020]. Our work is focused on leveraging expert data with hidden confounders and possible covariate shift in both the imitation and the RL settings. This work presented and analyzed the problem of using expert data with hidden confounders for both the imitation and RL settings. We showed that covariate shift of hidden confounders between the expert data and the online environment can result in learning catastrophic policies, rendering imitation learning hard, or even impossible (Theorem 2). In addition, we showed that when a reward is provided, using the expert data is still possible under arbitrary hidden covariate shift (Theorem 4). We proposed new algorithms for tackling this problem using corrective trajectory sampling (CTS). Our empirical evaluation demonstrates our results and suggests that taking the possibility of unknown covariate shift into account may signiﬁcantly improve overall performance. The imitation learning problem has been extensively studied in both the fully ofﬂine [Pomerleau,