Selecting the optimal recommender via online exploration-exploitation is catching increasing attention where the traditional A/B testing can be slow and costly, and oine evaluations are prone to the bias of history data. Finding the optimal online experiment is nontrivial since both the users and displayed recommendations carry contextual features that are informative to the reward. While the problem can be formalized via the lens of multi-armed bandits, the existing solutions are found less satisfactorily because the general methodologies do not account for the case-specic structures, particularly for the e-commerce recommendation we study. To ll in the gap, we leverage the D-optimal design from the classical statistics literature to achieve the maximum information gain during exploration, and reveal how it ts seamlessly with the modern infrastructure of online inference. To demonstrate the eectiveness of the optimal designs, we provide semi-synthetic simulation studies with published code and data for reproducibility purposes. We then use our deployment example on Walmart.com to fully illustrate the practical insights and eectiveness of the proposed methods. • Information systems → Retrieval models and ranking;• Computer systems organization →Real-time systems;• Mathematics of computing → Statistical paradigms. Recommender system; Multi-armed bandit; Exploration-exploitation; Optimal design; Deployment infrastructure ACM Reference Format: Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. 2021. Towards the D-Optimal Online Experiment Design for Recommender Selection. In Procee dings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’21), August 14–18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 10 pages. https://doi. org/10.1145/3447548.3467192 Developing and testing recommenders to optimize business goals are among the primary focuses of e-commerce machine learning. A crucial discrepancy between the business and machine learning world is that the target metrics, such as gross merchandise value (GMV), are dicult to interpret as tangible learning objectives. While a handful of surrogate losses and evaluation metrics have been found with particular empirical success [20, 44], online experimentation is perhaps the only rule-of-thumb for testing a candidate recommender’s real-world performance. In particular, there is a broad consensus on the various types of bias in the collected history data [11], which can cause the "feedback-loop eect" if the empirical metrics are used without correction [18]. Recently, there has been a surge of innovations in rening online A/B testings and correcting oine evaluation methods [18,29,47,52]. However, they still fall short in specic applications where either the complexity of the problem outweighs their potential benets, or their assumptions are not satised. Since our work discusses the e-commerce recommendations primarily, we assume the online shopping setting throughout the paper. On the A/B testing side, there is an increasing demand for interleaving the tested recommenders and targeted customers due to the growing interests in personalization [43]. The process of collecting enough observations and drawing inference with decent power is often slow and costly (in addition to the complication of dening the buckets in advance), since the number of combinations can grow exponentially. As for the recent advancements for oine A/B testing [18], even though certain unbiasedness and optimality results have been shown in theory, the real-world performance still depends on the fundamental causal assumptions [22], e.g. unconfounding, overlapping and identiability, which are rarely fullled in practice [49]. We point out that the role of both online and oine testings are irreplaceable regardless of their drawbacks; however, the current issues motivate us to discover more ecient solutions which can better leverage the randomized design of trac. The production scenario that motivates our work is to choose from several candidate recommenders who have shown comparable performances in oine evaluation. By the segment analysis, we nd each recommender more favorable to specic customer groups, but again the conclusion cannot be drawn entirely due to the exposure and selection bias in the history data. In other words, while it is safe to launch each candidate online, we still need randomized experiments to explore each candidates’ real-world performance for dierent customer groups. We want to design the experiment by accounting for the customer features (e.g. their segmentation information) to minimize the cost of trying suboptimal recommenders on a customer group. Notice that our goal deviates from the traditional controlled experiments because we care more about minimizing the cost than drawing rigorous inference. In the sequel, we characterize our mission as a recommender-wise exploration-exploitation problem, a novel application to the best of our knowledge. Before we proceed, we illustrate the fundamental dierences between our problem and learning the ensembles of recommenders [24]. The business metrics, such as GMV, are random quantities that depend on the recommended contents as well as the distributions that govern customers’ decision-making. Even if we have access to those distributions, we never know in advance the conditional distribution given the recommended contents. Therefore, the problem can not be appropriately described by any xed objective for learning the recommender ensembles. In our case, the exploration-exploitation strategy can be viewed as a sequential game between the developer and the customers. In each round𝑡 =1, . . . , 𝑛, where the role of𝑛will be made clear later, the developer chooses a recommender𝑎∈ {1, . . . , 𝑘}that produces the content𝑐, e.g. top-k recommendations, according to the frontend request𝑟, e.g. customer id, user features, page type, etc. Then the customer reveal the reward𝑦such as click or not-click. The problem setting resembles that of the multi-armed bandits (MAB) by viewing each recommender as the action (arm). The front-end request𝑟, together with the recommended content𝑐= 𝑎(𝑥), can be think of as the context. Obviously, the context is informative of the reward because the clicking will depend on how well the content matches the request. On the other hand, an (randomized) experiment design can be characterized by a distribution𝜋overÍ the candidate recommenders, i.e. 0< 𝜋(𝑎) <1,𝜋(𝑖) =1 for𝑖 =1, . . . , 𝑛. We point out that a formal dierence between our setting and classical contextual bandit is that the context here depends on the candidate actions. Nevertheless, its impact becomes negligible if choosing the best set of contents is still equivalent to choosing the optimal action. Consequently, the goal of nding the optimal experimentation can be readily converted to optimizing 𝜋, which is aligned with the bandit problems. The intuition is that by optimizing𝜋, we rene the estimation of the structures between context and reward, e.g. via supervised learning, at a low exploration cost. The critical concern of doing exploration in e-commerce, perhaps more worrying than the other domains, is that irrelevant recommendations can severely harm user experience and stickiness, which directly relates to GMV. Therefore, it is essential to leverage the problem-specic information, both the contextual structures and prior knowledge, to further design the randomized strategy for higher eciency. We use the following toy example to illustrate our argument. Example 1. Suppose that there are six items in total, and the frontend request consists of a uni-variate user feature𝑟∈ R. The reward mechanism is given by the linear model: where𝐼is the indicator variable on whether item𝑗is recommended. Consider the top-3 recommendation from four candidate recommenders as follow (in the format of one-hot encoding): If each recommender is explored with the probability, the role of𝑎is underrated since it is the only recommender that provides information about𝜃and𝜃. Also,𝑎and𝑎give the same outputs, so their exploration probability should be discounted by half. Similarly, the information provided by𝑆can be recovered by𝑆and𝑆(or𝑆) combined, so there is a linear dependency structure we may leverage. The example is representative of the real-world scenario, where the one-hot encodings and user features may simply be replaced by the pre-trained embeddings. By far, we provide an intuitive understanding of the benets from good online experiment designs. In Section 2, we introduce the notations and the formal background of bandit problems. We then summarize the relevant literature in Section 3. In Section 4, we present our optimal design methods and describe the corresponding online infrastructure. Both the simulation studies and real-world deployment analysis are provided in Section 5. We summarize the major contributions as follow. •We provide a novel setting for online recommender selection via the lens of exploration-exploitation. •We present an optimal experiment approach and describe the infrastructure and implementation. •We provide both open-source simulation studies and realworld deployment results to illustrate the eciency of the approaches studied. We start by concluding our notations in Table 1. By convention, we use lower and upper-case letters to denote scalars and random variables, and bold-font lower and upper-case letters to denote vectors and matrices. We use[𝑘]as a shorthand for the set of:{1,2, . . . , 𝑘}. The randomized experiment strategy (policy) is a mapping from the collected data to the recommenders, and it should maximizeÍ the overall reward𝑌. The interactive process of the online recommender selection can be described as follow. 1. The developer receives a front-end request 𝑟∼ 𝑃. 2. The developer computes the feature representations that combines the request and outputs from all candidate recommender: 3. The developer chooses a recommender𝑎according to the randomized experiment strategy 𝜋 (𝑎|X,®ℎ). 4. The customer reveals the reward 𝑦. In particular, the selected recommender𝑎depends on the request, candidate outputs, as well as the history data: and the observation we collect at each round is given by: We point out that compared with other bandit applications, the restriction on computation complexity per round is critical for realworld production. This is because the online selection experiment is essentially an additional layer on top of the candidate recommender systems, so the service will be called by tens of thousands of front-end requests per second. Consequently, the context-free exploration-exploitation methods, whose strategies focus on the cu-Í mulative rewards:𝑄 (𝑎) =𝑦1[𝑎 = 𝑗]and number of appear-Í ances:𝑁 (𝑎) =1[𝑎 = 𝑗](assume up to round𝑡) for𝑎 =1, . . . , 𝑘, are quite computationally feasible, e.g. • 𝜖-greedy: explores with probability𝜖under the uniform exploration policy𝜋 (𝑎) =1/𝑘, and selectsarg max𝑄 (𝑎)otherwise (for exploitation); • UCB: selectsarg max𝑄 (𝑎) +𝐶𝐼 (𝑎), where𝐶𝐼 (𝑎)characterizes the condence interval of the action-specic reward𝑄 (𝑎), and is given by:log 1/𝛿𝑁 (𝑎) for some pre-determined 𝛿. The more sophisticatedThompson samplingequips the sequential game with a Bayesian environment such that the developer: •selectsarg max˜𝑄 (𝑎), where˜𝑄 (𝑎)is sampled from the posterior distributionBeta(𝛼, 𝛽), and𝛼and𝛽combines the prior knowledge of average reward and the actual observed rewards. For Thompson sampling, it is clear that the front-end computations can be simplied to calculating the uni-variate indices (𝑄,𝑁, 𝛼,𝛽). For MAB, taking account of the context often requires em- ploying a parametric reward model:𝑦= 𝑓𝝓(𝑟, 𝑎(𝑟 )), so during exploration, we may also update the model parameters𝜽using 𝑘, 𝑚, 𝑛menders); the number of top recommendations;the number of exploration-exploitation rounds. 𝑅, 𝐴, 𝑌The frond-end request, action selected by thedeveloper, and the reward at round 𝑡. ®ℎ, 𝜋 (· | ·)randomized strategy (policy) that maps the con-texts and history data to a probability measure I, 𝑎(·)The whole set of items and the𝑖candidaterecommender, with 𝑎(·) ∈ I. 𝝓𝑟, 𝑎(𝑟)sentation inR, specically for the𝑡-roundfront-end request and the output contents for Table 1: A summary of the notations. By tradition, we use uppercase letters to denote random variables, and the corresponding lowercase letters as obser vations. the collected data. Suppose we have an optimization oracle that returnsˆ𝜽by tting the empirical observations, then all the above algorithms can be converted to the context-aware setting, e.g. • epoch-greedy: explores under𝜋 (𝑎) =1/𝑘for a epoch, and selects arg maxˆ𝑦:= 𝑓𝝓(𝑟, 𝑎(𝑟 ))otherwise; • LinUCB: by assuming the reward model is linear, it selects arg maxˆ𝑦+ 𝐶𝐼 (𝑎)where𝐶𝐼 (𝑎)characterizes the condence of the linear model’s estimation; • Thompson sampling: samplesˆ𝜽from the reward-model-specic posterior distribution of 𝜽, and selects arg maxˆ𝑦. We point out that the per-round model parameter update via the optimization oracle, which often involves expensive real-time computations, is impractical for most online services. Therefore, we adopt the stage-wise setting that divides exploration and exploitation (similar to epoch-greedy). The design of𝜋thus becomes very challenging since we may not have access to the most updated ˆ𝜽. Therefore, it is important to take advantage of the structure of 𝑓(·), which motivates us to connect our problem with the optimal design methods in the classical statistics literature. We briey discuss the existing bandit algorithms and explain their implications to our problem. Depending on how we perceive the environment, the solutions can be categorized into the frequentist and Bayesian setting. On the frequentist side, the reward model plays an important part in designing algorithms that connect to the more general expert advice framework [12]. The EXP4 and its variants are known as the theoretically optimal algorithms for the expert advice framework if the environment is adversarial [4,6,35]. However, customers often have a neutral attitude for recommendations, so it is unnecessary to assume adversarialness. In a neutral environment, the LinUCB algorithm and its variants have been shown highly eective [4,12]. In particular, when the contexts are viewed as i.i.d samples, several regret-optimal variants of LinUCB have been proposed [2,15]. Nevertheless, those solutions all require real-time model updates (via the optimization oracle), and are thus impractical as we discussed earlier. On the other hand, several suboptimal algorithms that follow the explore-then-commit framework can be made computationally feasible for large-scale applications [38]. The key idea is to divide exploration and exploitation into dierent stages, like the epochgreedy and phased exploration algorithms [1,28,39]. The model training and parameter updates only consume the back-end resources dedicated for exploitation, and the majority of front-end resources still take care of the model inference and exploration. Therefore, the stage-wise approach appeals to our online recommender selection problem, and it resolves certain infrastructural considerations that we explain later in Section 4. On the Bayesian side, the most widely-acknowledge algorithms belong to the Thompson sampling, which has a long history and fruitful theoretical results [10,40,41,45]. When applied to contextual bandit problems, the original Thompson sampling also requires perround parameter update for the reward model [10]. Nevertheless, the exibility of the Bayesian setting allows converting Thompson sampling to the stage-wise setting as well. In terms of real-world applications, online advertisement and news recommendation [3,30,31] are perhaps the two major domains where contextual bandits are investigated. Bandits have also been applied to our related problems such as item-level recommendation [26,32,53] and recommender ensemble [9]. To the best of our knowledge, none of the previous work studies contextual bandit for the recommender selection. As we discussed in the literature review, the stage-wise (phased) exploration and exploitation appeals to our problem because of their computation advantage and deployment exibility. To apply the stage-wise exploration-exploitation to online recommender selection, we describe a general framework in Algorithm 1. Input: Reward model 𝑓(·); the restart criteria; the 𝜋 (𝑎 | ·) =, and collect observation to®ℎ; minimization); The algorithm is deployment-friendly because Step 5 only involves front-end and cache operation, Step 6 is essentially a batchwise training on the back-end, and Step 7 applies directly to the standard front-end inference. Hence, the algorithm requires little modication from the existing infrastructure that supports realtime mdoel inference. Several additional advantages of the stagewise algorithms include: •the number of of exploration and exploitation rounds, which decides the proportion of trac for each task, can be adaptively adjusted by the resource availability and response time service level agreements; •the non-stationary environment, which are often detected via the hypothesis testing methods as described in [5,8,33], can be handled by setting the restart criteria accordingly. This section is dedicated to improving the eciency of exploration in Step 5. Throughout this paper, we emphasize the importance of leveraging the case-specic structures to minimize the number of exploration steps it may take to collect equal information for estimating𝜽. Recall from Example 1 that one particular structure is the relation among the recommended contents, whose role can be thought of as the design matrix in linear regression. Towards that end, our goal is aligned with the optimal design in the classical statistics literature [37], since both tasks aim at optimizing how the design matrix is constructed. Following the previous buildup, the reward model has one of the following forms: We start with the frequentist setting, i.e.𝜽do not admit a prior distribution. In each round𝑡, we try to nd a optimal design𝜋 (· | ·)such that the action sampled from𝜋leads to a maximum information for estimating𝜽. For statistical estimators, the Fisher information is a key quantity for evaluating the amount of information in the observations. For the general𝑓, the Fisher information under (2) is given by: where𝜋 (𝑎)is a shorthand for the designed policy. For the linear reward model, the Fisher information is simplied to: To understand the role Fisher information in evaluating the underlying uncertainty of a model, according to the textbook derivations for linear regression, we have: • var(ˆ𝜽 ) ∝ 𝑀 (𝜋 ); •the prediction variance for𝝓:= 𝝓𝑟, 𝑎(𝑟)is given by Therefore, the goal of optimal online experiment design can be explained as minimizing the uncertainty in the reward model, either for parameter estimation or prediction. In statistics, a D-optimal design minimizesdet |𝑀 (𝜋 )|from the perspective of estimation variance , and the G-optimal design minimizemax𝝓𝑀 (𝜋)𝝓 from the perspective of prediction variance. A celebrated result states the equivalence between D-optimal and G-optimal designs. Theorem 1 (Kiefer-Wolfowitz [27]). For a optimal design𝜋, the following statements are equivalent: • 𝜋= maxlog det𝑀 (𝜋); • 𝜋is D-optimal; • 𝜋is G-optimal. Theorem 1 suggests that we use convex optimization to nd the optimal design for both parameter estimation and prediction: However, a drawback of the above formulation is that it does not involve the observations collected in the previous exploration rounds. Also, the optimization problem does not apply to the Bayesian setting if we wish to use Thompson sampling. Luckily, we nd that optimal design for the Bayesian setting has a nice connection to the above problem, and it also leads to a straightforward solution that utilizes the history data as a prior for the optimal design. We still assume the linear reward setting, and the prior for𝜽 is given by𝜽 ∼ 𝑁 (0, R)whereRis the covariance matrix. Unlike in the frequentist setting, the Bayesian design focus on the design optimality in terms of certain utility function𝑈 (𝜋). A common choice is the expected gain in Shannon information, or equivalently, the Kullback-Leibler divergence between the prior and posterior distribution of𝜽. The intuition is that the larger the divergence, the more information there is in the observations. Let𝑦be the hypothetical rewards for𝝓𝑟, 𝑎(𝑟), . . . , 𝝓𝑟, 𝑎(𝑟). Then the gain in Shannon information is given by: where𝐶is a constant. Therefore, maximizing𝑈 (𝜋)is equivalent to maximizing log det𝑀 (𝜋) + R. Compared with the objective for the frequentist setting, there is now an additiveRterm inside the determinant. Notice thatRis the convariance of the prior, so given the previous history data, we can simply plug in the empirical estimation ofR. In particular, let®𝝓 be the collection of feature vectors from the previous exploration rounds:𝝓𝑥, 𝑎(𝑥), . . . , 𝝓𝑥, 𝑎(𝑥). ThenRis simply estimated by®𝝓®𝝓. Therefore, the objective for Bayesian optimal design, after integrating the prior from the history data, is given by: maximizelog det𝑀 (𝜋) + 𝜆𝝓𝝓, s.t.𝜋 (𝑎) = 1, (6) where we introduce the hyper parameter 𝜆 to control inuence of the history data. We refer the interested readers to [13,16,25,34] for the historic development of this topic. Moving beyond the linear setting, the results stated in the KieferWolfowitz theorem also holds for nonlinear reward model [46]. Unfortunately, it is very challenging to nd the exact optimal design when the reward model is nonlinear [51]. The diculty lies in the fact that∇𝑓𝝓𝑟, 𝑎(𝑟)now depends on𝜽, so the Fisher information (4) is also a function of the unknown𝜽. One solution which we nd computationally feasible is to consider a local linearization of 𝑓(·) using the Taylor expansion: where 𝜽is some local approximation. In this way, we have: and the local Fisher information will be given by𝑀 (𝜋;𝜽)according to (4). The eectiveness of local linearization completely depends on the choice of𝜽. Using the data gathered from previous exploration rounds is a reasonable way to estimate𝜽. We plug in𝜽to obtain the optimal designs for the following exploration rounds: maximizelog det𝑀 (𝜋; 𝜽)s.t.𝜋 (𝑎 We do not study the Bayesian optimal design under the nonlinear setting because even the linearization trick will be complicated. Moreover, by the way we construct𝜽above, we already pass a certain amount of prior information to the design. Remark 1. Before we proceed, we briey discuss what to expect from the optimal design in theory. Optimizing thelog det |·|objective is essentially nding the minimum-volume ellipsoid, also known as the John ellipsoid [19]. According to the previous results from the geometric studies, using the proposed optimal design will do no worse than the uniform exploration if the reward model is misspecied. Also,√ we can expect an average𝑑improvement in the frequentist’s linear√ reward setting [7], which means it only takes𝑜 (1/𝑑)of the previous exploration steps to estimate 𝜽 to the same precision. In this section, we introduce an ecient algorithm to solve the optimal designs in (9) and (6). We then couple the optimal designs to the stage-wise exploration-exploitation algorithms. The infrastructure for our real-world production is also discussed. We have shown earlier that nding the optimal design requires solving a convex optimization programming. Since the problem is often of moderate size as we do not expect the number of recommenders 𝑘to be large, we nd the Frank-Wolfe algorithm highly ecient [17,23]. We outline the solution for the most general non-linear reward case in Algorithm 2. The solutions for the other scenarios are included as special cases, e.g. by replacing𝑀 (𝜋)with𝑀 (𝜋) + R for the Bayesian setting. Referring to the standard analysis of Frank-Wolfe algorithm [23], we show that the it takes the solver at mostO(𝑑 log log 𝑘 + 𝑑/𝜖) updates to achieve a multiplicative(1+𝜖)optimal solution. Each update has anO(𝑘𝑑)computation complexity, but𝑑is usually small in practice (e.g.𝑑 =6 in Example 1), which we will illustrate with more detail in Section 5. By treating the optimal design solver as a subroutine, we now present the complete picture of the stage-wise exploration-exploitation with optimal design. To avoid unnecessary repetitions, we describe the algorithms for nonlinear reward model under frequentist setting (Algorithm 3), and for linear reward model under the Thompson sampling. They include the other scenarios as special cases. To adapt the optimal design to the Bayesian setting, we only need to make a few changes to the above algorithm: Algorithm 3:Stage-wise exploration-exploitation with optimal design. Input: Reward model 𝑓(·); restart criteria; initialize optimal design solver under 𝜽=ˆ𝜽 ; Play 𝑛rounds of exploration under 𝜋and collect observation to®ℎ; • the optimal design solver is now specied for solving (6); •instead of optimizingˆ𝜽via the empirical-risk minimization, we update the posterior of 𝜽 using the history data®ℎ; •in each exploitation round, we execute Algorithm 4 instead. Algorithm 4:Optimal design for Thompson sampling at exploitation rounds. prior and collected data ; For Thompson sampling, the computation complexity of exploration is the same as Algorithm 3. On the other hand, even with a conjugate prior distribution, the Bayesian linear regression has an unfriendly complexity for the posterior computations. Nevertheless, under our stage-wise setup, the heavy lifting can be done at the back-end in a batch-wise fashion, so the delay will not be signicant. In our simulation studies, we observe comparable performances from Algorithm 3 and 4. Nevertheless, each algorithm may experience specic tradeo in the stage-wise setting, and we leave it to the future work to characterize their behaviors rigorously. In the ideal setting, the online recommender selection can be viewed as another service layer, which we refer to as the system bandit service, on top of the model service infrastructure. An overview of the concept is provided in Figure 1. The system bandit module takes the request (which contains the relevant context), and the wrapped recommendation models. When the service is triggered, depending on the instruction from request distributor (to explore or exploit), the module either queries the pre-computed reward, nds the best model and outputs its content, or run the optimal design solver using the pre-computed quantities and choose a recommender to explore. The request distributor is essential for Figure 1: High-level overview of the system bandit service. keeping the resource availability and response time agreements, since the optimal-design computations can cause stress during the peak time. Also, we initiate the scoring (model inference) for the candidate recommenders in parallel to reduce the latency whenever there are spare resources. The pre-computations occur in the back-end training clusters, and their results (updated parameters, posterior of parameters, prior distributions) are stored in such as the mega cache for the front end. The logging system is another crucial component which maintains storage of the past reward signals, contexts, policy value, etc. The logging system works interactively with the training cluster to run the scheduled job for pre-computation. Another detail is that the rewards are often not immediately available, e.g. for the conversion rate, so we set up an event stream to collect the data. The system bandit service listens to the event streams and determines the rewards after each recommendation. For our deployment, we treat the system bandit serves as a middleware between the online and oine service. The details are presented in Figure 2, where we put together the relevant components from the above discussion. It is not unusual these days to leverage the "near-line computation", and our approach takes the full advantage of the current infrastructure to support the optimal online experiment design for recommender selection. Figure 2: The deployment details of the optimal online experiment design for recommender selection. We rst provide simulation studies to examine the eectiveness of the proposed optimal design approaches. We then discuss the relevant testing performance on Walmart.com. For the illustration and reproducibility purposes, we implement the proposed online recommender selection under a semi-synthetic setting with a benchmark movie recommendation data. To fully reect the exploration-exploitation dilemma in real-world production, we convert the benchmark dataset to an online setting such that it mimics the interactive process between the recommender and user behavior. A similar setting was also found in [9] that studies the non-contextual bandits as model ensemble methods, with which we also compare in our experiments. We consider the linear reward model setting for our simulation. Data-generating mechanism. In the beginning stage, 10% of the full data is selected as the training data to t the candidate recommendation models, and the rest of the data is treated as the testing set which generates the interaction data adaptively. The procedure can be described as follow. In each epoch, we recommend one item to each user. If the item has received a non-zero rating from that particular user in the testing data, we move it to the training data and endow it with a positive label if the rating is high, e.g.≥3 under the ve-point scale. Otherwise, we add the item to the rejection list and will not recommend it to this user again. After each epoch, we retrain the candidate models with both the past and the newly collected data. Similar to [9], we also use the cumulative recallas the performance metric, which is the ratio of the total number of successful recommendations (up to the current epoch) againt the total number of positive rating in the testing data. The reported results are averaged over ten runs. Dataset. We use the MoiveLens 1Mdataset which consists of the ratings from 6,040 users for 3,706 movies. Each user rates the movies from zero to ve. The movie ratings are binarized to{0,1}, i.e.≥2.5 or<2.5, and we use the metadata of movies and users as the contextual information for the reward model. In particular, we perform the one-hot transformation for the categorical data to obtain the feature mappings𝝓(·). For text features such as movie title, we train a word embedding model [36] with 50 dimensions. The nal representation is obtained by concatenating all the one-hot encoding and embedding. Figure 3: The comparisons of the cumulative recall (reward) per epoch for dierent bandit algorithms. Candidate recommenders. We employ the four classical recommendation models: user-based collaborative ltering (CF) [54], item-based CF [42], popularity-based recommendation, and a matrix factorization model [21]. To train the candidate recommenders during our simulations, we further split the 10% initial training data into equal-sized training and validation dataset, for grid-searching the best hyperparameters. The validation is conducted by running the same generation mechanism for 20 epochs, and examine the performance for the last epoch. For the user-based collaborative ltering, we set the number of nearest neighbors as 30. For itembased collaborative ltering, we compute the cosine similarity using the vector representations of movies. For relative item popularity model, the ranking is determined by the popularity of movies compared with the most-rated movies. For matrix factorization model, we adopt the same setting from [9]. Baselines. To elaborate the performance of the proposed methods, we employ the widely-acknowledged exploration-exploitation algorithms as the baselines: •The multi-armed bandit (MAB) algorithm without context: 𝜖-greedy and Thompson sampling. •Contextual bandit with the exploration conducted in the LinUCB fashion (Linear+UCB) and Thompson sampling fashion (Linear+Thompson). We denote our algorithms by theLinear+Frequentis optimal design and the Linear+Bayesian optimal design. Ablation studies We conduct ablation studies with respect to the contexts and the optimal design component to show the eectiveness of the proposed algorithms. Firstly, we experiment on removing the user context information. Secondly, we experiment with our algorithm without using the optimal designs. Results. The results on cumulative recall per epoch are provided in Figure 3. It is evident that as the proposed algorithm with optimal design outperforms the other bandit algorithms by signicant margins. In general, even though𝜖-greedy gives the worst performance, the fact that it is improving over the epochs suggests the validity of our simulation setup. The Thompson sampling under MAB performs better than𝜖-greedy, which is expected. The usefulness of context in the simulation is suggested by the slightly better performances from Linear+UCB and Linear+Thompson. However, they are outperformed by our proposed methods by signicant margins, which suggests the advantage of leveraging the optimal design in the exploration phase. Finally, we observe that among the optimal design methods, the Bayesian setting gives a slightly better performance, which may suggest the usefulness of the extra steps in Algorithm 4. The results for the ablation studies are provided in Figure 4. The left-most plot shows the improvements from including contexts for bandit algorithms, and suggests that our approaches are indeed capturing and leveraging the signals of the user context. In the middle and right-most plots, we observe the clear advantage of conducting the optimal design, specially in the beginning phases of exploration, as the methods with optimal design outperforms their counterparts. We conjecture that this is because the optimal designs aim at maximizing the information for the limited options, which is more helpful when the majority of options have not been explored such as in the beginning stage of the simulation. Finally, we present a case study to fully illustrate the eectiveness of the optimal design, which is shown in Figure 5. It appears that Figure 4: The dierence in cumulative recall for system bandit under dierent settings. The left gure shows dierence in performance between using and not using user context, under the frequentist and Bayesian setting, respectively. The other two gures compare using and not using the optimal design (uniform selection) in the exploration stages, both using the linear reward model. in our simulation studies, the matrix factorization and popularitybased recommendation are found to be more eective. With the optimal design, the trac concentrates more quickly to the two promising candidate recommenders than without the optimal design. The observations are in accordance with our previous conjecture that optimal design gives the algorithms more advantage in the beginning phases of explorations. Figure 5: The percentage of trac routed to each candidate recommender with and without using the optimal design under the frequentist setting. We deployed our online recommender selection with optimal design to the similaritem recommendation of grocery items on Walmart.com. A webpage snapshot is provided in Figure 6, where the recommendation appears on the item pages. The baseline model and we experiment with three enhanced models that adjust the original recommendations based on the brand anity, price anity and avor anity. We omit the details of each enhanced model since they are less relevant. The reward model leverages the item and user representations also described in our previous work. Specically, the item embeddings are obtained from the Product Knowledge Graph embedding [50], and the user embeddings are constructed via the temporal user-item graph embedding [14]. We adopt the frequentist setting where the reward is linear function of⟨item emb, user emb⟩, plus some user and item contextual features: 𝜃+𝜃⟨z, z⟩ + . . . +𝜃⟨z, z⟩ + 𝜽[user feats, items feats], and zand zare the user and item embeddings. Figure 7: The testing results for the propose d stage-wise exploration-exploitation with optimal design. We conduct a posthoc analysis by examining the proportion of trac directed to the frequent and infrequent user groups by the online recommender selection system (Figure 8). Interestingly, we observe dierent patterns where the brand and avor-adjusted models serve the frequent customers more often, and the unadjusted baseline and price-adjusted model get more appearances for the infrequent customers. The results indicate that our online selection approach is actively exploring and exploiting the user-item features that eventually benets the online performance. The simulation studies, on the other hand, reveal the superiority over the standard exploration-exploitation methods. Figure 8: The analysis on the proportion of trac directed to the frequent and infrequent customer. We study optimal experiment design for the critical online recommender selection. We propose a practical solution that optimizes the standard exploration-exploitation design and shows its eectiveness using simulation and real-world deployment results.