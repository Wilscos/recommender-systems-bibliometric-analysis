Keywords Learning-to-Rank · Neural Networks · E-Commerce · Recommender System As in all e-commerce services where users, items, and preferences are involved, a Recommender System (RS) [1] can be used to facilitate the navigation of big catalogs. Since Online Travel Agencies (OTAs) [2, 3, 4, 5] provide the main service of booking holidays, business trips and accommodations, an adequate recommendation can aid the user in ﬁnding the most appealing listing among many. Learning-To-Rank (LTR) techniques can be used to build such RS Corresponding Author apetrozziello@expediagroup.comxiaokliu@expediagroup.com Nowadays, Online Travel Agencies provide the main service for booking holidays, business trips, accommodations, etc. As in many e-commerce services where users, items, and preferences are involved, the use of a Recommender System facilitates the navigation of the marketplaces. One of the main challenges when productizing machine learning models (and in this case, Learning-to-Rank models) is the need of, not only consistent pre-processing transformations, but also input features maintaining a similar scale both at training and prediction time. However, the features’ scale does not necessarily stay the same in the real-world production environment, which could lead to unexpected ranking order. Normalization techniques such as feature standardization, batch normalization and layer normalization are commonly used to tackle the scaling issue. However, these techniques are sometimes infeasible in real world web applications. First, the normalization of features across thousands of items can signiﬁcantly impact the website latency. Second, the real-time inference of the item scores is usually distributed across multiple machines on the production environment, thus listwise operations, requiring information from all items to be available, are hard to achieve. To address this issue, in this paper we propose a novel scale-invariant ranking function (dubbed as SIR) which is accomplished by combining a deep and a wide neural network. We incorporate SIR with ﬁve state-of-the-art Learning-to-Rank models and compare the performance of the combined models with the classic algorithms on a large data set containing 56 million booked searches from the Hotels.com website. Besides, we simulate four real-world scenarios where the features’ scale at the test set is inconsistent with that at the training set. The results reveal that when the features’ scale is inconsistent at prediction time, Learning-To-Rank methods incorporating SIR outperform their original counterpart in all scenarios (with performance difference up to 14.7%), while when the features’ scale at the training and test set are consistent our proposal achieves comparable accuracy to the classic algorithms. by training on data consisting of lists of items with some partial order speciﬁed among items in each list. LTR aims to learn a scoring function that maps feature vectors to real-valued scores in a supervised setting. Scores computed by such a function induce an ordering of items in the list: the ranking [6]. The majority of existing LTR algorithms learn a parameterized function by optimizing a loss that acts on pairs of items (pairwise) [7, 8, 9] or a list of items (listwise) [10, 11, 12]. Booking data collected from user logs is an important source of observational data that can be utilized to train LTR algorithms [13, 14, 15, 16]. However, when used for training, this data not always reﬂect the same distribution or scale of the inputs received in the e-commerce production environment, leading to unexpected ranking. Many data driven companies employ A/B testing [17, 18, 19] to decide whether a new product or feature is good enough to be rolled out to all customers. In big organizations it is very likely that changes rolled out for testing in different departments would affect the model without the machine learning engineer even knowing about it [20, 21]. Those changes can unwillingly alter the inputs to the LTR model, modifying the sort order of the items on the marketplace. For example, an OTA can change (i.e., A/B test) how the price for a room is displayed to the user: one group of users can be presented with nightly prices, while another group with the price of the full stay. If this scaling issue is not addressed, the two groups of users will see different sort order for the same search query, invalidating the full experiment - i.e., the graphic interface change from nightly price to full price, should not have an impact on the ranking of the listings. Other examples could be: a change in currency, a change of the guest rating range (e.g., from 0-10 to 0-100), or hotel-landmarks distances expressed in kilometers instead of miles. A common solution to this issue is to conduct normalization on the training data, test data and at real-time inference on the production environment. While the ﬁrst two cases are trivial to handle, real-time normalization is usually unfeasible. The reason is three-fold: Firstly, when a user queries the website, the candidate set usually contains thousands of items (properties listing in our case). Normalization of such large sets take relatively long, which increases the latency of the whole website, negatively impacting the user experience. Secondly, for the sake of latency reduction, the inference of the items score is usually distributed across many machines. Each machine will only be able to access partial information of the list of items, making canonical normalization techniques unreliable. Lastly, listwise standardization doesn’t work if the observed list is truncated, which usually happens in real world production enviroment (for a given query, only a subset of the catalogue is considered for ranking). Since normalization of the input data is hard to achieve, it is necessary to build a LTR algorithm which is invariant to the features scale change. Here we tackle this problem by proposing a new scale invariant ranking function (dubbed as SIR) that works regardless of the received inputs scale at prediction time. The empirical evaluation demonstrates the usefulness of SIR when incorporated into ﬁve state-ofthe-art LTR algorithms (namely RankNet, LambdaRank, ListNet, ListMLE and SoftRank) that do not natively address the scale problem at prediction time. We test our solution on a data set containing 56 million booked searches from the Hotels.com website and assess the models’ accuracy using the Normalized Discounted Cumulative Gain (NDCG) metric [22]. Results show that adding the rank preserving feature does not impact the performance of the models when the features scale of the test set is the same as the one of the training set. However, when a transformation modifying the features’ scale is applied to the test set, our proposal is the only one preserving the models’ accuracy, while in all but two cases, the original formulations show a signiﬁcant drop (p-value < 0.001) in the evaluation metric (up to 14.7%). This result conﬁrms that the change in scale, if not addressed, is reﬂected as a change in the ranking order. Learning to rank [23] is an application of machine learning (usually supervised or semi-supervised) in the construction of ranking models for Information Retrieval systems [24]. LTR methods are classiﬁed into three categories: point-wise [25, 26], pair-wise [7, 9, 8], and list-wise [12, 27, 28, 10]. Point-wise approaches only look at a single item in the loss function. They consider each item individually and train a classiﬁer/regressor on it to predict how relevant it is for the current query (i.e., search). The ﬁnal ranking is achieved by simply sorting the result list by these items score. For point-wise approaches, the score for each item is independent of the other items that are in the results list for the query. All the standard regression and classiﬁcation algorithms [25, 26] can be directly used for point-wise learning to rank. Pair-wise approaches look at a pair of items at a time in the loss function. Given a pair of items, they try and come up with the optimal ordering for that pair and compare it to the ground truth. The goal for the ranker is to minimize the number of inversions in ranking (i.e. cases where the pair of results are in the wrong order relative to the ground truth). Pair-wise approaches work better in practice than point-wise approaches because predicting relative order is closer to the nature of ranking than predicting class label or relevance score [10]. Some of the most popular LTR algorithms like RankNet [7], LambdaRank [9] and LambdaMART [8] are pair-wise approaches. List-wise approaches directly look at the entire list of items and try to come up with the optimal ordering for it. There are two main sub-techniques for doing list-wise LTR: direct optimization of Information Retrieval measures such as NDCG (e.g. SoftRank [12], AdaRank [27]); minimization of a loss function that is deﬁned based on understanding the unique properties of the kind of ranking of the task at hand (e.g. ListNet [10], ListMLE [28]). List-wise approaches can get fairly complex compared to point-wise or pair-wise approaches and are usually yielding better ranking performance due to the fact that the features of the whole list are considered at training time. In this paper we incorporate two pairwise learning-to-rank methods (Ranknet and LambdaRank) and three listwise algorithms (Listnet, ListMLE and Softrank) with the proposed scale invariant ranking function. It is widely acknowledged that Recommender Systems should be invariant with repsective to the scale of object features and the scale of customer ratings [29]. Unfortunately, the classic RS models usually depend on such scale. For example, the computation of user similarity in collaborative ﬁltering (CF) highly depends on the scale of customer ratings, while different users usually use different rating scales [30]. A great research effort has been done in the last two decades to tackle the features scale problem for CF. To avoid the mismatch in the users’ internal ratings scale, Yu and Yang [31] proposed to use ordinal information rather than absolute ratings when computing user similarity. Lemire [32] introduced an adjusted pearson correlation similarity, Su and Khoshgoftaar [33] presented an adjusted cosine similarity. Both similarity metrics are invariant to the scale of customer ratings. Anand and Bharadwaj [34] suggested to transform the raw customer ratings to an optimal rating via genetic programming and then use the transformed ratings to determine user similarity. Besides, researchers employ normalization techniques to adapt to various rating habits of users, normalising ratings assigned by different users to the same scale by some criteria [35]. For example, Resnick et.al [36] developed a zscore normalization based method, Sarwar et.al [37] proposed a dimensionality reduction based method, Lemire [32] introduced an L Nowadays, Deep Neural Network (DNN) based models are widely used in Recommender Systems. The scale issue of DNN models has been identiﬁed through the literature. For example, DNN with ReLU activation function was proved to be scale invariant regarding to the weight space. If the ingoing weights of one hidden node are multiplied by a constant c and the outgoing weights of the hidden node are divided by c at the same time, the output of network is identical [39, 40]. The weight space invariance implies that there can be an inﬁnite amount of parameter points in the weight space that represent the same model [41]. The scale invariant property of DNN models regarding to the input features are mostly done by normalization. The scale of features can be uniﬁed by canonical normalization. For example, the scale of numerical features can be easily uniﬁed by transforming them into normal distributed variables with mean 0 and variance 1. Since Salimans and Kingma [42] proposed the batch normalization, researchers proposed a variety of normalization techniques to speed up the training of neural networks and tackle the scaling issue, including weight nomalization [42], layer normalization [43] and streaming normalization [44]. In batch normalization, the pre-activation of each neuron is normalized by the mean and the standard deviation of the outputs computed over the samples in the mini batch [41]. Adding batch normalization to the feed-forward neural networks can improve training speed. Layer normalization is proposed to overcome the disadvantage of batch normalization, where pre-activation of each neuron is normalized by all the hidden neurons in the same layers instead of the pre-activation of each neuron within a mini batch. While those proposed techniques are used to speed up the training of models, they are not feasible at inference time for several reasons. Firstly, the normalization of large sets (i.e., online queries can retrieve up to thousands of items at each time) negatively impacts the environment latency . Furthermore, items in the same search are usually handled and scored by different machines, meaning each machine only has partial information, making the normalization hard to achieve. In this paper we propose a neural network based ranking function which takes a list of items (represented by some item features and query features) in a query as input and returns a sorted list as output. Before diving into the formal explanation of the proposed method we clarify the notations we use throughout the paper. normalization method, and Traupman and Wilensky [38] presented a factor analysis based method. Feature vector of the item j in query i. It can be split into query features xand item features x. The query feature vector of query i. Items in the same query share same query features. Item feature vector of item j in query i. It can be further split into xand x. invariant). Note that the partition of features which have scaling issue is known. Here R numbers. The proposed scoring function f calculates the score of the items using a Deep Neural Network with three fully-connected layers (512, 256, and 128 neurons respectively) and a dense layer of 25 elements containing the scores (see more details in Figure 1); and a wide part fwhich takes x is then added to the score produced by f Details on the network structure can be found in Figure 1. Speciﬁcally, the score of item j in query i: Here fcompresses the query features by projecting them to R a weight parameter to tune, and < · > represents the inner product operator. Now assume the scale of x can take any positive value, then the score difference of these two items after the scale change: The partition of item features whose scale remain the same in all cases. The partition of item features whose scale can vary at prediction time (to which our method is scale Customer label of item j in query i where y= 1 for the booked item and y= 0 for others. The number of items in query i. If we denote the concatenation of vectors using ⊕, formula (3) can be expanded as follows, In short, Namely, the proposed scoring function gives the same score difference for two items in the same search with or without scaling. The difference of scores is invariant to the scale change. Accordingly, the ranking based on this scoring function is scale invariant as well. In query i, assume we have scores (f(x {1, 2, . . . , D where → indicates bijection. Here we rank items according to scores in decending order, i.e. our ranking function π satisﬁes f(x ranked in the j-th place in the query by the ranking function π The proposed method has some similarities to the NN&FM ensemble introduced in [19] and Wide&Deep approach presented in [18], however the main goal here is not to improve the model accuracy but to restrict the structure of our model to allow for more robust behavior when used in a production environment. This section presents the design of the empirical study we carried out to get an insight into the use of a ranking preserving function for Learning-To-Rank tasks. We ﬁrst present the research questions we aim to answer and then the data and techniques we experimented with, and the evaluation criteria we used to assess the results. Before checking whether the proposed approach is useful in solving the scale variance problem, we must ensure it yields comparable results to the current state-of-the-art techniques when there is no scale variance in the test set. Thus, the ﬁrst research question we aim to answer is: RQ1. Does our proposal provide at least comparable ranking accuracy than currently used LTR methods when training and test set features’ scale is consistent? To answer RQ1 we incorporate our proposal into ﬁve widely-used and well-studied state-of-the-art LTR methods, namely RankNet [7], LambdaRank [8], ListNet [10], ListMLE [11] and SoftRank [12], as detailed in Section 4.4. If SIR models achieve at least comparable performance to the original LTR algorithms, then it could be employed in a production environment Figure 1: The SIR architecture is a siamese network with m (i.e., number of items in each list) identical branches (i.e., shared weights), each composed of two parts: a deep part taking as input the query features (x invariant item features (x features (x (denoted as f deep part of the structure is used, taking all item features as input (x suffer of the scale variance problem. without losing any ranking accuracy. Following RQ1, we want to know whether the proposal outperforms the stateof-the-art techniques when the scale variance problem in the test set is considered. This motivates our next research question: RQ2. Is the proposal invariant to test set features’ scale changes? To answer RQ2 we perturb the scale of some test set features based on real-world scenarios, and compare again ranking accuracy of models incorporating SIR, to their scale variant counterpart. To validate our hypothesis we simulate four cases which could happen in a realworld e-commerce production environment. Case 1: The hotel price and discount passed to the model at prediction time are the full price/discount of the stay (price/discount of 1 night, times the number of nights), instead of the nightly price/discount. Case 2: The hotel price and discount respectively are passed to the model at prediction time in the local currency, instead of dollars (USD). To achieve that, we multiply the nightly price and discount by the daily exchange rate of the selected currency. Case 3: We combine Case 1 and Case 2, hence the price and discount passed to the model at prediction time are the full price/discount of the stay, in the local currency. Case 4: We select a currency with a very high exchange rate compared to the dollar (i.e., Korean Won, 1 USD = 1200 KWR) and simulate what would happen if all test set searches were performed from the Korean point of sale (this case is less realistic than the ﬁrst three, and only used to check performance of the models in case of a big shift in the scale of the features - i.e., price and discount). To carry out the empirical study we exploited a travel accommodation booking data set from the Hotels.com website. The data set has 56 million of booked searches, each of which contains up to 25 items (i.e., properties). Each item has 16 features shared (i.e., query features) with all the others in the same search (e.g., destination, check in date, check out date, point of sale, locale, local currency, user device, etc.), and 14 item features (e.g., room price, discount, star rating, review scores, point of supply, etc.). Each item also has one boolean target variable representing whether it has been booked. The task is to rank the items such that the booked one has the largest score (i.e., is ranked higher) based on the given query and item features. Before feeding the data to the ranking algorithms, we apply widely used features transformations to facilitate the learning procedure: All numerical features are standardized ((feature val − µ)/σ), where µ is the feature mean and σ the standard deviation; while all categorical features are indexed (i.e., 1 to #levels) and mapped to an embedding learned within the Neural Network at training time. This technique allows to deal with high cardinality features in a simple fashion [19]. We use a data set containing 56 million of booked searches from Hotels.com website to verify our proposal. In each search only one property is labeled as the booked one according to the customer’s booking record, while the others are labeled as non-booked. To verify the efﬁciency of the proposed method a validation process is required. We used a hold-out technique [45], where the data is split in two disjoint sets, the ﬁrst one used for training the model, the second one for evaluating its performance. We randomly split the data into a training set with 70% of data (38.68 million searches) and a test set with 30% data (16.90 million searches). To avoid over ﬁtting during the training procedure, we use 10% of the training data (3.86 mission searches) as a separate validation set, and stop the training procedure ). Each part produces a score: f(x, x) and f(x, x) for the deep and wide part respectively and fin the ﬁgure), which are then added together. When testing the original LTR methods, only the when either max training iteration is reached (i.e., 100) or the validation NDCG does not improve for 20 consecutive iterations. To guarantee consistency across the experiments we use the same proposed network structure (Figure 1 and scoring function (Eq. 1). Furthermore, all trained models are equally tuned with classic hyper-parameter optimization [46]. Concerning the evaluation of our proposed method, we employ one of the most widely used measure in Learning-ToRank, the NDCG [22]. The NDCG is a normalization of the Discounted Cumulative Gain (DCG) while DCG is a weighted sum of the degree of relevancy of the ranked items. Speciﬁcally, for given query i and associated D DCG at position l is deﬁned as: where Here G(·) is a gain function, D(·) is a position discount function, π Namely, DCG at position l is: while NDCG is a normalization of DCG. NDCG at position l is deﬁned as: Here G By a perfect ranking we mean in this ranking items with higher grades are always ranked higher. In this paper we set l = D across all queries on the test set as a measurement of the model performance. We compare the average NDCG of our models and ﬁve state-of-the-art models using two sample t-test. To be speciﬁc, the null hypothesis to be tested is: the NDCG provided by model M(SIR) is signiﬁcantly smaller than the NDCG provided by model M, (when M(SIR) and M are the scale invariant version and original formulation respectively), under signiﬁcance level α=0.05, applying Bonferroni correction for multiple hypothesis testing (α/n, where n (n = 25) is total number of comparisons). Learning-To-Rank methods can be classiﬁed into three categories: point-wise [25, 26], pair-wise [7, 9, 8], and listwise [12, 27, 10, 28]. In this work, we compare our proposed methods with ﬁve widely used LTR technique, which do not natively address the scale invariance problem, two from the pair-wise family (namely RankNet and LambdaRank) and three from the list-wise family (namely ListNet, ListMLE, and SoftRank). Point-wise methods have not been considered as they in general lead to inferior results than pair-wise and list-wise methods [47]. RankNet is a Neural Network based learning to rank algorithm with a pair-wise loss proposed by Burges et.al. in 2005 [7]. It trains a back-propagation deep neural network with a pairwise loss. Speciﬁcally, for a pair of items denoted by x higher than x modeled posterior probability that sample x desired target values for this posterior probability. Then the loss for the pair x function: (l) is the normalizing factor and is chosen such that a perfect ranking π’s NDCG score at position l is 1. as x x, and the relationship xand xare in the same relevance class as x== x. The where Then the total RankNet loss is deﬁned as: for query 1 6 i 6 N and all pairs of item x In the implementation of RankNet, due to the lack of supervised information for items in the same relevance class, we do not consider loss between tied items (loss between non-booked items). In other words we only consider loss between the booked one and the non-booked ones in each search. LambdaRank is an enhancement of the RankNet algorithm proposed by Chris et. al. in 2010 [8], which incorporates the evaluation metric in the learning procedure. The basic idea is to dynamically adjust the loss during the training based on the ranking evaluation metrics. Using NDCG as an example, ∆NDCG is deﬁned as the absolute difference between the NDCG values when two items j and k are swapped. LambdaRank updates the RankNet loss in equation (11) by reweighing the loss of each item pair by ∆NDCG in each iteration. Namely, the lambdarank loss for item j and item k in query i: where Neural network model trained using the above loss is called lambdarank [9]. ListNet is a Neural Network based learning to rank algorithm with a list-wise loss, proposed by Zhe Cao et.al. in 2007 [10]. ListNet trains a back-propagation deep Neural Network with categorical cross-entropy loss: where the target probability f(x) is the score of item x ListMLE, proposed by Xia et.al. [11], is a Neural Network based LTR algorithm with a listwise negative likelihood loss. The likelihood loss is deﬁned as: where ) and f(x) are the scores of item xand item xfrom the network. is the ranknet loss for the corresponding item pair. Then the total lambdarank loss: Here xrepresents the items in query i, f is the scoring function, y query i. y Same as RankNet, due to the lack of supervised information between items in the same relevance level, in the implementation of ListMLE we do not consider loss between tied items. SoftRank is a Neural Network based learning to rank algorithm which was proposed by Taylor et. al. in 2008 [12]. Instead of training on a proxy loss function, SoftRank directly optimizes the NDCG metric. However, since NDCG is non-smooth with respect to the ranking scores, a smoothed approximation to NDCG was proposed and then used to obtain the best rank. The main idea of SoftRank is to replace the discount function in NDCG that is discontinuous with respect to the scores with a function continuous to the scores and directly optimize this adjusted NDCG. In the SoftRank loss function there is one additional hyper-parameter (σ) indicating the standard deviation of the score distribution, which was tuned by cross-validation and the best value (σ = 0.15) was employed. Furthermore, for computation simplicity, we train the SoftRank on lists of 9 items (8 non-booked items and the booked one) instead of the original list of 25. The training time increases exponentially with the growing size of the lists while the accuracy does not signiﬁcantly increases when using more than 9 items. The 8 out of 24 negative samples are chosen randomly. Speciﬁcally, assume the true score of the item j in query i, s Here σ is a hyperparameter to tune indicating the variability of the score. Then for any pair of items j and k in query i, the true score difference between them follows a normal distribution as well, and the probability that item j ranks higher than item k: where h denotes the density function. Then the probability that item j ranks lower than item k, p distribution of the rank r a Binomial-like random variable, i.e. equal to the number of successes in (D we mean the case where another item beats item j (ranked higher than j). Item j is ranked in the r another (r above. For example, the probability that item j is beaten by another item k, is p be found in equation (22). Accordingly, the probability that item j ranked in position r can be computed. In this way, P (r= r) can be calculated for each r and then distribution of r Now this adjusted model trained with this objective function is called Softrank. In this section we discuss the results of the comparison between the performance of each LTR model with its counterpart incorporating SIR, under a scale invariant test set (i.e., all features in the test set have consistent scale with those used for training). We used the average NDCG across all searches on the test set to measure the performance of the ranking algorithms. The corresponding validation and test NDCG can be found in Table 1 (2 (j) is the index of the object which is ranked at position j in query i. − 1) items beat it while it beats the rest (D− r) items. The probability of success can be computed as Table 1: RQ1 results: Validation NDCG during model training and Test NDCG are reported (for the scale invariant scenario) for RankNet, LambdaRank, ListNet, ListMLE, SoftRank and their ranking preserving counterparts (RankNet ListNet features’ scale in the test set. Symbol *** on the table indicates the NDCG of SIR model is signiﬁcant higher than its counterpart with p-value¡0.001. As it can be seen from the 3 their SIR counterpart (up to the third decimal point). In particular, ListMLE achieved the highest test set NDCG value, 0.668, followed by RankNet, SoftRank, and LambdaRank with 0.663 and 0.661 and 0.656 respectively. ListNet obtained the lowest test NDCG, 0.526. SIR models achieved comparable performance with a drop in NDCG at the third digit (-0.001 for RankNet validation NDCG during training has been also reported for completeness. The poor performance of ListNet is due to its loss formulation which gives relatively high weight to the negative samples [10]. However, due to the large class imbalance in our data set (1 to 24), ListNet is not able to correctly learn the positive samples. Regarding RankNet and ListMLE, as mentioned in Section 4.4, to avoid noise, we only considered loss between items in different relevance levels and neglected ties. It could be shown that NDCG would decrease dramatically if taking loss between ties into consideration. Furthermore, owing to the large training set we have, NDCG for SoftRank would not increase signiﬁcantly when including more than 8 negative samples, while the training time would increase exponentially in the number of used negative samples. Answer to RQ1: in the scenario where the training and test set features have the same scale, although the introduction of a scale invariant ranking function is reducing the expressivity of the model (i.e., limiting the non-linear interaction only among scale invariant features), it does not impact its prediction performance. In this section, we investigate how the methods perform when the test set features’ scale is perturbed. Here we simulated four scenarios that could happen in a real-world e-commerce production environment, and report the test NDCG of the ﬁve LTR methods and their SIR counterpart (Table 1, 4 and discount features are recorded as the nightly values in a uniﬁed currency (i.e., USD). However, in the real-world ecommerce environment, for some point of sales, the price and discount are displayed as the full stay (i.e., the product of the nightly price/discount and the number of nights). In other words, the scale of the price and discount in the prediction phase is not always consistent with the one used for training. This situation is commonly seen due to differences in marketing strategies for different countries, and our goal is for the ranking system to perform consistently under this scale heterogeneity. In Case 1 we simulated this business scenario by multiplying the nightly price and discount on the test set by the number of nights. The results (4 NDCG achieved in RQ1, while the original methods have a signiﬁcant drop in NDCG. In particular, RankNet has a drop of -0.01, LambdaRank -0.009, ListMLE -0.014 and SoftRank -0.014. ListNet is the only method remaining the original NDCG, though as we have seen in previous results, this is the only model which struggled to learn the task at hand, and showed an NDCG which is more than 10 decimal points worse than all the other models. As already mentioned, while the price and discount currency is uniﬁed for training and analysis purposes, the one displayed on the production environment usually depends on the user choice. To simulate this (Case 2, 5 multiply hotel prices and discounts in the test set by the local currency recorded when the search has been performed. As before, SIR held the same NDCG registered in RQ1, while the methods not addressing the scale invariance had a signiﬁcant drop in NDCG: RankNet has a drop of -0.019 , LambdaRank -0.017, ListNet -0.008, ListMLE -0.021 and SoftRank -0.023. In Case 3 (6 even stronger shift in scale: the price and discount are passed to the model at prediction time as the full price/discount of the stay in the local currency. Our model was once again able to preserve the ranking with same test NDCG as the original case, while all the other methods had relatively lower NDCG (from -0.008 to -0.030). In detail, RankNet , ListMLE, and SoftRank). RQ2 results: Test NDCG is reported for the four cases (Case 1-4) where we change the has a drop of -0.024, LambdaRank -0.022, ListNet -0.008, ListMLE -0.029 and SoftRank -0.030. In the last case (7 column of Table 1), we transformed the test set hotel price and discount to Korean Won by multiplying the original values (expressed in dollars) with the Korean Won daily exchange rate (i.e., 1 USD = ˜1200 KWR), to simulate the business scenario that all bookings on the test set were done from the Korean point of sale. As it can be seen also in this last case, all the models adopting SIR have achieved an identical NDCG to the one reported in RQ1, while the models not addressing the problem had a substantial drop in NDCG (up to -0.098). In particular, RankNet has a drop of -0.086 , LambdaRank -0.083, ListNet -0.014, ListMLE -0.098 and SoftRank -0.097. As it can be seen in Table 1, the NDCG achieved by the SIR models in the four simulated cases, is signiﬁcantly higher (p-value < 0.001) than their original counterparts in all but one case. The original formulation of ListNet achieved better NDCG in Case 1 (which is the one with the smallest perturbation compared to the test set), while still showing a consistent drop in NDCG in the remaining three cases. ListNet and a consistent NDCG across all analysed scenarios. As reported in [18], when considering big e-commerce with millions of daily users, even a 0.01 difference in the ofﬂine evaluation metric (e.g., NDCG) can lead to statistically signiﬁcant changes in online metrics such as conversion rate [48] or user engagement [49]. Answer to RQ2: The proposed method is invariant to feature scale changes regardless of the magnitude. Furthermore, the original formulations are not ranking invariant, leading to a negative impact on the evaluation metric up to 14.7% depending on the approach and the considered scenario. Several factors can bias the validity of empirical studies. In this section we discuss the construct, conclusion and external validity threats that may affect our study. To satisfy construct validity a study has “to establish correct operational measures for the concepts being studied” [50]. This means that the features and target variables should precisely measure the concepts they claim to measure. We mitigated such a threat by using a real-world dataset and excluded all the independent variables that are not known at inference time and therefore cannot be used for prediction purposes. With regards to the conclusion validity, we carefully applied the statistical tests, verifying all the required assumptions. Moreover, to mitigate external validity threats we described the theorical framework behind the proposed method and used a large dataset spanning multiple years of data, cointaining 56 million booked searches. However we cannot claim that our results generalise beyond the subjects studied herein. This paper has introduced and evaluated a scale invariant ranking function for Learing-To-Rank tasks. Results indicate that our proposal is rank preserving when the features’ scale is perturbed at prediction time, maintaining the same NDCG performance. This feature is extremely important to make the Learning-To-Rank model robust in a production environment, when a sudden change in scale of a feature (e.g., due to A/B testing) could unexpectedly alter the ranking order. Our results also show that all original formulations are not rank preserving and their performance drops proportionally to the shift in the features’ scale. Given our proposal is able to answer positively both our research questions, we would suggest practitioners in the LTR domain to address the scale variance problem when deploying ranking models to the production environment in order to avoid unwilling changes in ranking.