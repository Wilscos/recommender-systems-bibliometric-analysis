Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Department of Computer Science University of Virginia Charlottesville, VA 22903, USA Modern recommender systems fundamentally shape our everyday life (Koren et al., 2009; He et al., 2017; Sarwar et al., 2001; Rendle, 2010; Aggarwal et al., 2016). As a result, how to explain the algorithm-made recommendations becomes crucial in building users’ trust in the systems (Zhang and Chen, 2020). Previous research shows that explanations, which illustrate how the recommendations are generated (Ribeiro et al., 2016; Lundberg and Lee, 2017) or why the users should pay attention to the recommendations (Wang et al., 2018a; Sun et al., 2020; Yang et al., 2021), can notably strengthen user engagement with the system and better assist them in making informed decisions (Bilgic and Mooney, 2005; Herlocker et al., 2000; Sinha and Swearingen, 2002). As recommendation is essentially a comparative (or ranking) process, a good explanation should illustrate to users why an item is believed to be better than another, i.e., comparative explanations about the recommended items. Ideally, after reading the explanations, a user should reach the same ranking of items as the system’s. Unfortunately, little research attention has yet been paid on such comparative explanations. In this work, we develop an extract-and-reﬁne architecture to explain the relative comparisons among a set of ranked items from a recommender system. For each recommended item, we ﬁrst extract one sentence from its associated reviews that best suits the desired comparison against a set of reference items. Then this extracted sentence is further articulated with respect to the target user through a generative model to better explain why the item is recommended. We design a new explanation quality metric based on BLEU to guide the end-to-end training of the extraction and reﬁnement components, which avoids generation of generic content. Extensive oﬄine evaluations on two large recommendation benchmark datasets and serious user studies against an array of state-of-the-art explainable recommendation algorithms demonstrate the necessity of comparative explanations and the eﬀectiveness of our solution. and-reﬁne Figure 1: An illustration about the necessity of comparative explanations. The recommended Hotel A, B, C are listed in a descending order, with the provided explanations to justify the ranking. But if we replace Hotel C’s explanation with the one in the dash box, users may no longer perceive the ranking of all three hotels. When being presented with a list of recommendations, typically sorted in a descending order, a user needs to decide which recommendation is his/her best choice. In other words, the provided explanations should help users compare the recommended items. Figure 1 illustrates the necessity of comparative explanations. As shown in the ﬁgure, by reading the explanations for the three recommended hotels, one can easily tell why the system ranks them in such an order. But if the system provided the explanation in the dashed box for Hotel C, it would confuse the users about the ranking, e.g., Hotel C becomes arguably comparable to top ranked Hotel A; but it was ranked at the bottom of the list. This unfortunately hurts users’ trust in all three recommended hotels. Existing explainable recommendation solutions are not optimized to help users make such comparative decisions for two major reasons. First, the explanation of a recommended item is often independently generated without considering other items in the recommendation list. As shown in Figure 1, one single low-quality generation (the one in the dashed box) might hamper a user’s understanding over the entire list of recommendations. Second, the popularly adopted neural text generation techniques are known to be ﬂawed of its generic content output (Holtzman et al., 2019; Welleck et al., 2019). Particularly, techniques like maximum likelihood training and sequence greedy decoding lead to short and repetitive sentences composed of globally frequent words (Weston et al., 2018). Such generic content cannot fulﬁll the need to diﬀerentiate the recommended items. Consider the example shown in Figure 1 again, “the hotel is good” is a very generic explanation and thus not informative. Its vague description (e.g., the word “good”) and lacks of speciﬁcity (e.g., the word “hotel”) make it applicable to many hotels, such that users can hardly tell the relative comparison of the recommended items from such explanations. In this work, we tackle the problem of comparative explanation generation to help users understand the comparisons between the recommended items. We focus on explaining how one item is compared with another; then by using a commonly shared set of items as references (e.g., items the user has reviewed before), the comparisons among the recommended items emerge. For example, if the explanations suggest item A is better than item B and item C is worse than item B, the comparison between A and C is apparent after reading the associated explanations. Since there are already plenty of eﬀective recommendation algorithms deployed in practice, we decide not to invent yet another. Instead, we assume the existence of a performing recommendation algorithm, and build our solution on top of its provided item rankings. We do not have any assumptions about how this recommender system ranks items (e.g., collaborative ﬁltering(Sarwar et al., 2001) or content-based (Balabanovi´c and Shoham, 1997)), but only require it to provide a ranking score for each item to our model (i.e., ordinal ranking) and this ranking score reﬂects a user’s preference over the recommended items. This makes our solution generic and readily applicable to explain most existing recommendation algorithms. We design an extract-and-reﬁne text generation architecture to explain the ranked items one at a time to the user, conditioned on their recommendation scores and associated review text content. We refer to the item to be explained in the ranked list as the target item, and user we are explaining to as the target user. First, the model extracts one sentence from the existing review sentences about the target item as a prototype, with a goal to maximize the likelihood of ﬁtting the intended opinion of the target user with respect to his/her writing style of comparative content, which is reﬂected in all review sentences written by this user before. Then we reﬁne the extracted prototype sentence through a generative model to further improve its explanation quality (e.g., informativeness, ﬂuency, and diversity of content). In this two stage procedure, the extraction module directly exploits the content already provided about the target item to ensure the relevance of generated explanations (e.g., avoid mentioning features that do not exist in the target item); and the reﬁnement module further polishes the wording and sentiment for the target user. We design a new explanation quality metric based on BLEU to guide the end-to-end training of the extraction and reﬁnement module, with a particular focus to penalize short and generic content in generated explanations. We compared the proposed solution with a rich set of state-of-the-art baselines for explanation generation on two large-scale recommendation datasets: RateBeer (McAuley et al., 2012) and TripAdvisor (Wang et al., 2010). Besides, we also conducted extensive user studies to have the generated explanations evaluated by real users. Positive results obtained on both oﬄine datasets and online user studies suggested the eﬀectiveness of comparative explanations in assisting users better understand the recommendations and make more informed choices. Most explainable recommendation solutions exploit user reviews as the source of training data. They either directly extract from reviews or synthesize content to mimic the reviews. Extractionbased solutions directly select representative text snippets from the target item’s existing reviews. For example, NARRE (Chen et al., 2018) selects the most attentive reviews as the explanation, based on the attention that is originally learned to enrich the user and item representations for recommendation. CARP (Li et al., 2019) uses the capsule network for the same purpose. (Wang et al., 2018b) adopt reinforcement learning to extract the most relevant review text that matches a given recommender system’s rating prediction. (Xian et al., 2021) extract attributes from reviews to explain a set of items based on users’ preferences. However, as such solutions are restricted to an item’s existing reviews, their eﬀectiveness is subject to the availability and quality of existing content. For items with limited exposure, e.g., a new item, these solutions can hardly provide any informative explanations. Generation-based solutions synthesize textual explanations that are not limited to existing reviews. One branch focuses on predicting important aspects of an item (such as item features) from its associated reviews as explanations (Wang et al., 2018a; Tao et al., 2019; He et al., 2015; Ai et al., 2018). For instance, MTER (Wang et al., 2018a) and FacT (Tao et al., 2019) predict item features that are most important for a user to justify the recommendation. They rely on predeﬁned text templates to deliver the predicted features. The other branch applies neural text generation techniques to synthesize natural language sentences. In particular, NRT (Li et al., 2017) models item recommendation and explanation generation in a shared user and item embedding space. It uses its predicted recommendation ratings as part of the initial state for explanation generation. MRG (Truong and Lauw, 2019) integrates multiple modalities from user reviews, including ratings, text, and associated images, for multi-task explanation modeling. Our work is closely related to two recent studies, DualPC (Sun et al., 2020) and SAER (Yang et al., 2021), which focus on strengthening the relation between recommendations and explanations. Speciﬁcally, DualPC introduces duality regularization based on the joint probability of explanations and recommendations to improve the correlation between recommendations and generated explanations. SAER introduces the idea of sentiment alignment in explanation generation. However, both of them operate in a pointwise fashion, i.e., independent explanation generation across items. Our solution focuses on explaining the comparisons between items. We should also emphasize our solution is to explain the comparison among a set of recommended items, rather than to ﬁnd comparable items (McAuley et al., 2015; Chen et al., 2020). There are also solutions exploiting other types of information for explainable recommendation, such as item-item relation (Chen et al., 2021), knowledge graph (Xian et al., 2019) and social network (Ji and Shen, 2016). But they are clearly beyond the scope of this work. Item recommendation in essence is a ranking problem: estimate a recommendation score for each item under a given user and rank the items accordingly, such that the utility of the recommendations can be maximized (Rendle et al., 2012; Karatzoglou et al., 2013). Instead of explaining how the recommendation scores are obtained, our work emphasizes on explaining how the comparisons between the ranked items are derived. To learn the explanation model, we assume an existing corpus of item reviews from the intended application domain (e.g., hotel reviews). Each review is uniquely associated with a user u and an item c, and a user-provided rating r reviews associated with user u to construct his/her proﬁle Ω where x opinion rating. r 2010); otherwise oﬀ-the-shelf sentiment analysis methods can be used for the purpose (interested users can refer to (Wang et al., 2018a; Zhang et al., 2014) for more details). We create the item proﬁle as Ψ Unlike the user proﬁle, the item proﬁle does not include ratings. This is because the ratings from diﬀerent users are not directly comparable, as individuals understand or use the numerical ratings diﬀerently. Our solution is agnostic to the number of entries in user proﬁle Ω in each user and item. We impose a generative process for a tuple (x, r and Ω from Ψ can be understood as the user will ﬁrst browse existing reviews of the item to understand how the other users evaluated this item. Then he/she will rewrite this selected sentence to reﬂect his/her intended opinion and own writing style. This can be considered as a set to sequence generation problem. For our purpose of explanation generation, we only concern the generation of opinionated text x. Hence, we take opinion rating r where P u, and P resulting model Comparative Explainer, or CompExp in short. In Eq Eq (1), P user u’s historical opinionated text content. To understand this, we can simply rewrite its condition part: deﬁne ∆r whether the sentence x conditioned on user u’s historical content Ω suggests the opinion conveyed in x is the i-th review sentence extracted from user u’s reviews and ris the corresponding = {x, x, ..., x}, where xis the j-th review sentence extracted from item c’s existing reviews. . We assume when user u is reviewing item c, he/she will ﬁrst select an existing sentence that is mostly related to the aspect he/she wants to cover about the item. Intuitively, this (x|r, Ω) speciﬁes the probability that xfrom item proﬁle Ψwill be selected by user (x|x, r, Ω) speciﬁes the probability that user u will rewrite xinto x. We name the note, P rfor item c by user u. One can parameterize P parameters based on the maximum likelihood principle over observations in Ω likelihood alone is insuﬃcient to generate high-quality explanations, as we should also emphasize on ﬂuency, brevity, and diversity of the generated explanations. To realize this generalized objective, assume a metric π(x|u, c) that measures the quality of generated explanation x for user u about item c, the training objective of CompExp is set to maximize the expected quality of its generated explanations under π(x|u, c), In this work, we present a customized BLUE score speciﬁcally for the comparative explanation generation problem to penalize short and generic content. Next, we dive into the detailed design of CompExp in Section 3.1, then present our metric π(x|u, c) for parameter estimation in Section 3.2 and 3.3, and ﬁnally illustrate how to estimate each component in CompExp end-to-end in Section 3.4. (x|x, r, Ω) quantiﬁes if x is a good rewriting of xto satisfy the desired opinion rating