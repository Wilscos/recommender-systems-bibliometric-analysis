Graph convolutional networks (GCNs) have recently enabled a popular class of algorithms for collaborative ﬁltering (CF). Nevertheless, the theoretical underpinnings o f their empirical successes remain elusive. In this paper, we endeavor to obtain a better understanding of GCN-based CF methods via the lens of graph signal processing. By identifying the critical role of smoothness, a key concept in graph signal processing, we develop a uniﬁed graph convolution-based framework for CF. We prove that many existing CF methods are special cases of this framework, including the neighborhood-based methods, low-rank matrix factorization, linear auto-encoders, and LightGCN, corresponding to diﬀerent lowpass ﬁlters. Based on our framework, we then present a simple and computationally eﬃcient CF baseline, w hich we shall refer to as Graph Filter based Collaborative Filtering (GF-CF). Given an implicit feedback matrix, GF-CF can be obtained in a closed form instead of expensive training with back-propagation. Experiments will show that GF-CF achieves competitive or better performance against deep learning-based methods on three well-known datasets, notably with a 70 % performance gain over LightGCN on the Amazon-book d ataset. • Information systems → Recommender systems. collaborative ﬁltering, graph convolution, graph signal processing ACM Reference Format: Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, Khaled B. Letaief, Dongsheng Li . 2021. How Powerful is Graph Convolution f or Recommendation?. In CIKM ’21: ACM International Conference on Information and Knowledge Management, Nov 01–05, 2021, Queensland, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3459637.3482264 Recommender systems have achieved great successes in many businesses, e.g., for produc t recommendation on Amazon [25] and playlist generation on Youtube [7], etc. As the algorithmic eﬀectiveness will have a direct impact on the commercial success, building a good recommendation engine, especially via collaborative ﬁltering (CF), remains an active research area, with consistent innovations in both conventional methods [6, 21, 31] and recently emerged deep learning approaches [13, 15, 24]. Over the past decade, we have witnessed great progress in CF algorithms. Mod el-based methods largely resort to low-dimensional structures in high-dimensional data [42], e.g., low-rank matrix factorization [6, 17, 22, 28] and autoencoders [24, 36, 43]. On the other hand, neighborhood-based methods [1, 39] achieve competitive performance based on simple similarity measures, e.g., the cosine similarity between items. Furthermore, these two types of methods can be incorporated together to improve the performance, e.g., SVD++ [21]. From t he graph perspec tive, the neighborhood-based methods and SVD++ eﬀectively exploit the one-hop information in the user-item interaction graph. To take advantage of the rich multi-hop neighborhood information, graph convolutional networks (GCNs), e.g., GC-MC [4], NGCF [41], LightGCN [13], have been recently propo sed and become state-of-the-art methods for CF. NGCF [41] was inspired by the GCNs developed for attribute graphs [19], and it inherits the key ingredients from GCNs, including initial embeddings, feature transformation, neighborhood aggregation, and nonlinear activation. As the graphs in CF tasks are non-attributed, these operations may not be necessary [13]. Therefore, in LightGCN [13], only the most import ant components, i.e., trained initial embeddings and graph convolution, are preserved. Removing the unnecessary components leads t o easier training and better generalization [46, 47 ], and thus LightGCN signiﬁcantly outperforms NGCF in both accuracy and eﬃciency. While these empirical studies have produced promising results, the underlying reasons for the eﬀectiveness of these methods remain elusive. From the theoretical perspective, an intriguing q uestion is what plays an essential role in the success of GCN-based methods for CF. From the practical perspective, it is interesting to investigate to what extent we can reduce the training cost while eﬀectively exploiting the rich information of the user-item interaction graph. This paper endeavo rs to obtain a better understanding of GCNbased methods and develop a uniﬁed framework based on graph convolution that incorporates classic method s. In particul ar, we identify the importance of a key concept in graph signal processing in developing CF algorithms, namely, smoothness. Conceptually, if a user interacted with an item, then their embe ddings should be similar. In graph signal processing, the similarity between the embeddings of the interacted user-item p air deﬁnes the smoothness of the embedding. Meanwhile, low-pass ﬁlters on graphs, e.g., the light convolution in LightGCN [13], are used to promote the smoothness of graph signals. We will therefore argue that it is the smoothness of the embeddings and the low-pass ﬁltering that play a pivotal role in GCN-based methods. By theo retical analysis and experiments, we will show that the performance of untrained LightGCN is competitive to a trained one when the embedding dimension is suﬃciently large, due to the smoothing eﬀect of the light convolution. Inspired by this ﬁnding, we derive a closed-form solution for the untrained LightGCN with Inﬁnitely Dimensional Embe dding (LGCN-IDE). It is shown that LGCN-IDE outperforms LightGCN by more than 40% on the Amazon-book dataset. Motivated by its simplicity and eﬀectiveness, we extend LGCNIDE t o incorporate general low-pass ﬁlters, which form a uniﬁed framework for CF. Surprisingly, it is proved that the neighborhoodbased methods [1], low-rank matrix factorization [6] , and linear auto-encoders [36] are all special cases of this framework with various classic low-pass ﬁlters. This ﬁnding veriﬁes the eﬀectiveness of graph convolution with low-pass ﬁlters for CF. We further present a simple and computationally eﬃcient CF method, which is an integration of linear ﬁlters and an ideal low-pass ﬁlter. Given an implicit fe edback matrix, our proposed method has a closed-form solution and as such it would not require expensive training. More importantly and despite of its simplicity, the proposed method achieves competitive or better performance compared with deep learning methods. To summarize, this work has made the following contributions. (1) By identifying the critical role of the smoothness and lowpass ﬁltering, we provide a novel perspective to understand the algorithms fo r CF. (2) Using both theoretical justiﬁcation and experiments, we show that the untrained LightGCN can achieve competitive performance as a trained one when the embedding dimension is suﬃciently large. We further derive a closed-form solution for untrained LightGCN with inﬁnitely dimensional embedd ing. (3) Built up on the closed-form solution, we develop a general graph ﬁlter-based framework for CF. We prove that the neighborho odbased methods, linear au to-encoders, and low-rank matrix factorization are special cases of this framework, corresponding to various classic low-pass ﬁlters. (4) We present a simple and computationally eﬃcient method, named GF-CF. With a small fractio n of training time, GFCF achieves competitive or higher performance compared with the state-of-the-art deep learning methods on three well-known datasets. The rest of this paper is organized as follows. Section 2 introduces some preliminaries for the rest of this paper. Sec tion 3 demonstrates the importance of smoothness. Section 4 provides the details of our method. Section 5 presents the experimental results. Section 6 discusses the related works in CF and GCNs. Finally, we conclude this work in Section 7. The code to reproduce the experiments is available at https://github.com/yshenaw/GF_CF. This subsect io n presents some useful notations and deﬁnitions. We ﬁrst deﬁne user set U and item set I. As in [13], this paper considers the recommendation problem with implicit feedback. The implicit feedback matrix 𝑹 ∈ {0, 1}is deﬁned as follows: 𝑹=1, if (𝑢, 𝑖) interaction is observed, and 𝒓denotes the 𝑢-th row of 𝑹. The adjacency matrix of the user-item interaction graph is given In this bipartite graph, we denote the neighbors of node 𝑘 as N, and its cardinality as 𝑁= |N|. We denote the all one column vector of any dimension as 1, and degree matr ices as 𝑫= Diag(𝑹 · 1) and 𝑫= Diag(1𝑹). The normalized rating matrix is denoted as with˜𝒓as the 𝑢-th row of˜𝑹. Similarly, the normalized user-item adjacency matrix is given by We t hen deﬁne an impo rtant concept, namely, Stiefel manifold, which can help to connect low-rank matrix factorization and GCNbased methods in Section 4.2. Deﬁnition 2.1. (Stiefel manifold) The Stiefel manifold St(𝑛,𝑚) is deﬁned as the subspace of orthonormal N-frames in R, namely, where 𝑰 is the identity matr ix . In this subsection, we introduce basic concepts of graph signal processing [8, 29]. We consider a weighted undirected graph G = (V, E) with 𝑛 nodes where V and E denote the vertex set and edge set, respectively. The graph can be represented as an adjacency matrix 𝑨 ∈ R. A graph signal is deﬁned as a function 𝑥 : V → R and it can be represented as a 𝑛-dimensional vector 𝒙 = [ 𝑥 (𝑖)]. For a graph signal, the derivative is deﬁned asp The smoothness of a graph signal can be measured by the graph quadratic form, which is t he squared norm of the graph derivative as deﬁned below: Here, 𝑳 = 𝑫 − 𝑨 is the graph Laplacian matrix. A smaller indicates smoother signals. In many applications, a graph signal is often described in a vector form 𝑥 : V → Rand its smoothness can be written as follows: As 𝑳 is real and symmetric, its eigendecomposition is given by 𝑳 = 𝑼 𝚲𝑼where 𝚲 = Diag(𝜆, ··· , 𝜆), 𝜆≤ 𝜆≤ ··· ≤ 𝜆, and 𝑼 = [𝒖, ··· , 𝒖] with 𝒖∈ Rbeing the eigenvector for eigenvalue 𝜆. Next we discuss the frequency of the graph signal and deﬁne Fourier Transform on graphs. Intuitively, the graph signal has a higher frequency if it is more oscillatory and not smooth. As 𝜆 is the smallest eigenvalue, for any graph signal 𝒙 ∈ R, we have ≥. Thus, the eigenve c tor with a smaller eigenvalue corresponds to a lower frequency signal component. We can deﬁne the Graph Fourier Transform (GFT) basis as the eigenvector matrix 𝑼 and we callˆ𝒙 = 𝑼𝒙 as the GFT of the graph signal 𝒙. Similar to the Fourier transform, GFT is a linear orthogonal transform and its inverse transform is given by 𝒙 = 𝑼ˆ𝒙. GFT enables us to deﬁne graph ﬁlters and graph convolution. Deﬁnition 2.2. (Graph Filter) Given a graph Laplacian matrix, as well as its eigenvectors and eigenvalues, then the graph ﬁlter H(𝑳) is deﬁned as follows: where ℎ(·) is the ﬁlter deﬁned on the eigenvalues. Deﬁnition 2.3. (Graph Convolution) The graph convolution of a input signal 𝒙 and the ﬁlter H(𝑳) is deﬁned as follows: Similar to the deﬁnition o f convolution in classic signal processing, the graph signal is transformed b y GFT 𝑼, multiplied by a ﬁlter ℎ(·), and t ransformed back by inverse GFT 𝑼 . In the context of CF, the graph signal is often the observed ratings for a given user [6], or the initial embeddings of users/items [1 3, 41]. In the signal processing literature, the signal is often smooth and with low frequency, and the noise is often non-smooth and with a high frequency. One important class of ﬁlters is the low-pass ﬁlters, which promotes smoothness of graph signals for denoising. The graph low-pass ﬁlters are deﬁned as follows. Deﬁnition 2.4. (Low-pass Filter) For 𝑘 = 1, ··· , 𝑛 − 1, we deﬁne the ratio The graph ﬁlter H(𝑳) is 𝑘-low-pass if and only if the low-pass ratio satisﬁes 𝜂∈ [0, 1). The low-pass ratio deﬁnes how much of t he high-frequency component of a signal is allowed to pass compared to the lowfrequency components. If 𝜂< 1, then the ﬁlter passes low-frequency signals and is called a low-pass ﬁlter. We here list so me impo rtant low-pass ﬁlters and will connect these ﬁlters with cl assic methods for recommendation in Section 4.2. Linear Fil te r. T he linear ﬁlter is given by where 𝛼is the ﬁlter’s coeﬃcient. It is called linear due to its similarity with linear time invariant ﬁlters in classic signal processing. We w ill show that this ﬁlter corresponds to LightGCN and neighborhood-based methods. Ideal Low-pass Filter. The ideal l ow-pass ﬁlter has a cut-oﬀ frequency¯𝜆. The ﬁlter is d eﬁned as It is called id e al as the high-frequency signals are ideally cut oﬀ with no leakage. We will show that this ﬁlt er corresponds to the low-rank matrix factor iz ation method. Opinion Dynamics. The opinion dynamics are a graph diﬀusion process, which is a GF-AR(1) model [11]: 𝒚= (1−𝛽)(𝑰 −𝛼𝑳)𝒚+ 𝛽𝒙. The steady state opinions are given by 𝒚 = lim𝒚= (𝑰 + ˜𝛼𝑳)𝒙 = H(𝑳)𝒙 where ˜𝛼 = 𝛽 (1 − 𝛼)𝛼. Thus, the corresponding graph ﬁlter is In opinion dynamics, the matrix inverse or eigenvalue decomposition is required. Thus, applying this ﬁlter introduces a high memory cost. We will show that it is closely related to the linear autoencoder method. LightGCN [13] is a state-of-the-art GCN-based method in CF. In this paper, LightGCN will be used as the vehicle to elaborate our theory and adopted as t he main baseline for performance comparison. LightGCN leverages the user-item interaction graph to propagate the embedding as follows: where 𝑬∈ Ris the learnable initial embedding matrix of users and items. For a 𝐾-layer LightGCN, the ﬁnal embeddings can be computed as follows: The model prediction is deﬁned as the inner product of the user’s and item’s ﬁnal representation 𝑦= 𝒆𝒆, where 𝒆and 𝒆are the corresponding rows of 𝑬 . To optimize LightGCN, the Bayesian personalized ranking (BPR) loss [31] is adopted: Figure 1: Performance of untrained LightGCN versus SOTA, where the SOTA line is LightGCN’s performance reported in [13] and LightGCN-R denotes the untrained LightGCN with diﬀerent embedding dime ns ions. In this section, we identify the importance of smoothness and lowpass ﬁlters in CF, by using the light convolution in LightGCN as a speciﬁc example. The embeddings play an essential role in CF while smoothness is a key concept in graph signal processing. We observe that there are strong connections between the good embeddings and their smoothness on the graph. We consider the dot product based embedd ing model. Speciﬁcally, let 𝒆denote the embedding for the 𝑢-th user and 𝒆denote the embedding for the 𝑖-th item. The predicted score for the 𝑢-th user and 𝑖-th item is deﬁned by the dot product 𝒆𝒆. If 𝑹= 1, we should promote the similarities between 𝒆and 𝒆. By the deﬁnition of smoothness of graph signals (3), if 𝒆and 𝒆are similar for connected user-item pairs, the embedd ings are smooth signals on the graph. Consequently, optimizing loss functions, e.g., BPR loss (9), and enhancing the smoo thness of the embeddings share the same goal: promoting the similarity between 𝒆and 𝒆for 𝑹= 1. The above discussion provides a qualitative intuition for the role of smoothness in embeddings. We will now analyze the linear ﬁlter in LightGCN to obtain quantitative results. The LightGCN consists of two components: the initial embedding and a linear ﬁlter. If the untraine d LightGCN achieves good performance, it must be the linear ﬁ lter playing the essential role as the initial embedding is random. The next proposition shows that untrained LightGCN will have a low BPR loss under certain conditions. Theorem 3.1. Denote 𝑁= max𝑁and 𝑁= min𝑁 where 𝑖 ∈ U ∪ I. If 𝑬∈ Rfollows an i.i.d. uniform distribution over the unit sphere with then for a one-layer untrained LightGCN, we have P𝒆𝒆> 𝒆𝒆|(𝑢,𝑖) ∈ S, (𝑢, 𝑗) ∈ S≥ 3/4, (11) where 𝐶 is an absolute constant, S= {(𝑢, 𝑖)|𝑹= 1}, S= {(𝑢, 𝑗)|𝑹= 0}. Remark. (Interpretations of Theorem 3.1) Equation (11) implies a low BPR loss as the predicted score of any positive pair is larger than that of any negative pair. Due to the smoot hing eﬀect of light convolution (linear ﬁlters), the ﬁnal embe ddings between interacted pairs are similar even if the initial embeddings are random. In Equation (10), 𝑁and 𝑁are adopted for a worst-case analysis, and in pr actice, we can replace them with the average degree. Equation (10) shows that the required embedding dimension of untrained LightGCN grows with the dataset density, which implies untrained LightGCN is more eﬀective on sparse datasets. For the Gaussian initialization adop ted in [13], the results are similar as high dimensional Gaussian random vectors concentrate around a sphere (refer to Sectio n 3.1 in [38]). The probability 3/4 can be improved to any probability approaches arbitrarily close to 1. The high-level reason for untrained LightGCN performing well is that the information contained in the rating matrix and the graph are identical. The BPR loss is adopted for exploiting t he information in rating matrix while the low-pass ﬁlters are to exploit information in the graph. T hus, a proper use of low-pass ﬁlters can accelerate the training or even avoid the training. Interestingly, some recent works also reveal that inﬁnitely wide random CNNs achieve better performance than trained ones [2]. Based on Theorem 3.1, we argue that the performance of untrained LightGCN improves with the embedding dimension and it should be co mpetitive to a trained o ne when the embedding dimension is suﬃciently large. To verify t his argument, we follow the experiment settings in [13] and condu ct the experiments for a 3 -layer untrained LightGCN. The initial embeddings 𝑬is initialized following an i.i.d. Gaussian distribution N(0, 0.1) as in the original paper. Once the model is initialized, we do not train it but simply compute the user/item embeddings using Equation (8) and then directly test it on the test dataset. We use two sp ar se datasets, i.e., Gowalla and Amazon-book. The test performance versus the embedding dimension is shown in Fig. 1. As the training/test splitting of two datasets is identical to [13], we regard the LightGCN’s performance reported in [13] as the state-of-the-art. We will also co mpare to LightGCN with large embedding dimensions in Table 4. The experiments agree with our theory well. As the linear ﬁlter is t o promote the smoothness, it demonstrat es t he crucial role of smoothness in CF. However, the untrained LightGCN is not a practical algorithm for recommendation as the large embedding dimension leads to an expensive memory cost and inference time. Fortunately, the untrained LightGCN with inﬁnitely dimensional embedding has a closed-form solution for predicted scores, as shown in the next theorem. Theorem 3.2. Consider an untrained LightGCN with 𝑬∈ Rfollowing an i.i.d. distribution with zero mean and non-zero variance. As 𝑑 → ∞, the predicted score of the untrained LightG CN follows where 𝛽are constants depending on [ 𝛼]in (8). Remark. (Interpretations o f˜𝑹˜𝑹) As shown in Theorem 3.2, the gram matrix˜𝑹˜𝑹 plays a pivotal role. For sparse binary data, (𝑫𝑹𝑹𝑫 deﬁnes the cosine similarity between item 𝑖 and item 𝑗 [1]. Likewise, (˜𝑹˜𝑹)provides a similarity measure between item 𝑖 and item 𝑗. Directly using the gram matrix as the item-item similarity results in the neighborhood-based method [ 1], which was the winner of Millions of Song Competitio n. The similarity between the neighborhood-based method and LightGCN is not surprising as LightGCN is based on the neighborhood p ropagation. As LightGCN consists of multi-hop propagation, the term˜𝑹˜𝑹 appears as polynomials. From the graph signal processing persp ective, it is a linear ﬁlter, which is low-pass. We call (12) as LightGCN with Inﬁnitely Dimensional Embedding (LGCN-IDE). The performance of LGCN-IDE is shown in Table 3. Remarkably, we see that on Amazon-book dataset, it outperforms the performance of LightGCN reported in [13] by more than 40% under exactly the same t raining/test data splits. In this section, we ﬁrst extend LGCN-IDE to incorporate general low-pass ﬁlters, which form a uniﬁed framework. Then we p rove that this framework uniﬁes the neighborhood-based approaches, low-rank matrix factorization, linear auto-encoders, and linear graph convolutional networks, where diﬀerent methods correspond to diﬀerent low pass-ﬁlters. Finally, we present a simple yet eﬀective algorithm for CF. In this subsection, we extend (12) to incorporate general graph ﬁlters. To simplify the notations, we denote˜𝑷 =˜𝑹˜𝑹 in the remaining of the article. Note that˜𝑷 can also be seen as a normalized adjacency matrix for an item-to-item graph, whose eigenvalues are between 0 and 1. The graph Laplacian of the item-to-item graph is deﬁned as ˜𝑳 = 𝑰 −˜𝑷. In t his way, we can apply graph signal processing to the item-to-item graph. Next we elaborate our uniﬁed framework, which is an extension of (12) with general graph ﬁ lters. We consider an input graph signal¯𝒓, which is so me transformation of the users’ observed ratings 𝒓. Then a low-pass ﬁlter is applied to the graph signal to obtain a ﬁ ltered signal. Finally, we may scale the obtained graph signal to get the ﬁnal prediction scores. Denoting the eigendecomposition by˜𝑳 = 𝑼 𝚲𝑼, the framework is given where¯𝒔is the ﬁltered predicted score, and ℎ (·) is a low-pass ﬁl ter. From the graph signal processing perspective, it is a graph convolution, i.e., a graph signal 𝒓∈ Rconvolving with a low-pass ﬁlter ℎ(·). Interestingly, some classic works for recommendation can be interpreted as graph signal processing approaches, where the low-pass ﬁlter plays an essential role. The classic methods typically involves auto-encoder-based [24, 27, 36], matrix factorization-based [6, 31], and GCN-based ones [13, 41, 51]. In this subsection, we will p rovide a uniﬁed view of the linear methods from the graph signal processing perspective. As the spectral convolution can be transformed into a sp atial convolution in GCNs by ﬁrst-order approximation [19], it is interesting to investigate what kind of GCNs will these classic methods induce. These GCNs induced by classic algorithms can also be se en as white-box neural networks [5]. A test of performance for these GCNs is left for future works. 4.2.1 Low-rank Matrix Factorization. Low-rank matrix factorization is one of the most classic algorithms for CF. Note that GFT is also a matrix factorization where the low-frequency signal comp onents correspond to t he principle comp onents of the rating matrix. This observation allows us to connect MF and graph-based methods. We take t he obje ctive function in a recent work [6] as an example. Denote 𝑑 as the embedding dimension, the model is given As shown in [ 6], 𝑽contains the smallest 𝐾 eigenvectors of˜𝑳 and 𝑼= 𝑹𝑫𝑽 . Viewing the eigendecomposition as GFT, it can be interpreted as an ideal low-pass ﬁlter (6) We then turn low-rank matrix factorizat ion into a spatial convolution fashion. This is more diﬃcult t han the conversion in GCN [19] due to the orthogonal constraint and non-convexity of problem (14). Observing that t he optimal solution 𝑽to (14) is also the optimal solution to the fol lowing problem We can rewrite (15) as spatial convolution by ﬁrst-order expansion like GCNs [19]. We begin with a random 𝑬∈ R, and the update rule is given by where (a) follows Proposition 7 in [ 18]. The ﬁnal embeddings are given b y Note that (16) is a spatial graph convolution, and (17) is coincidentally equivalent to convolutional normalization for CNNs [26] (refer to (6)-(8) in [26]). The convolutional normalization was proposed to accelerate the training of convolutional networks and improve robustness. From this view, the low-rank matrix factorization is equivalent to an inﬁnite layer GCN with convolutional normalization. As the number of layers is large, it suﬀers from the over-smoothing issue [23], w hich hurts the performance. 4.2.2 Linear Auto-encoders. In the linear auto-encoders, e.g., EASE [34] and SLIM [ 27], the predicted score vector of a u ser (¯𝒔) is obtained by the dot product where 𝑩 ∈ Ris a learnable weight matrix. The training ob-Í jective is some regularized or constrained version of mink¯𝒓− ¯𝒓𝑩k. From the graph signal pro cessing view, it can be interpreted as a graph signal¯𝒓convolving with a ﬁlter 𝑩, and¯𝒓=¯𝒓𝑩 is a steady state. This deﬁnes a graph diﬀusion on the corresponding graph like op inion dynamics (7) . Next, we show the e quivalence between a speciﬁc version of linear auto-encoders and the graph diﬀusion ﬁlter. As shown in [36], the foll owing linear auto -encoder is able to achieve competitive performance compared with the deep ones [24, 43]. Speciﬁcally, we consider the following formulation in [36] for simplicity: As (18) is a ridge regression, we can write down t he closed-form solution as Viewing the eigenvalue d ecomposition as GFT, the graph ﬁlter in (19) is given by To understand (20) in the content of low-pass ﬁlters, the low-pass ratio 𝜂in (4) is given by 𝜂==(1 − 𝜆)(1 + 𝜇 − 𝜆)(1 − 𝜆)(1 + 𝜇 − 𝜆) The convolutional ﬁlter in (20) is similar to opinion dynamics and is a kind of graph diﬀusion ﬁlter. Like other diﬀusion-based methods, t he memory cost of linear auto-enco der is high as we need to store matrix 𝑩 in (18). In the literature, the Neumann series are often adopted to convert the graph diﬀusion into a spatial convolution [20, 44 ]. Similarly, we can use it to interpret (19) as spatial GCNs. For 𝜇 > 1, (19) can be written as ! From this view, the initial embedding is an identity matrix 𝑬= 𝑰 ∈ R, the update of corresponding spatial convolution is given by and the ﬁnal embeddings can be obtained as The layer combination appears naturally and the coeﬃcients decrease quickly. As discussed in [13], the layer co mbination is the key to alleviate the over-smoothing issue and improve performance. 4.2.3 Neighborhood-based Approaches. The neighborhood-based approaches are often considered as exploiting ﬁrst-order graph information in the literature discussions [13]. We consider the following formulation, which utilizes the gram matrix as the similarity matrix [1], i.e.,¯𝒔= 𝒓˜𝑷. Obviously, the corresponding ﬁlter is a ﬁrst-order linear ﬁlter and the corresponding spatial GCN is a one-layer GCN. This ap proach is simple and scalable. However, it lacks higher-order information on the graph. 4.2.4 LGCN-IDE. For completeness, we analyze LGCN-IDE (12). By eigendecompo sition, the corresponding ﬁlter takes the form of Since it is still a LightGCN, LGCN-IDE naturally corresponds to multi-layer spatial GCN w it h a layer combination. In this subsection, we develop a simple yet eﬀective baseline algorithm, whose training is as eﬃcient as the inference of LightGCN with a big-O notation. We ﬁrst analyze the inference computational complexity of LightGCN. We denote the number of nonzero elements in 𝑹 as 𝜂. For a LightGCN with 𝑑-dimensional embedd ing, the inference time is O(𝜂𝑑). The general graph ﬁlters require eigendecomposition and thus are not eﬃcient for large-scale recommendation [13]. Fortunately, there are some graph ﬁlters that enjoy a high computational eﬃciency, i.e., linear ﬁlt ers and ideal low-pass ﬁlters. In order to obtain linear ﬁlters, only the normalization is required during training, and thus the t raining complexity is O(𝜂). A major drawback of the linear ﬁlters is that they can hardly obtain a high-order information of the graph. For the ideal low-pass ﬁlter, only t he top-K eigenvectors of˜𝑷 are required. Nevertheless, a direct computation for the top-K eigenvector of˜𝑷 is far from computation and memory eﬃcient because ˜𝑷 is not as sparse as 𝑹. By using the equivalent formulation in (15), the largest eigenvectors can be computed by (16), (17), and this it erative algorithm is called the generalized power method (GPM) in the optimization literature [18]. In GPM, we only need to store 𝑹 instead of˜𝑷, and the computational complexity is O((𝑑𝜂 + 𝑑) log(1/𝜖)) where 𝜖 is the desired accuracy for the eigenvectors. This algorithm is eﬃcient as long as 𝑑< 𝜂. As discussed before, the ideal low-pass ﬁlter is equivalent to an inﬁnite layer GCN without layer combination and it suﬀers from over-smoothing, which means that it lacks a low-order information in the graph. As a result, we argue that combining the linear ﬁlter and ideal low-pass ﬁlter will result in a strong baseline. Speciﬁcally, our proposed algorithm, named as Graph Filter based Collaborative Filtering (GF-CF), has the following form where 𝒔and 𝒓denote predict ed and o bserved scores, respectively. Likewise,¯𝑼 is the top-K singular vectors of˜𝑹, and 𝛼 is the tuned parameter. We acknowledge that learning 𝛼 or transforming (22) into GCNs may lead to better performance. Nevertheless, we will demonstrate that (22) already achieves the state-of-the-art performance. In this section, we ﬁrst describe the experimental settings, which exactly follow [1 3]. Next, we compare our method with the stateof-the-art deep learning methods. To keep the comparison fair, we use the same datasets, the same train/test splitting, and the identical evaluation metric as in [13]. The statistics of the datasets are listed in Table 2. The evaluation metrics are recall@20 and ndcg@20. Table 2: Statistics of the experimented data. Table 3: The comparison of overall p erformance among GFCF and competing methods. The performance of benchmarks is reproduced from [13]. Mult-VAE0.1641 0.1335 0.058 4 0 .04 50 0.0407 0.0315 GRMF0.1477 0.1205 0.057 1 0 .04 62 0.0354 0.0270 GRMF-norm0.1557 0.1261 0.056 1 0 .04 54 0.0352 0.0269 LightGCN0.1830 0.1554 0.0649 0.0530 0.0411 0.0315 GF-CF0.1849 0.1518 0.0697 0.0571 0.0710 0.0584 5.1.1 Benchmarks. We follow [13] to set up the benchmarks. (1) LightGCN [13]: LightGCN is the state-of-the-art method for CF. Please refer to Section 2.3 for a detailed description. (2) NGCF [41]: NGCF is a nonlinear deep GCN-based method. Besides the components in LightGCN, it contains of feature transformation, and nonlinear activation. (3) GRMF and GRMF-norm[13, 30]: GRM F ad ds a graph Laplacian regularizer to the training objective of BPR loss in matrix factorization. In GRMF-norm, the normaliz ed Laplacian is adopted instead of the graph Laplacian. (4) Mult-VAE [24]: This is a variational autoencoder based method. The d ata is assumed to be generated by a multinomial distribution and variational inference is adopted to estimate the parameters. In [41], it has been shown that NGCF outperforms GC-MC [4], Pinsage [49], NeuMF [15], CMN [10], MF [31], H OP-Rec [48] on the same train/test splitting. Thus, we will not include these methods as benchmarks. We also do not compare with full rank models [27, 35] due to the out of memory on Amazon-book dataset. The hyperparameter settings are identical to [13]. For the proposed graph ﬁlter based methods, we focus on the following two variants: (1) GF-CF: The proposed simple baseline method fo r CF in (22). (2) LGCN-IDE: The untrained LightGCN with inﬁnitely dimensional embedding. The closed-form is given in (12). For the implementation of graph ﬁl ters, we adopt Scipy [40] for sparse operation. The performance of the proposed methods and other benchmarks are shown in Table 3. Despite the simplicity, GF-CF achieves competitive or better performance than deep learning-based methods. 5.2.1 LGCN-IDE versus LightGCN. LGCN-IDE is an untrained LightGCN with an inﬁnitely dimensional embedding. On Gowalla and LightGCN-128 LightGCN-256 LightGCN-512 Yelp2018, which are of small sizes, LightGCN outperforms LGCNIDE. H owever, LGCN-IDE o utperforms LightGCN by a large margin on the large-scale dataset, i.e., the Amazon-book dataset. In LightGCN, the known scores are compressed into limited dimensional vectors, which restricts the expressiveness. In contrast, in LGCN-IDE, the ratings are directly used as the graph signal without compression. Additionally, LightGCN is trained with a stochastic gradient descent (SGD) while LGCN-IDE has a closed-form solution. As the size of the dataset increases, the optimization by SGD becomes more diﬃcult. We suspect that these two reasons contribute to the large performance gain of LGCN-IDE over LightGCN in the Amazon-book dataset. 5.2.2 Graph filters versus deep learning-based methods. In Table 3, the simple graph ﬁlter achieves competitive o r better performance compared with deep learning-based method s. LightGCN also outperforms NGCF by removing the non-linear transformations. From the universal approximation theory [16], deep neural networks can approximate linear functions easily. Nevertheless, linear functions are non-trivial to learn for a neural network trained with SGD. A recent theoretical study demonstrates that it is impossible for neural networks with tanh, cosine, or quadratic activation to extrapolate the linear functions well [ 46]. With ReLU activation, A neural network can extrapolate linear functions well if the training dat a cover all directions (e.g., a hypercube covering the origin) [46], which is not trivial to satisfy in practice. This theoretical result suggests that learning linear functions is a non-trivial task. In addition, deep neural networks do well in extracting complicated features, but CF with implicit feedback is in lack of rich features. Owing to these two facto rs, the linear models are able to outperform deep models in CF with implicit fee dback. In this subsection, we compare GF-CF with LightGCNs of diﬀerent embedding dimensions. For the untrained LightGCN, the performance improves signiﬁcantly with the dimension as shown in Fig. 1. The natural questions are 1) does the performance of trained LightGCN increase signiﬁcantly as the dimension grows; 2) how does GF-CF perform compared with LightGCN with large embed ding dimensions. We validate these questions empirically in Table 4. The experiments in this subsection are conducted on a server with an Intel Xeon(R) CPU E5-2698 v4 @ 2.20GHz and a Tesla V100 GPU. For the implementation of LightGCN, we download the source code from https://github.com/gusye1234/LightGCN-PyTorch and train 1000 epochs as the original paper. Due to the excessive training cost, we do not train LightGCN with an embedding dimension of more than 512. As shown in Table 4, GF-CF still achieves competitive or higher performance than LightGCN with large embedd ing dimensions. As the embedding dimension grows, the performance improvement of LightGCN becomes marginal, which is similar to matrix factorization and neural collaborative ﬁltering [32]. The overall training time of GF-CF is even smaller than 1 training epoch consumed by LightGCN. It demonstrates that GF-CF is a simple but hard-to-beat baseline method for CF. Collaborative ﬁltering (CF) plays a fundamental role in modern recommender systems [7]. One popular paradigm is the model-based CF methods. In such methods, the users and items are parameterized by (low-dimensional) vectors and the interactions are reconstructed based on the embeddings and mod el weights. The classic matrix factorization (MF) maps the ID of users and items as embedd ing vectors and uses the dot product between embedding vectors as predicted scores. The dot product model can be further improved by using neural networks [15, 37]. Another classic modelbased CF is to reconstruct the score for an item by a transformation of the scores for other items, from linear aut o-encoders (e.g., SLIM [27]) to de ep auto-encoders (e.g., Multi-VAE [24]). Another paradigm is graph-based CF methods. The early works (e.g., Item-rank [12] and Bi-rank [14]) exploit the label propagation on graph and belong to the neighborhood-based methods. These methods are often co nsidered as heuristics and inferior to model-base d methods due to the lack of training. Recent works address this issue by developing GCN-based methods and train GCNs in an end-to-end manner, e.g., GC-MC [4], NGCF [41], and LightGCN [13]. Notice that the information contained in the sparse rating matrix or graph formulation are identical and GFT is a matrix factorization. In this paper, we unify the two paradigms from the graph signal processing view and identify that the low-pass ﬁlters are the underlying key comp onent in the two paradigms. In addition, we show that diﬀerent paradigms correspond to diﬀerent low-pass ﬁlters and these ﬁlters can be incorporate d together to improve the performance. The spectral GCNs are developed from graph signal processing with learned graph ﬁlters, which enjoy theoretical guarantees from graph signal processing theory [33]. Nevertheless, GFT requires full eigendecomposition, which induces prohibitive computation for large-scale graphs. The spe ctral CF [52] and LCF [ 50] belong to this category and thus they cannot be applied on large-scale datasets. To speed up the computation, the spatial GCNs based on 1-hop neighbor propagation were propo sed [45]. In each layer of spatial GCNs, only neighborhoo d aggregations are requ ired, and thus the computational cost is extensively reduced. In the context of recommendation, spatial GCNs contain GCMC [4], NGCF [41], LightGCN [13], and PinSage [49]. A unique advantage o f these methods is the scalability, meaning that they can be applie d to large-scale sparse datasets. A recent theoretical study uniﬁed the spect ral and spatial GCNs and demonstrates that they are all lowpass ﬁlters [3]. In the paper, we also unify the classic CF methods via low-pass ﬁltering, which explains the success of GCNs in CF. In this paper, we identiﬁed the importance o f smoo thness in the embeddings in a successful recommendation both theoretically and empirically, which bridges CF and graph signal processing theory. Via the lens o f graph signal processing, we showed that the neighborhood-based methods, low-rank matrix comp letion, and linear au to-encoders are all graph convolution with low-pass ﬁlters. This further validat ed the power of graph convolution for recommendation. In addition to our the oretical analysis, we also developed a simple but hard-to-beat baseline algorithm, GF-CF. It was demonstrated that GF-CF achieves competitive or better performance than deep learning-based methods. We believe that t he insights of this investigation are inspirational to the principled GCN architecture design for recommender systems. In the fu ture, we will implement the GCNs induced by classic algor it hms in Table 1 and exploit additional information, e.g., so c ial networks and knowledge graphs. Proof. We ﬁrst prove that (11) holds when the mutual coherence [9]) of the embeddings satisﬁes and then show that as 𝑑 >, (23) holds with probability at least 3/4. where (a) follows the assumption that 𝜖 <. With Lemma 8.1, we see that 𝜖 <as 𝑑 > Lemma 8.1. (Theorem 3.5 in [42]) Let 𝑨 ∈ Rwith rows i.i.d. chosen from the uniform distribution on the sphere. Then with probability at least 3/4, where 𝐶 is an absolute constant. Proof. We ﬁrst separate t he embeddings 𝑬into user embeddings 𝑼and item embeddings 𝑽, and the individual update is given by The ﬁnal embeddings are and 𝑼 can be compu ted similarly. The ﬁnal prediction of untrained LightGCN with inﬁnitely dimensional embedding is given by For a pair of matrices 𝑿, 𝒀 , if the rows of 𝑿 ∈ R, 𝒀∈ R follow independently identical distribution, due to the linearity of dot product, we have lim𝑿𝒀= E[𝒙𝒚], where 𝒙(resp. 𝒚) denotes the ﬁrst column of 𝑿 (resp. 𝒀 ). Thus, as 𝑑 → ∞, we have where 𝛽depends on [𝛼]. For a given user 𝑢, the estimated scores is shown asÍ diagonal, eigenvalues of˜𝑨are a concatenation of eigenvalues of ˜𝑹˜𝑹and˜𝑹˜𝑹. For the largest eigenvalue, we have where (a) follows Lemma 8.2. As˜𝑹˜𝑹 is positive semi-deﬁnite, Lemma 8.2. Let 𝜆≤ 𝜆≤ ·· ·𝜆be eigenvalues of˜𝑨. Then −1 ≤ 𝜆≤ 𝜆≤ ·· ·𝜆= 1. Thus, −1 ≤ 𝒙(𝑰 −˜𝑨)𝒙 ≤ 1. Furthermore, using the vector 𝒙 = 𝑫1, we get ˜𝑨𝒙 = 𝑫𝑨𝑫𝑫1 = 𝑫𝑨1 = 𝑫diag(𝑫) = 𝑫1. This implies that the largest eigenvalue of˜