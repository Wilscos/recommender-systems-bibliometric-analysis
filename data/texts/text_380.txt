The Softmax bottleneck was ﬁrst identiﬁed in language modeling as a theoretical limit on the expressivity of Softmax-based models. Being one of the most widelyused methods to o utput probability, Softmax-based models have found a wide range of applications, including session-based recommender systems (SBRSs). Softmaxbased models consist of a Softmax function on top of a ﬁnal linear layer. The bottleneck has been shown to be caused by rank deﬁciency in the ﬁnal linear layer due to its connection with matrix factorization. In this paper, we show that there’s more aspects to the Softmax bottleneck in SBRSs. Contrary to common beliefs, overﬁtting does happen in the ﬁnal linear layer, while it’s often associated with complex networks. Furthermore, we identiﬁed that the common technique o f sharing item embeddings among session sequence s and the candidate pool creates a tight-coupling that also contributes to the bottleneck. We pro pose a simple yet effective meth od, Dropout and Decoupling (D&D), to alleviate these problems. Our experiments show that our method signiﬁcantly improves the accuracy of a variety of Softmaxbased SBRS algorithms. When compared to other computationally expen sive methods, such as MLP and MoS (Mixture of Softmaxes), our method performs on par with and at times even better than those methods, while keeping the same time complexity as Softmax-based models. Recommender systems are essential tools for online services to guide users through vast volumes of digital contents. As technology enab le s the Internet to become faster and more ubiquitous, the amount of online data is growing at an exponential rate. This rapid growth imposes challenges to both onlin e service providers and their users with informa tion overload. Desig ned to serve the purpose of ﬁltering and prioritizing useful information, recommender systems have since fo und a wide range of applications in social media, eCommerce, and stream ing services. The Softmax bottleneck is a theoretical limit on the expressivity of Softmax-based mode ls. It was ﬁrst discovered by Yang et al. in language modeling. Languag e modeling faces a similar challenge as recommender systems when selecting the next word from a massive vocabulary. A linearSoftmax output layer is often adopted in these classiﬁcation problems, when the number of labels is several o rders of magnitudes larger than th e dimension o f hidden states. It consists of a Softmax function around logits, which are dot products between the hidden state and candidate word embeddings. The authors pointed out that its linearity causes the learned logit matrix to be low-rank . As a re sult, it limits the expressivity of the whole network, hence getting the name of the Softmax bottleneck. Linear-Softmax layers are widely used in session-based recommender systems (SBRSs), which take a sequence of items in the session a s input an d predict the next item from a candidate pool. There are several reasons behind this. First, the accura cy of SBRSs really be neﬁts from learning from the whole candidate pool, instead o f having a candidate selection me chanism. Sin ce the candidate pool is often massive in size, a linear-Softmax layer is almost the only computatio nally efﬁcient choice. Second, the sim plicity of linear-Softmax layers allows them to be accelerated substantially in production. The monotonic property of the Softmax function allows the logits to be used directly for ranking, and modern search engines support dot pr oduct ranking through spatial indices with logarithmic time complexity. The above rea sons describe why Softmax output layers are almost unanimously adopted in SBRS research. In this paper, we introduce more aspects of the Softmax b ottleneck other than expressivity in the context of SBRSs. First, overﬁtting do es occur in linear-Softmax layers. O ften associated with complex and ﬂexible networks, we show that overﬁtting could be a serious issue even in a dot product. We also show that with proper dropout layers, the accuracy of the whole network could be improved signiﬁcantly on a variety of SBRSs. Second, althoug h not always stated explicitly, the technique of sharing the item embeddings among session sequences and the candidate pool is almost adopted universally in research Li et al. [2017], Liu et al. [2018], Wu et al. [2019] to boost p e rformance. This tight-couplin g simultaneously expects the embeddings to be the encod e r inputs an d also the w e ights in dot products. It could create interferen ce that stops accuracy from further improvement. We show th at a single layer of feedforward network could alleviate this problem. Combining the above two id e as, we pr opose Dropout and Decouplin g (D&D), a simple yet effective method that alleviates the previously mentioned problems. It also keeps the time comp lexity of linear-Softmax output layer s in both training and que ry time. We comp a re our method to other computationally expensive methods, including MLP and MoS from Yang et al. [2018], and show that D&D performs on pa r with an d at times even better than those methods. Our main contributions are summarized as follows: Softmax-based session-based recommender systems. • We show that sharing the item embedding layer among session sequence s and the candidate pool introduces a tight-coupling that restrains performan ce in sessionbased recommender systems. • We prop ose Dropout and De c oupling to effectively alleviate the above prob le ms while keeping the same time complexity during both training and query. In this section, we will review related works on the Softmax bottleneck and recommender systems. The Softma x bottlene ck was ﬁrst described by the work Yang et al. [2018] in language mod e ling. The authors identiﬁed linear-Softmax ou tput layers as the so urce of limited expr essivity. Natural languages are comm only believed to be highly diverse and context dependent Mikolov and Zweig [ 2012]. Modeling the probability logits using matrix factoriz a tion causes rank deﬁciency. To tackle this problem, MoS (mixture of Softmaxes) was propo sed. As w e ighted mixtures of multiple Softmax components, MoS improves on both per plexity and rank of logit matrices. Several later works Yang et al. [2019], Kanai et al. [ 2018], Ganea e t al. [2019] have proposed lightweight alterna tives to the computationally expensive Mo S, and also techniques to further improve MoS. Session-based recommender systems learn user beh avior from sequence data. Pioneer works in this ﬁeld employs statistical models to captur e sequential patterns. The problem has bee n formulated as decision tr ee learning Zimdars et al. [2001] and Markov decision process (MDP) Shani et al. [2002]. Later works Rendle et al. [2010], He and McAuley [2016] combine Markov chain with matrix facto rization (MF) to model personalized behavior patterns. Deep learning provides more sophisticated method s to encode user behavior sequences. RNN-based approaches Hidasi et al. [2016] were introduced to model sequential behavior. Later work Tan et al. [20 16] enhances performa nce by proper data augmentation and conside ring temporal shift in data. CNN-based approaches Tang an d Wan g [2018] were also prop osed. Dot-product attention Li et al. [2017], Liu et al. [2018] was introduced to capture user’s current interests. Methods with self-attentio n Kang and McA uley [2018], Sun et al. [2019] were proposed to capture complex sequ ence patterns. GNN-based approache s Wu et al. [2019] were also introduced to capture complex transitions between items. Collaborative ﬁltering (CF) tasks model interactions between u sers and items. They predict future interactions with a given set of past interactions. Pioneer works Salakhutd inov and Mnih [2007], Rendle et al. [2009] employed matrix factorization (MF) on user-item matrices. Autoencoder-based ap proaches Wu et al. [2016], Liang et al. [2018] w e re introduced to p roduce more robust reconstruction of user interac tions. NCF He e t al. [2017] generalizes MF and uses it in c ombination with MLP. CTR prediction tasks model click-through rate from more rich fe a tures. The features expand th e data dimension and sparsity becomes a problem. To tackle this pro blem, wide & deep network Cheng et al. [2016] was proposed to use MLP with cr oss-product features. Many later works were inspired by this method. DeepFM Guo et al. [2017] combines MLP with a factorization machine (FM). DCN Wang et al. [2017] combines MLP with a proposed c ross network. We will brieﬂy describe the Softmax bottleneck problem in language mode ling, and the theoretical limit it imposes on the expressivity of the whole model. A language model co nsists of a vocabulary of words V = {x, ..., x} and a probab ility distribution P d eﬁned over all sentences in V, such as (x, ..., x). With factorization, P could be rewritten as a product of condi-Q tional distributions P (X, ..., X)P (X|C), wher e C= Xis referred to as the context. This formula describes a random process which generates the next word in a given context. All sentences in the given corpus are assumed to be generated from a ground-truth distribution P(X|C). T he goal of language modeling is to approximate Pwith a parametric distribution P(X|C). The majority of language models adopt a Softmax output layer Yang et al. [2018]. In these models, a context, or an input sequence, is represented by a D-dimensional vector h= f (c), where c= (x, ..., x), and each word x in the vocabulary are also represented by a D-dimensional vector w. Then, the probability o f the next word in a context is given by the linear-Softmax formula in equation 1. θ is the parame te rs of the model. Tr aining is done by optimizing the cross-entropy loss function in equation 2. The Softmax bo ttleneck comes down to the assumption that the ground-truth distribution of a language model could be approximated by th e dot product of two low-rank matrices passed through a Softmax function. Let A ∈ R, H ∈ R, W ∈ Rdenote the log of the groundtruth distribution, an d the context rep resentation matrix, the word embedding matrix respec tively. In other words, A= log (P(x|c)), H= h, W= w. Let F (A) denote the set of all matrices obtained by applying row-wise shift on A. They will all produce the same distribution P due to the normalization property of the Softmax function. The linear-Softmax formula can approximate Pif and only if HWapproximates any ma trix in F (A). As shown in the original work Yang et al. [2018], the rank of HWcould not b e greater than the number of dimension D, but the rank o f A could be any number up to min(N, M), which is usually several orders of magnitude larger than D. Combining with the fact |rank(A) − rank(A)| ≤ 1 for all A∈ F (A), it’s almost certainly impossible for HWto be in F (A). This expressivity limit caused by this rank deﬁcien cy is called the Softmax bottlenec k. To overcome rank deﬁciency, Yang et al. proposed Mixture of Softmaxes, or MoS, to improve expressivity. MoS is formu late d as the weighted average over K Softm a x componen ts, shown in equation 3. The authors have shown empirically that MoS could produce a high-rank logit matrix. where πis the mixtu re weight of the k-th component and his the k-th context vector, shown in equations 4. The word embeddings are shared among all Softmax components to not signiﬁcan tly incre a se the model size. We will identify more aspects of the Softmax bottleneck problem, besides just expressivity, that restrain the accuracy of session-based recommen der systems. Then, we will introdu ce Dropout and Decou pling, a simple ye t effective method, to alleviate th ose problems. Before going into details, we will ﬁrst brieﬂy describe session-based recom mender systems (SBRS). A SBRS takes a session, or any of its preﬁx sequence, as input, and outputs a probability distribution over all candidate interactions to be the next one in the sequence. A session is a bounded sequence of interactions of a user with the system. The boundar y is often deﬁn ed at the end of a long period with no user activity. Since SBRSs with m ixed action types (a) GRU4Rec(b) NARM(c) SR-GNN are not discussed here, we will use the term items, the objects users interact with, to replace interaction s from here. Figure 1 shows a high -level architecture of a general SBRS network. This architecture is adopted by the majority of state-of-the-art research Hidasi et al. [2016], Li et al. [2017], Liu et al. [20 18], Wu et al. [2019]. The input consists of two parts: a sequence of session items, (x, ..., x), and a set of candid ate items, {x, ..., x}. Both groups of items are converted into dense vectors by a shared item embedding . Th e n, the sequence of session item vectors, (v, ..., v), is encoded into a single vector representation, s, by a sequence encoder. Finally, both the session representation vector, s, and the c a ndidate item vectors, {v, ..., v}, are fed into a probability encoder to produce output probabilities, {(x, p), ..., (x, p)}. In section 5, we will analyze Dropout and Decoupling on three representative SBRS models, which are GRU4Rec, NARM, and SR-GNN. All these three models ﬁt into the architecture in ﬁgure 1, and adopt lin ear-Softmax layers as their probability decoders. They only differ in their sequence encoders, which are shown in more detail in ﬁgure 2. Using them as exam ples, we will show that Dropout and Decouplin g could improve on a wide range of SBRS models. GRU4Rec employs a GRU la yer to encode session representations as hshown in equation 5. NARM applies a dot-product attention layer to the GRU outputs. Details of the attention layer is shown in equations 6, where q, c ∈ Rand W, W∈ Rare mo del parameters. The bilinear layer descr ibed in the original paper is moved in to probability decoders to ﬁt in ﬁgure 1, and the ﬁnal session representation becomes s. SR-GNN replaces the GRU layer with a gated graph neural network (GGNN) Li et al. [ 2016]. It transforms session sequences into unweighted directed graphs. Item f e atures are extracted b y propagating information along bo th directions of the graph edges. The item features are then fed into the same dot-product attention layer in equations 6. Overﬁtting is a phenomenon often associate d with the model being more expressive than the data. However, it could still occur in networks a s simp le as a linear-Softmax layer. This counter-intuitive idea leaves this problem overlooked by previous work. Previous SBRS models mainly focus on the session and encoder dropout layers in ﬁgure 1a. We prop ose to add another candidate dropou t layer, shown in ﬁgure 1b. In section 5, we will show that it signiﬁcantly improves the accuracy of a wide range of SBRS models on a variety of datasets. A linear-Softmax-based SBRS c ould be described mathematically by equatio ns 7, where v∈ Ris an item embedding, f (·) is the sequence encoder, and s ∈ Ris the session representation. W ∈ Ris an optional matrix which is only required when the numbers of dimension s D and E are d ifferent. During tr aining, dropout layers are applied to the model. Equations 8 show this mathematically, and the last dropout function around vis the candidate dropout layer that we’ve been proposing. s = f (dropout(v), ..., dropout(v)) p = softmaxdropout(s)W [dropout(v)] Sharing the same item embedding layer among session and candidate items creates a tight-coupling between sequence encoders and probability d e coders. It’s a common technique that’s been widely used in linear-Softmax-based SBRSs, shown in ﬁgure 1a. Hidasi et al. [2016], Li et al. [2017], Wu et al. [2019]. In practice, it’s c rucial to share learned information between sequence encoders and probability dec oders through item embeddings f or models to reach state-of-the-art accuracy. This tight-coupling could also stop accu racy from furth er improvement thr ough interference. The shared item embeddings are simultaneously expected to be the input of sequence en c oders and also the weights used to me asure its similarity with session repr esentations. Figure 3a shows a block diagram of linear pr obability decoder. We propose to add a single feedforward layer, which consists of a linear layer with an activation function, in front of candidate embeddings, shown in ﬁgu re 3b. By introducing ﬂexibility to the proba bility decoder, we will show that it alleviates the tight-coupling and improves accuracy in section 5. Dropout and Decoupling combines the two techniques mentioned above, shown in ﬁgure 1b. A candidate dropout layer is introduced to alleviate overﬁtting in the ﬁnal linear layer. Depending on the data set, an optional decoupling feedforward could be employed to loosen the tight-coupling between the sequence e ncoder and the probability decoder. We found Softplus works best among tanh, sigmoid, and ReLU as the activa tion function for the feedforward layer. Rank deﬁciency has been identiﬁed Yang et al. [2 018] as the source of the Softmax bottleneck. The logits produced by a ﬁnal linear layer for m a low-rank matrix. Its rank is restrained by the number of hidden dime nsions. MoS was proposed to tackle this issue. Figure 3d shows a block diagram of MoS. The session representation is encoded into K Softmax compo nents by K feedforward layers with tanh as activation functions. Then, the output probability distribution is calculated as a weighted average over the components. In user-based recommender systems, many works have proposed different ways to introduce ﬂexibility to the network, and reach higher accuracy than the tr aditional matrix factorization (MF) method, which includes the equivalent of a Linear-Softmax layer. In particular, the idea of combining MLP and factorization mac hine (FM) has inspired numerous research works. We will include a version of these networks as probability decoder in section 5, and compare it against MoS and our D&D. MLP model is shown by a block diagram in ﬁgure 3c. Inspired by the idea of combining MLP and FM, we stack a 2-layer MLP on top of a FM, which consists of an entrywise Decoupled O(N M D + M D) O(D ln M + L ln L) Table 1: Time complexity of probability decoders (N is the batch size. M is the ca ndidate pool size. D is the number of hidden dimensions. L is the size limit of query r e sults. K is the numb e r of MoS compone nts.) product of a session representation and a candidate item e mbedding both transformed by an in dividual linear layer. Time complexity is one important aspect of SBRSs during both training and query. Introducing complexity to the network could r esult in dramatic change in time complexity. During query, the time complexity is even more constrained. Breaking existing acceleration methods or r e quiring special o perations could result in signiﬁcant c osts. We will focus our analysis on probability decoders in ﬁgure 3. Table 1 summarizes the time complexity of different probability decoders. We assume that the candidate pool size M is signiﬁcantly larger than the other variables, which is over 10in all ou r datasets. The batch number N , the number of hidden dimensions D, the size limit of query results L are of similar magnitude, which a re usually around 10. The number of MoS componen ts K is usually around Training Time Complexity The time complexity of the linear decode r is O(N M D). The optional linear layer is omitted from the formula since the seq uence encoder must have greater or equal terms. The decoupled decoder adds a feedforward layer to the can didate item embeddings. Since embedd ings are the same fo r the whole batch, it could be calculated only once in each training step. Therefore, there’s only an ad ditional term O(MD), which could be omitted when M and N are of similar magnitude. The MLP decoder will increase the time complexity signiﬁcantly to O(N MD). It might require candidate selection mech a nisms to control the size of M and make training efﬁcient. The M oS decoder has K Softmax components, and its time complexity is also K times higher. Query Time Complexity The time complexity of the linear decoder is O(D ln M + L ln L) for two reasons. First, since Softmax func tion preserves the order of logit values, it could be ignored for ranking. Second, a pr e-built spatial index could be used to r etrieve the top L dot-product values in log tim e . The d ecoupled decoder has the same time complexity as the linear decoder, because the results of the feedforward layer could be pre-computed. The MLP and MoS decoder could not b e accelerated by conventional spatial indices. Therefore, their time c omplexities are linear to the candida te pool size M . Further more, the MoS deco der will require extra op erations to calculate th e denominator of K Softmax components, shown in equation 3 . They both might need candidate selection mechanisms dur ing query, which could hurt overall accuracy. In this section, we will ﬁrst describe the setup of our experiments. Then, we will analyze the experiment results, and show the effect of Dropout and Decou pling. We conducted ou r experiments o n the following public real-world datasets from a wide range of different applications. • Digineticawas released at CIKM Cu p 2016. It contains anonymized search and browsing logs collected from an e-com merce search en gine over 6 months. • RetailRocketwas released in a Kaggle contest. It contains sessions collected from a real-world ecommerce website over 5 months. • GowallaCho et al. [201 1] contains check-in data collected from a geo-location-based social networking website over 21 months. In preprocessing, w e follow the common practice Li et al. [2017], Liu et al. [2018], Wu et al. [2019] to remove items a ppearing less than 5 times and sessions with length 1. For Gowalla dataset, we split the user timelines into disjoint sessions at the end of each inactive period longer than 8 hours, and only keep items appearing no less than 20 times. Table 2 shows some statistics of the datasets after preprocessing, where # sessions does not include preﬁx sequences. In each session sequence, every element and its non-empty preceding sequence will be used as target and input of an individual sample. Preceding sequences are truncated to the latest 20 elements if its len gth exceeds this limit. We applied our meth od to sequence decoders of the following representative works in SBRS to show that it’s a general solution to the problems we mentioned. The encod ers are also shown in detail in ﬁgure 2. • GRU4Rec Hidasi e t al. [2016] is a RNN-based model, which encodes session representations using GRU layers. • NARM Li et al. [2017] employs RNNs with attention to capture users’ main purposes and sequ e ntial behaviors. • SR-GNN Wu et al. [2019] em ploys GGNNs with attention to obta in accurate item features throu gh taking complex transitions of items into account. We compared our decoupled decoder to the following decoders, which are shown in detail in ﬁgure 3. • Linear consists of a ﬁnal do t-product layer. • Decoupled passes candidate item embeddings through a feedforward layer before the ﬁnal dot-product layer. • MLP performs entrywise pr oducts between candidate embeddings a nd session representations, a nd th en passes the result through a ﬁnal MLP layer. • MoS Yang et al. [2018] recommends items similar to the previous item in the current session. Similarity is measured by a cosine index on the co-occurrence of items in the same session. We also included some baselin e algorithms for comparison, which could reveal different properties of each d ataset. • POP recommends the most frequent items in training set. • S-POP recommends the most freq uent items in the current session. • Item-KNN r ecommends items similar to the previous item in the session. Similarity is measured by a cosine index on co-occuren c e of items in the same session. We adopted the commonly used top-k metrics Hidasi et al. [2016], Li et al. [2017], Liu et al. [2018], Wu et al. [2019] in our evaluation, which are HR@20 (hit rate, also called P@20 and Recall@20 with equivalent deﬁnitions) and MRR@20 (mean reciprocal rank). In order to produ ce unbiased results, each dataset is split into training, validation, and test sets. Th e most recent 10% of the time period is reserved for the test set, and 10% of the rest sessions are reserved for the validation set. Reported metrics are measured on the test set when the same metric rea c hes the highest poin t on the validation set. This prevents information from test and validation sets to leak into the model. Each datapoint is an average of 5 individual runs. In order to have a common g round for comp arison, the dimension o f all hidden states and item embeddings are set to 100 for all m odels. We followed Tan et al. [2016], Li et al. [2017] to set the dropo ut ratios of GRU4Rec and NARM to/4 for session embeddings and 0 and/2 for encoder outputs, respectively. We ado pted the same dropout scheme for SR-GNN as NARM, because we found it produces more stable results. In training, A dam is employed for optimization. The batch size is 200 and the learning ra te is 10. Ther e ’s no learning rate sch edule and weight decay. For hyper-parameter tuning, we explored the following ranges: {0,/8,/4,/8,/2} for candidate dropout ratio, {1, 2, 3} for number of MLP layers, {2, 3, 4, 6, 8} for number of MoS components. Intended to reveal b a sic properties of eac h dataset, the comparison of baseline models is shown in the top part of table 3. POP shows popularity-ba sed recommendations generally perform poorly, while it’s slightly better in Gowalla than in the othe rs. S-POP shows session sequences are the most repetitive in RetailRocket and the least in Gowalla. Item-KNN sh ows the co-occ urrence relation be twe en items are th e strong est in Gowalla and the GRU4Rec 48.56 16.77 58.58 37.01 55.12 25.88 GRU4Rec 51.79 18.34 62.24 39.04 58.48 26.68 weakest in RetailRocket. With the introduction of the attention mechanism and graph neur al network, NARM and SR-GNN notably improve the performance over the pure RNN-based GRU4Rec on all 3 datasets. The comparison of linear-Softmax mode ls is shown in the middle part of table Dropout and Decoupling signiﬁcantly improve the performance of all sequence encode rs on all datasets. Details are shown in the bottom part of table 3. There’s two main effects to observe. First, the differences between sequence encoders become less signiﬁcant. While our method enables all 3 sequence encoders to reach state-of- the-art performance level, the gap between them shrinks. This implies the overﬁtting and tig ht-coup ling problems accoun t for a signiﬁcant part of the performa nce gap. Second, our method improves the most on RNN sequence encoder. It allows GRU4Rec to outperform more complex a lgorithms, NARM and SR-GNN, on Diginetica and Gowalla. It implies GRU4Rec is more susceptib le to overﬁtting and tightcoupling. Our me thod provides an opportunity to revitalize RNN-based mod e ls. The detailed comparison o f probability decoders is shown in table 4, where the sufﬁx δ indicates there’s a candidate dropout layer. First, let’s have a more detailed discussion on the optional decou pling feedforward layer in our method by comparing Linear-δ and Dec oupled-δ. While overﬁtting is a general problem in linea r-Softmax models, tight-coupling does not always harm performance. Decoupling gene rally improves performance with exceptions for GRU4 Rec encoder and Gowalla dataset, where its effect is mixed. Second, more complex decoders, MLP-δ and MoSδ, could reach even highe r performance metrics with much Decoulped-δ 51.40 18.33 62.03 39.01 57.50 26.34 Decoulped-δ 51.76 18.13 62.16 38.61 57.79 25.57 Decoulped-δ 51.54 18.03 62.76 39.2657.43 25.30 greater computation costs. There’s still no decoder that ou tperforms or unde rperforms others in all situations. The importance of sharin g item e mbeddin gs is shown by comparing SepEmb and Linear in table 5. SepEmb adopts separated embeddings for session and candidate items. The results show that metrics could drop sign iﬁca ntly when sharing is not employed, especially in dataset Diginetica and RetailRocket. This partially explains why decoupling is more effective in these datasets, since the coupling tightens as the mod e l learns more through shared embeddings. The effect of candidate dropout layers is also shown by comparing table 5 to table 4. Universally, candidate dropout layers improve the performance of all models on all d atasets in our evaluation. Conventional wisdom often associates overﬁtting with complex networks. In our experiments, candidate dropout layers are shown to be more effective in simpler networks, such as GRU4 Rec en coder, Linear and Decoupled decoders. This implies tha t these simpler networks are actually more prone to overﬁtting. The Softmax bottleneck is a serious problem in sessionbased recom mender systems, that hasn’t been examined by previous work. In this paper, the prob lem is investigated and more aspects of this issue are introduced. Overﬁtting Decoulped 48.98 17.38 59.17 37.63 54.97 25.67 Decoulped 51.27 17.79 61.79 37.94 56.92 25.23 Decoulped 50.81 17.61 62.14 38.45 56.85 25.27 in linear-Softmax layers and tight-coupling of item embeddings are identiﬁed as two major contributors to the b ottleneck. Dropout and Decoupling (D&D), a simple yet effective method, is proposed to alleviate this bottlen e ck. Comprehen sive experime nts conﬁrm that the proposed method can imp rove on a wide range of state-of-the-art models, while keeping the same time com plexity during training and query.