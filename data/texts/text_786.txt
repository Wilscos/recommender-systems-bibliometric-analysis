State-of-the-art contextualized models such as BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reﬂect how well a model represents the coupled word and context semantics. This study investigates this assumption by presenting the ﬁrst quantitative analysis (using probing baselines) on the context-word interaction being tested in major contextual lexical semantic tasks. Speciﬁcally, based on the probing baseline performance, we propose measures to calculate the degree of context or word biases in a dataset, and plot existing datasets on a continuum. The analysis shows most existing datasets fall into the extreme ends of the continuum (i.e. they are either heavily context-biased or target-wordbiased) while AM show lower overall biases to challenge a model to represent both the context and target words. Our case study on WiC reveals that human subjects do not share models’ strong context biases in the dataset (humans found semantic judgments much more difﬁcult when the target word is missing) and models are learning spurious correlations from context alone. This study demonstrates that models are usually not being tested for word-in-context representations as such in these tasks and results are therefore open to misinterpretation. We recommend our framework as sanity check for context and target word biases of future task design and application in lexical semantics. Meaning contextualization (i.e., identifying the correct meaning of a target word in linguistic context) is essential for understanding natural language, and has been the focus in many lexical semantic tasks. Pretrained contextualized models (PCMs) have brought large improvements in these tasks including WSD (Hadiwinoto et al., 2019; Loureiro Figure 1: Plotting context and target word biases when applying BERT across popular context-aware lexical semantic datasets. The green shaded area is for target-word-biased datasets (Bias>0.8) and the yellow shaded area is for context-biased datasets (Bias>0.8). The dashed red lines indicate 1.0 context (right) and 1.0 target word bias, indicating the datasets require the modeling of target words alone or context alone. and Jorge, 2019; Huang et al., 2019; Blevins and Zettlemoyer, 2020), WiC (Pilehvar and CamachoCollados, 2019; Garí Soler et al., 2019) and entity linking (EL) (Wu et al., 2020; Broscheit, 2019). These superior performances have been taken as proof that PCMs can successfully model word-incontext semantics. However, the evaluation benchmarks often vary in their emphasis on context vs target words. For example, WSD relies more on context by design as the target words are given. Moreover, models may ﬁnd shortcuts to avoid learning word-context interaction. What is missing in the current literature is an accurate quantiﬁcation of this word-context interplay being tested in each task so that we can fully understand task goals and model performance. In particular, we need to ﬂag heavy word and context reliance where a model can solve a task by relying solely on context or the target words. Notice that such word or context reliance is not necessarily a problem for an application, however it would be undesirable in terms of a scientiﬁc understanding of the models’ meaning contextualization abilities as it essentially bypasses the key word-context interaction challenge in meaning contextualization, which requires the modeling of both target words and their contexts. Therefore, we refer to models’ heavy reliance on target words or context in a dataset as target word biases or context biases. To achieve the aforementioned goal, we design an analysis framework to quantify context and target word biases. We ﬁrst run controlled probing baselines by masking the input to show the context or the target word alone. Based on model’s performance on these probing baselines, we calculate two ratios that reﬂect how much of the model performance in this dataset comes from context alone or the target word alone, i.e. the degree of context or target word biases (See Figure 1). The design of the probing baselines follows previous studies that applied input permutation techniques for model and task analysis in GLUE (Pham et al., 2020), NLI (Poliak et al., 2018; Wang et al., 2018; Talman et al., 2021) and relation extraction (Peng et al., 2020). While previous probing studies usually assume no meaningful information from a corrupted input without human veriﬁcation, we provide fairer comparison with model performance by collecting human judgment on the same partial input in a case study on WiC. Such comparison reveals whether there are biases speciﬁc to models or inherent in the task (i.e. for both humans and models). The underlying model for our main experiments is BERT (Devlin et al., 2019), one of the most successful PCMs that offer dynamic contextual word representations as bidirectional hidden layers from a transformer architecture. To ensure the general trend of our ﬁndings are consistent across different models, we also performed the analysis using ROBERTA (Liu et al., 2019), which improves upon BERT by optimized design decisions during training. In addition, we offer comparison with a more recent PCM, DEBERTA (He et al., 2020), which improves BERT with two novel techniques: disentangled attention that encodes a word’s content and position separately, and an enhanced masked decoder that incorporates absolute position for predicting masked tokens. Context vs. Word:For the main experiment, we design the WORD baseline where we input only the target wordto the model, and the CONTEXT baseline where the target word is replaced with a [MASK]token in the input. A high performance in CONTEXT or WORD will indicate strong context or target word bias. Example input is shown in Table 1. Lower Bound:Apart from a RANDOM baseline, we also set up a LABELbaseline where all the input is masked and the learning is only from the label distribution in the task. Human Evaluation: For fairer comparison with model performance, we collect human judgment (HUM) on the baseline input in a case study on WiC. We follow Pilehvar and Camacho-Collados (2019) to assign 4 sets of 100 examples sampled from the dev setto native English speakers. Each 100 example receives one annotation but there is a 50 example overlap overlap between two annotators for agreement calculation. The difference in our setup is the annotators performed meaning judgment based on the CONTEXT input and as an additional response they are encouraged to guess the target word. All the probing baselines are compared with model performance on the full input (FULL). We refer to model M’s performance in WORD, CON- Mrespectively. Table 1: An example of BERT’s contextual bias in WiC. The original WiC label for the example is F. Target words are in brackets. Based on a modelM’s performance on the full input and on the baseline input, we proposeBias andBias(as calculated in Equation (1) and Equation (2)) to measure context and target word biases in a dataset.Biasis the ratio ofMto Mwith the LABEL performanceMdeducted from bothMandM.Mhas to be deducted as it is unrelated to the input. Otherwise, the ratio will give an inﬂated bias measurement.Biasis calculated in the same way asBiasexcept that we replaceMwithMin the equation. The two measures can also be seen asMandMunder min-max normalization where the min value isM and the max value isM. As such,Biasand Biasreﬂect how much of what a model has learned from the input in a dataset is from context alone or target word alone, which will give us indicators of the degree of context and target word biases in the dataset. For example, we can interpret aBiasof 0.8 as 80% of what the model has learned from the full input can be achieved by the context alone. 3.1 Task Selection (Examples in Appendix A) Word Sense Disambiguation (WSD).WSD (Navigli, 2009; Raganato et al., 2017) requires a model to assign a sense label to a target word in context from a set of possible candidates for the target word. Following the standard practice, we use SemCor as the train set, Semeval2007 as dev, and report accuracy results on the concatenated ALL testset. The WiC-style Tasks (WiC, WiC-TSV, MCLWiC and XL-WiC).To alleviate WSD’s requirement for a sense inventory, WiC (Pilehvar and Camacho-Collados, 2019) presents a pairwise classiﬁcation task where each pair consists of two wordin-context instances. The model needs to judge whether the target words in a pair have the same contextual meanings. WiC-TSV (Breit et al., 2021) extends the WiC framework to multiple domains and settings. This study adopts the combined setting where each input consists of a word in context instance paired with a deﬁnition and a hypernym. The WiC-style tasks have also been extended to the multilingual and crosslingual settings in MCL-WiC (Martelli et al., 2021), XL-WiC (Raganato et al., 2020) and more recently in AM 2021). MCL-WiC provides test sets for ﬁve languages with full gold annotation scores. However, MCL-WiC only covers training data in English. To ensure the analysis will be testing the same data distribution during both training and testing, we will only use the English dataset of MCL-WiC. XLWiC extends WiC to 12 languages. While most languages in this task do not have training data, we perform analysis on its German dataset which does contain both train (50k) and test data (20k). English word-in-context instances with word-incontext instances in a target language. In this study, we perform analysis on the English-Chinese dataset which contain 13k train and 1k test data. Sense Retrieval (SR).Based on WSD with the same train and test data, SR (Loureiro and Jorge, 2019) requires a model to retrieve a correct entry from the full sense inventory of all words from WordNet (Miller, 1998). AIDA and Wikiﬁcation.An important application scenario for testing meaning contextualization is Entity Linking (EL). EL maps a mention (an entity in its context) to a knowledge base (KB) which is usually Wikipedia in the general domain. The target word and its context help solve name variations and lexical ambiguity, which are the main challenges in EL (Shen et al., 2014). In addition, the context itself can help learn better representations for rare or new entities (Schick and Schütze, 2019; Ji et al., 2017). We test on two popular Wikipedia-based EL benchmarks: AIDA (Hoffart et al., 2011) and Wikiﬁcation (Wiki) (Ratinov et al., 2011; Bunescu and Pa¸sca, 2006). AIDA provides manual annotations of entities with Wikipedia and YAGO2 labels for 946, 216 and 231 articles as train, dev and test sets respectively. The Wiki Dataset is based on the hyperlinks from Wikipedia. We randomly sampled 50k sentences from Wikipedia as the test and another 50k as the dev set. The rest is used for training. For both AIDA and Wiki, the search space is the full Wikipedia entity list. WikiMed and COMETA.To test domain effects, we evaluate on two medical EL tasks. We use the WikiMed corpus (Vashishth et al., 2020), an automatically extracted medical subset from Wikipedia, for medical wikiﬁcation. Each mention is mapped to a Wikipedia page linked to a concept in UMLS (Bodenreider, 2004), a massive medical concept KB. We deﬁne the search space as the Wikipedia entities covered in UMLS. With the same Wikipedia ontology but a different domain subset, WikiMed can be directly compared with Wiki for assessing domain inﬂuence. We also test on COMETA (Basaldella et al., 2020), a medical EL task in social media. COMETA consists of 20k English biomedical entity mentions from online posts in Reddit. The expert-annotated labels are linked to SNOMED CT (Donnelly et al., 2006), another widely-used medical KB. We report accuracy for WSD and all the WiC style tasks, and accuracy@1 for retrieval-based tasks including Wiki, Aida, etc. We adopt standard model ﬁnetuning setups in each task. We use the base uncased variant of BERT for general domain experiments and PUBMEDBERT (Gu et al., 2020) for the medical tasks. For WSD, We use GLOSSBERT (Huang et al., 2019) that learns a sentence-gloss pair classiﬁcation model based on BERT. For WiC and WiC-TSV, we follow the SuperGlue (Wang et al., 2019) practices to concatenate BERT’s last layer of[CLS]and the target words’ token representations for each input pair, followed by a linear classiﬁer. For the retrieval-based tasks including SR and EL, we adopt a bi-encoder architecture to model query and target candidates with BERT (Wu et al., 2020). For the query, we insert[and]to mark the start and end positions of the target word in context. Each target candidate is reformatted as “[CLS]Name || Description[SEP]”.Nameis an entity title (EL) or synset lemmas from WordNet (SR). Descriptionis the ﬁrst sentence in an entity’s Wikipedia page (Wiki & WikiMed), a gloss (SR), or n/a (COMETA). The model learns to draw closer the true query-target pairs’ representations using triplet loss with triplet miners during ﬁnetuning (Liu et al., 2020). For each experiment, we perform grid search for the learning rate in [1e − 5, 2e − 5, 3e − 5]and select models with early stopping on the dev set. Context vs Target Word Biases.Based on BERT’s baseline performance (Performance for each probing baseline is plotted in Appendix F), we calculateBiasandBiasfor each dataset, and plot the results in Figure 1. One obvious observation from the ﬁgure is that most datasets lie close to the dashed red lines which indicate 1.0 (i.e. 100%) context (right) or target word bias (top). Moreover, the datasets tend to lie in two ends: the retrieval-based datasets (eg. Wiki) lie in the top left corner, showing large target word bias and low context bias; the WiC style datasets and WSD lie in the bottom right corner with large context bias and low target word bias. XL-WiC is an exception as it contains both strong context and target word biases. Unlike other WiC-style datasets, XL-WiC shows strong target word bias because the dataset does not contain sufﬁcient ambiguous cases. We conﬁrm this by calculating the per-word average label entropy of the training data as 0.09, and on average a word has the same label for 94% of the contexts. Therefore, a model can perform very well by relying solely on the target words without needing context for disambiguation. Overall, we found that most of the existing datasets, albeit being context-sensitive lexical semantic tasks, are either “context” tasks or “target word” tasks with strong context or target word Table 2: Sense Entropy across retrieval-based tasks biases. There are currently few datasets that require the modeling of the context-word interaction, which should result in both low context and target word biases. SR and AM datasets which can be found further inside of the red lines in Figure 1. In SR, a system needs to model the target words in order to retrieve all the possible senses associated with the word, and because there is plenty of ambiguity in the dataset, context is also needed to identify the correct sense. versarial examples to penalize models that rely only on the context or on the target words. Domains and Ontologies.The retrieval-based tasks in this study offer comparison between two ontologies (WordNet vs Wikipedia) and between two domains (general vs medical). Overall, Wikipedia has a stronger target word bias than WordNet, and this bias is increased in the medical domain where relying on the target words alone gives the best performance (i.e. COMETA and WikiMed both have > 1.0 target word bias). Such divergence is arguably caused by the different degrees of lexical ambiguity in these tasks. In particular, domain could reduce ambiguity (Magnini et al., 2002; Koeling et al., 2005), and therefore affect the context and target word bias. As a quantitative measure for lexical ambiguity, we calculate average sense entropy across all words in each task’s training data, see Table 2. Conﬁrming our hypothesis, sense entropy (lexical ambiguity) in a task correlates well with the FULL-WORD gap. When there is little ambiguity in a task (eg. medical EL), model learning is dominated by the target words with the context being useless or even harmful (See Appendix B). Human vs Models.As in Table 3, humans exhibit a substantial FULL-CONTEXT gap(19%): the absence of the target words drastically decreases both performance and agreement, suggesting these target words are indeed crucial for solving the task. In comparison, BERT shows CONTEXT performance (66.35%) close to FULL (69%), even outperforming human CONTEXT baseline (61%). This Table 3: Comparing context biases from human and models in WiC (%). Human agreement is in parentheses. context bias is more prominent in DEBERTA with a much higher CONTEXT baseline (69.64%) which is on par with its FULL performance (70.35%) and thus largely accounts for its improved performance. As qualitative analysis on the human-model discrepancy on CONTEXT, we examined 20 cases where annotators did not predict WiC F labels (from the corresponding FULL input) while BERT did. In 11 cases, humans guessed other valid target words to justify their predictions resulting in a T (True) instead of the original F (False) label. Table 1 shows such an example. On CONTEXT input, one annotator gave T, guessing the target word is type so that the sentences in the pair become: The annotator’s T label here is reasonable as type ﬁts the contexts and does hold its meaning across the two sentences. The same annotator was able to give the WiC label F when we reveal the original target word (breed) which has the speciﬁc meaning of species in sentence1 and personality in sentence2 (see the FULL input in Table 4). BERT however still predicts F in 1. In fact, we perform preliminary analysis to test BERT on all the 11 cases where the human-elicited target words change the labels to T (We show more examples in the Appendix C.), and found that for 7 out of 11, BERT is insensitive to the changed target words and maintains its F prediction. This suggests BERT does not appreciate the same word-context interaction as humans, and is making prediction mainly based on contexts rather than modeling contextual lexical semantics in WiC. We presented an analysis framework to disentangle and quantify context-word interplay in application of popular contextual lexical semantic benchmarks. We plot datasets on the continuum from context-biased (eg. MCL-WiC, WiC) to targetword-biased (medical EL), and we found that most existing datasets lie on the two ends with excessive biases that essentially bypass the key challenges in contextual lexical semantics. We identify SR and necessitate representation of both word and context, and we call for more tasks that challenge models to do so. We further analyzed effect from domains and ontologies on the target word bias: medical>general and Wikipedia>Wordnet. Through controlled comparison and qualitative analysis, we found that models’ heavy context bias in WiC is not attested in humans who need both context and target words to perform well in the task. This suggests that the models are learning spurious correlations rather than genuine contextual lexical semantics. Our paper highlights the importance of understanding these biases in existing datasets and encourages future dataset design to control for these biases and to focus more on testing the challenging wordcontext interaction in context-sensitive lexical semantics. We thank the anonymous reviewers for their helpful feedback. We acknowledge Peterhouse College at University of Cambridge for funding Qianchu Liu’s PhD research. The work was also supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909) awarded to Anna Korhonen. We also appreciate many helpful discussions and feedback from our colleagues in the Language Technology Lab.