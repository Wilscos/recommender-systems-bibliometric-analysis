Abstract— This paper contributes a new high-quality dataset for hand gesture recognition in hand hygiene systems, named “MFH”. Generally, current datasets are not focused on: (i) ﬁne-grained actions; and (ii) data mismatch between different viewpoints, which are available under realistic settings. To address the aforementioned issues, the MFH dataset is proposed to contain a total of 731147 samples obtained by different camera views in 6 non-overlapping locations. Additionally, each sample belongs to one of seven steps introduced by the World Health Organization (WHO). As a minor contribution, inspired by advances in ﬁne-grained image recognition and distribution adaptation, this paper recommends using the self-supervised learning method to handle these preceding problems. The extensive experiments on the benchmarking MFH dataset show that the introduced method yields competitive performance in both the Accuracy and the Macro F1-score. The code and the MFH dataset are available at https://github.com/willogyteam/hand-gesture-recognition-smc2021. Hand hygiene, the so-called hand wash process, is an essential part that prevents infectious diseases in surgery. Even healthcare professionals frequently fail to follow the hand hygiene guidelines, hence, raise the chances of infection transmission. Fortunately, the hand hygiene technique is published by The World Health Organization (WHO) to help medical staffs keep all the surfaces of their hands clean during working. However, people may ﬁnd it difﬁcult to remember these steps correctly. Hence, it is substantial to automatically control the quality of the hand wash process in clinical environments. A computer vision system is one of the most efﬁcient methods to accomplish this. Speciﬁcally, hand gesture recognition technologies, in particular, have already been used to assess hand hygiene compliance [1]–[3]. In this paper, we consider the task of hand gesture recognition over a hand hygiene system. Our goal is to interpret the gestures of medical staff when they are washing their hands. However, different from other domains, hand gesture recognition for the hand wash process has two main aspects. First, hand hygiene is a process that contains ﬁne-grained actions. It means that a deep learning agent is about to deal with both signiﬁcant intra-class differences and subtle inter-class differences during their prediction process. For huge intra-class differences, the agent may recognize hand gestures that belong to the same category but may present quang}@willogy.io quang.tran}@aioz.io duynvm@bthh.org.vn , An T.Duong, and Quang D.Tran signiﬁcantly different poses and viewpoints. For subtle interclass differences, the agent may cope with gestures that belong to disparate steps but might be very similar apart from minor differences. Hand gesture recognition for ﬁnegrained actions has been introduced in many approaches [4]– [7]. However, in the hand hygiene domain, this aspect has not been focused on yet. Second, the hand hygiene data are mismatched in distribution between the training phase and the inference phase. Indeed, the behavior of medical staff in different locations and camera views are not the same. Some steps are missing or not taken correctly, which requires lots of human effort to annotate. As a result, although similar data from other data distributions might be readily available, only a limited amount of data from the target distribution can be collected. Many works on hand gesture recognition have mentioned this data-driven issue [8]–[10]. However, there is no current dataset for the hand hygiene domain challenging enough to compare various gesture recognition methods. By considering the two aspects above, our paper makes two contributions. The main contribution is to simulate both preceding problems by introducing a multi-viewpoint ﬁne-grained hand hygiene dataset, named the MFH dataset (Fig.1). It contains 731147 samples in total, which are collected by 6 camera views in 6 different locations. All samples are split into 7 classes in total. MFH dataset is distinguished from existing datasets in three aspects: the large intra-class difference, the subtle inter-class difference, and the data mismatch in distribution between the training phase and the inference phase. This dataset thus provides a more realistic benchmark. For performance evaluation, besides the accuracy, we recommend using the Macro F1score for a more comprehensive measurement. As a minor contribution, we address the preceding problems by applying the self-supervised learning approach for recognizing hand gestures in a hand hygiene system. Intuitively, the method is designed to maximize the mutual information between features extracted from multiple views of a shared context. This method is expected to deal with hand hygiene ﬁne-grained problems and reduce the negative effect of distribution mismatch. To our knowledge, there is no previous approach that leverages self-supervised learning in dealing with multi-viewpoint hand gesture recognition. Next, we review the related work in Section II. We then describe our dataset in Section III and the self-supervised learning method in Section IV . In Section V, we present extensive experimental results. Finally, we conclude the paper in Section VI. Hand Gesture Recognition. Hand gesture recognition in a hand hygiene system is not a trivial learning task. There are two groups of approaches to coping with this. The former one, the sensor-based work, leverages the information from different sensors to establish the recognition [11]–[14]. The latter one, the image-based approach, mainly leverages input images and their correlated data to give out hand gesture predictions [2], [3]. Speciﬁcally, in [4], [11], [15]–[19], the authors use information from depth sensors as input to give out hand gesture predictions. In [4], [8], [20]–[25], the skeleton information is used as input. Recently, Leap Motion has also been considered as an essential sensor for hand gesture recognition [18], [26]. Although sensors play a crucial role in many situations, most sensors are costly and may not be easy to conﬁgure. Different from sensor-based approaches, the image-based ones mainly take images as the input. Particularly, in [2], the authors extract HOG and HOF over images and then apply SVM for hand gesture classiﬁcation. In the most recent approach [27], the authors propose hand hygiene monitoring based on the segmentation for separating hand parts of interacting and self-occluded hands. To our knowledge, almost no works consider the ﬁne-grained characteristic of hand gestures and the data mismatch over different viewpoints in the hand hygiene system during recognition. Moreover, datasets used in these previous works are private [2], [27], do not handle the ﬁne-grained problem or the data mismatch problem [3]. Self-supervised Learning. Self-supervised learning (SSL) aims to self-generate robust representations from the unlabeled data according to the structure or characteristics of the data itself. SSL works as a supervision and beneﬁts almost all types of downstream tasks, e.g., classiﬁcation, recognition, or image retrieval [28]–[37]. To deal effectively with the image classiﬁcation task, the authors of [38] implement a hybrid system of self-supervised learning and semi-supervised learning. In [39], Pretext-Invariant Representation Learning (PIRL) is used to solve jigsaw puzzles and their rotation by enhancing the quality of the learned image representations. Recently, SSL is introduced to be more generalized since it can maximize the mutual information between features extracted from multiple views of a shared context [40]. Inspired by the SSL, we leverage the generalization of the AmDim setup [40] to deal with the data mismatch problem in hand gestures recognization from different viewpoints. The introduced MFH dataset is an on-top dataset using images from [3]. These images were collected monthly, and the total deployment duration was 3 months. During dataset collection, a total of 6 cameras were placed in 6 different locations. All cameras are set at 640 ×480 pixels resolution, and their frame rate is 30 fps. There are deﬁned seven different hand washing movements as recommended by the WHO. These movements are as follows: palm to palm (Step 1), palm over dorsum with ﬁngers interlaced (Step 2), palm to palm with ﬁngers interlaced (Step 3), back of ﬁngers to opposing palm (Step 4), rotational rubbing of the thumb (Step 5), ﬁngertips to palm (Step 6), turning off the faucet with a paper towel (Step 7). For more details, please visit Fig.1 which illustrates the visualization of these movements from different viewpoints. Additionally, it was necessary to identify whether a person is washing hands with a watch, a ring, or has lacquered nails. The reason is that these factors interfere with basic handwashing procedures and can be regarded as inappropriate for medical professionals in their work environment. The dataset consists of 1827 annotated video ﬁles, each of which corresponds to a single hand-wash episode. The video ﬁles are split into frames that are easier to access. For each video ﬁle, there is a matching .json ﬁle, which contains the annotations of each frame in JSON format. Overlapping exists among all viewpoints (scenes); the dataset contains up to 731147 samples (frames). Table I illustrates the detail statistics of the MFH dataset. Speciﬁcally, each row denotes the number of samples of each speciﬁc scene. There are 6 scenes in total. The column indicates 7 steps in the hand wash process, and the row demonstrates the index of different scenes. Besides, we provide the total samples of each scene in the ﬁnal column. Through statistical, there is bias in the number of samples over classes in each viewpoint, i.e., imbalanced data. Moreover, the distribution of the bias over classes between these viewpoints is highly distinct. Our approach is expected to provide an opportunity to compare and evaluate the performance of different hand gesture recognition networks under two challenging aspects: ﬁne-grained hand gestures and data distribution mismatch over different viewpoints. These aspects are consistent with practical usage. Hence, our dataset provides a testbed for methods applied in open systems. We typically use Accuracy to evaluate the effectiveness of different models in the hand gesture recognition task. Accuracy is calculated as the ratio between the number of correct predictions to the total number of predictions. The deﬁnition of Accuracy is also described as in (1). where C is the number of samples that are recognized correctly, A is the number of all samples in the test data. The Accuracy metric is essential in most cases. However, if the benchmarking dataset is not balanced, this metric has not much reference value. Since the MFH dataset is an imbalanced one, we introduce the Macro F1-score. Unlike Accuracy, which focuses on the importance of samples, the Macro F1-score puts the same importance on each class. The model that only performs well on the common classes while performing poorly on the rare classes will cause a low Macro F1-score. Macro F1-score, so called Macro-averaged F1 score, is deﬁned as the mean of class-wise/label-wise F1-scores. The F1-score F 1is the harmonic mean of precision and recall. Let TP, FP, FN, P, Rand F1be the true positives, false positives, false negatives, precision, recall, F1-score with regard to class i and H is the harmonic mean. The precision P, the recall R, and the F1-score F 1are computed as in (2), (3), and (4), respectively. where N is the number of classes, F1is the calculated F1 value on class i (1 ≤ i ≤ N). In the ﬁnal step, the Macro F1-score is then calculated using (5). A statistics comparison with existing datasets is shown in Table II. Our dataset contains 731147 samples, which is larger than the current largest dataset [3] by double. Different from [27] which uses Depth Infrared images as inputs, RGB images are leveraged in our introduced MFH dataset since they provide good visual information. The highlight of the MFH dataset, also the key difference when comparing MFH with other datasets, is that it contains 6 sub-datasets from 6 non-overlapping viewpoints. Under realistic Healthcare Industry settings, our dataset serves as an ideal benchmark for learning methods that focus on the generalization capacities and data mismatching by opening two different evaluation protocols for testing. Self-supervised learning derives from unsupervised learning and can be applied in any recognition or classiﬁcation task. It aims to learn semantically meaningful representations from unlabeled data. Generally, some portion of the data is retained, and the network is tasked with predicting it. One of the most effective approaches is to design a pretext task, which maximizes the mutual information between features extracted from multiple views of a shared context. The context here is the input images, and the preceding views are augmented from these inputs. By following [40], [41], we determine the mutual information (MI), which measures the shared information between two random variables X and Y . MI is deﬁned as the Kullback-Leiber (KL) divergence between the joint P (x, y) and the product of the marginals P (x) and P (y). Since it is not easy to direct access to the underlying distribution to estimate MI, we instead maximize a lower bound on MI by minimizing the Noise Contrastive Estimate (NCE) loss based on negative sampling. Our objective is to maximize MI between global features and local features from two views (x, x) of the same hand gesture input image .i.e., hf(x), f(x)i, hf(x), f(x)i and hf(x), f(x)i. Where f, f, fare the global feature, the encoder’s 5×5 local feature map and the encoder’s 7×7 feature map respectively. The NCE loss between f(x) and f(x) is deﬁned in (7). where Nare the negative samples of image x, φ is the distance metric function. The overall loss between xand xis the total of the NCE losses and is written in (8). It is worth noting that, after ﬁnishing the training process, the self-supervised learning network is leveraged as an encoder to extract features for the further classiﬁcation task. To achieve hand gesture recognition, we need to train a supervised learning network, i.e., a classiﬁer, on top of the aforementioned extracted features using the annotated hand gesture data. The structure of the self-supervised learning network is the standard ResNet [42]. For the classiﬁer, a linear layer or a multilayer perceptron is used as the structure. For more details about these structures, please visit [40]. Implementation details. All experiments are conducted on an NVIDIA Titan V GPU with 12GB RAM. All models are trained by using Stochastic Gradient Descent with a momentum of 0.9. The initial learning rate is set to 0.001, with exponential decay of 0.9 after every two epochs. The maximum number of epochs is set at 40. Data setup. In the MFH dataset, the data got from each viewpoint is split into a train set and a test set. Each set contains 50 percent of data, and samples in each are not overlapping. There are two scenarios for the evaluation phase. The ﬁrst scenario is that the model is trained and tested within the same camera data, i.e., “same scenes” scenario. The second scenario is that the model is trained in a speciﬁc scene notwithstanding its effectiveness is evaluated in the data collected from other scenes, i.e., “cross scenes” scenario. Baselines. MobileNetV2 [43], ResNet-18 [42], and InceptionNetV3 [44] are leveraged as the baseline network for our analysis. These models are pretrained on the Imagenet dataset [45] and then ﬁne-tuned in a speciﬁc sub-dataset so as to maximize their performance. AmDim [40], a selfsupervised representation learning baseline, is expected to work well under the limitation of our dataset. Following the setup of other baselines, the model is pre-trained on the Imagenet dataset [45]. Finally, we train a classiﬁcation on top of the features for the recognition of hand gestures. Fine-grained action analysis To identify the effectiveness of deep network over hand wash actions, we use the wellknown InceptionV3, which is pre-trained in the ImageNet dataset [45], as the baseline. An analysis is established in the 1-st scene data of the MFH dataset. In Figure 2, we present the confusion matrix over different tested samples. The results imply that there has a visible of confusing predictions over classes, e.g., The model tends to give out the predicted Step 1, 2 when it meets images from Step 5, 6. The main reason is that gestures that belong to disparate steps might be very similar apart from some minor differences. Data mismatch analysis. To further understand the MFH dataset and its challenges, we provide the recognition results between all scene pairs in Figure 3 where InceptionV3 is used as the baseline. Each value in the ﬁgure is the Accuracy( Figure 3- a) or Macro F1-score (Figure 3 - b) when we test a speciﬁc model in the corresponding test set. Note that the row denotes the train data, and the column denotes the test data. Through the ﬁgure, hand gesture recognition within the same camera view, i.e., the “same scenes” testing scenario, yields the highest accuracy score. On the other hand, as expected, the performance among different camera pairs, i.e., the “cross scenes” testing scenario, varies a lot. In most cases, InceptionV3 achieves low results due to the mismatch in distribution between train set and test set, regardless of the number of classes are not much, and the network itself is strong enough. Besides, the Macro F1-score is by far lower than the Accuracy in most “cross scenes” experiment results. It indicates that the imbalance over classes further increases the data mismatch between viewpoints. Table III demonstrates the results obtained by different typical deep network structures including Mobilenet [43], ResNet [42], and InceptionNet [44]. All of the preceding networks do not achieve good results during the inference phase of “cross scenes” in both the Accuracy and the Macro F1-score. These results imply that all benchmarking networks can not work well with the data mismatch problem. Self-supervised learning analysis The AmDim [40] is leveraged as a self-supervised learning baseline to deal with both analyzed problems. Table III demonstrates the performance comparison between AmDim and other learning methods. In the “same scenes” testing scenario, AmDim outperforms other networks by a large margin. This result indicates that AmDim can learn robust features for recognizing ﬁne-grained hand gestures. In the “cross scenes” testing scenario, AmDim also achieves signiﬁcant improvements in the Accuracy metric. Especially when comparing it with InceptionV3 - the most effective baseline, i.e., the Avg. The accuracy gap between AmDim and InceptionV3 is 0.1. Hence, the self-supervised learning approach can deal with the mismatch in data distribution over different viewpoints. It is worth noting that AmDim also outperforms other baselines in Macro F1-score, which validate the effectiveness of AmDim over imbalanced data problems (See Figure 4 for quantitative results of AmDim in over 6 viewpoints.) Table IV presents the AmDim performance with and without pretraining the model in the ImageNet dataset. In terms of the “cross-scene” scenario, the results show that both setups give better scores than the baseline InceptionV3 in two metrics, regardless that InceptionV3 is pre-trained on the ImageNet dataset. This again validates the effectiveness of selfsupervised learning. Besides, through empirical experiments, we have investigated that using a deeper neural network, e.g., the multilayer perceptron (MLP), for classiﬁcation achieves better results than using a linear one. We introduce a multi-viewpoint ﬁne-grained hand hygiene dataset (MFH) that reaches closer to realistic settings, especially in the Healthcare Industry. Our new dataset will enable research possibilities in multiple directions, e.g., deep learning, ﬁne-grained learning, multi-view learning, and data distribution learning. Besides, self-supervised learning (SSL) is presented to deal with ﬁne-grained hand gestures and data mismatch problems. The extensive experiments show that SSL yields the best performance with various competitive baselines in Accuracy and Macro F1-score. Special thanks to AIOZ Singapore company and Blood Transfusion Hematology Hospital Vietnam for the valuable support on the cooperation.