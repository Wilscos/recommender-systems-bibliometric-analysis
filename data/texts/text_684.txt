The ﬁeld of AI is experiencing rapid growth in terms of more accurate models and wider applicability. This is largely due to the design of new architectures and improvements on existing ones. Furthermore, state-of-the-art models are following the trend of higher resource usage in both energy and data. However, there are several risks involved in building larger models, namely the consequences of climate change and privacy violations. These crises are explored in detail with respect to CV, as well as key stakeholder groups in this context. In this work, we extend the work from [1] and fully develop the concept of enforcement via the ethical framework P4AI: Principlism for AI. The concept of enforcement is proposed, which outlines actionable considerations to minimize the impacts of building and training neural architectures. Enforcement works as an extension of principlism, an ethical framework specialized for engineering applications. [2]. AI and their neural architectures are becoming an ever-growing presence in our lives, as the global AI market is forecasted to reach $299.64 Billion by 2026, at a growth rate of 35.6% [3]. As society moves towards dependence on AI and ML technologies, the CV community has placed great signiﬁcance on ethical principles around data biases, privacy, and algorithmic biases without much consideration towards the neural architectures that underpin these models. [1] showed that CV models emit signiﬁcant CO2 emissions throughout their lifespan, referenced in Appendix A for context. We highlight two crises whose causes can be directly addressed by neural architectures: (1) The Climate Crisis and (2) Privacy. Climate Crisis: peer-reviewed reports on climate change for the scientiﬁc community to make informed decisions within their work. The 2019 IPCC Special Report proposed that limiting warming to 1.5 pre-industrial levels is necessary to mitigate extreme climate-related risks. In 2016, we emitted 52 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. University of TorontoUniversity of Toronto andre.fu@mail.utoronto.caelisa.ding@mail.utoronto.ca University of New BrunswickUniversity of Toronto mahdi.hosseini@unb.cakostas@ece.utoronto.ca The ﬁeld of computer vision is rapidly evolving, particularly in the context of new methods of neural architecture design. These models contribute to (1) the Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data leakage concerns. To address the often overlooked impact the Computer Vision (CV) community has on these crises, we outline a novel ethical framework, P4AI: Principlism for AI, an augmented principlistic view of ethical dilemmas within AI. We then suggest using P4AI to make concrete recommendations to the community to mitigate the climate and privacy crises. The Intergovernmental Panel on Climate Change (IPCC) provides transparent and GtCO2 and by 2030 we will have 52-58 GtCO2, while we should be at 25-30 GtCO2 within the same time-frame. However, the recent IPCC Sixth Assessment Report shows that “global warming of 1.5C and 2.0 greenhouse gas emissions occur in the coming decades” [4]. Additionally, the report extrapolates to possible warming scenarios depending on the levels of greenhouse gases (GHG) emitted. Global surface temperatures during 2081-2100 will likely increase 1.0-1.8 2.1-3.5C under intermediate GHG emissions, and 3.3-5.7 CO2 levels indicate that every person and community must play their part to curb the climate crisis, and it is urgent that we consider CO2 impact while creating and choosing neural architectures [1]. Privacy Crisis: computers are being linked together to move information at extreme rates. As we become more interconnected, the question of privacy arises, in particular how can we defend privacy as a human right [5] in the face of rising technology trends where data and privacy exploitation is common [6]. Accordingly, inspecting the AI and ML models that underpin modern technology is essential in protecting privacy. As we inspect these models, it becomes apparent that model security and the neural architecture itself form the basis for privacy. A framework that contextualizes model security and possible risks of breaches is the conﬁdentiality, integrity, and availability (CIA) model [7, 8]. Compromised conﬁdentiality may include leaking protected model details and private data (eg. medical records). Attacks on integrity result in biased outputs, for example by altering training data. Lastly, attempts to block access to model details are an example of compromised availability. While both the climate crisis and privacy crisis are global dilemmas within modern society, we scope this to current researchers and marginalized communities. In identifying these groups, we highlight the dynamic between the groups and the direct consequences they have on each other. Within the context of privacy protection, a plethora of concerns have risen around the computer vision and privacy risks. Recently, there have been discussions [9] around facial occlusion reconstruction and the maleﬁcent implications of using this technology to invade privacy, particularly in regions with oppressive regimes. Furthermore, computer vision models suffer from data-preservation gaps where models can be attacked where attackers can reconstruct private training data. These threats are often not considered by CV researchers, and greater awareness should be shown for the downstream applications and implications of neural architectures. Marginalized Communities on marginalized communities, which we deﬁne as groups experiencing exclusion from mainstream social, economic, educational, and/or cultural life [10]. As analyzed in [1] in terms of CO2 emissions, they found a rarely discussed marginalized group: our future generations. They deﬁned this group and highlighted a subgroup who would be most affected, those in equatorial and coastal regions. As CO2 emissions become an increasingly pressing issue, our future generations in equatorial regions will experience serious water-stress affecting between 75-250 million people in Africa. Furthermore, C will be exceeded in the 21st century unless deep reductions in CO2 and other Our society is moving towards a highly interconnected system, where people and those in coastal regions will experience 1-8 feet of sea level increase and be at higher risk of losing their homes. In terms of privacy, we particularly identify two marginalized populations: those in oppressive regimes and peoples of colour. Deep-learning models are becoming increasingly popular to identify marginalized groups within oppressive regimes [11,12] as they have the ability to efﬁciently target peoples. Furthermore, in the medical sector AI models can help diagnose those with niche diseases but via data-extraction techniques, patients can be personally identiﬁable through weak neural architecture design. In identifying the key stakeholders, we demonstrated that neural architecture design considerations should extend beyond test-accuracy or performance, but also deeply into the ethical, societal and real-world. Within the ethical AI community, we’ve adopted ‘Principlism’ [2,13] as the primary framework to analyze ethical issues that arise [14 pillars: • Respect for Autonomy: • Beneﬁcence: refers to the obligation to prevent harm and act in the beneﬁt of others • Justice: describes the requirement to ensure risks, costs and beneﬁts are fairly distributed • Non-maleﬁcence: In this work, we propose P4AI: Principlism for AI where we add a ﬁfth pillar of ethical AI - enforcement. Enforcement is the idea that our obligations to marginalized communities should be included within our research, furthermore these concepts should permeate our work. In realizing our impact, we can hold ourselves and our community accountable. Through Principlism, we can analyze the impacts of designing neural architectures that do not consider ethical and down-stream implications. Here we apply the four principles through the lens of the climate and privacy crises. Two cases are discussed in Appendix B to describe possible ethical risks and enforcement as a method to reduce such risks. Climate Crisis: capacity to be self-determining as we are forcing them to endure incredible amounts of water-stress, agricultural conﬂict and increased risk of extreme weather events. In contraposition to Beneﬁcence, we are not acting in the beneﬁt of future generations by actively emitting this level of CO2. As we currently enjoy a stable climate while marginalized groups and our future generations will experience the implications of our actions today, we’re not fairly distributing costs, beneﬁts an risks, violating Justice. Additionally, by causing harm to marginalized groups despite the signs of the climate crisis, we’re actively causing harm thus violating Non-maleﬁcence. Privacy Crisis: designing architectures that do not consider marginalized populations. We’re inadvertently removing the ability for those in marginalized groups to be self-determining by targeting those in oppressive regimes. By creating neural architectures that are easily attacked, they can leak sensitive information thus causing harm, violating beneﬁcence. determining, speciﬁcally with no coercion intentional harm As we emit this level of atmospheric CO2, we are overlooking future generation’s Within the context of privacy, we are unintentionally violating Principlism by CO2 Reduction Methods: sions in both the initial architecture search and lifetime use of models. FLOPs are the main established measure of a model’s computational cost and can be incorporated as a target for minimization. Three main optimizations can be made to reduce subsequent environmental cost [1]: • NAS-RL: building on hardware-aware NAS to include a CO2/FLOP objective • Evolution: adding a CO2/FLOP regularization parameter to the ﬁtness function. In addition to adopting these optimizations, researchers should strive to consistently report FLOPs values to build a culture of transparency and accountability. We can enforce optimizing on FLOPS by refusing to use high FLOPs networks, thereby communally penalizing researchers publishing these types of models [1]. Privacy Protection Methods: threats of attacks on systems and their data. Measures should be taken to protect against attacks throughout all phases of design, training, and inference. The following are suggestions that guard against external manipulation [17]: Ethics Cards: of machine learning models. These model cards provide documentation on the performance, datasets, and context of models. This includes information such as model details (developers, version/type, parameters, constraints), intended use, training/test data, and recommendations. We propose utilizing a similar technique to provide an accessible enforcement-based analysis to the community and public. These ethics cards should outline potential risks and promote transparent discussion regarding risks. Ethics Committee: involves extensive peer review, often with a conﬁdential and double blind process to ensure scientiﬁc integrity. However, there is no existing review process to ensure ethical integrity of AI research. We propose implementing an ethics committee to overview accepted papers and prepare a statement on the ethical considerations and risks of each paper. We have deﬁned the pressing concerns of the Climate and Privacy crises, as well as the role of AI and ML technologies. As researchers of these neural architectures, we hold strong responsibility for minimizing their CO2 emissions and associated privacy risks. This is further driven by consideration of marginalized communities who cannot defend themselves against the crises. Enforcement was introduced as an extension of Principlism, where actionable steps must be upheld by peers to achieve universally high ethical standards. Enforcement methods promote CO2 reduction and data protection directly during the development of neural architectures in addition to transparency of these architectures. Example cases were also presented to show how climate and privacy effects could be mitigated in prior work. We hope that enforcement can be implemented within the community to aid in developing positively impactful neural architectures. Gradient-based: including a CO2/FLOP constraint on the chosen-operations level of the bi-level Data poisoning detection: Altered data can be detected through statistical approaches, as poisoned samples lie outside of the expected data distribution. For example, Rubenstein et al. developed a PCA detection model to identify and limit outliers of a training set [18]. Furthermore, introducing a regularization term to loss functions can reduce complexity [18]. Mitchell et al. [19] developed the framework of model cards for transparent reporting