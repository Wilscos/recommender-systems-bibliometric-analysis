<title>A Bayesian semi-parametric approach for modeling memory decay in dynamic social networks</title> <title>Giuseppe Arena Joris Mulder Roger Th. A. J. Leenders September 7, 2021</title> <title>1 Introduction</title> <title>arXiv:2109.01881v1  [stat.ME]  4 Sep 2021</title> to develop network models that suit the inherent dynamic nature of these so-called relational event data. A relational event is deﬁned as an action initiated by a sender and targeted to one or more receivers at a speciﬁc point in time. The relational event modeling framework aims to model the event rate, that is the speed at which relational events occur over a period of time between the actors in the model. The event rate can be expressed as a function of characteristics that quantify endogenous network patterns or exogenous information which determine how the network unfolds at some point in time Butts, 2008). In sociological and psychological research, the application of these relational event models aims to ﬁnd behavioral patterns and to shed light on the emergence of a global structure from network dynamics occurring at a local level (Leenders, Contractor, & DeChurch, 2016). Of particular interest is to understand what triggers actors to interact with each other. Actors might decide which mutual recipient to target their actions to depending on various aspects such as homophily, norms of reciprocity, the volume of past social interactions, et cetera. Past relational events inﬂuence future events in diﬀerent ways. First, this inﬂuence depends on qualitative aspects of the past events, such as whether the interaction was positive or a negative or who was the sender of the past event. For example, receiving a message from the company’s president might have a greater eﬀect than a message from another colleague. Events with a negative connotation have been argued to have a greater eﬀect than events with a positive connotation. For instance a rebuke by the teacher may have a stronger eﬀect than a praise by the teacher towards a child Brass & Labianca, 1999). Second, more recently past events generally have a greater inﬂuence on the present than events that occurred a long time ago Butts, 2008). In a school setting a rebuke might have a longer lasting eﬀect on future interactions than a praise and a rebuke coming from the teacher may have a longer-lasting eﬀect than a rebuke coming from a classmate. This variability lays the foundation for the presence of an underlying memory process that shapes actors’ decisions over time. The probability of remembering previously stored information has already been discussed in past studies where the memory retention concept was conceived as a stochastic psychological process. There, memory retention was studied by examining the dynamics of the hazard function corresponding to this stochastic process (Chechile, 2003, 2006, 2009). In the relational event modeling literature, on the other hand, little attention has been paid to the possibility that recently occurred events may have a larger impact on the event rate than those events happened long ago. This is an important limitation as memory plays a crucial role in our understanding of social interaction between actors in a network. There are some noteworthy exceptions however. One approach has been to quantify a speciﬁc pattern of interactions according to two predeﬁned diﬀerent time interval deﬁnitions as in a 2-step approach: a short-term expression (calculated by considering recently passed events) and a long-term expression (considering only long passed events in the computation) Quintane, Pattison, Robins, & Mol, 2013). The estimated eﬀects of the two deﬁnitions describe how diﬀerent the impact of the speciﬁc pattern is on the event rate according to diﬀerent recency of events constituting the pattern itself. Another approach consists of estimating the model while using a moving time window of a speciﬁc memory length. The result is a discrete trend of the eﬀects over the windows J. Mulder & Leenders, 2019). An alternative to time-intervals-based methods weighs events by means of an exponentially decreasing function with a given half-life parameter that describes the elapsed time beyond which the weight of an event in the calculation of the statistic is halved Brandes, Lerner, & Snijders, 2009). A similar approach was suggested by Butts, 2008). In all these cases we need to predeﬁne the memory lengths both when deﬁning short and long-term statistics and when choosing the memory length in the time window, or to a particular steepness of decay in the case of the half-life. The problem of the 2-step approach is that (i) it is unlikely that memory decays in a discrete step-wise manner, and (ii) the time points of the steps need to be chosen a priori (which is done in an ad hoc manner). The exponential decay on the other hand is modeled as a smooth function, which seems more natural. However the the half-life parameter which determines the steepness of the decay is arbitrarily chosen by the researcher a priori. The steepness of the decay however is generally unknown in practice. The purpose of this work is to present a semi-parametric method for learning the shape of memory decay in relational event models. The method is semi-parametric in the sense that it does not make assumptions about a speciﬁc functional form for memory decay. Indeed, parameters that potentially govern the memory process and, in turn, determine its shape over time are often unknown and our intent is to minimize the risk of mistakes that a parametric misspeciﬁcation could induce. Our method can be used for ﬁnding any functional form of memory decay which could be an exponentially decreasing trend, a smoothed stepwise function, or other, possibly more (or less) complex, functional trends. In the case of event history data with combinations of positive and negative events, the proposed method will not only allow us to learn how much longer (or shorter) negative events are stored in the actor’s memory than positive events but also whether the memory decay of these diﬀerent sentiments follow diﬀerent functional shapes. Furthermore, this semi-parametric method combines the application of Bayesian inference in the context of a model selection problem (Bayesian Model Averaging) Volinsky, Raftery, Madigan, & Hoeting, 1999) with the use of the relational event modeling framework as in (Butts, 2008). The work is structured such that in Section 2 the relational modeling framework is introduced with a ﬁrst insight over the concept of memory decay. In Section 3 a stepwise memory decay model is formulated. In Section 4 a continuous memory decay model is presented as well as the potential use of stepwise models in approximating the continuous shape of the decay. In Section 5 the semi-parametric method based on a Bayesian Model Averaging is presented along with two weighting systems used for generating random draws from the posterior memory decay. Finally, in Section 6 the method is applied to empirical data and ﬁndings are discussed, thus concluding with Section 7 where a few ﬁnal considerations are taken around the methodology and its potential developments. <title>2 Relational event models that capture memory decay</title> where (at = 1) is assumed equal to zero or to the starting time point of the case study, , r , X , E , β ) is the rate of the event occurred at time and , r , X , E , β ) represents the event rate of any event that could have happened at time (including ). Indeed, belongs to which is the risk set consisting of all sender/receiver combinations, such as S × R : where and are respectively sets of all possible senders and receivers for the entire event sequence. Where all actors can be senders as well as receivers in an interaction, then S ≡ R and the set of actors is simply indicated with . Equation (1) can be viewed as the well-known survival model with time-varying covariates, where hazard and survival components form the likelihood in the same way (Lawless, 2002). where: • β with = 1 , . . . , P , are parameters describing the eﬀects of statistics on the logarithm of the event rate; • u , r , X , E ) with = 1 , . . . , P , are the statistics of interest and each one can depend either on transpired events (endogenous statistics calculated at each time point, given E , and for either all dyads or all actors) or on exogenous attributes (X ). In the standard speciﬁcation of the model, endogenous statistics describe patterns of interactions occurring in the network which are quantiﬁed at each time point by considering the whole history of events happened from the initial state of the network (ﬁrst observed relational event) until the time point before the current one (that is in (2)). For instance, consider the standard formulation of the inertia statistic, which is a dyadic endogenous statistic that quantiﬁes the volume of interactions of a speciﬁc dyad occurred until the current time point. Inertia quantiﬁes the extent to which speciﬁc relational events keep repeating in the network over time. The corresponding formula at a generic time point with history E will be the following, A positive estimate for means that actors interact at higher rates with those actors who were often receivers of their past interactions. This is a sign of social routinization: what happened in the past is bound to be repeated over and over into the future, For instance, consider Figure 1 where a sequence of events from to is represented on a time line. In order to calculate the inertia at time for the speciﬁc dyad ( i, j ) we need to count the number of past events in the history where targeted an action to which is 6 in the example. Although this approach would give insights about how previous interactions between actors have inﬂuence on the event rate, we would be assuming long passed events (such as those that happened 14 and 11 events ago, over two hours ago) to be Figure 1: Example of inertia calculation relative to the dyadic event ( i, j ) and given a sequence of events , . . . , t with the corresponding history .The event of interest in the calculation of the statistic is written in black, gray is otherwise. Without considering intervals, the value of inertia at time is 6. Whereas, when considering the interval deﬁnition (intervals delimited by upwards arrows) the value of inertia across the three intervals will become inertia = 1, inertia = 3 and inertia = 2. equally inﬂuential as recently passed ones (such as the ones that are only 1 or 4 events or 45 minutes or so old) in the computation of the statistics as well as on the event rate itself. This assumption may not be realistic for relational event data in practice as indicated earlier. Hence, the need of specifying a model that would be capable of accounting for this mutable eﬀect by past events on dyadic event rate. <title>3 A stepwise memory decay model</title> <title>3.1 Stepwise decay for ﬁrst order endogenous eﬀects</title> = {e ∈ E : (t − t ) ∈ (0secs, 30mins]} = {e ∈ E : (t − t ) ∈ (30mins, 2hrs]} (5) = {e ∈ E : (t − t ) ∈ (2hrs, ∞)} Where the ﬁrst sub-history contains all events transpired until 30 minutes before the second, , includes those events happened between 30 minutes and 2 hours before lastly, the third sub-history, , includes all events happened more than 2 hours before (the right bound is left undeﬁned here). In Figure 1, the partition into sub-histories is shown by the upwards arrows corresponding to the time lengths . Therefore, three values of inertia can be calculated at any time point in the observed sequence by considering the Figure 2: Stepwise eﬀect of Inertia on the event rate. three diﬀerent partitions of the event history according to the increasing time lengths (γ). The event rate for any possible event ∈ R at time where inertia is deﬁned in partitions will assume the form below, Once statistics are calculated across the partitions their corresponding parameters with = 1 , . . . , K , can be estimated using the likelihood function in (1). In the interval case for the inertia, parameters express how the propensity of actors in targeting their actions to the same past receivers changes as a function of the recency of past events themselves. The use of interval statistics according to K partitions of the event history, directly relates to the dynamic of the estimated eﬀects and their evolution will follow a step function as in Figure 2 with a mathematical function as in (11), that is based on the time lengths used to create the partitions. Stepwise memory eﬀects can also be modeled for other ﬁrst order endogenous statistics such as reciprocity, sender/receiver-in/out-degree whose formulas can be found in Appendix A.1. <title>3.2 Stepwise decay for higher order endogenous eﬀects</title> Besides statistics that are based only on past interaction within a given dyad, the eﬀects of higher order statistics involving more than two actors, can be used as well within this approach. Higher order endogenous statistics are characterized by more than one dyadic relational event in their formula. As such, the behavioral pattern of interest is more complex substantively as well as its computation. Indeed, in the case of triadic statistics, as with transitivity, the computation consists in the quantiﬁcation of the number of times a dyad could potentially close a particular triangular structure if it occurred as next interaction after a speciﬁc sequence of past events. where: • I l, j ) is the indicator variable that assumes value 1 if the event e ∈ E has and , 0 otherwise (the same reasoning applies to the other indicator variables in (12)); Figure 3: Figures from left to right describe the pattern of the transitivity closure in three steps. Given the event history , the possible event ( i, j ) occurring at (3c) can close a triad already opened with a third actor ( in the example) that acts as a broker in the process of information sharing/mediation. Events ( i, l ) (3a) and ( l, j ) (3b) occur by following the order in the example, where and at time are the transpired times since the two events ( i, l ) and ( l, j ), such that 0 ≤ δ < δ < t . Therefore, the order of their occurrence is taken into account. Gray nodes and dashed gray arrows indicate respectively inactive actors and events already occurred, whereas active actors and the occurring dyadic event are in black. Figure 4: Example of calculation of transitivity at for the dyad ( i, j ) and information mediator : the event history counts only two events ( l, j ), at time and . In order to quantify the contribute of to the transitivity i, j, t ) we consider the intervals [t − γ (t ), t ) and [t − γ (t ), t )] to seek for the ﬁrst event in the pattern, that is (i, l). The contribute of events and to the statistic are respectively 2 and 1. Then the value of transitivity for ( i, j ) at with mediator is 3, meaning that if ( i, j ) is the next event to occur it is going to close three potential triads where the information mediator was l. • γ (t ) = t − t is the time transpired at t since the event e ∈ E Figure 5: Stepwise eﬀect of Transitivity on the event rate. Figure 4 shows an example of the formula in (12) for just one l ∈ S \ {i, j} at time with a history of events . In the example, two dyadic events ( l, j ), noted as and occurred at and before . For each of them we seek backward for those events and that occurred within intervals based on the transpired time of ) = − t ) and ) = − t ) which are respectively [ − γ , t ) and [ − γ , t ). Hence, if any event or in these intervals has sender and receiver then the product of the two indicator variables in (12) will be one and so will be contribute to the sum, 0 otherwise. In the speciﬁc example, as to event we observe two dyadic events ( i, l ) happened in [ − γ , t ), whereas for we ﬁnd just one event ( i, l ) occurred in [ − γ , t ). Therefore, if the dyad ( i, j ) is going to occur at it would close at least three potential triangular structures of the type in Figure 3 where the actor is the information mediator. The new formula for transitivity closure accounts for the time order of events in the triadic behavioral pattern and assumes that those events ( i, l ) happened earlier than an event ( l, j will count in the formula if and only if they transpired within the same time span of the speciﬁc (l, j). A positive β means that the more partners s and r had in common in the past the more likely will chose as receiver of its coming interaction. Vice versa, when 0, the rate of the event lowers, meaning that there is a tendency by actors in discouraging closure and thus, in fewer interactions with those actors they shared a partner with. However, the statistic in (12) refers to the event history , that is the entire sequence of events since the onset until (including ). Whereas the eﬀect may not be constant, depending on how recently the event ( l, j ) occurred. Thus, transitivity can be re-deﬁned in intervals in the same way as inertia in Section 3.1. according to + 1 increasing time lengths (as in (7)). The transitivity as regards the k-th interval, for the dyad (i, j) at time t will be, where the quantiﬁcation of potential triads is divided through the intervals of the history , . . . , E according to the time transpired at since the event that is ). However, the seeking of the event still considers the time interval as in (12). By using the interval formulation we are interested in understanding whether there exists an evolution of the transitivity eﬀect on the event rate that depends on the recency of events constituting the triadic pattern. According to the stepwise formulation of the transitivity, we can rewrite the rate in (13) as follows, The eﬀect of transitivity across intervals conveys more information than in the case without intervals. Although, the interpretation of positive and negative eﬀects remains the same (i.e. positive eﬀects still promote the closure of triads as well as negative eﬀects keep discouraging it) the intensity of such behaviors that promote/discourage triadic closure can change over time and this one is the additional information we are after. For instance, if the eﬀects from the ﬁrst to the last interval are positive and decreasing, that is > . . . > β this means that the estimated eﬀects are encouraging the closure of triadic structures in such a way that the closer in time the events in the triad are to each other the faster the third event in the pattern is likely to happen. The function in (11) can be written also in the case of triadic statistics and it will be, A simple example of stepwise eﬀects as to the transitivity closure is shown in Figure 5: if we only consider the transitivity closure in the model we can state that the more the events characterizing the triad occurred in the recent times the more the third event in the triadic pattern is likely to happen. Formulas of further second order statistics can be found in Appendix A.1. <title>3.3 Estimation of a relational event model with a stepwise memory decay</title> intervals with bounds , . . . , γ . The bounds should be determined such that the stepwise function will be able to capture the expected memory. Thus for periods where a fast (slow) decay is expected narrow (wide) intervals should be chosen. Next, each endogenous statistic (e.g., inertia, transitivity) is split in separate statistics, which capture the volume of past interactions in the diﬀerent intervals of the transpired time. The ﬁnal set of relational event statistics can then be plugged into existing functions for ﬁtting relational event models. Despite the computational advantage, the stepwise memory decay in (11) and in (16) has two potential issues: a substantive issue is that it is unrealistic that memory decay occurs in a stepwise fashion in real life; a methodological issue is that it is unclear how many intervals ) should be chosen and where the boundaries = ( , . . . , γ ) should be placed. Even though we could increase the number of intervals, we would still be constraining results to pre-speciﬁed boundaries and eﬀects estimates would lose accuracy as this would greatly increase the number of free parameters in the model to be estimated and reduce the number of events per interval. <title>4 The continuous nature of memory decay</title> where is now conceived as a continuous function on , describing the trend of the eﬀect of such that D → R and \ {γ > γ , with being a time length limit either due to the empirical data or simply justiﬁed by the researcher. The set of parameters θ ∈ S ) deﬁnes the shape of the decay, where ) is their support. We propose here several monotonous decreasing functions γ, θ ) that might reﬂect the actual underlying memory decay. The continuous trends below assume eﬀects to be positive and decreasing towards zero as the time transpired since the event increases. • linear decrease (Figure 6a): where {θ , θ 0 is the maximum value assumed by the function and (with θ > 0) is the slope of the line which describes the steepness of the decrease; • exponential and one-smooth-step decrease (Figure 6b and Figure 6c): where the set of parameters {θ , θ , θ consists of: 0 and 0 which are scale parameters ( corresponds to the maximum value assumed by the function), Figure 6: Possible evolution over for the relative eﬀect ( ) of the past event constituting the endogenous statistic of interest. In these speciﬁc examples, trends decrease towards zero with diﬀerent shapes depending on a set of parameters : (a) linear, (b) exponential, (c) an initial step with a smoothed decrease, (d) two smoothed and decreasing steps. 0 is a shape parameter. The survival function of a Weibull distribution is a speciﬁc case of the function (19) where the maximum value is = 1. Moreover, where = 1, , the (19) reduces to the exponential decreasing weight in Brandes et al., 2009) and the halﬂife parameter is then calculated as log 2. The trend in most of the cases (except for the exponential one) starts evolving at an initial constant value (one-smooth-step trend) that is the maximum value and then decreases to zero as γ increases; smoothed multiple steps (Figure 6d): it is a combination of two or more smoothed one-step trends. The relative inﬂuence of past events on the dyadic event rate can follow other more complex shapes than those presented in Figure 6. As a result of this continuous deﬁnition of eﬀects, inertia as well as other endogenous statistics are no longer computed as the accumulated number of past events but now consist of a sum of weights, where each weight changes according to the transpired time of each event; this reﬂects the relative importance of past events updated at . Therefore, the event rate in (10) where only inertia eﬀect is considered and inertia is divided in K intervals becomes: Figure 7: Examples of approximation of three diﬀerent decays (red lines) by means of three types of stepwise functions (black lines). From the top to bottom: exponential decay, one-smooth-step decay and linear decay. From left to right: increasing sizes, decreasing sizes and equal sizes intervals. The maximum time width is γ = 7.5. where , θ ) is a continuous function that returns the relative eﬀect as to the event contributing to the inertia statistics, ) = − t is the time transpired at since and it increases over time, is the set of parameters that describe the shape of the decay. A formal mathematical procedure about moving from a stepwise eﬀect function to a continuous eﬀect function can be found in the Appendix A.2. scenario that the inﬂuence of past events changes as a continuous function of their elapsed time since the current time, comes at the expense of constantly changing values of the network statistics, which increases the complexity of their estimation. By having stated and presented the continuous nature of the decay we also acknowledged the need of a less burdensome method to estimate parameters that govern memory. With such purpose, in the next subsection the use of a stepwise approach is revalued and presented in a Bayesian approach by which the posterior memory decay of eﬀects is estimated. <title>5 A semi-parametric approach to estimate a smooth memory decay</title> We introduce here a statistical approach based on the Bayesian Model Averaging (BMA) Volinsky et al., 1999) which: (i) takes the computational advantage of the stepwise model introduced in Section (3.1), (ii) avoids the issue of arbitrarily choosing intervals and (iii) results in a semi-continuous estimate of memory decay. <title>5.1 Deﬁning a bag of stepwise relational event models</title> The stepwise model we deﬁned in the previous section pays oﬀ in the approximation of the memory decay, since the estimated eﬀect represents the most likely relative eﬀect that is assumed to be constant within the k-th interval and given the sequence of . The level of approximation may depend both on the size and on the number of intervals. Therefore, a trade oﬀ between these two aspects can often result in a potentially satisfying approximation of the decay. If on one side the stepwise memory decay appears to be the least suitable model, on the other side we can take advantage of the approximation obtained by a set of diﬀerent stepwise models and perform a Bayesian model averaging over the posterior trend of the eﬀects. Such semi-parametric approach is introduced in the next section. At this stage, is necessary to describe a method for generating the sequences of time widths for each stepwise model that takes part in the ﬁnal averaging. The sequences of time widths may be generated according to three features reﬂecting three possible changes of the decay over time (a few examples are shown in Figure 7): (i) when memory change is likely to be stronger for the more recent events and to change less for events that already are in the farther past (where it is approximately constant) (e.g. exponential decay), then intervals with increasing sizes will better catch this behavior and their widths will follow the inequality: − γ < γ − γ for = 1 , . . . , K − 1. In other words, here memory is short such that events are "forgotten" fairly fast and the most recent events carry a much higher weight than somewhat less recent events, and fairly distant events have as little eﬀect on the future as events from the far past. events from the past week matter, but anything beyond that is quickly forgotten). (iii) if the decay is decreasing with a constant pace (e.g. linear decreasing function), intervals with the same size will better emulate this behavior. Increasing sizes intervals (i) are generated by means of an algorithm based on the Dirichlet distribution and its pseudocode can be found in Appendix A.3. Decreasing sizes intervals (ii) are generated by ﬁrst drawing random intervals of the type (i) and then inverting the order of the widths. <title>5.2 Bayesian model averaging for approximating smooth decay functions</title> We can take advantage of the information provided by a set of stepwise models where each model , with = 1 , . . . , Q , will be based on predeﬁned intervals of the elapsed time whose number and time lengths are diﬀerent across models and are respectively noted as and = ( , γ , . . . , γ ) with = 1 , . . . , Q . By means of BMA one can elicit a posterior estimate of a quantity of interest as well as its average posterior predictive distribution by ﬁnding the optimal linear combination of a set of models, and accounting, in turn, for their uncertainty. A crucial aspect in BMA is the use of model weights which quantify the relative importance of models according to their posterior probability. The challenging aspect here is that the true model , which may have a smooth shape for memory decay, is not a stepwise model. In the literature this scenario is called an M-open problem Yao, Vehtari, Simpson, & Gelman, 2018; Bernardo & Smith, 2000). The idea however is that the true smooth model can be found by using a linear combination of stepwise models using appropriate weights using Bayesian model averaging, where represents the weight of the q-th stepwise model. Weighting systems for the posterior probabilities of stepwise models can be calculated using a measure that quantiﬁes the goodness of ﬁt (via the likelihood) or the goodness of prediction (via the prediction of future observations based on past observations). The former type comprises all of those measures that are functions of some Information Criterion, such as BIC (Bayesian Information Criterion). The latter refers to those measures which quantify predictive performance of the models, such as ELPD (Expected Log-pointwise Predictive Density). Therefore, in light of this distinction, we considered two weighting systems which can be employed in the estimation of the posterior trend: one that is the most common choice in BMA (and is based on the BIC) and one that is speciﬁcally recommended for M-open problems in other types of model selection problems (and is based on an approximation of the ELPD Watanabe, 2013; Vehtari, Gelman, & Gabry, 2017; Yao et al., 2018)). Thereby we will explore which type of weighting system can best be used for this challenging model averaging problem. In BMA, the posterior estimate of the parameter of interest can be calculated as the weighted mean of the posterior estimates provided by each model in the averaging, model. In order to approximate the posterior distribution of over we ﬁrst need to ﬁnd the posterior distribution of (without conditioning on any ) and each draw is obtained with the three steps below: 1. Draw a value from q ∼ Multinomial ). This step consists in randomly choosing one of the models according to the normalized vector of weights (w , . . . , w (weighting systems are discussed in 5.3.1 and 5.3.2); 2. Generate a vector of posterior eﬀects from β|M , E ∼ M V N ). The posterior distribution for the stepwise model (the model drawn at the ﬁrst step) is approximated by a multivariate normal distribution with parameters given by maximum likelihood estimates of model q; 3. Repeat steps 1 and 2 a suﬃcient number of times. After these three steps, the resulting posterior distribution of over looks like the one in Figure 8a. Afterwards, the posterior decay of the eﬀect over is estimated as follows: (i) a (dense) grid is deﬁned with evenly spaced γ ∈ [0 , γ ], where is usually based on the data (Figure 8b); (ii) for each the corresponding interval eﬀect in each posterior draw is selected (as it is shown by stepwise functions in (11) and (16)), therefore this selection will result in a posterior density at a given (Figure 8c); (iii) the posterior mode of these densities is calculated at each , resulting in a semi-continuous eﬀect decay (Figure 8d). As a consequence of this, the posterior estimate of those statistics that are not deﬁned in intervals (e.g, baseline eﬀect) is simply obtained with the draws generated after the three initial steps. <title>5.3 Weighting systems for Bayesian model averaging</title> When performing BMA, each model in the set is considered as a generative model and it is weighted according to its posterior probability. The marginal likelihood |M ) is sensitive to the prior assumption |M ) of each model where, for instance, parameters β can be assumed mutually independent. The marginal likelihoods of models can be approximated by means of a function of the Bayesian Information Criterion (BIC). The formula to calculate the unnormalized weight of a model in the set is, Figure 8: (a) result from repeating step 1. and 2. in the approximation of the posterior distribution of over by means of stepwise models; (b) selecting a grid of ’s (vertical dashed lines); (c) posterior conditional densities at given ’s; (d) example of the posterior trend of over a denser grid of ’s (the grey region represents the highest posterior density interval at 95%). stepwise model were randomly generated. Moreover, the marginal likelihood |M ) is approximated (with error (1)) by a function of the BIC of the q-th model S. & Kitagawa, 2008). Thus, the normalized BIC weight for the q-th model will be, exp {−BIC /2} (25) exp {−BIC /2} Resulting weights are based on a ﬁtting measure and the model that best ﬁts the data will have the highest inﬂuence in estimating posterior distribution of interest, resulting in a poor approximation of the posterior trend ). Furthermore, in the M-open case the use of posterior model probabilities is no longer suitable since the true model is out of the set of models. where: • M is the number of observed relational events in the sequence; • L is a predeﬁned minimum number of relational events from the ordered sequence that will be required to make predictions for future events in the WAIC; • p , . . . , e |E , β ) is the posterior predictive density of the model for A steps ahead predictions, that are future events occurring after the i-th event and given the past history ( ) of events until , with L, . . . , M − A ); is the b-th vector of posterior draws for the q-th stepwise model and it is generated from a multivariate normal distribution ∼ M V N ) with parameters equal to the maximum likelihood estimates obtained by estimating the model over the whole sequence of events; <title>6 Case study: investigating the presence of memory decay in the sequence of demands sent among Indian socio-political actors</title> We have now introduced our modeling approach, starting from a purely stepwise decay model to a continuous decay model based on model averaging of a set of stepwise models. In this section we illustrate the method by applying it to empirical data to provide insights about memory decay in contiuous time in a real-life social network. First, the empirical application and data are described. Next, the analyses are presented using diﬀerent prespeciﬁed step-wise decay functions, followed by an application of the Bayesian model averaging estimated to obtain approximate smooth decay functions. <title>6.1 Relational events between socio-political actors</title> Data are retrieved from the ICEWS (Integrated Crisis Early Warning System) Boschee et al., 2015), which are available in the Harvard Dataverse repository. In ICEWS, relational events consist of interactions between socio-political actors which were extracted from news articles. Information as to the source actor, target actor and event type of the relational event are recorded along with geographical and temporal data that are available within the same article. Furthermore, event types are coded according to the CAMEO (Conﬂict and Mediation Event Observations) ontology. In this analysis, we focus on the sequence of relational events within the country of India. Each event represented a request generated from an actor and targeted to another actor. Such requests range from humanitarian ones to military or economic ones and in this analysis this distinction isn’t made. The speciﬁc relational event sequence used in our analysis is available upon request. The event sequence includes M = 7567 dyadic events between June 2012 and April 2020 among the ten most active actor types: citizens, government, police, member of the Judiciary, India, Indian National Congress Party, Bharatiya Janata Party, ministry, education sector, and "other authorities". Since the time variable is recorded at a daily level, those events that occurred on the same day are considered evenly spaced throughout a day. (with |R| = N(N − 1) = 90, i.e. all the possible dyads are included in the risk set) is: log λ(s , r , E , β) = β inertia (s , r , t) + reciprocity (s , r , t)+ <title>6.2 Predeﬁned stepwise decay models</title> As the maximum time ( ) that past events may aﬀect current relational events we consider 180 days (roughly half an year). Furthermore three diﬀerent predeﬁned stepwise memory decay functions are considered by dividing the past in = 4 intervals with either increasing widths, equal widths, or decreasing widths, as described in Section 5.1. Figure 9 shows the estimated stepwise decay functions for inertia, reciprocity, and transitivity given the three diﬀerent interval conﬁgurations. As expected all three models result in diﬀerent estimated (discretized) shapes of memory decay. For instance, by comparing the estimates of the Transitivity Closure, we see that decreasing intervals and increasing intervals produces two contrasting decays where, not only both decays follow diﬀerent shapes, but also the magnitude of the eﬀect lies on diﬀerent levels, lower for the decreasing intervals. One more comparison over the eﬀect magnitude can be made between equal and decreasing intervals where trends evolve around the same magnitude, whereas for increasing intervals the maximum for the magnitude results to be more than twofold (Inertia and Reciprocity) if not tenfold (Transitivity Closure) the equal and the decreasing type. This discrepancy is not only given by the type of the intervals but is also generated by the time widths that characterize the stepwise models. In sum, stepwise models having predeﬁned interval conﬁgurations provide us with a very rough idea how fast memory decays in a given relational event network. On the other hand, predeﬁned step-wise memory decay models do not provide insights about the shape of memory decay over the transpired time, or, for example, whether an approximated exponential decay is more likely than a approximated smooth one-step decrease. To learn this from an observed relational event network, we need the proposed weighting system for a bag of step-wise models together with a Bayesian model averaging approach. <title>6.3 Approximately smooth memory decay models</title> Figure 9: MLE estimates (by column) of three stepwise models (with = 4), that are randomly chosen from the bag of the estimated models and each following one of the three interval types (increasing, equal and decreasing). The bold black line represents the stepwise function for each endogenous eﬀect in the model and the vertical dashed lines indicate the time bounds characterizing the intervals. Therefore, each model follows diﬀerent time widths. Figure 10: Posterior estimates resulting from the BMA with BIC (left) and WAIC (right) weights: posterior distribution for the intercept ( ), posterior trends for inertia, reciprocity and transitivity closure. The gray area (dashed lines for the intercept) is generated by the posterior highest density intervals calculated until 20 days. exp ≈ 0.0129 (similar for both BIC and WAIC weights; upper panels). Focusing on the results using the WAIC weights in Figure 10, all the three trends show a clear approximately exponential memory decay (right panels). This drastic decrease near zero suggests that the most recent requests have a much higher impact on the event rate than the less recent ones. Therefore, the trend observed for inertia suggests the presence of a persistence in actors that tend to keep sending requests to the same recipient of their more recent requests, showing inertia based on the requests that happened in a fairly recent past, rather than inertia based on requests over a longer period of time. For reciprocity, we see that the eﬀect drops a bit faster than inertia and stabilizes around a low value that decreases further, indicating that actors reciprocate on requests received in the very recent past, but requests that were not responded to quickly are soon forgotten and are unlikely to be followed up. Norms of reciprocity are clearly not strong and non-reciprocated requests disappear from social memory very quickly. Finally, transitivity eﬀect vanishes similarly fast. This can be explained as follows. Considering that dyadic requests only brieﬂy trigger the tendency to respond, it makes sense that common communication partners also have only very short-lived inﬂuence on future interaction between two parties. The resulting trends obtained by using the BIC weights approximately follow the same decays as found with WAIC. However, we see an approximate stepwise trend using BIC weights. This can be explained by the fact that the BIC becomes increasingly large for the stepwise model that is closest to the true (smooth) model (in terms of Kullback-Leibler distance Grünwald & van Ommen, 2017)). Thus the weight of the stepwise model that is closest to the true smooth model dominates over the weights of all other stepwise models. This illustrates that the BIC is useful for ﬁnding the best ﬁtting stepwise model, which, in this case, has increasing interval widths over the transpired time having a roughly exponential decay. On the other hand, the BIC is not useful for ﬁnding an approximate smooth decay trend. For this purpose the WAIC is recommended. <title>7 Discussion</title> In this paper we presented diﬀerent methods for learning how past interactions between social actors aﬀect future interactions in the network. First a -stepwise model was considered where memory decay about past interactions was approximated using a discrete stepwise trend. This model can be estimated using existing software functions for relational event analysis. The proposed Bayesian model averaged memory decay estimator will be made available in a new R package in the coming month. network dynamics (and, in turn, on the event rate) according to their recency. The promising aspect of this semi-parametric approach lies on its ability to learn the shape of the memory decay without making any parametric assumption about it. Furthermore by building on the stepwise model, the proposed method is computationally feasible. Two diﬀerent weighting systems were considered for Bayesian model averaging of a bag of stepwise models: The BIC and the WAIC. As was illustrated the BIC is useful for ﬁnding the best ﬁtting stepwise model for a given empirical relational event history. The BIC however is not suitable for ﬁnding an approximate smooth trend of memory decay, as all the weights are placed on the single stepwise model that is closest to the true smooth decay model. This issue is not present when using the WAIC as the Bayesian model average of many stepwise models results in a smooth trend of the decay of the actors’ memory about past events in social networks. Two advancements of the method will consist in extending it to diﬀerent event types (sentiments) and to other endogenous statistics that measure speciﬁc network dynamics. The former will be helpful for the understanding of the diﬀerences in memory decays given diﬀerent sentiments characterizing the interactions. For instance, one expects negative events (e.g., a country threatening another country, a pupil insulting a peer, a teacher rebuking a student) to have a memory decay that is slower and more persistent than the one observed for positive events (e.g., a teacher praising a student, a country cooperating with another country). This diﬀerence may apply as well to other event type settings where possible diﬀerent memory shapes can emerge. The other advancement will consist in ﬁrst formulating new endogenous statistics that measure speciﬁc behavioral patterns in the network, then in estimating and interpreting the decay of their eﬀect. Finally, future investigations could also focus on formulating and estimating key characteristics that describe the memory decay as well as on testing diﬀerences of such features across either sentiments of events or groups of actors. We expect that the acquired ability of both estimating and testing on parameters that describe this memory process is a crucial step towards a more accurate understanding of network dynamics developing at a local as well as at a global level. <title>Author biographies</title> Giuseppe Arena graduated from the University of Padova with a Master degree in Statistics. He is now pursuing a PhD at the Department of Methodology and Statistics at Tilburg University. His main research interests are social network analysis, Bayesian inference and analysis of memory retention process in relational event data. Joris Mulder is an associate professor in the Department of Methodology and Statistics at Tilburg University. He holds a PhD in applied Bayesian statistics from Utrecht University. His research focuses on Bayesian model selection and social network modeling. Roger Th. A. J. Leenders is a professor at the Jheronimus Academy of Data Science and in the Department of Organization Studies at Tilburg University. He holds a PhD in sociology from the University of Groningen. He has published broadly on social network analysis, teams, innovation, and organization behavior in leading journals such as Organization Science, the Journal of Applied Psychology, the Journal of Product Innovation Management, Social Networks, and the Academy of Management Journal. <title>A Appendix</title> <title>A.1 Endogenous statistics</title> In Table 1 the indicator variable for any event where (s = i, r = j) follows the short notation i, j ) and the same applies to any other dyad. Given each statistic, the formula in the ﬁrst row shows the interval deﬁnition of the statistic as regards dyad ( i, j ) in the k-th interval; whereas, the formula in the second row shows the continuous deﬁnition where γ, θ ) is the trend function that follows one of the decays discussed in Section 4 or another more complex evolution. Note how in the continuous formulas the event history at , that is E − t , doesn’t depend on any interval. <title>A.2 From stepwise to continuous eﬀects</title> Consider an increasing sequence of + 1 time widths = ( , γ , . . . , γ ), such that − γ = ∆ for = 1 , . . . , K (i.e. evenly spaced intervals). A graphical representation of intervals at t is presented in Figure 11. Figure 11: evenly spaced intervals, with time widths = ( , γ , . . . , γ ) such that − γ = ∆ for k = 1, . . . , K. Table 1: First and second order endogenous statistics: the formula in the ﬁrst row is the interval deﬁnition of the statistic, whereas the formula in the second row represents the continuous deﬁnition of the statistic where the function γ, θ ) describes the decay of the eﬀect. events could both occur 33 minutes earlier than the present time point but with the condition that the present time point they refer to is diﬀerent for both of them (because events are assumed not to occur at the same time point). Finally, the estimation of the eﬀects over such a large number of intervals is impractical and it serves only to convey insights about the possibility of continuously changing eﬀects in contrast to stepwise decays. Continuous eﬀects imply a more realistic view of the eﬀect dynamics in which events do not assume a constant eﬀect but this changes with their recency, whereas stepwise eﬀects suppose the less realistic scenario where eﬀects are assumed to be constant within intervals and this also entails that estimates signiﬁcantly depend both on the number and on the size of the intervals. <title>A.3 Interval generator (the algorithm)</title> Algorithm 1: Generating intervals with steps (having either increasing or decreasing sizes). <title>References</title> Brass, D. J., & Labianca, G. (1999). Social capital, social liabilities, and social resources management. In R. T. Leenders & S. M. Gabbay (Eds.), Corporate social capital and liability (pp. 323–338). Springer. Butts, C. (2008). A Relational Event Model for Social Action. Sociological Methodology, 38 (1), 155–20. doi: 10.2307/20451153 Chechile, R. A. (2003). Mathematical tools for hazard function analysis. Journal of Mathematical Psychology. doi: 10.1016/S0022-2496(03)00063-4 Chechile, R. A. (2006). Memory hazard functions: A vehicle for theory development and test. Psychological Review, 113 (1), 31–56. doi: 10.1037/0033-295X.113.1.31 Chechile, R. A. (2009). Corrigendum to: "Mathematical tools for hazard function analysis" [J. Math. Psychol. 47 (2003) 478-494] (DOI:10.1016/S0022-2496(03)00063-4). doi: 10.1016/j.jmp.2009.04.006 Grünwald, P., & van Ommen, T. (2017, 12). Inconsistency of bayesian inference for misspeciﬁed linear models, and a proposal for repairing it. Bayesian Analysis, 12 (4), 1069–1103. Retrieved from https://doi.org/10.1214/17-BA1085 doi: 10.1214/17BA1085 Lawless, J. F. (2002). Statistical models and methods for lifetime data. John Wiley & Sons. doi: 10.1002/9781118033005 Leenders, R. T. A., Contractor, N. S., & DeChurch, L. A. (2016). Once upon a time: Understanding team processes as relational event networks. Organizational Psychology Review, 6 (1), 92–115. doi: 10.1177/2041386615578312 Mulder, J., & Leenders, R. T. A. (2019). Modeling the evolution of interaction behavior in social networks : A dynamic relational event approach for real-time analysis. Chaos, Solitons and Fractals: the interdisciplinary journal of Nonlinear Science, and Nonequilibrium and Complex Phenomena, 119 , 73–85. doi: 10.1016/j.chaos.2018.11.027 Mulder, J. e. a. (2020). remverse [Computer software manual]. Perry, P. O., & Wolfe, P. J. (2013). Point process modelling for directed interaction networks. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 75 (5), 821–849. doi: 10.1111/rssb.12013 Quintane, E., Pattison, P. E., Robins, G. L., & Mol, J. M. (2013). Short- and long-term stability in organizational networks: Temporal structures of project teams. Social Networks, 35 (4), 528–540. doi: 10.1016/j.socnet.2013.07.001 S., K., & Kitagawa, G. (2008). Information Criteria and Statistical Modeling. Springer, New York. doi: https://doi.org/10.1007/978-0-387-71887-3 Stadtfeld, C., & Hollway, J. (2020). goldﬁsh: Goldﬁsh – statistical network models for dynamic network data [Computer software manual]. Retrieved from www.social-networks.ethz.ch/research/goldfish.html (R package version 1.4.8) Vehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27 (5), 1413–1432. doi: 10.1007/s11222-016-9696-4 Volinsky, C. T., Raftery, A. E., Madigan, D., & Hoeting, J. A. (1999). David Draper and E. I. George, and a rejoinder by the authors. Statistical Science, 14 (4), 382–417. doi: 10.1214/ss/1009212519 Watanabe, S. (2013). A widely applicable bayesian information criterion. Journal of Machine Learning Research, 14 (1), 867–897. Yao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using Stacking to Average Bayesian Predictive Distributions (with Discussion). Bayesian Analysis, 13 (3), 917–1007. doi: