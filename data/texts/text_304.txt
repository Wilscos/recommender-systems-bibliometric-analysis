Over the years, next basket recommendation (NBR) has received a considerable amount of interest from the research community [1, 13, 18]. Baskets, or sets of items that are purchased or consumed together, are pervasive in many real-world services, with e-commerce and grocery shopping as prominent examples [5, 12]. Given a sequence of baskets that a user has purchased or consumed in the past, the goal of a NBR system is to generate the next basket of items that the user would like to purchase or consume next. Within a basket, items have no temporal order and are equally important. A key diﬀerence between NBR and session-based or sequential item recommendations is that NBR systems need to deal with multiple simultaneous items in one set. Therefore, models designed for item-based recommendation are not ﬁt for basket-based recommendation, and dedicated NBR methods have been proposed [6, 14]. Numerous deep learning techniques have been developed to address sequential item recommendation problems, building on the well-documented capacity of neural methods to capture hidden relations and automatically learn representations [2]. Against this background, recent years have witnessed many proposals Ming Li, Sami Jullien, Mozhdeh Ariannezhad, and Maarten de Rijke Abstract. The goal of a next basket recommendation system is to recommend items for the next basket for a user, based on the sequence of their prior baskets. Recently, a number of methods with complex modules have been proposed that claim state-of-the-art performance. They rarely look into the predicted basket and just provide intuitive reasons for the observed improvements, e.g., better representation, capturing intentions or relations, etc. We provide a novel angle on the evaluation of next basket recommendation (NBR) methods, centered on the distinction between repetition and exploration: the next basket is typically composed of previously consumed items (i.e., repeat items) and new items (i.e, explore items). We propose a set of metrics that measure the repeat/explore ratio and performance of NBR models. Using these new metrics, we analyze the state-of-the-art NBR models. The results of our analysis help to clarify the extent of the actual progress achieved by existing NBR metho d s as well as the underlying reasons for the improvements. Overall, our work sheds light on the evaluation problem of NBR and provides useful insights into the model design for this task. Keywords: Next basket recommendation · Repetition and exploration · Evaluation. to address diﬀerent aspects of the NBR task with neural methods, e.g., item-toitem relations [10], cross-basket relations [19], noise within a basket [11]. A recent analysis of neural methods for sequential recommendation problems shows that such methods may be outperformed by simple nearest neighbor or graph-based baselines [3]. In this paper, we perform a similar analysis for the next basket recommendation task (instead the generic sequential recommendation problem). We notice that the same factors that account for the phenomena observed by Dacrema et al. [3] also feature in recent research on the next basket recommendation task. This includes weak or missing baselines, the use of diﬀerent datasets in diﬀerent papers, and of non-standard metrics. For example, some methods [1, 10, 11, 17, 18] only compare with previous (deep) learningbased methods and avoid comparing with a simple Personal-TopFreq baseline, which recommends the most k frequent items in the users’ historical records as the next basket. Many recent publications [4, 7, 11, 19] do not compare with each other. And in [11], sampled metrics [8] are used to evaluate the performance even though this is not encouraged [9]. Moreover, previous studies into models for NBR tend to use limited case studies and ablation studies to demonstrate the eﬀectiveness of their proposed methods. They forego a deeper analysis of what items are actually recommended in the next basket. Given the state of aﬀairs sketched above, we believe that it is important to perform a reality check on current NBR methods, in order to ﬁnd out how much progress we have actually made and whether a speciﬁc proposed module is responsible for the observed improvements. We propose a novel repetition and exploration perspective to evaluate next basket recommendation methods. Baskets recommended in a NBR scenario consist of repeat items (items that the user consumed before, in previous baskets) and explore items (items that are new to the user). In order to improve our understanding of the relative performance of NBR models, especially regarding repeat items and explore items, we introduce a set of task-speciﬁc metrics for NBR. Those metrics help us to understand what type of items are present in the recommended basket and assess the performance of models when proposing new items vs. already-purchased items. With the proposed metrics for NBR and a new baseline, GP-Freq, we evaluate the performance of state-of-the-art NBR models on three benchmark datasets, with diﬀerent repeat-explore patterns. We arrive at several important ﬁndings: (1) No NBR method can consistently outperform all other methods across different datasets. (2) All published methods are heavily skewed to either repetition or exploration compared to the ground-truth, which might harm long-term engagement. (3) There is a large performance gap between repetition and exploration; repeat item recommendation is much more easier. (4) Several deep learning-based neural methods are inferior to simple frequency baselines P-TopFreq and GP-TopFreq in many settings. (5) Being biased towards repetition and its improvements in repetition accounts for most of the performance gains of recently published methods, even though many complex modules or strategies speciﬁcally target explore items. (6) GP-TopFreq is a highly competitive baseline across diﬀerent datasets, and comparing to it, observed improvements of several state-of-the-art methods are limited. Overall, our work sheds light on the state-of-the-art of NBR, provides suggestions to revise the metrics of NBR evaluation, helps to understand the current progress and the underlying reasons for improvements, and provides further insights for the design of NBR models. In order to ensure the reproducibility of our study, we conduct our experiments on three publicly available real-world datasets, which vary in the ratio of repetition and exploration: – TaFeng – contains four months of shopping transactions collected from a Chinese grocery store. All products purchased on the same day by the same consumer are treated as a basket. – Dunnhumby – covers two years of household-level transactions at a retailer. All products bought by the same customer in the same transaction are treated as a basket. We use the ﬁrst two months of the data in our experiments. – Instacart – contains over three million grocery orders of Instacart users. We treat all items purchased in the same order as a basket. In each dataset, users with a basket size between three to 50 are sampled to conduct experiments. We also remove rare items. The remainder covers more than 95% of the interactions. The statistics of the processed datasets are summarized in Table 1, where the repetition ratio and exploration ratio are calculated based on the ground truth basket as the proportion of repeat items and explore items in the ground truth basket, respectively. For our experiments, we split every dataset into a training, validation, and test set with a ratio of 72%, 8%, and 20% for every user. We follow the same strategy as [3] to collect reproducible NBR papers. Papers targeting sequential set prediction are also considered to be relevant to NBR. For a fair comparison, we do not include methods that perform quite poorly https://www.kaggle.com/chiranjivdas09/ta-feng-grocery-dataset https://www.dunnhumby.com/source-files/ https://www.kaggle.com/c/instacart-market-basket-analysis/data [13, 15] or leverage additional information like explicit timestamp [16], item attributes [1], etc. Apart from the two widely known frequency-based baselines (global top-k and personal top-k), we propose a simple combination of the two as an additional baseline, i.e., GP-TopFreq. Frequency-based baselines. – G-TopFreq – uses the k most popular items in the dataset to form the recommended next basket. It is widely used in recommendation systems due to its eﬀectiveness and simplicity. – P-TopFreq – a personalized TopFreq method that treats the most frequent k items in the users’ historical records as the next basket. This method only has repeat items in the prediction. – GP-TopFreq – a combination of P-TopFreq and G-TopFreq that ﬁrst uses P-TopFreq to ﬁll the basket, then uses G-TopFeq to ﬁll any remaining slots. Neighbor-based methods. – TIFUKNN: – models the temporal dynamics of frequency information of users’ past baskets to introduce Personalized Frequency Information (PIF), then uses a nearest neighbor-based method on the PIF [7]. – UP-CF@r – a combination of the recency-aware user-wise popularity and user-wise collaborative ﬁltering. The recency of shopping behavior is considered in this method [4]. Deep learning-based methods. – Dream – the ﬁrst neural-based method that models users’ global sequential basket history for NBR. It uses a pooling strategy to generate basket representations, which are then fed into an RNN to learn user representations and predict the corresponding next set of items [18]. – Sets2Sets – uses a pooling operation to get basket embeddings and an attention mechanism to learn a user’s representation from their past interactions. Furthermore, item frequency information is adopted to improve performance [6]. – DNNTSP – leverages a GNN and self-attention techniques. It encodes itemitem relations via a graph and employs a self-attention mechanism to capture temporal dependencies of users’ basket sequence [19]. – Beacon – an RNN-based method that encodes the basket considering the incorporating information on pairwise correlations among items [10]. – CLEA: – an RNN-based method that uses a contrastive learning model to automatically extract items relevant to the target items and generates the representation via a GRU-based encoder [11]. For each method, we use the suggested hyper-parameters in the paper or related GitHub repository. We adopt three conventional metrics: Recall, Normalized Discounted Cumulative Gain (NDCG), and Personalized Hit Ratio (PHR), which are commonly used Table 2: Overall performance comparison of simple baselines and published stateof-the-art methods. Highlights indicate the highest score per basket size and metric. by previous NBR studies for evaluation. Recall measures the ability to ﬁnd all relevant elements; NDCG is a ranking quality measurement metric, which takes the order into consideration; PHR focuses on user-level performance and calculates the ratio of predictions that capture at least one item in the ground truth basket. F 1@k and P recision@k are also employed as evaluation metrics in recent work. However, we argue that these two metrics are not necessary when Recall@k is used. In the NBR setting, we usually recommend a basket (i.e., positive instances) with a ﬁxed size k to the user, which means that P recision@k and F 1@k are proportional to Recall@k for each user. We will therefore not use F 1@k and P recision@k. The performance results for the conventional metrics are shown in Table 2. First, the performance of diﬀerent methods varies across datasets and there is no method that unanimously outperforms all other methods, independent of datasets and basket size. This calls for a further analysis of the factors impacting performance, which we conduct in the next section. G-TopFreq 0.0803 0.0842 0.2489 0.0987 0.1054 0.4624 0.0721 0.0820 0.4543 P-TopFreq 0.1072 0.0959 0.3487 0.2319 0.2342 0.6569 0.3264 0.3381 0.8437 GP-TopFreq 0.1215 0.1019 0.3706 0.2356 0.2360 0.6660 0.3273 0.3387 0.8451 TIFUKNN 0.1259 0.1020 0.3871 0.2398 0.2411 0.6774 0.3608 0.3726 0.8640 G-TopFreq 0.1071 0.0937 0.3284 0.1267 0.1195 0.5315 0.1001 0.0970 0.5254 P-TopFreq 0.1403 0.1103 0.4349 0.3031 0.2697 0.7183 0.4306 0.3940 0.8944 GP-TopFreq 0.1703 0.1215 0.4854 0.3146 0.2747 0.7387 0.4353 0.3962 0.8999 TIFUKNN 0.1828 0.1237 0.5063 0.3201 0.2804 0.7427 0.4708 0.4324 0.9090 Next, mong simple baselines, P-TopFreq outperforms G-TopFreq in all scenarios, which suggests that personalization improves the recommendation performance. P-TopFreq could only recommend items that appeared in historical sequences and some users only have interacted with items fewer than the basket size. P-TopFreq is a competitive repetition baseline, as it only contains repeat items in the prediction. It is worth noting that the number of repetition candidates of the user may be smaller than the basket size, which means there might be empty slots in the recommended basket of P-TopFreq. In contrast, GP-TopFreq could take full use of given basket slots by ﬁlling the empty slots via top-k items derived by G-TopFreq. GP-TopFreq beat P-TopFreq with no surprise, and the gain shrinks as the repeat ratio of the dataset increases. It is also worth noting that all recent methods can fulﬁll the given basket. For a fair comparison, we believe the proposed GP-TopFreq should be the basic baseline that every NBR method should compare with, especially in a high exploration scenario. As to the neighbor-based methods, we can see that TIFUKNN and UPCF@r have similar performance across diﬀerent scenarios and outperform all simple frequency-based baselines. The two methods are similar in the sense that they both model the temporal information, combined with a user-based nearest neighbor method. Their performance in high exploration scenario is lower than several learning-based methods, but when it comes to Dunnhumby and Instacart datasets with relatively low exploration ratio, they are among the best performing methods. We observe that most of the training-based deep learning methods outperform G-TopFreq, which is the only simple frequency-based baseline in many papers. Surprisingly, P-TopFreq and GP-TopFreq also achieve a highly competitive performance and outperform four deep learning methods (i.e., Dream, Beacon, CLEA and Sets2sets), by a large margin in the Dunnhumby and Instacart datasets, e.g. the improvement w.r.t. Recall@10 is from 35.8% to 141.9% and from 53.6% to 353.3% respectively. Moreover, the proposed GP-Topfreq baseline could even outperform the deep-learning based Beacon, Dream and CLEA algorithm in Tafeng, the scenario with a high exploration ratio. Among these methods, DNNTSP is the only one to have a consistently high performance in all scenarios. Conventional metrics only provide a general idea of the performance of the methods. In order to understand the underlying reasons of the performance, we dive deeper into the basket components from a repetition and exploration perspective. In order to evaluate the quality of the recommended basket, we propose several novel metrics with regard to the repetition and exploration in a basket. An item is considered to be a repeat item e the user’s historical basket sequence, that is e Otherwise, the item is an explore item, denoted e basket B items, that is, P worth noting that the basket can consist only of explore or repeat items. To understand the elements in the predicted basket, we propose the repetition ratio RepR and the exploration ratio ExplR to measure the tendency of the recommendation. The overall RepR and ExplR are calculated over all test users as: where N denotes the number of test users, K is the size of the model’s predicted basket for user u exploration items e Furthermore, we pay attention to the basket’s ability to fulﬁll the users’ need for repetition and exploration, and we propose the following metrics to evaluate the performance. Recall w.r.t. the repetition performance. Similarly, we use Recall assess the exploration performance. These metrics are calculated as follows: Recall where N repeat elements and exploration elements respectively; ϕ(P, T ) returns 1 when P ∩ T 6= ∅, otherwise returns 0. We analyze the components of the recommended basket for each method to understand what forms up the recommendation. The results are shown in Figure 1. First, we can see that all the recommended baskets are heavily skewed towards either item repetition or exploration, relative to the ground-truth baskets that are much more balanced between already seen and new items. This means none of the existing published methods can properly balance the repeat items and explore items of users’ future interests, and might have negative eﬀects on the long-term engagement of user. According to the basket components, we can divide the existing methods into repeat-biased methods (i.e. P-TopFreq, Gp-TopFreq, Sets2sets, DNNTSP, UPCF@r, and TIFUKNN) and explore-biased methods (i.e. G-TopFreq, Dream, Beacon, and CLEA) and a large gap exist between the two types. Note that TIFUKNN and UP-CF@r both have hyper-parameters which could implicitly inﬂuence the tendency of recommendation, here we show the results of the hyperparameters that account for the best overall performance. Among explore-biased is also denoted as P, which comprises of repeat items and explore and Ndenote the number of users whose ground truth basket contains Fig. 1: The repetition and the exploration ratio of basket components; solid line indicates the repetition ratio of the ground-truth; dashed line indicates the upper bound of the repetition ratio for the basket size 10; dotted line indicates the upper bound of the repetition ratio for the basket size 20. methods, G-TopFreq is not a personalized method and always provides the popular items of the scenario. Dream, Beacon, and CLEA treat all items without any discrimination, which means the exploration items are more likely to be in the predicted basket and their basket components are similar to G-TopFreq. Together with the performance in Table 2, we can see that repeat-biased methods could generally perform much better than explore-biased methods on conventional metrics across the datasets, especially when the dataset has a relatively high repeat ratio. It is worth noting that the repeat ratio of P-TopFreq and GP-TopFreq serves as the upper bound repeat ratio of the recommended basket, and most recommended basket of the repeat-biased method are close to or reach this upper bound, even when the datasets have quite a low ratio of repeat behavior in ground-truth, except for two cases (Sets2sets and DNNTSP on the Tafeng dataset). Apart from that, the exploration ratio of repeat-biased methods increases from basket size 10 to 20, however, we believe this is because there are Table 3: Repetition and exploration performance comparison of simple baselines and published state-of-the-art methods. Highlights indicate the highest score per basket size and metric. no extra repeat items available, and does not mean that the model actively increases the explore ratio in a larger basket setting. The results w.r.t. repetition and exploration performance are shown in Table 3. First of all, using our newly proposed metrics, we observe that the repetition performance Recall when the explore items form almost 90% of the recommended basket. This shows that the repetition task (recommending repeat items) and the exploration task (recommending explore items) have diﬀerent levels of diﬃculty and capturing users’ repeat behavior is much easier. Second, three recurrent neural network (RNN)-embedding based methods perform worst w.r.t. the repeat item recommendation and best w.r.t. the explore item recommendation, as they are heavily skewed towards explore items. Moreover, the pooling strategy to get basket embeddings leads to information loss, which also limits performance on the repetition task. Also, we can see that there are indeed improvements in the exploration performance compared to GTopFreq with the same level of exploration ratio, which indicates that the representation learned by these methods does capture the hidden sequential transition relationship between items. Repeat-biased methods perform better w.r.t. repetition in all settings, since the baskets they predict contain more repeat items. Similarly, we can see that DNNTSP, UP-CF@r, and TIFUKNN perform better than P-TopFreq w.r.t. the repeat performance with the same or a lower level of repetition ratio. Third, explore-biased methods spend more resources on the more diﬃcult and uncertain task of explore item prediction, which is not an ideal option when considering the general performance. On the other side, being biased towards the easier task of repeat item prediction leads to performance gains in overall performance, which is positively correlated with the repeat ratio of the dataset. Even though a clear improvement w.r.t. either repeat or explore performance is observed in the previous section, it does not mean that is the reason for the better overall performance, since repeat and explore items account for diﬀerent ground-truth proportions in diﬀerent datasets. To better understand where the performance gains of the well-performing methods in Table 2 come from, we remove repeat items or explore items in the predicted basket and compare the performance with the original basket, respectively. Experimental results on three datasets are shown in Figure 2. We consider G-TopFreq, P-TopFreq, and GPTopFreq as simple baselines to compare with. From Figure 2, we conclude that Dream and Beacon perform better than G-TopFreq on the TaFeng dataset, as the main performance gain is from improvements in exploration prediction. As a consequence, in the Dunnhumby and Instcart datasets, Dream, Beacon, and G-TopFreq achieve similar performance, and the repeat prediction contributes the most to the performance, even when their recommended items are heavily skewed towards the explore items. Also, we observe that CLEA outperforms other explore-biased methods due to the improvements in the repetition performance without sacriﬁcing the exploration performance. At the same time, it is clear that TIFUKNN, UP-CF@r, Sets2Sets, and DNNTSP outperform explore-biased methods because of the improvements in the repetition performance, even at the detriment of exploration. The repeat items form up the majority of their correct recommendation. Speciﬁcally, the repeat recommendation contributes to over 97% of their overall performance in Dunnhumby and Instacart dataset. An interesting comparison is between Sets2Sets and P-TopFreq, the strong performance gain of Sets2sets on the TaFeng dataset is mainly due to the exploration part, whereas P-TopFreq outperforms it by a large margin on the other two datasets at the same level of repeat ratio, even though the personal frequency information is considered in the Sets2sets model. Apart from the information loss when embedding, we believe this indicates another issue of training-based method, which is that the repetition loss seems to be suppressed by the exploration loss during the training process, which weakens the inﬂuence of the frequency information. Recall that the number of repetition candidates of the user may be smaller than the basket size, which means that there might be empty slots in the recommended basket. From Figure 1, we observe that the empty slots account for Fig. 2: Performance contribution from repeat and explore recommendation of simple baselines and published state-of-the-art methods. a signiﬁcant proportion of exploration slots in many settings. However, existing studies omit this fact when making the comparison with P-TopFreq, leading to an unfair comparison and overestimation of the improvement, as their predictions leverage more slots. For example, Dream, Beacon, and CLEA can beat P-TopFreq, but they are inferior to the GP-TopFreq. TIFUKNN and UP-CF@r model the temporal order of the frequency information, leading to a higher repetition performance than P-TopFreq in general. We can see that even though the contribution of the repetition performance improvement is obvious on the Instacart dataset, it is less meaningful on the other two datasets, where the performance gain is mainly from the exploration part via ﬁlling the empty slots. When compared with the proposed GP-TopFreq on Tafeng and Dunnhumby, the improvement is around 3%. DNNTSP is always among the best-performing methods across three datasets and is able to model exploration more eﬀectively than other repeat-biased methods. What’s more, it could also actively recommend novel items, rather than being totally biased towards the repeat recommendation in high exploration scenarios. However, the improvement is limited due to the relatively high repeat ratios and the huge diﬃculty gap between repetition and exploration tasks. Speciﬁcally, compared with GP-TopFreq, the improvement of DNNTSP w.r.t Recall@10 on Dunnhumby and Instacart is merely 1.3% and 1.9% respectively, which is not promising considering the complexity and the training time added by DNNTSP. Obviously, even though many advanced NBR algorithms learn rich user and/or items representations, the main performance gains stem from the prediction of repeat behavior. Yet, limited progress has been made compared to the simple P-TopFreq and proposed GP-TopFreq baseline methods. From the analysis of our proposed new metrics and the new baselines, we observe that there is a clear diﬃculty gap and trade-oﬀ between the repetition task and the exploration task. Apart from the conventional metrics, the metrics w.r.t repetition and exploration in our paper should be considered as a set of fundamental metrics to understand the performance of NBR methods. As a rule of thumb, being biased towards the easier repetition task is an important hidden trick favoring the overall NBR performance. However, several training-based methods can not eﬀectively model the repetition behavior due to the embedding information loss or the optimization goal, and they are not able to outperform the simple frequency baseline GP-TopFreq, since they go in the opposite direction. Even though several recent state-of-the-art methods are able to trigger this trick and are skewed towards repetition task unintentionally, e.g. self-links in GNN [19], searching hyper-parameters [4, 7], none of these methods acknowledge this strategy in advance and the improvement is rather limited compared to the complexity and resources introduced by these methods. Speciﬁcally, the NBR task boils down to the repetition recommendation task in some cases. The last layer before the ﬁnal prediction is often regarded as the “representation” in many training-based neural methods. Neural models claim they learn “better representation” via considering and modeling several factors. These representations are claimed to be better than previous neural methods considering item importance [10], attention [6], noise within the basket [11], cross basket relations [19], etc. Hu et al. [7] argue that RNNs can not capture frequency information, however, these are rather weak or vague conclusions and none of them identify the underlying reasons for the improvement. We argue that without distinguishing between the diﬀerent components in the baskets and understanding the trade-oﬀ between the repeat and the explore items in the recommendation, more neural models with fancy modules will be proposed and claim better representation; while in fact, they might just trigger some hidden tricks which can be easily drawn from some basic analysis. Even though we are focusing on next basket recommendation, it would also be interesting to contrast the outcomes with an analysis of repetition and exploration behavior in traditional sequential recommendation. [1] Bai T, Nie JY, Zhao WX, Zhu Y, Du P, Wen JR (2018) An attribute- [2] Batmaz Z, Yurekli A, Bilge A, Kaleli C (2019) A review on deep learning [3] Dacrema MF, Cremonesi P, Jannach D (2019) Are we really making much [4] Faggioli G, Polato M, Aiolli F (2020) Recency aware collaborative ﬁltering [5] Hao MC, Hsu M, Dayal U, Wei SF, Sprenger T, Holenstein T (2001) Market [6] Hu H, He X (2019) Sets2sets: Learning from sequential sets with neural net- [7] Hu H, He X, Gao J, Zhang ZL (2020) Modeling personalized item frequency [8] Kang WC, McAuley J (2018) Self-attentive sequential recommendation. In: [9] Krichene W, Rendle S (2020) On sampled metrics for item recommenda- [10] Le DT, Lauw HW, Fang Y (2019) Correlation-sensitive next-basket recom- [11] Qin Y, Wang P, Li C (2021) The world is binary: Contrastive learning for [12] Raeder T, Chawla NV (2011) Market basket analysis with networks. Social aware neural attentive model for next basket recommendation. In: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pp 1201–1204 for recommender systems: Challenges and remedies. Artiﬁcial Intelligence Review 52:1–37 progress? A worrying analysis of recent neural recommendation approaches. In: Proceedings of the 13th ACM Conference on Recommender Systems, pp 101–109 for next basket recommendation. In: Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization, pp 80–87 basket analysis visualization on a spherical surface. In: Visual Data Exploration and Analysis VIII, International Society for Optics and Photonics, SPIE, vol 4302, pp 227 – 233 works. In: Proceedings of the 25th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, pp 1491–1499 information for next-basket recommendation. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 1071–1080 2018 IEEE International Conference on Data Mining (ICDM), IEEE, pp 197–206 tion. In: Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’20), ACM, New York, NY, USA, KDD ’20, p 1748–1757 mendation. In: Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19) denoising next basket recommendation. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 859–868 Network Analysis and Mining 1(2):97–113 [13] Rendle S, Freudenthaler C, Schmidt-Thieme L (2010) Factorizing personal- [14] Sun L, Bai Y, Du B, Liu C, Xiong H, Lv W (2020) Dual sequential network [15] Wang P, Guo J, Lan Y, Xu J, Wan S, Cheng X (2015) Learning hierarchical [16] Wang P, Zhang Y, Niu S, Guo J (2019) Modeling temporal dynamics of [17] Wang S, Hu L, Wang Y, Sheng QZ, Orgun M, Cao L (2020) Intention nets: [18] Yu F, Liu Q, Wu S, Wang L, Tan T (2016) A dynamic recurrent model [19] Yu L, Sun L, Du B, Liu C, Xiong H, Lv W (2020) Predicting temporal ized markov chains for next-basket recommendation. In: Proceedings of the 19th international conference on World wide web, pp 811–820 for temporal sets prediction. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 1439–1448 representation model for next basket recommendation. In: Proceedings of the 38th International ACM SIGIR conference on Research and Development in Information Retrieval, pp 403–412 users’ purchase behaviors for next basket prediction. Journal of Computer Science and Technology 34(6):1230–1240 Psychology-inspired user choice behavior modeling for next-basket prediction. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 34, pp 6259–6266 for next basket recommendation. In: Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pp 729–732 sets with deep neural networks. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp 1083– 1091