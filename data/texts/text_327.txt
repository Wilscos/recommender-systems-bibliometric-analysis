Recommender systems research is concerned with many aspects of recommender system behavior and eects than simply its eectiveness, and simulation can be a powerful tool for uncovering these eects. In this brief position paper, I identify specic types of research that simulation is uniquely well-suited to address along with a hierarchy of simulation types. Additional Key Words and Phrases: simulation, synthetic data, research methods 1 INTRODUCTION My research agenda is particularly concerned with understanding the and recommender systems, and quantifying their impact on the system’s operation, individual and social human experience, and our metrics for quantifying operation, behavior, and experience. This agenda requires signicant use of simulation for a variety of reasons, most stemming from the need to study counterfactuals. We can train a recommender system and measure its behavior on a data set, or on multiple dierent data sets that dier in key ways, but with such “found data” we do not have the ability to isolate specic phenomena and train on data that dier only in the way we want to study (e.g. the degree of popularity bias or preferential attachment users exhibit, or discriminatory biases in how users select and engage with content). Simulation and synthetic data each, in dierent ways, give us a powerful tool for isolating these phenomena and quantifying their (expected) impact. They allow us to study how the system would behave under alternative conditions, both to better understand the conditions of our present world and to anticipate the eects of social change (whether that change is endogenous or is cultivated through policy and/or education). This work is not easy, as the simulation must be credible, and it loses a certain documented delity to world it in which recommender systems will actually be deployed, but in exchange it opens the doors to questions that are intractable otherwise. I approach this from the understanding that the goal of scientic research on recommender systems is to understand how our world works, and how recommenders systems work or can be made to work in the world we inhabit. However, there is much we do not know about the relationship of our observations to the world; our observations may, in fact, be compatible with many dierent “true” congurations of the world. One approach, therefore, is to understand system behavior in multiple possible worlds, or hypothetical, worlds, in hopes that our actual world is represented among them and that additional research may narrow down the set that contains our world, and simulation can enable that. This conception also has the benet of providing tools to try to understand how recommendation would work in worlds that ours could transform into, for example if a particular educational eort to combat a kind of human bias were eective. 2 SIMULATION AND DATA Before getting in to applications of simulation, I want to rst lay out my perspective on the space of simulation methods and how dierent methods relate to each other. These methods sit on a spectrum (see Fig. 1), allowing an increasing degree of control over experimental conditions at the expense of decreased connection to observed behavior or interactions. Much recommender systems research, including the vast majority of my own, uses existing recommendation models and evaluate their behavior or eectiveness. These oine experiments can be viewed as a Behavior kind of simulation on their own [6, §2.5], because they use historical data traces to simulate what the system would do — and how users would respond — if it were actually applied to the task. There are many limitations to this approach (Kouki et al. [10]provide one recent example of work exploring these limitations) but it is highly approachable and widely-used. It has the advantage of being actual data collected from an actual system, along with the confounds that introduces [4] and a lack of knowledge about actual user characteristics and preferences behind the noisy observation process that yielded the recorded data. A typical static data set is the result of a particular population interacting with a particular (suite of) recommendation algorithms in one system. While other data sets reect dierent populations, algorithms, domains, and behaviors, they often vary across multiple dimensions simultaneously, making it dicult to isolate which features of the data drive a behavior of interest. Very little research has done this yet, but it seems possible toresample datato create a new data set that is as like an existing one as possible, except for altering the distribution with respect to a particular feature. This can be done, for example, by reweighting data to ensure equal gender representation [e.g.1]. This method increases our control from static data while maintaining a certain delity to the underlying collected data. We can also generatesynthetic datafrom a model that is trained to emulate existing data sets in key ways; this can be fully synthetic data [15] or it can be synthetic attributes to go with existing data [2]. There are a lot of open questions about how to ensure the data generator produces data that is realistic, as well as how to ensure the experimental process is truly studying recommender performance and not just the ability of the recommender to recover the data generator’s parameters, but it gives us full control over the composition of a data set (or a family of data sets), and we can produce multiple data sets that truly dier in only one parameter. Finally, we can run afull simulationthat produces a data set, and then simulates user response and system behavior over repeated training or online learning. This has been adopted for evaluating reinforcement learning agents [14] as well as understanding system dynamics [4], and has signicant potential for helping the community more fully understand how recommender systems work in practice and their response to phenomena of concern. 3 RETROSPECTIVE SIMULATION: STUDYING ASSUMPTIONS One of the major challenges to recommender systems research is the opacity of the data generating process and the various assumptions we must make about the relationship between observed data (whether in an oine, static data set or collected online) and underlying user preferences and needs. Friedler et al. [9]describe the relationship of observations and underlying mechanisms as that of underlying “construct spaces” (the construct feature space, in which entities’ true representations lie, and the construct decision space, representing ideal outcomes under complete and perfect information), that we observe through an observation process to obtain the “observed feature space” (how entities are represented after the incomplete and possibly biased observation process) and the “observed decision space” representing the decisions we make on the basis of these observed features. Their work was focused on algorithmic fairness, and grounding many fairness concerns in distortions between the various spaces; however, the framework for understanding data and decisions is far more general. In recommender systems, we can think of the construct feature space as holding users’ true preferences (or their time-varying and context-specic constituent components) and the construct decision space as representing their ideal recommendations; the observed feature space is the representation of users we can actually obtain through the data they make available to us. There are a variety of distortions that can occur between the construct and observed feature spaces, that can systematically aect both the presence of observations (recommender system data is missing-not-at-random [ and the values of those observations. Some of these distortions are well-documented in the recommender systems literature, such as popularity bias [3]. Two of the impacts of biased observation process are that the system on biased data and positive social impact. Unfortunately, due to the unobservability of the construct feature and decision spaces, we do not know the precise structure of these biases, or the underlying true preferences or counterfactual responses. Simulation can help with this. We can simulate the entire data-generating process, from preference to observation. Through this, we do but we do know the relationship between truth and observation in our data. We can use such simulated data to study the distortions in recommender system behavior (and metrics of that behavior or performance) between what would be observed in an experiment with observable data, and what would be observed in an experiment with access to the actual underlying truth through an oracle. One of my students has used this approach to measure biases in evaluation metrics that are induced by data missingness [ better-understand popularity bias. With simulation, not only can we create data sets that — subject to certain assumptions — provide both observations and their underlying truth, we can the sensitivity of our analysis to those assumptions; for example, if the bias in an evaluation metric is relatively stable across a range of plausible assumptions, that provides evidence that getting the data generating process exactly right is not so important and the metric may be reliable, but if it changes substantially with modest changes in assumptions, then we should treat results on that metric as highly tentative. 4 PROSPECTIVE SIMULATION: STUDYING FUTURE BEHAVIOR Simulation also allows us to estimate possible future behavior and impact of the system and its users, under controlled and variable conditions, particularly as the system and users respond to each other. Both Chaney et al and Hosanagar which recommenders push users to consume the same items vs. distribute their attention across a wide range of diverse items. This use of simulation has a wide range of applications, from homogeneity and popularity bias to lter bubbles to fairness concerns [ and society. The key idea of many these simulations is to simulate the process of users consuming items, producing traces for training the algorithm, receiving recommendations, and consuming more items, possibly in response to those . Both of these can cause a signicant problem in the system’s ability to deliver user and business value [8]have used simulation to study homogeneity eects in recommender systems: to quantify the extent to 5] and many others regarding both a system’s eectiveness and its impact on users, content creators, recommendations. These models enable researchers to encode a wide range of assumptions into the user response models and study system performance and behavior under varying conditions. 5 RECOMMENDER SYSTEM RESPONSE CURVES One of the major things that higher-degree simulation (anything above static data) aords in both of these, and other, scenarios is the ability to map outresponse curvesfor a recommender system or its surrounding experiments. In our study of recommender system metric bias [16], for example, we could extend the simulation to specically model a variety of known degrees of popularity bias or of data sparsity, and estimate how the evaluation metric bias changes as a function of known changes in data biases. We don’t necessarily know the degree of bias that is present in real data, but if we can understand the evaluation process’s response curve to that bias, it will produce knowledge that can be combined with future research that may provide a better idea of where in the curve any particular actual system lies. We have a similar set of problems when working on counteracting potentially discriminatory biases in recommender systems. We do not know, for example, what the distribution of author gender in book ratings and recommendations would be in an ideal world with no discriminatory factors aecting book production, reading, and recommendation [7] (Mitchell et al. [13]identify this as “the world as it can and should be” in their taxonomy of biase sources); through simulation, we can quantify system behavior and response under a range of possible world-states and targets. Existing and future research in a variety of elds will hopefully yield context to know what these respons cuves say about our existing world and book publishing ecosystem. 6 CONCLUSION Simulation is a powerful tool for reckoning with uncertainty about what lies behind our data, or about how recommender systems may behave in the future under various conditions. There is a lot of work to be done in order to understand how to develop, tune, and validate these simulations, but simulation has the promise to unlock types of research that are infeasible by any other means. One of these is to to examine system behavior under specic, controlled conditions, and isolate the eect of particular user, item, or ecosystem dynamics on recommender behavior and user response. Static data and actual applications dier in too many variables simultaneously to facilitate direct comparison that demonstrates the eect of specic features, but simulation allows us to create data sets or online responses that dier only in selected ways. This will enable us to understand the behavior and eects of recommender systems and hypothetical human responses under a range of plausible and extreme conditions, and better understand when systems exhibit what behavior. The ability to build systems that truly promote human ourishing and avoid harm depends on this analysis. ACKNOWLEDGMENTS This paper based on work supported by the National Science Foundation under Grant No. IIS 17-51278.