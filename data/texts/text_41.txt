By providing explanations for users and system designers to facilitate better understanding and decision making, explainable recommendation has been an important research problem. In this paper, we proposeCounterfactualExplainableRecommendation (CountER), which takes the insights of counterfactual reasoning from causal inference for explainable recommendation. CountER is able to formulate the complexity and the strength of explanations, and it adopts a counterfactual learning framework to seek simple (low complexity) and eÓÄùective (high strength) explanations for the model decision. Technically, for each item recommended to each user, CountER formulates a joint optimization problem to generate minimal changes on the item aspects so as to create a counterfactual item, such that the recommendation decision on the counterfactual item is reversed. These altered aspects constitute the explanation of why the original item is recommended. The counterfactual explanation helps both the users for better understanding and the system designers for better model debugging. Another contribution of the work is the evaluation of explainable recommendation, which has been a challenging task. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. To measure the explanation quality, we design two types of evaluation metrics, one from user‚Äôs perspective (i.e. why the user likes the item), and the other from model‚Äôs perspective (i.e. why the item is recommended by the model). We apply our counterfactual learning algorithm on a black-box recommender system and evaluate the generated explanations on ÓÄõve real-world datasets. Results show that our model generates more accurate and eÓÄùective explanations than state-of-the-art explainable recommendation models. Source code is available at https://github.com/chrisjtan/counter. ‚Ä¢ Computing methodologies ‚Üí Machine learning;‚Ä¢ Information systems ‚Üí Recommender systems. Explainable Recommendation; Counterfactual Explanation; Counterfactual Reasoning; Machine Learning; Explainable AI ACM Reference Format: Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang. 2021. Counterfactual Explainable Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ‚Äô21), November 1‚Äì5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482420 Explainability for recommender systems is crucial, because in recommendation scenarios we can rarely say that some recommendation is absolutely right or some other recommendation is absolutely wrong, instead, it all depends on good explanations to help users understand why an item is recommended so as to increase the transparency and trust and enable better decision making; good explanations also help system designers to track the behavior of the complicated recommendation models for better debugging [49,50]. One prominent approach is aspect-aware explainable recommendation [9,11,40,50], which takes the explicit item features/aspects to construct explanations. For example, Zhang et al.[50] proposed Explicit Factor Model (EFM) which aligns latent factors with explicit features such as color and price to generate sentence explanations in the form of ‚ÄúYou might be interested in [feature], on which this product performs well.‚Äù Wang et al.[40] learned the user-aspect preferences in a multi-task joint tensor factorization framework to construct the aspect-aware explanations. Chen et al.[9] explored attribute-aware collaborative ÓÄõltering for explainable substitute recommendation. Li et al.[29] proposed a Personalized Transformer to generate aspect-inspired natural language explanations. A more comprehensive review of related work is provided in Section 2. However, existing methods on aspect-aware explainable recommendation face several issues: 1) Most of the methods are designed as intrinsic explainable models, although they have the advantage of providing faithful explanations, it may be diÓÄúcult for them to explain other black-box recommendation models. 2) Existing methods do not consider the explanation complexity, in particular, they do not have the ability to decide how many aspects to use when generating explanations. Most methods generate explanations using exactly one aspect, however, the real reason of the recommendation may be triggered by multiple aspects. 3) Existing methods do not consider the explanation strength, i.e., to what extent the explanation really inÓÄûuences the recommendation result. This is mostly because existing methods are designed based on matching algorithms, which extracts associative signals such as feature importance and attention weights to construct explanations, while they seldom consider what happens if we intervene these signals to alternative values. Recent advances on counterfactual reasoning shed light on the possibility to solve the above problems. We use a toy example in Figure 1 to illustrate the intuition of counterfactual explanation and its diÓÄùerence from matching-based explanation. In this example, similar to Zhang et al.[50], each user has his/her preference score on each aspect, while each item has its performance score on each aspect. The recommendation algorithm uses the total score to rank items. For example, the total score of Phone A is the recommendation list. To explain the recommendation of Phone A, a matching-based model would select screen as the explanation because the multiplication score for screen (4.0√ó4.5=18) is the highest compared to the other aspects (5.0√ó3.0=15 for battery and 3.0√ó3.0=9 for price). However, Phone A actually performs the worst on screen among all products, which makes this explanation unreasonable. This shows that the aspects with high matching scores may not always be the reason of the recommendation. Counterfactual reasoning, on the contrary, aims to understand the underlying mechanism that really triggered the recommendation by applying interventions on the aspects and see what happens. As shown in the lower part of Figure 1, we can look for the minimal change on Phone A‚Äôs features such that Phone A will not be recommended anymore. In this case, the aspect battery will be selected as explanation because we only need to slightly change its score from 3 to 2.1 to reverse the recommendation decision, indicating that battery is a very inÓÄûuential factor on the recommendation result. Following the above intuition, in this paper, we propose Counterfactual Explainable Recommendation (CountER), which adopts counterfactual reasoning to extract aspect-aware explanations for recommendation. Inspired by the Occam‚Äôs Razor Principle [3], CountER is built on the fundamental idea of explanation complexity and explanation strength, where complexity measures how much change must be applied on the factual observations, and strength shows to what extent the applied change will inÓÄûuence the recommendation decision. Through our proposed counterfactual constrained learning framework, CountER aims to extract simple (low complexity) and eÓÄùective (high strength) explanations for the recommendation by looking for minimal changes on the facts that would alter the recommendation decision. Another important challenge in explainable recommendation (and explainable AI) research is how to evaluate the explanations. Due to the lack of standard oÓÄüine evaluation measures for explanation, previous research heavily relied on human subjects for evaluation, which makes the evaluation process expensive, unscalable, hard to standardize, and unfriendly to academic research settings [49]. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. In this paper, we propose two types of evaluation methods, one is user-oriented evaluation, and the other is model-oriented evaluation. For user-oriented evaluation, we adopt similar ideas as [26,28,29] by using each user‚Äôs mentioned aspects in their reviews as the ground-truth. By comparing our generated explanation with the ground-truth, we can Figure 1: Matching-based vs. counterfactual reasoning. The numbers beside the three aspects (screen, battery, and price) show how much the user cares about an aspect and how well the item performs on an aspect. In this example, matchingbased explanation methods would use ‚Äúscreen‚Äù to construct an explanation, while counterfactual reasoning method will use ‚Äúbattery‚Äù to construct the explanation. evaluate the feature coverage, precision, recall, andùêπscores. For model-oriented evaluation, we adopt the Probability of Necessity (PN), Probability of SuÓÄúciency (PS) and their harmonic meanùêπ to evaluate the suÓÄúciency and necessity of the explanations. More details are provided in the experiments. In summary, this work has the following contributions: (1)For the ÓÄõrst time, we explore the complexity and the strength of explanations in explainable recommendation and formulate the concepts in mathematical ways. (2)We formulate a counterfactual reasoning framework based on counterfactual constrained learning to extract simple and eÓÄùective explanations for recommendation. (3)We design both user-oriented and model-oriented metrics for standard evaluation of explainable recommendation. (4)We conduct extensive experiments on ÓÄõve real-world datasets to validate the eÓÄùectiveness of our proposed method. In this section, we review some related work on explainable recommendation and counterfactual reasoning. Explainable recommendation is a broad research area with many diÓÄùerent types of models, and it is diÓÄúcult to cover all of the works on this direction. Since our work is more closely related with aspectaware explainable recommendation, we mainly focus on this subarea in the section. A more complete review of explainable recommendation can be seen in [19, 37, 49]. Explainability of recommender systems is important because it improves the transparency, user satisfaction and trust over the recommendation system [49]. One representative way to generate user-friendly explanations is by modeling aspects in the recommended items. For instance, Zhang et al.[50] introduced an Explicit Factor Model (EFM) for explainable recommendation. It ÓÄõrst extracts the item aspects and the user opinions on these aspects from user reviews. Then, it trains a matrix factorization-based recommendation model to generate aspect-aware explanations by aligning the latent factors with the item aspects. Chen et al.[11] and Wang et al.[40] advanced from matrix factorization to tensor factorization models for explainable recommendation. He et al.[22] proposed a tripartite graph model to improve the interactivity of aspect-aware recommendation models. Gao et al.[18] proposed an explainable deep model to learn multi-level user proÓÄõle and infer which level of features best captures a user‚Äôs interest. Balog et al.[2] presented a set-based recommendation technique to improve the scrutability and transparency of recommender systems. Ren et al.[35] proposed a collaborative viewpoint regression for explainable social recommendation. Wang et al.[42] proposed a tree-enhanced embedding method to combine embedding-based and tree-based models for explainable recommendation. More recently, Chen et al.[9] applied a residual feed-forward neural network to model the user and item explicit features and generates explainable substitute recommendations. Pan et al.[33] presented a feature mapping approach to map the latent features onto the interpretable aspects to achieve both satisfactory accuracy and explainability. Li et al.[29] proposed a Personalized Transformer to generate aspect-inspired natural language explanations. Xian et al.[46] developed an attribute-aware algorithm for explainable item-set recommendation and deployed in Amazon. Some other explainable recommendation methods include knowledge graph-based explanations [1,44,45], neural logic explanations [53], visual explanations [10], natural language explanations [7,28‚Äì30], dynamic explanations [12], reinforcement learningbased explanations [41], conversational explanations [13,52], fair explanations [17], disentangled explanations [31], review-based explanations [6, 32], etc. Most of the existing approaches generate explanations based on a very similar hypothesis: If there exists an aspect that best matches between the user‚Äôs preference and the item‚Äôs performance, then this aspect would be the explanation of the recommendation. However, our method generates explanations from a counterfactual perspective: if an item would not have been recommended had it performed slightly worse on some aspects, then these aspects would be the reason for the model to recommend this item. Counterfactual reasoning, together with logical reasoning [8,36], are two important types of cognitive reasoning approaches. Recently, counterfactual reasoning has drawn attention in explainable AI research. It has some successful applications in several machine learning ÓÄõelds such as computer vision [21], natural language processing [16,23], and social fairness [15,39]. In the recommendation ÓÄõeld, recent works used counterfactual reasoning to improve both recommendation accuracy [43,47] and explainability [20,38,48] based on heterogeneous information networks [20], perturbation model [48] or inÓÄûuence functions [38], e.g., Ghazimatin et al.[20] tried to generate provider-side counterfactual explanations by looking for a minimal set of user‚Äôs historical actions (e.g. reviewing, purchasing, rating) such that the recommendation can be changed by removing the selected actions. Tran et al.[38] adopted inÓÄûuence functions for identifying training points most relevant to a recommendation while deducing a counterfactual set for explanations. A common factor between our work with prior work is that both of the proposed methods generate explanations based on extracted causalities rather than associative relationships. Yet, our work is diÓÄùerent from prior works on two key points: 1) In terms of problem deÓÄõnition, prior works generate counterfactual explanations on the user-side based on user actions while our method generates counterfactual explanations on the item-side based on item aspects, which are two diÓÄùerent types of explanations. 2) In terms of technique, our method adopts a counterfactual learning framework driven by the Occam‚Äôs Razor Principle [3] to directly learn an explanation of small complexity and large strength, so that our desire of ÓÄõnding simple and eÓÄùective explanation is directly encoded into the model objective. In this section, we ÓÄõrst describe the preliminaries and the counterfactual explainable recommendation problem. Then, we introduce the concepts of explanation complexity, explanation strength, and their relations. Finally, we introduce the intuition of counterfactual reasoning. We leave the more formal and mathematical deÓÄõnition of our counterfactual reasoning framework to the next section. Suppose we have a user set withùëöusersU = {ùë¢, ùë¢, ¬∑ ¬∑ ¬∑ ,ùë¢}and an item set withùëõitemsV = {ùë£, ùë£, ¬∑ ¬∑ ¬∑ , ùë£}. Let binary matrix ùêµ ‚àà {0,1}be the user-item interaction matrix, whereùêµ=1 if userùë¢interacted with itemùë£; otherwise,ùêµ=0. We useR(ùë¢, ùêæ) to represent the top-ùêærecommendation list for a userùë¢, and we sayùë£ ‚àà R (ùë¢, ùêæ)if itemùë£is recommended to userùë¢in the user‚Äôs top-ùêælist. Following the same method described in Zhang et al. [50], we apply the sentiment analysis toolkitbuilt in [51] to extract (Aspect, Opinion, Sentiment) triplets from the textual reviews. For example, in the Cell Phone domain, the extracted aspects would include color, price, screen, battery, etc. Besides, suppose we have a total number ofùëüitem aspectsA = {ùëé, ùëé, ¬∑ ¬∑ ¬∑ , ùëé}. Same as [51], we further compute the user-aspect preference matrixùëã ‚àà R and the item-aspect quality matrixùëå ‚àà R.ùëãindicates to what extent the userùë¢cares about the item aspectùëé. Similarly,ùëå indicates how well the itemùë£performs on the aspectùëé. More speciÓÄõcally, ùëã and ùëå are calculated as: ùëå=0, if item ùë£is not reviewed on aspect ùëé1 +, else whereùëÅis the rating scale in the system, which is 5-star in most cases.ùë°is the frequency that userùë¢mentioned aspectùëé.ùë° is the frequency that itemùë£is mentioned on aspectùëé, whileùë† is the average sentiment of these mentions. For both theùëãandùëå matrices, their elements are re-scaled into the range of(1, ùëÅ )using the sigmoid function (see Eq.(1)) to match with the system‚Äôs rating scale. Since the matrix construction process is not the focus of this work, we only brieÓÄûy describe this process and readers may refer to [50,51] for more details. The same user-aspect and item-aspect matrix construction technique is also used in [18, 27, 40]. With the above deÓÄõnitions, the objective of our counterfactual reasoning problem is to search for aspect-driven counterfactual explanations for a given black-box recommendation model. More speciÓÄõcally, for a given recommendation model, if itemùë£ is recommended to userùë¢, i.e.,ùë£‚àà R(ùë¢, ùêæ), then we look for a slight change vectorŒî = {ùõø, ùõø, ¬∑ ¬∑ ¬∑ , ùõø}for the item-aspect quality vectorùëå, such that ifŒîis applied on itemùë£‚Äôs quality vector, i.e., ùëå+ Œî, then it will change the recommendation result to make item ùë£disappear from the recommendation list, i.e.,ùë£‚àâ R(ùë¢, ùêæ). All the values inŒîare either zero or negative continuous values since an item will only be removed from the recommendation list if it performs worse on some aspects. With the optimized vectorŒî, we can construct the counterfactual explanation for itemùë£, which is composed of the aspects corresponding to the non-zero values in Œî. The counterfactual explanation takes the following form, If the item had been slightly worse on [aspect(s)], then it will not be recommended. where the [aspect(s)] are selected byŒîas mentioned above. In the following, we will deÓÄõne the properties ofŒîin more formal ways. To better understand the counterfactual explainable recommendation problem, we introduce two concepts to motivate explainable recommendation under the counterfactual reasoning background. The ÓÄõrst is Explanation Complexity (EC), which measures how complicated the explanation is. In our aspect-based explainable recommendation setting, the complexity can be deÓÄõned as 1) how many aspects are used to generate the explanation, which corresponds to the number of non-zero values inŒî, i.e.,‚à•Œî ‚à•, and 2) how many changes need to be applied on these aspects, which can be represented as the sum of square ofŒî, i.e.,‚à•Œî ‚à•. The ÓÄõnal complexity takes a weighted sum of the two factors: whereùõæis a hyper-parameter to control the trade-oÓÄù between these two terms. The second is Explanation Strength (ES), which measures how eÓÄùective the explanation is. In our counterfactual explainable recommendation setting, this can be deÓÄõned as to what extent applying the slight change vectorŒîwill inÓÄûuence the recommendation result of itemùë£. This can be further deÓÄõned as the decrease ofùë£‚Äôs ranking score in user ùë¢‚Äôs recommendation list after applying Œî: whereùë†is the original ranking score of itemùë£, andùë†is the ranking score ofùë£afterŒîis applied to its quality vector, i.e.,ùëå+ Œî. We should note that Eq.(2)and(3)are not the only way to deÓÄõne explanation complexity and strength. The deÓÄõnition depends on what we need in practice. Our counterfactual reasoning framework introduced in Section 4 is ÓÄûexible and can easily adapt to diÓÄùerent deÓÄõnitions of explanation complexity and strength. It is also worth discussing the relationship between explanation complexity and strength. Actually, complexity and strength are two orthogonal concepts, i.e., a complex explanation is not necessarily strong, and a simple explanation is not necessarily weak. There may well exist explanations that are complex but weak, or simple and strong. According to the Occam‚Äôs Razor Principle [3], if two explanations are equally eÓÄùective, we prefer the simpler explanation than the complex one. As a result, counterfactual explainable recommendation aims to seek the simple (low complexity) and eÓÄùective (high strength) explanations for recommendation. In this section, we ÓÄõrst brieÓÄûy introduce the black-box recommendation model for which we want to generate explanations. Then, we describe the details of our counterfactual constrained learning framework for counterfactual explainable recommendation. Suppose we have a black-box recommendation modelùëìthat predicts the user-item ranking score ùë†for user ùë¢and item ùë£by: whereùëãandùëåare the user-aspect vector and item-aspect vector, respectively, as deÓÄõned in Eq.(1);Œòis the model parameter, andùëç represents all other auxiliary information of the model. Depending on the application,ùëçcould be ratings, clicks, text, images, etc., andùëçis optional in the recommendation model. Basically, the recommendation modelùëìcan be any model as long as it takes the user‚Äôs and the item‚Äôs aspect vectors as part of the input, which makes our counterfactual reasoning framework applicable to a wide scope of models. In this work, to demonstrate the idea of counterfactual reasoning, we use a very simple deep neural network as the implementation of the recommendation modelùëì, which includes one fusion layer followed by three fully connected layers. The network concatenates the user‚Äôs and the item‚Äôs aspect vectors as input and outputs a onedimensional ranking score ùë†. The ÓÄõnal output layer is a sigmoid activation function so as to mapùë†into the range of(0,1). Then, we train the model with a cross-entropy loss: whereùêµ=1 if userùë¢previously interacted with itemùë£, otherwiseùêµ=0. In practice, sinceùêµis a very sparse matrix, we sample the negative samples with ratio 1:2, i.e., for each positive instance we sample two negative instances. With this pre-trained recommendation model, for a target user, we are able to recommend top-ùêæ items according to the predicted ranking scores. We build a counterfactual reasoning model to generate explanations for any item in the top-ùêærecommendation list provided by an existing recommendation model. The essential idea of the proposed explanation model is to discover slight changeŒîon the item‚Äôs aspects via solving a counterfactual optimization problem which is formulated in the following. Suppose itemùë£is in the top-ùêærecommendation list for userùë¢ (ùë£‚àà R (ùë¢, ùêæ)). As mentioned before, our counterfactual reasoning model aims to ÓÄõnd simple and eÓÄùective explanations forùë£, which can be shown as the following constrained optimization framework, Mathematically, according to our deÓÄõnition of explanation complexity and strength in Section 3, the framework can be realized with the following speciÓÄõc optimization problem, whereùë†= ùëì (ùëã, ùëå| ùëç, Œò),ùë†= ùëì (ùëã, ùëå+ Œî | ùëç, Œò). In the above equation,ùë†is the original ranking score of itemùë£,ùë†is the ranking score ofùë£when the slight change vectorŒîis applied onùë£‚Äôs aspect vector. The intuition of Eq.(7)is trying to ÓÄõnd an explanationŒîthat is both simple and eÓÄùective, where ‚Äúsimple‚Äù is reÓÄûected by the optimization objective, i.e., explanation complexity ùê∂ (Œî)is minimized, while ‚ÄúeÓÄùective‚Äù is reÓÄûected by the optimization constraint, i.e., the explanation strengthùëÜ (Œî)should be big enough to remove item ùë£from the top-ùêæ list. To realize the second goal (i.e., eÓÄùective/strong enough), we take the thresholdùúñas the margin between itemùë£‚Äôs score and theùêæ +1‚Äôs item‚Äôs score in the original recommendation list, i.e., whereùë†= ùëì (ùëã, ùëå| ùëç, Œò)is the ranking score of theùêæ +1‚Äôs item, and thus Eq.(7) can be simpliÓÄõed as, In this way, itemùë£will be ranked lower than theùêæ +1‚Äôs item and thus be removed from the top-ùêæ list. A big challenge to optimize Eq.(9)is that both the objective‚à•Œî ‚à•+ ùõæ ‚à•Œî ‚à•and the constraintùë†‚â§ ùë†are not diÓÄùerentiable. In the following, we relax the two parts to make the equation optimizable. For the objective, since‚à•Œî ‚à•is not convex, we relax it with‚Ñìnorm‚à•Œî ‚à•. This is shown to be eÓÄúcient and provides good vector sparsity in [4,5], thus helps to minimize the explanation complexity in terms of the number of aspects in the explanation. For the constraint ùë†‚â§ ùë†, we relax it as a hinge loss: and add it as a Lagrange term into the total objective. Thus, the ÓÄõnal optimization equation for generating explanation becomes: minimize‚à•Œî ‚à•+ ùõæ ‚à•Œî‚à•+ ùúÜùêø(ùë†, ùë†) where ùë†= ùëì (ùëã, ùëå+ Œî | ùëç, Œò), ùë†= ùëì (ùëã, ùëå| ùëç, Œò) In Eq.(11),ùúÜandùõºare hyper-parameters to control the explanation strength. A sacriÓÄõce of using relaxed optimization is that we lose the guarantee that itemùë£is removed from the top-ùêæ list, though the probability of removing is high due to minimizing theùêø(ùë†, ùë†)term. As a result, it requires a post-process to check ifùë†is indeed smaller thanùë†. We should only generate counterfactual explanations when the removal is successful. In the experiments, we will report the ÓÄõdelity of our explanation model to show what percentage of items can be explained by our method. Besides, there is a trade-oÓÄù in the relaxed model: by increasing the hyper-parameterùúÜ, the model will focus more on the explanation strength but less on the explanation complexity. We will also explore the inÓÄûuence ofùúÜand the relationship between explanation complexity and strength in the ablation study of the experiments. 4.4.1Explanation Complexity for Items at DiÓÄõerent Positions. With the above framework, we can see that the diÓÄúculty of removing diÓÄùerent items in the top-ùêælist are diÓÄùerent. Suppose for a certain user, the recommender system generates top-ùêærecommended items asùë£, ùë£, ¬∑ ¬∑ ¬∑ , ùë£according to the ranking scores. Intuitively, removingùë£from the list is more diÓÄúcult than removing ùë£from the list. The reason is that to removeùë£, the explanation strength should be at leastùúñ = ùë†‚àí ùë†, which is bigger than the strength needed for removingùë£, which isùúñ = ùë†‚àí ùë†. As a result, the generated explanations for the items at a higher position in the list will likely have higher explanation complexity, because the reasoning model has to apply larger changes or more aspects to generate high-strength explanations. This is a reasonable and desirable property of the counterfactual explainable recommendation framework‚Äîif the system ranks an item at a very high position, then it means that the system strongly recommends this item, which needs to be backed by strong explanation that contains more aspects. On the contrary, for an item ranked at lower positions in the list, it could be easily removed by changing only one or two aspects, which is in line with our intuition. In the experiments, we will show the average explanation complexity for items at diÓÄùerent positions to verify the above discussion. 4.4.2Controlling the Number of Aspects. Through Eq.(11), the model can automatically decide the number of aspects to construct the explanation. We believe this is better than choosing only one aspect as was done in previous aspect-aware explainable recommender systems [9,22,40,51]. However, if needed, our method can also generate explanations with a single aspect. To generate single aspect explanation, we adjust Eq.(11)by adding a trainable one-hot vectoraas a mask to make sure that only one aspect is changed during the training. The optimization problem is: Since we force the model to generate single aspect explanation, the ‚Ñì-norm term ofùê∂ (Œî)vanishes because‚à•Œî ‚à•=1. We will explore both single- and multi-aspect explanations in experiment. How to quantitatively evaluate explanations is a very important problem. Fortunately, compared to other explanation forms, counterfactual explanation is very suitable for quantitative oÓÄüine evaluation. In this section, we mathematically deÓÄõne two types of evaluation metrics‚Äîuser-oriented evaluation and model-oriented evaluation, which we believe can help the ÓÄõeld to move forward with standard evaluation of explainable recommendations. In user-oriented evaluation, we adopt the user‚Äôs review on the item as the ground-truth reason about why the user purchased the item, which is similar to previous works [6,14,28,30,40]. More speciÓÄõcally, from the textual review that a userùë¢provided on an itemùë£, we extract all the aspects thatùë¢mentioned with positive sentiment,ÓÄÇÓÄÉ which is deÓÄõned asùëÉ=ùëù, ùëù, ¬∑ ¬∑ ¬∑ , ùëù.ùëÉis a binary vector, whereùëù=1 if userùë¢has positive sentiment on the aspect ùëéin his/her review for itemùë£. Otherwise,ùëù=0. On the other hand, our model will produce the vectorŒî = {ùõø, ùõø, ¬∑ ¬∑ ¬∑ , ùõø}, and those aspect(s) corresponding to the non-zero values inŒîwill constitute the explanation. Then, for each user-item pair, we calculate the precision and recall of the generated explanationŒîwith regard to the groundtruth vector ùëÉ: Precision =√çùêº (ùõø), Recall =√ç(13) whereùêº (ùõø)is an identity function such thatùêº (ùõø) =1 whenùõø ‚â†0, andùêº (ùõø) =0 whenùõø =0. In our case, the Precision measures the percentage of aspects in our generated explanation that are really liked by the user, while Recall measures how many percentage of aspects liked by the user are really included in our explanation. We also calculate theùêπscore as the harmonic mean between the two, i.e.,ùêπ=2¬∑. Then, we average the scores of all pairs as the ÓÄõnal Precision, Recall, and ùêπmeasure. The user-oriented evaluation only answers the question of whether the generated explanations are consistent with user‚Äôs preferences. However, it does not tell us whether the explanation properly justiÓÄões the model‚Äôs behaviour, i.e., why the recommendation model recommends this item to the user. To test if our explanation model correctly explains the essential mechanism of the recommendation system, we use two scores, Probability of Necessity (PN) and Probability of SuÓÄúciency (PS) [34, p.112], to validate the explanations with model-oriented evaluation. In logic and mathematics, necessity and suÓÄúciency are terms used to describe a conditional or implicational relationship between two statements. Suppose we haveùëÜ ‚áí ùëÅ, i.e., ifùëÜhappens thenùëÅ will happen, then we sayùëÜis a suÓÄúcient condition forùëÅ. Meanwhile, we have the logically equivalent contrapositive¬¨ùëÅ ‚áí ¬¨ùëÜ, i.e., ifùëÅdoes not happen, thenùëÜwill not happen, as a result, we say ùëÅ is a necessary condition for ùëÜ. Probability of Necessity (PN)[34]: In causal inference theory, probability of necessity evaluates the extent that a condition is necessary. To calculate PN for the generated explanation, suppose a set of aspectsA‚äÇ Acompose the explanation for the recommendation of itemùë£to userùë¢. The essential idea of the PN score is: if in a counterfactual world, the aspects inAdidnotexist in the system, then what is the probability that itemùë£wouldnotbe recommended for user ùë¢. Following this idea, we calculate the frequency of the generated explanations that meet the PN deÓÄõnition. LetùëÖbe userùë¢‚Äôs original recommendation list. Letùë£‚àà ùëÖbe a recommended item that our framework generated a nonempty explanationA‚â† ‚àÖ. Then for all the items in the universal item setV, we alter the aspect values in the item-aspect quality matrixùëåto 0 if they are inA. In this way, we create a counterfactual item setVwhich results in a counterfactual recommendation listùëÖfor userùë¢by the recommendation algorithm. Then, the PN score is: PN =√ç√çùêº (A‚â† ‚àÖ), where PN=1, if ùë£‚àâ ùëÖ0, else whereùêº (A‚â† ‚àÖ)is an identity function such thatùêº (A‚â† ‚àÖ) =1 if the conditionA‚â† ‚àÖholds and 0 otherwise. Basically, the denominator is the total number of items that the algorithm successfully generated an explanation for, and the numerator is the number of explanations that if we remove the related aspects then it will cause the item to be removed from the recommendation list. Probability of SuÓÄúciency (PS)[34]: The PS score evaluates the extent that a condition is suÓÄúcient. The essential idea of the PS score is: if in a counterfactual world, the aspects inAwere theonlyaspects existed in the system, then what is the probability that item ùë£would still be recommended for user ùë¢. Similarly, we calculate the frequency of the generated explanations that meet the PS deÓÄõnition. For all the items inV, we alter the aspect values in the item-aspect quality matrixùëåto 0 if they are not inA. In this way, we create a counterfactual item setV which results in a counterfactual recommendation listùëÖfor user ùë¢by the recommendation algorithm. Then, the PS score is: PS =√ç√çùêº (A‚â† ‚àÖ), where PS=1, if ùë£‚àà ùëÖ0, else whereùêº (A‚â† ‚àÖ)is still the identity function as above. Basically, the denominator is still the total number of items that the algorithm successfully generated an explanation for, and the numerator is the number of explanations that alone can still recommend the item to the recommendation list. Similar to the user-oriented evaluation, we also calculate the harmonic mean of PS and PN to measure the overall performance, which is ùêπ=. In this section, we ÓÄõrst introduce the datasets, the comparison baselines and the implementation details. Then we present studies on the two main expected properties in this paper: complexity and strength of the explanations. We also present ablation studies to explore how our model performs under diÓÄùerent conditions. We test our method on Yelpand Amazondatasets. The Yelp dataset contains users‚Äô reviews on various kinds of businesses such as restaurants, dentists, salons, etc. The Amazon dataset [32] contains user reviews on products in Amazon e-commerce system. The Amazon dataset contains 29 sub-datasets corresponding to 29 product categories. We adopt four datasets of diÓÄùerent scales to evaluate our method, which are Electronic, Cell Phones and Accessories, Kindle Store and CDs and Vinyl. Since the Yelp and Amazon datasets are very sparse, similar as previous work [40,44,45,50], we remove the users with less than 20 reviews for Yelp dataset, and 10 reviews for Amazon dataset. The statistics of the datasets are shown in Table 1. We compare our model with three aspect-aware explainable recommendation models. We also include a random explanation baseline to show the overall diÓÄúculty of the evaluation tasks. EFM[50]: The Explicit Factor Model. This work combines matrix factorization with sentiment analysis technique to align latent factors with explicit aspects. In this way, it predicts the user-aspect preference scores and item-aspect quality scores. The top-1 aligned aspect is used to construct aspect-based explanation. MTER[40]: The Multi-Task Explainable Recommendation model. This work predicts a tensorùëã ‚àà R, which represents the aÓÄúnity score among the users, items, aspects, and an extra dimension for the overall rating. This tensorùëãis acquired via Tucker decomposition [24,25]. We should note that since the overall rating for a user on an item is predicted in the extra dimension via decomposition, which is not directly predicted by the explicit aspects, this method is not suitable for the model-oriented evaluation. As a result, we only report this model‚Äôs explanation performance on user-oriented evaluation. A2CF[9]: The Attribute-Aware Collaborative Filtering model. This work leverages a residual feed-forward network to predict the missing values in the user-aspect matrixùëãand the item-aspect matrixùëå. The method originally considers both the user-item preference and the item-item similarity to generate explainable substitute recommendations. We remove the item-item factor to make it compatible with our problem setting to generate explanations for any item. Similar to [50], the top-1 aligned aspect will be used for explanation. Random: For each item recommended to a user, we randomly choose one or multiple aspects from the aspect space and generate the explanation based on them. The evaluation scores of the random baseline can indicate the diÓÄúculty of the task. 6.3.1Preprocessing. The preprocessing includes two parts: 1) Generating the user-aspect vector ùëã and the item-aspect vector ùëå from the user reviews. 2) Training the base recommender system. In the preprocessing phase, we hold-out the last 5 interacted items for each user, which serve as the test data to evaluate both the recommendation and the explanation. The deep neural network in the base recommendation model consists of 1 fusion layer and 3 fully connected layers with {512, 256, 1} output dimensions, respectively. We apply ReLU activation function after all the layers except the last one, which is followed by a Sigmoid function to re-scale the predicted scores to the range of(0,1). The model parameters are optimized by stochastic gradient descent (SGD) optimizer with a learning rate of 0.01. After the recommendation model is trained, all the parameters will be ÓÄõxed in the counterfactual reasoning phase and explanation evaluation phase. 6.3.2Generating Explanations. The base recommendation model generates the top-ùêærecommendation list for each user.ùêæis set to 5 in the experiment. We then apply the counterfactual reasoning method to generate explanations for the items in the list. The hyper-parameterùúÜis set to 100 for all the datasets. The ablation study on the inÓÄûuence ofùúÜcan be seen in Section 6.5. We notice that the‚Ñì-norm‚à•Œî ‚à•and the‚Ñì-norm‚à•Œî ‚à•is almost in the same scale, so that we always setùõæto 1 in our model. For the margin valueùõºin the hinge loss, we tested diÓÄùerent values for ùõºin{0.1,0.2,0.3, ¬∑ ¬∑ ¬∑ ,1.0}and ÓÄõnd that the performance does not change too much forùõº =0.1,0.2, ¬∑ ¬∑ ¬∑0.5 and then signiÓÄõcantly drops forùõº >0.5. As a result, we setùõº =0.2 throughout the experiments. To compare with the baselines, for each recommended item, we generate both multi-aspect and single-aspect explanation through Eq.(11) and Eq.(12), respectively. 6.3.3Aspect Masking. When generating explanations, our model directly chooses aspects from the entire aspect space, which is reÓÄûected by the change vectorŒî. However, in the user-oriented evaluation, a strong bias exists in the user‚Äôs ground-truth review, which is that a user is more possible to mention the aspects that they have mentioned before. This may result from the personal linguistic preferences. As a result, all the baseline models (EFM, MTER, A2CF) only choose aspects from the user‚Äôs previously mentioned aspects to construct the explanation. To fairly compare with them in the user-oriented evaluation, we provide an adjusted version of our model. LetùëÄ‚àà {0,1}be the userùë¢‚Äôs mask vector.ùëÄis a binary vector, whereùëÄ=1 if aspectùëéis an aspect thatùë¢cares about, i.e.,ùëã‚â†0. Otherwise,ùëÄ=0. We apply this mask onŒî to generate explanation by choosing aspects only from the user‚Äôs preference space. which is: minimize‚à•Œî‚à•+ ùõæ ‚à•Œî‚à•+ ùúÜùêø(ùë†, ùë†) where Œî= ùëÄ‚äô Œî; ùë†= ùëì (ùëã, ùëå+ Œî|ùëç, Œò)(16) In this case, theŒîis used to generate explanation. This aspect mask can also be applied on the single-aspect formula, i.e., Œî= ùëÄ‚äô a ‚äô Œî. Notice that applying the mask does not introduce the data leakage problem because the mask is calculated based on the training set. In the evaluation, we evaluate both the original CountER model and the masked CountER model in both useroriented and model-oriented evaluations. Table 3: User-oriented evaluation of the explanations. All numbers in the table are percentage numbers with ‚Äò%‚Äô omitted. CountER (w/ mask) 33.94 25.67 28.31 29.21 20.26 22.85 40.94 36.19 37.73 39.06 25.93 29.33 12.96 12.96 12.96 CountER (w/ mask) 25.68 45.78 29.73 21.72 42.82 26.97 32.24 84.20 44.57 20.95 68.98 30.00 9.09 28.57 13.57 6.3.4Compatible with the Baselines. One issue in comparison with baselines is that our model is able to generate both multi-aspect and single-aspect explanations. When generating multi-aspect explanations, the model automatically decides the best number of aspects in model learning. However, the baseline methods can only generate single-aspect explanations using the top-1 aligned aspect since they do not have the ability to decide the number of aspects. Thus, to make the baseline models also comparable in multi-aspect explanation, we use our model as a guideline to tell the baseline models how many aspects should they use to generate multi-aspect explanations. For this reason, we only compare the explanations generated for the intersection of the items which are recommended by both our model and the baseline models in multi-aspect setting. 6.3.5Evaluable Explanations. Even if the explanation model can generate explanations for all the recommended items, not all of the explanations can be evaluated from the user‚Äôs perspective. This is because the user-oriented evaluation requires the user reviews as the ground-truth data. Thus, we can only evaluate the explanations generated for the ‚Äúcorrectly recommended‚Äù items which appear in the test data. For model-oriented evaluation, all the explanations can be evaluated. We ÓÄõrst report the ÓÄõdelity of our explanation method in Table 2, which shows for what percentage of recommended items that our method can successfully generate an explanation. We notice that the ÓÄõdelity of the single-aspect version is lower than that of the multi-aspect version. This indicates that for our model, using only one aspect as the explanation may not lead to enough explanation strength to eÓÄùectively explain the recommendations. However, if we allow the model to use multiple aspects, we can ÓÄõnd strong enough explanations in most cases (80%‚àº100%). The overall average number of aspects in multi-aspect explanation is 2.79. We then evaluate the explanations generated by CountER (original version and masked version) and the baselines. The useroriented evaluation is reported in Table 3 and the model-oriented evaluation is reported in Table 4. We note that the random baseline performs very bad on both evaluations, which shows the diÓÄúculty of the task and that randomly choosing aspects as explanations can barely reveal the reasons of the recommendations. For the user-oriented evaluation, the results show that when applying the mask for fair comparison, CountER outperforms all the baselines on all the datasets in terms ofùêπscores. Moreover, CountER performs better than the baselines on precision in 90% cases and on recall in 80% cases. We note that our model has a very huge improvement than the baselines on Yelp dataset even without mask, and the reason may be that the Yelp dataset is much denser and has more reviews than other datasets so that the user‚Äôs review bias has smaller impact. This also indicates that CountER has more advantages when the size of dataset increases. For model-oriented evaluation, the mask limits CountER‚Äôs ability to explain the model‚Äôs behavior. However, no matter with or without the mask, our model has better performance than all the baselines according toùêπscore. With mask, CountER has 15.63% average improvement than the best performance of the baselines onùêπ. Without the mask, the average improvement is 38.49%. Another observation is that the baselines commonly have higher PS scores, despite much lower on the PN scores. One possible reason is due to the mechanism of the matching based explanation methods. For an item on the recommendation list, they try to ÓÄõnd well-aligned aspects where the user and the item both perform well. When computing the PS score, the baselines only reserve these well-aligned aspects in the aspect space for all the items, thus the recommended item will highly possibly still stay in the recommendation list, which results in a high PS score. However, when computing the PN score, though the baselines remove all these aspects, the recommended item may still perform better on other aspects compared with other items and thus still remain in the recommendation list, which results in a lower PN score. This also sheds light on why the matching based methods may not discover the true reasons behind the recommendations. SinceùúÜis applied on the hinge lossùêø(ùë†, ùë†)(Eq.(11)and(12)), a largerùúÜemphasizes more on the explanation strength and reduces Table 4: Model-oriented evaluation of the explanations. All numbers in the table are percentage numbers with ‚Äò%‚Äô omitted. MTER is not suitable for the model-oriented evaluation and the reason can be found in Section 6.2. CountER (w/ mask) 56.73 62.03 59.26 70.11 54.71 61.46 35.39 46.91 40.34 75.17 49.18 59.46 58.52 52.56 55.38 CountER (w/ mask) 77.96 89.26 83.23 86.62 91.78 89.13 60.70 80.10 69.06 72.47 67.72 70.01 96.73 94.39 95.55 Figure 2: Ablation Studies. (a) Change of ùêπscore, explanation complexity and ÓÄõdelity w.r.t. ùúÜ. ( b) Change of explanation strength and ùêπscore w.r.t. ùúÜ. (c) Average explanation complexity/strength for items at diÓÄùerent positions. (d) Relationship between ùêπscore with explanation complexity/strength. (e) Distribution of ùëÉùëÅ the importance of complexity. As shown in Figure 2(a), whenùúÜ increases, the model successfully generates explanations for more items, but the explanation complexity also increases because more aspects are needed to explain those ‚Äúhard‚Äù items. However, the higherùúÜis, the worse our model performs on the user-oriented evaluation. Besides, Figure 2(b) shows the explanation strength does not change withùúÜ. This is because theùõºin the hinge loss controls the required margin on the ranking score to ÓÄûip a recommendation. Additionally, we notice that the model-oriented performance is also independent to ùúÜ, which is the same as the explanation strength. Based on the above results, we hypothesize that the user-oriented performance may be related to the explanation complexity, while the model-oriented performance may be related to the explanation strength. This hypothesis is justiÓÄõed in Section 6.6. In this section, we study the Explanation Complexity and Explanation Strength. In Section 4.4.1, we discussed that the diÓÄúculty of removing the items in the top-ùêælist are diÓÄùerent based on their positions. Figure 2(c) shows the mean complexity and strength of the explanations for items at diÓÄùerent positions (i.e., from the ÓÄõrst to the ÓÄõfth). Generally speaking, it requires 1.59 more aspects to remove the items at the ÓÄõrst position from the recommendation list than the items at the ÓÄõfth position. Because they require larger change to be removed. This is in line with the fact that the strongly recommended items have more reasons to be recommended. In Figure 2(d), we illustrate the relationship betweenùêπscore and explanation complexity/strength. The scale of the marker points represent how large theùêπscore is. It shows that the user-oriented evaluation scoreùêπdecreases as the complexity increases, meanwhile, ùêπis relatively independent from the explanation strength. On the contrary, in Figure 2(e), we plot the distribution of the explanations that are both necessary and suÓÄúcient (i.e.,PN‚àß PS=1), as well as the distribution of explanations that are either unnecessary or insuÓÄúcient (i.e.,PN‚àß PS=0). We can see that as the explanation strength increases, more and more portion of the explanations are both necessary and suÓÄúcient. However, this tends to be irrelevant with the complexity of the explanations. These observations are important because: 1) They indicate that the Explanation Complexity and Explanation Strength are two orthogonal concepts. Both of them are very important since the complexity is related to the coverage on the user‚Äôs preference and the strength is related to the model‚Äôs mechanism. 2) It legitimatizes the Occam‚Äôs Razor Principle and further justiÓÄões the motivation of our CountER model, which is to extract both simple (low complexity) and eÓÄùective (high strength) explanations for recommendations. In this paper, we proposed CountER, a counterfactual explainable recommendation framework, which generates explanations based on counterfactual changes on item aspects. Counterfactual reasoning is still in early stages for explainable recommendation, which has a lot of room for further explorations. For instance, CountER only explored changes on the item aspects. However, we can also explore counterfactual changes on other various forms of information such as images and textual descriptions. The essential idea of CountER is also suitable for explainable decision making over knowledge graphs or graph neural networks, which are very promising directions to explore in the future. We appreciate the valuable feedback and suggestions of the reviewers. This work was supported in part by NSF IIS-1910154 and IIS-2007907. Any opinions, ÓÄõndings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reÓÄûect those of the sponsors.