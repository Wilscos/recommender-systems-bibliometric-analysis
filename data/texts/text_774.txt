An edge system typically consists of a central hub and geographically distributed edges. The reason for having geographically distributed edges is to reduce response times between users and a web-service such as a reinforcement learning (RL) recommendation system. The beneﬁt of faster response times comes at a cost, both for training and execution: one can no longer deploy centralized machine learning algorithms. If one naively trains independent models using local edge data, suboptimal solutions will be computed. Even if one managed to train a consistent global model, which could be updated on a regular basis (for example, every night), execution would still be a problem, since network latencies vary and temporary outages (for example, a server going oﬄine) can occur. In real time when the algorithm needs to choose an action, it may need to consider dynamic or historical features of assets from all the edges. For example, Most reinforcement learning (RL) recommendation systems designed for edge computing must either synchronize during recommendation selection or depend on an unprincipled patchwork collection of algorithms. In this work, we build on asynchronous coagent policy gradient algorithms (Kostas et al., 2020) to propose a principled solution to this problem. The class of algorithms that we propose can be distributed over the internet and run asynchronously and in real-time. When a given edge fails to respond to a request for data with suﬃcient speed, this is not a problem; the algorithm is designed to function and learn in the edge setting, and network issues are part of this setting. The result is a principled, theoretically grounded RL algorithm designed to be distributed in and learn in this asynchronous environment. In this work, we describe this algorithm and a proposed class of architectures in detail, and demonstrate that they work well in practice in the asynchronous setting, even as the network quality degrades. to recommend an item at the current edge the algorithm may need to know recent historical performance of the item from other edges. Another possibility is that all documents may not reside on all edges; for example, to recommend a document at the current edge, the algorithm may need to send a signal to an algorithm component residing on another edge, and then receive relevant document information from that remote component. This work proposes the use of asynchronous coagent networks (Kostas et al., 2020), a type of stochastic neural network for reinforcement learning (RL), to provide recommendations while distributed over the internet. These networks provide principled, theoretically-ground learning rules for training in the distributed, edge-compatible, asynchronous recommender systems setting, where outages and variable or extreme latencies may occur. The use of machine learning to optimize recommender systems without expert human supervision dates back at least to the work of Joachims (2002), who proposed a support vector machine algorithm to optimize clickthrough rates for search engine results. Many works have studied the RL recommendation problem. Choi et al. (2018) use biclustering to reduce the dimensionalities of the state and action spaces, thus making the problem more tractable for RL algorithms. Chen et al. (2019) propose an RL-based recommender system that scales to a large action space and that corrects for bias induced by learning from data collected based on recommendations from an earlier recommender system. Theocharous et al. (2009) study a partially-observable setting in which an algorithm must teach human students a task; they propose learning expert policies for diﬀerent settings, and a switching policy that learns when to switch between these experts. Ie et al. (2019) also study the RL recommendation problem, with a particular focus on the setting in which an algorithm should consider the long-term eﬀects of its actions. They assume that they are given the user-choice model (the model that describes how a given user will interact with a set of recommendations), and propose an algorithm, SlateQ, which makes the problem tractible under certain assumptions. Zhao et al. (2018) study the recommender systems setting in which users can provide real-time feedback. They propose an actor-critic-like learning algorithm and deep recurrent architecture to solve the problem. Theocharous et al. (2018) develop a posterior sampling for reinforcement learning algorithm that is eﬀective at learning to give personalized recommendations. They prove a regret bound and empirically show that the algorithm is eﬀective. Other works study bandit problems (settings where the myopic policy is optimial) and ranking problems. Swaminathan et al. (2017) study the recommendation problem in the contextual bandit setting. They propose the pseudoinverse estimator, which estimates the performance of a policy, thus providing a method for evaluating a policy using oﬀ-policy data. Ai et al. (2018) propose a recurrent architecture and algorithm to reﬁne and re-rank the results from another ranking algorithm. Bello et al. (2018) study the ranking problem, and formalize it as a sequence prediction problem. They incorporate pointer networks, which are a model specialized for the ranking problem, into an algorithm designed to solve the problem. Jiang et al. (2018) point out that many popular recommender algorithms employ greedy ranking; these algorithms cannot account for biases in a slate layout and in interactions between recommendations. They propose an algorithm that uses conditional generative modeling to directly generate an entire slate of recommendations, so as to correctly account for these biases and interactions. Rhuggenaath et al. (2020) study the slate bandit setting in which rewards are not a simple function of the individual components of the slate. In other words, the authors eliminate common assumptions about the reward which make the combinatorial slate problem more feasible. They make an independence assumption about the structure of the reward function, and use this assumption to propose an algorithm with sub-linear regret with respect to the time horizon. However, none of these works address the recommender setting which we study: the setting in which the algorithm and/or data itself must be distributed and run in an asynchronous manner. Federated learning (Qi et al., 2021) is distributed RL, but in an entirely diﬀerent sense of the word “distributed” than in this paper; Section B of the supplementary material discusses this distinction in more detail. We study the setting in which the recommendation problem is a Markov decision process (MDP). Some problems in our setting are contextual bandits, which are MDPs with only one timestep per episode. We denote the state and action spaces respectively. The transition function, of transition to a state, given a state and action: is parameterized by maximize the objective where “given Policy gradient algorithms are a popular class of RL algorithms that aim to maximize the objective by performing stochastic gradient ascent using estimates of the objective’s derivative, ∇J(θ). We propose a uniﬁed distributed training and execution approach based on asynchronous coagent networks (Kostas et al., 2020). Coagent networks are a type of asynchronous stochastic neural network that are comprised of conjugate agents, or coagents, operating asynchronously in continuous time. Each coagent is an RL algorithm learning and acting cooperatively with the other coagents Arespectively. We denote the timestep ast. The random variables ,A∈ A, andR∈ Rdenote the state, action, and reward at timet, a). We write the reward discount parameter asγ ∈[0,1]. The agent θ” means that an agent parameterized byθchooses the actions. in its network. During a coagent’s execution, it takes as input the state of the environment and the outputs from the other coagents (not necessarily the whole state space or the outputs of all other coagents, it could take a small subset of the union of these spaces), and computes some output, which the coagent continuously outputs until its next execution. The output of the coagent is fed into other coagents and/or the network’s output (i.e., action). A coagent’s probability (mass and/or density) of updating at any time can be a deterministic or stochastic function of its input and the state of the environment. For example, one coagent might be designed to execute at 10 Hz, another coagent might be designed to execute each microsecond with a probability of 10 might execute at times based on some (stochastic or deterministic) function of the environment’s state and/or the outputs of the coagents. During training, each coagent computes a local gradient; the gradient rules are straightforward to derive (Kostas et al., 2020). The asynchronous coagent policy gradient theorem states that if each coagent updates its parameters using its local gradient, then the entire network will be updated as if the global policy gradient was computed. The theory holds for asynchronous networks where the units in the neural network do not execute simultaneously or at the same rate. See Section 4.2 for an example of this class of algorithm, and see Section A of the supplementary material for more intuition about the central result that enables the class of asynchronous recommender algorithms that we propose. The asynchronous nature of training and execution makes the approach natural for distributed implementations and particularly for our edge setting. We propose an architecture in which every edge has its own coagents that compute recommendations scores for each of its items. In addition, each edge has a coagent that decides how to combine the computations of adjacent edges. The learning algorithm accounts for all network delays in a theoretically-grounded way. For example, it will account for the situation where it had to give a suboptimal recommendation due to a delay and how that delay and suboptimal recommendation should inﬂuence the follow-up recommendation. Figure 1 shows a high-level view of the architecture we propose. Figure 2 demonstrates how this architecture can be distributed in the edge setting; see the captions for more details. In this setting, time is continuous: no global syncing of the algorithm is ever necessary for action selection. Each component of the network (coagent) simply executes (computes its action) whenever a new signal reaches it. The following update describes a simple example asynchronous coagent Figure 1: A high-level view of our recommendation architecture. The small empty square represents a coagent or group of coagents. At a given time-step, for each document, the network takes user attributes (including the query, if applicable) and document attributes as inputs. These observations are processed, and a scalar or vector is computed for the document. The parameters used to generate each document’s scalar or vector are the same (that is, the parameters are shared across the network). The recommendation generation component takes these scalars as input (one per document) and outputs a recommendation. The recommendation generation process may be a simple ﬁxed algorithm: a sorting of scalars (one per document) may be eﬀective if the user choice model can be assumed to be a cascade (Craswell et al., 2008) or logit model (Ie et al., 2019). A softmax could be used (similar to the approach of Chen et al. (2019)) for more eﬃcient exploration. Figure 2: Each user/document computation may take place on a diﬀerent edge, and the ﬁnal computation which computes the recommendation takes the results and computes the action (recommendation) on the local edge. Connections for the local edge and edge 2 are visualized as arrows; connections for edge the hub are not visualized, but the process is the same as for edge 2. Since some results may not arrive until after the recommendations must be displayed to the user (in other words, at any given time, the algorithm may not be able to communicate completely with itself and/or may not be able to access some documents), the algorithm must learn to achieve the best result it can for each recommendation, given the limitations of the edge setting. learning algorithm based on REINFORCE (Williams, 1992). Let Θ feasible set for coagent size, and timet ∈ R the integers, as is typical in RL, but instead takes a value in the reals). Let Ube the output space of the coagents). Let the random variable of interactions with the environment. Let x ∈ X . . ., and environment interactions is: In practice, a more sophisticated policy gradient algorithm than this simple variant of REINFORCE may be used. In this section, we give results demonstrating that the algorithm and architecture proposed learn eﬀectively in the asynchronous edge setting. Note that these results are not intended to demonstrate state-of-the-art results for the standard RL recommender-system setting (and, since no other algorithm we are aware of can be used in our asynchronous setting, there is no baseline to compare against). Instead, these results show that the proposed algorithm can learn and function eﬀectively in this setting. All experiments were conducted using a set of simulators developed from the MSLR-WEB10K dataset (Qin and Liu, 2013). See Section C of the supplementary material for simulator details. All learning curves are plotted from 30 runs (i.e., 30 trials), and all error bars show the standard deviation (between runs). See Section D of the supplementary material for more plot details. In this section, we demonstrate that the algorithm can function in the asynchronous edge setting, as described above. We created diﬀerent versions of the simulator based on an unreliability parameter. An unreliability of 0 that the network is fully synchronous, and an unreliability of 1 Gbe the return (that is, the cumulative discounted reward) up to (notice that because time is continuous,tdoes not take a value in coagent (a subset ofS × U× U× · · · × U, wheremis the number of , and the random variableU∈ Ube the action (output) of the coagent at . Letndenote the number of times a coagent executes during a sequence coagent, which gives the probability of an outputu ∈ Ugiven an input in and a set of parameters inθ∈Θ:π(x, u, θ):=Pr(U=u|X=x, θ). , t, . . . , tbe the times of the coagent’s ﬁrst execution, second execution, nexecution. The update each coagent will follow after a sequence of Figure 3: Asynchronous edge setting results. These demonstrate that the algorithm can continue to learn eﬀective recommendation strategies in an increasingly asynchronous edge setting. Note that these results do not indicate that the algorithm is performing worse given more unreliability. Rather, as the unreliability increases, the environment is becoming more challenging, degrading the agent’s ability to access documents and to communicate with itself; even an optimal policy would have progressively lower return as unreliability increases. These results demonstrate that our algorithm can learn eﬀective recommendation strategies even in harsh, extremely asynchronous conditions. Figure 4: RL results. Similar to the results above, these results demonstrate that the algorithm can learn eﬀective recommendation strategies in an increasingly asynchronous edge setting. the network is fully asynchronous (which would make communication between edges, and thus recommendations, impossible). More generally, an unreliability compute a scalar in time to respond to the local edge before it must display results to the user (and so that document will be unavailable to the algorithm for that timestep). The results are shown in Figure 3; they demonstrate that the algorithm can continue to learn eﬀective recommendation strategies in an increasingly harsh asynchronous edge setting. Next, we added a temporal aspect to the simulation; see Section C.1 of the supplemental material for details. The results are displayed in Figure 4. These results demonstrate that the algorithm learns eﬀective recommendation strategies in this setting even as network quality degrades. As in Section 5.1, note that the results do not indicate that the algorithm is getting worse in more asynchronous settings. Rather, the problem and environment are becoming more diﬃcult as the problem becomes more asynchronous (such that even the optimal policy would result in lower returns), and the algorithm is learning eﬀective recommendation strategies despite this fact. In this paper, we propose a distributed class of RL recommender algorithms and architectures based on asynchronous coagent networks. This class of algorithms [0,1] means that, with probabilityp, each document/query pair will not is designed to learn and function in an asynchronous edge recommender setting using principled and theoretically-grounded learning rules. Using simulations based on real-world data, we show that this approach works well in practice, even when the asynchronous edge setting interferes with the ability of the algorithm to access documents and to communicate with itself.