University of Waterloo 200 University Ave W, Waterloo, ON N2L 3G1 CA University of Alberta 116 Street and 85 Avenue, Edmonton, AB T6G 2R3 CA University of Waterloo 200 University Ave W, Waterloo, ON N2L 3G1 CA University of Waterloo 200 University Ave W, Waterloo, ON N2L 3G1 CA Reinforcement learning (RL) research is growing and expanding rapidly, however, this method still ﬁnds only limited applications in practical real-world settings (Dulac-Arnold et al., 2021). One major reason for this is that RL algorithms typically have high sample complexity and can learn effective policies only after experiencing millions of data samples in simulation (Kakade, 2003). Multi-agent reinforcement learning (MARL) extends RL to domains where more than one agent learn In the last decade, there have been signiﬁcant advances in multi-agent reinforcement learning (MARL) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. However, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. An interesting question which arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. In this paper, we provide a principled framework for incorporating action recommendations from online suboptimal advisors in multi-agent settings. We describe the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL) in nonrestrictive general-sum stochastic game environments and present two novelQ-learning based algorithms:ADMIRAL - Decision Making (ADMIRAL-DM) andADMIRAL - Advisor Evaluation (ADMIRAL-AE), which allow us to improve learning by appropriately incorporating advice from an advisor (ADMIRAL-DM), and evaluate the effectiveness of an advisor (ADMIRAL-AE). We analyze the algorithms theoretically and provide ﬁxed-point guarantees regarding their learning in general-sum stochastic games. Furthermore, extensive experiments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors. simultaneously in the environment (Shoham and Leyton-Brown, 2008). Moving from single-agent to multi-agent settings introduces new challenges including non-stationary environments and the curse-of-dimensionality (Hernandez-Leal et al., 2019), while concerns from single-agent RL such as exploration-exploitation trade-offs and sample efﬁciency remain (Yogeswaran and Ponnambalam, 2012). In MARL environments, it has been reported that learning complex tasks from scratch is even impractical due to its poor sample complexity (Silva and Costa, 2019). In this regard, it becomes necessary for agents to obtain guidance from an external source to have any possibility of scaling up to real-world domains. Further, during the early stages of learning, the policy is quite random and dangerous, which makes it almost impossible to use them in real-world environments. Thus, it is hard to improve upon these policies by only using direct interactions with the environment. In this paper, we aim to tackle the problem of improving sample efﬁciency in MARL through the use of other available sources of knowledge, particularly during the early stages of training. been successful in a variety of domains. The advisors provide actions to the agent at different states to bootstrap learning by targeted exploration (Nair et al., 2018). However, the biases of sub-optimal advisors pose a challenge to successful learning (Gao et al., 2018). Further, many of these approaches do not directly extend to multi-agent environments due to the additional complications present in the multi-agent environments. Though learning from external sources of knowledge have been explored in multi-agent settings, many previous works assume the presence of fully optimal experts (Natarajan et al., 2010; Hadﬁeld-Menell et al., 2016; Yu et al., 2019). Generally, they entail additional assumptions such as having simpliﬁed environments with only two agents (Lin et al., 2019) and consider restrictive environments such as being a competitive zero-sum setting (Wang and Klabjan, 2018) or a fully cooperative setting where all agents are speciﬁed to share a common goal (Natarajan et al., 2010; Le et al., 2017; Peng et al., 2020). Additionally, some approaches such as Lin et al. (2019) restrict themselves to simple multi-agent environments with discrete state and action spaces. The use of sub-optimal advisors in multi-agent general-sum settings with an arbitrary number of agents has been less explored, and to the best of our knowledge, there has been no comprehensive analysis of this approach, especially from the theoretical perspective. We would like to provide two motivational examples relevant to the goals of our paper. These examples will help in providing an intuition towards the kind of practical problems where our approach can be used and clarify the potential impact of this line of research. forest ﬁres. Wildﬁre response is a very complicated process that requires systematic planning of important resources and a good understanding of wildﬁre behaviour for proper estimation and combat (Thompson et al., 2018). The ﬁreﬁghters and ﬁre managers need to make many critical decisions related to wildﬁre control, and on many occasions, these decisions could be the difference between life and death (Thompson et al., 2019). Additionally, these decisions have a high ecological impact. This is a multi-agent problem (multiple ﬁre-ﬁghters aim to ﬁght ﬁre) where artiﬁcial learning agents can learn suitable policies to aid in ﬁre-ﬁghting efforts (Nikitin et al., 2019). Machine learning, particularly reinforcement learning, has a huge potential for making an impact in this area but has been underutilized so far (Jain et al., 2020). Notably, in these sustainability-based domains, there is a general paucity of data (Tymstra et al., 2020), since obtaining good quality high-resolution sensor In single-agent RL use of external knowledge sources such as advisors to drive exploration has Motivational Example 1:As a ﬁrst example, consider the scenario of ﬁghting wildland data is expensive and hard. Hence, current state-of-the-art MARL algorithms are incapable of being used in such problems due to poor sample efﬁciency. Notably, current practical ﬁre-ﬁghting efforts use physics-based models (Rothermel, 1972) that help in predicting the spread of ﬁres given its current location and intensity. In spite of being state-of-the-art, these models are very sub-optimal, have low accuracies (Jahdi et al., 2015), and possess other problems like under-prediction bias and lack of generalizability to regions outside North America (Cruz and Alexander, 2010). Thus, we have particular knowledge sources that are not optimal but are still used in practice (particularly due to lack of alternatives). Our work in this paper will enable MARL training to use these physics-based models to speed up learning. The policies that the MARL algorithms will ﬁnally arrive at will have the potential to be better than these physics-based models since the MARL algorithms will also simultaneously learn from data. the competitive domain of product marketing. Multi-agent algorithms have the potential to learn from available data and formulate effective marketing and price management strategies to improve ﬁnancial proﬁt for companies (Ganesh et al., 2019). However, the problem of poor sample efﬁciency prevents the usage of MARL for this problem, since many companies would ﬁnd it difﬁcult to procure sufﬁcient data for MARL training. There are many mathematical marketing models in the literature that typically help companies formulate marketing strategies (Eryigit, 2017), however, these models are sub-optimal, with scope for improvement, especially in adapting to changing trends (Storbacka and Moser, 2020). These models can serve as external knowledge sources that MARL training can leverage to learn good policies. content sources, which we broadly refer to as “advisors” is useful for MARL training. We will formally introduce this problem and study it further in this paper. Imitation learning includes various methods for learning the behaviour of advisors, the simplest being behaviour cloning, where supervised learning is used to mimic the advisor policy. This method dates back to the early 90s, where agents were shown to be successful in copying the behaviour of the demonstrator in autonomous driving tasks such as road following and perception (Pomerleau, 1988, 1991). This method comes with certain theoretical guarantees, where prior works have conducted formal analysis and established that near-optimal advisors are the easiest to imitate, requiring far fewer demonstrations than sub-optimal advisors to achieve the same performance as the advisors being imitated (Syed and Schapire, 2010). Behaviour cloning is hard to generalize to different unseen environments since the agent is learning in a supervised fashion (Kiran et al., 2021). Prior works in the area of behaviour cloning have also introduced methods to detect and safeguard against a few noisy/bad demonstrations (Zheng et al., 2014; Hussein et al., 2021), however, in general, the demonstrations are assumed to be near-optimal to enable learning reasonable policies. Further, it has been noticed that behaviour cloning methods are prone to a problem of distribution drift, where the trajectory distribution at the test time drifts away from the distribution learned from the advisor during training (Ross et al., 2011; Rajeswaran et al., 2018). In autonomous driving environments, on-policy data collection has been shown to mitigate this problem to some extent (Santana and Hotz, 2016). Some recent approaches propose off-policy solutions, along with techniques of expanding the input (image)-action space using data-augmentation methods (Codevilla et al., 2018; Laskey et al., Motivational Example 2:Now, we would like to provide a second motivating example in Hence, as elaborated in these real-world use cases, learning from possibly sub-optimal external 2017). However, several other limitations like dataset bias and high variance in neural network-based solutions have been reported to limit the application of behaviour cloning to real-world environments (Codevilla et al., 2019). Russell, 2000), where the objective is to learn the reward function from demonstrations. This framework typically assumes that the environment does not have a reward function and/or it is difﬁcult to formulate good reward functions, but expert demonstrations of good behaviour are easier to obtain. A good example is autonomous driving, where formulating a complete reward function that covers all scenarios is hard while obtaining demonstrations of good driver behaviour is much simpler. Initial approaches to IRL used maximum margin methods where an initial estimate of the reward function for the demonstrator keeps being iteratively improved, such that the performance of the demonstrator is at least more than a “margin” of the previous reward estimate for the demonstrator (Abbeel and Ng, 2004; Ratliff et al., 2006a). This iteration is repeated until no such improvement is possible. The problem with the maximum margin methods are their sensitivity to noise and imperfection in the demonstrator behaviour. To alleviate this problem, probabilistic approaches using principles of maximum entropy have been proposed for the IRL framework (Ziebart et al., 2008; Boularias et al., 2011). These approaches reason over a set of possible behaviours, rather than monotonically improving upon estimates of reward or policies. Neural networks have also been considered to learn a suitable reward function, where convolutional networks aim to map the relationship between input images to ﬁnal rewards (Wulfmeier et al., 2015). Several techniques from supervised learning such as Gaussian processes (Rasmussen, 2003) and ensemble methods (Dietterich, 2000) have also been used in the IRL paradigm (Levine et al., 2011; Ratliff et al., 2006b). A recent approach, Generative Adversarial Imitation Learning (GAIL), aims to recover the policy of the expert directly instead of extracting an explicit reward function using Generative Adversarial Networks (GANs) (Ho and Ermon, 2016). Since GAIL is not learning a reward function, it may not be considered an IRL technique and since it is not learning in a supervised fashion it may not be considered a behaviour cloning technique as well. This approach opened up a new class of methods in the intersection of imitation learning and generative adversarial networks (Miyato et al., 2018; Kueﬂer et al., 2017). Some of these approaches aim to extract an explicit reward function from the demonstrations using GANs (Finn et al., 2016; Fu et al., 2018) and these can be considered to fall within the IRL framework. method. This algorithm has strong guarantees of performance while learning stationary deterministic policies in environments with an online advisor that can be queried interactively for additional feedback. However, the major aim of this algorithm is to obtain a policy that guarantees “no-regret” under its induced distribution of states and does not aim to improve upon the provided demonstrator. This method belongs to a wider set of online algorithms that aim to provide the no-regret guarantee while learning based on demonstrations from a perfect advisor (Hazan et al., 2007; Cesa-Bianchi et al., 2001; Shalev-Shwartz and Kakade, 2008). with the major goal being to copy the policy or behaviour of the experts. Since MARL problems are non-stationary, there is little expectation of obtaining perfect experts. Rather, we expect guidance on action choices that aid agents in learning and improving over time. Further, several multi-agent imitation-based methods in the literature are restricted to cooperative games (Barrett et al., 2017; Bogert and Doshi, 2014) or games with strict restrictions on the nature of the reward function (such Another popular imitation learning framework is inverse reinforcement learning (IRL) (Ng and The DAGGER algorithm introduced by Ross et al. (2011) is yet another imitation learning In general, imitation learning methods assume fully optimal or near-optimal advisors (or experts), as having linear relations to some underlying feature) (Reddy et al., 2012; Waugh et al., 2011), in addition to assuming the availability of (near) optimal experts. On the other hand, other approaches based on IRL in multi-agent settings are restricted to zero-sum competitive games (Lin et al., 2017; Wang and Klabjan, 2018). Some recent approaches aim to apply IRL in general-sum games (Yu et al., 2019; Song et al., 2018), however the assumption of availability of perfect experts are present in these works as well. A different approach from Price and Boutilier (2003) studies imitating more experienced peers in a multi-agent setting. However, this work considers a very restrictive environment, where each agent’s dynamics is independent of other agents. Further, strict assumptions on the reward function exist, such as obtaining the same numerical reward over a part of the state space and having an independent reward function that does not depend on other agents’ actions. Such assumptions are hard to verify in real-world multi-agent environments. Pure imitation methods generally lack the ability to exceed the performance of the available expert/advisor. cloning approach of learning from expert demonstrations and the reinforcement learning-based approach of directly learning from the environment to reach a suitable goal. Here, the objective is not to simply perform imitation learning, but to use imitation learning as a bootstrap mechanism that can speed up the training of RL agents. The RL algorithm enables further ﬁne-tuning of the policy learned from imitation, which provides an opportunity for improving upon the performance of the advisor and learning goal-oriented policies. The algorithms in this approach use the environmental rewards along with expert/advisor demonstrations collected ofﬂine. Unlike the IRL paradigm, the environmental rewards are assumed to be available, and the rewards need not be extracted from suitable expert demonstrations. Our work in this paper is most related to the LfD framework. Early works in this area studied the LfD approach using model-based reinforcement learning, which found applications in classical RL environments like cart-pole (Schaal, 1996) and robot arm learning to balance a pendulum (Atkeson and Schaal, 1997). More recently, the model-free RL approaches gained prominence, especially after the exceptional performance shown by Deep Q-learning (DQN) on Atari games (Mnih et al., 2015). Model-free RL using replay buffers for training have been successful in the LfD framework as well (Piot et al., 2014; Chemali and Lazaric, 2015). One stateof-the-art algorithm, Deep Q-learning from Demonstrations (DQfD) (Hester et al., 2018) pre-trains the agent using demonstration data, keeps this data permanently for training, and combines an imitation-based hinge loss with a temporal difference (TD) error. This additional loss helps DQfD learn faster from demonstrations, but also makes DQfD prone to the problem of overﬁtting to the demonstration data. Normalized Actor-Critic (NAC) (Jing et al., 2020) drops the imitation loss and hence is more robust to imperfect demonstrations (from bad, almost adversarial advisors) than DQfD. However, we ﬁnd that the performance of DQfD is at least as good as NAC for reasonable advisors (due to the imitation loss). Goecks et al. (2020) introduce the Cycle-of-Learning (CoL) algorithm that provides a novel LfD mechanism in which additional human inputs can be obtained during training in environments where humans are present in the loop to help agents train faster. The agents can make use of the human feedback in addition to the demonstration from other sources for training. Notably, LfD algorithms have found numerous applications in robotics. Some early applications have focussed on teaching primitive movements of motors to policy gradient based RL algorithms that can learn to perform a suite of relatively simple robotic tasks, such as manoeuvring gaps (Peters and Schaal, 2008; Theodorou et al., 2010). Recently, Rajeswaran et al. (2018) introduce an effective algorithm that can learn highly complex dexterous manipulation such as object relocation and in-hand manipulation in response to sensor inputs. They introduce an algorithm, Demonstration Another approach, Learning from Demonstrations (LfD), combines the imitation-based behaviour Augmented Policy Gradient (DAPG) that uses an on-policy policy gradient (Sutton et al., 1999) update as opposed to the off-policy nature of prior approaches such as Hester et al. (2018). Zhu et al. (2018) provides yet another approach that uses the LfD technique for robot manipulation tasks such as block lifting, block stacking and pouring liquids, where the agents need to learn effective visuomotor policies that can take actions in response to image inputs from a camera or sensor. The well-known RL algorithm, Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al., 2016) has been adapted to the LfD framework to incorporate human demonstrations and learn continuous control object manipulation robotic tasks such as peg-insertion and clip-insertion in both real and simulated environments (Vecerik et al., 2017). LfD is not a good ﬁt for MARL. In MARL, since the environments are non-stationary with dynamic opponents, real-time action advising would be more useful as the advisors can teach agents to adapt to changing opponents. Recently, some multi-agent approaches have used the LfD method, where the algorithms can in-theory do without fully optimal advisors (Silver et al., 2016; Wang and Klabjan, 2018; Hu et al., 2018), however, these works are applicable only to a restrictive set of MARL environments. The Alpha-Go approach from Silver et al. (2016) and the approach from Wang and Klabjan (2018) are restricted to zero-sum competitive games and cannot naturally extend to general-sum games. The work by Hu et al. (2018) is designed for a very particular application (StarCraft Micromanagement), where the authors require the availability of specialized human-made opponents that contain speciﬁc pre-deﬁned tactics about game-playing. In MARL, prior works have also studied peer-to-peer teaching, where each agent can learn when and what advice needs to be extended to peers in addition to learning how to use the available advice and improve its own learning (Silva et al., 2017; Omidshaﬁei et al., 2019; Ye et al., 2020). The agents can switch between the roles of student or teacher at different points of time based on the situation. However, as evident from the setting, this method is only applicable to fully cooperative environments. but this undermines the convergence guarantees of knowledge to deﬁne a potential function over the state space provides an approach known as potential-based reward shaping, which preserves the total order over policies and does not affect the convergence guarantees (Ng et al., 1999; Wiewiora et al., 2003). The approach of reward shaping has been popular in single-agent RL (Marom and Rosman, 2018), and very recently adopted to the MARL setting as well (Devlin et al., 2011; Gangwani et al., 2020; Xiao et al., 2021). The work by Gangwani et al. (2020) uniformly redistributes the rewards accumulated at the end of a trajectory, to each state-action pair along the length of the trajectory. This approach is based on independent learning, which hurts convergence guarantees in the MARL setting (Tan, 1997). Although it shows good empirical performance in single-agent tasks, this approach performs poorly in many MARL tasks, since the credit-assignment is not always accurate (Xiao et al., 2021). On the other hand, the approach by Xiao et al. (2021) adapts potential-based reward shaping to the MARL setting. There are two important limitations of this potential-based reward shaping approach, formulated by Xiao et al. (2021) in MARL. The ﬁrst is that the shaping advice is a heuristic that needs to come from an expert who has complete prior knowledge about the entire problem domain and is capable of designing these heuristics. Obtaining such experts for complex MARL tasks is not always possible. Second, the shaping advice is provided at the beginning and then ﬁxed for the duration of training. However, MARL requires adaptive advising at different parts of the state based on opponent behaviour. While powerful, the requirement for ofﬂine demonstrations commonly seen in prior works on Expert demonstrations have also been used for reward shaping in single-agent RL (Laud, 2004), summarize limited ofﬂine demonstrations (from sources like humans) into decision tree-based expert rules that boost learning online. Conﬁdence-based human-agent transfer (CHAT) (Wang and Taylor, 2017) improves HAT by adding conﬁdence measurements to safeguard against bad demonstrations. Both these methods demonstrate good performance in a multi-agent “Keep Away” game, although the algorithms themselves are single-agent only. These algorithms are independent methods that consider the other agent(s) as part of the state and do not track opponent actions or perform any kind of opponent modelling. In MARL environments, changes in opponent behaviour play a crucial role in determining the agent rewards (Shoham and Leyton-Brown, 2008). Particularly in competitive environments, these agents are expected to adapt between risk-seeking and risk-averse strategies based on the nature of their opponents (Conitzer and Sandholm, 2003). Without tracking opponent behaviour, this adaptability is not possible, since the differences in opponent behaviour in the same state would not stimulate different responses by the learning agent. until no further iterations are desirable or required (Littman and Moore, 1996). An RL algorithm’s ability to converge to a ﬁxed point provides a clear picture of the goal towards which the algorithm is progressing. We would particularly like to note that many of the prior works referenced here do not contain a theoretical analysis of the learning algorithms that provide conditions for ﬁxed point guarantees regarding learning in MARL environments. Since all these methods involve the presence of external sources in the learning process, it is unknown if previous guarantees in RL convergence extend to these approaches. Without such guarantees, it is unclear whether the algorithms will learn reasonable policies in any generic environment (beyond those considered in the paper) and whether the algorithms will progress towards obtaining a suitable solution for the current problem. Since there are many solutions concepts in multi-agent environments (Shoham and Leyton-Brown, 2008), the kind of solutions that are likely to be obtained by these algorithms are unclear. Some approaches establish the existence of unique solution concepts in the particular model of multi-agent environment considered, yet still lack ﬁxed point guarantees for any RL method and theoretical guarantees of arriving at the established solution concept by any learning algorithm (Yu et al., 2019; Song et al., 2018). algorithms that learn from external sources of knowledge, which hinders their applicability to real-world multi-agent environments. All prior works can be seen to contain one or more of these limitations. 1) Strict assumptions on the quality of advisors, 2) algorithms designed as single-agent based independent methods that consider other agents as simply part of the state in the environment, though the actions of these agents strongly inﬂuence the rewards for the learning agent, 3) ofﬂine advising, where demonstrations are collected and used for training agents ofﬂine, which is not well-suited for MARL due to the adaptive nature of opponents, where real-time feedback is critical, 4) algorithms designed towards a restricted class of MARL environments that are not generally applicable to many other environments, and 5) lack of thorough analysis for the conditions under which theoretical ﬁxed point guarantees can be provided. In this paper, we study advising in MARL under the stochastic game model (Shapley, 1953) and aim to resolve the ﬁve major limitations of prior methods discussed in Section 1.2. We will explore Another group of methods such as Human agent transfer (HAT) (Taylor et al., 2011) aim to At their core, RL (and MARL) algorithms are ﬁxed-point iterative methods that iterate recursively From the above discussion, we can see that there are ﬁve fundamental limitations of existing the use of advisors in multi-agent reinforcement learning (MARL) under general-sum settings, where advisors suggest (possibly sub-optimal) actions to different agents in the environment. The advisors can belong to a broad class of categories, such as pre-trained policies, rule-based systems or other systems that continue to learn and/or adapt during gameplay (this is important to evolve speciﬁc strategies that can respond to opponents in non-stationary MARL environments). Note, we do not hold any assumptions on the quality or type of the advisors themselves. The advisors are assumed to be available online so that agents can get real-time feedback while training in dynamic MARL environments. We will also assume that each agent will have access to at most one advisor. Communication between the agent and the advisor is assumed to be free. The advisor receives the state of an agent and provides an action recommendation for the current state. These action recommendations can be deterministic or stochastic. Reinforcement Agents algorithms (Watkins and Dayan, 1992). The ﬁrst algorithm, forcement Agents - Decision Making advisor-guidance, while the second, sor Evaluation advisor in the current MARL context. To the best of our knowledge, we are the ﬁrst to propose a method to evaluate a knowledge source before using it for learning in MARL. We will empirically study the performance of the proposed algorithms in suitable test-beds, along with a comparison to related baselines. Theoretically, we will establish conditions under which we can provide ﬁxed-point guarantees regarding the learning of our ADMIRAL algorithms in general-sum stochastic games. from external advisors in MARL, 2) analyzing two important challenges in learning from advisors in MARL, 3) presenting a suitable algorithm for each of these challenges, 4) establishing conditions for appropriate ﬁxed point guarantees in these algorithms, 5) proving that it is possible to provide convergence results under less restrictive assumptions compared to prior work, and 6) empirically showing that our algorithm can adapt and perform well in many challenges in MARL. In this section, we present the key concepts used in this paper. We will start with a brief introduction to single-agent RL before describing a generalized multi-agent RL setting we use in this paper. Deﬁnition 1. states, transition function and 0 ≤ γ < 1 is the discount factor. and transitions to state π : S 7→ ∆A (the notation sum of future rewards, of following some policyP the optimal policy, v(s, π We introduce a principled framework of studying the problem ofADvising Multiple Intelligent Speciﬁcally, our contributions in this work are: 1) introducing a general paradigm for learning Ais a set of actions,R : S × A 7→ Ris the reward function,T : S × A × S 7→ [0, 1]is the Given an MDP, it is assumed that the agent starts in some states ∈ S, takes some actiona ∈ A, γE[r|s= s, π]whereris the reward collected at timet. The objective of the agent is to ﬁnd ). An alternative approach is to use Q-values. A Q-value,Q(s, a), of a state-action pair, is the expected future reward estimate that can be obtained from taking action the policy Q(s, a) is the optimal action-value function that returns the maximum Q-value in all the states. the other agents. Generalizations of MDPs, called stochastic games, are used to model multi-agent settings and form the basis of MARL. Deﬁnition 2. is the ﬁnite set of agents, ﬁnite action set of an agent action is the set of reward functions, where is the discount factor (0 ≤ β < 1). (where the stochastic game (Shapley, 1953). The environment provides this (global) state to each agent participating in the stochastic game. At each time step current state i). Subsequently, the agent obtains a reward aof all agents in the environment at state environment to the next state, (a) of all agents in the environment. Further, the transition function is ﬁxed and satisﬁes the constraint,P byπ = (π setting, each individual agent tries to maximize their value function. However, this optimization depends on the policies of others: v literature. The general-sum model is the most general formulation, where the rewards that an agent receive at any time step can be related to the rewards obtained by other agents in an arbitrary fashion. Special cases of general-sum games are zero-sum games that restrict the sum of rewards obtained by all the agents at any time step to be zero, and identical interest coordination games that require all agents in the environment to obtain the same numerical reward at every time step. We will use the general-sum formulation in this work. select an individual action and then receive some (possibly different) payoff, which depends on the joint action taken. This can be formally deﬁned as follows. Deﬁnition 3. agent k’s payoff function, specifying a payoff for agent k for each joint action (a a stable point in the joint policy-space. We ﬁrst formally deﬁne a Nash equilibrium in a stage game, before moving on to the generalization of this concept in stochastic games. We switch terminology slightly and will refer to agents’ strategies to be consistent with the game-theoretic literature, although we can use strategy and policy interchangeably. In particular, an agent’s strategy in a stage game is simply a probability distribution over actions, given the underlying state πfrom there. The optimal policyπis obtained byπ(s) = arg maxQ(s, a)where In multi-agent settings, the optimal policy of an agent may depend on the policies followed by a∈ A. Furthermore,P : S × A × S 7→ [0, 1]is the transition function,R = {R, . . . , R} In a stochastic game, the common assumption is that all agents share the same set of statesS Sis called as the state space), which contains information about all agents participating in P (s|s, a) = 1for alls ∈ Sanda ∈ A. Given a stochastic game, a joint policy is represented , . . . , π), whereπis the stochastic policy followed by an agenti. As in the single-agent There are several formulations of the stochastic game model that have been considered in the It is useful to think of a stochastic game as a multi-period stage game. In a stage game, agents The main solution concept we are interested in is the Nash equilibrium (Nash, 1951), namely be the strategy of agent than k, φ The term of taking speciﬁc actions by an agent. This is multiplied with the value of that action as denoted by Deﬁnition 5. such that for all states s ∈ S and agents i = 1, . . . , n, for all π That is, no agent has incentive to unilaterally change their strategy in a Nash equilibrium. Now, we deﬁne the Nash Q-function (Hu and Wellman, 2003) as follows, Deﬁnition 6. its discounted future rewards when all agents follow a joint Nash equilibrium strategy where joint action starting from s The Finally, we deﬁne an approximate Nash equilibrium concept, an beneﬁts of agents’ deviations from the joint Nash equilibrium strategy. In this section, we introduce the problem of ADvising Multiple Intelligent Reinforcement Agents (ADMIRAL). In our setting, we have a set of agents that can either take an action using their own policy or consult an advisor that provides action recommendations at each time step. Each agent has access to at most one advisor. An advisor can be any external source of knowledge, such as a rule-based agent, a pre-trained policy, or any other system that continue to learn during gameplay. φφMis a scalar value. The product of strategies denotes the product of probabilities . The dimensionality of Mis equal to the action space |A|. For a stochastic game, the strategies for an agent apply to the entire time horizon of the game. ∈ Πwhere Πis the set of strategies available to the agent i. r(s, a)is the immediate one-stage reward of the agentiat statesand the corresponding Q-values of the NashQ-function are denoted as the NashQ-value in Hu and Wellman (2003). ∈ Πand ∀s) The advisor provides an action recommendation (a stochastic sample or deterministic) to an agent it supervises by receiving the current state at a particular time step. The advisor is assumed to be available online with the possibility of providing instantaneous action recommendations to an agent. Further, we consider a centralized training setting, where agents can observe the state and the local actions and rewards of all other agents. Another speciﬁcation is that, in our setting, the advisor and agent communication is free, while the agents cannot communicate amongst themselves. There is no communication amongst the agents themselves since we are studying mixed settings where individual agents may be globally as well as locally competing against other agents in the environment. In some potentially cooperative environments like wildﬁre ﬁghting, communication, even if available, could be very limited since individual agents may be very further apart (Phan and Liu, 2008). However, all agents can receive global inputs from satellite/airborne sensors (Leblon et al., 2012). The centralized setting we consider is similar to that in prior works (Hu and Wellman, 2003). Subsequently, in Section 3.4 we will show that our method can be adapted to the popular centralized training and decentralized execution (Lowe et al., 2017) paradigm, which provides a suitable relaxation of the centralization assumption. algorithmic implementations. The ﬁrst challenge is learning a policy with the help of an advisor. We introduce an algorithm for this challenge, which we call ADvising Multiple Intelligent Reinforcement Agents - Decision Making (ADMIRAL-DM). In this setting, each agent aims to learn a suitable policy that provides the best responses to the opponent(s) and performs effectively in the given multi-agent environment. An agent has access to a (possibly sub-optimal) advisor that could be leveraged to improve the speed of learning. Hence, at each time step, the agent could choose to follow its own policy or that of the advisor. At the earlier stages of learning, the dependence on the advisor is greater, and this dependence gradually reduces in the later stages of learning as the agent’s policy improves. If an agent does not receive an action recommendation at any time step, they could simply use their own current policy. Hence, we do not require the advisor to be capable of providing action advice at every state in the state space. ADvising Multiple Intelligent Reinforcement Agents - Advisor Evaluation (ADMIRAL-AE) that tackles this challenge. Before using an advisor for learning, it is useful to evaluate the available advisor to determine whether the advisor will provide effective advice. Hence, we propose a ‘prelearning’ phase (i.e., a distinct phase before the beginning of training of ADMIRAL-DM) where the ADMIRAL-AE is used with the objective of getting a good understanding of the capabilities of the advisor in the current environment. In this setting, we will assume that a single advisor to be evaluated exists in the system and this advisor could be evaluated by one or more agents. This makes it possible for the agent(s) to determine the value of using the advisor in the concerned environment. being currently used in practice. This method could be normally useful, but may not be suitable for every context and against every possible opponent in MARL environments. Hence, it is possible to face situations in which the available advisor is very good and close to the optimum in some cases, while in some other cases the available advisor is bad (even just random). This is due to the non-stationary nature of multi-agent environments arising from other agents’ changing behaviour. Intuitively, to learn well, agents must listen more to the advisor when the available advisor is good and well-suited to the current context and listen less (or not at all) to the available advisor when it is bad. We would like to brieﬂy recall the ﬁre-ﬁghting example discussed in Section 1.1. For this Particularly, we study two challenges for learning from advisors in MARL and provide the The second challenge is the evaluation of the advisor itself. We provide an algorithm called Many real-world multi-agent application domains may have a state-of-the-art solution method problem, an example of a well-known ﬁre simulator model is the Farsite (Finney, 1998) simulator that is actively being used in practice to model the spread of ﬁre. This ﬁre simulator model predicts the spread of ﬁre in the future and forms the basis of the ﬁre control strategies of ﬁreﬁghters. Notably, through extensive experimentation, it has been reported that this model does not give satisfactory performances under certain environmental conditions such as extreme downslope winds (Zigner et al., 2020). However, since real ﬁres are affected by many dynamic environmental factors at the same time, coming up with suitable heuristics for deciding the ﬁt of the given model is almost impossible. Also, making use of bad ﬁre models for training the MARL algorithms may harm learning with bad advice. This could hurt the overall sample complexity of the algorithm and become counter-productive for our objective. Hence, in this section, we recommend evaluating the advisor before using it for learning, if possible. We motivate the idea of decoupling the problem of “advisor-evaluation” where the objective is to study the suitability of the advisor in the given MARL environment and “decision-making” which aims to improve the training of MARL algorithms by making use of advisor knowledge. the training of the decision-making algorithm. In this phase we perform an “advisor-evaluation” study, either in simulation or in real-world environments (particularly in environments that are not safety-critical like recommendation systems and board games such as chess) that helps to answer two important questions before beginning to learn from the advisor. 1) Does the advisor have some good knowledge of the domain that could be helpful for the MARL algorithms during training? and 2) How much should one listen to the given advisor? Performing advisor evaluation before beginning the process of learning helps the agent gain a good understanding of the advisor before learning from it, which would help in leveraging it effectively in learning a suitable decision-making policy. Most importantly, in MARL, advisors (especially good ones) could continue to learn and/or adapt online, during gameplay, based on the nature of opponents. Such advisors cannot be evaluated effectively unless their learning and evolution is captured using a principled method designed to evaluate them. If advisors are being evaluated by agents along with agents simultaneously using them for learning decision-making policies, the evaluation becomes limited and advisors are prone to be discarded quickly based on the metrics of performance and consistency at the early stages of training. During this time, the advisor could be still evolving its strategies based on the nature of the opponent(s), and hence, the evaluation is not accurate. For example, in the algorithm CHAT from Wang and Taylor (2017), the conﬁdence of an agent on a demonstrator is determined based on the demonstrator’s consistency in action recommendations for the same state. This approach works well in the single-agent context. However, in MARL, due to the adaptive nature of opponents, good advisors could adapt based on the opponent and possibly evolve mixed (stochastic) strategies that will provide different actions at the same state. An approach such as CHAT would reduce the conﬁdence of such an advisor, but this is not accurate since the advisor is good and should be leveraged more for better performance. This is another motivation for our approach of studying “advisor-evaluation” and “decision-making” separately and not combining the two in the same approach. We will start by proposing an algorithm for the most important challenge, which is learning to act in the environment by leveraging the available advisor. As mentioned earlier, we propose to conduct a ‘pre-learning’ phase before the beginning of Algorithm 1 DM) agents in the environment use the same algorithmic steps for learning, as done in prior works (Hu and Wellman, 2003). Subsequently, the same algorithm can be used in other scenarios where different agents use different algorithms for learning as well. Further, as in Hu and Wellman (2003), we assume that all agents maintain a copy of the during training, agents are in a centralized setting and can observe the local actions and rewards of all other agents at each time step. This helps in predicting the actions of opponents needed for providing the best responses. Q(s, a) and actions advisor that it could query during learning. Whenever the agent needs to choose an action, it would do so based on its current the case may be (lines 8–15). The dependence on the advisor’s recommendation and the random exploration is denoted by two hyperparameters, executed and the actions and rewards of the other agents are observed, including the next state For allj,s ∈ S, anda∈ A, letQ(s, a) = 0, wherea = (a, . . . , a). Initialize a value for hyperparameters  and (i.e. value for and ) Deﬁne policy derived fromQto return a random action with probability, advisor suggested action with probability and greedy action with probability 1 − −  For eachj, choose the next greedy action for all other agents from the copy of their respective policies using s. The next greedy actions are chosen using the current observed actions of other agents actions of other agents ADMIRAL-DM is described in Algorithm 1. First, for simplicity, we will assume that all the A learning agent (represented byj) starts with an arbitrary assignment for its initialQ-value . One such assignment would be to setQ(s, a) = 0, for all agentsj, all statess ∈ S (line 6). During training, at each time step, the agent picks the possible next actions of other agents in line 7 using its copy of other agents action based on ADMIRAL-DM algorithm’s policy which chooses a random action and an advisor action with diminishing probabilities, and a greedy action with increasing probabilities, such that it becomes greedy in the limit with inﬁnite exploration (GLIE). Hence, the agent is guaranteed to train without any further advisor inﬂuence after some ﬁnite time the dependence on the advisor’s recommendation is decayed linearly (line 19). In this process, the dependence of an agent is more on the advisor during the earlier stages of learning, when its own policy is quite bad. This dependence gradually reduces as its own policy improves. Now, the Q-values are updated (line 16) following an update scheme given by, where the actions for all the agents at state rate. The other variables have the usual meaning described in Section 2. The algorithm’s steps are repeated continuously until either the convergence, as is commonly seen in practice (Sutton and Barto, 1998). algorithm from Hu and Wellman (2003). At each time step, a learning agent states in the environment be represented by space of the agent entries in the space complexity can be given by version of ADMIRAL-DM is linear in the number of states, polynomial in the number of actions, and exponential in the number of agents, which is the same as the guarantees for the NashQ algorithm in Hu and Wellman (2003). However, in the case of time complexity, ADMIRAL-DM has the same guarantees as given for the space complexity, since in the worst case, each entry in the table needs to be queried before updating a Wellman (2003), which had exponential time complexity in the states and actions, even in the case of a two-player game. This is because the NashQ algorithm has a requirement of determining the Nash equilibrium at each stage game, which has exponential time complexity even for two-player games (Neumann, 1928). We do not have this requirement. Now we come to our second challenge, which is that of evaluating the advisor to determine its nature and its suitability to the given context. As described in the previous sub-section, the ADMIRAL-DM uses an advisor if one exists. In this sub-section, we provide an algorithm (ADMIRAL-AE) that evaluates a potential advisor and helps in guiding the conﬁguration of Algorithm 1 by setting the initial value of good advisors and listen less (or not at all) to bad advisors. The ADMIRAL-AE is used in the ‘pre-learning’ phase discussed earlier, where agent(s) are evaluating the advisor in the context of the given environment and opponents. a = (a, . . . , a)denotes the actions for all agents at statesanda= (a, . . . , a)denotes The ADMIRAL-DM algorithm’s time and space complexity can be compared to the NashQ , . . . , Q), for all statess ∈ S, and all actionsa∈ A, . . . , a∈ A. Let, the total number of Qto be|S||A|. Also, if the learning agent needs to update a total ofn Q-tables, then In this regard, we start with a deﬁnition of an advisor strategy of the given advisor. Algorithm 2 AE) Deﬁnition 8. of n strategies (σ recommendations to an agent in the multi-agent environment, the advisor is assumed to be capable of predicting the actions of other agents as well. All the advisor’s predictions and/or recommendations towards every agent in the environment constitute the advisor solution as in Deﬁnition 8. Here, we do not restrict our setting to environments where the advisor can predict the actions of all agents. The advisor may not be able to predict actions for some agents, wherein these agents can get random or placeholder strategies in the advisor solution formulated in Deﬁnition 8. in the environment using the same algorithmic steps. In each state we can form a stage game agent which will form the advisor solution the stochastic game, having access to the state and the advisor allows an agent to have access to the full advisor solution at every state s ∈ S for all time t. with an arbitrary value of current time step and all agents follow the advisor solution from the next state onward for inﬁnite periods. We deﬁne an action selection scheme (lines 5–12) that chooses to directly use the advisor’s recommendation with probability Choose a greedy actionafrom theQ-function usingsand the observed previous actions of other agents Since the advisor is speciﬁed to be a general model that can receive the state and provide action Similar to our decision-making setting, we ﬁrst provide an algorithm that will have all the agents refers to a vector that contains theQ-values of taking each action in the action space of the j. The advisor receives the state and provides its predictions/recommendation for each agent, Algorithm 2 describes our ADMIRAL-AE algorithm. A representative learning agentjstarts represents a value obtained by the agentjat states, when all agents take the joint actionaat the the following the advisor at the current time step and choosing an action that maximizes the value of following the advisor at later stages for the action selection. We also perform a small percentage of random actions to ensure sufﬁcient exploration of the environment. At each time observes the current state (including itself), the reward it obtains and the new state setting, here we do not require all agents to maintain copies of the updates of other agents and hence the rewards of other agents are not required to be observed by the agent advisor solution (Deﬁnition 8) from the advisor for the next state agent j updates its Q-value as follows (using β ∈ [0, 1), as the discount factor) : where agent lated as solution at state since the value of each agent’s payoff at state Since the advisor recommendation to each agent can be a stochastic sample, the as a vector that contains the probability of taking each action in the action space of from the of taking each action in the action space of using a component-wise multiplication of the advisor solution and the a = (a line 15 of Algorithm 2. The above-described steps continue until convergence, or until the come within a small threshold of convergence, as in ADMIRAL-DM. ADMIRAL-AE using the given advisor in the ‘pre-learning’ phase, the performance of the algorithm can be used to determine heuristic in this paper. We propose that the performance of ADMIRAL-AE (in terms of cumulative rewards) be compared against the maximum possible performance of any algorithm (maximum cumulative rewards). The ADMIRAL-AE’s performance using the given advisor should lie between the maximum possible performance in that environment and the performance of ADMIRAL-AE using a random advisor. This can then be normalized in the range of This normalization is given in Eq. 7. where across multiple trials), random advisor (averaged across multiple trials), reward in the given environment pensate for the loss in performance from random exploration in ADMIRAL-AE (hyperparameter Q-values at the current state with probability1 − η − η. The idea is to mix between directly α∈ (0, 1)is the learning rate. The termAdvisorQ(s), is the total value (payoff) that the jwill obtain at stateswhen all agents (including itself) play the advisor solution. This is calcu- AdvisorQ(s) = σ(s) · · · σ(s)·Q(s), where(σ(s), . . . , σ(s))denotes the advisor Q-function of the agentj, we can obtainQ(s), which is a vector that contains theQ-value , . . . , a), denotes the actions for all the agents at state s. TheQ-values of all agents are updated using the advisor strategies at each iteration, as given in Recall that the primary purpose of this algorithm is advisor evaluation. After implementing CRdenotes the cumulative reward obtained by ADMIRAL-AE using the advisor (averaged of ADMIRAL-DM where its value is linearly decayed in line with the steps in Algorithm 1. We experimentally illustrate these steps later in Section 5.1. A more elaborate study demonstrating the effectiveness of this technique is given in Section 5.4. simulated or baseline opponents (refer to Section 5.2). An important advantage of ADMIRAL-AE is in situations of adapting advisors, as discussed earlier. An experimental illustration of this advantage is given in Appendix C. The ADMIRAL-AE algorithm is off-policy, as the update policy (line 15) and policy followed (lines 5–12) are different. Due to this off-policy nature of ADMIRAL-AE, we do not require an agent to follow the advisor at every state in this setting. The evaluation is happening through the being updated as in any off-policy algorithm. The convergence guarantees in off-policy methods do not require using speciﬁc action selection policies as long as sufﬁcient exploration is guaranteed (Jaakkola et al., 1994; Sutton and Barto, 1998). will be linear in the number of states, polynomial in the number of actions, and exponential in the number of agents, same as that described for ADMIRAL-DM. However, the space and time complexity of ADMIRAL-AE can be represented by not have the product term of the number of agents is due to the fact that ADMIRAL-AE does not have the requirement of each agent maintaining copies of the time complexity of ADMIRAL-AE is much better than that of NashQ (Hu and Wellman, 2003). 3.3 Illustrative example for Algorithm 2 In this sub-section, we show the calculations of some steps in the a single state system, to serve as a demonstration of this algorithm. Our objective is to provide a practical illustration of the various steps involved. Since Algorithm 1 has similarities to the well-known for Algorithm 1. Additionally, since previous works have not considered the idea of evaluation of advisors before using them, we include a numerical example of Algorithm 2 for more clarity. agent) can perform two actions “Up” and “Down” and the second agent (row agent) can also perform two actions “Left” and “Right”. The system has only one state, and discount factor initial state, at time in Table 1a. After obtaining the value offrom ADMIRAL-AE, this hyperparameter is used in the training Another way of using ADMIRAL-AE is to study the effectiveness of the available advisor against In a tabular implementation of the ADMIRAL-AE algorithm, both space and time complexities Q-values of other agents as in ADMIRAL-DM. As described in the previous sub-section, this Let us consider a two agent game with all the initialQ-values set to 0. The ﬁrst agent (column actions “Up” and “Left” respectively. They execute these actions and obtain a reward of 2 each. Now the agents receive the next state, which is state for the column agent be value of the tuple in this notation is the probability of taking the ﬁrst action, and the second value of the tuple is the probability of the second action for the respective agents. This means that the advisor recommends both the agents to perform the “Up” and “Left” actions respectively, with probability 1, and the other action with probability 0. The Q update will be as follows: equation also holds for the row agent and the new stage game with is given in Table 1b. from their respective agents obtain a reward of 2 each. The next state is observed, and let the advisor solution here be for each of the actions for both the agents. Again, we calculate the = 1.8 + 0.9 Table 1c. Similarly, the to a small threshold of convergence. based on the advisor solutions. Such a process, at convergence, will lead to the agent(s) evaluating the advisor obtain a value for the advisor at each state in the state space of the environment. At states, in timet = 0, let us assume that both agents decide to use the advisor recommended (s, Up, Left) = Q(s, Up, Left) + αr+ βAdvisorQ(s) − Q(s, Up, Left) (s, Up, Left) = Q(s, Up, Left) + αr+ βσ(s) · σ(s) · Q(s) − Q(s, Up, Left) (s, Up, Left) = 0 + 0.92 + 0.9 × 0 − 0 (s, Up, Left) = 1.8 The superscript forQrepresents the agent index (1 represents the column player). The above Now, at statesand timet = 1, let us assume that the agents decide to perform the best actions ) = (0.5, 0.5)andσ(s) = (0.5, 0.5). This means that the advisor assigns a probability of 0.5 (s, Up, Left) = Q(s, Up, Left) + αr+ βAdvisorQ(s) − Q(s, Up, Left) (s, Up, Left) = Q(s, Up, Left) + αr+ βσ(s) · σ(s) · Q(s) − Q(s, Up, Left) (s, Up, Left) (s, Up, Left) = 2.34 The above equation also holds for the row agent and the new stage game att = 2is given in In the above steps, we have demonstrated theQ-values of different actions at the given state It is well known that tabular algorithms are not applicable for environments with large state and action spaces in RL (Mnih et al., 2016). All our algorithms can be extended to large state-action space environments using function approximations as is common in RL (Mnih et al., 2015, 2016), where neural networks serve as the function approximators. In this section, we give a neural network-based implementation of ADMIRAL-DM and ADMIRAL-AE. (Mnih et al., 2015) with the update rule in ADMIRAL-DM to obtain its neural network implementation in Algorithm 3. To highlight differences from the tabular implementation, we would like to make note of some parts of Algorithm 3. All agents maintain two networks (evaluation and target) throughout the training process. Both the evaluation and target networks start with the same conﬁguration. The evaluation network is updated periodically at every training step and used for action selection at each step. The target network provides the target value for calculating the Bellman errors during training and is updated less frequently compared to the evaluation network. In line 18 of Algorithm 3, all agents store the experience tuples in their respective replay buffers. After every episode in lines 21 – 25, the evaluation networks are trained using the temporal difference (TD) errors as the loss function. The TD target is obtained from the Bellman equation given in Eq. 5. After every ﬁnite number of training steps, the target network parameters are updated by copying over values from the evaluation network in line 26 as previously performed in Mnih et al. (2015). et al., 2015) with the update rule in ADMIRAL-AE to obtain Algorithm 4. The important changes are similar to our discussion with the ADMIRAL-DM case, where the algorithm uses two networks and a replay buffer for training. The target for the loss function is obtained from Eq. 6 (line 21 in Algorithm 4). forcement Agents - Decision Making (Actor-Critic) algorithm uses the follows a Centralized Training and Decentralized Execution (CTDE) scheme (Lowe et al., 2017), where the critic uses the information associated with other agents during the training time and the actors can act independently without access to other agent information during execution. This allows our methods to be applicable in environments where global information (i.e., information associated with other agents) is available during training but not available during execution. For example, the CTDE scheme has been used for studies related to autonomous driving (Zhou et al., 2020). The CTDE scheme extends our algorithm to partially observable environments, where the actor can just use the local observations of the agent for action selection (during both training and execution), while the critic can use the joint observation of all agents during training. As discussed in Lowe et al. (2017) in the simplest case, the all agents, but it could also include additional state information when available. The critic is not required during execution in this setting. Additionally, ADMIRAL-DM(AC) makes our method applicable to continuous action spaces as well. in Algorithm 5. This algorithm also uses neural networks, like other algorithms described in this sub-section. All agents maintain two networks during training. The ﬁrst network is the value network that serves as a critic, and the second network is a policy network that serves as the actor (line 1). We incorporate techniques introduced in the well-known DeepQ-learning (DQN) algorithm Similar to our approach with ADMIRAL-DM, we incorporate the techniques of DQN (Mnih We also extend Algorithm 3 to an actor-critic method —ADvising Multiple Intelligent Rein- We provide the complete pseudocode for the actor-critic implementation of ADMIRAL-DM Algorithm 3 ADMIRAL-DM Neural Network Implementation InitializeQ, Q, whereφrepresent the evaluation (eval) net andπrepresents the target net, for all j ∈ {1, . . . , n}. Initialize a value for hyperparameters  and (i.e. value for and ) Deﬁne policy derived fromQto return the random actionawith probability, advisor suggested action with probability and greedy action with probability 1 − −  For eachj, choose the next greedy action for all other agents from the copy of their respective policies using s. The next greedy actions are chosen using the current observed actions of other agents other agents a = a, . . . , a; r = r, . . . , rand a= a, . . . , a; for each agent j Update the parameters of the target network for each agent by copying over the eval network every T steps: π←− φ After each experience, the value (critic) network is updated in line 16 using the TD errors (from Eq. 5) as the loss function. The actor is updated in line 17 using policy gradients. Also, since the algorithm is maintaining a stochastic policy which explores naturally, we do not perform a random action selection for exploration in this method (unlike the other algorithms). In this section, we ﬁrst show that the advisor which forms an depend on the nature of the advisor used. Further, we prove that the converges to the Nash Q-value, thus ﬁnding the Nash equilibrium of the stochastic game. game was provided by Hu and Wellman (2003). However, this result relies on a very restrictive assumption that states that every stage game of the stochastic game contains a Nash equilibrium that is either a global optimum or a saddle point. Additionally, an agent must use the payoff at this equilibrium to update its (2000), this assumption implies that every stage game should use the same kind of equilibrium, it for all j ∈ {1, . . . , n}. Initialize the hyperparameters η and η a = a, . . . , aand a= a, . . . , a every T steps: π←− φ The primary convergence result forQ-learning based algorithms in a general-sum stochastic Algorithm 5 ADMIRAL-DM(AC) Neural Network Implementation cannot oscillate between being a global optimum or saddle point between stage games. There is almost no game that satisﬁes this condition in practice (Hu and Wellman, 2003). We will show in this section that the convergence results in our setting can be provided under a set of assumptions weaker than that used by Hu and Wellman (2003). each theorem depend on a set of lemmas that we provide. We have included the statement of these lemmas in this section, while the complete proofs of the lemmas can be found in Appendix A. result, extending a result of Szepesvári and Littman (1999), which will form the foundation of our convergence result in Theorem 2. Towards the same, we restate some formal deﬁnitions relating to translation and invariance of operators from Szepesvári and Littman (1999) in Appendix B to stay self-contained. Theorem 1. X. Let a mapping that associates subsets of T = (T 0 ≤ G point (v hyperparameters  and (i.e. value for and ) gested action with probability and greedy action with probability 1 − −  where a= (a, . . . , a, a, . . . , a) In this section, we provide three important theorems with their detailed proofs. The proofs of We start by providing a general result for stochastic processes. Theorem 1 is a technical T : B −→ B, be an arbitrary operator. LetF ⊆ B, be a subset ofBand letF: F −→ 2be , T, . . .)be a sequence of random operators,TmappingB × BtoB, that approximate vand for initial values fromF(v). Further, assume thatFis invariant underT. Let ∈ F(v), and deﬁneV= T(V, V). If there exist random functions0 ≤ F(x) ≤ 1and (x) ≤ 1satisfying the conditions below with probability 1 (w. p. 1), thenVconverges to a − S) w. p. 1 in the norm of B(X ): where λ ∀x ∈ X S(x) = 0 ∀x. different variables in Theorem 1 to RL. The operators commonly seen in Q-functions and the Q-functions. The variable the ﬁxed point of the These relations will become more explicit in our upcoming results. (1999), there are signiﬁcant differences in our detailed proof arguments which stems from the differences in the nature of our results. First, Szepesvári and Littman (1999) required an inequality condition in condition 2 to hold for all uses an exact equivalence, includes an additional term to capture the difference in the other terms and the constraint on this additional term needs to hold only in the time limit (condition 3). As a consequence, we are restricted to showing convergence to a point close to the ﬁxed point instead of exactly to theorem combined with the other conditions turns which is hard to satisfy or ensure in multi-agent environments. While Hu and Wellman (2003) use a very restrictive assumption to overcome this problem, we show that this problem can be altogether avoided using our Theorem 1 (which is the core motivation for this theorem). We also use an exact equivalence in condition 1 and condition 5 to avoid the contraction condition. The complete proof of Theorem 1 is given next. Proof. and let, 1. For all Uand U∈ Fand all x ∈ X , 2. For all U and V ∈ F, and all x ∈ X , we can ﬁnd a ﬁnite sequence k(x) such that, −→ 0 w. p. 1 as t −→ ∞ and k(x) is ﬁnite for all values of x and t. 3. k(x) converges to a ﬁnite point (independent of time) K(x), as t −→ ∞. 4. For all l > 0, ΠG(x) converges to 0 uniformly in x as n −→ ∞. 5. There exists 0 ≤ γ < 1 such that for all x ∈ X and large enough t, F(x) = γ(1 − G(x)) The pointScan be represented by the equationS(x) =(γC+ K(x)C), ifK(x) 6= 0, , where0 <ˆβ ≤ 1andCis a small positive constant. IfK(x) = 0 ∀x ∈ X, then Before providing the proof of Theorem 1, we aim to provide an intuitive grasp by relating the Fmaps to2). Speciﬁc instances (U, V) ofFare considered. These can be seen as particular Although our proof for Theorem 1 is structurally similar to that from Szepesvári and Littman LetUbe a value function inF(v)and letU= T(U, v). SinceTapproximatesTat converges to T v= vw. p. 1 uniformly over X . Let to a point (independent of t) S, w. p. 1, which implies that V of splitting the sum under the norm is negligible. Regarding the last step, let us denote the term in the Theorem 1 is that ﬁrst case is the theorem will then effectively change to Theorem 1 in Szepesvári and Littman (1999), where the authors prove that δ of time) by keeping a modiﬁed process process, it can be written in the form holds for all to another point, that is and another equivalent formulation of this process that can be obtained by keeping it bounded by scaling. where G holds for all We know that∆(x)converges to 0 becauseUconverges tov. We will show thatδconverges Now from the conditions of Theorem 1 we have, δ(x) = U(x) − V(x) = T(U, v)(x) − T(V, V)(x) = T(U, v)(x) − T(V, v)(x) + T(V, v)(x) − T(V, V)(x) = G(x)(U(x) − V(x)) + F(x)(||v− V|| + λ+ k(x)||v− V ||) = G(x)δ(x) + F(x)(||v− V|| + λ+ k(x)||v− V ||)(12) = G(x)δ(x) + F(x)(||v− U+ U− V|| + λ+ k(x)||v− U+ U− V||) = G(x)δ(x) + F(x)(||δ+ ∆|| + λ+ k(x)(||δ+ ∆||)) ≈ G(x)δ(x) + F(x)(||δ|| + λ+ k(x)||δ|| + ||∆|| + k(x)||∆||) = G(x)δ(x) + F(x)(||δ|| + k(x)||δ|| + ) The (1) comes from the fact that∆is guaranteed to converge to 0 in the limit, so the effect + ||∆|| + k(x)||∆||)byas all these terms converge to 0 in the limit. Another assumption K(x) = 0 ∀x ∈ Xand the second case is whenK(x) 6= 0. In the ﬁrst case, notice that, For the second case, we provide the proof for the processδto converge to a point (independent Similar to Szepesvári and Littman (1999), we will begin by considering a homogenous process Let us consider a process of the form, : B × B −→ B is a homogeneous random function, i.e., bounded by re-scaling, namely the process The homogeneous process is the scaling factor applied. two converging processes. Again, this result will be used later in Lemma 4. Lemma 2. of mappings, and sequences and θ and and t = 0, 1, 2, . . . , then lim converge to some point independent of in the proof of Lemma 4. Lemma 3. Let Z be an arbitrary set and consider the process where is an element in F(z) = γ(1 − G the limit. Then, x that a stochastic process that can be represented by Eq. 12 will converge to a point independent of which is our required result. Now, consider another process, that is obtained from modifying process in Eq. 13 by keeping it We denote the solution of Eq. 13 corresponding to an initial condition ofx= ωand a sequence }byx(ω, ). Similarly, we denote the solution of Eq. 15 corresponding to the initial Next, we state a lemma that provides a relationship between convergence of the sequence (i) y(w, ) converges to a point (independent of t) D (ii) The sequence  converges to 0 in the limit (t −→ ∞) Next, we will state another lemma that provides conditions for the convergence of a cascade of LetXandYbe normed vector spaces,U: X × Y −→ X(t = 0, 1, 2, . . .)be a sequence x= U(x, θ), andy= U(y − t, θ)and suppose thatxandθconverge tox respectively, in the norm of the appropriate spaces. LetLbe the uniform Lipschitz index ofU(x, θ)with respect toθatθand, similarly, letL Lsatisfy the relationsL≤ C(1 − L), andΠL= 0whereC > 0is some constant Now, we would like to show that stochastic processes having certain special structure will x, F, G≥ 0are random processes,||x|| < C < ∞w. p. 1 for someC > 0, andz Having given the previous results, we are now in a position to conclude the proof by showing Lemma 4. Consider an equation of the form where the sequence uniformly in further that a point represented by uniformly over Z. Here follows. The expression for point S is derived in Lemma 4. where be considered a randomized version of the operator E[V Theorem 1. Corollary 1. Assume that the process deﬁned by converges to v such that ﬁnite k Then, the iteration deﬁned by Eq. 18 converges to a point Theorem 1. Proof. form, will converge to T V w. p. 1 where V ∈ B(X ). The convergence is due to Lemma 5 below. Lemma 5. that E[w converges to A w. p. 1. Hence, we have proved that Eq. 10 has converged to a point independent oft, and thus Theorem 1 0 ≤ f(x) ≤ 1is a relaxation parameter and the sequenceP: B(X ) −→ B(X )can (x)] = U(x). Also let,E[PV](x) = T V (x). Now we can state the following corollary to 1. There exist a scalarγsatisfying0 ≤ γ < 1and a sequenceλ≥ 0converging to 0 w. p. 1 ||Pv− PV || = γ||v− V || + λ+ γk(x)||v− V ||holds for allV ∈ B(X )and for (x). 2. k(x) converges to ﬁnite point K(x) as t goes to ∞.P 3. 0 ≤ f(x) ≤ 1, t ≥ 0, andf(x) goes to inﬁnity uniformly in x as n −→ ∞. Here,Pis a randomized version of an operatorT. It can be proved that a process of the LetFbe an increasing sequence ofσ-ﬁelds, let0 ≤ αandwbe random variables such αandwareFmeasurable. Assume that the following hold w. p. 1:E[w|F, α6= 0] = A,PP |F] < B < ∞,α= ∞andα< C < ∞for someB, C > 0. Then the process We know that operator T V Moreover, observe that and 3, it can be readily veriﬁed that coefﬁcients rest of the conditions of Theorem 1, and this yields that the process The ﬁrst two are commonly used in RL (Jaakkola et al., 1994; Singh et al., 2000). Assumption 1. inﬁnitely often, and the reward function for all agents stay bounded. Assumption 3. s ∈ S solutions, but in the limit it is guaranteed to settle down and provide the same advisor strategy for a particular state s ∈ S. the advisor strategies become a constant for a given state. Now, directly, it can be seen that this assumption is much weaker than the restrictive assumption in Hu and Wellman (2003). Firstly, Assumption 3 does not involve the computation of Nash equilibrium of every stage game in the stochastic game. Secondly, agents need not use the Nash equilibrium of every stage game to update their the advisor will need to only suggest the Nash equilibrium at every stage game, greatly reducing the scope of the advisor. Thirdly, the Nash equilibrium at every stage game does not need to be a global optimum or saddle point. This condition in Hu and Wellman (2003) is not satisﬁed in any practical game, which we have relaxed. Lemma 6. condition that, there exists a scalar p. 1, and a ﬁnite sequence Let the random operator sequence T: B(X ) × B(X ) −→ B(X ) be deﬁned by for allV ∈ B(X )by Lemma 5 and the process deﬁned by Eq. 19 converges tovby deﬁnition. Now, we deﬁne Poperator in the context of Algorithm 2. , . . . , PQ), where k = (1, . . . , n),sis the state at timet + 1, and(σ(s), . . . , σ(s))is an advisor solution ThePoperator’s value depends on the advisor solution. Next, we will state some assumptions. will become stationary in the limit (t −→ ∞). In other words, the advisor can adapt its Assumption 3 allows the advisor to learn and adapt during gameplay. However, in the limit, Q-values unlike the assumption in Hu and Wellman (2003). If this condition were needed, then Next, we state a lemma needed to prove convergence. for all Additionally, Q converges to (Q with Deﬁnition 7. The theorem will be an application of Lemma 6. The bound advisor through its advisor solutions. equilibrium point. However, there can be more than one Nash equilibrium point, in which case, the the Nash equilibrium point of the stochastic game to be unique in Theorem 2 as assumed in prior works (Hu and Wellman, 2003). Theorem 2. bounded distance from the Nash limit (t −→ ∞). The point S is as given in Theorem 1. Proof. We will state a lemma before we begin the proof of the theorem. Lemma 7. For a n-player stochastic game, E[P given three conditions. The condition pertaining to the expectation relationship is satisﬁed by Lemma 7. expressions pertaining to distances between a formal way of determining the distance between any Q-function and the Nash Q-function. where Q Here, Q, and alls ∈ S. Assume further that,k(s)converges to a ﬁnite pointK(s)in the limit. Theorem 2 proves that theQ-updates in Algorithm 2, converges to an-equilibrium consistent As proved by Fink et al. (1964), every stochastic game is guaranteed to have at least one Nash in Theorem 2, could be the NashQ-function of any Nash equilibrium strategy. We do not require The proof of the Theorem 2 is a direct application of Lemma 6, which establishes convergence To satisfy the ﬁrst condition (distances betweenQ-functions), we will begin by providing two Consider two Q functions, Q, Q∈ Q. Then we can get, = (Q, . . . , Q), Qis the Nash Q-function of the agent j. The second expression below pertains to distances under corresponding Poperators. (σ(s), . . . , σ(s))is an advisor solution for the stage game(Q(s), . . . , Q(s))at timetand (s), . . . , σ(s))is an advisor solution for the stage game(Q(s), . . . , Q(s))at timet. Also is the Nash Q-function of agent j. Deﬁnition 9. Since the state the last step. Eq. 27 is satisﬁed for all that expression are guaranteed to be ﬁnite due to Assumption 1. This satisﬁes another condition of Lemma 6. the limit (t −→ ∞). expression, Eq. 26 is being used. The third expression is from Assumption 3. Now, from Eq. 28, we can see that last condition of Lemma 6. equilibrium point reached is an epsilon equilibrium, where the epsilon value can be given by the point in Eq. 28. Here the agents can still unilaterally deviate and possibly obtain an additional payoff consistent with Deﬁnition 7. that this algorithm will converge to a Nash equilibrium under a set of assumptions. Again, these assumptions are weaker than others previously considered in literature. To prove this convergence result for Algorithm 1 we will retain the ﬁrst two assumptions but replace Assumption 3 with two other assumptions. Assumption 4. The algorithm is Greedy in the limit with Inﬁnite Exploration (GLIE). Assumption 5. the stochastic game. the policy to choose greedy actions. Assumption 5 is strong, however, Hu and Wellman (2003) show that similar assumptions are required to prove convergence in theory but not necessary to In the second step, the corresponding reward terms from thePoperators get cancelled from We state the equation in Lemma 6 again in Eq. 27. Now, we can ﬁnd a ﬁnitek(s)such that the To satisfy the last condition of Lemma 6, we rewrite the Eq. 27, to get an expression fork(s)in The ﬁrst expression in Eq. 28 is obtained asλis guaranteed to go to 0 in the limit. In the second Hence, all the conditions of Lemma 6 are satisﬁed and therefore, the processQ= (1 − + α[PQ]converges to a bounded distance from the Nash equilibrium(Q− S). Thus, the S. The expression for this point is as given in Theorem 1 with the value ofKbeing given Now, we move on to providing theoretical guarantees for Algorithm 1. We will show in Theorem 3 Assumption 4 allows the policy to explore, but in the limit (t −→ ∞) this assumption requires observe convergence in practice, which is consistent with our observations as well. Thus, even if such assumptions are violated in practice, convergence is still observed. Further, it is to be noted that Assumption 5 is a weaker condition than the assumption in Hu and Wellman (2003), since it does not require calculating the Nash equilibrium at each stage game and using the same to update the Q-values. Theorem 3. function under Assumptions 1, 2, 4, and 5, in the limit (t −→ ∞). The proof is along the lines of Theorem 1 in Singh et al. (2000), but involves signiﬁcant modiﬁcations to cater to the multi-agent scenario. Proof. of this theorem by stating the ﬁrst lemma. Lemma 8. A random iterative process where properties hold: learning rates α Here assume that and the notation var refers to the variance. Nash operator is deﬁned using the following equation, where stage game representative agent k. Lemma 9. with the ﬁxed point being the Nash Q-value of the game. satisﬁed for some β ∈ [0, 1) and all Q. Here Q drop the agent index in all the expressions for simplicity. The rest of the proof is conducted for the Q-values of a representative agent. Theorem 3 proves that the Q-updates in Algorithm 1 converges to the Nash equilibrium strategies. The proof will involve the usage of two lemmas from previous work. We will start the proof x ∈ X,t = 0, 1, . . . , ∞, converges to zero with probability one (w. p. 1) if the following 1. The set of possible states X is ﬁnite.PP 2.0 ≤ α(x) ≤ 1,α(x) = ∞,α(x) < ∞w. p. 1, where the probability is over the 3. || E{F(x)|P}||≤ K ||∆||+ c, where K ∈ [0, 1) and cconverges to zero w. p. 1. 4. var{F(x)|P} ≤ K(1 + ||∆||), where K is some constant. Pis an increasing sequence ofσ-ﬁelds that includes the past of the process. In particular, we Let us deﬁne a Nash operatorP, consistent with the deﬁnition in Hu and Wellman (2003). The PQ(s, a, . . . , a) = E[r(s, a, . . . , a) + γπ(s) · · · π(s)Q(s)](30) sis the state at timet + 1,(π(s), . . . , π(s))is the Nash equilibrium solution for the Under Assumption 5, the Nash operator as deﬁned in Eq. 30 forms a contraction mapping Now, since thePoperator forms a contraction mapping,||PQ − PQ|| ≤ β||Q − Q||, is We will apply Lemma 8 to show that theQ-values converge to the NashQ-value. We will The ﬁrst two conditions of Lemma 8 is satisﬁed from the assumptions. Comparing Eq. 29 and associated with as the Nash Q-value (Q-values under the Nash Q-function). where We deﬁne We also deﬁne random variables values are condition of Lemma 8. (see the proof in Lemma 10 of Hu and Wellman (2003)). Hence, from Lemma 9, we can show that the (Lemma 7). Here, the norm is the maximum norm on the joint action. Now from Eq. 32, ≤ γ||∆ This satisﬁes the third condition of Lemma 8 if p. 1. operates over the action space of the representative agent. (x) = r+ γv(s) − Q(s, a, . . . , a) + γ[Q(s, a, . . . , a) − v(s)] r+ γv(s) − Q(s, a, . . . , a) + C(s, a, . . . , a) F(s, a, . . . , a) + C(s, a, . . . , a) F(s, a, . . . , a) = F(s, a, . . . , a)=C(s, a, . . . , a) = 0if(s, a) 6= (s, a). Pmeasurable which makes∆andF,Pmeasurable and this satisﬁes the measurability Hu and Wellman (2003) showed thatv(s) , v(s, π, . . . , π) = π(s) · · · π(s)Q(s) E[F]forms a contraction mapping. This can be done using the fact thatE(PQ) = Q Now, we have the following for all t, || E[F(s, a, . . . , a)|P]|| ≤ γ||Q(s, a, . . . , a) − Q(s, a, . . . , a)|| = γ||∆||(33) (s, a, . . . , a)|P]|| ≤ || E[F(s, a, . . . , a)|P]|| + || E[C(s, a, . . . , a)|P]|| || + || E[C(s, a, . . . , a)|P]|| Let us rewrite the deﬁnition of the C, In the second step, we are using the assumption that theQ-value is GLIE. The max operator saddle point. Now, if it is a global optimum, the value of maximizing all the actions in Eq. 35 will lead to the global optimum for all the agents and this will be the Nash payoff, thus leading optima are guaranteed to have the same values (Hu and Wellman, 2003). Alternatively, if the Nash equilibrium is a saddle point, consider a stage game, with saddle point equilibrium payoff, Then, the equilibrium strategy will leave an agent worse off by deﬁnition of a Nash equilibrium. Also, (see Deﬁnition 13 in Hu and Wellman (2003)). Thus, we will get the relation, generality. Hence, the following is also true, is the same in the saddle points and the value would be Nash value if all the agents are being greedy given the strategies of all other agents. Thus we have proved that for all cases the limit. Nash Q-function Q to predict the actions of other agents using an equilibrium calculation are unnecessary. The other agents’ current policy can be used directly instead of the equilibrium calculations. Assumption 5 is strong enough to ensure that such processes converge. Also, Theorem 3 proves convergence without any restrictions on the nature of advisors. They could be sub-optimal or adversarial. Thus, in both the Theorem 2 and Theorem 3, we have proved ﬁxed point guarantees in general-sum stochastic games with considerably weaker assumptions than those used by Hu and Wellman (2003), which provides the state-of-the-art theoretical convergence results for such environments. Further, we would also like to note that Theorem 3 just assumes that the advisor inﬂuence decays to 0 in the limit, and hence is also applicable to learning algorithms without advisors. AE and ADMIRAL-DM) have a suitable ﬁxed point and a guarantee of converging to that ﬁxed point in the limit. This shows that our algorithms are theoretically grounded. In this section we experimentally validate our algorithms, showing their effectiveness in a variety of situations using different testbeds. We also demonstrate superior performance to common baselines previously used in literature. After article acceptance we will release the complete source code for all the experiments. The objective of this section is to provide a simple illustration of the tabular version of our algorithms using different kinds of advisors. According to the Assumption 5, the Nash equilibrium could only be a global optimum or a evaluating to 0 in the limit. A global optimum is always a Nash equilibrium, also all global σσQ(s) ≥ πσQ(s), as deviating from the equilibrium when the others are playing Q(s) ≤ πσQ(s), as in a saddle point, if others deviate the agent should be better off Q(s). Sinceσandπare saddle points, the previous argument holds without the loss of The fourth condition of the Lemma 8 is satisﬁed since we have the reward to be bounded Thus, it follows from Lemma 8 that the process∆converges to 0 and hence,Qconverges to Theorem 3 shows that complicated steps in algorithms such as Nash-Q (Hu and Wellman, 2003) To conclude, from the Theorem 2 and Theorem 3 we see that both our algorithms (i.e., ADMIRALwe control the quality of the advisors. To this end, we use a empirical evaluation, where the red and blue agents are trying to reach a yellow goal, with black pitfalls which the agents must avoid. Figure 1 gives the schematic representation of this domain. This is a cooperative game where the two agents need to learn to coordinate perfectly in reaching the goal together to get the maximum positive rewards. However, the agents are unable to communicate directly in this environment. Thus, the agent has to learn to take the correct actions to reach the goal in relation to the observed behaviour of the other agent. The game terminates if at least one of the agents reaches the goal state or hits a pitfall. The presence of the second agent makes this domain harder to solve. We implemented four rule-based advisors of decreasing quality, from Advisor 1, which provides the best action for each state (relative to the other advisors), to Advisor 4, which makes random suggestions. The state in this game corresponds to the positions (as grid coordinates) of both the agents and the action involves moving to one of the four cardinal directions. A detailed description of the environment, reward function, action selection, and more details on the advisors are in Appendix D. using the different advisors in the Grid Maze domain. In our implementations, both agents play a separate instance of the same algorithm (ADMIRAL-AE with a particular advisor). We consider the cumulative rewards obtained by ADMIRAL-AE with each of the advisors over a period of 2000 episodes of training. Figure 2(a) shows the results as an average of ﬁve runs. Since perfect coordination gives the large positive rewards, we can see from Figure 2(a) that the ADMIRAL-AE implementation with the best advisor (Advisor 1) is able to give the highest performance in the task. The performance progressively degrades from Advisor 1 to Advisor 4. We highlight that the performance of ADMIRAL-AE depends on the quality of the advisor, with the best advisor (Advisor 1) leading to the best overall performance. This also shows that we would ﬁnd most value in learning from Advisor 1, as compared to the other advisors. Thus, from Figure 2(a) we conclude that there is great value in using ADMIRAL-AE when the quality and suitability of advisors are the objective of study. action) obtained from playing ADMIRAL-AE (using the best advisor, Advisor 1) at the end of each episode and the true value function that pertains to the policy of the advisor for the entire stochastic game. We obtain For our ﬁrst experiment, we investigate the performance of ADMIRAL-AE algorithm, where First we conduct the ‘pre-learning’ phase where we study the performance of ADMIRAL-AE Further, we plot the mean square error (MSE) between theQ-values (of every state and joint Figure 2: Experimental ﬁndings using the tabular version of ADMIRAL-AE with different advisors. (a) shows that ADMIRAL-AE with the best advisor (Advisor 1) gives the best overall performance, and ADMIRAL-AE with the worst advisor (Advisor 4) gives the worst overall performance. (b) shows that the MSE between the progressively reduces, and hence ADMIRAL-AE evaluates correctly. All results show an average of ﬁve runs, and they have negligible standard deviation. this value by running trajectories from each state and joint action pair until the end of the episode and calculating the expected discounted sum of rewards obtained. The plot of the MSE is given in Figure 2(b). From this ﬁgure, we see that the MSE approaches close to zero after about 2000 episodes of training, which shows that the ADMIRAL-AE algorithm will correctly evaluate the advisor. Though we chose to use the Advisor 1 for this plot, the MSE for other advisors also show similar behaviour. This result serves as an experimental illustration of the Theorem 2, where we proved that the Q-values will converge to the value of the advisor. all the advisors (from Figure 2(a)) to ﬁnd a value for inﬂuence in ADMIRAL-DM. Recall that this is a major objective of the ‘pre-learning’ phase. We use the Eq. 7 given in Section 3.2. The performance of ADMIRAL-AE using each of the advisors should lie between the maximum possible performance and the performance of ADMIRAL-AE using a random advisor. We normalize the average performances between the range of be used as the initial value of Table 2 is adjusted for random exploration, which is approximately 5% of all actions. Hence, we subtract this portion from the theoretical possible maximum performance of 4000 for this domain. The initial values of From this table, it can be seen that the to be gained in listening to this advisor. On the other hand, Advisor 4 this value is 0, which means there will be no advisor inﬂuence while using this advisor. Sine Advisor 4 is very bad (just random), the agent is better off listening to its own policy than the Advisor 4. Thus, using the values given in Table 2, ADMIRAL-DM would listen more to the good advisor and listen less (or not at all) to the bad advisors, which is the correct approach. In Table 2 we use the average cumulative performances (5 runs) of ADMIRAL-AE along with Figure 3: Experimental ﬁndings on the Grid Maze domain with both agents playing a tabular implementation of ADMIRAL-DM with different advisors with hyperparameter (a) shows that using a tuned value for ADMIRAL-DM with the different advisors. However, better advisors help in getting a relatively better performance. (b) shows that the MSE between the current using Advisor 1, and the Nash runs, and they have negligible standard deviation. (Algorithm 1). We use the same Grid Maze domain along with the same four advisors for this study. In this setting, we have the ADMIRAL-DM algorithm training along with each of the advisors for 2000 episodes. In each implementation, both agents use the same algorithm for training (ADMIRALDM with an advisor) similar to the previous experiments. We set the initial value of the values obtained from ADMIRAL-AE (given in Table 2). As described, since the Advisor 4 is bad, the value of for all the implementations, the advisor inﬂuence through described in the algorithmic steps for ADMIRAL-DM. More details regarding the game conditions and reward functions are given in Appendix D. The results (cumulative rewards) are in Figure 3(a), where we plot the averages of ﬁve experimental runs. We note that learning from the best advisor (Advisor 1) using ADMIRAL-DM obtains the best overall performance, while ADMIRAL-DM with the other advisors requires more episodes to obtain similar performances to that of the Advisor 1. Since Advisor 1 teaches useful strategies, ADMIRAL-DM using Advisor 1 sees a good performance early on in training, even when there have been only limited interactions with the environment. This shows the value of positive inﬂuences from good advisors for improving the sample efﬁciency of MARL algorithms. the of the stochastic game. In Figure 3(b) we plot the MSE between the joint action) of ADMIRAL-DM using the Advisor 1, and the Nash Q-value we construct the Nash policy and obtain the value of this policy by running trajectories from each state and joint action pair till the end of the episode and calculating the expected discounted sum of rewards (similar to obtaining the value of the advisor in the previous experiment). In this environment, the Nash equilibrium strategies will provide the actions of perfect coordination that obtains large positive rewards. The MSE in Figure 3(b) approaches very close to zero after 2000 episodes of training, showing that the in the limit. 5.2 Experimental Results - Function Approximation - ADMIRAL-AE In this sub-section, we present results for our advisor evaluation algorithm (Algorithm 2) on the large state-action Pommerman environment (Resnick et al., 2018). The objective is to conduct the ‘pre-learning’ phase to evaluate a set of advisors and pick a suitable ADMIRAL-DM, which we will study in the upcoming sub-sections. We will use a two-agent version of Pommerman, which we denote as Domain OneVsOne of Pommerman (we will consider another domain of Pommerman shortly). Pommerman is a complex multi-agent domains, with each state containing more than 200 elements describing the position of the board, special features like bombs, and the position of other agents. Each agent can perform 6 actions, which include moving in the grid and laying bombs to kill the opponent. The reward function in Pommerman is quite sparse with the agents getting a +1 for winning the game, -1 for losing or a draw, with nothing in between. This game is general-sum since both agents get -1 for a draw. There is a maximum of 800 steps and the games where there are no winners after 800 steps are declared to be a draw. It is very hard for RL agents to learn good performance in Pommerman due to difﬁculty in balancing the twin goals of the killing of opponent and protecting themselves (Gao et al., 2019). Next, we show an illustration of a tabular implementation of our ADMIRAL-DM algorithm Now, we aim to provide an experimental illustration of the Theorem 3, where we showed that Q-values of an agent following the ADMIRAL-DM algorithm converges to the NashQ-value Figure 4: Analysis of ADMIRAL-AE algorithm on Pommerman (Domain OneVsOne) against DQN. The standard deviation in (a) and (d) are very small (negligible). The best advisor (Advisor 1) makes the agent reach the best overall performance. The performance steadily decreases from Advisor 1 to Advisor 4. All results are averages of 10 experimental runs ((a) and (d) have negligible standard deviations). rithm (Algorithm 4). We will consider four different advisors. The quality of the advisors reduces from Advisor 1 to Advisor 4, where Advisor 1 is the best advisor among the four and Advisor 4 is the worst advisor (mostly just random). The Advisors 1 and 2 have a positive inﬂuence on learning, as they can teach many useful techniques to win the game, while Advisor 4 has a negative inﬂuence on learning. The Advisor 3 is also capable of teaching some useful strategies and in general, is better than a random advisor (Advisor 4). However, it is much worse compared to Advisor 1 or Advisor 2. Refer to Appendix D for the complete details of the advisors and the implementation details of the algorithms used. a full Pommerman game containing a maximum of 800 steps. Each experiment has a DQN (Mnih et al., 2015) agent and an ADMIRAL-AE agent training and competing against each other. The experiments analyze the performance of ADMIRAL-AE with each of the four advisors against the common opponent (DQN). The performance is plotted in Figure 4. We repeat the experiments 10 times and plot the averages and standard deviations. We can see that the best advisor (Advisor 1) easily gives the best performance of the ADMIRAL-AE algorithm obtaining an overall cumulative reward reaching around 60,000 (Figure 4(a)). The second-best advisor (Advisor 2) reaches around For these experiments, we use the neural network implementation of the ADMIRAL-AE algo- We conduct all experimental runs for 100,000 Pommerman games (episodes). Each episode is 35,000 (Figure 4(b)). When the ADMIRAL-AE uses Advisor 3 and Advisor 4, DQN gives a better performance cumulatively than the ADMIRAL-AE algorithm (Figures 4(c) and (d)). The results (Figure 4) show that the ADMIRAL-AE algorithm can distinguish between different quality advisors based on performance. From Figure 4, it is clear that the advisor of choice for learning in this domain should be Advisor 1. This result is obtained by running a separate instance of the ADMIRAL-AE algorithm with each of the advisors. This is consistent with our description of possible ways of evaluating the advisors using the ADMIRAL-AE algorithm in Section 3.2. value for column for maximum possible performance value for 10% random exploration as done in Table 2. The Advisor 1 along with its initial value of ADMIRAL-DM method. Table 3: Finding  5.3 Experimental Results - Function Approximation - ADMIRAL-DM In this sub-section, using a set of experiments, we show that it is possible to relax several restrictions of our problem setting that makes our algorithms more generally applicable. We will use the neural network-based versions of our algorithms (see Section 3.4). times, and we plot the mean and standard deviation of performance. The important elements of our experimental domains are mentioned here, while the complete details of the domains and implementation details of all algorithms are in Appendix D. Neural network implementations of decision-making algorithms (ADMIRAL-DM, ADMIRAL-DM(AC)) are used in this sub-section. The ﬁrst domain we consider is Domain OneVsOne of Pommerman introduced in Section 5.2. Our baselines are DQfD, CHAT, and DQN. We perform 50,000 episodes of training, where the algorithms train against speciﬁc opponents. Each episode is a full Pommerman game (lasting a maximum of 800 steps). All the algorithms relying on demonstrations (DQfD, CHAT, ADMIRAL-DM, and ADMIRAL-DM(AC)) use the Advisor 1 considered in Section 5.2. The probability of using the advisor action ( close to zero at the end of training for both ADMIRAL-DM and ADMIRAL-DM(AC). To provide In Table 3 we tabulate the results and normalize the average performances to obtain a suitable (using Eq. 7). The procedure is the same as that adopted in Section 5.1. We adjust the We perform comparative experiments in three domains. All our experiments are repeated 20 data for ofﬂine pretraining in the case of DQfD, two instances of Advisor 1 is used to play many Pommerman games that generate the required data. The DQfD is pretrained with all of this data, before entering the training phase of our experiments. Figure 5: Pommerman competition against ADMIRAL-DM. The ADMIRAL-DM beats all baselines in the execution phase. In the training phase ADMIRAL-DM(AC) performs better than ADMIRALDM in a head-to-head challenge. games where there is no more training, no further exploration and additionally ADMIRAL-DM and ADMIRAL-DM(AC) play without any advisor inﬂuence. ADMIRAL-DM(AC) is a CTDE technique, which only performs decentralized execution in face-off using the trained actor-network. We plot the cumulative rewards in the training phase (Figure 5 (a), (b), (c), (d)), from which it can be seen that ADMIRAL-DM’s performance is better than the baselines (DQN, DQfD, and CHAT). The face-off plots in Figure 5(e) show that ADMIRAL-DM wins more games on average against all the other baselines, showing its dominance. DQfD relies on pretraining, which is harder in MARL, as the nature of opponents that an agent will face during competition is impossible to determine upfront. The algorithms that use online advisors to give real-time feedback (that captures the changing nature of the opponent) tend to do better. DQfD has also been previously reported to have over-ﬁtting issues (Gao et al., 2018), which is likely to hurt its performance more in multi-agent environments After the training phase, the trained algorithms will enter a face-off competition of 10,000 as compared to single-agent environments. In multi-agent environments, it is more important to be able to generalize to unseen dynamic opponent behaviour, which is different from that seen in pre-collected demonstration data. As discussed previously, CHAT maintains a conﬁdence measure on the advisor, which depends on the advisor’s consistency in action recommendations at different states. In MARL, this measure is not completely reliable, since even good advisors may need to formulate stochastic action recommendations as responses to the opponent. DQN, on the other hand, learns directly from interaction experiences and cannot learn from advisor inputs. This is a disadvantage in environments where external sources of knowledge, such as advisors, are available to be leveraged. Also, since our baselines are independent algorithms (that consider opponents to be part of the state), they lose out to ADMIRAL-DM, which explicitly track opponent action. ADMIRAL-DM loses to ADMIRAL-DM(AC) during training (Figure 5 (d)). Though ADMIRALDM(AC) shows slower learning overall (as it is training both actor and critic), ultimately learns a higher performing policy. One important reason is that the actor-critic method trains a stochastic policy that can explore naturally, whereas the a forced exploration ( each recent experience, while the ADMIRAL-DM has delayed learning using the replay buffer. However, in the face-off, ADMIRAL-DM has an edge over ADMIRAL-DM(AC) (Figure 5(e)), probably due to being centralized. Since the performance of ADMIRAL-DM and ADMIRAL-DM (AC) in the face-off results given in Figure 5(e) are close, we perform a Fischer’s exact test for the average performances to check statistical signiﬁcance. We get p < 0.03 which shows that this result is statistically signiﬁcant (we treat p < 0.05 as statistically signiﬁcant as in common practice). Figure 6: Pommerman competition against ADMIRAL-DM(AC). ADMIRAL-DM(AC) defeats all other baselines in both the training and execution phases. baselines. The ADMIRAL-DM(AC) is explicitly compared to all the baselines in a training and face-off scheme similar to that done with ADMIRAL-DM. To recall, we perform training experiments of 50,000 full Pommerman games and face-off contests of 10,000 games, where the trained agents compete against each other without any further training or advisor inﬂuence. The results are plotted in Figure 6, where the ADMIRAL-DM(AC) can be seen to show better performance than the baselines (in both train and face-off contests). In training, it can be seen that ADMIRAL-DM(AC) dominates all the other baselines by winning around 20,000 – 30,000 games in the total 50,000 games conducted. As observed in the previous experiments, the ADMIRAL-DM(AC) algorithm’s learning is slower than that of the rewards), against the baselines to be lower than that of the corresponding training of ADMIRAL-DM against the baselines in Figure 5. In the face-off contests, the ADMIRAL-DM(AC) algorithm wins more than 50% of the games against all baselines except ADMIRAL-DM. As noted previously, the ADMIRAL-DM algorithm has a slight edge in performance over that of ADMIRAL-DM(AC) in the face-off stage. (Gupta et al., 2017). The experiments with these domains have two phases — training and execution. The algorithms train for 1000 games in the training phase and then enter an execution phase, where they execute the trained policy for 100 games (no further training). We choose to set the value of advisor inﬂuence experiments with Pommerman (since we are using good advisors). In the execution phase, there is no more inﬂuence of the advisor and no further exploratory actions, for all algorithms. learning algorithms, trying to capture 30 evaders moving randomly in the grid-based environment. Rewards have a local structure, where the pursuers participating in the capture of an evader or the pursuers encountering evaders are rewarded individually. The game is general-sum and does not have a global reward structure. The local reward structure helps to tackle the issue of credit assignment, as discussed earlier. We use a trained policy of DQN (trained for 1000 episodes) as the advisor. We plot the reward obtained (averaged per agent) for the training and execution phases in Figures 7(a) and (b). The execution performance bars in Figure 7(b) is the average performance across the 100 execution games. The results show that ADMIRAL-DM has a better performance than all other baselines, including DQN used as the advisor, in both phases. Thus, our algorithm can ultimately outperform the advisor. This environment is highly non-stationary (due to having more number of learning agents), so completely centralized ADMIRAL-DM has an edge over ADMIRAL-DM(AC) which uses decentralized actors. statistical signiﬁcance. Regarding the performances of CHAT and ADMIRAL-DM(AC) we get a value of p < 0.02 and similarly for the performances of ADMIRAL-DM and ADMIRAL-DM(AC) we get a p < 0.02, which shows that both these comparisons are statistically signiﬁcant (we consider p < 0.05 as statistically signiﬁcant). 5 pursuer agents trying to consume food and avoid poison. The actions are continuous-valued thrust that the agents can apply to move in a particular direction and speed. Here, multiple pursuer agents need to work together to consume food. Agents get rewards based on foods captured and punishments based on poison consumed. Rewards have a local structure similar to the Pursuit environment. The Next, we conduct similar experiments with ADMIRAL-DM(AC) that show it outperforms the Next, we use two cooperative domains from the Stanford Intelligent Systems Laboratory (SISL) The ﬁrst SISL environment is a Pursuit environment, that contains 8 pursuers controlled by Since some performances in Figure 7(b) are close, we perform an unpaired 2-sided t-test for Our second SISL environment is the continuous action space Waterworld environment, which has Figure 7: SISL Environments - Training and Execution. The ADMIRAL algorithms gives a better performance than all the baselines in both the phases. Training graphs have been smoothed with a running average of 100. advisor here is a trained proximal policy optimization (PPO) (Schulman et al., 2017) agent. Similar to the Pursuit environment, we plot the performances for both training and execution phases in the Waterworld environment (see Figures 7(c) and (d)). The ADMIRAL-DM(AC) alone is used for these experiments, as the two popular RL algorithms for continuous control, PPO and deep deterministic policy gradients (DDPG) (Lillicrap et al., 2016) as baselines. The results show that ADMIRAL-DM(AC) has better performance than others in both phases (Figure 7(c) and (d)). The ADMIRAL-DM(AC) algorithm has two important advantages over the other baselines here. The ﬁrst advantage is that it is capable of leveraging an advisor and the second advantage is that ADMIRAL-DM(AC) is training in a centralized fashion by tracking the opponent behaviour while the other algorithms are independent methods. Still, ADMIRAL-DM(AC) is decentralized in execution. Notably, the ADMIRAL-DM(AC) algorithm also improves upon PPO, used as the advisor, similar to our observation in the pursuit environment. phases for both algorithms, ADMIRAL-DM and ADMIRAL-DM(AC), as compared to the ﬁnal training performances. This is due to the fact that, at the end of the training, there is still a small amount of exploration and advisor inﬂuence involved (1 % of actions), whereas during execution both these inﬂuences are completely removed, which contributes to a net improvement in performance. In both the above SISL experiments, we see a small improvement in performance in the execution algorithms will make the best use of advisors in multi-agent settings compared to the other state-ofthe-art algorithms. After the advisor inﬂuence completely stops (faceoff and execution phases), the performance of ADMIRAL-DM and ADMIRAL-DM(AC) is better than the others. We have also demonstrated that our methods can be extended to continuous action spaces and work in decentralized environments using the popular CTDE technique. 5.4 Performance of ADMIRAL-DM under the inﬂuence of different advisors In this sub-section, we consider a very different problem compared to the previous sub-section. We would like to study the impact of using the ADMIRAL-AE in a ‘pre-learning’ phase to determine the value of choose to use a different algorithm compared to the baselines considered in the previous sub-section (where the objective was to show better performances of ADMIRAL-DM as compared to these baselines). The algorithm we choose to use as the opponent is Deep Sarsa, which is similar to DQN but uses a “Sarsa-like” (Sutton and Barto, 1998) Bellman update for the our objective in this section is not to show better performances against any baseline (which we have already done in Section 5.3). on different advisors with the common opponent (Deep Sarsa) and shows that ADMIRAL-DM is capable of recovering from bad action advice. All results reported in this section use averages and standard deviations of 20 runs. In the ﬁrst set of experiments, we will use the neural network implementation of ADMIRAL-DM and ADMIRAL-AE on the Domain OneVsOne of Pommerman using the four different advisors introduced in Section 5.2. To recall, Advisor 1 is the best advisor who can give the best action (relative to other advisors) at all states and Advisor 4 is the worst. The quality of advisors reduces from Advisor 1 to Advisor 4. As mentioned, we use a common opponent as an agent playing Deep Sarsa. ‘pre-learning’ phase. To do this, we run a series of training experiments, where we implement ADMIRAL-AE using each of the four advisors against Deep Sarsa. The results are plotted in Figure 8. As observed earlier, the best advisor gives the highest overall performance and the worst advisor gives the least performance. Using these performances the Table 4 (using Eq. 7). It can be seen that the suggested value of and the least (0) for the worst advisor. These values of good advisors and listen less (or not at all) to the bad ones. advisors. We will use four initial values of will correspond to the choice of reﬂected in Table 4. In addition to these four values, we will also consider a value of 0 for considers the performance of ADMIRAL-DM with no advisor inputs. to serve as a baseline. As done previously, the value of experiments. All training is conducted for 100,000 episodes with the advisor inﬂuence ( linearly decayed to 0 at 50,000 episodes, i.e. there is no advisor inﬂuence after 50,000 episodes. Each To summarize, our experimental results show that the ADMIRAL-DM and ADMIRAL-DM(AC) . Towards the same, we would like to use an algorithm to serve as a common opponent. We Further, in this section, we provide some additional experiments that evaluates ADMIRAL-DM In this sub-section, we have two sets of experiments with two different domains of Pommerman. First, we wish to evaluate the given advisors against the performance of Deep Sarsa in the Now, we will run the ADMIRAL-DM algorithm against Deep Sarsa, using each of the four Figure 8: Results in Domain OneVsOne of Pommerman using different advisors with ADMIRAL-AE and Deep Sarsa. The best advisor (Advisor 1) gives the highest overall performance while the worst advisor (Advisor 4) gives the lowest performance. Table 4: Finding an initial value for against Deep Sarsa episode was a complete Pommerman game involving a maximum of 800 steps as in the previous experiments. More details of the game conditions and the advisors can be found in Appendix D. The results showing the performance of ADMIRAL-DM in each of these experiments are plotted in Figure 9. Figure 9: Results in Domain OneVsOne of Pommerman using different advisors with ADMIRALDM and Deep Sarsa. The result plots show the performance of ADMIRAL-DM in the training against Deep Sarsa. The results show that Table 4 show either the best performance or is very close to the best possible performances amongst all the  that ADMIRAL-DM using the good advisors (Advisor 1 and Advisor 2) get the maximum overall performance since the positive inﬂuence from the good advisors helps. However, if the advisor inﬂuence is limited ( increases, we see that the performance of ADMIRAL-DM using the ﬁrst two advisors experiences an improvement (Figures 9(a) and (b)), as expected. Since Advisor 1 is even better than Advisor 2, we see from Figures 9(a) and (b) that ADMIRAL-DM using Advisor 1 clearly shows superior performance to that of Advisor 2 for the highest value of as 0.5), performance using both Advisor 1 and Advisor 2 were comparable since these advisors did not have many opportunities to make an impact. Notably, ADMIRAL-DM using Advisor 1 shows the best performance for ADMIRAL-DM using Advisor 2 almost provides the same performances for the highest values of 0.7 and 0.9. Additional inputs from this advisor are not as useful (compared to Advisor 1), since it is weaker. Hence, a value of  that it is much inferior compared to the other two advisors but still has a limited positive inﬂuence (Figure 9(c)) and the performance using this advisor is considerably better than using no advisor at values. We would like to highlight several observations from these results. Figure 9(a) and (b) show Turning our attention to the performance of ADMIRAL-DM with the third advisor, we ﬁnd Figure 10: Results of ADMIRAL-DM vs Deep Sarsa using the Advisor 4, which is the worst advisor among the ones considered. The plots correspond to the OneVsOne domain. All the ﬁgures show that, ADMIRAL-DM is capable of recovering from bad action advice. Greater inﬂuence of the bad advisor (larger  all ( appreciable improvement in performance. This shows that more inﬂuence of a comparatively less effective advisor does not lead to much improvement in performance. Again, the value suggested by ADMIRAL-AE (0.3), comes very close to the best possible performance with other values of  advisor has a negative inﬂuence on learning and makes ADMIRAL-DM lose out for the ﬁrst few episodes (Figure 9(d)). However, ADMIRAL-DM recovers after the advisor inﬂuence wane in all the cases. As the value of detrimental and a larger number of episodes is required before ADMIRAL-DM can show signs of recovery. Hence, the best value of learning. Again, this was the value obtained for Advisor 4, in Table 4. We present a more elaborate set of results on the experiments with Advisor 4 in Figure 10. Here we show that for all cases of , ADMIRAL-DM is capable of recovering from bad action advising and after a suitable number of episodes, can overtake the performance of Deep Sarsa. However, higher values of learning from the bad advisor very problematic, since ADMIRAL-DM shows signs of recovery but still cannot overtake the cumulative performance of Deep Sarsa even after 300,000 episodes (Fig: 10(c)). = 0). However, while using Advisor 3, we notice that, as the values ofincreases, there is no The performance of ADMIRAL-DM using the last advisor (Advisor 4) is interesting. This Figure 11: Results in Domain TwoVsTwo of Pommerman using different advisors with ADMIRALAE and Deep Sarsa. The best advisor (Advisor 1) gives the highest overall performance while the worst advisor (Advisor 4) gives the lowest performance. The standard deviation of (a) and (d) are negligible. this sub-section. Domain TwoVsTwo is a larger version of Pommerman, where there are a total of four agents, with two of the four belonging to the same team. The state space is much larger than Domain OneVsOne, with each state containing 372 elements. The reward function remains sparse, with the two agents belonging to the winning team getting +1 and the two agents belonging to the losing team getting -1 at the end of the game. In case of a draw, all the agents get -1. In Domain TwoVsTwo, we consider one team of Deep Sarsa and one team of the ADMIRAL-DM. This makes this domain harder than the Domain OneVsOne, as the agents must learn to cooperate amongst the members of the same team and compete against the members of the opponent team to win the game. The Domain TwoVsTwo is a mixed competitive-cooperative domain, which is different from Domain OneVsOne that had only pure competition. We will use the same four advisors as considered before for the Domain TwoVsTwo. Deep Sarsa using the ADMIRAL-AE algorithm. The results are plotted in Figure 11. Again, the best advisor gives the maximum overall performance and the worst advisor gives the minimum performance, as observed earlier in experiments with ADMIRAL-AE. The based on these results in Table 5 (using Eq. 7). These values are used in further experiments using ADMIRAL-DM and Deep Sarsa in the Domain TwoVsTwo of Pommerman. We will now provide a brief description of another domain of Pommerman we consider in Similar to the experiments with Domain OneVsOne, we ﬁrst evaluate the advisors against Figure 12: Results in Domain TwoVsTwo of Pommerman using different advisors with ADMIRALDM and Deep Sarsa. The result plots show the performance of team playing ADMIRAL-DM in training competitions against Deep Sarsa. For this domain too, performance of ADMIRAL-AE in Table 5 show either the best performance or is very close to the best possible performances amongst all the  team of Pommerman agents playing ADMIRAL-DM and each agent in the other team playing Deep Sarsa. We will consider four different initial values of Our next set of experiments will run training in the Domain TwoVsTwo with each agent in one Figure 13: Results of ADMIRAL-DM vs. Deep Sarsa using the Advisor 4 in the TwoVsTwo domain. Similar to the ﬁrst domain, all the ﬁgures show that ADMIRAL-DM is capable of eventually recovering from bad action advice to the value obtained in Table 5), which is the same as our experiments in the OneVsOne domain. In addition to these corresponds to the situation of ADMIRAL-DM learning with no advisor inﬂuence. All training is run for 100,000 episodes, with the value of is no more inﬂuence from advisors for the last 50,000 episodes of training. The results showing the performance of the team playing ADMIRAL-DM (in the competition against a team playing Deep Sarsa) are in Figure 12. The ADMIRAL-DM performances show similar characteristics to that seen in Domain OneVsOne. ADMIRAL-DM using the best advisor (Advisor 1) shows the best performance for all values of ADMIRAL-DM using the best advisor is better in the case of Domain TwoVsTwo as compared to the Domain OneVsOne (earns even higher cumulative rewards). This suggests that a good advisor has a comparatively higher impact when the tasks get harder. This is because there are far more strategies that need to be learned to do well in this domain and the opponent learning from scratch needs more time for learning the hard task. A good advisor, on the other hand, can teach the different strategies needed much faster and provide an early lead for ADMIRAL-DM. Similar observations can be made about the performances of ADMIRAL-DM using Advisor 2 as well (refer Figure 12(b) and Figure 9(b)). (refer Figure 12(a)). Notably, comparing Figure 12(a) and Figure 9(a), the performance of ure 12(c)) as observed in the case of Domain OneVsOne. Comparing Figure 12(c) and Figure 9(c), we note that the best performance of ADMIRAL-DM using the Advisor 3 is better for the Domain TwoVsTwo as compared to Domain OneVsOne, reinforcing our inferences earlier about a higher potential for impact in useful advisors when the tasks get harder. Regarding Advisor 4, the greater negative inﬂuence from this advisor necessitates a longer time for recovery (refer Figure 12(d)). tained from Table 5 gives the best possible performance or comes quite close to the best possible performance, compared to other possible values of ADMIRAL-AE. In Figures 13(a), (b) and (c), ADMIRAL-DM shows signs of recovery for all values better. Now, making a comparison between the Figure 10 and Figure13, we note that, as the negative inﬂuence from the advisor increases (through a higher the case of Domain TwoVsTwo and is greater than that needed in the Domain OneVsOne. As the complexity of the tasks increase, the agents need a lot more time to learn good policies that recovers the loss of performance from bad action advice. This shows that negative inﬂuence from an advisor is more costly in the case of harder MARL tasks/environments as against comparatively simpler environments. concluded that ADMIRAL-DM is capable of recovering from bad advisor recommendations and suitably leveraging other advisors who have some positive inﬂuence on learning. However, if possible, it is best to evaluate the advisor using the ADMIRAL-AE method and obtain a suitable initial value for the hyperparameter that determines the advisor inﬂuence ( faster. tabular domain. We showed that, while using ADMIRAL-AE, the best advisor gives the best overall performance. Further, ADMIRAL-AE provides a suitable value for the hyperparameter when used by ADMIRAL-DM subsequently, provides good performances with different types of advisors. We also provided an experimental illustration of our theoretical convergence results in the case of both ADMIRAL-DM and ADMIRAL-AE. In Section 5.2 we provided an illustration of ADMIRAL-AE in a large environment with neural networks as function approximators. Again, we illustrated that using ADMIRAL-AE with the best advisor provides the best performance amongst other (comparatively worse) advisors. Obtaining an appropriate value for showed that ADMIRAL-DM and ADMIRAL-DM(AC) provide better performances than a set of baselines in Section 5.3. We tested our algorithms in both competitive and cooperative domains, as well as settings with discrete and continuous action spaces. obtain a suitable value for possible, it would be best to use ADMIRAL-AE for determining we illustrated that ADMIRAL-DM is capable of recovering from bad action advice from advisors if appropriate values for  method of ADMIRAL-AE for advisor evaluation in environments having dynamically learning and adapting advisors. We show that a principled method like ADMIRAL-AE would ﬁnd a suitable value For Advisor 3, the performance does not change much with varying values of(refer Fig- Again, from Figure 12, for all the four advisors, we can observe that the value foras obwhile using Advisor 4 for learning. However, regarding Advisor 4, the lesser the value ofthe Hence, from both these sets of experiments on the two domains of Pommerman, it can be To summarize, in Section 5.1 we showed an experimental illustration of our algorithms in a In Section 5.4, we used two Pommerman domains to illustrate that using ADMIRAL-AE to Additionally, in Appendix C we show another important advantage of using the principled for may have a high chance of failure. In this paper, we introduced the problem of learning under the inﬂuence of external advisors in MARL. We provided a principled framework for MARL algorithms learning to use advisors. Using Q-learning based methods, we proposed two MARL algorithms for this problem. We conduct theoretical analyses of these algorithms, establishing conditions under which ﬁxed point guarantees can be provided regarding their learning in general-sum stochastic games. We have proved that previous theoretical results can extend to this setting under a comparatively weaker set of assumptions than previously considered. Empirically, we showed that our algorithms can be scaled to domains with large state-action spaces using traditional function approximators like neural networks. We also introduced an additional actor-critic variant of our ADMIRAL-DM algorithm that can operate under the CTDE paradigm and can learn in environments with continuous action spaces. Our empirical results further established the superiority of our algorithms compared to previous baselines. Also, empirically, we have shown that our methods would be useful in a wide variety of problems and that the algorithms can recover from the inﬂuence of weak/bad advisors during learning. RL (Bignold et al., 2021), MARL provides additional challenges which mean that not all the results and approaches can transfer over directly. We discussed the important problems of directly using the single-agent based methods that learn from external sources in MARL. Additionally, we performed direct comparison experiments to elucidate a few of these problems. Our approach to learning from advisors in MARL may look more complex compared to other single-agent approaches, however, the non-stationarity of the environment makes learning under the inﬂuence of advisors in MARL considerably more challenging. In MARL, quick adaptation to the changing environment is the key to better performance (Littman, 2001). Our approach of using an online advisor is a more appropriate formulation of advisors in MARL, as real-time feedback against non-stationary opponents are critical for learning effective multi-agent policies, as demonstrated in our experiments. quality of the advisor, and no restrictions on the relation between the reward functions of different agents (general-sum). Particularly in MARL, the assumption of very optimal advisors could be strong, since performance depends on the nature of opponents as well. The advisor could be capable of providing good feedback in strategizing against a particular class of opponents and be almost useless against another class of opponents. Additionally, a sub-optimal advisor could be good only in a very narrow portion of the state space, which is still useful for an agent learning from scratch in this environment. By explicitly allowing nonrestrictive applicable than previous methods that make an assumption of completely optimal (or near-optimal) experts to help RL training (Ross et al., 2011; Giusti et al., 2015; Sonabend et al., 2020). ADMIRAL algorithms under the simultaneous inﬂuence of multiple advisors providing conﬂicting demonstrations. This problem has been studied in single-agent RL environments (Li et al., 2019), but not yet in the MARL context. There is an emerging line of work that studies the possibility of multiple agents learning from peers in cooperative MARL settings (Omidshaﬁei et al., 2019). Our paper has the potential to contribute to this line of work as well. Further, in this paper, we when it is presented with a learning advisor, where other methods based on simple heuristics While there is a rich body of literature on the use of external knowledge sources in single-agent Importantly, we consider a very general setting, where we had no restrictions on the type or From the empirical perspective, as future work, we would like to study the performance of provided a simple technique of making use of the evaluation of advisors in a learning algorithm by setting the value of and using the results for learning faster and more effective decision-making policies is left to future work. An observation about ADMIRAL-AE is that, in MARL, the advisors can be used as a way to predict the behaviour of other agents as well, which is not relevant in single-agent settings. In MARL, each agent needs to have the ability to perform accurate opponent modelling, based on its observations, to obtain strong performances (Hernandez-Leal et al., 2019). This is because the reward function and the transition dynamics depends on the joint action at each state. Previous methods have used several techniques, such as using a separate neural network for learning opponent behaviour (He and Boyd-Graber, 2016), learning policy features from raw observations (Hong et al., 2018), and using the agent’s own policy to predict opponent actions (Raileanu et al., 2018). However, many of these methods are computationally expensive and scale poorly with the number of states, actions, and agents. Another possibility for opponent modelling is leveraging an external advisor that can possibly predict opponent behaviour, as done in ADMIRAL-AE, which could be relatively computationally friendly given the availability of an appropriate advisor. This could open up a very interesting research direction in learning from advisors in MARL. gence rates of our algorithms. Additionally, some of our theoretical assumptions for the environmental settings may seem restrictive, however, we assert that this work is the ﬁrst to provide a theoretical foundation for MARL with advisors and that these assumptions are useful in understanding the strengths and limitations of such an approach. Furthermore, we note that the assumptions we make are also made by other works exploring the foundations of MARL, such as Hu and Wellman (2003). In future work, we wish to explore the ramiﬁcations of relaxing some of these assumptions. The theoretical understanding of MARL, in general, is still in its infancy and much more research into MARL theory is required to enhance our understanding of this area (Zhang et al., 2019). In this paper, we restrict the theoretical analysis to tabular settings, which is in line with the state-of-the-art in theoretical analysis of learning in general-sum stochastic games (Zhang et al., 2019). The objective is to provide a theoretical guarantee in the most basic (baseline) setting possible. Using a similar approach to single-agent RL methods that extend the tabular results to the function approximation setting (Carvalho et al., 2020), it would be possible to extend our theoretical results in this paper to the function approximation setting as well. An elaborate theoretical study of this is left to future work. these examples, other domains such as managing natural disasters, controlling disease outbreaks, and learning to play multi-player/multi-team sports would also ﬁnd beneﬁt from our approach. These domains also have state-of-the-art advisor models. These models, while not being optimal, are still used in practice. Due to the inherent poor sample efﬁciency of MARL methods, it becomes critical to use all available sources of knowledge judiciously. Hence, learning from fallible advisors is important for many real-world domains that typically have the structure of multiple agent interaction. is expected to improve the sample complexity of MARL algorithms and is an important step towards making MARL methods usable in real-world environments. We expect that our paper will spark more work in the area of accelerating RL training using other available sources of knowledge (or advisors) and that it will interest a broad community of researchers in the area of RL, game theory, machine learning, and multi-agent systems. From the theoretical perspective, as future work, we would like to fully characterize the conver- Two speciﬁc motivational applications for our work were discussed in Section 1. Similar to The framework we have introduced in this paper on using external information through advisors