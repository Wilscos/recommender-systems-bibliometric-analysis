The increasing use of automated decision-making systems trained with real-world data has raised serious concerns with potential unfairness caused by biased data, learning algorithms, and models. Decisions made by these systems have lasting and diverse effects on different social groups. For example, in predictive policing (Lum and Isaac, 2016), each time, the decisions about the locations of future crimes are made by the predictive models. As a result, the discovered crime rates in different communities might change dynamically and interactively as feedback to the decisions being made by the models. Thus, the error rates of the predictive models for different communities might change over time, and error gaps among communities could possibly exacerbate in the long run. Similar feedback loops could also exist in the applications such as recommender systems (e.g., the user satisfactions from different demographic groups diverge over time), temporal resource allocation systems (e.g., the uneven resource allocation become more skew over one group than the others), etc. This interplay between algorithmic decisions and the heterogeneous reactions caused by the decisions further complicates the analysis of (un)fairness problems in the dynamic environment. Most prior works mainly focus on studying static fairness notions (e.g., demographic parity (Dwork et al., 2012) and equalized odds (Hardt et al., 2016)) in the settings of classiﬁcation (Hardt et al., 2016; Zafar et al., Algorithmic decisions made by machine learning models in high-stakes domains may have lasting impacts over time. Unfortunately, naive applications of standard fairness criterion in static settings over temporal domains may lead to delayed and adverse effects. To understand the dynamics of performance disparity, we study a fairness problem in Markov decision processes (MDPs). Speciﬁcally, we propose return parity, a fairness notion that requires MDPs from different demographic groups that share the same state and action spaces to achieve approximately the same expected time-discounted rewards. We ﬁrst provide a decomposition theorem for return disparity, which decomposes the return disparity of any two MDPs into the distance between group-wise reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions induced by the group policies. Motivated by our decomposition theorem, we propose algorithms to mitigate return disparity via learning a shared group policy with state visitation distributional alignment using integral probability metrics. We conduct experiments to corroborate our results, showing that the proposed algorithm can successfully close the disparity gap while maintaining the performance of policies on two real-world recommender system benchmark datasets. 2017; Zhao and Gordon, 2019; Jiang et al., 2020) and regression (Komiyama et al., 2018; Agarwal et al., 2019; Chzhen et al., 2020; Chi et al., 2021). In a seminal work, Liu et al. (2018) show that somewhat contrary to the common belief, enforcing static fairness constraints could do harm to the minority group even in a one-step feedback model. Motivated by this observation, a line of work aims to extend static fairness notions in the settings of online learning (Blum et al., 2018; Bechavod et al., 2019) and multi-armed bandit (Joseph et al., 2016; Patil et al., 2020; Chen et al., 2020). However, these works do not take into account the interactions between the models and the environment: the decisions made by the models could potentially inﬂuence the state of our environment as well, as demonstrated by the predictive policing example. Other works study the interplay between (static) fairness notions and the population dynamics under simpliﬁed temporal dynamic models (Hu and Chen, 2018; Ensign et al., 2018; Hashimoto et al., 2018; Mouzannar et al., 2019; Zhang et al., 2019; Elzayn et al., 2019; Liu et al., 2020b). However, those simpliﬁed temporal dynamic models explicitly make task-speciﬁc assumptions on temporal dynamics and might not be able to precisely characterize the complex dynamics of a more general changing environment. In this work, we study a fairness problem in Markov decision processes (MDPs) to understand the dynamics of performance disparity in the changing environments, taking into account the feedback loop caused by policies to the environments. Speciﬁcally, we propose return parity, a novel fairness notion that requires MDPs from different demographic groups that share the same state and action spaces to achieve approximately the same expected time-discounted rewards. To the best of our knowledge, our work is the ﬁrst of this kind, in the sense that we study the long-term impact of policy functions in general MDPs. First, we formally show exact return parity cannot always be satisﬁed for any two MDPs and provide sufﬁcient conditions that ensure return parity in terms of transitions, initial distributions, and the reward functions. Next, we derive a decomposition theorem for return disparity that decomposes it into the distance between group-wise reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions induced by the group policies. Motivated by the decomposition theorem, we propose algorithms to mitigate return disparity via learning a shared group policy with state visitation distributional alignment using integral probability metrics. We conduct experiments on two real-world benchmark datasets to corroborate the effectiveness of our proposed algorithms in reducing return disparity. Experimental results demonstrate that our proposed algorithms help to mitigate return disparity while maintaining the performance of policies. Notation and Problem Setup state and action spaces are ﬁnite. are the state space and the action space, respectively; state transition function where function where we assume that the reward function is uniformly bounded, i.e., denote the discount factor. Given a policy state transition under distribution over states under the policy π at time step t is The state visitation distribution (i.e., time-discounted distribution over states) is (1 − γ) isρ( s, a) = µ where learning is to ﬁnd a policy to maximize the value (expected return) under the initial state distribution: Let|S| = m action enforced on the individual could lead to the change of features of the individual. We also assume there are two Markov decision processes that represent two different demographic groups (e.g., male/female, white/non-white) and the two MDPs share the same state space, action space, and discount factor but might differ in initial distributions, transitions, and reward functions. We use the subscript two groups. For example, in our paper, we mainly discuss the setting there are two different demographic groups, but the underlying theory and algorithms could easily be extended to the case with ﬁnite K > 2 groups. Next, we deﬁne approximately the same long-term rewards under a policy: Deﬁnition 2.1 In different scenarios, return parity could have different implications: In predictive policing, return parity asks for approximately equalized prediction accuracy of models over time; In recommender systems, return parity might seek similar users’ satisfaction across different demographic groups. The complex temporal dynamic of the above scenarios could be modeled by MDPs. The goal in our setting is then to ﬁnd two policies that maximize the weighted combination of expected returns of two MDPs respectively while satisfying parity: where λ ∈ [0, 1] represents the proportion of group 0 over the entire population. Integral Probability Metrics probability distributions over the same probability space (Müller, 1997). Formally, given two probability distributions a class of real-valued bounded measurable functions on the space where the distributions are deﬁned on. Different choices of denotes the Lipschitz semi-norm, then F = {f : kf k d( P, Q) becomes maximum mean discrepancy MMD(P, Q). ∑( γP)µand the occupancy measure (i.e., time-discounted distribution over state-action pairs) ( s) = E[∑γr| S= s]and the Q-function asq( s, a) = E[∑γr| S= s, A= a], ris the immediate reward at time stept. With all the notation deﬁned above, the goal of reinforcement η:= E[v( s)] =11 −γr(s, a)ρ( s, a) =11 −γr(s, a)µ( s)π(a | s). and|A| = n. In practice, each states ∈ Smight represent features of an individual and the [v( s)]| ≤ e. PandQ, the IPMs are deﬁned asd( P, Q) = sup|f dP −f dQ|, whereFis In this section, we ﬁrst show that return parity cannot always be satisﬁed between two MDPs that share the same state and action spaces and provide sufﬁcient conditions under which return parity is possible. Then, we provide more insights into return disparity by proving an upper bound of the return gap between two MDPs. We defer all the detailed proofs to Appendix A due to the space limit. Before we provide a rigorous analysis of return disparity in MDPs, it is vital to ask whether the exact return parity is always achievable. The following proposition gives a negative answer to this question: For example, consider two MDPs share two states 0, ∀a ∈ A µ= [1, 0] to satisfy speciﬁc conditions. Proposition 3.2. groups, i.e., r Deﬁne there exists satisﬁed. Proposition 3.2 states that exact return parity can be satisﬁed as long as product between the directions of transition difference vectors d In addition, it is also natural to ask whether it is feasible to maximize the expected returns of the two MDPs simultaneously while achieving (LP) approach for MDPs (De Farias, 2002; De Farias and Van Roy, 2003) and the duality of LP, it is equivalent to solve the following dual LP: where tation of discounted state-action counts of the policy when variables of the e-return parity. The dual constraints are state transitions under the learned policies. We can now characterize ,T(s| s, a) = T(s| s, a) = 0andT(s| s, a) = T(s| s, a) = 1, ∀a ∈ A. Given andµ= [0, 1], then the return gap∆= c > 0. In this case, it is impossible to ﬁnd policies e-return parity for anye < c. Next we provide a sufﬁcient condition to satisfy0-return parity in d:= [T( s| a, s) − T( s| a, s), . . . , T( s| a, s) − T( s| a, s)]. For∀c ∈ R\1, if i ∈ [n], j ∈ [m], such thathc, di ≤ 0, then there existπ, π∈ Πsuch that0-return parity is ˆρ( s, a),ˆρ( s, a), b, b≥ 0, ∀s, aare dual variables. Note thatˆρ( s, a)andˆρ( s, a)have the interprea sufﬁcient condition for the optimal policies above. In light of the dual LP formulation, the ﬁrst two inequalities in Proposition 3.3 indicate the probability masses of the initial distributions in at least one state are greater than zero, and the last inequality requires the maximum value of the objective function in the dual LP formulation is no greater than zero. The linear programming methods to solve the return disparity problem of MDPs become impractical in continuous or high-dimensional discrete state and action spaces. However, in many real-world scenarios where return parity is desired, the number of the states or actions is often large (e.g., recommend items to different demographic groups of users). In this section, we shall provide an upper bound to (1) quantitatively characterize return disparity in terms of the distance between the reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions and, (2) motivate our algorithm design to mitigate return disparity in continuous or high-dimensional discrete state and action spaces (Sec. 4). Theorem 3.1. F = {f : S × A → R} a ∈ A and g ∈ {0, 1}, then the following holds: where d Remark reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions of the two MDPs. Given any two MDPs, the distance between group-wise reward functions is constant. If we further assume the two MDPs share the same reward functions (i.e., In this case, it implies a sufﬁcient condition to minimize return disparity is to ﬁnd policies mize the distance between induced state visitation distributions in the two MDPs. In the subsequent sections, we assume the reward functions of different groups are (approximately) the same and propose algorithmic interventions to mitigate return disparity. For completeness, we also provide another decomposition theorem ( s):= kπ( · | s) −π( · | s)k. We see that return disparity is upper bounded by three terms: the distance between group-wise (Theorem C.1) in Appendix C and motivate the design of another family of algorithms based on Theorem C.1 in Appendix D. We present the Theorem 3.1 in the main text since the algorithms (see Sec. 4) motivated by it are more efﬁcient and stable in the application scenarios (e.g., recommender system) we consider. Inspired by the theoretical results in Theorem 3.1, we introduce a state visitation distribution alignment procedure, which can be naturally incorporated into existing RL methods, to encourage policies to maximize expected returns while keeping state visitation distribution similar to each other. We use deep Q-networks (Mnih et al., 2015) as our baseline backbone algorithm, which has demonstrated its superior performance in recommender application (Zheng et al., 2018), in which we will conduct experiments next. In what follows, we ﬁrst brieﬂy introduce how to learn the deep Q-networks and then discuss state visitation distributional alignment via IPMs. We give the main pipeline of our algorithms in Algorithm 1. The main idea behind learning deep Q-networks (Q-learning) is to approximate the value functions in high dimensional state and action spaces. Speciﬁcally, we aim at learning deep Q-network to approximate the reward when taking an action in a given state. Given the deep Q-network, we can construct the policy that maximizes the rewards: recommendation applications, the Q-network is often implemented as where the value in output dimension i represents estimated reward when taking action a During model training, the parameter interactive recommendation process as an example: at each time step user’s state 1 −e then receives the reward the user’s feedback and stores the experience buffer with batches of experiences from different users, the agent then optimizes the following loss function to improve the Q-network: with To avoid the overestimation problem (Thrun and Schwartz, 1993) in original DQN, we adopt the double DQN architecture (Van Hasselt et al., 2016): a target network the online network Q. The online network is updated at each model update step, and the target network is a duplicate of the online network and updated with delay (soft update): where τ controls the update frequency. With the double DQN architecture, y s, and takes an actiona(e.g., recommend an item) via thee-greedy policy (i.e., with probability taking an action with the max Q-value, and with probabilityechoosing a random action). The agent Algorithm 1 Algorithm to mitigate return disparity under the framework of DQN. Inspired by Theorem 3.1, we propose to align the state visitation distribution via learning group-invariant representation. Speciﬁcally, we introduce two feature extractors Q-function. We alternatively optimize the loss of state visitation distributional alignment. At each iteration, the feature extractors and the Q-function, parameterized by the feature extractors are then updated to minimize the loss of state visitation distributional alignment via minimizing the estimated integral probability metrics between different groups. The whole algorithm is shown in Algorithm 1. Next, we introduce the detailed steps of state visitation distributional alignment via Wasserstein-1 distance, which is a kind of IPM choosing total variation distance from the optimization perspective (Arjovsky et al., 2017). Wasserstein-1 Distance from the state visitation distribution of group 1 respectively, two feature extractors receiving the feature representations from both groups, we use a critic function estimate Wasserstein-1 distance (Arjovsky et al., 2017) by maximizing the following objective function: To enforce the Lipschitz constraint on the critic function loss (Gulrajani et al., 2017) meanwhile: where Sample a mini-batch of experiences fromDandD; Update online and target DQNs and feature extractors f, g ∈ {0, 1} using Eq. 1 and Eq. 3; ψrespectively, map the samples to feature representations:h= f( s)andh= f( s). Upon ˆDrepresents the distribution of a uniformly distributed linear interpolations between the group visitation Figure 1: Illustration of our algorithms. At every iteration, we ﬁrst update the models (e.g., Q-function and feature extractors). The feature extractors are then updated to minimize the loss of state visitation distributional alignment via minimizing the estimated integral probability metrics between different groups. distributions. Finally, the overall minimax objective becomes where training stability and do less harm to the group that obtains lower return, we use block coordinate descent algorithm (Wright, 2015) to update the feature extractors: at each iteration of state visitation distributional alignment, we ﬁrst identify the group with higher return choose to update its feature extractor while ﬁxing the parameters of the feature extractor in the other group. Figure 1 illustrates our algorithm framework. MMD Variant state visitation distributions. Let squared MMD between the induced feature distributions of two groups D To align state visitation distributions, we optimize the feature extractor using Eq. 8. In the implementation of the MMD variant, we use a linear combination of RBF kernels with bandwidths since it remains an open problem on choosing the optimal kernels. βis the balancing coefﬁcient and by convention it is set to be 10 (Gulrajani et al., 2017). To ensure the MMD(D, D):=E[k(h, h)] + E[k(h, h)] −2E[k(h, h)]. In the following, we conduct empirical evaluation on two real-world datasets, showing that our proposed algorithms help to mitigate return disparity while maintaining the performance of policies. Datasets demographic information (e.g., gender and age): •MovieLens-1M more than 4000 movies on the MovieLens website. The movie ratings range from 1 to 5 and the users are provided with demographic information such as gender and age. •Book-Crossing than 1 million ratings from more than 278k users on about 271k books. The book ratings range from 0 to 10, and the users are provided with demographic information such as age. Our goal of using these datasets is to learn a recommender system that maximizes the expected long-term user satisfaction while keeping the user satisfaction in different demographic groups similar. Environment Simulator datasets by setting up an environment simulator (Chen et al., 2019; Zhou et al., 2020) to mimic online environments. Speciﬁcally, we normalize the user ratings in each dataset into the range a user’s historical interaction with the recommender system at time step system recommends an item (action Following (Chen et al., 2019), we give the details of the state representation scheme used in this work in Figure 2. As shown in Figure 2, the user state output of a recurrent neural network (RNN). The user status at as the number of positive/negative rewards before time step composed of three signals: the recommended item, the reward gained by recommending the corresponding The two real-world datasets we use are benchmark recommender system datasets with user item, and the user status. Note that each item and reward are mapped to an embedding vector and a one-hot vector before inputting to the RNN. We perform matrix factorization (Koren et al., 2009) to train an item embedding for each item to recommend. Figure 3: Learning curves of DQN, DQN-WASS and DQN-MMD in three different settings. The legend DQN WASS (DQN MMD) X:Y indicates the interval of model update versus the interval of state visitation distributional alignment with Wasserstein-1 distance (MMD distance) is X:Y (i.e., smaller numbers means more frequent updates). With the increase of the frequency of state visitation distributional alignment, return disparities are decreasing at the cost of performances of policies in all environments. For each dataset, we randomly split the users into two parts: 80% of the users are used for training, and the other 20% are used for testing. Due to the way we perform the train/test split in our datasets, our experiments are cold-start scenarios: the users in the test set have never been seen during training, and there is no interaction between the recommender system and the users at ﬁrst. To deal with the problem, our recommender system recommends a popular item among the training users to a test user at time step recommends non-repeated items to the user interactively according to users’ feedback. The episode length is set to be 32 for each user in the two datasets in our experiments. Methods and Implementation Details that perform state visitation distributional alignment via Wasserstein-1 distance (DQN-WASS) and maximum mean discrepancy (DQN-MMD). To the best of our knowledge, our work is the ﬁrst work that studies the return disparity problem in MDPs. We also adapt the state-of-the-art reduction approach, constrained policy optimization (CPO) (Achiam et al., 2017) to our problem setting and ﬁnd its training processes cannot converge, so we do not include the results here. We suspect the failure of CPO is because their setting is different from ours: they assume the constraint of the policy is a deterministic function determined by states and actions, while in our setting, the reward gap in each environment step is dynamically changing, making it hard to estimate. We take gender and age in MovieLens dataset and age in Book-Crossing dataset as the binary demographic groups (e.g., male/female, young/old). In each environment, we vary the model update frequency and the state visitation distributional alignment update frequency in DQN-WASS and DQN-MMD and report the corresponding overall return and the return disparity between groups. We average the results over ﬁve different random seeds and visualize the performance curves in each setting. The detailed data pre-processing pipelines and hyper-parameter conﬁgurations in our experiments are presented in Appendix B. The performance curves of DQN, DQN-WASS and DQN-MMD are shown in Figure 3. We can see that with the increase of the frequency of state visitation distributional alignment, return disparities are decreasing at the cost of performances of policies in all environments, which demonstrates the trade-offs between return maximization and return parity. Our methods can ﬂexibly tune the trade-off between return maximization and return parity by controlling the relative update frequency ratio between the model update step and state visitation distributional alignment update step. Compared to DQN-WASS, DQN-WASS leads to more stable training processes with slightly higher overall returns and return disparities on average. Next, we provide more insights into how our algorithms work. Since the MMD distance can be estimated analytically using Eq. 8, we visualize the learning curves of estimated MMD distances between the (induced) state visitation distributions in different MMD update settings in MovieLens (Gender) in Figure 4. We can see that the more frequent the MMD update is, the smaller the MMD distance. We also perform principal component analysis (PCA) on sampled state representations of different groups for DQN and DQN-MMD in MovieLens (Gender). The visualization results are presented in Figure 5. We can see from Figure 5 that DQN-MMD helps to align the state visitation distributions of different groups, which is consistent with our theoretical ﬁndings in Theorem 3.1. Fairness in MDPs select an action if the long-term (discounted) reward of choosing that action is higher than the others in MDPs; Wen et al. (2019) extend static fairness notions to Markov decision processes and propose model-based and model-free algorithms to maximize expected return while satisfying the fairness constraints. However, their proposed algorithms are based on linear programming or evolutionary algorithm, which cannot be scaled up to solve complex tasks compared to deep reinforcement learning approaches; Siddique et al. (2020) consider the fairness of multi-objective MDPs and propose to learn a policy with objective function satisfy the generalized Gini social welfare function (Weymark, 1981); Ge et al. (2021) consider the fairness of item exposure problem in recommendation systems and extend CPO algorithm (Achiam et al., 2017) to satisfy the fairness constraint; Thomas et al. (2019) propose an ofﬂine RL algorithm called quasi-Sheldonian Figure 4: Training visualization of estimated MMD distance in MovieLens (Gender). With the increased frequency of MMD update, the MMD distance becomes smaller. Figure 5: PCA visualization of sampled state visitation representations of different groups for DQN and DQN-MMD in MovieLens (Gender). Our method helps to align the state visitation distributions of different groups. reinforcement learning algorithm to determine whether given sets of policy distributions satisfy a set of return constraints with guarantees; Zhang et al. (2020) analyze how static fairness constraints affect the dynamics of group qualiﬁcation rates. In contrast to their work, we propose the notion of return parity to quantify the dynamics of performance disparity in general changing environments: we theoretically analyze the notion of return parity by providing sufﬁcient conditions where return parity can be satisﬁed and propose a decomposition theorem of return disparity. Based on our decomposition theorem, we propose algorithms to mitigate return disparity and empirically show the effectiveness of our algorithms. Fairness under Other Temporal Models learning (Blum et al., 2018; Bechavod et al., 2019; Gupta and Kamble, 2019), multi-armed bandit (Joseph et al., 2016; Patil et al., 2020; Chen et al., 2020), and one-step feedback model(Liu et al., 2018; Hu and Chen, 2018; Kannan et al., 2019). In particular, Hashimoto et al. (2018) show that empirical risk minimization ampliﬁes representation disparity over time with a low group retention rate for the underrepresented group. They further propose distributionally robust optimization to minimize the worst-case risk overall group distributions. Heidari and Krause (2018) propose a time-dependent individual fairness notion that requires similar individuals should receive similar outcomes during the same time epoch. Ensign et al. (2018) model predictive policing problem using Pólya urn model and show that all police ofﬁcers will be allocated to one location if more ofﬁcers are constantly assigned to the locations with higher predicted crime rates. Heidari et al. (2019) propose effort-based fairness, which measures unfairness as the disparity in the effort made by individuals from each group to get a target outcome. Zhang et al. (2019) quantify the condition of the exacerbation of relative group ratio under the fairness constraints such as demographic parity and equalized odds. In this paper, we investigate the problem of return parity in MDPs theoretically and empirically. In particular, we prove a decomposition theorem for return disparity which decomposes the return disparity of any two MDPs into the distance between group-wise reward functions, the discrepancy of group policies, and the discrepancy between state visitation distributions. We then provide algorithmic interventions to mitigate return disparity via state visitation distributional alignment with IPMs. To corroborate our results, we conduct experiments on two real-world benchmark datasets. Experimental results suggest that our proposed algorithms help to mitigate return disparity while maintaining the performance of policies. We believe our work takes an important step towards better understanding the dynamics of performance disparity in changing environments.