systems that help users ﬁlter a large number of invalid information to obtain information or items by estimating their interests and preferences. The mainstream traditional recommendation systems mainly use ofﬂine and historical user data to continuously train and optimize ofﬂine models, and then recommend items for online users. There are three main problems: the unrealistic estimation of user preferences based on sparse and noisy historical data [1], [2], [3], the ignorance of online contextual factors that affect user behavior [4], [5], [6], and the unreliable assumption that users are aware of their preferences by default [7]. Inspired by the dialogue systems, which focus on the user’s realtime feedback data and obtains the user’s current interaction intentions, a new recommendation method, Conversational Recommendation System (CRS), is trying to combine the interactive form of the dialogue systems with the recommendation task and becomes an effective means to solve the traditional recommendation problem. Through online interactive methods, CRS can guide and capture user’ current preferences and interests by asking user whether he/she likes an item attribute or not, and utilize the user’s preferred attributes or rejected attributes to provide recommendation results that most ﬁt users’ current preferences in time. The core research difﬁculty of CRS is exploitation and exploration (E&E) problem. Speciﬁcally, at each turn of the conversational session, CRS may choose to utilize the user preference information obtained during the previous interaction to output the current best recommended items, which is the exploitation of E&E; or choose to continue to interact with users to obtain more real-time user preference information, which is the exploration of E&E. The difﬁculty of solving CRS’s E&E problem lies in achieving a special Nash equilibrium in the ﬁeld of recommendation systems: if CRS is too inclined to choose exploitation action (recommend) without understanding the current user preferences, there will be a lot of failed recommendations which poor user experience; in contrast, if CRS adopts excessive exploration action (ask), the interaction burden on users will be increased and the risk of user churn will become serious. At present, CRS researchers mostly use Deep Reinforcement Learning (DRL) to learn a conversational recommendation policy to solve the E&E problem. Because DRL has the ability of sequential decision-making, and it naturally ﬁts with the interaction form of CRS. From the experimental results of the previous work, the CRS based on DRL achieves much better results than other methods, and the state-of-the-art CRS methods are all based on DRL. However, the existing works of conversational recommendation fail to put forward an effective solution to E&E problem and build an efﬁcient and accurate CRS, due to the lack of information utilization of the interactive environment. In this paper, we believe that sufﬁcient modeling of environment information is the key point to solving the E&E problem. Therefore, our goal is to build a CRS that can make good use of the information of contextual environment at each turn of conversational recommendations. From the perspective of data enhancement, we propose a novel solution named Knowledge Graph-enhanced Sampling (KGenSam), which solves the insufﬁcient information modeling problem in CRS. In KGenSam, we introduce Knowledge Graph (KG) into the basic CRS framework as its interactive environment, and design two sampler modules to model and enhance the information of KG environment. Hence, CRS will learn to make E&E decisions based on enhanced knowledge. KGenSam adopts the sampling method of KG nodes to enhance knowledge in CRS. The design of the two samplers in KGenSam is inspired by solving the following two speciﬁc problems which prevent CRS from making the optimal decisions of E&E: (1) What attributes should CRS ask to help the recommender ﬁt the current user’s preferences efﬁciently? [8] In a conversational recommendation session, CRS needs to obtain sufﬁcient user preference information as much as possible to achieve a successful recommendation that ﬁts the current user preference. Obviously, on the premise of successful recommendation in the ﬁnal round, the fewer attributes the CRS asks, the shorter the interaction rounds, and the higher the efﬁciency of CRS. We believe that every asking opportunity of CRS is extremely valuable, and CRS needs to obtain as much user preference information as possible at every ask opportunity. In the design of this work, we naturally refer to the core idea of Active Learning, that is, when CRS chooses ask action, high-uncertainty attribute candidates that can not be judged by the current model need to be given priority attention. Because these attribute samples with high uncertainty will greatly improve the recommendation conﬁdence after being annotated by users and will make great contributions to achieving the goal of obtaining sufﬁcient user preferences with as little user interaction burden as possible. Therefore, to screen out the fuzzy samples with high uncertainty, we design an Active Sampler, which takes the sample distribution state in KG and conversation state as input and learns the features of fuzzy samples via active reinforcement learning. (2) How can CRS update the recommender precisely to ﬁt the implicit preference information in user feedback? To train an accurate personalized recommender, both positive and negative samples of user feedback are required [9], [10].The user interaction form of CRS naturally solves the one-class problem [11] of traditional recommendation that most user data is positive feedback. In CRS, besides the user’s positive feedback data, the user’s explicit negative feedback data can also be directly obtained. But CRS also presents a new research challenge. The number of conversation turns is often limited to more than a dozen, which causes the amount of user feedback data in a conversation recommendation session is also very limited. Generally, sparse user feedback and huge candidate item space require a lot of training data to train an effective recommender. How to update the recommender by using rare online feedback data has become a new challenge for researchers. Inspired by the idea of solving one-class problems by negative sampling in traditional recommendation systems, we try to supplement sparse interactive data by enhancing the positive and negative sample pairs in KG. We design a Negative Sampler, which refers to the hard negative samples mining method in image recognition. The Negative Sampler takes the sample distribution state in KG and conversation state as input, learns the features of hard negative samples via reinforcement learning, and constructs the positive and negative sample pairs enhanced from the user feedback data to accurately update the recommender. To evaluate the effectiveness of KGenSam, we conducted experiments on the Yelp and LastFM datasets. The experimental results show that KGenSam signiﬁcantly outperforms the state-of-the-art CRS methods, indicating that KGenSam has the ability to enhance the user experience of the whole CRS. We release the codes, datasets and demo of the KGenSam-based online CRS at https://github.com/ORZisemoji/KGenSam. In a nutshell, this work makes the following main contributions: new angle for building CRS. By sampling in KG, KGenSam enhances the contextual knowledge of CRS and solves the E&E problem caused by insufﬁcient information modeling of CRS. To the best of our knowledge, it is the ﬁrst time to introduce the knowledge graphenhanced sampling method into the conversational recommendation. sample mining methods into CRS. By incorporating the fuzzy samples and hard-negative samples in the CRS task, we facilitate the utilization of KG for CRS, which contributes to mine the user preference information in the KG environment to solve the data sparsity problem of the user feedback. benchmark datasets, demonstrating that KGenSambased CRS is able to achieve better performance than state-of-the-arts with much fewer conversation turns, which indicates high sample efﬁciency. ness of our method in real application scenarios. Recommender systems are the important applications of artiﬁcial intelligence, which can help users ﬁnd the information that ﬁts their preferences in the huge amount of data when there is no clear demand or the environment information is overloaded. At present, the mainstream recommendation systems are based on ofﬂine, historical user data, constantly optimize the ofﬂine performance, train the model, and then recommend items for online users. This ofﬂine update mode makes the recommender systems have inherent weakness [1], [2], [3], that is, it is difﬁcult to ﬁt the user’s online behavior, and the user’s real-time feedback will not be received in time [4], [5], [6], [7], so it is difﬁcult for the systems to know the user’s current intention and interest when using the recommender systems. The development of dialogue technology in dialogue systems provides a new solution to the problems existing in traditional recommendation systems, which is called conversational recommendation. Conversational recommendation breaks the barrier of information asymmetry between systems and user in static recommendation systems through rich interaction behavior, and allows recommendation systems to capture user preferences dynamically in the interactive conversation with users. Based on the idea of conversational recommendation, CRS is a task-oriented and multi-rounds recommendation system, on the one hand, it explores users’ current interest preferences, guides users to ﬁnd new interest points, which realizes users’ long-term retention; on the other hand, it also can receive feedback from users and update the recommender in real-time, which realizes dynamic learning of preferences. Recently years, researchers have proposed different CRS models designed from different perspectives. To deﬁne a standard form for conversation recommendation, Zhang et al. [12] proposed a conversation search engine called SAUR. To solve the problem of user cold-start in the movie recommendation scenario, Li et al. [13] designed a CRS that can interact with users through natural language. To obtain both long-term and real-time user preferences, Christakopoulou et al. [14] designed a CRS that can take user interaction history as part of input data. After that, Sun et al. [15] considered multi-rounds interaction with the user and designed a CRS that can obtain user attribute-level preferences multiple times. Lei et al. [8] proposed a multi-rounds CRS framework named EAR, which can obtain both item-level and attribute-level feedback during the interaction. In order to take full advantage of conversational recommendation, Lei et al. [16] introduced KG into CRS and proposed a new CRS framework called CPR. In order to make full use of user feedback information, Xu et al. [17] proposed FPAN framework, in which Gate Mechanism is used to dynamically modify the representations of items and users based on the feedback information. Overall, the research of CRS is committed to improving the satisfaction of online recommendations and the longterm retention rate of users in the system, and DRL has been widely used in CRS for learning conversational recommendation policy. However, the standard DRL model can not meet the needs of CRS, researchers are faced with the difﬁcult convergence problem of CRS’s training and low effectiveness problem of CRS’s interaction. The main reason for these problems is the lack of clear rules in the contextual environment in CRS which leads to the lack of clear reward function and clear action space in the training of the reinforcement model. This can be summarized as the problem of insufﬁcient environmental information modeling, which delays the updating of parameters in the training interaction, so CRS needs more interaction rounds to get user preference information and achieve a successful recommendation. 3.1 Problem Formulation In this subsection, We formally present interaction data and KG in our Task. Interaction Data. U, I and A denote the set of users, items and attributes, respectively. Each item i ∈ I is associated with a set of attributes P⊆ A. For each user, we view each observed user-item interaction as a positive instance, while sampling a negative item to pair the user. And training interactive data is structured into two pairwise data sets, Oand O. Oand Orepresent the user feedback on items and attributes, respectively: O= {(u, i, i)|u ∈ U, i∈ I, i∈ I where Idenotes the sets of positive items, each of which has at least one historical interaction record with the user, and Idenote the sets of negative items, most of which are implicit negative feedback data of the user, and a few items are explicit negative feedback directly obtained by CRS. O= {(u, p, p)|u ∈ U, p∈ A, i∈ A where Adenotes the sets of positive attributes that are the attributes of positive items or the user preferences Pacquired during the conversational recommendation session, and Adenote the sets of negative attributes that are the attributes of negative items or the ask attributes rejected by the user during the conversational recommendation session. Knowledge Graph. As prior efforts [18] show, items in user-item interaction data can be aligned with the corresponding entities in external KG. With such alignments, the external knowledge of item attributes as well as the interaction data are organized in the form of heterogeneous KG [19]. As shown in Figure 1, the user is the interaction object of CRS, and CRS mainly consists of a recommender module and an interactive policy module: the recommender module is responsible for generating a recommendation list based on the user preference information; the interactive policy module is responsible for deciding whether to ask users for more preference information or output recommended items at current conversational turn. As shown in Figure 1, a conversational recommendation session starts when the user enters the system, and then CRS chooses the recommend action or the ask action according to the current understanding of the current user’s preferences. If the recommend action is selected, the system will output the recommended items ranked to the top by the recommender to the user, and if the ask action is selected, the system will output the asked item attributes selected by the interactive policy to the user. After that, back to the user side, the user gives binary feedback (accept or reject) on the recommended items or the asked item attributes. The whole interactive session ends when a successful recommendation is achieved or the set maximum round T is reached. User simulator is used to replace real users to interact with the system, making CRS tasks more suitable for ofﬂine academic research. We use the same user simulator design as [8], [15]. In the existing research work of CRS, researchers tend to choose the simple but effective recommendation model, Factorization Machine(FM), as the recommender of CRS. Because of its effectiveness and agility, FM can meet the recommendation requirements of CRS without increasing the complexity of CRS algorithm. Given user u and his/her preferred attributes Pin the conversation, FM predict how likely u will like current item i and current attribute p in the conversation session as: The FM is trained by optimizing the pairwise Bayesian Personalized Ranking (BPR) [9] objective which assumes that the ground truth sample (item or attribute) should be ranked higher than other samples, and the training losses of the FM model are as follows: Loss=− ln σ(˜f(i| u, P) −˜f(i| u, P)) Loss=− ln σ(˜f(p| u, P) −˜f(p| u, P)) 3.2.3 Interact Policy Network We use a two-layer feed forward neural network as the interact policy network and use the standard Deep Q-learning [20] for optimization. The policy network takes the current session state stateis as input and outputs the values Q(s, a) for the two actions, aand a. CRS will choose the action with higher estimated reward reward, which is composed of reward, reward, reward, reward, reward. The value setting of reward function is shown in section 5.2.3. 4.1 Overview Figure 2 illustrates the architecture of the proposed KGenSam, a general solution framework for knowledgeenhanced conversational recommendation. The middle subgraph in Figure 2 shows the main body of a typical CRS. The workﬂow of the KGenSam CRS framework is as follows: (1) Firstly, we construct the KG of user interaction data, and integrate the external KG to enrich the context information of interaction environment, and use knowledge to assist CRS interaction agent to make the decisions of actions. (2) On the basis of the KG environment, we use the KG sampling methods [21], [22] to enhance the knowledge and design two samplers based on reinforcement learning for CRS. Two samplers sample fuzzy samples and negative samples in KG respectively. Speciﬁcally, ﬁrst, the Active Sampler focuses on the item attribute sample nodes representing user preferences in the KG, outputs fuzzy samples with large uncertainty through active reinforcement learning, and uses fuzzy samples for ask action of CRS, so as to effectively improve the conﬁdence of the model and improve the interaction efﬁciency of the system; second, the Negative Sampler focuses on the item sample nodes in the KG, outputs high-quality negative samples through reinforcement learning, supplements sparse online user data with negative samples, and updates the recommender with positive and negative sample pairs, so as to improve the update efﬁciency of the recommendation model. The two samplers work together to achieve the efﬁcient acquisition of user preferences and reduce the interaction rounds, which reduce the user interaction burden. The details of these two samplers modules will be discussed in section 4.2 and section 4.3 respectively. (3) Finally, with the help of two samplers, the interactive policy network module learns the conversational strategy based on the enhanced knowledge, outputs the action (ask or recommend) to be taken by the system, which guides the conversational direction, and realizes the acquisition of online user preferences and the successful recommendation of items that ﬁt the user’s current preferences at the last turn. As discussed before, one of the problems in CRS research works is that ”What attributes to ask ?”, of which the speciﬁc research content is to help the recommender ﬁt the current user’s preferences efﬁciently. Corresponding to tackling this problem, the task of the Active Sampler is to choose fuzzy samples of item attributes to ask so as to shorten the conversation, as shown in Figure 3. In order to deﬁne the active learning process more easily, we regard the recommender as a two-classiﬁer. For each sample, the recommender predicts whether belongs to user preferences, and the output result is between 0 and 1. Further, we cast active sampling as a Markov Decision Process (MDP) and design a policy network to learn the optimal ask strategy. 4.2.1 MDP of Active Sampling in KG In KG environment, there are 33 attributes in LastFM, 29 ﬁrst-level attributes and 590 second-level attributes in Yelp. Considering that there are not many item attribute nodes, we put all attribute nodes that are not labeled by users into the candidate P ool(it should be noted here that it may be a better method to collect the attribute nodes with one or two hops of the current user node as the candidate pool for the dataset with too many attributes, which is worth trying in the future). Intuitively, given the condition of the current state in KG, the Active Sampler takes an action by selecting the next fuzzy attribute node to ask. It is then rewarded by the performance gain of the FM trained with the updated set of labeled nodes. Formally, the key elements of MDP (MDP = {S, A, P, R}) are deﬁned as follows. State. The state vector is a concatenation of four component vectors that encode environmental information from different perspectives: state =state⊕ state⊕ state(7) to measure the degree of chaos and uncertainty of a system. The larger the entropy is, the more chaotic and uncertain the information of the system is. On the contrary, the smaller the entropy is, the more stable and deterministic the system is. In this binary classiﬁcation scenario, the entropy of candidate samples is an important basis for the Active Sampler to sample. Therefore, we calculate the entropy of the candidate attribute sample nodes to calculate the uncertainty of current environmental information, which is deﬁned as follows : − (ˆylog ˆy+ (1 − ˆy) log (1 − ˆy)) the network, the relevant graph node centrality dimen- sion is often used to deﬁne and measure the importance of the node [23]. Consequently, we use degree centrality, the most common graph node centrality dimension, to measure the importance of attribute sample nodes in KG. The deﬁnition is as follows : where the hyperparameter γ (set to 20) scale node degree and clip Stateto 1. sure the ”distance” (asymmetric) between two probabilities, so KL divergence can be used to calculate those data samples with large deviation [24], [25]. We compute the average KL divergence of the sample predicted label distribution between the attribute sample node and its neighbors , which can help the Active Sampler better identify potential clusters and decision boundaries in KG. The Statemeasures local similarity as: and responded, state(p) = 1; if p has not been chosen yet, state(p) = 0. candidate attribute nodes, we also take the structural information of the current subgraph in KG as part of the state. We construct the adjacency list dictionary of P oolas stateto take advantage of the KG structure. Action. The space of actions is P ool, and Active Sampler chooses an action afrom P oolbased on the output results of active policy network, π(·|state). As the state is changing during the interaction, the action space P oolis dynamic. Reward. The reward measures the quality of the ask attribute sample at each turn. However, there is no ground truth in the selection of the best fuzzy samples in a active learning model, reward rely on the feedback from the recommender. Consequently, we use the increase of the FM model’s performance score from t to t + 1 turn to deﬁne the soft reward function and design the reward as: where ˆyis the evaluation score of the FM model at turn t and Gain Score calculates the increase of the score, where we use AUC as the evaluation metric. State Transition Dynamic. Given an action aat state state, the transition to the next state stateis determined as: Objective Function. Towards learning a stochastic policy π, we maximize the expected cumulative discounted reward. The objective function with respect to the Active Sampler parameters Θis as follows: where λis the decay factor. 4.2.2 Active Policy Network As the right subgraph in Figure 2 shows, to effectively propagate useful information over the KG and thereby better measure node informativeness, we ﬁrst parameterize the policy network π as a one-layer Graph Convolutional Network (GCN) [26]. Then We use ReLU as the activation function σ. On top of the GCN, we apply a linear layer to output the ﬁnal output embedding and get the score for each node. These scores are then normalized by a softmax layer to generate a probability distribution over all candidate attributes p in P ool. By maximizing its long-term returns with policy gradient [27], active policy network can effectively learn to optimize the long-term performance of the recommender in an end-to-end fashion. After solving the problem that CRS asks what attributes to make each opportunity of ask the most efﬁcient, the next problem naturally arises, that is, how can the feedback signal of users for fuzzy samples be accurately learned by the recommender ? In previous CRS work, all unobserved interactions are regarded as negative samples, so it is easy to produce low-quality negative samples, and it is difﬁcult to update the model parameters of the recommender. Considering that CRS has very few session turns, the update of recommender need to be accurate and rapid. In recommendation systems, users often directly ignore the uninterested items, which will not produce users’ explicit negative feedback behavior and leads to the common One-Class Problem of recommender training data. To solve the negative sample missing problem caused by One-Class data, traditional recommendation systems often construct effective positive and negative sample pairs by negative sampling to learn user preferences. Drawing on the same idea, we design a Negative Sampler to help the recommender learn preference information as efﬁciently and timely as the Active Sampler’s acquisition of user feedback information. The collected high-quality negative samples supplement the rare online negative feedback and user history positive samples. Similar to the active sampling, we cast negative sampling as a MDP and design a policy network to learn the features of high quality negative samples in conversational recommendation. 4.3.1 MDP of Negative Sampling in KG In KG environment, different from the Active Sampler, the Negative Sampler needs to focus on the item nodes, so the candidate set to been sampled is very large. Since the Negative Sampler requires labor-intensive negative discovering and is memory-consuming and time-consuming to distill useful signals in large-scale KG. Thus, we design the Negative Sampler to navigate from a positive item over the KG structure like i→ p → i and yield two-hops neighbors as candidate negative items . Conditioned on the navigated local subgraph state, the Negative Sampler takes an action by selecting a batch of possible negative samples from this two-hops candidate items and construct training pairs with ground-truth positive items. Similar to the reward design of Active Sampler, the reward of Negative Sampler is the performance score of the FM trained with the constructed positive and negative pairs. Formally, the key elements of MDP (MDP = {S, A, P, R}) are deﬁned as follows. State. Given user u, the stateat turn t is deﬁned as subgraph paths (u → · · · → i), where iis the node the Negative Sampler visits currently. Action. The space of actions is P ool, which is the candidate sets of two hop neighbors. Reward. The design goal of reward is to measure the quality of negative samples. Similar to the concept of hard negative in the ﬁeld of Image Detection, we need to ﬁnd the discriminative negative samples which are easily misjudged by the recommender in the CRS task. Inspired by the negative sampling model (KGPolicy) proposed by Wang et al. [22], we apply the hypothesis and deﬁnition of ”Real Negative Sample” to the negative sampling task scenario of CRS. The deﬁnition of high-quality negative sample has two characteristics: it is similar to the user representation [28], [29]; it is close to the real positive sample. The negative samples with these two characteristics can represent the real negative samples and provide a larger gradient for the parameter update of the recommender. From this point of view, we deﬁne the reward of negative sampler as follows. reward= (P ool)T · u + (P ool)T · i(14) State Transition Dynamic. Given an action aat state state, the transition to the next state stateis determined as: Objective Function. To maximize the sum of expected rewards obtained from following policy π over the training graphs, the objective function with respect to the negative policy network parameters Θis: where λis the decay factor. 4.3.2 Negative Policy Network As shown in the left side of Figure 2, the Negative Sampler ﬁrst obtains the initial representation of the nodes through one layer of GCN, which captures the knowledge-aware negative signals, and then output a batch of negative item nodes with high weight through the attention model constructed by two-layer activation function. 5.1 Experimental Setup 5.1.1 Datasets As summarized in Table 1, we use two publicly available datasets: LastFM and Yelp. Each dataset is composed of two components, the user-item interactions and KG derived from external data sources, Freebase for LastFM and local business information network for Yelp. The cut ratio of training, validation and testing sets is 7: 1: 2. The yelp dataset we use here is slightly different from EAR [8] and CPR, using 590 second-level attributes instead of 29 ﬁrstlevel attributes. Thus, for real application scenarios, LastFM represents the dataset with fewer attributes, Yelp represents the dataset with more attributes. 5.1.2 Parameter Setting The maximum turn T is set as 15. All hyperparameters for ofﬂine training of FM remains the same as EAR [8]. The detailed rewards and the hyperparameters of interact policy net remain the same as CPR [16]. All the hyperparameters related samplers training are tuned according to the validation set. 5.1.3 Training Steps The training process is made up of three stages. Stage 1: Pretraining of recommender for scoring items and attributes. Strictly following [8], we use the training set of Oto optimize FM ofﬂine. We refer the readers to the original paper [8] for more Information. Stage 2: Pretraining of two samplers. The pretrained FM is used to provide reward scores for two sampling networks. The detailed pretraining process of samplers are shown in Algorithm 1 and 2 respectively. Stage 3: Online training of interact network. We use a user simulator to interact with the user to train the policy network. The interaction algorithm ﬂow of CRS based on our KGenSam is shown in Algorithm 3. 5.1.4 Comparison Methods We compare our method against the following baseline methods: the model only recommends items at each turn and updates itself until it ﬁnally makes successful recommendation. entropy rule. It always chooses an attribute with the maximum entropy within the current candidate item set [31] to ask. Details can be found at [8]. signed for single-round recommendation. CRM records user’s preference into a belief tracker, and uses reinforcement learning to learn the policy of interaction. We follow [8] to adapt it to the multi-round scenario. posed a three stage solution, Estimation Action Reﬂection. Algorithm 1 Pretrain the Active Sampler Input: O, FM Output: Active Sampler Policy π for (u, p, p) ∈ Odo P ool← A for t = 1 to T do select a batch ranked pfrom P ool if pin Athen else end if remove pfrom P ool end for evaluate FM on the validation set to get the rewardP signalreward update π with policy gradient end for Algorithm 2 Pretrain the Negative Sampler Input: O, FM Output: Negative Sampler Policy π for (u, i, i) ∈ Odo Batch← {}, P ool← {}, i← i for s = 1 to steps do get neighbors Neighborsof i traverse N eighborsas i for each i∈ Neighborsdo end for select a batch ranked Batchfrom P ool update FM based on Batchand i evaluate FM on the validation set to get the rewardP signalreward update π with policy gradient end for end for the state-of-the-art work, CPR, which introduces KG in CRS for the ﬁrst time. attention to the correction effect of user feedback on recommendation and adjusts the node representation of KG through the Gate Mechanism, which signiﬁcantly improves the CRS recommendation results hence being the most comparable model. 5.1.5 Evaluation Metrics The evaluation follows [8]. We use the success rate at the 15th turn of conversation (SR@15) to measure the cumulative ratio of successful recommendation by turn 15 . We also use and average average turns (AT) of conversation length when the interaction process ends to record the average number of turns for a successful recommendation. Algorithm 3 KGenSam-based CRS Algorithm Input: O, O, FM, Active Sampler, Negative Sampler Output: Conversation Recommendation Policy π for (u, i, i) ∈ O, (u, p, p) ∈ Odo P← {p}, P← {}, P ool← A P ool← Neighbors− I for t = 1 to T do select an action afrom {ask, rec} Negative Sampler selects negative batch Batch if a= ask then remove pfrom P ool Active Sampler selects a pfrom P ool if user u accepts pthen else end if else FM outputs ito recommend if user u accept ithen else end if end if update FM based on P, Pand Batch end for reach the maximum turns and quit! end for Therefore, the higher SR@15 and the lower AT indicate a higher performance and overall higher efﬁciency. Success Rate@15. where θ= 1, if the instinctive feedback of the recommended item given by the simulator is acceptance. Average Turns. where Tis the total turns when the current conversation session ends. 5.2 Results and Discussion 5.2.1 Overall Performance Comparison The results are summarized in Table 2. Overall, it can be seen that the proposed KGenSam architecture outperforms other baselines in both terms of SR@15 and AT, which validates our hypothesis that enhancing the information modeling of interactive environment is an effective strategy to improve the performance of CRS. By intuitively presenting the performance comparison in Figure 4, we also have following discoveries: performance of all baseline methods is not satisfactory, CRM and EAR models can not even achieve the performance of Max Entropy method. whole conversation, we can clearly ﬁnd that the ﬁrst six rounds of interaction of the baseline methods are in a ”Chilling Period”, during which the success rate of recommendation is very low, and after the sixth round, the success rate of CRS slowly increases. However, KGenSam does not have such a ”Chilling Period”, which can improve the success rate after each round of interaction, and the improvement of the early interaction is even greater than that of the later interaction, which is consistent with the trend of the accuracy improvement curve of Active Learning in the classiﬁcation model annotation task. This consistency veriﬁes our hypothesis that the idea of active learning can effectively integrate with CRS task. 5.2.2 Evaluating Key Designs in KGenSam The key designs of KGenSam are the two sampler modules. In order to evaluate whether the two samplers can meet our expectations, that is, to accurately select the asked attribute samples and efﬁciently update the recommender, we designed the following ablation experiments. (1) w/o Active Sampler : We select attribute samples with the maximum scores (calculated by propagating messages like CPR [16]) to ask. (2) w/o Negative Sampler : We randomly select item samples without interaction as negative samples to update the recommender. (3) w/o Samplers : Neither of the two sampling modules works. We only use KG as the environment, and the selection methods of asked attributes and negative items are consistent with (1) and (2). Table 3 shows the success rate of every three turns and the average rounds of a session under the above three ablation experimental settings. It can be seen that when two samplers work at the same time, the performance of the complete KGenSam is the best. In addition, we have the following conclusions based on the success rate trend in Figure 5: rate curve and makes it convex upward, indicating that the Active Sampler changes the magnitude of user preference information acquisition per turn. As indicated in Figure 5, the Active Sampler can greatly improve the performance of CRS in the ﬁrst six rounds of the dialogue, which means it can help CRS solve the problem of ”Chilling Period” (the phenomenon mentioned in the previous section). Figure 5 illustrates that the fuzzy attribute samples output by the Active Sampler are indeed the most uncertain user preferences in the current system, which means the attribute knowledge enhanced by the Active Learning idea can help CRS ask the user preference information accurately. line and from the orange line to the red line, we can see that the Negative Sampler makes the success rate curve move upward as a whole, which indicates that the Negative Sampler improves the learning speed of user preference information, and makes the success rate rise to a relatively high level earlier. Active and Negative Samplers removed separately behaves differently on the two datasets. For the LastFm with fewer attributes, the effect of removing the active sampler is less than that of removing the Negative Sampler; while for Yelp with more attributes, the opposite is true. Through the analysis of this result, we believe that the Active Sampler can play a greater role in the complex data environment because it focuses on the selection of attribute nodes. Thus, the Active Sampler has a greater impact on Yelp which has a large number of attributes. are responsible for obtaining a large amount of user preference information, and the Negative Samplers are responsible for learning the acquired information. Two key designs achieve a high-performance CRS by jointly enhancing the environmental information in KG. In the process of experiment, we found a phenomenon worthy of attention, that is, the reward setting of interact policy network has a great impact on the ﬁnal evaluation result of CRS, which may be due to the current evaluation metrics of CRS is not comprehensive enough. For example, the value settings of reward in the two representative CRS works, EAR and CPR, are totally different. We use the same reward setting of CPR which is also based on KG as the default reward setting in our KGenSam framework. In order to further verify the effect of reward setting in CRS, we design several kinds of reward settings as shown in Table 4, in which reward, reward, reward, rewardand rewardrepresent ”ask attribute succeed”, ”ask attribute fail”, ”recommend succeed”, ”recommend fail” and ”reach maximum turn” respectively. R(default setting) and Rare the reward settings of CPR and EAR. In addition, we increase reward and decrease rewardto set Rwhich can tend to ask, in the same way, we set Rto incline recommend. It can be seen from Figure 6 that reward setting can affect the probability of taking recommend action. The reward setting of CPR makes the rec ratio of each round extremely high, and the ﬁnal performance on both SR@T an AT is also the best, which veriﬁes our previous conjecture. We boldly draw the conclusion that rec ratio is positively correlated with the two evaluation indicators, which needs to be veriﬁed by more research in the future. In this paper, we provide a new idea for the research of CRS, that is to solve the problem of insufﬁcient environmental information modeling in CRS from the perspective of knowledge enhancement. The experimental results show that this perspective is reliable and effective. We construct an Active Sampler to focus on the fuzzy attribute nodes in the graph to maximize the amount of information obtained by each ask turn. At the same time, the Negative Sampler we constructed focus on the high-quality negative samples in KG, so as to maximize the learning speed of each recommender update. As far as we know, we are the ﬁrst to deﬁne the idea of Active Learning into the CRS problem and achieve the experimental results overcoming ”Chilling Period” problem. This idea of Active Learning to screen fuzzy samples in KG can also be applied to the cold start problem of KG based recommendation system. In addition, we also introduce hard negative Samples into CRS to achieve efﬁcient CRS online update and promote the commercial online application of CRS. In the future, we will try to introduce more evaluation metrics to solve the incomplete evaluation problem in CRS research. In addition, the current CRS research is based on single session (including single-round and multi-rounds). Our KGenSam method has achieved high evaluation results in the single session scenario where the conversation ends after only one successful recommendation, but we believe that single-session CRS is obviously not in line with the real recommendation scenario. Therefore, from the perspective of sequential recommendation modeling, we will study multi-session modeling of CRS to extend our KGenSam framework, which may improve the possibility of CRS being widely used in business scenarios. This work is supported by the National Key R&D Program of China (2018AAA0100604), the Fundamental Research Funds for the Central Universities (2021RC217), the Beijing Natural Science Foundation (JQ20023), the National Natural Science Foundation of China (61632002, 61832004, 62036012, 61720106006).