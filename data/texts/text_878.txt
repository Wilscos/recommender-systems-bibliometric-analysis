<title>A Survey on Gender Bias in Natural Language Processing</title> <title>arXiv:2112.14168v1  [cs.CL]  28 Dec 2021</title> et al 2016; Voigt et al 2018]. Further, prior studies have shown bias in underlying NLP algorithms such as word embeddings [Bolukbasi et al 2016] and language models [Nadeem et al 2021], as well as in the downstream tasks they are employed for, e.g., machine translation [Savoldi et al 2021], coreference resolution [Rudinger et al 2018; Webster et al 2018; Zhao et al 2018a], language generation [Sheng et al. 2020], and part-of-speech tagging and parsing [Garimella et al. 2019]. However, the rapid increase in research on gender bias has led to a state where the research is fractured across communities and publications often do not engage with parallel research. Thus, there is a need to summarise and critically analyse the developments hitherto, to identify the limitations of prior work and suggest recommendations for future progress. Therefore, in this paper, we present an overview of 304 papers on gender bias in natural language processing. We begin with a brief outline our methodology and explore the evolution of the eld in popular NLP venues (§2). Then, we discuss dierent denitions of gender in society (§3). Further, we dene gender bias and sexism in general and in NLP, in particular, incorporating a discussion of their ethical considerations (§4). Next, we gather common lexica and datasets curated for research on gender bias (§5). Subsequently, we discuss formal denitions of gender bias (§6). Then, we discuss methods developed for gender bias detection (§7) and mitigation (§8). We nd that existing research on gender bias has four main limitations and see addressing these limitations as necessary future focus areas of research on gender bias. Firstly, despite the wide range of research across multiple language tasks predominantly only two genders are distinguished, male and female, neglecting the uidity and continuity of gender as a variable. Natural language has started to adopt gender-neutral linguistic forms to recognise non-binary nature of gender such as singular they in English and hen in Swedish, thus presenting a need for NLP researchers to incorporate this social development into their datasets and algorithms [Sun et al 2021]. Otherwise, modelling gender as a binary variable can lead to a number of harms such as misgendering and erasure via invalidation or obscuring of non-binary gender identities [Behm-Morawitz and Mastro 2008; Fast et al 2016]. Addressing this issue is critical not just to improve the quality of our systems, but more importantly to minimise these harms [Larson 2017]. Secondly, most prior research on gender bias has been monolingual, focusing predominantly on English or a small number of further high-resource languages such as Chinese [Liang et al 2020] and Spanish [Zhao et al 2020]. Only limited work has been conducted in a broader multilingual context with notable exceptions of analysis of gender bias in machine translation [Prates et al 2020] and language models [Stańczak et al. 2021]. Thirdly, despite a plethora of studies showing evidence of presence of systematic gender bias in prolically applied NLP methods [Bolukbasi et al 2016; Nadeem et al 2021; Nangia et al 2020], researchers are not required to test the models they publish with respect to biases they perpetuate. In particular, still most of the recently published models do not include a study of (gender) bias and ethical considerations alongside their publication [Conneau et al 2020; Devlin et al 2019; Rael et al 2020; Zhang et al 2020] with the noteworthy exclusion of GPT-3 [Brown et al 2020]. In general, these methods are tested for biases only post-hoc when already being deployed in real-life applications potentially posing harm to dierent social groups [Mitchell et al. 2019]. Lastly, we argue that methodologies within gender bias detection often lack baselines and do not engage with parallel research. We nd that similarly to research within societal biases Blodgett et al [2020], work on gender bias in particular, is fundamentally awed suering incoherence in usage of evaluation metrics. Publications consider often limited denitions of bias that address only one of many ways gender bias manifests itself in language. The following survey is an overview of all papers identied by the authors on analysing gender bias in NLP, which spans a total of 304 papers. To collect these relevant papers, the ACL Anthology, NeurIPS, and FAccT were queried for all papers with the keywords ‘gender bias’, ‘gender’ or ‘bias’ made available prior to June 2021. Additionally, we expand the spectrum of the papers with relevant social science publications and other relevant publications cited in the collected papers. We retained all papers about gender bias and discarded papers focusing on other denitions of the keywords (e.g., inductive bias, social bias). We review papers analysing gender bias in natural language and methods presenting an encompassing overview of gender bias in language. We analyse the number of published papers in ACL venues mentioning the selected keywords either in the title or the abstract of the paper and present the results in Figure 1. We observe a steady increase in the number of papers since 2015 with notable peaks in 2019 (83 publications) and 2020 (a total of 107 publications). This trend suggests 2021 might end with another record in the number of papers on gender bias per year. Indeed, in 2021, we have already identied a total of 40 papers covering the topic of gender bias in NLP. This development demonstrates that the area of research has established itself within NLP research. Denitions of gender used in the linguistics literature vary substantially across subelds and are often implicit [Ackerman 2019]. Depending on the context, the concept of gender refers to a person’s self-determined identity and the way they express it, how they are perceived, and others’ social expectations of them [Ackerman 2019; Lucy and Bamman 2021]. Compared to sex, a term that solely refers to one’s set of physical and physiological characteristics such as chromosomes, gene expressions, and genitalia, gender is considered a social construct [Butler 1989; Risman 2004]. In particular, Risman [2004] argue gender is a social construct and, as such, has consequences on person’s individual development, both in interactions and institutional domains. However, linguistic categories of gender do not map well to social categories [Cao and Daumé III 2020]. Literature on gender in linguistics often distinguishes the following types of gender that are summarised below. We note that these types are not all-encompassing and merely outline gender categories presented in the literature. • Grammatical gender : refers to a classication of nouns based on a principle of a grammatical agreement into categories. Depending on the language, the number of grammatical gender classes ranges from two (e.g., masculine and feminine in French, Hindi, and Latvian) to several tens (in Bantu languages and Tuyuca) [Corbett 1991]. Many of these languages also assign grammatical gender to inanimate nouns. • Referential gender : identies referents as female, male or neuter [Cao and Daumé III 2020]. A very similar concept is described by conceptual gender referred to as a gender that is expressed, inferred and used by a perceiver to classify a referent [Cao and Daumé III 2020]. • Lexical gender : refers to an existence of lexical units carrying the property of gender, maleor female-specic words such as father and waitress [Cao and Daumé III 2020; Fuertes-Olivera 2007]. • (Bio-)social gender : refers to the imposition of gender roles or traits based on phenotype, social and cultural norms, gender expression, and identity (such as gender roles) [Ackerman 2019; Kramarae and Treichler 1985]. Non-binary gender. Since the grammatical, referential, and lexical gender are denitions widely followed in NLP research, most NLP research that includes gender as a variable in downstream tasks treats it as a categorical variable with binary values (in English) [Brooke 2019]. However, the binarisation of gender in computational studies usually does not agree with critical theorists. For instance, Butler [1989] show how gender is not simply a biological given, nor a valid dichotomy, and even though many people t into the binary categories, there are more than two genders [Bing, Janet and Bergvall 1998]. Thus, gender can be viewed as a broad spectrum. More recently, natural language started adopting linguistic forms to recognise the non-binary nature of gender, such as singular they in English, hen in Swedish and hän in Finnish. These linguistic forms are not new concepts and were used by native speakers to refer to someone whose gender is unknown. However, their popularity has increased to denote a person whose gender is non-binary. The increased popularity of gender-neutral linguistic forms in natural language presents a challenge to incorporate this social development into the datasets and algorithms [Sun et al 2021]. However, some words that are relevant in this discussion such as cisgender and binarism are either missing or underrepresented in corpora and databases [Hicks et al. 2016]. Determining gender. To include gender as a variable in a NLP method, it often needs to be determined from the data rst since it is often not explicitly given which is generally dicult to accomplish with high precision. A popular method to determine gender is to infer it from a person’s name, assuming that this information is given. In many languages, gender-dierentiated names for men and women make gender assignment possible based on gendered name dictionaries. For instance, in Slavic languages, the ending of the last name is gender-specic (e.g., with -i vs. -a). On the other hand, gender-neutral rst names are common for Chinese, Turkish, and many other languages. Additionally, names often have dierent gender associations depending on the country and language, such as Andrea being a male name in Italian and a female one in English, German and Spanish. Notably, many of the primarily Western-based name lists used for determining gender do not always generalise to names from other countries and cultures [Lucy and Bamman 2021]. Due to these aspects, all of the above methods of determining gender tend to be imprecise and neglect non-binary genders. In the following, we state general denitions of gender bias and sexism and distinguish among their dierent types. Further, we outline the potential harms they might cause for individuals and society as a whole. Blodgett et al [2020] warn that papers about NLP systems developed for the same task often conceptualise bias dierently. Therefore, we state the most common denitions of gender bias in the following. Gender bias is dened as the systematic, unequal treatment based on one’s gender [Sun et al 2019]. More specically, Friedman and Nissenbaum [1996] use the term bias to refer to behaviour that systematically discriminates against specic individuals or groups in favor of others and distinguish three bias categories: pre-existing bias, technical bias, and emergent bias. Pre-existing bias arises when computer systems incorporate biases that appear independently and often prior to the creation of the system [Friedman and Nissenbaum 1996]. It can originate both from individuals, biased software developers or from society, private or public organizations and institutions, or especially in case of gender bias – historical and cultural context. Thus, this type of bias emerges not only through conscious decisions of individuals or institutions but can also appear unintended. On the other hand, technical bias emerges from models’ technical deign such as hardware and software limitations. While it is almost always possible to identify pre-existing bias and technical bias in a system design at the time of creation or implementation, emergent bias arises when the context the system was used for has changed - due to changes in society, population, or cultural values (e.g., when social media feeds are inuenced by user’s gender). Further literature outlines reporting and interpretation bias. Reporting bias refers to the phenomenon that the frequency with which situations of a certain type are described in text does not necessarily correspond to their relative likelihood in the world, or the subjective frequency captured in human beliefs [Gordon and Van Durme 2013]. On the other hand, interpretation bias is a phenomenon of researchers assuming that gender is a relevant variable which ultimately leads to analyses that are incapable of revealing violations of this assumption [Bamman et al 2014; Koolen and van Cranenburgh 2017]. The results are not questioned, especially if they align with common often stereotypical knowledge [Koolen and van Cranenburgh 2017]. Sexism can be dened as discrimination, stereotyping, or prejudice based on one’s sex (as opposed to one’s gender). According to the ambivalent sexism theory [Glick and Fiske 1996], sexism can be divided as: Hostile: follows the classic denition of prejudice - an explicitly negative sentiment that is sexist. Benevolent: subjectively positive attitude, which is sexist. Despite the seemingly positive sentiment, benevolent sexism has been shown to aect women’s cognitive performance stronger than hostile sexism [Dardenne et al 2007]. For instance, female gender associations with any word, even a subjectively positive word such as attractive, can cause discrimination against women if it reduces their association with other words, such as professional. Despite the positive sentiment of benevolent sexism, it can be backtracked to masculine dominance and stereotyping. We note that sexism is considered a subset of hate speech [Waseem and Hovy 2016] and therefore is often analysed together with other forms of aggression [Sa Samghabadi et al. 2020]. Gender bias and sexism result in harms aecting individuals and society as a whole. Recently, Crawford [2017] present a framework classifying algorithmic biases by the type of harm they cause and distinguish between allocational and representational harms. Representational harms refer to portrayals of certain groups that are discriminatory. In general, following Crawford [2017] representational harms can be divided into: stereotyping, underrepresentation, denigration, recognition, and ex-nomination. Stereotyping, in particular, perpetuates common (often negative) depictions of a certain gender. Under-representation bias is the disproportionately low representation of a specic group. Denigration refers to the use of culturally or historically derogatory terms, while recognition bias involves a given algorithm’s inaccuracy in recognition tasks. Finally, ex-nomination describes a practice where a specic category or way of being is framed as the norm by not giving it a name or not specifying it as a category in itself (e.g., ‘politician’ vs. ‘female politician’). On the other hand, allocational harms refer to the unjust distribution of opportunities and resources due to algorithmic intervention. They can result in systematic dierences in treatment or denial of a particular service and complete ruling out of certain groups, for instance in job applications. Allocation bias can be framed as an economic issue in which a system unfairly allocates resources to certain groups over others, while representation bias occurs when systems detract from the social identity and representation of certain groups [Crawford 2017; Sun et al. 2019]. Another harmful outcome of gender bias and sexism presents itself in gender gaps that arise from these asymmetrical valuations, e.g., where men are typically over-represented and have higher salaries compared to women [Mitra 2003]. The public sphere is often associated with male and agents characteristics (assertiveness, competitiveness) in domains like politics and entrepreneurship. Private or domestic domains linked to family and social relationships are traditionally related to women, although social relationships are considered more important by people independent of gender [Friedman et al. 2019]. Above we have introduced gender bias and sexism as general terms. In the following, we discuss how these biases emerge in natural language and ultimately inuence many downstream tasks. Language can be used as a substantial means of expressing gender bias. Gender biases are translated from source data to existing algorithms that may reect and amplify existing cultural prejudices and inequalities by replicating human behavior and perpetuating bias [Sweeney 2013]. This phenomenon is not unique to NLP, but the lure of making general claims with big data, coupled with NLP’s semblance of objectivity, makes it a particularly pressing topic for the discipline [Koolen and van Cranenburgh 2017]. Alongside the types of biases described above, there are forms of bias that apply specically in NLP research. In particular, Hitti et al [2019] dene gender bias in a text as the use of words or syntactic constructs that connote or imply an inclination or prejudice against one gender. Further, Hitti et al [2019] note that gender bias can manifest itself structurally, contextually or in both of these forms. Structural bias arises when the construction of sentences shows patterns that are closely tied to the presence of gender bias. It encompasses gender generalisation (i.e., when a gender-neutral term is assumed to refer to a specic gender-based on some (stereotypical) assumptions) and explicit labeling of sex. On the other hand, contextual bias manifests itself in a tone, the words used, or the context of a sentence. Unlike structural bias, this type of bias cannot be observed through grammatical structure but requires contextual background information and human perception. Contextual bias can be divided into societal stereotypes (which showcase traditional gender roles that reect social norms) and behavioral stereotypes (attributes and traits used to describe a specic person or gender). Therefore, gender bias can be detected using both linguistic and extra-linguistic cues, and can manifest itself with dierent intensities, which can be subtle or explicit, posing a challenge in this line of research. Gender bias is known to perpetuate to models and downstream tasks posing harm for the endusers [Bolukbasi et al 2016]. These harms can emerge as representational and allocational harms and gender gaps. Allocation harm is reected when models often perform better on data associated with the majority gender. In the context of NLP, this is often the case for machine translation [Sap et al 2017] and coreference resolution [Webster et al 2018] (see §7.3). Representation harm is reected when associations between gender with certain concepts are captured in word embeddings and model parameters [Sun et al 2019], for instance, as shown in [Bolukbasi et al 2016; Zhao et al 2018b]. On the other hand, gender gap is a phenomenon inuencing gender bias in the text. Since women are underrepresented in most areas of society, it is not surprising that available texts mainly discuss and quote men [Asr et al 2021], which leads, for example, to biased corpora researchers train their models on. Comprehensive data resources are crucial in probing for gender bias in language. However, many of the datasets in NLP are inadequate for measuring gender bias since they are often severely gender imbalanced with a substantial under-representation of female and non-binary instances. Further, analysing gender bias often requires a dataset of a specic structure or including certain information to enable proper isolation of the eect of gender [Sun et al 2019]. Thus, evaluation on widely-used datasets (e.g., SNLI [Rudinger et al 2017]) might not reveal gender bias due to inherent biases encoded in the data, presenting a need in research for targeted datasets for gender bias detection. We note that the choice of a dataset is dependent on the considered denition of bias (discussed in §4) that needs to be targeted specically, the NLP task at hand, domain, etc. Here, we describe the most popular publicly available lexica (§5.1) and datasets (§5.2) that have been used to analyse gender bias in NLP with respect to the above-mentioned aspects. Lexicon matching is an interpretable and technically simple approach, and thus, it has been frequently adopted by NLP practitioners. In particular, in gender bias detection, lexica representing genderness, sentiment, and the aect dimensions of valence, arousal, and dominance have been widely employed since these measures are often used as proxies for bias. In Table 1, we present the most popular lexica used for gender bias detection, and in the following, we describe measures they quantify. 5.1.1 Sentiment. Dierences in sentiment towards people of dierent genders have been analysed in the context of gender bias in numerous papers [Cho et al 2019; Hoyle et al 2019; Stańczak et al 2021; Touileb et al 2020], which have exploited sentiment lexica for this purpose. Since creating a comprehensive overview of sentiment lexica is outside the scope of this paper, we refer the reader to Taboada et al [2011] for such an overview. However, we note that sentiment is indicative solely of hostile biases rather than more nuanced ones. 5.1.2 Gender Ladenness. Gender ladenness is a measure to quantitatively represent a normative rating of the perceived feminine or masculine association of a word [Paivio et al 1968]. In particular, this metric indicates the gender specicity of individual words, with extreme values assigned to highly stereotypical concepts. For instance, in Ramakrishna et al [2015]’s lexicon, which is based on movie scripts, the word bride would be assigned the gender ladenness value of 0.84 on a scale from -1 (most masculine) to 1 (most feminine). Similarly, Williams and Best [1990] use a list of pre-selected adjectives, Sap et al [2014] use words collected on social media, and Clark and Paivio [2004] select a list of nouns to create a genderness lexicon. 5.1.3 Valence, Arousal, and Dominance. Based on social psychology, NLP research has identied three primary aect dimensions: power/dominance (strength/weakness), valence (goodness/badness), and agency/arousal (activeness/passiveness of an identity) [Field and Tsvetkov 2019]. Since a common stereotype associates female gender with weakness, passiveness, and submissiveness, lexica reporting measures for these dimensions are a valuable resource in gender bias analysis, and going beyond sentiment, they can be applied in unveiling benevolent biases. 5.1.4 Limitations. By their nature, lexicon approaches are limited to known words [Field et al 2019], and they assume that the context of the words remains constant [Lucy et al 2020]. However, collecting exhaustive lexica can be very resource-consuming since they rely on human-generated annotations [Lucy et al 2020]. Moreover, we note that all the lexica listed in Table 1 are created solely for English. There has been very little research enabling multi-lingual gender bias analysis employing lexica, with the notable exception of Stańczak et al. [2021]. In order to measure gender bias in NLP methods and downstream applications, a number of datsets have been developed. We list the well-established datasets in Table 2 together with the tasks they can probe and biases they provide a testbed for. Below we discussed three groups of datasets: those based on simple template structures, those based on natural language data, and datasets that have been developed to detect gender bias in language models. 5.2.1 Template-Based Datasets. A number of studies accounting for gender bias in natural language processing have been conducted on benchmark datasets consisting of template sentences of simple structures such as “He/She is a/an [occupation/adjective].” where [person/adjective] is populated with occupations or positive/negative descriptors [Bhaskaran and Bhallamudi 2019; Cho et al 2019; Prates et al 2020; Saunders and Byrne 2020]. Similarly, the EEC dataset Kiritchenko and Mohammad [2018] includes sentence templates such as [Person] feels [emotional state word]. and The [person] has two children. The EEC dataset has been widely used in other projects [Bhardwaj et al 2021] and has been extended with German sentences by Bartl et al [2020]. Another multilingual dataset has been proposed by Nozza et al [2021] that create a template-based dataset in 6 languages (English, Italian, French, Portuguese, Romanian, and Spanish) similarly consisting of a subject and a predicate. Another strain of work has utilised the structure of Winograd Schemas [Levesque et al 2012]: WinoBias [Zhao et al 2018a], WinoGender [Rudinger et al 2018], and WinoMT [Stanovsky et al 2019]. Since Winograd Schema Challenge is a coreference resolution task with human-generated sentence templates which requires reasoning with commonsense knowledge, it has been employed to analyse if reasoning of coreference system is dependent on a gender of a pronoun in a sentence and to measure stereotypical and non-stereotypical gender associations for dierent occupations. WinoBias [Zhao et al 2018a] contains two types of sentences that require the linking of gendered pronouns to either male or female stereotypical occupations. None of the examples can be disambiguated by the gender of the pronoun, but this cue can potentially distract the model. The WinoBias sentences have been constructed so that, in the absence of stereotypes, there is no objective way to choose between dierent gender pronouns. In parallel, Rudinger et al [2018] develop a WinoGender dataset [Levesque et al 2012]. As in the WinoBias dataset, each sentence contains three variables: occupation, person and pronoun. For each occupation, Winogender includes two similar sentence templates: one in which pronoun is coreferent with occupation, and one coreferent with person. Notably, WinoGender sentences unlike WinoBias also include gender-neutral pronouns. Finally, sentences in WinoGender are not resolvable from syntax alone, unlike in the WinoBias dataset, which might enable better isolation of the eect of gender bias. Both of these datasets have been employed in a number of analysis on gender bias in coreference resolution [de Vassimon Manela et al. 2021; Jin et al. 2021; Tan and Celis 2019; Vig et al. 2020]. Building on WinoGender and WinoBias, Stanovsky et al [2019] curate WinoMT, a probing dataset for machine translation, with sentences with stereotypical and non-stereotypical genderrole assignments. WinoMT has become widely applied as a challenge dataset for gender bias detection in MT systems [Basta et al 2020; Renduchintala et al 2021; Saunders and Byrne 2020; Stafanovičs et al 2020] with Saunders et al [2020] developing a version of the WinoMT dataset with binary templates lled with singuar they pronoun. Similarly, the Occupations Test dataset [Escudé Font and Costa-jussà 2019] contains template sentences to test MT systems on. Ultimately, both Occupations Test and WinoMT test if the grammatical gender of the translation is aligned with the gender of the pronoun in the original sentence which limits the aspects of gender bias they can probe for. 5.2.2 Natural Language Based Datasets. Probing datasets utilise also available natural language resources and extend them with annotations to tune it for the gender bias detection task. Importantly, these datasets can be applied to analyse gender bias in natural language and in algorithms, and are not limited by articial structures of the template-based approaches to collecting data. A number of popular datasets rely on data collected from Wikipedia. For instance, GAP [Webster et al 2018] is a human-labeled corpus derived from Wikipedia including sentences relevant for coreference resolution task. Unlike WinoGender and WinoBias, GAP focuses on relations where the antecedent is a named entity instead of pronouns [Webster et al 2018] and thus, can be used to unravel biases towards entities. Similarly, to analyse gender bias in coreference resolution, Emami et al [2019] develop the KNOWREF dataset, which is scraped from Wikipedia together with OpenSubtitles, and Reddit comments. Then, after initial ltering they infer the genders of antecedents based on their rst names and ask human annotators to predict which antecedent was the correct coreferent of the pronoun. Due a relatively large size of these datasets, both GAP and KNOWREF can be used as an alternative to sentence template based datasets. Another line of work is analysing gender bias in biographies. [De-Arteaga et al 2019] develop the BiosBias dataset, which consists of biographies with labelled occupations and gender identied within Common Crawl. The dataset has been created for the task of correctly classifying the subject’s occupation from their biography assuming that there are dierences between mens’ and womens’ online biographies other than gender indicators De-Arteaga et al [2019]. Further, GeBioCorpus [Costa-jussà et al 2020] present a dataset with biography and gender information from Wikipedia which has been widely used to analyse gender bias in MT (for English, Spanish, and Catalan) [Basta et al. 2020; Escudé Font and Costa-jussà 2019; Vanmassenhove et al. 2018]. Datasets employ also other online data sources. For instance, RtGender [Voigt et al 2018] is a dataset of online communication to enable research in communication directed to people of a specic gender. Studies on detecting misogynist or toxic language on social media released Twitter-based datasets [Anzovino et al 2018; Hewitt et al 2016]. Bentivogli et al [2020] develop MuST-SHE, a multilingual benchmark based on TED data for gender bias detection in machine and speech translation. Recently, Marjanovic et al [2021] create a dataset with Reddit comments to study gender biases that appear in online political discussion. 5.2.3 Probing Language Models. A signicant, though relatively recent and thus undiscovered, research direction has concentrated on analysing gender bias in language models. To this end, specic datasets have been curated. In particular, Nadeem et al [2021] present StereoSet, which is a dataset to measure stereotypical biases in gender, among other domains. It consists of triplets of sentences with each instance corresponding to a stereotypical, anti-stereotypical or a meaningless association. This dataset enables ranking language models based on probabilities they assign to each of these triplets. In parallel, Nangia et al [2020] introduce CrowS-Pairs, a crowdsourced, templatebased challenge set for measuring social biases, including gender bias, that are present in current language models. In CrowS-Pairs, each example consists of a pair of sentences, a stereotypical and anti-stereotypical. Both of these datasets are a signicant starting point for creating a benchmark for evaluating gender bias in language models. Notably, Stańczak et al [2021] propose a method for generating multilingual datasets for analysing gender bias towards named entities in LMs. Above we have discussed popular datasets employed for analysing gender bias. We note that datasets based on simple template structures allow for a controlled experiment environment. However, we warn that the limitations they impose might include articial biases, and the results of models tested on them may not map to a more natural environment. Since the above datasets provide means of conducting diagnostic tests for gender bias, they have a high positive and low negative predictive value for the presence of gender bias [Rudinger et al 2018]. Therefore, using these datasets, it is only possible to demonstrate the presence of gender bias in a system but not to prove its absence. Although datasets based on natural language obviate the downsides of the benchmark datasets with simple patterns, they often concentrate on data from one domain, e.g., social media, Wikipedia, or news. Therefore, the results might not generalise well to other domains and should be treated with caution. We note that natural language data might encode gender bias itself so that it is impossible to isolate bias from the data and the tested model. For instance, Chaloner and Maldonado [2019] nd evidence of bias in word embeddings trained on the GAP dataset when testing on a standard bias benchmark. They assume that this is due to gender bias on Wikipedia, GAP’s underlying data. However, irrespectively if based on natural language or sentence templates, most of these lexica and datasets are only available for English. Only datasets to analayse gender bias in machine translation, due to the nature of the task, are available in other languages. However, they often consider high-resource languages such as Spanish or German. Similarly, most of these datasets restrict themselves to the binary view on gender presenting a major gap in the research. Thus, we encourage data collection for gender inclusive task-specic datasets. Further, many of the popular publications have focused solely on occupational biases without accounting for a nuanced nature of gender bias. Finally, despite a number of datasets curated specically to assess for gender bias, only a few can be considered as benchmarks for a targeted downstream task and they come predominantly from the machine translation and coreference resolution domain. Therefore, we strongly encourage further research along the lines of establishing evaluation benchmarks for the underlying models such as Nadeem et al. [2021]; Nangia et al. [2020]. In the following, we list the common formal denitions of bias that are utilised to quantify the social concepts presented in Section 4 and divide them into denitions used for detecting gender bias in language (§6.1), either natural or generated, and in NLP methods (§6.2). Gender bias manifests itself in texts in many ways and can be identied using both linguistic and extra-linguistic cues [Marjanovic et al 2021]. Already structure of the data, e.g., the distribution of genders mentioned in the text, can be a bias indicator and the dierences in these distributions can be used as a measure for bias. However, in the following, we focus on more complex textual biases, i.e., lexical biases, and discuss measures for quantifying dierences in portrayals of genders, and their stereotypical depictions. 6.1.1 Dierences in Gender Descriptions. Dierences in depictions of men and women have been prolicly quantied using point-wise mutual information (PMI) [Hoyle et al 2019; Rudinger et al 2017; Stańczak et al 2021]. In particular, PMI investigates the co-occurrence of words with a particular gender. In PMI descriptors (such as adjectives or verbs) linked to a gendered entity are counted and the probability of their co-occurrence to a gender across entity is calculated. More formally, PMI is dened as: In general, words with high PMI values for one gender are suggested to have a high gender bias. However, Rudinger et al [2017] note that bias at the level of word co-occurrences is likely to lead to overgeneralisation when applied to a heterogenous dataset. Notably, PMI can also be used to measure dierences in word choice for genders beyond the binary [Stańczak et al. 2021]. Further, Hoyle et al [2019] extend the PMI approach and propose an unsupervised model that jointly represents descriptors with their sentiment to investigate gender bias in words used to describe men and women together with word’s sentiment. 6.1.2 Stereotypical and Occupational Bias. Occupational gender segregation and stereotyping is a major problem in the labor market often caused by gender roles and stereotypes present in society and as such has been in focus in a numerous research [Lu et al 2020]. To this end, Qian [2019] calculate an overall stereotype score of a text as the sum of stereotype scores of all the by denition gender-neutral words with gendered words in the text, divided by the total count of words calculated. Then, Qian [2019] dene the gender stereotype score of a word: where is a set of female words (e.g., she, girl, and woman), and 𝑐 (word, 𝑔) is the number of times a gender-neutral word co-occurs with gendered words. A word is used in a neutral way, if the stereotype score is 0, which means it occurs equally frequently with male words and females word in the text. Qian [2019] assess occupation stereotypes score in a text as the average stereotype score of a list of gender-neutral occupations in the text. This denitions of stereotypical and occupational bias have been employed in subsequent research [Bordia and Bowman 2019; Qian et al. 2019]. With the prevalence of NLP systems and their increasing application areas, researchers have developed measures to probe for gender biases encoded in these methods. In the following, we discuss dierent denitions used for bias detection in NLP methods. 6.2.1 Bias influencing Performance. For downstream tasks where there exists a gold gender, researchers have utilised performance-based measures to quantify bias. In particular, these measures are relevant for applications such as machine translation and coreference resolution where the objective involves the correct handling of gendered (pro-)nouns. Then, the amount of bias encoded in NLP systems can be quantied using: accuracy (percentage of observations with the correctly gendered entity) [Saunders and Byrne 2020]; dierence in accuracy between the set of sentences with anti-stereotypical and stereotypical sentences; score and dierence in score between the stereotypical and anti-stereotypical gender role assignments [de Vassimon Manela et al 2021; Webster et al 2018; Zhao et al 2018a]; log-loss of the probability estimates [Webster et al 2019]; false positive rates [Jin et al 2021; Kennedy et al 2020]; ratio of observations with masculine and feminine predictions; gender dierences in distributions of and within occupations Kirk et al. [2021]. Depending on the downstream task, task-specic performance measures are used to evaluate gender bias. For instance, to assess gender bias in dependency parsing, the labeled attachment score that measures the percentage of tokens that have a correct assignment and the correct dependency relation has been applied [Garimella et al 2019]. Next, BLEU is used in machine translation to assess the quality of the translated text [Saunders and Byrne 2020]. If the MT system is gender biased, the system produces an incorrect gender predicition even when no ambiguity exists [Costa-jussà and de Jorge 2020]. Thus, the lower the bias, the better the translation quality in terms of BLEU score and accuracy [Basta et al 2020; Escudé Font and Costa-jussà 2019; Stanovsky et al 2019]. However, Bentivogli et al [2020] point out that previously obtained BLEU gains [Moryossef et al 2019; Vanmassenhove et al 2018] cannot be ascribed with certainty to a better control of gender features and following previous research [Elaraby et al 2018; Vanmassenhove et al 2018] underlie the importance of applying gender-swapping in BLEU-based evaluations focused on gender translation. 6.2.2 Stereotypical Bias. Another stream of research attempts to quantify gender bias in terms of stereotypical associations that a method conveys. For instance, Zhao et al [2018a] consider a system gender biased if it links pronouns to occupations more accurately for the stereotypical pronoun, rather than the anti-stereotypical one. Next, in order to assess stereotypical associations encoded in NLP methods, Kurita et al [2019] suggest to measure how much more a model prefers the male association with a certain attribute, e.g., a programmer, compared to the female gender. To this end, Kurita et al [2019] propose to create template sentences, similar to the ones discussed in §5.2.1, and calculate a log probability bias score for BERT predictions when lling in a template with the gendered words and the target word. This measure has been widely applied in numerous research [Bartl et al 2020; Vig et al 2020]. Building up on this approach, Munro and Morrison [2020] calculate the ratio of the actual probabilities instead of log probabilities, claiming that ratios allow for more transparent comparisons. For datasets where each instance contains at least two versions of the same template sentence, e.g., male and female, the paired t-test has been used to measure if the mean predicted class probabilities are dierent across genders [Bhaskaran and Bhallamudi 2019; Kiritchenko and Mohammad 2018]. Similarly, Nangia et al [2020] propose a metric that calculates the percentage of examples for which the language model is in favor of the more stereotyping sentence. To measure this, Nangia et al [2020] rst break each sentence in an example into two parts: the modied tokens that appear only in one of the sentences and the unmodied part that is shared. Then, using pseudo-log-likelihood masked language model scoring [Salazar et al 2020], they estimate the probability of the unmodied tokens conditioned on the ones. Due to their simplicity and interpretability the above measures have been widely adopted to measure gender bias. However, these methods cover only stereotypical bias neglecting many other ways in which gender bias can be expressed. 6.2.3 Causal Bias. Causal testing presents another way of measuring gender bias in NLP systems. Then, gender bias is dened as the disparity in the output when model is feeded with dierent genders [Qian et al 2019]. Lu et al [2020] dene bias as the expected dierence in scores assigned to expected absolute bias across dierent genders. Later, Qian et al [2019] limit the above bias evaluation to a set of gender-neutral occupations and measure how the probabilities of occupation words depend on the gendered word and in reverse, how the probabilities of gendered wordsdepend on the occupation words. Similarly, Emami et al [2019] propose consistency as a bias metric, where they duplicate the dataset by switching the candidate antecedents each time they appear in a sentence. If a coreference model relies on knowledge and contextual understanding, its prediction should dier between the two versions. Emami et al [2019] dene the consistency score as the percentage of predictions that change from the original instances to the switched instances. Causal testing in gender bias detection has been used to dene bias in terms of stereotypical bias, rather than approaching other possible harms, which sets a possible ground for future work. where +𝑝 +𝑝 1 and , 𝑝 , 𝑝 ∈ [ for each . TGBI is equal to 1 in optimum when all the predictions incorporate gender-neutral terms. Cho et al [2019] expect TGBI to be a representative measure for inter-system comparison, especially if the gap between the systems is noticeable. Recently, Ramesh et al [2021] extend TGBI to Hindi. In general, this is a suitable method for applications where male default is the predominant risk. 6.2.5 Bias in Word Embeddings. In recent years, a myriad of publications have approached quantifying bias in word embeddings. In the following, we present the according to our judgement most inuential research in this eld. Projection-Based Measures. In the initial work on gender bias in word embeddings, Bolukbasi et al [2016] distinguish between two types of bias, direct and indirect. Following Bolukbasi et al −→ [2016] direct bias of a word embedding 𝑤 can be quantied as: where is a set of gender neutral words, is the gender direction and is a parameter determining how strict bias is dened. The direct bias manifests itself in relative similarities between gendered and gender-neutral words. However, since gender bias could also aect the relative geometry between gender neutral words themselves, Bolukbasi et al [2016] introduce notion of indirect gender bias which manifests as associations between gender neutral words that are arising from gender. In particular, if word such as businessman and genius are closer to football, a word with an embedding closer in the gender subspace to a man, it can indicate indirect gender bias. However, Gonen and Goldberg [2019] argue that the indirect bias has been disregarded to some extent and complain that mitigation methods are not provided. Another researched distance-based metric to measure gender bias in word embeddings uses the relative norm distance between two groups [Garg et al. 2018]: where is the set of neutral word vectors and is the average vector for group . The more positive (negative) that the relative norm distance is, the more associated the neutral words are towards group two (one). Thus, the above metric captures the relative distance (i.e., relative strength of association) between the group words and the neutral word list of interest. Similarly, Friedman et al [2019] compute bias as the average axis projection of a neutral word set onto the male-female axis and evaluate it for any region’s word embedding computing its correlation to gender gaps. Since the above denitions are straightforward and geometrically grounded, they have been often employed to quantify gender bias in word embeddings. However, bias is much more profound and systematic than the projection of words [Gonen and Goldberg 2019]. Word Embedding Association Test (WEAT). The WEAT has been developed as a benchmark for testing gender bias in word embeddings via semantic similarities. In particular, the WEAT compares set of target concepts (e.g., male and female words) denoted as and (each of equal size ), with a set of attributes to measure bias over social attributes and roles (e.g., career/family words) denoted as 𝐴 and 𝐵. The resulting test statistics is dened as a permutation test over 𝑋 and 𝑌 : where 𝑠𝑖𝑚 is the cosine similarity. The resulting eect size is then the measure of association: The null hypothesis suggests there is no dierence between and in terms of their relative similarity to and . In Caliskan et al [2017], the null hypothesis is tested through a permutation test, i.e., the probability that there is no dierence between and (in relation to and ) and therefore, that the word category is not biased. However, we note that results obtained with WEAT should be treated with a grain of salt since Ethayarajh et al [2019] prove that WEAT systematically overestimates bias. Sentence Embedding Association Test (SEAT). Based on the WEAT, May et al [2019] develop an analogous method, SEAT, that compares sets of sentences, rather than words. In particular, May et al [2019] apply WEAT to the sentence representation. Thus, WEAT can be seen as a special case of SEAT in which the sentence is a single word. To extend a word-level test to sentence contexts, May et al. [2019] slot each word into each of several semantically bleached sentence templates. Bias Amplication. Previous research has shown that NLP models are able not only to perpetuate biases extant in language, but also to amplify them [Zhao et al 2017]. In particular, Zhao et al [2017] interpret gender bias as correlations that are potentially amplied by the model and dene gender bias towards a 𝑚𝑎𝑛 for each word as: where 𝑐 (𝑤𝑜𝑟𝑑,𝑚𝑎𝑛) is the number of occurrences of a word and male gender in a corpus. If 𝑏(𝑤𝑜𝑟𝑑, 𝑚𝑎𝑛) > /|𝐺 | 𝐺 = {𝑚𝑎𝑛, 𝑤𝑜𝑚𝑎𝑛} under gender binarity assumption), then a word is positively correlated with gender and may exhibit bias. To evaluate the degree of bias amplication, Zhao et al [2017] propose to compare bias scores on the training set, (𝑤𝑜𝑟𝑑,𝑚𝑎𝑛) , with bias scores on an unlabeled evaluation set. We note that this method is applicable solely to individual words and would require an extension to be used as a general evaluation metric. 6.2.6 alitative Assessment. Alongside the above discussed quantitative gender bias measures, some research includes qualitative measures to analyse the extent of gender bias. For instance, Moryossef et al [2019] conduct a syntactic analysis of generated translations examining inection statistics for sentence templates from the dataset. Escudé Font and Costa-jussà [2019] introduce clustering as a measure of gender bias. Then, the higher the clustering accuracy for stereotypicallygendered words, the more bias the word embeddings trained on the dataset have. We nd this line of work particularly interesting as it encourages better model understanding and interpretability. Gender bias can be expressed in language in many nuanced ways which poses stating a comprehensive denition as one of the main challenges in this research eld. In this section, we have examined dierent gender bias denitions. We nd that they vary dramatically across and within algorithms and tasks, which supports ndings made by Blodgett et al [2020] that analyse bias denitions in general. Bias is often described only implicitly without any formal denition. Even when a paper states a formal denition, it essentially covers only one type of bias which oversimplies the task and thus, makes it impossible to detect all harmful signals in language. In particular, we discuss a number of methods to quantify bias in word embeddings which are utilised in many downstream tasks. However, most of them consider only one way of dening bias and do not engage enough parallel research to combine these methods. We here support [Silva et al 2021]’s claim that solely using one bias metric or test is not enough – diversifying metrics to ensure robustness of the evaluations is thus important. Additionally, we strongly encourage developing standard evaluation measures and tests to enhance comparability. Another limitation we see is that dening bias in terms of decreasing performance, however straight-forward, carries a risk of capturing bias only as long as it inuences the performance. This way bias detection is only a means of enhancing model’s performance instead of being a goal on its own which can raise ethical considerations. Moreover, some of the performance measures have been previously criticised as evaluation benchmarks for tasks they address. For instance, it is widely acknowledged in machine translation that BLEU score is a coarse and indirect indicator of a machine translation system’s performance [Callison-Burch et al. 2006]. Finally, similarly to our observations regarding datasets, most of the measures developed for quantifying gender bias are created and calculated only for binary genders. Even if a specic metric allows for analysing non-binary genders, it usually remains unmentioned. Armed with datasets (§5) suitable for gender bias analysis and formal gender bias denitions (§6), we focus herein on research on detecting and analysing the nature of gender bias in natural language, NLP algorithms, and downstream tasks. We discuss its challenges, and inuential lines of work. Natural language is known to exhibit societal biases. Gender bias, in particular, has been studied in a broad spectrum of texts such as portrayals of characters in movies, books, news and media. Choueiti et al [2014]; Ramakrishna et al [2015, 2017] examined gender dierences in portrayal of characters in movies and consistently show that female characters appear to be more positive in language use with fewer references to death and fewer swear words compared to male characters. However, Sap et al [2017] nd that, high-agency women frames are rare in modern lms. Rashkin et al [2018] use commonsense inference tasks on movie scripts’ corpus to unveil presence of gender bias nding that women’s looks and sexuality are highlighted, while men’s actions are motivated by violence, with strong negative reactions. Moreover, Bamman and Smith [2014] employ a probabilistic latent-variable model to extract event classes from biographies and nd that characterisation bias on Wikipedia with biographies of women containing signicantly more emphasis on events of marriage and divorce than biographies of men. Field and Tsvetkov [2019] show that although powerful women are frequently portrayed in the media, they are typically described as less powerful than their actual role in society. However, Asr et al [2021] report that there, in fact, is a gender gap in coverage of women in Canadian news outlets. Further, Hoyle et al [2019] use an unsupervised model to nd that dierences between descriptions of males and females in literature align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men. However, Garg et al [2018] show that gender bias has decreased in the last 100 years and that the women’s movement in the 1960s and 1970s had a signicant eect on women’s portrayals in literature and culture. To this end, Garg et al [2018] use word embeddings as a tool to observe the development of adjectives associated with men and women. This is possible since word embeddings learn harmful associations and stereotypes from the underlying data and thus, may serve as a means to extract implicit gender associations from a corpus to detect gender associations present in society [Bolukbasi et al 2016]. Similarly, Wevers [2019] show that word embeddings can be used to investigate shifts in language related to gender, while Friedman et al [2019] prove that word embeddings are able to characterise and predict statistical gender gaps in education politics, economics and health across cultures. A number of research has investigated dierences in language directed towards men and women. For instance, Tsou et al [2014] nd that comments on TED talks are more likely to be about the presenter than the content if the presenter is a woman. Fu et al [2016] analyse questions directed at male and female tennis players, nding that questions to men are rather about the game while questions directed at women are often about their appearance and relationships. Further, Voigt et al [2018] corroborate the former ndings, such as remarks on appearance being more often targeted towards women, responses to women being more emotive (non-neutral sentiment) and of higher sentiment in general which can be ascribed to benevolent sexism. While the above research unveils some of the ways gender bias is manifested in natural language, it gives only a limited view since most of this research has concentrated on binary gender identities and was mostly conducted in English. We note that there exist real-life applications with societal implications to algorithms detecting gender bias in natural language such as warning systems classifying texts as biased to notify readers. Biased datasets used in the training process are the primary source of gender bias in NLP methods [Zhao et al 2017]. Tan and Celis [2019]; Zhao et al [2019] examine datasets that were used as training corpora for the popular NLP methods and nd that the occurrence of male pronouns is consistently higher across all datasets and evidence of stereotypical associations. These gender imbalances lead to gender bias in the NLP systems, such as coreference resolution Zhao et al [2018a]. It has been shown that the level of bias encoded in a model diers depending on the training data. For instance, Chaloner and Maldonado [2019] study dierences in bias in a number of word embeddings trained on corpora from four domains showing the lowest bias in word embeddings trained on a biomedical corpus and the highest bias when trained on news data (higher than social media and Wikipedia-based corpus). Surprisingly, Lauscher and Glavaš [2019]’s ndings conrm that gender bias seems to be less pronounced in embeddings trained on social media texts. A common phenomenon leading to gender bias is a generic masculine pronoun which arises when the masculine form is taken as the generic form to designate all persons of any gender. This is especially the case in the gendered languages [Carl et al 2004]. Generic masculine poses a challenge in text interpretation since it is unclear if a given person denotation refers to a particular person or a generic form to describe all people in a specic group. For instance, in a sentence “A researcher must always test his model for biases.”, it is ambiguous if a particular researcher is considered or researchers in general. In particular, Hitti et al [2019] analyse data from Project Gutenberg and IMDB to identify such gender generalisations and detect that even 5% of each corpus is aected. Due to simple interpretation and ability to capture gender stereotypes occupation words have become a common domain for gender bias detection [Garg et al 2018]. Bolukbasi et al [2016] project the occupation words onto the she-he axis and nd that the projections are strongly correlated with the stereotypicality estimates of these words, suggesting that the geometric bias of word embeddings is aligned with crowd judgment of gender stereotypes. Sahlgren and Olsson [2019] show that male names are on average more similar to stereotypically male occupations with an according observation applying to female names. Rudinger et al [2018] demonstrate how occupation-specic bias is correlated with employment statistics and often so magnied. Although the majority of the research has focused on analysing gender bias in methods developed on English corpora, there have been some advances in extending this line of work to other languages. Developing language-specic methods to assess language model’s limitations is crucial to prevent bias propagation to downstream tasks in the analysed language [Bartl et al. 2020; Sun et al. 2019]. Findings made for English do not automatically extend to other languages, especially if those exhibit morphological gender agreement [Nozza et al 2021]. In particular, gender bias in word embeddings of languages with grammatical gender can be expressed in dierent ways, such as in a discrepancy in semantics between the masculine and feminine forms of the same noun in word embeddings. For example, it has been shown that when aligning Spanish to English word embeddings, the word “abogado” (male lawyer) is closer to “lawyer” than “abogada” (female lawyer) [Zhou et al 2019]. Interestingly, Lauscher and Glavaš [2019] nd that the level of bias in cross-lingual embedding spaces can roughly be predicted from the bias of the corresponding monolingual embedding spaces. Model architecture is analysed as one of the inuencing factors for bias in algorithms. For instance, Lauscher and Glavaš [2019] hypothesise that the bias eects reected in the distributional space depend on the preprocessing steps of the embedding model. Additionally, discovering bias in transformer models has proven to be more nuanced than bias-discovery in word embedding models [Kurita et al 2019; May et al 2019]. Nadeem et al [2021] hyphotesise that an ideal language model should not only be able to perform the task of language modeling, but also cannot exhibit stereotypical bias – it should avoid ranking stereotypical contexts higher than anti-stereotypical contexts. Recent research has aimed to rank language models in terms of bias they perpetuate [Nangia et al 2020; Silva et al 2021]. However, these studies present partially contradictory results presenting a need for more exhaustive testing. The inuence of the model’s size on the encoded (gender) bias has been examined. For instance, Silva et al [2021] nd that distilled models almost always exhibit statistically signicant bias and that the bias eect sizes are often much stronger than in the original models. Vig et al [2020] show that gender bias increases with the size of a model. Recently, Bender et al [2021] conrm this claim warning from potential risks associated with large language models. However, in a study of gender bias in cross-lingual language models Stańczak et al. [2021] do not nd signicant results to support this claim. It is dicult to understand the nature of biases encoded in large language models due to their complexity. However, applying interpretability methods can shed light on the models and biases preserved. For instance, Vig [2019] use visualisations to reveal attention patterns generated by GPT2 in the task of conditional language generation and show that the model’s coreference resolution might be biased. Vig et al [2020] probe neural models to analyse the role of individual neurons and attention heads in mediating gender bias and nd out that the source of gender bias is concentrated in a small part of the model. Moreover, Bhardwaj et al [2021] identify gender informative features (and discard them from the model as a mitigation technique). Until now research has aimed to detect gender bias in a strictly binary setting. We want to highlight the importance of a gender-inclusive research and discuss below publications that have step up to this task. Hicks et al [2015] collect a data set and develop visualisation tools that show relative frequency and co-occurrence networks for American English trans words on Twitter. Manzini et al [2019] extend the method presented in Bolukbasi et al [2016] and use their approach to nd non-binary gender bias by aggregating n-tuples instead of gender pairs. Saunders et al [2020] explore applying tagging to indicate gender-neutral referents in coreference sentences with a gender-neutral pronoun. Recently, Vig et al [2020] test the probability of a model to generate the pronoun they for a number of templates. The probability of the pronoun they is relatively low, however constant across probed professions. Bias in the above methods inuences many downstream tasks for which these methods are used, which presents a risk of propagating and amplifying gender bias [Zhao et al 2017, 2018a]. Thus, in the following, we analyse literature on gender bias in downstream applications. Machine Translation. Popular online machine learning services, such as Google Translate or Microsoft Translator, were shown to exhibit biases and to default to the masculine pronoun [Escudé Font and Costa-jussà 2019]. NLP models may learn associations of gender-specied pronouns (for a gendered language) and gender-neutral ones for lexicon pairs that frequently collocate in the corpora [Cho et al 2019]. This kind of phenomenon threatens the fairness of a translation system since it lacks generality and inserts social bias to the inference. Moreover, the output is not fully correct (considering gender-neutrality) and poses ethical considerations. When translating from a language without grammatical gender to a gendered one, the required clue about the noun’s gender is missing which poses a challenge for MT systems. Saunders et al [2020] nd that existing approaches tend to overgeneralise and incorrectly use the same inection for every entity in the sentence. However, gender is incorrectly predicted not only in the absence of the gender information. MT methods produce stereotyped translations even when gender information is present in the sentence. Schiebinger [2014] argue that scientic research fails to take this issue into account. Recently, Prates et al [2020] show that Google Translate still exhibits a strong tendency towards male defaults, in particular for elds typically associated with unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering, and Mathematics) jobs. Prates et al [2020] hypothesise that gender neutrality in language and communication leads to improved gender equality. Thus, translations should aim gender-neutrality, instead of defaulting to male or female variants. Coreference Resolution. Various aspects of gender are embedded in coreference inferences, both because gender can show up explicitly (e.g., pronouns in English, morphology in Arabic) and because societal expectations and stereotypes around gender roles may be explicitly or implicitly assumed by speakers or listeners [Cao and Daumé III 2020]. Although existing corpora have promoted research into coreference resolution, they suer from gender bias [Zhao et al. 2018a]. Webster et al [2018] nd that existing resolvers do not perform well and are biased to favour better resolution of masculine pronouns. Rudinger et al [2018] show how overall, male pronouns are more likely to be resolved as occupation than female or neutral pronouns across all systems. Moreover, Zhao et al [2018a] demonstrate that neural coreference systems all link gendered pronouns to stereotypical entities with higher accuracy than anti-stereotypical entities. Zhao et al [2018a] warn that bias encoded in word embeddings leads to sexism in coreference resolution. Further, Bao and Qiao [2019] show signicant gender bias when using popular NLP methods for coreference resolution on both sentence and word level, indicating that women are associated with family while men are associated with career. Language Generation. Henderson et al [2018] suggest that, due to their subjective nature and goal of mimicking human behaviour, data-driven dialogue models are prone to implicitly encode underlying biases in human dialogue, similar to related studies on biased lexical semantics derived from large corpora [Bolukbasi et al 2016; Caliskan et al 2017]. Cercas Curry and Rieser [2018] estimate that as many as 4% of conversations with chatbased systems are sexually charged. Further, Bartl et al [2020] nd that the monolingual BERT reects the real-world bias of the male- and female-typical profession groups through stereotypical associations. Stories generated by GPT-3 dier based on a perceived gender of the character in a prompt with female characters being more often associated with family, emotions and appearance, even in spite of a presence of power verbs in a prompt [Lucy and Bamman 2021]. Sentiment Analysis. Kiritchenko and Mohammad [2018] test 219 automatic sentiment analysis systems that participated in SemEval-2018 Task 1 Aect in Tweets [Mohammad et al 2018]. In particular, Kiritchenko and Mohammad [2018] examine a hypothesis that a system should equally rate the intensity of the emotion expressed by two sentences that dier only in the gender of a person mentioned and nd that the majority of the systems studied show statistically signicant bias. In particular, they consistently provide slightly higher sentiment intensity predictions for sentences associated with one gender (gender with more positive sentiment varies based on a task and system used). When predicting anger, joy, or valence, the number of systems with consistently higher sentiment for sentences with female noun phrases is higher than for male noun phrases. Bhaskaran and Bhallamudi [2019] show that analysed sentiment analysis methods exhibit dierences in mean predicted class probability between genders, though the directions vary again. As seen above, NLP methods tend to be consistently biased and associate harmful stereotypes with genders. Despite this fact, most of the papers that have focused on detecting gender bias in natural language, methods, or downstream tasks, have seen bias detection as a goal in itself or a means of analysing the nature of bias in domains of their interest. Some of this research has been followed up with bias mitigation methods (discussed in §8). However, often enough, ndings of this line of research are treated solely as a fact statement and not an action trigger. In particular, despite a number of evidence showing that NLP methods encode gender bias, developers are not required to provide any formal testing prior to releasing new models. Widely acknowledged models that have led in recent years to signicant gains on many NLP tasks have not included any study of bias alongside the publication [Conneau et al 2020; Devlin et al 2019; Peters et al 2018; Radford et al 2019]. Since these models were probed for gender bias only after their release, they might have already caused harm in real life applications. We strongly encourage including bias detection into the model development pipeline and see it as a necessary future development. So far, research has predominantly aimed to detect bias towards male and female gender, ignoring non-binary gender identities. However, it is crucial to design studies on gender bias detection that are gender-inclusive at all stages, from dening gender and bias, dataset choice to selecting bias detection method. As discussed in §3, gender manifests itself in dierent ways across languages. Hence, it can be expected that it’s also the case for gender bias. For instance, languages such as German, Hebrew and Russian use gender inections that reect grammatical genders of the nouns. Further, gender bias is grounded in societal and cultural views on gender and thus, its expressions potentially vary across languages. Expanding research to languages beyond English and including data from outside of the Anglosphere would lead to gaining a broader view on gender bias in societies. In particular, analysing cross-lingual data might enable a comparative studies of gender bias. While it is impossible to altogether remove gender bias from language or from NLP algorithms, research on gender bias mitigation is a signicant step towards developing fair systems. In specic applications, one might argue that gender biases in algorithms could capture valuable statistics such as a higher probability of a nurse being a female. Nevertheless, given the potential risk of employing machine learning algorithms that amplify gender stereotypes, Bolukbasi et al. [2016] recommend erring on the side of neutrality and using debiased methods. However, following D’Ignazio [2021], mitigating gender bias in AI systems is a short-term solution that needs to be combined with higher-level long-term projects in challenging the current social status quo. The main challenge in debiasing task is to strike a trade-o between maintaining model performance on downstream tasks while reducing the encoded gender bias [de Vassimon Manela et al 2021; Zhao et al 2018a]. Further, Bartl et al [2020]; Sun et al [2019] emphasise the need for more typological variety in NLP research as well as for language-specic solutions. Many of the mitigation methods rely on pre-dened words lists that are not scalable in a multilingual setup and are tedious to create. However, recent work on dictionary denitions for debiasing might obviate the need for predened word lists [Kaneko and Bollegala 2021b]. While prior work has mainly focused on mitigating gender bias in English, more recently, researchers have started to apply methods initially developed for English to other languages as well. Naturally, a signicant chunk of work for multilingual settings has been researched in the context of neural machine translation [Prates et al 2020; Vanmassenhove et al 2018]. This stream of research has conrmed that language-specic solutions are required, since gender is expressed in dierent ways across languages. For instance, transferring a method successful in gender bias mitigation for English to German may be ineective which emphasises the need for more typological variety in research as well as language-specic solutions [Bartl et al 2020]. Therefore, it is crucial to develop (language-specic) debiasing methods, especially for relatively new methods, to assess these limitations. Next, Kiritchenko and Mohammad [2018] observed that dierent debiasing approaches have varying eects on the analysed word embedding architectures. Many of the current debiasing methods are evaluated only on selected downstream tasks without testing them in a broader scope. Hence, additional and potentially costly tests are required before applying these techniques to other, previously un-tested tasks since their eectiveness there is unclear [Jin et al 2021]. Therefore, we encourage research on debiasing methods in the early modelling stages. Dierent approaches have been developed to mitigate gender bias in NLP. In this paper, we classify each of these methods following the two main categories, similarly to Sun et al [2019] – debiasing using data manipulation 8.1 and by adjusting algorithms 8.2 – while extending the scope of our analysis with recent publications and incorporating word embeddings mitigation methods into the methodoligical adjustment category. We summarise the identied lines of gender bias mitigation methods in Table 3 together with the respective publications. Debiasing using data manipulation commonly refers to counterfactual data augmentation, gender tagging, adding context, and balanced ne-tuning. Below we describe these approaches in detail. 8.1.1 Data Augmentation. Many concerns have been posed regarding modern NLP systems having been trained on potentially biased datasets, as as these biases can be perpetuated to downstream tasks and eventually society in the form of allocational harms [Hovy and Prabhumoye 2021]. Therefore, Costa-jussà and de Jorge [2020] claim that developing methods trained on balanced data is a rst step to eliminating representational harms. In order to attenuate the impact of gender bias from the dataset used, Zhao et al [2018a] propose a rule-based approach to generate an auxiliary dataset where all-male entities are replaced by female entities (and vice-versa) and suggest to train methods on the union of the original and augmented dataset. Thus, both male and female genders are equally represented in the dataset. For instance, a sentence My son plays with a car. would be transformed into My daughter plays with a car. Therefore, to apply this method, a list of gendered pairs (such as son–daughter) is required. Similarly, Emami et al [2019] propose to extend a training set for coreference resolution by switching every entity pair. A method for debiasing gender-inected languages is demonstrated in Zmigrod et al [2019], where sentences are reinected from masculine to feminine (and vice-versa) in a counterfactual data augmentation (CDA) scheme. Since this method analyses each word separately, it is not applicable to more complex sentences involving coreference resolution. However, it introduces a feasible debiasing approach for languages beyond English. Hall Maudslay et al [2019] develop a name-based version of CDA, in which the gender of words denoting persons in a training corpus are swapped probabilistically in order to counterbalance bias. Due to its simple implementation, counterfactual data augmentation has been widely applied to mitigate gender bias. Since the model observes the same scenario in the doubled (for binary gender) sentences, it can learn to abstract away from the entities to the context [Emami et al 2019]. This method has shown encouraging results in mitigating bias in contextualised word representations such as ELMo and monolingual BERT [Bartl et al 2020; de Vassimon Manela et al 2021; Sen et al 2021; Zhao et al 2019], and for hate speech detection [Park et al 2018]. Nonetheless, collecting annotated lists for gender-specic pairs can be expensive, and the method essentially doubles the size of the training data. To this end, de Vassimon Manela et al [2021] compare ne-tuning contextualised word representation on augmented and un-augmented datasets and show that ne-tuning solely on an augmented corpus successfully decreases gender bias. Another method of gender bias mitigation via data augmentation is presented in Stanovsky et al [2019] who suggest a simple approach of “ghting bias with bias” and add stereotypical adjectives to describe entities of the respective gender, e.g., “The pretty doctor asked the nurse to help her in the procedure.”. However impractical this method is, relying on accurate coreference resolution, it has shown to reduce bias in the tested languages. 8.1.2 Gender Tagging. Another stream of work has concentrated on incorporating external or internal gender information during training. This method has been widely employed in debiasing neural machine translation models to mitigate the issue of male default. Moryossef et al [2019] append a short phrase at inference time which acts as an indicator for the speaker’s gender, e.g., “She said:”, while similarly, Vanmassenhove et al [2018] use sentence-level annotations. In order to extend the mitigation method to be applicable to sentences with more than one gendered entity, Stafanovičs et al [2020] utilise token-level annotations for the subject’s grammatical gender. Habash et al [2019] propose a post-processing method that is an intersection of gender tagging and CDA and test it on Arabic. In gender-aware debiasing, a gender-blind system is being turned into a gender-aware one by identifying gender-specic phrases in the system’s output and subsequently oering alternative reinections. In the domain of machine translation, Saunders et al [2020] propose an approach based on ne-tuning a model on a small, articial dataset of sentences with gender inection tags which are then replaced by placeholders. However, the results of this scheme are ambiguous, and this method is not well suited for translating sentences with multiple entities. Methods relying on gender tagging are a exible tool for controlling for bias. However, we note that these methods do not inherently remove gender bias from the system [Cho et al 2019]. Additionally, gender tagging requires meta-information on the gender of the speaker, which is often either expensive or unavailable. 8.1.3 Adding Context. Alongside including the speaker’s information as in the above examples, Basta et al [2020] concatenate the previous sentence from a corpus to increase the context. Using the additional information only in the decoder part of the Transformer architecture ultimately reduces training parameters, simplies the model, and requires no further information for training or inference. Basta et al [2020] show that this method improves the performance of machine translation with coreference resolution tasks. However, Savoldi et al [2021] note that this improvement might not be due to the added gender context, but for instance, a regularisation eect. 8.1.4 Balanced Fine-Tuning. Balanced ne-tuning incorporates transfer learning from a less biased dataset. In the rst step, a model is trained on a large, unbiased dataset for the same or similar downstream task and is then ne-tuned on a target dataset which is more biased [Park et al 2018]. Such a training regime obviates potential over-tting to a biased dataset. This method suers from a severe limitation, namely assuming an existence of an unbiased dataset in its initial step, which is usually infeasible to obtain and thus, not applicable in real-life applications. On the other hand, Saunders and Byrne [2020] consider gender bias in machine translation as a domain adaptation task and use a handcrafted gender-balanced dataset together with a lattice re-scoring module to mitigate the consequences of initial training on unbalanced data. Saunders and Byrne [2020] consider three aspects of the adaptation problem: creating less biased adaptation data, parameter adaptation using this data, and inference with the debiased models produced by adaptation. However, the need for a gender-balanced dataset for a specic domain might be a drawback of this approach. Costa-jussà and de Jorge [2020] use an inverse approach and train their model on a larger corpus and ne-tune it with a gender-balanced corpus showing that their approach successfully mitigates gender bias and increases performance quality even if the balanced dataset is coming from a dierent domain. However, Savoldi et al [2021] note that this approach does not account for the qualitative dierences in how men and women are portrayed [Savoldi et al. 2021]. Instead of manipulating the underlying data, a number of gender debiasing methods have been implemented to approach the issue via algorithm adjustment. Such techniques can be categorised into the following groups: projection-based debiasing, constraining models’ predictions, applying adversarial learning approaches, and other. 8.2.1 Projection-Based Debiasing. To the best of our knowledge, Schmidt [2015] propose the rst word embedding debiasing algorithm and remove multiple gender dimensions from word vectors. In parallel, instead of completely removing gender information, Bolukbasi et al [2016] suggest shifting word embeddings to be equally male and female in terms of their vector direction. For instance, a debiased embeddings for grandmother and grandfather will be equally close to babysit without neglecting the analogy to gender. More formally, Bolukbasi et al [2016] propose two debiasing methods, hard- and soft-debiasing. The rst step for both of them consists of identifying a list of gender-neutral words and a direction of the embedding that captures the bias. Hard-debiasing (or ‘Neutralise and Equalise method’) ensures that gender-neutral words are zero in the gender subspace and equalises sets of words outside the subspace and thereby enforces the property that any neutral word is equidistant to all words in each equality set (a set of words which dier only in the gender component). For instance, if (grandmother, grandfather) and (guy, gal) were two equality sets, then after equalisation, ‘babysit’ would be equidistant to grandmother and grandfather and also to gal and guy, but closer to the grandparents and further from the gal and guy. This approach is suitable for applications where one does not want any such pair to display any bias with respect to neutral words. The disadvantage of equalising sets of words outside the subspace is that it removes particular distinctions that are valuable in specic applications. For instance, one may wish a language model to assign a higher probability to the phrase to ‘grandfather a regulation’ since it is an idiom, unlike ‘grandmother a regulation’. The soft-debiasing algorithm reduces dierences between these sets while maintaining as much similarity to the original embedding as possible, with a parameter that controls for this trade-o. In particular, soft-debiasing applies a linear transformation that seeks to preserve pairwise inner products between all the word vectors while minimising the projection of the gender-neutral words onto the gender subspace. Both hard- and soft-debiasing approaches have been applied in research to word embeddings and language models. Bordia and Bowman [2019] validate the soft-debiasing approach to mitigate bias in LSTM based word-level language models. Park et al [2018] compare hard-debiasing method to other methods in the context of abusive language detection. Sahlgren and Olsson [2019] apply hard-debiasing to Swedish word embeddings and show that this method does not have the desired eect when tested on selected downstream tasks. Sahlgren and Olsson [2019] argue that these unsatisfactory results are due to including person names in their training process. Interestingly, Ethayarajh et al [2019] show that debiasing word embeddings post hoc using subspace projection is, under certain conditions, equivalent to training on an unbiased corpus. Similarly to Bolukbasi et al [2016], Karve et al [2019]; Sedoc and Ungar [2019] aim to identify the bias subspace in word embeddings using a set of target words and a debiasing conceptor , a mathematical representation of subspaces that can be operated on and composed by logic-based manipulations. However, these methods strongly rely on the pre-dened lists of gender-neutral words Sedoc and Ungar [2019]. Moreover, Zhao et al [2018b] prove that an error in identifying gender-neutral words aects the performance of the downstream model. Bordia and Bowman [2019]; Zhao et al [2018b] notice a trade-o between perplexity and gender bias as in an unbiased model, male and female words are predicted with an equal probability. This can be undesirable in domains such as social science and medicine. While Gonen and Goldberg [2019] claim that debiasing is primarily supercial since a lot of the supposedly removed bias can still be recovered due to the geometry of the word representation of the gender neutralised words, Prost et al [2019] show that soft-debiasing can even increase the bias of a downstream classier by removing noise from gender-neutral words and ultimately providing a less noisy communication channel for gender clues. Recently, Liang et al [2020] use DensRay [Dufter and Schütze 2019], an interpretable method for identifying the embedding subspace using projections and then evaluate gender bias in masked language models by comparing the dierence in the log-likelihood between male and female pronouns in a template “[MASK] is a/an [occupation].”. However, this method heavily relies on a list of occupations and a simple template. Further, Dev et al [2020] employ an orthogonal projection to gender direction [Dev and Phillips 2019] to debias contextualised embeddings and test it on a NLI task with gender-biased hypothesis pairs. However, this method can only be applied to the model’s non-contextualised layers. Kaneko and Bollegala [2021a] obviate this limitation in a ne-tuning setting. Their method applies an orthogonal projection only in the hidden layers and proves to outperform Dev et al [2020]. Additionally, this method is independent of model architectures or their pre-training method. However, this approach requires a list of attribute words (e.g., she, man, her) and target words (e.g., occupations) to extract relevant sentences from the corpus, making their method highly reliant on this list. 8.2.2 Constraining Output. A simple approach to debiasing algorithms is to constrain model output post-hoc. To this end, Zhao et al [2017] propose a debiasing techique that constrains model predictions to follow a distribution from a training corpus, e.g., the ratio of male and female pronouns. Thus, this method is highly dependent on the gender balance and bias in the underlying data. In the eld of language generation, Ma et al [2020] introduce controllable debiasing as an unsupervised text revision task that aims to correct the implicit bias against or towards a specic character portrayed in a language model generated text. For this purpose, they create an encoderdecoder model that rewrites a text to portray females as more agent (in terms of Sap et al [2017]’s connotation frames). However, their approach relies strongly on an external corpus of paraphrases. 8.2.3 Adversarial Learning. Another strain of work has employed adversarial learning as a debiasing method. Li et al [2018] propose a method for removing model biases by explicitly protecting demographic information (such as gender) during model training. However, Elazar and Goldberg [2018] claim that word representations preserve traces of the protected attributes and recommend external verication of the method. Similarly, Zhang et al [2018] apply adversarial learning by including gender as a protected variable and having the generator learn with respect to it. In general, the objective of such a model is to maximise the predictor’s ability to predict a variable of interest while fooling the adversary to predict the protected attribute. However, in general, adversarial learning is often an unstable method and can only be used when gender is a protected attribute rather than a variable of interest. 8.2.4 Other. Several other methods have been tested to mitigate gender bias in NLP methods. Alongside projection-based methods for debiasing word embeddings, another approach to debiasing word embeddings has aimed to learn their gender-neutralised variant. In particular, Zhao et al [2018b] propose to train word embeddings such that protected attributes are neutralised in some of the dimensions, resulting in gender-neutral word representations. Restricting the information of protected attributes in certain dimensions enables its removal from an embedding. Additionally, other than the method presented in Bolukbasi et al [2016] gender-neutral words are learned jointly in the training process instead of being manually created. However, Sun et al [2019] note that it is unclear if gender-neutralised word embeddings are applicable to languages with grammatical genders. Adjusting the loss function has proven to be another viable method for gender bias mitigation. In particular, Qian et al [2019] introduces a new term to the loss function, which attempts to equalise the probabilities of male and female words (based on a pre-dened list) in the output and evaluate it on a text generation task. We see two main limitations of this approach. First, it relies on a straightforward denition of bias (i.e., an equal number of gender mentions). Second, as with many other methods, it requires a list of gender pairs, a limitation we discuss above. Gender-preserving debiasing has been introduced to mitigate gender bias, accounting that not all gender associations are stereotypical. Kaneko and Bollegala [2019] split a given vocabulary into four mutually exclusive sets of word categories: words that are female-biased but non-discriminative, male-biased but non-discriminative, gender-neutral words, and words perpetuating stereotypes. Kaneko and Bollegala [2019] learn word embeddings that preserve the information for the gendered but non-stereotypical words protects the neutrality of the gender-neutral words while removing the gender-related biases from stereotypical words. The embedding is learnt using an encoder in a denoising autoencoder, while the decoder is trained to reconstruct the original word embeddings from the debiased embeddings. However, creating a word list with the above-mentioned categories of words is time-consuming, and word categorisation might not be straightforward. Jin et al [2021] investigate incorporating bias mitigation into the model’s objective. First, an upstream model is ne-tuned with a bias mitigation objective which consists of a text encoder and a classier head. Subsequently, the encoder from the upstream model, jointly with the new classication layer, are again ne-tuned on a downstream task. Jin et al [2021] note that upstream bias mitigation, while less eective, is more ecient than direct bias mitigation methods without ne-tuning. However, it requires a tailored evaluation for the downstream task. After presenting probing datasets, formal denitions, detection, and mitigation methods, we next present the main ndings we make throughout this survey. We nd that existing research on gender bias has four main limitations and discuss them in the following. Gender in NLP. It is not uncommon for studies about gender to be reported without any explanation of how gender labels are ascribed, and the ones that do, explain the imputation of gender categories in a debatable way [Larson 2017]. Using gender as a variable in NLP is an ethical issue, thus unreectively assigning gender category labels may violate ethical frameworks that demand transparency and accountability from researchers [Larson 2017]. Therefore, it is crucial to ask how researchers can use NLP tools to investigate the relationship between gender and text meaningfully, yet without harmful stereotypes Koolen and van Cranenburgh [2017]. To obviate this risk, Larson [2017] suggest formulating research questions with explicit denitions of gender, avoiding using gender as a variable unless it is necessary. Not being explicit about the ascription of the category of gender as a variable to participants brings into question the internal and external validity of research ndings because it makes it dicult to near-impossible for other scholars to reproduce, test, or extend study ndings [Larson 2017]. We nd that researchers often decide to dene gender in their study as binary. However, making this assumption is an oversimplication of gender complexity and can perpetuate harms to nonbinary people [Behm-Morawitz and Mastro 2008; Fast et al 2016]. We encourage researchers to dene gender in a transparent and inclusive manner, to expand corpora with inclusive pronouns, and evaluate models on non-binary pronouns as well to mitigate these harms. So far models’ performance on downstream tasks has been consistently lower for non-binary pronouns compared to the binary pronouns [Cao and Daumé III 2020; Sun et al. 2021]. Monolingual focus. Gender bias is grounded in societal and cultural views on gender, and thus, its expressions vary across languages. Expanding research to languages beyond English and including data from outside of the Anglosphere would lead to gaining a broader view on gender bias in societies which we strongly encourage. However, most prior research on gender bias has been monolingual, focusing predominantly on English or a small number of other high-resource languages such as Chinese [Liang et al 2020] and Spanish [Zhao et al 2020] with the notable exception of a broader multilingual analysis of gender bias in machine translation [Prates et al 2020] and language models [Stańczak et al. 2021]. Need for formal testing. Most of the papers that have focused on detecting gender bias in natural language, methods, or downstream tasks, have seen bias detection as a goal in itself or a means of analysing the nature of bias in domains of their interest. Widely acknowledged models that have led in recent years to signicant gains on many NLP tasks have not included any study of bias alongside the publication [Conneau et al 2020; Devlin et al 2019; Peters et al 2018; Radford et al 2019]. In general, these methods are tested for biases only post-hoc when already being deployed in real-life applications, potentially posing harm to dierent social groups [Mitchell et al 2019]. Since these models were probed for gender bias only after their release, they might have already caused societal harms. We nd that bias detection should be included in the model development pipeline at early stages and see enforcing this change as a primary challenge. The way to ensure that researchers abide by ethical principles is to hold them accountable when research projects are planned, i.e., requiring project proposals and publications to include ethical considerations and, later, during the peer review process. Limited denitions. However, to introduce formal testing comprehensive and multi-faceted bias measures are required. We nd that similarly to research within societal biases Blodgett et al [2020], work on gender bias in particular, suers from incoherence in usage of evaluation metrics. Most of the publications on gender bias consider only one way of dening bias and do not engage enough parallel research to combine these methods. Gender bias can be expressed in language in many nuanced ways which poses stating a comprehensive denition as one of the main challenges in this research eld. Finally, we strongly encourage developing standard evaluation benchmarks and tests to enhance comparability. In this paper, we present a comprehensive survey of 304 papers on gender bias in natural language and NLP methods published since gender bias has been studied in NLP. We nd four major limitations in the existing research and see overcoming these limitations as crucial for further development of this eld. First, most research lacks transparent and inclusive gender and gender bias denitions. Gender is mainly treated as a binary variable which disagrees with social science position. Next, the majority of the work disregards low-resource languages, concentrating solely on English and other highresource languages such as Spanish and Chinese, which imposes a strongly restricted view on the nature of gender bias in NLP. Moreover, despite a myriad of papers on gender bias in NLP methods, most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. This leads to deployment of models that lead to potential societal harms. Finally, we nd that the methodology used in this research eld is fundamentally awed, covering only limited aspects of gender bias and lacking baselines for evaluation and testing pipelines.