North Carolina State UniversityNorth Carolina State University Testing machine learning software for ethical bias has become a pressing current concern. In response, recent research has proposed a plethora of new fairness metrics, for example, the dozens of fairness metrics in the IBM AIF360 toolkit. This raises the question: How can any fairness tool satisfy such a diverse range of goals? While we cannot completely simplify the task of fairness testing, we can certainly reduce the problem. This paper shows that many of those fairness metrics eectively measure the same thing. Based on experiments using seven real-world datasets, we nd that (a) 26 classication metrics can be clustered into seven groups, and (b) four dataset metrics can be clustered into three groups. Further, each reduced set may actually predict dierent things. Hence, it is no longer necessary (or even possible) to satisfy all fairness metrics. In summary, to simplify the fairness testing problem, we recommend the following steps: (1) determine what type of fairness is desirable (and we oer a handful of such types); then (2) lookup those types in our clusters; then (3) just test for one item per cluster. To support that processing, all our scripts (and example datasets) are available at https://github.com/Repoanonymous/ Fairness_Metrics. Ethics in Software Engineering, Machine Learning with and for SE Increasingly, software is being used for critical decision-making processes, such as patient release from hospitals [15,83], credit card applications [48], hiring [81], and admissions [19]. According to guidelines from the European Union [13] and IEEE [16], a software cannot be used in real-life applications if it is found to be discriminatory toward an individual based on any sensitive attribute such as gender, race, or age. Hence “fairness testing” is now an open and pressing problem in software engineering. As shown in Table 1, researchers have proposed a plethora of fairness metrics for measuring fairness, and that number is growing. We can nd multiple ways to measure “fairness”. For example: (a) The Fairlearn [20] tool lists 16 metrics; (b) The Fairkit-learn toolkit [62] comes with 16 metrics, and (c) The IBM AIF360 toolkit [24] implements 45 fairness metrics. However, researchers in this area only use a few metrics in their papers [39,53,59,64,75,92]. For example, in our literature review papers from the last three years, we see only a handful of papers (13 out of 60 to the best of our knowledge) North Carolina State University using more than ve fairness metrics to evaluate their method. This is surprising since all of them ignore more than half the known metrics. But is that wise? • Should we reject papers that “only” use (e.g.) ve metrics? • Or should researchers always use dozens of metrics? •When we use automatic tools to optimize for fairness, should we optimize for dozens of goals? • Or is optimizing for a smaller set sucient? The conjecture that is tested by this paper is that too many spurious metrics which all measure very similar things. If that were true, then it should be possible to simplify fairness assessment as follows: Run metrics on real-world data. Find clusters of correlated metrics. Prune “insensitive clusters”. Only use one metric per surviving cluster. This paper experiments with seven datasets and nds that (a) 26 classication fairness metrics can be clustered into just seven groups; (b) four dataset metrics can be clustered into three groups and that (c) these clusters actually predict for dierent things. That is, it is no longer necessary (or even possible) to satisfy all these fairness metrics. Hence, to simplify fairness testing, we recommend (a) determining what type of fairness is desirable (and we oer a handful of such types); then (b) looking up those types in our clusters; then (c) testing for one item per cluster. This paper is structured around these research questions. RQ1:Do current fairness metrics agree with each other? Our experiments show that current fairness metrics often disagree, markedly. RQ2:Can we group (cluster) fairness metrics based on similarity? We nd sets of similar metrics using agglomerative clustering [5]. RQ3:Are some fairness metrics more sensitive to change than others? While most are sensitive, some are not. RQ4:Can we achieve fairness based on all the metrics at the same time? It is challenging to do so since some of them are competing goals and some are contradictory based on denitions. We assert this work isnovel,signicant,sound&veriable. Novel:Our metrics selection tactic prunes spurious metrics (i.e., metrics which merely echo the results of other metrics). This tactic was tested in an extensive case study applying 30 fairness metrics and grouped them into clusters (RQ1 & RQ2). This study is the rst one to perform such a sensitivity meta-analysis of fairness testing and to warn that some metrics are unresponsive to data changes Table 1: Mathematical denitions of the classication and dataset metrics used in this research. Denitions are collected from IBM AIF360 [24], Fairkit-learn [62] & Fairlearn [20]. For denitions of the terms used here, see Table 3. (RQ3). This study also presents a meta-analysis of metrics ability to achieve fairness after application of bias mitigation technique (RQ4). Signicant:Our work signicantly claries and simplies fairness testing. We show not only how to assess fairness metrics but which metrics oer little additional information content. This is important since the art of software fairness testing is evolving rapidly. Studies like the one are essential to documenting what methods are “best” (as opposed to those that might distract from core issues). Sound:All our empirical results were repeated 25 times. Also, all our analytical results (in §5.1) were double-checked via empirical analysis. Further, this study is far more detailed than prior work since it explores multiple bias mitigation algorithms on seven datasets (than prior work [38,40–42,58] was tested on far fewer metrics and far fewer datasets). Veriable:In order to support replication and reproduction of our results, all our datasets and scripts are publicly available at https://github.com/Repoanonymous/Fairness_Metrics. Before beginning, we digress to make three points. Firstly, Table 1 lists dozens of metrics currently seen in the SE fairness testing literature. This paper makes an empirical argument that this list is too long since many of these metrics oer similar conclusions. One alternative to our empirical argument is an analytical argument that metric X (e.g.) is equivalent to metric Y. Later in this paper (see §5.1), we make the case that to reduce the space of metrics to be explored, that kind of analytical argument may actually be misleading. Secondly to be clear, while we can reduce dozens of metrics down to ten, there will still be issues of how to trade-o within this reduced set. That said, we assert our work is valuable since debating the merits of, say, ten metrics is a far simpler task than trying to resolve all the conicts between 30. Further, and more importantly, our methods could be used as a litmus test to prune away spurious new metrics that merely report old ideas but in a dierent way. Also, thirdly, even after our mitigation algorithms, some fairness metrics still can contradict each other regarding presence of bias. Hence, in §5.3, we oer an extensive discussion on what to do in that situation. There is much evidence of machine learning (ML) software showing biased behavior. For example, language processing tools are more accurate on English written by Anglo-Saxons than written by people of other races [32]. An Amazon hiring tool was found to be biased against women [12]. YouTube makes more mistakes while generating closed captions for videos with female voices than males [71,84]. A popular risk-score predicting algorithm was found to be heavily biased against African Americans showing a higher error rate while predicting future criminals [8]. Gender bias is also prevalent in Google [34] and Bing [62] translators. Due to so many undesirable events, academic researchers and big industries have started giving immense importance to ML software fairness. Microsoft has launched ethical principles of AI where “fairness” has been given the topmost priority [18]. IBM has built a toolkit called AI Fairness 360 [11] containing the most noted works in the fairness domain. In recent years, the software engineering research community has also started exploring this topic actively. ICSE’18 held a special workshop for “software fairness” [14]. ASE’19 held another workshop called EXPLAIN, where fairness and explainability of ML models were discussed [17]. Johnson et al. have created a public GitHub repository for data scientists to evaluate ML models based on quality and fairness metrics simultaneously [62]. As to technology developed to detect and x these issues of fairness, we can see three groups: fairness testing, model-based mitigation, and fairness metrics, . Fairness Testing: The idea here is to generating discriminatory test cases and nding whether the model shows discrimination or not. The rst work on this was THEMIS, done by Galhotra, et al. [57]. THEMIS generates test cases by randomly perturbing attributes. AEQUITAS [86] improves the way of test case generation to become more ecient. Aggarwal, et al. combined local explanation and symbolic execution to generate a better black-box testing strategy [21]. Model Bias Mitigation: There are three techniques used to remove bias from model behavior. The rst one is “pre-processing” where before model training, bias is removed from training data. Some popular prior work includes optimized pre-processing [35], Fair-SMOTE [41] and reweighing [65]. The second one is “in- processing” where after model training, the trained model is optimized for fairness. Popular prior work includes prejudice remover regularizer [68] and meta fair classier [37]. The last one is “postprocessing” where while making predictions, model output is changed to remove discrimination. Some noted works include reject option classication [67] and calibration [75]. There is some work that combines two or more of these techniques, such as Fairway [42], a combination of “pre-processing” and “in-processing”. While the fairness testing and model bias mitigation are important areas, we note that before we can declare success in those two areas, we rst need some way to measure that success. Accordingly, this paper focuses on the third area called: Fairness Metrics: Early work in this area was done by Verma, et al. [89] who divided 20 fairness metrics into ve groups based on the theoretical denitions. Hinnefeld, et al. made a comparative analysis of four fairness metrics [60]. Wang, et al. did a user study to nd a relation between fairness metrics and human judgments [93]. There are also some papers coming from industry on the topic. LinkedIn has created a toolkit called LiFT for scalable computation of fairness metrics as part of large ML systems [88]. Recently, Amazon internally published an empirical study based on 18 fairness metrics [52]. While all that research is certainly insightful, in some sense that work has been too successful. As mentioned in the introduction, the above work has now generated a plethora of metrics. Hence, for the rest of this paper, we check if we can simplify the current space of metrics. In our work, we collected all the metric denitions from the IBM AI Fairness 360 GitHub repository. Table 1 lists the metrics studied in this paper. The Fairkit and Fairlearn columns in Table 1 show the metrics that are common among the IBM AIF360 metrics and metrics from Fairkit [62] (16 out of 16 available metrics) and Fairlearn [20] (7 out of 16 metrics) toolkit. Before explaining fairness metrics, we need to understand some terminology. Table 2 contains seven binary classication datasets. The binary outcomes are favorable if it gives an advantage to the receiver (i.e., being hired for a job, getting credit card approved). Each of these datasets has at least one protecte d attribute that divides the population into two groups (privileged & unprivileged) that have dierences in terms of benets received. “sex”, “race”, “age” are examples of protected attributes. The goal of group fairness is, based on the protected attribute, privileged and unprivileged groups will be treated similarly. While individual fairness tries to provide similar outcomes to similar individuals. A fairness metric is a quantication of unwanted bias in training data or models. Table 1 shows a sample of such metrics. When selecting these particular metrics, we skipped over: •Metrics for which we could not access precise denitions and implementations in IBM AIF360 toolkit [24]; •Metrics for which we could not nd publications to use as baselines in this paper. These two selection rules resulted in the 30 metrics of Table 1, which divide as follows: Classication Metrics:These measure fairness based on classication results and are labeled in Table 1 using a Metric Id beginning with C. Two inputs are needed to measure this: the rst one is original dataset with true labels and the second one is predicted dataset. In case of binary classication, classication metrics can be calculated from confusion matrix. Table 3 shows a combined confusion matrix where every cell is divided based on the protected attribute. Dataset Metrics:While classication metrics relate to predictions made from models, dataset metrics discuss learner-independent properties of the data. These are labeled in Table 1 using a Metric Id beginning with D. Only one input is needed to compute this: original dataset or transformed (by some bias mitigation algorithm) dataset. It can be applied for both group and individual fairness. Distortion Metrics:For completeness, we note that AIF360 includes a third set of metrics called distortion metrics. While these metrics are not seen extensively in the current literature, they would be a worthy target for future research. In Table 1, each metric has an ideal value representing the bestcase scenario. This means at ideal value according to the metric privileged and unprivileged groups are treated equally. For most of the metrics, the ideal value is zero, while in some cases where the metric is a ratio, the ideal value is one. If the ideal value for a metric is zero, a positive value denotes an advantage for the unprivileged group, while a negative value denotes an advantage for the privileged group. On the other hand, if the ideal value for a metric is one, a value < one denotes an advantage for the privileged group and > one denotes an advantage for the unprivileged group. To use these metrics, some threshold must be applied to report “fair” or “unfair”; •For metrics with ideal value 0: the IBM AIF360 toolkit [24] uses the following denition of “fair”: ranges between -0.1 to 0.1 as “fair” (so “unfair” means values outside that range). •For metrics with ideal value 1: the IBM AIF360 toolkit [24] uses the following denition of “fair”: ranges between 0.8 to 1.2 as “fair” (so “unfair” means values outside that range). This paper analyzes the 30 fairness metrics in Table 1 using the seven datasets described in Table 2. In that work, we use one Table 3: Mathematical denition of various confusion matrix based rates. These are used to calculate fairness metrics dened in Table 1. PredictedTPPPV = TP/(TP+FP)FPFDR = FP/(TP+FP) .PositiveTPR = TP/(TP+FN)FPR = FP/(FP+TN) PredictedFNFOR = FN/(TN+FN)TNNPV = TN/(TN+FN) NegativeFNR = FN/(TP+FN)TNR=TN/(TN+FP) baseline model and two models tuned by pre-processing and inprocessing algorithms to compare with: • Baseline:We used a logistic regression model for creating baseline results. Logistic regression is widely used in the fairness domain as baseline model [36,42–44,68]. We used scikit-learn implementation with ‘l2’ regularization, ‘lbfgs’ solver, and maximum iteration of 1000. • Reweighing:A widely used [22,25,40,63,78] pre-processing method proposed by Kamiran et al. [65]. Here, before model training, examples in each group, and label are given dierent weights to ensure fairness. • Meta Fair Classier:An in-processing method proposed by Celis et al. [37], which is a widely used meta algorithm [27,38, 58,74]. The optimization algorithm is developed to improve 11 fairness metrics with minimal loss in accuracy. The last two bias mitigation algorithm implementations are taken from IBM AIF360 [26]. Our metrics selection strategy, requires a clustering algorithm. Two class of such clustering algorithms are (a) partitioning clustering and (b) hierarchical clustering. Here we are grouping fairness metrics based on similarity, not on distance, and we have no prior idea about the number of clusters. Thus, in this case, the ideal choice is hierarchical clustering. Agglomerative clustering [5] is a hierarchical bottom-up clustering approach that is widely used in the ML community [23,49–51,54,73,77,82,95]. In this approach, the closest pairs of items are grouped together. These closest of these groups are then grouped into a higher-level group. This repeats Figure 1: Agglomerative clustering of classication metrics (using Spearman rank correlation). Here x-axis shows the classication metric Ids from Table 1 and y-axis shows the dissimilarity measure between clusters. until everything falls into one group. We used the average pairwise dissimilarity between objects in two dierent clusters as linkage method between groups. This process creates a dendrogram, a hierarchical structure of the groups/clusters obtained by betweencluster distance or dissimilarity. From this tree of groupings, we use the within-cluster similarity from the dendrogram, look for the largest distance that we can travel vertically without crossing any horizontal line [1,47,85], and extract the clusters at the largest change in SSE. Figure 1 shows the dendrogram created for the classication metrics using the above described method. Table 4 shows that we get seven clusters from 26 classication metrics. Following a similar process for dataset metrics we get three clusters as shown in Table 5. To build these clusters and dendrograms, we measure the similarity of two metrics. In this paper, by “similarity” we mean, they are measuring the similar bias in the models/dataset. Such similar metrics will show a similar pattern of changes in bias when models are built using dierent parts of the data or dierent bias removal algorithms are used. To compute this similarity, we sample from our model training procedure (see §3.4.2) that computes our metrics 25 times, each time using dierent train/validation/test samples of the data. Next, for each dataset, for those 25 numbers, we use correlation to assess similarity. Two widely used denitions of correlation [45,49–51,61,76, 82,95] are the (a) Pearson correlation (which evaluates the linear relationship between two continuous variables) and the (b) Spearman rank correlation (which is a non-parametric measure of rank correlation that evaluates the monotonic relationship between two continuous or ordinal variables). We choose Spearman rank correlation, as it measures the monotonic relationship between two variables and is less aected by outliers. We summarize our experimental setup as follows. 3.4.1Data Pre-processing: Three dierent pre-processing steps are performed before using the data [56,72,80] for model building. At rst, each categorical value in the dataset is converted either using a label encoder or by one hot encoder. Then the protected attributes are changed into ones and zeros from their original values. Here we denote the privileged attribute as one and unprivileged as zero. Finally, we use min-max normalization in the datasets to normalize the data before building the models. 3.4.2Model Training: We used ve fold cross-validation repeated ve times with random seeds build training/ test sets (as recommended by [66,80,87,90]). This step is to divide the data into multiple subsets of data with various degrees of bias. We train three models in each iteration (a) baseline model: here we use the training data to build a logistic regression model; (b) Reweighing model: here we rst train the reweighing method, then use the learned model to transform the training data to achieve group fairness. Using the transformed data, we train a logistic regression from scikit-learn with ‘l2’ regularization, ‘lbfgs’ solver and maximum iteration of 1000; and (c) Meta Fair Classier model: here to train the meta fair classier model, we use the training data to build multiple meta fair classier model with dierent values of𝜏(a hyperparameter for fairness penalty in the model) and measure the bias in the model using the validation set. Then to build the nal model, we select the𝜏for which the model had the lowest bias in the validation set and build the nal meta fair classier model. 3.4.3Fairness Metric Calculation: We collect the performance of each model based on 26 classication and four dataset metrics for each iteration of the cross-validation. So for each iteration, we use the test data for prediction and then use the predicted values along with the ground truth to calculate the 26 classication metrics. Similarly, we collect the four dataset metrics on the baseline and reweighing method. Meta fair classier is not applicable in the case of dataset metrics. 3.4.4 Measure for Fairness: Data Pre-processing, Model Training, and Fairness Metric Calculation steps are performed for each of seven datasets with ve fold ve repeat cross-validation. Then to measure if the model built on a dataset is fair or unfair according to a metric, we selected a threshold for each of the metrics. As mentioned in Section 2.2, that threshold is the fair range. If a metric value falls in that range, we say it “fair” otherwise “unfair”. 3.4.5Building Clusters: One of the main goals of this study is to group a set of metrics together that perform similarly and measure similar kinds of bias. We use 26 classication metrics calculated on seven datasets with three dierent methods to calculate metric to metric correlation based on Spearman rank correlation coecient. We do the same for the four dataset metrics as well. This provides us two correlation matrices: one 26x26 and one 4x4. After that, to build the clusters using the agglomerative clustering, we convert the similarity matrix into a dissimilarity matrix [49,61] using equation 1. We use this dissimilarity matrix to create the clusters. The agglomerative clustering process creates a dendrogram as shown in Figure 1. Now to select the number of clusters, we cut the dendrogram at a height, where the clusters will remain unchanged with the most increase/decrease of the cutting threshold. For classication metrics, we cut the dendrogram (Figure 1) at 0.57 as the Table 4: Cluster based results for 26 classication metrics on seven datasets. For a metric with ideal an value of zero, anything below -0.1 and above 0.1 is “unfair”. For a metric with an ideal value of one, anything <0.8 or >1.2 is “unfair”. Table 5: Cluster based results for four dataset metrics on seven datasets. For a metric with ideal value of zero, anything below -0.1 and above 0.1 is “unfair”. For a metric with ideal value of one, anything <0.8 or >1.2 is “unfair”. clusters will remain unchanged between the cuto value 0.49 and 0.64. Finally, we get the clusters containing classication metrics measuring similar kinds of bias. We perform the same process for dataset metrics and cut the dendrogram at a height of 0.4. 3.4.6Calculating Sensitivity: Research question four asks about the consistency of the metric values for three cases: (a) raw data, b) after applying Reweighing (RW), (c) after applying Meta Fair Classier (MFC). As we are using ve cross fold ve repeats for all the datasets, we get 25 results for each dataset and report for all seven datasets: • the median value: the 50th percentile (or 𝑄); • the IQR: the (75-25)th percentile (or 𝑄− 𝑄) Our results are organized based on four research questions. RQ1: Do current fairness metrics agree with each other? At rst, we need to verify our motivation. In real life, do the fairness metrics contradict? Table 4 contains results for 26 classication metrics; Table 5 contains results for four dataset metrics. The learner here is logistic regression. The last row contains the percentage of metrics marking the specic dataset as unfair in both tables. If we combine last rows of Table 4 & 5 and sort them in ascending order, we get the following list: { 23, 34, 50, 50, 50, 54, 58, 65, 75, 75, 75, 75, 77, 100 }% The median value here is 62%; i.e., nearly half the time the metrics make dierent conclusions about the same data. This means that researchers and practitioners will be spending much eort trying to understand their systems using disagreeing oracles (a result that motivates this entire paper). RQ2: Can we group (cluster) fairness metrics based on similarity? Table 4 shows that 26 classication metrics can be divided into seven clusters. Table 5 shows that four dataset metrics can be divided into three clusters. More importantly, we note that: •RQ1 reported intra-project disagreement on “fair“-vs-“unfair”; •We note that there is much intra-cluster agreement for each data set in Table 4 and Table 5. As evidence, we note that the majority fairness decision is always the same within the clusters for each dataset. In Table 4, the row Percentage of agreement comments on the uniformity of decisions within each cluster (for each dataset). Note that uniformity is very high (often 100%). That means metrics inside each cluster agree with each other for every dataset. Among the seven clusters, we see six clusters (except cluster two) show 100% agreement considering median value across seven datasets. For example, in case of cluster zero, percentage of agreement is 100% for ve datasets; 75% for one; 50% for one. Majority is 100%. That is true for clusters 1,3,4,5,6 & 7. We see similar agreement pattern inside clusters in Table 5 also. For reference purposes, the last column of Table 4 and Table 5 oers names for those clusters: • Misclassication: these metrics try to measure the dierence or ratio of misclassication errors between groups; • Dierential fairness: these metrics try to measure if probabilities of the outcomes are similar regardless of the combination of protected attributes [55]; • Individual Fairness: It measures if two similar individuals with respect to the classication task receive the same outcome or not; • Confusion matrix base d group fairness: these metrics measure dierence or ratio between groups based on confusion matrix; • Between group individual fairness: measures the dierence or ratio of individual fairness between groups; • Intermediate metrics: these are intermediate metrics. From a practitioner viewpoint, this clustering is useful because: •The clustering reduces the confusion of having too many metrics and not knowing their similarity. •As the metrics inside the same cluster measure same kind of bias and behave in the same manner; we can choose just one metric from each cluster. Thus we measure a few metrics but can cover a much more comprehensive range of fairness notions. •If we see agreement among all the metrics inside a cluster for a particular dataset, then one metric can be chosen as representative of the whole cluster. •In case of intra-cluster conicts, choosing only one metric can be risky. In these cases, practitioners need to do a proper risk assessment before selecting metrics. That said, if there is intracluster conict among metrics, we can choose one from the ‘fair’ group and one from the ‘unfair’ group to mitigate that risk. As part of this study, we further analyzed each cluster mathematically to verify if our cluster of metrics and their mathematical denitions coincide. A detailed analysis of these clusters and their mathematical analysis has been discussed in section 5.1. RQ3: Are some fairness metrics more sensitive to change than others? An ideal metric is responsive to the dataset it examines. An “insensitive” metric is one that delivers the same conclusions, no matter what data is being examined. An “insensitive” cluster is one containing mostly insensitive metrics. Such insensitive clusters could be ignored since they are not informative. We measure sensitivity by looking at the variability of our metrics scores using the intra-quartile range (IQR=𝑄− 𝑄1). For each data set, we found the IQR across all clusters. Next, wehighlight the sensitive results; i.e. those with an IQR greater that𝑑*standard deviation. The remaining, unhighlighted results are the insensitive metrics. As to what value of𝑑to use in this analysis, we take the advice of a widely cited paper by Sawilowsky [79] (this 2009 paper has 1100 citations). That paper asserts that “small” and “medium” eects can be measured using𝑑 =0.2 and𝑑 =0.5 (respectively). We will analyze this data by splitting the dierence looking for dierences larger than 𝑑 = (0.5 + 0.2)/2 = 0.35. Turning now to Table 6 and Table 7 we see that most clusters havehighlightIQR results. However, in Table 6, we see the clusters formed by metrics C16, C18, C20 (individual fairness) and C17, C18, C21, C22, C23, C24 (between group individual fairness) are insensitive. This, in turn, means that we shouldnotcriticize a fairness analysis that ignores these metrics. RQ4: Can we achieve fairness based on all the metrics at the same time? Dierent fairness metrics measure dierent kinds of bias. If any of the metrics complain about the fairness of the test results, then we can not trust the model blindly, and it should go through further scrutiny and improvement. Bias mitigation algorithms try to make unfair models fairer. Here we are verifying even after applying bias mitigation algorithms; can we achieve fairness based on all the metrics or not? We have chosen two highly cited algorithms from IBM AIF360: Reweighing (RW) by Kamiran et al. [65] and Meta Fair Classier (MFR) by Celis et al [37]. Table 8 shows those results collected for seven datasets after using RW and MFC algorithms. For every dataset (row-wise), we Table 6: This table shows sensitivity of the classication metrics on the three dierent models used in this study (a) Baseline; (b) Reweighing(RW); and (c) Meta Fair Classier(MFC). The table shows the median and IQR values of three datasets. Here the cells in IQR columns are marked with “red” those that change by more than a small amount (35th percentile of the standard deviation of the IQR values). The insensitive metrics are those that usually have white IQR values. Table 7: This table is similar to Table 6, showing the sensitivity of the dataset metrics on (a) Baseline; (b) Reweighing (RW). Table 8: This table shows the number of classication metrics that move towards or away from the ideal value when either Reweighing or Meta Fair Classier is used to remove bias in the models. Here “UF” shows the number of metrics that moved towards the ideal metric value, while “FU” shows the opposite. Finally “NC” shows the number of metrics that did not change at all. show the number of metrics changed towards or away from its ideal value. In that table: • FU denotes the metrics that changed towards ideal value; •UF denotes the metrics that moved away from the ideal value, • NC means the metrics which did not change. Note that majority of the metrics move towards “fair”, but there are some metrics that move towards “unfair”. For Reweighing, some metrics show “no change”, but we have veried they always remain in the fair range. The main takeaway here is no longer necessary (or even possible) to satisfy all these fairness metrics. While our analysis can reduce dozens of metrics down to ten, there will still be issues of how to trade-o within this reduced set. Even after applying bias mitigation approaches, some metrics still conict with others. This nding is similar to the claim made by others: •Berk et al. [28] oer an “Impossibility Theorem” that says there is no way to satisfy all kinds of fairness together. •As Yuriy Brun said at his keynote atICSSP’2020“we ne ed to work the system in a biased way sometimes” [33]. We have described all of our results. Here we are summarizing the results in a comprehensible way to reach a stable conclusion. The main idea of this work is to reduce the complexity of measuring fairness. That said, it is imperative we narrate our conclusions in a very easy way. We discuss here three major concerns that arise from §4 and try to simplify fairness measurement to our best. This paper has oered an empirical analysis that many of the metrics in Table 4 are synonymous since, when clustered, they fell together into just a few similar groups. In this section, we check if the same conclusions can be achieved from a more analytical analysis that looked at the structure of the equations for the fairness metrics. Sometimes, a group generated by formula’s analytical structure is similar to the clusters we generated above. For example: •In cluster three (from Table 4), all metrics are based on FDR, which suggests that both from an empirical and analytical point of view, they should be similar. •Also, In cluster zero, we see that all those metrics are based on FOR and error rate. Intuitively, this seems sensible since here metrics try to measure amount of misclassication. That said, as shown by the following three examples, there are many examples where an equation’s analytical structure does not predict for its empirical cluster. EXAMPLE #1: If we look at cluster ve, all six metrics inside this cluster are related to “between group individual fairness”. This metric is based on the same benet function: (For more details on that. see Table 1 metric id C16). We note that cluster two is also based on Equation 2, but the metrics inside this cluster represent individual fairness for each group separately. That means Although all metrics inside cluster two and cluster ve are based on the same benet function, they measure dierent denitions of fairness. That is, a formal analysis of the analysis might combine these clusters, whereas a data-oriented empirical analysis would argue for their separation. EXAMPLE #2: In cluster four from Table 1, the metrics C0, C1, C2, C5, C6 and C9 dependent on TPR, FPR and FNR. Recall that FPR and FNR report type one and type two errors ( misclassication on fairness); Now TPR can be expressed as 1 - FPR, which means the change in TPR will mirror changes in FPR. In contrast, in this cluster, the other two metrics C14 and C15 are based on selection rate (ratio of number of predicted positive and number of instances). Although there is not much similarity in the formula between these two and other metrics in this cluster, we can see they perform similarly when measuring fairness. That is: An analytical analysis does not always reect the measurement of fairness in the real world scenario. Verma et al. [91] in their paper notice a similar phenomenon where they observe that: Equal Predictive parity (a measure they explore) should also have equal FDR ... but when measured from an empirical point of view, they showed they are not the same. EXAMPLE #3: In cluster one, metrics C10 and C25 have very dierent mathematical formulas. C10 is based on FPR while C25 is based on smoothed EDF– the Empirical dierential fairness. EDF is calculated based on Dirichlet smoothed base rates for each intersecting group in the dataset, which is based on count of predicted positive. Here as well, we see that Two formulas with a dierent analytical structure can have a similar performance w.r.t. fairness. To summarize the above, we quote Alfred Korzybski, who warned: While the analytical structure of the formula oers intuitions about the nature of fairness, those intuitions had better be checked via empirical analysis. We have established the requirement of empirical analysis and we have also done that analysis. We need to nd out whether this analysis would be helpful in real-life applications or not. Here we describe various scenarios of fairness contradiction and how our study helps to remove that. Imagine a college admission decision scenario, where the system might be seen as biased against group B if applicants from group A are accepted more than group B. Here group A and group B are divided based on dierent values of a protected attribute. The college applies a bias mitigation approach to solve this problem using a group fairness metric by changing group A’s or B’s scoring threshold. Now, if a member of group A is rejected, while a member of group B has been accepted with an equal or lower score, then the system might be seen as biased against that individual. The main takeaway from this story is that there is a conict between “individual fairness” and “group fairness” [30]. The concept of fairness is very much application-specic and choosing the appropriate metric is the sole responsibility of the policymaker. An ideal scenario will be building a machine learning model which does not show any kind of bias. However, that is too good to be true. Brun et al. found out that if a model is adjusted to be fair based on one protected attribute (e.g., sex), in some cases model becomes more biased based on another protected attribute (e.g., race) [14]. Kleinberg and other researchers argue that dierent notions of fairness are incompatible with each other and hence it is impossible to satisfy all kinds of fairness simultaneously [70]. Here one thing to remember while doing prediction is that fairness is not the only concern. Prediction performance is the most important goal. Berk et al. found out that accuracy and fairness are competing goals [29]. This trade-o makes the job even more complicated since damaging model performance while making it fair may be unacceptable. As researchers, we know that satisfying all kinds of fairness together is not possible. A policymaker has to choose which fairness denitions are most important for the particular domain and ignore the rest. Our work of dividing fairness tries to make the choice easier, as choosing metrics from a group of 10 options is much simpler than choosing from 30 choices. Using our results of Table 4 and Table 5, in a specic domain, if group fairness is more important than individual fairness, then cluster four will be given more priority than clusters two and ve (Table 4). Once a cluster is given priority, one or two metrics can be chosen to represent the whole cluster. That means our whole work boils down to minimizing the number of metrics to look at and covering a wide range of fairness. We believe future researchers and industry practitioners will use our work as a guide and that will be the fulllment of this study. We have seen that there are scenarios where fairness metrics contradict each other. According to some metrics, the prediction is fair, where some other metrics disagree. Fairness metrics nd out how critical the errors of a prediction model are. It is the decision of the policymaker or the domain expert to choose appropriate fairness metrics based on what kind of bias is more important for the specic domain. For example, consider the following two scenarios: •Suppose we are predicting if a patient has cancer or not, depending on the symptoms. Here predicting a benign case as malignant is not very dangerous but predicting a malignant case as benign is extremely dangerous. A wrong diagnosis for an actual cancer patient will delay the treatment, and the patient may die. That means false negative is more important here. •Suppose we are predicting if future performance of a student based on previous records. Here if we predict a good student as bad, that is not that fatal. However, if a student who really needs special attention and help from teachers, is given a good rating then it will be misery for that student. That means false p ositive is more important here. If we know which metrics look at what kind of error, it will be easier for the decision-maker to choose. That said, based on the guidance we have provided, in case of contradiction among metrics, one metric over another will be given priority. This paper explores machine learning methods for software engineering. One issue with any paper like this is a few selection and evaluation biases along with construct and external validity based on the choice of models, datasets, and methods. In the future, we plan to address the apparent threats to validity that this paper has not fully addressed. Construct Validity:Here, we have used popular hierarchical clustering called agglomerative approach, as the number of clusters were not known beforehand. In future, we need to experiment with other clustering techniques to check for conclusion stability. This analysis used logistic regression (LR), as much prior work on fairness has also used LR [24,42]. Nevertheless, in future work, we need to explore some other classication models including DL models. Also, the metric clusters found in Table 4 and Table 5 are created using the results of our choice ML models, dissimilarity measures, and cutting point in the dendrogram. Thus, choosing one metric from each cluster may contain some risk, and researchers need to be careful while making informed choices about metric selection. Evaluation Bias:We have used 30 metrics taken from IBM AIF360 [24]. We have also covered most of the metrics from Fairkitlearn [62] and Fairlearn [20]. There are other metrics and denitions of fairness, thus the results of this study may not generalize to all available metrics. But the 30 metrics covered in this study are widely used in the fairness domain [31, 46, 56, 69, 94]. External Validity:We have used seven datasets. In the fairness domain, one big challenge is the availability of adequate datasets. It would be insightful to re-run this study on new datasets and also on other domains. Sampling Bias:In this work we used thresholds recommended by IBM AIF360 (“fair” means -0.1,0.1 or 0.8,1.2 for dierent kinds of metric). Future work should explore the sensitivity of our conclusions to changes in those thresholds. Another issue with sampling bias is that our analysis is based on the data of Table 2. We recommend that when new data becomes available, we test the conclusions of this paper against that new data. That would not be an arduous task (and to simplify that task, we have placed all our scripts online in order). Fairness is a rapidly evolving domain and the number of fairness metrics is increasing exponentially. While performing our literature review we saw the current practice in this domain is to relying on a handful of metrics and ignoring the rest. But which metrics can be ignored? Which are essential? To answer these questions, this paper has experimented with the following metrics selection tactic: When applied, the paper reported that this tactic could reduce dozens of metrics to just a handful. We found: •RQ1 showed that all the metrics do not agree with each other when labeling a model as fair or unfair. •RQ2 showed that metrics can be clustered together based on how they measure bias. Each of the resultant clusters measures dierent types of bias and selecting one metric from each cluster should be representative enough to measure increase or decrease in bias in other metrics in the same cluster. •RQ3 showed that we could ignore at least two of those clusters, since they were not “sensitive”. Recall that by “insensitive” clusters, we mean those where changes to the data did not change the fairness scores. •RQ4 showed this reduced set actually predicts for dierent things. That said, it is no longer necessary (or even possible) to satisfy all these fairness metrics. From these results, we argue that: •There are many spurious fairness metrics; i.e. metrics that measure very similar things. •To simplify fairness testing, just (a) determine what type of fairness is desirable (for a list of types, see Table 4 and Table 5 ); then (b) look up those types in our clusters; then (c) just test for one item per cluster. •While this approach does not completely remove all issues with fairness testing, it does reduce a very complex problem of (say) 30 metrics to a much smaller and manageable set. •Also, the methods of this paper could be used as a litmus test to prune away spurious new metrics that merely report the same thing as existing metrics. The work was partially funded by LAS and NSF grant #1908762.