Federated learning enables many local devices to train a deep learning model jointly without sharing the local data. Currently, most of federated training schemes learns a global model by averaging the parameters of local models. However, most of these training schemes suffer from high communication cost resulted from transmitting full local model parameters. Moreover, directly averaging model parameters leads to a signiﬁcant performance degradation, due to the class-imbalanced non-iid data on different devices. Especially for the real life federated learning tasks involving extreme classiﬁcation, (1) communication becomes the main bottleneck since the model size increases proportionally to the number of output classes; (2) extreme classiﬁcation (such as user recommendation) normally have extremely imbalanced classes and heterogeneous data on different devices. To overcome this problem, we propose federated multiple label hashing (FedMLH), which leverages label hashing to simultaneously reduce the model size (up to3.40×decrease) with communication cost (up to better accuracy (up to35.5%relative accuracy improvement) and faster convergence rate (up to for free on the federated extreme classiﬁcation tasks compared to federated average algorithm. As a lot of modern edge devices, like smart phones and IoT devices, keep generating massive data, learning a model locally on a large number of devices has become more and more important for machine learning. With growing computation power of the edge devices and higher requirement of data privacy, federated learning (FL) has become one of the important domains in large scale machine learning. Different from the traditional centralized learning, federated learning does not store the data and train machine learning models on a central server. Instead, the data are saved on the local clients without sharing with others, and most of the computations are completed locally. In detail, FL lets the local clients learn a local model using its user generated data. To train a global model that can be generalized to different users, FL uses a central server to collect the local model parameters and aggregate them into a global model periodically. FL has achieved a great success in many different types of machine learning tasks. In this project, we focused on FL on extreme classiﬁcation tasks (federated extreme classiﬁcation). Extreme classiﬁcation task requires to predict the Equal contributionDepartment of Statistics, Rice University, Texas, USADepartment of Computer Science, Rice University, Texas, USA. Correspondence to: Anshumali Shrivastava<anshumali@rice.edu>. labels of a large number of different classes (Choromanska & Langford, 2014; Prabhu et al., 2018; Hsu et al., 2009; Medini et al., 2019). Due to the more and more strict privacy regulations, federated extreme classiﬁcation has found a wide range of application scenarios. For example, many social media companies are interested in training NLP models (most NLP models involve predicting word from extreme large vocabulary) based on user generated content for sentimental analysis, inappropriate language detection or text auto completion. However, due to the privacy regulations like GDPR, it becomes difﬁcult to share user data on social media across boarders (like the investigations on Facebook and WhatsApp user data policy (GDP, 2021)). Hence, FL is a good solution to learn a good NLP model without sharing users’ data from different countries (Lin et al., 2021). Another example is training product/advertisement recommendation systems on e-commerce platforms based on users’ data. The recommendation systems also involve hundreds thousands of different items as output labels. Due to the similar dilemma faced by the social media platforms, we may also need to train a recommendation system using FL algorithms (Yang et al., 2020). Different from the traditional FL tasks, federated extreme classiﬁcation faces some unique challenges. First, federated extreme classiﬁcation usually has highly imbalanced class distribution (ﬁgure 2a). Most of the classes only have a few positive instances despite of the large sample size. Second, due to large number of output classes, extreme Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks classiﬁcation models has memory blow up in the last fully connected layer. Hence, the communication cost of each synchronization round is huge. Moreover, FL usually requires more training epochs before converges, which adds extra burden to the model communications. Third, for the non-iid partitioned local datasets, in federated extreme classiﬁcation tasks, the class distributions are divergent between different local datasets. A local dataset may have a lot of positive instances in classjwhile another local dataset may only contain negative instances of classj. Especially when the FL task has a large number of classes (like federated extreme classiﬁcation tasks), this divergence is even more obvious compared to the FL tasks with a small number of classes (see theorem 2). Previous research suggests that the class distribution divergence signiﬁcantly hurt the generalization performance of the global model (Zhao et al., 2018; Karimireddy et al., 2020). However, these challenges have not been well addressed by the current FL algorithms. For example, as the most popular FL algorithm, federated average algorithm (FedAvg) (McMahan et al., 2017) learns a global model by periodically transmitting and averaging the parameters of local models trained on the local data. However, FedAvg does not overcome the above challenges: (1) large data heterogeneity signiﬁcantly degrades the performance of FedAvg and slows down the convergence (Sahu et al., 2018; Karimireddy et al., 2020); (2) Though models are only periodically synchronized, transmitting all local full models still results in high communication cost. Thus, naively applying FedAvg on extreme classiﬁcation will have large performance degradation, slow convergence and extremely high communication cost. Our Contributions:In this paper, we propose a novel method, Federated Multiple Label Hashing (FedMLH), for federated extreme classiﬁcation tasks. Compared to FedAvg algorithm, FedMLH signiﬁcantly reduces the communication cost, improves the model accuracy and speeds up the training. Moreover, FedMLH compresses the model size, adjusts the imbalanced class size and also reduces the class distribution divergence between different local clients. We evaluate FedMLH on four different extreme classiﬁcation datasets, and FedMLH consistently demonstrates much better performance. FedMLH reduces the communication cost by up to18.75×, improves the absolute prediction accuracy by up to9%and relative accuracy improvement by up to35.5%, speeds up the convergence rate by up to5.5×, which is a remarkable improvement. We also provide theoretical analysis to show that FedMLH relieves problem of imbalanced class size and divergent class distribution between clients. First, we deﬁne the setup of federated learning in our paper. Let(x, y)be the input features and labels pair, where feature vectorx ∈ Rand labely = {0, 1}. Assume we haveK local devices, and each devicekgenerates its own dataset Dfork = 1, 2, . . . , K. Letnbe sample size ofD, n= |D|, andNis the total number of samples on all the local devices. A local modelwon devicekis trained using datasetDand the corresponding empirical loss function isP deﬁnedL(w|D) =`(x, y|w), where `(x, y|w)is the loss function for sample(x, y)under parameterw. A global modelwis learned from the local modelsw. The performance of the federated learning model is measured using the global modelwon the testing set. Non-iid local data distributionIn many FL tasks, the local datasets,D(data onk-th client), are not following the same distribution, i.e., for(x, y) ∈ D,(x, y) ∼ FbutF varies across different local clients. Especially for extreme classiﬁcation tasks, since the number of classes is huge, the local data distributionFis even more divergent. Hence, it is more practical to assume theFare non-iid. In the experiment section, we design a data partition mechanism to ensure Dfollowing totally different distributions. We start by introducing the FedAvg algorithm and how to use Count Sketch to compress the data. FedAvg algorithm learns a global model by directly averaging the local parameters at synchronization. Assume each global iteration involvesMepochs. At the beginning of a global iterationt, the central server randomly picks a subsetSofKlocal devices and broadcast the global weightwto the selected local devices. Then, on the selected local devicek, it updates the model parameters from wtowusing the local loss function,f. At synchronization, the global model is updated by averagingw,P w=w. The FedAvg algorithm runs for T global iterations to learn a global model. Count sketch is a probabilistic data structure widely used to recover the heavy hitters. A count sketch consists ofK hash tables and each hash table hasRbuckets. A vector x = (x, x, . . . , x) ∈ Ris mapped into the hash tables usingKindependent hash functions. LetM ∈ R be the matrix storing the values in the count sketches, Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks and leth, h, . . . , hbe independent uniform hash functionsh: {1, 2, . . . , p} → {1, 2, . . . , R}. In addition, count sketch uses sign hash functionss: {1, 2, . . . , p} → {+1, −1}to map the components of the vectors randomly to {+1, −1}(Algorithm 1, line 3 to 5). To retrieve the estimate ofX, again, count sketch computes the hashing locations, (h(i), h(i), . . . , h(i))and retrieves the values stored in the corresponding buckets. Then, take the median of the retrieved values as the estimate,ˆµ= medianM·s(i). We may also take the mean ofM· s(i)instead of taking median since by the law of large numbers, mean also gives a good central estimate. Algorithm 1 Count Sketch Algorithm 1: Input:x ∈ R,Kindependent uniform hash functions h, h, . . . , h, and sign functions s, s, . . . , s 2:Initialize entries of hash table arrayM ∈ Rto zero 3: for i = 1, 2, . . . , p do 4:Insertion: updateM+ = x· s(i)forj = 1, 2, . . . , K 5: end for 6: Retrieval:estimate ofx,ˆµ= medianM· In the introduction section, we have discussed the challenges of federated extreme classiﬁcation. To address these challenges, we propose FedMLH, which reduces the communication cost and adjust the non-iid class distributions. Compress the last layerDue to the large number of output classes, the last fully connected layer of extreme classiﬁcation models have a huge amount of parameters, which becomes the main communication bottleneck of the federated extreme classiﬁcation. FedMLH leverages the idea of count sketch and hashes every class intoRindependent hash tables (ﬁgure 1a). For samplenand hash tablej, its corresponding label ofi-th bucket,z, is equal to the union of the class labels that are hashed into the same bucket (line 4-7, algorithm 2). Thenz∈ Rcontains thej-th hash table’s bucket labels of all samples on clientk. To compress the size of the last fully connect layer, we set the number of buckets (B) in each hash table to be much smaller than the number of classes. During the training, we use the bucket labels as the training target. Similar to the idea of count sketch, during the inference, to estimate the log-probability of a classj, we just go back toRbuckets that classjis hashed into, and take the mean of the buckets’ log-likelihoods as the loglikelihood of classj(ﬁgure 1b). Since theRhash tables are independent to each other, for each hash table, we train an independent model (denoted as“sub-model”) to learn the corresponding bucket labels. Thus, FedMLH trainsR sub-models simultaneously. Training and model synchronizationFirst, the central server generates hash table sizeBandRindependent 2universal hash functions,h, h, . . . , h, and then broadcast to the local clients (line 3, algorithm 2). On the local clients, the output classes are hashed intoRhash tables usingh. We also initializeRindependent sub-models which use the bucket labels as the training target. Letwbe the parameter of the j-th sub-model on the k-th client. Algorithm 2 FedMLH 1: Input:Number of selected clientsS, hash function numberR, hash table sizeB, training data onk-th client (x, y). 2: On central server:GenerateRuniform hash functions, h, h, . . . , h, on the server, whereh: {0, 1, . . . , p− 1} → {0, 1, . . . , B − 1} 3:Broadcast theRhash functions and hash table sizeB to each local client 4: Label hashing on client k, k = 1, 2, . . . , K 5: for i = 1, 2, . . . , B,j = 1, 2, . . . , R,n = 1, 2, . . . , n 8: Training (on server): 9: for Synchronization round t = 1, 2, . . . , T do 10:Randomly select a setKthat includesSout ofK clients to collect 21: DeviceTrain(j, k; w): 22: Receive wfrom server 23: for each local epoch i = 1, 2, , . . . , E do 24:Update the parameters (in parallel):w= Train(x, z; w) for j = 1, 2, . . . , R 25: end for 26: Pass wto the server Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks During thet-th synchronization round, FedMLH randomly picks a set ofSclients (line 10, algorithm 2). On each selected client, all the sub-models are trained in parallel for Eepochs (line 11-15, algorithm 2). Then, send the updated local parameters to the central server. To update the global parameters, for each sub-model, the central server average the corresponding model parameters collect (line 16-18, algorithm 2). Finally, the global parameters are shared to all the local clients for the training of next synchronization round. Parallelizable between sub-modelsSince different submodels are fully independent, during the training, we do not need to communicate any parameters between different sub-models on the same clients. Actually, FedMLH learns a federated model for each sub-model in parallel. For the federated extreme classiﬁcation tasks, there exists two signiﬁcant problems: 1) The class distribution is highly imbalanced (due to the large number of classes); 2) The class distribution diverges between different clients (In FedAvg, class distribution means the number positive instances of different classes and in FedMLH, it refers to the number of positive instances in different buckets). We found FedMLH is helpful to relieve both problems. When number of classes is huge, the number of positive instances are not evenly distributed among classes. In the real datasets, ﬁgure 2a suggests that only a proportion of classes have a lot of positive samples (called “frequent classes”), while the majority of the classes just have a few samples (called “infrequent classes”). On the other hand, these infrequent classes cannot be ignored. For example, ﬁgure 2b shows that for the “LFAmazonTitle” dataset, the classes with normalized label frequency (normalized positive instance frequency = # of positive instances/sample size) less than 10(less than130positive instances) contributes about 70%of positive instances. Therefore, if the classiﬁcation model cannot predict the infrequent classes, it may miss more than70%of the positive instances, which is a huge loss. However, it is difﬁcult to classify the infrequent classes in general due to the lack of positive instances. Theorem 1 suggests that if the class lacks enough positive instances, it is difﬁcult to infer the distribution of positive samples on the embedding space, and both the centroid and radius of the positive sample cluster cannot well estimated. Theorem 1Assume we observeni.i.d. samples of(x, y), (x, y), wherex∈ Ris the feature vector and y∈ {0, 1}is the label ofx. Letn, nbe the number of positive/negative samples ofy(n+ n= n). Assume the model learns a embedding vectorm(x)which follows a mixed distribution ofm(x) ∼ πf(m(x)|µ, Σ) + (1 − π)f(m(x)|µ, Σ), whereµ, µare the mean offand f, andΣ, Σare the variance offandf.πis the prior probability ofP [y = 1]. Thus,(m(x)|y = 0) ∼ f(m(x)|µ, Σ)and(m(x)|y = 1) ∼ f(m(x)|µ, Σ). Assume the Fisher information ofµ,µ,ΣandΣ are bounded, then for all of their unbiased estimators, we haveMSE (ˆµ(i)) , MSEˆΣ(j, k)≥ O()and MSE (ˆµ(i)) , MSEˆΣ(j, k)≥ O(), where MSE is the mean square error.ˆµ(i)is thei-th element ofˆµ, and ˆΣ(j, k) is the (j, k)-th element ofˆΣ. While for FedMLH, since the number of bucketsBis much smaller than the number of classes, multiple classes are merged into the same bucket. Thus, a bucket has much more positive instances on average compared to the positive instances in a class. Therefore, by theorem 1, it is much easier for FedMLH to learn the bucket labels in each submodel. In lemma 1, we further quantify the change of positive instances. Since when the bucket sizeBis not very small,is almost negligible compared to the size of (N−n). Hence, for any classj, the number of positive instances in its corresponding bucket increases by around (N− n), which is a signiﬁcant change especially for infrequent classes. For a example, if a class haspositive instances (equals to the average number of positive instances owned by a class), using the setup in our “AMZtitle” experiment, the corresponding bucket has32times more positive instances in expectation, which will signiﬁcantly improves the estimation accuracy of positive sample cluster according to theorem 1. Lemma 1Assume classjis hashed into bucketiin a hash table. Letnbe the number of positive instances in classj. DenoteNas the total number of positiveP instancesN=n. If the labels of different classes are independent to each other, then the expected number of positive instances in bucketiis lower bounded by E (B| h(j) = i) ≥ n+(N− n) −. However, the reduction of hash table size also faces some constrains. A typical constrain is the distinguishability of different classes. To ensure the classes are distinguishable, FedMLH uses multiple hash tables, and the size of each hash table cannot be too small. Lemma 2 gives requirement to ensure FedMLH is able to distinguish between different classes with high probability. If the size of the hash table is too small, there may exist some classes that are hashed into Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks Figure 1.(a) An example of FedMLH with two clients and each client has two different hash tables and predictor networks. In each hash table,pclasses are hashed intoBmetaclass buckets. (b) FedMLH will merge the output probability for each hash table with h, j = 1, 2, 3, 4. (a) positive instance distribution(b) positive instance proportion distribution Figure 2.Distribution of (a) normalized positive instance frequency and (b) positive instance proportion. For each point line, (a)yis the empirical proportion of(normalized positive instance frequency ≤ x) by the classes with normalized positive instance frequency less thanx law in all the datasets. But infrequent classes also contribute a lot of positive instances. (c) non-iid data partition for extreme classiﬁcation datasets in our setting. Each color represents the training samples associated with one frequent class. This bar plot shows the distribution of frequent class samples on local clients. Y axis is the client id. the same bucket in all the hash tables. Lemma 2Assume theRhash functions used by FedMLH are independent to each. Given aR ≥ 1, whenB ≥ exist any two classes collide with each other in all the hash tables. Under the federated learning setup, the class distributions usually diverge a lot between different clients. In the federated extreme classiﬁcation tasks, this problem becomes even more severe since divergence of class distributions increases with number of classes in general. However, FedMLH . The distribution of positive instance frequency follows a power can relieve this problem in every sub-model. Theorem 2 suggests that the divergence of class distribution strictly decreases if we hashpclasses into less number of buckets. Moreover, as we hash into less buckets, the divergence monotone decreases in expectation. Therefore, FedMLH is helpful to adjust the non-iid class distributions and make the distributions more similar between different local clients. Theorem 2Assume for each sample, only one class’s label is positive. On clientk, letn=be the number of positive instances of classj. Then,π= (π, π, . . . , π) is the proportion of positive instances of all the classes, whereπ=is the proportion of positive instances of classj (π> 0). FedMLH hashes thep Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks classes intoBbuckets, and on thek-th client, the proportions of positive instances of different buckets areω=P (ω, ω, . . . , ω), whereω= 1andω> 0. Then, for any two clientsaandb, the Kullback–Leibler (KL) divergence betweenωandωis always smaller than that between πand π. We perform experiments to evaluate the performance of FedMLH on four different large scale extreme classiﬁcation datasets, including EURLex-4K (Mencia & F¨urnkranz, 2008) (Eurlex), Wiki10-31K (Zubiaga, 2012) (Wiki31), LFAmazonTitle-131K (McAuley & Leskovec, 2013) (AMZtitle) and LF-WikiSeeAlsoTitles-320K (Bhatia et al., 2016) (Wikititle). These datasets focus on potentially important application areas of federated learning with user generated data (NLP and recommendation system). The details of the four datasets are listed in Table 1. Since the input features are sparse for most the extreme classiﬁcation datasets, feature hashing is widely used to reduce the memory cost. Here, we also use feature hashing to reduce the feature dimension (Table 1 shows the hashed feature dimension). For training both baseline and FedMLH we use the same cluster of NVIDIA P100 gpus. Baselines:To evaluate our method, we compare FedMLH to the FedAvg algorithm. Both algorithm use the same MLP network (with two hidden layers) for each dataset, besides the last fully connect layer (FedMLH has less output). For different datasets, we vary the number of hash tables/submodels (R) and number of buckets (B) used in each hash table (see Table 2). Non-iid data partitionWe manually partition the training samples to ensure the data on different local clients are non-iid distributed. Since the class distribution is highly imbalanced and most of the samples have at least one positive instances among the frequent classes, we try to partition the samples with frequent classes unevenly and make sure that the frequent classes on different local workers are distinct. In detail, for a frequent classj, we collect the training samples whose label of classjis positive, denoted byD(D= {(x, y) : y= 1}wherey is classj’s label of samplei). Then, we randomly pick a local clientk, and assignDto clientk(Figure 2c). By this approach, different local clients have totally different frequent classes samples, thus have non-iid distributed data(1b). Since most of the samples have multiple labels, it is possible thatDandDhave non-empty intersections (D∩ D= {(x, y) : y= 1 and y= 1}). Therefore, samples with more than one positive instances among frequent class are assigned to multiple clients. Table 1.The statistics of feature dimensiond, feature hashing dimension˜d, number of classespand number of training samples Table 2.Number of hash tables/sub-models (R) and buckets (B) used by FedMLH. FL setups & training detailsOur experiment includes 10 local clients, and during synchronization, we randomly pick 4 local clients to share the model parameters with central server. The models are trained for70synchronization rounds, and each synchronization rounds contains5epochs. We also apply early stopping on both baselines to achieve better accuracy and prevent overﬁtting. Due to the large input dimensions, we also perform feature hashing to all the datasets, and both baselines are run on the feature hashed data. Performance metricsSince both baselines multi-label classiﬁcation models, traditional accuracy does not apply here. Instead, we evaluate the prediction accuracy using the top1,3and5accuracy. The top-kaccuracy is measured by the precision of the topkclasses with largest predicted log-probability, which deﬁned as follows: top k accuracy =|P(x) ∩ S(x)|Nk, where theP(x)is the set of topkclasses with largest predicted probability of samplei, andS(x)is the set of classes whose labels of sample i are positive (y= 1). Communication costCommunication cost is another important concern of FL algorithms. We compare the communication volume of both baselines. The communication volume is deﬁned as the size of the model parameters (in bytes) communicated between local clients and central Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks Figure 3.FedMLH vs FedAvg test accuracy of total classes, frequent classes and infrequent classes of Eurlex dataset in synchronization rounds. @1, @3, @5 means the precision at top 1, 3 and 5 selected classes server during the training. Here we measure the communication volume until the model achieves the best accuracy (the average of top 1, 3 and 5 accuracy). We evaluate the performance of FedMLH in terms of the predicton accuracy, communication cost, model size, convergence rate and training time. Prediction AccuracyTable 3 suggests that compared to FedAvg, FedMLH signiﬁcantly improves prediction accuracy in all the experiments. Especially for the EURLex-4K experiment, the top1,3and5accuracy are improved by 9%,7.3%and5.1%respectively, which is a signiﬁcant boost. Moreover, in the LF-AmazonTitle-131K experiment, although the absolute accuracy improvement is not as high as that in the EURLex-4K experiment, considering the low baseline accuracy of FedAvg algorithm, the relatively accuracy improvements are even more remarkable (relative accuracy improvement is deﬁned as: absolute accuracy improvement/baseline accuracy), which reach31.8%,33.6% and 35.5% for the top 1, 3 and 5 accuracy respectively. We further evaluate the prediction accuracy of the frequent and infrequent classes. The top-kfrequent/infrequent class accuracy is deﬁned as, # of correctly predicted frequent/infrequent class labels/k(sum of top-kfrequent class accuracy and infrequent class accuracy is the overall top-k accuracy deﬁned in “Performance metrics”). We ﬁnd most of the accuracy improvement comes from the infrequent class accuracy. In the EURLex-4K experiment, the top-kfrequent class accuracy of both baselines are almost the same (ﬁgure 3). But FedMLH signiﬁcantly outperforms FedAvg in terms of the infrequent class accuracy. This difference may be contributed to the adjustment of imbalanced class distribution accomplished by FedMLH (see section 5.1). Communication costDuring every synchronization round, FedMLH and FedAvg need to synchronize the model parameters between the local clients and central server. And the communication cost is a big bottleneck of FL algorithm. We compute the communication volume of both baselines, and table 4 suggests that FedMLH signiﬁcantly reduces the communication volume in all the experiments. Especially for the AMZtitle experiment, to reach the best accuracy, FedMLH achieves18.75×reduction of the communication volume, which will signiﬁcantly reduce the communication time. Model sizeFedMLH leverages label hashing to reduce the size of each sub-model by reducing the size of output layer. Although FedMLH requires multiple sub-models, the total model size is still reduced compared to that used by FedAvg. For example, in AMZtitle experiment, FedMLH reduces the model size from0.51GB to0.15GB, which also lowers the memory requirement of local computing devices. Convergence rateFedMLH not just decreases the model size, but also signiﬁcantly speeds up the convergence rate in terms of the synchronization rounds (or training epochs). Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks Figure 4.Test accuracy vs total communication volume transmitted by all workers. @1, @3, @5 means the precision at top 1, 3 and 5 selected classes. The vertical gray dash line indicates the place where we early stop the training of FedMLH. Table 4.Communication Volume of FedMLH and FedAvg to reach the best prediction accuracy. CC Ratio: Communication cost ratio of FedAvg over FedMLH. For example, in the AMZtitle experiment, FedMLH reduces the number of synchronization rounds from 66 rounds to only 12 rounds compared to FedAvg algorithm, which will signiﬁcantly reduce the training time. Local training timeSince FedMLH reduces the model size, it is also beneﬁcial to reduce the local training time. Table 7 measures the time to train a local synchronization round (5 epochs on a local client). Compared to FedAvg, FedMLH also has shorter local training time in all the experiments. Table 5.Model memory usage of FedMLH and FedAvg in each client. Memory Ratio is the memory cost ratio of FedAvg over FedMLH. FedMLH includes two hyper-parameters, hash table size BandRbefore running the experiments. A largerBorR leads to higher prediction accuracy. However, due to the memory constrain, we have to restrict the size ofBand R. In this section, we test the performance of FedMLH under differentBandR. First, we compare the sensitivity of FedMLH to the size of hash table size. Figure 5a and 5c suggest that the accuracy of FedMLH almost keeps the same when the number of hash tables doubles from4to8. Hence, keep increasing the number of hash tables may not be helpful. From the memory perspective, a smallerRis preferred. We also evaluated the effect of hash table size. Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks Figure 5.Test accuracy of FedMLH under different setups of hyper-parameters. @1, @3, @5 means the precision at top 1, 3 and 5 selected classes. Table 6.Number of synchronization rounds to reach optimal accuracy of FedMLH and FedAvg. Rounds ratio is the number of synchronization rounds of FedAvg over FedMLH. Again, FedMLH is robust to the changes of hash table size. For the Wiki10-31K dataset, a larger hash table size could boost the top 3 and 5 accuracy by5%. However, compared to FedAvg, FedMLH still signiﬁcantly outperforms FedAvg even when we reduces the hash table size from1, 000(in the previous experiment) to 500. Federated Learning in non-iid data Federated learning has signiﬁcant degraded performance in non iid datasets, ﬁrst empirically observed by (Zhao et al., 2018). In (Karimireddy et al., 2020), FedAvg is shown to suffer from so called client-drift. Several analysis of FedAvg bound this Table 7.Wall clock time of FedML and FedAvg of each synchronization round. Time ratio: computation time of each synchronization round of FedAvg over FedMLH. drift by assuming bounded gradients (Wang et al., 2019; Yu et al., 2019) while some view it as additional noise (Khaled et al., 2020). Some work proposes solutions to such problem such as using variance reduction (Liang et al., 2019), or adding regularization to the local worker training (Li et al., 2018). But none of the above considered federated learning on extreme classiﬁcation with imbalanced and they do not provide convergence speedup at the same time of reaching higher accuracy. Extreme classiﬁcationAs many important real life tasks can be modeled as the extreme classiﬁcation, it becomes one of the most important area of research. Several papers explore the extreme classiﬁcation in centralized training Federated Multiple Label Hashing (FedMLH): Communication Efﬁcient Federated Learning on Extreme Classiﬁcation Tasks scenario (Choromanska & Langford, 2014; Prabhu et al., 2018; Hsu et al., 2009; Medini et al., 2019). But FedMLH is the ﬁrst to explore and analyze extreme classiﬁcation in highly imbalanced and non iid data distribution in federated learning. In this project, we propose FedMLH for efﬁcient federated extreme classiﬁcation tasks. We demonstrate that our algorithm signiﬁcantly reduces the communication cost, improves the classiﬁcation accuracy and speeds up the training time, especially on the large scale extreme classiﬁcation datasets. We envision that FedMLH will be widely implemented in different ﬁelds like the recommendation system.