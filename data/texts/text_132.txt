Mahed Abroshan, Kai Hou Yip, Cem Tekin, Senior Member, IEEE, and Mihaela van der Schaar, Fellow, IEEE Abstract—In high-stakes applications of data-driven decision making like healthcare, it is of paramount importance to learn a policy that maximizes the reward while avoiding potentially dangerous actions when there is uncertainty. There are two main challenges usually associated with this problem. Firstly, learning through online exploration is not possible due to the critical nature of such applications. Therefore, we need to resort to observational datasets with no counterfactuals. Secondly, such datasets are usually imperfect, additionally cursed with missing values in the attributes of features. In this paper, we consider the problem of constructing personalized policies using logged data when there are missing values in the attributes of features in both training and test data. The goal is to recommend an action (treatment) when˜X, a degraded version of X with missing values, is observed. We consider three strategies for dealing with missingness. In particular, we introduce the conservative strategy where the policy is designed to safely handle the uncertainty due to missingness. In order to implement this strategy we need to estimate posterior distribution p(X |˜X), we use variational autoencoder to achieve this. In particular, our method is based on partial variational autoencoders (PVAE) which are designed to capture the underlying structure of features with missing values. Index Terms—Missing values, observational data, policy construction, variational autoencoder. In many real-life applications, the datasets suffer from various forms of imperfection. Missingness in the attributes of the features is one of the most common types of imperfection [1]. In the problem of constructing policies when there are missing values, one can simply use an imputation method to ﬁll out missing attributes and then use one of the many existing approaches for policy recommendation for the complete dataset. However, this does not reﬂect the uncertainty in the features. Multiple imputations [2] can be used instead of single imputation. In order to combine the recommended actions of different imputed instances, one simple idea is to use the mode of actions, another possibility is to use a stochastic policy where the probability of choosing an action is proportionate to its frequency. In this work, we address this problem in a systematic way. We suggest using a generative model, partial VAEs (PVAE) [3], to estimate the probability of different imputed features and use these probabilities as the scores of recommended actions for each particular complete feature. An advantage of using VAEs is that they make weak assumptions about the way the data is generated [4], [5]. Also, it has been shown that they are very effective in capturing the latent structure and the correlations among variables in several tasks [6], [7], [3], [8]. Using posterior probabilities produced by PVAE, we can estimate the action that maximizes the expected reward. However, simply maximizing expected reward, given that we have uncertainty about the true feature, might be problematic in sensitive applications like healthcare, since the chosen action that maximizes expected reward may impose poor reward in some of the less likely scenarios which is not safe. To address this, we suggest using conservative strategy for policy recommendations. With this strategy, we consider all likely scenarios (we can choose how prudent we need to be via a tuning parameter) and recommend an action that maximizes the reward in the worst-case scenario (a max-min criterion). The main factor which differentiates the problem of learning from observational data from supervised learning is that for each feature the reward is only known for the prescribed action, i.e. we do not know the counterfactuals. Another complicating factor is that the logging policy (aka propensity score) is usually not random, hence we need to deal with the selection bias. In addition to these two issues, in this work, we are considering that features have missing attributes. Note that, as a consequence of this, we not only do not know counterfactuals but also no longer have access to the actual reward for a given action and complete feature. The goal of this work is to address how one can deal with the uncertainty imposed from the missing attributes. Note that there are other sources of uncertainty in the problem that we are leaving for future work. In particular, here we are using inverse propensity score (IPS) for dealing with selection bias—a method that is known to have high variance (especially when there are not enough samples for actions with low propensity scores) [9], [10]. Here, we are not considering this uncertainty and implicitly assume that there are enough samples to have a low variance estimate of the propensity score. In summary, our contribution is as follows. We propose using a max-min criterion (conservative strategy) when there are missing values in the attributes for sensitive applications. We are proposing a new method based on VAEs for handling missing attributes in the counterfactual estimation problem. In one of our methods we use the VAE to produce a similarity score to determine how much each of the samples should contribute to the estimation of the outcome for the sample in hand. In the other method, we use a conditional VAE setup to directly estimate the reward via the network. We are using the inverse propensity score for dealing with the selection bias. Notation:We use capital letters for random variables, lowercase for realization, boldface letters for vectors, and calligraphic letters for denoting sets. The featurexis ad-dimensional vector belonging to the set X = X× X× · · · × X. HereXcan be a set of continuous, integer, or categorical variables. Deﬁne˜X= X∪{∗}, now the observed vector with missing attributes˜xbelongs to˜X =˜X× ˜X× · · · ×˜X. Deﬁne binary vectorMwhich determines the missingness pattern.M= 0means thatith element is observed andM= 1otherwise. We assume missing at random (MAR) mechanism for missingness. This means that the probability of a value to be missing may only depend on the observed data (see [11] for exact deﬁnition). For each observed covariate ˜x, we can recommend an actiona ∈ AwhereAis a ﬁnite set (note that we are not restricting actions to be binary). The rewardRgiven actionaand true featurexis drawn from an unknown distributionR ∼ Φ(R|x, a). We denoteE[R|x, a]by θ(x, a). The available dataset are triplets of (˜X, A, R): where actionsAare produced from an unknown logging policy π(A|˜X)(also called generalized propensity score). Note that we assume that the treatments in the dataset are administered by only observing˜X, hence Fig. 1 represents the causal graph that describes the problem. The conditional distribution of these variables are given in(2). With some abuse of notation the joint distribution is written as (X,˜X, A, R) ∼ p(X,˜X, A, R). We make the two standard assumptions about the logging policy and rewards in the potential outcome framework [12], [13]: 1)Common support:π(a|˜x) > 0for alla ∈ Aand˜x ∈˜X. 2)Unconfoundedness with missing values: For each feature vectorXthe set of possible rewards{R(a)}are statistically independent of the taken action:{R(a)}⊥⊥ Note that the second assumption can be inferred from our causal graph. This is required for using generalized propensity scoreπ(A|˜X)as we do in Section IV [14]. The missing data problem frequently arises in machine learning. A motivating example for our model is the following, assume a medical setting where at the time that the treatment was administered to the patient, some of the attributes were missing. This can happen for various reasons, for example, maybe because of the different practices across different hospitals (where some of the attributes are not recorded), lack of certain tests, or maybe emergency situations to name a few. Note that since the treatment was administered only by observing˜Xthe causal model of Fig. 1 holds. We are considering the problem of off-policy evaluation (also known as ofﬂine evaluation in bandit literature). Here, we are using the IPS reweighting method [15], [16] to deal with selection bias. We are using IPS in a deep network model, from this point of view our work is mostly related to [17], [18]. Direct method is another method for counterfactual estimation where the goal is to learn a function, mapping pairs of actions and features to rewards [19]. Doubly robust method combines the former two approaches [20], [21]. Note that none of these works consider missing attributes in the feature. In [22], authors provide an off-policy evaluation method based on the regression estimator given in [23], [24], when there are missing pairs of action and feature in the dataset. However, they do not consider missing attributes in the feature. The treatment effect estimation problem is another related line of work, where the goal is to ﬁnd the causal effect of a certain intervention (treatment) on the population or on individuals. The missing value problem is discussed in this framework from early on [14], [25]. The use of generalized propensity scores, computed via multiple imputations is suggested in [26]. A summary of several early works can be found in [27]. More recently, in [28] matrix factorization has been proposed for estimating confounders from noisy covariates (also includes covariates with missing values). In [29] a doubly robust based method is suggested. In [30], authors consider missing values only during test time and suggest a method based on information bottleneck technique. Finally, [31] suggests a new method based on VAEs (adopted for missing values) which learns distribution of the latent confounder and hence assumes a weaker condition than unconfoundedness with missing values which is harder to justify. In the experiment section, we compare our results with several of these recent works. Some of the other notable works in the treatment effect literature are [32], [33], [34], [6]. However, they do not consider missing values. Thus, we will compare our results with [6], by imputing the missing values to get complete features and train and use the algorithm on the complete feature. The problem studied in this work can be considered as an ofﬂine version of contextual bandits problem [35], [36], [37]. There are several works in bandit literature (and more generally in reinforcement learning) that are related to conservative strategy, e.g. [38], [39], [40]. The goal in bandit literature is to minimize regret, and conservatism in this area means guaranteeing that we are not performing poorly in the process of achieving a low regret (exploration). This is fundamentally different from conservatism in our problem which is due to uncertainty in the feature. Our method is based on PVAE introduced in [3]. A few other VAE based methods are also suggested for the imputation task [4], [41]. Methods based on PVAE has been suggested for other tasks. In [42], they use PVAE for hybrid recommender system and in [43] for element-wise training data acquisition. In this section, we discuss three different strategies for action recommendation when there is uncertainty (in our problem due to missing attributes) in the features. In the next section, we address all three of these strategies using two different methodologies. Assume that for featurex, the action that gives the highest expected reward is denoted by a(x), Since we observe˜x, the degraded version ofx, there is uncertainty in the the true value ofa(x). This uncertainty can be quantiﬁed using Shannon entropyH(a(X )|˜X =˜x). The following proposition presents an expansion for this quantity. Proposition 1.The uncertainty in the best actiona(X)when we observe˜X ∼ p(˜X|X) can be expressed as follows: H(a(X )|˜X) = H(a(X )) − (I(X;˜X) − I(X ;˜X|a(X)). Proof. See the proof in the appendix. The ﬁrst term of the right hand side,H(a(X )), represents the uncertainty ina(X)itself. This can be interpreted as the complexity of the functiona(.). For example, in the extreme case when there is a single action which is always the best action, thenH(a(X )) = 0. The second termI(X;˜X)is the mutual information betweenXand˜Xwhich represents the quality of the channel between these two variables, this channel is characterized by the conditional distributionp(˜X|X), i.e., I(X;˜X)shows how much information is passed to˜Xfrom X. The last term is subtracting the amount of information passed to˜Xthat is irrelevant toa(X). The probability that the best algorithm ﬁnda(X)by observing˜Xis given by we compute these quantities, we leave further discussion about fundamental limits to future work. We reiterate that in this work we ignore the uncertainty in the true value of reward, i.e., the uncertainty in the estimation ofθ(x, a). The above analysis holds for any degradation of the input. In particular, in this work we are considering missingness. The three strategies below can be used to deal with this type of uncertainty. Imputation:The ﬁrst strategy is to use an imputation algorithm in order to ﬁnd the most likely featurexgiven the observed incomplete feature˜x, i.e. Then the recommended actiona(˜x)can be found by maximizing the reward forˆx. Maximum expected reward:The imputation strategy recommends the action only based on one possible complete feature. This does not account for the uncertainty in the true feature. A natural way is to directly maximize the expected reward instead of ﬁnding a single potential complete feature. Assuming that attributes are discrete and|X |is ﬁnite (the summation below should be replaced with an integral if this is not the case), the expected reward when˜xis observed for a given policy like π(A|˜X) can be computed: E(R(˜x)) =θ(x, a)π(a|˜x)p(x|˜x), The policyπthat maximizes this expectation is a deterministic policy that recommends a(˜x) deﬁned as follows: Multiple imputation method (MI) is an approximation for this strategy, where we consider several possible complete features and recommend an action which maximizes the average reward over the imputed samples. This method is widely used in the literature (e.g. see [27], [44], [31]). Conservative strategy:In sensitive applications, the strategies presented above may not be acceptable, because in these applications we have to avoid less likely (but still possible) scenarios for which a very low reward is expected (e.g. death in healthcare application). To achieve this, we suggest a max-min criterion that recommends the action which maximizes the reward in the worst case scenario which is likely “enough”: Here the constant0 ≤ c < 1determines how prudent we want to be (ˆxis deﬁned in(5)). If we choosec = 0, we get the most conservative policy, where we essentially ignore observed input˜x, and choose the action which has the highest minimum reward for all inputs, whilec → 1is equivalent to the imputation method.R Remark.If we deﬁneR =p(x|˜x)dxwhereS = {x | p(x|˜x) < cp(ˆx|˜x)}, thenRis representing the risk of not considering the true feature in the process of recommending the action. When we choosec = 0, we haveR = 0, and it increases withc. The parametercthen can be thought of as a tuning parameter for this risk. As a proxy, we can modelp(x|˜x) with a multivariate Gaussian distribution, and can considerˆx to be the center of the distribution. Thus, we can compute this risk for a given c. In this section, we suggest two methods for counterfactual estimation. We show how we can implement the three strategies discussed in the previous section using these two methods. In the core of both methods, we use PVAE. In the ﬁrst method, we train the network using only˜xas the input, which will produce a similarity score for two features. We will use this similarity score to estimate the reward (we call this method SPVAE). In the second method (called CPVAE), we train a conditional VAE [45] using˜xincomplete context, and the reward (conditioned on action), and we will use the network for both estimatingp(x|˜x), and also estimating the expected rewards θ(x, a). We will be using the encoder of partial VAE that was introduced in [3]. In particular, we use the Pointnet Plus (PNP) setting. The structure of the encoder is represented in Fig. 2. PVAE is designed to deal with the missingness in the input and its structure allows the input dimension to vary. Assume that x, · · · , xare the observed attributes of the feature, each observed attributexwill be multiplied by an embedding vectorethat will represent the position of the observed attribute. Denote the element-wise multiplication ofeandx bys= x∗e. Nows’s will be fed toh, a shared neural net. Then there is a permutation invariant functiong(in our setupg is a summation similar to [3]) that maps(h(s), · · · , h(s)) toR(kis a hyperparameter). Finally, thisk-dimensional latent variable will be fed to a fully connected networkf(). Therefore, we haveZ = fg(h(s), · · · , h(s)). We refer to [3] for a more detailed discussion about the encoder. For the decoder of PVAE we use a fully connected network. Inspired from the decoder in the HI-VAE model [46], we consider the following distributions for different type of variables and mapZto the parameters of an appropriate distribution. This will enable us to handle heterogeneous features of the context. For continuous variables we have p(x|Z) = N (µ(Z), σ(Z)), whereµ(Z)andσ(Z)are outputs of the neural network with inputZ. For categorical attributes, we use one-hot encoding, the posterior distribution is given by a softmaxp(x= j|Z) =, where s(Z)is the output of the decoder corresponding to thetth category. The loss function is similar to the ELBO used for training of PVAE in [3]. log p(˜X) ≥ log p(˜X) − Dq(Z|˜X)||p(Z|˜X) = Elog p(˜X|Z)− Dq(Z|˜X)||p(Z). We consider normal distribution forp(Z) = N (0, 1). Similar to [3], [46] we assume the two following equations hold. The ﬁrst one states the independence of attributes given the latent variable, i.e. The second one states that all the information about unobserved attributes in˜xis encoded intoZ, i.e. ifxrepresents the set of missing attributes, then we have We suggest using the following simple estimator for ﬁnding ˆθ(x, a): wherew=are similarity scores corresponding to each of the data samples, andˆπ(a|˜X)is the estimation of the propensity score. We explain how to computeˆπ(a|˜X) in the next subsection. Essentiallywshows how much the reward of sample˜Xis relevant for estimating the reward for x. The inverse propensity score term adjusts for the selection bias in the data. For estimatingp(x|˜X), we can feed˜Xto PVAE, the output of the network gives the required posterior distribution. Recall that we assumed Gaussian distribution for the output of the VAE. Using (10) and (11) we have A variation of SPVAE method that computesˆθ(x, a)in a slightly different way is proposed in the supplementary material. Remark.Notice that the summation in(12)may become computationally costly. If this is the case, one can randomly chooseM < Nsamples from the dataset and estimateˆθ(x, a) only using thoseMsamples. The CPVAE method that we propose next will not have this issue, since it can estimate the reward with a singe forward pass through a network. For computingˆπ(A|˜X), we ﬁrst use a multiple imputation method to produce multiple complete datasets. Any standard multiple imputation method like [47] or [48] can be used. Then we ﬁt a standard multinominal logistic regression model similar to [17] on the completed features. In the test time we average the propensity score over multiple imputations. This is a classical method that is well studied in the literature [26], [27], [44]. (It is known that averaging the propensity score before performing casual inference gives better result [49].) A more advanced method for estimating propensity scores with missing values is recently introduced in [29]. We leave exploring effect of using more advanced methods for the future work. In this section, we modify PVAE and use it as an endto-end network to produce an estimation ofθ(x, a). We use conditional VAE and call this method CPVAE. Firstly, the input of the CPVAE is different. During training, the input is a subset of rewards, actions, and observed attributes that we represent with(˜X, A,˜R)(by denoting the rewards with ˜Rwe highlight that they might be missing from the input of the network). The idea is that in the test time, the reward can be treated as a missing attribute of the input, i.e. the input at test time will be the observed attributes and action(˜X, A, ∗). The decoder network attempts to reconstruct(X, R), hence it will produce an estimation for the reward (see Fig. 3). This method has the advantage that the correlations among different attributes of feature, reward, and action are expected to be captured by the latent variableZ. Also in comparison with SPVAE for producing the estimated rewards, we do not need to compute the summation in(12)and we can get the reward with a single forward pass through the network. (Note that it is expected to have a better quality of imputation using outcome in the imputation process [50], [27].) 1) Loss Function: The standard ELBO loss function for CPVAE can be written as follows: log p(˜X,˜R|A) ≥ log p(˜X,˜R|A) In order to account for the selection bias in the loss function, we use IPS technique and write the ﬁnal loss as: L =1Nlog p(˜X|Z, A) + log p(R|Z, A) − Dq(Z|˜X, A, R)||p(Z|A)1[A= a]˜ Notice thatE[L]is equal to the lower bound in(14). The IPS term can also be interpreted as a method to force the autoencoder to learn rare action-feature pairs more carefully by penalizing the loss function. In this section, we explain how to implement three strategies using our two suggested methods. ﬁnd the output of PVAE to impute the missing attributes of˜x(we do not change the values which are not missing). Denote the complete feature vector byx. We now use (12)to estimateˆθ(x, a)for all possible actionsa, and then recommend the action which maximizesˆθ(x, a). For CPVAE we simply feed˜xalong with different actions to the network and choose the action with highest expected reward. to PVAE, then samplettimes fromq(Z|˜x)(tis a hyperparameter) to getz, · · · , z. Denote the imputed output of the decoder network of thesetlatent variables byx, · · · , x. For alla ∈ Aandx, we use equation (12)to computeˆθ(x, a). Then, we recommendawhich maximizes the average reward of thesetinputs. We do similarly for CPVAE, the estimation of theˆθ(x, a)will be done by observing output of CPVAE. sion for all a ∈ A First we pass˜xthrough PVAE to getˆxand the posterior distributionp(x|˜x). We produceusamples from the generator model through random sampling. That is, we can randomly sampleutimes fromP (Z)to getz, · · · , z (recall thatp(Z) = N (0, 1)). Then output of the decoder gives usugenerated samplesx, · · · , x. Now using posterior distributionp(x|˜x)we ﬁnd samples which satisfy the constraints. Assume thatSis the set of all indices1 ≤ i ≤ uwhichp(x|˜x)satisﬁes the inequality constraint. Then for SPVAE, we computeminˆθ(x, a) using(12)for alla ∈ Aand recommendawhich maximizes this expression. For CPVAE, for alla ∈ Awe pass|S|samples along with actionsaand compute the minimum reward for each action. Then we recommend the action with highest reward. In this section, we evaluate our suggested methods using three experiments. First, we use MNIST dataset [51], and frame the usual classiﬁcation task for identifying handwritten digits in a logged bandit setup. Note that for policy recommendation problems, since counterfactuals are not available, evaluating an algorithm is not directly possible. That is why here, similar to many other works (e.g. see [17]), we use a classiﬁcation problem to evaluate our method. We use this dataset to highlight the differences between the three strategies discussed in the paper. Secondly, we use IHDP dataset [52], which is a widely used dataset in treatment effect literature, to compare the predictive capability of our methods in the presence of missing value with other recent suggested methods. We show that our methods outperform state-of-the-art methods for estimating average treatment effect in the presence of missing values. Finally, to further evaluate our method, we use OhioT1DM dataset [53], a medical dataset that includes blood glucose measurements and insulin doses for numerous type 1 diabetes mellitus patients using insulin pump therapy. In the ﬁrst experiment, we use the MNIST dataset. The goal of this experiment is to compare different strategies introduced in Section III. Thus, here we only implement CPVAE using the three strategies. The complete feature has784attributes, each one is a number between 0 to 255. We will erase a ﬁxed percentage of pixels (50 percent in this experiment) from each image uniformly at random. The goal is to predict the correct label associated with the image, hence the set of actions isA = {0, 1, · · · , 9}. The reward is deﬁned as a Gaussian, centered around the difference of the true label (y) and the predicted one (i.e. action A) Note that this is different from the standard binary reward deﬁned for classiﬁcation task (i.e.R = 1if the predicted label is correct, and zero otherwise). The reason we choose this reward is to highlight the differences between the three strategies and the necessary compromises that need to be made in the face of uncertainty. For example, assume that we are considering an image which is0with probability0.7and8 with probability0.3. In this scenario, using the reward that we deﬁned all three strategies are meaningful (i.e. you may choose 2 to avoid low probability) while with the binary reward, all three strategies coincide (all three recommenda = 0). The mechanism for assigning actions to images for creating dataset is as follows. For images representing even numbers, π(a|˜X) = 1/20for0 ≤ a < 5andπ(a|˜X) = 3/20for 5 ≤ a < 10. For odd images,π(a|˜X) = 3/20for0 ≤ a < 5, and π(a|˜X) = 1/20 for rest of the actions. In Table I the average reward of different strategies is reported. We are using CPVAE method in this experiment. The maximum expected strategy get the highest reward as expected, followed by the imputation strategy. It can be seen that, as we decrease parameterc, the expected reward decreases. In exchange, the number of instances for which we get a poor reward (here we considered reward less than−7) is decreasing with c. In Fig. 4, we show the distribution of recommended action for three conservative strategies where we change tuning parameter c, from top to downc = 0.001,c = 0.1, andc = 0.7. For the ﬁrst ﬁgure withc = 0.001it can be seen that the method always chooses action5, which is the safest action. This action avoids losses more than5. Sincecis too small, images of different digits can pass the condition on(8)and hence the best action would be5(or4). It can be seen that, as we increase c, fewer images with random digits pass the constraint and, as a result, the distribution of actions spread over different actions. The details of the experiment setup and some additional experiments are available in the supplementary materials. In this section we repeat the experiment in [31] on IHDP dataset. IHDP is a semi-synthetic datasets based on the Infant Health and Development Program (IHDP) compiled by Hill [52]. This experiment studies the effects of specialist home visits on future cognitive test scores. The dataset comprises 25 attributes for each instance and 747 instances in total (139 treated, i.e.,a = 1, and 698 instances witha = 0). Following [31] we report the in-sample mean absolute error in the estimation of Average Treatment Effect (ATE). ATE denoted by τ is deﬁned as follows: τ = E[R(1) − R(0)] = E[E[R(1) − R(0)|˜X]]. Since both values ofR(0)andR(1)for allX’s are known from the dataset we can calculate the mean absolute error exactly∆ =ˆτ −PR(1)− R(0). We consider scenario “B” of [52], whereR(0) ∼ N (µ, 1)andR(1) ∼ N (µ, 1). Here(µ, µ) = (exp(X + A)β, Xβ − ω),ωis chosen such that we have an average treatment effect ofτ = 4. The missing values are added with Missing Completely At Random (MCAR) mechanism. We compare three missing rates of10%,30%, and 50%. We compare our results with several recent methods in Table II. MI is the multiple imputation approach suggested in [44], [54] with 20 imputations. MF is the matrix factorization method introduced in [28], and MDC.process and MDC.mi are two methods introduced in [31]. They use a VAE to produce a latent space. Then for ﬁndingτthey ﬁt an estimator on the latent variable. For all three above methods the result of an OLS estimator and two different doubly robust estimators are reported in Table 1 of [31]. Here we only report the best of the three results for each of the methods for each setting and refer to [31] for the complete table. MIA.GRF is a doubly robust estimator suggested in [29]. Finally, CEVAE a method introduced in [6], is another baseline we compare with. This method is not designed to work with missing values, thus a mean imputation method was performed to get the complete features before applying this method for estimating ATE. For a more detailed explanation about these competitor methods we refer to [31][Section 4]. In Table II, for SPVAE and CPVAE we use the maximum expected reward strategy with 5 times imputations. We can see that both of our methods outperform other methods in50% missingness and also SPVAE has the best result for30%(and comparable result in10%). In the appendix we provide a table where we show our result is not sensitive to the choice of hyperparameters. For this experiment, we are using OhioT1DM dataset [53] which contains continuous glucose monitoring, insulin dosage, physiological sensor, and self-reported life-event data for six patients with type 1 diabetes for eight weeks. Patients receiving insulin therapy are exposed to risk of hyperglycemia and hypoglycemia due to underdosing and overdosing. Therefore it is important that they receive the right dosage of bolus insulin. Note that for evaluating a recommendation method we need to have access to counterfactuals which are not available, hence, it is not possible to directly use the dataset. Here, we follow [55], and ﬁrst use the dataset to train a simulator using gradient boosting and then use the trained model to produce glucose level for a pair of context (feature) and action (bolus insulin dosage). The corresponding reward will be computed using(16). There are nine attributes for each patient and ten actions uniformly chosen between 0 and 1 (corresponding to normalized insulin dosage). To create missingness, we erase each attribute independently with probability0.3. We refer to appendix for a more detailed explanation of the experiment setting. We compare our method with several baselines including logistic regression (LR) and random forest (RF). In LR1 and RF1 we consider the action as one of the attributes whereas in LR2 and RF2 we train 10 different models corresponding to each of the actions. Many of the more recent competing methods accommodate only two actions and hence cannot be directly used in our setting. Here we compare with GANITE [34] a GAN-based method which does not have a restriction on the number of actions. For these baseline methods we ﬁrst impute missing values using both mean imputation and PVAE (we only report the best performance of the two, and use multiple imputations when using PVAE) and then feed the completed feature to the algorithm. As demonstrated in Table III, CPVAE with MER strategy outperforms other methods and the proposed conservative strategy has fewer instances with rewards less than -2. In this work we proposed using a conservative strategy for dealing with uncertainty due to missingness. We suggested two methods for counterfactual estimation in the presence of missing data using VAEs. Our methods were based on using IPS which is known to have high variance. One direction for future work is to improve the method we used for estimation of the propensity scores (e.g. see [29]). We assumed unconfoundedness with missing values which may not hold in some scenarios. Looking for methods for relaxing this condition is another direction for future work. One direction for future work is to also account for the uncertainty due to the variance of our reward estimation which could vary for different actions and this can change our recommendation (we may decide to choose an action with smaller variance). Also, for computing (8)we used random sampling. The minimum in this expression can be approximated using constrained Bayesian optimization method [56]. A. Proposition 1 Proof.First notice that sinceH(a(X )|X ) = 0, hence we have H(a(X ), X,˜X) = H(X ,˜X). We also have: H(a(X ), X,˜X) = H(˜X) + H(a(X )|˜X) + H(X |˜X, a(X)). Therefore, H(˜X, X) = H(˜X) + H(a(X )|˜X) + H(X |˜X, a(X)). Thus following equations hold: H(a(X )|˜X) = H(˜X, X) − H(˜X) − H(X |˜X, a(X)) In the above equations we used the fact thatH(˜X, X) = H(˜X) + H(X |˜X) and H(a(X )|X ) = 0. Example 1.Assume thatX = {0, 1}, andXis uniformly distributed. The channel betweenXand˜Xis an erasure channel which erases each bit independently with probability 1/2. We haveA = {a, a}, and the reward is distributed as follows: R|x, a∼ Ber(x+ x3), R|x, a∼ Ber(x+ x3+ 0.1). Therefore, ifx+ x> x+ x,a(x) = a, otherwisea(x) = a. For instance,Xcould be results of four different tests (which a subset of them will be available) andAis the treatment assigned to the patient. In this setting we haveH(a(X )) = 0.896this is becausex+ x> x+ xholds with probability of5/16, hence, we haveH(a(X )) = h(5/16) = 0.896where his the binary entropy function. For computingI(X;˜X), note that since attributes ofXare independent and also each attribute will be erased independently we have: I(X;˜X) = 4(H(˜X) − H(˜X|X)) = 4(1.5 − 1) = 2. I(X;˜X|a(X)) = H(X|a(X)) − H(X |˜X, a(X)). Now for the second term we have: H(X |a(X )) =516H(X |a(X ) = a) +1116H(X |a(X ) = a) The other term can be computed similarly by considering the different cases of˜Xanda(X)and using the symmetries for simpliﬁcations. Thus,H(a(X )|˜X) = 0.570. Note that, the probability that the best algorithm ﬁnda(X)by observing˜X is given by Thus in our example, we can hope for guessinga(X)correctly on average in 67.3% of cases. Instead of using propensity score we can estimate the reward using the following equation, by simply matching the similar actions. For each actiona ∈ A, deﬁneN= {i : A= a}to be the set of all indices with action a. The idea is similar to original SPVAE, we estimate the reward with a weighted average of the reward of all instances for which actionawas prescribed. The weights measure the similarity betweenxand˜X. Comparing this estimator with Eq. (12) of the paper, we note that the denominator ofwhere is different and also the inverse propensity score is missing. Note that weP havew= 1in (12). However, on average onlyπ(a|˜X) fraction of samples satisfy1[A= a]. One can interpret the propensity score in eq (12) as a way for compensate this. Basically we have Here we are outlining details of experiment setting, including hyperparameters and architecture of the neural nets. To implement our experiments, we have used JADE, the UK Tier-2 HPC Server specialised for deep learning applications. In particular, all our models are trained with a Nvidia Tesla V100 GPU card, with access to 70GB memory (though we did not use up the whole memory space). One can also run experiments 1 and 2 on a personal laptop. Also, tensorﬂow 1.15 is the library used for implementation. 1) MNIST: For our MNIST experiment, we used a reduced MNIST dataset of 5000 data points, and performed a 80%-20% train-test split. We used the CPVAE architecture. The encoder followed the PNP-based architecture from [3]. We used 400 dimensional feature mappinghparameterized by a single fully connected network with ReLU activations, and 20 dimensional IDefor each variable. For Gaussian latent variables we used a 20 dimensional diagonal vector to represent it. The encoder (denoted by functionfin the paper) is ak-500-200-40, wherek is a vector resulted from the concatenation ofhanda, a onehot encoded action vector. The networkfis a fully connected neural network with ReLU activations. The decoder (generator) shares the similar architecture:Z-200-500-D, whereZis a vector resulted from the concatenation of the latent variable and a, thus here Z = 30. Also, D represents the output of the generator model which should produce pixels and the reward, henceD = 785. For the conservative strategy, we generated50 random samples (with the notation of paperu = 50). During the training phase, we created artiﬁcial missingness to dataset by randomly erasing50%of the pixels from each image. We used an Adam optimizer with default hyperparameter settings, a learning rate of0.001, and a batch size of 8. We trained the network for 20 epochs and repeated the experiment for 100 times to get our results. 2) Additional Experiment Results: In Fig. 5 the distribution of the rewards for MNIST experiment for three cases of conservative strategy withc = 0.001andc = 0.4and also imputation strategy is provided. As expected, this illustrates that imputation strategy has a longer tail in comparison with conservative strategies. Also, c = 0.001 has the shortest tail. 3) IHDP: We used both SPVAE and CPVAE on this dataset, for both models we used 20% of the whole dataset as our training data. Missingness was injected to the dataset by assuming MCAR mechanism at different level of missingness, i.e. 10%, 30% and 50%. We used 5 dimensional feature mappinghparameterized by a single fully connected network with ReLU activations, and 10 dimensional IDefor each variable. The Gaussian latent variablezis set to a 10 dimensional diagonal vector. The inference net is ah-20-20-20 fully connected network with ReLU activations. The generator net is az-20-20-D (where D is the observed feature dimension of IHDP dataset) fully connected network with ReLU activations. We trained the PVAE using the Adam optimizer with its default hyperparameter settings, a learning rate of 0.001 and batch size of 8. The network was trained for 25 epochs each time and the entire procedure is repeated 100 times for each missingness level. For CPVAE, we used the same architecture as SPVAE, and the training objective is to reproduce all the attributes with their corresponding rewards given the missing attributes and the action taken. The only difference is that one-hot encoded action was added as the input of encoder and also as an input to the decoder, similar to what we described for MNIST dataset. 4) Hyperparameters: Here in Table IV, we show that the dependence of our result for IHDP to hyperparameters is insigniﬁcant and our method consistently outperform competitors regardless of choice of hyperparameters. Here we consider several combinations of latent dimension (LD), batch size (BS), and valueKin the structure of PVAE which is output of the layer which encodes input variables and feed it to the ﬁrst encoder. 5) OhioT1DM: The OhioT1DM dataset contains 8 weeks information about 6 individuals with Diabetes 1 in a time series format. Since in the original dataset only the response to the actual dose that was administered exists, it is not possible to evaluate recommendation methods directly using dataset. Thus, we use a simulator suggested in [55], that is trained on the actual data to estimate the response to a bolus injection. The simulator maps a pair of context and action to the mean of continuous glucose monitoring (CGM). From CGM the reward can be calculated according to equation (16) in the paper. A Gradient Boosting regression model with Huber loss is used to achieve this. In the model, 100 trees of maximum depth of 5 is used as weak learner. Furthermore a multivariate Gaussian distribution is ﬁtted to approximate patients features. For producing dataset, we ﬁrst sample from this distribution to get the features, then we choose an action for this feature and then feed the pair of action and feature to the simulator to produce the output. Then we randomly remove30%of features and store the triple of feature (with missing values), action, and the simulated reward in the dataset. We have produced 5000 samples for training. The way we produce actions is to train a simple logistic regression model to learn the action that is prescribed in the original dataset, and use this model to produce actions. In order to guarantee the second condition of the assumptions in Section II (i.e.π(a|˜x) > 0), we choose the action half of times from the action generator (logistic regression model) and for the other half we randomly choose one of the 10 actions. In the test time, we sample from Gaussian distribution to get the features, then randomly remove30%of the features and feed it to our method to get the recommended action. Then we feed the complete feature and action to the simulator to get the reward. We refer to [55] for more details about the data generation mechanism. We chose the following hyperparameters for the model:K = 8,e= 5, latent dimension is 5, and batch size is 8. A two layer encoder and a two layer decoder is used, with 10-10-5 and 5-10-10 nodes respectively.