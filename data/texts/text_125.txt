In this work, we release a large and novel dataset of learners engaging with educational videos in-the-wild. The dataset, named Personalised Educational Engagement with Knowledge Topics (PEEK), is one of the rst publicly available datasets that address personalised educational engagement. Educational recommenders have received much less attention in comparison to e-commerce and entertainment-related recommenders, even though ecient personalised learning systems could improve learning gains signicantly. One of the main challenges in advancing this research direction is the scarcity of large, publicly available datasets. In the PEEK dataset, educational video lectures have been associated with Wikipedia concepts related to the material of the lecture, thus providing a humanly intuitive taxonomy. We believe that granular learner engagement signals, in unison with rich content representations, will pave the way to building powerful personalisation algorithms that will revolutionise educational and informational recommendation systems. Towards this goal, we 1) construct a novel dataset from a popular video lecture repository, 2) identify a set of benchmark algorithms to model engagement, and 3) run extensive experimentation on the PEEK dataset to demonstrate its value. Our experiments with the dataset show promise in building powerful informational recommender systems. The dataset and the support code is available at https://github.com/sahanbull/PEEK-Dataset. • Information systems → Users and interactive retrieval;Personalization;Recommender systems; Information extraction;• Applied computing → Interactive learning environments. ACM Reference Format: Sahan Bulathwela, María Pérez-Ortiz, Erik Novak, Emine Yilmazand John Shawe-Taylor. 2021. PEEK: A Large Dataset of Learner Engagement with Educational Videos. In ORSUM ’21: 4th Workshop on Online Recommender Systems and User Modeling at ACM RecSys 2021, October 02, 2021, Online. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn. nnnnnnn Developing articial intelligence systems that, mildly at least, understand the structure of knowledge is foundational to building eective recommendation systems for lifelong education[13,41,62], as well as for many other applications related to knowledge management and tracing[35,61]. In the context of personalised education, the research communities have tirelessly worked on building Intelligent Tutoring Systems (ITS) which have been their focus from the early applications of AI in education[19]. ITS focus heavily on personalising testing opportunities that will allow learners to demonstrate their mastery of a Knowledge Component (KC), an atomic unit of knowledge that can be learned and mastered. These systems are designed for learning experiences with limited scope (such as learning about a specic topic, short course, etc.) which makes hand crafting KCs and encouraging repetitive test taking operationally feasible. More recent developments in online education has led to the boom of Open Educational Resources (OERs) and Massively Open Online Courses (MOOCs) moves from ITS to educational recommendation systems and personalised e-learning platforms that can cater a more informal learning setting where self motivated learners discover educational materials online to pursue their lifelong learning goals. To succeed in this landscape, a wider range of factors such as the diversity of learners, their personal drivers and how these drivers change over time has to be accounted for. One of the major barriers in building next generation educational recommendation systems is the scarcity of publicly available datasets. We publish (P)ersonalised (E)ducational (E)ngagement Linked to (K)nowledge Topics (PEEK), a novel dataset that comprises of watch time interactions of over 20,000 informal learners watching over 10,200 unique educational video lectures in an OER repository. Our contribution is two-fold, consisting of: i) constructing a novel dataset to predict learner engagement with educational videos and ii) formalising a prediction task with several baselines. The video lectures are partitioned into multiple fragments where the parts are transcribed and associated with Wikipedia topics using entity linking. PEEK dataset uses a humanly intuitive content representation where the topics are atomic Wikipedia pages. Having humanly intuitive content representations allows building explainable learner models that encourage learner meta-cognition and self-regulation, a valuable feature of a technology enhanced learning system[16]. The normalised watch time of individual users is discretised and provided as a target label dening a binary (engaged vs. not engaged) prediction task. We further identify several baselines from prior work and establish a formal task to benchmark predictive performance on the proposed dataset. Publication of this dataset is inspired by advances in multiple research verticals. PEEK dataset is an educational recommendation dataset that contains learner interactions with fragments of video lectures. We outline the relevant works from these dierent research verticals in this section. Knowledge Tracing (KT) and Item Response Theory (IRT), the foundational methods used in Intelligent Tutoring Systems, are evolving from traditional machine learning[19,42,43,54,63] to deep learning models[33,46] that improve predictive performance by sacricing interpretability and transparency, crucial requirements for many of these systems[2,15]. It is also hard to overlook the data hunger of neural approaches considering the possible data scarcity in an educational setting where additional information should be exploited. While accompanying these additional drawbacks, deep learning models used in these tasks are still not guaranteed to outperform classical approaches[59]. Another major drawback of ITS research is its primary focus into predicting knowledge mastery via explicit learner feedback that comes through test taking. Although test taking is a reliable approach to verify learning, this approach comes with its fair share of limitations. Tests need to be carefully crafted by experts in a way that the skill mastery of learners on specic skills can be carefully veried with accompanying hints[18,25,26] costing substantial human capital. Scaling automatic question generation is an open challenge. The other stakeholder segment put into pressure by question solving is the learners themselves. Putting the learner in front of tests too frequently can signicantly hinder the user experience driving them to abandon the system. Beyond the use of costly explicit user interactions, there are various other implicit interactions learners execute within an interactive e-learning system that can be potentially exploited. Learner engagement with educational materials is also indicated by implicit interactions such as watching/pausing/rewinding/replaying video lecture, taking part in discussion fora etc.[48]. In the context of videos, although the potential of using watch time for engagement prediction[60] and personalisation[20] has led to positive results, this idea is under-explored in the education domain with only a handful of studies showing promise[9,29] in populationbased engagement prediction. Although there is a strong interest in building state-aware, personalised educational systems in the recent years[10,11], an obvious reason for slow growth of personalised educational recommenders is the scarcity of publicly available datasets. Unfortunately, there is very limited work in the Information Retrieval (IR) community that deals with a personalisation scenario similar to PEEK dataset as IR scenarios in education are signicantly dierent from general click behaviours for which datasets are abundantly available. TrueLearn Novel model[14] is the only recent algorithmic contribution that deals with a similar dataset where topical contents automatically extracted from educational videos are used to predict learner engagement. Apart from sophisticated learner modelling techniques such as TrueLearn Novel, other basic recommendation strategies, such as content-based ltering and collaborative ltering, are also applicable to this dataset[34]. Although majority of studies in the eld focus on recommending content items that are relevant to a learner, the possibility of recommending parts of items (e.g. a part of a video in contrast to an entire video) has been investigated lately. Proximity-aware information retrieval that exploits the positional structure of tokens is not a new idea[50] in IR. Segmenting videos and building a table of contents using video segments has proven to be useful in eciently summarising video content[28,38]. Breaking informational videos into fragments has also shown promise in ecient previewing[8,17,44] and enabling non-linear consumption of videos[53]. Our prior proposal, TrueLearn Novel[14] model demonstrates the potential of using fragment-wise recommendation in educational recommenders. For these experiments, we created video parts of 5,000 characters (5 minutes) in the fragmentation process. This allows the e-learning system to have video fragments that contain a satisfactory amount of knowledge while keeping the video fragment length at a favourable value in terms of retaining viewer engagement[29]. While TrueLearn Novel demonstrates promise, we strongly believe that availability of such a dataset to the public is critical in pushing the frontiers of research in (fragment-based) educational recommendation. PEEK dataset addresses this need. In addition to manual question generation, systems using KT and IRT often relies on expert labelling of theKnowledge Components (KCs)[51] (sometimes also for the hierarchy of knowledge[3] and dening a Q-matrix[52]), which is time consuming and not scalable to lifelong learning applications. There are various machine learning based unsupervised learning approaches that are used to extract latent topics from textual content. Approaches such as Latent Dirichlet Allocation (LDA)[5], Latent Semantic Analysis (LSA)[22] and other probabilistic approaches[31,36] are potential candidates in this area. However, these unsupervised learning approaches suer from complex hyper-parameter tuning processes and limited interpretability of the discovered latent KCs creating gaps in transparency. A step forward in this area has been using explicit representations based on taxonomies such as Wikipedia concepts[23, 27]. Wikication, a form of entity linking[6,24] has shown great promise for automatically capturing the KCs covered in an educational resource. This technology is on a promising path towards providing automatic, humanly-intuitive (symbolic) representations from Wikipedia, representing at the same time up-to-date knowledge about many domains. Wikipedia, being one of the largest encyclopedias in the world, evolves with time due to the contributor population that constantly updates it. Due to these reasons, we use Wikication[6] to generate KCs that are included in the PEEK dataset. The majority of datasets that are related to this task come from the knowledge tracing domain[19,46] which focuses on recovering knowledge/skill mastery of learners based on how they answer to specic exercises. This approach, as discussed in Section 2, is much more explicit and eort intense than inferring it using implicit feedback (e.g. watched patterns of educational videos). Majority of the research in this domain has been based on ASSISTments data[51], that records a series of learners solving tests in an ITS platform. These datasets are heavily biased towards mathematics knowledge as the ASSISTments tool was used for mathematics education from the beginning[26, 39]. A few publicly available datasets to solve knowledge tracing problem exist. They include mathematics ASSISTments[32,55], problem solving interactions[18], or multiple choice questions[56, 57]), all of which are noteworthy although they do not focus on implicit feedback and engagement. MOOCCube is a very recently released dataset that contains a spectrum of dierent statistics relating to learner-MOOC interactions including implicit and explicit test taking activity[62]. Although this dataset may contain data that can be used to predict learner engagement with implicit feedback, the dataset has only been used in prerequisite detection task which is very dierent. However, our prior experiments have consistently demonstrated that engagement prediction via online learning[14] can be achieved with a dataset that is similar to PEEK in structure. Although previous work based on popular video repositories such as edX[29], Khan Academy[37], VideoLectures[13,14] and YouTube[20,60] has evidenced the existence of datasets that include implicit engagement signals, these datasets are never made public due to their proprietary nature. The scarcity of large scale publicly available datasets for predicting learner engagement with educational videos constrain the growth of the eld. In this context, the Personalised Educational Engagement (PEEK) dataset, is the rst and largest learner video engagement dataset that will be publicly released with humanly-interpretable Wikipedia concepts and the concept coverage associated with the video lecture fragments. In this section, we describe how the Personalised Educational Engagement linked to Knowledge topics (PEEK) dataset is constructed. Figure 2 outlines the overall process of creating the PEEK dataset. PEEK dataset is constructed using video metadata and learner activity data extracted from VideoLectures.Net(VLN), a repository of scientic and educational video lectures. VLN repository records research talks and presentations from numerous academic venues accompanied by the lecture slides associated with the video. As the talks are recorded at peer-reviewed conferences and prestigious research venues, the lectures are reviewed and material is controlled for correctness of knowledge. Figure 1 depicts the dierent components of the VLN user interface where a user engages with an educational video. Every lecture has a title, event details (e.g. conference venue, date, location etc.) and lecture meta data associated with it. Although most lectures consist of one video, some video lectures may contain more than one video as shown in Figure 1. First, the videos in VLN repository are transcribed to its native language using the TransLectures project. Then the non-English lecture videos are translated to English as we will use English Wikipedia for entity linking. As described in Section 3.1, a lecture in VLN repository can contain one or more videos. To model user interactions in a much more granular level we further break each video into smaller parts. Once the transcription/translation is complete, we partition the transcript of each video into multiple fragments where each fragment covers approximately 5 minutes of lecture time. Having 5 minute fragments allow us to break the contents of a video into a more granular level while making sure that there is sucient amounts of information in each video fragment. In order to identify the Knowledge Components (KCs) that are contained in dierent video fragments, we use Wikication[6]. This allows annotating learning materials with humanly interpretable KCs (Wikipedia concepts) at scale with minimum human-expert intervention. This setup will make sure that recommendation strategies build on this dataset will be technologically feasible for webscale e-learning systems. Previous works have demonstrated the value of using Wikication in this task[14]. We restrict the dataset to English Wikipedia due to its richness in comparison to other languages. As per [6], Wikication produces two statistical values per annotated KC, namely, PageRank and Cosine Similarity scores. PageRank score is calculated by constructing a semantic graph where semantic relatedness (𝑆𝑅(𝑐, 𝑐)) between each Wikipedia concept pair 𝑐 and 𝑐in the graph are calculated using equation 1. 𝑆𝑅(𝑐, 𝑐) =log(𝑚𝑎𝑥 (|𝐿|, |𝐿|) − log(|𝐿∩ 𝐿|)log |𝑊 | − 𝑙𝑜𝑔(𝑚𝑖𝑛(|𝐿|, |𝐿|)(1) where𝐿represents the set of Wiki concepts with inwards links to Wikipedia concept𝑐,| · |represents the cardinality of the set and𝑊represents the set of all Wikipedia topics. This semantic relatedness graph is used for computing PageRank scores. PageRank algorithm[7] leads to heavily connected Wikipedia topics (i.e. more semantically related) within the lecture to get a higher score. The Cosine Similarity score is used as a proxy for topic coverage within the lecture fragment[14]. This score𝑐𝑜𝑠 (𝑠, 𝑐)between the Term Frequency-Inverse Document Frequency (TF-IDF) representations of the lecture transcript𝑠and the Wikipedia page𝑐is Figure 1: Screen layout of VideoLecture.Net website from which the data for PEEK dataset was sourced. Every lecture can have one or more videos and is associated with metadata. The lecture transcripts are also available for processing. calculated based on equation 2: whereTFIDF(𝑠)returns the TF-IDF vector of the string𝑠while|| · || represents the norm of the TF-IDF vector. The authors of [6] comment that a linearly weighted sum between the PageRank and Cosine score can be used to rank the importance of Wikipedia concepts as per equation 3. Rank(𝑐) = 𝛼 · 𝑃𝑎𝑔𝑒𝑅𝑎𝑛𝑘 (𝑐) + (1 − 𝛼) · 𝑐𝑜𝑠 (𝑠, 𝑐), where 𝛼 ∈ [0, 1] We experimented with dierent values for𝛼in equation 3 and empirically validated suitable linear combinations of weights for PageRank score and Cosine Similarity score. We observed that a weight of 0.8 on PageRank and 0.2 on Cosine similarity leads to the most suitable ranking of KCs[12]. We useRankto identify the ve top-ranked KCs for each lecture fragment. The cosine similarity score as per equation 2 is included in the dataset as a proxy for coverage of that KC in the lecture fragment. We restrict the number of KCs to 5 as larger numbers (e.g. 10 KCs) have shown to degrade performance of learner models built with similar datasets[14]. Figure 4(ii) provides a word cloud of the most dominant KCs in PEEK dataset. It is evident that the majority of KCs (Wikipedia topics) associated to the lecture fragments in this dataset are related to articial intelligence and machine learning. We restrict the nal dataset to lectures that have been viewed by at least 5 unique users to preserve k-anonymity[21] of users. Also, we report the timestamp of user view events in relation to the earliest event found in the dataset obfuscating the actual timestamp. That is we report the smallest timestamp in the dataset𝑡as 0s and any timestamp𝑡after that as𝑡− 𝑡. This allows us to publish the true order and the real dierences of duration between events without revealing the actual timestamps. Additionally, the lecture metadata such as title and authors etc. are not published to preserve the anonymity of the authors/lecturers. The motivation behind this decision is to avoid authors of the video lectures having unanticipated eects on their reputation by associating implicit learner engagement values to their content. The user interface of VLN website also records the video watching behaviours of its users (Interaction logs in Figure 2). The target label for learner engagement is a discrete variable based on video watch time which has been used as a proxy for video engagement in both non-educational[20,60] and educational[14, Figure 2: The video data and the learner interaction logs from VLN repository are processed separately to create the Wikipediabased KCs and also the discrete engagement signals that are published in PEEK dataset. Figure 3: Visual representation of the dierent data items available in the PEEK dataset. Each video is broken into multiple, non-overlapping 5 minute fragments that are linked with ranked Wikipedia-based KCs. The watched parts of the video (in red) are used to create discrete engagement labels. 29] contexts. Normalised learner watch time𝑒of learnerℓwith video fragment resource𝑟at time point𝑡is calculated as per equation 4. where𝑒∈ {0, 1},𝑊 (·)is a function that returns the watch time of learnerℓfor resource𝑟and𝐷 (·)is a function that returns the duration of lecture fragment𝑟. The ultimate label𝑒is derived by discretising𝑒where𝑒= 1when𝑒≥ .75and 𝑒= 0otherwise. The discretisation rule is motivated by the hypothesis that a learner should watch approximately 4 minutes of an educational video fragment that is approximately 5 minutes (duration of a video fragment that includes 5000 characters from the video transcript) in order to acquire knowledge from it[14]. The nal PEEK dataset consists of 290,535 interaction events from 20,019 distinct users with at least 5 events. These learner engage with 10,233 unique lecture videos that are partitioned into 39,113 fragments (3.82 fragments per video). The learner population in the dataset is divided into Training (14,050 learners) and Test (5,969 learners) datasets based on a 70:30 split. The label distribution in the dataset is also relatively balanced with only 56% of the labels being positive. As shown in Figure 4 (i), the majority of learners in the dataset have a relatively small number of events (under 80) making this dataset an excellent test bed for personalisation models designed to work in data scarce environments. VLN repository mainly publishes videos relating to Computer Science and Machine Learning leading to a learner audience who visit to learn about these subjects. This fact is conrmed by Figure 4 where it shows that the dataset is dominated by events with AI and ML related KCs. The nal dataset consists of 3 les. (1) train.csv, used for hyperparameter validation and training parameters. (2) test.csv, used as the held-out test set. (3) id_url_mapping.csv, which contains the mapping between KC IDs and Wikipedia page URL. train.csvandtest.csvles contain the actual learner session data where their interaction with lecture fragments are recorded. The two les contains 70% and 30% of the learners respectively. Both les contain 15 columns that are described in Table 1. As the name suggestsid_url_mapping.csvle contains a mapping between the KC ID in PEEK dataset and the URL of the Wikipedia page of the concept associated with that KC. Figure 4: Characteristics of the PEEK dataset: (i) number of learners in the training/test dataset based on the number of events in the session for individual learner and (ii) wordclouds depicting the most frequently detected Wikipediabased knowledge components. We benchmark the predictive performance of this dataset with respect to learner engagement prediction by proposing a set of baseline models and measuring their performance with the PEEK dataset. Unlike previous studies, which relied on small, expert-curated collections of open educational materials, our dataset is built using in-the-wild watch patterns of multi-lingual open educational materials available online. We construct two distinct datasets for our experiments. (i)Active 20is a smaller dataset that consists of 8,207 interaction events from the 20 most active users (410.35 events per learner) in the PEEK dataset. This dataset contains a substantially small number of users who have relatively longer sessions. The smaller dataset will allow us to understand how the proposed learner models will behave with mature learners that have more interaction information in the system. On the contrary, (ii)PEEKis the full dataset that consists of 290,535 interaction events from 20,019 distinct users with at least 5 events. However, the majority of users in this dataset have relatively smaller sessions (average of 14.51 events per learner). As the dataset is already divided into train and test splits, all our experiments use a hold-out validation (train-test split) approach where hyperparameters and parameters are learned on 70% of the learners and the model is evaluated on the remaining 30% with the selected hyperparameter combination. A sequential experimental design is employed, where^𝑒, engagement with fragment𝑡is predicted using fragments1to𝑡 − 1. Since engagement is binary, predictions for each fragment can be assembled into a confusion matrix, from which we compute wellknown binary classication metrics such as precision, recall and F1-measure. We focus on these measures as we are more interested in predictive lecture fragments that the learners are likely to engage with. We average these metrics per learner and weight each learner according to their amount of activity in the system. We use F1measure for model comparison as we are interested in improving both precision and recall. The primary objective of the experiments is to evaluate how the dierent attributes of the dataset are useful in modelling engagement of learners. Towards this goal, we run two primary experiments, 1) investigate how dierent recommendation models perform with predicting engagement, and 2) how the number of KCs impact the prediction model. The benchmark algorithms used for the experiments are outlined in Section 4.3. The results of the experiments are outlined in Section 5. PEEK is the rst of its kind, a dataset that records in-the-wild engagement of informal learners with video lecture fragments. Due to the novelty of this dataset, we struggle to nd already published baselines, except for the TrueLearn family of algorithms[14]. For the sake of comparing its predictive performance, we also propose a set of baselines that are based on content-based and collaborative ltering. Table 1: Detailed descriptions of the dierent columns of the train.csv and test.csv les included in the PEEK Dataset. Content-based Similarity. Content-based ltering can measure the similarity between two items. We compute a similarity value, 𝑠𝑖𝑚(𝑟, 𝑟)between two consecutive lecture fragments𝑟 and𝑟in the learnerℓ’s session. We use this similarity value to make an engagement prediction^𝑒based on equation 5. In this case, we investigate two similarity measures, namely 1) Cosine, 2) Concept-based Jaccard and User-based Jaccard. When computing cosine similarity, we represent each video fragment using the bag of concepts representation where the concepts are the super set of Wikipedia concepts mentioned in the dataset. The values in this sparse vector are the cosine similarities between respective Wikipedia concept and the lecture fragment transcript as per equation 2. An alternative approach to nding concept-wise similarity is Jaccard similarity. Concept-based Jaccard similarityJaccard(𝑟, 𝑟), between lecture fragments𝑟and𝑟is computed based on equation 6. whereC(·)is a function that returns the set of Wikipedia concepts in resource 𝑟 Similarly, one can also measure the similarity between two lecture fragments based on how many learners interact with both the lecture fragments. The user interactions in the training dataset is used exclusively to learn the similarity matrix in order to avoid data leakage. In this approach, we can calculate the user-wise jaccard similarity Jaccard(𝑟, 𝑟), as per equation 7. whereU(·)is a function that returns the set of learners that interacted with resource 𝑟 Knowledge Tracing (KT). KT builds a learner representation of knowledge of the learner[63]. This learning model is then used in predicting engagement of learnerℓwith lecture fragment resource 𝑟at time𝑡. As the PEEK dataset has a temporal dimension, we reformulate the KT algorithm into an online learning graphical model inspired by the reformulation in[4]. The skill variables in the KT model are Bernoulli variables (𝜃∼ Bernoulli(𝜋)), assuming that a learnerℓwould have either mastered a skill/ concept 𝑐or not (represented by probability𝜋). Skills are initialised (𝜃) using aBernoulli(.0)prior, assuming that the latent skill is not mastered in the beginning. A noise factor similar to what is found in the conventional KT model[19] is added to this model and is tuned using a grid search. TrueLearn Novel Model. Similar to KT model, TrueLearn model is also an online, graphical model that develops a learner model of the learner. This model is inspired by the TrueSkill model[30] which is an evolution of IRT[49]. Contrary to the KT model, TrueLearn model models skills as Gaussian variables (𝜃∼ N (𝜇, 𝜎). In addition to modelling knowledge, TrueLearn Novel model also models novelty of content which is a key aspect of educational recommendation[14]. Enforcing the same assumptions of KT model, the TrueLearn Novel skill parameters are initialised using aN (.0, 𝜎) prior where𝜎, initial variance is a hyperparameter tuned using a grid search. The predictive performance of multiple benchmark model (outlined in Section 4.3) with the PEEK dataset is evaluated. The hyperparameters of each model (including number of KCs) are tuned using a grid search. The model performance with the best hyperparameter combinations is presented in Table 2. Good predictive power of the similarity based approaches validate the expressiveness of the content representations based on Wikipedia concepts and cosine similarity. The poor performance of 𝐽𝑎𝑐𝑐𝑎𝑟𝑑Similarity model with the top 20 user dataset is expected as this model relies on learning similarities based on training data. Although 20 user dataset contains active users with more events per session, there is less data in total due to the small number of users which leads to learning a mostly incomplete similarity matrix between lecture fragments. It is seen that this condition changes with the full PEEK dataset where more data is available. The results in table 2 shows that TrueLearn Novel model performs best among the benchmarks investigated. This result further reinforces the superiority of TrueLearn Novel model investigated with a similar dataset[14]. The impact of the number of KCs for each model is also investigated and the results are reported in Figure 5. The plots indicate that the similarity based approaches tend to perform better with more KCs in the representation. This is natural as Similarity based approaches need more information from the exact pair of contents that are being compared in order to make a ne grained prediction. On the contrary, Knowledge Tracing and TrueLearn novel Models that develop a learner model perform better with less number of KCs. This observation is also expected as the learner model has the capacity to store information about various dierent KCs that the learner encountered over the past. This allows the learner model to make a good prediction with little information from the current resource. In the TrueLearn model, the eect of adding extra KCs to the representation makes a very small dierence in performance. Knowledge (skill mastery to be more specic) is usually measured from explicit test performance[19,40,46]. Contrary to narrowly scoped learning scenarios common to ITS, seeking explicit feedback is comparatively less realistic in lifelong learning settings where informal learners attempt to acquire knowledge without necessarily having the need to go through rigorous testing. Forcing learners into aggressive testing can hinder their learning experience. Additionally, technical challenges too exist in areas such as scalable test/ question generation and automatic test marking. However,[14] has demonstrated that modelling knowledge from educational watch patterns is a promising direction towards predicting future engagement. The opportunity that PEEK dataset brings is the capability to advance this research avenue by making the rst large dataset publicly available. This dataset will allow researchers to improve knowledge representation and also include other crucial factors such as learner interest[13] in the spirit of building integrative educational recommenders using implicit feedback. As Figure 4(i) shows, PEEK dataset also contains a large number of learners with limited activity making this dataset an excellent candidate to evaluate low resource, data ecient personalisation algorithms. PEEK dataset also acts as the bridge between multiple rich data sources. The Wikipedia concepts used in representing the KCs also enables connecting this dataset to the variety of auxiliary information available around Wikipedia, the world’s largest encyclopedia. Some examples of possibilities are the ability to leveraging additional data structures such as Wikipedia page contents, the hyperlink graph and the category tree. Other enriched derivatives of Wikipedia such as semantic relatedness[47] and knowledge bases[1,58] are also directly fused with this dataset to build more powerful content representations. In addition, the Wikipedia taxonomy also provides humanly intuitive representations (grounded on Wikipedia concepts) that enhances interpretability. Figure 4 (ii) is a prime example of how humanly intuitive KCs improves the expressiveness of content representations. A humanly intuitive representation also paves way to much needed interpretable learner models that are essential to triggering meta-cognition within learners[16]. PEEK dataset consists of a collection of learners making engagement choices across educational materials over time. This dataset clearly captures the temporal dynamics of the learners whose knowledge state and preferences change over time. The superiority of TrueLearn Novel in Table 2 also gives a strong indication on how a state-aware learner model that changes its learner representation over time, best captures the engagement dynamics of the dataset in comparison to naïve similarity-based approaches. This result rearms the relevance and usefulness of this dataset to invent online learning models that can identify personal and temporal dynamics of users. This section introduces the reader to the tasks that the PEEK dataset could be used for. The main application area of the PEEK dataset is engagement prediction with video lecture fragments in an webbased learning setting as demonstrated in Section 4. However, this task can be further extended by fusing the engagement labels from all the fragments of a video to carry out video recommendation which is more common[20] (contrary to video fragment recommendation). Moving away from engagement prediction, PEEK dataset can also be used to understand the structure of knowledge and learning pathways through it. Deducing the structure of knowledge using Figure 5: Predictive performance of PEEK dataset in terms of Precision (left), Recall (middle) and F1-Score (right) for the benchmark models when varying numbers of Knowledge Components (KCs) are used as the content representation. the co-occurrence patterns of KCs within the video lectures provide opportunities to understand inter-topic relationships and how knowledge is structured. Work in this direction can be used in identifying related materials and accounting for novelty in educational recommendation[14]. The dataset can also be used to identify different clusters/groups of learners and learning resources which will allow understanding the education landscape better. Studying the evolution of KCs within learner journeys over time can also unlock opportunities to understand prerequisites[62]. This work releases PEEK, the rst and largest publicly-available, humanly-intuitive dataset of learner interaction with educational videos opening up various avenues to the research community to push the frontiers of this line of research. It publishes learner interactions of over 20,000 learners with fragments of lectures over time. The predictive performance of several benchmark models is evaluated with this dataset. The power of this dataset is already portrayed by the promising experimental results presented. As future directions, we see the importance of modelling semantic relatedness between KCs[47] that are likely to boost performance of learner models such as Knowledge Tracing and TrueLearn. Exploiting other side information from other Wikipedia such as the category tree is also an interesting direction to explore. The potential of detecting other user signals such as learner interest from PEEK dataset can also lead to insightful ndings. PEEK dataset also allows experimenting with temporal dynamics such as interest decay[45]. This work is partially supported by the European Commission funded project "Humane AI: Toward AI Systems That Augment and Empower Humans by Understanding Us, our Society and the World Around Us" (grant 820437) and the EPSRC Fellowship titled "Task Based Information Retrieval" (grant EP/P024289/1).