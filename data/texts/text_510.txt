Abstract—In this survey, we discuss the challenges of executing scientiﬁc workﬂows as well as existing Machine Learning (ML) techniques to alleviate those challenges. We provide the context and motivation for applying ML to each step of the execution of these workﬂows. Furthermore, we provide recommendations on how to extend ML techniques to unresolved challenges in the execution of scientiﬁc workﬂows. Moreover, we discuss the possibility of using ML techniques for in-situ operations. We explore the challenges of in-situ workﬂows and provide suggestions for improving the performance of their execution using ML techniques. The past decade has seen a tremendous growth in Machine learning (ML) research as well as its use in a range of application domains. ML techniques can capture the patterns, relations, and behavior of different environments and datasets [1]. They enable machines to automatically learn from data, identify patterns, build analytical models, and make decisions with minimal human intervention. ML processes involve a training phase through which the characteristics of the data as well as patterns in the data can be learned and used to make predictions for current or future datasets and events. ML algorithms combined with recent advances in high performance computing (HPC) have the potential for providing new and timely insights in a wide range of domains. Recent years have also seen an increasing interest in the use of ML as part of scientiﬁc workﬂows to accelerate scientiﬁc discovery [2]. The growing volumes of data produced by experiments, observations as well as large-scale simulations, coupled with increasing computational power provide tremendous opportunities for leveraging ML techniques to improve and/or accelerate scientiﬁc insights [3]. Scientiﬁc workﬂow use ML techniques in multiple ways. For example, ML techniques are used to extract the underlying structure [4], construct predictive models, and help deal with the inherent complexity of the data [4]. ML techniques are also used to improve performance, resource provisioning, scalability, and reliability and robustness during workﬂow execution [5]. The goal of this paper is to survey the use of ML techniques as part of scientiﬁc workﬂows running on large-scale systems. In this paper, we explore the landscape of ML techniques in the context of scientiﬁc workﬂows across a range of domains and usecases. We focus on different aspects of scientiﬁc workﬂows, including preparing the inputs, executing the workﬂow and processing its outputs, and study which ML techniques are used and how they are used in a workﬂow. We also explore associated challenges and opportunities. For example, in preparing inputs for a workﬂow, we explore the role of ML for input handling, parameter tuning, model optimization, computational steering, and workﬂow composition. During workﬂow execution, we explore the application of ML techniques for scheduling, failure detection and fault management, data placement and movement, I/O and memory management, and overall performance optimization. In handling the outputs from a workﬂow, we explore efforts that use ML techniques for correctness and veriﬁability of the results. We ﬁnally focus particularly on the use of ML techniques by in-situ scientiﬁc workﬂows, which become increasingly important at extreme scales. Note that there have been other surveys of scientiﬁc workﬂows over the years that have primarily focused on identifying current and future challenges and providing possible solutions [6], [7]. This paper complements these surveys and focuses on the state of the art in the application of ML in scientiﬁc workﬂows. The following section provides the background context for this survey. To facilitate reading, a framework for the survey is presented in Figure 1. As illustrated in Fig.1, we focus on three key aspects of a scientiﬁc workﬂow: creation, execution, and output processing. We start with input preparation, parameterand model tuning, and workﬂow composition in Section III. As is shown in Fig.1, workﬂow composition is typically a function provided by Workﬂow Managements Systems (WMSs) that enables users to modify a workﬂow even while it is executing. Section IV focuses on key runtime aspects of WMSs including scheduling, monitoring, prediction of resource usage, I/O and memory, and failure detection. Management of workﬂow outputs and provenance is discussed in Section V. Finally, in Section VI, we focus on in-situ workﬂows. We present the key Fig. 1: Overview of the survey. Illustration of how the workﬂow is composed of input, then executed and lastly how its output managed. Section number describe the following contents in order of appearance. For executing a scientiﬁc workﬂow, ﬁrst data and model are prepared based on requirement of execution components. Then, simulation, and analysis/visualization form the core of workﬂow execution while WMSs perform vital operations, such as scheduling, monitoring, and resource allocation (I/O and memory) with the aim of optimizing the performance of workﬂow execution. And in the last part, how the output of the workﬂow is managed through keeping provenance information and/or interacting with WMSs. challenges associated with the in-situ execution of scientiﬁc workﬂows and how current ML techniques can be used to address them. Section VII presents a summary and concludes the survey. Scientiﬁc workﬂows have been used in many different domains including earth science [8], astronomy [9], physics [10], and bioinformatics [11] where datasets are large and heterogeneous (from several sources and different formats). Scientiﬁc workﬂows represent the data ﬂow and their dependencies in an acyclic graph, which typically represents complex computations and analytic. They include thousands of steps that involve diverse data ﬂows and dependencies as well as different execution mechanisms in a distributed environment. Workﬂow management systems automate the execution of computational tasks in the next steps and deﬁne the order of these steps based on data ﬂow and data dependency [12]. The scientiﬁc workﬂows give high-level abstraction to the graph of dataﬂows between various components and simplify the automation and management of the workﬂow life cycle. In addition to automation, reproducibility, result sharing, and result derivation are necessary for collaborative researchers to have faster analysis and processes [13]. Putting together, managing and representing such complex distributed systems have SectionIV many challenges from creation, reproducibility, provenance, monitoring, performance optimization, reliability, robustness, scalability and intelligent support for workﬂow design [14]. Considering the growth in computing power, decreasing price of computational resources, as well as the larger scale of available data in different scientiﬁc domains, new techniques need to be provided to solve the challenges of scientiﬁc applications. ML is a good candidate as it is able to tackle complex problems (e.g. scheduling) and ﬁnd interesting structures in large datasets relatively inexpensive. With the power of different learning algorithms, it is now possible to automatically model unstructured data, analyze larger scale, and more complex data [15]. As a consequence, ML can ﬁlter useful information that leads to major advancements [16], and deliver more accurate and faster results. At present, ML is being implemented in a wide variety of industries and various domains including self-driving cars [17], cyber-fraud detection [18], online recommendation systems [19], pattern and image recognition [20] and many more examples. This led researchers to investigate ML as a potential solution to automate and handle enormous, complex issues in scientiﬁc domains. ML techniques typically aim at enabling the machine to automatically learn and predict through the process of training and evaluating with respect to some objectives. ML algorithm learns from the present data in the training phase and predicts the task based on the future data. A larger volume of data in the training phase ensures higher accuracy of the predictive models. On the one hand, ML adds an intelligent learning capability for uncovering the underlying patterns in raw data. On the other hand, the scientiﬁc workﬂow deals with large-scale datasets and high computations that generate a massive amount of results. Putting these together, a great potential for scientiﬁc workﬂow is to leverage ML as an alternative to manually handle jobs by human interaction. However, to support distributed and heterogeneous datasets (e.g., data with heterogeneous features in bioinformatics), ML methods should be carefully selected, designed and optimized to operate properly in scientiﬁc execution. In this paper, we categorize these optimizations into the creation, execution, and output of a scientiﬁc workﬂow. Exploring the literature, these optimizations take various forms that can be roughly categorized into optimizations that improve the experience of users, improve the models used, tune workﬂow parameters, ease data handling, guard against failures, improve I/O performance, and system performance as a whole. Therefore, it is important to understand the role of each part for applying suitable ML algorithms. Creation of a workﬂow can be characterized in multiple dimensions: preparing input data, models and parameters, and composition of a workﬂow. If required, raw data from multiple sources are prepared and transformed into the right format for subsequent execution. These datasets can be unstructured, incomplete, and inconsistent, so need to be indexed and annotated for further accesses. In the second part, parameter tuning and computational steering are discussed and current ML solutions are highlighted. In the last part of this section, we discuss the importance of workﬂow composition, different methods to create them, and ML solutions to ease this process. Scientiﬁc workﬂows deal with a large volume of data and in the case of modern scientiﬁc applications running on HPC, they can even produce up to peta-scale data [21]. To gain insight from this vast amount of datasets, exploring the smaller region of interest, complex data analysis, or visualization have been widely used [22]. However, sheer volume of data prevents fast query or storing. Therefore, parallel ﬁle systems [23] and I/O middleware [24], parallel indexing and querying systems [25], were proposed to provide efﬁcient access to the scientiﬁc workloads. Datasets prior to (or during) the execution of workﬂows, are prepared, characterized [26], organized, indexed [27], sorted, or ﬁltered to facilitate further data analysis or visualization [21]. Here ML can be applied to recognize or predict the location of essential information, reduce the precision of retrieved data for exploratory process when it is not needed (ex. combustion simulation [28]), extract the most important features of data, classify the data that gain insight for visualization and data analytics, or ﬁnd the correlated data records and search operations [28]. Also, insitu middlewares [29], initially targeting I/O, have been used to accelerate preparation of data for the visualization, and data analytic [30]. Analyzing domain-speciﬁc data may require investigating multiple ﬁles and, in the case of simulation results, thousands of raw data ﬁles. As is shown in Figure 2, raw data often needs to be reformatted for related analysis since it should be in the format of particular analysis programs. Raw data can contain semantic errors, missing entries, inconsistent values, unresolved duplicates, or inconsistent formatting, thus it needs to be sanitized and transformed into usable data prior to analysis [31]. This process can be done manually or through scripts in programming languages such as Python, Perl, and R. Reformatting raw data can lead to new insights about the characteristics of data and required assumptions appropriate for the subsequent simulation/analysis. In some domains, it is required to integrate and synthesize complex data to use it in scientiﬁc workﬂows [32]. Heterogeneous data transformation and integration systems across multiple data models become more difﬁcult as the number of resources, analytical tools, and computational services increases [33]. The main challenge here is providing support for the heterogeneous and distributed data stored in different formats, dealing with high dimensional data where scalability, and efﬁciency of methods play an important role. For example, data obtained in bioinformatics, scientiﬁc instruments, astronomical, and sensors show various data formats and representations. To facilitate the handling of various data formats and relevant data elements, feature selection (e.g., genetic algorithm) and classiﬁcation algorithms (e.g., decision tree, support vector machine, neural network, logistic regression and k-nearest neighbor) can be considered as good candidates. The process of feature selection for largescale datasets can take a large amount of effort if based upon human expertise. Therefore, an automation approach at scale can optimize this long process and reduce the need of human interaction. For example, ensemble feature selection is able to extract important features in a distributed environment. While ML algorithms can greatly reduce the overhead of handling such large datasets, a lack of domain knowledge can lead to using the wrong algorithm and consequently obtaining unstable results. ML models can be constructed based on the characteristics of different data models and domains. Figure 1 summarizes the different applications of ML for achieving correction, transformation, and extraction of important features of input data. In scientiﬁc workﬂows, users who possess domain expertise need to consider and modify several conﬁguration parameters (data elements, datasets, attribute values, data transformation, tolerance, and error thresholds) for the execution of a computational problem or simulation that they are trying to solve or execute [34]. Due to the difﬁculty of determining the right values for these parameters before the execution, Fig. 2: Identiﬁed challenges related to data and corresponding ML-based solutions. Three main tasks related to preparing data for simulation/analysis/visualization and proposed ML methods in the literature. Choosing the right ML method can be based on type of data or characteristics of method itself to satisfy some needs including scalability, parallelism, and so on. dynamic workﬂows have been used to allow for ﬁne tuning ongoing experiments [35]. This allows users to dynamically change speciﬁc values for the parameters of the workﬂows while running on HPC systems. This can be observed in Figure 1. Online data analysis and online adaptation help datadriven decision [36], and are supported by monitoring tools [37]. Given this support, users can modify the conﬁguration, checkpoints, datasets, and values of parameters for execution. However, a large number of parameters and the combination of different values can easily complicate the online user steering and confuse the users if these adaptions and values are not properly monitored or registered [34]. Keeping track of each adaption (changes along with values and track of these events) as well as their order can reveal their effects on dataﬂow as well as workﬂow performance such as run-time and resource consumption [38]. This is also beneﬁcial for users to understand what input parameters can signiﬁcantly impact the results and what is the inﬂuence of speciﬁc input value for the parameters on the output [39]. The need for an online monitoring system that can relate parameter conﬁguration of the workﬂow to their effects, can be resolved by adopting ML models into the online monitoring systems. Thus, it is important to have an online mechanism that collects parameters and registers user steering data and correlates the output results with that conﬁguration. As an example, ML models in scientiﬁc domains deal with large and high-dimension data, resulting in millions or billions of parameters for training largescale datasets. This training is complex, time consuming, and demands parallel computations. An obvious question for dataintensive machine learning is how to efﬁciently manage these parameters and how to excavate complex behaviors hidden in large datasets while executing in a distributed environment. Active learning [40] can be used to obtain an optimal subset of training data for such ML models where there exist large amounts of unlabeled data for modeling the problems [41]. This leads us to the need for parallel and scalable scientiﬁc ML, considering various factors including energy saving, accuracy, and computational cost, batch size, and learning rate. There have been some studies focused on combining exascale computing with machine learning to address a range of loosely connected problems in cancer research [42] aims at testing hyperparameter optimization techniques for ML models. Moreover, studies have attempted to integrate ML and scientiﬁc workﬂow [43]. A recent study [44] attempted to shorten the time to obtain deployable scientiﬁc machine learning models from scratch. The automation of iterative processes, including building, testing, and reﬁning models, can eliminate the need for manually performing such repetitive and lengthy tasks. Accelerating the lengthy process of choosing the right parameters can drastically reduce the time. Fig. 3: The role of ML in the workﬂows composition system. Workﬂow can be selected from repository by user, or dynamically by WMSs. This composition can be modiﬁed based on the results obtained from simulation/analysis. Here, ML can extract features from the user’s request, compare and map those features against output result, track the history of compositions and provide an optimized composition or a suggestion for future workﬂow composition. Scientiﬁc workﬂows run on large-scale data, consisting of thousands of smaller jobs with data dependencies between each other. Generally, scientiﬁc workﬂows can be represented as Directed Acyclic Graphs (DAGs) [12] or Directed Cyclic Graphs [45] (non-DAGs) in workﬂow models. DAG-based workﬂows can be scheduled for serial execution or for concurrent execution. Figure 3 illustrates the process of composing a workﬂow. As it is shown in this ﬁgure, the process of composing a workﬂow starts with specifying the data movement across different components and assembled either using a tool that is provided to the user (user-directed), or automatically while it can be chosen from workﬂow repositories. Userdirected composition of a workﬂow reduces the complexity of the workﬂow design process and provides the user the ability to reconﬁgure and change the components of workﬂows. The execution of large-scale scientiﬁc workﬂows tends to be highly repetitive, both in terms of their iterative nature and in terms of the workload spread across tasks. Users may often desire to use automated workﬂow composition [46], through which they can choose and compare the desired conﬁgurations among past ones [47], specify different parameters and models [48] for creation and execution of a workﬂow or in an automated or semi-automated process to save time and effort during workﬂow composition. Users are able to edit these conﬁgurations either graphically [49], or using language modeling [50]. To specify the right combinations of components that are consistent and complete as a workﬂow, ML can be a good candidate. The question is how to verify the result of such assisted workﬂow composition in an automated way and make sure it achieves an optimized choice among all possible conﬁgurations. This requires considering input data, domain speciﬁc variables, tracking the data movements and corresponding outputs, storing previous execution results and models, and ﬁnally the relevant conﬁguration. Researchers need to deﬁne intelligent learners that describe and consider all the above parameters and behaviors of existing workﬂows by applying different input-model pairs and comparing their outputs. However, validating all possible scenarios of inputoutput is not feasible due to the large space of scientiﬁc workﬂows. ML can be used to give some guides to the selection of the right input and models after considering the result of past choices in an unsupervised manner. In an effort by [51], the proposed approach eases the limitation of manual job scripts for handling a large number of parameters and ﬁnding the right combination of them. In this approach, statistical training is used to ﬁnd the most important relationship between the features and output variables. At each step of a scientiﬁc workﬂow execution, different tasks with various data dependencies are executed. Coordination of these steps and the complex data ﬂow among them are handled by the workﬂow management system (WMS) [12]. As a main part of WMS, scheduling of distributed execution on heterogeneous resources aims to minimize the overall execution time while managing the complex problem of experiments over multiple steps [12]. Scheduling multivariate systems needs the measurement of time for different components such as process I/O, runtime, memory usage, and CPU utilization, or application characteristics, where this information later are used to automatically characterize workﬂow task requirements, or application runtime [52]. These approaches estimate the resource usage of each task or application’s runtime based on their input size, workﬂows’ structure, or characteristics of tasks/application. Their estimation models rely on investigating the proﬁle information obtained from the previous workﬂow’s execution. ML techniques such as ensemble learning, classiﬁcation trees, and regression trees also have been used to autonomously generate accurate performance models for scientiﬁc workﬂows. Utilizing ML helps to achieve higher accuracy for predicting execution time, memory, and disk consumption for each subtask [53] of workﬂow or application as a whole [54]. Predicting resource usage can be either at run-time [55] or over multiple executions for a single workﬂow or multiple workﬂow executions [56]. The prediction of execution time for each component of a scientiﬁc workﬂow helps to achieve better scheduling policies. However, with the complex dependencies between tasks and data ﬂow at each step (as well as many other factors in distributed systems), it is an open question as to whether it is possible to deliver a scheduling policy that optimizes execution based on all mentioned objectives. In the following, we discuss the two main roles of WMS: scheduling workﬂow and prediction resource usage. In Figure 4, two main roles of WMS have been highlighted: scheduling and prediction resource usage. In the left side of this ﬁgure, various policies for scheduling and their equivalent ML techniques for optimization have been shown and the right side summarizes the techniques that have been used for prediction and ML-based techniques. 1) Scheduling: Scheduling workﬂows in heterogeneous and distributed environments plays a crucial role in improving the efﬁciency of resource usage and is known to be NPcomplete [57]. Additionally, there is no single strategy that is suitable for scheduling distributed systems with different objectives [58]. As it shown in the Figure 4 exploring scheduling algorithms in the literature suggests different constraints [59]. First category of constraints from the user perspective such as: minimizing execution time [60], deadline based [61], and reliability [62]. Second category of constraints from service provider perspective: resource consumption [63], load balancing [64], and energy efﬁciency [65], where some methods supports multiple criteria [66]. Most of the approaches are based on heuristic and metaheuristic techniques. It is necessary to present comprehensive algorithms that consider multiobjectives optimization problems with multiple constraints. To provide an optimized solution to the problem of scheduling multi-objective workﬂows, several models have been proposed using ML algorithms [67]. Due to the similarity of mutliobjective optimization problem with multi-agents game theory, scheduling of scientiﬁc applications has recently been tackled with ML and Deep Learning methods [68]. As an example, scheduling multiple large-scale applications while satisfying storage constraints using sequential cooperative game algorithm has presented in [69]. This approach formulated the scheduling problem of individual applications as a sequential cooperative game by controlling cost, communication- and storage-aware optimization. Reinforcement learning and Qlearning-based algorithms have been proposed to the problem of scheduling one/ multi workﬂows in distributed systems by considering the heterogeneity of the nodes, the number of workﬂows, or the arrangement of tasks in a DAG of dependencies, with the aim of providing better execution time, or minimizing the load balancing [70]. Using advanced learning algorithms, such as reinforcement learning, that can independently communicate with the environment, fasten the approximation of an optimal decision and update with the changes, may improve the efﬁciency of scheduling algorithms in favor of desired objectives. Continuously monitoring the resources and tracking the execution of workﬂows in distributed systems are now achieve-able through intelligent learner [71]. In the light of recent advances in scheduling scientiﬁc workﬂows using ML, there is still further research that needs to be done to establish the role of different ML techniques in distributed environments with single or multiple workﬂows execution and diverse objectives. 2) Predicting Execution Time : In scientiﬁc applications, properly mapping different workﬂow components to resources can ensure better performance in a shared environment. Therefore, good scheduling requires an accurate estimation of resource consumption for each task [58]. In a distributed environment, estimating the resource consumption of different workﬂows’ components can help the scheduler to better arrange and execute them. In the last decade, there has been a growing interest in predicting the execution time of scientiﬁc workﬂows. The literature on predicting the execution time shows a variety of approaches: system proﬁles (e.g., system variables and time spent for each state of the system used to construct the prediction model, or the amount of computation per processor and memory hierarchy used to construct a network simulator) [72], analytical models (e.g. prediction done by solving queuing network models and using historical method, or static performance analysis technique for the external and internal contention on the use of resources by the tasks) [73], historical data (e.g. using spatial and temporal composition information useful for performance analysis) [74], simulation approaches (e.g using parallel discrete event simulation techniques) [75], statistical methods (e.g. predicting the execution time of processors using execution time of small subset of processors along with regression-based approaches to predict parallel program scalability, or time-series models for formulating and predicting the dynamic behavior of complex systems through observations made sequentially through time) [76]. In recent years, different ML-based scheduling have been proposed to provide accurate estimates of execution time Fig. 4: Summary of ML techniques for scheduling and predicting runtime. A WMSs may have multiple objectives for a scheduling policy. For each objective a ML-based scheduling policy is provided. On the bottom of the chart, a summary of goals and the equivalent ML-based solutions are provided. Resource usage and runtime prediction can also be taken into account for designing an optimal scheduling policy. Prediction can be based on characteristics of application and/or underlying system. [77]. The prediction can be based on system performance attributes [78], or application’s input data [79] or combination of both software and hardware properties [80]. As a simple ML method, regression method has been used in [81], where it tries to ﬁnd a relation between input and tasks’ run-time. In this case, selected features of the input (data, hardware characteristics, memory, and etc.) were used as independent variables to predict the performance as a dependent variable [82]. More advance ML algorithms for runtime prediction have been explored such as Regression Trees [52], neural networks [82], Long short term memory (LSTM) [53], clustering techniques [82], Support Vector Machine (SVM) [83], or combination of several of these methods [84]. Also, ML has been used to predict run-time of workﬂows over multiple executions [83] (using similar features of executions for Ensemble ML technique), or in an ofﬂine learning mode (using parameters such as task resource consumption, system conﬁguration and input data). As ofﬂine learning methods require the entire dataset to Goal be available for processing in a dynamic environment, where data are continuously generated, new approaches for leveraging ML techniques should be investigated. As an example, an incremental learning approach has been suggested [53] to achieve higher accuracy by considering parameters reﬂecting runtime information such as CPU utilization, memory usage, and I/O activities. With the ﬂood of data available today, simulation of realworld problems attracts more users than ever before. Simulations on HPC systems produce a large amount of output. Thus, tools are required to manage these large-scale simulations and consequently their results [37]. As HPC simulations are getting larger and more complex, an autonomous scheduler that monitors the resources and jobs at each step and provides real-time decisions for optimizing the resource usage, is required. Additionally, in some domains, users may not know the optimal value for the parameters of the simulation in advance (e.g., the best e-value in bioinformatics), or desire to change the parameters during the execution until the expected result is achieved or to reﬁne the outputs, and lastly if the results is satisfactory then stop the execution. These concerns have been addressed in the literature such as dynamic workﬂows, user steering, and computational steering [85]. The parameters for these simulations can obtain various values and reﬂect different scenarios. Due to the size of data and the complexity of jobs, applying all possible values to the parameters of an experiment is not a feasible solution [35]. Therefore, the optimization process for these parametric experiments are considered in various aspects. For example, a subset of values can be selected and explored [86]. In this regard, selecting a subset of values and optimizing the parameters can be observed as a mathematical problem and solved by employing ML techniques. Using learning strategies, the scheduler can ﬁnd the effective parameters and their optimized values to reduce the search space by comparing the past results and identifying unnecessary jobs, thus speeding up the experiments [86]. Also, real-time interactive control by which the user can modify the progress of execution, change its parameters and investigate the results through analysis or visualization, has been reﬂected in the literature [87]. Similarly, parameter reconﬁguration has been available in workﬂow management systems [88]. Users can gain insight about simulation’s behavior by exploring different parameters and models [89] and representing their effects via visualization [90]. Therefore there have been many efforts to enable users to interfere [91] or stop the execution or change some parameters, data or the execution [92] and initiate a new workﬂow [93] which necessitate providing some tools that let the users to interact and monitor the execution [94]. Also, in-situ processing has been used to accelerate the loop of simulation-analysis by eliminating the need of writing intermediate data back into ﬁles for analytic [95], [96]. Finally, ML techniques have been utilized to perform CFD simulation in a parallel and online mode [97]. For computationally expensive simulations, ML techniques can prioritize jobs, reduce the search space, predict failure in simulation [98] and help with prediction in favor of faster simulations. Developing efﬁcient methods for preparing subsequent data exploration, visualization, or detailed analysis [99], either manually or automatically [100] can be useful to science end users and can gain insights into characterizing data ’in-transit’. Users need to manipulate high-dimensional data [101] for their peta-scale applications, and the desire to uncover hidden patterns and extract informative features [102], [4] from these massive datasets for subsequent simulation/analytics [103]. In order to achieve this, staging nodes have been used to reduce the latency of accessing simulations’ output for selective data manipulation by ofﬂoading output data and exploiting computational power of staging nodes [21]. Data analysis as a main part of scientiﬁc workﬂow, becomes impossible when datasets are large and heterogeneous. ML comes as a solution by proposing clever alternatives to analyzing large volumes of data. By developing fast and efﬁcient algorithms and data-driven models for real-time processing of data, ML is able to produce accurate results and analysis. The more information is fed into the algorithms, the more accurate they become, as the scalability of data and information increases, it allows for much more accurate predictions than ever before. Machine (Deep) learning has been used to cluster scientiﬁc data [104], and dealing with high dimensional data [104]. It is also possible to integrate learning algorithms with workﬂow management system [105] in a distributed mode [106], construct a machine learning workﬂow [107] based on various objectives and input-data. This helps users to easily edit, add, and compare new algorithms to achieve their desired goal. Due to the unprecedented pace of scientiﬁc discovery, scientiﬁc applications are becoming more complex and dataintensive [108]. Simulations generate massive amounts of data [109], which must be handled by HPC I/O subsystems. Executing collections of complex processes, which requires transferring large amounts of data from one task to another (as well as intermediate data back and forth between the compute node and the storage system) can introduce a signiﬁcant I/O bottleneck [110]. As scientiﬁc experiments grow to exascale, the performance of these workﬂows are limited by the I/O constraints imposed by handling large-scale storage systems and complex memory hierarchy [111]. As a result, data movements between heterogeneous and multi-layers storage systems becomes a main challenge [112]. Many I/O middleware were introduced to mitigate the difﬁculty of handling complex memory hierarchy from the users such as ADIOS [113], staging of intermediate data using memory hierarchy, DataSpaces [29], using shared burst buffer, and local burst buffer. Slow reads and writes can drastically decrease the performance of high-speed and low-latency storage systems [114], as in scientiﬁc workﬂows data are generated by one component (e.g. simulation) and will likely be consumed by other components (either simulation or analysis) [115]. Data prefetching as a solution has been used to overcome this, by predicting ahead of time spatial and local data pattern [116]. Data prefetcher needs to accurately predict at which time, which data will be requested to increase the performance of the system (reduce the gap between computation node and I/O) [117]. Different strategies have been employed to ﬁnd optimal data placement [118], manage data movement [109], reduce data migration within and across different storage layers [119]. Another solution is to use in-situ techniques for compute intensive workﬂows [120], result in reduced amount of data migrated between simulation and analytic. Considering storage hierarchy in data placement can prevent common problems such as load imbalanced and resource contentions [121]. ML can be used to automatically identify an optimized decision for migrating data, or tasks to minimize their movement overhead across the deep memory hierarchy and the network [111], [122]. As a result, the characteristics of storage level and workﬂows can be given to the intelligent entity as an input, and it will investigate the output of the deﬁned decision in favor of I/O efﬁciency. Distributed environments bring this opportunity for largescale scientiﬁc applications to be executed when they generally require massive computational and data resources. Although executing scientiﬁc workﬂows at a large scale is feasible because of heterogeneous and complex distributed resources, they bring up new challenges [123]. Considering the growth in the size and complexity of such resources, failures in the system are inevitable. Consequently, detecting these failures in real time or even predicting them in advance opens up new research directions. It is essential to recognize the repetitive failures by considering the past executions and prevent them in the future ones. Similarly, anomaly is a growing concern that can lead to failure or unexpected behavior such as delayed or lengthy performance and can considerably reduce the efﬁciency. Anomaly detection is trending to be complicated as a result of having different factors in hardware, software, and conﬁguration. Monitoring such complicated systems is a key role in early detection of anomalies and can help to ﬁnd the cause and take actions to ease the effects of these unusual behaviors. For time-aware scientiﬁc applications [124] obtaining correct computational results requires the data to be available and updated in deadline-based manner. Dealing with these challenges throughout scientiﬁc workﬂow execution in a distributed environment needs an intelligent monitoring system that can track changes and identify the failure points in the system in an automated and online mode [125]. As a part of an effective workﬂow scheduler, a fault tolerance mechanism should be provided and guaranteed to recognize the possible failures in the system and output results as early as they occur and take actions accordingly for the recovery. Beyond the traditional techniques to handle failures such as create checkpoint [126], migrate computation closer to data resources [127], and automatically initiate another workﬂow, the current complexity and sheer size of data necessitate the existence of intelligent learners to predict and recover from a failure. ML can help with tracking the changes in resources, availability of data, predicting the pattern of tasks, vs failures, and many other factors to ensure a dynamic fault tolerance mechanism. Identifying and tracking system metrics can leverage ML techniques to detect the cause of failures, and use this information to predict future failures in real time [125]. In a recent effort by [128] an auto-regression approach based on statistical methods for online monitoring time-series data of scientiﬁc workﬂows is developed. In this work, they presented a framework for detecting performance anomalies in scientiﬁc workﬂows to improve the reliability of execution, in which anomaly triggers are generated after a certain threshold is exceeded by the error from the predicted auto-regression model. Anomaly detection in streaming data was proposed by [129] in which Hierarchical Temporal Memory (HTM) networks are used as continuous learning systems that learn incrementally in an unsupervised manner. In this approach, HTM model is used for each metric in the system to detect CPU or I/O anomalies in real-time execution. An online parallel ML algorithm for anomaly detection in computational ﬂuid dynamics (CFD) and turbulence analysis introduced by [130]. This study integrates simulation applications with various analysis applications with the use of DataSpaces [29] as a ﬂexible interaction and coordination in distributed and virtual shared space. Task failure prediction is considered by [131] in order to facilitate proactive fault tolerance for scientiﬁc workﬂow applications. Their goal was to design a ML based prediction model that can handle resource and task failures of scientiﬁc workﬂow execution. The proposed model is distributed in two modules where the ﬁrst module utilizes ML method for prediction of task failure using evaluated performance metrics, and the second module locates the actual failure after execution of the scientiﬁc workﬂow. The prediction models have implemented through Artiﬁcial Neural Network (ANN), Naive Bayes, Random Forest, Logistic Regression (LR) and have shown that the maximum accuracy can be obtained by naive Bayes classiﬁcation method. Another study [132] on prediction task failure in workﬂow execution, proposed an intelligent prediction using ML. In this work, the proposed prediction is combined with an ensemble prediction method to improve the accuracy of failure prediction. Again, Naive Byes are considered as the default classiﬁer along with other classiﬁers including decision tree, logistic regression, multi-layer perceptron, random forest, and bagging techniques. Intelligent predictive mechanism that forecasts the ﬁnal status of a dynamic job as a success or failure in a particle physics experiment was proposed in [133]. The developed framework uses a Deep Learning method known as Long Short Term Memory Neural Network, which performs prediction in real time in the context of scientiﬁc workﬂows and aims to improve the capability of designing fault tolerance systems. In a different approach aimed at ﬁnding the best fault tolerance methods to use in scientiﬁc workﬂow management systems, the authors in [134] proposed a machine learning-based recommender of fault tolerance techniques. Several classiﬁcation algorithms including decision trees, random forest, SVM, CN2, and naive bayes have examined to produce rules that deﬁne a suitable decision module, which result in proactively avoiding failures or detecting them at an early stage. Provenance Reproducibility of the result, tracking changes of intermediate data, and enabling query information help the users to understand and share knowledge with collaborators and researchers. However, the large amount of data produced by even a simple scientiﬁc workﬂow execution can lead to poor functionality for the users. Providing provenance information that potentially helps the users and encourages them to engage to re-process and reﬁning the execution is a necessity. As a new approach to investigate, ML can produce explainable and reproducible results while reducing the size of information by ﬁltering the informative data and intermediate results vs noninformative ones. ML can highlight useful data and hidden patterns in an effective approach while improving the users’ experience. At the same time, scientiﬁc users typically modify the models and parameters in a trial-and-error to obtain and explore different scenarios and results. ML as a candidate can extract similar features out of different executions and save time in this lengthy process. This approach has been examined in [86] by analyzing previous executions to provide and prioritize jobs for on-going experiments. Using ML for tuning models and parameters of scientiﬁc workﬂows, it can reduce a signiﬁcant amount of time and human expertise as it is capable of extracting knowledge from large-scale data as well as their underlying patterns. Among the different aspects of improving the performance of scientiﬁc workﬂow, provenance is increasingly becoming a vital factor in various applications. Provenance is attracting interest due to the importance of reproducing and re-using experimental results of scientiﬁc workﬂows. Taking this into account, the authors in [135] proposed an approach for data quality scoring based on ML technique to construct a quality function using provenance metadata. Accordingly, the presented model and architecture for data quality scoring employs ML as a classiﬁer to effectively determine the data quality score without a complete metadata record. The classiﬁer function in this study is a set of algorithms such as decision tree, MP3, and KStar, which is constructed using the WEKA engine [136]. Data generated by simulations are traditionally stored in permanent storage for subsequent post-hoc visualization or analysis. However, with HPC enabling exascale computing, simulations now can generate large volumes of data that cannot be stored or exported to the storage system. The gap between computation capability vs I/O performance and capacity is also increasing [137], [138]. Therefore, the volume of produced data from simulations exceeds the limitations of memory and I/O and at the same time complicates ofﬂine analysis and visualization. The large scale of data combined with the continuously increasing complexity of data analysis and visualization, has changed the traditional procedures in favor of processing the analytic/visualization operating in situ [139], [140]. Concurrent approaches based on in-situ have been used to help the data movement from simulation to data analysis pipeline [120], [29]. In-situ operations mitigate the effect of storing and exploring large volumes of data by enabling analytics that run simultaneously as the data producer on the same nodes. This can help especially with making a real-time decision. In the absence of these capabilities, ofﬂine analysis becomes infeasible at this scale. Insitu analysis eliminates the need to transfer entire intermediate datasets to permanent storage, thus it saves storage and I/O. Running analysis/visualization simultaneously with simulation is beneﬁcial when the results of the analysis can give early insights and discoveries about the data and simulations, and is useful for parameter tuning and modifying the models of simulation in ﬂight. The challenges of storing generated data and I/O difﬁculties for transferring this data from simulation to storage or visualization while operating in-situ have been addressed by presenting different reduction techniques (e.g. compression, multi-resolution output) [141], [142]. However, choosing the right reduction techniques depends on various factors including processing time, loss ratio, feature detection, and ﬁnding the most important data to save. To assist with this process, ML can be utilized for classifying different techniques based on their characteristics and connections with the domain feature to provide a better decision. Using ML, one can choose which techniques are more suitable for a speciﬁc use case with multiple constraints. For example, if interesting events happen in the simulation based on spatial and temporal locality, ML is able to detect and track them based on given importance feature metrics, then it can save the data accordingly [143], [144]. Furthermore, feature extraction of data has been used for selecting key time steps (based on similarities between time steps) with the goal of storing less data, and achieved by applying deep learning methods in Computational Fluid Dynamics (CFD) [?]. Also, with the popularity of applying machine/deep learning, new in-situ compression techniques can be explored [145]. Collectively, ML techniques might be considered for deciding which reduction techniques is suitable based on various domains, use-cases, objectives, and available I/O and memory (e.g. in CFD, deep learning can be a used for selecting time steps [?]). ML can be part of decisionmaking for deciding what data might be most informative (in a domain-speciﬁc context) for analysis/visualization and then try to compress and preserve the most informative data ﬁelds [146]. The sporadic nature of the analytics has been observed as an opportunity to utilize the idle resources of simulations [147]. In particular, by tracking the simulation patterns and identifying the connections between those patterns and the analytics, a prediction model, for estimating the idle time of simulations’/analytics resources, we can optimize the resource utilization. Furthermore, the resource usage of analysis/visualization tasks may vary during the workﬂow lifetime in some domains [148], which requires an optimal scheduler to adjust its decision according to the instantaneous resource usage. An efﬁcient ML algorithm could substantially improve the decision-making process of a scheduling policy to further approach the maximum resource utilization in the system. Most of the existing techniques do not support dynamic attach/detach of analytics code to/from the simulation, neither they change the node assignment. A few ML-based solutions have emerged, which consider resource usage optimization for in-situ workﬂows by monitoring data usage, data locality, and memory hierarchies [111], [122]. To ensure optimized resource usage, ML can help predicting the type of data transformation for future data analytics or visualization. Therefore, ML should understand the design space of in-situ workﬂows and decide accordingly. For example, the performance model of algorithms for analysis and visualization can be provided by ML through their characteristics [149]. This is useful to estimate the runtime for each task and decide to process them in-situ or postpone for post-hoc. We categorize the scientiﬁc workﬂow execution into three major stages (see Fig.1); steps prior to execution, execution, and result processing. In the ﬁrst stage, ML can be primarily employed to recognize/predict prominent features of the data, as well as eliminating the unnecessary precision of the data, or classifying the data for subsequent tasks. ML techniques should be ﬂexible enough to work in heterogeneous environments, to deal with distributed data resources and to distinguish various domain-speciﬁc requirements. In addition, the effects of parameter conﬁguration on the results can be obtained by adopting ML model in the online monitoring system. This requires a mechanism that collects this information, extracts the possible patterns, and inputs the user. ML techniques can also infer useful information from the historical conﬁgurations and use them to optimally assist the process of composing a new workﬂow in an automated/ semiautomated manner. In the second stage, there has been a large effort for leveraging ML in the execution of a scientiﬁc workﬂow by recognizing the possible patterns of execution, I/O behavior, and resource consumption that can be later used for managing different components of the system/workﬂow. The best applicable ML technique is an open problem for future studies that can be applied to online/ofﬂine learning over single/multiple executions or workﬂows and the scalability problem. Although ML has been used to tackle the multiobjective scheduling problem, it is of critical importance to design techniques with measurable performance, to further facilitate the comparison between the available techniques. For better scheduling, the resource consumption can be predicted, for which ML can be utilized with the information obtained from the monitoring system and/or the characteristics of tasks. Furthermore, the simulation and analysis/visualization expose several challenges that can also be dealt with using ML techniques. For instance, a simulation may run several times until it generates a desired result. Difﬁculties arise, however, when experiments have many parameters to tune. To optimize the process of ﬁnding the desired combination of values for these parameters, ML techniques can be used. The existing work reveals many different approaches, e.g., choosing a subset of values, interactive modiﬁcation of parameters in real time, and extracting the key elements of changing patterns. Moreover, data analysis with heterogeneous and large datasets generated by simulation can be cumbersome. Developing efﬁcient methods for handling and preparing large datasets may leverage ML techniques. For example, ML techniques can classify the data, reduce the dimensionality of the data, or compress data to reduce the transfer time between simulation and analysis. For reducing the time of data movement, ML techniques have been considered, where memory hierarchies and their characteristics deﬁne an optimized decision for data placement and migration across the system. Besides, detecting anomalies and failures in the second stage, discussed in many studies and reﬂected in various approaches, for instance, by tracking the changes that lead to the failure. In Particular, the pattern of failures in the previous runs can be detected upon the occurrence in the execution using the available ML techniques. Furthermore, task or resource failures can be detected as well. And ﬁnally, in the third stage in Fig.1, reproducibility of the results and sharing with collaborators have to be considered. However, the large volume of results makes the reproduction and sharing time-costly. To improve that, the key elements of the input/output should be extracted, which can be achieved by ﬁltering out the useful information, or by discovering the correlation of input parameters and output features. In other words, uncovering the hidden relation of input, output, and execution can be investigated by ML techniques through multiple executions. In this survey, we provide a review of machine learning applications in different stages of scientiﬁc workﬂows. We investigate the techniques that have been used in each part of the scientiﬁc workﬂow execution. Furthermore, we complement the discussion with key insights into the ML techniques that can be leveraged for in-situ operations, limitations, and suggestions for improving the performance of in-situ execution. Running scientiﬁc workﬂows, especially insitu workﬂows, require the maintenance of ex-scale systems along with conﬁguring a complex graph of dependent tasks running simultaneously on heterogeneous resources. Avoiding failures through runtime, scheduling the tasks to obtain the maximum performance, I/O and memory conﬁgurations, and predicting upcoming events for better scheduling are among the most critical aspects of scientiﬁc workﬂow execution. Many works in the literature utilize ML for performance optimization, accurate estimation of resource usage, and failure detection. In addition, ML shows the potential for extracting the pattern of workﬂows, data access, and hidden models in the execution. It can further be applied to ﬁnd automatically the optimal conﬁgurations with different sorts of objectives. ML can easily come to help for automatic fault detection and making decision accordingly, classifying workﬂow execution patterns and choose the optimal conﬁguration based on the underlying system or predicting data access pattern and choose the right decisions to mitigate the effect of data movement. Reliability and scalability of ML methods in all the above mentioned can be more investigated in future. Furthermore, insitu execution can greatly beneﬁt from ML, as data movements are costly. Thus, ML can detect the data access pattern and provide optimal data/task placements. Moreover, by tracking the generated data from simulation, ML is able to predict in advance what analysis or visualization might be interested and prepare the resources and data appropriately.