Click-Through Rate (CTR) prediction, which aims to predict the probability of the user clicking on the recommended items (e.g., music, advertisement), plays a core role in recommender systems. The estimated CTR may inﬂuence the subsequent recommendation decision-makings such as item ranking and ad placement. Accurate CTR prediction not only improves the user experience but also boosts the proﬁt for the service providers. Since 2016, deep learning has been introduced to CTR prediction due to its high capacity of modeling high-order patterns and end-to-end learning manner [2]. So far, various deep CTR models have been deployed in the recommendation services of industrial companies, such as Wide & Deep [3] in Google Play, DeepFM [4] in Huawei AppGallery and DIN [5] in Taobao. Most of the existing deep CTR prediction models consist of two key components: feature embedding learning and feature interaction modeling. Feature embeddings are learned via mapping categorical features into lowdimensional embedding vectors (short for embeddings). Feature interactions are learned by utilizing some functions to model the relationship among a set of feature embeddings. Many research works focus on designing interaction functions (or more generally, network architectures) to better capture feature interactions. At the early stage, Deep Neural Network models [3, 6, 7] are proposed to model feature interactions implicitly with the multi-layer perceptron (MLP) built on the feature embeddings. In theory, a DNN could explore arbitrary feature interactions according to its universal approximation property [8]. However, there is no guarantee that a DNN naturally converges to any expected function using gradientbased optimization. Recent works prove the insensitive gradient issue of DNN when the target is a large collection of uncorrelated functions [9, 10]. Simple DNN models may not ﬁnd the proper feature interactions, including the simple inner product [11]. Therefore, various complicated architectures have been proposed, such as DIN [5], DeepFM [4] and PNN [12]. Factorization Models (speciﬁed in Deﬁnition 1), such as FM [13], DeepFM, PNN, AFM [14], NFM [15], have been proposed to adopt an embedding layer and an inner-product feature extractor to respectively learn feature embedding and feature interactions. However, there are three signiﬁcant downsides in most of these existing models. First, all these models simply enumerate all feature interactions or require human efforts to identify important feature interactions. The former always brings large memory and computation cost to the model and is difﬁcult to extend into high-order interactions. Besides, useless interactions may bring unnecessary noise to cause overﬁtting and complicate the training process [10]. The latter, such as identifying important interactions manually in Wide & Deep [3], is of high labor cost and risks missing some counter-intuitive (but important) interactions. Second, all the feature interactions with different complexity are modeled with the same speciﬁc interaction function (IF), such as inner product [4, 12, 14] or a predeﬁned sub-network [9]. Nevertheless, using different interaction functions is conducive to better modeling various feature interactions under different mapping sub-spaces, especially for higher-order feature interactions as pointed out in AutoFeature [16]. Third, different features share the same embedding size to represent their embeddings. As pointed out in some remarkable works [17, 18, 19, 20], such a strategy may lead to memory inefﬁciency. More speciﬁcally, allocating the same embedding size to all features may lose the information of high predictive features while waste memory on nonpredictive features. Therefore, model size can be further optimized without sacriﬁcing performance by differentiating embedding sizes for individual features, i.e., assigning large embedding size to high predictive features, while assigning small embedding size to low predictive ones. Some recent works improve the existing models by solving part of the above three problems. AutoFIS [1] and AutoGroup [21] identify important feature interactions while performing identical IF for all the selected feature interactions with uniform embedding size (namely, solving the ﬁrst problem only). AutoFeature [16] solves the ﬁrst and second problems by automatically searching proper network architectures to model different feature interactions via an evolutionary algorithm with the Naive Bayes tree. However, high complexity hinders the application of this method to large-scale industrial scenarios. To solve the third problem, a bunch of previous works [17, 18, 19, 20] utilize various techniques to ﬁnd proper embedding sizes for each feature automatically, but the ﬁrst two issues about feature interactions are totally ignored in these works. In other words, none of these advanced work solves all these three important problems in a uniﬁed framework. To ﬁll this gap, in this paper, we propose a uniﬁed framework called Automatic Interaction Machine (AIM) with three core components, i.e., Feature Interaction Search (FIS), Interaction Function Search (IFS) and Embedding Dimension Search (EDS), which are elaborated in Figure 1. First, FIS component automatically learns which feature interactions are essential. Speciﬁcally, we introduce a gate (in open or closed status) for each feature interaction to control whether its output should be passed to the next layer. In previous works, the status of the gates are either speciﬁed beforehand by expert knowledge [3] or set as all open [4, 22]. From a data-driven point of view, whether to open or close a gate should depend on the contribution to the ﬁnal prediction. Apparently, those contributing little should be closed to prevent the learning procedure from introducing extra noise. However, it is an NP-Hard problem to ﬁnd the optimal set of open gates for model performance, as we face an incredibly huge space to search (2with n ﬁelds, even if we only consider 2-order feature interactions). To make the search efﬁcient in such a huge space, FIS component relaxes the choices of gates to be continuous such that gradient-based optimizations can be performed. Furthermore, FIS component is also able to select high-order feature interactions. To make the search process efﬁcient, we exploit the selected low-order interaction to ﬁrst restrict high-order interactions into a small candidate pool heuristically and then search from this pool rigorously, achieving O(n) complexity. As the second component of AIM, IFS can select appropriate IF for each essential feature interaction. To achieve this goal, a gate to each interaction-IF pair is needed. It can be observed that the search space of IFS is larger than that of FIS, which reaches to 2(with n ﬁelds and m IFs), if we only consider 2-order feature interactions. To avoid learning conﬂict among different IFs, IFS proposes to utilize function-wise embeddings, i.e., multiple embeddings learned for each feature, where each of such embeddings corresponds to one possible IF. As function-wise embeddings are used in IFS, the model size grows signiﬁcantly (about m times than shared embedding). The third component, EDS, is proposed to optimize the model size without sacriﬁcing performance by assigning small embedding sizes to low predictive features, which is achieved by pruning redundant embedding dimensions for each feature. Similar to FIS and IFS components, gates are also introduced, where a gate is annotated to each dimension of a feature embedding. In the end, the dimensions with open gates are recognized as important dimensions while the ones with closed gates are pruned. Inspired by the recent work DARTS [23, 24, 25] for neural architecture search, instead of searching over a discrete set of candidate gates (i.e., feature interactions in FIS, interactionIF pairs in IFS and embedding dimensions in EDS), we relax the choices to be continuous by introducing a set of architecture parameters (one for each gate) so that the relative importance can be learned by gradient descent. The architecture parameters are jointly optimized with neural network weights by GRDA optimizer [26], an optimizer that is easy to produce a sparse solution, so that the training process can automatically abandon unimportant feature interactions, useless interaction-IF pairs and unnecessary embedding dimensions with zero values as the architecture parameters and keep those important ones. Finally, we re-train the model with the selected feature interactions, interaction-IF pairs as well as embedding dimensions. Extensive experiments are conducted on three largescale datasets and the experimental results demonstrate that AIM can signiﬁcantly improve the CTR prediction performance in terms of AUC and log loss. Besides, as AIM can remove about 80% of 2-order interactions, our model can consistently achieve improvement on inference efﬁciency. AIM is able to model high-order feature interactions in a novel way with quadratic complexity. Experimental results show that with about 1%–5% of 3-order feature interactions and about 0.03%–0.6% of 4-order feature interactions selected, the AUC of factorization models can be improved by 0.1%–0.4% without introducing much inference cost. Furthermore, AIM has been deployed in the recommendation service of a mainstream app market. From a three-week online A/B test, AIM achieves 4.4% CTR improvement over the production model DeepFM, which contributes a signiﬁcant business revenue growth. To summarize, the main contributions of this paper can be highlighted as follows: FIS, IFS and EDS components) to select signiﬁcant feature interactions, appropriate IFs and necessary embedding dimensions automatically in a uniﬁed framework. with open (closed) status representing important (unimportant) candidates. We relax discrete selection of open gates to be continuous by introducing a set of architecture parameters that can be jointly optimized with neural network weights by GRDA optimizer. strate the superior performance of AIM. A three-week online A/B test in the recommendation service of a mainstream app market shows that AIM improves DeepFM model by 4.4% on average in terms of CTR. 2.1 CTR Models One core of CTR models is to extract effective low-order and high-order feature interactions, which is also one of the optimization targets of our work. Therefore, in this section, we discuss the CTR models that focus on learning feature interactions effectively. FM [13] projects each feature into a low-dimensional vector and models pair-wise feature interactions by inner product, which works well for sparse data. FFM [27] extends FM, by further assigning each feature with multiple vectors to interact with features from other ﬁelds. Despite the signiﬁcant improvement over FM, FFM introduces much more parameters and suffers from overﬁtting issues. HOFM [28] introduces the ANOVA kernel to approximate high-order feature interactions. However, it is shown in [14] that HOFM achieves only marginal improvement over FM whereas using much more parameters. Recently, deep learning models have achieved state-ofthe-art performance on some public CTR prediction benchmarks [29, 30]. As a powerful approach to learning feature representation, deep learning models have the potential to learn sophisticated feature interactions. Wide & Deep [3] jointly trains a wide model for artiﬁcial features and a deep model for raw features. Several models use MLP to improve FM, such as AFM [14], NFM [15] and DeepFM [4]. DeepFM uses an FM layer to replace the wide component in Wide & Deep. PNN [12] uses MLP to model high-order implicit information over the feature embeddings and interaction of FM layer while PIN [9] introduces a network-in-network architecture to model pairwise feature interactions with subnetworks rather than simple inner product operations in PNN and DeepFM. Due to the curse of dimensionality, DeepFM, PNN and PIN cannot explicitly model high-order feature interactions, which limits the further improvement of the model performance. xDeepFM [22] uses a CIN structure to enumerate and compress all feature interactions for modeling explicit interactions. However, it uses so many parameters that great challenges are posed to identify important feature interactions in the huge combination space. As stated earlier, all the above mentioned CTR models suffer from three limitations: (1) enumerating all the feature interactions or requiring human efforts; (2) utilizing the same IF to model all the feature interactions; (3) assigning the same embedding size to all the features. Our proposed AIM improves these models by solving such three downsides with AutoML techniques. 2.2 AutoML for CTR Models AutoML for CTR models has been an active research area and there exist some works using AutoML techniques to devise automated methods for architecture design including embedding dimension search and feature interaction search. Existing works leverage the AutoML to optimize the embeddings by searching embedding sizes for different features adaptively and automatically. NIS [20] and ESAPN [19] perform a hard selection strategy and use reinforcement learning (RL) to search for mixed feature embedding sizes automatically. On the contrary, soft selection strategies based on differentiable search algorithms (e.g., DARTS [23]) are proposed in AutoEmb [18] and AutoDim [17] by summing over the embeddings of the candidate sizes with learnable weights. The former leverages a controller network with Softmax layer to search the weights of different embedding sizes while the latter uses the Gumbel-softmax operation [31]. As stated earlier, for CTR models, feature interaction modeling is an important component, where the main tasks are to ﬁnd which feature interactions are useful and decide how the interaction should be modeled. Some research works are proposed to explore these two questions. AutoFIS [1] automatically identiﬁes and then selects important feature interactions for factorization models with a set of learnable architecture parameters. Besides, AutoGroup [21] proposes to generate some groups of features, such that their interactions of a given order are effective, which enables high-order feature interaction modeling. To determine suitable IFs, SIF [32] exploits the DARTS method to search proper interaction functions for matrix factorization. AutoFeature [16] applies an evolutionary algorithm with the Naive Bayes tree that recursively reduces the search space of IFs for feature interactions. However, AutoFeature trains excessive models for different structures to select the most appropriate structure, which requires a long period to search and a much high computational resource. However, the above mentioned works solve one or two limitations (the three limitations are mentioned in Section 1 and also in Section 2.1) of the existing CTR models. None of them is able to solve all the three important issues in a uniﬁed framework. Our proposed AIM designs a gate-based uniﬁed framework and formulates the problem of searching proper embedding size, effective feature interactions as well as appropriate IFs as a continuous searching problem by incorporating architecture parameters that can be jointly optimized with neural network weights. In this section, we ﬁrst formally deﬁne a family of popular CTR models (Factorization Model), with which our proposed AIM is able to work collaboratively to improve their performance. Then Automatic Interaction Machine (AIM) with three core components, namely, Feature Interaction Search (FIS), Interaction Function Search (IFS) and Embedding Dimension Search (EDS) is proposed to select signiﬁcant feature interactions, appropriate IFs and necessary embedding dimensions automatically in a uniﬁed framework, as shown in Figure 1. Finally we discuss some concrete training details. 3.1 Factorization Model (Base Model) First, we deﬁne factorization models: Deﬁnition 1. Factorization models are the models where the interaction of several embeddings from different features is modeled into a real number by some functions such as inner product or neural network. FM [13], DeepFM [4] and IPNN [12] (IPNN is one kind of PNN when the IF is inner product) are popular CTR models, which are in the family of factorization model. Therefore, we take FM, DeepFM, and IPNN as instances to formulate our algorithm and explore the performance on various datasets. Figure 2 presents the architectures of FM, DeepFM and IPNN models. FM consists of a feature embedding layer and a feature interaction layer. Besides these two layers, DeepFM and IPNN include an extra layer: MLP layer. The difference between DeepFM and IPNN is that feature interaction layer and MLP layer work in parallel in DeepFM while ordered in sequence in IPNN. In the subsequent subsections, we will ﬁrst elaborate on these layers. Then the details of how our proposed AIM working on the feature embedding layer and feature interaction layer are elaborated, i.e., selecting crucial feature interaction with proper interaction functions (IFs) and appropriate embedding sizes based on architecture parameters. Feature Embedding Layer. In most CTR prediction tasks, data is collected in a multi-ﬁeld categorical form. A typical data pre-process is to transform each data instance into a high-dimensional sparse vector via one-hot or multihot encoding. A ﬁeld is represented as a multi-hot encoding vector only when it is multivariate. The data instance can be represented as where n is the number of ﬁelds and xis the one-hot or multi-hot encoding vector of the iﬁeld. Then, a feature embedding layer is used to transform the encoding vector into a low-dimensional vector via where V∈ Ris the embedding matrix for the iﬁeld, his vocabulary size of the iﬁeld and d is embedding size. Then, the output of the feature embedding layer is represented as the concatenation of multiple embedding vectors: In the traditional paradigm, all feature embeddings in various ﬁelds have identical embedding sizes. However, not all features in distinct ﬁelds are equally predictive, i.e., some ﬁelds are informative while the other ﬁelds are not. Therefore, allocating the same embedding size to all features may lose the information of high predictive features while wasting memory on non-predictive features. To tackle this issue, we will later present EDS component to search different embedding sizes (by pruning useless embedding dimensions) for various features in the different ﬁelds. Feature Interaction Layer. After transforming the features to a low-dimensional space, the feature interactions can be modeled in such a space with the feature interaction layer. Various interaction functions (IFs) can be used to model distinctive interactive signals, such as inner product, outer product and kernel product: where h·, ·i is the inner product, ⊗ is the outer product and h·, ·iis the kernel product (with the kernel φ). Specially, we further divide the kernel product into matrix kernel product, vector kernel product, scalar kernel product according to the shape of kernel φ. Take the inner product as an example, we introduce how the feature interaction layer works. The inner product of all the pair-wise feature interactions is calculated: In FM, the output of the feature interaction layer is: However, the inner product operation may not be able to learn the interaction information of all the pairwise features. That is to say, various interaction functions (IFs) are needed when modeling different feature interactions, as also stated in [32]. Taking this concern into consideration, we extend FM model with multiple IFs as l= hw, xi +f(e, e where fis a predeﬁned IF (such as inner product, outer product, kernel product) and m is the number of IFs. In Equation 5, all the feature interactions are assumed to be equally important and therefore are designed to contribute equally to the prediction. Whereas not all the feature interactions are equally predictive and useless interactions may even degrade the performance. Therefore, we propose the FIS component to select important feature interactions automatically and efﬁciently. In addition, Equation 5 assumes k different IFs are all needed and contribute equally to model each feature interaction. As stated earlier, different IFs are needed when modeling individual feature interactions. To select suitable IFs for each feature interaction, we propose the IFS component, which will also be elaborated later. Besides pairwise feature interactions, we also target at identifying effective high-order feature interactions. Formally, we deﬁne p-order feature interaction (i.e., the combination of p ﬁelds) as: where ψcontains all p-order interactions (|ψ| = C) and q is one such p-order interaction. f(·) is the khighorder IF (high-order IFs are expanded by 2-order IFs and the speciﬁc expansions are described in Section 4.2.3). The complexity of feature interaction layer with porder interaction is O(n), exponential to the number of ﬁelds n, which makes searching high-order feature interactions in a brute-force manner unaffordable in practice. To tackle the efﬁciency issue, we propose a method to select high-order feature interactions with quadratic complexity, which will be illustrated in Section 3.2. MLP Layer. MLP layer consists of several fully connected layers with activation functions, which learn the implicit non-linear relationship among features. The output of one such layer is where a, W, bare the input, weight and bias of the llayer, respectively. The activation function is relu(z) = max(0, z). ais the input to MLP layers and a= MLP(a), where L is the depth of MLP layer. Output Layer. FM model has no MLP layer and connects the feature interaction layer with prediction layer directly: where ˆyis the predicted CTR. DeepFM combines feature interaction layer and MLP layers in parallel as While in IPNN, MLP layer is subsequent to feature interaction layer as Note that the MLP layer of IPNN can also serve as a re-weighting of the different feature interactions to capture their relative importance. This is also the reason that IPNN has a higher capacity than FM and DeepFM. However, with the IPNN formulation, one cannot retrieve the exact value corresponding to the relative contribution of each feature interaction. Therefore, the useless feature interactions in IPNN can be neither identiﬁed nor dropped, which brings extra noise and computation cost to the model. Objective Function. FM, DeepFM, and IPNN share the same objective function, i.e., to minimize the cross-entropy of predicted values ˆy and the labels y as where y ∈ {0, 1} is the label and ˆy ∈ [0, 1] is the predicted probability of y = 1. 3.2 Feature Interaction Search (FIS) In this section, we present one of the three core components, namely FIS component, which is performed on the feature interaction layer of any factorization model to automatically identify different orders of essential feature interactions. The overview of the FIS component is illustrated in Figure 3. To ease the presentation of FIS component, we introduce the gate operation to control whether to select a feature interaction, i.e., an open gate corresponds to selecting this feature interaction, while a closed gate results in dropping this interaction. The total number of gates corresponding to all the 2-order feature interactions is C. It is very challenging to ﬁnd the optimal set of open gates in a brute-force way, as we face an incredibly huge (2) space to search. In this work, we approach the problem from a different viewpoint. Instead of searching over a discrete set of open gates, we relax the choices to be continuous by introducing architecture parameters α, so that the relative importance of each feature interaction can be learned by gradient descent. This architecture selection scheme based on gradient learning is inspired by DARTS [23], where the objective is to select one operation from a set of candidate operations in convolutional neural network (CNN) architecture. To be speciﬁc, we reformulate the interaction layer in factorization models (shown in Equation 4) as where α = {α, · · · , α} are the architecture parameters and their value αcan represent the relative contribution of each feature interaction to the ﬁnal prediction. Then, we can decide the gate status of each feature interaction by setting those unimportant ones (i.e., with zero αvalues) closed. After the search, some unimportant interactions are thrown away automatically according to the architecture parameters α and the new model with remained feature interactions can be re-trained. High-order Feature Interaction Search. Note that, Equation 12 only formulates the case of selecting important 2order feature interactions. Besides the 2-order, FIS component also aims to search high-order feature interactions. The number of all p-order feature interaction is C, which is exponential to the number of ﬁelds n. Therefore, it is not practical to identify effective high-order feature interactions in a similar way as for 2-order feature interactions (by enumerating all the feature interactions ﬁrst). To search effective high-order feature interactions efﬁcient, we propose a two-step process to select p-order feature interactions from the selected (p − 1)-order interactions, as follows. interaction set to generate a preliminary candidate pool Mfor potential effective p-order interactions. eters to select effective p-order feature interactions M, from Mgenerated by Step 1.This two-step search process is summarized in Algorithm 1. We elucidate the core part of this algorithm, i.e., line 4 Algorithm 1 High-order Feature Interaction Search Require: ﬁeld number n, the highest order P , top number k for pool selection Ensure: the selected interactions M, M, M, · · · , M ← {(e), (e), · · · , (e)} to line 7. For line 4, we combine top important (p − 1)order feature interaction set Mand ﬁrst-order feature interaction set M(that is each feature) in a Cartesian product manner to get the candidate pool Mfor porder. For line 5, we leverage the FIS component to learn the relative importance of each p-order interaction by the architecture parameters. More speciﬁcally, we abandon the feature interactions with zero α value in Mand retain the rest to get the ﬁnal selected p-order interaction set M with their corresponding α. For line 6-7, preparing for the next round, we employ αto sort Mand get the top-k (k ≤ n) important p-order feature interaction to generate (p + 1)-order feature interaction set. Complexity Analysis. To select effective p-order feature interactions, there exists at most n feature interactions in M, therefore M(which combining Mand M) has at most O(n) p-order feature interactions. Both time and space complexity of feature interaction selection for each order by our method is O(n), instead of O(n) complexity by simple enumeration. 3.3 Interaction Function Search (IFS) FIS component selects important feature interactions which are modeled with the same speciﬁc interaction function (IF). However, as stated earlier, not all the feature interactions can be modeled with the same IF. That is to say, individual feature interactions may need different IFs. Due to this reason, the second core component, IFS, is proposed to select appropriate IFs for each important feature interaction. Similar to FIS, IFS component also leverages the gate operation to control the IF selection. For each feature interaction, one or more IFs from m candidate IFs are selected to learn the relationship of the features. As a special case, a feature interaction is abandoned if none of m IFs is selected. The architecture parameters αare introduced to get the relative importance of each interaction-IF pair, which can be learned by gradient descent (in a similar way as in FIS). The overview of IFS component is presented in Figure 4. We reformulate the interaction layer in the extend factorization models (shown in Equation 5) as l= hw, xi +αf(e, e where α= {α} are the architecture parameters. αcorresponding to the kIF for feature interaction (i, j). Those unimportant interaction-IF pairs (i.e., with zero αvalues) will be thrown away automatically. Function-wise Embeddings. In Equation 13, feature interaction modeling leverages shared embedding (i.e., each feature shares the same embedding for different IFs) to learn the latent effect over different IFs. However, the learning of shared feature embedding becomes difﬁcult as one such embedding receives different gradient signals from individual learning spaces, where each learning space corresponds to an IF. Such different gradient signals cause learning conﬂict and lead to sub-optimal performance, as also observed in [33, 34]. To avoid such learning conﬂict among different IFs, IFS proposes to utilize the function-wise embeddings (FWEs), i.e., multiple embeddings are learned for each feature, where each of such embedding corresponds to one IF. With a speciﬁc IF, the corresponding embedding vectors are used to model the interactive correlations. Equipped with FWE, Equation 13 is updated as: l= hw, xi +αf(e, e where erepresents the embedding of the ifeature for the kIF. In this way, each embedding only needs to learn the latent effect in one mapping space of a speciﬁc IF, such that learning conﬂict is avoided. In Section 4.4, we will show the superiority of function-wise embeddings over shared embedding. However, function-wise embeddings introduce more parameters, which leads to much more space cost. To reduce the space complexity, the third core component, EDS is proposed, which will be presented in Section 3.4. After the architecture parameters αare learned, appropriate IFs are chosen for each feature interaction automatically. More speciﬁcally, zero value of αindicates the interaction (i, j) with kIF is not effective and thus can be dropped. The other interaction-IF pairs with non-zero αvalues are retained. 3.4 Embedding Dimension Search (EDS) Although function-wise embeddings (FWEs) improve the performance, the model size is signiﬁcantly larger than the case of allocating the same embedding size (as shared embedding) to all features. To deal with the excessive parameters, the third component, EDS, is proposed to optimize the model size without sacriﬁcing performance by assigning low embedding size to non-predictive features, which is achieved by pruning redundant embedding dimensions for each feature. Similarly, a gate operation is introduced to determine whether the corresponding embedding dimension is selected for a ﬁeld. The total number of the gates is n × d, where n is the number of the ﬁelds and d is the largest embedding size. Here we make use of architecture parameters β ∈ Rto turn the embedding dimension search problem into a continuous process which can be solved by gradient descent. The overview of EDS component is shown in Figure 5. Note that FWEs in the same ﬁeld share the same set of architecture parameters. During the embedding dimension search, each embedding dimension multiplies to the embedding architecture parameter β of the corresponding ﬁeld, as where βis the embedding architecture parameter of the iﬁeld and it represents the relative importance among different dimensions in the iﬁeld. The feature embedding layer concatenates the embedding vector eof each feature, as Then Efeeds to the feature interaction layer for capturing interactive signals. After the search, the embedding size of each feature ﬁeld has been determined automatically according to β. More speciﬁcally, EDS component prunes redundant dimensions (i.e., with zero βvalue) and reserves necessary dimensions with their positions (since the positions also reﬂect the effect on various feature interactions). The embedding size for each ﬁeld is Besides, the position set of the retained embedding dimensions for the iﬁeld is To derive new embeddings of adaptive sizes, we design a function M ap(·) for the iﬁeld that maps from the position set of retained dimension φto the new embedding position set {1, 2, · · · , d} in order as where dis the new embedding size of the iﬁelds. Then we can re-train the model with length-adaptive embedding sizes. For each feature ein the iﬁeld, instead of initializing it with embedding size d, we initialize it with a shorter embedding size d. As some IFs require embedding sizes of features to be the same (such as inner product), in such cases, we can trivially utilize M apto transform embeddings with size dto ones with size d, by setting the dimensions that are not in the domain of M apwith zero values. Note that such transformation with Mapintroduces no extra parameters therefore does not increase the space complexity. 3.5 Automatic Interaction Machine (AIM) To summarize, FIS component searches for feature interactions; IFS component searches for appropriate IF for each feature interaction; and EDS component searches for proper embedding size for each feature. To fully exploit the advantages of these three components, we build the AIM framework by combining them, to select signiﬁcant feature interactions, appropriate IFs and necessary embedding dimensions automatically in a uniﬁed framework. Speciﬁcally, AIM has the following three stages as shown in Figure 6: use FIS component and IFS component to select useful interactions and appropriate IFs. interaction-IF pairs in the ﬁrst stage, and then use EDS component to select a suitable embedding size for each ﬁeld, by discarding useless embedding dimensions. interaction-IF pairs and learned embedding sizes in the ﬁrst two stages. Note that FIS component and IFS component can be trained together, however, it is inappropriate to combine the training process of them with EDS component due to the coupling problem between architecture parameters. Therefore, we have to divide the search process into Search interaction-IF stage and Search embed stage. Transferability. The chosen Interaction-IF pairs by gate architecture learned from a simple model could be transferred to the state-of-the-art models to boost their performance. We will verify this transferability in Section 4.6. 3.6 Training Details In this subsection, we present some training details in AIM, i.e., batch normalization and GRDA Optimizer. We take FIS component as an illustration example and similar training procedures are performed in IFS and EDS. Batch Normalization. From the viewpoint of the overall neural network, the contribution of a feature interaction is measured by α· he, ei (in Equation 12). Exactly the same contribution can be achieved by re-scaling this term as () · (η · he, ei), where η is a real number. Since the value of he, ei is jointly learned with α, the coupling of their scale would lead to unstable estimation of α, such that αcan no longer represent the relative importance of he, ei. To solve this problem, we apply Batch Normalization (BN) [35] on he, ei to eliminate its scale issue. BN has been adopted by training deep neural networks as a standard approach to achieve fast convergence and better performance. The way that BN normalizes values gives an efﬁcient yet effective way to solve the coupling problem of αand he, ei. The original BN normalizes the activated output with statistics information of a mini-batch. Speciﬁcally, where z,ˆz and zare input, normalized and output values of BN; µand σare the mean and standard deviation values of zover a mini-batch B; θ and δ are trainable scale and shift parameters of BN;  is a constant for numerical stability. To get stable estimation of α, we set the scale and shift parameters to be 1 and 0 respectively. The BN operation on each feature interaction he, ei is calculated as where µand σare the mean and standard deviation of he, ei in mini-batch B. GRDA Optimizer. Generalized regularized dual averaging (GRDA) optimizer [26] is aimed to get a sparse deep neural network. To update α at each gradient step t with data Zwe use the following equation: where g(t, γ) = cγ(tγ), and γ is the learning rate, c and µ are adjustable hyper-parameters to trade-off between accuracy and sparsity. With the GRDA optimizer, the important feature interactions (or functions, embedding dimensions) will be retained, and the unnecessary ones will be abandoned according to the architecture parameters (the unimportant ones will be learned to zero). In this way, GRDA optimizer adaptively determines the remained ones, avoiding artiﬁcially setting hyper-parameters for deciding how many feature interactions (or functions, embedding dimensions) to be remained. In this section, we conduct extensive ofﬂine experiments on two public benchmark datasets and a private dataset, as well as an online A/B test in the recommendation service of a mainstream app market, to evaluate the effectiveness of Automatic Interaction Machine (AIM). In particular, we answer the following research questions: with the selected interaction-IF pairs? IFS, EDS) contribute to the performance? Are the interactions, IFs and embedding dimensions selected by these components really important and valuable? compared with other models? transferred to other models for boosting their performance? model in a live recommender system? 4.1 Datasets Experiments are conducted on the following two public datasets (Avazu and Criteo) and one private dataset (Huawei), whose statistics are summarized in Table 1. We follow the existing works [1, 9, 12, 16, 21] to process Avazu and Criteo datasets. Avazu: Avazu was released in the CTR prediction contest on Kaggle. 80% of randomly shufﬂed data is allotted to training and validation with 20% for testing. Criteo: Criteo contains one month of click logs with billions of data samples. We select “data 6-12” as training and validation set while select “day-13” for evaluation. Huawei: The industrial dataset is sampled and collected from an app recommendation scenario of Huawei AppGallery for a week. The dataset contains 10 feature ﬁelds, including user behavior (user click list, etc.), app information (id, category, etc.), and context information (time, etc.). 4.2 Experimental Settings 4.2.1 Baselines and Evaluation Metrics We compare our propoed AIM with representative factorization models (i.e., FM [13], FwFM [36], AFM [14], FFM [27], DeepFM [4], IPNN [9]), AutoML-based model (AutoFeature [16], AutoGroup [21] and AutoFIS [1]) and some other interaction model (xDeepFM [22], FiGNN [37], AutoInt [38]). Note that AutoFIS use DeepFM as base model in our experiment. We use the common evaluation metrics for CTR prediction: AUC and Log loss. All the experiments are repeated ﬁve times by changing the random seeds. The two-tailed unpaired t-test is performed to detect signiﬁcant differences between our model and the best baseline. 4.2.2 Parameter Settings In AIM model, we set the embedding size d = 40 for Avazu dataset and d = 20 for Criteo dataset. The MLP structure in two datasets are both [700 × 5, 1]. With regard to GRDA parameters c and mu, we set c = 0.05, mu = 0.6 for Avazu dataset and c = 0.005, mu = 0.9 for Criteo dataset. The more detailed parameters for AIM model and the hyperparameters for other models can be shown in our code link. 4.2.3 Implementation Details In the search interaction-IF stage, we ﬁrst train the model with αto seek important interaction-IF pairs. Then in the search embed stage, we train the model with β to search embedding dimensions. Finally, we re-train the model with the selected components. We consider 4 different commonused IFs, containing inner product, outer product, vector kernel product and scalar kernel product. The output of outer product is a d × d vector (d is the embedding size), and we use a linear layer to transform it into a single output. To reduce the complexity of outer product, we utilize (we)(we) = weewto approximate outer product. Here e, eare the embeddings and w, ware learnable parameters. For high-order interaction implementation such as p-order interaction, inner product, vector kernel product and scalar kernel product are easily to be extended by element-wise multiplication with p vectors. To expand the outer product into p-order, we employ (we)(we) · · · (we) to calculate it (q ∈ ψwhere ψcontains all p-order interactions). To implement high-order interaction selection, we manipulate top k (set k = bn/2c) low-order interactions to construct a candidate pool (with at most n/2 interactions) ﬁrst and the high-order interactions are chosen from this pool. This method is applied in both AutoFIS and AIM. In the ﬁrst two stages, the selected architecture parameters are optimized by GRDA optimizer and the other parameters are optimized by Adam optimizer. In the re-train stage, all the parameters are optimized by Adam optimizer. Inspired by the transferability mentioned before, we sequentially introduce an MLP layer to learn more information and promote the performance in the re-train stage of AIM while this MLP layer does not exist in the search stage. 4.3 Overall Performance (RQ1) Table 2 shows the overall performance of AIM and baselines on three datasets. Table 3 summarizes the performance and FI ratio (i.e., percentage of selected feature interaction) with different maximum interaction order in AutoFIS and AIM. From these two tables, we have the following observations. 1) Compared with the Factorization Models and Other Interaction Models, AIM has achieved signiﬁcant improvement. Among these base models, IPNN leverages an MLP to model high-order implicit information over the feature embeddings and interaction, mostly achieving better performance than the others. However, AIM further improves IPNN in terms of AUC by 0.66% in Avazu and 0.21% in Criteo via performing automatic search of IFs and embedding dimensions. 2) In comparison with the existing representative AutoMLbased models, AIM achieves the best performance. AutoFIS and AutoGroup only consider selecting useful feature interactions, ignoring the effectiveness of different IFs. Besides, none of these AutoML-based models take the embedding dimension search into consideration. In contrast, AIM proposes three components (i.e., FIS, IFS and EDS) to select signiﬁcant feature interactions, appropriate IFs and necessary embedding dimensions automatically in a uniﬁed framework. 3) From Table 3, we can observe that about 80% of the 2-order interactions can be removed. As for high-order feature interaction selection, only about 2% of all the 3-order feature interactions and about 0.3% of all the 4-order feature interactions are selected with signiﬁcant performance improvement. With a small amount of highorder interactions integrated, the relative performance improvement of AIM (3-order) over AIM (2-order) is 0.27% and that of AIM (4-order) over AIM (2-order) is 0.37% in terms of AUC. 4.4 Ablation Study (RQ2) In this subsection, we will ﬁrst present the effectiveness of different components (namely, FIS, IFS and EDS) in AIM. Then we conduct experiments on real data to analyze why these three components are valid. 4.4.1 Effectiveness of different components in AIM To validate the effectiveness of individual components in AIM, we propose several variants. Recall that AIM has three core components: FIS, IFS, EDS, and we apply experiments on these variants to verify the effectiveness of these components. The relationship among different variants and their performance is presented in Table 4, from which we can get the following conclusions. 1) Comparing AIM-IFS-EDS with DeepFM, we can observe that removing those useless interactions can not only simpliﬁed the model but also signiﬁcantly boost the prediction accuracy. The relative performance improvement of AIM-IFS-EDS over DeepFM is 0.42% and 0.30% for Avazu and Criteo dataset respectively in terms of AUC, which demonstrates the effectiveness of FIS component. 2) Besides, the performance gap between AIM-EDS and AIM-IFS-EDS indicates that selecting proper IFs for each feature interaction conduces to better performance, which is the functionality of IFS component. 3) Comparing AIM-EDS and AIM, we can ﬁnd that introducing EDS component in AIM for pruning redundant dimensions will not sacriﬁce the performance distinctly. However, EDS reduces model parameters signiﬁcantly, by ×2.79 and ×3.16 in Avazu and Criteo dataset respectively, which will be shown in Section 4.4.4. 4.4.2 The Effectiveness of Selected Feature Interactions by FIS component To see how well α values are learned in FIS component, we analyze the relationship between α values and statistics AUC. We deﬁne statistics AUC to represent the importance of a feature interaction to the ﬁnal prediction. For a given interaction, we construct a predictor only considering this interaction where the prediction of a test instance is the statistical CTR (#downloads/#impressions) of speciﬁed feature interaction in the training set. Then the AUC of this predictor is deﬁned as statistics AUC with respect to this given feature interaction. Higher statistics AUC indicates a more important role of this feature interaction in prediction. As shown in Figure 7, we can ﬁnd that most of the feature interactions selected by FIS component (with high absolute α value) have high statistics AUC, but not all feature interactions with high statistics AUC are selected. That is because the information in these interactions may already exist in other interactions which are selected by our model. To evaluate the effectiveness of the selected interactions by FIS component, we select the top N 2-order feature interactions by FIS component and by statistics AUC separately. We re-train two models with these two sets of interactions and compare their performance. The experimental result shows that, compared with the model by statistics AUC with the same computational cost, the model with selected interactions by FIS component has improved AUC from 0.7804 to 0.7831 (log loss from 0.3794 to 0.3778), which demonstrates the superiority of FIS component. 4.4.3 The Effectiveness of Selected Interaction Functions by IFS component To demonstrate that IFS component is able to ﬁnd suitable IFs for individual interactions, we depict the number of selected IFs with respect to different orders of interactions on Avazu dataset, presented in the left sub-graph of Figure 8. The following observations can be concluded. (1) Each IF occupies a certain proportion, which veriﬁes the need of different IFs for individual interactions. The performance superiority of IFS component validates that suitable IFs are identiﬁed for different interactions. (2) Inner product takes the majority for high-order interactions and ranks at the second place for 2-order interaction. (3) Vector kernel product is the most popular IF for 2-order interaction but is unpopular for high-order interaction. (4) Scalar kernel product and outer product take only a small part of the overall selection, but still hold an indispensable part. Furthermore, we explore the relationship between the number of selected IFs and statistics AUC for each feature interaction, which is presented in the right sub-graph of Figure 8. The ﬁgure indicates that more important feature interactions are likely to retain more IFs. Moreover, as the number of selected IFs increases, the improvement of statistics AUC is gradually disappeared. This is reasonable, because it is more likely to be overﬁtting when more IFs are performed for a feature interaction. In other words, it is not the best to keep all the IFs. Instead, selecting suitable IFs for individual interactions is more appropriate. 4.4.4 The Effectiveness of Selected Embedding Dimensions by EDS component In this section, we analyze why the reserved embedding size of a given feature determined by EDS component is reasonable. Before that, we ﬁrst show the effectiveness of EDS. As mentioned in Section 3.4, function-wise embeddings (FWE) improve the performance compared with shared embedding (SE), therefore we compare the performance of AIM-EDS (SE) and AIM-EDS (FWE). As the model size is signiﬁcantly larger if the embedding size of FWE keeps the same as that of SE, which is the motivation of proposing EDS component. We further compare AIM (FWE) and AIMEDS (FWE) to validate the effectiveness of EDS. The above mentioned experimental comparisons are presented in Table 5, from which We can get the following conclusions. 1) The comparison between AIM-EDS (SE) and AIM-EDS (FWE) veriﬁes that multiple embeddings for different IFs can boost the performance but lead to much more parameters. 2) Comparing AIM (FWE) with AIM-EDS (FWE), equipping EDS component achieves almost the same high performance as AIM-EDS (FWE) but needs only one third of the parameters required by AIM-EDS (FWE). The parameters of AIM-EDS (FWE) are slightly more than that of AIMEDS (SE) in a comparable amount. To see the reserved embedding size of each ﬁeld, we summarize these numbers in Table 6, together with the vocabulary size of each ﬁeld (i.e., the number of features in a ﬁeld) on Avazu dataset. The correlation between vocabulary size and reserved embedding size presented in Table 6 indicates that the ﬁelds with larger vocabulary size tend to achieve larger embedding size. That is because these ﬁelds may be more informative and predictive to the task. However, as can also be observed, vocabulary size is not the only factor to determine the embedding size. Other unknown factors need to be learned by machine learning models, such as EDS component in AIM. 4.5 Space and Time Complexity (RQ3) In this section, we discuss the space and time complexity of AIM with 2-order interactions and high-order interactions, respectively. In order to facilitate calculation, we choose Avazu (with 645,195 features) to analyze the memory usage, training and inference efﬁciency of AIM in comparison with other models. To compare the training and inference efﬁciency, we train these models for the whole training set in one epoch and test these models with 200,000 instances (batch size=2,000) on an NVIDIA 1080Ti GPU. 4.5.1 Complexity Analysis for 2-order interaction First, we analyze the complexity of all the models with 2-order interaction modeling and present the results in Table 7. Note that we only consider the parameters and training time in the re-train stage of AutoFIS and AIM model. Because the useful feature interactions, effective interaction-IF pairs and reserved embedding dimensions can be searched once for all, while the model training over new data only needs to perform the re-train stage based on the searched results in the ﬁrst two search stages. To complete the complexity analysis of AIM, the complexity analysis of the search stages is presented in Section 4.5.2. As for the space complexity, the number of parameters of AutoFIS and its base model (DeepFM) are very similar. AIM-EDS consumes the largest amount of memory as it utilizes multiple embeddings for each feature. By integrating EDS component, AIM reduces the amount of parameters to a comparable level as other models without sacriﬁcing model performance (as already discussed in Section 4.4.4). In terms of training and inference efﬁciency, we ﬁnd that AutoFIS is apparently faster than the base model (DeepFM) both in training and inference time, because removing those useless interactions contributes to accelerating the model training and inference, with higher performance. Although the training time of AIM is acceptably larger than the other models, the inference of AIM is very efﬁcient (i.e., even more efﬁcient than DeepFM). Such time complexity makes it possible to deploy AIM in an industrial system. 4.5.2 Complexity Analysis for high-order interaction In this section, we analyze the complexity of AIM with highorder interactions. We consider the training and inference time as time complexity. The training time contains three parts: search interaction-IF stage, search embed stage and re-train stage. Also, we take the parameters into account to compare space complexity. From Table 8 and Table 9 we can ﬁnd that, thanks to the selection algorithm for high-order feature interaction proposed in Section 3.2 which reduces the time complexity of exploring p-order interactions from O(n) to O(n), the overhead of introducing high-order interactions in terms of training time and space complexity is acceptable. It takes about 155 minutes in total for AIM to search important 2-, 3- and 4-order feature interactions with appropriate IFs and proper embedding sizes for each ﬁeld with a single GPU. The same decisions will take the human engineers dozens of days or weeks to make by analysis and experiments. 4.6 Transferability of the Selected Interaction-IF pairs by AIM (RQ4) In this subsection, we investigate whether the important feature interactions with appropriate IFs learned by AIM could be transferred to other models for boosting their performance. We apply the searched interaction-IF pairs to two benchmark methods, i.e., DeepFM [4] and PNN [12], to explore its transferability. As shown in Table 10, using 2-order feature interactions and IFs selected by our model achieves much better performance in both DeepFM and IPNN, with around 16% of all the 2-order interactions in Avazu dataset and around 20% in Criteo dataset. Moreover, the promotion is more signiﬁcant by using high-order interactions with IFs. More speciﬁcally, with 4-order interactions and corresponding IFs, the performance is improved by 0.46% for DeepFM and 0.67% for PNN, respectively. Both evidences verify the transferability of the selected feature interactions with chosen IFs in AIM. 4.7 Deployment & Online Experiments (RQ5) Online experiments are conducted in the recommender system of a mainstream app market to verify the superior performance of our model AIM, where hundreds of millions of daily active users generate hundreds of billions of user log events every day in the form of implicit feedback such as browsing, clicking and downloading apps. In the online serving system, hundreds of candidate apps that are most likely to be downloaded by the users are retrieved from the universal app pool. Then these candidate apps are ranked by a ﬁne-tuned ranking model (such as DeepFM, AIM) before presenting to users. To guarantee user experience, the overall latency of the above-mentioned candidate selection and ranking is required to be within a few milliseconds. Because of the longer training time and more complicated functions of AIM, we simplify it and put a simpliﬁed version to deploy on our recommender system for the trade-off between efﬁciency and performance. The commercial model is deployed in a cluster, where each node is with 48 core Intel Xeon CPU E5-2670 (2.30GHZ), 400GB RAM and as well as 2 NVIDIA TESLA V100 GPU cards. Speciﬁcally, a three-week A/B test is conducted in a major list of an app recommendation scenario. Our baseline in online experiments is DeepFM, which is a strong baseline due to its extraordinary accuracy and high efﬁciency. For the control group, users are randomly selected and presented with recommendation results generated by DeepFM. For the experimental group (7% users), users are presented with recommendation apps generated by our AIM model. Figure 9 shows the improvement of the experimental group over the control group in terms of CTR (#downloads/#impressions). We can observe that the average improvement of CTR is 4.4% (statistically significant), which brings enormous commercial proﬁts. These results demonstrate the magniﬁcent effectiveness of our proposed model in industrial applications. In this work, we propose Automatic Interaction Machine (AIM) with three core components, namely, Feature Interaction Search (FIS), Interaction Function Search (IFS) and Embedding Dimension Search (EDS) to automatically select signiﬁcant feature interactions, appropriate IFs and necessary embedding dimensions in a uniﬁed framework. The proposed AIM is easy to implement with marginal search costs, and the performance improvement is significant in two benchmark datasets and one private dataset. Our model has been deployed onto the training platform of a mainstream app market recommendation service, with signiﬁcant economic proﬁt demonstrated. ACKNOWLEDGEMENTS The SJTU team is supported by “New Generation of AI 2030” Major Project (2018AAA0100900), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102) and National Natural Science Foundation of China (62076161, 62177033). The work is also sponsored by Huawei Innovation Research Program.