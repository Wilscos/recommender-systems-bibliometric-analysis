<title>Practical and Secure Federated Recommendation with Personalized Masks</title> <title>arXiv:2109.02464v1  [cs.IR]  18 Aug 2021</title> In recent years, federated learning has been a fast-growing research eld, which keeps private data locally at multiple parties and trains models collaboratively in a secure and privacy-preserving way [ 25 ]. The application scenarios range from cross-device ones [ 24 ] to cross-silo ones [ 34 ]. The participants of cross-device federated learning are mobile or IoT devices. And reducing communication costs and protecting user privacy are two major concentrations of cross-device federated learning. In contrast, cross-silo federated learning aims to build a model over the data silos of multiple organizations with condentiality and legal constraints. Privacy-preserving is one of the major challenges in federated learning. Data decentralization does alleviate privacy risks compared with the conventional data-center training scheme. However, the gradients transmitted among dierent parties could still leak user privacy [ 41 ]. Current solutions can be broadly divided into two categories. The rst-kind solutions utilize cryptographic methods such as homomorphic encryption (HE) [ 12 ] and secure multi-party computation (SMC) [ 35 ]. These methods could lead to lossless model performance. However, they producing extra computation and communication costs since federated learning needs a large amount of calculation and intermediate results exchange. The second-kind solutions utilize the obfuscation methods such as dierential privacy (DP) [ 11 ]. Although DP-based federated algorithms are ecient, they damage the accuracy of models. Therefore, these solutions all have diculties when applying to practical problems. Federated recommender system (FedRec) is an essential application of federated learning in the recommendation scenario [ 33 ]. We concentrate on the cross-device horizontal FedRec. Current cross-device horizontal FedRec methods distribute the parameters and training process on both the clients and server, where only gradients are transmitted. The motivation is to keep the user’s rating data privately on the local device. Similarly, gradients leak the original data. HE-based FedRec [ ] and DP-based FedRec [ 16 ] have been designed to provide a recommendation service without leaking the data privacy of multiple sources. However, they cannot satisfy both the two requirements of recommender system (RecSys), i.e., personalization and real-time. In this paper, we propose a novel FedRec framework, named federated masked matrix factorization (FedMMF). The designed FedMMF framework could protect the data privacy of FedRec without sacricing eciency or ecacy. Instead of using HE and DP, we utilize the secret sharing (SS) technique to incorporate the secure aggregation process of federated matrix factorization. Our designed secure aggregation randomly splits the private gradients of each party into many shares, then reconstructs the nal aggregated updates using the secret shares from dierent parties. Compared with HE, SS largely speeds up the whole training process. In addition, we introduce a new idea of protecting private data from leakage in federated learning, which is called the personalized mask. A personalized mask is a mask that adds on the original data for preserving privacy. “Personalized” means that the mask varies according to the user’s data and helps improve model accuracy. Gradients computed on the masked ratings of many participants could be secure enough to directly share with the server, which further relieves the eciency problem of FedRec. Therefore, on the one hand, the personalized mask could further improve eciency. On the other hand, the personalized mask also benets ecacy. We rst apply the personalized mask algorithm in the recommendation scenario, specically in the matrix factorization algorithm [ 19 ], because the rating information is the only privacy to protect in the recommendation scenario, which is relatively simple. Theoretically and empirically, we show the superiority of FedMMF. The personalized masks protect the original rating information, shown in Fig. 1. Thus, the server would access no user privacy. The paper is organized as follows, in Section 2, we rst introduce the basic models and the privacy leakage problem; in Section 3, we explain the FedMMF algorithm, the training process, and the privacy guarantee; in Section 4.1, we show the performance of FedMMF in two real-world datasets and talk about the relation between privacy and accuracy; in Section 5, we discuss two related topics in federated learning. They are somewhat similar to our work but dier in the main idea. The contributions of this paper are three-folded: We apply the secret sharing technique in the federated recommendation scenario, which largely speeds up the current federated matrix factorization solutions; We propose a new idea of personalized masks and show it could help solve the eciency and ecacy challenge of FedRec at the same time; We discuss related topics and the extension of personalized masks in general federated learning tasks. In this section, we rst introduce the traditional matrix factorization for recommendation. Then, based on the current challenges of RecSys, we explain federated matrix factorization (FedMF). Although FedMF alleviates the privacy problem of FedRec, there still exists leakage in the training process. Finally, we talk about the current solutions to secure FedMF. 2.1 Matrix Factorization Given a rating matrix 𝑹 ∈ R , the recommender system aims to ll in the missing values of the matrix. Matrix factorization (MF) is regarded as one of the most classic recommendation algorithm [ 19 ]. It decouples the original matrix into two low-rank matrices. The rating that user 𝑢 gives to the item 𝑖 can be approximated as: = 𝒒 , (1) where ∈ R represents the latent factors of item ∈ R represents the latent factors of user , and the latent dimension can be regarded as the item’s implicit characteristics. We could optimize the latent factors via minimizing the loss given below using the existing ratings: min (𝑟 − 𝒒 + 𝜆( ), (2) where stands for the set of user-item pairs whose rating is already known and is the regularization coecient. Stochastic gradient descent is utilized to update each parameter: ← 𝒑 − 𝛾 · (𝜆 · 𝒑 − 𝑒 · 𝒒 ), (4) where = 𝑟 − 𝒒 and is the learning rate. Conventional recommender systems centrally collect users’ private data and train MF algorithm on the server, which leads to immense privacy risks. 2.2.1 Vanilla FedMF. As the development of federated learning, federated recommender system (FedRec) was proposed to address the privacy and data silo problems in the recommendation scenarios [ 33 ]. In this paper, we focus on the horizontal FedRec where each party only contains the rating information of one individual user and the user’s private data is not allowed to leave the local device. Federated matrix factorization (FedMF) was designed to train recommendation models in such a naturally distributed situation. In the vanilla FedMF algorithm [ ], all the item latent factors 𝑸 ∈ R are maintained on the central server, while each user’s latent factors is kept on the local party. The training process is as followed and loops until the convergence of model parameters: Algorithm 1 Secret Sharing , 𝑎 , ..., 𝑎 and set 𝑎 = 𝑠; = (𝑎 + 𝑎 · 𝑗 + 𝑎 · 𝑗 + ... + 𝑎 · 𝑗 ) 𝑚𝑜𝑑 𝑙; , 𝑠 , ..., 𝑠 , 𝑗)}, 𝑦, 𝑥, 𝑙): = 𝑎 + 𝑎 · ( 𝑗 𝑚𝑜𝑑 𝑙) + 𝑎 · ( 𝑗 𝑚𝑜𝑑 𝑙) + ... + 𝑎 · ( 𝑗 𝑚𝑜𝑑 𝑙); where stands for the set of items that user has rated. Obviously, only and are unknown to the server. Combining equations 5, 6, and 7, the server could solve the unknown variables [21]. In this way, private raw ratings of each user are revealed. 2.2.3 Secure FedMF. To address the gradient leakage problem of vanilla FedMF, a few secure FedMF algorithms have been proposed. For example, HE-based FedMF [ ] and DP-based FedMF [ 16 ], respectively, utilize HE and DP to further preserve privacy. HE-based FedMF encrypts gradients of item latent factors with HE before transmitting them to the server. Then, the server performs secure aggregation on the encrypted gradients, updates item latent factors in ciphertext state, and distributes the new encrypted item latent factors to each user. In a similar way, DP-based FedMF adds noises to gradients before aggregation. However, the former one causes extra costs and the latter one results in accuracy losses. Neither of the current HE-based FedMF and DP-based FedMF could meet both the personalization and real-time requirements of recommender systems when protecting data privacy. In this section, we explain the proposed federated masked factorization (FedMMF) framework. First, FedMMF applies secret sharing in the secure gradients aggregation procedure, which largely speeds up the training process. Then, we design a new idea of the personalized mask to further alleviate the eciency and ecacy problem of FedRec. In our proposed FedMMF algorithm, we utilize a secure aggregation technique designed with secret sharing (SS) [ 31 ] to achieve both eciency and ecacy of federated learning. This secure aggregation technique was rst proposed in [ ]. In the following of this subsection, we rst introduce the secret sharing method. Then we explain the secure aggregation algorithm with secret sharing. 3.1.1 Secret Sharing. Secret sharing is designed to divide a secret into pieces , ..., 𝑠 . An adversary with any pieces or more can obtain the original secret. However, an adversary with any 𝑦 − 1 pieces or fewer cannot. Secret sharing is designed based on polynomial interpolation. The shares’ creating and secret-reconstructing processes are shown in Algorithm 1. For an arbitrary secret number , we could randomly generate shares {(𝑠 , 𝑗)} using a large prime number based on the CreateShares function. With and only with and more than shares, the original secret can be reconstructed via the ReconstructSecret function. 3.1.2 Secure Aggregation with Secret Sharing. We utilize a secure aggregation method with secret sharing. The details are shown in Fig. 2. Before the aggregation process, all parties agree on the number of users , the secret sharing threshold , the large prime number , the parameters of DieHellman (DH) method [ ], and an arbitrary encryption method for authentication. At the beginning of secure aggregation, each party generates DH key pairs < 𝑐 , 𝑐 and < 𝑠 , 𝑠 𝑆𝐾 and 𝑃𝐾 stand for private key and public key, respectively. Then, parties send the public keys to the server. After receiving the public keys from all parties, the server broadcasts them to each party. Next, each party generates randomly and for every other party via the Die-Hellman key agreement method. Besides, party computes secret shares for and respectively using the CreateShares function in Algorithm 1. Then, all the parties encrypt the shares using the authenticated encryption method and send them to the server. The server forwards the corresponding shares to each party. After that, each party 𝑢 computes the masked value 𝑡 for its secret value 𝑠 , shown as below: = (𝑠 + 𝑏 ) 𝑚𝑜𝑑 𝑙, (8) where equals for a specic pair of public keys (𝑠 , 𝑠 . Next, party sends its to the server. The server sends the existing party-list to all parties. Then, each party judges if the number of existing parties is larger than . If so, it decrypts the encrypted shares received in the previous step. Then it sends the share of for the alive parties and the share of for the dropped Algorithm 2 Federated Masked Matrix Factorization (FedMMF) , 𝒑 , 𝑓 , each party 𝑢 initializes 𝒑 and 𝑓 (𝜃 ). (𝜃 ) on local data 𝒓 according to Eq. 11 for each 𝑖 ∈ K = MaskedUpdate(𝒒 ); with secure aggregation protocol in Fig. 2; = 𝒒 − 𝛾 · for each 𝑖 ∈ I; = 𝑟 − 𝒒 for each 𝑖 ∈ K according to Eq. 7; according to Eq. 6 for each 𝑖 ∈ K to server with secure aggregation protocol in Fig. 2. parties to the server. Finally, the server could obtain the aggregated value via the following equation: The additional mask is a common practice of hiding raw data from revealing by other parties in federated learning [ 34 ]. It is generated randomly and privately by each party. Like the conventional mask, the main idea of the personalized mask is to cover the original ratings so that the server cannot access users’ private data. However, we produce the masks via private well-trained model separately at each party. We name the mask as personalized mask because it can also provide a performance improvement. Shown in Fig. 1, Federated Masked Matrix Factorization (FedMMF) algorithm applies the idea of personalized mask in the previous FedMF architecture. The whole training process is as followed. Firstly, before the federated training of latent factors, each local party trains a private model using only the user’s own data. The corresponding loss function is shown as below: (𝑟 − 𝑓 (𝑖)) (10) |K Without loss of generality, we dene the private model of user as . Then, the model is used to give prediction (𝑖) on each user-item pair 𝑢, 𝑖 , where 𝑖 ∈ K . The opposite of the prediction is regarded as the personalized mask. Finally, all parties collaboratively train a matrix factorization model on the masked rating: = 𝑟 − 𝑓 (𝑖). (11) The prediction of FedMMF algorithm for one specic user-item pair (𝑢, 𝑖) is: = 𝒒 + 𝑓 (𝑖). (12) The private model could be an arbitrary model which only trains on the local data. Therefore, side information may also be used, which contains user proles, item attributes, or context features (e.g., location, time, weather, etc.). On the one hand, We hope for a performance improvement of FedMMF compared to vanilla FedMF. The reason is that we actually utilize the idea of ensemble learning [ 30 ]. Ensemble learning is commonly used to combine multiple weak learners for a better prediction performance [8]. On the other hand, the well-behaved private model at each local party could protect the privacy of original ratings. Thus, parties with well-behaved private models are able to directly share their gradients computed on the masked ratings. In such way, the eciency is further improved. The details of FedMMF are shown in Algorithm 2. First, each party trains a private model only using local data. Then, personalized masked rating is generated using the private model. Next, federated training is performed on the personalized masked ratings. For each party, the updates of item’s latent factors are directly shared to the central server, if the private model is personalized enough to cover the private information. Other parties join the federated training via the secret sharing secure aggregation method 2. Privacy protection is relatively simple in the recommendation scenarios because we only need to care about the privacy of user ratings. However, when extending the personalized mask technique to general federated learning tasks, we should also prevent feature information from revealing. InstaHide oers us a novel privacy-preserving idea with data augmentation [ 18 ]. The authors utilize a simple encryption method to protect the privacy of original image features, while they did not consider the privacy of the label. Combining with the proposed encryption approach, the personalized mask could fulll the privacy requirement of general federated learning tasks. 3.3 Security Analysis The task of private model is to hide the information of ∈ R , which is the rating that each user 𝑢 ∈ U gives to item 𝑖 ∈ I . For user , the training data of is denoted by {(𝑖, 𝑟 )} . The training data is sampled from a joint distribution . We assume R ∈ [ This indicator could inform us of many things. It is interpreted in dierent ways when increasing and decreasing. When indicator starts to increase from a small number, there are two possible reasons to explain. Firstly, we leak more privacy if the local model is constrained too much. Thus, the model prediction is approximately the rating bias. In this case, the masks are not secure enough anymore since the bias is much easier to estimate. Secondly, privacy might be protected much better when the model is trained to make random inferences. However, model ecacy is also severely damaged. When indicator reduces to relatively small numbers, privacy is thought to be protected better. The local private model predicts more accurately, therefore personalized masks cover more information of the original ratings. Theorem 1 provides us how much privacy could be protected the most when privacy indicator 𝐽 continues decreasing. Theorem 1. FedMMF is (𝜖, 𝛿) −𝑝𝑟𝑖𝑣𝑎𝑡𝑒 for user if there exists a function ) × ( ) → N For any 𝜖, 𝛿 ∈ (0, 1) and any distribution 𝑃 , if 𝑛 > 𝑛 , then 𝑃𝑟 (𝐽 (𝑓 , 𝑃 ) ≤ min 𝐽 (𝑓 ,𝑃 ) + 𝜖) (14) ≥ 1 − 𝛿. Proof. For any ∈ F , the privacy indicator of user calculated on the training sample is: Each ∥R − 𝑓 (I )∥ is independent random variable with mean 𝐽 (𝑓 , 𝑃 . We further assume that ∥R − 𝑓 (I )∥ ∈ [0, 1]. According to Hoeding’s inequality , we obtain: 𝑃𝑟 (|(𝑓 , 𝑃 ) − 𝐽 (𝑓 , 𝑃 )| ≥ 𝜖) ≤ 2𝑒 , (16) then we could get: 𝑃𝑟 (|(𝑓 , 𝑃 ) − 𝐽 (𝑓 ,𝑃 )| ≤ 𝜖, (19) ∀𝑓 ∈ F ) ≥ 1 − 𝛿, which is equivalent to: 𝑃𝑟 (𝐽 (𝑓 , 𝑃 ) ≤ min 𝐽 (𝑓 ,𝑃 ) + 2𝜖) (20) ≥ 1 − 𝛿. ∀𝑓 ∈ F , |(𝑓 , 𝑃 ) − 𝐽 (𝑓 , 𝑃 )| ≤ 𝜖, (21) we could obtain step by step: = min 𝐽 (𝑓 , 𝑃 ) + 2𝜖. The function determines the sample complexity of user for training a FedMMF algorithm. It stands for how many samples at least are required to guarantee the privacy of FedMMF for user . We assume the hypothesis class is nite. However, it is not a necessary condition, and Theorem 1 can be further generalized. FedMMF is nearly able to achieve the best privacy-preserving performance in . Therefore, we should always try to nd a better hypothesis class for FedMMF to search on dierent data sets. In this section, we show that FedMMF could vastly improve model eciency as well as obtaining good ecacy without the loss of privacy. Firstly, we explain the data sets, baseline models, and other settings in the experiments. Then, we show the improvements of FedMMF on model eciency. The secret sharing secure aggregation method could promote eciency well. In addition, with the help of personalized masks, FedMMF further accelerates the training process. At last, we validate the model ecacy improvement of FedMMF with dierent kinds of personalized masks, comparing to the baseline model. We verify FedMMF on three real-world data sets. Two of them are MovieLens data sets [ 15 ], i.e., MovieLens 100K and MovieLens 10M. The other one is the LastFM data set [ ]. They are movie and music recommendation data sets, respectively. The detailed descriptions of data sets are shown in Tab. 1. In our experiment, each user is regarded as a participant in the collaborative training process. Therefore, the user’s own ratings are kept on the local party. Besides, we utilize the side information (i.e., user proles and item attributes) to train the local private model. The reason is that local collaborative ltering algorithms (e.g., local MF) are meaningless and cannot be used to construct personalized masks. Although the involvement of context data could result in unfair comparison more or less, we focus on the illustration of the personalized mask in this relatively simple recommendation scenario in this paper and will generalize to more complicated ones. To construct features from tags in the data set, we utilize TFIDF [ 29 ] and PCA [ ] techniques. Besides, we set bins for the listening counts of music of the LastFM data set and convert them into ratings scaling from 1 to 5. The baseline models are shown as below: • FedMF Parties collaboratively train matrix factorization models via sharing the latent factors of common users, where neither HE nor DP is utilized. • One-order FedMMF Each party locally learns linear personalized masks to hide private rating information via a linear regression model [ 26 ]. Then, all parties collaboratively train FedMF on the one-order masked ratings. • Two-order FedMMF Each party constructs two-order masks to protect private ratings via locally learning a factorization machine model [ 28 ]. Then, all parties collaboratively train FedMF on the two-order masked ratings. • High-order FedMMF Each party captures high-order and nonlinear feature interactions through a neural network model[ 36 ]. Then, the private model is utilized to mask local ratings. Finally, all parties collaboratively train FedMF on the high-order masked data. We do not compare FedMMF with the methods using HE or DP. The reasons are the followings. First, the FedMF algorithms using HE will denitely obtain a same or very close accuracy with vanilla FedMF, but largely slow down the training process. The performances are shown in the previous works [7]. Second, DP has a dierent privacy denition from ours. Therefore, it’s unfair to make such a comparison. Besides, we also show the performance of various local context models and federated context models for reference. In addition, the evaluation metrics of model ecacy are root mean square error (RMSE) and mean absolute error (MAE). They are averaged by each user-item pair but not each user, which is an alignment with most current works. Besides, we run each experiment 10 times to obtain the mean and standard deviation values. Compared with HE-based FedMF, rst, FedMMF largely speeds up the training process of federated learning via secret sharing. At the client side, the computation complexity of our designed secure aggregation method is 𝑂 (𝑥 + 𝑘𝑥𝑦) , the communication complexity is 𝑂 (𝑥 + 𝑘𝑦) , and the storage complexity is 𝑂 (𝑥 + 𝑘𝑦) . At the server side, the computation complexity is 𝑂 (𝑘𝑥𝑦 , the communication complexity is 𝑂 (𝑥 + 𝑘𝑥𝑦) , and the storage complexity is 𝑂 (𝑥 + 𝑘𝑦) ]. Then, the personalized mask technique could further improve the eciency of the secure aggregation process via sharing plain-text gradients of parties with well-protected ratings. We provide two attack methods for analyzing how much the personalized mask technique could further promote model ecacy. They are recovery attack and ranking attack. The rst one tries to recover the original ratings from the masked ratings, while the second one aims to rank the masked ratings and separate high ratings from low ratings. Taking two-order FedMMF on MovieLens 10M data set as an example, we conduct the attack experiments. The range of ratings in the MovieLens 10M data set is from 0.5 to 5.0. And the rating interval is 0.5. of the original ratings. Then, the adversary could scale the masked ratings to the range of original ratings for recovery. We dene as the error level. If the dierence between one recovered value and the corresponding original rating is less than , the recovery is regarded to be successful. Thus, there exists a recovery rate for each party’s masked rating. In Tab. 2, we show the proportion of parties whose recovery rate is in a certain range under dierent error levels. As we can see, when the error level is small, e.g., 𝑔 = 1 and 𝑔 = 2, the adversary could nearly reveal no party’s privacy with a recovery rate larger than 0.5. And as the error level increases, the recovery rate begins to grow. However, a higher error level means a more inaccurate recovery, and the utility of the recovered ratings is poorer. 4.2.2 Ranking Aack. Since the intuitive recovery attack seems not successful enough, we introduce another method named ranking attack. Instead of recovering the original concrete ratings, ranking attack tries to reveal the high-rating items from their masked ratings. First, for each party, the adversary ranks the rated items according to their masked ratings. Then, items in the top proportion of masked ratings are selected as the high-rating items. Similarly, given , we also sort these items with regard to their original ratings. Thus, we could evaluate the ranking attack with hit ratio . It is calculated as the ratio that items selected using masked ratings are in the true high-rating item set. Tab. 3 shows that, under dierent top proportion , the ranking attack could reveal the rating ranking privacy of parties. If the selected top proportion is small, e.g., ℎ = 1 and ℎ = 2, the attacks performed on most parties’ masked ratings obtain the hit ratio less than 0.5. It means that more than half of selected items do not have high ratings. When the adversary tunes larger, the attack becomes more eective. However, a large is relatively meaningless because the adversary does not want to choose all items to be high-rating in reality. According to the experiment results of the above two attack methods, we nd that a considerable number of participants get their rating privacy well-protected with the help of personalized masks. Therefore, on the basis of eciency improvement made by the secret sharing secure aggregation method, the personalized mask could further accelerate the training process of federated recommendation. Besides federated learning, the personalized masked ratings could also be centralized collected and used for training without privacy leakage. This operation is able to reduce the communication and computation cost once again. In this section, we verify the ecacy of FedMMF on three real-world data sets. The incorporation of personalized masks utilizes the idea of ensemble learning to combine weak learners for a better generalization ability. In the recommendation scenario, feature interactions are important information to capture. Therefore, we implement three private models to construct personalized masks with dierent properties: the one-order mask, two-order mask, and high-order mask. The performances of FedMMF with these three masks are shown in Fig. 3. RMSE and MAE are both regression evaluation metrics. Smaller value stands for better model ecacy. As we can see, FedMMF models with dierent personalized masks all outperform FedMF. Another observation is that, on all three data sets, two-order FedMMF and high-order FedMMF dominate alternatively. It means we should utilize cross features to construct personalized masks in the recommendation scenarios. The improvements could be divided into two parts. The rst part benets from the ensemble training scheme of FedMMF, which is our main focus. The second part takes advantage of the side information utilized in the private model of FedMMF. We cannot avoid this unfair improvement with regard to the particularity of recommendation scenarios. This is a compromise because the recommendation scenario is relatively simple with only rating information to protect, and we want to primarily verify the idea of personalized masks. However, we also compare FedMMF models with corresponding local context and federated context models, shown in Tab. 4. Comparing FedMMF with dierent local context models and federated context models, we could see that FedMMF also outperforms both of them. This observation veries the main contribution to the ecacy improvement is the incorporation of ensemble learning. On the other hand of the shield, FedMMF can also be regarded as an excellent way to combine collaborative information and feature information. 4.3.1 Discussion about Relation between Privacy and Personalization. The ecacy of FedMMF is also related to the privacy of the original ratings of each party. We take the performance of twoorder FedMMF on MovieLens 100K data set for illustration. We x the federated part in FedMMF and tune the hyper-parameters of the local FM model. Fig. 4 shows the relation between RMSE and privacy indicator , which is dened in Eq. 15. We could see that, as the penalty of the local model increases, privacy indicator keeps growing, while RMSE value rst goes down then goes up. Large regularization forces the model to learn the rating bias. We regard this case as privacy leakage, as masked ratings could easily reveal private information with our designed recovery attack and ranking attack. When we obtain the best model ecacy of FedMMF, the privacy leakage may be severe. Therefore, we should consider more for training a FedMMF model. In this section, we will introduce two related topics, i.e., personalized federated learning and privacy-preserving federated learning without cryptographic and obfuscation Methods. Besides, we discuss the dierence between them and our work. Personalized federated learning [ 20 22 32 ] is a new direction of federated learning, personalizing the single global model to get better adaptive to each local party. The motivation is that, in many cases, the single global model cannot outperform local models trained locally. In essence, personalized federated learning addresses the Non-IID problem [ 40 ] in federated learning. Current works can be divided into data-based and model-based approaches. Data-based methods aim to align the local data distribution, while model-based methods design dierent novel federated learning models for better adaptation on each party. For example, [ 10 ] train global and local models under regularization with Moreau envelopes. In [ 39 ], one specic party only federates with relevant parties, and model parameters are calculated as optimal weighted combinations of available ones based on the relations. [ 14 ] aims to nd a trade-o between the global federated model and the local private models. Furthermore, [ 13 ] provides the rst provably optimal method for this personalized federated learning approach. Several model-based methods also adopt ensemble learning schemes to learn global and local models. [ 27 37 ] utilize the mixture of experts (MoE) technique [ 23 ] to combine federated and private models. They both train an extra gating function for deciding which regions to trust one over another. [ ] adopts a similar training process as ours. Each party privately trains the local model. Then, the global model trains on the residuals of all local models. However, none of the above methods studies the privacy-preserving eect of the local model. Privacy protection is an important topic in federated learning. Current privacy-preserving works either utilize cryptographic or obfuscation methods. However, neither of them could maintain eciency and ecacy simultaneously. Thus, many researchers begin to nd new ways besides cryptographic and obfuscation methods InstaHide and TextHide utilize data augmentation methods to preserve data privacy [ 17 18 ]. InstaHide adopts the Mixup approach [ 38 ] to design a simple encryption algorithm. Before federated training, InstaHide performs the proposed encryption on local images. The whole process is ecient and aects little on prediction accuracy. The authors claim InstaHide can well protect data privacy. However, [ ] provides an attack that can successfully break the InstaHide encryption and reveal user privacy. Similarly, TextHide applies the same encryption logic in the NLP tasks. InstaHide and TextHide are data-based privacy-preserving approaches, while FedMMF could be regarded as model-based. Furthermore, our proposed personalized mask approach is benecial to model ecacy. In this paper, we provide a new idea of personalized masks to protect data privacy in federated learning, which neither slows the training process down nor damages model performance. Taking the recommendation scenario as an example, we apply it in the FedMMF algorithm. Combining with the secure aggregation method of secret sharing, the proposed model shows superiority theoretically and empirically. In our future work, we would like to extend personalized masks to general federated learning tasks that involves feature information in the collaborative process.