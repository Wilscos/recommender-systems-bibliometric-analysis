A classiﬁcation scheme of a scientiﬁc subject gives an overview of its body of knowledge. It can also be used to facilitate access to research articles and other materials related to the subject. For example, the ACM Computing Classiﬁcation System (CCS) is used in the ACM Digital Library search interface and also for indexing computer science papers. We observed that a comprehensive classiﬁcation system like CCS or Mathematics Subject Classiﬁcation (MSC) does not exist for Computational Linguistics (CL) and Natural Language Processing (NLP). We propose a classiﬁcation scheme – CLICKER for CL/NLP based on the analysis of online lectures from 77 university courses on this subject. The currently proposed taxonomy includes 334 topics and focuses on educational aspects of CL/NLP; it is based primarily, but not exclusively, on lecture notes from NLP courses. We discuss how such a taxonomy can help in various real-world applications, including tutoring platforms, resource retrieval, resource recommendation, prerequisite chain learning, and survey generation. As the scientiﬁc literature and educational resources continue to grow beyond an individual’s capacity to follow them, an indexing and classiﬁcation scheme can play an important role in facilitating access to different stakeholders. Additionally, a classiﬁcation scheme of an academic subject provides a cognitive map of the domain. For example, the current Mathematics Subject Classiﬁcation (MSC), Medical Subject Headings (MeSH), Physics Subject Headings (PhySH)provide both a cognitive map and a body of knowledge in Mathematics, the Life Sciences, and Physics respectively. These taxonomies are currently used to classify scientiﬁc articles on these subjects. In the context of computing, the ACM Computing Classiﬁcation System (CCS)has served as the standard for classifying the computing literature since 1964 (Rous, 2012). CCS is also used in the ACM Digital Library (DL) to “index content for subject-oriented searching; to ﬁnd similar documents; to create author expertise proﬁles; to identify strong research areas in Institutional Proﬁles; and to create the topical tag clouds found in aggregated SIG and conference views” (Rous, 2012). We observed that unlike MSC, PhySH, and ACMCCS, a comprehensive classiﬁcation scheme does not exist for Computational Linguistics or Natural Language Processing (CL/NLP) and this makes it difﬁcult to search for educational materials on a speciﬁc topic and scientiﬁc articles in the ACL Anthology (AA). We observed that several standard subject classiﬁcation schemes focus on ‘Language’ and ‘Linguistics’, but not on CL/NLP. For example, there are Class P in the Library of Congress classiﬁcation, class 400 and 410 in Dewey Decimal Classiﬁcation, and class P in Ranganathan’s Colon Classiﬁcation (Satija, 2017)). ACM-CCS has a set of classes speciﬁc to CL/NLP in the following category: CCS→Computing methodologies→Artiﬁcial intelligence→Natural language processing but its size is small. These classiﬁcation schemes are not comprehensive enough and cannot cover the topics in the ACL Anthology. For example, none of the classiﬁcation schemes mention even key areas of NLP such as ‘Summarization’, ‘QuestionAnswering’, or ‘Sentiment Analysis’. In this position paper, we highlight the need for a classiﬁcation scheme for CL/NLP, which, just like ACM-CCS, can be used to classify and index educational and research materials on this subject. Based on this, we propose a classiﬁcation scheme – CLICKER for CL/NLP based on the analysis of online lectures from a number of university courses on this subject. The folloing sections include (1) overview of the process of the classiﬁcation scheme creation, (2) basic statistics of the the classiﬁcation scheme, and (3) a list of possible applications. In this section, we survey various existing taxonomies, including one Academic NLP Taxonomy, several NLP taxonomies from the Web, CS Taxonomies and Non-CS Taxonomies. The ACL Anthology (AA) is the open-source archive of the proceedings of all ACL sponsored conferences and journal articles (Bird et al., 2008; Radev et al., 2013; Gildea et al., 2018). It currently hosts more than 60,000 papers. AA has served as a valuable resource to characterize the work of the ACL community. For example, (Hall et al., 2008) used latent Dirichlet Allocation (LDA; (Blei et al., 2003)) based topic models and observed the shift in ideas in the ﬁeld of CL/NLP. (Anderson et al., 2012) analyzed the computational history of AA using LDA; however, their analysis is people-centric. In another paper, (Anderson et al., 2012) group topics into high-level categories. However, it is hard to understand the process of their grouping. For example, ‘Dialog’ and ‘Summarization’ are grouped into a category named ’Linguistic Supervision’, while ‘Speech Recognition’ is considered as a part of ‘Government’ category. (Schumann and QasemiZadeh, 2015) also analyze paradigm changes in AA, however they focus only on ‘Machine Translation’. (Jurgens et al., 2018) also explore the evolution of AA, but through citation frames and not from a classiﬁcation perspective. (Belinkov and Glass, 2019) categorize research articles related to neural models of NLP from different perspectives such as linguistic information, the challenge set for evaluation of neural networks, and methods for adversarial examples in NLP. However, this analysis is only focused on neural models of NLP. (Rogers et al., 2021) propose a taxonomy related to Question Answering (QA) and Reading Comprehension (RC) resources along multiple dimensions. The authors use the taxonomy to categorize over 200 datasets related to QA/RC. (Uban et al., 2021) use topic modeling to track the evolution of topics in AA across three major dimensions: tasks, algorithms and data. Besides the ACL Anthology, other NLP Taxonomies exist. They have been developed by different groups of researchers, and they are not designed for the purpose of classifying publications, but rather for more general educational resources which are the same as our focus. We survey these similar NLP taxonomies in the following and compare them in Table 1. NLPExplorer(Parmar et al., 2020) is an automatic portal for collecting, indexing, and searching CL and NLP papers from the ACL Anthology. The papers are indexed under a manually curated list of topics categorized into ﬁve broad categories, Linguistic Targets, Tasks, Approaches, Languages, and Dataset Types. This approach is limited in that there are concepts that are not covered by the list of topics. Moreover, the two-level hierarchical structure does not allow for the further organization beyond the initial classiﬁcation of topics. The NLP Indexis a search engine containing over 3,000 repositories of NLP-related code with their corresponding research papers. The repositories can be searched using dozens of pre-deﬁned topic queries categorized into eight broad categories. Like NLPExplorer, this list of topics is not comprehensive and does not allow for the other categories beyond the initial ones. nn4nlp-conceptsis a concept hierarchy which attempts to cover the concepts needed to understand neural network models for NLP. The topics are generated both automatically and through manual annotation, and are organized in a hierarchical structure with a maximum depth of 3. However, these concepts are again only limited to neural network related topics. TutorialBank(Fabbri et al., 2018) is a manually collected corpus of NLP educational resources, including research papers, blog posts, tutorials, abnd lecture slides. The most recent versioncontains 23,193 resources. Together with the resources, they also propose a concept list containing 208 topics via crowdsourcing, with prerequisite annotations. Prerequisite relations are represented as a graph, Table 1: Existing Taxonomies. *We could not ﬁnd the exact depth of the Computer Science Ontology. instead of a tree structure for a taxonomy. LectureBank(Li et al., 2019) is a manually collected dataset containing thousands of NLP-centric university-level lecture slides as well as 322 concepts collected through crowdsourcing. These concepts cover the ﬁeld of NLP, basic machine learning, and deep learning for the purpose of prerequisite chain learning. However, like TutorialBank, these concepts have various granularity and forms in a graph structure. NLP-progressis a repository to track NLP progress. It contains the most advanced tasks and datasets, including entities from 14 languages. The current version contains 38 topics under English in a ﬂat structure. NLPediatracks the performance of more than 300 systems on 40 datasets and nine tasks. Additionally, they diagnose the strengths and weaknesses of a single system and interpret relationships between multiple systems (Liu et al., 2021). TheACM Computing Classiﬁcation Systemis a poly-hierarchical classiﬁcation scheme for the ﬁeld of computing that can be utilized in semantic web applications. The 2,113 topics are organized in a tree structure with a maximum depth of six. TheComputer Science Ontologyis a largescale taxonomy of computer science research areas automatically generated from about 16 million publications. It includes 14,164 topics and 162,121 semantic relationships and is organized in a tree structure with "Computer Science" as the root. The ontology includes other semantic relationships such as equivalency between topics or indication. Semantic Scholaris an AI-powered research tool for scientiﬁc literature. The search engine covers research papers and ﬁltering of results by ﬁeld of study, date range, publication type, etc. The site includes literature in a large number of subjects, including computer science, and more speciﬁcally NLP. The papers are tagged with topic keywords that are automatically extracted using machine learning techniques. The site provides a Wikipedia summary for each topic and lists related, broader, and narrower topics, suggesting that these topics are organized in a hierarchical structure. ArnetMiner (AMiner)(Tang et al., 2008) is an online AI-powered service designed to perform search and data mining operations on academic publications. The service aims to build a social network of academic researcher proﬁles by identifying connections between researchers, conferences, and publications using graph techniques. This service covers a number of subjects, including Mathematics, Biology, and Forestry, in addition to Computer Science. Papers with Codeis an open resource with Machine Learning papers, code, datasets, methods and evaluation tables. The Machine Learning portal includes 255,497 papers with code, 2,217 methods, and 4,948 datasets. The methods are organized in a tree structure with seven top-level categories, which are further categorized into several levels. There are other portals for the different ﬁelds of Computer Science, Physics, Mathematics, Astronomy, and Statistics, although these portals have fewer resources. Paper Readingis an index of research papers on Artiﬁcial Intelligence topics using pre-deﬁned topic tags. In includes 126 topic tags but they are not organized in any particular structure. Classiﬁcation schemes exist for other subjects, e.g., the Mathematics Subject Classiﬁcation taxonomy which covers mathematics topics, Medical Subject Headings (MeSH) for biomedical and health related topics, and Physics Subject Headings (PhySh) which covers Physics topics. These schemes are designed to help classify scientiﬁc literature on each of these respective subjects. The above mentioned existing taxonomies present ﬁne-grained but limited topics in terms of the number and coverage. To provide broader coverage of topics in CLICKER, we performed keyword extraction and analysis to generate a long candidate concept list which contains up to 10 thousand candidates. We believe that lecture notes contain clean and ﬁne-grained topics, for example, the header of a page. We started with the current version of of LectureBank (Li et al., 2019), which includes more than 2,000 lecture slides and texts, converted to textual format. We initially considered the lecture slide titles and the headers of each slide ﬁle page as a topic. We applied Textrank (Mihalcea and Tarau, 2004) to extract keywords and phrases. Finally, we ended up with a complete topic list of 4,397 candidates in descending order based on frequency. Similarly, we extracted another candidate topic list from the TutorialBank (Fabbri et al., 2018) corpus. We only conducted keyword extraction on the resource titles to keep ﬁne granularity. This resulted in a list consisting of 29,616 candidate topics. We then combined the different topics lists and sorted them by frequency. We kept the ones that appeared at least three times, which resulted in a clean list of 465 candidate topics. After some manual ﬁltering, we ended up with our current version (1.0) of the taxonomy, which includes 271 topics that cover primarily CL and NLP topics, along with some related topics from Artiﬁcial Intelligence, Speech Processing, and Information Retrieval classes, as well as some prerequisite mathematical and statistical concepts. We skipped some spurious titles such as "Reading List" Table 2: Most frequent and less frequent topics extracted from slide titles. The right hand side include 15 of the less frequent topics, randomly selected. and "Midterm Information" and manually grouped some similar-sounding and overlapping topics. We show a set of sample slide titles in Table 2. In this section, we show how to use CLICKER in two interesting applications. The ﬁrst one is to support resource classiﬁcation in an existing educational platform; the second is to take advantage of the taxonomy to learn prerequisites between concepts in a transfer learning setting. A taxonomy like CLICKER can be an important part of an online educational platform, AAN. AAN encompasses a corpus of resources on NLP and related ﬁelds. These are educational resources primarily lectures and tutorials. A total of 23,293 resources have been manually assigned to nodes of the taxonomy. The website allows users to browse resources from the top-level of the taxonomy in the interface, as shown in Figure 1. Here we illustrate the nine top-level topics. Once the user clicks on a topic, they will see the next (ﬁner) level of detail, as illustrated in Figure 2. The educational platform can support several other applications. One of them is resource retrieval. Figure 3 shows the top 5 results when a user types in the query keywordGAN, and the interface returns a list of resources based on relevancy. We utilized Figure 1: Top-level topics in AAN interface. the Apache Lucene Coreas the text search engine. For each result, the interface also shows some metadata. The Topic corresponds to the taxonomy node number. For example, the ﬁrst resource belongs to (broad) topic 72 (Deep Learning) in Figure 2. This interface supports keyword search as an alternative way of browsing resources mentioned in the previous section. 4.3 Resource Recommendation Another interesting application is to make topic and resource recommendations based on a description of an actual project that a user wants to work on. We include an example in Figure 4. The interface initially asks the user to input the title and a short abstract describing the project, as shown in Figure 4a. This sample project is about applying Transformers to do neural machine translation. Once the user submits this query, the system makes recommendations for both relevant concepts (Figure 4b) and possible resources to read (Figure 4c). We can see that the suggested topics successfully capture the main query keyword neural machine translation. Besides, some suggested resources are also relevant for the query project, i.e., No. 2 is about using seq2seq models for neural machine translation. The search function, supported by the Figure 2: Second-level topics in AAN interface. Apache Lucene Core library, sometimes misses important keywords in the abstract. In this case, the keyword (e.g.,transformer) is ignored, and no relevant resources are suggested in the top list. In the future, we plan to improve this basic recommendation function by using better keyword extraction algorithms. 4.4 Prerequisite Chain learning Prerequisite chain learning (Gordon et al., 2016) is used to help learners navigate through the space of topics within a domain by providing them with prerequisite concepts. A taxonomy like CLICKER can also help learn prerequisite chains for unknown concepts. This section discusses how to apply existing taxonomy relations to learn the prerequisite chain for unknown concepts in new domains. Existing work applies machine learning methods to solve this task by formulating it as a classiﬁcation task (Gordon et al., 2016; Li et al., 2019; Yu et al., 2020): given a concept pair A and B, A→B if A is a prerequisite concept of B. A typical method is to learn concept embeddings and conduct binary classiﬁcation on the input (A, B): the label is positive if A→B, negative otherwise. Materials used as learning concept relations include course content, video sequences, textbooks, lecture slides and Wikipedia articles (Pan et al., 2017; Li et al., 2019; Yu et al., 2020). LectureBankCD (Li et al., 2021b) is a dataset built for cross-domain prerequisite chain learning. It consists of labeled prerequisite concepts for different subjects, such as NLP (322 concepts) and Computer Vision (201 Figure 3: Resource retrieval results for the query GAN (the top 5 results). concepts). We then combine our taxonomy relations with their existing training set as the new training set. The evaluation is based on their test set. EvaluationWe followed the work of Li et al. (2021b,c) and ﬁrst trained concept embeddings. Speciﬁcally, we built a Phrase2Vec (P2V) (Artetxe et al., 2018) embedding for each concept using the resources from the same dataset with Li et al. (2021b). Then, we compared three methods: logistic regression (LR), a single-layer neural network (NN) and a variational graph autoencoder (Li et al., 2019) (VGAE). For each method, we compare our P2V embeddings (Pipeline+) and a basic pre-trained BERT model (BERT+). All classiﬁers are trained on NLP and then directly tested on CV. We show the results in Table 3: in summary, the NN model performs the best, with a large improvement over the other two models, especially with respect to Accuracy. Moreover, applying the P2V embeddings trained on resources discovered by our pipeline quantitatively improves upon the BERT model in most cases when looking at Accuracy and the F1 score. Case StudyWe keep the best model, 2-layer Neural Network, and directly apply it on CV and STATS concepts respectively, in order to reconstruct the prerequisite concept graph. In Figure 5, we show a portion of the concept graph from both domains. As can be seen, the model successfully captures correct relations, i.e., Video Classiﬁcation Figure 4: Resource Recommendation based on User Input. Table 3: Transfer learning results for prerequisite chain prediction: NLP→CV. We report Accuracy and F1 score. →Autonomous Driving and Neural Networks→ Face Recognition (Figure 5a), Conditional Probability→Variance and Conditional Probability→ Maximum Likelihood (Figure 5b). However, some of the relation predictions have room for improvement. For instance, the connection from Artiﬁcial Intelligence→Face Recognition is overestimated, given that there are in reality several additional concepts in the path between the two. Obtaining new knowledge for a speciﬁc topic within the taxonomy can be important from a learner’s perspective. To help readers get a quick understanding of this topic, a possible way is to do survey generation(Li et al., 2021a). People can Figure 5: Reconstructed concept graph from the best model. get rid of searching multiple websites, textbooks, and other web resources to learn about a new topic. Survey generation aims to generate a survey for a query topic automatically (Deutsch and Roth, 2019). Such a survey may contain a brief introduction, history, key ideas, variations, and applications in our scientiﬁc scenario. To achieve this, we followed the WikiSum (Liu et al., 2018) method to formulate this task as multidocument summarization. Given a query concept, we ﬁrst search for relevant web pages and extract them as free texts using existing search engines like Google. For each section (i.e., history, main idea), and pair the section name and extracted free texts as input to a pretrained BART (Lewis et al., 2019) model to generate a summary paragraph for each section. Table 4 shows the generated survey for the topictext summarization. As an early attempt, one may notice some contents about real facts are not always accurate and coherent. For example, in the Key Ideas section, the model suc- Table 4: Sample survey generation of the topic Text Summarization. cessfully captures the fact about extractive and abstractive summarization. But the last sentence talks about a new paper, which is irrelevant to the corresponding section. To the best of our knowledge, no other works attempt to apply neural methods to generate surveys using educational material, and our preliminary results show that this might be a very promising research direction and application. We leave improving survey quality as one of our future work. In this work, we introduced CLICKER, a practical CL/NLP classiﬁcation scheme for educational resources. We also showed how CLICKER can make a difference in ﬁve applications: educational platform, resource retrieval, resource recommendation, prerequisite chain learning, and survey generation.