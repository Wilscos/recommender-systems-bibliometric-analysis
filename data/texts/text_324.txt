Many real-world applications such as open-domain question answering [ search advertising [ text classiﬁcation (XMC) problem: given a text input, predict relevant labels from an enormous label collection of size it very challenging to design XMC models that are both accurate and efﬁcient to train. Recent works such as Parabel [ the labels to generate label partitions or hierarchical label trees (HLTs) which can be used to shortlist candidate labels to be considered during training and inference. While these methods are scalable in terms of the size of the label collection, most of them rely only on statistical representations (such as bag-of-words) or pooling from pre-generated token embeddings (such as word2vec) to vectorize text inputs. In light of the recent success of deep pretrained transformers models such as BERT [ and RoBerta [ ﬁne-tune pre-trained transformer models on XMC tasks to obtain new state-of-the-art results over the aforementioned approaches. Although transformers are able to better capture semantic meaning of textual inputs than statistical representations, text truncation is often needed in practice to reduce GPU memory footprint and maintain model efﬁciency. For example, X-Transformer truncates input texts 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Extreme multi-label text classiﬁcation (XMC) seeks to ﬁnd relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as XTransformer and LightXML, have shown signiﬁcant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the ﬁne-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively ﬁne-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XRTransformer takes signiﬁcantly less training time compared to other transformerbased XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from51%to 54%. Our code is publicly available at https://github.com/amzn/pecos. L. In these applications,Lranges from tens of thousands to millions, which makes 3], Bonsai [6], XR-Linear [7] and AttentionXML [8], exploit the correlations among 11] in various NLP applications, X-Transformer [12] and LightXML [13] propose to to contain the ﬁrst 128 tokens before feeding it into transformer models. Efﬁciency of transformer ﬁne-tuning poses another challenge for XMC applications. Directly ﬁne-tuning transformer models on the original XMC task with a very large label collection is infeasible as both the training time and the memory consumption are linear in LightXML adopt a similar approach to group byBand ﬁne-tune transformers on the task to identify relevant label clusters (instead of labels themselves). If of the ﬁne-tuning can be reduced to performance would deteriorate due to the information loss from label aggregation. Thus, both XTransformer and LightXML still choose a small constant As a result, transformers are still ﬁne-tuned on a task with around a much longer training time compared with non-transformer based models. For example, it takes X-Transformer 23 and 25 days respectively to train on Amazon-3M and Wiki-500K even with 8 Nvidia V100 GPUs. To address these issues, we propose XR-Transformer, an XMC architecture that leverages pre-trained transformer models and has much smaller training cost compared to other transformer-based XMC methods. Motivated by the multi-resolution learning in image generation [ learning [ signals and recursively ﬁne-tune the pre-trained transformer on the coarse-to-ﬁne objectives. In this paper, our contributions are as follows: Sparse Linear Models with Partitioning Techniques. ﬁxed input representations such as sparse TF-IDF features and study different partitioning techniques or surrogate loss functions on the large output spaces to reduce complexity. For example, sparse linear one-versus-all (OVA) methods such as DiSMEC [ explore parallelism to solve OVA losses and reduce the model size by weight truncations. The inference time complexity of OVA models is linear in the output space, which can be greatly improved by partitioning methods or approximate nearest neighbor (ANN) indexing on the label spaces. Initial works on tree-based methods [ (OVS) with logarithmic depth trees. Down that path, recent works on sparse linear models including Parabel [ partition labels with is logarithmic in the output space. On the other hand, low-dimensional embedding-based models often leverage ANN methods to speed up the inference procedure. For example, AnnexML [ SLICE [29 quantization variants such as ScaNN [32]. Shallow Embedding-based Methods. tectures to learn semantic embeddings of the input text. XML-CNN [ CNN on the input sequence and use the BCE loss without sampling, which is not scalable to XMC problems. AttentionXML [ better scalability to large output spaces, only a small number of positive and hard negative labels B ≈LandK ≈L, then both the training time and the memory requirement√ 17], we formulate the XMC problem as a series of sub-problems with multi-resolution label We propose XR-Transformer, a transformer based framework for extreme multi-label text classiﬁcation where the pre-trained transformer is recursively ﬁne-tuned on a series of easyto-hard training objectives deﬁned by a hierarchical label tree. This allows the transformers to be quickly ﬁne-tuned for a XMC problem with a very large number label collection progressively. To get better text representation and mitigate the information loss in text truncation for transformers, we take into account statistical text features in addition to the transformer text embeddings in our model. Also, a cost sensitive learning scheme by label aggregation is proposed to introduce richer information on the coarsiﬁed labels. We conduct experiments on 6 public XMC benchmarking datasets and our model takes signiﬁcantly lower training time compared to other transformer-based XMC models to yield better state-of-the-art results. For example, we improve the state-of-the-art Prec@1 result on Amazon-3M established by X-Transformer from 51.20% to 54.04% while reducing the required training time from 23 days to 29 hours using the same hardware. 3], eXtremeText [24], Bonsai [6], XReg [25], NAPKINXC [26,27] and XR-Linear [7] ] consider graph-based methods such as HNSW [30] while GLaS [31] considers product are used in model GPU training. Shallow embedding-based methods [ lookup followed by shallow MLP layers to obtain input embeddings. For instance, MACH [ MLP layers on several smaller XMC sub-problems induced by hashing tricks on the large label space. Similarly, DeepXML [ MLP encoders on XMC sub-problems induced by label clusters. They freeze the pre-trained word embedding and learn another MLP layer followed by a linear ranker with sampled hard negative labels from HNSW [ performance on short-text XMC problems where the number of input tokens is small [34, 35]. Deep Transformer Models. to XMC problems with promising results [ approach where the ﬁrst stage transformer-based encoders are learned on XMC sub-problems induced by balanced label clusters, and the second stage sparse TF-IDF is combined with the learned neural embeddings as the input to linear OVA models. APLC-XLNet [ on adaptive imbalanced label clusters based on label frequency similar to Adaptive Softmax [ LightXML [ namic negative sampling from the matching network trained on label cluster signals. Nonetheless, Transformer-based XMC models have larger model size and require longer training time, which hinders its practical usage on different downstream XMC problems. We assume we are given a training set y∈ {0, 1} The goal of eXtreme Multi-label Text Classiﬁcation (XMC) is to learn a function such that the largest straightforward model is one-versus-all (OVA) model: whereW = [w that maps bag-of-words (BOW) model or Term Frequency-Inverse Document Frequency (TFIDF) model, or a vectorizer with learnable parameters. With the recent development in deep learning, using pre-trained transformer as the text vectorizer has shown promising results in many XMC applications [ WhenLis large, however, training and inference of OVA model without sampling would be prohibitive due to the O(L) time complexity. To handle the extremely large output space, recent approaches partition the label space to shortlist the labels considered during training and inference. In particular, [ stage framework: partitioning, shortlisting, and ranking. First, label features are constructed to group labels into cluster. Then a shortlisting model is learned to match input x to relevant clusters in an OVA setting: Finally, a classiﬁcation model with output size L is trained on the shortlisted labels: whereS cluster is determined to be relevant to a input O(K + For transformer based methods, the dominant time is the evaluation of too small could still be problematic. Empirical results show that the model performance deteriorates when clusters are too big [ cluster will be aggregated and not distinguishable, where too big to ensure a reasonable label resolution for ﬁne-tuning. Also, as pointed out in [ transformer models on large output spaces can be prohibitive. As a result, the label clusters need to 13] ﬁne-tunes Transformer encoders with the OVA loss function end-to-end via dyis the one hot label vector withy= 1indicating that label`is relevant to instancei. f(x, `)denotes the relevance between the inputxand the label`. In practice, labels with kvalues are retrieved as the predicted relevant labels for a given inputx. The most , . . . , w] ∈ Rare the weight vectors andΦ : D 7→ Ris the text vectorizer xtod-dimensional feature vector.Φ(·)could be a deterministic text vectorizer, such as the KclustersC ∈ {0, 1}whereC= 1denotes that label`is included in thek-th (x) ⊂ [L]is the label set shortlisted byg(x, ·). In the extreme case where only one label ), which in the best case scenario is O(L) when K =L. Figure 1: Illustration of ﬁne-tuning on XMC tasks of different label resolutions. For an XMC task with a low label resolution, ﬁne-tuning can be fast but model performance might deteriorate due to large deviation from the original XMC task. In practice, X-Transformer and LightXML adopt a XMC task with a relatively higher label resolution to ensure reasonable model performance at the cost of longer training time. The proposed XR-Transformer leverages multi-resolution learning and model bootstrapping that achieves both fast ﬁne-tuning and good model performance. be constructed in a way to balance the model performance and ﬁne-tuning efﬁciency. In practice, both the transformer-based XMC models, such as X-Transformer and LightXML, adopt a small ﬁxed constant as the cluster size still very time consuming as the number of clusters K ≈ L/B. As noted above, the shortlisting problem sizewhere recursively on the shortlisting problem until a reasonably small output size is reached can therefore follow the curriculum learning scheme and ﬁne-tune the pre-trained transformers progressively on the sub-XMC problems with increasing output space ﬁne-tuning task, the candidate label set is shortlisted by the ﬁnal model at the previous task. The recursive shortlisting ensures that for any input, the number of candidate labels to include in training and inference is leverage the multi-step ﬁne-tuning and use the embedding generated at the previous task to bootstrap the non pre-trained part for the current task. We now describe the model design in detail. Hierarchical Label Tree (HLT). building a HLT [ applying text vectorizers on label text or from Positive Instance Feature Aggregation (PIFA): whereΦ : D 7→ R and use balanced k-means ( top-down fashion. The HLT is represented with a series of indexing matrices C∈ {0, 1} HLT can be built from bottom up through joining B adjacent clusters together. Multi-resolution Output Space. such as computer vision [ results in better image quality for generative adversarial networks [ in meta learning, [ classes. Nevertheless, multi-resolution learning has not been well-explored in the XMC literature. In XR-Transformer, we leverage the label hierarchy deﬁned by the HLT and train the transformer model on multi-resolution objectives. The XMC task can be viewed as generating an 1-D image input text Bis the cluster size. In XR-Transformer, we apply the same three stage framework O(B)and therefore the total number of considered labels isO(B log(L)). Also, we 41] of depthD. We ﬁrst construct label featuresZ ∈ R. This could be done by x. Just like a coarsiﬁed image could be obtained by a max or mean pooling of nearby pixels, the coarse label vector can be obtained by max-pooling of labels which are nearby in the label feature space. Once the HLT is constructed using label features, the true labels at layer can be determined by the true labels of the child clusters at resolution and can be used to generate learning tasks with easy-to-hard objectives. Direct use of the binarized several positive labels into one cluster. Ideally, a cluster containing several positive children is more relevant than a cluster with only one positive child. To add this lower level information to higher level learning objectives, we introduce the relevance matrix sub-problem where Different from cost-sensitive learning [ one cost matrix explicitly derived by evaluation metrics such as F1 score, in XR-Transformer, we consider the usage of cost-sensitive learning where the relevance matrices are recursively induced by the HLT structure. Speciﬁcally, given an HLT, we recursively construct relevance matrices for t = 1, . . . , D: and R where α is the hyper parameter to balance positive and negative weights. Label Shortlisting. clusters that have high chance of being positive. A necessary condition for a label at layer positive is that its parent label at level only train on the output space shortlisted by positive clusters of the parent layer. However, in practice we found this approach sometimes leads to sub-optimal result during inference with beam search. As an effort to balance explore and exploit, we further include the topthe model learned on the parent layer to mimic the beam search during inference. Thus at layer labels considered during training are shortlisted by the parent layer t − 1: where the row. For each instance We can therefore deﬁne a series of learning objectives for level t ∈ {1, 2, . . . , D} as: whereL model weights to be learned. Text Representation. two ways: statistical feature representations and semantic feature representations. Although the latter, in particular transformer models, have shown promising results on various NLP benchmarks, the self-attention mechanism makes transformers unscalable w.r.t. sequence length. To ensure efﬁciency, input texts are usually truncated [ statistical features, such as TFIDF, are fast to construct with the whole input taken into consideration. In XR-Transformer, we use a combination of these two feature representations and each component is a complement of lost information for the other one: = yis the original label matrix. This forms a series of learning signals with coarse-to-ﬁne = Y. Motivated by [48], we adopts the row-wise `normalized relevance matrix: Top(·, k)operator zeros out elements in a matrix except the top-klargest values in each is a point-wise loss such as hinge loss, squared hinge loss or BCE loss,W,Θare the Algorithm 1: Iterative_Learn(X, Y, C, θ, P) Input :X, Y, C, θ, P m, n ← C.shape if n = 1 then Algorithm 2: XR-Transformer training Input :X, Y, pre-trained transformer Φ Generate label hierarchy { θ= θ, P = None for t in 1, 2, 3, ··· , P ← Top( Generate label hierarchy {Y Fix θ, P = None for t in 1, 2, 3, ··· , D do P ← Top(W Return :Φ whereΦ predictions can be made by simply applying a linear projection on top of the text representation through (1). Training with bootstrapping. preliminary HLT is constructed using raw statistical features. Then a pre-trained transformer model is ﬁne-tuned recursively from low resolution output to high resolution. At each layer objective θdenotes the pre-trained transformer weights. Unlike the transformer warmed-up with pre-trained weights, the projection weights from scratch without good initialization. At the beginning of ﬁne-tuning, gradient ﬂow through these cold-start (usually randomly initialized) weights will usually worsen the pre-trained components. ← binarize(YC) , θ k; where v=Φ(x), ∀ ` ∈ [L] ← k-means-clustering(ˆZ) ← Iterative_Learn(X,ˆY,ˆC, θ, P) k; where v=PΦ(x), ∀ ` ∈ [L] ← k-means-clustering(Z) , _ ← Iterative_Learn(X, Y, C, θ, P) (·, θ), {C}, {W} (·, Θ)is the transformer parametrized byΘ. Once the text representation is constructed, (9)is optimized with initializationΘ = θthe best transformer weights of layert − 1. We leverage the recursive learning structure to tackle this issue by model bootstrapping. Concretely, Wis initialized as: In practice, Φ(X, θ a variety of parallel linear solvers, such as LIBLINEAR [49]. Once the ﬁne-tuning is complete, the reﬁned HLT is constructed with the text representation that combines statistical text feature and ﬁne-tuned semantic text embeddings. Then the ranking models are trained on top of the combined text features for the ﬁnal prediction. The detailed training procedure is described in Algorithm 1 and 2. Inference. transformer embedding and to retrieve relevant labels through beam search. Therefore, the inference time complexity is dimension and T is done with beam search through the reﬁned HLT, the transformer text embedding only need to be computed once per instance. Connections with other tree based methods. Although methods such as AttentionXML [8] also train on supervisions induced by label trees, the ﬁnal model is a chain of sub-models which each on is learned on single-resolution. In particular, given a hierarchical label tree with depth will train transformer encoder progressively on all layers of the tree. This difference leads to a longer inference time for AttentionXML than XR-Transformer since multiple text encoders need to be queried during inference, as shown in the comparison in the inference time in Appendix A.4.2. We evaluate XR-Transformer on 6 public XMC benchmarking datasets: Eurlex-4K, Wiki10-31K, AmazonCat-13K, Wiki-500K, Amazon-670K, Amazon-3M. Data statistics are given in Table 1. For fair comparison, we use the same raw text input, sparse feature representations and same train-test split as AttentionXML [ (P@k), which is widely-used in XMC literature [ Precision@k (PSP@k) are defer to Appendix A.4.3, which focus more on tail labels’ performance. Table 1: Data statistics. respectively. average number of instances per label. six publicly available benchmark datasets, including the sparse TF-IDF features are downloaded fromhttps://github.com/yourh/AttentionXML Transformer [12] and LightXML [13] for fair comparison. Baseline Methods. AnnexML [ CNN [33 (11)is fast to compute since the semantic text feature for the previous layer )is already computed and thus(11)can be solved very quickly on CPUs with The inference cost of XR-Transformer consists mainly of two parts: cost to compute Ddifferent text encoders on each layer of the tree where as XR-Transformer trains the same L: the number of labels.¯L: the average number of positive labels per instance.¯n: 28], DiSMEC [18], PfastreXML [41], Parabel [3], eXtremeText [24], Bonsai [50], XML], XR-Linear [7], AttentionXML [8], X-Transformer [12] and LightXML [13]. We obtain Table 2: Comparison of XR-Transformer with recent XMC methods on six public datasets. Results with a trailing reference are taken from [ AttentionXML matrix provided in [ The PSP@k results are available in Appendix A.4.3. eXtremeText [24] 79.17 66.80 56.09 83.66 73.28 64.51 92.50 78.12 63.51 XR-Transformer eXtremeText [24] 65.17 46.32 36.15 42.54 37.93 34.63 42.20 39.28 37.24 Table 3: Comparing training time (in hours) of DNN-based methods that produce the SOTA results in Table 2. The number following the model indicates the number of ensemble models used. most baseline results from [ algorithms [ AWS p3.16xlarge) and the same inputs (i.e., raw text, vectorized features, data split) to obtain the results of AttentionXML, X-Transformer and LightXML. The hyper-parameter of XR-Transformer and more empirical results are included in Appendix A.3. Model Performance. Table 2 and Table 8, respectively. The proposed XR-Transformer follows AttentionXML and LightXML to use an ensemble of 3 models, while X-Transformer uses an ensemble of 9 models [ More details about the ensemble setting can be found in Appendix A.3. The proposed XR-Transformer ,LightXML,X-TransformerandXR-Transformeron the same vectorized feature 8,12,13]. To have fair comparison on training time, we use the same hardware (i.e., Table 4: Single model comparison of DNN based XMC models. Training time on p3.16xlarge with 8 Nvidia V100 GPUs whereas time on single Nvidia V100 GPU T framework achieves new SOTA results in and P@k), and outperforms competitive methods on the large datasets. Next, we show the training time of XR-Transformer is signiﬁcantly less than other DNN-based models. Training Cost. ison, all the experiments are conducted with ﬂoat32 precision on AWS p3.16xlarge instance with 8 Nvidia V100 GPUs except for LightXML, which was run on single V100 GPU since multi-GPU training is not implemented. XR-Transformer consumes signiﬁcantly less training time compared with other transformer based models and the shallow BiLSTM model AttentionXML. On Amazon3M, XR-Transformer has 20x speedup over X-Transformer while achieving even better P@k. Finally, in table 4, we compare XR-Transformer with LightXML under the single model setup (no ensemble), where XR-Transformer still consistently outperforms LightXML in P@k and training time. Table 5: Comparing XR-Transformer with Pre-Trained and word2vec embeddings concatenated with TF-IDF features. Comparison of Different Semantic Embeddings. the improvement in performance comes from better semantic embedding rather than the introducing of TF-IDF features, we further tested models using Pre-Trained Transformer and word2vec embeddings concatenated with the same TF-IDF features. Table 5 summarizes the performance of these models on all 6 datasets. In particular, word2vec is using token embedding from word2vec-google-news-300 and for Pre-Trained we use the same setting as XR-Transformer (3-model ensemble). On large datasets such as Wiki-500K/Amazon670K/Amazon-3M, Pre-Trained +TF-IDF has marginal improvement compared to the baseline TF-IDF features. Nevertheless, our proposed XR-Transformer still enjoy signiﬁcant gain compared to Pre-Trained +TF-IDF. This suggests the major improvement is from learning more powerful neural semantic embeddings, rather than the use of TF-IDF. Table 8 shows the training time for these DNN-based models. To have fair compar- Effect of Cost-Sensitive Learning. on four XMC datasets with the largest output spaces. On most datasets, cost sensitive learning via aggregated labels yields better performance than those without. We also show that cost-sensitive learning is not only beneﬁcial to XR-Transformer, but also useful to its linear counterpart XRLinear [7]. See Appendix A.4.1 for more results. Table 6: Ablation of cost-sensitive learning on the single XR-Transformer model with or without Cost Sensitive (CS). Precision@1,3,5 P(@k) and Recall@1,3,5 (R@k) are reported. Effect of Label Resolution and Text Representation. of label resolution on the quality of the ﬁnetuned transformer embeddings. We ﬁnetune transformer models in a non-recursive manner on a two layer HLT with different leaf cluster size. Then the ﬁne-tuned transformer embeddings are used along or in combination with TF-IDF features to produce the predictions with reﬁned HLT. From Figure 2 we can observe that a larger cluster size will result in worse semantic features. Figure 2 also shows that combining semantic features features the model performance. In this paper, we have presented XR-Transformer approach, which is an XMC architecture that leverages multi-resolution objectives and cost sensitive learning to accelerate the ﬁne-tuning of pre-trained transformer models. Experiments show that the proposed method establishes new stateof-the-art results on public XMC datasets while taking signiﬁcantly less training time compared with earlier transformer based methods. Although the proposed architecture is designed for XMC, the ideas can be applied to other areas such as information retrieval or other DNN models such as CNNs/ResNets. Also, more extensive study is required to understand why the coarse-to-ﬁne scheme would lead to not only faster training but better overall quality. A hypothesis is that the problem is being solved at multiple scales hence leading to more robust learning of deep transformer models.