Abstract—In this paper, we detailedly describe our solution for the IEEE BigData Cup 2021: RL-based RecSys (Track 1: Item Combination Prediction). We ﬁrst conduct an exploratory data analysis on the dataset and then utilize the ﬁndings to design our framework. Speciﬁcally, we use a two-headed transformerbased network to predict user feedback and unlocked sessions, along with the proposed session-aware reweighted loss, multitasking with click behavior prediction, and randomness-in-session augmentation. In the ﬁnal private leaderboard on Kaggle, our method ranked 2nd with a categorization accuracy of 0.39224. Index Terms—recommender system, item combination prediction, transformer, loss reweighting The task of the IEEE BigData Cup 2021: RL-based RecSys (Track 1: Item Combination Prediction) [1], [2] is to predict each user’s purchasing feedback to nine exposed items, given this user’s click history, portrait features, and items’ features, which is similar to bundle recommendation [3]. The special setting in this task is that the nine items are grouped into three sessions. The user can only unlock the subsequent session after he/she buys all three items in the current session. More formally, given a user u (along with his/her clicking history c, c, ..., and some portrait features f, f, ..., f), and his/her nine exposed items i, i, ..., i f, f, ..., ffor each item i), the objective is to predict nine interactions y, y, ..., y∈ {0, 1}. Each one of the interactions indicates whether this user would buy the corresponding item or not. In addition, in this scenario, the middle three items i, i, iare not unlocked until the user has bought all of the ﬁrst three items i, i, i, and similarly, the last three items i, i, iare not unlocked until the user has bought all of the ﬁrst six items i, i, ..., i(c.f. Figure 1). The evaluation metric for this task is the Categorization Accuracy measure, which is deﬁned as follows, Fig. 1: Problem Setup. Each user is exposed to nine items simultaneously. However, the items are divided into three 3length sessions. The user can only unlock the subsequent three items after he/she buys all three items in the current session. We want to predict whether a user would buy the nine exposed items or not. where M denotes the number of users, yand ˆyare the predicted and ground-truth interactions, and [y= ˆy] is the Iverson bracket. Overall speaking, this task is challenging in two aspects. differently by the users. We cannot simply apply a single traditional recommendation method to predict each interaction independently. to correctly predict all of the nine interactions of a user, while partially correct predictions contribute nothing to the ﬁnal score. To overcome the above challenges, we propose a delicate two-headed transformer-based framework to predict both users’ buying behavior and unlocked sessions. The unlocked session prediction can be used to reﬁne unreasonable buy predictions. We further propose a randomness-in-session augmentation technique and a novel session-aware reweighted loss to address the unique characteristics in this scenario. Finally, a multi-tasking training procedure with click prediction is utilized to assist the learning of embedding layers. Extensive experiments and ablation studies have demonstrated the effectiveness of our method. In what follows, we will discuss related works in Section II, conduct an exploratory data analysis in Section III, describe our proposed method in Section IV, and ﬁnally conclude the paper with discussion and future works in Section V. Recommender systems aim to ﬁlter information for users, which has become one kind of fundamental service in today’s information platforms [4]. Generally, from the perspective of real-world application, the recommender systems contain two stages, matching and ranking. Recently, deep learning has become the state-of-the-art solution of recommender systems in both two stages [5]–[7]. As for the matching stage, of which the mainstream methods are collaborative ﬁltering [8], which learns user interests from historical behaviors, deep neural networks methods [9], or even graph neural networks [10], [11], achieve promising performance. As for the ranking stage, which is also known as click-through rate (CTR) prediction, deep learning-based models such as DeepFM with multilayer perceptron [12], xDeepFM with compressed interaction network [13], DIN [14] with attention mechanisms, etc., are demonstrated effective in learning from complex features of users and items. In this work, we develop a method based on transformer network, a recent advance of neural network with extraordinary achievements in many areas, for capturing the complex behavior of users in the task of item combination recommendation. Before diving into the model design, we conduct exploratory data analysis ﬁrsthand to master the whole picture of the dataset. Table I shows the overall statistics of this dataset. In total, there are 381 items. There are 260,087 buying entries for training and 206,254 buying entries for testing. These entries are also accompanied by 10,435,798 and 8,357,719 clicking logs, respectively. We will then analyze more details about the clicking and buying behavior of users in the following. Table II shows how many clicks and buys do items in each session possess. It’s worth noticing that an item would only appear in its speciﬁc session. We can see that items in later sessions are with more types, and items with earlier sessions possess more clicks and buys. This is reasonable since users need to buy early items in order to unlock items (with higher prices) in the later sessions. TABLE II: Click and buy statistics in different sessions. Fig. 2: Histogram of the number of buys of each user. Due to the dataset characteristics (c.f. Section I), we plot the histogram of the number of items each user bought in Fig. 2, and classify users into four groups according to the number of items they have bought as follows, We can see that a decent population (Group-0) didn’t buy anything, the number of users who bought 4∼6 items (Group2) are the fewest, and a large portion of users (Group-3) chose to buy no less than seven items. This indicates an hourglass shape of user distribution. It’s also worth noticing that very few people buy three or six items (c.f. Fig. 2). We hypothesize that this is because the main reason why a user buys three or six items is to unlock and buy items in the next session. We plot the histogram of the number of clicks of each user in Fig. 3. There are 28,184 users who did not click anything. However, we do see that the majority of users are with a decent number of clicks, which motivates us to utilize the clicking logs to assist the training. We further present user portrait features and item features in Table III and Table IV. We can see that all user portraits are discrete features, while two of the item features are continuous features. The overall structure of our method is shown in Fig. 4. In what follows, we will introduce each part of our framework. The ablation study results are shown in Table V. TABLE V: Experimental results of different models (take G as an example, it is built on F with an additional design of augmentation). The numbers in this table are ablation studies after the competition. * means the settings of the best submission during competition.  means the settings are providing unstable yet higher scores. Fig. 3: Histogram of the number of clicks of each user. A. Network design basic network. The network takes the following inputs: user proﬁle features, user clicked items’ id and features, nine exposed target items’ id and features. These inputs are processed by their corresponding embedding layers, and further fed to an MLP module. Then the network predicts whether the user will buy the nine exposed target items. We propose this framework since the nine items’ labels are correlated. For example, users might buy all of the ﬁrst six items, only to unlock and buy subsequent items. Therefore, it is not suitable to predict the nine feedback independently, and we need to ensure the network is able to predict nine feedback simultaneously. The training of the model is supervised by a vanilla binary cross entropy (BCE) loss on each item respectively as follows, where ˆyand ydenote the ground-truth and predicted feedback between user u and the exposed j-th item, respectively, and BCE(ˆy, y) = − ˆylog y(3) is the binary cross entropy term for each one of the nine items. We set the embedding size to 16 here, and the MLP-structure is set to {1440, 256, 64, 9}. This very simple basic model can achieve 0.29169 on validation set, and 0.33817 on test set. prevent over-ﬁtting and make training more robust, we randomly shufﬂe items’ orders within the same sessions during the training. Note that in this scenario, users are not sensitive to the items’ order within the same session. However, our network treats them with different parameters. So we propose to use this augmentation technique to alleviate this shortcoming. This strategy is also used for test time augmentation, where original prediction and predictions produced by shufﬂed inputs are averaged to produce the ﬁnal results. In our experiments, augmentation in training (Conﬁg-B) increases the score from 0.29169 to 0.29965 on validation set and from 0.33817 to 0.35007 on test set. However, this proposed augmentation method in inference is not so stable and sometimes might do some harm to the score. In Table V, we are just reporting the result of one experiment (Conﬁg-G), which improves the score. MLPs [14], [15], we switch the backbone part into a transformer [16], as their self-attention mechanism is proved to be effective on capturing inter-relations between different features. z=xE; xE; · · · ; xE+ E, z= MSA (LayerNorm (z)) + z, ` = 1 . . . L(4) z= MLP (LayerNorm (z)) + z y = LayerNorm (z) , where xis the corresponding one-hot vector of features, E ∈ R, E∈ R, N is the number of features, D is the embedding size, L is the number of transformer layers, and MSA(·) is the multi-head self attention. We set the embedding size to 128, number of layers to 3, number of selfattention head to 4, and the sizes of q,k,v in the self-attention module to 32, and MLP size to 64. Using the transformer backbone (Conﬁg-C) can improve our score from 0.29965 to 0.31140, and from 0.35007 to 0.36210 on validation and test set, respectively. However, we do note that this improvement compared to Conﬁg-B might in part come from a larger embedding and network size. We didn’t do that ablation study due to limited time. should notice that the above simple framework might introduce some invalid buy predictions that are impossible to happen in the real world. For example, the network might predict that the user buys two items, the 1st one and the 9th one. However, this is impossible since the user has to buy all of the ﬁrst 6 items in order to buy the 9th item. Thus, in addition to the buy prediction, we propose to also predict the group (as deﬁned in Section III-B) of each user, which forms a two-headed prediction network, as shown in Fig. 4(a). This group prediction part is supervised by a cross entropy loss as follows, where ˆg∈ Ris a one-hot ground-truth vector indicating which group user u belongs to. Here g∈ Ris the predicted group vector (after a softmax layer). The loss is added with previous ones and back-propagated together as follows, where we set λ= 0.8, λ= 0.1. After training, the predicted group vector gwill be used to reﬁne and ﬁx the unreasonable predicted buying behavior of the nine exposed items y∈ Ras follows, y=[1, 1, 1, y, y, y, y, y, y] arg maxg= 2 [1, 1, 1, 1, 1, 1, y, y, yg= 3. After reﬁned using the group predictions, our score improves from 0.31140 to 0.31475 on validation set and from 0.36210 to 0.36258 and test set. model users’ buying behaviors, we classify the nine exposed items into four types (weak positive, strong positive, strong negative, weak negative) as shown in Fig. 5. items should be treated as weak positives, as the user might buy these items only to unlock the later sessions. treated as strong positives and strong negatives. As the user unlocked and stopped in this session, items bought or not bought should be classiﬁed as strong signals. negatives, as users haven’t unlocked these sessions, we should not assume too strong preferences on these items. In practice, we assign different weights λ, λ, λ, λfor the above 4 types of items. The formally deﬁned loss can be written as follows, λΓˆg= [1, 0, 0, 0] where Γand Λdenote losses for weak positive/negative items and strong positive/negative items, respectively, which are formulated as follows, In our experiments, we replace the original Lwith L, and set λ= 0.5, λ= 1, λ= 1, λ= 0.5. This design can greatly boost our score from 0.31475 to 0.33090 on validation set, and from 0.36258 to 0.38355 on test set. from the buy prediction network described in Fig. 4(a), we propose to use another click prediction auxiliary network (Fig. 4(b)) to assist the learning procedure. Note that the two networks share the same embedding layers. The click prediction network takes the following inputs: user proﬁle features, the previously clicked items’ id and features, target items’ id, and features. It is trained to predict whether the user will click the target item or not. The loss function is deﬁned as follows, where ˆc, care groundtruth and predicted feedback from user u to his/her target item, and is the binary cross entropy term. The loss is added with previous ones and back-propagated together as follows, where we set λ= 0.8, λ= 0.1, λ= 0.1, and use the same network hyper-parameters as the buy prediction network here. With the auxiliary click prediction network multi-tasking, our score is improved from 0.33090 to 0.33323 and 0.38355 to 0.38805 on the validation set and test set, respectively. competition (0.33687 on validation set, 0.39224 on test set) is achieved by Conﬁg-F, as shown in in Table V. That training instance shows much better performance than our ablation studies conducted after the competition. However, these methods are still suffering from the performance variances with different random seeds, which may be caused by the scale of the dataset. We leave the efforts to address the issue of unstable performances as future work. Although the competition guidelines want us to recognize each buying entry as an individual user, we notice that there are entries with identical clicking histories and user portrait features (which means the same user produces two entries). Thus, it is more proper to split train and validation sets while taking the above observation into consideration. We propose to view all entries with identical user portrait features as the same user and use 85% users as train set and the rest 15% users as the validation set. This results in 243,775 and 16,312 entries for the train set and validation set, respectively. We use Adam [17] with default hyper-parameters in PyTorch [18]. The batch size is set to 32, and the learning rate is set to 1e-2 for ten epochs. Colab with one P100 GPU is used as our training platform, and each model takes about 2∼3 hours to train. Clicking data in the test set of both track1 and track-2 are used during our training. Checkpoint with the best score on the validation set is used for evaluation. All continuous features are discretized into bins. In this paper, we propose a framework for item combination prediction. Speciﬁcally, we propose several delicate designs to improve the performance, namely randomness-in-session augmentation, transformer backbone, two-headed prediction, session-aware loss reweighting, and multi-tasking with click prediction. Extensive experiments have proved the effectiveness of our framework. We have also tried several things that conceptually make sense but did not improve the score. Firstly, we tried an attention-like deep interest network [14] to reweight user clicked items, however, it didn’t improve the ﬁnal score. Given that we do not know how click data is collected, we think that users might present different preferences in the scenario where click data is collected. And thus, making the model more complex in this aspect doesn’t help. Secondly, we tried to add user embedding into the network yet encountered severe overﬁtting in training. Adding mini-batch aware regularization [14] can reduce over-ﬁtting, however, it still cannot make improvements to the ﬁnal score. Due to the fact that most users only have one training entry, this result is not very surprising. In addition, we tried adding timestamp as a feature, however, it also didn’t help. We originally thought that weekends or holidays might affect user behaviors. Future works shall include in-depth analysis and utilization with the actual meaning of user features, item features, and clicking data. It would also be interesting to investigate other network architectures that could address the multi-feedback item combination prediction scenario. Since our work does not introduce the model ensemble technique, it is also a promising direction for future works.