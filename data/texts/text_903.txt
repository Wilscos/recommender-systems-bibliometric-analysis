The Matérn kernel, widely used in Gaussian Processes (GP) statistical modeling, contains the modiﬁed Bessel function of the second kind, which is a special function. Special functions lack a formal deﬁnition and subsequently robustness of implementation, even without considering automatic differentiation. We suggest the simple mnemonic that a function can be considered special if it is not algebraic or lacks a predeﬁned universal implementation. The most problematic are functions such as the Bessel or the Anger function, or hypergeometric functions, which typically require dedicated library implementations, e.g. [ These libraries, however, focus entirely on special function evaluations, may offer few implementation details, and may provide inaccurate results when automatically differentiated. Previous work on the automatic differentiation (AD) of certain special functions required the development of entirely new tools, e.g. [ compute reliable derivatives. Very niche special functions, or less common derivatives, may not be available in automatic differentiation codes for special functions, which leaves a user with the single option of employing ﬁnite differences on special function evaluations. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. Argonne National LaboratoryRutgers University oanam@anl.govchristopher.geoga@rutgers.edu To target challenges in differentiable optimization we analyze and propose strategies for derivatives of the Matérn kernel with respect to the smoothness parameter. This problem is of high interest in Gaussian processes modelling due to the lack of robust derivatives of the modiﬁed Bessel function of second kind with respect to order. In the current work we focus on newly identiﬁed series expansions for the modiﬁed Bessel function of second kind valid for complex orders. Using these expansions we obtain highly accurate results using the complex step method. Furthermore, we show that the evaluations using the recommended expansions are also more efﬁcient than ﬁnite differences. Bessel functions are widely used in electromagnetics where they contribute to the kernel of the Helmholtz equation, or statistics where the modiﬁed Bessel functions enter the Matérn covariance kernel. The modiﬁed Bessel functions are the solution pair differential equation known as the modiﬁed Bessel functions of ﬁrst kind, Fig. 1. According to the nature of the order range of series expansion expressions are available for the solution pair the argument may differ signiﬁcantly from large argument expressions [ implemented as a truncated series expansion, these functions exhibit a high sensitivity to round-off errors, or platform and compiler variations, albeit only at computer precision level. Derivatives with respect to argument Bruno generalization of Taylor series, see [ are considerably more intricate both mathematically and numerically. For the Bessel function of second kind we note in Fig. 1 that the function has a pole at zero, and decays exponentially with increasing argument to order expressions are available for certain derivatives of special functions, in particular for integer orders, tolerances, interlacing analytical expressions with numerically sensitive expressions can lead to numerical artifacts increasing with the differentiation order automatic differentiation (AD) of a power series truncated at a ﬁxed level is not guaranteed to yield a derivative sufﬁciently converged at the same truncation level. To this end ﬁnite difference derivatives may provide superior accuracy in certain cases [ calculations is highly prone to round-off errors due to cancellations. These considerations indicate that differentiable programming of special functions is ﬁrst and foremost a mathematically unresolved issue. Differentiating through iterative processes, which also have a tolerance imposed convergence, has been considered within the AD literature, [ however, it has not been applied for series expansions. Furthermore, this approach may induce a computational overhead via extra computations if the differentiated series has very slow convergence. A notable strategy that alleviates the round-off error effects of ﬁnite differences, while preserving the computational complexity of forward mode automatic differentiation is the complex step method [ Provided the intricacy of the mathematical expression in complex space the complex step method has been largely disregarded as a suitable approach for special functions. With the advent of neural network models, and their inherent automatic differentiation companion, special function neural network models (SFNN) were considered as a stand-in for series expansions [ are a very promising approach, the lack of uniqueness of neural network models, and subsequently possible portability issues across platforms still prevents their use in production codes. In the current work, and motivated by statistical modelling problems, we focus on robust derivatives in Sec. 2, while in Sec. 3 we present and compare different differentiation approaches. Numerical results, outlining the trade-off between accuracy and efﬁciency of implementation are illustrated in Sec. 4. Considering that special functions are mostly encountered as kernels of integral equations, which is traditionally an academic ﬁeld, the computational cost of multi-precision computations at small scales has been acceptable. The surge of statistical modelling, particularly for large data sets, revives the demand for robust implementations of special function derivatives. Statistical modelling: Matérn Class Covariance Functions ubiquitous throughout the physical and numerical sciences. They are incredibly convenient in many ways, for example providing linear conditional expectations and being speciﬁed entirely by their ﬁrst www.advanpix.com/2016/05/12/accuracy-of-bessel-functions-in-matlab/ xgives rise to different asymptotic expansions, and small argument expressions [1] νat a sufﬁciently decayedxwe operate entirely at computer precision level. Analytical (x) = −K(x) −K(x)/x, but not all. Given that implementations depend on truncation for the entire spectrum of real positiveνandx. A background on the motivation is expanded two moments. A primary problem in the study of GPs is to properly specify the covariance function C, indexed by parameters x and x under that model. Among other purposes, the covariance function is crucial for determining the behavior of interpolants and forecasts, both with respect to predicted values and inferred second-order information. Arguably the most popular covariance function arising in variety of applications in the physical sciences is the Matérn class of covariance functions [13], given by The Matérn class holds an advantage over other positive-deﬁnite functions, such as the squared exponential or rational quadratic, by providing complete control of the degree of smoothness of Kat the origin. In turn this determines the number of mean-square derivatives sample paths will have. Further, in the common sampling regime of ﬁxed-domain asymptotics, where measurements are made more and more densely in a ﬁxed spatial domain, the mean-square differentiability of a process is one of the few quantities that is actually resolved better as more data is added to a sample. For more details, we refer readers to [ on equivalence and orthogonality of Gaussian measures. The optimization required to perform maximum likelihood estimation for Gaussian processes is computationally challenging, and the required derivatives of computed. The likelihood itself in the mean-zero case for data y is given by where Σ(θ) Thus, one requires the derivatives the gradient of the likelihood. Beyond the general difﬁculty of being nonlinear, the likelihood surface for many models is nearly ﬂat along level surfaces of certain nonlinear functions of several parameters due to them having similar interpolation properties (see [ surface). While it is clearly valuable to ﬁt the smoothness parameter to data, as opposed to the current practice of ﬁxing it ahead of time, an effective optimization demands reliable derivatives of C Various implementations and recommendations can be found in the literature of special functions. The NIST compendium [ see [6, Eq.10.38.2], considered valid for any real and different tracks have to be explored. Encouraged by earlier work in the space of automatic differentiation, which considers numerical analysis aspects to automate robust implementations method. Complex step method: appeal to automatic differentiation, see [ in complex space yields a reliable derivative evaluation. Consider the expansion around x Restricting the discussion to ﬁrst order derivatives, although higher orders are also possible, we have −2(∇`(θ))= trΣ(θ)∂∂θΣ(θ)− yΣ(θ)∂∂θΣ(θ)Σ(θ)y. ;7] we identify two series expansions which can be easily evaluated using the complex step f(x+ ih) = f(x) + ih∇f(x) −h2!∇f(x) − ih3!∇f(x) . which by equating the purely imaginary parts, provides a derivative as Note that Eq. 2 involves the evaluation of a complex number, however only one division by a small which eliminates the effect of cancellations crippling ﬁnite difference evaluations. The same Taylor expansions considerations are used in deriving ﬁnite difference schemes. The expression in Eq. 2 will yield a method of accuracy by having removed the subtraction of two very similar terms, and obtained a single evaluation the accuracy order is not the case for ﬁnite differences which become very sensitive to ﬂoating point errors at values belowh ≈ 10 compiler level using dual-types since a single evaluation sufﬁces for obtaining the derivative. Small arguments: manipulations and substitutions, using formulas in [ tions or ratios of close to zero-valued functions. We identiﬁed a numerically robust formula for the modiﬁed Bessel function valid for ν complex given by K(x) = whereΓ(·) up to argument values complex order for a preset and we simply have To implement this inﬁnite sum we accumulate terms for the summation index tolerance as a stopping criterion, implying the sum is truncated at a certain level N. Large arguments: in terms of the conﬂuent hypergeometric function U, also known as Tricomi’s function which can be written in terms of the second linearly independent conﬂuent hypergeometric function M, also now as Kummer’s function: where M can be computed via the generalized hyper-geometric series which requires truncation, and in practice N ≈ 10 provided close to computer precision accuracy. In Fig. 2 we compare the accuracy of the complex step series implementation to a ﬁnite differences approach, and a direct naive implementation of [ norm with respect to a derivative computed in multi-precision to serve as ground truth. The series expansion derived in Eq. 3 is highly stable and accurate, and superior to ﬁnite differences for arguments implementation and the complex step method approach gain more correct digits, see Fig. 2b, with the complex step being computer precision accurate. O(h)is assured to be free of round-off errors forhdown to computer precision. This . Moreover, the implementation of the complex step method can be accelerated at 2k!2Γ(ν)2Γ(1 + k − ν)+ Γ(−ν)2Γ(1 + k + ν) denotes the complex Gamma function. This series expansion is, however, accurate only U(a, b, z) =Γ(1 − b)Γ(a + 1 − b)M(a, b, z) +Γ(b − 1)Γ(a)zM(a + 1 − b, 2 − b, z), = 1, anda= a(a + 1)(a + 2) ···(a + n − 1). Note that Eq. 4 is also an inﬁnite series, x < 10, as in Fig. 2a. For increasingly large argumentsx > 10both the ﬁnite difference Efﬁciency considerations are assessed in Tab. 1, where we note the complex step method displays superior computational efﬁciency to second-order adaptive ﬁnite difference algorithm, using FiniteDifferences.jl Figure 2: Comparison of complex step (black markers), ﬁnite differences (red markers), naive implementation of [6, Eq.:10.38.2] (blue markers). ν = 0.25 1.39 0.98 1.52 1.32 1.63 1.60 1.74 1.70 2.88 1.96 ν = 0.56 1.42 0.93 1.56 1.28 1.65 1.57 1.77 1.72 3.03 1.80 ν = 0.88 1.37 0.93 1.63 1.55 1.75 1.54 1.87 1.82 3.20 1.78 ν = 1.19 1.39 0.93 1.54 1.45 1.77 1.76 1.90 1.82 3.00 1.79 Table 1: Timing comparisons in microseconds for the computation of derivatives (CS) with ﬁxed step h = 10 These expressions, although designed to be amenable to the complex step method, proved to have a superior performance also under black-box AD, as we show in [ have to specify the expressions provided here are not valid for edge-cases have to compute limit values. These limits are easy to derive and use in the complex step method, since they can be implemented as discrete values via ported to AD implementations which can assure higher efﬁciency. The current work assesses state-of-the art strategies for differentiating Bessel functions with respect to order. The focus is on the modiﬁed Bessel function of second kind, relevant in statistical modelling as a component of the Matérn covariance function. The function K(x)is numerically cumbersome and prone to large errors under classical differentiation strategies. We identiﬁed two novel series expansions that are highly accurate and efﬁcient to be used for derivative evaluations using the complex step method. The authors would like to thank Michael Stein (UChicago) for inspiring and encouraging this work, as well as Lois Curfmann McInnes and Paul Hovland (Argonne National Laboratory) for engaging L. White et al.. (2021). JuliaDiff/FiniteDifferences.jl: v0.12.18. 5146583 (a) Fixed small argument x = 3.94.(b) Fixed large argument x = 13.57. scientiﬁc discussions on automatic differentiation state-of-the-art strategies and neural network models. LLC, Operator of Argonne National Laboratory (“Argonne”). Argonne, a U.S. Department of Energy Ofﬁce of Science laboratory, is operated under Contract No. DE-AC02-06CH11357. The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government. Government License.The submitted manuscript has been created by UChicago Argonne,