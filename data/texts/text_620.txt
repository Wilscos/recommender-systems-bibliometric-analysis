Reinforcement Learning (RL) considers an agent in te racting with an unknown environment in a sequence of steps. At each of these steps, the agent must select an action. Consequently, the environment generates a reward and presen ts the agent with a n ew situation, also called a state. The goal of an agent is to maximise the cumulative sum of these rewards by learning the best action to take in each state. In this paper, we conside r the episodic reinforcement lear ning problem. Here, the agent interacts with the environment in a sequence of episodes. Each episode consists of a ﬁxed number of steps where the agent sees a state, selects an action, observes some reward an d transitions to th e next state. This feedba ck h elps the agent to learn to take better decisions in the futur e. There exist many theoretically successful algorithms for episodic reinforcement learning [Bartlett and Tewari, 2009, Jaksch et al., 2010, Filippi et al., 2010, Fruit et al., 2020, Azar et al., 2017, Dann et al., 2017]. The majority of these are built unde r the assumption that the feedback is immediate or, at the very least, received at the e nd of each episode. This allows the agent to quickly learn abou t the quality of the ir decisio ns and adapt to take better actions. However, in many practical settings, feedback is not immediate. Instead, it is received only after some d e la y. This can occur due to the nature of the environment or for computational reasons. Examples of the former arise in healthcare, ﬁnance and online recommender systems. Here, the outcome of a treatment protoc ol, particu lar investment strategy or sequence of r ecommendations often returns at some u nknown time in the future. Autonomous vehicles or wearable technology is an example of the latter, where heavy computation may occur on a separate machine m e aning that the agent only receives the feedback a t some later point in time. Currently, there is little theoretical understanding of the impact of delays on the regret o f reinforcement learning algorith ms. In this paper, we study the impact of sto chastic delays on the regret of optimistic reinforcement learning algorithms in episodic Markov Decision Processes (MDPs). We propose two procedures for dealing with delayed feedback, which we call active and lazy updatin g. In the active updating procedure, a base algorithm updates the policy as soon as feedback is received. We show that this updating scheme leads to an additive increase in regret for a broad class of algorithms (the so-called ’optimistic’ model-based algorithms). This additive p enalty dep e nds on th e expected delay and the base algorith m of choice. Imperial College LondonImperial College LondonImperial College London There are many provably efﬁcient algorithms for episodic reinforcement learning. However, these algorithm s are built under the assumption that the sequences of states, actions and rewards associated with each episode arrive immediately, allowing policy updates after every interactio n with the environment. This assumption is often unrealistic in practice, pa rticularly in area s such as healthc a re and online recommendation. In this paper, we study the impact of delayed feedback on several provably efﬁcient algorithms for regret minimisation in episodic reinf orcement learning. Firstly, we co nsider updating the policy as soon as new feedback becomes available. Using this updating scheme, we show that the regret increases by an additive term involvin g the number of states, actions, episode length and the expected delay. This additive term changes depen ding on the optimistic alg orithm of choice. We also show tha t updating the policy less frequently can lead to an im proved dependency of the regret on the delays. The lazy upd ating proto c ol uses the doubling trick a nd waits for the observed visits to a state-action-step to double before computin g a new policy. We show that this leads to an additive increase in regret that depends on the expected delay and is inde pendent of the chosen optimistic m odel-based algorithm, in some cases leading to improved results. Delays in Bandits Stochastically delayed feedb a ck has received much attention in the simpler multi-armed bandit and contextual bandit problems [Agarwal and Duchi, 2011, Jou la ni et al., 2013, Vernade et al., 2017, Manegueu et al., 2020, Dudik et al., 2011, Vernade et al., 2020]. The results in the multi-armed bandit setting show delayed feedback causes an additive penalty in the regret that scales with the number of actions and the expected delay. Thus, we can expect a similar re sult in the harder episodic RL problem. However, dealing with delays in RL is more challenging than in the (contextual) ban dit setting. Indeed in RL, since the environment is stoch a stic and changes depending on the actio ns taken, we cannot guarantee to observe a particular state nor the effects of taking a p a rticular action. Consequently, this means that approaches such as the queuin g technique of Jo ulani et al. [2013] cannot easily be applied. Delays in RL Despite being of practical importance, there is limited literature on stochastically delayed feedback in RL. Previo us work has considered constant delays in observing the curre nt state in an MDP [Katsikopoulos and Eng elbrecht, 2003]. However, the challenges in this setting are somewhat different to that of delayed feedba ck. M ore recently, Lancewicki et a l. [2021] considered delay ed feedback in adversarial MDPs. They consider adversarial delays and d evelop algorithms ba sed on policy optimisation whose regret depends on the sum of the delays, the number of states and the number of steps per episode. For stochastic MDPs and adversarial delays, they also state that the r egret will be increased by an additive term in the regret bound o f order: H all delays are less than τ expected delay, which is often considerably smaller than the maximal delay, that imp acts the regret. In this paper, we consider the task of learning to act optimally in an unknown episodic ﬁnite-horizon Markov Decision Process, EFH-MDP. We focus on the case where there is a delay between play ing an episode and observing the sequence of states, actions and rewards sampled by the agent; we refer to this seque nce as feedback throughout this paper. An EFH-MDP is formalisable as a quintuple: M = (S, A, H, P, R). Here, S is the set of states, A is the set of actions, H is the horizon and gives the number of steps p e r episode, P = {P over the next state and R = {R function is known, deterministic and bounded between zero and one for all state-action-step triples. In the episodic reinforce ment learning problem, an algorithm interacts with an MDP in a sequence of episodes: k = 1, 2, . . . , K. We denote the set of episodes by: [K] = {1, 2, . . . K}; a conventio n that we a dopt for sets o f integers. In ea ch of the episodes, the learne r takes a ﬁxed numb er of steps, H. At the start o f the k samples an initial state: s initial state. We co nsider algorithms that compute a deterministic policy π can show th at if an optim al policy exists, th ere is a deterministic optimal policy [Puterman, 1994]. The agent samples feedback from the environment by: selecting an action, a transitioning to the next state, s The feedback of the k setting, this f e edback is received immedia te ly. In th is paper, we will co nsider the setting where this feedback is received after some stochastic delay which will be formally introduced in Section 3. The main challenge i n model-based reinforcement learning lies in estimating the transition function. Thus, an extension to unknown bounded stochastic rewards is relatively str ai ghtforward. We measure the quality of a policy, π, using the value function, wh ich is the expected return at the end of the episode from th e current step, given the current state: Further, we d enote the optimal value func tion by: V ∀(s, h) ∈ S × [H]. When evaluating a learning a lgorithm, it is common to use regret. The regret measures the exp ected loss in rewards over a ﬁxed number of K episodes as a result of following (possibly ) sub-optimal policies π where T = KH denote the total num ber of steps taken in the EFH-MDP. Domingues et al. [2020] show that the lower bound for the regret in the standard episodic reinforcement learning setting with stage-dependent transitions is:√ Ω(HSAT ). Many provably efﬁcient algorithms exist for learning EFH-MDPs when feedback is immediate. In this paper, we focus on optimistic mode l-based reinfor c ement learning algorithms. These algorithms ma intain estimators of the tran sition probabilities for each (s, a, s where is the total number of v isits to the given state-action-step triple before the k model-based algorithms that compute an optimistic value f unction of the form: with high probability, where β Here, C value functions of this form. Examples of value-optimistic algorithms include: UBEV, UCBVI-CH, UCBVI-BF [Dann et al., 2017, Azar et al., 2017]. Howeve r, recent work proved that model-optimistic algorithms share this fo rm too [Neu and Pike-Burke, 2020]. Some model-optimistic algorithms include: UCRL2, KL-UCRL and UCRL2B [Jaksch et al., 2010, Filippi et al., 2010, Fruit et al., 2020]. For UCRL2, C [Neu and Pike-Burke, 2020]. Throu ghout this paper, we make use of the f ollowing assumption: Assumption 1. The exploration bonus upper bou nds th e estimation error: β for all (k, h) ∈ ×[K] × [H]. Using Assum ption 1, we can upper bound the regret by [Neu and Pike-Burke, 2020]: with probability 1 − δ Throughout: x ∨ y = max{x, y} and x ∧ y min{x, y}. and Bare algorithm-dependent quantities. Naturally, the value-optimistic algorithms compute From Equation (6), it is clear that upper bounding the sum of the bonuses leads to an upp er bound on the regret. Bounding the sum of bonuses involves up per bound ing the sum of: 1/ episodes. After r e arrangin g these summations to get indicator functions in the numerators, standard techniques show that: The result follows from realising that the term in the denomina tor increases by one whenever the indicator in th e numerator is equal to one. Appendix A.2 gives full details of the bounds in (7) and (8). Consequently, any algorithm that satisﬁes Assumption 1 has a regret bound of the form: C To model problems where feedback returns at some u nknown time in the future, we introdu ce a random variable for the delay between playing the k this paper, we make th e following assumption on the delays: Assumption 2. The d e lays are positive, independent and identically distributed random varia bles with ﬁnite expectation. Denoting the dela y of the k As a consequence of the delays, the feedback associated with an episode does not return immediately. Instead, it returns at some unknown time in the future: k + τ associated with the k When workin g with delayed feedback in RL, it is helpful to introduce the observed and missing visitation counters, and their relationship with the total visitation counter: Analogously to (9) we de ﬁne allowing for the estimation of transition probabilities. Instead of the total-visitation count, model-based optim istic algorithm s can only compute their bonuses using the observed visitation cou nter. The corresponding value functions are still op timistic, but they will c ontract to the optimal value function more slowly. From Equation (6), it is clear that we can control the regret by controlling the bonuses. With delayed feedback, bounding the bonus terms is more difﬁcult since th e observed visitation count can remain constant across n umerou s episodes. Therefore, (7) and (8) do not upper bound the equivalent summations involving the observed visitation counters. One approach to delaying with delayed feedback would be to maintain numerous versions of the base algorithm and bound their regret separately u sing the standard tec hniques [Lancewicki et al., 2021]. To do so would require one to maintain τ of this approach would would scale the regret of the base algorithm by the maximal delay. In addition to being highly inefﬁcient from a space-complexity perspective, in ca ses where the de la y d istribution has inﬁnite supp ort or th e bound on its distribution is large, this approach can have regret that is linear in the total number o f steps, T . Further, τ be large in comparison to the expected delay. In Section s 4 and 5, we show that it is possible to obtain regret bounds scaling with the expe cted delay r a ther than the maximal delay. In Sections 4 and 5, we propose two procedures for dealing with delayed feedback in episodic RL: one based on updating as soon as new da ta becomes available and the other based on updating less frequently. In eith e r case, we can bound the number of missing visits to each state-action-step triple by the number of missing episodes. The reasoning behind this du e to the agent playing only one state-ac tion pair per step in each episode. Lemma 1. Let S random variable s with ﬁnite expected value. Further, let: be th e failure event for a single k. The n, P(F Proof. The proof follows from applyin g Bernstein’s inequality to the sum of indica tor ra ndom variables. See Lemma 1 o f Appendix B for a full proo f. A direct conseque nce of this lemma is an upper bound o n the nu mber of missing episode s: which holds across all k ∈ Z We ﬁrst consider the effect of delays on the regret when we update the policy as soon as new data becomes available. This proce dure is outlined in Algorithm 1. We assume we have some base algorithm whic h takes all available data and outputs an optimistic policy. Theorem 1 (Active Updating). Under Assumption 1, with probability 1 − δ, the regret of any model-based algorithm under delayed feedback is u pper bou nded by: where C and B are universal upper bounds on the algorithm-dependent quantities in the exploration bo nus of (5). The second column of Table 1 presents regret bounds for various optimistic algorithms using active updating under delayed feedback that ﬁt into our framework. From Equation (6), it is clear that we must bound the summation of the bonuses. In the standard setting, one can use the fact tha t the total visitation counter increases by one between successive plays of a state-action-step triple. However, this is not the case in the delayed feedback setting. The following lem ma provides an upper bound on summations in the form of the bonuses calculated using the observed visitation counter, albeit without the terms in the nume rator. Proof. Unless otherwise stated, we let: N conve nience. First, we use the re la tionships be twe en the observed, missing and total visitation c ounters to split the summation into two parts in a similar mann er to Lancewicki et al. [2021]: From Equation (11), N since (1 +x) seen in the non-delay e d setting and so can be bounded using e.g. Equations (7) and (8). Bounding B req uires more care, as it involves the o bserved and missing visitation co unters. Recall that the algorithm plays one state-a ction pair at each step in every episode. Thus, the missing visitation counter is upper bounded by the number of missing episodes: N 1 −δ, S Clearly, B.1 ≤ A, as it is a summation over a subset of all the episodes. Using (11), it is possible to rewrite the indicator in the remaining term as: 1{N N(s, a) ≤ ψ Lemma 6 of Appendix A.2 gives an upper bound of : action-step tr iples gives: Therefore: as required. ≤ 1 +xfor p = 1/2 and p = 1 and any x > 0. Term A is the summation o f the total visitation counter ≤ ψacross all k ∈ Z. Splitting B usin g the observed visitation coun ts and the upper bound on Sgives: and N(s, a) ≥ 1. Therefore, 4.2 Proof of Theorem 1 Using Lemma 1 and 2, we can bound the regret of optimistic model-based algorithms using active updating to handle the d elayed feedbacks. Proof. We split the regret into two sets of episodes ba sed on whether their feedback is r eturned before the beginning of the ﬁnal episode : Trivially, the regret of each of the ﬁrst set of episode s can be bounded by the horizon, H. Lemma 1 gives an upper bound on the nu mber of missing episode s. Therefore, with probability 1 − δ For the second set of episo des, we bou nd the regret by summing the bonu ses. Using Equation (6): Therefore, all that remains is to bound the sum of the exploration bonuses. To do so , we let C ≥ C be u niversal up per bounds on the numerators in the exploration bonus. Then, the sum of the exploration bonuses has the following upper bound: An application of Lemma 2 shows that the regret from summing the bonuses is upper bounde d by: Bringing everything together gives an upper bound on the regret: UBEV UCRL2 Table 1: A selection of algorithms that ﬁt into our framework and the ir regret bounds under delaye d feedback. Here, Γ ≤ S is a unifo rm upper bound on the n umber of reachable states at a subsequent step. Further, L L= log(K/SA), and L = log(S, A, H, K, δ βs, a≤ 4CHSAT + 6CHSAψ+ 2BHSA log (8T ) + BHSAψlog (16 ψ) T logKπ6δ+ 8CHSAT + 4BHSA log (8T ) + 12CHSAψ+ 2BHSAψlog (16 ψ) = δ/(9 + 1) to accommodate for the addition a l failure event from Lemma 1 gives the ﬁnal result. HSAT L + HSALE [τ]HSAT L + HSAE [τ] L√√ HSAT L + HSALE [τ]HSAT L + HSAE [τ] L√√ HSAT L + HSALE [τ]HSAT L + HSAE [τ] L√√ In Section 4, we derived regret bounds for a wide range of model-based algorithms using optimism. Although the active updating scheme does maintain good theoretical guarantees, a slower approach to updating can lead to improved regret bounds in some cases. Brieﬂy, we consider waiting until the observed visits to a state-action-step double before updating the po licy. Algorithm 2 presents the pseudo-code for this slower approa ch to updating. Theorem 2 (Lazy Updating). Under Assumption 1, for T ≥ HSA, with probability 1 − δ, the regret of any modelbased algorithm under delayed feedback has the following upper bound: where L ration bonus of ( 5). Theorem 2 shows that the delay dependency is independent of the base algorithm. The third co lumn of Table 1 gives regret bounds for various algorithms using lazy updating to handle delayed feedback. Instead of updating as soon as the algorithm sees feedback, we consider waiting until the observed visits to a stateaction-step triple have doubled. Ea ch update marks the start of an epoch, which we de note by: j = 1, 2, ··· , J. At the start of each epoch, the base algorithm com putes a policy and uses it to sample feedback from the MDP. There fore, each e poch is just a set of episodes where the a lgorithm interacts with the environment using the same policy. Let kdenote the episode where the j observed while playing the j where 1{s, a, h} = 1{s have doubled since the start of the j Lemma 3. For K ≥ SA, Algorithm 2 e nsures that the number of ep ochs has the following upper bound: Proof. See Lemma 3 of Appendix B. = log(K/SA), C and B are unive rsal upper bounds on the alg orithm-dependent quantities in the explo- 5.2 Proof of Theorem 2 Using Lemma 3 a nd the argum ents in its proof ensures that the number of epochs is logarithmic in the numbe r of episodes and that we can make use of Lemma 19 of Jaksch et al. [2010] to bound the sum of the bonuses. Therefore, we can bound the regret o f model-based algorithms using lazy upda ting to handle the delayed feedback. Proof. Similarly to the ﬁrst step in Section 4.2, we can bound the regret of the episodes that are not returned by the beginn ing of the ﬁnal episode can be bounded by: Hψ before beginning the ﬁnal episode. Le mma 1 gives the following recursive regret decomposition for any model-based algorithm : where we omit the use of the indicator function: 1{k + τ remains is to bound the sum of the exploration bonuses. To do so, we utilise the fact that epochs are disjoint sets of episodes and that the algorithm uses the sam e bonus within each epoch, as the counts do not chan ge: The regret associated with the bon uses can then be split in two terms based on whethe r the state-action-step triples played in the j Focusing on the former and extending the inner-most summ ation to all episodes before the start of the next epoch gives: Recalling the fo rm of the bonuses gives: since n Jaksch et al. [2010], which is re stated in Appendix B.1 (Lemma 7). From Equation (4), β step of an episode allows the following bound o n the remaining term : epoch are observed before starting the next epoch: β(s, a)1 {s, a, h, k + τ< k}+β(s, a)1 {s, a, h, k + τ≥ k}. (s, a) ≤ N(s, a), due to the doubling scheme. The penultimate inequality follows fro m Lemma 19 of where the last ine quality used Lemma 3. Therefore, where L gives the ﬁnal result. Table 1 presents a selection of algorithms that ﬁt into our framewo rk and their regret boun ds under d elayed fee dback using both active and lazy upda ting. Here, we see that acting in delayed environmen ts causes an additive increase in regret for all optimistic algorithms considered. For active updatin g, this additive in crease scales with: max{C, B}HSAE[τ], where C and B are quantities depending o n the base algorithm of choice. Typically, C and B involve the horizon, H, and the num ber of states, S. For lazy updating, this additive increase scales with: H of the chosen base algor ithm. In either case, the results mirror what is seen in the ban dit setting, mo st algorithms inc ur an additive regret penalty of AE[τ] [Joulani et al., 2013]. For many algorithms such as UBEV, UCRL2, KL-UCRL, B = 0. In such a case, lazy updating is prefer able if: H log(K/SA) ≤ C, where C usually contains a logarithmic term at least as large as active updating is preferable. When considering active updating, algorithms with tighter regret bounds in the nondelayed settin g exhibit better dependency on the expected dela y. Further, UBEV, UCRL2 and KL-UCRL recover their standard regret bounds in the no n-delayed setting (whe n τ Typically, B > 0 for algo rithms using variance reduction techniques, f or example χ algorithm s, from Table 1, it is clear that it is always better to use active updating. Thus, we focus o ur discussion on this updating scheme. Neither χ due to boun ding the empirical variance of the optimistic value f unction in each episode by H. This was necessary to avoid a multiplicative in c rease in regret due to the delays. Further, introducing the empirical variance comes at th e expense of some lower-order term s: HS log(H, S, A, T, δ can be proble matic, as they now depe nd on the expected de la y. In particular, they lead to a term of ord er H in the regret bound. We note that the impact of these lower order terms when using non-standard counters has also been observed in settings such as differentially private reinforcement learning [Garcelo n et al., 2021]. Unfortu nately, UCBVI [Azar et al., 201 7] does not ﬁt into our framework. However, even if it did, we argue that it would not lead to improved results. Indeed, the algorithm makes use of Bernstein’s inequality to handle the estimation error, resulting in the need to bound an additional term of the form: H term alone introduces the following dependency on the delay: H algorithm s we consider in this paper. UBEV [Dann e t a l., 2017], for examp le, has a delay dependency of: H an improvement of HS. Moreover, due to bound ing th e empirical variance, UCBVI an d UBEV will have the same leading order term in their regret bound. Therefo re, we do not believe that using UCBVI in the de la yed setting will lead to an improvement over UBEV, an algorithm which does ﬁt into o ur framework. Lastly, we point ou t that although we consider model-based algorithms in this paper, we believe that lazy-updating could be used for a wider class of alg orithms. Analyzing the effect that this would have on the regret of such algorithms remains an open question. = log(K/SA). Rescaling δ= δ/(9 + 1) to accommodate for the additional failure event from Lemma 1