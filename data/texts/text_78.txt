In online platforms it is often the case to have multiple brands under the same group which may target dierent customer proles, or have dierent domains. For example, in the hospitality domain, Expedia Group has multiple brands like Brand Expedia, Hotels.com and Wotif which have either dierent traveler proles or are more relevant in a local context. In this context, learning embeddings for hotels that can be leveraged in recommendation tasks in multiple brands requires to have a common embedding that can be induced using alignment approaches. In the same time, one needs to ensure that this common embedding space does not degrade the performance in any of the brands. In this work we build upon the hotel2vec model and propose a simple regularization approach for aligning hotel embeddings of dierent brands via domain adaptation. We also explore alignment methods previously used in cross-lingual embeddings to align spaces of dierent languages. We present results on the task of next-hotel prediction using click sessions from two brands. The results show that the proposed approach can align the two embedding spaces while achieving good performance in both brands. Additionally, with respect to single-brand training we show that the proposed approach can signicantly reduce training time and improve the predictive performance. • Information Systems → Recommender Systems;• Applied Computing → Electronic Commerce;• Computing Methodologies → Machine Learning. embeddings, hotel2vec, alignment, domain adaptation, transfer learning ACM Reference Format: Ioannis Partalas. 2021. Aligning Hotel Embeddings using Domain Adaptation for Next-Item Recommendation. In Proceedings of ACM SIGIR Workshop on eCommerce (SIGIR eCom’21). ACM, New York, NY, USA, 5 pages. Online platforms often have multiple brands, for the same line of business, under the same group which may target dierent customer proles. As an example, in the hospitality domain Expedia Group (EG) has brands like Brand Expedia, Hotels.com and Wotif, that can either have dierent proles of travelers or be more relevant locally. A main task in such online platforms or more generally in retail platforms, is to recommend products to customers which requires to learn an embedding space that captures their salient attributes. Hence, enable similarity comparisons that can be leveraged from recommendation systems. In the recent years approaches that learn product embeddings from the interactions of the customers with the on-line platform have been proposed [7,11,20] as well as approached tailored to the hospitality domain [10,18]. These approaches leverage the seminal word2vec model [15] in order to learn the embedding space by treating the clicked items as tokens in a sentence. While it is common to learn such embeddings in a single domain/brand, in the context of electronic commerce we would like to be able to leverage such embeddings across dierent domains/brands. As mentioned previously, one can learn hotel embeddings on Brand Expedia and leverage them in Hotels.com in order to bootstrap or improve the hotel embeddings in the latter case. Subsequently these hotel representations can be used in tasks like personalized recommendations. To do so, one would need to align the embedding spaces of the dierent brands and use this aligned space to capture the intent of the users while searching on the on-line platform. In a very recent work, Bianchi et al. [5] study the alignment of product embeddings to enable zero-shot learning in a cross-shop scenario. Their setting is more general as in our case the multi-brands belong to the same domain, and usually we dispose a one-to-one mapping between products (hotels in our case). In this work we propose to align embeddings from dierent brands using a simple regularization approach from domain adaptation. We build upon the hotel2vec model [18] for learning hotel embeddings and extend it to accommodate alignment of embedding spaces. Our approach, can also be thought of as a transfer learning one as we are able to boostrap the learning procedure of hotel2vec in a target brand using the hotel2vec embeddings from a source brand. The main contributions of this paper are the following: •We propose a simple yet eective domain adaptation approach that adds a regularization term in the loss function of hotel2vec. •We present empirical results on the task of next-hotel prediction for two brands. •We also implement an alignment method borrowed from the task of alignment of cross-lingual embeddings. We show empirically, that to perform such a transpose to another domain one should be careful of the particularities of it. In recommendation systems learning embeddings for users and items that capture the semantics is a critical part. In the domain of electronic commerce, approaches as prod2vec have been proposed in order to learn representations of products [3,10,11] which leverage the skip-gram model [15]. In [7] the authors propose to learn embeddings for YouTube videos by combining multiple features, which are used for candidate generation and ranking. Other approaches have also been proposed that include metadata [20] or try to capture dierent aspects of the product from dierent sources (clickstream data, text and images) [19]. Aligning embedding spaces is a topic that has been extensively studied in the NLP domain and specically in the context of crosslingual embeddings. A simple and ecient approach to align two embedding spaces of dierent languages is a to learn a linear projection [14]. In [2] the authors employ an iterative approach starting from seed mappings of words (source language to target language) and do the linear projection by imposing an orthogonality constraint in the projection matrix. A survey on cross-lingual word embedding can be found in [17]. In this work we study the effectiveness of such alignment approaches in the context of hotel embeddings. Domain adaptation is also a topic that has attracted interest in the recommendation systems space [12,21] where the goal is to transfer knowledge from a source to a target task. In our setting we employ a domain adaptation approach in order to align dierent embedding spaces and we follow a straightforward regularization approach [8]. More recently, such approaches have been encompassed in a single framework for domain adaptation [13]. Other works, propose adversarial approaches for domain adaptation that learn in the same time the alignemnt as well as an embedding space that is invariant to the domain [6, 9, 16]. The most similar approach to our work is the one presented in [5] where the authors propose to align product embeddings to enable cross-shop recommendations. Specically, they propose dierent approaches either based on content (images and text) or using the clicked products in the dierent shops as supervised signals in methods for alignment of cross-lingual embeddings. Our use case has a simpler setting as we dispose a mapping between products (hotels) in the two embeddings spaces we wish to align. In order to learn representations of hotels, we use the hotel2vec model [18] which implements a neural model and is trained with the skip-gram model and negative sampling. The model learns dierent embeddings for click (𝑉), hotel properties (𝑉) and geographical information (𝑉) which are fused to learn an enriched representation. Specically, the embeddings are calculated as follows: where𝑓 (𝑥;𝑊 ) = ReLU()and𝐼, 𝐼and𝐼refer to the input features for the click, amenity and geographical embedding. The nal hotel2vec embedding is calculated as a projection of the concatenated embeddings: LetVbe the representation for hotelℎas calculated by equation 1 whereℎ∈ 𝐻. Then hotel2vec model minimizes the following loss function: 𝐽 (𝜃) = −log 𝜎𝑉𝑉+log 𝜎−𝑉𝑉(2) where𝐷are the skip-gram pairs of clicked hotels in the session that are generated using a xed length window.𝑁are the negative samples which are sampled from the same market of clicked hotels in the session, as a traveler searches in a specic destination. Finally, 𝜎 () is the sigmoid function. In this work we propose to extend the hotel2vec model in order to accommodate embedding spaces alignment in the case of multibrand representations. We do so by employing a domain adaptation scheme. DenoteD the source domain where we already learned a hotel2vec model by minimizing the loss function in Equation 2,𝐽(𝜃). Then in the target domainDwe learn hotel2vec representations by minimizing the following regularized function: where𝑉refers to the corresponding embedding of hotelℎin the source domain. Note that the embeddings of the source domain are xed and not re-trained. Also,𝜆is a parameter that controls how much knowledge we would like to transfer from the source domain. As in our case we want to align the embedding spaces, we would like to constraint the model to be as close as possible in the source domain, hence set𝜆equal to 1.0. In the experimental section we experiment with this parameter to understand the impact in the downstream task. Note that we dene the strength of regularization globally but in a more ne version it could be dened per hotel. We leave this for future work. The above regularizer assumes that we dispose a mapping between hotels in the two domains which is our case. Also, note that we do not suer here from the problem of absent hotels from the training data in the source domain as the model can impute these ones. We evaluate the proposed approach in the task of next-item prediction where we want to predict the next clicked hotel in the session based on the previous clicked hotel. We collect click sessions over one year of searches for both Hotels.com and Brand Expedia. It contains more than 65M user click sessions for each brand and around 700K unique hotels for Hotels.com dataset and over 1.4M for Brand Expedia respectively. We randomly split the sessions in training, validation and test with a ration of 8:1:1. We use a system with 64GB RAM, 8 CPU cores, and a Tesla V100 GPU. We use Python 3 as the programming language and the Tensorow [1] library for the neural network architecture and gradient calculations. For hotel2vec we follow the experimentation methodology in [18] for tuning the hyperparameters of the model. The model in both brands is trained with 𝐿2-regularization. For the alignment approaches, we use as source domain Hotels.com hotel2vec embeddings and Brand Expedia as the target one. For the proposed approach we set the parameter𝜆to 1.0 in order to force the alignment of the spaces. We also tried a value of 0.5 in order to understand the behavior of the approach. We compare the regularization approach with the linear projection alignment approach presented in [14]. In that case given the embeddings that we export from the trained models in Hotels.com and Brand Expedia domains, denotedVandVrespectively, we solve the following optimization problem: The alignment is learned only on the common hotel embeddings for the two sets of vectors in order to avoid injecting too much noise. Metrics: We compare the dierent approaches in terms of hits@k and MRR@k (Mean Reciprocal Rank). Hits@k measures the average number of times the correct selection (i.e. the hotels clicked by the users in a session) appears in the top k predicted hotels. MRR@k evaluates the average list quality of k items returned by the model. Both metrics are calculated over the ranking induced by cosine similarity of the embedding vectors. Note that when we have a crossbrand setting this corresponds to a zero-shot learning framework. We present in this section results for two settings: a) the zero-shot prediction of next clicked hotel where we use embeddings in a crossbrand setting and b) the supervised setting where we compare the in-brand hotel2vec model with and without domain adaptation. Table 1 presents the hits@k and MRR@k for𝑘 ∈ {10,100}to predict next-clicked hotel in the test sets for both Brand Expedia and Hotels.com. The rst two approaches, namedℎ𝑜𝑡𝑒𝑙2𝑣𝑒𝑐andℎ𝑜𝑡𝑒𝑙2𝑣𝑒𝑐, correspond to single-domain trained models with no further alignment. The results, show how the corresponding embeddings behave in a cross-brand zero-shot learning approach. The next two approaches present the corresponding metrics for the linear projection alignment approach (LP) as well as the proposed approach that employs domain adaptation (ℎ𝑜𝑡𝑒𝑙2𝑣𝑒𝑐). Focusing only in the single-domain approaches, we can observe that the embeddings can enable zero-shot learning in a cross-brand scenario as they can achieve good performance across all the metrics. For example, with theℎ𝑜𝑡𝑒𝑙2𝑣𝑒𝑐we can achieve 0.5073 of hits@100 compared to the in-brand trained modelℎ𝑜𝑡𝑒𝑙2𝑣𝑒𝑐that achieves a 0.5335 hits@100. The same observation stands in the case of Hotels.com even though we notice that the gap between the dierent performances is larger. Concerning the alignment approaches presented in the next two rows of the table, we notice that the LP method has a big drop in performance in both brands. As we mentioned earlier, even though we dispose a one-to-one mapping of the hotels between brands the method fails to keep the similarities in the projected space. This shows that extra care should be taken when aligning the embedding spaces, for example require the projection matrix to be orthogonal. Regarding the proposed approach we can observe that is able to achieve good performance in both brands. Actually, it achieves the same performance with the in-domain model in Hotels.com, 0.7535 versus 0.7573 for hits@100, while there is a drop in performance for the Brand Expedia. We recall here that we set the𝜆parameter that controls the transfer of knowledge to either 0.5 or 1.0 where the latter allows us to force the alignment between brands. In the case of less regularization strength (𝜆 =0.5) we can observe that the metrics slightly improve. Smaller values of the parameter can balance between a full transfer of knowledge and in-brand learning. For the improvement in the Hotels.com brand we hypothesize that allowing for less regularization we are able to allow more in-domain knowledge to ow and learn a better similarity space. In this section we present results when we compare the same inbrand model with and without domain adaptation. Note that in this case we let the neural model score the candidate hotels. Figures 2 and 1 present the hits@10 and hits@100 respectively for hotel2vec with domain adaptation and a single-brand model. We can notice that the proposed regularization scheme improves both the metrics as well as it can reduce training time. Also, the jump-start is signicant and shows that we can share knowledge between the two brands. Table 2 presents the hits@k metric for the in-brand hotel2vec model in comparison with the proposed approach when we use the model to score the next-hotel to be clicked. We can clearly see that the domain adaptation approach outperforms the single-brand approach by 3% for hits@100 metric. We presented in this work a simple yet eective regularization approach for aligning embedding spaces in a multi-brand scenario. For example, Expedia Group has multiple brands that operate in the same domain. The idea is to add a regularizer in the objective function of the model that learns the embeddings in order to force them to be as close as possible to the embeddings of the source brand, hence performing domain adaptation. This kind of approaches has also been explored in the past in Natural Language Processing tasks [8]. We evaluated the proposed approach in the next-hotel prediction task for two brands, that is Expedia and Hotels.com. We measured performance in terms of hits@k and MRR@k metrics. We also, Table 1: Hits@k and MRRk for the dierent approaches for emb eddings alignment. Note that the hotel2vec approach refers to the standard training with no alignment for the two brands. Figure 1: Hits@100 on the test set during training. We compare the domain adaptation approach with the single hotel2vec model on Brand Expedia clicks. Figure 2: Hits@10 on the test test during training. We compare the domain adaptation approach with the single hotel2vec model on Brand Expedia clicks. compared with linear projection alignment borrowed by the crosslingual approaches for aligning embeddings of dierent languages [14]. Table 2: Comparison of the single-brand hotel2vec model versus the domain adaptation approach in terms of hits@k. The metrics are measured when the scores for the next-hotel prediction are produced by the model. The results, showed that the proposed approach can align the spaces of the multiple brands achieving good performance in both brands. Additionally, we showed that the regularized version of the hotel2vec model can learn faster and improve the performance. We also observed that an approach like the linear projection without taking into account some particularities of the domain leads to worse performance. For future work we would like to add a more adaptive regularization parameter that can be dened per hotel rather than being global. In that way we may wish to transfer knowledge when we are certain that a pair of hotels in the source and target brands should have the same embedding. We would also like to explore the use of multiple source brands in order to align in the same time the embedding spaces. Multitask approaches can be leveraged to align the dierent embedding spaces [13]. Also, other alignment approaches could be explored [4]. Finally, we would like to explore adversarial cross-domain adaptation for aligning the embedding spaces [9]. In this case, we want to leverage the similar features across the brands while also learning specic embeddings for each brand. We would like to thank Phong Nguyen, Cédric Houdré and Clara Jordá Lope for discussions as well as Jan Krasnodebski and Daniele Dhongi for comments on the paper. Many thanks to Petros Baltzis for engineering support.