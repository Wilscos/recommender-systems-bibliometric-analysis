With the prevalence of social media, there has recently been a proliferation of recommenders that shift their focus from individual modeling to group recommendation. Since the group preference is a mixture of various predilections from group members, the fundamental challenge of group recommendation is to model the correlations among members. Existing methods mostly adopt heuristic or attention-based preference aggregation strategies to synthesize group preferences. However, these models mainly focus on the pairwise connections of users and ignore the complex high-order interactions within and beyond groups. Besides, group recommendation suers seriously from the problem of data sparsity due to severely sparse group-item interactions. In this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intraand inter-group interactions among users; (2) alleviating the data sparsity issue with the raw data itself. Technically, for (1), a hierarchical hypergraph convolutional network based on the userand group-level hypergraphs is developed to model the complex tuplewise correlations among users within and beyond groups. For (2), we design a double-scale node dropout strategy to create selfsupervision signals that can regularize user representations with dierent granularities against the sparsity issue. The experimental analysis on multiple benchmark datasets demonstrates the superiority of the proposed model and also elucidates the rationality of the hypergraph modeling and the double-scale self-supervision. • Information systems → Recommender systems. Group Recommendation, Graph Neural Networks, Hypergraph Learning, Self-Supervised Learning ACM Reference Format: Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3459637.3482426 Owing to the rapid development of social media in recent years, users with similar interests now have opportunities to form various online groups [34,43]. Traditional recommender systems designed for individuals can no longer serve the interest of groups. There is an urgent need to develop practical group recommender systems [3,6,50]. The group recommendation aims to reach a consensus among group members, generating suggestions that can cater to most group members [21,29]. However, the inuences of users vary from group to group, leading to hard decision-making. An illustrative example is shown in Figure 1: Alice and her colleagues (Bob and Tony) form a group that are fond of slow food. However, when Alice and her boss Allen (who likes fast food) go for lunch, fast food would be the rst choice due to Allen’s higher position. Meanwhile, since Allen has an appetite for pizza, pizza is also recommended to Alice and Bob. It is natural that collective decisions tend to be dynamic, i.e., a group’s preference may vary due to the inuence of other user members and other groups’ preferences. As such, a critical issue is to model the complex interactions of users and groups in the research of group recommendations. Figure 1: An illustrative example of group recommendation. However, existing group recommendation models fail to model the interactions of users and groups well. Many of them adopt heuristic rules, such as average [3,4], least misery [1], and maximum satisfaction [5], which ignore the impact of user interactions. With the success of deep learning, several group recommendation models employ the attention mechanism over user-item graphs to exploit user interactions and synthesize group preferences [20,35,44]. Nevertheless, these models keep attention on the pairwise connections of users and ignore the complex highorder interactions within and beyond groups. Besides, since many groups are formed temporally, the group-item interaction records are very sparse, making it more dicult to learn an accurate group preference directly from the interaction data [20,44,45]. To tackle the aforementioned two limitations, in this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intra- and intergroup interactions among users; (2) alleviating the data sparsity issue with the raw data itself. To handle the interaction issue, we explicitly model the complex tuplewise relationships as a hierarchical hypergraph, which aggregates the correlated users within and beyond groups. Distinct from the edge in simple graphs, the hyperedge can connect multiple nodes, which is a natural way to capture tuplewise relations. The proposed hierarchical hypergraph consists of two levels: user- and group-level, where each level of hypergraphs contains the corresponding type of nodes. In the user-level hypergraph, each hyperedge connects the set of users belonging to the same group. To capture the intra-group interactions among dierent users, we adopt the hypergraph convolutional network to aggregate the related users and generate the dynamic user embeddings. Meanwhile, for exploiting the informative inter-group interactions among users, we adopt triangular motifs [55] which allow each group to select the most relevant groups to acquire information to form the hyperedges in the group-level hypergraph. Despite the benets of hypergraph modeling, the group recommendation performance is still compromised by the data sparsity problem, which not only exists in group interaction but also in user interaction. To alleviate this issue, we integrate self-supervised learning (SSL), which can discover self-supervision signals from the raw data, into the training of the hierarchical hypergraph convolutional network. Many existing SSL-based models commonly conduct node/edge dropout in the user-item graph to augment supervised signals [16,31,47]. However, this may drop some essential information, especially when the raw group is very sparse, leading to unserviceable supervision signals. Therefore, we propose a double-scale node dropout strategy to create supervisory labels with dierent and ner granularities. The intuition behind is that not all user-user interactions signicantly aect the nal group decision, and leveraging partial information can help learn invariant representations. Meanwhile, two dierent-grained views may to some degree prevent the loss of essential information (e.g. drop important nodes). Concretely, at a coarse granularity, we randomly remove some nodes from the user-level hypergraph structure. As a result, when a removed node belongs to multiple groups, it will disappear in all related hyperedges. As for the ne-grained node dropout, we remove some member nodes only from a specic group, which does not aect this node’s existence in other groups. We then maximize the mutual information between the node representations learned from two granularities to regularize the user and group representation against the sparsity issue. Finally, we unify the recommendation task and the self-supervised learning task to optimize model parameters by a two-stage training approach. In summary, the main contributions of this paper are as follows: •We devise a hierarchical hypergraph learning framework to capture the intra- and inter-group interactions among users in the group recommendation. •We propose a SSL strategy with dierent granularities to enhance user and group representations and alleviate the data sparsity problem, which is seamlessly coupled with the hierarchical design of the hypergraph convolutional network. •We conduct extensive experiments on three group recommendation datasets to exhibit the superiority of the proposed model over some recent baselines and elucidate why the hypergraph learning and the double-scale SSL strategy can improve group recommendation. The code implementation of our model is released at https://github.com/0411tony/HHGR. The remainder of this paper is organized as follows. We rst provide preliminaries of the hypergraph and the denition of the group recommendation in Section 2. Then, we present our proposed model in detail in Section 3. Section 4 describes experimental research. Next, we further discuss the related work in Section 5. Finally, we summarize our work and look forward to future work in Section 6. To facilitate understanding, we present the denition of the hypergraph and the task of the group recommendation in this section. The hypergraph is dened as𝐺 = (V, E), whereVis the vertex set containing𝑀unique vertices, andEis the hyperedge set containing𝑁hyperedges. Each hyperedge𝜖 ∈ Econtains multiple vertices. The hypergraph can be represented by an incidence matrix 𝑯 ∈ R, where𝒉= 1if the hyperedge𝜖contains the vertex 𝑣 ∈ 𝑉, otherwise𝒉= 0. For each hypergraph, we use the diagonalÍ matrix𝑫to denote the degrees of vertex, where𝒅=𝒘𝒉. 𝑾 ∈ Ris the diagonal matrix of the weight of the hyperedge. Let diagonal matrix𝑩denote the degree of hyperedge, whereÍ 𝒃=𝒉represents the number of vertices connected by the hyperedge 𝜖. The task of the group recommendation is to predict the item ranking list for the given group that has multiple members. We consider U =𝑢, 𝑢, ..., 𝑢andI =𝑖, 𝑖, ..., 𝑖to denote the user set and item set, respectively. LetG =𝑔, 𝑔, ..., 𝑔be the group set, where𝑔∈ Gis the𝑚-th group. There are three observable interaction behaviors: user-item interaction, group-item interaction, and user-group interaction. Let𝑹 ∈ Rdenote the useritem interaction matrix, where𝑟= 1if the user𝑢consumed item𝑖, otherwise𝑟= 0.𝑺 ∈ Rdenotes the group-item interaction matrix, where𝑠= 1if the group𝑔consumed the item 𝑖, otherwise𝑠= 0. We embed each user𝑢into the space vector 𝒑∈ 𝑷of the dimension𝑑. Let𝒒∈ 𝑸represent the vector representation of item𝑖, which is randomly initialized with a d-dimensional vector.𝒛∈ 𝒁denotes the representation of group𝑔. We use bold capital letters (e.g.,𝑿) and bold lowercase letters (e.g., 𝒙) to represent matrices and vectors, respectively. In this section, we rst introduce the details of the proposed model HHGR(short for "HierarchicalHypergraph Learning Framework forGroupRecommendation"). Then, we present itsself-supervised variant,S-HHGR, with the double-scale dropout strategy. Finally, we conduct the complexity analysis on𝑆-HHGR to demonstrate its scalability. The proposed framework is shown in Figure 2, which includes three components: 1) Hierarchical hypergraph, which captures the user interactions within and beyond groups by propagating information from user level to group level; 2) Double-scale self-supervised learning, which contains coarse- and ne-grained node dropout strategies to rene the user and group representations and alleviate the data sparsity problem; and 3) Model optimization, which unies the objectives of group recommendation and selfsupervised learning to enhance both tasks. By organizing users and groups in a hierarchical hypergraph, we can leverage their connectives to help exhibit high-order tuplewise user interactions within and beyond groups, which is of crucial importance to group recommendation. Inspired by [10,23], we devise a new hierarchical hypergraph convolution to perform the embedding propagation mechanism over our hierarchical hypergraph on two levels, user-level and group-level. In what follows, we elaborate on these two ingredients. 3.1.1 User-level hypergraph.Hypergraph construction.Since users are correlated within and beyond groups in group recommendation scenario, it is vital to dene appropriate connections among them and exploit the users’ mutual interactions. The conventional graph structure can only support pairwise relations between users, which is not t for this case. Thus, we propose to model such user interactions with a hypergraph, which is shown in the bottom-left corner of Figure 2. In the user-level hypergraph𝐺, multiple users can be connected through one hyperedge (or group). User representation learning.We aim to exploit the tuplewise user interactions for learning their inuences, in which the correlated users should be aected by the intra-group users. To achieve that, we introduce the hypergraph convolution operation to capture user interactions among their neighbors and learn each user’s dynamic representation. For the specic user node𝑣, the hypergraph convolutional network rst learns the representations of all its connected hyperedges𝑒, which is from the gathered node features. Then the node𝑣’s representation would integrate the related hyperedge feature information. Simple graphs use the adjacency matrix to represent relationships between two connected nodes, whereas hypergraphs introduce the incidence matrix𝑯to describe the relationship between nodes and hyperedges, which is dened in Section 2.1. Referring to the spectral hypergraph convolution proposed in HGNN [10,38], the hypergraph convolution is dened as follows: where𝑫denotes the vertex degree matrix of the user-level hypergraph, and𝑩denotes the hyperedge degree matrix.𝑾is regarded as the weight matrix of the hyperedges. In this paper, we initialize the weight matrix𝑾with the identity matrix yielding equal weights for all hyperedges.𝑫and𝑩play a role of the normalization in hypergraph convolutional operator.𝚯is the parameter matrix between two convolutional layers.𝑷is the users’ embeddings in the𝑙-th hypergraph convolutional network, where 𝑷=˜𝑷.˜𝑷is the initial vectors of all users𝑢with𝑑-dimension. Since the nonlinear activation is found to be redundant in recent research [38], we remove this part in Eq. (1), which is shown as follows: Group representation learning.Our target is to obtain the group embeddings to estimate their preferences on items. Since group members have dierent importance, we perform a weighted sum on the representations of user members to generate the attentive group representation˜𝒁. The weights can reect the users’ contribution to the group’s decision-making. where𝛼is the weight of a user𝑢in the group decision,𝒑denotes other members vectors in the same group𝑔, and𝑾∈ Rand 𝒙 ∈ Rare parameters used to compute the weights. 3.1.2 Group-level hypergraph.Hypergraph construction.A user often belongs to multiple groups, which means she is connected with other users of dierent groups. However, not all user interactions aect user preferences. To select informative user interactions and optimize inter-group interactions among users, we adopt the triadic motif relation as the motif-induced hyperedge to construct the group-level hypergraph. Since the triadic motif can increase homophily cohesion in the social community [11,26], its interaction is a stronger bond, which is highly benecial to modeling user interactions. Specically, we rst convert the user-level hypergraph to a projected graph, where group hyperedges act as nodes. In the projected graph, if two group hyperedges have common user members, they will be connected, ensuring the relevance of preferences between groups. Then, we adopt the triadic motif to select the most relevant groups in the group-level hypergraph. If any three groups conform to the denition of the motif, we will dene these three groups to be divided into one motif-induced hyperedge. The group-level hypergraph is represented as 𝐺. Group representation learning.Intuitively, the motif can be regarded as a closed path connecting three dierent nodes. Referring to the motif-based PageRank model proposed in [52], when we do not consider the self-connection, motif-based adjacency matrix can be calculated as Eq. (5), which can be represented as 𝑯𝑯. where𝑯denotes the motif incidence matrix of the group-level hypergraph.𝑪 ∈ Ris the symmetric adjacency matrix of the projected graph, where𝑐= 1if there is a connection between group𝑖and group𝑗, otherwise𝑐= 0.𝑪𝑪denotes the paths connecting three vertices, and⊙ 𝑪transforms the paths into closed triangles. Similar to the hypergraph convolution proposed in the userlevel hypergraph, the transformed hypergraph convolutions in the group-level hypergraph can be dened as Eq. (6). Following the LightGCN and MHCN [18, 49], since the self-connection has little eect on performance, the Eq. (6) is equivalent to Eq. (2). where𝑫∈ Ris the degree matrix of the hypergraph motif 𝑯𝑯.𝑯𝑯can be regarded as the motif-induced adjacency matrix.𝚿is the parameter matrix in the𝑙-th layer.𝒁is the group representation in the𝑙-th layer of the hypergraph convolutional network, where𝒁=˜𝒁. The˜𝒁learns from the attention-based group preference aggregator. 3.1.3 Loss function. Here, we adopt the pairwise learning task loss function to optimize user and item representations in the user-level hypergraph, which is designed as follows: where˜𝒑represents the embedding vector for user𝑢.˜𝒒represents the embedding vector for item𝑖.𝑂represents the training set, in which each one includes the user𝑢, interacted item𝑖, and unobserved items𝑗. Our target is to make the margin between the positive samples(𝑢, 𝑖)and negative samples(𝑢, 𝑗)is as close to 1 as possible. For the group-level hypergraph, its loss function can be achieved as follows: where𝒛is the group representation learned from the attentionbased preference aggregation strategy. Similar to the user-level loss,𝑂represents the training set, which includes the group𝑔, interacted item𝑖, and unobserved items𝑗. To enhance the learning process of these two recommendation methods, we apply a joint training strategy, whose loss function consists of two parts: user preference loss and group preference loss. It is given by Despite the remarkable capability of hypergraph modeling, as groups may occasionally form, only minimal group-item interactions can be observed (i.e., the data sparsity issue), which may lead to sub-optimal recommendation performance [14,20,44]. Several models utilize the user member preferences to synthesize group preferences and alleviate the sparsity issue. However, they overlook the fact that the user-item interaction is also sparse. To address this problem and rene the representations of users and groups, we propose a double-scale node dropout strategy on the hierarchical hypergraph (including coarse and ne granularity) to augment the raw data and create two types of self-supervision signals for contrastive learning to boost the performance of HHGR. A selfsupervised variant of HHGR,𝑆-HHGR, is presented as follows. It should be noted that, for𝑆-HHGR, we upgrade the design in Section 3.1.1 and employ two new hypergraph convolutional networks to learn user representations, which can be seamlessly coupled with the double-scale self-supervised strategy. Coarse-grained node dropping strategy.We drop a certain portion of users at a coarse-grained granularity, where the magnitude of missing nodes is a hyperparameter. Unlike node dropping in a simple graph accompanied by deleting connected edges, the hyperedge may not be deleted when discarding some nodes in the hypergraph. Besides, when a deleted node belongs to multiple group hyperedges, the node would be deleted in all certain hyperedges. We dene the coarse-grained node dropping function𝑓, as follows: where𝒉represents the column vector of the coarse-grained hypergraph incidence matrix𝑯, and𝒂∈ {0, 1}is a mask vector with its entries being 0 at a given probability, controlling the dropout magnitude of the nodes in𝑯.⊙represents the elementwise product, which means the mask vector would multiply with each column in the user-level hypergraph incidence matrix𝑯. After the coarse-grained node dropping, we can obtain a perturbed user-level hypergraph. We encode it through a new hypergraph convolutional network𝑔(·)to get the coarse-grained user representation𝑷, which can be as one data augmentation of the raw user representations. where𝑫and𝑩are the vertex degree and hyperedge degree diagonal matrices of the coarse-grained hypergraph.𝚪denotes the parameter matrix in the coarse-grained hypergraph convolutional network.𝑷is the user representation in the𝑙-th layer of the coarse-grained hypergraph neural network, where𝑷=˜𝑷.˜𝑷is the randomly initialized d-dimensional representation matrix. Fine-grained node dropping strategy.We perturb the hypergraph construction by dropping a certain number of users in a particular group hyperedge. Analogously, we dene a ne-grained node dropping function 𝑓, which is represented as follows: where𝒉represents the column vector of the ne-grained hypergraph incidence matrix𝑯, and𝒂∈ {0, 1}is the ne-grained mask vector, which will drop nodes with a xed probability in only one hyperedge. Dierent from the coarse-grained strategy, each time𝒂is multiplied by𝒉, it is reassigned. Next, we encode𝑷 through 𝑔(·) to obtain a new user representation: where𝑫and𝑩are the vertex and hyperedge degree diagonal matrices of the ne-grained hypergraph convolutional network. 𝑯is the incidence matrix.𝚽represents the parameter matrix. 𝑷is the user representation in the𝑙-th layer of the ne-grained hypergraph neural network, where𝑷=˜𝑷. To avoid the loss of some essential information, we add two representations at the two granulates to get the nal user representation: 𝑷 = 𝑷+ 𝑷. Contrastive learning.Having established dierent granularity augmented views of nodes, we treat the granularity of the same node as the positive pairs and any dierent granularity of nodes as the negative pairs. We hope that the distribution of the user representation vectors from two pretext tasks can be as close as possible. Hence, we design a discriminator functionf(·)to learn a score between two input vectors and assign higher scores to positive pairs compared with negative samples. We adopt the crossentropy loss as contrastive loss function to enforce maximizing the agreement between positive pairs, which is dened as follows: where𝒑is the representation of user𝑗in the coarse-grained hypergraph;𝑛is the number of negative samples randomly selected from the same batch. To optimize the𝑆-HHGR’s parameters, we unify the loss functions of the task of group recommendation and self-supervised learning. We rst update the loss function of self-supervised learning,L, as the pre-training strategy to optimize the user representations. Then we optimize the supervised loss function ofLandL to obtain the representations of users and items. Meanwhile, the model will be ne-tuned and updated during the process of selfsupervised learning. The learned user embedding would be used to generate group representations, which are updated in the objective functionL. Specically, we train our model using the Adam algorithm. The overall objective is dened as follows: where𝛽is the hyper-parameter to balance the task of self-supervised learning and supervised learning task for group recommendation. In this section, we discuss the complexity of𝑆-HHGR from model size and time complexity. Model size.The parameters of our model consist of three parts: 1) hierarchical hypergraph convolutional network, 2) attentionbased group preference aggregator, and 3) discriminator network. The coarse-grained and ne-grained user-level hypergraph convolutional network have parameters of size2[𝑙×𝑑×𝑑]. The parameters size of the group-level hypergraph convolutional network is𝑙 ×𝑑 ×𝑑. The parameter size of the trainable weighted matrix in the preference aggregator is3 × 𝑑 × 𝑑. The discriminator measures the similarity between user representations of dierent granularities, the weight matrix inf(·)has a trainable weighted matrix𝑾 with the shape 𝑑 × 𝑑. Time complexity.The computational cost mainly comes from two parts: the hierarchical hypergraph convolution and the doublescale self-supervised learning strategy. The time complexity of hierarchical hypergraph is mainly from the information propagation consumption, through𝑙, is less than𝑂 (𝑙 × |𝑯 | × 𝑑), where|𝑯 | denotes the number of nonzero elements in the incidence matrix 𝑯. Since our model contains a user-level hypergraph convolutional network and a group-level hypergraph convolutional network, the time complexity of the hierarchical hypergraph convolutional neural network is about𝑂 (𝑙× |𝑯 |× 𝑑). As for the self-supervised learning strategy, the cost derives from the hypergraph generation of dierent perspectives and contrastive learning. The time cost of hypergraph generation is less than𝑂 (2 × 𝑀 × 𝑀). The time cost of contrastive learning is less than𝑂 (|U | × 𝑛), where𝑛is the number of negative samples. In this section, we conduct extensive experiments to justify our model’s superiority and reveal the reasons for its eectiveness. Specically, we will answer the following research questions to unfold the experiments. RQ1:Compared with the state-of-the-art group recommendation models, how does our model perform? RQ2:What are the benets of each component (i.e., the hierarchical hypergraph and the self-supervised learning) in our model? RQ3:How do the hyper-parameters inuence the eectiveness of the 𝑆-HHGR? 4.1.1 Datasets. We conduct experiments on three public datasets: Weeplaces [32], CAMRa2011 [28], and Douban [44]. Weeplaces dataset includes the users’ check-in history in a location-based social network. We follow GroupIM [32] to construct group interactions by using the user check-in records and their social network. As for the CAMRa2011, it is a movie rating dataset containing individual users and households records. We follow the idea of AGREE [6] and convert the explicit rating to implicit preference, where the rating records are regarded as 1. Douban dataset is from the Douban platform, which consists of a variety of group social activities. As the Douban dataset does not contain explicit group information, we follow the idea of SIGR [44] to extract implicit group data. Specically, we regard users’ friends who have participated in the same activity as the group members. The statistical information of the three datasets is shown in Table 1. We randomly split the set of all groups into training(70%), validation(10%), and test sets(20%). 4.1.2 Baselines. To answer theRQ1, we compare the proposed model with the following models: • Popularity. This method ranks items according to their popularity. • NeuMF[19]. NeuMF is a neural network-based collaborative ltering model to benchmark the recommendation performance. We treat all groups as virtual users and utilize group-item interactions to generate the group preferences. • AGREE[6]. This method utilizes attentional preference aggregation to compute group member weights and adopts neural collaborative ltering to learn the group-item interaction. • MoSAN[35]. MoSAN is a neural group recommender that employs a collection of sub-attentional networks to learn each user’s preference and model member interactions. • SIGR[44]. This is a state-of-the-art group recommendation model, which introduces a latent variable and the attention mechanism to learn users’ local and global social inuence. It also utilizes the bipartite graph embedding model to alleviate the data sparsity problem. • GroupIM[32]. This model aggregates the users’ preferences as the group preferences via the attention mechanism. It maximizes the mutual information between the user representations and its belonged group representations to alleviate the data sparsity problem. HHGRis the vanilla version of our proposed model, andS− HHGR represents the self-supervised version. 4.1.3 Evaluation metrics. To measure the performance of all methods, we employ the widely adopted metrics𝑁 𝐷𝐶𝐺@𝐾and𝑅𝑒𝑐𝑎𝑙𝑙@𝐾 with𝑘 ={20, 50}.𝑁 𝐷𝐶𝐺@𝐾evaluates the ranking of true items in the recommendation list.𝑅𝑒𝑐𝑎𝑙𝑙@𝐾is the fraction of relevant items that have been retrieved in the Top-K relevant items. 4.1.4 Parameter seings. For the general settings, the embedding size is 64, the batch size for the mini-batch is 512, and the number of negative samples is 10. During the training process of the doublescale self-supervised learning, the initial learning rate is5𝑒 − 4. For the group-level hypergraph training, the learning rate is1𝑒 − 4. The user-level and group-level hypergraph neural network structure is two-layer and one-layer, respectively. For the baseline models, we refer to their best parameter setups reported in the original papers and directly report their results if we use the same datasets and evaluation settings. 4.2.1 Overall performance comparison. In this part, we validate the superiority of HHGR and𝑆-HHGR on three datasets. Table 2 shows the experimental results of the proposed models’ performance compared with the baselines. We highlight the best results of all models in boldface. According to the results, we note the following key observations: 1) Among these methods, the attention-based group models outperform baseline recommenders (i.e., Popular and NeuMF) on most datasets due to their ability to dynamically model the user interactions within groups and learn various weights of dierent members in the group. 2) In the models based on attention mechanism, HHGR is better than most models (including AGREE, MoSAN, and SIGR). We believe that the performance improvement of HHGR veries the eectiveness of hypergraph and hypergraph neural network modules to exploit high-order user interactions. 3) SIGR performs better than AGREE and MoSAN due to considering the form of bipartite graph to represent user-item interactions, especially on the CAMRa2011 dataset. Besides, MoSAN also achieves better results because of its expressive power of preference aggregators to capture dierent personal weights in group-item interactions. 4) On the other hand, although HHGR is slightly inferior to GroupIM, the enhanced model𝑆-HHGR beats the most advanced group recommendation models on three datasets, which veries the eectiveness of self-supervised learning strategies. Figure 3: Performance comparison of attention-based group recommendation models on sparsity datasets. 4.2.2 Performance on sparsity datasets. We adopt the self-supervised learning strategy to alleviate the data sparsity problem. To validate the eectiveness of the proposed double-scale node dropout strategy, we study experiments on sparse datasets. We split the dataset into four groups based on the number of group-item interactions. We nd that the performance of all models varies with the number of group interactions. Due to the limited space, we only show the performance of attention-based models on the Weeplaces dataset. The results are shown in Figure 3. As shown in Figure 3, NDCG and Recall generally increase as the number of interacting groups of an item increases, since more interactions will provide more information for learning the group preference representation. Without the auxiliary information, models that adopt self-supervised learning (GroupIM and𝑆-HHGR) are competitive to the models without self-supervised learning. Besides, we can observe that𝑆-HHGR shows the best performance compared with other models. We argue that the eectiveness of𝑆HHGR can be attributed to the double-scale node dropout strategy in hypergraph neural networks, which can enhance the representations of group preferences. 4.3.1 Investigation of the hierarchical hypergraph. To investigate the eectiveness of the hierarchical hypergraph, we conduct experiments on Weeplaces and CAMRa2011 for two HHGR variants, each of which has one of the level removed.HHGR-wgandHHGRwuto denote the ablated model without group level or user level. HHGR-wg only considers the user-level hypergraph, which means the group preference representations generated from the preference aggregator are the nal group preference. HHGR-wu adopts the group-item interaction information to initialize the group preference and only considers the group level hypergraph to update the group preference. The experimental results are demonstrated in Table 3. From the results, we can observe that the performance of HHGR-wu falls to the performance of HHGR-wg and HHGR on two datasets. Compared with the Weeplace dataset, the performance of the HHGR and its variants on the CAMRa2011 is better. The possible reason might be that the CAMRa2011 dataset is denser, which is benecial to selecting informative inter- and intra-group user interactions. Table 3: Comparison between HHGR and its variants. 4.3.2 Investigation of self-supervised learning.Dierent types of self-supervised learning.To investigate the feasibility and eciency of self-supervised learning, we conduct an ablation study to investigate each component’s contributions. We propose two variants of𝑆-HHGR:HHGR-FandHHGR-C.HHGR-Fmeans that only ne-grained node dropping is considered.HHGR-Cmeans only coarse-grained node dropping is used. For the above two variant models, we maximize the mutual information between the initial node representations and the dropping ones. Considering the limited space of the paper, we only compare these models with 𝑆-HHGR on Weeplaces and CAMRa2011. Figure 4 shows that𝑆-HHGR achieves the best performance over independent granularity models, where self-supervised learning strategies contribute to the recommendation performance. When we only use ne-grained node dropping, the HHGR-F performs slightly worse than𝑆-HHGR but better than HHGR-C, which implies the ne-grained node dropping contributes more to the node representation learning. We argue that the ne-grained task would generate more hard samples, strengthening model training. When we only use the coarse-grained node dropout strategy, the group recommendation performance may be slightly better than HHGR. Besides, without the self-supervised learning strategy would result in a performance decline in most cases on two datasets. The performance of HHGR is worse than HHGR-F and HHGR-C in most cases, indicating that the self-supervised strategy is eective. Overall, the experimental results show that self-supervised learning is useful. Figure 4: The inuence of dierent self-supervised learning strategies. Dierent extent of self-supervised learning.We note that the use of dierent granularities of node dropping benets the node representation. We further analyze the extent of coarse and ne granularity node dropout. Specically, we set dierent dropping rates from 0.1 to 0.9 in two granularity tasks. When we change the rate of one granularity task, we will x another granularity’s rate. As presented in Figure 5, with the increase of dropping rate, there is a signicant increase in performance. When the rate reaches the peak, approximately 0.3 and 0.2 on ne and coarse granularity dropping, respectively, it decreases. It suggests that node dropping can improve the HHGR performance with increasing dropping strength, but too many deletions will worsen the data sparsity problem. Figure 5: The performance of dierent extent of two pretext tasks. This part focuses on pointing out which hyper-parameters aect our model. The analyzed hyper-parameters include the learning rate, the depth of hypergraph convolutional layers, the number of batch sizes, and the number of negative samples. Due to limited space, we only show the experimental results on the Weeplaces dataset. Specically, we search the proper value in a small interval and set the learning rate as{1𝑒 − 5, 5𝑒 − 5, 1𝑒 − 4, 5𝑒 − 4, 1𝑒 − 3}, and report the performance of𝑆-HHGR with ve values{1, 2, 3, 4, 5}of the hypergraph convolutional layer. Besides, the batch size changes from 16 to 512 at intervals of increasing powers of 2, and the number of negative samples is set from 5 to 30 at intervals of 5. Figure 6: The inuence of the model parameters. As shown in Figure 6 (a), we observe5𝑒 − 4is sucient for the learning rate. With the increase of the rate, the performance shows a trend of rising rst and then falling, reaching the peak at 5𝑒 − 4. According to Figure 6 (b), when the depth of hypergraph convolutional layers is 2, the model reaches the performance peak. As the depth of convolutional layers increases, the performance of𝑆-HHGR steadily declines. A possible reason is that a multilayer hypergraph convolutional network would lead to the oversmoothing problem. HHGR model also encounters this problem, but it tends to over-smoothing at lower layers. We believe that a self-supervised learning strategy can alleviate the data sparsity problem because it would delete some redundant information to enhance the model performance. Besides, the parameter of batch size is also important. Figure 6 (c) depicts the change of model performance with respect to dierent batch sizes from 16 to 512. The optimal size is 512. Therefore, we set the batch size to 512 when we train the model. As shown in Figure 6 (d), when the number of negative samples is 10, our model achieves the best performance in the Weeplaces dataset. We can conclude that a small number of negative samples can promote the recommendation task, while a bigger one would mislead it. Early work on group recommendation is usually based on the preference integration of group members or the score aggregation of an item across users. The three most common aggregations include the average [5], the least misery [1], and the maximum satisfaction strategy [3,4]. The average method takes the mean score of all members in the group. The least misery is committed to ltering all members’ smallest score to please everyone. The maximum satisfaction strategy aims to reach the maximum satisfaction of the member in the group. These three aggregating methods have a signicant drawback: they are oversimplied and ignore user interactions in groups. Recently, with the successful development of deep learning, models based on the attention mechanism are becoming more and more popular for modeling intra-group user interactions and learning dierent users’ inuences [6,35]. For example, GAME [20] utilizes the heterogeneous information network and attention mechanism to learn the nodes’ multi-view embeddings and members’ weights. Yin et al. [44] propose to take the attention mechanism to exploit each user’s intra-group user interactions and adopt the bipartite graph embedding to mitigate the data sparsity problem. Although the above methods based on the attention mechanism have made remarkable achievements in group recommendation, they hardly consider the high-order user interactions. Although the graph neural network approaches have achieved successful results on capturing high-order relations in various tasks [7,25,56], these approaches are only appropriate in pairwise connections, which has limitation in expressing complex structures of data. Hypergraph has shown promising potential in modeling complex high-order relations [36,46,51,53]. Recently, some studies try to combine the hypergraph with the recommender systems to improve their performances. For example, DHCF constructs two hypergraphs for users and items to model their high-order correlations and enhance recommendation models based on collaborative ltering [22]. Xia et al. [39] model session-based data as a hypergraph and use the hypergraph neural network to enhance session-based recommendation. Wang et al. [37] incorporate the hypergraph into next-item recommendation systems to represent the short-term item correlations. Yu et al. [49] propose a multi-channel hypergraph convolutional network and design multiple motif-induced hypergraphs to exploit high-order user relations patterns. Despite the tremendous eorts devoted to these models, they just adopt the paradigm of hypergraph representation learning to model high-order relations. Our models dier signicantly from previous approaches in that we use the motif to select informative group interactions. Self-supervised learning oers a new angle to generate additional supervised signals via transferring the unlabeled data [2,9,15,24, 48]. Generally, there are two types of self-supervised learning approaches: generative models and contrastive models. Generative models learn to reconstruct the input data and make it similar to raw data [12,27]. Contrastive models learn discriminative representations by contrasting positive and negative samples [8,13,17,30]. Recently, some studies attempt to explore the self-supervised learning framework for the recommendation [41,42,54]. For example, Zhou et al. [54] propose the𝑆-Rec model and design some selfsupervised training strategies to pre-train the model. Bert4Rec [33] adopts the Cloze objective to the sequential recommendation by predicting the random masked items in the sequence. Xin et al. [41] adopt the cross-entropy as self-supervised Q-learning to netune the next-item recommendation model. Xie et al. [40] propose three data augmentation operations: item crop, item mask, and item reorder, as self-supervision signals to enhance sequential recommendation. The closest work to ours is GroupIM [32], which maximizes the mutual information between users and groups to optimize the representation of both. Unlike the above approaches, our work is the rst to consider the double-scale node dropout strategies in hypergraph as the supervised signals in group recommendation. We maximize the mutual information between the same hypergraph’s dierent views to enhance data representations and improve recommendation performance. In this paper, we proposed a novel neural group recommendation with the hierarchical hypergraph convolutional network and selfsupervised learning strategy to capture inter- and intar-group user interactions and alleviate the data sparsity problem. With the triadic motifs, the proposed model can obtain more reliable user interactions beyond groups for more accurate group representations. We innovatively designed a double-scale node dropout strategy as the supervision signals against the data sparsity problem. We conducted extensive experiments on three public datasets to evaluate our models’ performance. The experimental results veried the superiority of HHGR and its enhanced version based on self-supervised learning (𝑆-HHGR) compared with other state-of-the-art models. In the future work, we tend to deepen the application of self-supervised learning in group recommendation models and design more general auxiliary tasks for the recommendation to improve the recommendation performance. This work was partially supported by National Natural Science Foundation of China (6217022345), Natural Science Foundation of Chongqing, China (cstc2020jcyj-msxmX0690), Fundamental Research Funds for the Central Universities of Chongqing University (2020CDJ-LHZZ-039), ARC Discovery Project (DP190101985), and ARC Future Fellowship (FT210100624).