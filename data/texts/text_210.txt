Coordination protocols help prog rammers of distributed systems reason about the effects of tran sactions o n the state of the system, but they’re not cheap. Coordination protocols may involve multiple rounds of communication, which can hurt system responsiveness. There exist many efforts in distributed computing for man aging the coordination-performance trade-off. More recent is a lin e of work that chara c te rizes the class of workloa ds for which coordination is not necessary for consistency: namely, logically monotonic programs [9]. In this paper, we present a case study of logical monotonicity in workloads typical to computational biolo gy. We leverage the Bloom langu a ge to write efﬁcient distributed programs, and compare th eir performance to equivalent programs written in UPC++, a popu la r language for writing distributed programs. Additionally, we leverage Bloom’s analysis tools to identify points-of-coordination, and use our own experience using Bloo m to recommend some higher-level abstractions for users without strong distributed computing backgro unds. The rapid rise of cloud computing in recent years has brought exciting new cha llenges in the area of programming languages. The overarching question is how we can make cloud progr a mming easier, and thus make the cloud accessible to a wider range of applications. The Bloom language is one of the leading efforts in this direction [2]. Bloom is a language for diso rderly distributed programming based on the principles of the CALM (consistency as logical monotonicity) theorem to guide coordination. The CALM theorem states that coordination avoidance is possible when programs are associativity, commutative, and idempotent. The ability to impleme nt an app lication according to these properties would naturally lead to a coordination-free implementation that would take greater advantage of cloud com puting. We validate some of the assumptions of Bloom by asking the following questions: First, how common are logically monotonic p rograms in the wild? Speciﬁcally, in the area of compu ta tional biology. And second, does merely k nowing where the coordination-points lie in a non-monotonic p rogram help us rewrite the program to move coordination off the cr itical path or othe rwise improve performance? In this work, we borrow an application from c omputational biology (k-mer counting) an d implement it in Bud (Bloom Under Development) a nd compare such an implementation with a standard UPC++ implementation. In addition, we reﬂect on the design of the probabilistic data structure count min sketch to reach a more space-efﬁcient implementation. Our work demonstrates that the assumptions of Bloom do in fact hold in practice, and Bloom is suitable for scientiﬁc computing. Additiona lly, we leverage our experience using Bloo m to propose some changes and a higherlevel abstrac tion, one that is more use-friendly to programmers without strong distributed computin g backgrounds. In sum, the second core contribution of this paper is “BuDDI”, an enhanced version of Bud using Distributed Data Independence (DDI). BuDDI hides the challenges of distributed computing such as load balancing, data locality, and fault tolerance from the programmer so that even a novice programmer can take advantage of distributed co mputing. This is critical as the explosion of data from many doma ins in recent years makes distributed computing a necessity rather than a commodity. BuDDI uses the concept of global state and achieves the same goal o f Bud in terms of coordination with comparable performance, while logically using global tables instead of local tables. The paper is organized as follows. Section 2 describes the key global table concept in BuDDI and how it inﬂuences the design of BuD D I. In Sec tion 3 we describe another key construct, the iterator, while in Section 4 we argue fo r a new design of the Nil c oncept for cloud programming. Section 5 illustrates in detail the implementation of th e k-mer cou nting case study as well as the design of the coun t min sketch, which is a prob abilistic data structure used for k-mer counting. Finally, Section 6 summarizes our conc lusions and future work. In Bud, each worker can only read from and write to its own tables; communication between workers is explicitly managed through channels. By restricting tables to strictly local visibility, Bud ensures that communica tion is only explicit when instructed by the developer, and is always asynchronous. That is, a table update never triggers expe nsive consensus or co mmunication protocols. From our perspective, these design decisions make the Bud programmer responsible for both: 1. The efﬁcient performance of distributed comp uting: load balancing, straggler mitigation, data locality, resource disaggregation, auto-scaling, etc. 2. The intended behavior of distributed computing: fault tolerance, fault detection, reliable communication, causal delivery, etc. By allowing the compiler and ru ntime to ma nage the data placement and communication of tuples stored in global tables, we signiﬁcantly reduce the burden on the programmer. By supporting global tables, we empower the compiler an d runtime to ma nage caching, replication, sharding, and other data allocation decisions. With a global tab le ab stra ction, the runtime environment can use online statistics and metadata to reorganize data as needed for latency, throughput, or re liability. In this section we will show that it is possible to provide a global table abstraction without sacriﬁcing the consistency and performance expected from Bloom languages. In this work, we rely h eavily on the work of CRDTs (conﬂictfree replicated data type s) for inspiration [11]. In BuDDI, the global tables are CRDTs [11]. This means that when a pr ogramme r inserts a tuple into a g lobal table, the insertion su cceeds imm ediately and the new tuple is shar ed asynchronously. This ensures that BuDDI workers remain responsive even in the presence of network partitio ns. The nature of CRDTs ensures that concurrent upda te s are eventually resolved. A BuDDI programmer statically spe c iﬁes the CRDT type of global tables, which can be: • G-Set: Grow-only set. • 2P-Set: Two-phase set comprised of p ositive and negative sets, each o f which is a G-Set. • LWW-Set: L ast-writer-wins set represented as a timestamped 2P-Set. • MV-Set: Multi-value set, a dynamo-esque 2P-Set. By statically deﬁning legal operations on global tables in advance, the developer enables the compiler and runtime to avoid unwanted coordination. G-Sets are coo rdination free on non- monotonic queries, and may wait to receive all tuples bef ore computing a non-monotonic query. 2P-Sets may have to wait to receive all tuples in both the positive and negative sets to service a monotonic read. The same coordination requirement applies when expressing equivalent computation in Bud, such as in the shopping cart example at checkout [2]. The re a der sho uld note that tuples in global tables (i.e. CRDTs) are not necessarily replicated by default. The decision whether to replicate (or cache or sha rd) depends on the programmer’s service-level goals and is a customizable target f or the compiler and runtime to hit. Whenever a Bud programmer organizes data such that the tuples are hash or range partitioned along the GROUP BY columns, the workers may compute aggregates without com munication . We refer to this com munication avoiding coordination strategy a s “compile-time coordination” because the programm er implicitly and statically informs every worker that data stored at other workers is irreleva nt for the computation of the aggregate. We will next show how the BuDDI compiler can manage d a ta allocation to exploit compile-time coordination automatically in global tables. Explicit hash or range partitioning of data might be an easy ta sk for Bud progra mmers, but ﬁxed data partitioning logic pr events the runtime from ad apting on the ﬂy to changes in the number of workers, per-worker processing rate, and network conditions. In addition, the distribution of a d ata set is not always known in advance by the developer. This could lead to serious load balancing problems if the data is skewed. Globa l tables allow the BuDDI compiler to parse the query to make data distribution decisions and inject switch o perators into the IR so th a t the runtime can dyna mically detect and adapt to ch a nges. For example, the data may be initially partitioned in a range, but after a skew is observed, the opera tors in the IR can switch to round-robin partitioning and inform the system of the need for communication-based coo rdination. Such adjustments are not easy to expr ess with only local tables and channels, but they follow naturally from the semantics of global tables based on CRDTs. In Section 5 .2, we show a k-mer counting implemen tation in BuDDI that uses a global table of type G-Set, and relies solely on compile-time coordin ation. Consider how a Bud programmer would remove items from a shopping ca rt: shoppingcart := shopping cart - bad items (1) Because shopping cart appears on the left hand side and right hand side of the assign ment, this is a recursive query tha t requir es stratiﬁcation: the query is evaluated repeatedly until a ﬁxed-point is reached [8]. It is possible to statically determine th e need for stratiﬁcation by detecting cycles in the dataﬂow graph. As we will show in this section, it is not necessary to execute all recursive queries to a ﬁx -point. Some recu rsive queries, such a s the one in Eq. 1, can be evaluated in one -shot. It is possible to evaluate th e recursive query in Eq. 1 in one shot because the same qu ery could be written using two G-Set: shopping The query is no longer recursive, and it is now statically possible to determine that the query may be safely evaluated in on e-shot, without the nee d for stratiﬁcation. In fact, this query is a manual implementation o f a 2P-Set, where wanted items are add e d to the “pos” set and unwanted items are added to the “neg” set. In BuDDI, the same query would be expressed using a 2P-Set: shopping cart := 2P Set(‘pos’, ‘neg’) ...(3) query(shoppingcart) If the semantics of a table provide the proper ﬁt for a 2P-Set, the programmer ben eﬁts from the following advantages when entering the table as a 2P-Set: 1. One-Shot Fixed-Point Computation : The runtime evaluates the contents of the table safely in one step. 2. Async Gar bage Collection: The runtime knows that it may safely delete elements from the pos-set iff it deletes the cor responding item from the neg-set. 3. Compile-Time Co ordination: T he compiler knows that it can make d ata alloc ation decisions such that tuples in the same grou p are on the same worker in both the pos-set and the neg-set. In other words, the runtime could check the neg-set only at the local worker witho ut comm unication and know if the tuple in the pos-set was deleted. As our reader probably already k nows, the n eg-sets o f 2P-Set are tombstones. Once a tuple is added to the neg- set, the same tuple ca n never be added to the 2P-Set again. Such a restriction violates the semantics of the set and makes it difﬁcult or impossible to implement stateful applications. A workaround for achieving stateful execution with CRDTs is to append a universally unique identiﬁer (uuid) to each tuple. This way, an object that has been removed from the 2P-Set can be added back with a new uuid. From now on we will refer to a 2P-Set which models the set sema ntics as True-Set. True-Set supports add, remove, add after remove, and u pdate. An advantage of the uuid approach is that it is possible to add tuples to a True-Set without coordination, since the pro bability of collisions guaranteed by crypto graphica lly secure random number generators is negligible. This quality we call zero- knowledge insertion, because a worker may insert tup les into True-Set without knowing the content stored in replicas. However, the uuid approach does not support zero knowledge deletion or update. To update the contents of an object stored in a True-Set , the worker must read the contents of the replica s to lear n the uuid of the object. Blindly deleting the obje ct could result in a n anomaly where the new tuple is deleted because the messages may be reord ered. Consequen tly, the delete o peration must be parameteriz ed by the uuid of the inte nded object and the request for the uuid must be served synchronously. Using timestamp s instead of uuid allows the support of updates and deletions with zero knowledge (in addition to insertions) for True-Set. The True-Set implements a variant of Last-Writer-Wins (LWW) or MultiValue-Set (MV) to contr ol concur rency. For example, a True-Set implementin g LWW would report the object with the latest timestam p. Outdated versions of the object with old tim estamps would be ignored or garbage collected. A worker could delete an object fro m the True-Set without a synchronous read by inserting the object with a later timestamp into the tombston e. If the timestamp of th e object in the tombstone is greater than all the timestamps of th e versions of that object in the pos-set, then the object is considered deleted. Clock synchronization is not exactly a coordinationfree affair, but such is the cost of sacriﬁcing commutativity in CRDTs and imposing order in the cloud. As global tables are to local tables, so are iterators to channels. In this section, we will show that iterators are languag e constructs that can increase the declarativity of Bloom languages, especially with respect to stream or unstructured data p rocessing. In early 2019, it was announced that a black hole had been ph otographed for the ﬁrst time [1]. Soon after, images of Dr. Katie Bouman, standing behind several stacks of hard drives, began circulating on Twitter. The data needed to map the blac k hole weighed a total of 4.5 petabytes. The question is, how can Bloom lang uages and the cloud in gene ral even read such large ﬁles? Database Management Systems can sort ﬁles of any size even with low memory specs. At the highest level, the key is to process the ﬁle incrementally, bringing it into memory one chunk at a time, and keeping track of the work that remains to be done. Most of this behavior is provided by the iter ator abstraction. The caller invokes next, an d the iterator retu rns a tuple or group of tuples (chunk), for pro cessing. The iterator en sures that the next chunk of work to be pr ocessed by th e caller. One approac h for managing chunk size is to use as many chunks as ther e are workers—to enable highthroughput reads without re quiring coordination . Unfortun a te ly, this approach is vulnerable to stragglers and cannot adapt on-the-ﬂy to workers that enter or leave the workpool by default. A workaround is to use a strategy similar to that used in MapReduce to allow the number of chunks to be much larger than the number of workers [6]. Chunks are assigned to workers fo r delivery as they complete work. Faster workers do more work than slower workers. When new workers join, they can be assigned work at any time. If a worker fails or leaves the group, data they have completed is not reassigned, and data they have not completed is returned to the po ol of pending work. At least once delivery under set sem a ntics is exactly-once semantics, because sets ﬁlter duplicates. But as we discussed in Section 2.4, we have reason to give the same entity more than one uu id, to enable re-insertion after deletion from a 2P-Set. The solution then is to g ive each data unit (e.g. tuple) two identiﬁers: one uuid identiﬁes the token (e.g. the byte-offset from the start of the ﬁle); the second uuid identiﬁes the use of the token. Thus, an object that g e ts re-inserted to a 2P- Set would have the same token-uuid as the one that is in the tombstone, but it would have a different use-uuid. We b elieve it’s possible to statically assign token-uuids to units of data without coor dination, such as when we stamp each k-mer with its offset fro m the start of the ﬁle. Use-uuids are assigned at runtime and may use either uuids or timestamps, as discussed in Section 2.4, With a token-uuid and use-uuid, it’s possible to exploit at-least-once delivery on CRDT True-Sets to achieve exactly-once semantics. All this with the beneﬁts of auto-scaling to workers entering or leaving the work group, or workers speeding up and slowing down. In developing BuDDI and implementing our case study in b oth BuDDI and Bud in the following section, the question arises as to how the programmer would interpret the vairable Nil. In particular, the question is whether Nil tells us th at the value does n ot exist for all queries and th us requires global coo rdination, or whether it tells us that th e local worker is una ble to ﬁnd it. The latter behavior is more appropriate for monotonic programming; although it might introduce anomalies, it can maintain responsiveness during a network partition. One could argue that the Nil con cept is an inherited appendage from the m onolithic compu te r e ra. Here we argue that for cloud pro gramming we should rethink the Nil concept as having not on e but two values: e ither DNE (“does not exist”, which is a global assertion) or IDK (“I don’t know”, which is a local assertion). If the network is healthy and the data ha s been partition e d so that coordination is free or inexpensive (Sectio n 2.2), the stronger property is considered and DNE is returned. Otherwise, IDK is returned. We believe that DNE and IDK are sufﬁciently well understood by programmers to be useful boolean primitives. For example, given a DNE, you could execute a blo ck of code, whereas given a IDK, you would perform some o ther operation, such as retry, crash, sleep, or guess and apologize. In Section 5 we will see how the Nil concept is not currently implemented in Bud and how its implementation would give the programmer more ﬂexibility. A common computation in Computational Biology is to count the frequency of ﬁxed length sequences known as k-mers. The k-mer histog ram tha t we obtain as a result is valuable for u nderstanding the distribution of biological subsequences and for p roﬁling genomic and metagenomic data. For example, we may be interested in subsequences that occur within a certain interval or above a certain threshold. The k-mer cou nting step often takes a large part of the to ta l application runtime and it is a key computation within p opular tools for taxonomic mapping [12], metagenome classiﬁcation [4], genome assembly [10]. The k-mer count is arguably a simple ca lc ulation, but its efﬁcient implementatio n is anything but simple. This problem has received much attention as an important target for shared memory parallelism. As data sets grow faster and faster, distributed memory parallelization is becoming more and mor e important. Nevertheless, the irregularity of the input data makes k-mer counting a dif ﬁcult problem for distributed memory parallelization. In particular, the k-mer distribution over biolog ical input data is not ﬁxed and can only be determined at runtime. In this work, we implem ent a toy version of the k-mer counting kernel in Bud and compare it to a UPC++ implementation, which resembles a m ore standard way of implementing this computation. UPC++, similarly to Bud, makes use of asynchronous communication. From a high level perspective, the main difference between the two codes is that the implementation based on Bud must follow Bloom’s rules of monotony a nd idempotency. UPC++ UPC++ [3] is a C++ library supporting Partitioned Global Addre ss Space (PGAS) programming. UPC++ is suited for impleme nting complex distributed data structures where communication is irregular or ﬁne grained. The main abstractions in UPC++ are: (a) global pointer to improve locality, (b) asynchronous remote procedure call (RPC), and (c) f utures. Listing 1 illustrates the key implementation for the UPC++ version of k-mer coun ter. The k-mer counter materializes as a distributed hash table divided by keys, where k-mers are keys and their frequencies are values. The p rogram ﬁrst parses the input data in parallel, so that each processor has a part of the input sequ e nces. Each processor parses its local sequences in k-mers and determines which k-mers remain local and wh ic h mu st be sent to ano ther processor based on a hash function . When a processor receives incoming data from other processors, it upda te s its lo c al partition of the k-mer hash table by incrementing the frequency corresponding to the received k-mers. A given k-mer is counted by one processor and only one proc essor. Communica tion in UPC++ is asynchronous, and in our implementation we use a remote procedure call to update values in the hashmap that do not belong to the local processor. A rem ote procedure call ca uses a procedure to be executed in a different address space, encoded like a norma l proc edure call, without the progr ammer explicitly coding the details for the remote interaction. /* <kmer, count> map */ using dobj_map_t = /* build empty map */ dobj_map_t local_map{{}}; /* compute owner for the given key */ int get_target_rank(const string &key) return hash<string>{}(key) % rank_n(); void local_update(unordered_map<string, auto it = lmap.find(key); if(it != lmap.end()) it->second++; future<> populate(const string &key) /* send rpc to the owner rank */ return rpc(get_target_rank(key), [](dobj_map_t &lmap,const string &key) }, local_map, key); Bud Listings 2–4 show d ifferent implementations that we developed for our case study in Bud. In particular, the implementation in Listing 3 uses domain-speciﬁc knowledge to optimiz e the memory footprint. Counting subsequences is monotone by nature, because once you have seen a k-mer instance, the k-mer count can only increa se or remain unchanged. One might ﬁrst think to use a lmap as a local partition, where k-mers are the keys and values are lmax lattices, since the k-me r count can o nly be incremented and lmax is deﬁned as an integer that can only increment. However, the default merge function of lmax is in fact the maximum between two entries, not the sum as we would wish. It is not po ssible to override the merge functio n or create a custom lattice that uses sum as the merge function because sum is not idemp onent. The co mputation can be made idempotent by replac ing lmax with lset in the local partition. In this implementation, each k-mer instance in the local partition is assigned a unique ide ntiﬁer. If ATAG occurs twice in the inpu t data set, the corresponding lset in the local lmap stores two unique identiﬁers. Once the computation is c omplete, we perform a local count of lset and display the k-mer frequency for eac h k-mer in the input data set. In this work, we refer to this implementation as Implementation A (Listing 2). Listing 2: Bud k-mer counting: Implementation A. This implementation is not m e mory efﬁcient becau se it stores as many uuid as k-mer instances. # parse file and store sequences in an array class DNA def readseq(myinput) sequences = Array.new ParseFasta::SeqFile.open(myinput).each_record sequences.push(rec.seq.upcase) end end defkmers(sequences, k) subsequences = Hash.new for item in sequences do i = 0 while i < item.length-k+1 do end end karray = Array.new karray = subsequences.to_a end end classCountKmer include Bud statedo scratch :receive, [:seq, :uuid] lmap :local lmap :incoming lmap :counter table :result channel :msg, [:seq, :uuid, :@addr] end bloom :ownership do # ip port based on sequence owner = hash(t.seq) % worldsize leave <= kmer {|t| [t.seq, t.uuid, owner]} msg <~ leave do |seq, uuid, owner| [seq, uuid, owner] end end # update local set with incoming data bloom :insert do receive <= msg do |seq, uuid, owner| if owner == ip_port end end # merge incoming kmer into local map incoming <= receive {|t| {t.seq => counter <= {ip_port => local} end bloom :count do result <= counter.to_collection do end end end Implementation A is monoton e and idem potent, however, it poses some concern about the memory consumption. In a medium -sized genome data set, we have billio ns of k-mers and some of these can occur hundreds of times in the input data set. Therefore, storing an identiﬁer for each k-mer instance looks extremely ine fﬁcient from a memory standpoint. Fortunately, dom a in-speciﬁc knowledge helps us in the design of our Implementation B (Listing 3). In general a user is only interested in subsequences that occur within a certain interval or below/above a certain threshold. This information c a n be elegantly integrated into the Bud implementation using custom lattices. Listing 3: Bud k-mer counting: Implementation B. This implementation uses domain-speciﬁc knowledge to save memory and stores uuid on ly up to a certain threshold. Implementation B uses a custom lattice Bud::BuddySetLattice which is a modiﬁed version of the standard Bud::SetLattice reported in Listing 4. class CountKmer include Bud [...] # update local set with incoming data bloom :insert do [..] # merge incoming kmer into local map incoming <= receive {|t| {t.seq => counter <+ {ip_port => local} end [...] end For example, if we are only interested in subsequences that occur above a certain threshold, we can create a custom lset where our merge function computes the union of two sets only if the reference set has not yet reache d the cu sto m threshold (Listing 4). The threshold is commonly much smaller than the maximum frequency of high-f requency k-mers, making Implementation B m uch more memory efﬁcient than Implementation A. Listing 4: Bud::BuddySetLattice is identical to the standard lset except for its merge function that uses domain-speciﬁc knowledge to avoid wasting memory. THRESHOLD = 10 class Bud::BuddySetLattice < Bud::Lattice wrapper_name :buddylset [...] def merge(i) if @v.size < THRESHOLD else end end [...] end Before we approached the solution with a custom lattice, we tried an im plementation with native lattices. In Listing 5 we use a naive lset lattice as the value in the lmap instead of the custom Buddylset in Listing 4. In this case we want to add the received k-mer to the local incoming lmap only if we have not yet reached the threshold for this k -mer key. The ﬁrst error we encountere d in impleme nting this version was a stratiﬁcation er ror, because when we merged incoming into local we used <= instead of <+. In this case, incoming merges with local, which in turn is used to calculate incoming. To ﬁx this error, we ha d to defe r the merging of incoming into local with <+ until the next time step. The stratiﬁcation problem is solved at this point, but we have the problem that local may not contain the key we are looking up, and this caused a runtime error complaining that it cannot ﬁnd the key. Here, we wonder if it would be possible to return a Nil value when a key is missing, rather than causing a runtime error. In Section 4, we brieﬂy discussed how introducing two values (DNE and IDK) fo r the Nil concept would enable us to implement this implementation of k-mer counting without using a custom lattice. In particular, returning a local IDK would be sufﬁcient in this case, since each k-mer (key) exists on one and only one process. Listing 5: Bud k-mer counting: Implementation B using only native typ es and related error. THRESHOLD = 10 class CountKmer include Bud [...] # update local set with incoming data bloom :insert do [..] # merge incoming kmer into local map incoming <= receive {|t| {t.seq => counter <+ {ip_port => local} end [...] end In this section, we demonstrate a h ypothetical implementation of the k-mer Counting algorithm in BuD D I. We use this example to demonstrate the use of G -Sets (a CRDT, or global table), and iterators (e.g. the call to open the ﬁle). T he take-away from this subsection is that (i) BuDDI programs are simple for Bloom progr ammers to write and understand, (ii) that their semantics are clear and unambiguous, and (ii) that the language is sufﬁciently declarative to allow for acceptable performance with the aid of an intelligent compiler working in concert with a specialized runtime. We leave both as future work. For reference on achievable perform a nce and semantics, the reader may want to refer back to Sections 2 and 3. The BuDDI programmer writes the k-mer counting program as a SQL query with aggregation over a globa l table. I n this case, the global table is called kmers, and it is implemented by a G-Set (or grow-only set). Aggregation, when exact, is a non-monotonic operation, but the runtime may begin processing the query on-the-ﬂy, and either report intermediate results or withhold them until the end as preferred by the user. BuDDI executions allow for workers to join or leave the worker pool on-the -ﬂy. New workers are added with the registerworker method which monotonically inserts th e worker’s metadata into a channel. BuDDI also uses an iterator (e.g . open in readdna file) to read the potentially massive ﬁle in parallel. As we discussed in Section 3. The iterator is able to estimate network conditions and worker perf ormance by the rate at which workers request data from the iterator. Thus, with the iterator, BuDDI is ab le to detect failures (a worker stops requesting data) and adapt on-the-ﬂy to slowdowns. The BuDDI compiler statically recognize s tha t this workload is hash-partitionable on the seq column of the kmers table, and may rendezvouz tuples with matching token- uuids (n o use-uuid required, since we are inserting into a G-Set), into the same worker, so no communication is required to compute an aggregate. If the from buddy import Channel, GSet, View from buddy import uuid, init worker = Channel(key=’@addr’, value=’file_uri’) kmers = GSet(key=’uuid’, value=’seq’) result = View(query=""" def read_dna_file(kmer_stream = open(worker.file_uri, req_id=uuid(), addr=worker.addr, mode=’char[4]’)): global kmers kmers += {(uuid(), kmer)for kmer in @init def register_worker(address: ’str’, dna_file_uri:str): global worker worker += {(address, dna_file_uri)} runtime does not r eplicate the data stored in the kmers CRDTs, then this execution an d schedule achieves comparable performance to the hand-tune d Bud implementation, with all the additional beneﬁts of declarativity. Count m in sketch [5] is a probabilistic d a ta structure that serves as a frequency ta ble of events in a stream of data. It is usually implemented as a data structure like a matrix, where h rows represent your hash functions and m columns represent th e ranges, where m is smaller than the number of k-mers because it is a sublinear data structure. When in serting a k-mer x, this is hashed with the h different hash functions an d the counter in the corresponding column of the matrix calculated as hashed value module m is incremente d. And then we take the minimum count over the h cells in the matrix where th is k-mer was hashed. In practice, m is related to the actual number of k-mers, and usually peop le use the Hyp erLogLog algorithm [7] to estimate the cardinality of the k-mer and chose m accordingly. The im plementation of a count-min sketch is more complicated than that of a regular k-mer counting algorithm, since the data distribution is no t straightforward and th e data access pattern is not contiguous. In this paper, we present two possible high-level designs and describe their current shortcomings. The ﬁrst design consists o f the compo sition of standard Bloom lattices while the second design is based on a custom lattice that is more similar to the original data structure. Using standard lattices, we can use a lset of size m, where each entry is a lmap of size h, where the key is the hash function id and the value is a lset of unique identiﬁers of k-mers; when merged, the lset increases its size by adding the corresp onding identiﬁer (uuid) to the entry [m]→[h]→[uuid]. In this case, the data distribution could be based on the m ranges, where for P processes e ach process has/P entries to take care of. This distribution is relatively easy to imple ment, but the irregular data access pattern makes it more complicated to imp le ment the min operator, because the h entries on which we want to perf orm the operation could potentially belong to h different processes. This data structure and distribution requires cross-process coordination, and the communication pattern resembles an MPI Reduce or Allreduce collective communication. The implementation of collective com munication in BUD will remain as f uture work. This ﬁrst design trade s coordination for memory usage, since each process has only one partition of the entire data structure, but may require coordination of all processes to reveal the ﬁnal result. A second design, based o n a cu sto m lattice, re sults in the opposite co mpromise: memory usage to reduce coordination. The custom lattice is very similar to the original data stru c ture, i.e . a h × m matrix A (e.g. two Ruby Arra y structures), where each A(i, j) entry is a k-mer unique identiﬁer lset. In this case, the distribution may be the same as we saw in the r egular k-mer counting algorithm, i.e. each k-mer is hashed to a single process, and this process calculates the h hash function on this k-mer and updates the corresponding matrix entries. Here each process has a local copy of the entire matrix, and the merge operation is the lset merge operation applied to each of the matrix entries. This means that consistency is ensured by an analysis at application level, which resolves write conﬂicts with a shared state. This design allows easier distribution and le ss coo rdination, but the m emory consumption is higher bec ause each p rocess has a local copy of the entire data structure. The use of identical copies of the matrix on each process justiﬁes the design and use of BuDDI and its logically global tables. In BuDDI, the runtime may do replication or partitioning of data struc tures, but this is abstracted for the programmer. In addition, BuDDI gives us access to the application-side code tha t we can use to learn th e semantics of the application layer. The application semantics may give BuDDI a way to resolve an apparent conﬂict, and can therefore allow BuDDI to reach good performance using w eak consistency. In this paper, we perfo rmed a case study of a representative HPC workload to evaluate the ﬁtness of CALM programming for computational science. In particular, we compared a Bud implementation with a UPC++ implementation and found that both are equally expressive. Although a detailed pe rformance comparison remains a future work, Bud provides enough low-level control to assume similar scaling be havior as UPC++. However, Bud’s low-level characteristics complicate distributed program ming by ofﬂoading performance and correctne ss considerations to the user. Our case study motivate d the design of BuDDI, a more declarative Bloom language that provides Distributed Data Independence. Our declarative abstractio ns rely o n CRDTs and Iterators to maintain acceptable con sistency and performance with the many bene ﬁts of declarative logic programming. Our hope is that the d e sign of BuDDI will motivate our readers to work with us to implement the compiler and runtime. RG worked primarily on the design of BuDDI , and discussion of Global Tables and Iterators. GG worked primarily on the case stu dy, implementation, and related discussion of k-mer counting in Bud and UPC++. RG and GG contributed equally to the writing of the report. This paper is submitted in fulﬁllment of the requirements of UC Berkeley’s CS294 gr a duate seminar: “Programming the Cloud.” Taught b y Pr ofessor Hellerstein and Dr. Milano.