A growing body of literature as well as intense civil and political debates draw attention to controversial information and communication technology (ICT) practices and their far-reaching impact on human societies. Privacy—including data protection—and what we can do to safeguard it is at center stage in those discussions. Concerns reﬂect a general sensation that existing privacy regulations have yet again become outdated and unable to protect us from the perils of novel ICTs, Modern information and communication technology practices present novel threats to privacy. We focus on some shortcomings in current data protection regulation’s ability to adequately address the ramiﬁcations of AI-driven data processing practices, in particular those of combining data sets. We propose that privacy regulation relies less on individuals’ privacy expectations and recommend regulatory reform in two directions: (1) abolishing the distinction between personal and anonymized data for the purposes of triggering the application of data protection laws and (2) developing methods to prioritize regulatory intervention based on the level of privacy risk posed by individual data processing actions. This is an interdisciplinary paper that intends to build a bridge between the various communities involved in privacy research. We put special emphasis on linking technical notions with their regulatory implications and introducing the relevant technical and legal terminology in use to foster more efﬁcient coordination between the policymaking and technical communities and enable a timely solution of the problems raised. see [Nis18] for a good overview. Yet, privacy—ﬁrst mentioned in the landmark US Supreme Court case [Olm28]—has always been a ﬂuid and elusive concept, making the determination of what and how exactly we aim to protect that much harder. social sciences—like law, philosophy, psychology, and sociology—to computer science, and conceptualizations of privacy vary both within and across disciplines. Interestingly, the boundaries of these strings of thoughts are somewhat blurred, but limited communication between them stands in the way of synthesizing their insights into a comprehensive privacy notion. sciences and in the eye of privacy regulators—from respecting certain social contexts to correcting the power imbalance or appropriately deﬁning boundaries between the individual and society, to protecting individuals’ autonomy. All of these are heavily ethically loaded questions with differing interpretations across cultures, which, together, inform privacy laws and policies at any given time. Social science literature as well as legal and policy debates are mostly concerned with these often pronounced disciplinary, ethical, legal, and cultural differences, attempting to achieve compromises that are acceptable for a given group of stakeholders—be it a single or a group of countries, or just speciﬁc communities. center around human expectations as determinants of privacy norms. While orienting on human expectations is a natural cornerstone of privacy scholarship, laws, and policies, we believe that, in light of the increasing ubiquity of artiﬁcial intelligence (AI) technologies and other modern data processing practices, this approach is no longer tenable. This is because, due to advances in computing capabilities and the widespread availability of large data sets, data processing practices have reached a level of complexity and sophistication that is no longer accessible without some technical training. Here, AI systems are understood as deﬁned by the Organisation for Economic Co-operation and Development (OECD) as ”a machine-based system that can, for a given set of human-deﬁned objectives, make predictions, recommendations, or decisions inﬂuencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy.” [Org19]. Consequently, the vast majority of the population lacks the necessary knowledge to understand their workings, let alone anticipate their privacy implications. Hence, continued reliance on privacy expectations would lead to vulnerabilities in our privacy frameworks due to a discrepancy between the expected and actual privacy implications of AI-driven data processing practices. In our increasingly automated, digital world, privacy regulation should be less reliant on individuals’ privacy expectations and make more use of—potentially automated—technical means of data protection. bining uninformative data sets and use Cambridge Analytica’s successful interference with the 2016 US elections as an example to show that there exist AI-driven data processing practices that challenge two fundamental assumptions underlying traditional principles of data protection: (1) Only personal data requires privacy protection and (2) given sufﬁcient information provided in plain language, individuals are able to adequately assess the privacy implications of their actions and protect their privacy interests, e.g., by providing informed consent. to some data processing methods—most notably the increasingly standard practice of combining data sets—anonymity has become a ﬂuid, dynamically evolving property of data sets. This ar- In part, this is because research on privacy is spread across multiple disciplines, reaching from As outlined in more detail in Section 2, protecting privacy means different things across social This paper focuses on a key similarity between these approaches to protect privacy: They all We provide a simpliﬁed mathematical model for demonstrating potential consequences of com- Based on our analysis, we put forward two recommendations for regulatory reform: (1) Due guably makes the traditional distinction between personal and anonymized data for the purposes of determining the scope of application of privacy regulation somewhat meaningless. In any case, it is cumbersome to implement in practice, as safeguards must work in a dynamic manner engaging and disengaging depending on whether the data is personal or anonymous at a given juncture. Against this background, we propose that this distinction be abolished such that privacy protection applies to all data at all times. (2) Risk-based regulation is currently a prevailing feature of AI and privacy regulation in both Europe and beyond [CJ21]. Prominent examples employing a riskbased approach include the General Data Protection Regulation [Eur16] and the proposed Artiﬁcial Intelligence Act [Eur21] and ePrivacy Regulation [Eur17] in the EU. Further examples are the ongoing work of the OECD Network of Experts on AI (ONE AI), which is implementing the OECD Principles on AI [Org19], the Feasibility Study of the Council of Europe’s Ad-Hoc Committee on Artiﬁcial Intelligence (CAHAI) [Ad 20], and existing and ongoing standardization efforts of both European and international standardization bodies, most notably the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), the International Standardization Organization (ISO), the International Electrotechnical Commission (IEC), and the Institute of Electrical and Electronics Engineers (IEEE). In the spirit of risk-based regulation, we recommend prioritizing regulatory intervention based on the degree of privacy risk posed by individual data processing actions. outside of the relevant community. The terms privacy and data protection are used interchangeably, but for the sake of precision, we note that the latter constitutes a subset of the former. We also highlight linkages between technical, legal, and other disciplinary notions relevant for addressing the privacy issues raised in the paper to facilitate interaction between various disciplines as well as the technical and policymaking communities for the purpose of solving these problems. This section gives an overview of existing privacy concepts various across disciplines and privacy regulation. From a regulatory perspective, it is important to consider these diverging insights to ensure the objectives and means of privacy regulation are optimally set. When designing regulations, it is paramount to improve interdisciplinary and multi-stakeholder coordination and communication, as well as working with a widely agreed-upon privacy notion, as these approaches increase the likelihood that newly created norms are accepted as legitimate by society [BCL12, EG18, EG20]. dependent, subjective feeling, which serves a self-protective purpose by controlling access to the self [Bat64, Alt75]. taining trust both among individuals and towards social institutions. Hence, it plays a crucial role in structuring interpersonal relationships, and establishing and maintaining orderly and just societies [ACCH17]. discourses have been dominated by a long-standing deadlock between Kantian and utilitarian thinking, until Nissenbaum’s doctrine of contextual integrity widely conquered both academic and policy circles [Nis09, Rul19, Nis18, The12]. Conversely to classical ethical theories’ endeavor to estab- Throughout the paper, we use both technical and legal terms in a manner accessible for readers Psychology approaches privacy from the individual’s perspective, emphasizing that it is a context- Sociology adds a macro layer to the inquiry, noting that privacy is central to building and main- Philosophy tackles a wide ﬁeld of fundamental dilemmas related to privacy. Many of these lish universally applicable behavioral guidelines, Nissenbaum’s analytical framework stresses that social space is not uniform. Rather, it is composed of distinct spheres, in which nuanced, contextrelative informational norms govern the ﬂow of information. Informational norms mirror dynamically evolving privacy expectations, which differ widely across societies and deﬁne appropriate behavior in different social contexts. Contextual integrity is given as long as information ﬂows conform with privacy expectations, but entrenched information norms sometimes need to be reassessed and adapted in the face of changing circumstances. cal framework for analyzing and making data available in a privacy-preserving way [DN03, DN04, DMNS06]. The starting point of their analysis is that a person does not necessarily have to share their personal data to suffer privacy loss. Take the hypothetical example of a monthly company bulletin, which publishes the total number of employees and the number of employees who earn more than $500, 000 a year. Let us assume that employee E leaves the company and there is no new hiring. The following month, the statistics show a decrease by one in both the number of total employees and that of high-income employees, leading to an immediate privacy loss of E regarding their income category. Admittedly, this is a very constructed example but it shows that the analysis and/or release of a certain data set can have an impact on a person’s privacy even if their personal data is not included in that data set. In order to avoid such unintentional privacy losses, in differential privacy settings, random noise is introduced to the collected data to enable its privacy-preserving analysis and release. The so called privacy loss parameter ε measures the deviation in privacy loss between a person’s opt-out scenario (i.e., where the person’s private information is not included in the data) and the execution of a differential privacy analysis (i.e., where the person’s private information is in the data). The value of ε is usually very small, meaning that including a person’s private information into a data set (e.g., via a questionnaire) will not result in a signiﬁcantly larger privacy loss compared to the opt-out scenario. Despite the limited setting in which differential privacy is applied, it is an important ﬁrst step in thinking about privacy in formal terms. We particularly appreciate the non-binary nature of the privacy notion: The idea of associating a privacy loss with each speciﬁc data processing step and assuming incremental increases in the individual’s risk of exposure every time their data is processed. fending individual rights against the collective and sees privacy’s primary purpose in protecting individual autonomy [WB90, Wes67, Ben84, Mok18]. Accordingly, privacy laws are characterized by a public private dichotomy and are typically only applicable to safeguard individuals’ reasonable expectations of privacy over publicly not available information from unjustiﬁed intrusions. This understanding is reﬂected in leading, internationally inﬂuential hard and soft legal sources of privacy regulation, see recitals 47 and 50 of the European General Data Protection Regulation (GDPR) [Eur16], the respect for context principle in the US Privacy Bill of Rights [The12], and the deﬁnitions of personal data—referred to as personally identiﬁable information (PII) in relevant international standards [Int13, Int21, Int19]—consent, and data controller [Eur16, Asi05, Org13, The12]. Note that this regulatory stance seems to borrow bits and pieces from all the above mentioned disciplines, except computer science. lenge the weight social sciences and regulators attribute to human expectations in shaping privacy regulations and advocate complementing traditional privacy protection mechanisms with more In computer science, a recent line of work introduced differential privacy, a formal mathemati- Privacy regulators are driven by a traditional liberal view of privacy, which centers on de- Our analysis builds on all of the aforementioned privacy concepts: We constructively chaltechnology-conscious ones that are perhaps more formal and/or potentially automated. Let us give a mathematical model of combining two data sets to show two things: (1) A data processing method may enable data processors—humans and in particular AI systems—to draw inferences well beyond the scope of distinct data sets, and use the inferred additional information to inﬂuence individuals’ opinions and behavior. (2) These consequences are far from obvious for an average person without technical expertise. internet service providers. Database DB person’s probability to vote on a speciﬁc topic Y (Figure 1a). For reasons of simplicity, it is assumed that the vote can only have two values: A positive (Y = 1) and a negative (Y = 0) choice. interaction of a user with a social media platform. From an internal survey of the social media platform, which asks for the user’s preference in the upcoming vote, someone tries to model user i’s vote Y P(Y, B) = P(Y )P(B) and P(Y = 1|B) ≈ 50% (Figure 1b). are competing indicators with respect to the person’s decision on the upcoming vote, such that the overall dynamics can be modeled as a nonlinear system exhibiting a cusp bifurcation [Zee76] using the Cusp function which has two local minima for and one local minimum otherwise (Figure 2a). The local minimum X and B (3) so that it constitutes a latent variable, from which the person’s probability P inferred A sample of 1000 simulated tuples (A vote, if this change crosses the cusp-curve Assume that we are dealing with two databases DBand DB, which are located at two different Database DBincludes a social media behavior index B, which is designed to quantify the based on the corresponding index B. As it turns out, Yand Bare statistically independent However, due to the nature of this constructed problem, it is assumed that variables A and B fulﬁlls Consequently, a small change of B could result into a spontaneous switching of the person’s Figure 1: Sensitivity of conditional probabilities with respect to independent variables for system exhibiting a cusp bifurcation. (a) Probability P(Y = 1|A) modelled from data of DB P(Y = 1|B) modelled from data of DB bifurcation based on variables A and B. Decisions Y and random right of the cusp curve. (d) Probability P(Y = 1|A, B) modelled from join of databases polynomial features considering only one independent variable (panels (a) and (b)), or both variables (panel (d)). Arrows indicate direction of change for independent variables A, B. and the person’s local minimum vanishes (Figure 1c). From the perspective of statistical modeling, this means that joining the information A formulation of a model, which can predict the user speciﬁc change of opinion P social media behavior B Appendix A. closest to the cusp curve (and hence most easily changed) and calculating how much inﬂuence to exert upon them require complex mathematical modeling. Of course, its efﬁciency depends on the characteristics of the available data sets. Let us also assume that the relevant terms and conditions, to which individuals consent when providing their personal data, are fully transparent on how information is processed. This likely involves reference to the possibility of combining data sets and processing them by automated means. Still, in the absence of at least some technical literacy, people will not realistically anticipate such far-reaching implications. We examine this (c) Databases DBand DB(d) Joined databases DBand DB and DB. Probabilities are estimated from logistic regression with L2-norm on 3rd-order This simpliﬁed example shows that both identifying the individual(s) whose opinion(s) is/are Figure 2: Bistability of latent variable X (a) and sampled votes (b). The sampling is described in Appendix A. Subﬁgure (b) also shows the cusp curve Eq.(2) (solid line) and the sudden changes of voting decisions happening at B = claim in more detail in the next section. Based on [Eur20], we now outline relevant aspects of the infamous Cambridge Analytica scandal as a real-life example, where data merging coupled with extensive use of machine learning (ML) algorithms has been employed to inﬂuence voters’ behavior in the 2016 US Elections. Information on terms and conditions and other pieces of communication between the parties involved in the Cambridge Analytica exercise was not always available. In such instances, we assume GDPRcompliant behavior in our example to show that the problems discussed below occur even if these currently highest (or most restrictive) standards of data protection are met. For the same reason, we also use the GDPR as a benchmark to evaluate other aspects of the event, even though it occurred in the US and was hence beyond the GDPR’s scope. Note, that people affected by this incident did not anticipate the privacy implications of their actions. This supports our point that the average individual—despite safeguards in privacy regulation—is unable to adequately assess the far-reaching consequences of AI-driven data processing practices. chine learning, and data mining literature use multiple terms to refer to some core concepts, let us brieﬂy explain the notion of supervised learning and give an overview of the choices of terminology in use. We will follow the notation of [JWHT21]. In supervised learning we are given a vector X of p predictor variables (also known as input, feature, or independent variables) and a response variable Y (also known as outcome, target, or dependent variable). We are assuming that there is some kind of a relationship between X and Y . Furthermore, we are given a training data set of n observations of the form {(x Because the example is in a supervised learning setting and statistics, statistical learning, mayis the response value for the i If Y takes values of a ﬁnite set, we have a classiﬁcation problem. In supervised learning, using the training data set, we try to understand the relationship of X and Y and how each predictor variable affects the response. With this knowledge, we use observed predictor vectors x in order to predict an unobserved y. infer individuals’ personality traits (response variable) based on their Facebook data (predictors) and (2) use this information to sway swing voters’ preferences in the desired direction. We focus on the ﬁrst part of this goal here because it is the one involving the combination of multiple data sets and their processing by ML algorithms—the data processing practices whose privacy implications we aim to analyze—and enabling the execution of the second part. What follows is a description of four privacy-relevant actions that Cambridge Analytica has taken to achieve the ﬁrst part of the above outlined objective. To help highlight the connection between relevant technical and legal concepts, as well as their likely interpretation by lay people, we do not only describe the respective actions but also analyze them from a technical (technical side) and legal (legal side) perspective, and discuss the validity of probable privacy expectations related to them. Note that we give a simpliﬁed account of events, only going into details relevant for our purposes. that, they have paid roughly 300, 000 people to ﬁll out the short version of a psychological survey (the International Personality Item Pool Representation of the NEO PI-R via a survey app. The technical side: At this point, anonymous data is collected and stored, creating the preconditions of further data processing. The legal side: The stated purpose of data collection is research. Because of the anonymous nature of the survey data, its processing—collection and storage are forms of processing according to Article 4 (2) GDPR—falls squarely outside the purview of the GDPR as deﬁned in Article 2 GDPR, so no speciﬁc information or consent requirements apply. This is a point of paramount importance to which we will return later, as such requirements would signal individuals the existence of privacy risks, potentially leading them to refrain from ﬁlling out the survey. Privacy expectations: From a lay person’s perspective, giving up the information required by the survey has no privacy implications, as it is used for a valid purpose and—as far as participants are concerned—cannot be traced back to them. Participants’ privacy expectations (no privacy risks) are thus very different from the actual privacy implications of ﬁlling out the survey (high privacy risks due to enabling further data processing). account and approve access for the survey app. Using this access, the app has collected all data associated with them (including personal data, links, posts, likes, and all the Facebook data of their friends). The technical side: Several key steps with substantial privacy implications happen here. For one, the hitherto anonymous survey data is linked to participants’ Facebook accounts and is thereby de-anonymized. This enables Cambridge Analytica to merge these two data sets—survey data revealing participants’ personality traits and data contained in their Facebook accounts—providing them with their initial training data set. The legal side: De-anonymization turns the thus far anonymized survey data into personal data within the meaning of Article 4 (1) GDPR, triggering the GDPR’s application. We could not access the app’s terms and conditions, so we assume that, complying with the GDPR, participants are informed that granting the app access to their Facebook account allows Cambridge Analytica to collect, store, and otherwise process in- Turning to the example, Cambridge Analytica’s objective was to (1) build an ML model that can Action 1: First, Cambridge Analytica had to create a training data set. As a ﬁrst step to achieve Action 2: To receive payment for ﬁlling out the survey, users had to login to their Facebook formation from those accounts. Let us also assume that participants are explicitly made aware of the possibility to combine the two data sets and process them by automated means—two other forms of processing according to Article 4 (2) GDPR—and that, based on this information, they consent to the further processing of their now personal data. Privacy expectations: Assuming participants actually read the app’s terms and conditions, they presumably understand that they are providing personal data stored on their Facebook account, even though it is unlikely that they realize the quantity and value of data they are giving up. However, based on the information provided in plain language in the terms and conditions, a person without technical training can arguably not anticipate the above outlined consequences of merging several data sets, nor the amount and value of information ML algorithms can extract from such combined data sets. So, it is fair to assume that the perceived privacy risks of action 2 remain well below its actual privacy risks. those parties, the exact nature of the collected data, and the terms and conditions under which the data was collected is unknown. The technical side: We assume those additional data sets—which are combined with Cambridge Analytica’s existing data sets—contain personal data, seeing as this step still aims to contribute to building the training data set by further enriching it. The legal side: Again, based on GDPR requirements, we assume the following: (1) Data subjects providing their personal data are informed of and consent to the purpose(s) and potential form(s) of data processing. (2) This information includes reference to the possibility of processing the provided data by all the aforementioned means and by sharing it with third parties—a processing action covered by the disclosure by transmission, dissemination or otherwise making available options of Article 4 (2) GDPR. (3) All this information is conveyed to data subjects in data controllers’/processors’—the terms used by Article 4 (7) and (8) GDPR for referring to parties on behalf of whom personal data is processed/who perform the processing—respective terms and conditions. Privacy expectations: As noted in respect of privacy expectations related to action 2, we are of the opinion that even if the affected individuals are clearly and unambiguously informed about all data processing purposes and methods, those without technical literacy are not in the position to adequately assess the aggregate privacy implications of the data processing actions involved. Analytica built their model with the objective to proﬁle any person based on their Facebook data. The technical side: There are two elements to distinguish within this action, both posing very high privacy risks. The ﬁrst one is the process of model building, that is, the use of statistical learning methods to ﬁnd the correlation between the predictors (e.g., Facebook data, consumer behavior) contained in the various data sets and the response (i.e., the personality traits associated with the predictors). Even though built based on the data of a relatively small number of individuals, this model—as all well-built models—is a very powerful tool that can subsequently be used to draw inferences from similar data on any individual. This ability of the model enables the second element of Action 4, namely for Cambridge Analytica to engage the model to proﬁle any US voter based on their Facebook (and potentially other) data. The legal side: Building a model and using it on data qualiﬁes as processing data by automated means according to Article 4 (2) GDPR. From this follows that—in respect of the survey participants and assuming that all other requirements of Article 5 GDPR are also fulﬁlled—all the data processing steps undertaken under Action 4 are compliant with the GDPR due to being covered by their consent to the terms and conditions of the app mentioned under Action 2, as well as to those of other data controllers/processors mentioned Action 3: Furthermore, consumer data was acquired from other parties, whereby the identity of Action 4: Based on the collected data sets and using machine learning algorithms, Cambridge under Action 3. Perhaps less obviously, however, the same holds for any member of the US voter population, whose data is later inputted in the model, as long as Facebook’s (and potentially other data controllers’/processors’) terms and conditions require those individuals’ consent to collecting, storing, and processing their data by sharing it with third parties, combining data sets, and processing them by automated means—which we assume here. Alternatively, insofar as data is manifestly made public by the data subject (e.g. data in a public Facebook account), consent is not even needed according to Article 9 (2) (e) GDPR. Privacy expectations: The above discussion shows that—provided all other conditions of Article 5 GDPR are also fulﬁlled—a valid consent given to a set of sufﬁciently comprehensive terms and conditions at the time of ﬁrst providing personal data to a single data controller/processor may cover a series of privacy-relevant data processing actions. This is despite the fact that the degree to which privacy risks may actually materialize at a later time is entirely unforeseeable for the data subject. Similar privacy risks arise in an even more covert fashion if data is collected in anonymized form and hence without requiring consent. We believe it is impossible for anyone—including technical experts—to anticipate what data sets will be combined, what models will be built and used on their anonymized and/or personal data, and to what purposes (which are easily formulated so as to be compliant with the purpose limitation principle laid down in Article 5 (1) (b) GDPR) in the future. Expected privacy risks are, therefore, once again well below actual privacy risks. ers’ personality traits by running the model on their Facebook (and potentially other) data, it was a straightforward exercise to show them personalized advertisements with a view to alter their voting behavior in the desired direction—an undertaking that ultimately brought about the unexpected outcome of the 2016 US elections. After providing a simpliﬁed mathematical model and a real-life example of how the potentially far-reaching consequences of merging data sets and processing them by ML algorithms challenges current data protection approaches, we now put forward two recommendations for regulatory reform to address the above identiﬁed privacy risks. the purposes of determining the scope of application of data protection laws and protecting all data: As mentioned earlier, most data protection laws and global data protection standards only come into play if personal data/PII is processed. Due to the dynamic workings of these rules—they apply only as long as the data processed qualiﬁes as personal data/PII —they provide relatively effective protection if and to the extent to which data controllers/processors choose to comply with them. This will to compliance makes a huge practical difference from a regulatory perspective, because breaches must be detected in order to be prosecuted. Detection is, however, generally a central regulatory problem in practice due to limited regulatory resources [BCL12]. This holds all the more true in data protection regulation in light of the enormous frequency of data processing actions. Existing solutions to enhance privacy protection, like differential privacy, do not address this problem, as their application heavily depends on data controllers’/processors’ ethical judgment. The fact that existing data protection rules do not provide any privacy protection when individuals give up data in anonymized form is equally troubling. As shown by the Cambridge Analytica Finally—turning to the second part of Cambridge Analytica’s objective—after discovering vot- Recommendation 1: Abolishing the distinction between personal and anonymized data for example, this is an important shortcoming, because this initial act of data collection enables further data processing (which may or may not be compliant with data protection laws) and hence has potentially severe privacy implications. Also, the lack of application of data protection laws gives people a false signal that giving up data in anonymized form does not entail privacy risks. We think that educating individuals about the possible privacy implications of providing anonymized data and requiring their consent to the processing of such data in a similar manner as is required for personal data would induce more caution—possibly keeping people from providing information in the ﬁrst place—and signiﬁcantly bolster privacy protection. The recently proposed EU ePrivacy Regulation [Eur17] seems to go in the same direction, in that it provides protection for electronic communications data even if it does not qualify as personal data, recognizing that the processing of such data may also give rise to privacy risks. vention based on the level of privacy risk posed by individual data processing actions: As noted in the introduction, existing and forthcoming European and key international regulatory instruments on data protection and AI adopt a risk-based approach to prioritize regulatory intervention. There is also a clear trend to align international, regional, and domestic policies and standards as far as possible to create an internationally consistent, interoperable regulatory framework in these domains. Some solutions laid down in these instruments could be useful starting points for developing approaches to target individual data processing actions based on the level of privacy risk they pose. We note that implementing such an approach is not without challenges: For one, setting up a risk assessment framework for data processing actions is far from easy, especially given the continuous evolution of data processing methods. The idea of attributing a given privacy loss to individual data processing actions mentioned in the context of differential privacy may be helpful here. Second, detecting relevant processing actions and requiring data subjects’ consent for each and every one of them is a serious challenge that is unlikely to be feasible without at least some level of automation. Ideas like engaging an algorithmic agent to help preserve individuals’ human agency [Ins19] could be a potentially promising initiative to crack this problem. between the use of AI technologies and privacy. There is a clear rise in demand for the comfort and beneﬁts AI may provide in many domains, such as healthcare, ﬁnance, and transportation, to name a few, as well as for ever more efﬁcient personalized services like movie recommender systems and customer services more generally. While we should strive to provide state-of-theart privacy protection at all times, we have to accept that the use of AI technologies inevitably involves privacy losses compared to the pre-AI age. So the real question is whether and to what extent we are willing to give up our privacy in exchange for reaping the beneﬁts of AI. This is a decision for society to make, whereby the aim should be to ﬁnd a socially optimal balance, mindful of the negative economic consequences of overly restrictive privacy policies [CD21]. Given the transnational nature of privacy protection, international and multi-stakeholder coordination will be indispensable. Arguably, there must be checks and balances in place to keep powerful actors—the blame is most commonly assigned to Big Tech, although governments and other actors also tend to wield signiﬁcant power—in check. That said, it is important to remember this tradeoff when determining whether and to what extent these parties overstep their boundaries while serving a very real demand. Recommendation 2: Applying risk-based regulatory principles to prioritize regulatory inter- As a ﬁnal remark, we believe society needs to come to terms with the fact that there is a tradeoff This paper proposed regulatory reform to address some problems stemming from AI-driven data processing practices, in particular from combining data sets—a well-known but to date insufﬁciently addressed issue. Solving these problems will require policy efforts involving transnational, interdisciplinary and multi-stakeholder collaboration, which, although much-discussed, remains tricky in practice. To highlight and facilitate the navigation of technical issues for policymakers, we not only presented an abstract mathematical model of combining data sets but also analyzed Cambridge Analytica’s interference with the 2016 US elections—a real-life example illustrating how powerful this data processing method coupled with the use of AI systems can be—from a technical, legal, and lay perspective. Our recommendations are in line with ongoing and/or impending policy and standard setting work in the EU, OECD ONE AI, European and international standard setting organizations, and other bodies. We referred to publicly available components of these work streams throughout the paper. In order to simulate a system exhibiting the sudden change of voting behavior, we are sampling i = 1, . . . , 1000 data points from In order to ﬁnd combinations of A initial guess for ﬁnding local minima of using the Nelder-Mead simplex algorithm [NM65] as implemented by the Python module SciPy Ver. 1.2.1 [VGO mapped to probability P with σ = 10. The classiﬁcation response Y In order to sample a classiﬁcation response Yfrom these data, the stationary points Xare