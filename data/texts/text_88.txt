Recently, leveraging dierent channels to model social semantic information and using self-supervised learning tasks to boost recommendation performance has been proven to be a very promising work. However, how to deeply dig out the relationship between dierent channels and make full use of it while maintaining the uniqueness of each channel is a problem that has not been well studied and resolved in this eld. Under such circumstances, this paper explores and veries the deciency of directly constructing contrastive learning tasks on dierent channels with practical experiments and proposes the scheme of interactive modeling and matching representation across dierent channels. This is the rst attempt in the eld of recommender systems, we believe the insight of this paper is inspirational to future self-supervised learning research based on multi-channel information. To solve this problem, we propose a cross-channel matching representation model based on attentive interaction, which realizes ecient modeling of the relationship between crosschannel information. Based on this, we also propose a hierarchical self-supervised learning model, which realize two levels of self-supervised learning within and between channels, which improves the ability of self-supervised tasks to autonomously mine dierent levels of potential information. We have conducted abundant experiments, and various metrics on multiple public datasets show that the method proposed in this paper has a signicant improvement compared with the state-of-the-art methods, no matter in the general or cold-start scenario. And in the experiment of model variant analysis, the benets of the cross-channel matching representation model and the hierarchical self-supervised model proposed in this paper is also fully veried. CCS Concepts: Neural networks. Additional Key Words and Phrases: Self-supervised learning, Graph neural networks, Hypergraph, Contrastive learning ACM Reference Format: Dongjie Zhu, Yundong Sun, Haiwen Du, and Zhaoshuo Tian. 2021. Self-supervised Recommendation with Cross-channel Matching Representation and Hierarchical Contrastive Learning. . J. ACM 37, 4, Article 111 (August 2021), 23 pages. https://doi.org/10.1145/1122445.1122456 • Information systems → Recommender systems;• Computing methodologies → 1 INTRODUCTION With the development of network technology and the popularization of mobile smart devices, especially the emergence of micro-video sharing platforms, such as Tiktok has shown explosive growth. Enormous information is stored on the network, but it is more and more dicult for us to obtain valuable information for ourselves, which brings greater challenges to the recommender systems[34]. In recent years, the graph neural networks(GNNs) have achieved unprecedented success in various tasks, such as node classication, link prediction, and graph classication with its strong ability to extract graph data structure, node attributes, and relationships between nodes[ At the same time, the application of GNNs in recommender systems has also become a hot topic. Some studies construct the bipartite graphs from interactive information between users and items and utilize GNNs to dig out multi-hop relationships between users and users or items and items higher-order relationships. In addition, some methods add knowledge graph [ information[ the performance of the recommender systems. More information means more complex interactions. The interactions can be dierent patterns of social relationships between users[ and high-order interactions can provide richer information and help improve recommendation performance. Although stacking multi-layer GNNs can capture the high-order relationship of multiple hops between nodes, it cannot capture the high-order relationship with specic patterns. The emergence of hypergraph neural networks (Hyper-GNNs) solved this problem. The hypergraph connects nodes with a specic relationship pattern through an edge. This natural structure makes it possible to mine specic interaction patterns between multiple nodes[7, 13]. Although the introduction of Hyper-GNNs and various interactive information with specic patterns has greatly improved the performance of the recommendation model, this performance improvement is guaranteed by enormous interactive information. For new users or items in the coldstart scenario, the recommendation performance is poor due to the lack of interactive information. At the same time, simply aggregating interaction information with specic patterns through dierent channels will also lose the characteristics of dierent channel data. Self-Supervised Learning (SSL) can realize label-free learning with the help of self-supervised tasks, and it can also dig out potentially valuable information autonomously, alleviating the cold-start problem to some extent. More importantly, it can fully mine and utilize the characteristics of dierent channel information to promote the improvement of recommendation performance. In the eld of Computer Vision (CV) and Natural Language Processing (NLP), various SSL methods[ deciencies of data labels and have achieved great success. However, due to the continuity of node attributes in the interaction graph and the complex relationship between nodes, it is dicult to directly apply the data augmentation methods in the eld of CV and NLP to recommendation[ Researchers have also conducted a lot of studies on SSL in the eld of recommender systems. The existing methods mainly focus on generating dierent local substructures by randomly removing edges, sampling nodes[ to capturing the graph structure with various patterns. However, there is rich semantic information of interactions in the recommender systems, such as various social relationships between users, and the association relationships between items and items. It is dicult to eectively mine the semantic information of multiple interaction patterns by simply using random methods for data ], breaking restrictions of the traditional collaborative ltering paradigms on capturing 47] as supplementary information to the interaction graph, which further improves augmentation. Dierent channels can not only provide richer semantic information, which is benecial to improve performance, but also provide new possibilities for self-supervision. Some of the latest methods[ interaction patterns and use dierent channels to capture the information of dierent patterns and construct self-supervised tasks in each channel. However, the existing methods cannot make full use of the relationship between dierent channels to construct self-supervised tasks. Simply constructing contrastive learning tasks for the data from dierent channels by some methods (e.g., maximum mutual information.) will make the data of each channel extremely homogeneous and lose their uniqueness (We also call it homogeneity, we proved this in exploratory experiments, see Prelimations 2.3.). Therefore, we need a new method to eciently model the relationship between the interactive information of dierent channels to get cross-channel information that can be used to establish self-supervised tasks, so recommendation performance can be boosted. At the same time, it reduces the dependence on data labels and data volume and further improves performance in cold-start scenarios. Based on the above analysis, we proposed a self-supervised recommendation framework with Cross-channel Matching representation and Hierarchical Co-contrastive learning, which is also called CMHC. First, inspired by MHCN[ is extracted, including Social-channel, Purchase-channel, and Joint-channel, and the Hyper-GNN is leveraged to encode information of each channel. On this basis, we innovatively propose a cross-channel matching representation model, which constructs rst-order ego-networks of the nodes under each channel and realizes the information transition between channels based on the proposed Attentive-Matching representation learning model. More importantly, to solve the data homogeneity problem of each channel (Preliminaries 2.3), a hierarchical SSL model based on cross-channel matching representation is proposed, and a self-supervised task is established for matching representations under dierent channels. The contributions of this paper can be summarized as follows: The rest of this paper is arranged as follows: Chapter 2 will introduce some denitions and basic technologies involved in this paper, such as hypergraph and hypergraph neural networks, and analyze our exploration experiments on self-supervised contrastive learning on multi-channel information. We will describe and analyze our method in Chapter 3 detailly. Chapter 4 is the section We make explorations and experimentally verify the deciency of simply constructing contrastive learning tasks for the data from dierent channels and propose the scheme of interactive modeling and matching representation across dierent channels. As far as we know, this is the rst attempt in the eld of recommender systems, which can help to facilitate subsequent self-supervised learning research based on multi-channel information. We propose a self-supervised recommendation framework with Cross-channel Matching representation and Hierarchical Co-contrastive learning, which is also called CMHC. Specically, we propose a cross-channel matching representation model based on attentive interaction, which realizes ecient modeling of the relationship between cross-channel information. Based on this, we also proposed a hierarchical self-supervised model, which realized two levels of self-supervised learning within and between channels and improved the ability of self-supervised tasks to autonomously mine dierent levels of potential information. We have conducted abundant experiments on various real datasets, and the results show that CMHC proposed in this paper outperforms the state-of-the-art methods by a big margin, no matter in the general or cold-start scenario. And in the model variant analysis experiment, the benets of the cross-channel matching representation model and the hierarchical selfsupervised model proposed in this paper is also fully veried. of the experiments. We will verify the method proposed in this paper, including performance comparison experiments, variant experiments, and stability experiments. Chapter 5 will introduce and analyze the research work related to this paper, including hypergraph neural network, self-supervised learning, and self-supervised graph learning for recommendation. Chapter 6 will conclude this paper and some future works. 2 PRELIMINARIES 2.1 Hypergraph Definition For a general graph, an edge can only connect two nodes, so that it can only describe a pairwise relationship. But in real scenarios, many relationships are not limited to two nodes, such as the stable triangular relationship in social networks[ hypergraph evolves an edge into a hyperedge, and a hyperedge can connect multiple nodes at the same time, which is also the core idea of the hypergraph[ 𝐺 = (𝑉, 𝐸) graph. The adjacency relationship between nodes can be represented as matrix where each element indicates whether hyperedge 𝑒 contains node 𝑣: Like a general graph, the degree of a node in hypergraph can be expressed by the number of hyperedges that contain the node, that is is no longer limited to connecting two nodes, the degree of the hyperedge can be additionally dened as the number of nodes connected by it, that is matrices the hyperedges and nodes in hypergraph 𝐺, respectively. 2.2 Extraction and Encoding of Information with Dierent Interaction Paerns Birds of a feather ock together. Closely connected people are most likely to have the same interests. At present, with the popularity of various social platforms and the accumulation of massive data, it has become a trend to integrate users’ social network information into the recommender systems[ makes the relationship between user-user and user-item richer and no longer limited to pairwise relationships, which brings both advantages and challenges to relationship mining. How to extract and encode information with various patterns is not the focus of this paper. We follow the relevant content in MHCN[43]. First, we extract the relationship of the three patterns, namely Purchase Motif, Joint Motif, and Social Motif, as shown in Figure 1. A large number of studies have proven that triangles are extremely important in social relationships[ , where𝑉represents the set of nodes, and𝐸represents the set of hyperedges in the 𝐷∈ Rand𝐷∈ Rformed by𝑑 (𝑒)and𝑑 (𝑣)represent the degree matrix of 42,43]. The fusion of social network information and User-Item interactive information most important and stable triangle social relationships users have a social relationship and bought the same product. Purchase Motif ( users who do not have social relationships have purchased the same product. Although they have no social relationships, their demands or interests are similar. This is a very important potential relationship for recommendations. The corresponding adjacency matrix extraction method is shown in Table 1. After obtaining the adjacency matrixÍ After obtaining the adjacency matrix in the three channels, the hypergraph convolutional neural network proposed in [7]can be used to encode information of each channel: where 𝐷 2.3 Empirical Explorations of Self-supervised Learning on Cross-Channel Information In this section, we explore how to leverage dierent channels of data with dierent interaction patterns to perform self-supervised learning. An intuitive idea is that after obtaining the representation of each channel, the contrastive loss function is used to make the representation between the dierent channels of the positive samples closer, and the representation of the negative samples more distant. Therefore, we add Triplet-SSL loss[ to build the contrastive learning on multi-channel information directly, formatting the model of 𝑆− 𝑀𝐻𝐶𝑁 verify the performance of the model variants on dierent data. The exploration results are shown in Figure 2. It can be seen from Figure 2 that the performance of on all metrics has a consistent decline compared with 𝑆 the contrastive learning on multi-channel information directly is not a satisfactory scheme. This is in line with the original assumption of this paper: building the contrastive learning on multichannel information directly will make the data of each channel extremely homogeneous and lose their uniqueness. The homogeneity problem not only fails to improve but also deteriorates the performance of the recommendation. Therefore, a new method is needed to eciently model ; the adjacency matrix of Joint Motifs is𝐴= 𝐴+𝐴; the adjacency matrix of Purchase ∈ Ris the degree matrix of 𝐴. Fig. 2. The performance exploration results of building the contrastive learning on multi-channel information directly. the relationships of dierent channels while maintaining their characteristics, to maximize the advantages of SSL, which is the key problem this paper to solve. This section will detail our proposed CMHC framework, its overall structure diagram is shown in Figure 3. First, we construct three channels of information with various interaction patterns based on Social Graph and User-Item Graph, namely Purchase Motif, Joint Motif, and Social Motif[ (Preliminaries 2.2). Then, the Hyper-GNN is used to encode information of each channel to obtain the user representation under each channel. For Item, the GNN is performed on User-Item Graph to obtain its representation (Preliminaries 2.2). Finally, based on the user embedding representations of each channel and the item representations, we build joint learning between the auxiliary task of the hierarchical SSL and the main task of recommendation. Cross-channel matching representation learning (section 3.1), cross-channel hierarchical co-contrastive learning model (section 3.2), and user representation fusion based on hierarchical attention (section 3.3) are the core of this paper. Fig. 3. Schematic diagram of the CMHC framework(one layer) proposed in this paper. Fig. 4. (a) Framework of Cross-channel Matching representation learning and (b) details of Aentive-Matching model. 3.1 Cross-channel Matching Representation Learning 3.1.1 Framework of Cross-channel Matching Representation Learning. We use the three channels as examples to illustrate and analyze the cross-channel matching representation framework proposed in this paper. The overall diagram of the architecture is shown in Figure 4 (a). First, for the data of any two channels, use the cross-channel representation learning model (Attentive-Matching) proposed in this article to obtain the transitional matching representation from channel 2 to channel 1 or from channel 1 to channel 2. Then the transitional matching representations from the other two channels to the current channel are summed to obtain the cross-channel transitional representation of the current channel. For example, the cross-channel matching representation of social -channel can be represented as the follows: where is the Attentive-Matching model we proposed in section 4.1.2. We extend it to a more general case (there are 𝐿 channels), and the matching representation of each channel is calculated as follows: where representation of the user under channel 𝑖. 3.1.2 Aentive-Matching Model. To obtain the transitional matching representation between two channels, this paper proposes an Attentive-Matching cross-channel representation learning model. First, we use the ego-network representations 𝑔 𝐻, 𝐻, 𝐻is the user representation under each channel obtained by equation (2),𝐴𝑡𝑡𝑚(·) 𝐻is the user representation under channel𝑖, and𝐻is the cross-channel matching 𝐺𝐶𝑁 #1[16] to gather information on the ego-network under each channel to obtain On this basis, the nodes of ego-network under other channels are used to perform cross-channel transitional representation modeling for the nodes of the current channel. Specically, we rstly calculate the cosine similarity between 𝑔 Then, we perform a weighted summation on the representation of all nodes in transitional representation of the nodes in 𝐺 Finally, we utilize representation matching representation of node 𝑖 from channel 2 to channel 1, as shown in equation(7). where 𝑓 where row of common vector cosine similarity of the input vectors: To better capture the local structure information of the nodes, gather the ego-network information to obtain the nal cross-channel matching representation under each channel. Similarly, we apply the same process to the nodes in 𝐺 3.2 Hierarchical Self-supervised Learning based on Cross-channel Matching Representation. In section 3.1, we obtained the transitional matching representation between dierent channel data. This kind of matching representation data contains the association relationship between dierent channels, which becomes the basis of applying contrastive learning among channels. At the same time, like general channel information, the matching representations under each channel still maintain dierent distribution, which contains unique characteristics. To preserve the uniqueness of the respective channels and make full use of the correlation between the channels of the matching representation, this paper proposes a hierarchical self-supervised learning model based on cross-channel matching representation. The diagram of the model is shown in Figure 5. First, we need to build a contrastive learning task on representations of multi-channels. The existing commonly used objective functions include the maximum mutual information model[ and the InfoNCE model[ instances between dierent channels. Specically, in each calculation process, one positive sample( in the queue are selected for calculation, and InfoNCE[ calculations: is a multi-view cosine matching function that compares two vectors of 𝑣and 𝑣: 𝑊 ∈ Ris a trainable parameter,𝑙is a multi-view parameter, and𝑊represents one 𝑊, which represents a single view.𝑑is the dimension of the input vectors, and𝑚 = , ..., 𝑚, ..., 𝑚]is a𝑙-dimensional matching vector. In special cases, if𝑙 = 1, it is the Fig. 5. (a) Framework of Cross-channel Matching representation learning and (b) Details of AentiveMatching model. where under a certain channel obtained in section 3.1. sample, and hyper-parameter. In practical implementation, to improve computational eciency and save GPU memory, we no longer set the cache queue of negative samples like the MoCo strategy[ on instances within the same mini-batch. From the perspective of matrix operations, it can be expressed as follows: where current mini-batch under channel product, dimensional matrix 𝐴. 𝜏 is the temperature hyper-parameter. Therefore, the nal cross-channel matching self-supervised contrastive loss function under the three channels is: To preserve the characteristics of the matching representation under dierent channels, SSL based on maximum mutual information is performed in each channel. Yu et al.[ their research that the maximum mutual information model used in in DGI[ directly between the target node representation and the channel-wise graph representation. This is a relatively coarse-grained strategy, which cannot guarantee that the encoder distills sucient information from the inputs. Especially when facing a large-scale graph, there is more signicant dierence between the node representation and the entire graph, the maximum mutual information will lose its eect, and even play a negative role. Therefore, this paper adopts the HMIN model proposed by Yu et al.[ channel. The rst level is the maximum mutual information from the node to its corresponding ego-network, and the second level is from the ego-network to corresponding graph. Therefore, we dene the self-supervised loss function in a single channel as: ℎis the representation of the query node, here it is the matching representation of node𝑞 ℎis the matching representation of the𝑖negative sample.𝜏is the temperature 𝐻∈ Rand𝐻∈ Rrepresent the matching representations of the nodes in the ×is matrix multiplication,𝐴represents the column-wise summation of the two- where of user is the negative sample of is the the graph under channel comprehensive SSL loss of the matching representation. And we regard it as the loss of auxiliary task 1: 3.3 Hierarchical Aention for Comprehensive User Representation In this section, we need to fuse two levels of user information, namely, the common representation and matching representation of each user within one channel, and the information of dierent channels. An intuitive idea is to simply perform summation, average or maximum, but the valuable information for the target task cannot be distilled for each user during the training process. Therefore, this paper proposes a hierarchical attention information fusion model in this section, which learns to extract the most valuable information from multi-levels. First, we need to fuse the common representation and matching representation of users under each channel. For each user, we learn the attention coecients which represent the weight of the user’s common representation and matching representation under dierent channels. The attention function in this section can be dened as follows: where each channel is expressed as: Secondly, after obtaining the user representation under each channel, the user representations of dierent channels need to be fused. Like the user information fusion within one channel, we adopt the multi-channel fusion technology based on the attention mechanism, and nally get the comprehensive user representation of the multi-channel: where to equation (15). 3.4 Overall Optimization After obtaining the user’s comprehensive representation (section 3.3), we use the interactive matrix 𝑅 between the users and the items to obtain the nal item representation: where 𝐷 Except for the hierarchical self-supervised learning tasks (auxiliary task 1) based on cross-channel matching representing. We refer to the MHCN[ 𝑈⊂ 𝑈is the user set of the current mini-batch,ℎ∈ Ris the matching representation ofÍ under channel𝑖,𝑧=𝑒∗ ℎis the matching representation of the ego-network 𝑢under channel𝑖,𝑒indicates the weight of the edges between𝑢and𝑣under channel𝑖.˜ℎ ˜𝑧.𝑧= 𝐴𝑣𝑒𝑟𝑎𝑔𝑒𝑃𝑜𝑜𝑙𝑖𝑛𝑔(𝐻)is the average of the matching representation of all nodes in 𝑎 ∈ Rand𝑊∈ Rare learnable parameters. Finally, the user representation under 𝛼is the attention coecient of the corresponding channel, and its learning method is similar ∈ Ris the degree matrix of 𝑅. Input: User-Item graph adjacency matrix 𝑅 and Social Graph adjacency matrix 𝐴. Output: Recommendation prediction result 1. Initialize all parameters 2. Obtain the relationship 𝐴 3. for i=1,2,3,...,𝑒𝑝𝑜𝑐ℎ do 18. end for 19. Return learning, and use it as an auxiliary task 2: where under channel 𝑢under channel negative sample of graph under channel 𝑖. For the main task, use the BPR loss function[27] to optimize the model: where coecient. In each training process, a randomly selected positive sample item representation according to equation (13). representation according to equation (14). representation (Auxiliary task 1). and matching representation under dierent channels according to equation (16). equation (18). (Auxiliary task 2). 𝑈is the user set of the current mini-batch,ℎ∈ Ris the common representation of user𝑢Í 𝑖,𝑧=𝑒∗ ℎis the common representation of the ego-network of user .𝑧= 𝐴𝑣𝑒𝑟𝑎𝑔𝑒𝑃𝑜𝑜𝑙𝑖𝑛𝑔(𝐻).is the average of the common representation of all nodes in the 𝐼 (𝑢)is the set of items purchased by user𝑢,𝜃is the model parameters,𝜆is regularization user 𝑢 and a undetected item 𝑗 form a triple for training optimization. prediction between 𝑢 and 𝑖, which is calculated by the representation of user and item: In summary, the auxiliary tasks 1 and 2 are jointly trained with the main task to optimize the model: where 1 illustrates the main steps that are applied to optimize the framework. 4 EXPERIMENTS In the experiment section, we will conduct various experiments (including performance comparison experiments, ablation experiments, and parameter sensitivity experiments) on real datasets to answer the following questions: • RQ1: • RQ2: • RQ3: How do dierent hyperparameter settings aect the model? 4.1 Experimental Protocol 4.1.1 Datasets. In the experiment, we select three public real datasets commonly used in recommender systems: Douban[ of the experiment is to perform Top-10 recommendations on the processed open-source data that comes from[27, 43]. The statistical information of the experimental data is shown in Table 2. 4.1.2 Baselines. To fully verify the performance of the CMHC framework proposed in this paper, we compare it with the most advanced and commonly used baseline methods. There are MFbased methods, GNN-based methods, methods with SSL auxiliary tasks, and methods without SSL auxiliary tasks. It is worth noting that SGL[ noted that the following baseline methods are from the open-source library QRec 𝛽and𝛽are the coecients of auxiliary task 1 and auxiliary task 2, respectively. Algorithm How does the method proposed in this paper as compared with the state-of-the-art recommendation methods? What are the benets of the cross-channel matching representation and hierarchical SSL on cross-channel information proposed in this paper? BPR[27]. This is a classic recommendation method. It rst proposed a general optimization method based on Bayesian Personalized Ranking. This paradigm has been used by subsequent research and is the basis of the ranking recommendation algorithms. SBPR[46]. This is an optimized version based on the BPR model[27]. It integrates social relationships into the BPR model and tends to give higher rankings to items liked by the friends of the target user. Compared with BPR, it achieves better performance in both general and cold-start scenarios. 4.1.3 Metrics. To comprehensively evaluate the models, we selected three commonly used metrics in the recommender systems, namely: Precision@10, Recall@10, and NDCG@10. 4.1.4 Experiment Seings. For the baseline methods, we use grid search to determine the best parameter settings. The maximum iteration is searched in {10,20,30,40,50,60,70,80,90,100}, for other parameters, we use the best settings suggested in the original paper to maintain its optimal performance in real scenarios experiment, the user and item embedding dimensions 2000. For our method, the regularization coecient are determined as in {0.01,0.02,0.05,0.1,0.2,0.5,0.6,0.7,0.8}, and the optimal value is nally determined as maximum iteration is determined as 30, and the model layer is set to 2. To ensure the stability, comprehensiveness, and credibility of the results, the result of each experiment is the average result NGCF[32]. This is a collaborative ltering recommendation algorithm based on GNN. It believes that the high-order relationship of user and items contains rich collaborative ltering information. Its core idea is to incorporate the high-order relationship into the process of representation learning, to better integrate collaborative ltering information into users and items. DiNet[36]. This is also a representative model of integrating social relationships into recommendations. However, it believes that the social inuence of the user is dynamic, and this inuence makes the interest of involving users constantly changing in the process of diusion. Based on this, a social inuence communication model is designed to improve the performance of the recommendation model. LightGCN[12]. This is a recent novel study. In this work, the author studied GNN-based collaborative ltering models such as NGCF[32] and found that the two standard operations(modelfeature conversion and nonlinearity operations) in the GNN are not necessary for collaborative ltering, but increase the diculty of training. Therefore, by simplifying the GNN model, they improved the recommendation performance of collaborative ltering while reducing training diculty and computational complexity. SGL[35]. It is the latest exploration to use SSL to solve the limitations of GNN-based recommendations under the supervised learning paradigm. It believes that existing GNN-based recommendation methods for learning the representation of use-item graphs have limitations in solving long-tail recommendations and resisting noise interaction. It designs three strategies of Node-Drop, Edge-Drop, and Random-Walk to achieve data augmentation, and obtains dierent graph structure views to construct SSL auxiliary tasks, which can eectively break through the limitations brought by GNNs. -MHCN[43]. This is the latest social recommendation model with self-supervised learning. It combines social networks with user-item graphs, constructs three-channel information, and encodes each channel through the hypergraph neural network. On this basis, it performs self-supervised learning through its proposed HMIN model in each view, which is regarded as an auxiliary task. But it does not fully explore the relationship between dierent channels, which is the signicant dierence with this paper. are searched in {0.001,0.002,0.003,0.005,0.01,0.02,0.05,0.1}, and nally, the optimal values Yelp of 5 cross-validations, and without special instructions, the result is the average of the 10 times experiments. 4.2 Performance Comparison(RQ1) In this section, we use performance comparison experiments to verify whether the CMHC framework proposed in this paper can outperform the most advanced recommendation methods. As analyzed previously, self-supervised learning can not only boost the recommendation performance in general scenarios by mining richer information but also can alleviate the problem of data sparseness in cold-start scenarios with the help of the correlation between multi-channel information. Therefore, we verify the performance of dierent methods on the complete datasets in the general scenario and the sparse data in the cold-start scenario. The sparse data in the cold-start scenario only retain users with less than 20 interactions. The experimental results are shown in Table 3 and Table 4, respectively, which is the best performance of 10 times experiments. Bold represents the best result; underline represents the sub-optimal result. By analyzing the results, we can draw the following conclusions: P@10 14.45% 18.50% 18.34% 17.19% 19.45% 20.46% 20.76% 21.18% +2.02% N@10 0.1669 0.2120 0.2069 0.1929 0.2227 0.2322 0.2358 0.2413 +2.33% P@10 25.62% 25.24% 24.37% 25.72% 0.21% 26.19% 26.00% 27.33% +4.35% R@1048.49% 46.03% 43.45% 48.43% 0.85% 49.59% 49.11% 51.34% +3.53% N@10 0.5687 0.5208 0.4778 0.5618 0.0052 0.5805 0.5797 0.5867 +1.07% N@10 0.0246 0.0339 0.0433 0.0403 0.0566 0.0496 0.0604 0.0609 +0.83% N@10 0.0252 0.0468 0.0407 0.0417 0.0504 0.0521 0.0541 0.0555 +2.59% P@10 10.51% 9.38% 8.70% 10.34% 0.13% 10.54% 10.74% 11.05% +2.89% R@10 44.42% 39.35% 36.79% 43.66% 1.02% 44.70% 45.50% 46.93% +3.14% N@10 0.4612 0.4012 0.3554 0.4502 0.0053 0.4733 0.4704 0.4906 +3.66% N@10 0.0.16 0.0279 0.0364 0.0349 0.0506 0.0525 0.0435 0.0535 +1.90% The CMHC framework proposed in this paper achieves the best performance among all methods. CMHC outperforms𝑆-MHCN (the most competitive baseline) by a distinct margin and the improvement is more obvious in the cold-start scenario, which veries the performance the benets of the cross-channel matching representation module and the hierarchical selfsupervising module proposed in this paper (This is also veried in section 4.3.2). LightGCN is a very competitive baseline method. Its performance is comparable to that of𝑆-MHCN, and it is a lighter model with lower computational complexity. Fig. 6. Schematic diagram of the contribution of dierent channel information on dierent datasets to the performance. 4.3 Ablation Study (RQ2) First, one of the highlights of this paper is to make full use of multi-channel information for self-supervised learning, to maintain the unique characteristics of dierent channels, and mine the associated information between dierent channels. Therefore, we rst conducted an ablation study on multi-channel information. Specically, we get dierent variants by removing dierent channels. For example, w/o Social means removing the Social-channel and retaining the Joint and Purchase channels. By comparing the performance of dierent variants and the complete model which including all channels on dierent datasets, the contribution of dierent channels to the performance is veried. The experimental results are shown in Figure 6. By analyzing the results in Figure 6, we can draw the following conclusions: 4.3.1 Eects of the Multi-channel Information. The overall performance of SSL methods, such as SGL,𝑆-MHCN and CMHC, is better than other methods, especially in the cold-start scenario, which strongly proves the benet of selfsupervised learning on the recommendation performance. Various self-supervised learning tasks can autonomous mine the potential characteristics from the raw data or the associated information between dierent channels, which can alleviate the problem of sparsity. This is particularly important in the recommendation scenario of massive data, and it also shows that research on self-supervised learning is a promising work in the eld of recommender systems. The GNN-based methods perform better than BPR and SBPR. Graph data has more advantages than traditional European data in relational modeling. The high-order relation mining ability of hypergraph has further improved the recommendation performance. This is also an important reason for the rapid industrial application of GNN-based methods in recommender systems in recent years. Methods that incorporate extra information such as social relationships, such as SBPR, DiNet,𝑆-MHCN, and CMHC possess more satisfactory result. At present, social networks have become the most eective carrier for mining user interests and potential behavior characteristics. The historical behavior of users on social networks can provide the most eective reference for future recommendations. This can also explain that although SGL uses self-supervised learning to promote the recommendation task, its performance does not meet expectations, especially on FilmTrust date set. To further verify the contribution of the information of dierent channels on dierent datasets to the performance, the attention coecients of dierent channels are recorded during the experiment. We draw the data distribution as shown in Figure 7. It can be seen from Figure 7 that on dierent datasets, the information of dierent channels does play dierent roles. On three datasets, Purchasechannel plays the most important role. On FilmTrust and Yelp, most attention values of Socialchannel are close to 0, which is consistent with the conclusion of MHCN[ that the contribution of Social-channel on the Douban dataset is higher than that of the Joint-channel. We believe that the reason may be that the addition of cross-channel matching representation learning, and hierarchical SSL makes the model mine more important social information. Jointly analyzing Figure 6 and Figure 7, we can nd that the attention distributions on the datasets in Figure 7 are consistent with the performance of the corresponding variants in Figure 6. This again veries the rationality of the multi-channel attention fusion model, that is, it can learn the attention that better integrates the information of each channel and maximizes the performance. 4.3.2 Eects of the Proposed Core Components. To comprehensively verify the rationality and eects of the core components of the model proposed in this paper, we rst conduct ablation experiments, and construct variants of the CMHC by removing dierent core components. CMHC represents the variant obtained by removing both the matching representation module (section 3.1) and hierarchical SSL (section 3.2) from CMHC. CMHC the hierarchical SSL model (section 3.2) from CMHC. Compared with CMHC the matching representation module (section 3.1). CMHC is the complete model proposed in this paper that includes both matching representation module (section 3.1) and hierarchical SSL model (section 3.2). The performance of the above three variants is shown in Figure 8. By analyzing the results in Figure 8, we can get the following conclusions: On all datasets, the complete model consistently outperforms the three variants that remove the single-channel data, which proves that the multiple channels are reasonable and ecient. This conclusion is consistent with MHCN[43]. By comparing the three variants, we can nd that their performances on dierent datasets are dierent. For example, on the Douban dataset, w/o Joint achieves the best performance, w/o Social is the second, and w/o Purchase achieves the worst performance. This shows that the value of Purchase-channel information is the largest on the Douban dataset, and the Social-channel is the second, Joint-channel is the smallest. The situation on other datasets is dierent, which shows that the information of dierent channels plays dierent roles on dierent datasets. This will be further analyzed in the experiment of the attention mechanism (Figure 7). 4.4 Parameter Sensitivity Analysis (RQ3) 4.4.1 Coeicients of SSL. In this section, we rst explore the two most important hyperparameters: 𝛽,the coecients of hierarchical self-supervised loss based on cross-channel matching representations(auxiliary task 1) and common representations(auxiliary task 2). Figure 9 clearly depicts the performance of dierent combinations of of the model is relatively stable, and it has a good tolerance for the selection of adjustment of the two parameters did not cause signicant variation in performance. It is worth noting that the two metrics Precision@10 and Recall@10 have the same performance trends as NDCG@10. Due to space limitations, we will not list specic data. 4.4.2 The depth of CMHC. Further, in this section, we investigate the impact of the depth of CMHC. Specically, we increase the depth from 1 to 5 and draw the performance curves of each metric on dierent datasets, as shown in Figure 10. From Figure 10, we can see that the model achieves the best performance when the depth is set to 2. As the model deepen, the performance on the Compared with CMHC, CMHCbrings consistent and signicant performance improvements on all metrics and all datasets. This shows that the matching representation module (section 3.1) can dig out the associated information between dierent channels by learning the cross-channel matching representations, which is extremely valuable for recommendation tasks, even if it has not been leveraged by self-supervised tasks (section 3.2). Compared with CMHC, the performance on all datasets of CMHC has been further improved. This shows that the hierarchical SSL model (section 3.2) proposed in this paper can make full use of the feature information captured by the matching representation module (section 3.1) to establish an eective self-supervised task, and as an auxiliary task to promote the performance of the main task. Through the joint analysis of Figure 6 and Figure 2, we can see that a reasonable design of the matching representation model between dierent channels and the self-supervision task based on this can not only eliminate the performance decline problem caused by directly performing contrastive learning on dierent channel and can bring signicant performance improvements. This shows that the original purpose of this paper has been achieved, which again veries the exploration and analysis conclusions of this paper in Prelimations 2.3. Fig. 9. The performance of CMHC with dierent combinations of 𝛽 FilmTrust dataset is slightly higher but the performance on the Douban and Yelp dataset declines obviously, which is consistent with MHCN[ on motifs naturally extracts the high-order neighborhood of nodes. Compared with the ordinary GNN model, as the model deepens, the over-tting phenomenon will appear earlier. This is also in line with the conclusions of exploration on the depth of model in some previous studies[ How to solve the over-tting caused by deep models is also a problem that needs to be further studied in future work about GNNs, especially the Hyper-GNNs. 4.4.3 Temperature Hyper-parameter of InfoNCE. Finally, we also investigate the impact of dierent 𝜏values on the performance of CMHC. It can be seen from Figure 11 that when the (below 0.05), all metrics on each dataset will drop dramatically. When the value of range of [0.05, 0.8], the performance of the model is better and more stable, indicating that this range is a reasonable setting. This conclusion is consistent with SGL[35]. 5 RELATED WORK In this section, we will review and analyze the research work related to this paper, including the following aspects: 5.1 Hypergraph and Hypergraph Neural Networks In recent years, the mining and analysis of graph data has become a research hotspot in the eld of data mining and articial intelligence. Graphs can well represent node attributes and topological structure, which can better model the anities between nodes[ emergence of GNNs has accelerated the industrialization process based on graph data research, such as: knowledge question answering and dialogue systems[ [2, 15, 26, 31, 47], intelligent search[4, 20], and etc. Fig. 10. The performance of CMHC with dierent depths on dierent datasets. However, with the generation of more and more interactive data, a relationship may not only be limited to two nodes, that is, node pairs can no longer represent some more complex interactive relationships, such as the stable triangular relationship in social networks[ relationship in the e-commerce networks[ the traditional graph, and its edge can connect any number of nodes, and no longer limited to node pairs. Recently, some scholars have also begun to explore modeling methods on hypergraphs. Feng et al.[ HGNN, which takes full advantage of the hypergraph fusion of multi-channel data by combining adjacent matrices. Compared with the traditional GNNs, it can better model the multi-channel interactive data of the social network. Some GNN-based methods for recommender systems are proposed recently. Wang et al.[ of items in recommendation scenario, but the traditional graph structure can only represent the pairwise relationship, which is not suitable for a real recommendation scenario. Therefore, they use sequential hypergraphs to model the user’s behavior in dierent periods and use the Hyper-GNN to capture the interaction of multi-hop connections. In addition, through the combination with residual gating and fusion layer, the model can distill user preference more accurately, and the ability of sequence recommendation is signicantly improved. Ji et al.[ traditional collaborative ltering recommendation based on matrix factorization or graph-based collaborative ltering recommendation, there are deciencies in the exibility of modeling the relationship between users and items and high-order relationships. Based on this analysis, they propose a hypergraph-based collaborative ltering recommendation model, DHCF. The model adopts a dual-channel strategy and uses Jump Hyper-GNN (JHConv) to model users and items. The experimental results prove the value of high-order information for representation learning and the eectiveness of the proposed dual-channel hypergraph model. This paper also uses Hyper-GNNs under dierent channels to model the high-order interaction between users and items, but it is fundamentally dierent from DHCF. This paper devotes to model the interaction information and from multi-channel with dierent patterns, not just interaction information within each channel. More importantly, the focus of this paper is to perform cross-channel matching representation learning to discover the potential association relationships between dierent interaction patterns, and then make full use of the it for self-supervised learning. 5.2 Self-Supervised Learning After many years of development, machine learning methods, especially deep learning methods, have achieved great success in image processing[ elds. The advantage of deep learning is to mine the valuable features of data from massive amounts of samples, but it requires a large amount of data and labels as input, which makes it dicult to Fig. 11. The performance of CMHC with dierent 𝜏 values on dierent datasets. 7] propose a hypergraph neural network representation learning framework, called apply deep learning models to some scenarios where data or label is scarce. The emergence of self-supervised learning (SSL) can well alleviate this problem. The core goal of SSL is to be able to extract valuable information from the data itself or the association between data. When label data cannot be obtained, SSL can play the role of unsupervised learning; in scenarios where limited label data is available, SSL can play a role of pre-training or tuning[ eld of image processing, such as image restoration and image denoising[ divided into contrastive model and prediction model[ encoder to encode the data pairs, and then distinguishes the positive instances by calculating the encoding feature distance between the positive and negative pairs (such as the maximum mutual information). The input of the prediction model is generally a single instance. First, a certain method is used to construct a label (generally called as self-generated label), then the data is encoded and predicted, and nally the loss between the predicted label and the generated label is calculated. At present, with the continuous in-depth research on graph data, some self-supervised learning methods for graph data have been proposed. Shi et al.[ framework,which encodes node representations from the network view and the meta-path view, respectively, which is used to establish a contrastive learning task. The framework improves the ability to extract both local and high-order structures of nodes. Qiu et al.[ attributes of nodes are not transferable, but the structure can be transferred between dierent graphs. Therefore, they focuse on studying how to mine the structural similarity of graphs through SSL tasks and transfer information between dierent graphs. The GCC framework they proposed is to sample the same node in the same ego network to obtain a positive subgraph, use noise interference from other ego networks to obtain multiple negative subgraphs, and then dierent GNN-based encoders are used to these subgraphs, based on which the contrastive learning task is established to pre-training the encoders. The experimental results show that the GCC pre-training framework can greatly improve the performance of the initial model. 5.3 Self-supervised Graph Learning for Recommendation At present, the graph-based recommendation model has become the most popular topic, which greatly boosts the performance of the recommendation. However, this performance improvement is guaranteed by enormous interactive information. On the one hand, with the higher real-time requirements of the recommender systems, the obstacle to obtain the latest high-quality label data in time makes it dicult for the recommender systems to iteratively train in real-time[ other hand, in large-scale application platforms, new users, or new items have made the cold-start problem more serious[ it can also improve the utilization of data, autonomously dig out potentially valuable information, and alleviate real-time recommendation scenarios and cold-start problems to some extent. The combination of SSL and recommender systems is a new research hotspot in the past two years. Data augmentation is the core of SSL. Due to the continuity of node attributes and the complex relationship between nodes, data augmentation methods in the eld of CV and NLP are dicult to directly apply to the recommendation eld[ Dropout, Node-Dropout, and Random Work to generate structural variants of nodes from the original graph. This data augmentation strategy is conducive to capturing the structural pattern of the graph. However, the patterns of social relations between users in the recommender systems can be diversied, so is items. It is dicult to mine the semantic interaction information under various channels in the recommender systems by random strategy. To explore the high-order interactions and semantic interaction patterns in the recommender systems, some recent studies have begun to combine Hyper-GNNs with SSL. Xin et al.[ rst attempt. First, they leverage the hypergraph to construct the session association relationship between items and then construct the association graph between dierent sessions, the proposed two-channel Hyper-GNN is used to obtain the feature representations of the two channels, based on which the SSL task is established. The experimental results show that the proposed SSL can signicantly improve the recommendation performance as an auxiliary task. Yu et al. propose the MHCN model[ Motifs, Joint Motifs, and Purchase Motif. Finally, in each channel, the proposed Hierarchical mutual information maximization model is used as a self-supervised auxiliary task to optimize the recommendation model, which greatly improves the recommendation performance. 6 CONCLUSION AND FUTURE WORK The key issue studied in this paper is how to use multi-channel data to perform more ecient self-supervised learning tasks to enhance the performance of recommendation. First , we make the assumption that constructing a contrastive learning task directly on the features of dierent channels will make the data of each channel homogeneous, which will deteriorate the recommendation performance. In the exploration experiment, we veried this assumption. To tackle this problem, we propose the CMHC framework, which can make full use of the information in each channel and the information across channels to construct self-supervised learning tasks, thereby improving recommendation performance. Specically, to comprehensively mine the associated information between dierent channels while avoiding the problem of homogenization, we propose a crosschannel matching representation learning, which learn the cross-channel matching representation by Attentive-Matching. On this basis, we innovatively proposed a cross-channel hierarchical SSL model based on matching representation, which realized two levels of self-supervised learning within and between channels and improved the ability of self-supervised tasks to autonomously mine dierent levels of potential information. Finally, we unify the recommendation task (main task), hierarchical self-supervised learning task based on cross-channel matching representations (auxiliary task 1), and self-supervised learning task based on intra-channel common representations (auxiliary task 2) for joint learning. A large number of experimental results on three real datasets show that CMHC outperforms the state-of-the art methods by big margins, and the ablation studies also prove the benets of each core components proposed in this paper. However, in the model depth exploration experiment (section 4.4.2), we also nd that as the model deepen, the performance on some datasets drops obviously. The reason is as analyzed in section 4.4.2: our proposed framework based on motifs naturally extracts the high-order neighborhood of nodes. Compared with the ordinary GNN model, as the model deepens, the over-tting phenomenon will appear earlier. This is also the pain point of hypergraph neural network[ addressed. Therefore, how to solve over-tting caused by the deep model is also a problem that needs to be further studied in future work about graph neural networks, especially the hypergraph neural networks. On the other hand, how to integrate valuable information such as text, sound, images, etc., is also a promising research work in recommender systems.