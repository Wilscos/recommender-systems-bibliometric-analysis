Machine learning based screening models have emerged as one of the most promising approaches for discovery of new materials either from repositories of known materials [ or/and structures [ Corresponding author: J.H. (http://www.cse.sc.edu/∼jianjunh) Data driven generative machine learning models have recently emerged as one of the most promising approaches for new materials discovery. While the generator models can generate millions of candidates, it is critical to train fast and accurate machine learning models to ﬁlter out stable, synthesizable materials with desired properties. However, such efforts to build supervised regression or classiﬁcation screening models have been severely hindered by the lack of unstable or unsynthesizable samples, which usually are not collected and deposited in materials databases such as ICSD and Materials Project (MP). At the same time, there are a signiﬁcant amount of unlabelled data available in these databases. Here we propose a semi-supervised deep neural network (TSDNN) model for high-performance formation energy and synthesizability prediction, which is achieved via its unique teacher-student dual network architecture and its effective exploitation of the large amount of unlabeled data. For formation energy based stability screening, our semi-supervised classiﬁer achieves an absolute 10.3% accuracy improvement compared to the baseline CGCNN regression model. For synthesizability prediction, our model signiﬁcantly increases the baseline PU learning’s true positive rate from 87.9% to 97.9% using 1/49 model parameters. To further prove the effectiveness of our models, we combined our TSDNN-energy and TSDNN-synthesizability models with our CubicGAN generator to discover novel stable cubic structures. Out of 1000 recommended candidate samples by our models, 512 of them have negative formation energies as validated by our DFT formation energy calculations. Our experimental results show that our semi-supervised deep neural networks can signiﬁcantly improve the screening accuracy in large-scale generative materials design. materials property prediction·semi-supervised learning·graph neural networks·materials discovery· 7,8] generated by generative deep learning models or by crystal structure prediction algorithms [9]. While existing materials repositories such as ICSD [ ﬁnding known synthesizable materials with new potential new functions, the success rate to discovering materials with extremely novel properties is severely constrained by the limited diversity and the number of known materials: the ICSD has only about materials in uncharted chemical space, it is important to develop the capability to screen stable and synthesizable hypothetical materials [ high-performance materials property prediction models to ﬁnd desired candidates [14, 15]. Given a material’s structure, its structural stability can be estimated by calculating its formation energy using ﬁrstprinciples computations such as density functional theory (DFT) and the phase stability of a structure can be quantiﬁed by the energy above hull (Ehull) [ expensive, which leads to a large number of machine learning models for formation energy/enthalpy prediction [ based on composition without [ the development of more than a dozen of formation energy/enthalpy prediction models, they all suffer from a neglected strong bias from the training data: most of the training samples from the repositories of know materials are stable structures with negative formation energy. For example, out of the 138,613 samples of Materials Project database, only 11,340 samples have positive formation energy. This makes it difﬁcult to train good supervised classiﬁcation or regression models that can differentiate stable materials from the unstable candidates. These methods usually formulate the formation energy prediction problem as a regression problem with models trained with a majority of negative formation energy. However, such formation energy prediction models are most interesting when they can be used to differentiate stable versus non-stable hypothetical materials, most of which tend to be unstable and have positive formation energy. Despite the claimed high accuracy of these models [ evaluated on the stable materials with negative formation energy, leading to their questionable extrapolation performance on out-of-distribution non-stable materials with positive formation energy [ ML models with a majority of samples with only negative formation energy while they are expected to differentiate stable materials with negative formation energy from unstable materials with positive formation energy. In addition to this issue, it is argued that the accurate prediction of formation alone does not correspond exactly to high accuracy of predicting stability which can be better measured by the quantity formation enthalpy (∆Hf)-composition space [25]. Synthesizability of a hypothetical material is another important property needed for effective materials screening [30,31] which is challenging to predict accurately [ tend to generate unsynthesizable candidates [ predicted using ML models or other computational methods [ given a material composition [ applied to the large scale of hypothetical materials. Another option is the ML based models for materials synthesizability prediction. For inorganic materials, a recent study using the Positive and Unlabelled semi-supervised machine learning algorithm (PU-learning) [13] has been applied to predict synthesizability with promising results. Here we propose a semi-supervised learning (SSL) approach for the materials formation energy and synthesizability prediction problems by considering both the database bias that most samples are stable, synthesizable materials with negative formation energy and the model application scenarios for which we need to apply the models to differentiate stable and unstable hypothetical materials. Semi-supervised learning [ computer vision[ data issue or just to improve the performance using unlabelled data. However, despite the well-known small data issue in materials ML problems, semi-supervised learning has rarely been used in such problems except in a few studies [43, 13, 44] for materials synthesis classiﬁcation, microstructure classiﬁcation, and synthesizability prediction. SSL algorithms are developed on several fundamental assumptions [ samples close to each other in the input space tend to have similar labels; (2) low-density assumption: the decision boundary should not pass through high-density areas in the input space; (3) manifold assumption: data points on the same low-dimensional manifold should have the same label. These assumptions can be interpreted as speciﬁc instances of the cluster assumption: similar points tend to belong to the same group/cluster. There are two main category of SSL algorithms including graph based transductive methods which focus on label propagation and inductive methods which aim to build a ML model f:x–>y by incorporating unlabelled data either in pre-processing steps, directly inside the loss function, or via a pseudo-labeling step. SSL algorithms have demonstrated strong performance especially in the deep learning framework [41]. In this work, we exploit a deep learning based SSL framework, the teacher-student deep neural networks (TSDNN) [45] to address the lack of negative samples in synthesizability prediction and formation energy prediction. TSDNN is characterized by a dual-network architecture with a teacher model trained using a supervised signal and an unsupervised 40] , natural language processing[41], medical diagnosis [42] to mainly address the scarce annotation feedback signal from the student to improve the teacher’s pseudo-labeling. The teacher provides pseudo-labels for unlabeled data for the student to learn from. Unlike the previous positive-unlabeled SSL algorithm for synthesizability prediction, our TSDNN algorithm requires much fewer models to be trained to determine negative samples and train a ﬁnal model while achieving 5.3% higher prediction accuracy improving the positive rate from 86.7% to 92.9% using the same performance evaluation. Extensive experiments on the formation energy classiﬁers also show that our TSDNN can screen negative formation energies with 7.5% higher precision, 10.3% higher F1 score, and 9.7% higher accuracy than the CGCNN regression model. Our contributions in this paper can be summarized as follows: We follow a generation-and-screening approach for the discovery of novel materials: ﬁrst, we use generative deep learning algorithms to generate hypothetical crystal structures in a high-throughput manner with millions of candidates [8]. The generated candidates will then be screened quickly using formation energy and synthesizability machine learning models. Finally, a set of top screened candidates will be veriﬁed by DFT based formation energy calculation and phonon dispersion veriﬁcation. It should be noted that generative algorithms of materials compositions [ be used here to ﬁrst generate and screen out top compositions that are then fed to crystal structure prediction algorithms for structure determination and follow-up DFT validation. In this work, we use our recently developed CubicGAN algorithm [ cubic crystal structures of three space groups (221,225, 216) which are reduced to 2.5 million unique candidate cubic structures. With such a high volume of candidates, how to ﬁnd the stable and synthesizable ones is almost like ﬁnding the needle in the haystack. To address this challenge, we develop semi-supervised deep learning based classiﬁcation models for identifying hypothetical materials candidates with negative formation energy and high synthesizability respectively. 2.2 Semi-supervised learning based screening models using teacher-student deep neural networks (TSDNN) In many problem domains and especially in materials science, the labeled dataset is too small, unbalanced, or has missing data classes. For example, there are only about 2700 crystal materials with labelled thermal conductivity values [47 in MP database, there are fewer than 8.2% materials labelled with positive formation energy. As a result, it becomes difﬁcult to train a well-converged supervised model to produce accurate classiﬁcations, especially on out-of-distribution samples. In this work, we propose to combine the TSDNN semi-supervised learning framework shown in Figure 1 with a crystal graph convolutional neural network (CGCNN) [ formation energy prediction. The teacher-student deep neural network (TSDNN) leverages unlabeled data to overcome the issues with the too few labeled samples and the severe issue of lack of negative samples: unstable samples with positive formation energy or non-synthesizable samples. TSDNN is a semi-supervised learning framework which is composed of two neural network models (Figure 1a): a teacher network and a student network. These two models are trained in parallel. The teacher model generates pseudo labels on the unlabelled data which are then used to train the student network. The teacher model is trained with two objectives in our case: labeled data (synthesizability or formation energy classiﬁcation) performance and a feedback signal [45 We identify the inherent dataset bias in formation energy and synthesizability prediction problems and propose to formulate both as semi-supervised classiﬁcation problems. We exploit a novel teacher-student dual network deep neural network model framework to achieve highperformance semi-supervised learning for both formation energy and synthesizability classiﬁcation. Compared to the previous approaches, our models achieved >10% performance improvement with much simpler model structures with 98% fewer model sizes. We evaluate our algorithms on different data set conﬁgurations and demonstrate the effectiveness and advantage of SSL for both problems. We apply our TSDNN based formation energy and synthesizability SSL model for screening new materials from the hypothetical cubic crystal materials and identify a set of new stable materials as veriﬁed by DFT formation energy calculations. ] and less than 1700 annotated piezoelectric materials in the Materials Project (MP) database [48]. Also ] from the student model based on its performance on the labelled dataset. This feedback signal provides a (a) TSDNN Framework. and negative labeled data respectively. Figure 1: Dataset generation and TSDNN training framework and comparison to the PU-learning framework guide for the teacher model in the case when the unlabeled samples are unlike the labeled data. The student model is trained only on unlabeled data with hard pseudo-labels provided by the teacher model. This leverages unlabeled data to improve further than supervised learning and smooth biases that may be found in the labeled data, such as through imbalances as with formation energy classiﬁcation. The training process goes as follows: ﬁrst, a batch of labeled and a batch of unlabeled data are sampled. The teacher’s loss is calculated on the labeled batch. The teacher provides pseudo-labels for the student, with which the student will be updated. The student’s performance on the labeled data is evaluated before the student is updated with the pseudo-labels and again after. The change in this performance that resulted from the teacher’s pseudo-labels is used to calculate the student’s feedback signal. This feedback signal is combined with the teacher’s loss over labeled data to update the teacher model. This results in a student that is able to learn the true labels of a large set of unlabeled data without introducing biases from the labeled dataset. Loss functions of our student and teacher network include: WhereL respect to the labels produced by the teacher T . This is the student’s only loss function. WhereL model T . A feedback signal from the student model[ improving its pseudo-labeling. This further reduces data bias introduction by introducing a dynamic teacher. While a static teacher model would replicate the implicit biases, this dynamic teacher is able to adapt, which leads to a less biased student model. For the method to work efﬁciently, a few conditions must be met. represents the cross-entropy lossCEon a batch of unlabeled datasetXfor the studentSnetwork with represents the standard supervised cross-entropy lossCEfor a batch of labeled data(x, y)for the teacher The labeled dataset should be accurate. Otherwise, the student model will be trying to optimize based on inaccurate teacher. The student feedback signal allows for some teacher inaccuracy to be resolved, but the majority of labels should be accurate. 3. There should be a set of representative samples from each class within the unlabeled dataset. To satisfy these criteria, we have introduced a novel approach of semi-supervised seeding similar to those found in semi-supervised clustering. By introducing a random distribution of data from the labeled set into the unlabeled set, we are able to effectively ensure that the student model has representative samples from each class that are similar to those found in the labeled set. In the teacher-student deep neural network framework, before training can commence, the dataset must be prepared for our semi-supervised framework. In the case of synthesizability, there is only positive data, so we must ﬁrst identify candidate negative samples. This method is described in Section 2.3. The labeled dataset should be optimized for accuracy by the smoothness assumption and low-density assumption. For both use-cases explored in this work, the labeled data selection is non-trivial. For synthesizability, selecting the most optimal negative labels, while resulting in fewer negative samples overall, results in the most optimal smoothness. For formation energy classiﬁcation, the low-density assumption is the most signiﬁcant to account for due to the large number of materials with near-zero formation energies, as shown in Figure 3. Once the dataset is prepared and the SSL assumptions are provided for, the model is trained. Comparison with the PU-learning SSL framework: (PU) learning problem, meaning that there are only materials with ICSD entries which have been previously synthesize and experimental materials which may or may not be able to be synthesized. For material synthesizability classiﬁcation, we refer to one PU learning framework in particular [ dataset and then optimize using our framework. This PU learning framework is a modiﬁed transductive bagging support vector machine [ model is trained with a random selection of unlabeled data set as the negative class of equal size to the positive class. This model then produces predictions on the remaining unlabeled set. After a given number of iterations, the unlabeled scores are averaged, resulting in a ﬁnal score. The motivation is to identify a cluster of samples that lie apart from the positive class and any that lie in the middle should have an uncertain prediction score close to 0.5. Though there are results from a seemingly well-converged model, it will be unclear what prediction score threshold is reliable for true negative labels. For this reason, any supervised model trained from these results would be similarly unreliable. Using our semi-supervised framework, we were able to both identify the threshold of true negative samples and optimize the uncertain predictions close to 0.5. We optimize the materials with uncertain results by introducing the student feedback signal for the teacher, which results in a dynamic teacher that will be adjusted by the signal to generate pseudo-labels that coincide with the labeled data. This is integral for correctly classifying outlier samples, as the teacher will produce a pseudo label that, if the student then performs worse, will be adjusted. This allows the student model to be more robust and more reliable across all samples, as any out-of-distribution samples will be learned and optimized for performance on the labeled dataset. Simply using a semi-supervised pseudo-labeling model is not able to overcome the unreliability of the selected negative labels, as any inaccurate labels would be carried through the pseudo-labels. However, since our student model is trained only on unlabeled data and the student’s predictions are optimized for labeled performance, as discussed above, we were able to identify the threshold at which the results became unreliable. Since the negative materials in the test set were withheld as a group instead of a random distribution, the performance of the model on these samples indicate the reliability of our data. It is not immediately clear whether this is from the labeled data or unlabeled data. However, by iteratively lowering the threshold of samples chosen as the negative set, we were able to increase the true negative rate performance until it became balanced with true positive rate, which indicated that we reached the reliability threshold. If the performance does not improve, this would indicate that the issue is with the unlabeled set and may beneﬁt from semi-supervised seeding as discussed above. The most signiﬁcant difference between these two frameworks is that the PU learning framework is specialized for positive and unknown learning scenarios. Our framework provides a more robust and generalized approach that could be applied and modiﬁed to ﬁt any scenario. With it, we were able to approach two distinct non-standard scenarios in which there is not a well-deﬁned labeled dataset. For synthesizability, this is in the lack of a positive class and for formation energy it is that it is continuous data, lacking clear classiﬁcation boundaries. In both scenarios, we were able to leverage our SSL framework to gain insight into the underlying motifs of the data and optimize our dataset formation. There should be a set of samples similar to the labeled dataset, or a subset therein, held within the unlabeled dataset. This provides an accurate foundation for the student that the teacher will be able to easily correctly classify. CGCNN model for structure based classiﬁcation: classiﬁcation, which can be combined with any material property prediction model. Here we adopt the CGCNN model for structure based synthesizablity and formation energy prediction. The CGCNN (Figure 2) works by converting material structures in their unit cell into crystal graphs by encoding atoms as nodes and bonds as the edges between them. Encoding both the atomic features and bond interactions between atoms, the inherent structural characteristics can be learned. A convolutional neural network is then build on top of the crystal graphs to extract the feature representations to use for classiﬁcation. 2.3 Synthesizability screening model using semi-supervised learning models Our synthesizability screening model (TSDNN-syn) is a binary classiﬁcation model trained using the above-mentioned semi-supervised teacher-student neural network (TSDNN). We obtained the training dataset from the Materials Project database. More speciﬁcally, we obtained 125,619 materials with 48,146 of them being ICSD entries. As such, they are labeled as 1 to indicate that they are synthesizable materials. However, in the case of synthesizability prediction, there are no known un-synthesizable materials to label as 0. There are only ICSD entries and virtual materials, the latter with an unknown synthesizability status. This lack of negative samples prevents a traditional supervised classiﬁcation model from being trained as it normally would. To overcome this, we used a positive and unknown (PU) learning method [13] to identify materials with a low synthesizability score, as discussed above for the initial experimental dataset. In this framework, shown in Figure 1a, our TSDNN model is trained 5 independent iterations. Each iteration, a random subset of the positive data is selected as seed data to be placed in the unlabeled set. A random subset of unlabeled dataset is selected, prior to adding the seed data, to be the negative set with a size equal to the number of samples in the positive set after removing the seed data. A TSDNN model is then trained on this data. This model makes predictions on the unlabeled samples not selected as the negative set. The ﬁnal predicted scores are averaged across the 5 iterations to provide well converged results. We then selected the 48,146 lowest-scored materials (all below 0.33) to match the 48,146 positive samples. This provides a full labeled dataset with negative labels selected with more conﬁdence than random selection to use as a foundation. This dataset could be directly used to trained a supervised or semi-supervised model, which was performed with the Balanced TSDNN and Supervised CGCNN models. However, since the negative labeled materials are selected as the result of an imperfect model’s predictions, there will be false negatives introduced into the training data. This increases as materials are selected that had prediction scores closer to 0.5 than to 0.0. As a countermeasure to this, we leverage our semi-supervised model to gain insight into the dataset and select optimal negative samples. When trained with our semi-supervised model, the true negative rate is especially low compared to the true positive rate. However, when the threshold for negative samples is moved lower from 0.33, this performance improves. By utilizing this, we were able to determine the optimal negative class threshold to balance the true positive rate and true negative rate, which resulted in the improved performance of the Unbalanced TSDNN. Random TSDNN: The Random TSDNN model uses a labeled dataset composed of the 48146 ICSD entry materials and an equal number of the lowest classiﬁcation score samples from the PU learning dataset generation. Unbalanced TSDNN: The Unbalanced TSDNN uses the 48146 ICSD entries with 9629 removed for the test set, resulting in 38517 total positive samples. The negative set is composed of the 6648 remaining negative samples that were below the reliability threshold after removing the 9629 for the test set. TSDNN: The TSDNN model uses a training set of 48146 positive samples and 48146 negative samples, each with 9629 removed resulting in a training set of 38517 positive and 38517 negative samples. Supervised: The Supervised model uses the same labeled dataset as TSDNN. The hyper-parameters of our TSDNN-syn model trainings are set as follows: 2.4 Formation energy based screener using semi-supervised TSDNN framework We designed two different TSDNN models for formation energy prediction to overcome biases inherent with previous methods due to having few samples with positive formation energy. We designed the ﬁrst model, Separated TSDNN, to classify whether the formation energy of a material is above or below a threshold of -2.0 eV. We chose this threshold since there are many materials with slightly negative formation energy (-2.0, 0) that may be very structurally similar to those with slightly positive formation energies (0, 1.0). For this model, we used the materials with formation energies below -2.0 eV (n=5549) as positive samples in the labeled dataset. We selected an equal number of samples with the highest formation energies as negative samples. For the second model, Unseparated TSDNN, we used only materials with positive formation energies (n=2444) as negative samples and an equal number of randomly selected materials with negative formation energy as positive samples. This is optimized for a representative distribution of positive samples, with the intent of ensuring dataset smoothness and a low-density. This allows for improved smoothness by including samples with near-zero eV formation energies while still ensuring a low-density near the classiﬁcation threshold of 0.0 eV. This is a general screener for positive vs. negative formation energy screening as opposed to the ﬁrst approach, which is optimized for strictly low eV classiﬁcation. This approach resulted in a high-precision model, where 78.4% of samples with predicted scores greater than 0.5 have a formation energy of less than -2.0 eV and 99.0% having a negative formation energy. It correctly classiﬁed 57.8% of the possible samples with formation energies less than -2.0 eV. In both models, we use an unlabeled dataset with 500,000 CubicGAN-generated structures. These two models ensure there is a low sample density at the classiﬁcation threshold. To use the dataset as-is with a threshold of 0.0 eV would result in a very high-density of materials at the threshold. As such, we use the different thresholds and data-selection methods to account for this. Each model has distinct beneﬁts that are best suited for different applications, as discussed in Section 3.2.2. We structure our datasets in this way to correct for biases and inconsistencies that models are ingrained with due to the unbalanced nature of formation energy datasets. As shown in Figure 3a, the Material Project has an overwhelming majority of <0 eV materials. If trained from the raw data, it is likely that a model will bias heavily toward predicting >0 eV materials as being <0 eV. For this reason, we seek to combine the beneﬁt of our TSDNN model with a balanced dataset to remove this bias. It is of particular importance that the model be unbiased when used with generated materials, such as those produced by our CubicGAN, as they contain many more >0 eV materials. We seek to apply our method to provide superior screening performance in identifying low formation energy materials. We evaluate the TSDNN-syn models based on their true positive rate on each model’s respective test set. We use a prediction score boundary of 0.5 to determine a positive or negative sample classiﬁcation. This classiﬁcation performance can be expressed as whereT P samples falsely negatively classiﬁed with a predicted score < 0.5. Since only positive samples are known, the true positive rate is the best indicator of performance in showing a model that accurately classiﬁes true positive samples. is the number of true positive samples with predicted scores >= 0.5 andF Nis the number of true positive (a) Material Project Formation Energies [eV/atom] We evaluate the TSDNN-fe models on three metrics with variable formation energy thresholds: accuracy, precision, and F1 Score. We again use a prediction score boundary of 0.5 to determine a positive or negative sample classiﬁcation. The accuracy metric is shown as whereT P 0.5.T N number false negative and false positive classiﬁcations using the same thresholds. The precision and recall metrics can be expressed as whereP T . We use inorganic material structures obtained from the Materials Project[ synthesizability prediction model and our formation energy prediction models. The MP database is a widely used material database consisting of materials obtained from the ICSD[ calculations. In the case of synthesizability, we use the MP materials with ICSD entries as the positive dataset and the negative labels selected from the virtual MP materials as described in Section 2.3. For our formation energy model, we use a combination of the MP database and a custom dataset of material structures generated by our CubicGAN model[8]. Our criteria for selecting positive and negative samples are detailed in Section 3.2.2. Table 2 shows the source and number of samples in each dataset for each model. To compare the performance of our TSDNN models with the baseline PU-learning method, we ﬁrst prepare a random test dataset in the same way as done in previous work [ which are composed of a random selection of 9629 positive (synthesizable) samples from the labeled set. We ﬁnd our algorithm achieves 97.90% true positive rate due to the test set materials being very structurally similar to those found Figure 3: Distribution of formation energy for MP dataset and the Cubic test dataset. denotes the number of samples with a formation energy below the thresholdTwith predicted scores >= is the number of samples with a formation energy aboveTwith predicted scores < 0.5.F NandF Pare the is the model’s precision andRis the model’s recall both with respect to the given formation energy threshold in the training set. To validate that our model is able to accurately classify materials structurally different than those in the training set, we prepared a balanced test set composed of the 9629 negative samples with the lowest classiﬁcation score from the PU learning dataset generation and a group of randomly selected 9629 positive samples. By introducing the negative samples, we are able to ensure that the model does not simply predict all materials as positive and has actually learned the structure features linked to synthesizability. The Supervised CGCNN and Balanced TSDNN models use the same labeled datasets. The Balanced TSDNN model is trained using the remaining samples as the unlabeled set. This uses the unoptimized dataset provided from the dataset generation step. The Unbalanced TSDNN uses the optimized labeled dataset from the optimization step discussed in Section 2.3. Due to the fact that our CubicGAN generative model producing strictly cubic structures, we utilized only cubic Materials Project structures to train a formation-energy classiﬁcation model to predict samples with negative formation energies. To achieve this, we used only the Material Project database’s cubic structures to train our models. We used two selections of data for our formation energy models. The ﬁrst model, the Distributed TSDNN, only materials with formation energies lower than 0.0 eV are used as negative data (n=2444). We then randomly selected an equal number from the remaining samples as positive data (n=2444). This allowed for a balanced labeled dataset with a solid distribution of negative formation energy samples represented. The second model, the Separated TSDNN, is trained using the lowest 25% eV samples (n=5539) as positive data and the highest eV materials (n=5539) as negative data. This excludes the range of materials close to 0.0 eV. This motivation for this is to further separate the positive and negative classes in the input space. The CGCNN regression model is trained using the full cubic training dataset. We validate our formation energy models’ performance by testing it on our own dataset of cubic structures produced by the CubicGAN with DFT-calculated formation energies. For each model, we used a test set of 36,847 CubicGAN-generated structures with DFT-calculated formation energies. This test set has 16,407 negative formation energy samples and 20,440 positive formation energy samples. 3.2 Performance evaluation of TSDNN based semi-supervised learning We compare our TSDNN-syn and TSDNN-fe models against previous structure-based methods for predicting synthesizability and formation energies respectively. For synthesizability classiﬁcation, we compare against the previous semi-supervised method of PU learning [ CGCNN regression model. We perform additional performance validation of our method by screening 2,545,713 novel CubicGAN-generated materials and selecting the top 1,000 for analysis. We perform DFT calculations to calculate their formation energies to analyze their stability and likely synthesizability. Due to the lack of known true negative samples (non-synthesizable samples) for synthesizability prediction, true positive rate is used here to evaluate the performance of the synthesizability prediction models. We include the accuracy metric for our tests as we utilize our method for selecting high-quality negative samples in addition to true positive rate. This is to validate that there is not simply a positive bias that results in a high true positive rate and there is in fact an observable differentiation in the model predictions. This is not possible with the previous method. We show the results of our synthesizability prediction in Table 3. The results denoted with a * were evaluated using a random subset of positive materials as the test set only. This is the most direct comparison to the PU-learning model[ which uses this evaluation method. We ﬁrst tested our Balanced TSDNN model using this test set, and its true positive rate (TPR) performance was particularly high, with a TPR of 97.90%, compared to the PU learning model that had a TPR of 87.90%. However, this test did not have negative samples in the test set. As such, there was no way to ensure this high performance was not a result of a model bias. For this reason, we used the full test dataset (P Test + N Test) shown in Figure 1b. This allows us to evaluate both the TPR and accuracy metrics. The Balanced TSDNN was trained using the full labeled dataset and a small unlabeled dataset to compare to the strictly supervised CGCNN classiﬁer method. These two models have equivalent performance, with the Supervised CGCNN achieving an 81.60% TPR and the Balanced TSDNN achieving an 81.20% TPR. To improve on this and beneﬁt from semi-supervised learning, we then use the optimized dataset described in Section 2.3 for training the Unbalanced TSDNN model, which achieved the highest accuracy of 94.11% along with TPR of 92.90%. We also evaluate this model by moving the test data into the unlabeled dataset for the Seeded TSDNN test. We use this test to evaluate the pseudo-labeling ability of our teacher model and to show that the true labels of data in the unlabeled set are learned correctly. The Seeded TSDNN achieves a TPR of 92.90% and accuracy of 91.48%, which demonstrates accurate teacher pseudo-labelling for unlabeled data. It increased the TPR of the Unbalanced TSDNN from 92.90% to 93.80%. This is the best comparison to real-world performance, as the unlabeled data would be the desired data to be classiﬁed. Figure 4: Scatter plot of our TSDNN predicted scores vs PU learning on ICSD materials from the test set. This shows that the PU learning method falsely classiﬁed many more materials as negative (Quadrant IV) than our TSDNN model (Quadrant II) for the PU learning predictions. In both the basic PU learning method for synthesizability [ 0.5 is used for determining synthesizable vs. unsynthesizable materials for both classiﬁers. To show the consistency and performance of both models, Figure 4 plots the probabilities of being stable materials for all the ICSD materials from our test set by the PU learning model against those predicted by our TSDNN model. The ﬁgure is divided into quadrants, with each quandrant signifying agreement or disagreement between the PU learning method and our TSDNN framework. The top right quadrant signiﬁes correct agreement between the models, where both models correctly classify the materials as positive. The bottom left quadrant, similarly, denotes the incorrect agreement that the materials should be classiﬁed as negative. The bottom right quadrant signiﬁes a disagreement in which the TSDNN model correctly classiﬁes the materials and the PU learning method does not. It can be easily found that the bottom right quadrant contains much more samples compared to the top left quadrant, solidly indicating that there are many materials with very high prediction scores correctly predicted by our TSDNN model, but were incorrectly classiﬁed by the PU learning method as being unstable (high formation energy). There were comparatively few materials in the top left quadrant that are samples correctly classiﬁed by the PU learning method and incorrectly classiﬁed by the TSDNN model. These results show that while our model has improved true positive rate, the improvement is not simply a result of materials being classiﬁed right at the 0.5 boundary. Formation energy based materials screening can be done using either regression models or classiﬁcation models, depending on the motivation of the screening. For screening hypothetical materials, the ﬁrst step is identifying potentially stable candidates with negative formation energies. As the exact formation energy is not needed, this can be done effectively by an accurate formation energy classiﬁcation model. To evaluate the performance of models for formation energy classiﬁcation, we consider accuracy, precision, and F1 score, as each metric corresponds to a speciﬁc screening motivation. We notably do not use recall as for our problem here, simply achieving a high recall may not be meaningful on its own because it may include many false positives that are not stable. F1 score better represents performance in this regard, as it measures the performance with balanced recall and precision. In this situation, predicting few false-positives while still correctly classifying a majority of the actual positive materials is desired. For precision, in situations in which it is imperative that the screened materials be below a given eV threshold (e.g. ﬁnding materials with high-conﬁdence stability), a high-precision model is the most optimal choice regardless of its accuracy or F1 score. Precision and F1 score are useful metrics at any eV threshold. Accuracy, however, is only signiﬁcant with an eV threshold of 0.0 eV for our test set as we are seeking to classify between samples with negative or positive formation energies. With lower eV thresholds, the number of negative samples vastly outweighs the number of positive samples, as shown in Figure 3b. A model could have a high accuracy at a low eV threshold while correctly classifying few actual positive samples. Accuracy is most useful for identifying >0.0 eV/atom materials. A high-accuracy model with a threshold of 0.0 eV achieves the best balance between correctly identifying actual positive and negative samples. Figure 5: Comparison of the formation energy distributions of test samples predicted/classiﬁed to have negative formation energy by three different models versus the ground truth. (a) The distribution of the formation energies of all test samples. 35% of them are positive. (b) Distribution of Ef of positive samples predicted by the Separated TSDNN model. (c) Distribution of Ef of positive samples predicted by the Unseparated TSDNN model. (d) Ef distribution of positive samples predicted by the CGCNN regression model. Table 4: Comparison of classiﬁcation performance for formation energy with an eV threshold of 0.0. Table 4 shows the classiﬁcation performance of three models on our test set of materials. Our Unseperated TSDNN model achieves a 74.60% F1 score compared to the CGCNN regression model’s F1 score of 64.3%, with a signiﬁcant absolute 10.3% improvement by using our semi-supervised learning approach. At the same time, this model achieves an accuracy of 74% , with an absolute 9.7% improvement over the CGCNN model. Our Separated TSDNN model shows that our approach is able to be tweaked for achieving higher precision by adjusting the training threshold, resulting in a high-conﬁdence model. Here the table shows that the model can be tuned to achieve 100% precision for identifying candidates which are highly likely to be stable materials. To further illustrate the advantage of our TSDNN models, we show the formation energy distributions of the positively classiﬁed samples (with negative formation energies) from our test set by our classiﬁers and the baseline CGCNN regression model. As shown in Figure 5a, our test set contains a large number of samples with positive formation energy to fully test the model’s ability to differentiate between samples with positive and negative formation energy. The desired formation energy distribution of screened samples is seen in the bottom group of samples around -2.0 eV. Figure 5b shows that our Seperated TSDNN model has just obtained the desired sample groups with the formation energy distributed around the peak of -2.2eV, which indicates that our Separated FE TSDNN is effective for applications which require a high certainty that a material will have a low formation energy because of its very high precision. For more general screening with an eV threshold of 0.0, our Unseparated TSDNN model is more suitable (Figure 5c). With the vast array of materials with formation energies very close to 0.0 eV, it is very challenging to train a model to accurately differentiate between materials with small positive and small negative formation energy. As shown in Figure 5d, the CGCNN model is not able to capture the full distribution of negative formation energy materials in the test set and has difﬁculty in differentiating between samples with positive and negative formation energies. As shown in Table 4, our Unseparated TSDNN model is able to improve greatly in performance with a 7.5% increase in precision, a 10.3% increase in F1 Score, and a 9.7% increase in accuracy from the CGCNN. This makes it preferred for applications that wish to screen for stable materials (usually with negative formation energy). 3.3 New materials discovery using both formation energy and synthesizability screening models 3.3.1 Generation of candidate cubic structures for screening CubicGAN [ GAN reports that when generating 10 million virtual cubic crystal structures, most of materials in training datasets, Materials Project and ICSD can rediscovered. Thus, We use CubicGAN to generate 10 millions of virtual cubic crystal structures, of which around 90% materials can be recognized as the same space groups they are assigned to. The next step is to remove duplicate crystal structures. We consider materials with the same compositions and the same corresponding atom positions as duplicate materials. Around 25% materials ( 2.5 millions) are left for further analysis. Starting with 2.5 millions of candidate materials, we ﬁrst apply our Separated TSDNN model to classify them into positive or negative formation energies. 918686 of them are predicted as having a negative formation energy. We then select 5000 of these materials with the highest prediction scores and apply our Unbalanced TSDNN synthesizability model to predict their probability of being able to be synthesized. We ﬁnally select the top 1000 samples with the highest probability to be synthesizable. These samples are sent for DFT relaxation and further validation. The density functional theory (DFT) based ﬁrst principle calculations were performed using the Vienna ab initio simulation package (VASP) [ augmented wave (PAW) pseudopotentials where 520 eV plane-wave cutoff energy was set[ correlation functional was considered with the generalized gradient approximation (GGA) based on the Perdew-BurkeErnzerhof (PBE) method [ were optimized with the force convergence criterion of 10 computed using the were determined based on the expression in Eq. 7, where 46] is a generative adversarial network based model for generating novel cubic crystal structures. Cubicconsidered structure, formula, and n is the total number of atoms in a unit formula(n = Out of 1000 crystal structures, which were optimized using DFT, 512 of them have negative formation energies. Table 5 shows the 10 cubic structures found with lowest formation energies. Interestingly, all the 10 materials have rare-earth elements. Half of them have ADF structures, F is the common element, and the rest of the elements make bonds with F (see Fig.6). Table 5: The chemical formulas and the space group symmetries for the materials found with lowest formation energies. With the advent of large-scale material databases and generative machine learning models, an immense expanse of the wider inorganic material chemical design space is now possible with high throughput experiments or computation. This extensive amount of data makes it a prime target for developing machine learning models for both synthesizability and formation energy based screening. However, there is comparatively little labeled data in both cases and particularly few negative samples. Obtaining new labeled data can be both costly, time-consuming, and unreliable. Previously, CGCNN-based regression models have been used to screen for stable material candidates using predicted formation energy. The issue with such models to screen for materials candidates with low formation energies is the introduction of model and prediction biases due to the dataset imbalance. As shown in Figure 3a, only 8.2% of the total MP database is comprised of materials with formation energy greater than 0 eV. This results in ML based regression models that bias their predictions heavily toward negative formation energies with true positive samples, as shown in Figure 7. Here we proposed a dual crystal graph convolutional neural network-based semi-supervised learning framework for synthesizability and formation energy prediction. Comprehensive testing and validation show that our TSDNN models can successfully exploit the unlabeled data in each use case in conjunction with existing labeled data to accurately and effectively predict synthesizability and formation energies. Our TSDNN models can be paired with existing and future material generation models for efﬁcient screening across a variety of applications, as shown with our CubicGAN. Our models’ integration with generative models provides for a greatly optimized and more reliable search for new materials. Compared to the CGCNN based regression model, which misclassiﬁed a large grouping of materials as having positive formation energies due to the bias caused by the dataset imbalance, our semi-supervised TSDNN classiﬁcation model reduces this bias, as it is designed with screening in mind from start. Furthermore, by using our TSDNN framework in conjunction with our CubicGAN model, we were able to use the large amount of unscreened data as unlabeled data to train our model for improved performance. Figure 7: Scatter plot of CGCNN predicted formation energy. With few samples with positive formation energy, the CGCNN model tends to underestimate true positive formation energy materials and overestimate true negative formation energy materials. Furthermore, it seems that the CGCNN has greatly overestimated a portion of the materials substantially. Another major ﬁnding of our study is that the evaluation method plays an important role in for objective evaluation of model performance. In previous studies [ holdout validation, but with only the positive class. While reliable for other problem domains, this is inadequate for inorganic material classiﬁcation as it allows for possible too high compositional and structural similarity between the test and training data to artiﬁcially inﬂate classiﬁcation performance. This is due to the tinkering materials discovery process over history, which leads to that materials deposited in materials repositories such as Materials Project are grouped into clusters with high similarity, which can lead to over-estimation of materials property prediction models [60,29]. Without a negative class in the test set, there is no guarantee that the model simply predicts every sample as positive indescriminantly. For instance, when evaluated using a random holdout test set, our synthesizability prediction model was able to achieve a true positive rate of 97.90%. It is for this reason that we elected to introduce a negative class composed of the lowest classiﬁcation score materials, as shown in Figure 1b. Machine learning based materials property prediction faces the big challenge of lack of sufﬁcient annotated property data and the issue of missing negative samples (non-stable materials), which is needed for building screening models for new materials discovery. To address these two issues, we propose a teacher-student twin graph neural network model (TSDNN) for materials property prediction using formation energy and synthesizability as examples. We formulate both problems as a semi-supervised binary classiﬁcation problem which matches well to the real-world screening scenarios where these ML screening models are used to pick stable and synthesizable materials candidates from the big pool of hypothetical materials. Our extensive experiments show that our TSDNN models are able to signiﬁcantly improve the prediction performance compared to previous methods in both synthesizability and formation energy prediction. We achieve a 92.9% true positive rate for synthesizability prediction with a much simpler model architecture and 74% prediction accuracy for formation energy screening. As further validation, we applied our models to the 2,545,713 hypothetical materials generated by our CubicGAN model. Overall, we screened 918686 materials that were positively classiﬁed by the formation energy model with our synthesizability prediction model. We select the top 1000 of these ﬁnal screened materials for DFT veriﬁcation and ﬁnd that 51.2% have negative formation energies. These results show that our TSDNN semi-supervised learning framework is effective for large-scale material discovery screening. The data that support the ﬁndings of this study are openly available in Materials Project database at http: www.materialsproject.org. The source code and our pretrained models are freely available at our github repository https://github.com/usccolumbia/tsdnn Conceptualization, J.H.; methodology, D.G., J.H., E.S, Y.Z., N.F.; software, D.G.; validation, E.S., J.H.; investigation, J.H., D.G., E.S., Y.Z.; resources, J.H.; data curation, J.H., and Y.Z.; writing–original draft preparation, D.G., J.H., E.S. ; writing–review and editing, J.H, D.G., N.F.; visualization, D.G.; supervision, J.H.; funding acquisition, J.H. The authors declare there no conﬂict of interests. Research reported in this work was supported in part by NSF under grants 1940099 and 1905775. The views, perspective, and content do not necessarily represent the ofﬁcial views of NSF. This work was supported in part by the South Carolina Honors College Research Program. This work is partially supported by a grant from the University of South Carolina Magellan Scholar Program.