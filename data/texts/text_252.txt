<title>Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems</title> <title>arXiv:2109.11064v1  [cs.CL]  22 Sep 2021</title> Customer service dialog systems have become widely-used in many everyday settings, but still make frustrating errors that are sometimes obvious to a human — for example, failing to understand a customer’s message and asking an irrelevant question as a result. In practice, tuning these systems to limit these behaviors is an expensive and time-consuming art. This paper describes the design, implementation and early results of an approach to improving overall dialog system quality by recognizing and addressing such individual failures. This is done by combining individual Actionable Conversational Quality Indicators (ACQIs) with a running Interaction Quality score (IQ), to show which problems were identiﬁed, what steps were taken to ﬁx them, and how these changes affected an overall assessment of the user experience. IQ is a standard conversational quality measure developed by Schmitt and Ultes (2015). ACQIs are designed so that each conversational quality indicator has associated recommended actions, and are introduced in this paper. The paper starts by explaining some of the background on how task-oriented dialog systems are built and maintained (Section 2): while there are several online tools that support this, many readers may be unfamiliar with their use. Section 3 introduces the datasets that are used for examples and experiments throughout the rest of the paper. Previous works on evaluating dialog system performance (discussed in Section 4) have investigated the use of conversation-level quality metrics, and individual turn-level assessments of good and bad interactions. In particular, the Interaction Quality (IQ) score of Schmitt and Ultes (2015) is explained in this section, and the combined use of ACQIs and IQ score plays a major role throughout the rest of the paper. Section 5 explains the heart of this paper: the design of the ACQI taxonomy. This explains the motivation and decisions behind ACQIs, including how they are made to be actionable, explainable, and to give feedback that is speciﬁcally tailored to the dialog system in question. Section 6 describes the annotation work for ACQI datasets, analysis of the ACQI distributions, and the work on combining ACQI and IQ scoring to distinguish those parts of the dialog that need particular improvement. Section 7 describes experiments on automatically predicting ACQIs based on the annotated datasets and features extracted from the dialog text. We demonstrate that correct ACQI labels can be predicted with a weighted average f1-score of 79%, and demonstrate the effectiveness of textual features to predict labeled IQ scores (1 to 5) with an average accuracy of 60%. Analyzing the distribution of ACQIs and suggested actions when IQ drops shows that the number of potential improvement strategies to evaluate could be reduced by up to 81%, depending on the accuracy of the ACQI and IQ classiﬁers. We argue that tools built using these approaches could improve the effectiveness and reduce cognitive burden on bot-builders. Since the year 2000, dialog systems (often called chatbots) have gone from mainly research demonstration systems to include various user-facing commercial offerings. Dialogue systems fall into three broad categories (Deriu et al. 2020): conversational agents, question answering systems, and task-oriented systems (which are the topic of this paper). Each category has corresponding dialog quality measurement strategies. Conversational agents, which often receive the most attention in news articles when released, are typically unstructured and open-domain, with no particular objective other than an engaging conversation. Question answering systems can be evaluated by considering the accuracy of answers given. Task-oriented systems, which are the focus of this article, typically have a rigid structure and a limited scope. They are built to resolve the consumer’s issue, answer questions, route to an appropriate representative, or guide the user through a task as efﬁciently as possible. Like question answering systems, task-oriented systems have clear ‘failure’ cases: just as a question answering system can fail to answer a question, a task-oriented system can fail to complete a task. Task-oriented systems are prevalent in customer service: ideally, they can automate simple tasks like routing requests to the right agent, freeing up human customer service specialists to handle more demanding situations. Several technology companies offer dialog system services to support such chatbots. These include offerings from large and general technology companies such as Microsoft LUIS, IBM Watson, Google DialogFlow, and from more specialist providers such as Salesforce, Intercom, and LivePerson. While there are several differences between these platforms, there are some typical themes: • The platforms provide various tools and widgets for customers to build and deploy their own dialog systems. • The dialogs built in this way are ‘designed’ or ‘scripted’. The process of building a dialog involves declaring various steps, inputs the user can be encouraged to make at those steps, and what action the system should take in response. • This process admits the possibility of error or failure states, when then input from a user is something the system (knowingly or unknowingly) cannot process successfully. A simple example might be that the dialog is at a stage where the user is asked to pick from a list of available options, such as “Enter 1 for English, Ingrese 2 para Español”. In this case, any entry other than the numbers ‘1’ or ‘2’ would be a problem. This problem can be addressed in a few ways. A traditional keyboard interface might say “Option unrecognized, please type 1 or 2.” A form-ﬁlling approach might be to use a “Select from Menu” element instead of a “Prompted Text Entry” element, so that the customer can only give input that corresponds to the actions the dialog system can take right now. An example from the LivePerson Conversation Builder is shown in Figure 1. The user is about to add an new interaction to the conversation so far, has a choice of widgets including those for text entry, scheduling, payments, and various formats for asking questions and tracking the responses. There are analogous features in other bot-building platforms. It is crucial to note that dialog system platforms are typically used by customer support specialists, not the customers themselves. In the rest of this paper, these users will be described using the colloquial but industry-wide term ‘bot-builders’. This is emphasized because one of the ways to improve these dialog systems is to provide tools that make bot-builders more effective. The work described in this paper shows that two established approaches to measuring conversations, scoring individual actions and assessing the conversation quality as a whole, can be combined to provide such a tool that helps bot-builders to identify and ﬁx particular pain-points in a dialog system. The intuitively appealing assumption that useful actionable feedback about dialog systems can be produced through (semi-) automated methods is supported by Hockey et al. (2003), which shows that users of a task-oriented dialog system who received actionable feedback in failure cases outperformed the control group that did not. The datasets used in this paper are introduced here and referred to throughout this paper. The following two datasets were annotated and used in the prediction experiments in subsequent sections. Statistics about these datasets are summarized in Table 1. LEGOv2 represents a portion of the CMU Let’s Go Public DataSet (Raux et al. 2005 2006), which was instrumental in the development of the Interaction Quality (IQ) score of Schmitt and Ultes (2015), explained in Section 4. Let’s Go Public is a record of phone-mediated customer-service interaction between an automated dialogue system and callers drawn from the general population in the vicinity of Pittsburgh, Pennsylvania. It has been studied intensively since its initial creation, and subsets have been used, in particular, to support analysis and prediction of IQ (Stoyanchev et al. 2017). The LEGOv2 dataset still represents many of the challenges of deploying task-oriented spoken language systems ‘in the wild’: • The callers are drawn from the general population. • The task-at-hand is authentic: callers have a presumed need to access bus information. • The callers are using standard personal or public telephones in real-world settings that include such challenges as third party speech, television programs in the background, very variable audio quality, and irritated and or amused speech. Things that have changed since the creating of LEGOv2 include: • Speech recognition technology is far more capable now than it was when Let’s Go began. • The public is much more familiar with conversational AI systems. • Both speech-mediated and text-mediated dialogue systems are commercially important and widely deployed, so lessons learned from Let’s Go have greater potential impact. Previous approaches for modeling quality in LEGOv2 have included features automatically extracted from the speech recognition and dialog system (Stoyanchev et al. 2017). We have chosen to use only features derived from the text itself from a transcribed version of LEGOv2, as this approach is applicable to text-based dialog systems including those developed by LivePerson, and avoids using system implementation decisions as features from the dataset itself. An approved collection of transcripts of customer-service conversations with LivePerson dialog systems was also extracted and annotated. We have intentionally chosen a set of bots that serve different functions, come from different industries, and have different overall quality (as measured by ﬁnal IQ score). To this end, 130 conversations were chosen from each of 4 dialog systems, giving a total of 520 conversations, a comparable number of conversations to those used from the LEGOv2 dataset, though the LivePerson conversations themselves on average have fewer turns (Table 1). These dialog systems often respond with structured content, such as embedded HTML with buttons, toggles, or drop-down menus. In these cases we represent the text of the options separated by ‘***’. For example: “BUTTON OPTIONS *** Main Menu *** Pick a Color *** Pick a different item”. Meaningful evaluation of automated dialogue system performance has long been recognized as crucial to progress in dialogue system research Deriu et al. (2020) and is an active area of development, including the recent introduction of the ‘sensibleness and speciﬁcity’ metric of Adiwardana et al. (2020). Since the introduction of bot-building platforms as described in Section 2, it has also become a daily concern for customer service bot-builders. One of the most common industry measures for dialog system effectiveness is the automation rate or containment rate. Containment rate directly affects the cost savings that a business may make through automation, for example “Over three years and a conservative 25% containment rate, the cost savings is worth more than $13.0 million to the organization (Forrester Research 2020).” However, analysis of containment or automation rates is still relatively rare in the research literature, and is mainly found in evaluation of speech / voice driven dialog systems (Pieraccini et al. 2009). The tradeoff between the automation rate and the number of setbacks a user can encounter is analysed in Witt (2011) — also in speech. The relative lack of research literature on containment rates can be easily attributed to different settings and incentives. The case study presented in Forrester Research (2020) notes that “Over three years and a conservative 25% containment rate, the cost savings is worth more than $13.0 million to the organization.” By contrast, most dialog systems in research settings do not have human agents to respond to escalations, so the systems cannot escalate, and containment cannot be measured. For the task-oriented dialogue systems that are our primary concern, there is an underlying concept of giving a ‘right’ response. Early evaluation work based on this idea compared the actual responses to a predeﬁned key of reference answers (Hirschman et al. 1990). The portion of matches to the key gave the measure of performance. Well known weaknesses of this approach include being speciﬁc to particular systems, domains and dialogue strategies. More portable strategies that measure inappropriate utterance ratio, turn correction ratio, or implicit recovery (Danieli and Gerbino 1995; Hirschman and Pao 1993; Polifroni et al. 1992; Shriberg et al. 1992) are intellectual ancestors of the ACQI part of our approach, in that they identify events that are indicators of the quality of the conversation. Both of these early approaches share the limitation of being unable to model or compare the contribution that the various factors have on performance. The PARADISE approach (Walker et al. 1997) overcomes this limitation by using a decisiontheoretic framework to specify the relative contribution of various factors to a dialogue system’s overall performance. This and other ideas introduced by PARADISE, such as separating the accomplishment of a task from how the system does it, support evaluation that is portable across different systems, domains and dialogue strategies. We have tried to emulate some of these best practices by deﬁning ACQIs that avoid being too implementation-speciﬁc, though without insisting that all ACQIs must be relevant to all dialog systems (see Section 5). For example, in contrast with Stoyanchev et al. (2017), we have chosen not to utilize any features extracted from the dialog systems themselves (e.g. failed intent match from the natural language understanding system) and rely solely on the text of the dialog. More recent work has developed several dialog quality measurement strategies which are categorized by Bodigutla et al. (2019) as: sentiment detection; per turn dialog quality (e.g. Interaction Quality (Schmitt et al. 2012) and Response Quality (RQ)); explicitly soliciting feedback from the user; task success; and dialog-level satisfaction ratings as in PARADISE (Walker et al. 1997). These methods are useful but have well-known limitations: for example, sentiment analysis on messages misses many problems, there is response bias in user polling, and negative outcomes weigh more in the consumer’s mind than positive ones (Han and Anderson 2020). The method in this paper most directly builds on the IQ method of Schmitt et al. (2012); Stoyanchev et al. (2019). The methods involve annotating a conversation, typically adding one point for a good interaction and subtracting a point for a bad interaction, with some exceptional cases, for example when the dialog ‘obviously collapses’. A beneﬁt of these methods is that they can help to identify where there are problems in a dialog system: Our work can be seen as an extension of the IQ approach (Schmitt et al. 2011) in our use of a running dialogue quality measure. ACQI improves over IQ by recommending how to resolve a problem instead of only identifying where it exists. This central section describes the design of the ACQI taxonomy and how it improves over the IQ system introduced above. A running score like IQ allows bot-builders to identify dialog system responses where there is a meaningful decrease in score, but does not provide direct diagnosis of the problems that may be there. If nothing other than the running score is available, bot-builders have little option other than to manually review, form their own taxonomy of failure causes, and come up with appropriate ﬁxes. This process in practice typically takes days or weeks to complete, and is prioritized largely by operator intuition. To make the problems explicit and to suggest solutions, we introduce Actionable Conversation Quality Indicators or ACQIs. ACQIs highlight moments in chatbot conversations that impact customer experience. Our ACQI taxonomy can be found in Table 2. Note that for each ACQI, there is an associated set of an actions a bot-builder can take to mitigate the issue. The ACQIs and the taxonomy dimensions are all derived from analyzing the user experiences directly. As well as making them actionable, this is motivated by wanting to make the issues aggregable, so that bot-builders can analyze aggregated statistics about conversations and prioritize ﬁxing the most prevalent issues accordingly. By exposing predicted ACQIs in an appropriately aggregated format, we empower bot-builders to make more data-driven decisions when improving their dialog systems. Not all ACQIs are relevant to all dialog systems: for example, the CMU LEGOv2 system does not support transfer to a human agent, so ‘Bad Transfer’ cannot occur (though ‘Set Transfer Expectations’ can still potentially be useful, even to say that there are no human agents available). The ACQI taxonomy is inspired from a variety of sources, including literature on the (human) evaluation of open-domain dialog systems, consultation with experts, and ongoing feedback from expert users involved in bot-building. We carried out a series of interviews with bot-builders and assembled a repertoire of tuning actions based on their practices. We identiﬁed 28 distinct actions bot-builders take at LivePerson. In the case of LEGOv2, we consulted with 2 domain experts and identiﬁed 31 actions. For each of our ACQIs we assigned a set of actions that the bot-builder may make (Table 2). For each of our ACQIs we assigned a set of actions that the bot builder may take (Table 2). The mapping showed that our ACQIs could guide bot builders to take 23 of 28 possible actions for LivePerson systems, and 25 of 31 possible actions for CMU. The mapping from ACQIs to actions was created by a LivePerson expert in improving dialog systems, and later reﬁned with further feedback from LivePerson bot-builders. Because IQ+ACQI will be aggregated and used to improve consumer experience (CX) we put an explicit emphasis on CX and actionability. Finch and Choi (2020) list 21 dimensions across 20 publications that human evaluators have used to measure the quality of open-domain dialog systems. While these dimensions are not used directly, they partially inspire our taxonomy of failure states (Table 5, leftmost column). For instance the ‘Doesn’t Understand’ failure state is inspired by Coherence (Luo et al. 2018; Wu et al. 2019), Correctness (Liu et al. 2018; Wang et al. 2020), Relevance (Moghe et al. 2018; Lin et al. 2019; Qiu et al. 2019), Logic (Li and Sun 2018) and Sensibleness (Adiwardana et al. 2020). Other elements of our taxonomy are informed by Jain et al. (2018) which provides a set of best practices when developing dialog systems for messaging based bots. We separated ‘misunderstanding’ into ‘Input Rejected’, ‘Ignores Consumer’ and ‘Does Not Understand’ categories, because each of these require different actions that can mitigate the understanding issue. Such separation of failure states highlights the design-for-actionability of the ACQI taxonomy. A good ACQI taxonomy should be actionable, easy to understand, have highly bot-dependent ACQI incidence rates, and have a signiﬁcant impact on consumer experience. In the following sections we will justify these properties, and provide measurement strategies where appropriate. For some ACQIs and dialog systems it may be appropriate to bypass the need for annotation by automatically extracting ACQIs from the system logs. An example for this would be an ACQI that indicates that the NLU returned a conﬁdence score less than a predeﬁned threshold and responded with a request for the user to rephrase their intent. Without actionable ACQIs, bot-builders are left to the time consuming process of reviewing large amounts of transcripts and/or a careful analysis of the model’s feature space, from which they can attempt to deduce what mitigation strategies are appropriate. Unfortunately many features are not actionable from the bot-builder’s perspective. For instance, conversation length can be highly predictive of conversation quality, but from their perspective, there are no clear steps to uniformly reduce conversation length. The problem is that conversation length is not the cause of a bad conversation, it is a consequence of it. For example, for the dialog system ‘Food Expert’, the initial turn was predicted to have ignored a consumer’s initial intent. This can lead to longer conversations, as can an order with complicated modiﬁcations, but the associated dialog quality improvement strategies for the operator are quite different. ACQI allows for separation and sizing of these situations: the second is varied and complicated, whereas the ﬁrst is a single problem with a negative affect on IQ. From the bot-builder’s perspective this one is a relatively easy ﬁx: they just need to make sure that intent recognition is applied to the ﬁrst customer utterance. For bot-builders, understanding is a necessary condition for ﬁxing an issue. The terms used in the ACQI taxonomy are deliberately chosen to be familiar to bot-builders, and have been reﬁned with use to add clarity where requested. The relative success of this effort is reﬂected in the encouraging inter-annotator agreements reported in Table 4 below. However, this ﬁnding is potentially inﬂuenced partly by the use of experts in bot-building and conversation modeling as annotators. This has yet to be corroborated with less experienced bot-builders. As the purpose of the ACQI taxonomy is to guide bot-builders to appropriate ﬁxes it is imperative that the rates at which ACQIs are present are bot-dependent. From Figure 3 we can see this is indeed the case. As our ACQI taxonomy is meant to ﬁt a wide breadth of task-driven dialog systems, teams working on very speciﬁc dialog systems may wish to use a subset, or create their own smaller taxonomy. For instance, if the dialog system does not allow for transfers (this is the case for LEGOv2) ‘Bad Transfer’ should be excluded from the taxonomy. Each ACQI should have an actionable relevance to CX. Figure 4 illustrates this property for our ACQI taxonomy as each score change distribution differs from the overall distribution (topmost bar). Several elements of the taxonomy only indicate a poor customer experience in particular circumstances. For example, the ‘Ask for Conﬁrmation’ is more likely to indicate a negative CX when it occurs multiple times within a given dialog (Figure 5). This concludes the summary of the ACQI taxonomy itself. In practice we found that ACQIs were most reliably predicted and effectively used in combination with a running quality score. This work is described next. The section describes the work done on annotating ACQIs along with IQ, which turned out to be necessary for distinguishing those ACQIs that warrant action. This is due to the fact that dialog context matters. For instance, a system attempting to correct a misunderstanding of a malformed user statements is quite different than the system failing to understand an unambiguous answer to a direct question. Our initial approach to building a model for ﬁnding ACQIs and making associated recommendations to bot-builders was based on the assumption that particular ACQIs are bad for the user experience and should be avoided. This turned out not to be the case. As the conversations from the LivePerson datasets were annotated, we observed that ACQIs alone may be bad or good or neither. For example, ‘Ask for Conﬁrmation’ is good when the user input is genuinely vague, but is bad when the user input was clear and the system should have understood. For example, asking the user to conﬁrm that ‘next Wednesday’ refers to a particular calendar date is sometimes helpful (especially if today is Tuesday!). Asking if the user’s response was ‘3’ if the user just selected ‘3’ can by contrast be obvious and irritating. If a consumer uses ‘Restart’ one time, it can be seen as a good signal that the consumer requested to go back and the system responded appropriately, but if it is used more frequently it can be a signal that their request is not being properly handled. Based on ACQI designations alone, bot-builders cannot always be sure whether a corrective action is needed. Essentially, ACQIs are context dependent and combine (with context and themselves) nonlinearly. In this work we explore the context dependency via the relationship with IQ. We leave a more structured statistical modeling framework for future work. To mitigate the issue of ACQI instances not being universally good, bad, or neutral, it was decided to combine ACQIs with an overall quality score such as Response Quality (Bodigutla et al. 2020) or Interaction Quality (Schmitt and Ultes 2015), discussed in Section 4. For these purposes, IQ was chosen, partly because of the availability of the LEGOv2 dataset and comparable prior work. There are several differences between the guidelines given in Schmitt et al. (2011) and our own. Most importantly, the removal of all guidelines relating to how much the score can be increased/decreased. Due to their restrictive guidelines the change in IQ is almost entirely (99.7%) in increments of 0,1, or -1. From observations in negativity bias (Rozin and Royzman 2001) we know this does not reﬂect how consumers feel when encountering undesirable behaviours. The impact of the removal of these guidelines can be seen in ﬁgure 2. In spite of these changes our annotator agreement has slightly increased with ρ = .69, .72 for them and ρ = .76 for us. We also altered the ‘dissatisfaction scale’ used by Schmitt et al. (2011) (5-satisfactory, 4slightly unsatisfactory, 3-unsatisfactory, 2-very unsatisfactory, 1-extremely unsatisfactory ) to 5-good, 4-satisfactory, 3-bad, 2-very bad, 1-terrible. We added the ‘good‘ category as we want our IQ score to show that there is room for improvement even if the recommended improvement is not currently technically feasible. The guideline dictating that each conversation should start with a satisfactory rating was removed as our dialog systems the consumer is often the party that initiates the dialog (in LEGOv2 the system always opens the dialog with a greeting), so the dialog system can make a mistake with interpreting the very ﬁrst message in the conversation. Annotation of the 531 LEGOv2 conversations and 520 LivePerson conversations was carried out by three annotators employed by LivePerson who are experienced live customer service agents. Following Schmitt et al. (2011) we took the median score of our 3 annotators as ground truth. For 3 way ties for ACQI (6.4% of turns) we chose to use the most common label for that particular dialog system out of the labels the annotators have chosen. So as in Schmitt et al. (2011) and Stoyanchev et al. (2019) we use third parties rather than the users of the dialog system to judge the turn by turn quality, and like Stoyanchev et al. (2019) our annotators are expert rather than crowdsourced. The biggest differences between their annotation work and ours is that the guidelines for the running IQ score are simpliﬁed, and we include an additional annotation task for ACQI to recommend an appropriate ﬁx. The averages of the minimum and ﬁnal IQ scores are shows in Table 3, which shows that the CMU LEGOv2 has the lowest dips in IQ score during a conversation, but by the end of the conversations, the IQ for the various bots is quite similar. In spite of having similar overall IQ outcomes, the CMU LEGOv2 dialog system has fewer neutral neutral steps, and more positive and negative turns, whereas LivePerson dialog systems have more neutral turns. We measured inter-annotator agreement to measure the clarity of our ACQI taxonomy and IQ rules when applied to real dialog systems. The results have a Cohen’s Kappa (CK) of .68, which is substantial agreement according to Landis and Koch (1977). See Table 4 for more details. We are now in a position to analyze the correspondence between individual ACQIs and changes in the overall IQ score. By looking at the turn to turn change in IQ given the presence of a particular ACQI we gain a more nuanced understanding of how a dialog system is performing. In previous attempts to measure dialog quality a lack of understanding and an inability to resolve a consumers issue (task completion) are taken to be unequivocally bad. This is not always the case. From Figure 4 we see that with the exception of ‘Bad Transfer’ (which was not annotated as negative) all of our ACQIs can be positive, negative or neutral. ‘Does Not Understand’, ‘Ignored Consumer Statement’ and ‘Input Rejected’ are usually negative, but can be reasonable responses if the consumer is typing things that are out of scope or incomprehensible. However, they are negative when the consumers utterance should have been understood by the system. ‘Unable to Resolve’ is positive when the system correctly identiﬁes that the consumer’s request is out of scope. ‘Provides Assistance’ is positive if the assistance was requested and appropriate, but negative if it was not, or if the assistance has already been provided. Similarly, ‘Ask for Information’ can be positive or negative, depending on the relevance of the requested information. Analyzing the annotated datasets by ACQI and dialog system is also instructive (Table 5). For most of the dialog systems, ‘Does Not Understand’ is the largest category of ACQI associated to a decrease in score. The Junior Sales Assistant is an exception, and for this, 54% of the ACQIs are marked as ‘Provides Assistance’, indicating that this bot is making too many improper transfers or inappropriate suggestions to the customer. While having the score alone would be somewhat valuable in locating this issue, the ACQI gives additional guidance on what steps can be taken to mitigate the undesirable behavior. We can also analyze combinations of ACQIs and their affect on a conversation. For example, Figure 5 shows that asking for conﬁrmation once normally leads to an increase in IQ score, but after this the effectiveness decreases and asking for conﬁrmation more that 6 times can be actively harmful. It is important to realize that the ACQIs combine nonlinearly. That is, a single instance of an ACQI in a conversation may be healthy, but multiple copies of it may be a negative indicator. The extent to which the ACQIs have known meaning is the extent to which structural statistical models can be built, allowing bot creators the ability to test and reﬁne explicit hypotheses about how the conversational events actually aggregate to the dialog system user experience. The careful deﬁnition and annotation work described so far enables us to start quantifying such effects in ways that were not hitherto available. Analyzing such combinations of causes and effects in dialog systems will be extended in future work. While the annotation and analysis so far was already able to provide some useful insights, the larger goal of these efforts is to build systems that can automatically highlight crucial ACQIs in a dialog system so that they can be ﬁxed. There is prior work in predicting IQ from labelled conversations, using a variety of methods. The original work of Schmitt et al. (2011) used Support Vector Machines, and more recent work has used stateful neural network models including LSTMs (Rach et al. 2017) and biLSTMs (Bodigutla et al. 2020). In common with these approaches, we adopted a vector representation for features. However, we deliberately restricted our model to use only features extracted directly from the conversation text and the annotation, so that the method could be more universally applicable, and in particular, to be able to use the same annotation setup and featurization processes for data from the LEGOv2 and LivePerson dialog systems. For our ‘text’ features (Table 6), we use the pretrained contextual sentence embeddings of BERT (Reimers and Gurevych 2019). Our embedding dimension is 768 for each speaker’s response. When predicting ACQI and IQ we concatenate the embeddings for both consumer and dialog system for this turn and the previous. This results in a 4 × 768 = 3072 dimensional vector. When the previous utterance is unavailable, we use the zero vector of the appropriate dimension. This feature vector was used to represent a system where the model uses only surface textual features for the previous two turns per user. To compare with a system that also uses its observations and predictions of the rest of the conversation so far, we experimented with adding features derived from the ACQI labels. For the features ‘only-gold-acqi’ and ‘only-pred-acqi’ we use cumulative counts on the one-hot encoding for the presence of ACQIs. These feature sets were also combined as ‘gold-acqi+text’ and ‘pred-acqi+text’, for which we concatenated the cumulative counts to the contextual sentence embeddings with our cumulative ACQI counts. To get a good representation of many ACQIs across the training and test sets, we used the multilabel Scikit Learn python package and methodology of Szyma nski and Kajdanowicz (2017), which supports balanced multi-label train/test splits which we adapted for nested cross-validation (Krstajic et al. 2014). Utilizing this package we implemented nested cross-validation using 5 cross-validated folds for the inner and outer loop. For predicting ACQI we tested logistic regression, random forest, and xgboost. For IQ we tested the above and a linear regressor. We found that the best performing text-based model for predicting both ACQIs and IQ was logistic regression with C (inverse regularization strength) set to 0.01 and with the ‘balanced’ class weight setting in Scikit Learn. For the IQ prediction experiment, results for each of the dialog systems using the features above are presented in Table 6. The results found can also be compared with the annotator agreement ﬁndings of Table 4. Points to highlight include: • The text-only prediction, which is the easiest to implement, achieves linear weighted Cohen Kappa performance slightly lower but comparable to annotator agreement (average 0.49 vs 0.52). • Using only ACQI information (either gold or predicted) leads to a loss in recall in all but one dialog system, and on average reduces recall by around 10% using only gold labels and 20% using only predicted labels. • Though the use of gold labels along with text features leads to improved correlation with annotators (for example, an average 6% improvement in kappa score), these expectations do not transfer reliably to the more realistic case of using predicted ACQI labels as features. • The increase in average recall from using gold labels is small (only 2%), and the use of predicted labels causes average recall to drop by 7%. These experiments leave much room for optimization and improvement of various kinds, including trying different text featurizers, and the number of turns and relative weights of messages used in the vector encoding. The important ﬁndings are that we can predict the exact IQ score approximately 60% of the time, and that the use of vector embeddings derived directly from the message texts is the most reliable practical method tested for building features. For the IQ prediction experiment, results for each of the dialog systems using only text-derived features and logistic regression as a classiﬁer (as described above) are presented in Table 6. Points to note include: • The overall weighted average f1-score is 0.790. We are predicting the correct ACQI nearly 80% of the time overall. • The accuracy of the classiﬁcation depends signiﬁcantly on the support of the class: common ACQIs are predicted much more accurately that rare ones. • Because of this, the macro average performance (not weighted by support) is worse, with an f1-score of just 0.574. • The score can be cross-referenced against Figure 4 and Table 5 to guide improvement efforts. For example, ‘Does Not Understand’ occurs relatively frequently and with overwhelmingly negative impact on IQ score, but the F1-score for predicting this class is only 0.509. Improving ACQI classiﬁcation performance for this class would therefore be especially impactful. As a ﬁnal result, we estimate the extent to which predicting the correct ACQI could help botbuilders involved in bot-tuning. Referring back to the ACQI taxonomy in Table 2, without any extra contextual guidance, bot-builders have 28 possible action strategies with LivePerson dialog systems and 31 with LEGOv2 dialog systems. This was compared with the number of options that would be available using the predict ACQI labeling. For this simple simulation, we made the following assumptions: (1) Each appropriate action is equally likely in the absence of IQ/ACQI. (2) If IQ is available, tuning is only required when the score decrements. If IQ is unavailable then all actions related to the system are relevant. (3) Given the presence of a decremented IQ score, each action is equally likely. (4) If an ACQI is available, all actions that are not assigned to at least one ACQI are included in our list of options that the bot-builder can make. (5) There is always a special action, hNo Actioni, that may applicable for the bot-builder. The results of this exercise are presented in Table 8. We found that the largest single simpliﬁcation comes from the use of IQ — if IQ can be modelled accurately, then the average number of recommended options is reduced from 28.6 to 9.73 (about 34%). Adding ACQI classiﬁcations as well reduces the average down to 5.4 (about 19% of the original number). This makes a hypothetical but strong case: if IQ and ACQI can be accurately predicted on a turn-by-turn level, the amount of effort it takes a bot-builder to diagnose problems and suggest possible solutions could be reduced by an estimated 81%. While this is an optimistic hypothesis, the potential reward is large enough to encourage more development in this area. Actionable Conversation Quality Indicators (ACQIs) are designed to to provide bot-builders with actionable explanations of why their dialog systems fail. We have explored the key desirable properties for building an ACQI taxonomy, based on recommendations from the literature, interviews and collaboration with dialog system experts. Based on an annotated dataset of just over 1000 conversations, we have shown that ACQIs are particularly useful when combined with Interaction Quality (IQ), in particular so that the decision of whether to take a recommended action can be focused on places in the dialog where quality decreases. The annotated datasets were used to train predictive models, which achieved a weighted average f1-score of 79% using features based just on vectorized embeddings of recent messages in order and logistic regression for classiﬁcation. While these results are preliminary, such a classiﬁcation model could be used to reduce the number of options for a bot-builder to consider by as much as 81%. Results like this should inﬂuence UI design for bot-builders directly: if the ACQI-based suggestions show up as a tooltip (similar to refactoring tips in software integrated development environments), they may be useful in the majority of cases, while being easy to ignore in the remainder. The prioritization of the bot-builder as a key user persona is the driving principle for much of this work. We hope that more research focused on making bot-builders more effective is encouraged and highlighted in the dialog system community, as a crucial route to optimizing the experience of dialog system users overall. All authors are employed at LivePerson, Inc. The authors declare no other competing interest.