Abstract—Different from large-scale platforms such as Taobao and Amazon, developing CVR models in small-scale recommendation scenarios is more challenging due to the severe Data Distribution Fluctuation (DDF) issue. DDF prevents existing CVR models from being effective since 1) several months of data are needed to train CVR models sufﬁciently in small scenarios, leading to considerable distribution discrepancy between training and online serving; and 2) e-commerce promotions, which are being more prevalent and important to attract customers and boost sales, have much more signiﬁcant impacts on small scenarios, leading to distribution uncertainty of the upcoming time period. In this work, we propose a novel CVR method named MetaCVR from a perspective of meta learning to address the DDF issue. Firstly, a base CVR model which consists of a Feature Representation Network (FRN) and output layers is elaborately designed and trained sufﬁciently with samples across months. Then we treat time periods with different data distributions as different occasions and obtain positive and negative prototypes for each occasion using the corresponding samples and the pretrained FRN. Subsequently, a Distance Metric Network (DMN) is devised to calculate the distance metrics between each sample and all prototypes to facilitate mitigating the distribution uncertainty. At last, we develop an Ensemble Prediction Network (EPN) which incorporates the output of FRN and DMN to make the ﬁnal CVR prediction. In this stage, we freeze the FRN and train the DMN and EPN with samples from recent time period, therefore effectively easing the distribution discrepancy. To the best of our knowledge, this is the ﬁrst study of CVR prediction targeting the DDF issue in small-scale recommendation scenarios. Experimental results on real-world datasets validate the superiority of our MetaCVR and online A/B test also shows our model achieves impressive gains of 11.92% on PCVR and 8.64% on GMV. Index Terms—Recommender System, Conversion Rate Prediction, Meta Learning, Data Distribution Fluctuation As an essential part of recommender system, Conversion Rate (CVR) prediction has been widely used in modern ecommerce and attracted huge attention from both academia and industry. Generally, CVR modeling methods employ similar techniques developed for Click-Through Rate (CTR) prediction, which use high-order interactions of features to improve their representation capacity [8], [31], [39] and leverage sequential user behaviors to model users in a dynamic manner [36], [41], [42]. However, due to label collection and dataset size problems, CVR modeling becomes quite different and challenging. The major difﬁculties of CVR modeling are and Bo Cao introduced by the well-known Sample Selection Bias (SSB) [40] and Data Sparsity (DS) [13] issues. Several studies have been carried out to tackle these challenges, e.g., ESMM [16], ESM[35] and HM[34]. However, most of the related works assume that data distribution of CVR training samples collected from different time periods is identical. For large-scale platforms such as Taobao and Amazon, this assumption is guaranteed by collecting training samples within a short time window, e.g., a week or two, which is however far from enough in small scenarios to train well-performing deep CVR models. In small scenarios, usually several months of data are needed for CVR modeling. The large time span may result in discrepant data distributions between training and online serving, which hurts the generalization performance. Transfer learning [1], [2] can be applied to alleviate this problem by taking advantage of recent period of samples to ﬁne-tune a pre-trained network that is trained with months of samples. However, the beneﬁt of a pre-trained network decreases as the task on which the network was trained diverges from the target task [38]. Besides, in e-commerce, users’ shopping decisions can be inﬂuenced by different occasions [30], which refer to time periods with different data distributions and are related to particular time or events. Especially, due to the intensiﬁcation of e-commerce competition, promotions become quite frequent. For example, in our recommendation scenario, there exist two or more promotions every month with each promotion impacting about a week and resulting in various occasions. These promotions have remarkable impacts on the data distribution within a short time window. In such situations, CVR modeling is highly challenging since the distribution of the upcoming occasion is uncertain and training samples of each occasion become very scarce. As a result, the aforementioned transfer learning approaches [1], [2] are not effective in handling this challenge. As far as we know, there are no CVR methods focusing on tackling this problem. We refer to the problems mentioned above as Data Distribution Fluctuation (DDF) issue, which widely exists in small-scale recommendation scenarios while remaining underexplored in CVR prediction. After a detailed analysis of the logs, we observe that purchases on different occasions show different patterns. For example, people tend to purchase items of intrinsic preferences on normal days while they tend to engage with emerging hot items which are usually accompanied by attractive discounts during promotions. Moreover, user behaviors before, during and after promotions also vary a lot. Inspired by this, we consider that samples of each class on each occasion form a single prototype representation. Taking the distribution difference into consideration, we decompose the small scenario into four occasions, i.e., Before-Promotion (BP), During-Promotion (DP), After-Promotion (AP) and NotPromotion (NP). If all prototypes could be obtained and their distances to a sample to be predicted could be calculated, we can tackle the distribution uncertainty accordingly and thus address the DDF issue. In this paper, we propose a novel CVR method named MetaCVR from a perspective of metric-based meta learning [9], [23] to embody the above idea. The meta-learning perspective is appealing as it provides an effective way to perform transfer learning across occasions, enabling us to cope with the further scarce training samples on each occasion after decomposition. Concretely, we design a base CVR model and train it sufﬁciently with samples across months despite the non-identical distribution. Discarding the output layers, the base CVR model is used as Feature Representation Network (FRN) which generates positive and negative prototypes for each occasion with corresponding samples. Then we elaborately devise a Distance Metric Network (DMN) to compute the distances between each sample and all prototype representations so that a sample’s priori conversion tendencies on different occasions are provided. At last, we develop an Ensemble Prediction Network (EPN) to incorporate the output of FRN and DMN and predict the ﬁnal CVR score. When training the DMN and EPN, the FRN is frozen and samples from recent time period are adopted. Our main contributions are summarized as follows: CVR prediction that handles the DDF issue in smallscale recommendation scenarios. We introduce the idea of decomposing the small scenario into different occasions according to their distribution difference. learning to tackle the challenges introduced by the DDF issue, which models each occasion with prototype representations. Taking advantage of meta learning, our model achieves efﬁcient knowledge sharing across occasions. demonstrate the superiority of our model over representative methods. We also perform extensive analyses to conﬁrm the effectiveness of our design. CVR prediction is a key component of many online applications, such as search engines [18], recommender systems [33] and online advertising [15], [37]. Two critical issues make the CVR task quite challenging, i.e., the SSB and DS issues. SSB refers to the issue that the sample space of CVR prediction during training and inference can be systematically different, and DS refers to the fact that the amount of training samples for CVR prediction is much less than that for CTR prediction. For the SSB problem, Pan et al. [19] propose All Missing As Negative (AMAN) and treat unclicked samples as negative examples, which results in underestimated predictions. For the DS issue, oversampling samples of the rare class [32] is a widely used method. However, it is sensitive to sampling rates and not easy to achieve optimal results consistently. To relieve these problems, ESMM [16] models the “impression→click→purchase” path for the CVR task and trains the CVR model over the entire space. In this way, the SSB and DS issues can be mitigated and better performance can be achieved. Following the idea of ESMM and taking a step further, ESM[35] and HM[34] elaborately design more complete post-click behavior decomposition and make use of purchase-related behaviors, leveraging the abundant supervisory signals to achieve better performance. In this paper, we focus on the DDF issue in CVR prediction, which has not been well-explored in existing methods. Actually, it should be treated as the primary problem to be solved in CVR modeling for small-scale scenarios, where most of the existing CVR methods can not perform well. Meta learning, or learning to learn, has been studied for a long time [21], [22], [25], and attracted increasing attention recently due to its potential in developing human-level artiﬁcial intelligence. Most existing meta learning approaches belong to either optimization-based or metric-based category. For optimization-based works, MAML [7], Meta-SGD [14] and TAML [10] aim to learn good model initialization so that new tasks can be learned well with a small amount of training samples, while another line of works focuses on learning an optimizer, including the LSTM-based metalearner [20] and the weight-update mechanism with an external memory [17]. Metric-based meta learning is closely related to metric learning, which is usually initialized with a pretrained model that projects inputs into a representation suitable for computing distance between query and support instances. Based on the representations and distance metrics, the whole model or a part of it can be trained further. For example, Siamese Network [11] compares new samples with existing ones in a learned metric space. Matching Network [28] and Prototypical Network [23] obtain the prediction of samples in query set by comparing the distance between the query set and the support set. Relation Network [24] shares the similar idea, but it replaces distance with a learnable relation module. Although meta learning has also been explored in recommender systems, e.g., for algorithm selection [4], [5] and addressing the cold-start problem [6], [12], [26], [29], no attempt has been made to deal with the DDF issue in CVR prediction. In this study, we follow the idea of metric-based meta learning [23], [26] to ﬁll the gap. In CVR prediction, the model takes input as (x, y) ∼ (X, Y ), where x is the features and y ∈ {0, 1} is the conversion label. Speciﬁcally, the features for CVR prediction mainly consist of ﬁve parts. The ﬁrst part is the user behavior sequence, which records user history of clicked/purchased items. The second part consists of the user features, including user proﬁle (e.g., age and gender) and statistic features from user history. The third part consists of the item features, e.g., item id, category, brand and related statistic features. The fourth part is the interaction features of the target item and user, e.g., clicks/purchases of the user in the category/shop during last 24 hours. The ﬁfth part consists of context features, such as position, device, time information and occasion signals, in which occasion signals are sensitive to promotions. The goal of CVR prediction is to learn a model fparameterized by θ that minimizes the empirical risk: where L is the loss function. In small-scale recommendation scenarios, the training dataset D usually consists of samples collected from months of data in order to train CVR models sufﬁciently. Besides, an extra dataset Dis also constructed by collecting samples from recent time period. As illustrated in the introduction, we assume samples of each class on each occasion form a single prototype representation, i.e., purchases driven by different occasions cluster around different patterns. Given a user-item pair, it is important to exploit the linkage between the representation of input features x and occasion-driven purchase patterns to predict whether the conversion will occur. Since different patterns coexist on each occasion with different impacts, simply distinguishing the source occasion of samples by occasion signals in input features would not deliver good performance. Instead, we formulate the CVR prediction in small scenarios as: where F(x) denotes feature representation of x and d(·) denotes the distance metric function. With cls ∈ {+, −} and occ ∈ {BP, DP, AP, NP }, pdenotes the pattern of class cls on occasion occ, and g(·) is the ﬁnal prediction function. Symbol + denotes the positive class while − denotes the negative. In this paper, we refer to the patterns as prototypes and implement F(·), d(·) and g(·) as FRN, DMN and EPN respectively, which are shown in Figure 1 and will be detailed in the following sections. Since the performance of our proposed method greatly relies on F(x), it is of great importance to design a base CVR model, which can generate a high-quality feature representation and predict an accurate CVR score accordingly. With output layers discarded, the base CVR model can serve as the FRN which projects input features into an implicit representation space. The details are presented as follows. Firstly, a shared embedding layer is adopted to handle the input features x as mentioned in Section III-A. They can be further grouped into two kinds of features: categorical feature and numerical feature. We discrete the numerical features based on their boundary values, transforming them into the categorical type. Then each categorical feature is encoded as a one-hot vector. Due to the sparseness nature of one-hot encoding, we apply linear fully connected layers to obtain low dimensional embedding. After embedding, e, e, eand edenote user features, item features, user-item interaction features and context features respectively. Then the embedding of user behavior sequence is formed with item embeddings in the sequence, i.e., e= {e, ..., e} where edenotes the item embedding of tuser behavior and t is the sequence length. Subsequently, MainNet is devised to model the target item and user. Three kinds of attention [27] weights are calculated based on the embedding of user behavior sequence since it contains rich information about user interest. First, we use a multi-head self-attention network to model user preference from multiple views of interest. For self-attention, Query, Key, and Value all refer to eand ˆe= {ˆe, ..., ˆe} is the output. On top of the self-attention network, user attention and target attention are performed in parallel. With eas Query and ˆeas Key and Value, user attention sis calculated to mine personalized information and suppress noisy behavior. Similarly, with eas Query and ˆeas Key and Value, target attention sis applied to activate historical interests related to the target item. At last, e, e, e, sand sare concatenated and fed into a Multi-Layer Perception (MLP), generating h as the output of MainNet. It can be observed that different users in different context usually behave differently even to similar items. Therefore, we concatenate ewith eand feed them into another MLP (BiasNet) to model the bias. h denotes the output of the BiasNet. Finally, we obtain the output of FRN and feed it to output layers to predict base CVR scores. We adopt the widely-used logloss as loss function to train the base CVR model, i.e., ˆy= f (F(x)) = fNorm([h; h]), l= −1|D|(y logˆy+ (1 − y) log(1 − ˆy)) ,(3) where [; ] refers to concatenation of vectors, and f (·) is a ranking function implemented as a 3-layer MLP of which the last layer uses Sigmoid as activation function while the other layers use ReLU. Considering the differences of user behaviors on different occasions, we build the support set for each occasion via picking a day of this occasion in Dand splitting its samples into 2 subsets, i.e., positive support set and negative support set. The positive support set includes all purchase samples of the day while the negative includes the rest of clicked samples. Then we take advantage of the pre-trained FRN to map the input into a representation space and calculate the class’s prototype as the mean feature of its support set, i.e., where Sdenotes the support set of class cls on occasion occ, and xdenotes input features of kth sample in S. In this way, we obtain 4 pairs of prototypes, i.e., {p, p}, {p, p}, {p, p}, {p, p}. In metric-based meta learning, the choice of distance metric is crucial. In this paper, since the representation space of FRN is highly non-linear, it may not be suitable to choose ﬁxed linear distance metrics such as cosine distance and Euclidean distance adopted in [23], [28]. We consider that a learnable distance metric can be a more generalizable solution and propose a trainable Space Projection Distance Metric (SPDM) which is formulated as follows: where Wis a trainable projection matrix and bis a trainable bias scalar. It is worth mentioning that cosine distance is a special case of SPDM when Wis an identity matrix. Another style of trainable distance metric is similar to Relation Network [24], which aims to learn the relation between query sample and support sets as a transferrable deep metric. In this paper, we also borrow this idea and propose a Neural Network based Distance Metric (NNDM), i.e., dF(x), p= WF(x); p+ b,(6) {d, d}, {d, d}, {d, d}, {d, d}. In most of the existing metric-based works, classiﬁcation of a query sample is then performed by simply ﬁnding its nearest prototype, which is not directly applicable for CVR prediction since prototypes do not maintain ﬁne-grained personalized information after mean pooling, which however is essential for a well-performing CVR model. Alternatively, we incorporate these distance metrics with the output of FRN in an ensemble approach, i.e., ˆy = f(s, {s}) , s= f (F(x)), s= d− d,(7) where sdenotes CVR prediction of the base model and f (·) refers to the ranking function in Eq. (3). srepresents how likely the purchase would happen on occasion occ. The ﬁnal CVR score ˆy is predicted by a fully connected layer, i.e., f(·), with s, s, s, sand sas input and Sigmoid as activation function. Similarly to Eq. (3), logloss is adopted to train the whole model, which is deﬁned as: Note that in this stage, we only train the parameters of DMN and EPN on Dby stopping the gradient propagation to FRN. Algorithm 1 shows the detailed two-stage training procedure. In this section, we conduct a series of experiments to investigate the following research questions: RQ1 How does our proposed MetaCVR method perform compared to the state-of-the-art (SOTA) models for CVR prediction in the small-scale recommendation scenario? RQ2 How do different distance metrics affect the performance of MetaCVR? Algorithm 1 Training Procedure of the proposed MetaCVR Input: training datasets: D and D; Output: optimal θ (parameters in Equation 1); tion 4 RQ3 Why is our MetaCVR method effective in small-scale recommendation scenarios? A. Experimental Setup 1) Datasets: We establish the whole dataset by collecting the users’ interaction logsfrom a small-scale recommendation scenario of our online e-commerce platform, where promotions are highly frequent and have a considerable impact on the recommender system. Logs are sampled from 2021/01/15 to 2021/03/31, including 3 monthly promotions (21st of each month) and promotions for Spring Festival, Valentine’s Day and Women’s Day. We split the entire dataset into nonoverlapped training set D (2021/01/15-2021/03/15) and validation set Daccording to the timestamp of the prediction behavior, effectively avoiding feature leakage. In this way, the training set is about 80% of the whole dataset and the left 20% of data is used as the validation set. From the training set, we further build Dwith data from 2021/03/01 to 2021/03/15. The detailed statistics of these datasets are summarized in Table I. All the data we use have been anonymously processed by the log system and user’s information is protected. 2) Evaluation Metrics: Area under ROC curve (AUC) is used as the ofﬂine evaluation metric. For online A/B testing, we choose PCVR= p(conversion|click, impression) and GMV (Gross Merchandise Volume), which are widely adopted in industrial recommender systems for evaluating online performance. Improving PCVR and GMV simultaneously implies more accurate recommendations and business revenue growth. 3) Comparison Methods: The representative comparison methods are described as follows. 1) XGBoost [3] is a treebased model which can produce competitive, robust and interpretable results for CVR prediction and is especially suitable when training samples are not sufﬁcient for training deep models. 2) DCN [31] applies feature crossing at each layer. The advanced ability to capture high-order feature interactions makes DCN a better choice than DNN for CVR prediction. 3) BASE refers to the base CVR model proposed in Section III-C, which models sequential user behaviors via the attention mechanism. 4) BASE-F is the same model as BASE except that it adopts a two-stage training process to relieve the distribution discrepancy caused by the large time span. 5) ESMM [16] mitigates the SSB and DS issues by modeling CVR on user sequential path “impression→click→purchase”. 6) ESM [35] extends ESMM and models purchase-related post-click behaviors in a uniﬁed multi-task learning framework. Features for all the above methods are the same except that XGBoost and DCN discard user behavior sequence. Besides, since XGBoost requires less training data than deep models, we train two XGBoost models in this work, one with D and the other with samples of the last 20 days in D. The latter is used as a comparison model since it outperforms the former. For a fair comparison, the BASE structure is adopted for each task in ESMM and ESM. HMis not compared since micro behaviors are not available in our scenario. 4) Implementation Details: For the XGBoost model, the number and depth of trees, minimum instance numbers for splitting a node, sampling rate of the training set and sampling rate of features for each iteration are set to 70, 5, 10, 0.6 and 0.6, respectively. All the other deep models are implemented in distributed Tensorﬂow 1.4. During training, we use 2 parameter severs and 3 Nvidia Tesla V100 16GB GPUs. Item ID, category ID and brand ID have an embedding size of 32 while 8 for the other categorical features. We use 8-head attention structures with a hidden size of 128. Adagrad optimizer with a learning rate of 0.01 and a mini-batch size of 256 are used for training. We report the results of each method under its empirically optimal hyper-parameters settings. B. Experimental Results: RQ1 The ofﬂine and online comparison results are presented in Table II. For ofﬂine evaluation, all experiments are repeated 3 times on D. For online A/B testing, XGBoost was used as the baseline and the other models were tested in turn since the online trafﬁc was not enough for testing all models simultaneously. Time for A/B testing of each model covered all occasions mentioned in this paper. The major observations are summarized as follows: which ranks third and outperforms all the other deep models except for ESMmodel and our MetaCVR. It requires fewer training samples and thus alleviates the distribution discrepancy caused by the large time span. DCN model, validating the effectiveness of our sequential modeling method. Moreover, the BASE-F further improves the performance by ﬁne-tuning the BASE model using recent samples, implying impacts of the distribution discrepancy do matter. However, it still lags behind XGBoost. A possible reason is that increasing model parameters imposes difﬁculties in training in small scenarios. user behavior decomposition. With auxiliary tasks, the SSB and DS issues are mitigated and models are trained more sufﬁciently. ESMM is comparable to XGBoost while ESMoutperforms XGBoost by adopting more purchase-related signals, becoming the runner-up method. ofﬂine and online settings by addressing the DDF issue effectively via meta learning, outperforming the runnerup methods by a large margin. C. Impact of distance metrics: RQ2 Then, we explore the impacts of distance metrics in the DMN module. Concretely, we train four MetaCVR models using cosine distance, Euclidean distance, NNDM and SPDM in the DMN respectively. The results on Dare presented in Table III. We can ﬁnd that performance achieved by different distance metric methods varies signiﬁcantly, which conﬁrms that the choice of distance metric is crucial. SPDM and NNDM outperform cosine and Euclidean obviously, validating our idea that learnable distance metrics are better solutions especially when the characteristics of representation space are unknown. Besides, our proposed SPDM outperforms NNDM and achieves the best performance. It is because the inductive bias of SPDM is simpler than NNDM when measuring distance metrics, which is beneﬁcial in the context of limited data. D. Effectiveness analysis: RQ3 As summarized in Section IV-B, BASE-F performs better than BASE, and more impressive improvements are achieved when MetaCVR takes advantage of prototypes. Intuitively, BASE-F tackles the distribution discrepancy caused by the large time span, while MetaCVR partially employs this idea and further tackles the distribution uncertainty introduced by promotions, which we consider as the main reason for performance improvements. Taking a step further, we investigate the effectiveness of prototypes. First, days of different occasions are picked from both training set and validation set. Then prototype representations of each chosen day are obtained according to Section III-D and we compute cosine similarity of positive and negative prototypes respectively. As shown in Figure 2, prototypes of the same occasion have high similarities while prototypes of different occasions share low similarities. For example, the negative prototypes in Figure 2(b) of an occasion (03/02-03/04) have high similarities with that of the same occasion from another promotion (03/18-03/20) while prototypes of different occasions from the same promotion (03/18-03/22) share low similarities. The visualization empirically conﬁrms our assumption that prototypes of different occasions can distinguish from each other. With the learnable DMN, prototypes can be used to provide a sample’s priori conversion tendencies on different occasions and thus help to handle the distribution uncertainty. In this paper, we investigate the severe DDF issue in smallscale recommendation scenarios and propose a novel CVR prediction method named MetaCVR. It leverages the idea of metric-based meta learning to cope with the distribution uncertainty caused by frequent promotions and delivers promising transfer learning performance across occasions. Experiments on both ofﬂine datasets and online A/B test show MetaCVR signiﬁcantly outperforms representative models. In the future, we intend to combine our method with the idea of ESM for further improvements, which is motivated by experimental results in Table II.