With the growing emphasis on massive datasets in many modern applications, the need for sophisticated and precise approaches to high dimensional and heterogeneous data analysis is increasing. As an example, in healthcare research and personalized medicine, many Electronic Medical Records (EMR) include data on millions of patients, harboring large numbers of variables (e.g., demographics, diagnostic/procedure codes, lab/imaging results). These massive biomedical datasets, among others, provide opportunities to advance clinical and biomedical research, including clinical phenotyping (i.e., learning clinical trait-related features), but such analyses require shifting from human-guided solutions toward machine-learning (ML)-driven approaches. ML can increase clinical prediction accuracy and contribute to clinical phenotyping. However, many of these datasets are incomplete and include signiﬁcant components of missing data. ML algorithms cannot function without complete data matrices. Removing or imputing missing data can reduce sample sizes or bias outcomes. A critical foundational element for studying large datasets includes properly addressing the problem of missing and incorrect data (Little and Rubin (2014)). Many ad hoc techniques have been developed to deal with this problem, including sample deletion, mean value or nearest neighbor imputation, etc.; in general these suﬀer from information loss that leads to inaccurate predictions. Missing data form an important problem in medical record datasets. In particular, the HCUP Report #2015-01 by Houchens (2015), stresses the need to address missing data in the National Inpatient Sample (NIS) and State Inpatient Databases (SID). As an example, suppose that there are missing data for discharges (total charge) in rural hospitals. Such missing data can lead to erroneous estimates of total charges, potentially biasing or otherwise misdirecting state/federal funding policies. It is important to obtain accurate and unbiased estimates of missing data. Current state-of-the-art imputation algorithms recommended by the HCUP report #2015-01 include Predicted Mean Matching (PMM), Predicted Posterior Distribution (PPD) and linear regression (White, Royston and Wood (2011); Schafer (1999)). These algorithms often are sub-optimal, in particular for noisy signals. Furthermore, Bayesian methods such as Data Augmentation (DA) see Schafer (1997)) and Bootstrapping Expectation Maximization (BEM) algorithms (see Honaker, King and Blackwell (2011)) suﬀer from poor accuracy. Moreover, our numerical results are signiﬁcantly more accurate, and we show that population statistics of imputed validation sets are faithfully reproduced with the Kriging/BLUP approach. We note that a recent approach by Bertsimas, Pawlowski and Zhuo (2018) improves on the accuracy of traditional methods such as PMM, PPD, BEM, etc, by using a so-called optimization layer. It is unclear how accurately the population statistics of the validation set are reproduced, but the numerical results are promising. This motivates the application of stochastic optimization approaches, including Kriging (Nielsen, Lophaven and Søndergaard (2002)), which produces the Best Linear Unbiased Predictor (BLUP). The technique is based on a principled optimal probabilistic representation of the data. These methods can lead to optimized imputation by taking advantage of essentially all available data. However, Kriging methods in their general application are often costly and unstable numerically on massive datasets. This has been a limiting factor for application of Kriging to imputation for massive datasets outside of the spatio-temporal domain. One of the goals of this paper is to motivate the application of Computational Applied Mathematical (CAM) techniques to solve large scale stochastic optimization problems. In particular, to address the above challenges, we propose to apply the recently developed multi-level Kriging approach that is designed to tackle computing cost eﬀectiveness and numerical instability (Castrill´onCand´as, Li and Eijkhout (2013); Castrill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021)). These techniques originate from the ﬁelds of numerical analysis and uncertainty quantiﬁcation (Castrill´on-Cand´as, Nobile and Tempone (2016); Castrill´on-Cand´as (2021); Nobile and Tempone (2009); Babuska, Nobile and Tempone (2010)) and have been eﬀective in solving (stochastic) partial diﬀerential equations. Indeed, the present Kriging optimization problem has many connections to the solution of Partial Diﬀerential Equations (PDEs). We introduce the above techniques in the context of statistical methods including Kriging, and demonstrate their power to solve hard stochastic optimization large scale problems. By remapping an original stochastic optimization problem onto a multi-level space, we can signiﬁcantly mitigate numerical instabilities and reduce computational burdens. In particular, the BLUP is remapped onto an equivalent formulation with multi-level spaces. Mathematically the multi-level prediction is exact, i.e., it precisely solves the original BLUP problem. In practice, numerical eﬃciency augmentations involving factors of the order of tens of thousands can be gained for 20 dimensional problems, as compared with traditional Conjugate Gradient (CG) approaches for estimates with the same accuracy, as is shown in Section (4). To demonstrate the accuracy of the multi-level Kriging imputation method, we benchmark it on the U.S. National Inpatient Sample (NIS) datasets (see HCUP (2012), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare Research and Quality). Signiﬁcant improvements over state of the art methods including PPM, PPD, DA and EM are shown. More importantly, it is shown that the imputed values accurately reﬂect the overall statistics of the population. This contrasts with other approaches, including kNN-R, kNN, GLS, PPM, etc, which often also suﬀer from poor accuracy. Suppose that for a Gaussian random ﬁeld Y we have the model: where d is the number of spatial dimensions, k : R of the spatial location x ∈ R and ε is a stationary Gaussian random ﬁeld with mean zero and parametric covariance function φ(x, y; θ) ≡ cov(ε(x), ε(y)) : R is an unknown vector of positive parameters. We assume φ(x, y; θ) is positive deﬁnite. Suppose that we collect N ≥ p observations of the Gaussian random ﬁeld process Y at diﬀerent locations in R (Y (x where the elements in S are assumed non-collinear. Let C(θ) = cov(Y, Y θ ∈ R rank. Since the model (1) is a Gaussian random ﬁeld, the samples in S can be written in vector form as where ε is a Normal random vector, more precisely ε ∼ N(0, C(θ)). The aim is to estimate the unknown vectors β and θ and predict Y (x location x θ are estimated from the data using a log-likelihood function (see Castrill´onCand´as, Genton and Yokota (2016)) `(β, θ) = − which can be proﬁled by Generalized Least Squares (GLS) with For the prediction problem, consider the Best Linear Unbiased Predictor (BLUP) implies λ imization of E[{Y (x (2002)) where c(θ) = cov{Y, Y (x Solving the Kriging estimation and prediction problem involves inverting the covariance matrix C(θ). Two main approaches exist. Direct methods, such as Gaussian elimination and Cholesky factorizations are popular for small datasets, but as the number of observations N increases the memory constraints grow as ), . . . , Y (x))is obtained from locations in the set S := {x, . . . , x}, be the covariance matrix of Y and assume it is positive deﬁnite for all . Let X =k(x) . . . k(x)∈ Rand assume that it is full column with a stochastic optimization method. The unknown vectors β and ˆY (x) = λ+ λY, where λ = (λ, . . . , λ). The unbiased constraint = 0 and Xλ = k(x). Under the constraint Xλ = k(x), the min- O(N the Conjugate Gradient method (CG), avoid computing the covariance matrix, thus making them a good choice for large datasets. The key problem with direct and iterative methods is that they are sensitive to the condition number of the covariance matrix. Large condition numbers leads to numerical instability with the consequence of inaccurate solutions. It can be shown that the accuracy for any inversion numerical algorithm is ≈ κ(C(θ)) is the relative machine precision. For most computers, double precision  large condition numbers (see Golub and Van Loan (1996)). For many practical covariance functions, the condition numbers of C(θ) are large. Challenge: The number of observations is large and the covariance matrix C(θ) in many practical cases is ill-conditioned, leading to slow and inaccurate estimates of the Kriging / BLUP predictor. In the paper written by Castrill´onCand´as, Genton and Yokota (2016), the authors propose a new transformation of the data vector Y, leading to a decoupled multi-level description of the Kriging model without any loss of structure for R the introduction, missing data in the NIS and SID datasets form an important issue that is underscored in the HCUP report #2015-01 (Houchens (2015)). The report shows as an example, the problem of missing data for discharges (total charge) in rural hospitals. This can potentially lead to erroneous statistics of the total charge, leading to sub-optimal or misinformed state and federal policy decisions. Missing data rates for total charge are at 2.08% for the NIS 2012 dataset. However, the Michigan SID dataset missing data rate for total charge is signiﬁcantly higher at, 19.79%, underscoring the episodic problems of much higher impact for missing data in certain communities. The total number of samples is 7,296,968. In this section we contrast the accuracy performance of the multilevel Kriging/BLUP method with the recommended imputation algorithms in HCUP report #2015-01, which include Predicted Mean Matching (PMM). We make further comparison with more traditional methods such as K-Nearest Neighbors (KNN) and KNN regression. In particular, we test the accuracy of the various methods on the above-mentioned total charge variable, containing the highest missing data rate. The diﬀerent methods where tested on the 2013 NIS dataset, which was available to us for analysis. Our calculations show the overall missing data rate is 2% for total charge. The multi-level representation leads to significant computational beneﬁts when computing β and the prediction equation (5). There is a wealth of publications that attempt to address computational costs of the above approaches. Most of these approaches are challenged by stringent a priori assumptions on the statistical properties of data (Sun, Li and Genton (2012); Sun and Stein (2015); Stein, Chen and Anitescu (2013, 2012); Stein, Chi and Welty (2004); Furrer and Genton (2011); Furrer, Genton and Nychka (2006); Anitescu, Chen and Wang (2012)). Recently, from the computational mathematics community, a promising hi- ), which makes it infeasible for large datasets. Iterative methods, such as (see Jabbari), where κ(C(θ)) is the condition number of C(θ) and . Furthermore, iterative methods such as CG are slow for matrices with erarchical matrix approach has been developed by Litvinenko et al. (2019) to accurately compress covariance matrices, leading to signiﬁcant speed-ups. However, the application is restricted to zero-mean data without a component trend. Another promising approach is based on the pivoted Cholesky decomposition developed in Liu and Matthies (2019). Nonetheless, if the condition number of the covariance matrix is large, as happens in practice often, these numerical methods will have diﬃculty solving the Kriging problem with any accuracy. A common ad hoc technique to improve the condition number involves the use of a so-called nugget; however, it is known that this leads to numerical inaccuracies. Ill-conditioned matrices cannot be inverted with accuracy (see Jabbari). Furthermore, a covariance matrix can in general be large and unable to reside in computer memory. Thus the solution of the GLS would require inverting the covariance matrix p times using an iterative method, with p the number of columns of the design matrix. The multi-level approach developed in Castrill´on-Cand´as, Li and Eijkhout (2013); Castrill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021), avoids forming the covariance matrix C, by transforming the problem into a multi-level form with signiﬁcantly smaller condition numbers. In particular, for the BLUP problem the transformation is one-to-one and onto. This implies that the solution for the multi-level form exactly solves the original BLUP problem (5). Although this appears impossible due to the ill-conditioning and accuracy issue mentioned above, it can be shown that the prediction is a solution to a constrained optimization problem (e.g. unbiased constraint). Constructing a multi-level basis that spans a complementary constrained space leads to wellconditioned and accurate numerical algorithms. Furthermore, a single matrix inversion is all that is required. We describe the main ideas of the multilevel approach are used to tackle the above-mentioned numerical challenges. The details of this method developed by Castrill´on-Cand´as (2021) can be involved for the reader not well versed in advanced numerical analysis; here We present a simpliﬁed exposition. Let P there exist orthogonal projections L : R where P are constructed eﬃciently with an oct or binary kd-tree as shown in Theorem 3.1. Theorem 3.1. Suppose that we have a kd-tree representation, with t levels, of all the observation locations in S. Then: i) The linear operators L and W can be constructed in O(Nt) computational ii) The linear operators L and W have at most O(Nt) non-zero elements. iii) The operator (S) be the span of the columns of the design matrix X. Suppose that (S)is the orthogonal complement of P(S). The operators L and W steps and memory. Proof. See Castrill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021). Remark. For most practical datasets S, the number of levels of the kd-tree t is ≈ log Letting Y Wε. Note the trend component Xβ is removed from the data Z. The new log-likelihood function for the estimation of θ becomes where C pling of the likelihood function is not the only advantage of using C following theorem shows that C Theorem 3.2. If κ(A) → R is the condition number of the matrix A ∈ R then Proof. See Castrill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021). Remark. Evaluating the likelihood function ` log det{ C a Cholesky factor (see Golub and Van Loan (1996)) of C this is more stable numerically than evaluating `(θ). Nonetheless, the computational eﬃciency can be signiﬁcantly increased by constructing a sparse matrix version of C trix. The sparse matrix trill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021)). A sparse Cholesky factorization can now be computed. An alternative method for evaluating C Van Loan (1996)). This is discussed more in detail below. Note that in practice to estimate θ accurately, it is unnecessary to compute the Cholesky factor of the entire sparse matrix will be signiﬁcantly reduced (See Castrill´on-Cand´as, Genton and Yokota (2016); Castrill´on-Cand´as (2021) for details). We now show how to construct a multilevel predictor that gives rise to well conditioned multilevel covariance matrices. As pointed out in our problem setup, this is equivalent to a best linear unbiased predictor but much easier to solve numerically, making it suitable for missing data problems in large datasets. Consider the system of equations From the argument given by Nielsen, Lophaven and Søndergaard (2002) it is not hard to show that the solution of (3) leads to the GLS estimate of β (θ) = WC(θ)Wand Y∼ N(0, WC(θ)W). The decou- (equation (4)) and and the Mean Squared Error (MSE) at the target point x where From equation (3) rewrite C(θ) Now apply the matrix W to equation (8) and obtain W{C(θ)W WY. Since the columns for X are in P The advantage of this form is that C 3.2 and γ ˆγ can be computed by applying the transformation GLS estimate Remark. It is remarkable that estimate need to invert the covariance matrix C(θ). The linear system of equations (9) can be solved using a direct or iterative approach. If N is relatively small, a direct method such as a Cholesky factorization (Golub and Van Loan (1996)) will work well. However, for large N, due to well-conditioning of the matrix C method is a better approach. Let γ of γ computing the matrix vector products C as: ˜u:= (XC(θ)c(θ) − k(x)). (S) and can be uniquely rewritten asˆγ = Wγfor some γ∈ R. ˆβ and in turn,ˆβ can be solved as a least squares problem without the , where γis the initial guess. The main cost of the CG method is in For problems in Rand Rthis we can achieve this eﬃciently (O(N ) computational cost) using a Kernel Independent Fast Multipole Method (KIFMM) (Ying, Biros and Zorin (2004); Castrill´on-Cand´as, Genton and Yokota (2016)) or a Hierarchical Matrix (Litvinenko et al. (2019)). For d > 3 dimensions the direct approach is used with a cost of O(N). Remark. A preconditioner P the CG method, and the system of equations is solved instead of (9). For the multilevel method, P the diagonal entries of C condition numbers. If this is the case no preconditioner is used. Remark. Given k CG iterations, the total computational cost for computing and 3 dimensional problems, the parameter α is 1 with the use of the KIFMM method. For higher dimensions, a direct approach is used, and thus α = 2. The residual error for the CG method decays exponentially with respect to k and at a rate that is a function of the condition number. Small condition numbers lead to fast convergence, see Golub and Van Loan (1996) for details. Remark. The multilevel method is implemented with MATLAB (2016) and C/C++. More details can be found in the paper by Castrill´on-Cand´as (2021). However, for this paper we have further optimized the code which now runs at least twice as fast. As discussed in the introduction, missing data in the NIS and SID datasets form an important problem underscored in the HCUP report #2015-01 (Houchens (2015)). The report highlights, the problem of missing data for discharge information (total charge) in rural hospitals with potential consequences involving erroneous statistics and consequently possibly sub-optimal and even misinformed state and federal policy decisions. The missing data rates for total charge are at 2.08% for the NIS 2012 data. The Michigan SID data has a total charge missing data rate signiﬁcantly higher at 19.79%. We test the multilevel approach on the NIS 2013 dataset. The NIS 2013 missing data rate for total charge was 2.00 %. In this section we contrast the accuracy performance of the multilevel Kriging/BLUP method against recommended imputation algorithms in HCUP report #2015-01, including Predicted Mean Matching and Predicted Posterior Distribution methods. We make further comparison with more traditional methods such as K-Nearest Neighbors (KNN) and KNN regression. In particular, we test the accuracy of the various methods on the total charge variable, with its highest missing data rate. The computational and accuracy performance of the multi-level Kriging method is analyzed with the following choice of Mat´ern covariance function matrix vector product C(θ)γis obtained. ) from (7) using the multilevel approach is O(p+ (k + 1)N+ 2N t). For 2 with Γ the gamma function, ν > 0, ∞ > ρ > 0, and where K Bessel function of the second kind. The parameter ν controls the shape of the Mat´ern kernel and ρ is the length correlation. Thus for this case θ = (ν, ρ), and the stochastic optimization approach seeks the estimate of θ that best explains the data. To demonstrate the numerical eﬃciency of the multi-level method, we generate a series of random observation nodes on a n-sphere S 1} with dimension d. We create a series of nested sets of nodes S that vary with N = 2000, 4000 to N = 128, 000 in size. The ﬁnal set S 128,000 randomly selected points on the n-sphere S choose the ﬁrst d − 1 dimensions. In other words, forming a matrix of node coordinates (N ) by the number of dimensions d, we pick the ﬁrst d −1 columns as our covariate nodes. The last dimension (column) is chosen to be the observations. The polynomial basis chosen for the design matrix X is Total Degree (TD) with maximum degree w. The imputation performance of the multi-level Kriging method is tested on the National Inpatient Sample (NIS) datasets (HCUP (2012)), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare Research and Quality with the 2013 data set. Among all 190 variables in this dataset, totchg (total charge), as, the most problematic, is a good candidate to test the performance of the multi-level method. The variables npr (number of procedures), ndx (number of diagnoses), los (length of stay) and age are used as predictors. Note that as an experimental comparison we also test los as a candidate response variable, though this would not be necessary in practice since its missing data rate is 0.004 % in the NIS 2013 dataset. We extract from the NIS 2013 data matrix these ﬁve variables and remove any incomplete rows. To test the imputation performance of the multi-level Kriging method, N rows are selected at random for N = {2, 000; 5, 000; 10, 000; 50, 000; 100, 000 }. The error performance is measured using the relative root-mean-square error (RMSE), mean absolute percentage error (MAPE), and the log of the accuracy Ratio (lnQ). The relative RMSE represents the sample standard deviation of the diﬀerences between predicted and observed values normalized by the mean of the square of the observed values. The MAPE corresponds to the averaging the ratio of diﬀerences between predicted values and observed values to observed values; here there is a bias towards small predictions. lnQ overcomes this issue by using an accuracy measure based on the ratio of the predicted to actual value. The Kriging predictor is compared with other methods such as the Generalized Least Squares (GLS), k-nearest neighbors (KNN) and KNN regression. In addition, comparisons are made with the following four well known imputation methods: PMM (predicted mean matching), PPD (posterior prediction distribution), BEM (bootstrapping EM) and DA (data augmentation). Kriging/BLUP provides 38% reduction in error for rMSE, 75% for MAPE and 72% for lnQ compared to PPD (see Table 2). Similar performance is also achieved compared to PPM, DA and BEM, which we will analyze in more detail in this section. Our error rates are signiﬁcantly lower than the state of the art methods recommended by HCUP report#2015-01, with up to a 75% reduction. Indeed, this can have a strong impact on funding as an example of policy decision-making. As an example, if for half of a group of rural hospitals the total charge is missing, mean estimates could be signiﬁcantly oﬀ under recommended methods, with poor funding and related policies as a consequence. In particular, our numerical results show that MAPE errors for PPD, BEM, DA and can be more than 390% greater than the multilevel method, with a ﬁgure of 140% for PMM. The numerical performance of the multilevel approach is tested on the datasets Sfor k = 4, . . . , 7, d = 20 and d = 25 dimensional problems. Since d > 3, a fast summation (convolution) method such as the KIFMM is unavailable. Each matrix-vector product of the conjugate gradient iterations is computed with the direct approach using a combination of the Graphics Processing Unit (GPU, Nvidia GTX 970) and a single i7-3770 CPU @ 3.40GHz processor. We test the performance of BLUP only since the multi-level estimation computational burden is almost negligible in comparison (See Castrill´on-Cand´as (2021) for more detail). In Table 1 (a) and (b) numerical results for computing the BLUP parameters CG relative residual tolerance accuracy is set to tol = 10 ber of CG iterations needed to achieve tol residual for any matrix A. MB(s) is the wall-clock time in seconds needed to compute the multilevel basis. Itr(s) is similarly time needed to solve for time needed to solve for computing approach is given by Eﬀ We ﬁrst notice that the condition number for C is large (κ(C) ≈ 10 for relatively small problems. This has several numerical stability implications including a sever downgrade for maximal accuracy using any numerical inversion algorithm. A single precision computation would lead to erroneous results. Using a double precision computation can ameliorate the accuracy problem, but still exhibit slow convergence. In comparison κ(C leading to a stable and fast matrix inversion algorithm. Compared to the traditional iterative approach using the covariance matrix C (single level representation), the multilevel method is thousands to tens of thousands times faster for the same accuracy. This can be observed in Table 1 (a) and (b). The source of this eﬃciency is due to: i) The number of iterations for convergence to the same tolerance accuracy is signiﬁcantly smaller. ii) The multilevel approach only needs one iterative matrix inversion, in comparison to p iterative matrix inversions needed for the traditional single level representation approach. This is relevant for large problems where the matrix cannot reside in memory. We can now test the accuracy of the Kriging method. In particular we test the performance of the multi-level Kriging method with respect to the following regression problems: i) los ∼ totchg + npr + ndx, ii) totchg ∼ los + npr + ndx + age and iii) log(totchg) ∼ log(los) + log(npr) + ndx + age (normalized). i) totchg ∼ los + npr + ndx + age. For predicting total charge, we employ ˆβ andˆγ with the original covariance matrix C and the multilevel ii) los ∼ totchg + npr + ndx. For the experiments of predicting length of los, npr, ndx and age as predictors for datasets of size N = 2, 000 to N = 100, 000 with a 90% training and 10% validation split. This is a 4 dimensional problem, so we cannot use a fast summation method. In Figure 1 Kriging is compared to kNN-R, kNN and GLS for the prediction on the validation set. Observe that as the number of observations increases, Kriging outperforms all the other methods. Furthermore from Figure 2 (a) the general shape of the population, including mean and variance, of the validation dataset is well captured by Kriging, but signiﬁcantly degrades for kNN-R, KNN and GLS. The same phenomenon was also observed for PMM, PPD, BEM and DA. In addition, from the table in Figure 2 (b) we observe that Kriging outperforms GLS and k-NN consistently in all three measures of accuracy. In Table 2 (a) we observe that Kriging outperforms state-of-the-art traditional imputation packages such as PMM, PPD, BEM and DA for N = 100, 000 data points with the 90%-10% training/validation split. stay, totchg, npr and ndx are used as predictors due to the high correlation with los and run the simulations for datasets (training set plus testing set) of size N = 100, 000. Note that since this is a three dimensional problem, the Kriging multi-level code is signiﬁcantly faster due to the application of the KIFMM. From Figure 2 (b) the result of the experiment shows that the Kriging was not always the best predictor consistently. Intuitively, in this case, this could be due to the violation of the Gaussian assumption of the linear model. However, as observed for i) Kriging still outperformed signiﬁcantly all the other methods. For that case the Gaussian assumption was not necessarily satisﬁed as shown by the large improvements of tradi- Fig 1: Prediction error comparison (totchg) for the 2013 NIS Dataset with respect to the number of data points N, where 90% are used for training and 10% for validation. Fig 2: (a) Population histogram statistical comparison of kNN-R, kNN, GLS and Kriging with respect to the validation data set for 90,000 training and 10,000 validation datasets (N = 100, 000). Notice that Kriging more faithfully reproduces the population statistics of the validation total charge data set. This is the advantage of the unbiased constrained in the stochastic optimization. Note that PMM, PPD, BEM and DA methods also give similar results to kNN-R. (b) Total charge (totchg) imputation statistical errors comparisons. Kriging provides the best imputation performance for all error measures. (c) Length of stay (los) imputation statistical errors comparisons. For los, in general Kriging performs well. iii) log(totchg) ∼ log(los) + log(npr) + ndx + age. (normalized) In this paper we introduce novel techniques from Computational Applied Mathematics to solve large scale statistical problems. In particular, the problem of tional methods by using a log transformation as shown in iii). In addition, Kriging seeks an optimal prediction that appears at the very least to have the same performance as the traditional methods, in particular when the training set is large. For this experiment the same dataset as in i) is used. However, a log transformation is and normalization step is applied. From Table 2 (b) it is observed that the accuracy of traditional imputations methods improves. Although the accuracy of the Kriging method improves somewhat, it still outperforms all others. This indicates that the proposed Kriging method has the capacity of handling raw and rough models when traditional methods tend to fail. imputation is solved with the new multi-level Kriging method. Due to the numerical and stability problems associated with the stochastic optimization Kriging method, until recently this had limited applicability to imputation for large datasets. Due to the introduction of multi-level methods from the CAM community, many of these limitations have been resolved. Our results show that the multi-level Kriging method is computationally feasible, stable numerically, accurate and mathematically principled. In particular, it is shown that the multilevel BLUP is exact and signiﬁcantly outperforms current state-of-the-art methods. Furthermore, it is robust and applies to a large class of missing data problems such as massive medical records. Multiple imputation is an important strategy for quantifying the uncertainty of predictions. There are many methods such as bootstrapping used to created multiple realizations of data. These realizations are used to quantify the variances of predictions. Such methods can also be used to create multiple realizations for the multilevel Kriging/BLUP approach. However, the extension is not trivial. To more faithfully reproduce realizations of the data, the bootstrapping approach needs to take into account the Cholesky decomposition (see Golub and Van Loan (1996)) of the covariance matrix C, which is very diﬃcult since the matrix is large and ill-conditioned. However, alternatively we can use a Karhunen Lo´eve (KL) expansion (see Castrill´on-Cand´as and Kon (2021)) to create multiple realizations with the Mat´ern covariance function from the Gaussian process representation of the data. This would involve computing the eigenstructure of the covariance function, which is signiﬁcantly more stable to compute even if the matrix is ill-conditioned. Moreover, by using a Kernel Independent Fast Multipole (KIFMM) approach Ying, Biros and Zorin (2004) the computation of the eigenstructure in principle can be relatively fast for problems in R the covariance matrix. However, for higher dimensional problems, such as predicting the total charge missing data involving 4 dimensions, there are no known fast summation methods such as the KIFMM. It is still possible to compute the eigenstructure for a relatively large dataset on a powerful Graphics Processing Unit (GPU). Alternatively, there exists a set of linear equations for the BLUP that solve for the Mean Square Error (MSE) of the prediction. This involves inverting the covariance matrix C; thus for large datasets it can be numerically unstable and intractable. Castrill´on-Cand´as, Genton and Yokota (2016) show that there exists a formulation for the multilevel approach that is signiﬁcantly faster and numerically stable. However, it can still be intractable for estimating large numbers of missing data. We shall investigate these approaches in more detail in a future publication. We appreciate the help and advice from Bindu Kalesian, in particular, for the access to the NIS dataset. Futhermore, the feedback from Karen Kafadar has been invaluable. This material is based upon work supported by the National Science Foundation under Grant No. 1736392. . An advantage of the KL expansion is that it does not involve inversion of