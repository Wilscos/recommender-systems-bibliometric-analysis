Content caching at network edge, such as base stations, is a promising solution to deal with the explosively increasing trafﬁc demand and to improve user experience [1]. This approach is beneﬁcial for both the users and the network operators as the former can access the content at a reduced latency, and latter can alleviate the load on backhaul links. The performance of edg e caching, however, can be further improved by utilizing content recommendation and optimizing information freshness. Originally, recommender systems have been used for presenting content items that bes t match user interests and preferences. In fact, the reports in [2], [3] show that 80% of requests on content distribu tion platforms are due to content recommendations. Recently, a number of studies have proposed to utilize content recommendation for improving caching efﬁciency. In [4], [5], recommendation is utilized to steer user requests toward the contents that are both stored in the cache and of interest to u sers. More recently, content recommendation is employed to satisfy content requests using alternative and related contents. Namely, instead of the initially requested content that is absent from the cache, some other related con tents are recommended [6], [7], [8]. This approach is o f interest to many applications such as video and image retrieval, and en tertainment-based ones [6]. Another important aspect that arises naturally in the context of content caching is the freshness of information [9]. As cached contents may become obsolete with time, we n eed to also account for updating the content items. Information freshness is quantiﬁed by age of information (AoI) which is deﬁned as the amount of time elapsed with respect to the time stamp of the information in the most recent update [10]. The AoI grows linearly between two successive updates. In this study, we address optimal scheduling for updating the cache for a time-slotted system where content recommendation and AoI are jointly accounted for. The cache has a capacity limit, and the content items vary by size. Moreover, u pdating the cache in a time slot is subject to a network capacity limit. For a content request, if the content is available in the cache, the request is served using the stored content. Otherwise, a set of related and cached contents will be recommended. If one of the recommended contents is accepted, then the request will be again served from the cache with the accepted content. If not, the request will be served by the remote server with a higher cos t. It is worth noting that incentive mechanisms may be utilized to motivate users to accept the recommended conten ts (e.g., zero-rating services) [6]. For each content item, there is a cost function that is monotonically increasing in the AoI. Thus, caching a content with higher AoI results in a higher cost. The optimization decision consists of, the selection of the content items for updating the cache, and a recommendation set for each non-cached content. Th e objective is to ﬁnd the schedule minimizing the total cost over the scheduling horizon. Our work consists in the following contributions for the outlined cache optimization problem with recommendation and AoI (COPRA): when contents are of uniform size, based on a reduction from 3-satisﬁability (3-SAT). lation for the problem in its general form, enabling the use of general-purpose optimization so lvers to approach the problem. This is particularly useful for solving smallscale problem instances to op timality, and to enable to accurately evaluate low-complexity though sub-optimal algorithms. to the ILP, allowing for decomposing the problem into two subproblems, each with special structures. The ﬁrst subproblem itself further decomposes into smaller problems, each o f which can be mapped to ﬁnding a shortest path in a graph . The second subproblem also decomposes to sm aller problems. However, the problem size remains exponential, and therefore we propos e column generation for gaining o ptimality. Moreover, we demonstrate that the pricing problem of column generation can be solved via dynamic programming (DP). It is also worth noting that, decomposition enables parallel computation. In addition, our algorithm computes a lower bound (LBD) that can be used to evaluate the quality of any given so lution. performance of our algorithm by comparing its solution to global optimum for small-scale scenarios, and to the LBD otherwise. The evaluations sh ow that our algorithm provides s olutions within 8% of global optimality. To the best of our knowledge there is no work that jointly s tudy content caching, recommendation, and AoI. In the following, we ﬁrst review the works that have studied content caching and AoI, and then those on caching and recommendation. The works in [9], [11], [12], [13], [14], [15], [16], [17] have studied content caching when AoI is accounted for. The general problem setup in these works is what contents to cache and when to update them with an objective function based on AoI. In [9], [11] the objective is minimizing the expected AoI wh en interupdate intervals of each item or the total number of updates are known. In [12], [13], [14] content caching is studied where both popularity and AoI are considered. In [12], cache miss is minimized, and in [13] the load of backhaul link is minimized via balancing the AoI and cache updates. In [14], partially updating a content, which depends on th e type of co ntent and its AoI, is enough to completely update the content. In [15], the overall utility of a cache deﬁned based on AoI of contents is maximized, subject to limited cache and backhaul link capacities. For a given origin, a set of users, and a (set of) cache between them, the AoI at the cache(s) and users is analyzed in [17]. As an extension of [17], the work in [16] studied the trade-off between obtainin g a content from the origin with longer transmission time and from the cache with higher AoI. A recent survey of AoI can be found in [18]. In general, the works that studied content caching and recommendation can be classiﬁed into two categories. In the ﬁrst category, content recommendation is utilized to shape the requests and steer the content demand toward the contents that are both stored in the cache and interesting to the users [4], [5], [19], [20], [21], [22], [23], [24], [25]. In [4], [5] a preference “distortion” tolerance measure is used to quantify how much the engineered recommendations distort the original u ser conten t preferences. In [19], an experiment is conducted to demonstrate the effect of content recommendation on caching efﬁciency in practice. In [20], the ob jective is to maximize both th e quality of recommendation and streamin g rate, and th e authors proposed a polynomial-time algorithm with approximation guarantee. In [21], [22], caching and recommendation decisions are optimized based on the p reference distribution of individual users. In [23], content caching and recommendation are optimized taking in to account the temporalspatial variability of user requests. In [24], the authors study the fairness issues of recommendation where some contents get more visible than others by recommendatio n. In [25 ], reinforcement learning is utilized for learning user behavior and optimizing caching and recommendation. In the second category of studies, recommendation is utilized to satis fy a request when the requested content is not available in the cache, by recommending some other cached and related contents [6], [7], [8 ], [26], [27], [28]. The idea of recommend ing related contents in case of a cache miss is formally introduced in [6] where the authors referred to the scenario as “soft cache hit”. In this reference, the authors illustrate how “soft cache hit” is able to improve the caching performance. They also consider a caching problem with the objective of maximizing the cache hit rate where all co ntents in the cache can be recommended. Using the submodularity property of th e objective function, they propose algorithms with performance guarantee. Later, in [8], the authors consider a more realistic system model in which only a limited number of contents can be recommended. Then, they propose a polynomial-time algorithm based on ﬁrst solving the caching problem, and then ﬁnding the recomm endations sets. In [26], the authors model th e relation amon g contents as a graph, an d then studied the characteristics of this graph to predict wheth er it is worth to ﬁnd the optimal solution or a low complexity heuristic will b e sufﬁcient. In [27], the authors try to ﬁnd the best caching policy for a sequence of requests where recommendation is accounted for. In [28] a multi-hop cache network is studied where soft cache hit is allowed in one of th e caches along the path to the end node that stores the initially requested content. The closest works to our study are [6], [8] in the sense that they also cons idered soft cache hits. However, there are signiﬁcant differences. To the best our knowledge, it is novel that that caching decision, content recommendation, and information freshness are jointly o ptimized. Moreover, in our work we account for cache update costs, as well as the capacities of cache and backhaul links. SIS 3.1 System Scenario The system s cen ario consists of a content server, a base station (BS), and a set of content items I = {1, 2, . . . , I}. The server has all the contents, and the BS is equipped with a cache of capacity S. The BS is connected to the content server with a communication link of capacity L via which the cache con tents can be updated. The size of content item i ∈ I is deno ted by s. We consider a time-slotted system with a time period of T time slots, denoted by T = {1, 2, . . . , T }. At the beginning of each time slot, the contents of th e cache are subject to updates. Namely, some stored contents may be removed from the cache, some new contents may be added to the cache by downloading from the server, and some existing conten ts may be refreshed. The AoI of an item in the cache is the time difference between the current slot and the time slot in which the item was most recently downloaded to the cache. Each time an item is downloaded to the cache, the item’s AoI is zero, i.e., maximum info rm ation freshness. The Ao I then increases by on e fo r each time slot, until the next update. In other words, the AoI of any cached content item is linear in time, if the content is not refreshed. For content i ∈ I, the relevant AoI has a limit A. The content is considered obsolete if the AoI exceeds A. Hence, a cached content i in time slot t can take one of the AoIs in A= {0, ..., min(A, t − 1)}. The cost associated with content item i with AoI a in time slot t is characterized by a cost function fthat is monotonically increasing in AoI a. For a request of conten t i, if the conten t is stored in the cache and the AoI is no more than A, th e request is satisﬁed from the cache. Otherwise, a set of related cached contents, h ereinafter referred to as a recommendation set, is recommended to the user. If th e user accepts any element of the recommendation set, the request is satisﬁed by the cache. If no t, the requ es t needs to be satisﬁed from the server. Note that since a user may not be interested in getting a long list of recommended contents, we limit the size of recommendation set to be at most N [8], [29]. Denote by R= {1, 2 , ..., R} the index set of all co ntents related to content i. This set can be determined from past statistics and/or learning algorithms [6 ]. Obvious ly, the index set of any recommendation set for content i is a subset of R. Note that the recommendation set may change from a time slot to another. Denote by hthe nu mber of reques ts for content i ∈ I in time slot t ∈ T . The value of hcan be estimated via recent requests of the contents, p opularity of the co ntents, an d/or machine learning algorithms [6], [30]. In this study, for the ease of exposition, we consider the total number of requests for a content instead of individual user requests. Similarly, the acceptance probability of a content does not vary from a user to another. Note that individual user requests and acceptance probability can be easily accommodated in ou r formulations and alg orithms. Denote by c and cthe costs for downloading one unit of data from the server and from th e cache to a user, respectively. Downloading cost from server to cache is c− c. Intuitively, c> cto encourage downloading from the cache. The cache optimization problem with recommend ation and AoI, or COPRA in short, is to determine which content items to store, update, and recommend in each time slot, such that the total cost of content requests over time horizon 1, 2, ..., T is minimized, subject to cache and b ackhaul link capacities. 3.2 Cost Model Denote by xa binary optimizatio n variable that equals one if and only if content i with AoI a is stored in the cache at time slot t. Hence, x= 1 means that the conten t i at time slot t is just downloaded from the server to the cache with AoI zero. Then, the overall downloading cost is shown in (1) . In (1), the ﬁrst term is the downloading cost from server to the cache due to cache updates and the second term is the downloading cost of requests that are delivered usin g cached contents. Next, we calculate the downloading cost related to content recommendation. Denote by pthe probability of accepting content j ∈ Rwith AoI a instead of content i. This probability depends both on the correlation between the two contents as well as the AoI of content j. The value of pcan be calculated bas ed on historical statistics [6], item-item recommendation [31], and/or collaborative ﬁltering techniques [32]. Denote by c a generic candidate set of contents for recommendation. Because of AoI, each element of c is a tuple of a recommended co ntent and its AoI. We refer to c as the recommendation set. Denote by Cthe set of all such recommendation s ets for conten t i ∈ I in time slot t. Denote by va binary optimizatio n variable that takes value one if and only if (some content) in recommendation set c ∈ Cis accepted ins tead of content i in time slot t. The probability of notQ accepting any of the contents in c is˜P=(1 − p). Thus, the probability of accepting at least one of them is 1 −˜P, and hence the expected costis: 3.3 Problem Formulation COPRA can be formulated using integer-linear programmin g (ILP), as shown in (3). In (3 ), we use yas an auxiliary binary variable that equals one if and only if content item i is cached in time slot t. Constraints (7b) state that if content i is cached in time slot t, then it should exactly take one of the possible AoIs a in A. Constraints (7c) and (7d) together ensure that co ntent i in time slo t t has AoI a (i.e., x= 1) if and only if three conditions hold: Item i is in the cache (y= 1), it has AoI a − 1 in time slot t − 1 (x= 1), and it is not refreshed again in slot t (x= 0). Constraints (3e) indicate that either content i is cached in time slot t or some set c ∈ Cis recommended. Constraints (3f) ensure that the contents in recommendation set c are indeed cached. Constraints (3h) and (3g) formulate the cache and backhaul capacities. Finally, Constraints (3i)-(3k) state the variable domain. ILP : min∆+ ∆(3a) x∈ {0, 1}, t ∈ T , i ∈ I, a ∈ A(3j) v∈ {0, 1}, t ∈ T , i ∈ I, c ∈ C(3k) As the nu mber of recommendations set are expo nentially many, the ILP is exponential in size. However, the ILP is of interest for solving small-scale problem instances for gauging the performance of other suboptimal so lutions. In this section, we rigoro usly prove the NP-hardness of COPRA based on a reduction from the 3-SAT. Next, we show the tractability of the problem for a single time slot when the contents are partitioned into subcategories with uniform probabilities. Theorem 1. COPRA is NP-hard. Proof. We adopt a polynomial-time reduction from the 3-SAT problem that is NP-complete [33]. Consider any 3-SAT instance with k clauses and n Boolean variables u, u, ..., u. A variable or its negation is called a literal. Denote by ˆuthe negation of u, i = 1, 2, ..., n. Each clause consis ts o f a disju nction of exactly three different literals, for example, ˆu∨ u∨ u. The task is to determine if th ere is an assignment of true/false values to the variables, such that all clauses are satisﬁed (i.e., at least one literal has value true in every clause). We construct a reduction from 3-SAT as follows. Each literal or clause represents a content, referred to as literal and clause contents, respectively. Moreover, n auxiliary contents are deﬁned, one for each pair of variable and its negation. Hence, there are in total 3n + k contents, and I = {1 , 2, ..., 3n + k}. All contents have unit size, i.e., s= 1 for i ∈ I. Each variable, its negation, and the correspon ding aux iliary content are related mutually with acceptance probability of 1. Thus, the requests made for any of them can be fully satisﬁed by any of the other two contents. The number of time slots is one, i.e., T = {1 }, and the size of cache is n, i.e., S = n. The number of requests for each clau se and literal content is 1, i.e., h= 1 if i is a literal or a clause content. Each clause content is related to the corresponding three literal con tents with acceptance probability of 1. Hence for a request made for a clause content, the system can recommend the three literals if some or all of them are cached. There are n + k + 1 requests for each auxiliary content, i.e., h= n + k + 1 if i is an auxiliary content. No relation is present between contents other than those speciﬁed above. Note that the acceptan ce probability is symmetric between any two related contents. Parameters cand care set to 1 and 2, respectively. We now show there is an optimal solution such that the cache stores exactly either a variable or its negation. Suppose an auxiliary and/or a clause content is cached. In the former case, swapping this auxiliary con tent with a non-cached literal content of the corresponding pair will not increase but possibly improve the cost. Because, by swapping, more clause contents m ay also be satisﬁed from the cache. Now, suppos e a clause content is cached. Then, at least one auxiliary content must be served us ing the server with cost 2(n+k+1). For the oth er contents, the best possible o utcome is (n − 1)(n + k + 1) + 2n + k + 1. Hence, the total cost is ∆= 2(n + k + 1) + (n − 1)(n + k + 1) + 2n + k + 1. The cost when exactly one literal of each literal pair is cached is no more than ∆= 3n + n(n + k + 1) + 2k, assuming all clause contents are served using the server. It can be veriﬁed easily that ∆> ∆. Therefore, at an optimum, the cache stores exactly either a variable or its negation. Thus the optimal total cost for the literal and auxiliary contents is known. Clearly the construction above is polynomial in size. If there is no solution to the 3-SAT, then at least one clause content need to be downloaded from server with cost c= 2, and each of the other clause contents has at least the cost of c= 1. Thus, the total cost is at least δ= k + 1 . If there is a solution to 3-SAT, then the cost fo r all clause contents is at most δ= k. As can be seen δ> δ. Thus, whether or not there exists a caching strategy with a total cos t of no more than δgives the correct answer to 3-SAT. Therefore, the recognition versions of COPRA is NP-complete and its optimization version is NP-hard. In practice, the content items may naturally fall into different subcategories based on the type of the content, e.g., video contents can be catagorized based on if it is science ﬁction, drama, or comedy, etc., [21], [22]. If all items in a subcategory are related with the sam e acceptan ce probability, then we show th e optimal solution of the problem with uniform size and one time slot can be computed in polynomial time via DP. Note that the probability from a subcategory to another may still differ. We refer to this special case as COPRA-CAT. Theorem 2. COPRA-CAT can be solved in polyno mial time. Proof. We compute a m atrix, called cost matrix and denote it by g, in which entry g(k, i) represents the total cost by caching i content items of category k. This value is computed simp ly from the ﬁrst i contents with the highest requests. Below, a recursive function is introduced to derive the o ptimal caching solution over all categories. We deﬁne a second matrix, called the op timal cost matrix, and denote it by w, in which w(k, s) represents the cost of the optimal s olution by considering the ﬁrst k categories using a cache size of s, s= 0, 1, ..., S. The value of w(q, s) is computed by the following recursion: w(k, s{g(k, r) + w(k − 1, s− r)}(4) Using Equation (4), the optim al cost for the ﬁrst k categories is computed given the optimal cost of the ﬁrst k − 1 categories. For the overall solution, the o ptimal cost can be computed using the above recursion for cache size S and K categories. We prove it by induction. First, when k = 1, i.e., we have only one category, We have w(1, s) = ming(1, r) for all r = 0, 1, ..., s. Obviously r= s, that is, to allocate th e whole capacity to this category. Now, assume w(l, s) is optimal for some l. We prove that w(l + 1, s) is optimal. According to the recursive function: w(l + 1, s) = min{g(l + 1, r) + w(l, s− r)}(5) The possible values for r = 0, 1, ..., s, an d for each of the possible values of r, w(l, s− r) is optimal. This together gives the conclusion that the minimum will be obtained indeed by the min operation. Thus, w(l + 1, s) is optimal. Finally, we show that w(K, S) can be computed in polynomial time. The comp lexity of computing g is of O(KI). By the above, the computational complexity of w is of O(KS) where S is up to the nu mber of contents. A commonly considered strategy for fast but suboptimal solution is a greedy approach (GA) that builds up a solution incrementally. GA tries to min imize the total cost of each time slot by considering the items on e by one. The algorithm is shown in Algorithm 1. For each time slot and each item, GA calculates an overall score based on the number of requests, the relations to other contents, and the size of the content, see Line 7. Then, GA treats items based on their scores in descending order. For a content under processing, it is downloaded from server to the cache if there is enough cache and backhaul capacities, see Lines 11-13. Otherwise, GA checks if the content is cached in the previous time slot, and if there is enough cache capacity to store the content, see Lines 14-17. When all contents are processed, GA ﬁnds recommendation sets for the non-cached items. For each non-cached item, GA looks at the cached and related items, and pick the ones of highest acceptance p ro babilities, see Lines 18-21. GA is simple but it turns out the performance is not satisfactory, and therefore there is a need of developing a better algorithm. Algorithm 1 Greedy Algorithm Input: T , S, L, p, h, A, s, i ∈ I Output: y, x, v ← ∅, t ∈ T , i ∈ I then probabilities with respect to i} We propose an algorithm by applying Lagrangian decomposition (LD) to ILP (3). In LD, some variables are duplicated, with equalities constraints requiring that the d uplicates are equal to the original variables. Next, these constraints are relaxed using Lagrangian relaxation an d some method (often a subgradient method [34], [35]) is ap plied to solve resulting Lagrangian d ual. 6.1 Lagrangian Decomposition In our LD-based algo rith m (LDA), we duplicate the x variables. Speciﬁcally, we replace x variables in AoI constraints (3b)-(3d) by xand add a set of constraints requiring x = x. Next, we relax constraints x = xwith multipliers λ, and the resulting Lagrangian relaxation is given in (6). Note that ∆is the same as ∆but the x variables are replaced by x. As can be seen ILP (6) is decomposed into to two subproblems, one consists of all terms containing x, and the other all terms with x. Below, we formally state each of them. x≤ x, t ∈ T \ {1}, i ∈ I, a ∈ A\ {0} (6d) (3e) − (3h) 6.2 Subproblem One Subproblem 1, hereinafter referred to as SP, is shown in (7). The SPconsists of all terms having x, namely th e downloading cost and Lagrangian multiplier terms as the objective function, and constraints related to AoI. We exploit th e structure of SPas follows. First, as there is no constraint bundling the content items together, SPdecomposes by content, leading to I smaller problems . The optimization problem corresponding to content i ∈ I is denoted by SPand consists of the terms of SPfor content i. Secon d, we show that SP, i ∈ I, can be solved as a shortest path problem. Theorem 3. SP, i ∈ I, can be solved in polynomial time as a shortest p ath problem. Proof. C onsider content i ∈ I. We constru ct an acyclic directed graph such that ﬁnding the shortest path from the orig in to distention is equivalent to solving SP. The graph is shown in Figure 1. The objective function of SPis: The graph is constructed as follows. Nodes O and D are used to represent the origin and d es tination respectively. For time slot t, there are 1 + min{A, t − 1} vertically aligned nodes. A path passing node Vand Vcorresponds to the following two scenarios, resp ectively: 1) The content is not in the cache. 2) The content is in the cache and has Ao I a, a ∈ A. For each node Vthere are two outgoing arcs, one to Vwhich corresponds to that the con tent is not stored in the next time slot and the arc hence has weight zero, and the other to Vwhich has weight d= (c− c)s+ cshf+ λcorresponding to the case that the content is downloaded to the cache in the next time slot and has AoI zero. For each node Vthere three outgoing arcs to V, V, and V, respectively. A path passing through the ﬁrst, second, and the third arcs corresponds to the following three scenarios, respectively: 1) Content is deleted for the next time slot with arc weight zero. 2) The content is re-downloaded from the cache and has AoI zero with weight d. 3) The content is kept and its AoI increases with one time unit and has weight d= Finally, there are T arcs from Vand Vto D, each with weight zero. Given any s olution of SP, by construction of the graph, the solution directly maps to a path from the origin to the destination with th e same objective function. Conversely, given a path we construct an ILP solution. For time slot t, if the path contain n ode Vand V, we set y= 0. If th e path passes through node V, we set y= 1 and x= 1. The resulting ILP solution has the same objective function value as the path leng th in terms of the arc weights. Hence the conclusion. 6.3 Subproblem Two Subproblem 2, hereinafter referred to as SP, consists of all those terms of (6) containing x. SPdecomposes by time slot, leading to T smaller problems. Denote by SPthe problem corresponding to time slot t, shown in (9). The number of v variables in SPis exponentially many, as there are exponential number of recommendation sets. Hence, having all v variables in the ILP is impractical. To deal with this issue, we apply colum n generation to the v variables in the LP relaxation of (9), to generate only the promising recommendation sets. Column generation is a powerful method to obtain the global optimum of some structured linear programs with expo nential number of variables. In a column generation algorithm, the most promising variables are generated in a iterative process by solving alternatively a master problem (MP) and a pricing problem (PP). Each time PP is solved, a new variable that possibly improves the objective function is generated. The beneﬁt of column generation is to exploit the fact that at optimum only a few variables are positive. Below we deﬁne the MP an d PP for solving SP. In the following, to ease the presentation, we cons ider a generic time slot and drop the index t. 6.3.1 MP and RMP MP is the continuous version of (9). Restricted MP (RMP) is the MP but with a small subset C⊆ Cfor any content i ∈ I. Denote by Cthe cardinality of C. 6.3.2 Pricing problem The PP uses the dual information to generate new variables/columns. Denote by vthe optimal solution of RMP. After obtaining v, we need to check whether vis the global optimum of RMP. This can be determined by ﬁnding a column with the minimum reduced cost for each content i ∈ I. This means the PP decomposes to I smaller problems, one corresponding to each content i. If all these minimum reduced cost values are nonnegative, the current solution is optimal. Otherwise, we add the columns with negative reduced costs to their recommendation sets. Consider content i ∈ I. Denote by πand β= {β|j ∈ R, a ∈ A} the op timal dual values of the co unterpart constraints of (9d) and (9 e) in the RMP, respectively. Hence, the reduced cost of the v-variable of content i and recommendation set c is: in which˜P=Q(1−p). This reduced cost is nonlinear due to the term˜P. But, we can linearize it using logarithm. Let = log ( hs(c− c)) +log(1 − p) Now, the reduced cost can expressed as: where p= log (hs(c− c)) +Plog(1 − p). AsP p∈ (0, 1),log(1 − p) is zero or a negative value. Thus , th e minimum and maximum values that pcanP take are p= log (hs(c− c)) +log(1 − p). and p= log (h(c− c)), respectively. Hence p∈ [p, p]. The above expression is for a given v-variable. In the following, we deﬁn e PP, that is an auxiliary problem, of which the optimum will tell us the not-yet-present variable with minimum reduced cost. Denote by PPthe PP corresponding to content i. Let z be a binary optimization variable that takes value one if and o nly if content j with AoI a is in the set to be generated. Then PP can be expressed as (13). Note that the terms cshand πare constants here, and hence can b e dropped in the optimization process. Constraints (13c) ensure that for each content in the recommendation set, exactly one AoI value is selected. Constraint (13d) states that the total number of contents in the recommend ation set can not exceed the given up per bound. In the fo llowing, we show that PPcan be solved via DP. PP: min 10+βz(13a) s.t. p= log (hs(c− c)) +log(1 − p)z z∈ {0, 1}, j ∈ R, a ∈ A(13e) Theorem 4. PPcan be solved to any desired a ccuracy via DP. Proof. We ﬁrst perform two preposses sing steps, and then apply DP to the resulting problem. First, as the objective function is minimization and pis a continu ous variable, constraint (13b) can be stated equivalently as a greater-than-or-equal constraint. We re-express the constraint as: Since p∈ [p, p], the minimum and maximum values that the right-hand-side of the constraint can take are zero andPP Second, the problem can be solved to any desired accuracy (though not exactly the op timum), by quantizing the interval of pinto W steps; this corresponds to multiplying the coefﬁcients with some number M and rounding. Let W =PP M− log(1 − p) denote the max imum value of the right-hand-side of (14) after multiplying it by M . Similarly let q= −M log(1 − p) for j ∈ R, a ∈ A. Note that the minimum value that the right-hand-side can take is still zero. Hence, p∈ [0, W ]. After these two steps, (13) can be re-expressed as (15). Formulation (1 5) resembles an inversed multip le-choice knapsack problem with an upper bound (15d) on the number of items. The difference is that we have pas one additional variable with a term in the objective function. The selection of paffects the right-handside, correspond ing to changing the knapsack capacity. Knapsack problem is solved via DP efﬁciently. The interesting point is that DP provides not only the optim al function valu e of z with the given capacity, but also those for all intermediate values starting from zero, implying that one computation is en ough to examine the effect of all pvalues. Then the optimum can be obtained by post processing considering the functio n term with p. The DP alg orithm for solving PPis shown in Algorithm 3. Lines 1-5 are the initialization steps. Lines 6-13 solves (15) with maximum capacity W in which matrix Bis the optimal cost matrix. Entry B(w, j, n) represents the cost of the optimal solution when up to n contents of the ﬁrst j contents can be in the recommendation set with a knapsack capacity w ∈ [0, W ]. Matrix Ais an aux iliary matrix that stores the AoI corresponding to optimum for each tuple (w, j, n) where w ∈ [0, W ], j = 1, .., R, and n = 1, 2, ..., min{j, N}. Lines 14-32 perform the post processing step. Namely for each intermediate value p∈ [0, W ], the corresponding objective function value is calculated and compared to the minimum value found so for, in order to ﬁnd the global minimum of the problem . The complexity of this algorithm is of O(W RNA). The colu mn generation algorithm for solvin g SPis shown in Algorithm 2 in which Algorithm 3 is used for solving PP, i ∈ I. [0, j, n] ← 0 for any j and n [w, 0, n] ← ∞ for any w > 0 and any n 6.4 Attaining Integer Feasible Solutions The solutions of the two subproblems will likely violate some original cons traints , and we present an approach to generate feasible solutions based o n SP. We take the solutions of SP, t ∈ T , and “repair” them in order to construct an integer solution for COPRA. The reason o f using SP, t ∈ T is that its solution contains the information of recommendation sets, an d hence it resembles more a solution to the original problem. However, these solutions do not respect the AoI evolution of contents across the time slots as each SP, t ∈ T , is solved independ ently from the others. The repairing algo rith m (RA) is shown in Algorithm 4, which consists of three main steps. In the algorithm, symb ol ← is used to indicate the assignment of a value. Symbol ⇔ is used to indicate that an assigned value of an optimization variable is kept ﬁxed subsequently. In the ﬁrst step, we take the solution of SP, t ∈ T , and perform an iterative rounding process on the y-variables to obtain an integer solution. More speciﬁcally, we ﬁrst ﬁx the current yvariables having value one, followed by ﬁxing the variable with the largest fractional value to one if there is enough capacity and zero otherwise. We then solve SPagain. Now, if the solution is integer, we stop. Otherwise, this process is repeated until an integer solution is obtained. Obviously, in the worst case, I iterations are needed to obtain an integer solution. Denote by ˆy = {ˆy: t ∈ T and i ∈ I} the obtained values of y-variables of each SPfor t ∈ T . This step corresponds to Lines 1-9 in Algorithm 4. In the second step, we utilizeˆy as input to the optimization problem stated in (16). Therein, the y-variables have the same meaning as deﬁned earlier in Section 3.3. Solving (16) provides a caching solution in which the AoI evolution of co ntents across time slots are respected. The objective fun ction is m aximization, to encourage setting the y-variables to be as similar toˆy as possible. Here, ǫ is a small positive number, to encourage caching contents even if ˆy is zero. This step correspon ds to Line 10 in Algorithm 4. After these two steps, we have a complete caching solution over time slo ts. Fin ally, for each non-cached content item, we choose the N highest related cached contents as its recommendation set. This step corresponds to Lines 11-13 in Algorithm 4. We remark that formulation (16) is an integer program. However in practice this is solved rapidly. Moreover, the repairing operation does not need to be done in every iteration of subgradient optimization. The main steps of LDA is shown in Algorithm 5. Line 1 initialize the total number of iterations K to perform, and tolerance parameters ǫand ǫ. Lines 2 and 3 initialize the vector of Lagrangian multiplier λ to 0, the iteration counter k = 1, the lower b ound LBD to zero, and the best found solution ¯w to ∞. Lines 5 and 6 solve the SPfor i ∈ I and SPfor t ∈ T , respectively. Lines 7 and 8 calculate the Lagrangian function value, and update the LBD if a higher lower bound is found. Lines 9 ﬁnds a solution for the problem, and then Line 10 updates the current upper bound if a solution with lower o bjective function value is obtained. Line 11 updates th e Lagrange multipliers, and Line 12 increases the iteration counter by o ne. Finally, Line 13 checks whether a stopping criterion is met. ˆy ← {y: t ∈ T , i ∈ I} Algorithm 5 The main steps of LDA objective function value U In this section, we present performance evaluation results of LDA and GA. We ﬁrst consider small-size problem instances, and evaluate the performances of LDA and GA by comparing them to the global optimum obtained from ILP (3). We report the (relative) deviation from the optimum, referred to as the optimality gap. For large-size problem instances, it is computationally difﬁcult to obtain global optimum. Instead, we use the LBD derived from LDA as the reference value. This is a valid comparison because the deviation with respect to the glob al optimum will never exceed the deviation from the LBD. We will see that, numerically, using the LBD remains accurate in evaluating optimality. The content popularity is modeled by a Zip F distribution, i.e., the probability where the i-th content is requested is [36], [37]. Here γ is the shape parameter and it is set to γ = 0.56 [36]. The sizes of content items are generated within interval [1, 1 0]. We have set the the cache capacity to 50% of theP total size of content items, i.e., C = 0.5s. The capacityP of backhaul link is set to L = ρswhere parameter ρ steers the back haul capacity in relation to the total size of content items. The p ro bability of accepting a related content is generated in interval [0.6, 1). The maximum AoI that a content can take is set to two. We use content-speciﬁc and time-speciﬁc functions including linear and nonlinear ones from the literature [38], [39] to model the AoI cost of content items. Speciﬁcally, for each content, one of the following functions is randomly selected: f= 1 + αa, f=, and f= e. The functions are made content-sp eciﬁc and time-speciﬁc by varying parameter α. We remark that the p erformance of LDA remains largely the same if only one type of function is used for all contents. The use of multiple functions is to s how that the algorithm works in general with diverse function s. We will vary parameters I, T , and ρ, and study their impact on the overall cost and algorithm performance. For each input setu p, we have generated 10 problem instances and we report the average cost. Figure 2 shows the total cost returned by LDA when recommendation is utilized, and LDA with no recommendation (denoted by LDC-NC). The ﬁgure shows that, interestingly, the total cost decreases by more than 50% with recommendation. Another interesting point is that the reduction is even more when the n umber of content items increases. From this result, the consideration o f recommendation op timization is relevant. Figures 3-5 and Figures 6-8 show the performance results for the small-size and large-size problem instances, respectively. In Figures 3-5, the green line represents the global optimum computed using ILP (3). In Figures 6-8, the black line represents the LBD obtained from LDA. In all ﬁgures, the blue and red lines represent the overall cost returned by LDA and GA, respectively. The deviation from global optimum for LDA is within a few percent, while for GA it is signiﬁcantly larger. Moreover, the results for both small-size and large-size problem instances are consistent. Figure 3 shows the impact of content items on the total cost for small-size problem instances. The overall cost slightly decreases with the number of contents. This is due to the fact that th e capacity of cache is set relatively to the total. Namely, with larger number of contents, more capacity is available, and hence more opportunity to serve content requests from the cache. This effect, however, can not be seen for large problem instances due to a saturation effect, see Figure 6. As can be seen the cost has ﬂuctuations due to instable solutions of GA. For small-size problems, the optimality gap of GA is about 57%, while for LDA it is about 7% from global optimu m. For large-size problems, the performance of LDA remains the same, while that of GA increases to 70%. Intuitively, the reason is that with larger number of items, the problem becomes too difﬁcult for a simple algorithm such as GA. Figures 4 shows the impact of time slots for small-size problem instances. As can be seen, the cost increases with number of time slots. Apparently, this is because with more time slots, there are more requests to serve, and hence higher cost. GA has an optimality gap around 60%, while for LDA the gap is only 8%. The results for large-size problems are shown in Figure 7. LDA consistently shows good performance, whereas the results of GA are very sub-op timal. It is worth noting that the optimality gaps o f both LDA and GA s lightly increase with the number of time slo ts. Figure 5 shows the impact o f ρ on the total cost. Larger ρ means higher backhaul capacity. The costs of both LDA and GA decrease sharply when ρ increases from 10% to 20%, then the decrease slows down due to a saturation effect. The optimality gap of LDA is 17.5% when ρ = 10%. This is because when the backhaul capacity is extremely limited, very few content items can be updated in a time slot, and as a result even one or two sub-optimal choices would largely impact the performance. When ρ increases to 20%, the cost signiﬁcantly decreases and the optimality gap decreases as well to 7.8 %. For higher value of ρ, the gap sligh tly decreases further and stays around 7%. For GA the deviation from optimality is high no matter ρ is sm all or not. Similar trend s can be seen for large-size problems, see Figure 8. Note that in th e ﬁgure it may not be clear that the result of LDA and LBD both decrease with ρ. To show this, we have plotted a subﬁgure in the middle-right section of the ﬁgure. We have studied optimal scheduling of cache updates where AoI of contents and recommendation are jointly taken into account. With both AoI and recommendation, the problem is hard even for one single tim e slot. We formulated the problem as an integer liner program (ILP). The ILP provides optimal solutio ns, but it is not practical to large problem instances. Simple algorithms are not likely to be effective, and this ﬁnding is obtained via the poor performance of a greedy algorithm (GA). To arrive at goo d solutions efﬁciently, one has to analyze and exploit the structure of this optimization problem. We achieve this by the Lagrangian decomposition algorithm (LDA) that allows for decomposition for handling large-scale problem instances. LDA decomposes the problem into several subproblems where each of them can be solved efﬁciently. The algorithm provides solu tions within a few percentage from global optimality.