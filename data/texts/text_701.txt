<title>Adversarial Robustness of Deep Reinforcement Learning based Dynamic Recommender Systems</title> <title>arXiv:2112.00973v1  [cs.LG]  2 Dec 2021</title> strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several other crafting methods. Recommendation systems are an effective means of alleviating information overload for Internet users. They generally ﬁlter out those less irrelevant ones from massive items of choice to improve user experience in multiple scenarios. Traditional recommendation systems extract features about user preferences, items, and users’ past interactions with items to conduct content-based, collaborative, or hybrid recommendation (G.Adomavicius and A.Tuzhilin, 2005; Zhang et al., 2019). These models have not considered the changes in user preferences over time. In this regard, interactive recommendation systems emerge to capture personalized user preference dynamics. Generally, interactive recommendation systems cater to users’ dynamic and personalized requirements by improving the rigid strategy of conversational recommendation systems (Mahmood and Ricci, 2007; Thompson et al., 2004; Taghipour and Kardan, 2008). In recent years, they have been attracting increasing attention and employed in leading companies (e.g., Amazon, Netﬂix, and YouTube) for personalized recommendations. Interactive recommendation systems can be considered a decision-making process where the system chooses an optimal action in each discrete step to maximize the user response evaluation. Common practices to model the interactions between recommendation systems and users include Multi-Armed Bandit (MAB) or Reinforcement Learning (RL). The former views the action choice as a repeated single process while the latter considers immediate and future rewards to better model long-term user preference behaviors. In RL-based recommender systems, a Markov Decision Process (MDP) agent estimates the value function by both action and state rather than merely by action as done by MAB. However, small disturbances in the input data may fool the above practices (Szegedy et al., 2013; Goodfellow et al., 2014). Small imperceptible noises, such as adversarial examples, may increase prediction error or reduce reward in supervised and RL tasks — the input noise can be transferred to attack different parameters even different models, including recurrent network and RL (Gao et al., 2018; Huang et al., 2017). Besides, the vector representations of entity/relation embedding of the input of RL-based recommendation models make it challenging for humans to tell the true value or dig out the real issues in the models. Recently, Browne and Swift (2020) points out that counterfactual reasoning can be used to generate the adversarial samples. From the perspective of causal inference, counterfactual data can be generated by intervening in some aspect of input data. Both perturbations and counterfactual reasoning target the state space by introducing noise. Attackers can easily leverage such characteristics of embedding vectors to disrupt recommendation systems silently. Therefore, it is important to study attack and defense methods for RL-based recommendation systems. This work aims to develop a general detection model to detect attacks and increase the defense ability and robustness, which provides a practical strategy to overcome the dynamic ’arm-race’ of attack and defense in the long run. The problem is nontrivial due to three reasons. First, online attacks are inherently difﬁcult to track or predict. Second, man-in-middle methods can attack the interactions between recommendation systems and users in web applications, giving opportunities for malicious people to disrupt recommendation systems in either a white-box or a black-box way. Third, the huge number of actions in RL-based recommendation systems poses a barrier to detecting user feedback since the exhaustively numerous items and users embedding vectors are not feasible to ﬁnd the abnormal inputs. We propose an attack-agnostic detection model against adversarial examples for RL-based recommendation systems to overcome the above challenges. To the best of our knowledge, this is the ﬁrst work that focuses on the adversarial detection of RL-based Recommendation Systems. We make the following contributions: We systematically investigate different types of adversarial attacks and detection approaches focusing on reinforcement learning-based recommendation systems and demonstrate the effectiveness of the designed adversarial examples and strategically-timed attack. We propose an encoder-classiﬁcation detection model for attack-agnostic detection. The encoder captures the temporal relationship among sequence actions in reinforcement learning. We further use an attention-based classiﬁer to highlight the critical time steps out of a large interactive space. We empirically show that even small perturbations or counterfactual states can signiﬁcantly reduce most attack methods’ performance. Our statistical validation shows that multiple attack methods generate similar actions of the attacked system, providing insights for improving the efﬁcacy of the detection performance. RL-based interactive recommendation. Reinforcement learning is a popular approach to interactive recommendation. Traditional research applies Q-learning (Taghipour et al., 2007; Taghipour and Kardan, 2008) and Markov Decision Process (Mahmood and Ricci, 2009) to web recommendation and conversational recommendation problems. Mahmood and Ricci (2007) ﬁrst introduce reinforcement learning into interactive recommendation by modifying MDP. Since then, deep learning has inspired more interest in interactive recommendation. For example, Christakopoulou et al. (2018) employ reinforcement learning to improve feedback quality in interactive recommendation; Chen et al. (2019a) adopt policy gradient to improve the scalability of interactive recommendation. Adversarial attacks. Szegedy et al. (2013) ﬁrst ﬁnd that hardly perceptible perturbation can cause erroneous outputs of a convolutional neural network on image classiﬁcation tasks. Goodfellow et al. (2014) exploit this topic further and incorporate the Fast Gradient Sign Method to attack neural networks; they ﬁnd the strong linear component of neural networks have a relationship with adversarial attack success. Further studies involve selecting attack points (Papernot et al., 2016), untargeted (Moosavi-Dezfooli et al., 2016) and targeted attack models(Lin et al., 2017), and experimental comparisons of random noise and adversarial examples (Kos and Song, 2017). Speciﬁcally, Lin et al. (2017) design strategically-timed attacks and craft deceptive images to induce the agent to make the desired actions. Browne and Swift (2020) argue that counterfactual explanations produce adversarial examples in DL research, which modify the input to cause misclassiﬁcation of the network. Huang et al. (2017) explore the adversarial attack deep Q network in video game playing and conclude that retraining with adversarial examples can make the network more robust. Another thread of research models adversarial attacks into environments for robust adversarial training. They either regard the attack as a destabilizing force to break the balance of agents in 3D scenarios (Pinto et al., 2017) or develop adversarial agents in multi-agent tasks during reinforcement learning (Gleave et al., 2019). Generally, creating adversarial examples helps reduce the reward of on DQN and DDPG (Pattanaik et al., 2017), and a detection method can help better explore the potential of adversarial examples and make agents more robust in a dynamic process. Adversarial example detection. Many adversarial detection methods are vulnerable to loss functions targeted to fool them (Carlini and Wagner, 2017). Bendale et al. Bendale and Boult (2016) present OpenMax to estimate the probability of data input from unknown classes explicitly. Since then, researchers have proposed statistical approach (Hendrycks and Gimpel, 2016), binary classiﬁcation approach (Metzen et al., 2017), outlier detection approach (Grosse et al., 2017), and history queries-based approach (Chen et al., 2019b) to detect adversarial examples. Our work differs from Chen et al. (2019b) in exploiting the nature of reinforcement learning besides query-based black-box attacks. This section introduces the components of an RL-based recommendation system, attack techniques that generate adversarial examples, and our scheme to detect black-box adversarial attacks. Figure 1. Our proposed Adversarial Attack and Detection Approach for RL-based Recommender Systems. Interactive recommendation systems suggest items to users and receives feedback. Given a user ∈ U = {u , u , u , ..., u , a set of items I = {i , i , i , ..., i , and the user feedback history , i , ..., i the recommendation system suggests a new item . This problem can be regarded as a Markov Decision Process, which comprises the following: State ( ): a historical interaction between user and the recommendation system computed by an embedding or encoder module. ): an item or a set of items recommended by the RL agent. ): a variable related to user’s feedback to guide the reinforcement model towards true user preference. The reinforcement agent could be an Actor-Critic Algorithm that consists of a critic network and an actor network (Xian et al., 2019). The attack model may generate adversarial examples using either the critic network (Huang et al., 2017) or the actor network (Pattanaik et al., 2017). FGSM-based attack. We deﬁne an adversarial example as a little perturbation added onto the benign examples to reduce the cumulative reward of a reinforcement learning system. Suppose is a sequence of feature vectors piped into reinforcement learning model π(s can be a composition of embedding vectors of users, relations, and items (Xian et al., 2019), or a feature vector encoded user and item information (Chen et al., 2019a). Unlike perturbations on images or texts, can be large in interactive recommendation systems due to the enormous manual work to check the embedding vectors or feature vectors of massive users and items. We deﬁne an adversarial example as follows: where is the total reward of the recommendation agent, is the length of a time step, is the optimal policy learned by the training process, < l ) is a similarity metric that measures the distance between benign and adversarial examples. is commonly deﬁned as bounded perturbation, or |δ| (Carlini et al., 2019). The computation of determines the method of attack. We aim to build a model with the generalization ability to detect examples from unknown adversarial distributions. Thus, we adopt three attack methods to validate the detection model performance: FGSM (Goodfellow et al., 2014) and its variant (Huang et al., 2017), JSMA (Papernot et al., 2016), and Deepfool (Moosavi-Dezfooli et al., 2016). FGSM can be presented as follows: where is the loss function, is the critic function Q(s , a . Optimizing will lead to the critic value satisfying the Bellman equation. The FGSM method uses the gradient of the loss function, which can be computed efﬁciently, thus requiring a small amount of additional computation. To construct a detection model with the generalization ability, we train the detection model with FGSM examples and conduct the detection using other perturbation methods. We adopt the two norm variations in (Huang et al., 2017) and deﬁne the norm constraint of perturbations as follows: Attack with smaller frequency. The strategically-timed attack (Lin et al., 2017) aims to decrease the attack frequency without sacriﬁcing the performance of the un-targeted reinforcement attack. We formally present it below: = δ ∗ c ∈ {0, 1}, where is a binary variable that controls when to attack; d < T is the frequency of adversarial examples. There are two approaches to generate the binary sequence optimizing a hard integer programming problem and generating sequences via heuristic methods. Let , p be the two maximum probability of an policy π, we deﬁne c as follows, which is different from (Lin et al., 2017): In our experiments, we let the RL-based recommendation system have a peak probability at the maximum action to test the importance of the action to attackers using the above formula. In contrast to the above methods, Jacobian-based Saliency Map Attack (JSMA) and Deepfool are based on the gradient of actions rather than the gradient of value. One key component of JSMA is saliency map computation used to decide which dimension of vectors (in Image classiﬁcation is pixels) are modiﬁed. Deepfool pinpoints the attack dimension by comparison of afﬁne distances between some class and temporal classes. More details can be found in Papernot et al. (2016) and Moosavi-Dezfooli et al. (2016). Counterfactual Based Attack. Counterfactual can ﬁnd a similar version of the query input within some distributions, changing the model decisions and receiving a different classiﬁcation. This helps to explain why a speciﬁc classiﬁcation decision was made by a model and improve the interpretation of model boundaries (Yang et al., 2021), which is known as counterfactual explanations. Recent work reveals that counterfactual explanations produce adversarial examples in deep neural networks (Browne and Swift, 2020). Therefore, we propose to generate counterfactual data samples to be the counterfactual-based attack. Most of the adversarial examples are generated by adding perturbations. The counterfactual-based attack is recognized as one sub-type of adversarial examples, which is different from traditional perturbations. One of the majority differences is that the counterfactual-based attack is generated by causal reasoning. To capture the casual relationships, we introduce Structural Causal Model(SCM) M = hU, V, F i , given by a directed acyclic graph (DAG) G, where: , ..., U is a set of exogenous variables determined by unobserved and omitted factors. We assume that these noises are independent variables such that is independent of all other noise variables. , ..., V ) is a set of endogenous variables that are observed nodes in the DAG. , ..., f is a set of structural equations representing the set of edges in the DAG. Each represents a causal mechanism that expresses the value of as a function of the values of parents of in G and the noise term, that is V = f (P a(V , U ). To simplify counterfactual reasoning, we assume that the input states follow the Local Causal Models (LCMs) (Pitis et al., 2020). Hence, we assume the set of edges in satisfying the Structural Minimality hypothesis, stating that is a parent node of in if and only if there is a direct edge from to such that setting the value of will have a direct effect on through . With this assumption, a large subspace often exists for each pair of nodes ( P a(V , V ) in the DAG, in which two components are causally independent conditioning on a subset of parents nodes of V so that can be considered separately for training and inference. Speciﬁcally, given two states with the same local factorization, we ﬁnd the intersection of these two states. The intersection parts remain unchanged in the MDP process, representing the critical components containing user identiﬁable information. Leaving the critical components untouched, we produce a new counterfactual-based attack by swapping a subset of the other factors of two states under the assumption that two components are locally independent. The algorithm is given in Algorithm 1. This process can be interpreted by making intervention do(S ) = S on the Local Causal Models to obtain the simulation result (Pitis et al., 2020). The detection model is a supervised classiﬁer, which detects adversarial examples based on the actions of the reinforcement agent in a general feature space. Suppose the action distributions of an agent are shifted by adversarial examples (Section 4 shows statistical evidence of the drift). Given an abnormal action sequence a = π (a|s + δ) or a counterfactual action sequence, the detection model aims to establish a separating hyperplane between adversarial examples and normal examples, thereby measuring the probability p(y|a, θ) or p(y|π , s, δ, θ), where y is a binary variable indicating whether the input data are attacked. To detect the adversarial examples presented in the last section, we employ an attention-based classiﬁer. We ﬁrst conduct statistical analysis on the attacked actions whose result is shown in section 4. The detection model consists of two parts. The ﬁrst is an encoder to encode the action methods into a low-dimensional feature vector. The second is a classiﬁer to separate different data. We adopt this encoder-decoder model to make a bottleneck and ﬁlter out noisy information. The formulation of GRU is as follows: (6) att, hid = p = Sof tmax(W att + b where is the combined vector of action embedding and hidden states hid —we compute attention weights from embedding vectors and employ a liner unit to distribute probabilities to input time steps; is the output of encoder. The vectors processed through the attention layer is then piped into a linear unit with softmax to compute the probability of adversarial examples. The loss function is the cross entropy between the true label and corresponding probability, In this section, we report our experiments to evaluate attack methods and our detection model. We ﬁrst introduce the datasets and then provide quantitative evaluation and discussion on different attacks and our detection model. Figure 2. Comparison of three attack methods, , and on three datasets (from top to bottom): Amazon Beauty, Amazon Cellphones, and Amazon Clothing. We conduct experiments based on two reinforcement learning interactive recommendation systems. Following Chen et al. (2019a) and Xian et al. (2019) over the real-world dataset, Amazon dataset (He and McAuley, 2016). This public dataset contains user reviews and metadata of the Amazon e-commerce platform from 1996–2014. We utilize three subsets named Beauty, Cellphones, and Clothing as our dataset. We directly use the dataset provided by Xian et al. (2019) on Github to reproduce their experiments. Details about Amazon dataset analysis can be found in Xian et al. (2019). We conduct our attack and detection experiments based on Xian et al. (2019). We preprocess the dataset by ﬁltering out feature words with higher TF-IDF scores than 0.1. Then, we use 70% data in each dataset as the training set (and the rest as the test set) and actions of reinforcement agent as the detection data. We deﬁne the actions of PGPR (Xian et al., 2019) as heterogeneous graph paths that start from users and have a length of 4. The three Amazon sub-dataset (Beauty, Cellphones, and Clothing) contain 22,363, 27,879, and 39,387 users. To accelerate experiments, the ﬁrst 10,000 users of each dataset are extracted for adversarial example production. Users in Beauty get on average 127.51 paths. The counterparts for Cellphones and Clothing are 121.92 and 122.71. We adopt the action ﬁle of attack with an epsilon of 0.5 as the training set. As the number of paths is large, we utilize the ﬁrst 100,000 paths for train and validation. The ratio of train validation is 80/20. Regarding the test, 100,000 paths from each action ﬁle are randomly sampled as the test set. We slightly modify JSMA and Deepfool for our experiments—we create the saliency map by calculating the product of the target label and temporal label to achieve both effectiveness and higher efﬁciency (by 0.32 seconds per iteration) of JSMA; we also use sampling to decrease the computation load on a group of gradients for Deepfool. Besides, we set the hidden size of the GRU to 32 for the encoder, the drop rate of the attention-based classiﬁer to 0.5, the maximum length of a user-item path to 4 (according to Xian et al. (2019)), and the learning rate and weight decay of the optimization solver, Adam, to 5e-4 and 0.01, respectively. This section reports our experiments on adversarial attacks. The ﬁrst part shows the attack experiment results, followed by an analysis of the impact of attack frequency, attack intensity, and the action space of the recommendation system on the attack performance. Adversarial attack results. We are interested in how vulnerable the agent is to perturbation in semantic embedding space. We consider an attack to be effective if a small perturbation leads to a notable performance reduction. We experimentally compare the performance of different attack methods (described in Section 3) in Table 1. We reuse the evaluation metrics of the original model, namely Normalized Discounted Cumulative Gain (NDCG), Recall, Hit Ratio (HR), and Precision for evaluation on the amazon dataset. Table 1 shows the attack results share the same trend with the distribution discrepancy in Table 2. Most attack methods signiﬁcantly reduce the performance of the reinforcement system. FGSM achieves the best performance. It reveals that attacks on a single dimension can change the neural network’s action drastically. Compared with and methods, FGSM is less effective on three datasets, where the evaluation metrics are mostly same in contrast to the case without attack (The original baseline in Table 2). It is worth mentioning that counterfactual attack does not perform well as the others. One of the possible reasons is that the generated counterfactual state still falls in the original latent space. The counterfactual attack can introduce noise to the current state by introducing irrelevant information from future states. are not easily perceptible by humans, decreasing attack intensity might degrade attack effectiveness. To demonstrate the impact of different attack intensities in the context of RL-based recommender systems, we conduct the empirical experiment by varying the attack intensity, which is reﬂected by the parameter shown in Equation 2 and Equation 3. Experiment results of attack with epsilon variation of FGSM attack methods on three Amazon datasets (Figure 2) show that compared to a 0.0 value epsilon, all metric values decline as Epsilon increases, and attack achieves the best result. follows a similar yet more abrupt trend than the attack, while the attack achieves the worst performance regardless of the epsilon value. Huang et al. (2017) propose to attack the reinforcement learning applied on games such as Atari. Their experiments reveal that the attack achieves comparable performance as and attack do. To exclude the possibility that the might be more effective with larger epsilon values, we set to 20.0 to test if but the result is the same. This observation that the attack in user-item-feature embedding space shows different characteristics from attacks in the pixel space. Another interesting observation is that the metric values show different trends depending on the datasets— unlike on Beauty and Cellphones, the attack achieves comparable performance to on the Clothing dataset when the is larger than 0.3. The result on the Cellphones dataset shows that the effectiveness of the l attack diminishes as the  continues increasing beyond 0.1. Impact of attack frequency. We conduct two experiments on attack frequency, random attack and strategic attack. In the random attack method, the adversarial examples are crafted with a frequency parameter, . In the strategically-timed attack, the adversarial examples are generated by the method shown in Section 3.2. The NDCG metric is presented in Figure 3; other metrics have a similar trend. It can be seen from 3 that the random attack performs worse than the strategically-timed attack. Generating strategically adversarial examples one third to half time steps achieves a signiﬁcant reduction in all metrics. Analysis of adversarial examples. We use Maximum Mean Discrepancy as statistical measures of high dimensional data distribution distance. This divergence is deﬁned as: where is the kernel function, i.e., a radial basis function, which measures the distance between the means of two distributions (Table 2 shows the results); , X are benign and adversarial examples. The data is randomly sampled from generated action embeddings of interactive recommendation systems. Each MMD is computed by averaging 40 batches of 500 samples. The actions generated by the RL agent are paths of the users-relation-item graph. As mentioned in Section 4.1, each user gets over 100 paths, which Figure 3. NDCG of attack frequency on Amazon Beauty and Clothing. Dashdot lines represent random attacks, solid lines are strategically-timed attacks. Blue lines are FGSM attacks, green lines are FGSM attacks. determines the overlapping of original data and adversarial examples decrease with the time step of the path growing. We choose the embedding of the last step to represent the recommended items. MMD-org shows the discrepancy between the original and adversarial datasets, where MMDpresents the discrepancy between different attack methods. The results (Table 2) show that the adversarial distribution is different from the original distribution. Also, the disturbed distributions are closed to each other regardless of the attack type. This insight clariﬁes that we can use a classiﬁer to separate benign data and adversarial data, and it can detect several attacks simultaneously, which might be transferred to other reinforcement learning attack detection tasks. Detection Performance. From a statistical perspective, above analysis shows that one classiﬁer can detect multiple types of attacks. We evaluate the detection performance of different models using Precision, Recall, and F1 score. We adopt an attention-based network for detection experiments. The detection model is trained on FGSM attack with at 0.1 for all datasets. The results (Table 3) show that our detection model achieves better performance on attacks that cause serious disruption. The detection precision and recall rise as the attack is stronger. attack validates this trend, which shows that our model can detect weaker attacks as well. The result of detection on l attack can be reasoned with the MMD analysis shown above. High precision and low recall show that most adversarial examples are close to benign data, which confuses the detector. The attack with  = 1.0 validates our detector performs well yet achieves worse performance on other tests on the Cellphones dataset. Our model can also detect the counterfactual-based attack since the data distribution has been changed, verifying that our detection model can detect different types of attacks. Our results on factor analysis (Table 3) show that the detection model can detect attacks even under low attack frequencies. But the detection accuracy decreases as the attack frequency drops—the recall decreases signiﬁcantly to 40.1% when 11.8% examples represent attacks. Adversarial attacks on reinforcement agents can greatly degrade user experience in interactive recommendation systems, as an intervention on causal factors can result in a different recommended result. In this paper, we systematically study adversarial attacks on reinforcement learning-based recommendation systems by investigating different attack methods and the impact of attack intensity and frequency on the performance of adversarial examples. We conduct statistical analysis to show that classiﬁers, especially an attention-based detector, can well separate the detection data. Our extensive experiments show the excellent performance of our attack and detection models.