Parameter estimations are important steps in parametric statistical modeling. Estimators of parameters can be derived from the maximum likelihood approach, the plug-in methods, the moment methods, etc. Of course, by far Maximum Likelihood Estimators (MLE) are preferred because of the statistical meaning of its derivation. Moments estimators (ME)’s and (MLE)’s may exists without having closed-form expressions. When a MLE does no have a closed-from estimator, the ME is a backup solution for authors who wish to a clear idea of the estimation and a quick and more controlled ways of computation. Finding ME’s is an important step in modeling. However, deriving the related statistical tests is needed for accepting or rejecting hypotheses. For more that two parameters, it is more practicable to join the individual normal asymptotic laws for each parameters into one chi-square asymptotic laws which is qualiﬁed as omnibus following the Jarque-Berra chisquare asymptotic law. This motivates us to investigate asymptotic laws of ME’s estimators of as much as possible of usual and non-usual statistical laws. The found law should be validated by simulation studies before being proposed to potential users. For a large review of asymptotic estimations and statistical tests, we refer to Van der Vaart (2000), Billingsley (1968), etc. Especially, for methods including functional empirical process, van der Vaart and Wellner (1996) is recommended. However the main tool used here, but not limited to, is the function empirical process (fep) transformed into a instrument tools in Lo (2016) (see below). let us begin by giving a few words on that tool and next basic notation. In this paper, we used the fep tool to show direct and eﬃcient ways for deriving asymptotic statistical tests for moment estimators for a selected set of four probability laws. Four these laws, all the computations are given in details. Computer codes for simulations are also provided. We could have treated more statistical distributions. However, we wanted this paper to be a model for researchers who need asymptotic statistical tests. Later, we expect to compose a handbook which includes a great number of laws. The paper is organized as follows. We will close this introductory section by describing the lofep tool in Subsection 1.1 and, in Section 1.2 by showing how to derive omnibus chi-square tests from the Gaussian asymptotic theorems for distributions of more than two parameters. In Section 2, we expose the asymptotic laws of the moments estimators of gamma, uniform, beta and Fisher distributions. The proofs and the implementation of the fep tool on these distributions are stated in Section 4. In Section 3, we proceed to a simulation study on the asymptotic results and show that the omnibus chi-square tests work ﬁne for small samples. The codes used for the simulations are stated in an appendix from page 27. The paper ends with conclusions and perspectives in Section 5. 1.1. A brief reminder of the fep. Let Z pendent copies of a random variable Z deﬁned on the same probability space with values on some metric space (S, d). Deﬁne for each n ≥ 1, the functional empirical process by where f is a real and measurable function deﬁned on R such that which entails Denote by F(S) - F for short -the class of real-valued measurable functions that are deﬁned on S such that (1.1) holds. The space F , when endowed with the addition and the external multiplication by real scalars, is a linear space. Next, it remarkable that G and for (a, b) ∈ R Lemma 1. Given the notation above, then for any ﬁnite number of elements f, ..., fof S, k ≥ 1, we have where Proof. It is enough to use the Cram´er-Wold Criterion (see for example Billingsley (1968), page 45), that is to show that for any a =(a, ..., a) ∈ R, by denoting T=(G(f), ..., G(f)), we have < a, T> < a, T > where T follows the N(0, Γ(f, f)) law and < ◦, ◦ > stands for the usual product scalar in R. But, by the standard central limit theorem in R, we have where, for g =af, and this easily gives so that N(0, σ) is the law of < a, T > . The proof is ﬁnished. 1.2. Main notations in the fep. In the context of this paper, we use univariate samples X, X tion function (cdf ) F We will usually need the cumulants m ﬁned by and their plug-in estimators with the special case of the empirical mean X variance will be preferred to the plug-in estimator µ that any moment of order k ≥ 1 exists whenever it is used. The moment method in a parametric estimation related to the studied random variable having ` ≥ 1 parameters (θ sample {X of these equations r ∈ {1, ··· , `} being the equality between a cumulant or a moment of order k order h take equations between the ` whenever exists and statistics of the empirical cumulant or moments, is the vector moment estimator (ME). Once the ME’s are found, we will need the joint asymptotic law of the vector ˆθ. The tool of the fep will greatly help in that target. We will go beyond and derive chi-square tests as much as possible. , ··· , X} consisted in simultaneously solving ` equations, each where all order kare pairwise distinct. In general, it is simpler to The rest of the paper is organized as follows... 1.3. Chi-square law derivation. We are going to show how to derive asymptotic chi-square laws from moment estimators for at least two parameters. In each case below, we treat a two-parameter estimation problem. Suppose that the two parameters are denoted by a and b and their moment estimators are denoted by ˆaandˆb, n ≥ 2. We will get in each case a ﬁrst law in the form: as n → +∞, (1.3)n(ˆa− a),n(ˆb− b) Z, Z ∼ N(0, Σ), where σ= Σ, σ= Σand σ= Σ. From usual properties of Gaussian vectors, we have that, whenever det(Σ) = σσ− σ6= 0, (See for example Lo (2018), Proposition 12, page 150). By the continuous mapping theorem (see for example Lo et al. (2016), Proposition 03, page 34 ), we will have, as n → +∞, which, as n → +∞, leads to So, below, for each treated case, we will state two results according to (1.3) and (1.4). In that section, we are going to treat the following probability laws: (1)X ∼ γ(a, b), (2)X ∼ β(a, b), (3)X ∼ U(a, b) and (4)X ∼ F(a, b) These results are meant to be interesting examples for other cases not handing here. We stress that the techniques in Lo (2016) will be extensively used in the following. 2.1. Gamma laws γ(a, b) of parameters a > 0, b > 0. The gamma law γ(a, b) has the probability density function pdf Here are the results for the γ-law of parameters a > 0 and b > 0. Theorem 1. We have with f(x) =xexp(−bx)1with Γ(a) =xexp(−x) dx. 2.2. Beta law β(a, b) of parameters a > 0, b > 0. The Beta law has the following probability distribution function The expectation is given by and the second moment order cumulant is given by The moment estimators ˆaandˆbare solutions of the equations a/(a + b) = Here are the results for the β-law of parameters a > 0 and b > 0. 2.3. The Uniform law of parameters U(a, b) , a > 0 and b > a. The probability distribution function of the uniform law is given by Σ= Var(H(X)), Σ= Var(L(X)), Σ= Cov(H(X), L(X)), σ(m− 2µ + 1) + 2µ(1 − µ)(µ − m)(µ − 1) (σ+ µ − m) det(Σ) The moment estimators are the solutions of the equations where λ = 12/2. Here are the results for the Uniform-law. Theorem 3. We have with where and where λ = 12/2. 2.4. Fisher law F(a, b) of parameters a > 0 and b > 0. For a Fisher law with a and b degrees of freedom, the parameters are supposed to be integers. But in the general case, the probability density function has the same form and is associated to the quotient of two independent random variables Z are positive. The pdf is expressed as follows: Here are the results for the Fisher-law. We are going to describe our simulation works for one of studied distributions. Next we will explain their outputs and their interpretations. Finally, we will display results for all cases. Important scripts will be posted in the appendix 27. 3.1. Simulation works. . In all cases, we estimate two parameters. In the case of the γ(a, b) law, the moment estimators are denoted by achap and and bchap. We will have three parts. A- Computing the exact moments and other coeﬃcients of the estimators. (a) Before proceeding to the Monte-Carlo method, we have to computed the function H and L, demoted as bigH and bigL. (b) We proceed to numerical methods for computing EH(X), sigmaHexaC = Var(E)H(X), EL(X), sigmaLexaC = Var(E)L(X) and the exact co-variance SigmaHLexa = Cov(H(X), L(X), where X stands for random variable with the studied law (here a γ(a, b) law). The trapezoidal method algorithm is used for all integral computations here. In page 27, the related script is given under the title A1 - Computing exact coeﬃcients . Table gives exact values of the variances for diﬀerent pairs (a, b). B- Monte-Carlo estimation. (a) Fix a sample size n ≥ 2. Fix values to a and b. (b) Fix the number of repetitions B = 1000 (big enough to ensure the stability of outcomes). (c) At each repetition j ∈ {1, ··· , B}, we generate an sample of X of size n. Next (1) DA[j] = (2) DB[j] = (3) V H[j] = sd(bigH(X)) (4) V L[j] = sd(bigL(X) (5) V HL[j] = cov(bigH(X), bigL(X) In page 28, the related script is under the title A2- Script Monte Carlo works. C- Computing the empirical moments and other coeﬃcients. (a) Now, we have: (1) an estimate of Σ vector V H, (2) an estimate of Σ V H and (3) an estimate of Σ them as SigmaHEMP, SigmaLEMP and SigmaHLEMP. (b) We also have: (1) an estimate of Σ estimate of Σ by empirical covariance between DA and DB. We denote them as sigmaHSAMP, sigmaLSAMP and sigmaHLSAMP. In page 29, the related script for computing sigmaHEMP , sigmaLEMP, sigmaHLEMP, sigmaHSAMP, sigmaLSAMP and sigmaHLSAMP is given under the title A3- Over/under estimations of variances and covariances from the script A2- Script Monte Carlo works in page 28. In Tables 2, 3 and 4 display the quotients of empirical coeﬃcients over the true coeﬃcients, allowing to over or under-estimation, for three values of pairs (a, b). D- Statistical tests for Computing the empirical moments and other coeﬃcients. (1) Performance of the point estimation. From the script A2- Script Monte Carlo works in page 28, we can compute the mean error (ME), the mean absolute error (MAE) and the square-root of the mean square error (MSE)of Error type n=25 n=50 n=75 n=100 n=200 n=300 n=1000 the point estimations on a and b the R codes mean(DACHAP-a), mean(DBCHAPb), mean(abs(DACHAP-a)). mean(abs(DBCHAP-b)), sd(DACHAP-a) and sd(DBCHAPb). We report their values in Table 5 In page 29, the scripts A4- Computations of the p-values, for each parameter a and b, we compute the empirical p-values for each sequence, as the frequency of element of the sequence exceeding 1.96. The test is satisfactory if that p value is less of around 5%. The diﬀerent p-values for a = 2 and b = 3 are given for diﬀerent values of n in Table 6. To test the quality of the normal approximations, we display the QQ-plots and the Parzen estimators graphs for each parameter in Fig 1 (QQ-plots and Parzen estimators related to the parameter a for n=50, according to the type of estimation of the coeﬃcients), in Fig 2 (QQ-plots and Parzen estimators related to the parameter b for n=50, according to the type of estimation of the coeﬃcients),in Fig 3 (QQ-plots and Parzen estimators related to the parameter a for n=300, according to the type of estimation of the coeﬃcients) and in Fig 4 (QQ-plots and Parzen estimators related to the parameter b for n=300, according to the type of estimation of the coefﬁcients) in Appendix C (Page 32). (E) Omnibus test. We mean by omnibus test that the combine both test into a chi-square test as in Part (b) of each of Theorems 4, 3, 2 . Depending on the use of exact values, empirical values or sample values of the variance of co-variances, we have three statistics that can be used each for the chi-square test: QEMPDB[j] = QEMPDB[j] = Table 7 provides the p-value related to the omnibus test for diﬀerent sizes according to the estimations of the coeﬃcients used. The related test is given in the script under the title A5- p-values for the omnibus statistical test in page 30. (D) - Conclusions of recommendations from simulations. The simulation studies show that the omnibus statistical test is very good even for sizes as small as n = 50 for all estimations of the coeﬃcients in the test statistics. When we do Gaussian separate tests for a and b, the outcomes are remarkable in the use the variance and covariance of QExa[j] =sigmaLExaC(achap[j] − a) n(b− b). This is observable in the QQ-plots, the Parzen graphs and in the p-values of the tests. The separate tests seem to recommend the tests when n is bigger than 100. But, deﬁnitively, the omnibus works ﬁne for small sizes as n = 11 with p-values 5.6%, 0%, 1.7%. We strongly suggest to no use the tests with empirical estimations of the variance and covariance which lead to severe under or over estimation. Here, we provide the computations for each treated probability law. 4.1. Gamma Law γ(a, b) of parameters a > 0, b > 0. We have and By equations (4.1), (4.7) and by lemma 11 in Lo (2016), we have where 4.2. Beta Law β(a, b) of parameters a > 0, b > 0. The moment estimators = µ (µ − m) + nG((µh− µh) + (µ − m) h) + On. 4.3. Uniform Law U(a, b), of parameters a > 0 and b > a. We have and 4.4. Fisher Law F(a, b) of parameters a and b. The moment estimators are deﬁned below. The ﬁrst moment estimator is So we have, We also have where So the denominator (4.17)/(4.19) is expanded as where Hence where Then Hence denom = (2 − µ)σ− µ(µ − 1) + nG(L) + o, Moment estimators for four statistical distributions been studied through their asymptotic Gaussian laws with the help of the fep tool. Chi-square omnibus tests have been derived for each distribution. The results have been simulated and the chi-square tests revealed themselves eﬃcient for small sample sizes. The R codes of the simulations are attached to the paper in an appendix. The main perspective is to develop a full chapter in with the study of a large number of distributions. Acknowledgment. The authors Niang and Ngom express their thanks to Professor Lo for guidance, and moral and ﬁnancial assistance.