In the past, people used to live with their own relatives in small societies and present gradual introduction patterns. With the exponential population growth, large societies with frequent interactions between strangers appeared [1]. Then, with the enhancements in technology and the crucial place of world wide web in our lives, standards in human communication improved. We can inte ract with people thousands of kilometers far from our location in a few seconds. This opportunity reduced the value of a single communication. People want to meet more people in shorter periods, a nd try to get as much information as possible soon. We as humans usually beneﬁt from the power of this connectivity. For example , students and experts exchange information with like-minded people with the help of social media [2]. Yet, getting to know other people and evalu ating their characteristics can be tricky especially with the rush in our era. It is a popular claim that people need to be careful about their interactions with strangers and should not trust them easily. In this work, we assert that being too skept ical can also be harmful against cooperation possibilities in Prisoner’s Dilemma game. M oreover, optimism can bring a more successful view of people to some extent. 2.1 Prisoner’s Dilemma Prisoner’s Dilemma is a simple game to evaluat e the success of v arious game strategies by trying them against an opponent whos e acts are unknown and unpredictable [3]. Two agents playing Prisoner’s dilemma are expected to choose between defecting and cooperating. According to their choices, they are awarded or punished. If an agent cooperates while the other defects, the cooperator get s the sucker payoff S and the defector gets the temptation payoff T . If both players choose to cooperate, then they both get the reward payoff R. In t he case of mu tual defection, both players get the punishment payoff P . In Prisoner’s Dilemma game, the payoffs should satisfy both S < P < R < T and S + T < 2R [3]. Cooperation is a costly sacriﬁce and surprisingly it survived against mutation and selection in the nature [4] . For one round, it is a lways better to defect without regarding the act of the opponent agent if there is no speciﬁc modiﬁcation as in The Expected Prisoner’s Dilemma [5] . However, in real life we come up against the consequences of our previous acts. Therefore, Iterated Prisone r’s Dilemma(IPD) is introduced. Iterated Prisoner’s Dilemma models are often used to explain sentient human behaviors [6]. In the m odel, agents play with each other consecutively and the result is determined according to the sum of consecutive rounds. Agents are able to remember their opponents ’ previous decisions. Then they decide by ta king the past rounds into account. This generally promotes the power of cooperation since P < R and reciprocal trust yields better results than reciprocal distrust. Yet, IPD with 2 people (2 IPD) is insufﬁcient to represent most of the real-world problems [7]. Hence we use a system with N agents in our experiments. 2.2 IPDwRec Iterated Prisoner’s Dilemma with recommendation (IPDwRec) [8] is a model developed to simulate an environment, where agents play with each other in several combinations and make use of recommendations in their choices. We built our model over IPDwRec by appending some additional terms and procedures. 2.2.1 Population Iterated Pris oner’s Dilemma ha s some well-known strategies such as TitForTat, Grim, and Pavlov [9]. In thes e strategies, agents have the total control of their behaviors. However, IPDwRec is different. The population of IPDwRec consists of N self-interested agents [10]. Half of the agents are cooperators and the other half are defectors. Cooperation and defection are vulnerable to an error with rate of ǫ. Hence, a cooperator is expe cte d to cooperate with probability of 1 − ǫ and a defector is expected to cooperate with probability of ǫ. Agents have a ﬁxed memory size of M . Memory ratio µ = M/N determines the proportion of popu lation s ize that an agent can store in its memory. Agents are assumed to store recently played agents in their m e m ory. Thus, a n agent needs to forget one of the agents in its me m ory if the memory is full a nd the recently played opponent was not in the memory before. There a re three strategies for forgetting: (i) Forget cooperators (FC), where the agent to forget is chosen randomly among agents perceived as coopera tors , (ii) Forget defectors (FD), where the agent to forget is chosen randomly among agents perceived as defectors, (iii) Forget random (FR), where the agent to forget is chosen randomly among all agents. Agents decide whether an opponent is a defector or cooperator by calcula ting a perceived cooperation ratio. It is a measure of one-sided trust between agent s. Suppose agent j is an agent in the memory of agent i. Then deﬁne tthat represents the perceived cooperation ratio by agent i for agent j as where cis the number of time s that j cooperated a gainst i and dis the number of times that j defected against i. This formulation is derived from Laplace’s Rule of Succession [11]. The formulation helps injecting trust proportional with the number of samples. For instance, when (c, d) = (1, 0), t= 2/3. tincreases t o 3/4, 4/5 and so on as the observed number of cooperation increases. Recommendation is the most signiﬁcant feature of IPDwRec when compared to classical IPD. Recommendation is provided with the following p rocedure: i. Agent i is matched with an opponent j who does not exist in its memory. ii. Then agent i inquires other agents in its memory about agent j. The ones who keep information about agent j in their memories are called members of the set of recommenders (R). All agent k’s where k ∈ Rshare the perceived cooperation ratio (t) with a gent i. For instance, if agent i remembers agent k and, agent k remembers agent j, then agent k pas ses tto agent iii. Agent i collects all ratios and evaluates to make a decision. Agents have three types of inna te disposition: (i) Optimism, where agents take t he best recommendation into account, (ii) Realism, where agents t ake the mean of recommendations, (iii) Pessimism, where agents consider the worst recommendation as the cooperation ratio. 2.2.6 Perception Graph Perception among the agents can be modeled with a weighted directed graph where the vertices are agents. Directed edges between two vertices are formed when two agents i and j are ma tched and played with each other. Weights of the edges are determined by perceived cooperation ratios. An edge from i to j has weight of tand an edge from j to i has weight t. Both of these e dges are formed when agents i and j play a game. An edge remains still until the agent who is the source of the edge forgets the destinat ion agent. Yet, the edges do not hav e to maintain symmetric connectivity. An agent i may forget agent j and the edge from i to j can be broken whereas the edge from j to i remains if agent j still remembers agent i. Recommendation helps agents to retrieve information about their opponents at distance of two edges. 2.2.7 Decision to Play Agents are matched uniformly at random to play with each other. A matched agent may reject to play. Cooperators, having high probability to cooperate, can only protect themselves from defectors by refusing to play with them. Therefore, they need to be able to detect their opponents’ characteristic by their memory or recommendation. If both of the matched agents decide to play, they pla y a game. Otherwise, the game is canceled. The procedure of decision to play is like the following: i. If agent i remembers agent j, it calcula tes t. If t> 0.5, i decides to play; otherwise rejects. ii. If agent i does not remember agent j, it inquires recommendation. Agent i calculates twith the recommendation procedure explained in Sec.2.2.4 and according to its disposition. If t> 0.5, i decides to play; otherwise rejects. iii. If no recommendation is received, agent i decides to play. Our model introduces some new features and modiﬁes some rules in IPDwRec [8] to analyze the system from a more realistic point of view. 3.1 Defectors We m odify the behavior of defectors. (i) In our model, defectors always play. In IPDwRec [8], if agent perceives its opponent as defector, it rejects to play. This behavior is correct for cooperators, but meaningless for defectors. For a nonnegative payoff matrix, a defector has nothing to lose even if the opponent is a defector [12]. (ii) Moreover, defectors do not give recommendations since it is not realistic for them to help cooperators to detect defectors. 3.2 Recommendation Evaluation In IPDwRec, realist agents calculate tfrom recommendation as We realized tha t with this approach, the ratios received from all recommenders have the same we ight . Thus, the ones who played with j many times are not separated from the others, although their recommendation is more trustworthy. In our model, agents calculate tfrom recommendation as the following: where cis the number of times that j cooperated against k and dis the number of times that j defected against k. 3.3 Optimism Threshold In IPDwRec model, agents decide to play if t> 0.5. In our model, we relax this by deﬁning a new parameter optimism threshold, denoted by α. An a ge nt i plays with j if t> α. By deﬁning optimism threshold, we aimed to adjust the method of asserting dispositions. For instance, agent i with α= 0.3 is considered to be m ore optimist ic than agent j with α= 0.5. Consider following: Agent j defects against i in ﬁrst gam e . t= 1/3 ≈ 0.33 is calculated. If α= 0.5 > 0.33, agent i rejects j in the se cond game. I f α= 0.3 < 0.33, i gifts a second chance to j. Note that α = 0 for defectors since they always play. New model does not include the dispositions of optimist and pessimist, α determines disposition. 3.4 Tolerance In IPDwRec m odel, agents make the decision to play after the ﬁrst game immediately. Consider the case where two cooperators i and j are played and j defected. Agent i considers j as defector and never plays with it as long as j stays in its memory. In our model, we introduce tolerance β for cooperators, which is the number of games cooperators must play with an opponent before rejection. Agents deﬁne their perceptions abou t their opponents as undeﬁned before the t olerance ends. Undeﬁned implies that a ge nt is not well-known enough, therefore, no bias should be shown against it. If perception about an opponent is undeﬁned, the opponent is not rejected. Note that tolerance is not applied if recommendation is inquired, and calculated t< α. This is because recommendation evaluation accumulates information from s e veral agents. In this case, op ponent can be considered wellknown. 3.5 Metrics 3.5.1 Payoff Ratio We use the same metric deﬁned in Ref [8] for payoff ratio. Payoff ratio is used to evaluate performances cooperators and defectors. As stated in IPDwRec model, average payoff of a s e t of agents A is deﬁned as where pa yoff(i) is the total payoff collected by agent i among all games played by i. Now call the set of cooperators in the population as C and defectors as D. We deﬁne payoff ratio as where φ > 1 means cooperators are more succes sful then defectors. 3.5.2 Misjudgments The metric for m isjudgments is similar t o the one deﬁned in Ref [8]. There are two possible misjudgments done b y cooperators: (i) η: number of times when cooperator opponent is perceived as defector, (ii) η: number of times when defector opponent is perceived as cooperator. Note that the defectors do not need to judge their opponents since the y always play. 3.5.3 System Knowledge To evaluate the system knowledge, we collect the data from the memories of all the cooperat ors at the end of the simulation. A cooperator i ∈ C keeps agent j in its memory, which is denoted by j ∈ M. That is, cand dcounts will be stored by i. Since agent j can be cooperator or defector, there are four cases. Considering j being a cooperator, i.e., j ∈ C, the number of cooperations and defections by all the cooperators are Similarly, the number of cooperations and defections done by defectors, i.e. , j ∈ D, are given as Note that these counts include only the numbers kept in memories and excludes the forgotten data. As a measure system knowledge, perceived cooperation ratios for cooperators and defectors in the system are deﬁned as In a well informed system, one expects that t≈ 1 − ǫ and t≈ ǫ. We used payoff matrix of (S, P, R, T ) = (0, 1, 3, 5) in our simulations. In each realization, we used population size of N = 100, where 50 agents are cooperators and 50 agents are defectors. We set ǫ = 0.1. Thus, cooperation probabilities are 0.9 and 0.1 for cooperators and defectors, respectively. A realization includes τgames where in each game two agents are picked randomly at uniform and offered to play. Therefore, any pair of agents are matched to play τ times on average. We choose τ = 30 for ou r realizations. Note that, matched age nts play if none of the parties reject. We call memory ratio, optimism threshold, tolerance, and forgetting strategy as mode l parameters. All cooperators have the same model parameters in a simulation. We report the ave ra ge of 30 realizations to narrow down the conﬁdence interval of model responses. Fig. 1: Payoff ratios φ for different memory ratio µ and optimism threshold α pairs for three forgetting strategies. (β = 1). Fig. 2: Payoff ratios φ for different memory ratio µ and optimism threshold α pairs for tolerance v alues β = 2 and 3, where forgetting stra tegy FC is applied. Note that β = 1 is given in Fig.1a By introducing optimism threshold and tolerance, we had the chance to examine new dispositions and their effects on cooperator performances. As an overview, we have the following observations. In Fig. 1, where β = 1, we have payoff ratios for different forgetting st rategies. One obs e rve s that cooperators perform better for lower va lues of α in a ll strategies. First, we saw that different misjudgments cause different patterns. Perceiving a defector as a cooperator is safer than perceiving a cooperator as a defector. In the ﬁrst case, agent keeps on playing with the opponent perceived as cooperator. After a couple of defections, it realizes the opponent is a defector and stops playing. Hence, the misjudgment is recovered. In the second case, the blacklisted cooperator can not obtain a new chance t o prove that it is not a defector. Fig. 3: Number of cooperators in mem ory with µ = 0.3 for different optimism thresholds α, in forgetting st rategy FC. (β = 1) There is no recovery of this misjudgment unless the agent is forgotten. α being low causes the ﬁrst type of misjudgment whereas α being high causes the se cond one. Therefore, low α brings better results than high α. Second, FC and FD strategies suffer from misjudged opponents. A cooperator labeled as a defector is not forgotten easily in FC strategy. In FD case, the opposit e occurs. A defector labeled as a cooperator is not forgotten easily. FR offers a uniform process that eliminates biases and leads to a more accurate system knowledge. If β > 1, as in Fig.2, agents have u ndeﬁned perceptions. If most of the agents in memory are perceived as undeﬁned, they a re forgotten ra ndomly. FC and FD turns into FR implicitly in this case. This is beneﬁcial because less known opponents should not be subject to biased decisions. Although FR has advantages in s ome cases, FC is more successful when it is su pported with low optimism threshold or tolerance and forgetting biases are prevented. Finally, we realized that t olerance has more advantages when ǫ 6= 0 since it resolves the misunderstandings usually. When ǫ = 0, there is no need for tolerance since agents can be sure that their decision after the ﬁrst game is deﬁnitely correct. Being optimistic was considered to bring vulnerability according to [8], yet there are some adva ntages as we will explain. 5.1 Success of Low Optimism Thresholds First, we analyzed the results for all optimism thresholds without introducing tolerance, i.e., β = 1. Fig.1 presents an overview of the effects of different optimism thresholds combined with different memory ratios. Note that IPDwRec [8] worked on the area of α = 0.5 in Fig.1, and we ex pand the debate here. First outcome is the sudden drop in payoff ratios when α ≥ 0.7. The reason is the following: Suppose agent i is matched with agent j for the ﬁrst t im e and no recommendation is received. Then, i has to play. Even if j cooperates, (c, d) = (1, 0) leads to t= 2/3 ≈ 0.66 < α. That is, independent of cooperation or defection of j, i will keep rejecting j until it forgets j. Therefore, α ≥ 0.7 represents a meaningless pess imism. Second, α ≤ 0.1 means a meaningless optimism since cooperators fail to be selectiv e . For instance, i will still accept to play with j despite 8 defections, since (c, d) = (0, 8) leads to t= 1/10 = 0.1 ≥ α. Third, cooperators are usually success ful when 0.2 ≤ α ≤ 0.6. The interesting result is that the most successful results are obtained when α = 0.3. This is going to be investigated. 5.1.1 µ ≥ 0.6 Since 50% of t he agents are defectors, theoretically a cooperator with µ ≥ 0.5 has enough memory to keep all defectors but this is not the case in practice. Cooperators also keep other cooperators in their memories. Yet, we observed that when µ ≥ 0.6, agent memories have empty spots at the end of the simulation. This is consistent with the results in [8]. Recall that forgetting occurs if memory is full. Then forgetting occurs rarely when µ ≥ 0.6. Therefore forgetting strategies ha ve only minor effects when µ ≥ 0.6. As expecte d, the right parts of the plots in Fig.1 are similar to each other. Optimism threshold α = 0.3 brings the best results in each of the forgetting strategies in Fig.1. This is due to the recovery of misjudgments. Suppose agent j defected once. Then we ha ve 0.3 < t= 1/3 < 0.5. Consider α = 0.3 and α = 0.5 cases. (i) If α = 0.3, j will get another change when they are matched again. If j cooperates, i will keep playing with j. If j defects, twill drop below α = 0.3 and i will stop playing with j. Hence, ηwill not increase more. (ii) If α = 0.5, i will not give another chance to j. If j is a cooperator who defected in the ﬁrst game with error, its rejection will boost ηand this misjudgment cannot be recovered unless j is forgotten. Hence, recovery of misjudgment is more possible when defectors are perceived as cooperators. By looking at Fig.4d, Fig. 4e, and Fig. 4f, where µ = 0.6, one can see that minimum number of misjudgments appear to be at α = 0.3. 5.1.2 µ < 0.6 For µ < 0.6, the effect of optimism thresholds depend on forgetting strategy. Forget Cooperators. As Fig. 1a shows, α = 0.3 brings the best results for µ < 0.6 too. To understand this, we collected data from m e m ories of all the cooperators at the end of the simulation. Fig.3 shows the downside of FC. Perceived cooperat ors are forgotten but cooperators perceived as defectors are kept in m e mory. The number of perceived cooperators is closer to the number of actual cooperators in low thresholds as seen in Fig.3. Hence, it is essential to decrease α to decrease ηwithout increasing η. As shown in Fig.4a, misjudgments are optimized when α = 0.3. Forget Random. Unlike FC strategy, FR allows agents to forget independent of their types . That is, agents in memory have equal changes to be forgotten. Hence, cooperators who are perceived as defectors do not remain in memory for long time, which keeps ηunder control. As observed in Fig.4b, the optimal tradeoff converges to α = 0.5. Forget Defectors. FD strategy is dangerous when agents do not keep sufﬁcient defectors in their memories and become vulnerable to them. Thus, when µ < 0.5, cooperators can not defeat defectors as seen in Fig. 1c. 5.2 Success of Tolerance As we explained in S e c.3.4, IPDwRec model has no term called tolerance. Agent s play with their opponent once and decide on the characteristic of the opponent. This is the situation when β = 1. In this section, we are going to evaluate the effects of tolerances β = 2 and β = 3. For simplicity, FC is going to be used. Yet, the effect of t olerance is similar among all forgetting strategies. 5.2.1 Extreme Thresholds In Fig.2, payoff ratios φ are given as a function of memory ratio µ and optimism threshold α for tolerance values β = 1, 2, 3. As previously discussed in Sec.5.1, α > 2/3 is meaningless for β = 1 case since even if opponent j cooperates in the ﬁrst game, tbecomes 2/3. Similar discussion leads that α > 3/4 and α > 4/5 are meaningless for β = 2 and β = 3, respectively. Hence, increasing β also expands the meaningful region of α. 5.2.2 Low µ values Generally, when agents have low me mory rat ios, cooperators suffer failures due to forgetting biases. This causes the system to keep inaccurate data. Tolerance, by forcing agents to give more chances to their opponents, prevents misjudgments generally. There are two main reasons for this: First, agents t olerate the ﬁrs t game errors and, assuming that ǫ is small, the probability for an agent to make a move with error two ga mes in a row is low. Second, when agents are matched k times, if k < β, they keep each other in a n undeﬁned list, neither as cooperator nor as defector. Then if an agent needs to forget an agent, it ﬁrst looks at cooperators list or defectors list depending on the forgetting strategy. If the preferred list is empty, agent p icks an agent from undeﬁned list to forget. Since undeﬁned list is a random collection of agents, both FC and FD strategies act like FR strategy. To sum up, tolerance makes agents uncertain about less known agents, which leads to random forgetting. FR strategy e liminates bias. One anomaly that cooperators encounter can be shown in Fig.5a. When β = 1, a nd FC strategy is applied at µ = 0.2, φ drops signiﬁcantly to around 0.55. Similar drop of φ around µ = 0.2 is obse rve d in ref [8]. Table 1 can be used to study this problem. Table presents statistics from simulations where α = 0.5 and µ = 0.2. Two rows compare the case s of β = 1 and β = 3. The statistics in the table are calculated at the end of the simulations by getting data from memories of cooperators. The values are for an average memory. Actual and perceived characteristics of agents in the memory are given. Undeﬁned means the number of games played with an agent is lower than β yet, t hus no perception appears about the agent a nd tolerance will be applied. The difference in ηstatistics implies that cooperators refuse to play with each other frequently if β = 1. Wrong perception is the reason for this. For µ = 0.2, an agent can keep 20 agents in its memory. There are a pproximately 7 actual cooperators and 13 actual defectors kept in the memory whereas only 0.739 (on average) cooperators are perceived as cooperators. Then an average cooperator can said to refuse m ost of the cooperators in its memory. t being 0.211 instead of 0.9 proves this wrong perception. The improvement is clear in the latter row where β = 3. Although almost no one is perceived as cooperator, approximately 8 agents are kept as undeﬁned in memory, which means those 8 agents will not be rejected when matched. Being tolerant moved cooperators that perceived as defectors to t he undeﬁned list, which can be seen as giving more chances to the agents who are not surely defectors. 5.2.3 High µ values Fig.5a shows φ as a function of µ when β = 1, 2, 3. The plots of β = 2 and 3 are higher than that of β = 1 for high values of µ, too. Even though there is no bias due to forgetting or lack of memory when µ ≥ 0.6, being tolerant is better. This is because of error denial. Approximately 0.1 of the cooperators defect and 0.1 of the defectors cooperate in the ﬁrst game since ǫ = 0.1. After that, their opponents will always perceive thos e agents incorrectly when β = 1. Because of that they will not gift a new chance and t hey will never forget. To test this hypothesis, we can take a look at t he results in Fig.5b where ǫ = 0, which means cooperators are pure cooperators who never defect and defectors are pure defectors who never cooperate. It can be claimed that toleration have no beneﬁt when µ ≥ 0.6 and ǫ = 0. Then we conclude that tolerance repairs the issues due to errors when µ ≥ 0.6. 5.3 Tradeoff Between Threshold and Tolerance Both optimism threshold and tolerance have effects on the dispositions and performances of cooperators. Having a low optimism threshold can be interpreted as being optimistic as cooperators give se cond chances to their opponents even they face with a defection in the ﬁrst game. Having high tolerance also makes agents more opt imistic because agents keep the hope that the opponent can still be a cooperator until tolerance ends. Fig. 5: Payoff rat ios φ for different m e mory ratios µ for three tolerance values β, where optimism threshold α = 0.5 and FC applied. Although being op timistic with these terms increased the performance by eliminating biases and incorrect perceptions, applying b oth of them did not turn up to be successful after a while. Fig.6 shows that there is a tradeoff between increasing β and decreasing α. The red areas show high success and the positive sloped pattern of red areas indicate that optimism threshold and tolerance should be considered together. If α is too low, increasing β can destroy the performance of cooperators. Hence, to increase β, α should also be increased. Being more optimistic in sense Fig. 6: Payoff Ratios φ for tolerance β and optimism threshold α pairs when FC applied and µ = 0.3. of tolerance requires being more pessimistic in sense of threshold. In this work, we aimed to provide new disposition parameters like α and β and analyze the effects. But we kept those parameters same within the simulation and compared simulations among e ach other. To reach more concrete conclusions, an evolutionary approach can be applied with a system in which agents differ in α and β. Optimal α and β can be obtained with surviving parameter. Moreover, agents can change their own α and β in time depending on some conditions as memory status or number of games played. This ﬂexibility can lead to a more successful performance. We imported the same forgetting strategies as in [8] to our model. Different forgetting strategies can also be considered on this model. Additionally, effects of system parameters as τ, N and ǫ can be investigated. If we increase τ , game becomes longer and higher β may be required. Also, we observed the difference bet we e n ǫ = 0 and ǫ = 0.1. If ǫ be comes higher, optimal α and β values can change. Playing with the parameters can be interesting. We introduced new parameters to iterated prisoner’s dilemma game with limited memory to inject more precise human dispositions into the system. Defectors always play and do not provide recommendations. This makes the life of cooperators more difﬁcult compare to the previous studies. Yet, cooperators can be successful in getting higher average payoff ratios than defectors if they are more opt im istic. Namely, if they forgive the ﬁrs t defections by means of tolerance, op timism threshold or both. We related this result to misjudgments and revealed that perceiving a cooperator as a defector is more dangerous than perceiving a defector as a cooperator. Moreover, FR strategy is better a gainst less known agents since FC and FD strategies cause bia s, which may be unfair if the agents are not well known. Since even a small probability of error ǫ = 0.1 results in incurable misjudgments, acting tolerant and unbia sed is necessary to obtain a more accurate perception for cooperators.