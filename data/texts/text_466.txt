<title>Learning to Recommend Using Non-Uniform Data</title> Recommendations are now ubiquitous in various settings such as streaming services, e- <title>arXiv:2110.11248v1  [cs.IR]  21 Oct 2021</title> product recommendations. Personalized recommendation, an eﬀort to suggest to users dif- advertisements and top-seller lists (Linden et al. 2003). Personalized recommender system is powered by methods that can estimate user pref- single user. Such is the main concept behind a class of commonly used methods called col- techniques approach the recommendation task as one of recovering a matrix from its par- to items and the entries contain user preferences. Most of these matrix completion methods have theoretical guarantees under the assump- preferences, so as to improve prediction accuracy. sampling process in the prediction task. Let us consider an example of a popular matrix com- is to predict observed entries close to the observed values and at the same time penal- rows and the columns. However, as pointed out by Foygel et al. (2011), such ‘margin-based’ weighted correction becomes suboptimal when the sampling scheme is not a product distri- analysis shows that the margin weighting strategy can be further improved. Foygel et al. guarantees. benchmarks on synthetic and real data. penalized regression and marginal weighted-trace-norm penalized regression as special cases. We then prove an upper error bound for this uniﬁed weighted objective under general non- uniform sampling scheme. This error bound is on how accurate the estimation of the under- data and real-world data. algorithms). and Li (2010) and references therein for further discussion on this. for collaborative-ﬁltering-based recommender systems (Ramlatchan et al. 2018). Some pop- most of them assumed the restrictive uniform or margin-based sampling scheme while design- ing their algorithm as well as providing theoretical guarantees. (2012) study theoretically the marginal weighting strategy proposed by Srebro and Salakhut- and Hamidi and Bayati (2019). We note that Hamidi and Bayati (2019) consider the trace- problem, their approach is equivalent to the uniform sampling scheme. We will be working under the regime of missing completely at random (MCAR) data, trace-norm penalized regression with MNAR and the reference therein. The rest of this paper is organized as follows. We describe the problem formulation in Sec- ‘NU-Recommend’. In Section 4, we provide a proof sketch for our main theorem. Finally, presented in Section 5. Proofs are relegated to the appendices. We use bold capital letters (e.g. B) for matrices and non-bold capital letters for vectors (e.g., indices k ∈[d ] correspond to items. We will use rating data as a running example. That is, also some additional structures we impose on B to make it recoverable. variance at most σ . For any positive integer m, e (m), e (m), .. ., e (m) denote the standard basis vectors for R . Deﬁne the design matrix X := e (d )e (d ), then B = hB , X i, hB , B i:= Tr(B ) . following form: [X(B)] := hB, X i. Y = X(B ) + E. is that of estimating B , having observed Y and the design matrices X , i ∈[n]. sampling matrix such that entries in P are not necessarily equal to each other. Equivalently, We further assume that each element is sampled with positive probability and the proba- bilities are bounded. item, denoted by R ∈R and C ∈R respectively. They are also called row marginal proba- sampled and C is the probability that item k is sampled, and = 1, = 1. and C . However, this sampling model is limiting P as a product distribution, i.e. a rank-1 sampling matrix, which is a rather restrictive assumption. Consider the follow- probability of observing each entry is 0.25, thus each entry is assumed to be uniformly sam- than only assuming the marginal probability. so that there are very few scattered observed entries of this ground truth matrix. That is, system of equations with many solutions which would overﬁt the observed data and the noise. In many instances, the matrix we wish to recover is known to be low-rank. We recall that, order (up to logarithmic factors) to control for the recovery error. Hence a small r, i.e. low- rank structure entails that only a few factors contribute to a user’s tastes or preferences. many methods have been designed. This structure can be very useful while estimating the sampling distribution. single one is observed. Therefore, the recoverability of a matrix also depends on its spikiness. of spikiness by the following form: spikiness of B := will use this notion of spikiness and provide speciﬁc form of N(·) in our analysis. Our task of estimating B based on the noisy observations Y is combined with the assump- is the Frobenius error: convergence properties of the L (Π) error: where kB k := E[hB, Xi ], in which the expectation is taken with respect to a proba- entries. The weighted matrix completion algorithm NU-Recommend we propose builds on the afore- mentioned trace-norm penalized regression, which solves the following convex program: margin-weighted trace-norm penalized regression: penalized regression, where W is not speciﬁed at this point yet: Note that if we take W to be a matrix of all ones, then this formulation reduces to (3.1). So our formulation covers both the regular trace-norm penalized regression and the margin- weighted trace-norm penalized regression. the closeness between W and P , we introduce the following scalar term l: that l is not too large. an increasing function in l, n and ˜r. Theorem 1. Let σ be the noise variance. We have: the regular (non-weighted) trace-norm penalized regression under uniform sampling scheme. l and γ are hyper-parameters: subject to ≤l and kQ ◦B ≤γ. version of Q , where Q denotes elementwise square of Q, i.e. will use two estimates of B and P and plug them in the following program: subject to ≤l and kQ ◦ Bk ≤γ. plugs it in the weighted objective (3.3) to obtain the estimator B. recovery. we can ﬁrst estimate the marginal probability R and C by the following formula: /n and /n. that does not restrict the rank of P to 1. matrix and the number of samples, i.e., M ∼Poisson(n P ). By deﬁnition of Poisson dis- P(M = k) = based on the realizations of the entries. Thus, this is a low-rank matrix recovery problem. objective: 1 presents the pseudo-code for PMLSVT. In this algorithm, P(Z) = , where I = n, Algorithm 1 Low-rank sampling matrix estimation using PMLSVT , λ , . . . , λ do {singular value decomposition} ←([Σ] −λ/t) , i = 1, . . . , d ←X{record previous step} ), then t ←ηt, go to 4. )|< 10 then exit current iteration and go to the next iteration; = X /sum(X). the number of samples. Cost function f(X) = is-non-zero(M) ◦X −M ◦log(X). Now that we have obtained a weight matrix which makes the upper error bound small, program and can be solved by casting it into a semideﬁnite programming (SDP) problem, descent method more readily. Consider the following mapping: B →N := W ◦B. Consequently, let the modiﬁed obser- vation operator be: X(N)] = hN, i for all i ∈[n], where ◦X . Note that X(N) = X(B) by construction. Then Problem (3.3) can be formulated as the following: Notice that Problem (3.6) resembles the trace-norm penalized regression in Problem (3.1), can then report the estimator for B as B = N. To proceed with the projected gradient descent method to solve (3.6), let us denote g(N) = function whose gradient is ∇g(N) = X(N) −Y ), where denotes the dual operator. Recall that the proximal mapping is deﬁned to be: prox (N) = arg min kN −M k + h(M), and the generalized gradient of L(N), denoted as G (N), is: (N) = where N = U Σ V is the SVD. Now we are ready to write out the proximal-gradient-descent- based Algorithm 2 to solve Problem (3.6). a) Repeat: (i) Store previous value: N := N. t. After exiting this forthtracking while loop, return to the second enlarge t := to the last step size t := βt. (iii) Update: N := N −tG (N). until stopping criterion is satisﬁed, i.e. kN −N k ≤tol. of the theorem that prompts us to use Program (3.5) to construct the weight matrix. results, and with some algebra, derive concrete error bounds with appropriate constant terms. Before we delve into the deterministic result, we need the following assumption that con- trols the spikiness of the matrix by bounding N( W ◦B ): Assumption 2. Assume that N( W ◦B ) ≤n for some n > 0. directly apply Theorem 3.1 from Hamidi and Bayati (2019) to derive the following deter- ministic result. Proposition 1 (Theorem 3.1 from Hamidi and Bayati (2019)). Deﬁne ˜η = 72˜r. where Σ := . Then for any matrix B such that N = W ◦ B satisﬁes L( N) ≤ N −N ≤( ) ∨4n ˜ν. on Π: for all B such that N( W ◦B) ≤1, where the expectations are with respect to Π. the derivation of the non-asymptotic error bounds for matrix estimation problems. Intu- strongly convex in a restricted set C of directions ∆. That is to say, the observation opera- probability for the modiﬁed observation operator X: := where {ζ is an i.i.d. sequence with Rademacher distribution. ˜α := β := 6696˜rc , and ˜ν := to derive the following bound: Therefore, when λ ≥CE c, holds with probability at least 1 −P( λ < 3k Σk ) −2 exp(− ). integrating Lemma 5 and 6 into the probabilistic bound (4.3), we obtain Theorem 1. real data from two diﬀerent domains: movie recommendations and lab test recommendations. the sampling matrix. We compare our NU-Recommend method against (i) regular trace-norm penalized regres- marginal-weighted trace-norm penalized regression that goes back to Srebro and Salakhut- ground truth matrix. Later, when we deal with the real data, we will be using the cross- pick the latter because it is computationally faster. 2.08% better than the Margin. the top 25% users who rated the most and the top 25% movies that are rated the most. these RMSE with 2 standard errors in Figure 3. test RMSE among all methods. As we can see, NU-Recommend improves RMSE by 1.76% compared with the IPW+Uniform method, by 0.51% compared with the Uniform method, Prize competition, the best team’s RMSE moved from 0.8627 to 0.8567 (less than 0.7% improvement). done for the MovieLens data. We repeat the process by 10 splits of the evaluation set. Margin method. Overall, these results are consistent with our result on MovieLens Data. This paper introduces a new method to learn user preferences for recommender systems. be further enhanced. A number of exciting future research are as follows. Our theoretical upper bound is moti- vated by the oracle inequalities in high-dimensional statistics. It is interesting to see that we could take this upper bound and then from it derive a better version of the weighted estima- to obtain the desired weight matrix, the scalability of our approach can be compromised. Another important direction is to include side information to aid the sampling matrix esti- each other to make a better informed estimation. Last but not least, as mentioned before, missingness of an entry may depend on the true value of that entry.