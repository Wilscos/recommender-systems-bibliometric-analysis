Increasing amounts of high-dimensional data, in ﬁelds such as molecular and systems’ biology, require development of fast and scalable feature ranking algorithms [14]. By being able to prioritize the feature space with respect to a given target, feature ranking algorithms already oﬀer, e.g., novel biomarker candidates. However, the amount of available labeled data is potentially much smaller when compared to the amount of unlabeled data, which remains largely unexploited. In response, unsupervised feature ranking algorithms (that operate only on unlabeled data) are actively developed. schematically shown in Fig. 1. FRANe achieves state-of-the-art performance by exploiting data-derived relations between the features (which form an undirected weighted graph). The contributions of this work are manifold, and can be summarized as follows: Supported by the Slovenian Research Agency (grant P2-0103 and a young researcher grant), and European Commission (grant 952215). Urh Primoˇziˇc, BlaˇzˇSkrlj, Saˇso Dˇzeroski, and Matej Petkovi´c Abstract. The need for learning from unlabeled data is increasing in contemporary machine learning. Methods for unsupervised feature ranking, which identify the most important features in such data are thus gaining attention, and so are their applications in studying high throughput biological experiments or user bases for recommender systems. We propose FRANe (Feature Ranking via Attribute Networks), an unsupervised algorithm capable of ﬁnding key features in given unlabeled data set. FRANe is based on ideas from network reconstruction and network analysis. FRANe performs better than state-of-the-art competitors, as we empirically demonstrate on a large collection of benchmarks. Moreover, we provide the time complexity analysis of FRANe further demonstrating its scalability. Finally, FRANe oﬀers as the result the interpretable relational structures used to derive the feature importances. Keywords: feature ranking · feature selection · unsupervised learning · attribute networks · PageRank. We propose FRANe, a Feature Ranking approach based on Attribute Networks), 1. We propose FRANe, a fast algorithm for unsupervised feature ranking based 2. We demonstrate the algorithm’s state-of-the-art performance on 26 datasets, 3. We present an extensive theoretical analysis of the proposed algorithm. 4. We oﬀer an implementation of FRANe as a simple-to-use, freely available The remainder of this work is structured as follows. In Section 2, we discuss the related work that has led us to propose FRANe. We describe the proposed method in Section 3. Next, we discuss the experimental setup (Section 4), followed by our results (Section 5) and conclusions (Section 6). Unsupervised feature ranking is a relatively new research endeavor. An overview of unsupervised ranking algorithms [13] was published only recently. Some of the currently well-established methods for unsupervised feature ranking include: Laplace [8], MCFS, and NDFS. All of them construct a network of instances by employing an instance similarity measure. Finally, recent work – awarded the best paper award at ECML PKDD 2019 – uses autoencoder [7]: the AgnoS-S algorithm gives feature ranking scores as a parameter vector at the early stages of a neural network, which learns to reconstruct the input space and assigns each input variable a score as a side-result. ration from the literature on network reconstruction and its applications in gene expression analysis [12,9,5]. Network reconstruction derives a network from a tabular data set, so that relations between instances (rows) or features (columns) on reconstructing attribute networks and subsequent node ranking. validating our claims via Bayesian and classical performance analysis. Python library, which also includes other baseline approaches. Apart from the unsupervised feature ranking literature, we also draw inspiare identiﬁed, maintained, and used for a given down-stream task. Once a tabular data set is converted to a network (graph), various centrality measures can be used to determine the centrality of the nodes in the network. Our method uses PageRank centrality measure [11] and its generalization to weighted graphs. While we use it in the unsupervised fashion (somewhat similarly to [16]), it can be also used in the supervised scenario [1]. Real data often consists of groups of similar features. Intuitively, each such group has a representative feature that is most similar to all others. This feature can be expected to predict the values of the other features in the group reasonably well, making the others redundant. Thus, the most central features are potentially good candidates for a set of features that a feature selection algorithm would return. When the number of features in the data goes into thousands and more, it is expected that many of them are eﬀectively random noise or completely redundant. The corresponding noisy weights could prevent discovering the wanted centrality values: We therefore introduce a minimal weight threshold and only connect the features that are similar enough. try out a set of candidate thresholds, following geometric threshold progression and ranging from the minimal to the maximal edge weight. We calculate the centrality (feature importance) values from the corresponding graphs, and obtain a set of feature rankings. Among those, we choose the one that maximizes the heuristic that is based on the intuition that the feature importance values in a good ranking have a large spread. Rankings obtained with low thresholds are expected to be similar, whereas small increases of high thresholds can cause large changes in the rankings. Sets of candidate thresholds with few low thresholds and many large ones, e.g., the geometric threshold sequence, are expected to give good results. Let X = [x n is the number of features. The i-th example (row in the matrix X), 1 ≤ i ≤ m, is given as x vector (column in the matrix X) f data X, a minimal edge threshold and the number of iterations I. First, it computes the feature similarly matrix W = [w the geometric sequence T of (edge-weight) thresholds as follows. First, we deﬁne the set of similarities between diﬀerent features W with M It is not clear in advance which threshold value is the best. Therefore, we The computation of FRANe is given in Alg. 1. At input, it takes the (training) = max(W) and m= min(W). Then, the dissimilarity values D = − w|w ∈ W∧ w < M} are computed. Finally, the thresholds t∈ T are deﬁned as T = [t The temporary resort to dissimilarities is necessary, because we want to analyze the region of larger similarities more thoroughly. For every threshold t build a weighted graph G(t edge with the weight w avoid too sparse graphs, we consider only those, for which the average degree ¯e = |{w r(t of the node of feature f all thresholds, calculating the rankings r(t the ranking with the highest value of the ranking quality heuristic RQH, where The second largest and smallest scores are taken for stability reasons as the medians of the three largest and smallest scores, respectively. Algorithm 1: FRANe(X, ¯e ties, yielding time complexity of O(mn structed in the total time of O(n then incrementally remove the edges with the weights on the intervals [t Using the power method for PageRank and assuming that the number of iterations is upper-bounded with some constant [11], computing PageRank takes O(n Note that the most time-consuming step (similarity computation) can be easily parallelized, and that computing PageRank demands only vectorizable matrixvector multiplication and vector-vector addition. |j < k ∧ w≥ t}|/n exceeds ¯e= 1. We run a PageRank on the graph G(t), which returns a possible ranking ) = [PR(f), . . . , PR(f)], where PR(f) is the PageRank importance PR (j) The ﬁrst step of the algorithm requires the computation of pairwise similari- ) steps. Thus, the total number of steps in the algorithm is O(m·n+I ·n). In this section, we describe the experimental procedure that we employ to investigate the following questions: i) How does FRANe compare to state-of-the-art methods for unsupervised feature ranking, and ii) What is the inﬂuence of the diﬀerent parameters or FRANe on its performance? evaluation procedure and ﬁnish with the parametrization of the methods. Note that the code that allows for replicating our experiments (including the computation of training and testing splits) is freely available at https://github .com/FRANe-team/FRANe-dev. to use all the datasets, but had to exclude three data sets from the study (orlraws10P, lung-small and warpAR10P) to meet the independence assumptions of the statistical tests. Table 1 gives a more detailed description of the data, including their domains. When evaluating the feature ranking algorithms, Table 1: Number of features (n), examples (m) and the domain of the used benchmarks. we follow the approach of [7]. Here, an algorithm is evaluated via 10-fold crossvalidation. For a given partition of a data set into test part (one of the folds) and train part (the remaining 9 folds), feature ranking is computed on the train part. Then, the n (5NN) model that uses only these features for predicting the values of all the features is trained (on the train part of the data). Finally, the performance of the feature ranking algorithm is measured in terms of the predictive performance of the 5NN model on the test set. As evaluation measure, we use the average relative mean absolute error RMAE = We ﬁrst give a brief description of the data sets used, continue with the We obtained the data from the Scikit-feature repository [10]. We wanted is the number of examples in the test set, ˆxis the 5NN’s prediction forp , and σ(f) =Var (f) is the standard deviation of the feature f. A low value of RMAE means that the subset of n all the feature values. the predictive performance of 5NN changes as more and more top-ranked features are considered, one can build a series of 5NN models that use n {1, 2, . . . , 2 may be more informative, but is harder to analyze when comparing diﬀerent algorithms through statistical tests. For such comparisons, performance at n is chosen. The hierarchical Bayesian t-test considered in this work is discussed in more detail in [2]. The test approximates the posterior probability of the difference in performance between a pair of classiﬁers. The posterior plot can be visualized as a simplex, where each point represents a sample from the posterior distribution. By counting such samples in diﬀerent parts of the simplex, the probability of one classiﬁer outperforming the other is estimated. for the average number of edges was set to ¯e PageRank, the recommended value of δ = 0.85 was used. For other algorithms, we used the recommended parameter values. Additionally, the number of clusters for the methods MCFS and NDFST was set to the number of classes in the datasets at hand. This was possible since we used classiﬁcation datasets from the Scikit-feature repository. The classes were otherwise ignored. In this section, we ﬁrst report the results of the comparison between FRANe and its competitors. We then focus on diﬀerent parts of FRANe and consider alternative design choices. sponding 5NN models, are shown in Table 2. We can see that FRANe outperforms its competitors. First of all, it has the best average rank (1.88) among the considered algorithms. The second best algorithm (in terms of the average rank) is Laplace with an average rank of 2.54. The diﬀerence between FRANe and the other algorithms is even more visible when we compare the numbers of wins: FRANe is the best performing algorithm in 12 cases (46% win rate). The second highest number of wins (5) is achieved by NDFS. we employ the Bayesian hierarchical t-test [2], since it directly answers which of the two compared algorithms is better. The other popular option – frequentist non-parametric tests such as Friedman and Bonferroni-Dunn [6] – allow for comparison of more than one algorithm, but these tests are typically too weak (as follows from their deﬁnitions [6]), and are harder to interpret. The Bayesian comparison indicates that FRANe dominates its closest competitor (Laplace), in 26% of the cases, whereas the Laplace method is better in only 2% of the cases. In the other cases, the diﬀerence in performance is smaller than 0.001 and is considered practically insigniﬁcant. This is consistent with the results in The obtained RMAE values are averaged over the 10 folds. To see how The number of iterations in FRANe was set to I = 100 and the threshold The RMAE values for the diﬀerent feature ranking methods, i.e., the corre- To also show some statistical evidence for the quality of the FRANe rankings, Table 2: The performance (measured in terms of RMAE) of 5NN models that use the table additionally give the average rank of each algorithm and its number of wins, i.e., the number of times it is ranked ﬁrst. The best result in each row is shown in bold. Table 2: the overall win-rate of FRANe is notably higher (12 against 4), even though these two algorithms diﬀer by less than one in average rank values. A detailed (and more global) comparison of the rankings (where the number of chosen features varies from 1 to n) on Gisette data set is given in Fig. 2. It is clear that the FRANe rankings are the best as its corresponding curve is below the curves of all other rankings. = 16 top-ranked features from a given feature ranking. The last two rows of the After we have proved that FRANe oﬀers state-of-the-art performance, we now investigate the sensitivity of its performance to varying its key components. Due to space constraints, we only vary the similarity measure used in the computation of the matrix W , the threshold progression that deﬁnes the list of edge-weight thresholds T , and the ranking quality heuristic RQH, while the node-centrality measure is left ﬁxed (PageRank), and left for further work. We ﬁrst give a brief description of the considered threshold progressions and similarity measures between diﬀerent features W and M Similarity measures. Let f feature vectors. Besides correlation, other similarity measures can be used. They are all based on diﬀerent distance measures d(f ii) Chebyshev (max iv) Euclidean ( are deﬁned as sim(f Threshold functions. The deﬁnition of the thresholds t follows the geometric progression. The alternatives are: i) Linear(m Linear(mean(W t= i-th I-quantile of W The motivation for using linear progression that starts at the mean (or its more stable analogue the median) of the W are more interesting to analyze, since the corresponding graphs are sparser. the chosen threshold progression and to the chosen similarity measure. Except for the correlation similarity (works best for 10/26 data sets), and the geometric threshold progression (works best in 9/26 cases), all the similarity measures and threshold progressions perform approximately equally well. Still, no ﬁxed (progression, similarity) pair has more than 3 wins. The detailed results are available at https://github.com/FRANe-team/FRANe). They also include the experiments with RQH, where we show that RQH outperforms random search (in 22/26 cases), which is often considered a strong baseline in optimization [3]. Fig. 2: Error curves for the diﬀerent rankings on Gisette dataset. = max W. The results (see Fig. 3) show that FRANe is quite robust with respect to Fig. 3: Average ranks (over datasets) of diﬀerent combinations similarity metric - threshold progression. The legend denotes the average rank of a given metricprogression combination (the lower, the better). In this work we have presented FRANe, an algorithm for network-based unsupervised feature ranking. In contrast to existing approaches, FRANe attempts to reconstruct a representative network of features. By ranking nodes in this network via the eﬃcient PageRank approach, we achieve state-of-the-art results for the task of unsupervised feature ranking. indeed a strong competitor to the existing approaches. Theoretical analysis indicates the O(n bottlenecks. The current implementation of FRANe, however, exploits highly optimized compiled routines and scales seamlessly for each of the considered data sets. An extension which would reduce the quadratic complexity could include random subspace sampling (where the probability of choosing a feature depends on its variance). as the key nodes (features) and their, e.g., correlation-based neighborhoods are easily inspected. This can potentially oﬀer novel insights into key parts of the feature space governing a given data set’s structure. of PageRank scores (maintaining the graph in the memory), we believe that an option for further scalability could potentially include distributed storagebased matrix operations [4,15], which would facilitate ranking of attributes when considering very large data sets. computed in latent space, where embeddings of features would be ﬁrst obtained (via the transposed feature matrix), potentially speeding up the correlation computation, as well as providing more robust rankings. Furthermore, the body of work related to metric learning could similarly prove useful when determining the most suitable similarity score. The results indicate that the proposed unsupervised ranking algorithm is The proposed methodology is suitable from the interpretability point of view, Given that the main spatial bottleneck is related directly to computation As further work, we believe that distances between features could be also