<title>Privacy-Preserving Communication-Efﬁcient Federated Multi-Armed Bandits</title> <title>Tan Li, Linqi Song City University of Hong Kong, City University of Hong Kong Shenzhen Research Institute {tanli6-c@my., linqi.song@}cityu.edu.hk</title> <title>arXiv:2111.01570v1  [cs.LG]  2 Nov 2021</title> Federated multi-armed bandits (MAB), combining the conventional MAB model and federated learning, is an emerging framework in distributed sequential decision-making, especially for many real-world wireless distributed systems [1] [2]. For example, consider wireless coupon distribution systems in chain stores that make local recommendations to their customers; they wish to aggregate the overall responses without revealing users’ personal information to provide better recommendations [3] [4] [5]. Another application example is the Internet of Vehicles (IoV), where we need to effectively collect information from each vehicle for joint path planning when communication resources are limited [6] [7]. Two critical concerns of federated MAB are the communication bottleneck and data privacy. A substantial number of local agents/devices periodically exchanging local model updates for model aggregation require a large amount of communication resources [8], which is often scarce in wireless networks. Several communication efﬁcient learning techniques have been studied recently, including local model compression [9], partial device participation [10], and less frequent aggregation [11]. On the other hand, exchanging learning model information while protecting data privacy at local agents involves privacy mechanism design, e.g., differential privacy [12]. Some existing works have began studying how communication constraints and privacy requirements affect the learning performance in supervised learning [13], however, these works did not look into this question in the federated bandit problem. In this paper, we explore how we design federated bandit algorithms and communication protocols to better inform what decisions to make at the distributed agents to achieve a certain level of data privacy under communication constraints. We consider different network structures for agents to exchange learning knowledge: a master-worker, a decentralized, and a hybrid structure that combines the above two. In the master-worker network structure, a central server collects M agents’ individual model parameters and returns back an aggregated result to all agents. We propose an eliminationbased federated bandit algorithm and an epoch-based communication protocol. In this approach, an exponentially increasing number of time slots consist of an epoch and agents locally explore a set of active arms (out of total K arms) during each epoch. At the end of each epoch, local agents send the estimated mean of each arm with additional noise sampled from a Laplace distribution with parameter , in order to fulﬁll the differential privacy requirement. The server reduces the size of this active set by eliminating empirically inferior arms based on the aggregation result and returns elimination results to agents. Our CDP-MAB algorithm achieves a regret of O(max{ } with a communication cost of no more than O(c M log T ), where c is the cost of building a serverto-agent link. This indicates a trade-off between the privacy and learning performance, the second term in the regret increases with the decreasing of  (higher level of privacy), i.e., inversely proportional to . Furthermore, when communication constraints are posed, we investigate an efﬁcient communication protocol that allows only a certain number of communication epochs R and at each epoch allows only a fraction p of users to upload learning knowledge. Again, we consider an exponentially increasing length of epochs, but scaled according to R. In this case, the server needs to ﬁne-tune the elimination threshold based on insufﬁcient collections of learning knowledge in order to gain a certain level of conﬁdence to remove an empirically inferior arm. This algorithm achieves min{M, T /p } times more regret than the noncommunication-constrained setting with a communication cost of c pMR. We next extend to a decentralized network structure where agents could exchange protected learning knowledge with their neighbors to make better decisions. We consider the communication network as an undirected graph G(V, E) with vertices corresponding to the agents and edges depicting neighbor relationships. We propose a multi-hop information propagation protocol, termed Global Information Synchronization (GIS) protocol, to ensure that all agents can receive messages from other M −1 agents after each communication round, however, with a certain transmission delay depending on the network structure. Before the private information is fully synchronized, all agents exploit their locally observed best arm so far. Compared with the centralized setting, there is an additional term O(Md ) in the decentralized regret, which can be seen as caused by the information dissemination in the network graph G with diameter d In addition, we consider a hybrid network structure combining master-worker and decentralized network together with a two-layer communication protocol. Each agent ﬁrst performs local exploration and sends the protected means to a “sink agent" inside a component (local communication). After this ﬁrst-step information exchange, the second-step communication only occurs among sink agents of each component and a server (global communication). The server aggregates protected learning parameters and sends global elimination results to each agent. Results show that hybrid structure can help to achieve communication efﬁciency without deteriorating the regret, by reducing both agent-to-agent and server-to-agent links. Finally, we empirically show trade-offs between learning regret and communication/privacy, which are consistent with our theoretical ﬁndings. These results also provide important insights into designing practical communication-efﬁcient privacy-aware federated MAB systems. The MAB model is widely used in many applications, like recommendation systems and clinic trials, due to its simplicity and efﬁciency [14] [15]. Recently, privacy issues have raised concerns in the bandit studies. Early works focus on single-agent MAB problems, where several differential privacy-based bandit learning algorithms have been proposed by adding noise to partial sums of rewards [16] [17]. Given a certain level of differential privacy requirement, lower bounds of learning performance in terms of the regret have been given in [18] [19]. There has been another line of literature recently that talks about the distributed/federated MAB problems, where agents aim to collaboratively make decisions by exchanging information with others [20]. Two critical concerns of federated MAB are the communication bottleneck and data privacy. In our previous conference work [1], the MAB problem has been extended into a multi-agent setting where both a ‘master-worker’ and a fully decentralized structure are studied together with a tree-based privacy preserving mechanism. In [21]–[23], federated linear bandit problem is investigated through a decentralized network of agents via privacy preserving gossip approach. On the one hand, these works add Laplacian noise to the local estimated means at each time slot before communication, which leads to an O(K log (T )) order privacy-related regret. Instead, we use a simple and efﬁcient privacypreserving mechanism to scale this term as O(log T ). On the other hand, these works did not take into account of communication efﬁciency in the protocols. They force the agents to communication to (one of) their neighbors at each time slot to reach global consensus in ﬁnite time slots, which incurs O(T ) communication cost. Federated MAB under limited communication has received more attention recently [20]. The most common way is to achieve through the central server. In the master-worker structure, a number of agents periodically upload local parameters to the server to reduce the communication rounds. Speciﬁcally, in [24], a mixture bandit model to balance the generalization and personalization is studied. In [2], a federated bandit problem with client sampling is studied with communication cost counted in the regret, yet without privacy preserving mechanisms. They implicitly bound the total communication rounds by O(log T) using an action elimination-based algorithm. Different from them, we study how the regret will be affected when the communication resource is clearly constrained, for example, when the communication round is ﬁxed. In addition, we also proved that under the proposed decentralized and hybrid structure, the O(log T ) regret can be achieved with O(log T ) communication cost. We consider a federated bandit problem with M agents in either master-worker or decentralized network structures or both. In the master-worker structure, agents communicate with a central server to train a model together; in the decentralized network structure, agents communicate directly with their neighbours via a given network structure and train local models at each agent. All M agents are associated with K arms (e.g., movies, ads, news, or items) from an arm set A := {1, 2, ..., K}. At time slot t, each agent i chooses to pull an arm a (t) ∈ A. Then the arm k ∈ A chosen by agent i ∈ [M] generates an i.i.d. reward r (t) ∈ [0, 1] from a ﬁxed but unknown distribution at time t. We denote by µ the unknown mean of reward distribution. In our model, we ﬁrst assume a homogeneous reward structure, that for all arms 1 ≤ k ≤ K, = µ = ··· = µ , and thus in the rest of the paper we use µ for simplicity. Without loss of generality, we assume that µ is the best arm. Then the suboptimality gap can be deﬁned as ∆ := µ − µ for any arm k 6= 1. We also denote by ∆ the minimal non-zero suboptimality gap among all ∆ . In Section VI, we extend our setting to the heterogeneous reward structure, where µ = µ (i 6= j) does not necessarily hold and agents aim to learn /M, and discuss its theoretical results. The objective of the M agents is to minimize the regret, which is deﬁned as the expected reward difference between the best arm and the online learning policies of the agents as follows:. R(T ) = T Mµ − E[ (t)], (1) where the expectation is taken over the randomness in the choice of arms. In this section, we ﬁrst talk in detail the communication network structures in federated bandits, leading to different ways of information exchanging. • Master-worker structure. In this structure, individual agents ﬁrst perform local learning (pulling some arms) according to some privacy-preserving learning strategies, and then upload protected model parameters to a central server. The central server aggregates information sent from local agents and sends the global parameters back to all agents. This will be conducted iteratively. • Decentralized network structure. This decentralized network structure is described by an undirected, connected graph G(V, E), where V is the set of all M agents and E is the set of all communication links. Agents learn locally and communicate with their neighbors iteratively to exchange their learned knowledge. • Hybrid network structure. We also consider a hybrid structure, where agents ﬁrst form decentralized network structures, called components, and then components are connected with a central server. The information exchange consists of two levels: local communication where agents within a same component exchange information with the assigned “sink agent" and global communication where components upload information to the central server and the central server sends back aggregated information to all agents. In our framework, we consider that communication is constrained due to the scarce of communication resources so that communication-efﬁcient learning strategies are needed. In particular, we consider the communication cost C(T ) to be the cost of building total number of (two-way) communication links when agents exchange information up to time horizon T . Consider that the communication happens for a total of R rounds. The r-th communication round contains t time slots. At each time slot 1 ≤ t ≤ t in the r-th round, L communication links are built and each of them incurs a cost c for 1 ≤ l ≤ L . Then, we have the communication cost over time horizon T as: C(T ) = . (2) Refer to the three communication structures we introduced above. We assume that the cost required to build a server-to-agent link is c , while the cost required to establish an agent-to-agent connection is c . Therefore, c in Eq. (2) can take the value of either c or c . In general, the communication cost is related to the energy consumption, available bandwidths, etc. to build the links. For example, in the decentralized structure, an agentto-agent link is established in the local area network within a short distance and with low energy-consumption. The central server in the master-worker structure is deployed in the remote cloud or an edge computing platform, and it consumes more energy and bandwidth resources to establish a server-to-agent link. In this paper, we reasonably assume c >> c for the hybrid structure where we ﬁrst consider the peer-to-peer communication and then the peer-server communication. Next, we look into details of the information that is being transmitted from an agent. Let us denote by H (t) = (a (1), r (1), ..., a (t), r (t)) the historical action-reward pairs observed by agent i until time t. Consider at some time slot t when communication happens, agent i creates a message I (t) = f(H (t)) and sends it to the server or its neighbors, where f() is a privacy-preserving function of agent i’s history, such as a noised version of empirical reward mean or cumulative sampling numbers. In the federated bandit setting, we consider a privacy model that aims to protect the private historical data when revealing messages I (·) that are sent out by each agent. where the adjacent sequences H (t), H (t) differ in at most one position. We can see that - differentially private mechanism f() in federated bandits that generates each message I (t) is individually private, regardless of the receiver’ algorithm. We next introduce the Laplace mechanism that convert some function g(H (t)) into a differentially private mechanism f (H (t)) by adding some Laplace noise Laplace() according to the Laplace distribution (which can be multi-dimensional with each dimension being generated i.i.d.). where s(g) is the sensitivity function under the l norm: s(g) = max ||g(H (t)) − g(H (t))|| , (5) which gives an upper bound on how much we must perturb its output in order to preserve privacy. Our design goal is to design differentially private bandit learning algorithms in the federated bandit framework under communication constraints. In this section, we present the communication-efﬁcient privacy-preserving algorithms for federated MAB in the master-worker structure, where a server collects M agents’ individual model parameters and returns back an aggregated result to all agents. Our algorithms need to consider: 1) When and what to communicate? That is, how to design the communication protocol to balance the exploration-exploitation dilemma in federated bandit learning? 2) How to protect the privacy of messages when communication happens? More speciﬁcally, how much noise should be added to the parameters to achieve the desired privacy level? Burying these two questions in mind, we ﬁrst design a federated bandit algorithm that considers privacy requirements with sufﬁcient communications, and then develop a federated algorithm under communication constraints. The CDP-MAB algorithm is described in Alg. 1. The server maintains an active arm set I , initialized as I = [K], and uses an elimination method to gradually eliminate suboptimal arms while learning the optimal arm. The algorithm operates in epochs and each of them can be divided into two sub-phases: 1) Local Exploration: In epoch r, all agents receive a new arm set I broadcast by the server. Agents then explore all the active arms in this set for the same number of S(r) − S(r −1) times (Line 4) and update the empirical means based on observed rewards. Speciﬁcally, S(r) is doubling-increasing, so S(r) − S(r − 1) is also doubling-increasing. 2) Communication and Aggregation: : After S(r) −S(r −1) times of local explorations, the empirical mean of an active arm k at agent i is denoted by ˆx (r), which is only calculated based on the reward at round r. Each agent ﬁrst transfers ˆx (r) to the privacypreserving or protected version ˆy (r) with additional noise sampled from the distribution Lap( ). In order to use the previous rewards without revealing privacy, we introduce ¯y (r) to record the private mean of history, S(r − 1) S(r) − S(r − 1) ¯y (r) = ¯y (r − 1) + ˆy (r) (6) S(r) S(r) Then, all M workers upload the protected mean ¯y (r), ∀k ∈ I to the server. The server aggregates the means and privately eliminates suboptimal arms based on the conﬁdence interval C(r) (Line 5). If there exists only one arm (empirically best arm) in the active arm set, then all agents just play this arm until time T . Algorithm 1 Centralized Differentially Private Multi-armed Bandit Algorithm (CDP-MAB) Input: Time horizon: T; Privacy parameter ; number of agent M; Initialization: t = 1, r = 1; I = [K]; x (1) = ˆy (1) = ¯y (1) = 0; S(0) = 0 | > 1 do ← 2 for {S(r) − S(r − 1)} times; (r); (r) = ˆx (r) + Lap( ); (r) = ¯y (r −1) + ˆy (r); (r) to server; (r) = ¯y (r); (r) = max ¯y (r); (r) − ¯y ((r)) ≥ 2C(r) then = I /{k}. |[S(r) − S(r − 1)] The performance of the CDP-MAB algorithm is shown in Theorem 1. K log T log(KT log T ) log(KT log T ) O(max{ }). (7) M Remark 1. Theorem 1 indicates a trade-off between the privacy and learning performance . The regret O( ) increases with the decreasing of  (higher level of privacy), until reaching O( ). The communication cost C (T ) = O(c M log T ) is the minimum cost needed to achieve O(log T ) order regret. We next show that any C(T ) less than c M log T may bring the performance loss. proof outline: The privacy guarantee can be directly derived from the deﬁnition of the Laplace mechanism. Since the reward of each arm is bounded by [0, 1], the sensitivity (difference of the mean of each arm) of two neighboring reward sequences is less than Thus, the additional noise sampled from Lap( ) leads to M-level privacy. The regret is incurred by playing suboptimal arms before they are correctly eliminated. We deﬁne as the epoch up to which ∆ exceeds 2 . Then we investigate the regret incurred by the following three events: i) Exploration of each suboptimal arm k before epoch r ; ii) Fail to eliminate suboptimal arm k after the epoch r , and iii) Optimal arm is eliminated by server. Note that ii) and iii) can lead to at most MT ∆ regret for arm k. We show that after epoch log( ) + 1 , these events will not happen with a high probability. Yet regret caused by i) is unavoidable and dominates the total regret. Note that, all arms in the active arm set are pulled for the same number of times up to S(r) in epoch r. Thus, the exploration number of arm k can be bounded by S(r ). Calculating the total amount of exploration and multiplying by the reward gaps, we achieve the cumulative regret. Since r communication rounds are needed to identify the suboptimal arm k, we in total need log( ) + 1 rounds to identify the best arm, leading to c log( ) + 1 ≤ O(c M log T ) communication cost. The full proof is provided in detail in Appendix. B. In real world applications, c is usually large since it is energy-consuming for the devices to establish connections with a remote server, which leads to a large C (T ). We now consider how to extend the above Alg. 1 to the case under communication constraints by utilizing efﬁcient communication strategies as follows: • Less Frequent Communication. A natural strategy is to communicate less frequently. Given the total time horizon T , we set R as the constraint for the number of communication rounds. Therefore, T can be divided into R + 1 epochs. How to determine the length of each epoch r ∈ R is not trivial. Given a ﬁxed communication round R, if agents communicate too early, then their estimations of rewards are likely to be poor, as they are based on fewer samples. Thus, they may not be able to get the desired learning knowledge after all communication rounds. On the other hand, if they communicate too late, they cannot make full use of the information of others and the regret will scale with the number of agents. As a result, choosing the right time slots for communication is crucial. The key idea to solve this is to ensure that the algorithm can identify the best arm after the R-th communication round with high probability. Otherwise, the time period from the end of R-th communication to T may bring a linear growth of regret without further exploration. The doubling-increasing epoch length in Alg. 1 may not meet this requirement since R may be less than log(1/∆). To tackle this, we use an exponentially increasing length of epochs, scaled according to R: = ∆ (replacing Line 3 in Alg. 1). • Partial Participation. Another way to reduce the communication cost is to deactivate some of the communication links. We set p as the link participation rate. We set p as the communication link participation rate and N = dpMe agents are enabled to communicate with the server during each communication round. In this way, NS(r) samples can be observed at the server after the r-th communication round. Thus, the server needs to ﬁne-tune the length of local exploration and elimination threshold according to p in order to have a sufﬁcient level of conﬁdence to remove an empirically inferior arm. Speciﬁcally, we redesign S(r) as S (r) = max{ } (replacing Line 4 in Alg. 1), and C(r) as C (r) = (replacing Line 5 in Alg. 1). Compared with S(r), S (r) has a scale factor of or for the two terms inside the max{}; so does C (r) for its two terms compared with C(r). At each communication round, the server randomly select N users (i.e., M −N sleep link) to upload their empirical arm means with additional noise sampled from Lap( ). Then it reduces the active arm sized based on this partial aggregation as in Lines 11 to 14 in Alg. 1. We show the performance of the CDP-MAB algorithm under communication constraints in the following theorem. Theorem 2 (Performance of the CDP-MAB Algorithm Under Communication Constraints). Given a participation rate p and a limited number of communication rounds R, the CDPMAB algorithm under communication constraints, • is M-differentially private; • incurs communication cost C (T ) = c dpMeR; • incurs regret R (T ) upper bounded by log(R KT ) log(R KT ) } · max }, ) (8) O(min{M, M Remark 2. can be seen as the performance loss term due to the limited communication. When p = 1 and R is larger than O(log T ), we can recover R (T ). If each agent runs CDPMAB separately, we can obtain MR (T ) regret. In order to make our performance not worse than the non-communication case, we set the min{M, } operation, which indicates that /p ≤ M → R ≥ proof outline: Similar to Theorem 1, we ﬁrst prove that the event E = {|¯y (r) − µ | < (r)} occurs for each epoch r with high probability according to the Hoeffding bound and the concentration of the Laplace distribution. We further argue that when E holds for all epoch r, the best arm will not be eliminated from the active arm set and the suboptimal arm k will be eliminated after epoch r when ∆ exceeds 2∆ . Thus, we need rounds to identify suboptimal arm k and R communication rounds to identify the best arm. Calculating the total amount of explorations before r and multiplying by the reward gap , we achieve the cumulative regret. The full proof is in Appendix C. 1) Privacy-regret trade-off: The ﬁnal order of R (T ) is determined by the relationship between M/ log(T ) and the smallest suboptimal gap ∆. If ∆ < , the ﬁrst term dominate and we achieve O( ) regret. Otherwise, we obtain a O( ) regret. Indeed, this is determined by the two terms of S(r). The ﬁrst term in S(r) can be considered as the number of samples needed to make ¯x (r) concentrated with µ within a certain conﬁdence level. The second term can be treated as the number of samples to eliminate the effect caused by the added noise ¯y (r) − ¯x (r). Clearly, if we require a stronger privacy level (smaller ), more noise needs to be added on ˆx . Then there is a larger difference between ˆx and ˆy and hence S(r) is mainly determined by . 2) Communication-regret trade-off: Here, p and R both indicate the trade-off between communication and learning performance. Speciﬁcally, they result in sub-linear and exponential deterioration terms. When R is larger than O(log T ), the terms T and T turn to be a constant and p (T ) regret can be achieved. When p = 1, the worst-cast regret is still MR (T ) with R = 3 log T/4(log M). In this section, we extend our CDP-MAB to the decentralized setting. We consider the communication network as an undirected graph G(V, E) with vertices corresponding to the agents and directed edges depicting neighbor relationships. Without central coordination, the agents remove the inferior arms after aggregating information from their neighbors. However, the irregular connections lead to different local aggregation results after communication round r. Thus, each agent obtains the unequal-length active arm sets. This results in the asynchronous local exploration phases among the agents at the start of epoch r + 1. To avoid this, we need to ensure that: i) each agent has the same number of active arms at the beginning of each epoch. ii) the agents pull each active arm the same number of times during the local exploration phases. The key to achieve this is to make all agents reach the global consensus through multiple information exchanges with neighbors. One of the simplest ideas is to use ﬂooding protocol. In ﬂooding protocol, an agent wishing to disseminate a piece of data across the network starts by sending a copy of this data to all of its neighbors. Whenever an agent receives new data, it makes copies of the data and sends the data to all of its neighbors, except the node from which it just received the data. The algorithm ﬁnishes when all the nodes in the network have received a copy of the data. Though ﬂooding can converge fast, it has implosion or the overlap problems. Gossiping is an alternative to the ﬂooding approach that uses randomization. Instead of indiscriminately forwarding data to all its neighbors, a gossiping agent only forwards data on to one randomly selected neighbor. However, gossiping distributes information slowly and incurs large end-to-end delay. In order to achieve fast convergence and avoid repeated message transmissions, we propose the following algorithm with a GIS (Global Information Synchronization) communication protocol. DDP-MAB operates at each agent in epochs and each of them can be divided into following sub-phases: 1) Local exploration: : Each agent i perform S(r) −S(r −1) times local exploration for each arm k ∈ I , update the empirical mean ˆx (r) and transfer it to the private version. Noting, we chose the same S(r) as we set in the CDP-MAB. 2) GIS communication protocol: The communication round starts after all agents ﬁnish their local exploration and ends when they receive private means from other M − 1 agents. We assume there is an additional synchronization clock to inform the start and end of each communication round by monitoring the status of each agent i’s observation list l (·), which collects historical records of reward and arm selection information received from other agents. Each communication round may contain multiple time slots, each of them includes three handshake stages (ADV-REQ-DATA) for message exchange. For each agent i, the commu- nication slot n starts when it obtains new observations (private means) that it is willing to disseminate. It does this by sending a message ADV (n) to its neighbors j ∈ N , naming the agent labels (ADV stage). Upon receiving an ADV (n), the neighboring node j checks to see whether it has already received the advertised observations. If not, it responds by sending an REQ (n) message for the missing observations back to the sender i (REQ stage). The communication step completes when i responds to the REQ (n) with a DATA (n) message, containing the missing observations (DATA stage). Fig. 2 shows an example of the protocol using two-step communication and the details of the GIS protocol is shown in Alg. 2. Algorithm 2 GIS Communication Protocol Initialization: counter n = 0; An observation list l (0) = {i} for all i. (r) are added to l (n) then (n) = {f} to neighbors j ∈ N (n) messages from j ∈ N (n) back to serve the requests. (n) = {g} message from j ∈ N then (r) exist in l (n). If not, send REQ (n) = {g} back to j; (n) and update the l (n). (n)| = M for all i then = n. 3) Local exploitation: Each communication round lasts t time slots, which can be seen as the delay suffered before information synchronization. Instead of waiting in the communication round, all agents are required to pull the best empirical arm based on local observation for t times. 4) Local aggregation and elimination: When communication ends, each agent i generates the aggregated mean ¯y (r) = ¯y (r) for each arm k. After that, each agent i performs local elimination with C(r) (Lines 9 and 10 in Alg. 3). Theorem 3 (Performance of DDP-MAB). Given time horizon T and privacy parameter , K log T log(KT log T ) log(KT log T ) O(max{ } + M(d − 1)) (9) M where d is the diameter of G and d is the degree of agent i. proof outline: The communication cost is determined by the number of agent-to-agent links established in all communication rounds. Since i)All agents can synchronously identify the best arm after log( ) + 1 rounds; ii) Each communication rounds last t time slots, upper bounded by the diameter of the graph d ; iii) In each time slot, at most /2 connections are established. Multiply these three items we can conclude C (T ). The ﬁrst term in regret is caused by local explorations of all M agents before they eliminate all suboptimal arms, which recover R (T ) by the same choice of S(r), C(r). The second term is incurred by local exploitation. The communication round r contains at most d − 1 time slots. If all 2 suboptimal arms are successfully eliminated in the previous round, and arm 1 is not eliminated, at most M(d −1)2 regret will be introduced. If arm 1 is eliminated, then at most Md regret will be introduced. We complete the proof by summing up the regrets incurred by all required communication rounds. The detailed proof can be found in Appendix. D. To constrain the total communication round as R to reduce the communication cost, we can just set ← ∆ in Line 3 of Alg.3, which leads to the following results: log(R KT ) log(R KT ) (T ) = O(min{M, T } · max + M(d − 1)) M with communication cost C (T ) = c /2. The detailed proof is shown in Appendix. E. Remark 3. When R is larger than O(log T ), we can recover R (T ). To ensure our performance not worse than the non-communication case, R ≥ 2 log T / log M. 1) Communication cost: In the centralized setting, each communication round only accounts for one time slot. Thus the communication cost is determined by c , the participating agents and the number of total communication rounds. In the decentralized setting, the communication cost is jointly decided by c , the total communication round and the links built during each round. If c M = c /2, the communication costs incurred by these two settings are the same. 2) Regret: Compare with R (T ), there is an additional term O(Md ) in R (T ), which can be regarded as the extra regret incurred by the inconsistency between local estimation and global estimation. d indicates the convergence rate of local estimation to global estimation on graph G. This term only depends on the agent number M as well as the diameter of the graph d , and not depending on T . 3) Trade-off: There is a non-proportional trade-off relationship between the above two. Sparse graphs build fewer agent-to-agent links in each communication round, but suffer a considerable delay before information synchronization; Dense graphs have a fast convergence rate but demand more connections. The graphs having both fewer edges and smaller d such as star or multi-star graphs that are close to the centralized setting, can achieve the best performance. We summarize the communication costs and regrets of several typical graphs in Table. I. 4) Discussion: R (T ) achieves the similar form of regret as in [21]. That is, the decentralized regret is equal to the centralized regret plus another graph-related term. In our work, this term is determined by M and the diameter of the graph d , while in [21], this term is determined by M and the eigenvalue of the graph Laplacian matrix. In fact, both of these two terms indicate the convergence rate of local estimation to global estimation on speciﬁc graph structures. Although we can ﬁnally achieve a consistent regret using different communication protocols, their method requires O(T ) communication cost which is not communicationefﬁcient. Besides, the privacy mechanism they used results in a O(K log (T )/) regret. Compared to them, we only need O(log T ) communication cost and the privacy-related regret is upper bounded by O(K log (T )/( M)). This also demonstrates the trade-off between communication and privacy: more frequent communications require more noise to be added, which further leads to a larger regret. In practice, CDP-MAB builds M server-to-agents links, thereby introducing high communication cost. Although we propose a partially sampling method to achieve communication efﬁciency in Section IV-B, it inevitably brings performance loss. DDP-MAB is more suitable for devices in a small-size network, otherwise the delay for reaching consensus in each communication round is unacceptable. In this section, we propose a hybrid communication structure (see Fig. 3) that combines the centralized and decentralized settings. This structure is a natural extension of the classic wireless sensor network (WSN). Each sub-network in WSN contains some general sensors and a sink node, that can communicate with the WSN server through the gateway after collecting information in the sub-network. There are Q << M components, each of them consists M agents. The agents belonging to the same component are allowed to communicate over a sub-graph G (E , V ). There also exists a sever, coordinating the communication among components. Assume there is a sink agent SA of component q. The server only communicates with the sink agents, which scales down the number of server-to-agent links from M to Q . HDP-MAB operates in epochs and each of them can be divided into the following subphases: 1) Local exploration: Each agent i perform up to S(r) times local exploration for each arm k ∈ I , update the empirical mean ˆx (r) and transfer it to the private version. Noting, we chose the S(r) as we set in the CDP-MAB and DDP-MAB. The communication round starts when agents ﬁnish their local exploration and ends when all agents receive the update active arm set from the server. We utilize a two-layer communication protocol: 2) Local communication and aggregation: Different from the fully-decentralized setting, we no longer use the GIS protocol to achieve “global information synchronization”. Instead, we use the Sink Agent Collection (SAC) protocol to realize “one-way message passing”, where all agents in q send their private means to SA . The local communication in q ends when SA observes information from other M −1 agents. This inevitably introduces some local communication delay, deﬁned as t . However, we can wisely select SA to minimize this delay by Alg. 4. Given the communication graph G of component q, we randomly assign any agent i as the sink agent, and calculate the shortest path from all other agents to i. The maximum value among them is the local delay introduced by SA = i, denoted as delay . Then, SA should be the agent with minimal local delay: SA = arg min {delay }. After local communication, the sink agents perform local aggregations. Remark 4. In the fully-decentralized setting, each node cannot access the structure of the entire graph, but can only gradually exchange messages with its neighbors. While in the hybrid structure, the server has stronger information collection and computing capabilities, so that it can ﬁnd all sink agents {SA , ..., SA } by running Alg. 4. Algorithm 5 Hybrid Differentially Private Multi-armed Bandit Algorithm (HDP-MAB) Input: Time horizon: T ; Privacy parameter ; number of agents M; a set of communication graph {G (V , E )} Initialization: t = 1, r = 1; I = [K]; x (1) = ˆy (1) = ¯y (1) = 0; S(0) = 0; {SA , ..., SA } = FindSinkAgents(G (V , E ), ..., G (V , E )) | > 1 do ← 2 ] runs Line 7 - 10 in Alg.1; ] sends ¯y (r) to SA aggregates ¯y (r) = upload ¯y (r) to the server; ] keeps pulling local empirically best arm until receive the update I from server. (r) = if ¯y (r) − ¯y (r) ≥ 2C(r) to all agents. |(S(r) − S(r − 1)) + t , r = r + 1, K log T log(KT log T ) log(KT log T ) O(max{ }) + M max {d − 1} (10) M where d is the diameter of component q. The detailed proof can be found in Appendix.F. To constrain the total communication rounds as R to realize communication efﬁciency, we can just set ← ∆ in Line 3 of Alg.5, which leads to the following results: log(R KT ) log(R KT ) (T ) = O(min{M, T } · max + M max {d }) M (11) with communication cost C (T ) = O(c M max {d − 1} + c Q)R. This result can be obtained by combining the proof of Theorem 4 and Corollary 1. Remark 5. When R is larger than O(log T ), we can recover R (T ). To ensure our performance not worse than the non-communication case, R ≥ 2 log T / log M. 1) Communication cost: Employing the hybrid structure is helpful to achieve communication efﬁciency. On the one hand, the GIS protocol only allows information exchange between neighbors to completely diffuse the information. The communication cost thereby is determined by d and the number of graph edges. The SAC protocol realizes one-way information aggregation to a ﬁxed sink node with the help of the server, which reduces the number of agent-to-agent links inside a component. On the other hand, the number of server-to-agents links is only proportional to the number of components Q, not M, which signiﬁcantly reduces the burden of the uplink. 2) Regret: Compared with R (T ), the additional term of R (T ) no longer depends on but max {d }. If M is distributed in different components, the size of each sub-graph and corresponding local delay decrease, which ultimately leads to the reduction of the regret introduced by the local exploitation. In one extreme case, when Q = M, each agent is directly connected to the server, then the delay of each communication round is 0 and we recover (T ). In addition, we can recover R (T ) with Q = 1. 3) Trade-off: The above results provide important insights into designing practical communication efﬁcient federated MAB systems. i) Instead of utilizing the master-worker structure, we can reduce the number of server-to-agent links by disturbing the agents into different components. ii) Second, the unbalanced agent distribution may cause a large delay as it is determined by the slowest component that completes the local communication. Thus, we should try to ensure the balance of agents in each component. iii) Finally, enforcing the subgraph to be close to the center (or multi-center) rather than fully connected or ring structure can also help reduce communication costs as well as regrets. 1) Comparison of three communication structures: We summary the procedure of one speciﬁc epoch for three algorithms in Fig. 3 and compare their performance in terms of communication cost, delay and regret. (T ) is determined by the number of server-to-agent links and the number of communication rounds required to remove all inferior arms; C (T ) and (T ) are decided by he number of built agent-to-agent links, the required communication rounds as well as the t of each round. in decentralized communication is introduced by the GIS protocol, and in hybrid communication it is the maximum value of delay introduced by local communication among all components. (T ) achieves O(log T ) regret. Both R (T ) and R (T ) have the addable items based on R (T ), which are caused by the local exploit during t . The items are only related to the graph structure and do not scale with T . Thus, both three algorithms can achieve the same O(log T ) order regret without considering privacy. 2) Heterogeneous reward structure: We now discuss the extension of the above algorithms to the heterogeneous rewards setting. Similar to [2], [21]: We consider that for arm k and players i, j, the expected mean µ and µ are not equal in general. There exists a true reward or global reward of arm k ∈ [K] that equals to the average of the means of all agents’ expected rewards: µ , that implies the true reward can be obtained by averaging and thus cancelling out local biases. No individual agents can make a correct inference by simply collecting individual rewards. Therefore, they must collaborate with each other to estimate the true rewards in a federated fashion. We claim that our methods in this work can be applied to the above heterogeneous setting without modiﬁcation. Corollary 3. Under the heterogeneous reward setting, (T ) regret ; (T ) + O(M(d − 1) log(T )) regret; (T ) + O(M max {d − 1}log T ) regret. Proof. The core idea of our work and [2][21] is the same: Through communication, the estimated mean at the server (or each agent) can converge on the average value of empirical means from all M agents, which can cancel local bias. In the centralized and hybrid setting, the server can capture the global knowledge without local bias. In other words, the server can observe unbiased estimation of true means, which directly avoids the impact of heterogeneity. In the decentralized setting, our proposed GIS protocol ensures that each agent can receive empirical means from other M − 1 agents in each epoch. Then, the local estimated mean (at each agent) can converge to the averaged empirical means in ﬁnite time slots. That is, each agent can reach consensus, like the server in the centralized setting. Note that both our decentralized and hybrid algorithm have a local exploitation phase. During this period, we need to pull the local empirical optimal arm, which may be inconsistent with the global optimal one. Since we need O(log T ) communication rounds, each of them lasts for at most (in the decentralized setting) or max {d } (in the hybrid setting) time slots. So local exploitation can introduce at most O(log T ) order regret under the heterogeneous reward setting, which does not affect the ﬁnal regret order. In this section, we conduct experiments to empirically verify the theoretical results of previous sections, that is, the trade-offs between communication, privacy and learning regret under different communication protocols. Experimental Settings: We consider M = 50 agents connected with a central server. Each of them plays a Bernoulli MAB with 100 arms. The means are randomly generated from [0, 1]. We set c , the communication cost to build a server-to-agent link, equal to 25. We ﬁrst consider the homologous reward structure, where all agents see the same set of arm means. 1) Privacy-regret trade-off: In this part, we allow all agents to communicate with the server at each communication round and only investigate the effect of privacy level . We consider 4 different privacy parameters  = {0.1, 0.3, 0.5, 1}. Fig. 4 shows that the regret increases with the decreasing of  (higher level of privacy), since larger noises are added on the local update estimations.  = 1 indicates the non-private case. 2) Communication-regret trade-off: We then ﬁx the privacy-preserving level and discuss the effect of communication-reduction strategies. Participating rate p: we ﬁrst consider 5 different participating rates p = {0.2, 0.4, 0.6, 0.8, 1}. Among that, p = 1 indicates the fully participating case that all agents send their perturbed models to the server after each epoch. Figure. 5 shows that the regrets at T decrease as p increases. Communication round R: We then ﬁx the participating rate as p = 1, and show the effect of the communication rounds constraints. We consider the number of communication rounds R varying in {2, 3, 4, 5}. Fig.6 shows that the regrets decrease with R increase. It is worth noting that the regret we reached at R = 4 and 5 are almost the same. This is because we need about log(T ) ≈ 4 communication rounds before eliminating all suboptimal arms. When R > 4, the term T in Thereon 2 tends to a constant and do not affect the total regrets. Combination of p and R: we ﬁnally combine these two communication constraints p and R with privacy parameter  = 1. With different pairs of (p, R), we compare the ﬁnal cumulative regret R (T ) achieved at time slot T as well as the total communication cost C (T ). Fig. 7 shows a clear trade-off between the communication cost and learning performance. Note that R = 5, p = 1 is close to the largest amount of required communication cost of the centralized setting. Correspondingly, the regret of this case is close to the performance in Fig. 4 with  = 1. 3) Heterogeneous reward setting: We use a 5-agent 10-armed small instance to illustrate the effectiveness of CDP-MAB on heterogeneous rewards. For the deﬁnition of heterogeneous reward, please refer to Section VI-D. From Fig. 8 we see that our proposed method obtain similar regrets under two reward structures, which reached 1/M of the single-agent performance under homogeneous reward. For heterogeneous rewards, the single-agent method cannot converge due to the inconsistency between the local and global best arm. After completing the local best arm identiﬁcation and entering the exploit phase, the inconsistency will bring a linear regret with T . In this part we investigate how the properties of graphs can affect the learning performance in the decentralized setting. 1) Network structure: We consider a 50-agent 100-armed homogeneous problem instance. We set c , the communication cost for each agent-to-agent link as 1. The agents are connected based on four kinds of graphs: {Fully-connected, Star, Ring, Random}. Some examples are shown in Fig. 9. From Fig. 10 we can see that the Star and Fully-connected graph achieve the smallest regrets while the Ring graph incurs larger regret. This is consistent with our results in Theorem 3. The R (T ) is equal to the R (T ) plus an item dominated by the diameter of the graph. The diameters of Fully-connected and Star graph are 1 and 2 respectively, thus their performances are close to the centralized setting. The SPIN protocol runnning in the Ring graph has the largest delay, leading to the largest regret caused by the local exploitation. The communication cost is determined by both the diameter and the total number of edges. Therefore, regret and communication cost are not directly proportional. The Star graph is closest to the centralized setting and has the smallest communication cost, since the information can be completely diffused in the network as long as it is transmitted in two steps. To achieve this goal, we need to create M(M − 1)/2 edges in the fully-connected graph, which introduces a huge communication cost. In this section, we divide 100 agents into several components with different properties. We set c = 50 and c = 1. 1) Component structures: We list the structure of each component q a in Table II. The sub-graphs in case1, case2 and case3 are both fully connected, however, the number of agents assigned in each component is different. Case 1, case 4 and case 5 have the same number of agents in each components while communicating over different structures of sub-graphs. In particular, case 3 can be regarded as the fully-decentralized setting where all agents are located at the vertices of one graph. Case 6 can be regarded as a centralized setting since there is only one agent in each component, which is directly connected to the server. From Fig. 11 we can see that, although case1, case2 and case3 can ﬁnally achieve similar per-agent regrets, while the communication cost gradually increases. This is because maintaining a fully connected graph with more agents requires us to establish more agent-to-agent links. Case 1, case 4 and case 5 have a balanced agents distribution. Comparing these three case, the sub-graphs close to central setting (like star graph) achieve smaller communication cost. It also implies that the hybrid structure signiﬁcantly reduces the communication cost compared with the centralized setting (case 6) or the fully-decentralized setting(case 3). 2) Heterogeneous Reward: Finally, we examine the inﬂuence of heterogeneous reward on the above three communication structures. Fig. 12 is consistent with our statements in Corollary 3. The CDP-MAB achieves the smallest regret. As we analyzed before, it can completely avoid the local bias by the coordination of the server. The other two structures are slightly affected by the heterogeneous reward. The reason is that the agents perform local exploitation during communication rounds (waiting for consensus or sink agent collection). Due to the smaller size of the sub-graph, the hybrid structure can achieve smaller delay as well as better performance than the decentralized network. In this paper, we propose a privacy-preserving communication-efﬁcient framework to tackle the privacy leakage and large communication overhead issues in the FMAB problem. To protect the user privacy, we use DP techniques by adding noise before the agents send their local parameters. Theoretical results show that the DP mechanism brings a trade-off between privacy and utility. Furthermore, partial participation and less frequent communication strategies are utilized to reduce the communication cost. Decentralized structure is combined with GIS protocol to realize global information synchronization at the end of each communication round. Hybrid structure and SAC protocol are together considered to complete local aggregation inside the component ﬁrst, and then implement global aggregation at the server. The above two schemes introduce an additional item related to the graph/component structure on the basis of the centralized setting regret. We compared the effectiveness of the three structures from both theoretical and experimental results. We would like to thank Prof. Christina Fragouli from UCLA for her valuable discussions and comments when we were conducting this work. We also thank Guangfeng Yan for his insightful discussion for the theoretical proofs. ≤ |(¯y (r) − µ ) − (¯y (r) − µ )| ≤ |(¯y (r) − µ )| + |(¯y (r) − µ )| (T ) ≤ S(r 8r 2 log(8Kr T ) 8 log(8|I |r T ) ≤ max{ 32 log( ) + 1 2 log(8KT ( log( ) + 1 16 log(8K( log( ) + 1 T ) ≤ max{ M∆ ∆ log T log(KT log T ) log(KT log T ) = O(max{ }) (22) M∆ ∆ The last equality is due to the fact that log( ) + 1 < log T . When E does not hold, the maximum regret is ∆ · T ·Pr{ } ≤ ∆ ≤ 1. Therefore, we only consider the regret when E holds with probability 1 − 1/T . Summing up all M agents and K arms we can conclude, C. Proof of Theorem 2 Proof. The communication cost C (T ) can be directly derived from C (T ) by replacing the participants M as pM and required communication round O(log T ) as ﬁxed R. We use the similar techniques in Theorem 1 to investigate the regret. The regret incurred by Algorithm 1 can be decomposed by the local exploration of each suboptimal arm k before it it is eliminated by the central server. We deﬁne r to be the epoch up to which ∆ exceeds 2 = 2∆ . We then show that after round r , arm k will be eliminated properly with high probability. Notice the used in Theorem 1 is doubling-decreasing, while it is exponentially decreasing with scale R in the modiﬁed algorithm. This directly leads to a different r required for eliminate arm k. Step 1: Compared with S(r) set in Alg.1, the S (r) has a scale factor of or for the two terms inside the max{}. Since the value of C (r) is determined by S (r), it is scaled equally with respect to C(r) set in Alg.1. This means that the events the event = {∀k, r, |¯y (r) − µ | ≤ C (r)} for all arm k in all epoch r still holds with probability 1 − , where ¯y (r) = ¯y (r) is the empirical averaged mean aggregate from N participants. Step 2: If E holds, in any epoch r, two of the following events happens: 1) the optimal arm always remains in epoch r; 2) the algorithm eliminates all suboptimal arms with gap ∆ larger than 2 = 2∆ . This also demonstrates that the second best arm with gap ∆ is removed from the active arm set after the R−th communication since = ∆ = ∆. We conclude by computing the total number of arms pulls n (T ) required for each suboptimal arm k at each agent i. Speciﬁcally, arm k 6= 1 does not survive round r with, 16r 6 log(r KT ) 96 log(r KT ) max p∆ · ∆ Mp ∆ · ∆ 16R 6 log(R KT ) 96 log(R KT ) max p∆ · ∆ Mp ∆ · ∆ = O( max log(R KT ), log(R KT ) p∆ Mp ∆ log(R KT ) log(R KT ) ≤ O( max p∆ Mp ∆ log(R KT ) log(R KT ) ≤ O( max ) (26) M∆ The second last equality is due to the fact ∆ = O( ). If ∆ < , even if we play suboptimal arms for all T slots, we can only incur a regret less than 1. The last equality is due to > T and p > p Summing up all M agents and K arms, the regret is upper bounded by, log(R KT ) log(R KT ) · n (T ) = O( max ) (27) M In order to make our performance not worse than the non-communication case, we need to ensure that T /p ≤ M → R ≥ log T/ log pM. We can conclude the cumulative regret of the modiﬁed algorithm by setting the min{M, } operation. We ﬁrst investigate C (T ). According to our deﬁnition of communication cost, it is determined by the number of connections established in all communication rounds. We ﬁrst analyze the total number of required communication rounds. Consider that S(r) and C(r) we set in Alg.2 are the same as Alg.1, and the GIS protocol can ensure that each agent can aggregate the same average mean as the server in Alg.1 after communication round r. So we can directly use the conclusion in Theorem 1, that is, all agents can synchronously identify the best arm after log( ) + 1 rounds. We next examine the number of connections established by each communication round. First, the duration of each round is t time slots. In each time slot, the upper bound of connections is the number of edges in the graph /2. It is also known that the max t is the diameter of the graph d , which is the time delay for the two vertices farthest apart on the graph to receive information from each other. Thus, the upper bound of connections established by each communication round is d /2. Multiply this with the total required communication round we can conclude C (T ). We then calculate the regret R (T ), which can be further divided into two terms as: If event E does not hold, the max regret that can be caused by each pull is 1. Then at most Md regret will be introduced at round r and Md log( ) + 1 will be incurred by all communication rounds. We can bound the R by: The last equality is due to the facts log T/T → 0 with large T . We complete the proof by summing up R (T ) + R (T ). Proof. The communication cost C (T ) can be directly derived C (T ) by replacing the required communication rounds as R. We then calculate the regret R (T ), which can be further divided into two terms as: log(R KT ) log(R KT ) O(min{M, T } · max + M(d − 1) ) (31) | {z } M | {z } The ﬁrst term is caused by local explorations of all M agents, which recover the result of Alg.1 R (T ) = R (T ) with p = 1. Next, we examine the second term introduced in each slot of t when exploit suboptimal arm. Recall the proof in Theorem 2, when event E holds, suboptimal arm with ∆ > 2 2∆ will be eliminated after round r − 1 and the optimal arm is never eliminated (Step 2). Therefore, each pull can cause at most ∆ regret and each communication round incurs at most Md regret. Summing up all required round we obtain: M(d − 1) (∆ ) = M(d − 1)(1 + ∆ + ∆ + ... + ∆ 1 − ∆ = M(d − 1)( ) (32) 1 − ∆ If event E does not hold, the max regret that can be caused by each pull is 1. Then at most Md regret will be introduced at round r and Md R will be incurred by all communication rounds. We can bound the R by: 1 − ∆ (T ) ≤ Pr{E }(M(d − 1)( )) + Pr{ }(M(d − 1)R) 1 − ∆ 1 − ∆ ≤ M(d − 1)( ) = O(M(d − 1)) (33) 1 − ∆ The last equality is due to the facts: i) ≤ 1 since R ≥ 1; ii) R is no larger than T . We complete the proof by summing above two terms. If event E does not hold, the max regret that can be caused by each pull is 1. We have, ≤ (1 − )(2M max {d − 1}(1 − ∆)) + ( )M max {d − 1} log( ) + 1 log T ≤ M max {d − 1}(2 + ) = O(Md max {d − 1}) (35) The last equality is due to the fact → 0 with large T . We complete the proof by summing (T ) and R (T ).