Designing recommendation systems with limited or no available training data remains a challenge. To that end, a new combinatorial optimization problem is formulated to generate optimized item selection for experimentation with the goal to shorten the time for collecting randomized training data. We ﬁrst present an overview of the optimized item selection problem and a multi-level optimization framework to solve it. The approach integrates techniques from discrete optimization, unsupervised clustering, and latent text embeddings. We then discuss how to incorporate optimized item selection with active learning as part of randomized exploration in an ongoing fashion. Recommender systems have become central in our daily lives and are widely employed to help users discover relevant content. The classical setting is composed of a set of users, U , and a set of items, I, from which top-k items are chosen and shown to the user. In this setting, notice there is an apriori decision to determine the universe of items I that can be recommended. We refer to this combinatorial problem as the Item Selection Problem (ISP). In our recent work[Kadioglu et al., 2021], we presented a multi-level optimization approach for selecting items to be included in experimentation. The main goal of this approach is to minimize the cardinality of the item universe while maximizing item diversity. By minimizing the cardinality, we reduce the experimentation time window and mitigate undesired user experience and business impacts while the recommender system is collecting the necessary training data to build personalization models. To that end, we show how to use a latent embedding space to calculate diversity measures between items and maximize the diversity of the selected items. Using the item embedding we propose a simple warm-start procedure to enable transfer learning from the randomized exploration phase to the personalized exploitation phase. More broadly, our hybrid approach stands out as an integration block between modern recommender systems and classical discrete optimization techniques. Figure 1: Item selection to speed-up experimentation for personalized recommendations. In this paper, we start with an overview of the problem setting (Section 2) and then present our multi-level optimization framework (Section 3) based on[Kadioglu et al., 2021]. We then show how to apply transfer learning via warm-start (Section 4) to increase personalization capacity. The work outlined so far considers solving ISP once at the inception of the system. Our main contribution is a proposal to take this approach a step further by incorporating active learning (Section 5) for effective experimentation and exploration in a continuous fashion. Item Selection Problem (ISP): Given a set of items I, the goal of the ISP is to ﬁnd the minimum subset S ⊆ I that covers a set of labels Lwithin each category c ∈ C while maximizing the diversity of the selection S in the latent embedding space of items E(I). Illustrative Example: Consider for example a movie recommendation system. In a movie recommender system, the items I correspond to all available movie titles that could be recommended. The categories of interest, C can include the genre and language for example. Within each category c ∈ C, we can have a set of labels for genre (e.g., action, comedy, thriller) and language (e.g., English and French). The ISP seeks to include at least one movie from each label Lfor different categories c ∈ C, while maximizing the diversity of selected movies in the latent embedding space E(I). The latent representation can be based on textual data (e.g., synopses, movie reviews) or image data (e.g., cover art). The ISP is most relevant when there exists limited or no historical data. As illustrated in Figure 1, randomized experimentation is employed to collect training data Dthat is used to build personalization models M. The longer the exploration phase takes, the worse user experience and business outcomes are. To mitigate this, our strategy focuses on solving the ISP to guide randomized exploration which is later augmented with warm-started models M. Our approach to solving the ISP is closely related to the classical Set Covering Problem (SCP)[Beasley, 1987]which we embed in a multi-level optimization framework. It consists of three levels; ﬁnding the minimum subset size, maximizing diversity and maximizing coverage within a ﬁxed bound. Minimum subset size: Let Pbe a standard covering formulation to select a subset of items that cover all predeﬁned labels. Assume unicost selection ⊆ I is the solution with k number of selected items. Maximizing diversity: Given the minimum subset size k from the solution of P, we cluster the embedding space of items E(I) into k clusters and let K denote the cluster centers. We reformulate Pby changing its cost structure such that the inclusion of item i incurs cost, c, based on the distance to its closest cluster. The solution of P, denoted by diverse selection, is the minimum subset of items that are most spread out from each other in the embedding space E(I) while still covering all predeﬁned labels. Bounded subset size: Given t ≤ |P| we select up to t items from diverse selection such that coverage is maximized and refer to this formulation as P. Bringing these components together, Algorithm 1 depicts our multi-level optimization framework that consists of solving P, Pand P. For more details and the problem formulations, we refer to our recent work[Kadioglu et al., 2021]. Given the solution from ISP, the experimentation phase can start, which yields training data Dthat is used to build personalization model M. As shown in Figure 1, we can use transfer learning[Caruana et al., 2004]to expand the capacity of personalization via warm-start. We propose a procedure to warm-start items s∈ S: I \ S to build Mby sharing knowledge from M. We take advantage of the latent embedding space E(I) to compute pairwise distances between items and ﬁnd the closest item s ∈ S for each untrained item s. To use s for the warm-start of s, we enforce distance(s, s) ≤ w for w > 0 to ensure that the items are sufﬁciently similar. We obtain the distance threshold w from the distribution of pairwise distances within a speciﬁed quantile q. Notice how this allows us to dynamically set the threshold w for the data at hand without requiring a tuning process. For transfer learning between s and s, we leverage the training data Dor trained parameters of model M. Multi-Level Optimization for ISP(I, M, E, t) In: Items: I In: Incident Matrix: M [label][item] In: Embedding Space: E(I) In: Maximum Subset Size: t Out: Selected Items: S ⊆ I // First Level: Minimize the subset size Formulate P(I, M) unicost selection ← solve(P) // Second Level: Maximize diversity k ← |unicost selection| K ← cluster(E(I), num clusters = k) Initialize cost ← zeros(|I|) for all item ∈ I do cost← min(distance(item, centroids ∈ K)) end for Formulate P(I, M, cost, unicost selection) diverse selection ← solve(P) // Third Level: Maximize bounded coverage t ← |diversity selection| Formulate P(diverse selection, M, t) S = max coverage ← solve(P) return S The key idea behind Active Learning (AL)[Settles, 2009]is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from which it learns. In the context of recommender systems, this is accomplished by letting the system inﬂuence which items a user is exposed to in order to learn users’ preferences[Rubens et al., 2015]more efﬁciently. In this section, we outline how the ISP can be incorporated in an active learning framework to continuously inform randomized exploration. For an operational recommender system to work effectively in a dynamic environment, we need to solve two challenging problems: initially, on Day–0, a large number of items have no or inadequate feedback data. Subsequently, on Day–1+, new items are periodically added to the system with no historical training data to learn from. Solving the ISP, as outlined earlier, addresses the ﬁrst problem on Day–0. It provides an ofﬂine selection that helps minimize the cardinality of the item universe while maximizing the diversity of items. The important realization is that there is an ongoing need for experimentation for a recommender system to continue to explore users’ behaviors and interests. In fact, the continued exploration is a more challenging task than solving the oneshot ISP. In the beginning, all items are candidates for the initial ISP selection. However, as time progresses, the system needs to distinguish between items that are trained effectively vs. items for which further engagement data is still necessary. Figure 2: ISP with active learning in exploration. Recommender systems need to balance exploration and exploitation in some form. Hence, the ISP continues to be relevant beyond Day–0, and value can be gained by solving it in an ongoing fashion and incorporating its solution dynamically as the system proceeds. As part of our ongoing work, let us share initial directions on how the ISP can be further integrated into the exploration component of a recommender system. The main goal of this line of research is to adopt an exploration strategy that is not purely random but provides a principled approach that can be tailored to speciﬁc criteria. For instance, beyond improving the accuracy of recommendations, we can aim to explore items to warm-start the maximum number of cold items. Figure 2 illustrates our initial design on how to perform active exploration by integrating ISP within the recommender system. In contrast to Figure 1, instead of performing a onceoff item selection, the idea is to solve for which items to include in the exploration continuously. As shown in Algorithm 1, the multi-level optimization algorithm for solving the ISP can be applied recursively to select a ﬁxed number of items from the set of remaining cold (untrained) items without replacement until all items are sufﬁciently trained. Another exploration policy is to select items from iterations until all are selected and at each iteration with ISP, items are assigned a weight based on the order in which it was selected. The earlier selected items receive a higher weight. Using these weights, we can generate recommendations in the exploration stage that favor items yielding better coverage of item labels and are more diverse as guaranteed by the provably optimal ISP solution. Alternative exploration policies could also be readily implemented with minor modiﬁcations to the ISP. For example, we could assign the highest weight to items that greedily warm-start the most number of cold items. Moreover, the active exploration component can be made interactive with human inputs, letting the procedure become an instance of human-in-loop optimization. Besides coverage and diversity, the active exploration can be further guided to cover other preferences, such as time-sensitive or seasonal information. For instance, in an e-commerce application, a certain category of seasonal items can be weighted more in the exploration for a ﬁxed period of time. Similarly, newly released products with large potential to promote sales could be of more interest from a business perspective. Such constraints (preferences) can be incorporated into our ISP formulation as side-constraints. Let us ﬁrst provide a summary of our numerical results that demonstrate the speed-up enabled by solving the ISP in the randomization phase while ensuring diversity and transfer learning capacity. We then conclude with an evaluation protocol that can be used to evaluate the effectiveness of combining ISP in the Active Learning process as suggested in Section 5. Datasets: We use the Goodreads Book Reviews[Wan and McAuley, 2018]dataset with 11,123 books and the MovieLens (ml-25m) Movie Recommendations[Harper and Konstan, 2015]dataset with 62,423 movies. We consider two randomly selected subsets, small and large versions with 1,000 and 10,000 items, respectively. We are interested in selecting movies (books) for exploration that cover all (or maximum) genres, producers (publishers), languages, and genrelanguage (genre-publisher) combinations. Detailed descriptions of the datasets can be found in[Kadioglu et al., 2021]. Analysis of coverage: To ﬁnd the minimum set of items covering all labels, we solve Pand compare the number of selected items and the resulting label coverage between our approach and several challenger algorithms. We show that all labels can be covered by selecting a fraction of items thereby reducing exploration time. Compared to Random and KM eans based challengers the set covering formulations are shown to provide substantially better coverage. Pachieves complete coverage by selecting between 5% and 37% of available items. For the same number of items, Random and KMeans can only achieve coverage of between 30% and 45%. Analysis of bounded coverage: To further demonstrate potential speed-up in random experimentation, we vary the subset bound t and analyze the label coverage before and after warm-starts for P, KM eans and Random. Critically we show that for a given coverage level, the required number of items t is always lower for P and that for the same bounded t, it yields coverage 2-4X higher than challenger methods. After the warm-start, coverage increases for each method at each t, and notably, the coverage for Pcontinues to rank highest. Analysis of warm-start: To assess the effectiveness of the warm-start procedure we perform sensitivity analysis on the distance quantile q and evaluate the average number of labels covered per item (unit coverage) after warm-start. We show that Pconsistently has the highest unit coverage and performs signiﬁcantly better than Random and KM eans for top (semi-) deciles, i.e., q ≤ 0.1. Analysis of embedding space: To evaluate the sensitivity of ISP to the item embedding, we solve Pwith several embeddings using TEXTWISER[Kilitcioglu and Kadioglu, 2021]. Besides a baseline Term Frequency Inverse Document Frequency (TFIDF)[Jones, 1972]embedding, we employ Word2Vec[Mikolov et al., 2013], GloVe[Pennington et al., 2014]and Byte-Pair[Sennrich et al., 2016]embeddings to learn word and character level information in the movie and book descriptions. We show that the different embeddings yield similar unit coverage, but that sophisticated NLP methods such as Word2Vec can provide better coverage compared to simpler frequency based embeddings. Given the signiﬁcant reductions that ISP provides in the time window required for the initial experimentation, our current work is focused on assessing additional beneﬁts of ISP + AL as proposed in Section 5. Ideally, it is desirable to test active exploration in an online recommendation setting. However, the deployment of such a system for testing requires signiﬁcant effort, and it is also highly undesirable from an end-user perspective. It is therefore mandatory to start with an ofﬂine evaluation. Designing an experimental protocol that can closely mimic an online assessment is non-trivial. For that purpose, we propose an ofﬂine simulation using the following experimental protocol: Data: Given a set of K number of items, we randomly select k items and set those as warm items, referring to the set M in Figure 1 and Section 4. We set the remaining items as cold items. For all K items, we retrieve their categories and latent embedding as in MovieLens, Goodreads, or other wellknown benchmarks. Approaches: The high-level design given in Figure 2 leaves it open how to combine ISP and AL. The randomized exploration using all K items is a simple baseline method for comparison. Another baseline strategy is to apply the ISP as-is at each round following Day–0. Alternatively, we can also consider the order of the selection of cold items. Items selected by ISP earlier in the process, which means they become part of the optimized selection quickly, will have higher weights in the randomized exploration. If we can distinguish between items whose trained models are still unstable, for example, by quantifying the uncertainty in their predictions as shown in[Wang and Kadioglu, 2021], we can put higher weights to the uncertain models in randomized exploration. Evaluations: We repeat the simulation n times with varying sets of K items. For each simulation, we enable exploration for one time period. We then compare how effective the different active exploration strategies are in reducing the number of cold items compared to randomized exploration. This is captured by the total number of models available in the Personalized Recommendations component in Figure 1 and Figure 2. This is the union of the set of previous warm items, the set of newly added warm items after exploration, and the set of cold items being successfully warm-started by the former two sets. After the exploration, the union of all available models is M∪ M, again shown in both ﬁgures. We also calculate the ratio of successful warm-starts by dividing the number of warm-started items by the total number of cold items subject to warm-start, with a higher percentage indicating a more successful strategy.