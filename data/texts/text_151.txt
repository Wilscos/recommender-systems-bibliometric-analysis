Carnegie Mellon University Pittsburgh, PA 15213 Carnegie Mellon University Pittsburgh, PA 15213 Carnegie Mellon University Pittsburgh, PA 15213 The multi-armed bandit (MAB) problem falls under the class of sequential decision making problems. In the classical multi-armed bandit setting, the player is asked to sample one of the receives a random reward distributions are assumed to be unknown to the player, and the most commonly studied objective is to maximize the long-term cumulative reward; e.g., see the early work by Lai and Robbins [ classical settings [ contextual [5], structured bandits [6] etc. In this paper we consider the problem of best-arm identiﬁcation in multi-armed bandits in the ﬁxed conﬁdence setting, where the goal is to identify, with probability 1− δfor some δ >0, the arm with the highest mean reward in minimum possible samples from the set of armsK. Most existing best-arm identiﬁcation algorithms and analyses operate under the assumption that the rewards corresponding to diﬀerent arms are independent of each other. We propose a novel correlated bandit framework that captures domain knowledge about correlation between arms in the form of upper bounds on expected conditional reward of an arm, given a reward realization from another arm. Our proposed algorithm C-LUCB, which generalizes the LUCB algorithm utilizes this partial knowledge of correlations to sharply reduce the sample complexity of best-arm identiﬁcation. More interestingly, we show thatP the total samples obtained by C-LUCB are of the form Ologas opposed toP the typical Ologsamples required in the independent reward setting. The improvement comes, as the O(log(1/δ)) term is summed only for the set of competitive armsC, which is a subset of the original set of armsK. The size of the setC, depending on the problem setting, can be as small as 2, and hence using C-LUCB in the correlated bandits setting can lead to signiﬁcant performance improvements. Our theoretical ﬁndings are supported by experiments on the Movielens and Goodreads recommendation datasets. Karms at every roundt= 1,2, . . .. Upon sampling armkat roundt, the player 1]. Since then, the reward maximization problem has received attention in both Figure 1: The ratings of a user corresponding to diﬀerent versions of the same ad are likely to be correlated. For example, if a person likes ﬁrst version, there is a good chance that they will also like the 2nd one as it also related to tennis. However, the population composition is unknown, i.e., the fraction of people liking the ﬁrst/second or the last version is unknown. Best-arm Identiﬁcation in Bandits with Independent Arms. the cumulative reward, an alternative objective in the Multi-Armed Bandit setting is to identify the best arm (i.e., the arm with the largest mean reward) from as few samples as possible. While reward maximization has been studied extensively, the best-arm identiﬁcation problem is seldom explored in settings outside of the classical MAB framework, i.e., the setting where rewards corresponding to diﬀerent arms are independent of each other. The best-arm identiﬁcation problem can be formulated in two diﬀerent ways, namely ﬁxed conﬁdence [ and ﬁxed budget [ parameter possible identiﬁcation of the best arm with a probability of at least 1 budget setting, the number of samples that the player can receive is ﬁxed, and the goal is to identify the best arm with the highest possible conﬁdence. In this paper, we focus on the ﬁxed conﬁdence setting. racing/successive elimination, law of iterated logarithm upper conﬁdence bound (lil’UCB) and lower and upper conﬁdence bound (LUCB) based approaches. These algorithms maintain upper and lower conﬁdence bound indices for each arm and usually stop once the lower conﬁdence index of one arm becomes larger than upper conﬁdence bound of all other arms (discussed in more detail in Section 3). These three approaches diﬀer in their approach of sampling arms. The successive elimination approach samples arms in a round robin manner, lil’UCB samples the arm with the largest upper conﬁdence bound index at round samples two distinct arms at each round, ﬁrst it samples the arm with the largest empirical mean and then amongst the rest it samples an arm with the largest upper conﬁdence bound index. tion settings, such as clinical trials [ δand their goal is to achieve the fastest (i.e., with the least number of samples) The best arm identiﬁcation problem has been explored in the classical MAB framework 10,11,12,13,14,15] and three distinct approaches have shown promise, namely, the These best-arm identiﬁcation algorithms have found their use in a wide variety of applica- Figure 2: Upon observing a reward upper bound on the conditional expectation of the reward from arm reward to diﬀerent arms. advertisements, items to be ranked and hyperparameters as the arms in the multi-armed bandit problem. Best-arm Identiﬁcation when Rewards are Correlated across arms. tioned best-arm identiﬁcation algorithms all operate under the assumption that the rewards from diﬀerent arms are independent of each other; e.g., at a given round obtained from arm have received if they sampled another arm applications of MABs. For instance, the response of a user for diﬀerent advertisements in an ad-campaign is likely to be correlated as the ad designs may be related or starkly diﬀerent with each other (see Figure 1). One way to learn these correlations would be to pull multiple arms at each round partial information about such correlations is available a priori. In practice, the presence of such correlations may be known beforehand either through domain expertise or through controlled studies where each user is presented with multiple arms. For example, before starting ad campaign, partial information may be known about the expected reward we would receive from a user by showing that ad version argument can be made in the application domain of clinical trials, namely in identifying the best drug for an unknown disease. There, the eﬀect of diﬀerent drugs on an individual may be correlated if the drugs share similar or contrasting components among them. In this context, the correlations would be expected to be known by the domain expertise of the physicians involved. The current best-arm identiﬁcation algorithms cannot leverage these correlations to reduce the number of samples required in identifying the best arm. This papers aims to ﬁll this gap in the literature through a new MAB model introduced next. A Novel Correlated MAB model. framework where rewards corresponding to diﬀerent arms are correlated. We model the partial knowledge of correlations through pseudo-rewards that represent upper bounds on the rfrom armk. These pseudo-rewards models the correlation in rewards corresponding ] and hyperparameter optimization [18] by treating diﬀerent diﬀerent drugs/treatments, conditional mean rewards. The pseudo-rewards provide us an upper bound on the expected reward from arm `, given that the response from arm k was r (See Figure 2), i.e., A key advantage of this model is that pseudo-rewards are just upper bounds on the conditional expected reward and they can be arbitrarily loose. In the case where all bounds are trivial, our framework reduces to that of the classical Multi-armed bandit setting. This model was ﬁrst proposed by us in [ seemingly related models are the structured [ models. Comparison with Contextual and Structured bandits: context features of the user (i.e., the user to whom ad is recommended) are assumed to be known, and the goal is to learn a mapping from the context features to the expected rewards so that each user can be given a personalized recommendation. In contrast, our model focuses on a setting where context features of the users are not known and the goal is to ﬁnd a single recommendation for the entire demographic. restrictions on the joint probability distribution of rewards. To the best of our knowledge, existing work on best-arm identiﬁcation in structured bandits focus on settings where mean rewards of the arms are related to one another through a hidden parameter the mean reward of arm It assumes that the mean reward mappings parameter is unknown. While the mean rewards are related to one another in these works, the rewards are not necessarily correlated. A more detailed comparison is presented in Section 3. In this work, we explicitly model the correlation through knowledge of pseudo-rewards. Proposed C-LUCB Algorithm and its Sample Complexity. correlated bandit model, we then focus on designing best-arm identiﬁcation algorithms, that are able to make use of this correlation information to identify the best-arm in fewer samples than the classical best-arm identiﬁcation algorithms. In particular, we propose an approach that makes use of the pseudo-reward information and extends the LUCB approach to the correlated bandit setting. Our sample complexity analysis shows that the proposed C-LUCB approach is able to explore certain arms without explicitly sampling them. Due to this, we see that these arms, termed as non-competitive contribute only an O(1) term in the sample complexity as to the typical O are able to provide better sample complexity results than LUCB in the correlated bandit setting. In particular, the LUCB algorithm stops with probability 1 most mean reward of optimal arm the gap between best and second best arm and after at most Our work falls under the class of structured bandits, which in its full generality, poses with 2≤ |C| ≤ Kdepending on the problem instance. As the size of the setCcan Figure 3: This plot illustrates the number of samples required by diﬀerent algorithms to identify the best movie genre out of the 18 possible movie genres in the Movielens dataset with conﬁdence 1 arm. As our proposed C-LUCB and C-LUCB++ algorithms utilize correlation information, they identify the best arm in fewer samples relative to Racing, lil’UCB, LUCB and LUCB++. be smaller than of best-arm identiﬁcation. This theoretical advantage gets reﬂected in our experiments on two real-world recommendation datasets, namely, Movielens and Goodreads. For instance, Figure 3 illustrates the performance of our proposed algorithms in a correlated bandit framework, where the goal is to identify the best movie genre from the set of 18 movie genres in the Movielens dataset. As our proposed approach utilizes the correlations in the problem, they draw fewer samples than the Racing, lil’UCB and the LUCB based approaches. Organization of the rest of the paper. multi-armed bandit framework, where correlation between arms is captured in the form of pseudo-rewards. We also discuss how pseudo-rewards can be computed in practical settings in Section 2. In Section 3, we review state-of-the-art best-arm identiﬁcation algorithms such as successive elimination (or racing), lil’UCB, and LUCB designed for the classical (independent arm) framework. We also discuss how our proposed correlated multi-armed bandit framework compares with the structured and linear bandit frameworks that have been studied previously. In Section 4 we propose the C-LUCB algorithm, and compare it with state-of-the-art approaches. We discuss several variants of C-LUCB in Section 6. In Section 5 we analyze the sample complexity analysis of C-LUCB and discuss its proof technique and implications. This analysis reveals that utilizing correlations can lead to signiﬁcant reduction in the number of samples required to identify the best-arm. Finally, in Section 7 we demonstrate the practical applicability our proposed model and algorithm via extensive experiments on real-world recommendation datasets. Table 1: The top row shows the pseudo-rewards of arms 1 and 2, i.e., upper bounds on the conditional expected rewards (which are known to the player). The bottom row depicts two possible joint probability distribution (unknown to the player). Under distribution (a), Arm 1 is optimal whereas Arm 2 is optimal under distribution (b). Consider a Multi-Armed Bandit setting with sample an arm k we denote the arm with the largest mean reward as the best-arm In the ﬁxed-conﬁdence setting [ as possible. In particular, given some round T (a random variable) and declares an arm k Put diﬀerently, we aim to ﬁnd the best arm with probability at least 1 the total number of samples drawn from the arms. We note that the number of samples can be diﬀerent from the number of rounds multiple arms in one round. Using the total number of samples drawn until round us to compare them fairly against algorithms that draw only one sample at each round (e.g., lil’UCB). are independent. That is, implies that, a user corresponding to diﬀerent arms might be correlated, we consider a setup where reward from arm `. Consequently, due to such correlations, we have E [R distribution of correlated arms in the form of pseudo-rewards, as deﬁned below: Deﬁnition 1 pseudo-reward of arm conditional expected reward of arm `, i.e., The classical multi-armed bandit setting implicitly assumes that the rewardsR, R, . . . , R (r|r)6=f(r), withf(r) denoting the probability distribution function of the In our problem setting, we consider that the player has partial knowledge about the joint Observation from Arm 1 Table 2: If some pseudo-reward entries are unknown (due to lack of domain knowledge), those entries can be replaced with the maximum possible reward and then used in the C-LUCB algorithm. We do that here by entering 2 for the entries where pseudo-rewards are unknown. Remark 1. reward and not hard bounds on the conditional reward itself. This makes our problem setup practical as upper bounds on expected conditional reward are easier to obtain, as illustrated below. This information can be obtained in practice through either domain and expert knowledge or from controlled surveys. For instance, in the context of medical testing, where the goal is to identify the best drug to treat an ailment from among a set of the eﬀectiveness of two drugs is correlated when the drugs share some common ingredients. Through domain knowledge of doctors, it is possible to answer questions such as “what are the chances that drug we can infer the pseudo-rewards. Computing Pseudo-Rewards from domain knowledge or historical data. pseudo-rewards can also be obtained from domain knowledge or through oﬄine pilot surveys in which users are presented with all Through such data, we can evaluate an estimate on the conditional expected rewards. For example in Table 1, we can look at all users who obtained 0 reward for Arm 1 and calculate their average reward for Arm 2, say Remark 2 are unknown, then all pseudo-reward entries can be ﬁlled with maximum possible reward for The pseudo-reward information consists of a set ofK × Kfunctionss(r) over [0, b]. E[R|R= 0], we can use any one of the following approaches to set the pseudo-reward (0). 1.The pseudo-rewards(0) can be set toˆµ(0) +ˆσ(0), whereˆµ(0) is the empirical average of conditional rewards ofRgivenR= 0 andˆσ(0) is the empirical standard deviation. Adding the standard deviation ensures that the pseudo-reward is an upper bound on the conditional expected reward E[R|R= 0] with high probability. 2.Alternately, pseudo-rewards for any unknown conditional mean reward could be set to b, the maximum possible reward for the arm (recall thatR∈[0, b]). Table 2 shows an example where unknown pseudo-rewards are set to 2, the maximum possible reward. 3.If through the training data, we obtain a soft upper bounduonE[R|R= 0]that holds with probability 1− δ, then we can translate it to the pseudo-rewards(0) = u × (1 − δ) + 2 ×δ, (assuming maximum possible reward is 2). each arm, that is, paper reduces to the setting of the classical Multi-Armed Bandit problem. distribution of rewards is unknown. For instance, Table 1(a) and Table 1(b) show two joint probability distributions of the rewards that are both possible given the pseudo-rewards at the top of Table 1. If the joint distribution is as given in Table 1(a), then Arm 1 is optimal, while Arm 2 is optimal if the joint distribution is as given in Table 1(b). Consider a scenario where a company needs to run a display advertising campaign in a community for one of their products, and their design team has proposed several diﬀerent designs. The traction (i.e., the number of clicks, time spent on the ad) that the company generates is likely to be dependent on the design that is used for publicity. In order to ﬁnd the best design, the company can run a best-arm identiﬁcation algorithm by viewing the problem as a multi-armed bandit problem. Here, at each round community enters the system and they show one of the The reward is received through the response of the user to the ad. A straightforward solution would be to treat this problem as a classical multi-armed bandit problem and use a well known best-arm identiﬁcation algorithm such as lil’UCB, LUCB or successive elimination to identify the best design for the community. But, in practice, the rewards corresponding to diﬀerent designs are likely to be correlated to one another. Consider the example shown in Figure 1, over there if a user reacts positively to the ﬁrst design, the user is also likely to react positively to the second ad as both ads are related to tennis. Such correlations, when accounted for in the form of pseudo-rewards, can help us identify the best-arm in much fewer samples relative to algorithms such as lil’UCB, LUCB and Successive elimination that do not account for correlations in choices. campaign performed in a diﬀerent demographic. For instance, from these surveys one can interpret information such as "users who like ad 1 representing tennis tend to like ad 2 that also represents tennis but not ad K which represents soccer". If a company wants to identify the best ad in a new demographic, it can use this learned correlation information to identify the best-ad in a quick manner. Note that the population composition in the two demographics may be very diﬀerent, i.e., the fraction of users liking tennis may be very diﬀerent, but it is likely that the correlation in choices remain consistent across the two demographics. One can also consider the example of identifying best policy to publicize for a political campaign, where users preferences towards diﬀerent policies (i.e., climate change, gun control, abortion laws) are often correlated in all demographics, but the marginal distribution of people advocating for a single policy is very diﬀerent in diﬀerent communities. In such scenarios, transferring correlation information from one demographic to another by modeling them through pseudo-reward in our correlated bandit framework can help reduce the number of samples needed to identify the best-arm. of identifying the best drug for the treatment of an unknown disease. The eﬀectiveness of diﬀerent drugs is likely to be correlated as they often contain similar components. In such a While the pseudo-rewards are known in our setup, the underlying joint probability These correlations could be known from a controlled survey or a previous advertisement These pseudo-rewards can also be known from domain knowledge. Consider the problem Figure 4: A special case of our proposed problem framework is a setting in which rewards for diﬀerent arms are correlated through a hidden random variable X. At each round realization in bounds and upper bounds on realization 1, reward of arm 1 is a random variable bounded between 2 and 4. situation, the domain expertise of doctors can tell us "what are the chances that drug y will be eﬀective given drug x was eﬀective?". One can use a conservative upper bound on the answer to this question to model pseudo-rewards. Alternatively, such correlation information could also be obtained on how diﬀerent people react to diﬀerent drugs in a community. As the eﬀectiveness of drugs depends on underlying medical conditions of the patients, their response would be correlated. This correlation knowledge can then be transferred to identify the best treatment in a diﬀerent community, where the distribution of underlying medical conditions may be very diﬀerent. 2.3 Special Case: Correlated Bandits with a Latent Random Source The studied correlated multi-armed bandit can generalize several other interesting and unexplored multi-armed bandit problems. For example, one special case is the correlated multi-armed bandit model where rewards are correlated through a latent random source realization For the application setting of ad-recommendation, the random variable the features (i.e., age/occupation/income etc.) of the user. At each round a new user with feature for the whole population in as few samples as possible. The feature player due to privacy concerns. Additionally, the reward the k of upper and lower bounds on bounds can be probabilistic, e.g., they may hold with probability 0 instance, the information on prior information represents the knowledge that children of age 5-10 rate documentaries only in the range 1-3 out of 5 in 80% cases. While such prior knowledge may be known from domain expertise or previous ad-campaigns performed in ] (See Figure 4). In this problem setup, the hidden random variableXtakes an i.i.d. X∈ Xat roundtand upon pulling armkat roundt, rewardY(X) is observed. Xenters the system, and the goal is to identify the single best ad recommendation ad for the user with feature X. In this problem setup, the correlation information is known to the player in the form a diﬀerent demographic, the age distribution of the community may be unknown. Due to which, the best-arm remains unknown and it needs to be found in an online manner. translating the mappings represent an upper bound on the conditional expectation of the rewards. In this framework, we can construct pseudo-reward as follows: where reward by ﬁrst ﬁnding the range of values within which probability 1 with probability 1 at-most the pseudo-reward as shown in ﬁts in the general framework described in this paper and we can use the algorithms proposed for this setting directly. mean rewards of diﬀerent arms, but the parameter from [ hidden feature bounds on is to provide personalized recommendation to a user whose features the latent random source model (and the general correlated bandit model) is appropriate for application settings where the goal is to identify a single recommendation for the global demographic. random variable random source representation in the form of more complicated and one may not be able to represent them. It is important to note that our proposed model in the most general setting works without having to construct a hidden feature representation through which arms are correlated. This is a key advantage of our general model over the latent random source model and the model presented in [23], which requires modeling the problem through a hidden parameter utilizes the available prior information directly and our algorithms adapt to the information to identify the best-arm in fewer samples relative to classical best-arm identiﬁcation algorithms. The design of best-arm identiﬁcation algorithms in the ﬁxed-conﬁdence setting have three key design components: i) their sampling strategy, i.e., which arm to pick at round their elimination criteria, i.e., when to declare an arm as sub-optimal and remove it from This particular correlated bandit setting can be reduced to our general framework by (x) and¯g(x) are soft lower and upper bounds, i.e.,g(x)≤ Y(x)≤ ¯g(x) w.p. 1− κ, Mis the maximum possible reward an arm can provide. We evaluate this pseudomax¯g(x). As the maximum possible reward isMotherwise, we get The presented model resembles the structured bandit model studied in [23] in which 23] in two key ways – i) In [23], instead of a hidden random variableX, there is a are known, whereas in our model we consider the knowledge of soft upper and lower Note that the model presented in this subsection requires the understanding of hidden Table 3: All best-arm identiﬁcation algorithms have three key components, i) Sampling strategy at each round of the algorithm. We compare these for Racing, lil’UCB, LUCB and LUCB++ algorithms and see the diﬀerences in their operation. The indices used for our proposed C-LUCB and C-LUCB++ are deﬁned in (8) and (10). the rest of the sampling procedure; and iii) their stopping criteria, i.e., when to stop the algorithm and declare an arm as the best arm. mean conﬁdence bound on the mean of arm of arm index are maintained for each arm bound [9, 24] constructed such that the conﬁdence interval uniformly for all outside the conﬁdence interval [ In contrast to the Hoeﬀding bound, which are only valid for a ﬁxed and deterministic the anytime conﬁdence bound holds true uniformly for all We refer the reader to [ bounds B(n In order to accomplish the task of best-arm identiﬁcation, algorithms use the empirical ˆµ(t) for armkat roundt. In addition to this, upper conﬁdence bound and lower k,n(t), and the input conﬁdence parameterδ. In particular, the upper conﬁdence Note that the anytime conﬁdence interval bound the probability of the mean lying outside There are three well-known approaches to the best-arm identiﬁcation problem: i) Successive Elimination (also called racing) [ Conﬁdence Bound) [ we brieﬂy introduce these algorithms, and present a summary of their arm sampling strategies and elimination and stopping criteria in Table 3 [7] that provides a comprehensive survey of best-arm identiﬁcation in the ﬁxed conﬁdence setting. Successive Elimination or Racing: egy maintains a set of active arms fashion from the set of active arms and at the end of each round, it eliminates an arm from the set of active arms if the lower conﬁdence index of some other arm is strictly larger than the upper conﬁdence index of arm until a single arm is left in the set other algorithms, Exponential-gap elimination [ elimination to provide stronger theoretical guarantees. However, their empirical performance is not promising as noted in [7]. lil’UCB [9]: index till round arm as the best-arm. LUCB [13, 7]: the largest UCB index conﬁdence bound of the ﬁrst arm arms. similar manner to LUCB but constructs the upper conﬁdence and lower conﬁdence indices with diﬀerent conﬁdence parameters for and lower conﬁdence indices for each of these algorithms are presented in Table 3. Note that our metric for comparison is the total number of samples collectively drawn from the arms. As LUCB algorithms sample two arms at each round, the total number of samples drawn from the LUCB algorithms is two times the number of rounds number of samples and not the number of rounds performance of LUCB and lil’UCB algorithm. be either sub-Gaussian or bounded. Furthermore, if the class of distribution is known (e.g., U(n, δ) at roundtand stops when an arm has been sampled more thantimes t. In practice, the value ofαis taken to be 9. It then declares the most sampled (t) is the arm with the largest empirical reward till roundt, andm(t) is the arm with Subsequently, another algorithm LUCB++ [12,11] was designed that operates in a All the approaches described above work well for the case where rewards are known to Table 4: Description of the well-known best-arm identiﬁcation algorithms and the conﬁdence bound have evolved with time due to the development of tighter 1 our proposed algorithm in the rest of the paper. The reported sample complexity is for the task of identifying best movie genre from the set of 18 movie genres in the Movielens dataset. Experimental setup is described in detail in Section 7. it is known that rewards are Gaussian with known more approaches known in the literature, namely Top Two Thompson Sampling (TTTS) reward of each arm and then applies Thompson sampling on the posterior to obtain two samples. It stops when the posterior probability of an arm threshold the computation of posterior probability in each round of their algorithm. In [ evaluate a lower bound for the Multi-Armed bandit problem in the form of an optimization problem. They propose a tracking based approach, that solves the optimization problem at each round to obtain an estimated rate at which each arm should be sampled at round sample arms in proportion to that rate. More recently, [ to the track-and-stop algorithm that do not require solving an optimization problem at each round. Instead, they view the optimization problem as an unknown game and have sampling rules based on iterative saddle point strategies. All of the approaches listed above require knowing the class of reward distribution. Since we only assume that the rewards are bounded and not the class of distribution, we do not focus on extending TTTS or Tracking based approaches to the correlated bandit setting in this paper. B(n, δ) that they use for [0,1] bounded rewards. All the three types of algorithms , δ). We see that the algorithms perform best with the conﬁdence bound suggested in ], and hence we use that for all our implementations of Racing, LUCB, LUCB++ and ] and Tracking [29]. In TTTS, the player computes a posterior distribution on the mean τ(n, δ). The TTTS algorithm can be computationally intensive as it involves It is important to note that the performance of the algorithms described above depends critically on the tightness of the conﬁdence bound algorithm was proposed with the conﬁdence interval (See [ developed, which led to performance improvements in the LUCB algorithm. See Table 4 for a comparison diﬀerent conﬁdence bound developed over time and how they aﬀect the empirical performance of the best-arm identiﬁcation algorithms diﬀerent conﬁdence bounds B our knowledge, the tightest 1 random variables is proposed in [24], which constructs Due to this observation, which is also supported by empirical evidence in Table 4, we use the bound suggested by [ proposed algorithm. However, our algorithm and analysis extend to arbitrary 1 conﬁdence interval B(n oretical sample complexity (in terms of its dependency on the number of arms algorithm stops with probability 1 samples, where ∆ reward of arm k. And ∆ known that lil’UCB algorithm has a sample complexity O it avoids the sample complexity. However, it has been observed (both in [ its empirical performance is inferior to that of the LUCB algorithm. Due to this reason, we focus on proposing an algorithm C-LUCB that extends the LUCB approach to the correlated bandit setting. We have included the performance of lil’UCB in all our experiments. Unlike the regret-minimization problem, the best-arm identiﬁcation problem is relatively unexplored outside of the classical multi-armed bandit setting. A rare exception is the structured bandit setting, where mean rewards corresponding to diﬀerent arms are related to one another through a hidden parameter 13]) for [0,1] bounded random variables. Subsequently tighter bounds as in [7], [10] were We would also like to highlight the fact that lil’UCB is known to have the best known thebut the mean reward mappings case of structured bandits, where mean reward mappings are of the form to the player. The best-arm identiﬁcation problem has been studied in [ bandits and in [ bandits include global bandits [ in these special cases. Note that in the full generality, the structured bandit framework is simply a bandit problem with constraints on the joint probability distribution [ that setting has only been studied for the objective of regret minimization and not best-arm identiﬁcation. To the best of our knowledge, the structured bandits work studying best-arm identiﬁcation [ rewards of diﬀerent arms are related to one another. Our correlated bandit framework focuses on structured bandit settings by modeling the correlations explicitly through the knowledge of pseudo-rewards. which assumes that the arms are the nodes of known a weighted graph, with noting the weight between arms restriction on the relationship between mean rewards of individual arms by assuming thatP structured bandit framework as detailed below. In the correlated MAB framework, the rewards observed from one arm can help estimate the rewards from other arms. Our key idea is to use this information to reduce the number of ]; to the best of our knowledge the best arm identiﬁcation problem has not been addressed Recently, best-arm identiﬁcation was studied under the spectral bandit framework [37], w≤ R, where R is known to the player. The correlated bandit model considered in this paper is fundamentally diﬀerent from the The model studied here explicitly models the correlations in the rewards of diﬀerent arms at any given roundt. In structured bandits, the mean rewards are related to each other, but the reward realizations at a given round are not necessarily correlated. Similar to structured bandits, the work on spectral bandits [37] considers a setup with constrains between mean rewards of diﬀerent arms, but does not capture the correlations explicitly in their framework. It is also possible to use the structured bandit framework for the objective of identify best global recommendation in an ad-campaign. However, there are two major challenges i) In deciding upon the hidden parameterθthat we need to use, through which the mean rewards are related to one another. ii) Secondly, in the structured bandits framework, the reward mappings fromθtoµ(θ) need to be exact. If they happen to be incorrect, then the algorithms for structured bandit cannot be used as they rely on the correctness ofµ(θ) to construct conﬁdence intervals on the unknown parameterθ. In contrast, the model studied here only relies on the pseudo-rewards being upper bounds on the conditional expectationsE[R|R= r]. Our proposed algorithm works even when these bounds are not tight. The lack of hidden parameterθand pseudo-rewards being upper bounds on conditional expectations make the model studied in this paper more suitable for practical scenarios where the goal is to identify the best global recommendation. samples taken before stopping. We do so by maintaining the empirical pseudo-rewards of all pairs of distinct arms at each round t. In our correlated MAB framework, pseudo-reward of arm us an estimate on the reward of arm now deﬁne the notion of empirical pseudo-reward which can be used to obtain an optimistic estimate of µ The expected pseudo-reward of arm ` with respect to arm k is deﬁned as For convenience, we set sampling arm k. pseudo-reward, we now deﬁne auxiliary UCB indices, namely crossUCB and pseudoUCB indices, which are used in the selection and elimination strategy of the C-LUCB algorithm. Deﬁnition 3 arm k. Using these, we deﬁne the CrossUCB Index of arm ` with respect to arm k as Furthermore, we deﬁne i.e., the tightest of the K upper bounds, Note that the CrossUCB index for arm through the samples obtained from arm which coincides with the standard upper conﬁdence index used in the best-arm identiﬁcation literature. We use the conﬁdence bound suggested by [ of B(n t) times. Using thesen(t) reward realizations, we can construct the empirical pseudoˆφ(t) for each arm ` with respect to arm k as follows. (t) is deﬁned with respect to armkand it is only a function of the rewards observed by Observe thatE[s(R)] ≥ E [E [R|R= r]]=µ. Due to this, empirical pseudo-reward (t) can serve as an estimated upper bound onµ. Using the deﬁnitions of empirical , δ) for [0, b] bounded random variables, i.e., As pseudo-rewards are upper bounds on conditional expected reward, they can only be used to construct alternative upper bounds on the mean reward of other arms and not alternative lower bounds. Due to this reason, we keep the deﬁnition of lower conﬁdence index the same as that in the classical multi-armed bandit setting, i.e., In addition to the CrossUCB and the LCB index for each arm, we now deﬁne the PseudoUCB index of arm and analysis of our proposed algorithm. Deﬁnition 4 respect to arm k as follows. Note that the PseudoUCB Index uses a conﬁdence bound, in the UCB1 algorithm ([ property that the pseudoUCB index t. This property allows us to show desirable sample complexity results for our proposed algorithm in Section 5. We now present the C-LUCB algorithm, that makes use of the PseudoUCB, CrossUCB and LCB indices in its strategy for sampling arms, eliminating arms and stopping the algorithm. The C-LUCB algorithm maintains a set of active arms of all arms decides whether to stop as described below. 1. Sampling Strategy: 2. Elimination Criteria: 3. Stopping Criteria: m(t) and m(t), where the CrossUCB index of armkis smaller than the LCB index of some other arm inA, optimal arm with 1 −δ conﬁdence. as to resolve the ambiguity among them as fast as possible. However, C-LUCB uses the additional pseudo-reward information to modify its choice of the use of from samples of other arms. Similarly, using the CrossUCB index A comparison of the operation of C-LUCB with LUCB and Racing based algorithms is presented in Table 3. We show that the proposed C-LUCB algorithm is 1 analyze its sample complexity in the next section. As the key diﬀerence between C-LUCB and LUCB is in its sampling strategy, we explore some other variants of C-LUCB in Section 6, where we study the eﬀect of performance on altering the deﬁnitions of m In this section, we analyze sample complexity of the proposed C-LUCB algorithm, that is, the number of samples required to identify the best arm with probability 1 that some arms, referred to as non-competitive arms, are explored implicitly through the samples of the optimal arm while other arms called competitive arms have an O complexity of the C-LUCB algorithm. The correlation information enables us to identify the non-competitive arms using samples from other arms and eliminate them early. For the sample complexity analysis, we assume that the rewards are bounded between [0 Note that the algorithms do not require this condition and the analysis can also be generalized to any bounded rewards. We now deﬁne the notion of competitive and non-competitive arms, which are important to interpret our sample complexity results for the C-LUCB algorithm. Let with the largest mean and k Deﬁnition 5 competitive if the expected reward of the second best arm pseudo-reward of arm Similarly, an arm the pseudo-gap of arm as C and the total number of competitive arms as C in this paper. and arms. As φ not be selected as explicitly. Furthermore, the non-competitive arms can be eliminated from the information obtained through arm Both LUCB and C-LUCB sample the top two arms at roundtinm(t) andm(t) so I(t) in deﬁnition ofm(t) avoids the sampling of an arm that appears sub-optimal (t, δ/2K), allows the C-LUCB to eliminate some arms earlier than the LUCB algorithm. The best armkand second best armkhave pseudo-gaps˜∆= (µ− φ)<0 ˜∆= (µ−φ)≤0 respectively, and hence are counted in the set of competitive The central idea behind our C-LUCB approach is that after sampling the optimal arm suﬃciently large number of times, the non-competitive (and thus sub-optimal) arms will in the sample complexity, i.e., the contribution is independent of the conﬁdence parameter However, the competitive arms cannot be discerned as sub-optimal by just using the rewards observed from the optimal arm, and have to be explored O are able to reduce a of competitive arms. We start by ﬁrst proving the (1 sample complexity in terms of the number of samples obtained until the stopping criterion is satisﬁed. Proof Sketch. To prove theorem 1, we deﬁne three events event that empirical mean of all arm lie within their conﬁdence intervals uniformly for all t ≥ 1 Deﬁne other arms lie within their CrossUCB indices uniformly for all t ≥ 1, i.e., arms with respect to the optimal arm lies within their CrossUCB indices uniformly for all t ≥ 1, i.e., Furthermore, we deﬁne E to be the intersection of the three events, i.e., Due to the nature of anytime conﬁdence intervals (See (4)) and union bound over the set of arms, we have we show that, when event This gives us the desired result in Theorem 1. A detailed proof is given in the Appendix F. as the best arm with probability 1 − δ. Eto be the event that empirical pseudo-reward of optimal arm with respect to all Similarly deﬁneEto be the event that the empirical pseudo-reward of the sub-optimal Theorem 2. C-LUCB until stopping, is bounded as that depends on the type of conﬁdence bound used to construct tighter the bound, the smaller the i.e., the diﬀerence in mean reward of optimal arm Appendix E. Proof Sketch. In order to bound the total number of samples drawn by C-LUCB, we bound the total number of rounds pulls two arms We obtain an upper bound on the total number of rounds counts of the number of rounds and obtain an upper bound for each of them under the event We can now see that T ≤ T giving us E wheret=infτ ≥2 : ∆≥4∀k /∈ Candζis a universal constant , min∆, i.e., the gap between best and second best arm. We present a brief proof outline below, while the detailed proof is available in the : LetTdenote the number of rounds in whichI(t)< µ, i.e., the count of events in which the pseudoUCB index of armkis smaller than the mean of armkat round t. : DeﬁneTto be the number of rounds in whichm(t), m(t)∈ Cand event I(t) < µdoes not occur. m(t) /∈ C, m(t) 6= k. : DeﬁneTto be the number of rounds in whichm(t) =k, m(t)/∈ Cor m(t) = k, m(t) /∈ C . Pr(I(t) < µ|E) =Pr(I< µ, E)Pr(E)≤Pr(I< µ, E)1 − δ≤Pr(I< µ)1 − δ≤Kt1 − δ, We then evaluate an upper bound on O(1) constant, i.e., we have C, m takes place only ﬁnitely many time steps almost surely. Similarly following result bounding the total number of samples drawn from the C-LUCB algorithm with probability 1 − δ. Corollary 1. The number of samples obtained by C-LUCB is upper bounded as where competitive arms C, in contrast to the LUCB algorithm where the sample complexity term involves summation of a O reduces a K-armed bandit problem to a C-armed bandit problem. arg max with high-probability. This in turn ensures that the non-competitive arms are not selected above by a O(1) constant. The LUCB algorithm is known to stop after obtaining at most samples with probability at least 1 − δ. More formally, We compare this result with the one that we prove for C-LUCB algorithm in Theorem 2. Reduction to a C-Armed Bandit problem: approach, the O to the LUCB algorithm which has O( Putting these results together, we obtain the result of Theorem 2. Furthermore, asET|E, ET|Eis upper bounded by an O(1) constant asδ →0,P (t)6=k. By Borel-Cantelli Lemma 1, this implies that with probability 1, the eventE d=max(d, d). Note that the Ologterm is only summed for the set of The key intuition behind our sample complexity result is that the sampling ofm(t) = (t) orm(t), due to which we see that their expected number of samples are bounded sense, C-LUCB algorithm reduces a Depending on the problem instance, the value of C can vary between 2 and K. Slightly larger number of samples from competitive arms: bution coming from a competitive arm in C-LUCB algorithm is is slightly larger than the contribution coming from a sub-optimal arm in LUCB algorithm, where each arm contributes fact that we construct slightly wider conﬁdence intervals, in C-LUCB to take advantage of the correlations present in the problem. We see in Section 7 that this small increase in the width of conﬁdence intervals does not have a signiﬁcant impact on the empirical performance of the algorithm. Theorem 2’s result is in conditional expectation: of the LUCB algorithm bounds the total number of samples taken with probability 1 sample complexity result bounds the expected samples taken by C-LUCB algorithm under the event component, because it tries to avoid sampling non-competitive arm at each round with high probability. We have a result in Corollary 1 that evaluates an upper bound which holds with probability 1 only characterize the expected sample complexity of our C-LUCB algorithm for the cases where the event does not occur. While such results are hard to obtain theoretically, in all our experiments we observed that the variance in the number of samples drawn by C-LUCB is not much, and is in fact similar to that of the LUCB algorithm in all the experiments performed. This indicates that even when algorithm stops with an incorrect arm, the number of samples obtained are similar to the samples obtained under the good event E. The log algorithm [ result. This is avoidable in the classical MAB framework if one uses the lil’UCB algorithm, which is known to have the optimal theoretical sample complexity in the classical bandit setting as it avoids the use of lil’UCB algorithm leads to worse empirical performance as seen in our experiments and prior work [ correlated bandit setting. The LUCB++ algorithm has a sample complexity of the form avoids the for the optimal arm algorithm empirically. In our next section, we propose the C-LUCB++ algorithm, which is a heuristic extension of LUCB++ to the correlated bandit setting and show that it ﬁnds the optimal arm with probability at least 1 − δ. E(Theorem 2). This arises as the analysis of our algorithm requires a transient (K)term in numerator:Just like the sample complexity result of the LUCB Plog+log. The LUCB++ algorithm log(K) term in the sample complexity for the sub-optimal arms and has it only Table 5: We study two intuitive variants of C-LUCB which diﬀer in their sampling strategy of algorithm. We report the number of samples needed to identify the best genre from the set of 18 movie genres in the Movielens dataset. While all of these are smaller than the samples drawn by LUCB (which is 61175.4 in this case), the diﬀerence between the variants of C-LUCB is minimal. Experimental details are described in detail in Section 7, we set the value of this experiment. Such similarity in empirical performance has also been observed in our other experiments and we found no clear winner among the three when compared on their empirical performance. Dependency with K: complexity on conﬁdence regime), our proposed algorithms outperform the classical bandit algorithms (See Figure 3). In our proposed C-LUCB algorithm, at each round we sample two arms where sampling such as this allowed us to show 1 analyse its sample complexity (Theorem 2). In this section, we explore two other algorithms, that we call maxmin-LUCB and 2-LUCB, that sample diﬀerent t, but have the same elimination and stopping criteria as that of C-LUCB. In Table 5, we contrast their sampling strategy with respect to C-LUCB. While we are able to show that both maxmin-LUCB and 2-LUCB algorithm will stop with the best-arm with probability at least 1 −δ, we are unable to provide a sample complexity result for them. world recommendation dataset, and found their empirical performance to be similar to C-LUCB. We chose to use C-LUCB as our proposed algorithm as it is possible to provide theoretical guarantees as in Theorem 1 and Theorem 2. Moreover, we ﬁnd its empirical performance to be superior than classical bandit algorithms in correlated bandit settings, as we illustrate through our experiments in the next section. 6.1 C-LUCB++: Heuristic extension of LUCB++ The LUCB++ algorithm as illustrated in Section 3, is able to improve upon LUCB, by modifying its stopping criteria and in its sampling of (t) andm(t). Both of them have same elimination and stopping criteria as the C-LUCB is loose. For our theoretical results, we focus on studying the dependence of sample We also evaluated the empirical performance of maxmin-LUCB and 2-LUCB on a real- Figure 5: Number of samples drawn by Racing, lil’UCB, LUCB, LUCB++, C-LUCB and C-LUCB++ to identify the best movie genre out of 18 possible genres in the Movielens dataset. Here, maximum possible reward (i.e., 5). When available that our proposed C-LUCB and C-LUCB++ algorithms exploit to reduce the number of samples needed to identify the best movie genre. When correlation information available, in which case our proposed C-LUCB and C-LUCB++ algorithms have a performance similar to LUCB and LUCB++ respectively. extension, C-LUCB++, that extends the LUCB++ algorithm to the correlated bandit setting. The comparison of C-LUCB++ and LUCB++ in its sampling, elimination and stopping criteria is presented in Table 3. While we are able to show that the C-LUCB++ stops with the best arm with probability at least 1 complexity remains an open problem. We compare the performance of C-LUCB++, with C-LUCB, LUCB, Racing and lil’UCB algorithms extensively through our experiments on Movielens and Goodreads datasets in the next section. We now evaluate the performance of our proposed C-LUCB and C-LUCB++ algorithms in a real-world setting. By comparing the performance against classical best-arm identiﬁcation algorithms on the movielens and goodreads datasets, we show that our proposed algorithms are able to exploit correlation to identify the best-arm in fewer samples. All results reported in our paper are presented after conducting 10 independent trials and computing their average. Additionally, in all our plots we show the error bars of width 2 is the standard deviation in the number of samples drawn by an algorithm across the 10 independent trials. The MovieLens dataset [ by 6040 Users. Each movie is rated on a scale of 1-5 by the users. Moreover, each movie is associated with one (and in some cases, multiple) genres. For our experiments, of the possibly several genres associated with each movie, one is picked uniformly at random. To perform our experiments, we split the data into two parts, with the ﬁrst half containing ratings of the users who provided the most number of ratings. This half is used to learn the pseudo-reward entries, the other half is the test set which is used to evaluate the performance of the proposed algorithms. Doing such a split ensures that the rating distribution is diﬀerent in the training and test data. Best Genre identiﬁcation. genre among the 18 diﬀerent genre in the test population in fewest possible samples. The pseudo-reward entry `that are rated by the users who rated genre entries might not be available, we randomly replace p-fraction of the pseudo-reward entries by maximum possible reward, i.e., 5. We then run our best-arm identiﬁcation algorithms on the test data to identify the best-arm with 99% conﬁdence. Figure 5 shows the average samples taken by C-LUCB and C-LUCB++ algorithm relative to the classical best-arm identiﬁcation algorithms for diﬀerent value of are removed). We see that C-LUCB and C-LUCB++ algorithms signiﬁcantly outperform all Racing, lil’UCB, LUCB and LUCB++ algorithms for exploit the correlations present in the problem to identify the best arm in a faster manner. performance of C-LUCB is only slightly worse than that of LUCB algorithm. This is due to the construction of slightly wide conﬁdence interval relative to LUCB algorithm that uses and C-LUCB++ algorithm (which is an extension of LUCB++) outperform C-LUCB, which is due to the known superiority of LUCB++ over LUCB [12, 11]. Variation with δ. for diﬀerent value of C-LUCB++ to identify the best arm with 90% (i.e., 20% of pseudo-reward entries are replaced by 5). As C-LUCB and C-LUCB++ are able to make use of the available correlation information, we see our proposed algorithms require fewer samples than the Racing, lil’UCB, LUCB and LUCB++ algorithms in each of the four settings. The Goodreads dataset [ users. Each rating is on a scale of 1-5. For our experiments, we only consider the poetry section and focus on the goal of identify the most liked poem for the population. The poetry dataset has 36,182 diﬀerent poems rated by 267,821 diﬀerent users. We do the pre-processing of goodreads dataset in the same manner as that of the MovieLens dataset, by splitting the dataset into two halves, train and test. The train dataset contains the ratings of the users with most number of recommendations. In the scenario where all pseudo-reward entries are unknown, i.e.,p= 1, we see that the Figure 6: Number of samples needed by Racing, lil’UCB, LUCB, LUCB++, C-LUCB and C-LUCB++ to identify the best poem out of the set of 25 poem books in the Goodreads dataset. Here possible reward and that pseudo-reward entries may be noisy. Our proposed C-LUCB and C-LUCB++ utilize correlation information and require signiﬁcantly less samples than the classical best-arm identiﬁcation algorithms. Best book identiﬁcation. aim to identify the best book in fewest possible samples with 99% conﬁdence. After obtaining the pseudo-reward entries from the training data, we replace the highest possible reward (i.e., 5) as some pseudo-rewards may be unknown in practice. To account for the fact that these pseudo-reward entries may be noisy in practice, we add a safety buﬀer of 0 to be empirical conditional mean (obtained from training data) plus the safety buﬀer We perform experiment on the test data and compare the number of samples obtained for diﬀerent algorithms in Figure 6 for two diﬀerent values of p. We see that in both the cases, our C-LUCB and C-LUCB++ algorithms outperform other algorithms as they are able to exploit the correlations in the rewards. In this work, we studied a new multi-armed bandit problem, where rewards corresponding to diﬀerent arms are correlated to each other and this correlation is known and modeled through the knowledge of pseudo-rewards. These pseudo-rewards are loose upper bounds on conditional expected rewards and can be evaluated in practical scenarios through controlled surveys or from domain expertise. We then extended an LUCB based approach to perform best-arm identiﬁcation in the correlated bandit setting. Our approach makes use of the pseudo-rewards to reduce the number of samples taken before stopping. In particular, our approach avoids the sampling of non-competitive arms leading to a stark reduction in sample complexity. The theoretical superiority of our proposed approach is reﬂected in practical scenarios. Our experimental results on Movielens and Goodreads recommendation dataset show that the presence of correlation, when exploited by our C-LUCB approach, can lead to signiﬁcant reduction in the number of samples required to identify the best-arm with probability 1 − δ. following: PAC-C-LUCB: probability 1 correct) algorithm, that identiﬁes an arm which is within stopping criteria of C-LUCB algorithm. More speciﬁcally, if one compares max analyse a PAC algorithm in the correlated multi-armed bandit setting. Using Pseudo-Lower bounds: conditional expected rewards, in the form of pseudo-upper-bounds, are known to the player. In practical settings, it may also be possible to obtain pseudo-lower-bounds, that may allow us to know information about lower bound on conditional expected reward. In presence of such knowledge, we believe C-LUCB algorithm will need a modiﬁcation in its deﬁnition of lower conﬁdence bound crossUCB index for upper bound, we can re-deﬁne lower conﬁdence bound index can help us to incorporate cases where pseudo-lower bounds are also known. Top m arms identiﬁcation: optimal arm from the set of to ﬁnd the best the correlated-multi armed bandit setting. We believe such a problem would be even more interesting if the pseudo-lower bounds are known. An open problem is to extend a C-LUCB like approach to identify the best m arms from the set of K arms. Lower bound and optimal solution: empirical performance and has some theoretical guarantees, it may not be the optimal solution for the correlated bandit problem studied in this paper. Studying a lower bound and correspondingly an optimal solution to this problem remains an open problem. This work opens up several interesting future directions, including but not limited to the . We believe such an algorithm can be constructed by modifying the elimination and L(n, δ) in the C-LUCB’s elimination criteria, it may be possible to design and