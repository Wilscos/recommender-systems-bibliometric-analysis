Modern researchers have access to a large number of scientific publications and grants. While this abundance has allowed them to get more information quickly, it has also made it more overwhelming. Many scientists consider the search for related work as a highly time-consuming part of their responsibilities [1]. While modern researchers have access to a wealth of information, we need methods and tools to help navigate them. Scientists find articles by following references in other articles that they find interesting. This method might lead scientists to citation communities where all articles refer to each other while causing a heavy bias towards already-cited papers [2]. A complementary method to explore articles is using keyword search. While powerful, this approach is limited to finding the keyword that matches the scientist’s provided set of keywords. For example, it would be challenging to form queries without knowing the space of available keywords. Furthermore, keyword search does not adapt as keyword definitions might depend on sometimes outdated perceptions of the field. This paper describes a recommendation system for publications and grants called EILEEN (Exploratory Innovator of LitEraturE Networks). This system uses the behavior of scientists to learn how to improve its predictions. The first part gives an overview of related work and discusses the advantages and disadvantages of existing approaches. The central part introduces its architecture and the approach for modeling keyphrases and recommendations. The last part of the paper gives insights into the search engine and the performance of our recommendation system. Over the last 16 years, more than 200 articles have been published about research paper recommendation systems. Among them, 55% of the system applied content-based filtering, while collaborative filtering was applied by only 18%, and graph-based recommendations by 16% [3]. Citation databases such as CiteSeer apply citation analysis to identify articles that are similar to the input article. Scholarly search engines such as Google Scholar focus on classic text mining and citation counts [4]. However, each concept has disadvantages, such as focusing on already-cited articles or showing only limited sources. All these disadvantages limit their suitability for generating recommendations. Keyphrase extraction is the task of representing the main content of a document with keywords. Keyphrases or keywords are a collection of words about the document that best describes the document’s nature and core content. Document summarization, categorization, and clustering are tasks where keyphrase extraction is essential [20]. There are several unsupervised learning approaches to extracting keyphrases from the document, such as Text Rank algorithm, RAKE (Rapid Automatic Keyphrase Extraction) algorithm, and KERT (Keyphrase Extraction and Ranking by Topic). We now review them. The TextRank algorithm is a graph-based ranking model for text processing [16]. In a graph-based keyphrase extraction algorithm, the ranking of the keyphrase is determined by the importance of the vertex. Vertices cast a “Vote” or “Rank” other connected vertices. The number of votes cast to a vertex is directly proportional to the importance and rank of the vertex (TextRank). Graphs are built to represent text, depending on the application at hand. Text units of various sizes and characteristics can be added as vertices in the graph, e.g., words, collocations, entire sentences. Rapid automatic keyphrase extraction (RAKE) is an approach introduced in 2010 [17]. It depends on four steps: candidate selection, properties calculation, scoring, and word selection. RAKE works by letting users select the criteria to extract keyphrases. The criteria include selecting the minimum number of characters in each word, the maximum number of words in each phrase, and the minimum number of times the key phrase appears in the document. In addition, punctuation marks are treated as sentence boundaries, and all the words in the stopwords list are also treated as sentence boundaries. Finally, after the candidates are selected, they are scored by their frequency and length. Among the few weaknesses of RAKE are its limited accuracy and inability to normalize the keyphrases using stemming and lemmatization. Keyphrase extraction and Ranking by topic (KERT) is an approach that does not follow the traditional approach of n-gram tokenization and ranking system [18]. Instead, KERT constructs topical phrases immediately after clustering the unigrams. KERT goes through three steps in extracting keyphrases. Step one includes clustering words in the document dataset into several foreground topics and one background topic, using background LDA [24]. Step two includes extracting candidate keyphrases for each topic according to the word topic assignment. Finally, step three includes ranking the keyphrases in each foreground topic by integrating the criteria several criteria to rank the results. We developed EILEEN to help junior faculty and researchers find the right people, grant programs, and journals similar to their interests. The ultimate goal is to serve as an open, research-driven platform that produces 1. curated datasets from a combination of sources, 2. data about scientists’ system usage, and 3. results of experiments run for a different combination of users. Currently, the systems that offer recommendations such as Semantic Scholar, Microsoft Academic, and Google Scholar are not open. It is hard or impossible to access their data (except Microsoft Academic through the Microsoft Academic Graph), the user’s behavior, and run experiments on them. It is the hope that EILEEN fills this gap in recommendation systems research for science. EILEEN is provided through a web interface available at https://eileen.io. It aims to have a user-friendly and responsive interface. EILEEN records all the actions that scientists take on the website. For example, we know what they preferred in the past and their current search; therefore, we can predict which documents will be found relevant. The overall architecture of EILEEN contains data ingestion, data processing, modeling, and the back and front end (Fig. 1). Data ingestion. The data ingestion and processing run automatically using an Apache Airflow job. Currently, we refresh the data weekly. The data processing is done with an Apache Spark cluster with ten nodes, a total of 768 GB of RAM, 40 TB of disk space, running a Hadoop cluster. The entire data source comprises 28 million records (numbers from 2018), of which 25 million are extracted from MEDLINE, 2.6 million grants from Federal Exporter, 1 million from Pubmed Open Access Subset, 0.6 million for Arxiv, and 14K from the National Bureau of Economic Research [NBER]. Apache Spark is used with Python to pull the data from the above sources. The raw data obtained from the third-party sources is cleaned, processed using Spark’s DataFrame and Machine Learning APIs. Finally, the relevant and required fields are extracted from all data sources, and a consolidated model is created with is later exported to the Elasticsearch index. The model contains the following fields along with the description: 2. Source: the source of the document (PubMed, arXiv, etc.) 3. Source_id: source-specific ID of the document 4. Type: publication/grant 5. Title: Title of the publication or grant 6. Venue: Venue where the publication was published (e.g., journal or conference name) 7. Abstract: Abstract of the publication or grant summary 8. Scientists: authors of the publication or grant 9. Organizations: affiliations of authors in publication or grant 10. Date: date the publication published or grant awarded 11. Content: 12. End_Date: Grant’s end date 13. City: City of the publication 14. Country: Country of the publication 15. OtherID: in case the document has multiple identifiers such as DOI or PMID 16. Tfidf: TFIDF vector for the document 17. Topic: Latent Semantic Analysis of the TFIDF vector 18. Topic_Norm: Topic vector, normalized 19. Buckets: Locality-sensitive hashing of the Topic_norm vector, used for fast recommendations The data is dumped periodically from Apache Spark to EILEEN’s Elasticsearch server [22]. Back and front end. The front end uses AngularJS, an open-source front-end web framework. The user has access to all the EILEEN APIs. We also have an SQL database, which is used for maintaining user data. All the user actions are recorded in the database, including logging in, adding publications and grants to the user’s library, and tracking the user’s activities. Oauth2 is used to implement login for Google, Facebook, and Orcid APIs. Keyphrase and search. Keyphrases are words that capture the main topics of a document. The key phrase extraction system serves two purposes. First, they are displayed on the keyphrase popularity chart in the user library section, extracting relevant key phrases from the user’s voted publications and grants. Their popularity is shown across different years. This popularity chart gives users the ability to understand where their interests lie more broadly. Second, they are used to assist the autocomplete function in EILEEN’s search functionality. Recommendation system. Once users have chosen a set of publications or grants that interest them, they can use the recommendation system to explore which other publications and grants are similar to their interests. We will explain this part in the methods section. The datasets for the recommendation system are scientific publications and grants from online repositories such as arXiv, MEDLINE, and Federal ExPORTER. ● arXiv has e-prints of scientific papers in fields such as mathematics, physics, astronomy, ● MEDLINE contains citations and abstracts for biomedical literature. Some of them have ● Federal ExPORTER provides information about the projects, abstracts, and publications computer science, quantitative biology, statistics, and quantitative finance. We download 1.8M scientific papers from arXiv through its API. links to full-text articles. The website has academic journals covering medicine, nursing, pharmacy, dentistry, veterinary medicine, and health care. It also has literature in biology and biochemistry, as well as fields such as molecular evolution. In total, we download around 28M articles from more than 8K journals from MEDLINE. for different federal agencies. We downloaded 2.4M past grants from 14 federal agencies: ○ Administration for Children and Families (ACF) ○ Agency for Healthcare Research and Quality (AHRQ) ○ Agriculture Research Service (ARS) ○ Center for Neuroscience and Regenerative Medicine (CNRM) ○ Centers for Disease Control and Prevention (CDC) The data captured by eileen.io contains many valuable behavioral items. Of all the users who have visited the website, 28% created a profile and accessed recommendations based on their preferences. As of September 2018, which is the data we use for this study, we have comprehensive data from 68 anonymized users tracked over many months—a high-quality subset of the total. These users spend an average of 2 minutes and 32 seconds on the website, using the search functionality 9.7 times, and saving 14.6 items in their profile. To build a dataset for a recommendation system, we need to extract several interactions for each user from the backend. We first need to extract the search behavior and user preference from the tracking data to do this. We then find the relationship between the documents searched by the user and the user’s votes (i.e., relevant or irrelevant). Then, along with the search result from the Elasticsearch on each user query input, we can build the relationship between users. First, we go through the tracking data to get the queries for the search and the preference on the user's library in the sequence of time. Based on this, we can get all the documents Elasticsearch will return for each query users search for. This process creates three types of preferences: 1. Those users get from the search and vote as ‘relevant,’ 2. Those users get from the search and do not vote on; we may think those as not so related 3. Those users got from the search and voted as ‘irrelevant,’ which users do not want to have From the result of Elasticsearch for each document, we will have the topics represented for this document in the documents set. So for each user’s library, we can calculate the average vector based on all the relevant documents the user voted for. It will represent the topics for this user’s library. These steps will allow us to have a dataset for learning a recommendation system for scientists. ○ Congressionally Directed Medical Research Programs (CDMRP) ○ Defense and Veterans Brain Injury Center (DVBIC) ○ Environmental Protection Agency (EPA) ○ Food and Drug Administration (FDA) ○ Forest Service (FS) ○ National Aeronautics and Space Administration (NASA) ○ National Institute of Food and Agriculture (NIFA) ○ National Institutes of Health (NIH) ○ National Institute on Disability, Independent Living, and Rehabilitation Research (NIDILRR) ○ National Science Foundation (NSF) ○ U.S. Department of Veterans Affairs (VA) to the topic users search for, in the search result. The recommendation system we are using is based on the content of a document instead of applying collaborative filters to the data. Thus, we prevent Matthew's effect by avoiding collaborative filters, commonly found in most recommendation systems, which can be an obstacle to scientific exploration [4]. Another feature of our recommendation system is that the users can get feedback immediately after changing their preference by marking one document as relevant or irrelevant. Our recommendation system is seeded by a Rocchio algorithm, extensively used in other scientific recommendation systems [5]. To build this recommendation system, we have to transform the text into vectors, perform dimensionality reduction, create a nearest-neighbor search for initial recommendations, and a learning-to-rank method for re-ranking. We explain these steps now: Text preprocessing and term weighting schemes. The text preprocessing technology we used on our documents set includes removing stop words, stemming with the Porter Stemming algorithm [5], tokenizing the documents into unigrams and bigrams, and filtering it based on term frequency and tf-idf. In particular, we removed the terms that appear fewer than three times or terms whose tf-idf was more than 0.8. We will explain tf-idf now. There are many different weighting schemes from the researches. They all have different considerations on balancing between the local structure of each document and the global structure of the whole document set. We use this simple weighting scheme called term frequency-inverse document frequency (tf-idf) for our text processing. And the tf-idf would reweight term frequency by a global factor as follows where f documents. Latent Semantic Analysis. Latent semantic analysis (LSA) analyzes relationships between a set of documents and the terms they contain. LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis). A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from an extensive collection of documents. A mathematical technique called singular value decomposition (SVD) is used to reduce the dimensionality of the data. Rocchio Algorithm. The Rocchio algorithm provides recommendations based on users’ preferences on the documents, and their votes are either relevant or irrelevant [6]. Given a set of relevant documents which some specific user voted previously {r is the number of documents that contain the term i and n is the total number of irrelevant documents {u follows: where Q present the weight of the relevant documents set and the irrelevant documents set for the final query vector. For our project, we calculate the search on the keywords into the final document vector; therefore, our query vector is as follows: Our recommendation system cares more about the documents that users are interested in than those they show no interest in. So we set the b to be one and the c to be 0. Once the preference vector q has been created for a user, we will rank the recommendation result from Elasticsearch by the distance to the query vector. Finally, we compute the cosine distance between the word vector representing each recommended document and the query vector in our implementation. Random Forest is an ensemble learning method for classification, regression, and other tasks [7]. And we can use it to rank the importance of variables in a regression or classification problem [8]. After we have all data prepared, we can do the Random Forest classification on all the documents returned from users’ search and predict which is more likely to be clicked. We will use the result from the first page only, which means at most ten documents per search. Since we only care about what users are interested in, we will have two classes, one for those documents voted as relevant, and one for those documents remain un-voted from the search. We compute twelve features in total. Five of them are for the document, including length of the title, authors, venue, abstract, and search query length. Another set of features is the year of the document and the score returned by Elasticsearch. This Elasticsearch scoring is based on BM25 and other details inherited from the Lucene indexing method . For the rest, we fit the tf-idf on the combined text from query, title, authors, venue, and abstract and then transform the model on each of these five fields. Then we calculate the cosine distance between the tf-idf of the query and the tf-idf of the rest of the fields. This calculation gives us four features. The last feature is the cosine distance between the topics for the document and those of the user’s library. This distance is calculated differently based on the document’s class. Relevance is calculated as the cosine distance between the current document and the user’s preferences. And is the mean document vector of the users’ search on the keywords. Parameters b and c for the un-voted documents, it is the cosine distance between this document and the library’s state. To summarize, the total set of features is We use the RandomForestClassifier from PySpark with 500 trees. Based on the size of our data, the number of total entries we have is 1302. We randomly split the user in a 4:1 ratio for test and validation. We build five models based on the same data set: The outline of the technique uses a standard algorithm to generate candidate keyword lists and then a model to rerank this initial set of candidates. The initial set of candidates. The algorithm RAKE works in three steps. First, all possible keywords are extracted from the document. Second, a set of properties is calculated to determine the score of each candidate. Third, candidates with top scores (e.g., top 10) are selected. With RAKE, every keyphrase should have five words in length, and it should occur at least three times across the document. For every keyphrase found using RAKE, we include the feature term count, term length, maximum word length, spread, lexical cohesion, and absolute first and last occurrence. Training process. We use a Random Forest using the features from RAKE to learn what constitutes good key phrases or not. Data from [19] is used as the training data for the model that re-ranks the initial set provided by RAKE. It provides 244 scientific documents, out of which 100 are test data set, 144 are training data set, and 40 are validation data set. 1. Length of the query 2. Length of the title 3. Length of the authors 4. Length of the venue 5. Length of the abstract 6. Year 7. Score from Elasticsearch 8. Cosine between query and title 9. Cosine between query and authors 10. Cosine between query and venue 11. Cosine between query and abstract 12. Cosine between document and library, which stands for user preference 1. All 12 features 2. All 12 features without the score from Elasticsearch 3. All 12 features without the cosine distance between each document and the library 4. Score from Elasticsearch only, and 5. Score from Elastic search and the cosine distance between the document and library As a baseline, we estimated the performance of simple Elasticsearch based on the default weighting of terms in the search query (Fig. 2, left panel). The AUC of such a model is 0.53. By adding the preference vector representation to the model and measuring the cosine distance of such vector with the document vector, the model increases performance significantly (AUC = 0.66). This baseline will be used for comparison. When testing the performance of all the features described in the Materials and Methods section, we found a performance of AUC = 0.901. We further checked whether the semantic representation of the documents helped this model, but we found that the performance was very close (AUC = 0.903). This performance may indicate that the initial keyword matching captures most documents. These results show that the full model is significantly better than keyword weighting schemes. With Random Forest, we can interpret which features are most important for scientists when choosing relevant documents (Fig. 2, right panel). For example, we found that for scientists, the abstracts of the documents are more important than titles. In contrast, information retrieval for the web (i.e., normal users) usually finds the essential information piece in the title. Importantly, we found that the score from Elasticsearch is only the 6th most crucial feature. The keyphrases generated by RAKE are matched with the extracted keywords provided by semEval. In the documents we are analyzing, we have millions of candidate keywords (Table 1). The intersection of all the keyphrases from RAKE and semEval is 1, and non-matched is 0. The training set of 144 research articles with the condition that keyphrases should be at least five words in length and occur three times across the document produces 1,122 RAKE keyphrases. Out of these, 119 are labeled 1 (Table 1). We ran the Random Forest classifier on the trained data. Cross-validation is performed to find the model with the best binary classification evaluation performance. An AUC = 0.73 is achieved (Fig. 3) Keyphrase prediction for MEDLINE. A stop word list file, which consists of around 550 stopwords, filters the stopwords. The minimum length of each keyword is 3, the maximum number of keywords is 4, and the minimum occurrences of the phrase are 2. Out of 5,896,505 key phrases from the MEDLINE data, only 40,005 are keyphrases over four keywords, which account for only 0.67% of all human-curated keyphrases. Hence those are filtered out, and the maximum number of keywords in one phrase is set to 4. Examples of keyphrases produced by our model: See Appendix section A.1 for other examples. This article analyzed a unique dataset of scientists’ behavior when searching for publications and grants. We found that the distance of summary to the query is more important than to title, which seems contrary to regular users of the web. Furthermore, we found that a model with the scientists’ preferences and relationships between search terms and the document produced a surprisingly high AUC equal to 0.9. Our analysis shows that we can learn a great deal about scientists’ search behavior with only limited data (compared to other systems such as Google Scholar or Semantic Scholar). One of the issues of this study is that the data is somewhat limited. For example, a typical information retrieval dataset might have thousands of user interactions [23]. Therefore, it could be the case that once we have more data about scientists, the differences found by our model will start to disappear. Also, we do not have impact measures, such as citations or paper downloads, which would better inform our algorithm. However, impact measures are hard to obtain because indexing companies like Elsevier and Thomsons Reuters are usually under intellectual protection. In the future, we will match the documents from MEDLINE to recently released datasets of citations from Microsoft Academic Graph. Still, our findings could inform future research into recommendation systems for scientists. Another limitation of our study is that the recommendation system only works based on individuals, not using collective behavior. For example, we could imagine an improved system that learns faster about users by combining individual preferences with the preferences of similar scientists. These systems are called hybrid recommendation systems, combining collaborative filtering and content-based recommendation systems. We plan to explore this possibility in the future. This work introduces several improvements compared to previous work. First, our dataset is unique as it is the only one about scientists’ behavior using recommendation and search systems. Second, we also make the code to run the analysis openly available so that the scientific community tests other approaches and compares them to our work. With this, we hope to make research in scientific recommendation systems more reproducible and robust. Finally, we make the results of our best algorithm available through http://eileen.io. Thus our proposal introduced several significant improvements to scientific recommendation systems. Modern science depends on critically examining the most relevant and recent developments. Recommendation systems should play an increasing role in helping through this examination. In addition, these new developments could potentially spur faster development of innovations which could improve technologies. DEA was supported by NSF grants #1646763 and #1800956. ZY was supported by NSF grant #1933803. KN, PM, and ZQ were supported by NSF grant #1646763.