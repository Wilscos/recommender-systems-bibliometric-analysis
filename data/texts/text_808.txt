Conversational recommendation systems (CRS) engage with users by inferring user preferences from dialog history, providing accurate recommendations, and generating appropriate responses. Previous CRSs use knowledge graph (KG) based recommendation modules and integrate KG with language models for response generation. Although KG-based approaches prove effective, two issues remain to be solved. First, KG-based approaches ignore the information in the conversational context but only rely on entity relations and bag of words to recommend items. Second, it requires substantial engineering efforts to maintain KGs that model domain-speciﬁc relations, thus leading to less ﬂexibility. In this paper, we propose a simple yet effective architecture comprising a pre-trained language model (PLM) and an item metadata encoder. The encoder learns to map item metadata to embeddings that can reﬂect the semantic information in the dialog context. The PLM then consumes the semantic-aligned item embeddings together with dialog context to generate high-quality recommendations and responses. Instead of modeling entity relations with KGs, our model reduces engineering complexity by directly converting each item to an embedding. Experimental results on the benchmark dataset REDIAL show that our model obtains state-of-the-art results on both recommendation and response generation tasks. An automated conversational recommendation system (CRS) (Li et al., 2019; Zhou et al., 2020) is intended to interact with users and provide accurate product recommendations (e.g., movies, songs, and consumables). It has been a focal point of research lately due to its potential applications in the e-commerce industry. Traditional recommendation systems collect user preferences from implicit feedback such as click-through-rate (Zhou et al., 2018) or purchase history and apply collaborative ﬁltering (Su and Khoshgoftaar, 2009; Shi et al., 2014) or deep learning models (Covington et al., 2016; He et al., 2017) to construct latent spaces for user preferences. Unlike traditional recommendation systems, CRSs directly extract user preferences from live dialog history instead of implicit interactive records, thus can provide better recommendations that precisely address the users’ needs. Although some progress has been made in this area, there is still room for improvement. First, previous CRSs (Chen et al., 2019; Zhou et al., 2020; Li et al., 2021) track entities mentioned in the dialog context, and then search related items in knowledge graphs to recommend to users. However, these systems require a named-entity recognition (NER) module to extract mentioned entities from the dialog context, thus we need to collect additional domain-speciﬁc data to train the NER module. In practice, such NER modules have deﬁcient performance, leading to a bad accuracy of CRS. Additionally, entity-dependent recommendation modules lack of contextual information. For example, a user may say:“I watched Ant-Man but did not enjoy it.”. In this case,Ant-Manis extracted as a mentioned entity and the system will recommend a similar movie to Ant-Man, which is not an appropriate recommendation. Moreover, in some domains, users are more inclined to express their needs with pure language. Since corpora in such domains barely contain named entities, existing entity-based CRSs perform poorly on these domains. Ideally, a CRS should condition its recommendation on the integrated contextual information of the entire dialog context and mentioned entities. Second, existing CRSs built upon graph neural networks (Kipf and Welling, 2017; Schlichtkrull et al., 2017) cannot quickly scale up or respond to rapid changes of the underlining entities. In e-commerce companies, items for recommendation change daily or even hourly due to constant updates of merchants and products. Existing approaches with graph neural networks require either re-training the entire system when the structure of knowledge graph changes (Dettmers et al., 2018) or adding complex architectures on top to be adaptive (Wu et al., 2019). A more ﬂexible architecture can help the system react to rapid changes and adapt itself to new items. Driven by the motivations above, we present aMetadataEnhanced learning approach via SemanticExtraction from dialog context i.e. MESE. The major components of MESE contain a pre-trained language model (PLM) and an item encoder architecture. The item encoder takes item metadata as input and is jointly trained with the PLM and dialog context. After training, the item encoder can map item metadata in a systematic way such that contextual information from the dialog is reﬂected in the constructed embeddings. Item embeddings are then consumed together with dialog context by the self-attention mechanism of the PLM to perform recommendation and response generation. In order for the model to scale up with the size of the item database, we pose recommendation as a two-phase process with candidate selection and candidate ranking following (Covington et al., 2016). The key contributions of this paper are summarized as follows: This paper presents MESE, a novel CRS framework that considers both item metadata and dialog context for recommendations. Our model employs a simple yet effective item metadata encoder that learns to construct item embeddings during training, thus can adapt to database changes quickly and be independent on task-speciﬁc architectures. Extensive experiments on standard dataset REDIAL demonstrate that MESE outperforms previous state-of-the-art methods on both response generation and recommendation with a large margin. Current CRS paradigm contains two major modules: a recommendation module that suggest items based on conversational context and a response generation module that generate responses based on dialog history and the recommended items. How to integrate these two modules to perform well on both tasks has been a major challenge. (Chen et al., 2019) leverages external knowledge and employees graph neural networks as the backbone to model entities and entity relations in the knowledge graph (KG) to enhance the performance. In (Zhou et al., 2020), a word-level KG (ConceptNet (Speer et al., 2018)) is introduced to the system with semantic fusion (Sun et al., 2020) to enhance the semantic representations of words and items. Since item information and dialog context are processed separately in the above approaches, they suffer from loss of integrated sentence-level information. We propose to condition recommendation on an integrated contextual information of both dialog context and mentioned entities. More recent works try to adopt pre-trained language models (PLM) (Vaswani et al., 2017; Radford et al., 2018; Zhang et al., 2020) and template based methods to facilitate response generation. (Liang et al., 2021) generates a response template that contains a mixture of contextual words and slot locations to better incorporate recommended items. (Wang et al., 2021) expands the vocabulary list of the PLM to include items to unify the process of item recommendation with response generation. We propose to enhance our PLM with an item metadata encoder to extract context-aware representations by jointly training on both recommendation and response generation tasks. We also generate response templates with slot locations to better incorporate recommended items into responses. Our work is also inspired by studies from other areas. Recent works have shown that crossmodality training across vision and language tasks can lead to outstanding results in building multimodal representations (Tan and Bansal, 2019; Lu et al., 2019). In (Tan and Bansal, 2019), a largescale transformer based model is adapted with cross-modal encoders to connect visual and linguistic semantics and pre-trained on vision-language pairs to learn intra-modality and cross-modality relationships. Prompt tuning (Li and Liang, 2021; Gao et al., 2021) methods prove that PLMs are capable of integrating different sources of information into the same embedding space and perform well on downstream tasks. In terms of using PLM as a recommendation system, (Sun et al., 2019) trains a bidirectional self-attention model to predict masked items and their model outperforms previous sequential models on various recommendation tasks. Inspired by the above studies, we propose to use an encoder module to map item meta information to an embedding space. By jointly training on dialog context and encoded item representations, the system can align these two streams of information by fusing the semantic spaces. The joint embedding representations are then processed by the self-attention mechanism of the PLM to perform recommendation and response generation. In this section, we present our framework MESE that integrates item metadata with dialog context. We ﬁrst introduce how to encode item metadata and how to process dialog context with the encoded metadata. We then illustrate how the recommendation module and the response generation module are built and how the encoded metadata is incorporated into both modules. Finally, we describe the training objectives and the testing process. Instead of modeling item representations based on their relations with other items in the knowledge graphs (Chen et al., 2019; Wang et al., 2021), we propose to use an item encoder to directly map the metadata of each item to an embedding. In the movie recommendation setting, description on title, genre, actors, directors, and plot are collected as metadata and concatenated with a "[SEP]" token for each movie. This concatenated information is the input to the item encoder which produces a vector representation for each item. The item encoder consists of a DistilBERT (Sanh et al., 2020) model that maps the input sequence to a sequence of vector embeddings, a pooling layer that condenses the sequence embeddings to a single vector embedding, and a feed-forward layer to produce the output embedding. A visualization of this module is shown in Figure 1. We construct the embeddings for all items in the database. Next, we discuss how to incorporate items into dialog context with the encoded embeddings and the GPT-2 model (Radford et al., 2018). Previous studies have shown that KG-based frameworks cannot always integrate recommended items into generated replies (Wang et al., 2021). To solve this issue, we introduce a special placeholder token "[PH]" to the vocabulary list of GPT-2. Every occurrence of item name in the corpus is replaced Figure 1: Item Encoder takes in the metadata of an item and outputs an embedding of certain dimensionality. with this "[PH]" token. This modiﬁed dialog sequence is then mapped to a sequence of word token embeddings (WTE) by the vocabulary embedding matrix of GPT-2. An instance of the item encoder is used to encode item metadata into token embeddings. The item encoder takes in item metadata and outputs an item token embedding (ITE) with the same dimensionality as a WTE of the GPT-2 model. The ITE is then concatenated with the WTEs constructed from the dialog context to be consumed by GPT-2. An example is shown in 2. Figure 2: Dialog context is represented as a concatenation of WTEs and ITEs to be consumed by GPT-2. 3.2 Recommendation Module Similar to (Covington et al., 2016), we pose recommendation as a two-phase process: candidate selection and candidate ranking. During candidate selection, the entire item database is traversed and narrowed down to a few hundred candidates based on a calculated similarity score between the dialog context and the item metadata. During candidate ranking, similarity scores between the dialog context and the generated candidates are calculated with ﬁner granularity because only a few hundred items are being considered rather than the entire database. The top candidates after sorting is then Figure 3: Overview of MESE. During training, M examples are sampled from the item database and participate in computing the joint loss Land L, which are then combined with the response generation loss L and jointly optimized. During testing, the entire metadata DB is stored as a nearest neighbor index (NNI). An approximate nearest neighbor search is performed on D Encoder to compute ﬁnal scores and the the highest-ranked candidate will be presented to the user in the generated response. used as prompts for response generation. 3.2.1 Candidate Selection In this section, we describe the training objective of candidate selection. We add a special token "[REC]" to the vocabulary embedding matrix of GPT-2. This token is used to indicate the start of the recommendation process and to summarize dialog context. At the end of each turn, a token embedding sequence is created following Figure 2 in the format of an interleaving of word token embeddings (WTE) and item token embeddings (ITE) to represent all previous dialog context. When recommendation is labeled in a conversation turn in the training dataset, the WTE of "[REC]" is appended to the previous token embedding sequence to form a new sequenceD. Next, GPT-2 takes inDand produces an output embedding sequence. We denote the last vector of this output embedding sequence asDwhich corresponds to the appended special token "[REC]".Dsummarizes dialog context and can be used to retrieve candidate items. In order to let the model learn how to ﬁnd candidates based on their relevance to dialog context, we randomly sample M items and their metadata from the database as negative examples and combine them with the ground truth item labeled in the dataset to get the training samples. Another instance of the item encoder, is used to create candito get candidate items, which is then fed to the ITE date token embeddings for each item in the training samples. The item Encoder takes in metadata of the samples items and outputs a set of candidate vector embeddingsC = (c, c, ..., c), each with the sample dimensionality asD. The recommendation task at this phase is posed as a multi-class classiﬁcation problem of predicting the ground truth item over the negative samples. The probability of each candidate item is deﬁned in (1) and optimized by a cross-entropy loss function, denoted asL: Note that the purpose of this learning objective is to let the model learn how to construct theD representation instead of learning the probabilities of candidate items. The Drepresentation is later used in an approximate nearest neighbor search (Liu et al., 2004) to select candidates from the entire database in testing 3.5. In this section, we describe the training objective of candidate ranking. The goal of candidate ranking is to further perform ﬁne-grained scoring on the similarities between generated candidates and dialog context so that the ﬁnal rankings of items can better reﬂect users’ preferences. Unlikely previous studies where knowledge graphs are applied to complete this task, we propose to use GPT-2 directly. During training, the same context token embedding sequenceDand the same training sample with M negative examples are used. The ITE encoder from section 3.1 is used to map the metadata of the sample to an ITE setT = (t, t, ..., t), where the subscript of each tcorresponds to their index in the database. A concatenation of context sequenceDandTare created and consumed by the same GPT-2 model used above and the output embeddings are computed. Note that the order of candidate items should not make a difference on the values of the outputs. Therefore, we add the same positional encoding to each ITE inTand remove the auto-regressive attention masks among the ITEs. We select from the output embeddings the vectors that correspond to the vectors inT. For each output vector selected, a feed-forward layer is applied to reduce each vector from a higher dimension to a single number with dimensionality equals 1. This set of numbers are denoted by Q =(q, q, ..., q)where the index of each number corresponds to their index inT. The ﬁnal ranking score of each candidate item is deﬁned in (2) and optimized by a cross-entropy loss function, denoted In this section, we describe how to train the model to generate responses based on the recommended items’ metadata. The same token embedding sequenceDis used as context and current system utteranceU = (w, w, ..., w)is used as targets where eachwrepresents a WTE. We only optimize the GPT-2 model to reconstruct system utterances. If the current utterance contains recommendations, we create ITEs by passing metadata of the recommended items through the item Encoder used in 2 and append the ITEs to context token embedding sequenceDto obtainD. If the current utterance doesn’t contain recommendations,Dis simply set to beD. The GPT-2 model is trained to reconstruct the ground truthUbased onD. The probability of generated response is formulated as: The loss function is set to be: Where N is the total number of system utterances in one dialog. Finally, we use the following combined loss to jointly train both the encoders and the GPT-2 model: Loss = a · L+ b · L+ c · L(5) Where a, b and c are the weights of language training and recommendation training objectives. During training, all weight parameters of the two item encoders, the GPT-2 model and relevant feedforward layers participate in back-propagation. An overview of training is shown in Figure 3 During testing, a candidate embedding set over the entire item database is built by running metadata through the item encoder used in section 3.2.1 and stored with a nearest neighbor index (NNI) (Muja and Lowe, 2014). During response generation, when a "[REC]" token is generated, candidate selection 3.2.1 is activated. An approximate nearest neighbor search is conducted over the NNI and K closest candidates are selected based on their similarities from theDvector. Candidate ranking is then activated and the GPT-2 and the item encoder from 2 are used to generate a score for each candidate. When ranking ﬁnishes, the ITE that receives the highest ranking score is appended to the dialog contextDand response generation continues until the end of sentence token is generated. After generation is completed, we replace the occurrence of the placeholder token "[PH]" with the title of the recommended item to form the ﬁnal response. Note that when there is no need for recommendation, our GPT-2 model simply generates a clariﬁcation question or a chitchat response with no placeholder tokens. We only present the case when there’s only one ground truth recommendation in the utterance. However, it’s easy to extend the above approach to multiple recommendations. An overview of testing is shown in Figure 3 In this section, we discuss the datasets used, experiment setup, experiment results on both recommendation and language metrics, and report analysis results with ablation studies. 4.1 Datasets We evaluated our model on two datasets: ReDial dataset (Li et al., 2019) for comparison with previous models and INSPIRED dataset (Hayati et al., 2020) for ablation studies. Both datasets are collected on Amazon Mechanical Turk (AMT) platform where workers make conversations related to movie seeking and recommending following a set of extensive instructions. The statistics of both datasets are shown in Table 1 4.2 Experimental Setup 4.2.1 baselines The baseline models for evaluation on the ReDial dataset is described below: ReDial(Li et al., 2019): A dialogue generation model using HRED (Sordoni et al., 2015) as backbone for dialog module KBRD(Chen et al., 2019): The dialog generation module based on the Transformer architecture (Vaswani et al., 2017). It exploits external knowledge to perform recommendations and language generation. KGSF(Zhou et al., 2020): Concept-net is used alongside knowledge graph to perform semanticaware recommendations. CR-Walker(Ma et al., 2021): performs treestructured reasoning on a knowledge graph and guides language generation with dialog acts CRFR(Zhou et al., 2021): conversational context-based reinforcement learning model with multi-hop reasoning on KGs. NTRD(Liang et al., 2021): an encoder-decoder model is used to generate a response template with slot locations to be ﬁlled in with recommended items using a sufﬁcient attention mechanism. RID(Wang et al., 2021): pre-trained language model and knowledge graph are used to improve CRS performance. We employed GPT-2 model (Radford et al., 2018) as the backbone of MESE for dialog generation, which contains 12 layers, 768 hidden units, 12 heads, with 117M parameters. We recruited 2 item encoders (Sanh et al., 2020) to encoder items in candidate generation 3.2.1 and candidate ranking 3.2.2, respectively, each has a distil-bert model with 6 layers, 768 hidden units, 12 heads, with 66M parameters. We used the AdamW optimizer (Loshchilov and Hutter, 2019) with epsilon set to 1e, learning rate set to3e. The model was trained for 8 epochs on ReDial dataset, and the ﬁrst epoch was dedicated to warm up with a linear scheduler. We set the sample size M during candidate generation and candidate ranking to be 150. We chose K = 500 for the number of candidates during testing. We set a=0.3, b = 1.0 and c = 0.5 as coefﬁcients for 3 loss functions respectively. We performed two evaluations, recommendation evaluation and dialog evaluation, for the model. For recommendation evaluation, we used Recall@X (R@X), which shows whether the top X items recommended by the system include the ground truth item suggested by human recommenders. In particular, we chose R@1, R@10 and R@50 following previous works (Chen et al., 2019; Zhou et al., 2020). We also deﬁne recall accuracy of MESE to be the percentage of ground truth items that appear among the 500 generated candidates in the candidate generation phase 3.2.1 and ranking accuracy to be the percentage of items that appear in the top k (k=1, 10, 50) position of the sorted candidates in the candidate ranking phase 3.2.2. The product of the recall and ranking accuracy is the ﬁnal recommendation accuracy of MESE. We also adopted end-to-end response evaluation following (Wang et al., 2021). We computed response recall (ReR) as whether the ﬁnal response contains the target items recommended by human annotators. For dialog evaluation, we measured perplexity, distinct n-grams (Li et al., 2016), and BLEU score (Papineni et al., 2002). We ﬁrst report recall, ranking, and ﬁnal accuracy on REDIAL dataset of MESE in table 3. From the results, it can be seen that candidate ranking has remarkable performance gains in scoring the items. It demonstrates that pre-trained language model have great potential in making recommendations. One possible reason behind this is that the selfattention mechanism is effective in learning the discrepancies between item semantics and dialog semantics. top k Ranking Acc Recall Acc Final Acc Table 3: Recall, Ranking and Final Accuracy of MESE. Table 2 compares different models on REDIAL dataset. The superiority of MESE persists across recommendation and language generation. On all recommendation metrics, including R@1, R@10, and R@50, MESE outperforms the state-of-the-art models by a large margin. We argue in 5.2 that this signiﬁcant gain of performance is due to the effectiveness of the item encoder. MESE also performs well on the ReR score, which indicates that the ﬁlling placeholder tokens can help integrate recommended items into responses. For language generation, MESE also achieves signiﬁcantly better performance than all other models on distinct ngrams and bleu scores with the exception that the PPL is worse than those of KGSF and NTRD. This indicates that MESE can generate more diverse responses while sticking to the topic. 5.2 Ablation Studies and Analysis In this section, we ﬁrst analyze the reason behind the performance gain of our recommendation module by analyzing the embeddings learned by the item encoder. How much does metadata help with recommendation?We argue that our training objectives on recommendation enable the item encoder to selectively extract useful features pertinent to the recommendation task from item metadata and construct item representations that resonate with instructional semantic properties in the dialog histories. For example, in REDIAL dataset, movie genre information is the most frequently mentioned property in dialog histories and human recommenders often make recommendation decisions based on this property. Although other properties like actors also help with recommendation, they do not appear in the corpus as often as genres or movie plots. We designed the following experiments to test our hypothesis. First, we train MESE with movie genre and plot information removed from the metadata, which we refer to as MESE w/o content, and compare its recommendation performance with MESE in Table 4. Table 4: Comparison Results of MESE and MESE w/o content. As we can see from the table, there is a signiﬁcant performance decrease after we remove genre and plot information, which indicates that MESE depends on the input features to make high quality recommendations. We also point out that movie titles contain weak genre information but are not able to provide adequate features for the item encoder to extract from. How does the item encoder help with recommendation?Speciﬁcally, we select all movie items with only one genre as our candidates, resulting in a subset of ~600 movies. We then select 2 item encoders (section 3.2.2) from MESE, MESE w/o content, and the item encoder before training (MESE raw), respectively, and obtain 3 sets of item embeddings by passing the movie subset to these encoders. On each set of embeddings, we run a K-means clustering algorithm with K being set to be 3, 4, and 5, respectively. For each cluster obtained, we calculated the proportion of the majority genre among all item candidates. This process is repeated 20 times for each K and the average accuracy is reported in Table 5. The reasoning behind this setup is that the more features a set of embeddings contain about genre, the closer the clusters should be towards its central point, thus the higher the accuracy should be for its majority genre. MESE w/o content 0.555 0.589 0.606 As we can see from the table, without training, MESE raw, being the least sensitive to genre information, achieves the lowest accuracy scores on all clusters. MESE w/o content, although deprived of genre and plot, still has slightly higher accuracy than MESE raw due to its exposure to REDIAL conversations. MESE is most sensitive to genre information. This is an indication that by aligning recommendation related information in both dialog context and item metadata, our model is able to generate meaningful representations for the task, which can facilitate the language model to produce better rankings through its multi-head attention mechanism and result in better recommendation performance. Previous KG-based recommendation module, although also modeled metadata as relations with Graph Convolutional Networks, did not compose item representations in a systematic way. What if we remove mentioned entities from dialog context?Mentioned entities are crucial to previous approaches (Chen et al., 2019; Zhou et al., 2020) in terms of recommendations. We train MESE with mentioned entities removed from dialog history and compare its performance with MESE on REDIAL dataset and INSPIRED dataset in table 6. Table 6: Results of MESE and MESE w/o on REDIAL and INSPIRED. We can see removing the entities led to an average of 26.3% performance drop on REDIAL and an average of 11.2% performance drop on INSPIRED. The recommendation performance on REDIAL are more impacted by the removal of entities. That is because the conversations in REDIAL are rich with entities and weak in semantic information, whereas, INSPIRED is more sparse on entities but contains more semantics. REDIAL has 10006 conversations, in which there is 1 mentioned movies among every 21.85 word tokens. Its sentence level distinct 1-grams and 3-grams are 0.15 and 2.81. However, INSPIRED dataset has 1001 conversations, in which there is 1 mentioned movies among every 63.54 word tokens. Its sentence level distinct 1-grams and 3-grams are 0.59 and 6.84. This proves that our model can efﬁciently infer user interests from texts to make high-quality recommendations without explicitly using mentioned entities. This property could be useful in an e-commerce setting where users tend to convey their requirements more with texts than entities. It could also be useful in a cold start scenario where we don’t have many entities in the context and are forced to only rely on semantics. In this paper, we introduced MESE, a novel CRS framework. By utilizing item encoders to construct embeddings from metadata, MESE can provide high-quality recommendations that align with the dialog history. We also analyzed various behaviors of MESE to better understand its underlining mechanisms. Our approach yields better performance than existing state-of-the-art models. As for future work, we will consider applying this approach to a broader domain of CRS datasets. Currently, we only experiment on movie recommendations. As mentioned in section 5.2, MESE is capable of efﬁciently utilizing dialog history as a whole and construct item embeddings that reﬂects user preferences. It has the potential to work well with cross-modal tasks. For example, multimodal works can be explored in the e-commerce domain with MESE based architectures.