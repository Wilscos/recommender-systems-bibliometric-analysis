Keywords: Simulation study, ANOVA, factorial and fractional factorial designs, tables, data reduction, Taguchi robust parameter design. It would be diﬃcult to impossible to ﬁnd an issue of a major research journal in the statistical and data sciences without a simulation study of some sort. Simulation studies typically generate random realizations of samples from populations, systematically varying population or model parameters across the diﬀerent realizations. At each combination of parameters, the performance of models, inferential methods or algorithms are evaluated empirically (e.g. signiﬁcance level, power, predictive ability, etc.). The systematic exploration of parameter combinations enables reporting of results via tables. In other words, a designed experiment is performed on the statistical procedure. So, why not use well-established DAE techniques to design simulation studies and synthesize the results? cient simulation studies, and statistical tools for the analysis of experiments lead to the discovery of easy to interpret results. A systematic and quantitative approach oﬀers the potential of discovering deeper interpretations from simulation Statisticians recommend the Design and Analysis of Experiments (DAE) for evidence-based research but often use tables to present their own simulation studies. Could DAE do better? We outline how DAE methods can be used to plan and analyze simulation studies. Tools for planning include ﬁshbone diagrams, factorial and fractional factorial designs. Analysis is carried out via ANOVA, main-eﬀect and interaction plots and other DAE tools. We also demonstrate how Taguchi Robust Parameter Design can be used to study the robustness of methods to a variety of uncontrollable population parameters. We illustrate how tools for the design of physical experiments can yield eﬃstudies than would be evident from simply looking at tabulated results. This is especially true for larger studies, where 4, 5 or more factors are varied. In such cases, where the layout of the table could either reveal or hide relationships, it’s unrealistic to think that looking at large tables will reveal everything. Tools such as ANOVA are designed for identiﬁcation and interpretation of important eﬀects. We illustrate how design of experiments can save time while giving accurate and intepretable results. For example, rather than run a full factorial design at all combinations of factor levels, fractional factorial designs save computational eﬀort and permit the systematic study of more factors. We also illustrate Taguchi’s Robust Parameter Design (TRPD), a design and analysis tool that focuses on identifying and controlling the eﬀects of random variation. In the context of studies of statistical procedures, a TRPD approach helps us ﬁnd models and methods whose performance is stable across uncontrollable factors such as unknown popluation parameters, sampling schemes and violations of model assumptions. For example, we will use TRPD to guide the choice of a testing procedure and tail direction, so that type I error is stable and as close as possible to the nominal level, across diﬀerent sample sizes, noise levels and degree of censoring. (2002), who argue that tables in statistical papers would be better presented as graphs. Likewise, we argue that statisticians would do well to use their own tools for their own studies. Our focus is somewhat diﬀerent than Gelman et al.. They consider a variety of tables, often with diﬀerent types of measurement in the same table (e.g. both estimator bias and standard error). Our tables contain a single type of measurement, because of the type of tools we use. DAE tools are usually intended for a “single response” case, in which a single measurement is made at diﬀerent combinations of multiple factors. Fortunately, this type of study is commonplace, making our approach widely applicable. introducing a simulation study originally presented in Krishnamoorthy, Mallick and Mathew (2011). Their work examines the eﬀect of 5 diﬀerent factors on the type I error rate of statistical hypothesis tests. Section 3 presents our general approach to the planning, execution and analysis of a statistical study, based on the ﬁve stages of an industrial experimental study. We return in Section 4 to the motivating example, illustrating the ﬁve stages in a detailed analysis. The ﬁndings of the original study are identiﬁed, along with several new results. A limitation of re-analyzing a previous study is that we cannot plan the investigation using DAE tools. With this in mind, Section 5 presents an end-to-end example in which we design, execute and analyze a study of the performance of 2 statistical learning algorithms. We conclude the paper with discussion of some related issues. Our work is similar in spirit similar to that of Gelman, Pasarica and Dodhia The paper is outlined as follows. In Section 2, we motivate the problem by To motivate the application of DAE to the planning and analysis of simulation studies, we consider a previously published study. The point of our illustration is to demonstrate that more can be gained by using the tools of DAE and not to be critical. Krishnamoorthy, Mallick and Mathew (2011, hereafter KMM) examined the performance of 4 hypothesis testing procedures for the mean of a log-normal distribution, using censored data. The performance of left-, rightand two-tailed tests with varying sample sizes were considered. Data were simulated from diﬀerent populations, varying the proportion of left-censored observations and population standard deviation. Thus a total of 5 factors were varied in the study: method, tail, n, p0 and sigma. izations of a sample from the lognormal distribution were generated under the null hypothesis. Type I error rates are estimated by the proportion of the 1,000 simulated datasets for which the method rejects H statements summarizing patterns in the table. ment with 5 experimental factors (method and tail of the analysis procedure, n of the data collection process and p0 and sigma of the true population model). These factors and their levels are summarized in Table 2. There are a total of 4 × 3 × 3 × 4 × 3 = 432 combinations of method × tail × n × p0 × sigma. In DAE language, this is a 3 corresponding to 1 of the 432 cells in Table 1. such. Table 3 displays an ANOVA for a 4th order model based on all runs with “type I error rate” (recorded as a value between 0 and 100) as the response. The remaining 72 degrees of freedom are treated as residual, although they could be assigned to the 5th order interaction (leaving no estimate of residual error). Factors n, p0 and sigma are numeric, but treated as categorical in the ANOVA. eﬀects, including high-order interactions. For example, the 2-way interaction method:tail is very large, indicating that type I error rates vary with both factors, and that the way in which the error rates vary across tail type varies by method. Such interactions are diﬃcult to spot by inspecting tabulated results, and often relies on how the factors are arranged in rows and columns as in Table 1. In Section 4 we demonstrate how ANOVA gives insight into the study. KMM’s results are reported in Table 1. For each entry, 1,000 diﬀerent real- The simulation study can be thought of as a full factorial designed experi- Since the simulation study is a designed experiment, it can be analyzed as Even a cursory inspection of the ANOVA table indicates many signiﬁcant Table 1: Estimated type I error rates reported in KMM. Entries are percentages on a 0 - 100 scale. The example in the previous section highlights that more can be learned from a simulation study than merely inspecting a table of the quantities of interest (QoIs) from each simulation. Instead, we propose using the tools. tion, (ii) planning, (iii) execution, (iv) analysis and (v) conclusions (e.g. MacKay & Oldford 2000). Keeping in mind that downstream stages can impact upstream decisions, the stages are very much connected. For instance, the choice of QoI in the problem deﬁnition impacts the analysis of the simulation experiment, but the choice of analysis method impacts the simulation design in the earlier planning stage. simulation studies. We describe this for very generic simulation studies, but the ideas can be adapted to almost any setting. The goal (i) is frequently to assess the performance of methods that combines the simulation results into QoIs that allow for comparison (e.g., power, integrated mean square error, conﬁdence interval coverage, etc.). Comparing methods is not the only possible objective. For example, the goal may be comparing the performance of estimators using an appropriate QoI. If the comparison to be made is between using the sample mean and the sample median for the estimator of the population mean, the QoI may be mean square error of each estimator over the simulation runs. We will return to this running example at several points in this section. the list of factors to include in the experiment. A large number of potential factors should be identiﬁed, and then reduced. A reduced set of factors will lead to an experimental design that can be run in a reasonable amount of time. method Hypothesis testing(AN = Asymptotic Normality of the Broadly speaking, an experiment consists of ﬁve stages: (i) problem deﬁni- While not identical to physical experiments, the stages can be adapted to Planning (ii) a simulation study is trickier. A key early task is to establish Table 3: ANOVA table for 432-run full-factorial experiment. A useful tool for organizing the factors that might impact a QoI is a causeand-eﬀect diagram. In physical experiments, the diﬀerent factors impacting the performance of a process are usually grouped as branches on the cause-andeﬀect diagram. These branches correspond to the major categories: methods, machines, people, materials and measurement. In a simulation study for comparing methods, the factors that one might consider adjusting can instead be categorized under methods, population, population design, simulation design and validation design as depicted in Figure 1. In a standard cause-and-eﬀect diagram the population design settings would usually be given by sub-structures within the population design. We have found it easier to visualize by separating the two. Under methods, for example, the list of statistical methods to be compared is likely clear from the context. This might be, for example, a proposed method and other competing methods. Similarly, Population is a factor when there is more than one population model from which simulations are to be drawn (e.g., data arising from a normal population or a t-distribution). The population design is a little more complicated. Under this branch, the population parameters (e.g., mean structure, eﬀect size, variance and correlation amongst the predictors and responses) are potential factors. Together, the population and population design branches determine the distributions from which the data are simulated . simulation design consist of factors that are external to the population such as the size of the sample and covariate settings. Of course, there are grey areas. For example, ﬁxed covariates are likely to placed under the simulation Looking at Figure 1, it is straightforward to ﬁll in some of the branches. Facing any experimenter is the issue of sampling from the population. The design. If random covariates are being used, the covariate distributions may be speciﬁed under the population design. The validation design typically would entertain similar factors as generating the simulation data, but may consider other population parameters if robustness is being investigated, or diﬀerent covariate settings if extrapolation is being studied. factors, but typically lists more factors than can be accommodated in most physical experiments. The factors thought to be of most interest are then selected from the factors listed on the cause-and-eﬀect diagram, while the remaining factors are ﬁxed at nominal levels in subsequent stages. We recommend doing the same. or factor levels, to include in the simulation study. For some factors, this task is straightforward. If model is a factor, then the factor levels are simply the list of models included in the study. On the other hand, for a factor such as the magnitude of a regression coeﬃcient or an error variance, the choice in levels need be realistic in real-world settings. Furthermore, the levels settings for each factor should be suﬃciently distant so that potentially meaningful diﬀerences in the QoI can be observed. sample mean and median, the number of factors is likely not too large and the cause-and-eﬀect diagram is fairly simple (e.g., Figure 2). Estimators are the only factors under the Methods branch. The sampling populations are chosen to be normal and gamma distributions (Population branch). Under the Population Design branch we have a ﬁxed mean, and variances of 1 and 10. Also explored (under the Simulation Design branch) is the sample size (n = 10, 50 and 100). This example is not a predictive model, so there are no covariates, or any other factors under the Validation Design branch. responds to choosing the experimental design. For example, in the mean vs. median example, a full factorial design could be formed with all 2 × 2 × 2 × 3 possible combinations of the levels of the 4 factors. If there are q factors, each with only two levels, there are 2 for a full factorial design. As the number of factors and the number of levels grows, the number of runs per replicate of the simulation design increases quickly. If there are ten factors, each with two levels, there are 1024 possible runs for each replicate of the simulation study. A study this large can be too time-consuming or expensive to execute. When full factorial designs cannot be run, experimenters typically turn to fractional factorial designs (e.g., Box, Hunter & Hunter 2005). We propose the use of fractional factorial experiments for simulation studies as well. In the illustrations considered in this paper, we consider only two-level fractional factorial designs, however there is a rich literature on fractional factorial designs with more than two levels and also diﬀerent numbers of levels for each factor (Wu & Hamada 2009; Mukerjee & Wu, 2006). simulation experimental design) in hand, it is tempting to move directly to The cause-and-eﬀect diagram is a ﬁrst step at identifying and organizing the At this point, one is faced with the selection of the settings of each factor, Looking back to the example of estimating the population mean with the The choice of factor level combinations to include in the simulation cor- With the choice of level settings and factor level combinations (i.e., the Figure 2: Cause-and-eﬀect diagram for comparing the sample mean and sample median as estimators of the population mean. stage (iii) and execute the entire simulation study. Before doing so, there are still some details to address. First, one must decide how many replicates of the simulation design to conduct. Indeed, it is likely that the size of the simulation design and the number of replicates are chosen together. Second, it is frequently worth running a small pilot study to see if the simulations can be executed as planned. The pilot study also allows evaluation of the choice of each factor levels settings and develops an estimate of the required computational resources to complete the study. For example, if interest lies is comparing the mean and the median as estimators of the population mean and the population variances considered were 1 and 2 and the sample sizes were also small, it is likely that the diﬀerences would be too small to reach concrete conclusions. A small pilot study (in which a wide range of sample sizes were considered while holding other factors ﬁxed) would likely detect this and lead to diﬀerent choices of factor level settings. simple step insofar as the desired simulations are performed. A single instance of the simulation may include simulating data for estimating a model, simulating a validation set, model ﬁtting, making predictions or conducting hypothesis tests, and computing the QoI. If the tasks to be performed are relatively fast, then the simulation study may be done in a sequence. On the other hand, a simulation may involve inversion of large matrices, challenging optimization, many steps of a MCMC, or combinations of these. In such cases, it will be necessary to parallelize the simulation runs. and the factors under study. As with the analysis of designed experiments, Following the pilot study, the simulation design is run. This is a conceptually The analysis stage (iv) is the study of the relationship between the QoI we recommend ANOVA models and associated tools be the primary techniques used in this stage. The usual “statistical toolbox” for analysis, including model diagnostics, hypothesis testing, transformation, prediction intervals, etc, will be useful. Indeed, the purpose of a careful simulation design is to make the analysis eﬃcient, interpretable and also relatively easy to do. tion tool for settings in which some factors are usually under the control of researchers, such as the choice of inferential procedure or model, and other factors are uncontrollable, such as an unknown population variance. In TRPD, we seek combinations of the “control” factors that give results that are robust across the levels of the uncontrollable (or “noise”) factors. In the case of comparing estimators of the population mean, the estimators would be control factors, and the population distribution would typically be a noise factor. A TRPD analysis would focus on the “best” estimator, but also identify the estimator that was more robust to changes in population distribution. In terms of the categories for our cause-and-eﬀect diagrams, it is often the case that factors on the Method branch will be control factors, and factors under the other branches will be noise factors. For an introductory overview of TRPD, see Nair et al. (1992) and references therein. feature of ANOVA models is the ability to identify important factors and, in the case of factors with more than two levels, determine which factor levels are signiﬁcantly diﬀerent. For signiﬁcant main eﬀects and interaction eﬀects, we recommend using plots to observe the magnitude and nature of the eﬀects. to the design of simulations studies. In the following sections, we illustrate how the ﬁve experimental stages can aid in understanding the performance of methods in two examples. We revisit the KMM study, considering the ﬁve stages of experimentation outlined in Section 3. One complete cycle of the stages is demonstrated in Section 4.1, including a TRPD analysis. Additional analysis of higher-order interactions in Section 4.2 gives further insights. Our conclusions are compared to the original ﬁndings in the KMM paper in Section 4.3. We end by demonstrating in Section 4.4 that similar conclusions could have been reached by a smaller experiment. (i) Problem deﬁnition: The aim of the KMM study was to investigate the merits of four hypothesis testing procedures for the lognormal mean, in the presence of left-censoring, using simulation. As such, the problem deﬁnition is identiﬁed, and the chosen Later in the analysis, we also illustrate TRPD as a modelling and optimiza- The conclusions (v) should ﬂow naturally from the analysis. A convenient The aim of this paper is to propose that the tools outlined above be applied QoI for the simulation is the type I error rate. Ideally, the type I error is close to the nominal 5%. As in KMM, we will assume that it is preferable to be at or slightly below the nominal level. So, a 4% type I error is preferable to 6% because it controls the type I error below the nominal level of 5%. (ii) Planning: Since we did not participate in the planning of the experiment, a full description this stage is impossible. In particular, a full list of potential variables to include in the experiment is not available. The cause-and-eﬀect diagram for the full 3 full factorial design in the simulation study is presented in Figure 3. We include a few factors that were held ﬁxed, but KMM would likely been considered as potential factors, such as the distributional family (lognormal) and population mean (set at 0). For each factor level combination, 1,000 simulations were drawn from the lognormal distribution under the null hypothesis with a nominal type I error rate of 5%. We would normally recommend a small pilot study to investigate whether the level settings are likely to result in meaningful diﬀerences in the QoI. The pilot study allows the experimenters to estimate the run time of computations for the full suite of simulations. In the KMM study, computation time was probably not a major hurdle. would have been identiﬁed as having much larger variation in type I error than any other method. Reading across any AN row of Table 1 reveals much more Figure 3: Cause-and-eﬀect diagram for the KMM simulation study. If a pilot study had been carried out, it is quite possible that the AN method variation in type I error for this method than any other method. A pilot study, looking a small subset of the full table would be likely to identify the poor performance of AN. In light of the terrible performance of the AN method, we remove the 108 runs with method = AN, and analyze the remaining 324 runs as a 3 design. In the event that a pilot study did not identify AN’s terrible performance, it would show up in a preliminary analysis. For example, the main eﬀects plot and 2-way interaction plots (not shown) indicate the poor type I error performance of the AN method, including a mean type I error rate of 7.6%. ulated QoI (e.g. Table 1). Detection of all but the largest eﬀects can be timeconsuming and depends heavily on how the table was constructed (i.e., which factors are placed on columns and rows, and also which are inner and outer column factors). Instead, we propose using ANOVA, or related methods, for the analysis of the simulations. the 324 runs. In comparison to the ANOVA of the 432 runs (Table 3), fewer interactions are now signiﬁcant, and the estimate of residual variance (MSE, the Mean Square for residuals) is considerably smaller than for the original 4th order ANOVA (original MSE = 0.29, new MSE = 0.19). Main eﬀects are displayed in Figure 4(a). Of note are: eﬀects are small, the type I error is always close to the desired 5%. However when interpreting eﬀect plots, it’s important to bear in mind that they show one eﬀect at a time. The full prediction from the model is additive in main eﬀects and interactions. Thus we proceed to look at 2-way interactions. ted in Figure 4(b) and (c). While other interaction eﬀects are signiﬁcant, they are much smaller. Large interaction eﬀects are seen as non-parallel lines. ods exhibit considerable range in type I error rate over the 3 diﬀerent tails. For It is common to analyse simulation studies by visual inspection of the tab- Table 4 shows a summary of an ANOVA for the 4th order model using • The largest main eﬀect is tail. • Main eﬀect magnitudes are small, and the main eﬀect of n is negligible. • Response means are close to the nominal level of α = 5%. • For the numeric factors n, p0, sigma, eﬀects are monotone or almost so. The most visible example is increasing mean response as sigma increases from 1 to 3. It might be tempting to conclude from the main eﬀect plot that since main The two largest two-factor interactions, method:tail and tail:p0 are plot- The largest two-factor interaction is method:tail (Figure 4(b)). All 3 meth- Table 4: ANOVA table for 324-run full-factorial experiment excluding method = AN. Figure 4: Estimated main eﬀects (a) and select 2-factor interactions (b), (c). The vertical axis is the response, “Type I error rate”, which should have a nominal level of 5%, indicated by the horizontal blue line. example, when method=MS, the mean type I error rate is just above 6% for a left-tailed test, while it is well below 4% for a right-tailed test. Thus, a change in tail from left to right produces an (interaction) eﬀect for the MS method of about 2.5%. This eﬀect will be additive with the eﬀects involving other terms, and could push predictions of type I error rate much higher or lower. left- and right-tailed tests varies with censoring proportion p0. The downwardsloping dashed line indicates that right-tailed tests become increasingly conservative as p0 increases. The nearly ﬂat solid line indicates that for two-tailed tests, type I error rate is stable and close to 5%. Ideally, the choice of method would be one that results in a type I error rate close to 5% across changes of factors that are not under the control of the experimenter. This desire for robustness leads us to consider the analysis of the KMM study in the context of TRPD. The interpretation above of the tail:p0 interaction suggests that the two-tailed tests are robust to changes in p0. The idea of TRPD is to identify experimental factors that can be controlled, and choose levels of those factors so that the response has a value equal to a target, with small variation. Figure 4(c) shows the tail:p0 interaction eﬀects. The type I error rate for Here is a quick primer on TRPD in this setting: • The factors method and tail are control factors, which can be chosen by an analyst. • The factors n, p0 and sigma are noise factors, which are largely beyond The previously identiﬁed tail:p0 interaction in (Figure 4(c)) can be interpreted in the context of TRPD. The tail of the hypothesis test is the control factor since the user could choose that. The user cannot control censoring level p0, making it a noise factor. We want to choose a tail so that no matter what p0 is, our Type I error rate is close to the desired 5%. Choosing tail = T gives a more robust response (a line closer to horizontal). and look at the variation in response at each combination of the 2 control factors method and tail. Figure 5 does this, showing histograms of the response for each of the 9 combinations of method and tail. We see that: bution of response values. Choice of control factor levels as a way of controlling both a target (i.e., location) and variation is a central element of TRPD. The TRPD analysis has identiﬁed some important conclusions, in addition to the previous conclusions that the AN method is terrible, and some 2-way interactions are larger than the fairly small main eﬀects. We postpone further discussion of conclusions until Section 4.3. the control of the analyst. One might argue that n can be controlled, but for illustrative purposes, we will assume that samples are expensive, and so changing n is unlikely to be controllable. • The target value for the response in this example is the nominal 5% level. • The idea of TRPD is to choose a level of the control factors so that responses are close to the target and are robust (insensitive) to the uncontrollable (noise) factors. Large interactions between control and noise factors provide an opportunity to achieve such robustness. Another way to examine robustness is to “lump together” the noise eﬀects • 2-tailed tests give type I error closer to the desired 5%, no matter what method is used. The histograms in the right column are centered around 5% and have small variation. • If a left-tailed test is desired, the SL method is preferred over the other methods. It is slightly conservative with levels just below 5% and less variation in level than the other methods. The other methods are too liberal and have some levels well above 5%. • If a right-tailed test is desired, GV and MS are conservative (sometimes quite so), while SL is liberal. All exhibit considerable variation in type I error, but the conservative tests may be preferred. Notice that the ﬁndings relate to both the centre and spread of the distri- Figure 5: Histograms of response within each combination of method:tail. Columns of the grid correspond to tail and rows correspond to method, as indicated by titles above each histogram. Having identiﬁed and examined main eﬀects and two-way interactions and considered robustness, an analysis would often be complete at this point. Higherorder eﬀects are often small. We have illustrated the ﬁve stages. In this example, however, it’s worthwhile to press on further. Higher-order interactions appear large (ANOVA in Table 4). Comparing the two-way interaction plots (Figure 4) and the range of type I errors (extremes visible in Figure 5), we see the range exceeds the mean levels in the interaction plot. This also suggests that higher order interactions may explain additional variation in type I error. The presence of signiﬁcant and large 3rd and 4th order eﬀects (ANOVA in Table 4) indicates that further analysis beyond main eﬀects and 2-way interactions is necessary. As in the last section, we analyze the subset of 324 runs with method 6= AN. Visualizations need some modiﬁcation to simultaneously display the eﬀect of 3 or more factors on the response. and tail into a single factor with 9 method/tail levels (GV/L, GV/R, GV/T, MS/L, ..., SL/T) and plot the new combined control factor against the 3rd factor and the response. The layout is similar to 2-way interaction plots in Figure 4, but with 9 lines for the 9 control factor combinations. Line type and color represent method and tail respectively. Such a modiﬁed plot isn’t always required for TRPD, but the prevalence of higher-order interactions in To visualize 3-way interactions, we combine the two “control” factors method Figure 6: Three-factor interaction plots for TRPD, showing interaction eﬀects method:tail:p0 (a) and method:tail:sigma (b). Line type represents method and line colour represents tail. this example makes it necessary. interactions as method:tail:p0 and method:tail:sigma. These interactions are visualized in Figure 6. The fact that control by noise interactions are large suggests that TRPD can be used to identify sources of uncontrollable variation, giving more insight into the methods. As mentioned earlier, the idea of TRPD is that we want to choose a level of the control factor so that responses are robust (insensitive) to the uncontrollable noise factors. That is, we want to choose a method and tail so that no matter what n, p0 and sigma are, our Type I error rate is close to the desired 5%. that they only show a portion of the full ANOVA contributions of all 5 factors to the response. For example, Figure 6(a) shows a method:tail:p0 interaction. The previous ANOVA table (Table 4) identiﬁes the two largest three-factor Some interpretations of these results (not previously seen in the histograms): • From the method:tail:p0 interaction plot (Figure 6(a)), both the GV and the MS methods have poor type I error for large p0 (i.e. large amounts of censoring). The solid and dashed lines have an upward slope for left-tail (red) and downward slope for right-tail tests (green). • From the method:tail:p0 interaction plot (Figure 6(a)), the SL method does not change its level as censoring changes (dotted lines are horizontal). However, the left-tailed SL test (red dotted) is conservative and the righttailed SL test (green dotted) is liberal. • From the method:tail:sigma interaction plot (Figure 6(b)), the MS and SL methods have better type I errors for sigma=1 than sigma = 2 or 3. The GV method seems less sensitive to sigma. That is, the 3 solid lines are closer to horizontal. One ﬁnal issue to bear in mind when interpreting the interaction plots is The predicted response may shift up or down, depending on eﬀects involving sigma and n. The analysis we carry out focuses on the plots one at a time, eﬀectively averaging over other factors. studied in Supplementary Materials. KMM summarize their ﬁndings of this study over two paragraphs of text. Below, we quote their ﬁndings and compare them to our conclusions (in italic). We then identify some additional ﬁndings not given in KMM. 1. “The test based on the asymptotic normality of the MLE (denoted by 2. “Performance depends on sigma, p0 and tail.” We observe this also, 3. “No test emerges as the clear winner in all the scenarios.” We come to 4. “For left-tailed testing, the SLRT appears to exhibit the most satisfactory 5. “For right-tailed testing, it is the GV approach that provides the most 6. “For two-tailed testing, all the three procedures provide satisfactory per- 7. “As the sample size gets large, there is very little diﬀerence between the In the KMM study, some fourth-order eﬀects are also large. These are AN in Table 1) seems to be the worst among all tests....Thus, the test AN should be avoided in applications.” This was overwhelmingly clear and we dropped runs corresponding to method=AN in our analysis. A pilot study might have identiﬁed and eliminated AN before carrying out the full experiment. although we additionally note that the dependence is complex with higherorder interactions that would be diﬃcult to see by visual inspection of the results. Some eﬀects containing n are statistically signiﬁcant, but are relatively small contributions. the same conclusion (e.g., see Figure 5). performance, even though it is conservative in some cases. Both the GV approach and the MSLRT appear to be quite liberal in most cases, especially when sigma is large.” Figure 5 also conﬁrms all these conclusions. satisfactory solution.” GV is good for right tail tests, but so is MS (Figure 5). formance, even though the SLRT is liberal and the MSLRT is conservative in a few cases. The GV approach appears to be quite satisfactory for twosided testing, for all the cases considered in the simulation.” Our ﬁndings are similar. GV approach and the MSLRT. In several instances, the MSLRT is not as satisfactory as the SLRT.” Table 3 indicates that only one signiﬁcant eﬀect involves n, namely method:tail:n. KMM’s ﬁnding is based partly on a seperate run not included in the study, with n = 100. Over the range n = 20, 30, 50, the eﬀect of sample size is relatively small. As previously Can you learn the same thing with less? That is, could a smaller number of runs have been used to reach similar conclusions? Two ways to reduce the number of runs are i) using fewer levels for the factors and ii) using a fractional factorial design rather than considering all combinations of the runs - both reminiscent of issues in DAE. Here we consider reducing the number of levels (i). See Section 5.1 for an illustration of analysis of a fractional factorial. noted, we would expect that the relatively small eﬀect of n might have been detected in a pilot study, and potentially suggested including n=100 in places of one of the other settings (likely n=30). Did we discover anything KMM did not? Perhaps: • Type I error rates vary in a complex way over the 5 factors. This is evidenced by large and signiﬁcant interaction eﬀects, even at 3rd and 4th order. • After excluding method = AN, left tail tests are, on average a bit liberal. Right tail tests are a bit conservative, and two-tailed tests are on average at the correct level. • Sample size n has a relatively small eﬀect, compared to the other factors. This may be in part due to the range of values chosen for n. Indeed the authors report some additional results for n = 100, not contained in the main study. • KMM identify only GV as the preferred method for right-tail tests. We ﬁnd that method = MS is just as good (Figure 5). • By systematically analyzing the response, we were able to identify the important factors. In this case, it turned out that there were signiﬁcant and important high-order interactions, many involving method and tail. While this complicated the interpretation somewhat, the end result was an analysis in which we are conﬁdent we have found the most important eﬀects. • The small eﬀects involving sample size n could be due to a rather small range of factor levels (20 to 50). One might hope that with suﬃciently large samples, type I error rates would approach the nominal level, as asymptotics begin to play a role. • The presence of large interactions between the 4 factors method, tail, sigma and p0 (see Supplementary Materials) suggest that some combinations of method and tail may be more stable across uncontrollable factors. As sigma and p0 increase, one-sided tests deviate more from the nominal level (“cone shape” of Y vs. p0 in Figure 10 (Supplementary Materials), with more deviations for large sigma). (n = 20 or 50, p0 = 0.2 or 0.7 and sigma = 1 or 3). We also restrict attention to the 3 “better” methods, excluding method = AN. This reduces the original table with 432 entries (324 after discarding method = AN) to 3×3×2×2×2 = 72 runs. signiﬁcant eﬀects than in the analysis of the 324 runs in Table 4. However, the ﬁve largest eﬀects (by SS) are the same as in Table 4: tail, method:tail, tail:p0, method:tail:sigma and method:tail:p0:sigma. All are signiﬁcant in the reduced analysis. main conclusions are the same, namely that n is not an important factor, but that (some) interactions between the other 4 factors are important and that there are high-order interactions. Figures 4 - 6 are re-done for the 72 runs, and presented in the Supplementary Materials. The ﬁndings are generally the same. of simulations. However for this study, 5 factors is a smallish number and computations are quite fast, leaving little reason to do so. Additionally, the choice of fractional factorial designs for mixed-level data is a somewhat advanced topic. In this section, the ﬁve stages of experimentation are illustrated using a study that investigates the predictive accuracy of two statistical learning models with a continuous response. The purpose of the illustration is to highlight how the ﬁve stages can be considered in practice. A fractional factorial design is introduced as a way to reduce the number of runs. Our study examines predictive accuracy of possible models for supervised learning. A variety of response variables could be used to measure predictive accuracy. We will focus on R (n = 10, 000) test sets. To begin, a cause-and-eﬀect diagram (Figure 7) is constructed to organize the list of potential factors. While one can imagine many potential factors to include, an experiment based solely on those in the Figure 7 can get quite For numeric factors with more than 2 levels, we use only the extreme levels An ANOVA for this reduced data is shown in Table 5. There are fewer Analysis of the reduced 72 runs uncovers many of the same patterns. The One could use a fractional factorial design to further reduce the number Table 5: ANOVA table for reduced 72-run full-factorial experiment, excluding method = AN. Figure 7: Cause-and-eﬀect diagram for the statistical learning simulation study. large. Some choices have to be made to keep the run size manageable. To keep things simple, for example, two statistical learning procedures (lasso and random forests) are compared and only a normal parent population is used. We eventually chose seven factors, each with two levels, and excluded or held ﬁxed all other factors (e.g. we used only normal errors and did not investigate outliers). The factors are summarized in Table 6. training and test data are generated from a linear model with normal errors, where X is a p−vector of predictor variables formed from q measured variables (q < p). (“Measured variables” is used to identify distinct variables that can be thought of as each being “measured”; predictor variables are constructed from the “measured variables”). The predictors in the linear model will include Table 6: Factors in the statistical learning study. See text for details. We now discuss the choice of levels for each of the factors. In all simulations, linear terms equal to individual measured variables and product terms formed by multiplying together 2 measured variables. Thus there are a total of p = q + β are exactly 0, corresponding to inactive predictors. 1. Least squares regression using the lasso and potential predictors consist- 2. Random forest regression using q measured variables as predictors. Since the form of the lasso model matches the data-generating mechanism, we expect it to outperform random forests. of lasso regression, there is a single penalty parameter λ which will be chosen by running 10-fold cross-validation within the training set. In the case of random forests, three potential tuning parameters are mtry, the proportion of predictors that are randomly sampled when growing each branch of a tree, ntree, the number of trees in the random forest ensemble, and maxnodes, the maximum number of nodes that a tree can contain. Although there are 3 tuning parameters, pilot runs indicated that ntree = 500 and no limit on maxnodes gave the best results. The remaining random forest parameter, mtry, will be chosen from a grid of proportion values (0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 1), using the out-of-bag data. Thus, both lasso regression and random forests are using sample-reuse techniques to choose the optimal tuning parameter. equal to beta.mu. The error standard deviation sigma corresponds to σ in (1). The q “measured variables” are simulated as multivariate normals with mean vector 0 and covariance matrix Σ. The diagonal elements of Σ are always 1, giving measured variables that are standard normal variates. The factor x.cor controls the correlation between the measured variables, with the correlation between variables i and j given by This decaying correlation gives a (maximal) correlation of x.cor between measured variables with adjacent indices (e.g. j = i +1), and smaller correlation for other pairs of measured variables. This formulation of the measured variables results in “product terms” having mean 0 and variance 1, so that scaling of the predictors is not needed. than having a ﬁxed pattern, this is done at random. An “eﬀect heredity” prior distribution (Chipman 1996) speciﬁes a prior probability that a main eﬀect will be active as π, assuming that activity of main eﬀect are independent of each other. Conditional on whether “parent” main eﬀects A and B are active, the = q(q + 1)/2 predictor variables. Some elements of the coeﬃcient vector The two statistical learning models are : ing of all possible linear eﬀects and product eﬀects (total of q(q + 1)/2 predictors). Both procedures have tuning parameter(s) that must be chosen. In the case The sample size of the training set is n. The nonzero elements of β will all be Some elements of the regression coeﬃcient vector β are set to zero. Rather probability that the AB product term is active is given as P (AB active | activity of A, B) = For given values of q, π, c 2-way interactions with 2 active parents, and active 2-way interactions with 1 active parent can be calculated. In our experiment, π, c that: For given values of ENE and q, and using the 50/25/25 split of active eﬀects speciﬁed above, the parameters π, c calculated. Derivations are sketched in the Supplementary Materials. In each simulation run, the actual number of active eﬀects will be a draw from this distribution. The division of the active eﬀects among main eﬀects, single-parent 2-factor interactions and two-parent 2-factor interactions will also be random. fractional factorial designs as a way of saving computational eﬀort. In this example, we will choose a design with 32 runs. Fractionating a 2-level full factorial design is a well-studied problem and is a standard topic in introductory DAE texts (e.g. Box, Hunter & Hunter 2005). A 2-level fractional factorial is typically constructed by selecting rows based on “generators”, which are identities that apply to the columns of the design matrix. To simplify notation we label the 7 factors in our experiment as n = A, q = B, ENE = C, beta.mu = D, sigma = E, x.cor = F and model = G, and assume each 2-level factor is coded with levels −1 and +1. The generators we will use are ABCE = +1 and BCDF = +1. Of the 128 runs of the full-factorial (without replication), 32 runs satisfy both these conditions, making this a quarter fraction of the full factorial design. For these runs, the generators will imply aliases between eﬀects. That is, an eﬀect estimate will actually correspond to the sum of several eﬀect estimates. For example, the AB interaction will be aliased with the CE interaction (implied by the ﬁrst generator). The AB interaction will also be aliased with other higher-order terms. The generators imply the following aliasing patterns in this case: 1. Main eﬀects A - E are each aliased with 2 diﬀerent three-way interactions, • A total of ENE eﬀects are expected to be active. • ENE/2 main eﬀects are expected to be active. • ENE/4 2-way interactions with 2 active parents are expected to be active. • ENE/4 2-way interactions with 1 active parents are expected to be active. The factor ENE is set at a value chosen as part of the experimental design. Instead of running a full factorial 2design (with 128 runs), we introduce and higher order terms. 3. Two-way interactions that do not involve G are aliased in pairs, e.g. AB 4. Two-way interactions involving G are aliased with four-way interactions The presence of aliasing between pairs of two-way interactions implies that this is a resolution IV design. is of special interest as the only control factor in TRPD. Similar to the analysis of the KMM study in Section 4, control by noise interactions give insight into the robustness of the models with respect to uncontrollable factors. The generators chosen enable estimation of the main eﬀect for G and two-way interaction eﬀects involving G, provided we assume that fourth-order and higher interactions are negligible. This does come at a cost, namely the inability to disentangle two-way interactions not involving G. Pilot studies were used to prototype the code and check for reasonable factor levels. Random Forests required more compute time, so during prototyping we temporarily set ntree = 50. Table 6 contains the levels settings for the ﬁnal experiment. However, the original choices for the level of the training sample size were n = 1, 000 and n = 10, 000. It turned out that both sample sizes were large enough that non-meaningful diﬀerences in the response variable was observed. Instead, we changed the levels shown in Table 6. independent draw from the population model. Runs were parallelized, using 2 cores of an 8-core laptop and taking under 30 minutes to execute. The response analyzed (QoI) was a logistic transform of the test R is the vector of observed (i.e. simulated) y values in (1) and yhat.test is the corresponding vector of model predictions for an independent test set, then we deﬁne R Preliminary analysis of an untransformed R residual diagnostics and predictions of R The ANOVA model used here is based on least-squares / normal theory, rather than being a GLM. (Daniel 1959) can replace an ANOVA table as the primary analysis technique. In balanced designs like the ones we have discussed, all eﬀects are estimated by = CE. and higher-order interactions. This particular fractional factorial was chosen because the factor model (G) The 32-run quarter fraction design was executed. Every run was a completely An advantage of using 2-level designs is that the simpler half-normal plot Figure 8: Half-normal plot of estimated eﬀects, statistical learning experiment. contrasts with equal variance. Eﬀect estimates for terms that are inactive will all be normally distributed with mean 0. The half-normal plot is a quantilequantile plot of absolute eﬀect estimates against corresponding quantiles of a half-normal distribution (i.e. the absolute value of a standard normal). Eﬀects that are small will fall on a straight line, while large eﬀects will be larger than this line (to the right of the plots in our ﬁgures). important. The large eﬀects are labelled while the remaining smaller, unlabelled, eﬀects appear to form a linear trend. The 6 largest eﬀects are model, sigma, beta.mu, sigma:model, x.cor and beta.mu:model. The 4 large main eﬀects are aliased with 3-way interactions, while the 2 interactions involving model are aliased with 4-way interactions. sigma, x.cor, and interactions beta.mu:model and sigma:model. These eﬀects are represented visually in Figure 9(a) and (b) - (c). better” measure. With this in mind, the largest main eﬀects and their interpretations are: The half-normal plot in Figure 8 suggests that between 6 and 10 eﬀects are The dominant eﬀect is model, with other large eﬀects including beta.mu, Although the response is a transformation of R, it is still a “larger the • model, with lasso performing much better than random forests. • sigma, with better predictive accuracy when the training set has less noise (sigma = 0.5). • beta.mu, with better predictive accuracy when the coeﬃcients are larger (beta.mu = 3). Figure 9: Main-eﬀect plot (a) and select interaction plots (b,c) for supervised learning experiment. Keeping in mind that a large interaction will appear as a large change in slope, rather than mean level, the interaction eﬀects and their interpretations are: In this experiment, the lasso is the clear winner. This should not be surprising since the form of the population model matches the lasso regression. Considering TRPD, interaction plots (Figure 9(b), (c)) show less variation in the performance of random forests than in the lasso, across levels of the noise factors beta.mu and sigma. But even though the lasso model has more variable performance, it is always superior to random forests. The better performance (higher R for this study TRPD turns out to be a secondary consideration. • x.cor, with better predictive accuracy when the measured variables are correlated (x.cor = 0.8). This is likely because the test set has the same correlation patterns among the measured variables, and the performance measure is predictive accuracy, not selection of the correct subset of predictors. In the interaction plots, the largest eﬀects involve model and one other term. • beta.mu:model, with lasso models seeing a larger gain in predictive accuracy than random forests, when signal levels are high (beta.mu = 3). That is, the slope of the lasso line is much steeper than random forests. • sigma:model, with lasso models seeing a larger loss in predictive accuracy than, when noise levels are high (sigma = 0.5). That is, the slope of the lasso line is negative and larger than the slope of the random forests line. ) of lasso dominates the increased sensitivity to noise factors, and so Other experimental designs and their analysis were considered but not reported here. These include a 2 the same design (with 256 or more runs). Supplementary materials demonstrate that for this application, the conclusions would be eﬀectively the same. aliasing did not impact the conclusions. In cases where aliasing was a problem, additional runs could be later added to reduce aliasing due to fractionation. between 2-way interaction terms. Doing so would treat eﬀects more symmetrically, rather than prioritizing eﬀects involving the only control factor, method (labelled G). For example, the generators ABCF = +1 and ABDEG = +1 give a resolution 4 design, but with just 6 out of 21 two-way interactions aliased with a two-way interaction (compared to 15 of 21 in the design we chose). uncontrollable factors), the simplest fractional factorial designs to analyze are crossed arrays, in which designs are constructed separately in the control factors and in the noise factors, and then the designs are “crossed”. That is, for every run in the control-factor design, all runs in the noise factor design are carried out. One or both of the crossed designs could be fractionated. In this example we have 6 noise factors, and our design is a crossed array, choosing a quarter fraction of a design in the noise factors, and running this design once for G=1 (lasso) and once for G = -1 (random forests). This is not rocket science. The tools we have used would usually be covered in an undergraduate course in DAE, with perhaps the exception of TRPD. We hope that readers will decide that it’s obvious to design and analyze statistical studies using statistical tools, that the process of doing so is straightforward, and that there are real beneﬁts to doing so. normally be carried out, such as identifying relevant factors to consider and appropriate levels (both in the planning stage). But other parts are (sadly) novel: Even if a full-factorial experiment is run (planning stage), an ANOVA combined with graphical exploration of eﬀects should make it easier to discover all the important eﬀects (analysis stage). In circumstances where there are many factors that are important to consider in the study, fractional factorial designs (planning, again) oﬀer the possibility of exploring them all, rather than artiﬁcially eliminating some factors in order to design a study that can be run. The TRPD framework (planning and analysis) formalizes the common objective that when choosing among the models being compared, it’s desirable to pick a method that is stable (i.e. robust) across the scenarios we cannot control. Fractionated designs will imply some eﬀects are aliased. In this case, the A diﬀerent fractional factorial could have been chosen, with fewer aliases If TRPD considerations are important (e.g., we want to study robustness to A systematic approach like this formalizes some of the steps that would There are several considerations and open problems not discussed in the paper that we mention brieﬂy. Randomization restrictions to reduce noise, blocking and random effects Our treatment (and both examples) implicitly assume that the random errors are independent and identically distributed across runs. For example, in the statistical learning study, every run in our design drew a separate, independent realization of data from the population model. Thinking further about that example, a natural temptation would be to “pair” together runs that are the same except for the learning method used (lasso vs. random forests). So the 32 runs used in our example could be grouped into 16 pairs, with 16 diﬀerent datasets (rather than 32). Both methods would be applied to the 16 datasets, yielding 32 runs. One might argue that it would be a “fairer” comparison of the 2 models to have them analyze exactly the same data. to the design. Such a design could be analyzed via block terms or a random eﬀects model. Essentially we would be analyzing the performance diﬀerence between the 2 models as a function of the other 6 factors in the study. strictions be worthwhile? It seems intuitive that it would be worthwhile when the restrictions reduce the error variance, enabling better estimation of eﬀects with the same computational resources. In our speciﬁc statistical learning example, the variation in performance could be mostly explained by the experimental factors, and so pairing or other randomization restrictions were not worth pursuing. If there was more unexplained variation due to random “dataset to dataset” variation there would be a stronger case to employ randomization restrictions to reduce the impact of this variation. above. If you are doing a TRPD and have a crossed array design, a row of the noise factor design gives you a realization of simulated data. You can use the same realization of simulated data across all rows of the control factor design. Another way to generalize could include nested restrictions. For example, in our statistical learning example, for every realization of data, we simulate a new noise realization ε in (1). Instead, we might have grouped together runs so that only sigma changed within each group. Then generate 1 realization of ε instead of 3, and just multiply ε by the value of sigma to scale the noise. The studies typically involve the comparison of multiple methods. It’s important that the methods be on an equal footing. This would include choice of tuning parameters, prior distributions (for a Bayesian model), and so on. For example in the statistical learning study, we took pains to identify the pertinent tuning parameters (λ for the lasso, and mtry for random forests) and use reasonable methods (cross-validation or OOB sample reuse) to choose them. researchers to share suﬃcient code that the entire study could be independently Thinking about this structure, this would be equivalent to adding 16 blocks Randomization restrictions will complicate analysis. When might such re- Randomization restrictions can be made more general than the example An important and related idea is that of reproducibility. We encourage replicated. In some circumstances (i.e., specialized or commercial software is being used to estimate the model(s) under study), it may only be possible to share the results and the code used to analyze them. Connecting back to the idea of calibration and fairness, open code makes it possible for others to assess the extent to which methods are fairly compared. example, if one was studying power of hypothesis tests as a followup to the KMM study, a real problem is that fair comparison isn’t possible unless the type I error is controlled across methods. We saw in that study that type I error was not entirely controllable in all situations. Moreover, it would be unrealistic to artiﬁcially control type I error rate in a simulation if such control was impossible in real applications. It may not always be easy or even possible to have fair comparisons. For