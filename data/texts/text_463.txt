Keywords Bipartite graphs · Hierarchical co-clustering · Power-law · Textual documents · Unsupervised learning Tags are words or short sequences associated with a resource or a document. Depending on the context, the role of a tag differs. They could be used to describe feeling, category, source, content, ownership and others [1]. The tags associated with a document form a bag of words, where the order does not matter. Tags can be weighted or ranked by relevance, helping the machine or user to know which are the most accurate or speciﬁc to the document. Tags can be obtained with two different approaches: they can be machine extracted, where an algorithm process the resources and extract some of their characteristics [ expert case, the vocabulary is controlled, and the results would be comparable to machine extracted tags. In contrast, folksonomy corresponds to non-expert tags, leading to a large corpus with redundant or mispelt tags. The corpus is often represented using the vector space model, where a document is represented by a sparse binary vector, where a1encode the presence of a tag within the document. The document’s tags cannot be rank by importance because a document is described by a binary vector. Tags can only be ranked within the corpus, and often shows power-law distribution [4], making their analysis difﬁcult because of the scarcity of frequent keywords and an abundance of unfrequent tags. Additionally, their number per document is relatively smaller than for usual textual documents, as the goal of tags is to provide a synthetic view of the document. This reduces the precision of an analysis when a tag is missing, as there is not enough tag redundancy to compensate for the absence of this tag. A way to improve the tag vector representation is to rely on an external database [ to add additional related tags to enrich the initial tag vector description. Another possibility is to rely on a probabilistic model [8], modeling keywords distribution based on available data. Tags can be use for information retrieval and recommendation. For a machine, it allows proposing related documents using tag co-occurrences. For a human, tags can be used to create tag clouds [ suggesting related keywords. A tag cloud displays the best co-occurring tags, adjusting the size, color, opacity of the tags to the context. Nonetheless, the tag cloud utility is limited due to the amount of irrelevant unorganized information [10], [11]. Rather than ranking tags against an initial query, another option is to cluster tags [ context. Many algorithms, trying to cluster keywords alone, without clustering documents [ do not take advantage of the duality between samples and features. For our particular setup where documents are succinctly described, it seems more relevant to use an algorithm clustering samples and features at the same time to improve the clustering quality. Co-clustering approaches cluster both on rows and columns together. Many approaches focus on the bipartite graph representation [15,16,17] of keywords and documents. [ samples and features using their eigenvector representation. The work [ which given an initial partitioning alternates between row clusters reﬁnement and column clusters reﬁnement. There are two main problems in clustering: deﬁning the target number of clusters, and deﬁning the rules for cluster assignment. Concerning the cluster count, we propose a hierarchical agglomerative algorithm that stores the merging operation history. It free us from setting a priori a speciﬁc number of clusters. Nonetheless, we suggest a stopping criterion to select an optimal partitioning. We follow the probabilistic and information theory approaches to cluster tags with limited information available. We follow a co-clustering approach to take advantage of the synergies between documents and keywords clustering. In this paper, we describe in the ﬁrst part, a procedure to enhance keywords and documents context without the use of an external database. Next, the algorithm and its cost function are detailed. The details about datasets and metrics used are presented in the experimental setup section, followed by the experimental results. Then, the article ends with a discussion and a conclusion. BeX = {x}the set ofndocuments and of documents. Using the vector space model, a document is represented under the form of a binary vector [x, ..., x]where a1encodes the keyword’s presence while a applies to keywords, represented asy= [y The collection of document vectors is often represented in a matrix form. The documents-keywords matrix is M ∈ {0, 1}, with M= x= y In practice, the documents-keywords matrix is very sparse as a document is assumed to be tagged with only a few relevant keywords. For a keyword, the sparsity leads to very few keyword co-occurrence pairs with very low weight. In our particular case, we assume that keywords are exact and relevant, accurately extracted by the algorithms or experts, and none due to mistakes. This assumption simpliﬁes the task of enhancing the keywords-documents matrix. Rather than searching for incorrect pairs ﬁrst for data cleaning, all the keywords pairs are taken into account to improve the keywords-document matrix. To improve the matrix, we take advantage of the co-occurrence matrices, one measuring the document’s similarity, the other measuring the keyword’s similarity. A cosine-like similarity is used to compute the similarity between pairs of documents and keywords, respectively, taking values in[0, 1]. Instead of running the cosine similarity on binary vector, the values are adjusted by the relative frequency. For documenti, each keyword is weighted by its frequency that frequent keywords count less than infrequent ones. The same normalization is performed on keyword vectors, normalized by the number of keywords within each document almost no effect, as the number of tags per documents is relatively homogeneous, but affect a lot keyword normalization, as they follow a power-law distribution. , ..., y], where a1encodes the occurrence of keywordjin a document. , where rows represent documents and columns the keywords. The document similarity is deﬁned as: The equation 1 is adapted to computeS and normalized by the number of keywords describing the document document frequency used in TF-IDF to score keywords by relevance, enabling to focus on singular keywords rather than frequent one. The similarity matrix represents on a scale from by a value close to1while unrelatedness by context. In some way, the similarity matrix represents the co-occurrence laws. To exploit these relationships, we suggest transforming the similarity matrices into the form of Markov transition probability matrices, whereT= P r(j → i) betweenTandM, we would obtain a vector with weights representing the probability of obtaining a given keyword given the initial tags. A simple normalization ofSis not enough to obtain item to its neighbors is preserved after transformation, ie fairly attributed. Highly frequent keywords are more receptive than infrequent ones, as frequent items. To rebalance the mass attribution, the matrix j) = P r(j → i). We use the Sinkhorn-Knopp algorithm [ normalization vectorcand the best row normalization vector (Sc). The transition matrix is obtained by: where D(z) is the diagonal matrix with element D The two transition matrices are used to smooth the initial documents-keywords matrix using: The detail of a term of Mobtained following 3 is: The termMof 4 corresponds to all possible transitions to a document keyword pairs M. The application ofTpreserves the mass on the rows whileP only the global mass is preservedM This transformation redistributes the weights for documents and keywords without changing the global mass of the system. The matrix Mwill be used for the co-clustering instead of the binary matrix M. An agglomerative clustering algorithm iteratively aggregates items from The algorithm starts with an initial partitioning where each item of Tagged Documents Co-Clustering by making the sum over the rows rather than the columns (i.e.,P), 0to1how well two elements are related. The closeness is characterized 0. Similar keywords can be either synonymous or occurring in the same is the probability to move to statejstarting fromi. Using the product Tis obtained by bi-stochastization, leading toT = T, ieP r(i → =P r(x|x)P r(y|y)M(4) {x}}. The clusters to merge are selected according to a cost function pairs. The pair(c, c)with the lowest cost are merged together to form a new cluster process take n − 1 steps for the rows, where n = |X|. The cost function affects the algorithm outcome [ assumption over the cluster shape. For instance, single linkage focusses on merging clusters with the smallest gapP C(c, c) =mind(x, x), without taking into account the cluster mass, while complete linkage focusses on merging clusters with the lowest maximal distance different behaviors: single linkage is sensitive to noise, as it would create artiﬁcial bridges between clusters, while complete linkage is sensitive to outliers, preventing the merge of clusters containing some of them. On the vector space model, the use of distance measures is not satisfactory [ too short to get an accurate representation. Instead of distance, the dissimilarity between clusters is measured using the divergence between their probability distribution. A partitioning with one large cluster and many singleton clusters made of outliers is similar to a ﬁltering algorithm. A clustering with such an outcome is not desirable as no true group exists. A partition must be composed of clusters with equivalent size, without high disparity. Some algorithms naturally take into account the cluster size. For example, in the case of complete linkage, where the larger a cluster becomes, the harder it is to merge as the maximal distance to other clusters tends to grow. When using a cluster probability distribution, all items within the clusters are represented by a single prototype independent of the cluster size. To remediate to the fact that cluster prototypes do not include the knowledge of their size, we deﬁne our agglomerative algorithm cost as the product between the cluster prototype divergence and the cost relative to their size: whereD(.)is the divergence part, while are similar across clusters. These two parts will be deﬁned in the following. For simplicity, the features are relatively deﬁned to the samples considered. When looking at rows, the features represented by columns, while when looking at columns, the relative features correspond to the rows. The Shannon entropy is a way to measure the number of bits required on average to code an information. The more bits are needed, the more information would transit. For a random variable with discrete values entropy is deﬁned as: with the log corresponding to the base 2 logarithm log We state that a partitioningCis informative if items are distributed into clusters of equivalent size, maximizing the entropyH(C). Here, the probabilities associated to each cluster is not related to the item frequency, ieP P r(X = x). Otherwise, the goal would be to isolate highly frequent keywords into individual clusters and gather infrequent keywords on a large cluster, which is by no means more interesting than clustering infrequent keywords alone. Therefore, the cluster contribution is proportional to the number of items To make the distinction with entropy using item distribution, we refer to the entropy using item count as the partition entropy. For a partitioning intokclusters,H(C) partitioning with the largest entropy is the one with a single item in each cluster. To compare fairly two partitions at different stages of the agglomerative process regardless the number of clusters, the partition entropy is normalized by its theoretical maximum: Merge(.)corresponds to the size part; this ensures that quality and quantity X = {x}and associated probabilitiesP r(X = x) = p, the Shannon H(X) = −plog p(6) is maximal forP r(C = c) =∀iwith a maximal value oflog k. The H(C) =H(C)log |C|(7) which is deﬁned for any partitioning with at least convenient state comparison regardless the number of partitions. At the start, each item forms its own cluster, leading to a various sizes which affects the entropy’s quality. To keep this value maximal, we study the entropy variation following a merge. For two clusterscandcmerged together, with probability the previous term: whereCcorresponds to the new partition with The total entropy is improved by a factor clusters contribution∆(p, p; k), the behavior can be estimated for large holds. The merge impact is equivalent to fis negative, concave and monotonically decreasing over the interval to the limit of what could be considered as small clusters, which is respected for many partitions as a consequence,f(p) + f(p) < f (p+ p For largek,∆(p, p) < 0. To study the size inﬂuence, two merges are compared: the merge of probabilitiespand, and the merge of be shown that∆(p, p) = α∆(p, p). The cost increases with the size of clusters merged. To maximize the relative partitioning entropy cost over the agglomerative process, small clusters must be preferentially merged to limit the loss, which can be compensated by the former term H For any pair of clusters(c, c), the term Two different merges are distinguished by the value of ∆(p We discussed about cluster size in the previous paragraphs. The following describes the evaluation of clusters’ content similarity. Given a cluster c∈ Cand a partitioning C and for the distribution of cluster cover C For two clustersc, c∈ C, the probability distribution over symbols, such asa= P r(c|c)and 2clusters. The relative entropy takes values in[0, 1], which allows = H(C)+ ∆(p, p; k) k − 1clusters andCthe previous partition withcandcunmerged. ∆(p, p; k) ≈(f(p) + f(p) − f(p+ p))wheref(x) = x log x. )which leads to an entropy decrease, compensated to some extend by cwithcwith respective probabilitiesαpandαp, whereα ∈ R. It can H(C)in 8 is the same regardless of which clusters are merged. Merge(c, c; C) = −∆(p, p b= P r(c|c). The Kullback-Leibler (KL) divergence is a way to measure the distance between probability distributions A and B: The same equation is obtained forc, c divergence is that mass of distributionBmust be present where be rewritten asKL(AkB) = H(A, B) − H(A) While the entropyH(A)corresponds to the average number of bits exchanged to communicate symbols of most optimal code, the cross-entropyH given the optimal code to transmitB. If the two clusters share the same distribution, the code is likely to be similar, and the associated cost low. There is no order when merging two clusters, as symmetric. Instead, we use the J-symmetrized KL divergence, which is deﬁned as: with the balance factorα =. The symmetrization ensures that both which leads to a more discriminative function as both a The cost term takes into account the clusters similarity in eq. 5 is between prototypes’ distribution according to the clustered features. The KL represents a high content similarity between considered clusters. Given the initial smoothed documents-keywords matrix pairs of clusters inCand pairs inC This operation is computationally expensive, as it requires O(nm(m + n)) operations. The merge cost is recomputed at each round, and the total cost is computed for each pair. The pair of clusters from CorCwith the lowest cost is selected and merged. The two divergence matrices are updated after the merge operation. If two clustersc, c∈ Care merged together, all the pairs involving recomputed with the new cluster characteristics requiresO(n(t)m(t))operations, where left after t merge operations. ConcerningKL(C), all the items are affected. Nonetheless, the matrix can easily be updated, by looking at the difference between merged and unmerged state. For two column clusters A and B respectively, the cost variation is: where∆KL(AkB)corresponds to the non-symmetric byKL(c, c) + ∆KL(AkB) + ∆ the feature pairs. In total, a step requiresO(n(t) operations to update the cost pairsKL two-column clusters by exchanging n(t) with m(t) in the formula. KL(AkB) =alogab(12) ∈ C, witha= P r(c|c)in this case. The intuition of theKL whereH(A, B)is the cross-entropy, andH(A)the regular entropy. (A, B)corresponds to the average number of bits exchanged to transmitA c∪ c= c∪ c. However, the Kullback-Leibler divergence is not . This lead to two initial divergence matricesKL(C)andKL(C). c∪ c, leading to a matrix with one dimension less. This ﬁrst update n(t) = |C|andm(t) = |C|is the number of row and column clusters KL(AkB) = (a+ a) log KL(AkB). This updating step requiresm(t)operations to update all + m(t))operations to select the best pair, andO(n(t)m(t) + m(t)) when merging two row clusters. The same reasoning applies when merging The main motivation for tag co-clustering arose from scientiﬁc papers literature. The DBLP dataset [ gathering computer science papers, with meta-data such as title, publication year, references, authors, conference/journal, ﬁeld of study, available for a large number of papers. We used the most recent version (v12) for our experiments. The ﬁeld of study is a list of descriptive keywords about the ﬁeld (e.g. Cryptography, Biology), the method (Matrix factorization), or other related concepts (Bullwhip effect) discussed in a paper. Each paper has, on average tags. Hopefully, tags are already well pre-processed, and no steaming nor stop-words removal need to be done. The algorithm complexity is more than quadratic, which prevents the scaling to a large database. Documents are sampled to make the clustering possible on a regular machine. A particular tag is selected, and all papers with the tag included are gathered. Then,5000documents are selected at random from this pre-selection. All keywords with less than5occurrences are discarded, which leads to around documents-keywords matrix. The real-world dataset does not contain any label, which prevents the evaluation with objective metrics. We propose to generate a sparse synthetic dataset with clustering structures to test the performance of our model. A synthetic sparse matrixMof sizen× n Rows are split intokclusters of equal size clusters. The matrixMis ﬁlled with0and1according to the partitioning. The tile intersection between the row and column clusters c Some of the tiles selected with probability ﬁlling rate. For a tileT (a, b)to ﬁll, the ﬁlling rate rate. For each item(i, j)in tile(a, b)to be ﬁlled, its value is with rate. In our case, the global and local ﬁlling rate are set to dataset of2%. For all experiments,n resulting matrix looks like a regular grid and would be called, for this reason, the checkerboard dataset (see Fig. 1). The experiments are done for the samenand X and Y together, but the performances are not affected by asymmetric choices. To evaluate our algorithm, we selected some supervised and unsupervised metrics to evaluate the quality of the partition recovery and estimate cluster quality in the absence of labels. TheV-measure is a supervised metric comparing the real clusters to the estimated ones using entropy measures. It is analogous to accuracy on classiﬁcation problems. Two sub-measures are ﬁrst computed: the homogeneity, which corresponds to the fact that a good cluster contains a single class, and the completeness, which measures how well elements from a given class are grouped. The homogeneity is deﬁned as: withLthe real cluster labels andKthe hypothetic labels obtained using a clustering. The completeness is deﬁned similarly as: Tagged Documents Co-Clustering partitioned overXandYdimensions is constructed the following way. bc + {0, 1}. The same regular partitioning is performed onYwithk α ∈ [0, 1]are ﬁlled, leaving(1 − α)of the tiles empty, whereαis the global = n= 1000andk= kwould be adjusted over the experiments. The n, and identicalkandk. This choice is made to aggregate results over h = 1 −H(L|K)H(L)(15) c = 1 −H(K|L)H(K)(16) The V -measure is then deﬁned as: where the parameterβ ∈ Rbalances the contribution of each term. For lim= c. The values obtained lie within case. TheV-measure can also be used to compare two partitioning obtained with different parameters or algorithms, measuring the similarity degree. Random GuessSupposeCis a partitioning of items into partitioning entropy isH(C) = log k. Be ﬁlled with items selected at random. The overlap probability between Consequently, the joint entropy isH(C, C and homogeneity are both equal to 0, leading to an undeﬁned converges to zero. Compared to accuracy measure, where a random guess’s accuracy is discriminative. At the start, the partitioning entropy deﬁned in eq. 7 is maximal. However, the clustering is non-informative as only singleton clusters exist. Instead, knowing that ﬁnal clusters would have a critical size with more than clusters’ contribution can be discarded, considering them as outliers. The restricted relative partitioning entropy is then deﬁned as: At initialization, the value is0as no cluster of sufﬁcient size exists for and enables to track clusters creation. This measure allows to evaluate the partitioning distribution without considering cluster content. When monitoring the cluster’s content, the information variations are monitored. In this case, the entropy is computed using the sample probabilities, deﬁned in equations 10 and 11. The mutual information corresponds to the information shared between X and Y . This measure is deﬁned as: It corresponds to the gain of codingXwith bounded by0(independent variables) and information between partitioningI(C decrease due to information loss during the compression. We proposed to compare our algorithm to the co-clustering algorithm presented in [ decomposition. First, two diagonal matrices D=PM. Then, the normalized matrix decomposition such asM= USV. The vectors algorithm ﬁnishes by performing ak-means clustering on the dimension. V =(1 + β) × hcβh + c(17) [0, 1], where1is attributed to the best clustering, while0to the worse Ca randomly guessed partitioning withkclusters of equal size too, but ) = log k= 2 log k, leading toH(C|C) = H(C| C) = log k. Completeness (C; r) =log |C|(18) Y, i.e. the amount of redundancy between the two variables. This value is min(H(X), H(Y ))(correlated variables). We would monitor the mutual , C). As the number of clusters decrease over time, this value would Figure 1: Matrix smoothing forn= n ﬁlling ratio is α = 0.2 and the local ﬁlling ratio is β = 0.2. Left: binary matrix, right: smoothed matrix. The smoothing proposed enables to switch from binary values to real values by redistributing the weights. A visual example is presented in Fig 1. The ﬁlled tiles are identiﬁable on the binary matrix. For the tiles with low boundaries are hard to identify. On the smoothed matrix, the weights are completely redistributed leading to the visual identiﬁcation of tiles, even the ones that were not ﬁlled at all. All items belonging to the same clusters tend to have a more similar feature vector. The information is of lower intensity locally but is better distributed across the different features, even on tiles that were not ﬁlled. This experiment compares the composite cost deﬁned in eq. 5 to the version where only content similarity obtained usingKLcost is taken into account. For this purpose, we compared for several numbers of cluster V-measure averaged over5independent trials for each value of V . We did the same by extracting the maximal restricted relative entropy, removing clusters of size smaller or equal to and extracting the correspondingˆk. The results are presented in Fig. 2, with the composite cost denoted and the simple cost KL. As a general remark, for all setups, it is easier to cluster many clusters with sufﬁcient size. For very few clusters, the accuracy quickly decreases. As the global ﬁlling rate is randomness, a single one or none could be ﬁlled, leading to less information for clustering. For a large number of partition, a cluster is well deﬁned in the sense that enough tile are ﬁlled. The problem of insufﬁcient information occurs at the sample level. For a row sample of size the local ﬁlling rate isβ≤ 0.2, the probability that none of the slot are ﬁlled grows with less disturbing than the former as redundancy exists. TheV-measure of the composite cost is always higher than for the simple cost setup. The accuracy of drops for largekwith smaller cluster sizes. The absolute from the optimal, with a consistent error unless for very small cluster size, while the error for the simple cost is too large to ﬁt in the ﬁgure. Looking at the maximal relative partitioning entropy in Fig. 3, the composite cost leads to very = 1000andk= k= 15, ie around67points per cluster. The global nandkfeature partitioning, there areslot for a given feature cluster. As Figure 2: Bestv-measure and average absolute error for corresponding with co-clustering and weighting according to correspond to the co-clustering setup without taking into account cluster distribution. Red squares correspond to the double weighting approach but rows and columns are clustered independently. Each point corresponds to the average for 5 independent trials. Figure 3: Left: BestH(C)with cluster of size lower or equal to correspondingˆk. Same color code and setup as 2. Blue rounds and red circles overlap on the left sub-ﬁgure. good cluster distributions for anyk, while the simple cost is low for larger values. On the right side of Fig. 3, the even for largek. This means that the number of clusters obtained when stopping the agglomeration procedure with the partitioning entropy criterion is close from the true initial cluster numbers for all cluster sizes. When using the simple cost, clusters number is far from the true number of cluster. In both case, the entropy stopping criterion leads to a smaller error over the number of estimated clusters. One of the initial hypothesis concerns the synergy between joint reduction. We compare the co-clustering setup to the independent setup, where the partitioning partitioning Cis obtained using the unaggregated rows X. This setup is denoted Indep in Fig. 2 and 3. The results obtained with the independent clustering are similar to the co-clustering but with a lower large number of clustersk. The obtained concerned, independent clustering performs as well as the co-clustering, and the co-clustering advantage is limited for large clusters / small with smaller size clusters for large k. KLcost and cluster distribution entropyH(C). Orange triangles Cis obtained using the uncompressed featuresY, as well as the ˆkfrom thev-measure are close to the co-clustering ones. As far as shape is Figure 4: Comparative results between Spectral co-clustering and Agglomerative clustering. On the right, maximal V -measure obtained for We compare our algorithm to spectral co-clustering presented in [ clusters to search for. When comparing the agglomerative algorithm with the spectral algorithm, the spectral algorithm is run with the exactkprovided, compared to the partitioning obtained with the agglomerative algorithm with remaining clusters. TheV-measure and the relative cluster partitioning entropy partitioning. The results are presented in Fig. 4. The spectral algorithm results are lower than the one obtained for the agglomerative approach. However, the shape of the clusters obtained are equivalent. The spectral approach is quite robust in general, but the sparsity level affects the results. With a higher ﬁlling rate ( closer to the agglomerative one. We also tested with DBSCAN, which has been used in some papers. As it is impossible to select the wished number of clusters, and because the results were lower than spectral decomposition, the results are not presented. Nonetheless, the smoothed matrix’s use improved the partitioning, allowing the algorithm to discover more clusters than with the regular binary matrix. The initial goal was to cluster tags associated with scientiﬁc papers to identify topics. In the dataset used, there is no high-level classiﬁcation or paper grouping to evaluate our clustering. Despite the lack of objectivity, we present the results on two subsets of papers, obtained for the Payment ﬁeld of study, and the second for Biometry. We take advantage of the hierarchical form to present the results using a dendrogram. Around unmerged, and the three most frequent keywords are displayed for analysis. For the two, the relative partitioning entropy was around H(C) = 0.95 for keywords. PaymentFig. 5 corresponds to the clustering of keywords co-occurring with the Payment tag. Three high level clusters are identiﬁed. The one on the left corresponds to things related to economics. The right one corresponds to what could be considered as the core of the payment ﬁeld, oriented toward users, with the new payment methods (Cryptocurrency, Mobile payment) and intricated topics (Computer security, Marketing and Advertising). The bottom cluster corresponds to the medium or technology used in the payment but is not speciﬁc. For instance, Artiﬁcial intelligence is used in payment systems for fraud detection or biometric authentication, but it is not speciﬁc to payment. Surprisingly, the keyword Payment is located on the bottom cluster, near Cash and Crowdsourcing, which seems conceptually incorrect. This is due partially to our sampling method, where all documents with keywords Payment were selected. As it co-occurs with all keywords, there is no way to identify true relationships. Payment is located on a cluster were the other keywords are related to Crowds, with additional keywords such as Reputation, Social network, Audit and Crowdsensing. This artefact is not limited to the selected keywords but to the most frequent keywords. A second example is Computer science on the right, in a cluster related to the Internet, with additional keywords like the World Wide Web, Mobile device, Service provider and Mobile computing. BiometryThe second partitioning uses the Biometrics tag as a reference. The resulting dendrogram is presented in Fig. 6. Two large clusters are identiﬁed. The main on the left gathered keywords about biometric methods and algorithms to extract a digital identity. It is subdivided into two subgroups. The top one gathered keywords related to computer-vision, with Image processing, gait and face analysis. The bottom one corresponds to the other methods, with ﬁngerprint, Speaker recognition/veriﬁcation. The cluster with Biometrics, Computer science and Speech recognition corresponds to an artefact gathering highly frequent keywords together. The right cluster corresponds to the security part, with Cryptography, Password, Authentication and others. The checkerboard experiments were evaluated, knowing the number of clusters. In an unsupervised setup, this knowledge is often unavailable. To select the cluster number, one has to look at a speciﬁc criterion indicating if the partitioning is satisfying. For instance, the algorithm successively splits the existing clusters. The splitting decision is based on the split’s likelihood, assuming the data corresponds to a Gaussian mixture. For more general clustering algorithm, the silhouette coefﬁcient, measuring the distance to the nearest cluster versus the radius of the cluster. On our type of data, the silhouette is not suitable as cluster are not well separated. The goodness criterion of the algorithm must be in accordance to the goal achieved by the algorithm. Reminding our cost deﬁnition in 5, it is the product between cluster size and content related costs. The ﬁrst part of the answer to this problem is to look at the restricted relative partitioning entropy deﬁned in 18, with small clusters of size1or less removed. In Fig. 3, aggregated. However, this is a particular case where all clusters have the same size, leading to a particular conﬁguration whereHis maximal. In a more general conﬁguration, there is no particular reason for clusters of exactly the same size. The second cost part takes into account content. On the information theory-based work of [ deﬁned as minimizing the quantityI(X, Y ) − I(C agglomeration of clusters is a form of compression which mechanically reduces the information available. The restricted relative entropy is maximal towards the end of the agglomeration process, while the information is maximal at the beginning and minimal at the end. A good compromise between the two is to look for the value for which the product of I(C, C) and H(C X’s best partitioning is not necessarily simultaneous with that of different. The co-clustering only exploit synergies to ﬁnd clusters more accurately. In general, the partitioning with the lowest number of dimensions would be merged more frequently until reaching a size comparable to its feature size. As a rough guide, forkfeatures, the maximum entropy is higher the number of features, the higher the cost will be because the probability of conﬁguration. With a higher cost, the clustering will preferably select the cluster pairs with the smallest number of features. This criterion was tested on the checkerboard dataset. The estimated average absolute deviation close to1. This criterion was used to build the dendrogram, where the estimated cluster number was between 15 ∼ 20 clusters depending on the main selected keyword. The checkerboard model differs from a documents-keywords matrix obtained from tag on two majors points. The ﬁrst difference concerns distribution. Tags follow a Zipf’s law, which is not modeled here, as all columns have on average the same strength. Nonetheless, the unbalanced distribution is corrected by the matrix smoothing protocol, which decreases the weight of these frequent keywords to less frequent one. The second difference is the hierarchical division of keywords. The associated tags range from very broad domains (Chemistry, Mathematics), to ﬁelds (Inorganic chemistry, Databases), to specialities and other lower levels. The checkerboard model is made of independent clusters which are not hierarchicaly organized. Nonetheless, due to the H(C; 2)is already a good indicator of when clusters are sufﬁciently ) or H(C) is maximal: scaling limitation of the proposed algorithm, the restriction to around visibility of such organizations. In this paper, we addressed the problem of tag clustering, where the tag amount per document is limited. To this purpose, using the correlation between tags and keywords, a method to enhance context without the use of an external database or model was proposed. With the assumption that a clustering is informative if the partition entropy is large, we proposed an agglomerative co-clustering algorithm taking into account the content as well as the cluster size. The algorithm showed good recovery performance on synthetic datasets with the same sparsity level. It showed conceptually correct clustering results on scientiﬁc paper tags, up to highly frequent keywords where no discriminative relationship could be found. The algorithm’s complexity is polynomial but more than quadratic, which restricts its usage on a small dataset. Some improvement can be made by splitting the dataset into independent parts or ﬁnding cost approximations. Nonetheless, the idea of building groups of equivalent size could be mixed with other agglomerative measures to include distant items to their closest cluster.