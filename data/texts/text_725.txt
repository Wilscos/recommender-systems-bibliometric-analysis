In many multi-agent systems, agents often prepare their actions in advance before a deadline when their payoffs are realized. Such systems can be captured by revision games Kamada and Kandori [2020a]. In a revision game, at the initial time, agents choose actions and wait for the deadline. Between the initial time and the deadline, they may have opportunities to revise actions. The revision opportunities arrive stochastically and the probability to get another revision opportunity vanishes as time approaches the deadline. A player’s only payoff is obtained at the deadline, depending on all agents’ ﬁnal actions. in stock markets such as Nasdaq or Euronext. Traders can submit orders before the opening of the market, which can be changed until the opening time. The online auction websites like eBay hold auctions which have a deadline and bidders actually play a revision game. The eBay auctions usually have a deadline, before which bidders can revise their bids many times. Bidders’ opportunities to use eBay and to change bids are following a stochastic process (many human activities can be characterized by a Poisson process). Electoral campaign is also a good example. During the 2020 U.S. presidential election, the COVID−19 pandemic brings about a series of political agenda shifts and campaign tactic adjustments for the major presidential candidates. AI game playing problems can also be modeled as revision games. Many recent multiplayer video games (such as Need for Speed series and Call of Duty series) involve some form of time limit on their online servers. Whichever team/player holds the lead at the expiration of the time limit is declared the winner. Another example is federated learning. Large companies may ask individuals to submit their data to train a model before some ﬁxed deadline. The relation between different individuals can be modeled as a Prisoner’s Dilemma. Moreover, Revision game is a very new model formulating the situation where players can prepare and revise their actions in advance before a deadline when payoffs are realized. We identify the Limited Retaliation (LR) strategy for revision games which sustains a high level of mutual cooperation and is robust to players’ occasional mistakes. The LR strategy stipulates that, (i) players ﬁrst follow a recommended cooperative plan; (ii) if anyone deviates from the plan, the LR player retaliates by using the defection action for a limited duration; (iii) after the retaliation, the LR player returns to the cooperative plan. The LR strategy has two good features. First, it is vengeful, in the sense that it deters the opponent from non-cooperative action by threatening a retaliation. Second, it is forgiving, because it returns to cooperation after a proper retaliation. The vengeful feature makes it constitute a subgame perfect equilibrium, while the forgiving feature makes it tolerate occasional mistakes. These are in clear contrast to the existing strategies for revision games which all assume players are extremely grim and never forgive. Besides its contribution as a new robust and welfare-optimizing equilibrium strategy, our results about LR strategy can also be used to explain how easy cooperation can happen, and why forgiveness emerges in real-world multi-agent interactions. Many real-world scenarios can be modeled as revision games. A well-studied case is the pre-opening phase crowdsourcing competitions or contests on Github or TopCoder usually have a deadline; participants modify their submissions many times before the deadline. original idea can be traced back to the authors’ online working draft before 2014. They identify the very ﬁrst strategy for revision games, which is based on the grim trigger mechanism, and constitutes equilibrium. Following Kamada and Kanrodi, all the existing strategies of revision games follow a grim trigger style. Strategies of this style has a very ﬁerce punishment and is never forgiving; it only proves cooperation is possible but does not answer how easy it is in revision games. It also excludes the possibility of resuming cooperation. A few works study the grim trigger equilibrium in games where the revision process is player-speciﬁc Calcagno et al. [2014], Kamada and Sugaya [2014], Gensbittel et al. [2018]. A later work examines the trembling hand equilibrium for revision games with imperfect information Moroni [2015]. Another work goes even further and extends the solution concept to Markov perfect equilibrium Lovo and Tomala [2015]. Besides these theoretical achievements, researches also investigate different applications Ambrus and Lu [2015], Kamada and Kandori [2020a] and even conduct laboratory experiments for revision games Roy [2017]. How to sustain cooperation is a central challenge in game theory. In conventional games such as prisoner’s dilemma and the tragedy of the commons game, the only Nash equilibrium strategy is non-cooperative. But why cooperation is commonly seen in the real world? One reason is real-world games are more complex than theoretic models. Real-world games could be dynamic or repeated; players may convey information to one another, etc. Our problem is about how to sustain mutual cooperation and how to improve the social welfare in revision games. Revision game are dynamic. It lasts for a certain duration (i.e., time interval [−T, 0]). When players are engaged in a revision game, mutual cooperation could be achieved due to the fact that agents must consider not only the immediate best response but also the long-term expected payoffs. Thus one player has the ability to deter the opponent from non-cooperative immediate action by threatening retaliation that reduces the opponent’s expected payoff. nents: One is the recommended collusive plan which achieves an optimal social welfare for players; The other is the retaliation phase in which the defection action is used for a limited period. The LR strategy stipulates that, if anyone fails to follow the collusive plan, players switch to the retaliation phase. LR strategies work in a “carrot-and-stick” style, where the collusive plan is the carrot and retaliation phase is the stick. Speciﬁcally, our contributions are: 1. We derive a necessary and sufﬁcient condition for a collusive plan to be sustained in revision games. The 2. By satisfying the incentive constraint using backward induction, we ﬁnd a class of collusive plans for 3. The retaliation phase of LR strategies is designed to have a limited and controllable ﬁerceness. It not only 4. We also implement the optimal LR strategies in both the revision versions of Prisoner’s Dilemma and non-cooperative action at any time by threatening a sufﬁcient retaliation that reduces the opponent’s expected payoff in the remaining time. By using this vengeful feature, LR strategies can constitute cooperative subgame perfect equilibrium. The other feature is they are forgiving. Since the retaliation phase has a controllable The general framework of revision games is formally published in Kamada and Kandori [2020b]. But the We identify the Limited Retaliation (LR) strategies for revision games. LR strategies consist of two compoconditions are captured by an incentive constraint which is the difference between the gain for current deviation and the loss for future retaliation. LR strategies. The collusive plans are given in a simple piecewise constant (PWC) form and are easy to implement. We also obtain the social welfare maximizing PWC plans. can exert enough threat on the opponent to ensure him not to deviate, but also allows players to forgive the deviator once his deviation has been sufﬁciently retaliated. Thus players can resume collusion after this retaliation phase. the Cournot games. Empirical results show that LR strategies show a signiﬁcant robustness than grim trigger strategies. The LR strategies have two good features. The ﬁrst one is they are vengeful. It can deter the opponent from length, even when there is an intentional or unintentional deviation, LR players can return back to mutual cooperation after the retaliation. This forgiving feature makes LR strategies especially suitable for real-world games where players confront observation or action errors Gilpin and Sandholm [2007], Fundenberg and Maskin [1990], or where players have limited rationality Doyle [1992]. LR strategies are in contrast to existing strategies for revision games which assumes players are extremely grim and never forgive. The forgiveness in LR strategies is of great importance. This is because in realistic games, due to agents’ observation/action errors or bounded rationality, deviation occasionally happens either intentionally or unintentionally. If players play a non-forgiving strategy, they can never return back to mutual cooperation and the social welfare would be signiﬁcantly harmed. Moreover, forgiveness widely exists in the real world, our LR strategy is the ﬁrst attempt to explain the incentive and mechanism for cooperation and forgiveness in multi-agent games with deadline. Consider two-player symmetric games with the following properties: Such games widely exist. For example, the well-known Prisoner’s Dilemma with continuous actions and Cournot duopoly both have these features. When such a game happens only once, we call it the stage game. ends at a deadline time 0. At time −T , players choose their actions simultaneously. At any time −t ∈ [−T, 0], they may have revision opportunities to play the stage game again, and the opportunities arrive according to a Poisson process with arrival rate λ > 0. They can revise their actions only when they have a revision opportunity. Players’ payoffs π of players’ revised actions at the ﬁnal revision opportunity. After the ﬁnal revision opportunity, their actions stay the same till the deadline. as a payoff-relevant state variable for the players. A player’s behavior in the revision game is captured by a mapping σ(t) : [−T, 0] → A, which speciﬁes an action a for every remaining time length t. Note that when there is a revision opportunity at time point −t, a player may or may not adjust her action a. We formally call this mapping σ(t) the strategy of a player. A strategy decides an action a for each time point −t. We say that a is cooperative or a sustains a certain level of cooperation, if it provides a higher payoff than the Nash equilibrium payoff of the stage game. That is, when both players use a, then π(a) = π see there are different levels of cooperation. remaining time length t and λe payoff to each player associated with strategy σ(t) over time span (−T, 0] can be written as: Note that π is that, later we will analyze symmetric equilibria. Eq.(1) is constructed as follows: σ(T ) speciﬁes a player’s initial action at time −T . Payoff π during time span (−T, 0]. This happens with a probability e −t, then player’s payoff is π expected payoff when the last revision opportunity arrives in time span (−T, 0]. • There exists a unique pure symmetric Nash equilibrium action proﬁle (a, a), where both players use the defection action, and their payoffs π:= π(a, a); There is also an optimal symmetric action proﬁle (a, a) and a= arg maxπ(a, a); aand aare distinct. The action space A is a convex subset (interval) in R. • Assume player i’s payoff πis continuous. Denote the payoff for a symmetric action proﬁle (a, a) as π(a). Assume it is strictly increasing for a ∈ [a, a] and non-decreasing for a > a, if a< a (symmetric conditions hold if a> a). A revision game is an extension of the above stage game. It starts at a beginning time −T (T > 0) and In Revision games, the value t is the remaining time length until the deadline, which can be regarded Let λ be the arrival rate of a Poisson process. Then eis the probability of no Poisson arrival in the σ(t)is the payoff for a symmetric action proﬁle. The reason of using symmetric action proﬁles In this section, we propose Limited Retaliation (LR) strategies which can constitute a subgame perfect equilibrium. LR strategies have a simple form and achieve a high level of cooperation. More importantly, they allow both players to resume collusion even after some of them deviated. The principle for designing LR strategies is: Firstly, it should recommend a collusive plan. If both players follow this plan, they will have high expected payoffs; Secondly, to ensure players not to deviate from the collusive plan, the strategy should also provide a proper retaliation which is used to threaten the opponent not to deviate; Finally, if a retaliation is too soft, it cannot have enough threatening; if it is too strong, it will be hard to return to collusion. Thus the retaliation should be limited to a proper degree. Deﬁnition 1 (LR Strategy). A Limited Retaliation strategy is a tuple σ = {x, k}. Function x is a recommended collusive plan specifying actions a = x(t) for different −t. Constant k characterizes the length of the retaliation phase, in which players use the defection action a action a players to avoid taking this defection action a is deﬁned as follows. Deﬁnition 2 (Collusive Plan). A plan x(·) recommended by the LR strategy is called a collusive plan, iff ∀ − t, x(t) ≥ a This objective is guaranteed by threatening the opponent by a k-degree retaliation. An LR strategy runs as follows: x (i.e., players’ strategies are identical to the collusive plan σ = x). This indicates that with LR strategies, cooperation can emerge. If there is any deviation from any player, then the retaliation phase is triggered. This is a threat to the opponent not to deviate, indicating that cooperation can be secured. LR strategies are “forgiving” and the ﬁerceness of retaliation is controlled. After the limited retaliation, an LR strategy returns to cooperation. This indicates cooperation can be restored. The LR strategy can be computed by Algorithm 1 in the Appendix-A. Now the problems are: how to ﬁnd x(·) and k such that a pair of σ = {x(·), k} can constitute an equilibrium, and to what extent they can sustain cooperation? To tackle these problems, in this subsection we will analyze players’ incentives for LR strategies. First deﬁne the following terminologies. Deﬁnition 3 (Deviation Gain). For the stage games, denote i’s maximum deviation gain at a symmetric action proﬁle (a, a) by G(a) := max Deﬁnition 4 (Retaliation Loss). For the stage games, denote i’s retaliation loss at a symmetric action proﬁle (a, a) by L(a) := π We ﬁrst clarify what is a collusive plan. Recall that in stage games (e.g., Prisoner’s Dilemma), the defection leads to the lowest social welfare. A fundamental objective in revision games is to incentivize the Thus the intuition of an LR strategy in Deﬁnition 1 is: it wants both players to follow the collusive plan. • If both players follow the recommended collusive plan x at the previous revision opportunity, then still follow this plan at the next revision opportunity. • As long as any player deviates from x at any time point −t, then start the retaliation phase by using the defection action aat any future revision opportunities arriving during time span [−t, −t + kt], where k ∈ (0, 1). • After retaliation, revert to the collusive plan x no matter what the opponent did during [−t, −t + kt]. If the retaliation is never triggered, then both players perfectly follow the recommended collusive plan deviates to her payoff maximizing action arg max quantiﬁed in Deﬁnition 3. After i deviates, both i and j will switch to the defection action a suffer from a retaliation loss which is quantiﬁed in Deﬁnition 4. Without loss of generality, we assume G(a) is strictly increasing on a ∈ [a Assume there is a recommended collusive plan x for all players and the players are using an LR strategy σ = {x(·), k}. At the very beginning −T , all players use action x(T ). If a player deviates from x at time −t, then according to Deﬁnition 3, her gain from deviation will be G materialize if there is no revision opportunity in the remaining time t, which happens with probability e so the expected deviation gain at time −t is G(x(t))e kt. During the following duration kt, at any time −s, if there is any revision opportunity, both players will use the defection action a s arrives during time span [−t, −t + kt], the realized payoff is decreased from π(x(s)) to the Nash payoff expected retaliation loss is given as an integral the expected deviation gain should be not larger than the expected retaliation loss, which is formally written as the following incentive constraint: for each −t ∈ [−T, 0], The incentive constraint (2) is the necessary and sufﬁcient condition for a subgame perfect equilibrium: it shows at any time −t, a player cannot increase her payoff by deviating from the collusive plan now and possibly obtaining the Nash payoff in the subgames after the deviation. In the following sections, we will derive a branch of collusive plans x which have a simple form and satisfy this constraint. There could be inﬁnitely many collusive plan x which can satisfy Eq.(2). Our goal is to ﬁnd a function that is simple and achieves high degree of cooperation. In this section, we consider the monotone and piecewise constant (MPC) plan. To derive an MPC plan, we ﬁrst segment the whole time span (−T, 0] into small slots, and then assign a constant action for each slot. We calculate the constant actions by recurrence relation and mathematical induction. Finally, all these constant actions together compose an MPC plan x. as (−T, −κT ], (−κT, −κ MPC plan x is In any stage game at time −t, if j follows the action speciﬁed by collusive plan x(t) but i unilaterally Next we extend the deviation gain and the retaliation loss of stage games into the whole revision game. According to the deﬁnition of LR strategy, this deviation is followed by a mutual retaliation with duration . As explained above, the density of the last revision opportunity at time −s is λe, and therefore, theRR In a word, the condition for any player not to deviate from plan x is that at any time instance −t ∈ [−T, 0], First, divide the whole time span (−T, 0] into many small slots: (−T, −(1 − k)T, (−(1 − k)T, −(1 − , · · · ,− (1 − k)T, −(1 − k)T, · · · . For ease of notation, deﬁne κ = 1 − k, these slots are rewritten a. This length shrinks as n increases, meaning the actions change more frequently as the deadline approaches. We show an illustration of such an MPC plan in Figure 1. Figure 1: An MPC collusive plan x(t) for LR strategies in Prisoner’s Dilemma revision game. x(t) is characterized by a piecewise constant function, which is different from the plan for the existing grim trigger strategy. strategies. We will do this by induction. According to Eq.(3), if there is any deviation from the n-th action a it must be that the deviation happens at a time −t ∈ (−κ retaliation with duration kt. This indicates the ending time of retaliation −t + kt = −κt will surpass the ending time of a retaliation loss from −t to −κt, which is decomposed into two parts: π(a retaliation loss for [−t, −κ π(a expected retaliation loss for [−κ expected retaliation loss for deviating from collusive plan x at −t. The above incentive constraint Eq.(4) for action a Thus it should hold for both the lower-bound −t → −κ LHS is the expected deviation gain in this slot. On the RHS, 1 − e The next step is to derive the value of each constant agoverned by the incentive constraint Eq.(2) for LR Similar as Eq.(2), the LHS is the expected deviation gain for deviating at time −t. The RHS is the expected (i) From −t to −κT , the retaliation is still against the planned action aand the retaliation loss is L(a) = ) − π, which takes place with probability eat each time instance −s ∈ [−t, −κT ]. The expected (ii) From −κT to −t+kt, the planned action already becomes a, thus the retaliation loss is L(a) = ) − π, which takes place with probability eat each time instance −s ∈ [−κT , −κt]. The On the LHS, eis the probability that no revision opportunity arrives in the (n + 1)-th slot. Thus the revision opportunities in the (n + 1)-th slot. If there is a revision in this slot, retaliation will happen. Since the deviation time is −κ retaliation loss in the (n + 1)-th slot. ﬁnd the MPC plan is about how to choose the terminal term of a For the terminal action, consider when time approaches the deadline, i.e., n = c where c is a large enough number. We call (−κ Eq.(3), for the penultimate slot, the planned constant action is a Eq.(4). Substituting the lower-bound of the penultimate slot −κ This inequality is similar to Eq.(6). But the difference here is that on both sides, the variables are a feasible range of a Taking the recurrence relation and the terminal action together into consideration, we directly have the following theorem which is a major result of this paper. Theorem 1. A proﬁle of Limited Retaliation strategies σ = {x(·), k} is a subgame perfect equilibrium (SPE), (1) Plan x has the piecewise constant form as Eq.(3); (2) The actions given by x follow recurrence relation Eq.(6); (3) There exists an action given by x satisfying Eq.(8). and the initial term (terminal condition, in our case) are given, every member in the sequence can be generated inductively. This theorem can be used to construct subgame perfect equilibrium LR strategies. We call them SPE-LR strategies. satisfy, and it degenerates to a limit problem. To see this, divide both sides by e decreasing in c. To tackle this limit problem and also to make it convenient for algorithm design, we can use the approximate equilibrium solution. The constraint for the approximate SPE is where ∆ is a small positive constant, but c is not necessarily extremely large. Since the RHS is larger than that of Eq.(8), it is easier to be satisﬁed. This leads to Corollary 1. Corollary 1. A proﬁle of LR strategies strictly satisfying conditions (1) and (2) but approximately satisfying condition (3) in Theorem 1 is an approximate SPE. SPE. It is worth noting that a plan such that ∀t, x(t) = a strategy using this trivial plan can support SPE. However, this one cannot sustain any level of cooperation. For LR strategies to constitute a cooperative SPE, we have the following requirements. Based on the recurrence relation in Eq.(6), we can generate each ainductively. Now the only problem to This theorem is just from the principle of mathematical induction such that when the recurrence relation When time is extremely close to the deadline, the terminal condition Eq.(8) becomes more difﬁcult to With a given k, if a plan x can make a proﬁle of LR strategies constitute SPE, we say plan x supports Corollary 2. A collusive plan x in the form of Eq.(3) supports SPE, only if for all n, a Proof. Recall that Eq.(6) is the equilibrium constraint for the n-th slot. Divide both sides by e G(a L(a) < L(a Corollary 3. A collusive plan x in the form of Eq.(3) supports cooperative SPE, only if there exists a constant c, such that a Proof. Prove by contrapositive. Divide both sides of Eq.(6) by e 1. Let a relation, this further requires a satisfying Eq.(6) is that a also not cooperative. 3 show basic requirements for a collusive plan to be a good choice for LR strategies and for cooperative LR strategies, respectively. Any LR strategy satisfying Theorem 1 can guarantee cooperation and also can return to cooperation even after occasional deviation occurs. In the next section, we will quantify to what extent LR strategies can sustain cooperation. In this section, we will show that among all piecewise constant plans, those which are monotonically decreasing with time and have upperbound a on this principle, we devise an inductive optimization method for calculating the social welfare maximizing LR strategies. In what follows, we will show that MPC plans have the highest social welfare among all piecewise constant plans. The following analysis allows us to ignore any MPC plan x which contains action x(t) < a x(t) > a According Corollary 2, we know that if a plan x supports SPE, it must satisfy that ∀t, x(t) ≥ a on x, we construct an outer function ¯x as follows. Obviously, ¯x is still piecewise constant. Moreover, since it replaces all actions larger than a that ∀t, x(t) ∈ [a us to only concentrate on them. Lemma 1. For every piecewise constant plan x that supports SPE, there always exists a bounded piecewise constant plan ¯x which also supports SPE, and has higher social welfare than x. Proof. A player’s expected payoff during (−T, 0] is given by V (x) in Eq.(1). Given a piecewise constant plan x, we construct ¯x following Eq.(10). chooses the same or a better action at any time, according to the monotonicity of π(a), it must be that ∀t, π(¯x(t)) ≥ π(x(t)). The expected payoff V is adding up π(t) over all t. Therefore, V (¯x) ≥ V (x). The social welfare is the sum of all players’ expected payoffs, thus ¯x leads to a higher social welfare. ) ≤ L(a)e− 1. On the RHS, L(a) is monotonically increasing for a < a, and if a < a, ≥ afor any n ≥ 1. = a, then L(a) = 0. Then the inequality holds only when G(a) = 0. By the same recurrence In summary, Theorem 1 presents the condition for LR strategies to constitute SPE, while Corollaries 2 and , but only focus on bounded plans which have upper-bound action aand lower-bound action a. First, we prove V¯x≥ Vx. Recall aresults in the maximum stage game payoff. Since ¯x always x(t). Since G(a) is strictly increasing on [a L(a) is maximized by a that x(t) satisﬁes equilibrium constraint Eq.(2), where the LHS is decreasing as G decreases and the RHS is increasing as L increases. Replacing x by ¯x still satisﬁes Eq.(2), which means ¯x also supports SPE. among all piecewise constant plans. Based on this fact, we only need to concentrate on a special subset of bounded piecewise constant plans and optimize the player’s payoff on this set. The second step will further reﬁne the above result and show that, among all the bounded PWC plans, we can focus on the bounded monotone and piecewise constant (bounded MPC) plans. over (−T, 0]. Assume two numbers i < j. Assume in the action sequence of plan ¯x, a which is larger than some actions before it. This means x is not monotonically decreasing. Speciﬁcally, assume all actions after a and a (−κ follows. All actions in ˆx are from ¯x which is bounded, then ˆx is also bounded. Moreover, ˆx replaces all the ‘nonmonotone’ actions ˆx(t) for t ∈ (−κ restores the monotonicity. In Lemma 2 we use ¯x and ˆx to ﬁnd that, it sufﬁces to only consider the bounded MPC plans. Lemma 2. For every bounded piecewise constant plan ¯x which supports SPE, there always exits a bounded monotone piecewise constant (bounded MPC) plan ˆx, which also supports SPE and leads to higher social welfare than ¯x. Proof Sketch. Given a bounded piecewise constant plan ¯x, construct ˆx following Eq.(11). We need to prove that ˆx supports SPE and has higher expected payoff than ¯x. The whole time interval (−T, 0] can be segmented into four parts: (i) time before −κ retaliation loss for ¯x and ˆx. For a formal proof, please see Supplementary Information Part-III. possible. Theorem 2. Among all piecewise constant plans which support SPE, a bounded MPC plan leads to the maximum social welfare. wise constant plans. To maximize the social welfare, now we only need to search in the domain of all bounded MPC plans. The previous subsection shows the bounded MPC plans have the highest social welfare among all piecewise constant plans. In this subsection, we will ﬁnally ﬁnd the social welfare maximizing plan within this domain. Then, we prove that ¯x supports SPE. On the one hand, by Eq.(10), at any time t, the two actions ¯x(t) ≤ Lemma 1 shows that, in equilibrium, bounded piecewise constant plans have the highest social welfare Consider a bounded piecewise constant plan ¯x which supports SPE but is not monotonically decreasing are (−κT, −κT ] and (−κT, −κT ], respectively. Thus the time-span after aand before ais T, −κT ]. Based on the above introduction of the non-monotone x, we construct another plan ˆx as ; and (iv) time after −κ. In each segment, we can compare the expected deviation gain and expected With Lemma 1 and Lemma 2, we have the following result which can reduce the search space as much as Theorem 2 indicates that the bounded MPC plan following the form of Eq.(3) is optimal among all piece- From Theorem 1 and Theorem 2 we know that, given a ﬁxed constant number c, the collusive plan x = bounded and monotone. Thus the social welfare maximizing problem is formally represented as follows. This is a typical non-linear optimizations problem which has Karush-Kuhn-Tucker (KKT) optimality conditions. In general, the optimal solution can be obtained by the Lagrange multiplier method. This means, with a given c, an optimal plan among all pieceswise constant plans can be obtained by the Lagrange multiplayer method. From the analytical results in Theorem 2, the optimization space has been reduced to bounded MPC plans. Eq.(12) can be transformed into many sub-problems. To see this, let x series of sub-problems, respectively. We prove x Since a since L(a) is strictly increasing, we have . This means, another plan ¯x = {a V (¯x) > V (x problems for each a where a done inductively. For the ultimate slot n = c, the optimal action a satisfying Eq.(8). Introducing this a after obtaining a is maximized inductively. With this obtained optimal plan x maximum value of the expected payoff for the whole time span (−T, 0] is just V (x The existing methods of revision games are all based on the grim trigger (GT) strategy Kamada and Kandori [2020b], where as long as there is any deviation (either by unintentional action error or intentional deviation), both players will never forgive, and retaliate to the deadline. In this section, we examine the performance of LR strategies, and compare them with GT strategy in Prisoner’s Dilemma where players may make occasional mistakes. B(a) > C(a) and B(a) and C(a) denote the player’s beneﬁt and cost, respectively Killingback et al. [1999]. , · · · , a, · · · , a} can maximize the social welfare V (x) if and only if it satisﬁes Eq.(6) and Eq.(7), and is = ˙a, aand ˙ahave the same constraints. π(a) is strictly increasing, thus a< ˙a. Moreover, This allow us to even further simplify the above maximization problem. We transform it into many subis already obtained in the previous sub-problem. This means the maximization in Eq.(12) can be Consider in a continuous Prisoner’s Dilemma, player-i’s payoff is π(a, a) = B(a) − C(a), where Figure 2: Equilibrium Payoffs of LR and GT in the Prisoner’s Dilemma revision games with different error rates. Without loss of generality, we use B(a) = 2a, C(a) = a version of this stage game, we set λ = 1 for the Poisson arrival rate. For the LR strategy, we set the retaliation ﬁerceness k = 0.33 as an example. The collusive plan for this LR strategy has been shown in Figure 1. Now we set different lengths and different error rates of the revision games and show the simulation results in Figure 3. Players’ action error rates in the subﬁgures (a), (b) and (c) are low (2%), medium (10%) and high (30%), respectively. is, in each subﬁgure, 50 different revision games are simulated, with game lengths varying from T = 0 T = 50. The y-axis is the expected equilibrium payoff. There are two colored curves in each subﬁgure, one is for LR strategy equilibrium and the other is for GT strategy equilibrium. The payoff for each strategy in each simulated game is depicted by a colored dot, and all the dots compose a colored curve. For each simulated game (i.e., each dot), we sample players’ actions according to the error rate. For example, in subﬁgure (a), players follow the plan x of LR (or the plan of GT) with a probability 98%, but has a probability 2% to make a mistake at any revision opportunity. We have two observations from the simulations. 1. By comparing the two curves in all three subﬁgures, we can see that, as the error rate increases, LR 2. In each subﬁgure, the payoff of LR is relatively stable for different game lengths (different values on the In Appendix-E, we show more simulations regarding different values of k. We also show results for LR strategies in the Cournot game. From all numerical results, we can see that the novel LR strategies provide an effective scheme to correct agents’ errors in revision games. With LR strategies, cooperation not only can be sustained but also can be restored. As long as error exists, players with LR strategies can reap a stable and high payoff, which is in a sharp contrast to the GT strategy. This makes LR strategies more realistic than GT for real-world revision games. We analyze the incentive constraint for cooperation and forgiveness in revision games and identify the strategy of limited retaliation with retaliation duration limited to k (LR strategies). We prove that LR strategies can − a. The Nash action is a= 0 while the fully cooperative action is a= 1. For the revision In each subﬁgure, each value of T on the x-axis represents a total length of a simulated revision game. That becomes better and better than GT. Thus in a noisy environment, forgiveness is a crucial component to correct agents’ errors and LR can help agents to obtain high payoffs. Evidence of forgiveness has commonly been seen in the real world. Real people are usually not extremely grim. A society in which people are cooperative, vengeful but also forgiving usually have higher social welfare. x-axis), while the payoff for GT decreases drastically as the game length increases. Thus for a revision game with a longer duration, LR performs better than GT. Such an observation is coincident with realworld scenarios that in people’s long-run interactions, forgiveness plays a more important role for people to go to a win-win outcome. A longer future interaction confers upon people more incentive to forgive a mistake, as long as this mistake can be properly punished. sustain cooperation in subgame perfect equilibrium. We also design an algorithm to ﬁnd the optimal piecewise constant plan for LR strategies. We ﬁnally implement this algorithm in both the continuous prisoner’s dilemma and the Cournot revision games. Our derivation of SPE is following a backward induction style. ity of resuming cooperation. These strategies are extremely ﬁerce, which are not like realistic human behavior. More importantly, if players have any observation or action error, or imperfect rationality, the strategy players will soon go to a loss-loss situation and can never escape. This makes the social welfare become low. LR strategies can return back to mutual cooperation, even when there is an intentional or unintentional deviation by any player. They are cooperative, vengeful, and also forgiving. These properties make them especially suitable for real-world games where players confront observation or action errors. They well capture the mechanism of forgiveness, which has not yet been discussed in revision games. By the theory, GT is better than LR in revision games if there is no error, this is because it provides the strongest threat. But the real world is noisy. As long as there is any action mistake, imperfect information or bounded rationality, a proper LR is better. To conquer the problems of GT, famous strategies including Tit-for-Tat, Win-Stay Loss-Shift have been identiﬁed. But LR is the ﬁrst strategy better than GT in noisy revision games. Our immediate future work is to ﬁnd other collusive plans and investigate their relationship with piecewise constant plans. Moreover, for AI community, it is an interesting topic to design a game simulation or tournament for revision games under uncertainty. Researches on counterfactual regret minimization Zinkevich et al. [2007], Celli et al. [2020] and dynamic games cooperation Martinez-Vaquero et al. [2015] could have underlying relationships with limited retaliation and forgiveness. Finally, applications of LR in various realistic games are also challenging but valuable topics. Before LR strategies, the exisiting Grim Trigger strategies are never forgiving, which excludes the possibil-