Transfer learning (TL) is a set of techniques of using abundant somewhat related source data p(X to ensure that a model can generalize well to the target domain, deﬁned as either little amount of labelled data p(X learning). Transfer Learning is most commonly achieved either via ﬁne-tuning or co-training. Finetuning (FT) is a process of adapting a model trained on source data by using target data to do several optimization steps (for example, SGD) that update the model parameters. Co-training on source and target data usually involves reweighting the instances in some way or enforcing domain irrelevance on feature representation layer, such that the model trained on such combined data works well on target data. Fine-tuning is becoming increasing popular because large models like ImageNet [Krizhevsky Fine-tuning of large pre-trained image and language models on small customized datasets has become increasingly popular for improved prediction and eﬃcient use of limited resources. Finetuning requires identiﬁcation of best models to transfer-learn from and quantifying transferability prevents expensive re-training on all of the candidate models/tasks pairs. In this paper, we show that the statistical problems with covariance estimation drive the poor performance of H-score [Bao et al., 2019] — a common baseline for newer metrics — and propose shrinkage-based estimator. This results in up to 80% absolute gain in H-score correlation performance, making it competitive with the state-of-the-art LogME measure by You et al. [2021]. Our shrinkage-based H-score is 3 − 55 times faster to compute compared to LogME. Additionally, we look into a less common setting of target (as opposed to source) task selection. We demonstrate previously overlooked problems in such settings with diﬀerent number of labels, class-imbalance ratios etc. for some recent metrics e.g., NCE [Tran et al., 2019], LEEP [Nguyen et al., 2020] that resulted in them being misrepresented as leading measures. We propose a correction and recommend measuring correlation performance against relative accuracy in such settings. We also outline the diﬃculties of comparing feature-dependent metrics, both supervised (e.g. H-score) and unsupervised measures (e.g., Maximum Mean Discrepancy [Long et al., 2015] and Central Moment Discrepancy [Zellinger et al., 2019]), across source models/layers with widely varying feature embedding dimension. We show that dimensionality reduction methods allow for meaningful comparison across models, cheaper computation (6×) and improved correlation performance of some of these measures. We investigate performance of 14 diﬀerent supervised and unsupervised metrics and demonstrate that even unsupervised metrics can identify the leading models for domain adaptation. We support our ﬁndings with ∼ 65,000 (ﬁne-tuning trials) experiments. et al., 2012], Bert [Devlin et al., 2018] etc. are released by companies and are easily modiﬁable. Training such large models from scratch is often prohibitively expensive for the end user. In this paper, we are primarily interested in eﬀectively measuring transferability before training of the ﬁnal model begins. Given a source data/model, a transferability measure (TM) quantiﬁes how much knowledge of source domain/model is transferable to the target model. Transferability measures (TMs) are important for various reasons: they allow understanding of relationships between diﬀerent learning tasks, selection of highly transferable tasks for joint training on source and target domains, selection of optimal pre-trained source models for the relevant target task, prevention of trial procedures attempting to transfer from each source domain and optimal policy learning in reinforcement learning scenarios (e.g. optimal selection of next task to learn by a robot). If a measure is capable of eﬃciently and accurately measuring transferability across arbitrary tasks, the problem of task transfer learning is greatly simpliﬁed by using the measure to search over candidate transfer sources and targets. Contributions We study both supervised and unsupervised TMs in the context of ﬁne-tuning. For supervised TMs, our contributions are three-fold: 1. We show that H-score, commonly used as a baseline for newer supervised TMs, suﬀers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in 9/15 cases against all newer TMs across various ﬁne-tuning scenarios. 2. We present a fast implementation of our estimator that is 3 − 55 times faster than state-of-the-art LogME measure. Unlike LogME, our optimized implementation for our estimator is tractable even for really high-dimensional feature embeddings ∼ 10 3. We identify problems with 3 other supervised TMs (NCE, LEEP and NLEEP) in target task selection when either the number of target classes or the class imbalance varies across candidate target tasks. We propose measuring correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios. For unsupervised TMs, we outline computational challenges and propose dimensionality reduction methods for better estimation and eﬀective comparison of such measures when the feature dimensions are large and/or diﬀerent across various source models. We show that with our proposed modiﬁcations, even unsupervised TMs can be eﬀective in identifying the best source ImageNet model. Our large set of 65,000 ﬁne-tuning experiments with multiple ImageNet models and diﬀerent regimes generated from CIFAR-100 and CIFAR-10 image datasets shows usefulness of our proposals. This paper is organized as follows. Section 2 describes general ﬁne-tuning regimes and transfer learning tasks. Section 3 discusses supervised TM and addresses shortcomings of the pioneer TM (H-Score) that arise due to limited target data (subsection 3.1). In subsection 3.2 we demonstrate problems with recent NCE, LEEP and NLEEP metrics and propose a way to address them. Section 4 highlights shortcomings of diﬀerent commonly used unsupervised measures for source selection and proposes alternatives that oﬀer improvements. Finally, Section 5 presents a meta study of all metrics. We consider the following ﬁne-tuning scenarios based on existing literature. (i) Source Model Selection (SMS): For a particular target data/task, this regime aims to select the “optimal” source model (or data) to transfer-learn from, from a collection of candidate models/data. (ii) Target Task Selection (TTS): For a particular (source) model, this regime aims to ﬁnd the most related target data/task. In addition, we primarily consider two diﬀerent ﬁne-tuning strategies: (i) Linear FT/head only FT : All layers except for the penultimate layer are frozen. Only the weights of the head classiﬁer are re-trained while ﬁne-tuning. (ii) Nonlinear FT : Any arbitrary layer can be designated as a feature extractor, up to which all the layers are frozen; the subsequent layers include nonlinear transformations and are re-trained along with the head on target data. Related Work Recent literature in transfer learning has proposed computationally eﬃcient TMs. We categorize measures that require target labels as supervised TMs. Inspired by principles in information theory, Negative Conditional Entropy (NCE) Tran et al. [2019] uses pre-trained source model and evaluates conditional entropy between target pseudo labels (source models’ assigned labels) and real target labels. Log Expected Empirical Predictor (LEEP) [Nguyen et al., 2020] modiﬁes NCE by using soft predictions from the source model. Both NCE and LEEP do not directly use feature information, hence they are not applicable for layer selection. Cui et al. [2018] propose representing each output class by the mean of all images from that class and computing Earth Mover’s distance between the centroids of the source classes and target classes. Bao et al. [2019], Li et al. [2021], Huang et al. [2021], You et al. [2021], Deshpande et al. [2021] proposed metrics that capture information from both the (learnt) feature representations and the real target labels. These metrics are more appealing as these can be broadly applicable for models that are pre-trained in either supervised or unsupervised fashion. Li et al. [2021] proposed NLEEP that ﬁts a Gaussian mixture model on the target feature embeddings and computes the LEEP score between the probabalistic assignment of target features to diﬀerent clusters and the target labels. Huang et al. [2021] proposed TransRate — a computationally-friendly surrogate of mutual information (using coding rate) between the target feature embeddings and the target labels. Bao et al. [2019] introduced H-score that takes into account inter-class feature variance and feature redundancy. You et al. [2021] proposed LogME that considers an optimization problem rooted in Bayesian statistics to maximize the marginal likelihood under a linear classiﬁer head. Deshpande et al. [2021] introduced LFC to measure in-class similarity of target feature embeddings across samples. Finally, Tan et al. [2021] used Optimal Transport to evaluate domain distance, and combined it, via a linear combination, with NCE. To learn such a measure, a portion of target tasks were set aside, the models were transferred onto these tasks and the results were used to learn the coeﬃcients for the combined Optimal Transport based Conditional Entropy (OTCE) metric. While the resulting metric appears to be superior over other non-composite metrics like H-score and LEEP, it is expensive to compute since it requires ﬁnding the appropriate coeﬃcients for the combination. Additionally, our results indicate that both components of the measure seem to be individually sub-optimal in measuring transferability against corresponding supervised and unsupervised measures. H-score [Bao et al., 2019] is one of the pioneer measures that is often used as a baseline for newer supervised TMs, which often demonstrate the improved performance. It characterizes discriminatory strength of feature embedding for classiﬁcation: where, d is the embedding dimension, f feature extractor (h : R F ∈ R data labels, Σ Z ∈ R sample covariance matrix of z. Intuitively, H(f ) captures the notion that higher inter-class variance and small feature redundancy results in better transferability. denotes the corresponding target feature matrix, Y = Y∈ Y = {1, ··· , C} are the target denotes the corresponding target conditioned feature matrix, Σ∈ Rdenotes the We hypothesize that the sub-optimal performance of H-Score (compared to that of more recent metrics) for measuring transferability in many of the evaluation cases, e.g., in [Nguyen et al., 2020], is due to lack of robust estimation of H-Score. Given that many of the deep learning models in the context of transfer learning have high-dimensional feature embedding space — typically larger than the number of target samples — the estimation of the two covariance matrices in H-score becomes challenging: the sample covariance matrix of the feature embedding has a large condition number in small data regimes. In many cases, it cannot even be inverted. Bao et al. [2019] used a pseudoinverse of the covariance matrix Σ sub-optimal as inversion can amplify estimation error [Ledoit and Wolf, 2004]. We propose to use well-conditioned shrinkage estimators motivated by the rich literature in statistics on the estimation of high-dimensional covariance (and precision) matrices [Pourahmadi, 2013]. We show that the use of such shrinkage estimators can oﬀer signiﬁcant gain in the performance of H-score in predicting transferability. In many cases, as our experiments show, the gain is so signiﬁcant that H-score becomes a leading TM, surpassing the performance of state-of-the-art measures. Proposed Transferability Measure We propose the following shrinkage based H-score: Estimating Σ madi, 2013], we present an approach that considers a linear operation on the eigenvalues of the sample version of the feature embedding covariance matrix. Similar ideas of using well-conditioned plug-in covariance matrices are used in the context of discriminant analysis [Hastie et al., 2001]. In particular, we improve the conditioning of the covariance matrix by considering its weighted convex combination with a scalar multiple of the identity matrix: where α ∈ [0, 1] is the shrinkage parameter and σ is the average variance computed as tr(Σ that the inverse of Σ shrinkage parameter controls the bias and variance trade-oﬀ; the optimal α needs to be selected. This distribution-free estimator is well-suited for our application as the explicit convex linear combination is easy to compute and makes the covariance estimates well-conditioned and more accurate [Ledoit and Wolf, 2004, Chen et al., 2010, Sch¨afer and Strimmer, 2005]. Understanding (1 − α)Σ covariance matrix estimation under a ridge penalty: where λ ≥ 0 is the ridge penalty. Choosing λ = α/(1 − α), it becomes clear that (1 − α)Σ regularized covariance matrix. Choice of α Ledoit and Wolf [2004] proposed a covariance matrix estimator that minimizes mean squared error loss between the shrinkage based covariance estimator and the true covariance matrix. The optimization with respect to α considers the following objective: where kAk shrinkage parameter, which is given by: where (7) deﬁnes a valid estimator (not dependent on true covariance matrix) for practical use. For proof, we refer the readers to Section 2.1 and 3.3 in Ledoit and Wolf [2004]. Additional Discussion The covariance Σ mation of H where f Cov(f|Y ) denotes the class-conditioned covariances. We can write Comparing (9) with (8), we see that the same shrinkage parameter α should be used when using shrinkage estimators, to preserve law of total covariance. The ﬁrst two terms on the right side in (9) can be understood as shrinkage of class-conditioned covariances to the average (global) variance. The third term in (9) (e.g. (1 − α)Σ Eﬃcient Computation for small target data For small target data (C ≤ n implementation of H based H-Score that exploits diagonal plus low-rank structure of Σ the low-rank structure of Σ spondingly Z) are centered. The optimized computation of H where R = The derivation is provided in the Supplement Section S2.1. We make a timing comparison of our optimized implementation of H measure and demonstrate 3 − 55 times faster computation (see Table 6 in Section 5.3). Next, we pursue a deeper investigation of some of the newer metrics that are reported to be superior to H-Score and bring to light what appears to be some overlooked issues with these metrics in Target Task Selection (TTS) scenario. Target task selection has received less attention than Source Model Selection (SMS). To our knowledge, we are the ﬁrst to bring to light some problematic aspects with NCE, LEEP and NLEEP, which can potentially lead to the misuse of these metrics in measuring transferability. These measures are sensitive to the number of target classes (C) and tend to be smaller when C is larger (see Fig. 1[Left]). Therefore, use of these measures for target tasks with diﬀerent C will most likely result in selecting the task with a smaller C. However, in practice, it is not always the case that transferring to a task with a smaller C is easier; for example, reframing a multiclass classiﬁcation into a set of binary tasks can create more diﬃcult to learn boundaries [Friedman et al., 2000]. Furthermore, = tr(AA)/d. This optimization problem permits a closed-form solution for the optimal α= E[||Σ− Σ||]E[||Σ− (tr(Σ)/d) · I|| (f)— the two covariances are coupled by the law of total covariance: denotes the feature embedding of target samples that belong to class Y ∈ Y and Σ= i.e, Σ= (1 − α)Σ+ αtr(Σ)dI= (1 − α)E[Σ] + αtr(Σ)dI+ (1 − α)Σ Figure 1: Relation of NCE, LEEP & NLEEP to [Left] number of classes (log-scale) and [Right] class imbalance, max(n classes. For [Right], we randomly select 2 classes and vary the class imbalances. the measures are also problematic if two candidate target tasks have diﬀerent degree of imbalance in their classes even if the number of classes is the same. The measures would predict higher transferability for imbalanced data regimes over balanced settings (see Fig. 1[Right]). However, imbalanced datasets are typically harder to learn. If these measures are correlated against vanilla accuracy, which tends to be higher as the imbalance increases e.g. for binary classiﬁcation, the measures would falsely suggest they are good indicators of performance. Earlier work has failed to consider both these aspects and erroneously shown good correlation of these metrics against vanilla accuracy to show dominance of their proposed metrics in target task selection with diﬀerent C [Nguyen et al., 2020, Tan et al., 2021] and imbalance [Tan et al., 2021]. Here, we propose a method to ameliorate the shortcomings of NCE, LEEP and NLEEP to prevent misuse of these measures, so that they lead to more reliable conclusions. We propose to standardize the metrics by the entropy of the target label priors, leading to the deﬁnitions in (11). This standardization considers both the class imbalance as well as number of classes through the entropy of the target label priors. Our proposed normalizations in (11) ensures the normalized NCE is bounded between [0, 1]. For proof, see Supplement Section S2.2. n-NCE is in fact equivalent to normalized mutual information and has been extensively used to measure correlation between two diﬀerent labelings/clustering of samples [Vinh et al., 2010]. Given the similar behavior of LEEP and NCE to diﬀerent C and class imbalance as shown in Fig. 1, we suggest the same normalization as given in (11). However, this normalization does not ensure boundedness of n-LEEP score (and by extension n-NLEEP) in the range [0, 1] as in the case of n-NCE. For scenarios where candidate target tasks have diﬀerent C, we propose an alternative evaluation criteria (relative accuracy) instead of vanilla accuracy — see Section 5 for more details. We provide empirical validation of the proposed normalization to these measures in Table 2 in Section 5.1. We also show that our proposed shrinkage-based H-Score is the leading metric even in these scenarios. Since most of the existing TMs essentially estimate the diﬀerence between source and target distributions (be it just the labels as in LEEP and NCE or between distributions of feature embeddings and labels as in H-score or LFC), one can consider unsupervised discrepancy measures that don’t use target labels, but rely only on source and target data feature embeddings. They have been used for regularization in the context of domain adaptation [Li et al., 2020, Zellinger et al., 2019], however they have not been studied for characterizing transferability in the context of ﬁne-tuning. It remains to be seen how informative these metrics are as TMs for ﬁne-tuning. These metrics are important because they can be used with unlabeled target data when supervised metrics cannot be used. Related Work In early work, Ben-David et al. [2006] proposed A-distance to estimate domain divergence from unlabelled source and target data. This metric can be empirically estimated with an additional model trained to predict the domain of the data (source or target) and uses the domain classiﬁcation accuracy as a proxy. Training such a model is expensive, requires choosing an architecture and/or tuning hyperparameters and may end up being more expensive than actual transfer learning. We consider more standard discrepancy measures as unsupervised TMs. Maximum mean discrepancy (MMD) [Long et al., 2015] measures mean diﬀerences between distributions in some rich kernel space and has been previously used to detect covariate shift between input distributions [Rabanser et al., 2019]. Central moment discrepancy (CMD) [Zellinger et al., 2019] measures diﬀerences in mean and higher order moments. Similarly, correlation alignment (CORAL) [Sun and Saenko, 2016] measures the diﬀerence in covariance matrices of source and target feature distributions. Kullback-Leibler Divergence (KLD) [Kullback and Leibler, 1951] is a standard divergence metric for two distributions, which can be calculated either by making an assumption on the distributions or using non-parametric estimators. Optimal transport (OT) [Bonneel et al., 2011] considers the optimal energy required to shift distributions from source features to target features. We also consider Wasserstein distance (WD) [Kantorovich, 1939], under normality assumptions for source and target features. Supplement Section S3.2 contains formulas for all the unsupervised TMs. Challenges of comparing feature distribution discrepancy across task pairs Next, we discuss challenges in using unsupervised discrepancy metrics on the feature embeddings of source and target data across model/task pairs in both source model selection and target task selection scenarios. Given that none of the discrepancy measures (mentioned above) are normalized, direct comparison of these measures across model/task pairs leads to the following challenges: 1. Scale of the features across diﬀerent source models can be arbitrary, for example ImageNet models with or without Batch Normalization layers. 2. The feature dimension (d) across diﬀerent source models even for the penultimate layer can vary signiﬁcantly e.g. from 1024 in MobileNet to 4096 in VGG19. Normalization is not straightforward for many metrics such as MMD, KLD etc. Such diﬀerences makes source model/layer selection for ﬁne-tuning highly problematic. 3. d may be huge. Measuring discrepancy in high-dimensional spaces is both challenging and not well-established due to curse of dimensionality [Rabanser et al., 2019]. Our proposals to address challenges: We address the ﬁrst challenge (outlined above) by standardizing both source and target feature embeddings via feature-wise standardization using source features’ ﬁrst and second order moments (z-score). See Supplement Section S3.1 for more details. This standardization is in contrast to independent z-score normalization of source and target embeddings, ensuring that measures that consider diﬀerences in moments e.g., CMD can eﬀectively capture such information even after standardization. To address the second and third challenge (outlined above), we propose dimensionality reduction of feature embeddings before computing unsupervised discrepancy measures. We project feature embeddings to a lower q-dimensional space, where q is taken to be the same across the competing K models/layers and satisﬁes: q ≤ min the feature spaces. The dimensionality reduction allows for more meaningful comparison of measures across source/target pairs; this is relevant for source/layer selection. More generally, it also allows for faster and more robust estimate for limited target samples case (n ﬁne-tuning. In the case of nonlinear ﬁne-tuning, the intermediate layers of visual and language models have really large d ∼ 10 We consider Principal Component Analysis (PCA) and Gaussian Random Projection (RP). Both use a linear transformation matrix V to derive the transformed features an optimal orthogonal transformation to capture as much of the variance in the data, while the latter samples components from N(0, the dataset. Untrained auto-encoders (AE) are other alternatives that have been used to detect covariate shifts in input distributions by Rabanser et al. [2019]. It is not known how sensitive these untrained AE are to the underlying architecture—using trained AE is less appropriate for use in transferability measurement for ﬁne-tuning as those maybe more time-consuming than the actual ﬁnetuning. We demonstrate improved correlation performance of unsupervised discrepancy measures with dimensionality reduction in Table 3 in Section 5.1 for target task selection and Table 5 in Section 5.2 for source model selection. We evaluate existing TMs and our proposed modiﬁcations in various ﬁne-tuning regimes and data settings. We draw inspiration from Nguyen et al. [2020] who consider target task selection and source model selection. The experimental setup highlights important aspects of TMs, e.g., dataset size for computing empirical distributions, covariance matrices and discrepancy measures, number of target classes, and feature dimension etc. Some of these aspects have been overlooked when evaluating TMs, leading to improper conclusions. Recent work usually considers either supervised or unsupervised domain adaptation. The proposed measures to quantify transferability have mostly relied on the availability of target labels; hence, they have been evaluated in supervised domain adaptation with ﬁne-tuning. It has not been previously shown how these measures compare against metrics that only rely on feature distributions e.g. MMD, CMD etc. Therefore, we also provide an empirical evaluation of these measures and demonstrate that even these measures can be eﬀective at identifying the best model to transfer from—this allows for a much broader applicability of the use of these discrepancy measures. Fine-tuning with hyperparameter optimization The optimal choice of hyperparameters for ﬁne-tuning is not only target data dependent but also sensitive to the domain similarity between the source and target datasets [Li et al., 2020]. We thus tune the hyperparameters for ﬁne-tuning: we use Adam optimizer and tune batch size, learning rate, number of epochs and weight decay (L2 regularization on the classiﬁer head). For tuning, we set aside a portion of the training data (20%) and try 100 random draws from hyperparameters’ multi-dimensional grid. With this additional tuning complexity, we performed 650×100 ﬁne-tuning experiments. See additional information and motivation in Supplement S4. Evaluation criteria TMs are often evaluated by how well they correlate with the test accuracy after ﬁne-tuning the model on target data. Following Tran et al. [2019], Nguyen et al. [2020], Huang et al. [2021], we used Pearson correlation. We include additional results with respect to rank correlations (e.g., Spearman) in Supplement Section S7. We argue that considering correlation with the target test accuracy is ﬂawed in some scenarios. In particular, for target task selection, it is wrong to compare target tasks based on accuracy when C is diﬀerent e.g 5 vs 10 classes. In such a case, task with 10 classes will have a high chance of arriving at lesser test accuracy compared to that for task with 5 classes. In this case, it is more appropriate to consider the gain in accuracy achieved by the model over it’s random baseline. Hence we use relative accuracy (for balanced classes): measure is more eﬀective in capturing the performance gain achieved by the same model in transferring to two domains with diﬀerent C. This also highlights the limitation of NCE, LEEP and NLEEP which are sensitive to C and tend to have smaller values with higher C; these measures do not provide useful information about how hard these diﬀerent tasks when evaluated on the original accuracy scale. Correlations marked with asterisks (*) in Tables 1, 2, 3, 4 are not statistically signiﬁcant (p-value > 0.05). Hyphen (-) indicates the computation ran out of memory or was really slow. We consider Small balanced (S-B) target data, small imbalanced (S-IB) and Large Balance (L-B). See additional details in Supplement Section S6. Validation of H H-score against the original measure by [Bao et al., 2019] with pseudo-inverse matrix of the sample feature covariance. Table 1 demonstrates 80% absolute gains in correlation performance of H H(f), making it a leading metric in many cases in small target data regimes. Table 2 demonstrates how various supervised TMs perform on target task selection when the number of target classes varies. H(f) dominates the performance in both cases, surpassing all supervised TMs. Table 1: Correlation comparison of supervised TMs against ﬁne-tuned target accuracy. Larger correlations indicate better identiﬁability as quantiﬁed by TM. We compared our proposed H original H(f) and state-of-the-art measures. Table 2: Correlation performance of TMs against relative accuracy for Large Balanced CIFAR-100 data with diﬀerent number of classes across target sets. Validating Dimensionality Reduction for Unsupervised TMs Table 3 shows that unsupervised discrepancy measures greatly beneﬁt from dimensionality reduction (DR). Our ﬁndings are summarized below: • MMD and KLD greatly beneﬁt from dimensionality reduction as they indicate signiﬁcant improvement in all cases where the correlations hold signiﬁcance. • For both CMD and WD, dimensionality reduction improve the correlation in 2/3 cases. • OT does not gain from DR in estimating transferability. Table 3: Correlation performance of unsupervised TMs (see Section 4) in ﬁne-tuning with/without DR in target task selection for ImageNet Models. We select 9 small to large ImageNet models. We evaluate source model selection for ﬁne-tuning under small sample setting. We sample 50 images per class from all classes available in the original train split of CIFAR-100/CIFAR-10. We designate 10 samples per class for hyperparameter tuning. Validation of H duction techniques in Section 4 are also pertinent for supervised metrics like H-score for source model selection with diﬀerent feature dimensions because the metric is measured in the feature space (despite using target labels). Given that the feature dimensions vary signiﬁcantly across diﬀerent models , we apply RP to project to 128-dimensional space (q = 128). This allows for more meaningful comparison of H-score across source models and provides the gains of proposed H well for small samples as given in Table 4, making it again a leading metric in source model selection. Table 4: Correlation of proposed H source model selection of ImageNet Models in small data regime constructed from CIFAR-100. Comparison of Unsupervised TMs with Supervised TMs in identifying top model Table 5 demonstrates which of the metrics can be used to predict the best source model (Top-3 indicates that the best model is in top 3 predictions sorted by this metric). We show that even unsupervised TMs, which can be used both for supervised and unsupervised domain adaptation, exhibit excellent performance in identifying top performing source. Note this performance of unsupervised TMs is with dimensionality reduction (as outlined in Section 4). For linear ﬁne-tuning, all of the supervised measures (except for original H-score and TransRate) and unsupervised metrics (except for OT) include the best model in their top 3 predictions. For tuning additional layers, all unsupervised metrics are able to do the same with random projection. CIFAR-100 Table 5: Model identiﬁcation (as measured by Top-3) by supervised/unsupervised measures for ﬁnetuning in source model selection of ImageNet Models in small data regime. Table 6: Timing comparison of LogME and our shrinkage-based H-score. All times are in ms. We empirically investigate the computational times of H mentation in (10). For this exercise, we generate synthetic multi-class classiﬁcation data using Sklearn [Pedregosa et al., 2011] multi-class dataset generation function that is adapted from Guyon [2003]. We investigate diﬀerent values for number of samples (n (C). For data generation, we set number of informative features to be 100 with the rest of the features ﬁlled with random noise. Table 6 demonstrates a signiﬁcant computational advantage of H LogME. We observe 3 − 55 times faster computational times. LogME seems intractable both with respect to memory and time for d ∼ 10 We study both supervised and unsupervised TMs in the context of ﬁne-tuning. For supervised TMs, our contributions are three-fold. First, we show that H-score measure, commonly used as a baseline for newer supervised TMs, suﬀers from instability due to poor estimation of covariance matrices. We propose shrinkage-based estimation of H-score with regularized covariance estimation techniques from statistical literature. We show 80% absolute increase over the original H-score and show superior performance in 9/15 cases against all newer TMs across various ﬁne-tuning scenarios and data settings. Second, we present a fast implementation of our estimator that provides a 3 −55 times computational advantage over state-of-the-art LogME measure. Unlike LogME, our optimized implementation for our estimator is tractable even for really high-dimensional feature embeddings ∼ 10 identify problems with 3 other supervised TMs (NCE, LEEP and NLEEP) in target task selection (an understudied ﬁne-tuning scenario than source model selection) when either the number of target classes or the class imbalance varies across candidate target tasks. We propose an alternative evaluation scheme that measures correlation against relative target accuracy (instead of vanilla accuracy) in such scenarios. For unsupervised TMs, we identify challenges with computation and propose dimensionality reduction methods for better estimation and eﬀective comparison of such measures when the feature dimensions are large and/or diﬀerent across various source models. We show that even unsupervised TMs can be eﬀective in identifying the best source ImageNet model with our proposals. Our large set of 65,000 ﬁne-tuning experiments with multiple ImageNet models and diﬀerent regimes generated from CIFAR-100 and CIFAR-10 image datasets demonstrates usefulness of our proposals. We leave it for future work to explore how predictive various TMs are for co-training regimes (as opposed to ﬁne-tuning).