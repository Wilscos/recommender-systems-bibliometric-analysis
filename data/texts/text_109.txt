Additional Key Words and Phrases: fairness in AI, health misinformation, bias reduction 1 Introduction Most Fairness in AI research focuses on exposing biases in AI systems. A broader lens on fairness reveals that AI can serve a greater aspiration: rooting out societal inequities from their source. Speciﬁcally, we focus on inequities in health information, and aim to redu ce bias in that domain using AI. The AI algorithms under the hood of search engines and social media, many of which are based on recommender systems, have an outsized impact on the quality of medical and health information online. T herefore, embedding bias detection and reduction into these recommender systems serving up med ical and health content online could have an outsized positive impact on patient outcomes and wellbeing. In this po sitio n paper, we oﬀer t he following contribut io ns: (1) we propose a novel framework of Fairness via AI, inspired by insights from medical education, sociology and antiracism; (2) we deﬁne a new term, bisinformation, which is related to, but distinct from, misinformation, and encourage researchers to stud y it; (3) we p ropose using AI to study, detect and mitigate biased, harmful, and/or false health information that disproportionately hurts minority groups in society; and (4) we suggest several pillars and pose several open problems in order to seed inquiry in this new space. While part (3) of this work speciﬁcally focuses on the health domain, the fundamental computer science advances and contributions stemming from research eﬀorts in bias reduction and Fairness via AI have broad implications in all areas of society. 2 Fairness via AI The vast majority o f Fairness in AI work focuses on exposing the bias in AI system, in order to showcase where the AI system is biased. However, AI systems will continue to be biased so long as the data they are receiving are biased, according to “Bias In, Bias Out” principle [ 16]. So long as signiﬁcant structural inequalities exist in the real world, AI systems will continue to replicate and exacerbate them. The dominant Fairness in AI approach, then, risks engaging in a Sisyphean task of minimizing bias in AI, with attempts to debias AI datasets, models and algorithms continually needing to be “ﬁxed” as they learn biased outcomes and are bound to hit a ceiling of fairness: that of real world settings that are inherently biased. Under t his approach, our highest aspiration in designing AI systems seems to be one of avoidance: tweaking our models to refrain from adding to society’s ills and inequities. Rooted in insights from medical education, sociology, and antiracism, we oﬀer a broader lens on fairness, revealing that AI can serve a far greater aspiration: enabling important restorative work and rooting out societal inequities from their source, with a deeper and more meaningful impact. In this position pap er, we therefore reverse the traditional direction of fairness: rather than aiming to achieve Fairness in or of AI, we propose focusing on Fairness via AI. With this approach, one can use AI to study, detect, mitigate and remedy situations that are inherently unequal, unjust and unfair in society. With this ambitious yet grounded approach, our potential impact is unbounded, and can accelerate progress towards a more fair, equal and just world. In other words, we can use AI to thoughtfully, carefully and ethically debias the world, rather than simply trying to debias AI. Speciﬁcally, the AI algorithms under the hood of search engines and social media, many of which are based on recommender systems, have an outsized impact on the quality of information available online. T herefore, embedding bias detection and reduction directly into these recommender systems could have an outsized positive impact on the information ecosystem. 3 Bisinformation We coin the t erm bisinformation to represent biased information, referring to a unique and chall enging asp ect of the information landscape. We are particular ly interested in health b isinformation, where bias and language misuse have a detrimental impact on patient outco mes, t hough the term can easily apply in any ﬁeld. Bisinformation may overlap with, but is not identical to, misinformation. The use of biased language or inappropriate social identiﬁers in a medical context, for example, can be harmful even if st rictly true - consider the case of referring to the prevalence of an ill ness in a racial category without contextualizing it in Social or Structural Determinants of Health (SSDoH), such as systemic racism or income inequities [17]. On the other hand, certain types of bisinformation are, in fact, also a form of misinformation, such as the long-debunked Salt Gene Hypothesis [19]. To the best of our knowledge, no studies have computationally studied health bisinformation at a large scale. Medical bisinformation (and misinformation). As an illustration of societal inequit ies in dire need of the Fairness via A I approach, consider t he ﬁeld of medicine and medical educatio n. The ﬁeld is marred by a long and painful history of overt and covert forms of social injustice, bias, and racism, as illustrated by the American Medical Association’s recent pledge to take action to confront systemic racism [14, 22]. Studies continue to demonstrate that physicians possess implicit biases in a number of diﬀerent areas such as race/ethnicity, gender, sex, age, weight, substance use and mental illness [7]. This comes into play signiﬁcantly in medical institutio ns, which continue to teach biased medicine in preclinical years [see, e.g. 23]. Many educators, for example, continue to inappropriately use race as a proxy for genetics or ancestry, or even as a “risk factor” for numerous health outcomes often erroneously associated with race while ignoring SSDoH [1, 2, 8, 11, 17]. Many educators continue to inappropriately use gender and sex terms and perpetuate the idea that sex and gender are binary and stagnant (versus ﬂuid). Likewise, mo st medical e ducators are unaware of the numerous biases in the types o f images they use in their lectures or assessment materials as well [5, 6, 13]. By equating social identiﬁers to biology without social or structural context, medical educators are unknowingly perpetuating a curriculum that can have an adverse eﬀect on health outcomes [4, 15]. Bias reduction in curricular and assessment content is critical for educating future physicians in accurate evidence-based medicine [12, 21], but is a manual, costly and time consuming eﬀort. SOTA AI and NLP approaches can be used to scale up these eﬀorts signiﬁcantly. Naturally, bisinformation and misinformation that p ersist in the medical establishment, are also disseminated and extensively present in online medical resources, websites and news articles, and social media, with large negative effects on historically underserved populations, also reinforcing biased narratives and stereotypes abou t minority groups. Recommender systems play an outsized role in serving up such content online, but improving these systems to reduce bias will prove extremely challenging if we don’t understand the underlying mechanisms in which such bias is perpetuated and disseminated. Prior work has suggested that controversy online is highly unevenly distributed, and that a popu lation-sensitive model is needed in order to properl y model this [10]; we hypothesize that the same approach may be neede d in the computational stud y of bisinformation and misinformation. For example, the COVID-19 pandemic and its associated “infodemic” has brought health mis- and disinformation to the forefront of national and scientiﬁc attention. However, trust in the medical establishment may be understandably low among African-Americans [3] and other minority groups [9]. Recent work suggests, moreover, that health mis- and disinformation is qualitatively distinct in diﬀerent population groups [e.g. 9, 20]. To cite just one example, COVID-19 vaccine hesitancy has been demonstrated to be higher among racial and ethnic minorities [9, 18]. A few guiding pillars underlie and drive our Fairness via AI research agenda, which we encourage others to adopt. First, we argue that Fairness via AI is a more eﬀective and impactful marshalling of research resources than “standard” Fairness in AI work (important though the latter may be). Second, we argue that collaboration across disciplinary ﬁelds is critically needed in order to eﬀectively and ethically study and understand society’s biggest challenges, to say nothing of mitigating them. Researchers in other ﬁelds, including but in no way not limited to the social sciences, have immense expertise in stu dying and addressing societal inequities; computer scientists cannot, and should not, go this alone. Finally, we argue that biased information interacts w ith false information in complex ways that must be studied carefully in order to reduce bias in recommender systems and other information delivery systems (such as search engines). With these pillars ﬁrmly in mind, we pose the following open questions: Q1. What are eﬀective approaches to identify societal problems that are in most need of, and lend themselves to, the Fairness via AI framework? Q2. Which existing and/or novel AI approaches need to be deployed and developed in order to address such societal issues? Q3. How can we encourage collaboration across disciplinary boundaries in order to leverage hard-won insights from other ﬁelds, and infuse our Fairness research with t hem? Q4. Speciﬁcal ly wit h respect to b isinformation, several research questions arise: a. How and where does bisinformation spread online? Is information (including mis- and bisinformation) distributed and disseminated diﬀerentially among diverse population groups? If so, how? b. Which categories or types of bisinformation and misinformation are most problematic and harmful, and thus deserving the most diligent fact checking, and countermessaging eﬀorts? In other words, how should we tr iage mis- and bisinformation, combining best practices in the public health and fact checking spheres with state-of-the-art comp utational approaches? In this position paper, we introduced a novel Fairness via AI framework; coined a new term, bisinformation, to describe biased information, and demonstrated t hat is is overlapping with yet distinct from misinformation; brieﬂy discussed t he documente d presence of bisinformation in medical curricula and posit t hat this extends to other information environments, such as online; and po sed several open questions to guide research agendas on the subject. Acknowledgements. This material is based in part upon work supported by the National Science Foundation under Grant No. 1951091. Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the National Science Foundation.