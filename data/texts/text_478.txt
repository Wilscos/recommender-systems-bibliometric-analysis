The problem of low-rank matrix completion with heterog e neous and sub-exponential (as opposed to homogeneous and Gaussian) noise is particularly relevant to a number of applications in modern commerce. Examples include panel sales data and data c ollected from web-commerce systems such as recommendation engines. An important unresolved question for this problem is characterizing the distribution of estimated matrix entries under common low-rank estimators. Such a characterization is essential to any application that requires quantiﬁcation of uncertainty in these estimates and has heretofore only been available under the assumption of homogenous Gaussian noise. Here we characterize the distribution of estimated matrix entries when the observation noise is heterogeneous sub-ex ponential and provide, as an application, explicit formulas for this distribution when observed entries are Poisson or Binary distributed. Consider the problem of low-rank matrix completion: there exists a low-rank matrix that we seek to recover, having obse rved only a subset of its entries, each perturbed by additive noise. A rich stream of research over the past two decades has essentially solved this problem – there exist eﬃcient algorithms which achieve order-optimal recovery guara ntees under provably-minimal a ssumptions (Candès and Recht 2009, Candes and Plan 2010, Keshavan et al. 2010). Further advances have yielded (and continue to yield) algorithmic improvements (Mazumder et al. 2010, Jain e t al. 2013, Tanner and Wei 2016, Dong et al. 2021), and a deeper understanding of the optimization landscape itself (Ge et al. 2016, Zhu et al. 2017). Naturally, these algorithms have been applied in a vast array of applications, including recommendation systems, bioinformatics, network localization, and modern commerce (Su and Khoshgoftaar 2009, Natarajan and Dhillon 2014, So and Ye 2007, Amjad and Shah 2017), just to name a few. Now many of these applica tio ns require, in addition to scalability and accuracy, the ability to quantify the uncertainty of an estimator – for example, something as seemingly-straightforward as conﬁdence intervals on the estimated entries of a matrix. Such an uncertainty quantiﬁcation pro cedure, analogous to existing procedures for problems like linear regression, would ideally (a) apply to a commonly-used estimator, (b) req uire no more additiona l computation than the estimator itself, and (c) be justiﬁed by a (limiting) distributional characterization. Given the volume and success of the research just described, it is perhaps surprising that this problem has been large ly unsolved (see the Related Work for past progress). Fortunately, there was a rece nt “breakthrough.” Applying newer techniques such as the leave-oneout technique and ﬁne-grained entry-wise analysis (Ma et al. 2018, Ding and Chen 2020, Abbe et al. 2020), Chen et al. (201 9, 2020) proposed an uncertainty quantiﬁcation technique for matr ix co mpletion, which satisﬁes the three “ideal” conditions above, in the case of homogeneous Gaussian noise. Further progress in Xia and Yuan (2021) extended this to homogeneous s ub- Gaussian noise . Toward “Realistic” Noise: A gap still exists when we seek to apply these inferential results in practice, since many applications have more sophisticated noise models (namely, heterogeneous and sub-exponential noise). For example, in discrete panel sales data, the observation for sales at a location during a period of time is commonly modeled as Poisson with a certain Table 1: A comparis on of uncertainty formulas for diﬀerent noise models when r = 1, i.e., M details in Section 4. exp ected sales rate (Amjad and Shah 2017, Shi e t al. 2014). Similarly in web-commerce systems, data indicating clicks or purchases is often binary and modeled as Bernoulli random variables (Ansari a nd Mela 2003, Grover and Srinivasan 1987). Thus motivated, in this work we establish the ﬁrst uncertainty quantiﬁcation r e sults for matrix completion with heterogenous and sub-exponen tial noise. Precisely, we characterize the distribution of recovered matrix entries from common estimators. An application of our re sults can already be seen in Table 1, where we have derived explicit formulas under Poisson and Binary noise, which are distinctive from the homogeneous Gaussian noise ca se already existing in the literature. In addition, we demonstrate the quality of our procedure through experiments on real sales data. The proof of our main result generalizes the proof framework in (Chen et al. 2019), leveraging recent results for sub-exponential matrix completion from McRae a nd Davenport (2019), and a new highdimension concentration bound (Lemma 1), which may be of independent interest. Related Work: This paper is related to at least three streams of work. The ﬁrst is, naturally, uncertainty quantiﬁcation in matrix completion. Besides the works described above, prior appro aches to this were based on either (a) converting recovery guarantees on matrix norms to conﬁdence regions (Carpentier et al. 2015, 2018), (b) the Bayesian formulation of matrix co mpletion (Salakhutdinov and Mnih 2 008, Fazayeli et al. 2014, Tanaka 2021, Alquier et al. 2015), or (c) deeplearning-bsa e d methods (Lakshminarayanan e t al. McRae and Davenport (2019) established guarantees on the Frobenius error kˆM − Mk; Farias et a l. (2021) established entry-wise error guarantees. This wor k makes takes one step further with an entry-wise distributional characterization of the error. Finally, there is a line of work, in multivariate linear regression, a dvocating the use of heteroskedasticity-robust variance estimators instead of homoskedasticity estimators, since the former are more robust to heterogeneous noise (Long and E rvin 2000, Hayes and Cai 2007, Imbens and Kolesar 2016, Cribari-Neto and Maria da Glória 2014). Our work is in the same spirit, but in the context of matrix completion. Notation: The sub-ex ponential no rm of a random variable X is deﬁned as kX k:= inf{t > 0 : E (exp(|X|/t)) ≤ 2}. For a matrix A ∈ R,PP we abbreviateAasAwhen no ambiguity exists. We require a few matrix norms:P kAk:= maxA, kAk= max|A|, andP kAk=A. The spectral norm is denoted kAk. Let M∈ Rbe a rank-r matrix, where m ≤ n without loss of generality. Let O = M+E be the realization of Mcorrupted by a noise matrix E ∈ R. We obse rve P(O), which is the s ubse t of entries of O restricted to an observation set Ω ⊂ [m ] × [n]: The matrix completion problem is to recover Mfrom this noisy and partial observation P(O). Let M= UΣVbe the SVD of M. Here, Σ∈ Ris a diagonal matrix with singular values σ= σ≥ σ≥ . . . ≥ σ= σ; and U∈ R, V∈ Rcontain the left and right-singular vectors. Let κ = σ/σbe the condition number of M. We will make three assumptions. The ﬁrst two are, by this point, canonical in the matrix completion literature (Candes and Plan 2010, Keshavan et al. 2010, Ma et al. 2018, Abbe et al. 2020, Chen et al. 2019): Assumption 1 (Uniform Sampling). Each element of [m] × [n] is included in Ω independently, and with probability p. Assumption 2 (Incoherence). Finally, our third assumption is a generalization of the independent (and often homogeneous), subGaussian noise that is typically assumed in the literature (Chen et al. 2019, Xia and Yuan 2021). As described above, this generalization enables a host of practical applications, such as those arising in counting data and panel sales data (Amjad and Shah 2017, Ansari and Mela 2003). Assumption 3 (Indepe ndent Sub-exponential Noise). The entries of E are independent, mean-zero random variables with variances σ, and are also independent from Ω. Furthermore, kEk≤ L for every (i, j), where k · kis the sub-exponential norm. In this section, we describe a “de-biased” estimator M for M. This was originally proposed in (Chen e t al. 2019), where the uncertainty quantiﬁcation fo r M is characterized under homogeneous, Gaussian nois e . Motivated by practical applica tio ns, we study new uncertainty quantiﬁcation formulas for Munder heterogenous sub-exponential noise. Here,ˆM is the projection of M into the set of rank-r matrices in regard to Euclidean distance (res tricted on the set Ω). Directly solving Eq. (2) turns out to be a challenge task. A popular method is to represe nt M= XY where X ∈ R, Y ∈ Rare low-rank factors, and solve the following non-convex regularized optimization problem where With proper initializations, simple ﬁrst-order methods are often suﬃcient to solve Eq. (3) (Sun and Luo 2016). The regula rizer λ > 0 here is used to promote additional structure properties. Fo r example, when gradient descent is performed, a positive λ is critical for analyzing the convergence properties and also helps to achieve a balance between X and Y (Chen et al. 2020). However, the use of λ also introduces additional bias to the estimator in Eq. (3), which has been a major obstacle to ana lyze the uncertainty quantiﬁcation properties. (Chen et al. 2019) proposes a de-bias procedure to remove the bias brought by λ, based on the solution of Eq. (3). The algorithm is summarized below. Algorithm 1 Gradient Descent with De-bias Input: P(O)√ 1: Spectral initialization: X= UΣ, Y=√ VΣ where UΣVis the top-r partia l SVD decomposition ofP(O). 2: Gradient updates: for t = 0, 1, . . . , t− 1 do where η determines the learning rate. 3: De-bias: Steps 1 and 2 in Algorithm 1 form a typical gradient descent procedure for solving Eq. (3). The de-biasing step, i.e. Eqs. (5) and (6) in Alg orithm 1, is critical for e nabling the uncertainty quantiﬁcation analysis. We will use the remainder of this section (which can be skipped without loss of continuity) to provide so me intuition for the peculiar form of Eqs. (5) and (6) based on ﬁrst-order conditions. Consider an example with p = 1 (no entry is missing). Since O is fully observed, let O = UΣV+ UΣVbe the SVD of O, where Σcorresponds to the largest r singular values and Σcorresponds to the remaining one. Then it follows that the optimal solution of Eq. (2) isˆM = UΣV(Eckart and Young 1936). Next, consider the regularized objective Eq. (3). We can derive that the optimal solution (X, Y ) for Eq. (3) has the for m X = U(Σ− λI), Y = V(Σ− λI). In fact, this can be veriﬁed from the ﬁrst-or der condi- We assume λ ≍ L log(n)√np, t≍ n, η ≍ 1/(nκσ) throughout the paper, if not speciﬁed explicitly. tions, ∂f(X, Y )= (XY− O)Y + λX where in (i) we use that VY = VV(Σ− λI)= 0, and in (ii) we use that VV= I. Similarly= 0 also vanishes. Then, this justiﬁes the particula r de-biased form in Eqs. (5) and (6): Similarly, Y= VΣ. Thus XY= UΣVis the desired optimal solution of Eq. (2). tainty quantiﬁcation for Munder heterogeneous, subexp onential noise. Theorem 1. Assum e mp ≫ κµrlogn andpp L log(n)n/p ≪ σ/κµr log n. Then for every (i, j) ∈ [m] × [n], we have where Φ(·) is the CDF of the standard Gaussian, and s> 0 is given by To quickly parse this r e sult, note that a typical scaling of the parameters would see m = Θ(n), np & log(n), µ = r = κ = L = O(1), σ= Ω(n), σ= Ω(1), a ndp kVk = kUk = Ω(1/n). Theorem 1 would then imply that where sis deﬁned in Eq. (7). This is precisely the typ e of characterization we sought at the outset. The form of s, as deﬁned in Eq. (7), is of course critical to the characterization, and probably best under stood via a few examples: 1. Homogeneous Gaussian Noise. First as a sanity check, when E∼ N(0 , σ), Theorem 1 reduces to the same variance formula as Theorem 2 in (Chen et al. 2019): 2. Poisson Noise. When the observations are Poisson, i.e. O∼ Poisson(M), the variance of the noise Eis σ= Var(O− M) = M. Then applying Theorem 1, we have that M−M∼ N(0 , s) where A special case is when r = 1 and M= σuv, for which we have which corres ponds to the formula in Table 1. in a reco mmender system or e-commerce platform, O∈ {0, 1} can represent whether the ith user viewed (or purchased) the jth item (or product) (Ansari and Mela 2 003, Grover and Srinivasan 1987, Farias and Li 2019). A common noise model for such observations is to assume the Oare Bernoulli random variables w ith mean M, i.e., O∼ Ber(M). With such binary observations , the variance of the noise Eis σ= Va r(O− M) = M(1 − M). Then stakes the form Empirical Inference: In practice, the underlying U and Vare no t known, and thus scannot be computed exactly. We propose the use of the corresponding empirical estimators to estimate sfor the purposes of inference. Le t M= UΣVbe the SVD of M. For example, in the Poisson noise scenario, we would use the following empirical estimator for s: In cases where σis also unknown, we letˆE= O− Mbe the empirical es timator for the noise. T his procedure (i.e. the use of empirical estimators) can be justiﬁed via the following result: Corollary 1. Follow the settings in Theorem 1. Assume that ∀(i, l), σ= Θ(L) and Let be the empirical estimator of s. Then under the same assumptions made in Theorem 1, we have that Additional justiﬁcation for this procedure is given as exp eriments later on. Aside: When s≈ 0. Curious readers may note that smay be too small for Theorem 1 and Corollary 1 to apply. In this ca se, although the Gaus sian approx imation in Theorem 1 does not hold, an entry-wise error bound still holds, and may be suﬃcient for many applications (see the Appendix for details): An uncertainty characterization when s≈ 0 involves a second-order error analysis and remains an open question. In this section, we pres e nt the proof fra mework of Theorem 1 (see details in Appendix A). In order to extend to heterogeneous sub-exponential noise from ho mogeneous Gaussian, we generalize the proof of (Chen et al. 2019) with the help of recent sub-exponential matrix completion results (McRae and Davenport 2019) and a sub-exponential variant of matrix Bernstein inequality (Lemma 1). Similar to Chen et al. (2019), our proof is based on the leave-one-out technique that has been recently used for providing breakthr ough bounds for entry-wise analysis in matrix completion problems (dated back to Ma et al. (2018), also see Ding and Chen (2020), Abbe et al. (2020), Chen et al. (2020)). We establish the following key results to characterize the decomposition of low-rank factors (X, Y), as a heterogeneous sub-e xponential generalization of Theorem 5 in Chen et al. (2019). Theorem 2. Assum e mp ≫ κµrlogn andq L log(n)≪√. There exists a rotation matrix H∈ Oand Φ∈ R, Φ∈ Rsuch that the following holds with probability 1 − O(n), where Proof. At a high level, the proof of Theorem 2 follows a simila r proof of Theorem 5 in (Chen et al. 2019), but with replacements that employ more ﬁne-grained analyses of E for whenever the Gaussianity of E is used in (Chen et al. 2019). These a nalyses aim to address the sub-exponentiality and heterogeneity of E, with the help o f the following two lemmas. Lemma 1. Given k independent random m× m matrices X, X, . . . , Xwith E[X] = 0. Let Suppose kkXkk≤ B for i ∈ [k]. Then, kX+ X+ . . . + Xk . V log(k(m+ m)) + B log(k(m+ m)) log(k) with probability 1 − O(k) for any constant c. Lemma 2. Suppose E ∈ R(m ≤ n) whose kEk≤ L for any (i, j) ∈ [m]×[n]. Let Ω ∈ [m]×[n] be the subset of indices where each index (i, j) is included in Ω independent ly with probability p. Suppose np ≥ clogn for some suﬃcient large constant c. then, with probability 1 − O(n), Here, Lemma 1 is a generalization of matrix Bernstein inequality in Theorem 6.1.1 of (Tropp et al. 2015). Lemma 2 is an implication of Lemma 4 in (McRae and Davenport 20 19). Equipped with Lemmas 1 and 2, the desired bounds for sub-exponential E can be e stablished. Following we provide an example of using Lemma 1 to bound kXP(E)Yk, which is critical for obtaining the bounds in Theorem 2. To begin, note that where δ∼ Ber(p) indicates whether (k, l) ∈ Ω. Let A:= XYδEfor k ∈ [m], l ∈ [n]. Then, Note that A∈ Rare independent z ero-mean random matrices and we aim to invoke Lemma 1 to boundP B := maxkkAkk. Note that where in (i) we use the fact that E(x) ≤ 2kxk for an sub-exponential zero-mean random variable kPE[AA]k. Hence V ≤ 2LpkXkkYk. B := maxkkAkk where (i) we use that kEδk≤ kEk≤ L. Then apply Lemma 1, with probability 1 − O (n), we o bta in the desired bound for kXP(E)Yk A.V log(n) + B log(n) where in (i) we use that kXk= kYk≤ σr and the incoherence condition Eq. (1), in (ii) we use that To establish a similar bound for kXP(E)Yk, Chen et al. (2019) uses the Gaussianity of E, which is not applicable her e . Similar to this e xample, we apply Lemma 1 and Lemma 2 with more ﬁne grained analyses to address the sub-exponentiality and heterogeneity of E. See Appendix A.3 for full deta ils . Note that the error of M− Mis closely related to the errors of low-rank factor s XH−X, YH−Y through the following where in (i) we ignore the second-order error term (XH− X)(YH− Y). Note that Theorem 2 implies that The results of Theorem 1 then follow from the above approximation and the use of Berry-Esseen type of inequalities. See Appendix A.5 for full details. We evaluate the results in Theorem 1 for sy nthetic data under multiple settings. We then compare the performances of var ious uncer tainty q uantiﬁcation formulas in real data. Synthetic Data. We generate an ensemble of instances. Each instance consists of a few parameters: (i) (m, n): the size of M; (ii) r: the ra nk of M; (iii) p: the probability of an entry being observed; (iv)P ¯M: the entry-wise mean of M(¯M=M). Given (m, n , r, p,¯M), we follow the typical procedures of generating random non-negative low-rank matrices in (Cemgil 2008, Farias e t al. 202 1). Each instance is generated in two steps: (i) Generate M: let U∈ R, V∈ Rbe random matrices with independent entries from Gamma(2, 1). Set M= kUVP where k ∈ R is picked such thatM=¯M. (ii) Generate P(O): then O= Poisson(M) and entries in Ω is sampled independently with probability We ﬁrs t verify the entry-wise distributional characterization M−M∼ N(0 , s) where sis speciﬁed in Eq. (7). See a demonstratio n of the Gaussian approximality of the empirical distribution (M− M)/s in Fig. 1. Given an instance, we compute the coverage rate (the percentage of coverage of entries) that correponds to the 95% conﬁdence interval, where an “cover age” of an entry (i, j) oc curs if Here, we focus on the results of Poisson noise, where the results u nder the binary noise are similar in the experiments. The average coverage rates under diﬀerent settings are shown in Table 2. The clos e ness of the r e sults (rang ing from 91% −95%) to the “true” coverage rate 95% suggests the applicability of inference based on our variance formula. The trends in Table 2 are also consistent with the intuition: the performance starts to degrade when r increases, p decreases, a nd the noise to signal ratio increases (decrease of¯M). Figure 1: Empirical distribution of (M− M)/s with m = n = 300, r = 2, p = 0.6, and¯M= 20. Table 2: Coverage rates for diﬀerent (r, p,¯M) with m = n = 500. The empirical mean and empirical standard deviation are reported over 10 0 instances. Real Data. Next, we study a real dataset consisting of daily sales for 1115 units with 942 days (Ross mann 2021). To compare diﬀerent uncertainty quantiﬁcation formulas, we consider the coverage rate max imization task that aims to maximize the coverage rate given the total interval length constraint. Coverage Rate Maximization. In particular, given a uncertainty quantiﬁcation formula, suppose one can provide an interval predictor [a, b] for each entry (i, j) in a set˜Ω. The “coverage” of (i, j) occurs if M∈ [a, b] where Mis the true value of entry (i, j). The task aims to maximize the coverage rateP given that the total length of intervalsb−a is constrained by a budget threshold α: maximize˜ subject tob− a We are interested in comparing the performances of the above task using diﬀerent variance predictor s s, either provided by Eq. (9) with the homogeneous Gaussian noise assumption (Theorem 2 in Chen et al. (2019)), or by our Theorem 1, capable of addressing the heterogeneous sub-exponential noise. Note that both results in Chen et al. (2019) and our Theorem 1 predict that M∼ N(M, s). With this distributional assumption, we tackle Eq. (11) by a greedy algorithm that achieves the maximal exp e c ted coverage rate with the budget constraint. Speciﬁcally, with given {M, s}, we provide the interval predictors {[a, b]} by solving the following problem: maximize|˜Ω| subject tob− a≤ α rankness of the dataset is veriﬁed and the “true” rank, as well as the “true” underlying matrix M , is predetermined through the spectrum of singular value decomposition. We split the entries uniformly into a training set Ω (with probability p) and a test set˜Ω. We use the observations in Ω to learn Mwith Algorithm 1. Let M= UΣVbe the SVD of M. The empirical variance sfor homogeneo us Gaussian noise is computed by (Chen et al. 2019) where ˆσ:=P(O−M)/|Ω| is the empirical estimator for the noise variance. Given M, sand s, the coverage rate maximization task is evaluated in the test set˜Ω. The results for various budgets α are reported in Fig. 2. The Poisson nois e formula shows a higher coverage r ate than the homogeneous Gaussian formula, as the former is more robust to addressing heterogeneous noises in sales data. This improvement tends to vanish with more pres e nce s of missing entries, which might be due to the degrading acc uracy of matrix completion and variance e stimation when p decre ases. Figure 2: Coverage rates for diﬀerence var iance formulas correspond to the total-interval-length budget. We solved the uncertainty quantiﬁcation problem for matrix completion with heterogeneous a nd subexp onential nois e . The error variance of a common estimator wsa determined and the asymptotical nor mality with infer e nce results were e stablished. The explicit formulas for va rious scenarios such as Poisson noise and Binary noise were analyzed. Experimental results showe d signiﬁcant improvements of our new uncertainty q uantiﬁcation formulas over existing ones. One ex c iting direction for further work is in assuming less restrictive Ω. As in most of the matrix completion literature, we made the uniform sampling assumption for Ω, which may not be applicable in some practical applications. The study of uncertainty quantiﬁcation for matrix completion with non- uniform sampling patterns is e specially valuable, given the recent progress on deterministic ma trix completion, e.g., (Chatterjee 2020).