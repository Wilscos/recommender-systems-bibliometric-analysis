Ranking problems arise in many domains such as sports tournaments [5, 2], recommendation systems [12] and even in the study of systems in biology [4, 28]. Typically, the data associated with these problems are binary outcomes of pairwise comparisons. For example in a sports tournament, the data comprises of the result of each game played between two teams or players (team i beat team j). The aim then is to recover a ranking of the items from the pairwise outcomes, often by estimating a score for each item, that represents its latent strength/quality. We remark that comparison data are frequently prefered to the attribution of a score by the users due to the sensitivity Many applications such as recommendation systems or sports tournaments involve pairwise comparisons within a collection of n items, the goal being to aggregate the binary outcomes of the comparisons in order to recover the latent strength and/or global ranking of the items. In recent years, this problem has received signiﬁcant interest from a theoretical perspective with a number of methods being proposed, along with associated statistical guarantees under the assumption of a suitable generative model. While these results typically collect the pairwise comparisons as one comparison graph G, however in many applications – such as the outcomes of soccer matches during a tournament – the nature of pairwise outcomes can evolve with time. Theoretical results for such a dynamic setting are relatively limited compared to the aforementioned static setting. We study in this paper an extension of the classic BTL (Bradley-Terry-Luce) model for the static setting to our dynamic setup under the assumption that the probabilities of the pairwise outcomes evolve smoothly over the time domain [0, 1]. Given a sequence of comparison graphs (G)on a regular grid T ⊂ [0, 1], we aim at recovering the latent strengths of the items w∈ Rat any time t ∈ [0, 1]. To this end, we adapt the Rank Centrality method – a popular spectral approach for ranking in the static case – by locally averaging the available data on a suitable neighborhood of t. When (G)is a sequence of Erd¨os-Renyi graphs, we provide non-asymptotic `and ` error bounds for estimating wwhich in particular establishes the consistency of this method in terms of n, and the grid size |T |. We also complement our theoretical analysis with experiments on real and synthetic data. to bias of the latter. Indeed, the evaluation of the items and the strategy of scoring usually varies considerably between users and thus lead to rather inconsistent scores [12]. The collection of pairs of items which are compared can be naturally represented as an undirected graph G = ([n], E) where the n vertices are the items to rank, and an edge {i, j} ∈ E represents a comparison between items i and j. Note that in order to recover the underlying ranking, the graph G needs necessarily needs to be connected. Indeed, if this is not the case, then there will exist two connected components of G such that there is no information available to rank items in one component with respect to the other. Classic BTL model. A classic statistical model for ranking problems was introduced by Bradley and Terry [3] and completed by Luce [17]; this is now referred to as the BTL (Bradley-Terry-Luce) model. Denoting w beats item i, namely y Bernoulli random variables paramaterized by y extensively through many appplications, such as sports tournaments [5], measurements of pain [18] or the estimation of the risk of car-crashes [14]. Moreover, numerous methods have been proposed to analyse the model theoretically with the goal of estimating the latent strengths. Two methods which have received particular attention are the maximum likelihood estimator (MLE) [3, 23], and a spectral method called Rank Centrality introduced by Neghaban et al. [23]. Both these methods have been analyzed in detail over the years for general graph topologies (eg., [23]) and also for the special case of Erd¨os-Renyi graphs G ∼ G(n, p) (eg., [7, 6]). For instance, let us denote π to be the normalized vector of true scores, and bπ its estimate returned by Rank Centrality. Then assuming that each comparison between two items has been performed L times (for L large enough) and p & Here ., & hide absolute constants, see notations in Section 2. Analogous bounds hold for the MLE as well, and moreover, these bounds are optimal [7, 6]. The ` as they readily lead to conditions for exact recovery of the true ranks of the items. A weaker requirement is the identiﬁcation of the set of top-K ranked items for some small K, see [7]. Dynamic ranking. The aformentioned setup considers the ‘static’ case where only one comparison graph is available. However, in many applications, the latent strengths of the items evolves over time. Indeed, a user preference changes with time in recommendation systems, as does the level of a player or a team in sports tournaments. This dynamic setting has received signiﬁcantly less attention than its static counterpart and has mostly been studied with a focus on applications, such as sports tournaments [5, 11, 21]. It has rarely been analysed theoretically in the past however, although some results exist for a state-space generalization of the BTL model [9, 10, 19] and for a Bayesian framework [10, 16]. Dynamic BTL. The focus of this paper is to consider a dynamic version of the classical BTL model for ranking. In this dynamic scenario, we are given pairwise comparisons on a discrete set of time instants T , hence leading to a sequence of comparison graphs (G w> 0 to be the strength of item i at time t, the probability that item j beats item i at time , it holds with high probability (w.h.p) that [7, 6] t∈ T is now given by recover the underlying ranking at any given time instant t, where t does not necessarily belong to T . Furthermore, we make the following remarks. • Suppose t lies in T , then we could consider only using the pairwise comparison data cor- • Very recently, a couple of theoretical results were obtained for the dynamic ranking problem. Our contributions. We provide in this paper the following contributions. • We consider a model adapted to the dynamic ranking setting which involves using the BTL • We provide a detailed theoretical analysis of the previous algorithm by deriving ` • Finally, we perform extensive experiments on synthetic data which validates our theoretical Outline of the paper. After presenting the dynamic BTL model and our algorithm in Section 2, we summarize our main theoretical results for the ` then describe in detail the ` conduct experiments on both synthetic and real data in Section 6, and discuss other closely related work in Section 7. Notation For any probability vector π ∈ R norm kxk responding to G, along with a suitable algorithm for the static case. However, Gis not necessarily going to be connected; indeed, all the comparison graphs can now be disconnected and be very sparse. Hence this approach will fail in such scenarios. This suggests that it is in some sense crucial that we utilize the pairwise comparison information across diﬀerent time instants in T in order to get meaningful rank recovery at time t. This approach can of course be relevant even if Gwas connected for all t∈ T . Li and Wakin [15] include in their model the observed diﬀerence of scores (for example the score of a football game) which clearly contains richer information in comparison to simple binary outcomes. Bong et al. [2] considered an adaptation of the BTL model to the dynamic setting similar to what has been described above. They ﬁrst combine pairwise comparison information by applying kernel smoothing to the entire dataset, and then use the MLE for the smoothed data to recover the ranks. We discuss these results in more detail in Section 7. model at each time point, but assumes that the pairwise outcome probabilities are Lipschitz functions of time. In this setting, we propose an adaptation of the Rank Centrality method, namely Dynamic Rank Centrality (Algorithm 1) to recover the ranks at any given time t by averaging the data belonging to a suitable neighborhood of t. error bounds for estimating the latent strength vector w= (w, . . . , w)at any given time instant t ∈ [0, 1]. In particular, assuming T ⊂ [0, 1] is a uniform grid (|T | = T + 1) and the comparison graphs (G)are Erd¨os-Renyi, we show (see Corollaries 1,2) for T large enough that the error rates are of the order O(T) w.h.p. This result holds in very sparse regimes where each Gcould be potentially disconnected. Moreover, this rate matches the classical pointwise estimation error rate for Lipschitz functions. ﬁndings. We also evaluate our method on a real data set and show that it performs well. =πx. For a matrix A, the corresponding induced matrix norm is then deﬁned as kAk where kAk is denoted by kAk λ(A). Positive (absolute) constants are denoted by c, C, a, b ≥ 0, we write a . b if there exists a constant C > 0 such that a ≤ Cb. Moreover, we write a  b if a . b and b . a. Let us formally introduce our model for dynamic pairwise comparisons, inspired by the BradleyTerry-Luce (BTL) model. We consider a set of items [n] = {1, 2, . . . , n}, with a certain quality at each time t ∈ [0, 1], represented by the weight vector w each i ∈ [n]. Let us denote where we will require that b(t) is ﬁnite for each t ∈ [0, 1]. Our data consists of pairwise comparisons on this set of items at times t ∈ T = The outcomes at each time t ∈ T are gathered into an undirected comparison graph G where E compared items, i.e. the set of edges E To model such data, we use the BTL model at each time t ∈ T . This model posits that the probability that an item j wins over an item i is proportional to its strength. At each time t ∈ T , for each pair of compared items {i, j} ∈ E are independent Bernoulli variables, deﬁned for l ∈ {1, . . . , L} by y The proportion of times j won over i at time t is given by y true proportion is denoted by Smooth evolution of pairwise outcomes. Our goal is to recover w Suppose for the moment that t is on the grid, then if G a positive scaling. However in our dynamic setting G connected. Therefore for meaningful recovery of w the evolution of the pairwise outcomes over time. To this end, we make the following smoothness assumption. Assumption 1 (Lipschitz smoothness). There exists M ≥ 0 such that = supkxAk. Note that some simple inequalities follow from these deﬁnitions. denotes the spectral norm, i.e., largest singular value, of A. The Frobenius norm of A is the set of edges. While the set of items [n] is supposed to be the same throughout, the This assumption suggests that the pairwise outcomes at nearby time instants are similar, hence it is plausible that w neighborhood of t. To formalize this intuition, let us deﬁne a neighborhood at any time t by where δ ∈ [0, T ]. Note that if δ < hence we will consider δ ∈ [1/2, T ]. It is easy to verify that δ ≤ |N will also be useful to denote Neighborhood graph. For any time t ∈ [0, 1], the data contained in the neighborhood N can be gathered into a union graph, deﬁned as G maximum and minimum degree of a vertex in G respectively. For each edge {i, j} ∈ E N(t) where i and j are compared as along with the quantities General recovery idea. Given the above setup, a general idea for recovering w the union graph G Due to Assumption 1 we know that hence if δ = o(T ), then for each {i, j} ∈ E Moreover, if the corresponding sequence of graphs G to a positive scaling) is ensured. The connectivity requirement on G than requiring the individual graph(s) to be connected, which is also a key diﬀerence between the static and dynamic settings. Also note that the computed statistcs ¯y estimators as they average the data over a suitable neighborhood of t. More generally, one could also compute ¯y With the above discussion in mind, a general scheme for recovering w would be to ﬁrst form G ¯y(t) as in (2.6), and ﬁnally apply any existing method for the static case using the comparison graph G Negahban et al. [23] – a popular spectral algorithm known to achieve state of the art performance – and adapt it to our dynamic setting. (t) and the data (¯y(t)). We will focus on the Rank Centrality algorithm of The Rank Centrality [23] method is based on the connection between pairwise comparisons and a random walk on a directed graph. In the static case, a faithful estimation of the weight vector is given by the stationary distribution of the Markov chain induced by a suitably constructed transition matrix. In the dynamic setting, this method can be adapted for estimating w constructing the transition matrix using G More precisely, we deﬁne a transition matrix where d each time t, ﬁnite state space. Thus there always exists at least one stationary distribution associated to Moreover, stochastic matrices admit 1 as leading eigenvalue and so a candidate as the stationary distribution is its leading left eigenvector, i.e. Besides, the vector of true weights w bution of a transition matrix on the union graph. Speciﬁcally, denoting π easily show that π since One can reasonably expect bπ(t) to be close to π and¯P (t) respectively, the latter of which are expected to be close. Indeed, one has the following bias-variance trade-oﬀ where the variance term is typically expected to decrease with δ (due to averaging over N the bias term will scale as O(δ/T ) (due to the smoothness assumption 1). Hence for a suitably chosen δ = o(T ) we will then have (for n, T large enough) (t) ≥ d(t) is a suitably chosen normalization term. Then, one can easily verify that at bP (t) is a transition matrix (bP (t) is stochastic) corresponding to a Markov chain on a ¯P (t) and π(t) verify the detailed balance equation of reversibility [13] Remark 1. For meaningful recovery the vector π if the associated Markov chain is irreducible which in turn is ensured by the connectivity of the underlying graph (here, the union graph G [13]. The condition on the weights is guaranteed in our setup since (indeed, w Based on the above discussion we can outline the steps of our method for ranking in the dynamic setting in the form of Algorithm 1. Our goal now is to establish conditions under which bπ(t) is Algorithm 1 Spectral algorithm for dynamic ranking (Dynamic Rank Centrality) results of comparisons as statistics (y in (2.4). close to π particular, we will strive to establish consistency results (i.e., the error approaching zero) when the grid size T → ∞. Before proceeding, we need to deﬁne some additional quantities related to the union graph G(t) which will appear in the following sections. Let L walk Laplacian of G adjacency matrix. We denote ξ is the second largest eigenvalue (in absolute value) of L since it is similar to the symmetric Laplacian D In Section 3.1, we present bounds on the ` bounds on the ` For a given sequence of graphs (G(t (holding w.h.p) which in particular highlights the dependence on parameters related to the union graph G Theorem 1. For any given t ∈ [0, 1], suppose that δ ∈ [ for some constant c that if > 0 for each t ∈ [0, 1] and i ∈ [n]). (t) under the `and `norms. These results are summarized in the next section. In (t), the grid size T and the neighborhood size δ. then it holds with probability at least 1 − O(n kbπ(t) − π Let us make the following observations. 1. The ﬁrst term in the RHS of (3.2) corresponds to the bias and arises from the regularity Now we consider the important case where the comparison graphs are Erd¨os-Renyi graphs, i.e., G= G(n, p(t Renyi denoted by G(n, p In this setting, the bound in Theorem 1 can be simplﬁed using concentration results for parameters related to G In particular, we will choose the normalization factor d Lemma 11 also states that if p considerations lead to the following simpliﬁcation of Theorem 1. Theorem 2. Suppose that G t ∈ [0, 1]) with p let δ ∈ [ c> 0 as in Theorem 1, and constants c holds, we have with probability at least 1 − O(n kπ(t)kT ξ(t)d(t)d(t)ξ(t)d(t)LN(t) assumption 1, while the second term therein is the variance term. Moreover, note that the error depends on δ – either explicitly, or through certain quantities such as d(t), N(t) etc. In order to obtain a more explicit dependence in terms of δ, we will need to make speciﬁc assumptions on the graphs G, t∈ T . Below, we will consider the setting where the graphs are Erd¨os-Renyi graphs and derive explicit conditions on δ that lead to consistency with respect to T . M = 0 and δ = 1/2, so N(t), N(t) ≡ 1. Denoting d(t), d(t), ξ(t) to be the corresponding quantities with the δ suﬃx suppressed, condition (3.1) is satisﬁed for L largeq enough. Moreover, the error bound is then O() which matches the `bound of Negahban et al. [23, Theorem 1] with thelog n factor therein removed, but with an extra b(t) factor. (t) (see Lemma 11). Speciﬁcally, we have that if p(t) & log n/n, then w.h.p , T ] be such that n ≥ clog n, np(t) ≥ clog n, and p(t) ≥ clog n with constant The following remarks are in order. 1. As can be seen, the bias term is O( 2. In the static case we observe a single comparison graph G We now derive an appropriate choice for δ that leads to an ` end, we ﬁrst need to explicitly show the dependence on δ for p simplicity that Since δ ≤ |N Proposition 1, p meaning that the conditions on p Remark 2. The condition p concentrate around p condition would disappear and we would only be left with the requirement p would of course impose a less stringent lower bound on T as it would then suﬃce that δp Since our setup is not limited to Erd¨os-Renyi graphs, we prefer to keep |N Corollary 1. Under the same notations as in Theorem 2, for all t ∈ [0, 1] suppose that n & log n and p such that δ & Hence if p(t) grows with δ, then the variance error will reduce as δ increases. p(t) ≡ 1 and the condition p(t) & log n is not needed, while p(t) = p(t). Hence, if p(t) &and L is suitably large, the `error is bounded by O(√), which corresponds to the bound obtained by Chen et al. [7, Theorem 9]. So our result is coherent with existing results for the static case for Erd¨os-Renyi graphs. d(t) to be a constant (≥ 1) multiple of d(t) as well. However for the `analysis later on, it will be crucial to choose d(t) as a constant times np(t) for technical reasons arising in the analysis. Similar considerations for the choice of the normalization factor exist in the static setting as well (see [7, 6]). Note that this choice of d(t) requires us to know p(t), but in case we don’t know p(t) in practice, we can instead use its empirical estimate which can be easily computed. is as in (3.5). Choosing δ = minT, Tand d(t) = 3np(t), if T is The following observations are useful to note. We now discuss our results for bounding the ` bounds are particularly desirable in the context of ranking as they lead to guarantees for recovering the ranks of the items. We will assume that all the comparison graphs (at each t Renyi graphs. The following theorem is the ` Section 5. Theorem 3. Under the notation and assumptions of Theorem 2, there exists a constant such that if additionally then there exist constants kbπ(t) − π where γ As before for Theorem 2, let us interpret Theorem 3 for the static setting where t = t t∈ T , and only G large enough, then w.h.p, the ` Hence if b(t) = O(1) then the bound is O( Chen et al. [7, Theorem 3]. Let us now denote so that b that leads to a ` kbπ(t) − π(t)k= O(T). This matches the rate for the pointwise risk for estimating univariate Lipschitz functions (see for e.g. [24, Theorem 1.3.1]). Indeed, the problem is then the same as the setting where the comparison graph is ∪G, and we observe (a potentially diﬀerent number of) i.i.d pairwise outcomes for each given edge in this graph. In this case, the corollary states that provided T is large enough, the `error is .. This is logically faster than the Tnonparametric rate, and is analogous to the optimal `bound for Erd¨os Renyi graphs in the static setting (see [7, Theorem 9]). (t) := (1 +max{b(t),√}) and b(t) is as in (2.5). Corollary 2. Under the same notations as in Theorem 3, for all t ∈ [0, 1] suppose that n & log n, pis as in (3.5) and b and d then with probability at least 1 −O(n kbπ(t) − π Let us make the following useful observations. We now describe the main ideas that lead to the ` in three steps following the ideas in [7]. kbπ(t) − π (i) The ﬁrst step is easy to verify, due to the deﬁnition of the norm k.k (ii) This is given by the combination of Lemmas 1, 2 and 3 which in turn are derived using [7, ) = O(1) for some t∈ T then it is not diﬃcult to verify that this implies b(t) = O(1) for each t∈ T (and hence b= O(1)), due to Assumption 1. Additionally, this implies that γ(t) = O(1). enough, then w.h.p kbπ(t) − π(t)k= O(T). This matches the rate for the pointwise risk for estimating univariate Lipschitz functions (see for e.g. [24, Theorem 1.3.1]). Theorem 8] and [23, Lemma 6]. bound kπ(t)∆(t)kand kπ(t)∆(t)k. The second term is completely deterministic and can be bounded using Assumption 1. A bound on kπ(t)∆(t)kis found following the same steps as in the proof of [6, Theorem 9]. A bound on kπ this theorem gives the bound provided that the following condition holds. First let us note that these eigenvalues are real, and so (4.4) is well deﬁned. Indeed, denoting Π(t) = diag(π to the reversibility of To prove (4.4), we will use results similar to [23, Lemma’s 3,4]. The main idea is to write the following decomposition where We now provide bounds on k∆ all results in this section are outlined in Appendix C. Lemma 1 (Bound on k∆ The proof of Lemma 1 follows from the smoothness condition in Assumption 1, while the proof of Lemma 2 follows the proof steps of [23, Lemma 3]. Next, we show that if ξ that G ensure (4.4). ¯P (t)) to be the second largest eigenvalue of¯P (t) in absolute value, i.e., bP(t) = EbP (t) whose entries are given by (t) is connected) and if the perturbation kbP (t) −¯P (t)kis suﬃciently small, then we can The statement is analogous to that of [23, Lemma 4]. The bound on 1 − λ the crucial statement, and requires using [23, Lemma 6]. We remark in passing that the dependence on b(t) is b b(t)). For completeness, we outline the proof of Lemma 3 in Appendix C. Condition (4.6) is ensured via Lemma’s 1 and 2 (with high probability) whenever (3.1) holds. Then, (2.1) readily implies that thus ensuring (4.4). Using (4.3) and (4.7) we ﬁnally obtain (4.1) as follows. kbπ(t) − π where the last inequality uses (2.1). Lemma 4. The statement follows directly from Lemma 1. The proof of Lemma 5 follows the ideas in the proof of [7, Theorem 9]. Applying these bounds in (4.8) ﬁnally leads to the stated bound in Theorem 1. (t) is the Laplacian of G(t)) and b(t) := max=. If ξ(t) > 0 and d(t) ≥ (t) in Lemma 3, we could not verify the dependence stated in [23, Lemma 6] (which is This theorem follows directly from Theorem 1 and from the propreties of Erd¨os-Renyi graphs gathered in Lemma 11. Using (3.4) and Lemma 11 along with the choice d with probability at least 1 − O(n Hence, condition (3.1) is satisﬁed with high probability and Theorem 1 implies that Again, using Lemma 11, we can simplify the above bound so that with probability at least 1 − O(n Since p p(t) & 1 (due to Proposition 1), thus satisfying the requirements of Theorem 2. Additionally, δ is required to satisfy δ ≤ T , and also condition (3.4), i.e., If δ satisﬁes the three aforementioned conditions, and if n & log n, we have with probability at least 1 − O(n The optimal choice of δ ∈ (0, T ] that minimizes the RHS of (4.11) is easily veriﬁed to be Now it remains to ensure that δ and condition (4.10) are equivalent to the stated conditions on T in the corollary. Hence provided δ  δ corollary. kbπ(t) − π(t)k≤ 32Mδ|E(t)|d(t)b(t)+ 8˜Cb(t)N(t)d(t). kπ(t)kT ξ(t)d(t)d(t)ξ(t)d(t)LN(t) kbπ(t) − π(t)k≤ 1536Mδnp(t)b(t)+ 64˜Cb(t)3p(t)np(t) (t) ≥ pδ, therefore the condition δ &implies p(t) & log n, as well as , T satisﬁes the stated conditions, and n & log n, we arrive at the stated `bound in the The main goal of this section is to present the steps of the proof of Theorem 3. We will follow the steps of the analysis carried out by Chen et al. [7], and adapt it to our setting. The proofs of all results from this section are provided in Appendix D. Let us ﬁrst focus on bounding |I the occurrence of some statistical dependencies therein. Since bounded using Hoeﬀding’s inequality. Lemma 6. Suppose that np Lemma 11. Then there exists a constant C Lemma 8. Recall that b clog n for constants c holds with probability at least 1 −O(n The last term to bound is more diﬃcult to handle due to the statistical dependency between bπ(t) and matrix (t) − π(t) =bπ(t)bP (t)−π(t)¯P (t) |I| ≤1 −112b(t)+ Clog nLnp(t)p(t)kbπ(t) − π(t)k bP (t). The idea is then to use the same “leave-one-out” trick as in [7] and introduce a new bP(t) with entries given by (for all i 6= j) Here, the m the union graph G vector is now statistically independent of the connectivity and the pairwise comparison outputs involving the m decompose the last term as Lemma 9. Suppose that np Lemma 11. Then there exist constants C then it holds with probability at least 1 − O(n kbπ Lemma 10. Suppose that np Lemma 11; n ≥ c 2 holds. Then there exist constants C we have for all m ∈ [n], bπ(t) − π(t)P(t) =bπ(t) − bπ(t)P(t)+bπ(t) − π(t)P(t). (t)) due to Lemma 11. This is shown in the following lemma. (t) − bπ(t)k≤ 192b(t)4MδT+ Clog nLnp(t)p(t)kπ(t)k+ kbπ(t) − π(t)k. | ≤ 192b(t)p4MδT+ Clog nLnp(t)p(t)kπ(t)k+1pkbπ(t) − π(t)k. As seen in Section 5, the bound in Theorem 3 follows from the combination of the bounds on depending on kπ written as where β < 1, which in turn implies Concerning the terms in α, one can divide them in two groups depending on whether they depend on T or not. The sum of the terms depending on T is for some constant for some constant Combining (5.6) and (5.7), we readily arrive at the stated bound in Theorem 3. , I, I, and I. Note that we can identify two types of terms in these bounds – those 1 − β =1 −1 −112b(t)+ Clog nLnp(t)p(t)+1p+ Clog nnp(t) Lnp(t)p(t)3Lp(t)np(t)Lnp(t)p(t) ≤˜C1 +b(t)√maxb(t),log nplog nLnp(t)p(t)( since np(t) ≥ log n) If δ satisﬁes these assumptions and if n & log n, we have with probability at least 1 −O(n The optimal choice of δ ∈ (0, T ] minimizing the RHS of (5.8) is given by It now remains to check that δ satisﬁes the stated conditions and n & log n, we arrive at the stated expression for the ` bound in the corollary. We now empirically evaluate the performance of our method via numerical experiments thetic data, and on a real dataset. We will in particular compare our method with the MLE approach of Bong et al. [2], as it is for now the only other method we are aware of that theoretically analyzes a ‘smoothly evolving’ dynamic BTL model as us. The synthetic data is generated as follows. 1. For i ∈ [n], we simulate the strength of item i across the grid w 2. For all t ∈ T , we simulate an Erd¨os-Renyi comparison graph G(n, p(t)) with p(t) chosen (t) & 1. Moreover, δ has to statisfy δ ≤ T and condition (3.6), i.e. and condition (3.6) are ensured for the stated conditions on T . Hence, for δ  δ, if T gaussian process GP (µ, Σ) as in [2]. • µ= (µ(0), . . . , µ(T )) with µ(t) ∼ N(0, 0.1) for all t ∈ T , and • Σis a Toeplitz symmetric matrix, with its coeﬃcients in the ﬁrst row deﬁned as Σ= We then deﬁne w= exp (GP (µ, Σ)) ∈ Rfor each i ∈ [n]. randomly from the interval [,]. We check that the union graph of all the data on the grid T is connected. Indeed, it is a suﬃcient condition for the existence at all times t ∈ [0, 1] of a δ such that the union graph G(t) is connected (which is required for the ranking recovery). 3. For all t ∈ T , for all 1 ≤ i < j ≤ n and for all l ∈ [L], we draw the outcomes of the Starting with an initial value of δ as in Corollary 1 (δ ≈ T graph G This process is repeated over 30 Monte Carlo runs. Apart from the MLE approach [2], we also evaluate against an adaptation of the simple Borda Count method from the static setting to the dynamic setup. Borda Count. This method, analysed by Ammar and Shah [1] in the static case, gives a score to each item based on its win rate. To estimate the scores at time t in our dynamic setup, we compute the win rate of each item i using the neighborhood N The scores (s(t, i)) item, the better its rank). Ranking error. In order to compare the rankings produced by these three methods, we compute for all them an estimation error with the error metric deﬁned by Negahban [23]. Let π denote the normalized true weight vector and σ denote an estimated ranking, with σ better than j. The error metric is then deﬁned It has been shown in [23, Lemma 1] that this error criterion is related to the ` bounded in Theorem 1. If the ranking σ comes from a weight vector bπ, then Although (6.1) doesn’t necessarily require bπ to satisfy kbπk since bπ will be the strength estimates returned by the methods being compared. Interpreting the results. In Figure 1, we consider T ranging from 10 to 150, and ﬁx n = 100, L = 5. Figure 1a shows the evolution of the mean ranking error D for each T the mean is taken across all time instants in T , and all Monte Carlo runs. This is shown for our Algorithm 1 (dubbed DRC for Dynamic Rank Centrality), as well as the MLE and Borda Count methods, with σ, π denoting the estimated ranks and weights by these algorithms. In Figure 1b, we plot the mean ` not designed for recovering the latent weight vector w with T for both DRC and MLE, which is consistent with the theoretical results for these methods developed in the present paper, and in [2]. Using (6.1), this implies that D with T which is what we observe in Figure 1a. One can note that both MLE and DRC have similar comparisons as y(t) ∼ Band deﬁne y(t) = 1 − y(t). (t) is connected. We then recover the weight vectors wfor all t ∈ T using Algorithm 1. performance for both error criteria. The Borda Count method performs well for rank recovery, as its error is only slightly worse than the other methods for n = 100. The error bars in Figure 1b illustrate that the variance of the errors typically decreases with T . In Figure 2, we show the evolution of the errors for n = 400 with the other parameters chosen as in Figure 1. We note that the observations made for the case n = 100 still hold and the error curves are lower than those for n = 100. Moreover, the variance of the errors in Figure 2b are slightly smaller than in Figure 1b. Figure 1: Evolution of estimation errors with T for Dynamic Rank Centrality, the MLE and Borda Count method for n = 100. The results are averaged over the grid T as well as 30 Monte Carlo runs. Figure 2: Evolution of estimation errors with T for Dynamic Rank Centrality, the MLE and Borda Count method for n = 400. The results are averaged over the grid T as well as 30 Monte Carlo runs. We now evaluate our method on a real dataset which consists of the results of National Football League (NFL) games for each season between 2009 and 2015, that are available in the nflWAR package [30]. The aim is to recover the ranking of the n = 32 teams at the end of each season, which contains T = 16 rounds. The dataset is hence composed of 16 comparison graphs with 32 nodes each, and comparison outcomes (y in round t. We ﬁt our model to this data by estimating the underlying strengths (and therefore the ranks) of the teams, at the end of a season. To do so, we tune the parameter δ using a Leave-One-Out Cross-Validation (LOOCV) procedure, described below. 1. Fix a list of potential values of δ and compute for each of them the associated estimates of 2. For every possible values of δ, repeat the following steps several times. An analogous LOOCV procedure is performed in the MLE approach [2]. We then compare our estimated ranking with the ones obtained by the MLE method, the Borda Count method and also with the ELO ratings. The latter are reputed to be relevant estimations of the teams qualities and are also openly available [22]. Figure 3 contains the estimation of the top 10 teams by all these methods, for the seasons 2011 to 2015. Considering the ELO ranks as the true ranks, one can observe that the DRC and MLE methods perform similarly, as a majority of the top 10 teams are recovered for each season. The Borda Count does not perform as well as it did on synthetic data; We can observe that it recovers the same ranks for several teams. This is explained by the fact that in this dataset, each team plays a small number of games, and thus the win rates take a ﬁnite (and small) number of values. However, it still recovers a large fraction of the top 10 teams. We now provide a detailed discussion with closely related work for dynamic ranking, and conclude with future directions for research. As mentioned in Section 1, existing theoretical results for the dynamic ranking setup are limited, and the only works we are aware of are the recent results of Bong et al. [2] and of Li and Wakin [15]. We now discuss these two results in more detail. the strength bπ at the end of the season. • Select randomly a game during the season, identiﬁed by a time t and the pair of compared teams {i, j}. • Consider the dataset obtained by removing the outcome y(t) of this game. Use this dataset to compute the estimated strength ˜π(t) at this time t. • Compute the prediction error ky(t) −k. We then compute the mean of these prediction errors for each value of δ. of the strengths at the end of the season the associated vector bπ. Comparison to Bong et al.[2]. The dynamic BTL model proposed by Bong et al. [2] is closely related to the one we presented in Section 2. They consider the logit version of the BTL model where β with our model are that (a) the grid T can be non-uniform, and (b) the number of comparisons L(t) made for each pair {i, j} at each time t ∈ [0, 1] can vary. We assumed the grid T to be uniform only for simplicity, our analysis can be easily extended to handle the non-uniform setting as long as T is “suﬃciently regular”. Similarly, one can easily extend our analysis to handle varying number of comparisons L in the proofs as is typically done for the static setting (see for eg., [6, Remark 3]). The pairwise comparison data at each time t they smooth the data using a kernel function; this is analogous to (weighted) averaging the data in a suitable neighborhood. More precisely, to estimate the ranks at time t, they ﬁrst compute the smoothed data as where W Figure 3: The top 10 teams for seasons 2011 to 2015, using ELO ranks, DRC, the MLE, and Borda Count. Teams highlighted in yellow for a particular recovery method are teams appearing in the top 10 list for ELO rankings. Teams highlighted in green are recovered at the same rank as in the ELO rankings. (t) represents the vector of scores at time t with w(t) = exp(β(t)). The main diﬀerences ) is the number of times i beat j at time t. In order to use the temporal aspect of the data, is a kernel function with bandwidth h. Then, βis estimated by minimizing the negative log-likelihood, i.e., using a proximal gradient descent algorithm. If the matrix matrix of a weighted directed graph, then the strong connectivity of this graph is suﬃcient for the unique existence of the solution of (7.1). The main assumptions needed for their theoretical analysis are the following. • The probabilities P(i beats j at time t) are Lispchitz functions of time t ∈ [0, 1] for all i 6= • Each pair of teams {i, j} has been compared at least at one time point t We remark that this last assumption is a stronger assumption than the connectivity assumption on the union graph G Bong et al. derive bounds in the ` denoting that with high probability, where δ same number of games at all times. However, the dependence of δ that they also recover the T they also derive a rate for the uniform error. For h & that with high probability, While we provide pointwise estimation error bounds for any given t ∈ [0, 1], it is also possible to extend our results to obtain error bounds holding uniformly over all t ∈ [0, 1]. The ﬁrst main idea here would be to observe that there are O(T) diﬀerent number of neighborhoods in the construction ofbP (t) in (2.7), which implies that there are O(T ) diﬀerent possible values of bπ(t) over all t ∈ [0, 1]. Secondly, one can verify that π union bound argument can be used to establish ` with probability at least 1 −O(T n taken c to be 10 (resp. 9) for the ` the expense of worsening the other constants in the accompanying theorems in Section 3. Comparison to Li and Wakin [15]. The model introduced by Li and Wakin [15] aims at recovering a pairwise comparison matrix X(T ) at a time T from noisy linear measurements of the matrices X(t) for t ∈ [T ]. For each pair of items {i, j}, X j ∈ [n] (same as Assumption 1). our notation, this means that the union graph ∪Gis complete. bβ(t) to be the MLE estimator (i.e., the solution of (7.1)), it is shown [2, Theorem 5.2] (t) is denoted to be a discrepancy parameter in [2], and is small when all the teams play the over item i at time t with X X(t) to be the goal diﬀerence during a football game between the teams j and i at time t. Note that these data contain more information than the simple binary outcomes we use in the dynamic BTL model. A speciﬁcity of this model is that the preferences over the collection of items are supposed to depend on 2r latent factors, captured in the matrices S model considered in [15] assumes that The matrix Q contains information on factors that do not depend on time. The time-dependent data are contained in the matrix S where E model comes from the case of a single factor (r = 1) with Q ∈ R case, for each pair of items {i, j}, The outcomes of the comparisons then, as in the BTL, only depend on the strength of each item at time t. Then by recovering the matrix X(T ), one can subsequently also derive a ranking of the items (see for eg., [8]). Denoting M to be the number of measurements available at each time t, they show a bound on the estimation error kX(T ) − where one can then derive a ranking of the items from this estimated comparison matrix For future work it would be interesting to extend our study to other models. Indeed, the BTL model is classic for the ranking problem but suﬀers from some limitations. 1. Independence. One of the main assumptions of the dynamic BTL model we consider is the 2. Home eﬀect. Another model, used in the study of sports tournaments by Cattelan et al. [5] 3. Non-parametric dynamic models. Considering a parametric model as the BTL model has is an innovation matrix with i.i.d. centered Gaussian entries. The intuition behind this bX(T ) is computed as the solution of an optimization problem. It is however unclear how independence of the outcomes for all comparisons at a given time t, and the independence across diﬀerent time points. The latter assumption can in particular be questioned, as the choices of one user across time are typically not going to be independent. It would be interesting to model these dependencies across time, for instance, by modelling them as a Markov process. adapts the BTL to include a home eﬀect, which is beneﬁcial to the hosting team. Indeed, because of the familiar environment and the supporting public, a team is more likely to win if they play in their stadium. It would be interesting to theoretically analyze such a model. limitations as it assumes that the preference outcomes depend only on one parameter w, which can be seen as a strong transitivity constraint [27]. That is why one can instead use non-parametric models as described in [27, 28, 25], where only a certain transitivity assumption is made on the comparison matrix. In particular, the BTL model belongs to this class of models. It has been shown by Shah et al. [27] that in the static case, the matrix of probabilities can be estimated at the same rate as in a parametric model. As such extensions can be considered in the static setting, it would be interesting to adapt these models to the dynamic case and study their performance.