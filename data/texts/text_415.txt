Abstract—With the growth of the academic engines, the mining and analysis acquisition of massive researcher data, such as collaborator recommendation and researcher retrieval, has become indispensable. It can improve the quality of services and intelligence of academic engines. Most of the existing studies for researcher data mining focus on a single task for a particular application scenario and learning a task-speciﬁc model, which is usually unable to transfer to out-of-scope tasks. For example, the collaborator recommendation models maybe not be suitable to solve the researcher classiﬁcation problem. The pre-training technology provides a generalized and sharing model to capture valuable information from enormous unlabeled data. The model can accomplish multiple downstream tasks via a few ﬁnetuning steps. Although pre-training models have achieved great success in many domains, existing models cannot be directly applied to researcher data, which is heterogeneous and contains textual attributes and graph-structured social relationships. In this paper, we propose a multi-task self-supervised learning-based researcher data pre-training model named RPT, which is efﬁcient to accomplish multiple researcher data mining tasks. Speciﬁcally, we divide the researchers’ data into semantic document sets and community graph. We design the hierarchical Transformer and the local community encoder to capture information from the two categories of data, respectively. Then, we propose three self-supervised learning objectives to train the whole model. For RPT’s main task, we leverage contrastive learning to discriminate whether these captured two kinds of information belong to the same researcher. In addition, two auxiliary tasks, named hierarchical masked language model and community relation prediction for extracting semantic and community information, are integrated to improve pre-training. Finally, we also propose two transfer modes of RPT for ﬁne-tuning in different scenarios. We conduct extensive experiments to evaluate RPT, results on three downstream tasks verify the effectiveness of pre-training for researcher data mining. Index Terms—pre-training, contrastive learning, Transformer, graph representation learning With the pervasiveness of digital bibliographic search engines, e.g., Google Scholar, Aminer, Microsoft Academic Search, and DBLP, efforts have been dedicated to mining scientiﬁc data, one of the main focuses is researcher data mining, which aims to mine researchers’ semantic attributes and community relationships for important applications, including: collaborator recommendation [1], [2], academic network analysis [3]–[5], and expert ﬁnding [6], [7]. Millions of researchers and relevant data have been added to digital bibliographic datasets every year. The scientiﬁc research achievements and academic cooperation of researchers saw a continuation in the upward trend. However, different researcher data mining tasks usually choose particular features and design unique models, and minimal works can be transferred to out-of-domain tasks. For example, previous studies are more inclined to use graph representation learning-based models to explore the researcher community graphs in the collaborator recommendation task. In researchers’ research ﬁeld classiﬁcation task, their publications are more valuable features, and the semantic models are more often used. Hence, when multiple mining tasks on a tremendous amount of researcher data, it would be laborious for feature selection and computationally expensive to train different task-speciﬁc models, especially when these models need to be trained from scratch. Also, most researcher data mining task need labeled data, which is usually quite expensive and time-consuming, especially involving manual effort. The deﬁciency of labeled data makes supervised models are easily over-ﬁtting. At the same time, the unlabelled graph data is usually easily and cheaply collected. Inspired by these, we aim to exploit the intrinsic data information disclosed by the unlabeled researcher data to train a generalized model for researcher data mining, which is transferable to various downstream tasks. The pre-training technology is a proper solution, which has drawn increasing attention recently in many domains, such as natural language processing (NLP) [8], [9], computer vision (CV) [10], [11], and graph data mining [12], [13]. The idea is to ﬁrst pre-train a general model to capture useful information on a large unlabeled dataset via self-supervised learning, and then, the pre-trained model is treated as a good initialization of the downstream tasks, further trained along with the downstream models for different application tasks with a few ﬁne-tuning steps. The pre-training model is a transferable model and convenient to train because the unlabeled data is easily available. In ﬁne-tuning, the downstream models can be very lightweight. Thus, this sharing mechanism of pre-training models is more efﬁcient than independently training various task-speciﬁc models. Early attempts to pretraining mainly focus on learning word or node representation, which optimizes the embedding vectors by preserving some similarity measure, such as the word co-occurrence frequency in texts and the network proximity in graphs, and directly uses the learned embeddings for downstream tasks. However, the embeddings are limited in preserving extensive information in large datasets. In contrast, recent studies consider a transfer setting, and the goal is to pre-train a generic model which can be applied to different downstream tasks. There have been some representative pre-training models which achieved great performance in their areas. Pre-training was ﬁrst successfully applied to CV by ﬁrstly pre-training the model on a large supervised dataset such as ImageNet [14], and then ﬁnetuning the pre-trained model in a downstream task or directly extracting the representations as features. SimCLR [11] utilize data augmentation technology to Enhance the generalization ability of the model, and MoCo [10] propose a pre-training mechanism for building dynamic dictionaries for contrastive learning, which leverages instance discrimination as a pretext task via momentum contrast. The language pre-training model BERT [8] is designed to pre-train deep bidirectional representations from the unlabeled text by two self-supervised text reconstructing tasks, and it obtains state-of-the-art results on eleven NLP tasks. The graph pre-training model GCC [12] can capture the universal network topological properties across multiple networks based on contrastive learning and achieve high performance on graph representation learning. Inspired by these improvements, we aim to conduct pretraining on researcher data for mining tasks. The study in [15] has proved the effectiveness of domain data for pre-training, and some work has leveraged pre-training models on academic domain data. For example, SciBERT [16] leverages BERT on scientiﬁc publications to improve the performance on downstream scientiﬁc NLP tasks. SPECTER [17] introduces the citation information into the pre-training to learn documentlevel embeddings of scientiﬁc papers. However, existing pretraining models can not be directly applied to researcher data pre-training. Because the researcher data is more heterogeneous, including textual attributes (e.g., proﬁles, publications, patents, etc.) and graph-structured community relationships (e.g., collaborating, advisor-advisee, etc.). In contrast, most present models can only deal with a speciﬁc type of data. Also, to extract information from the heterogeneous researcher data, the pre-training model should be more complex and heavyweight than traditional ones, which brings new challenges to the pre-training of models on large-scale data. In this paper, we leverage the idea of multi-task learning and self-supervised learning to design the Researcher data PreTraining(RPT) model. Speciﬁcally, for each researcher, we propose a hierarchical Transformer as the textual encoder to capture semantic information in researcher textual attributes. We also propose a linear local community sampling strategy and an efﬁcient graph neural network (GNN) based local community encoder to capture the local community information of researchers. To leverage optimization on these two encoders, the main task of RPT is a contrastive learning model to discriminate whether these two captured information belongs to the same researcher. We also design two auxiliary tasks, Hierarchical Masked Language Model (HMLM) and Community Relation Prediction (CRP), respectively, to extract token-level semantic information and link-level relation information to improve the ﬁne-grained performance of pre-training. Finally, we pre-train RPT on a big unlabeled researcher dataset extracted from real-world scientiﬁc data. We set two transfer modes for RPT and ﬁne-tune RPT on three researcher data mining tasks to verify the effectiveness and transferability of RPT. We also conduct model analysis experiments to evaluate different components and hyper-parameters of RPT. The contributions of our paper are summarised as follows: 1) We introduce the pre-training idea to handle the multiple mining tasks on abundant and heterogeneous researcher data. We propose the RPT framework, including the pretraining model and two ﬁne-tuning modes, which can extract and transfer useful information from big unlabeled scientiﬁc data to beneﬁt researcher data mining tasks. 2) To perform the pre-training model in considering heterogeneity, generalization, scalability, and transferability, we propose a Transformer, GNN based information extractor, and a multi-task self-supervised learning objective including a hierarchical masked language model on the hierarchical Transformer, a relation prediction model on the local community encoder, and a global contrastive learning model overall to capture both the semantic features and local community information of researchers. 3) We apply our pre-training model on real-world researcher data, which is extracted from DBLP, ACM, and MAG digital library. Then, we ﬁne-tune the pretrained model on three tasks: researcher classiﬁcation, collaborator prediction, and top-k researcher retrieval to evaluate the effectiveness and transferability of RPT. The results demonstrate RPT can signiﬁcantly beneﬁt various downstream tasks. We also perform ablation studies and hyper-parameters sensitivity experiments to analyze the underlying mechanism of RPT. The increasing availability of digital scholarly data offers unprecedented opportunities to explore the structure and evolution of science [18]. Multiple data sources such as Google Scholar, Microsoft Academic, ArnetMiner, Scopus, and PubMed cover millions of data points pertaining to researchers(also known as scholars and scientists) community and their output. Analysis and mining based on big data technology have been implemented on these data and the analyses of researchers have been a hot topic. The researcher data analysis tasks including collaborator recommendation [19], [20], collaboration sustainability prediction [2], [21], reviewer recommendation [22], [23], expert ﬁnding [24], [25], advisoradvisee discovery [26], [27], academic inﬂuence prediction [19], [28], etc. Mainstream works focus on mining the various academic characteristics and community graph properties of researchers, then learn task-speciﬁc researcher representations for various tasks. For example, here are some recent representative works, [1] recommends context-aware collaborator for researchers by exploring the semantic similarity between researchers’ published literature and restricted research topics. [2] use researchers’ personal properties and network properties as input features to predict the collaboration sustainability. [6] propose an expert ﬁnding model for reviewer recommendation, which learn hierarchical representations to express the semantic information of researchers. [5] propose a network representation learning method on scientiﬁc collaboration networks to discover advisor-advisee relationships. [29] study the problem of citation recommendation for researchers and use the generative adversarial network to integrates network structure and the vertex content into researcher representations. [30] incorporate both network structures and researcher features into convolutional neural and attention networks and learn representations for social inﬂuence prediction. [31] study the problem of top-k similarity search of researchers on the academic network, the content and meta-path based structure information is embedded into researcher representations. Self-supervised learning is a form of unsupervised learning which aims to train a pretext task where the supervised signals are obtained by data itself automatically, it can guide the learning model to capture the underlying patterns of the data. The key of self-supervised learning is to design the pretext tasks. In the area of computer vision (CV), various self-supervised learning pretext tasks have been widely exploited, such as predicting image rotations [32], solving jigsaw puzzles [33], and predicting relative patch locations [34]. In natural language processing (NLP), many works propose pretext tasks based on language models, including the context-word prediction [35] the Cloze task, the next sentence prediction [8], [9] and so on [36]. For graph data, the pretext task are usually designed to predict the central nodes given node context [37], [38] or subgraph context [39], or maximize mutual information between local and global graph [40], [41]. Recently, many works [42]–[45] on different domains has integrated self-supervised learning with multi-task learning, i.e., joint training multiple self-supervised tasks on the underlying models, which can introduce useful information of different facets and improve the generalization performance. With the idea of self-supervised learning, pre-training models can be applied on big unlabeled data to build more universal representations that work across a wider variety of tasks and datasets [46]. The pre-training models can be classiﬁed as feature-based models and end-to-end models. Early pre-training studies are mainly feature-based, which directly parameterizes the entity embeddings and optimizes them by preserving some similarity measure. The learned embeddings are used as input features, in combination with downstream models to accomplish different tasks. For example, Word2vec [35], [47], Glove [48] and Doc2vec [49] in NLP, which are optimized via textual context information. Early graph pre-training models are similar to NLP, like Deepwalk [50], LINE [51], node2vec [52] and metapath2vec [4], which aim to learn node embeddings to preserve network proximity or graph-based context. Differently, recent pre-training models pre-train deep neural network-based encoders and ﬁne-tune them along with downstream models in end-to-end manner. Typical examples includes: MoCo [10] and SimCLR [11] for unlabeled image dataset; BERT [8], RoBERTa [53] and XLNet [9] for unlabeled text dataset; GCC [12] and GPTGNN [13] for unlabeled graph dataset. Our proposed RPT is a meaningful attempt to apply a pre-training model on domain-speciﬁc and heterogeneous data as the researcher data is scientiﬁc data and contains researcher textual attributes and researcher community. To leverage pre-training models on big unlabeled researcher data, we need to design proper self-supervised tasks to capture the underlying patterns of the data. The key idea of selfsupervised learning is to automatically generate supervisory signals or pseudo labels based on the data itself. Given the raw data of N researchers, denoted by A = {a, a, ..., a}, we ﬁrst explore the researcher data and extract two categories of researcher features: (1) semantic document set and (2) community graph, for pre-training. Semantic Document Set. A researcher may have multiple textual features, including published literature, patents, proﬁles, curriculum vitae(CV), and personal homepage, and these features may contain rich semantic information with various lengths and different properties. We collect the text information of these features as documents and compose these documents of each researcher together to a organized semantic document set. Formally, the semantic document set of researcher a∈ A is expressed as D= {d, d, ..., d}, where every document is formed as a token sequence d= {t, t, .., t}. Noted that each semantic document set D and each document dmay have arbitrary lengths. Community Graph. Besides the semantic features, the social communications and relationships are also signiﬁcant for researcher and have been widely utilized to analysis in previous studies, which can be expressed as graph-structured data. As the relations between researcher may have different types, We construct the researcher community graph in a heterogeneous graph manner, expressed as G = {(a, r, a)} ⊆ A × R × A, where (a, r, a) represent one link between researchers, R is the relation set, and r∈ R. Multiple types of relations between researchers are automatically extracted based on the original data to construct G. For example, we can make the rules that if two researchers coauthor a same paper, there is a relation named Collaboraing between them, if two researchers work for the same organization, there is a relation named Colleague between them. Noted that two researchers can have multiple relations of the same type in G. For instance, if they collaborate on n papers, there would be n Collaboraing relations between them. Researcher Data Pre-Training. Formally, the problem of researcher data pre-training is deﬁned as: given N researchers A = {a, a, ..., a}, their semantic document sets D = {D, D, ..., D}, and their community graph G = {(a, r, a)} ⊆ A ×R×A, we aim to pre-train a generalized model via self-supervised learning, which is expected to capture both the semantic information and the community information of researchers into low-dimensional representation space, where we hope the researchers with similar researcher topics and academic community are close with each other. Then, in ﬁne-tuning, the learned model is treated as a generic initialized model for beneﬁting various downstream researcher mining tasks and is optimized by the task objectives via a few gradient descent steps. Formally, in the pre-training stages, the pre-training model is expressed as f= Ψ(θ; D, G) and the output is researcher representations Z = {z, z, ..., z}. Let Lbe the self-supervised loss functions, which extract samples and pseudo-labels in researcher data D and G for pretraining. Thus, the objective of pre-training is to optimize the following: Based on the motivation described above, the pre-training model foptimized by objective Lshould have the following properties: from multi-type textual document data and community information from heterogeneous graph data. pre-training should integrate heterogeneous information from massive unlabeled data into generalized model parameters and researcher representations. contains rich information, the model should be heavyweight enough to extract the information on the one hand. On the other hand, it needs to be friendly to mini-batch training and parallel computing. model should be compatible with researcher document features and community graph in the downstream tasks. As such, the pre-trained model fcan be adopted on multiple researcher mining tasks. Noted that the focus of this work is on the practicability of the pre-training framework on the researcher data. The goal is to make the model satisfy the above properties, making it different from the common text embedding model or graph embedding model research. Framework Overview. Figure 2 shows the architecture of the proposed RPT framework, which consists of three components: (a) the hierarchical Transformer, (b) the local community encoder, and (c) the multi-task and self-supervised learning objective. Speciﬁcally, for Heterogeneity, the hierarchical Transformer is a two-level text encoder, including the Document Transformer and Researcher Transformer, which aims to extract the semantic information in researchers’ semantic document sets. The local community encoder employs a linear random sampling strategy to sample the researcher’s local communities and conduct a GNN encoder to capture the community information. For Generalization, we design a multi-task and self-supervised learning objective by automatically extracting supervised signals from unlabeled data. The self-supervised tasks include global contrastive learning, hierarchical masked language model, and community relation prediction, which are proposed for integrating the information from researcher data into generalized model parameters and researcher representations. For Scalability, the model adapt Transformer model, which encode long-sequencial text in a parallel manner, and sub-graph sampling based community encoder, which encode graph data via mini-batch GNN propagation on sub-grpahs with parallel computing. For Transferability, the proposed model is transferable for multiple downstream researcher mining tasks. Also, we propose two ﬁne-tuning modes in different applying scenarios. The Transformer model is the state-of-the-art text encoder, which has demonstrated very competitive ability in combination with language pre-training models [8], [53]. Comparing with RNN-like models as text encoder, Transformer is more efﬁcient to learn the long-range dependencies between words. Also, RNN-like models extract text information via sequentially propagation on sequence, which need serial operation of the time complexity of sequence length. While self-attention layers in Transformer connect all positions with constant number of sequentially executed operation. Therefore, Transformer encode text in parallel. A Transformer usually has multiple layers. A layer of Transformer encoder (i.e, a Transformer block) usually consists of a Multi-Head Self-Attention Mechanism, a Residual Connections and Layer Normalization Layer, a Feed Forward Layer, and a Residual Connections and Normalization Layer, which can be written as: H = LayerN orm(X+ M ultiHead(X where X= [x, x, ..., x] is the input sequence of l-th layer of Transformer, s is the length of input sequence, x∈ Rand d in the dimension. LayerNorm is layer normalization, MLP denotes a two-layer feed-forward network with ReLU activation function, and MultiHead denotes the multihead attention mechanism, which is calculated as follows: MultiHead(X) = Concat(head, ..., head)W(4) where W, W, W∈ Rare weight metrics, The h outputs from the attention calculations are concatenated and transformed using a output weight matrix W∈ R. In summary, Given the input sequence embeddings X= [x, x, ..., x], after the propagation of Equation 2 and 3 on a L-layers Transformer, formulized as X= T ransformer(X), we can obtain the ﬁnal output embeddings of this sequence X= [x, x, ..., x], where each embedding has contained the context information in this sequence.The main hyper-parameters of a Transformer are the number of layers (i.e., Transformer blocks), the number of self-attention heads, and the maximum length of inputs. For the semantic document set D= {d, d, ..., d} of researcher a, we aim to use the Transformer proposed in [54] to encode the text information in Dinto the researcher representation u. Considering that each researcher may have multiple documents with rich text and documents may have different properties and should be processed separately, while the original Transformer can only handle the inputs of a single sentence. Thus, we propose a two-level hierarchical Transformer model, which consists of a Document Transformer to ﬁrst encode the text in each document into a document representation, and a Researcher Transformer to integrate multiple documents into the researcher representations. Document Transformer. For each document d∈ D, suppose its token sequence is {t, t, .., t} and the corresponding token embeddings are {t, t, .., t}. we deﬁne a new token named [SOD] (Start Of Document) and random initialize its embedding t, then we concatenate it with the embedding sequence of dinto the Document Transformer, which is a multi-layer bidirectional Transformer, and take the ﬁnal output of [SOD] as the document representation: d= [t, t, t, .., t] = T ransformer(t, t, t, .., t) where dis the d’s representation, T ransformer() is the forward function of Document Transformer, Lis the number of layers, and [t, t, t, .., t] is the output. The input token embeddings are initialized by Word2vec [47], we collect all the texts in the dataset to train the Word2vec model. Also, for each token, its input representation is constructed by summing its embedding with its corresponding position embeddings in documents. Researcher Transformer. Then, given a’s semantic document set D= {d, d, ..., d}, we can obtain the documents’ representations {d, d, ..., d} from the Document Transformer and input them into the Researcher Transformer, which is yet another multi-layer bidirectional Transformer but applied on document level, followed with a mean-pooling layer: u= P ooling([d, d, .., d]) = P ooling(T ransformer(d, d, , .., d)) where uis the semantic representation of researcher a. Pooling() represent the average of all documents’ ﬁnal outputs. T ransformer() is the forward function of Researcher Transformer, Lis the number of layers, and [d, d, d, .., d] is the output. Thus, in this Researcher Transformer, for each researcher’s documents set, each document in the set can collect information from other documents with different attention weights contributed by the self-attention mechanism of the Transformer. So that we can obtain context-aware and researcher-speciﬁc document outputs for different researchers, rather than assuming a document (e.g., a paper) has the same contribution to different owners. Figure 2 shows the architecture of the proposed hierarchical Transformer, where the Document Transformer is shared by different document inputs and the Researcher Transformer is shared by different researcher inputs. Given the researcher community graph G = {(a, r, a)} ⊆ A ×R×A, ﬁrst we aim to extract community information of researchers from this graph. Recently, Graph Neural Networks (GNNs) have achieved state-of-the-art performance on handling graph data. Typically, GNNs output node representations via a message-passing mechanism, i.e., they stack K layers to encode node features into low-dimensional representations by aggregating K-hop local neighbors’ information of nodes. However, most GNN based models take the whole graph as the input, which can hardly be applied on large-scale graph data due to memory limitation. Also, the inter-connected graph structure prevents parallel computing on complete graph topology, making the GNNs propagation on large graph data extremely time-consuming [39]. One prominent direction for improving the scalability of GNNs use sub-graph sampling strategies, For example, SEAL [55] extracts k-hop enclosing subgraphs to perform link prediction. GraphSAINT [56] propose random walk samplers to construct mini-batches during training. Thus, we propose to sample a sub-graph of G to represent the local community graph of researcher a, denoted by G, which can preserve the interactions and relations of a with other researchers. An intuitive way to sample Gis to directly sample a’s h-hops neighborhoods, e.g., to sample all a’s neighbors within h-hops as well as corresponding relations to compose G. However, the community graph of researchers usually is denser than other kinds of graphs, each researcher may have dozens of neighbors. In this sampling way, the size of Gmight grow geometrically and it would become expensive to process it in training when h increases. Linear Random Sampling. In our paper, we propose a linear random sampling strategy, which can make the number of sampled neighbors increase linearly with the number of sampling hops h. The procedure of linear random sampling is presented as follow: Algorithm 1: Procedure of linear random sampling. Output: The local community graph Gof a. = SampleOnehopNeighborhood(a, n, G); denoted by a; where the SampleOnehopN eighborhood(a, n, G) represents the process that randomly sampling n numbers of a’s 1-hop links, expressed as (a, r, a) ∈ G, to compose G, Gis the sampled local community within s-hop. In this procedure, we obtain a sub-graph of a’s h-hop neighborhood with n neighbors in each hop. This sampling strategy has the following advantages: (1) The size of local community graphs are linearly correlated to the number of sampling hops, so it would not increase the time complexity of computing community embeddings below when h increases. (2) The sampled neighbor size of each researcher is ﬁxed as h × n and neighbors with more links are more likely to be sampled. (3) Noted that we re-sample the local community graphs in each training step, so that all the links in the local community may be sampled after multiple training steps. (4) The sampling strategy can be seen as a data augmentation operation by masking partial neighbors, which is widely used in graph embedding models [39], [40], [57]. It can help to improve the generalization of models like the mechanism of Dropout [58]. GNN Encoder. Obtained the local community graph Gof a, we use GNN model to encode Ginto a community embedding. Traditional GNNs can only learn the representations of nodes by aggregating the features of their neighborhood nodes, we refer to these node representations as patch representations. Then, we utilize a Readout function to summarize all the obtained patch representations into a ﬁxed length graph-level representation. Formally, the propagation of L-layer GNN is represent as: dh= Aggregation(h: (a, r, a) ∈ G where 0 ≤ l ≤ L, his the output hidden vector of node aat the l-th layer of GNN and h= u,dh represents the neighborhood message of apassing from all its neighbors in Gat the l-th layer, Aggregation(·) and Combine(·) are component functions of the l-th GNN layer, and N(G) is the node set of G. After L-layer propagation, the output community embedding of Gis summarized on node representation vectors through the Readout(·) function, which can be a simple permutation invariant function such as averaging or more sophisticated graph-level pooling function [59]. As the relation between researchers may have multiple types, in practice,, we choose the classic and widely used RGCN [60], which can be applied on heterogeneous graphs, as the encoder of sub-graphs. Also, we use averaging function for Readout(·) in consider of efﬁciency. Substitutability of Local Community Encoder. It is worth mentioning that our method places no constraints on the choices of the local community’s sampling strategy and GNN encoder. Our framework is ﬂexible with other neighborhood sampling methods, such as random work with restart [61] and forest ﬁre [62]. Also, traditional GNN such as GCN [63], GraphSAGE [57], and other GNN models that can encode graph-level representations are available for local community graph encoding, such as graph isomorphism network [64], DiffPol [59] can work in our framework. The design of our framework mainly consider the heterogeneity of researcher community graph and the efﬁciency as the pre-training dataset is very large, the comparison of different sampling strategies and encoders is not the focus of this paper. In this section, we propose the multi-task self-supervised objective for pre-training, which consists of the main task: global contrastive learning, and two auxiliary self-supervised tasks: hierarchical mask language model and community relation prediction. 1) Global Contrastive Learning: We consider the strong correlation between a researcher’s semantic attributes and the local community to design a self-supervised contrastive learning task. The assumption is that given a researcher’s local community, we can use the community embedding to infer the semantic information of this researcher, based on the fact that we can usually infer a researcher’s research topics according to the community he/she belongs to, and vice versa. Obtained a researcher embedding uand the embedding cof one sampled local community, this task aim to discriminate whether they belong to the same researcher. We deﬁne the similarity between them as the dot-product of their embeddings and adopt the infoNCE [65] loss as our learning objective: L= −logexp(cu/τ)exp(cu/τ) +Pexp(cu/τ) where N(a) is the random sampled negative researchers set for a, its size is ﬁxed as k. τ is the temperature hyperparameter. By minimize L, we can simultaneously optimize the semantic encoder and local community encoder for researchers. Thus, the purpose of the contrastive learning is to preserve the semantic and community information into the model parameters and help the model to integrade these two kinds of information. 2) Hierarchical Masked Language Model: In the main task, the researcher-level representations are trained via the contrastive learning with their community embeddings. While the document-level and token-level representations are not directly trained. Inspired by the Mask Language Model(MLM) task in Bert, we propose the Hierarchical Masked Language Model(HMLM) on the hierarchical Transformer to train the hidden outputs of documents and tokens, which can further improve the researcher representations. The MLM task masks a few tokens in each document and use the Transformer outputs corresponding to these tokens, which have captured the context token information, to predict the original tokens. As our semantic encoder is a two-level hierarchical Transformer, besides the token-level context captured in the Document Transformer, the Researcher Transformer can capture document-level context (i.e., other documents belong to the same researchers) information, which we assume is also helpful to predict the masked token. Thus, Given a researcher a’s semantic document set D= {d, d, ..., d}, where d∈ Dis one document of aand the textual sequence of d is expressed as d= {t, t, ..., t}. We ﬁrst mask 15% of the tokens in each document. Suppose t∈ dis one masked token and it is replaced with [MASK], the HMLM task aims to predict the original token tbased on the sequence context in dand document context in D. First, we obtain the output of the Document Transformer in the position of t: t= [t, t, t, .., t] = T ransformer(t, t, t, .., t) where the input embedding tof tis replaced as the embedding of [MASK]. Then, we obtain the output of Researcher Transformer in the position of document d, which is the document tis in: d= [d, d, d, .., d] = T ransformer(d, d, d, .., d) After that, We sum up these two outputs and fed it into a linear transformation and a softmax function: where ˆy∈ Ris the probability of tover all tokens, V is the vocabulary size. Finally, the HMLM loss can be formulazed as a cross-entropy loss for predicting all masked tokens in a’s documents: where yis the one-hot encoding of t, Mis the set of masked tokens in document d. 3) Community Relation Prediction: Also, in the main task, we represent the relation information between researchers as the graph-level local community embeddings via linear random sampling and local community encoder. However, the link-level relatedness between researchers is not directly learned. In this auxiliary task, we propose another selfsupervised learning task named Community Relation Prediction(CRP), which utilize the links in sampled local communities to construct supervisory signals. The CRP task has two objectives, the ﬁrst is to predict the relation type between two researchers, the second is to predict the hop counts between two researchers. Speciﬁcally, given a researcher a’s one sampled local community graph G, we random select 15% links in Gas the inputs of CRP task, expressed as L= {(a, l, a)}, lis the link type between researcher aand a, which can be a atomic relation or composite relations. For example, if aand aare linked with the relation r ∈ R, l= r; if they are linked by a path a−→ a−→ ...−−→ a, r, r, ..., r∈ R, lis the composition from rto r, i.e., l= r◦ r◦, ..., ◦r.. In each link (a, l, a). lhas the properties of relation type and hop count, which is what the CRP task aim to predict given the researcher aand a. Thus, we ﬁrst input the element-wise multiplication of aand a’s output representations into two linear transformations and softmax functions: ˆy= Softmax(W· (h◦ h) + b where ˆy∈ Ris the probabilities of l’s type over all link types and T is the number of link types, ˆy∈ Ris the probabilities of l’s hop count range from 1 to H and H is the maximum hop count from ato its neighbors in G. Next we input these two probabilities into respective cross-entropy losses to compose the loss function of CRP task: where yis the one-hot encoding of l’s link type index, yis the one-hot encoding of l’s hop count. With the guidance of this task, the model can learn ﬁne-grained interaction between researchers such that the GNN encoder is capable of ﬁnely capture community information. Pre-training. We leverage a multi-task learning objective by combining the main task and two auxiliary tasks for pretraining. The ﬁnal loss function of RPT can be written as: L(f; D, G) = L+ λL+ λL(21) where the loss weights λand λare hyper-parameters. We sample mini-batches of researchers’ semantic document sets and local community graphs as the inputs to train the whole model, and The parameters is optimized by back propagation consistently. Fine-tuning. After pre-training, we obtain the pre-trained RPT model f, which is able to extract valuable information from the researcher semantic document features and community network. Thus, in the ﬁne-tuning stage, given the researcher data Dand Gfor a speciﬁc ﬁne-tuning task, we can obtains the semantic representation uand the community representation cof each researcher a. Suppose Uand Xis the semantic and community representation matrix of the ﬁne-tuning task, respectively, the ﬁnal researcher representation matrix Zis obtained from the following: where Merge(·) is a merging function to fuse the semantic representation and community representation of researchers, in practice, we directly use the concatenating operation as these two representations have been integrated with each other via contrastive learning in Ep. 13. In our framework, we propose two transfer modes of RPT for ﬁne-tuning: (fb). We treat the RPT as an pre-trained representation generator that ﬁrst extracts researchers’ original features into a low dimensional representation vectors Z. Then, the encoded representations are used as input initial features of researchers for the downstream tasks. (e2e). The pre-trained model fwith hierarchical Transformer and local community encoder is trained together with each ﬁne-tuning downstream task. Suppose the models of ﬁne-tuning task is g(·) with the parameters φ, the objective of RPT (e2e) can be written as: θ, φ= arg minL(g(f, φ); D, G where θand φis the optimized parameters, and L is the loss function of the ﬁne-tuning task. All parameters are optimized end-to-end. RPT (e2e) can further extract semantic and community information useful for the downstream researcher data mining tasks. While RPT (fb) without saving and training the pretrained model is more efﬁcient than RPT (e2e) in the ﬁnetuning stage. But compared with traditional solutions training different task-speciﬁc models for different tasks from scratch, both these two transfer modes are relatively inexpensive, as the downstream model can be very lightweight and converge quickly with the help of pre-training. In this section, we ﬁrst introduce the experimental settings including dataset, baselines, pre-training and ﬁne-tuning parameter setting and implemental hardware and software. Then we ﬁne-tune the pre-trained model on three tasks: researcher classiﬁcation, collaborator prediction, and top-k researcher retrieval to evaluate the effectiveness and transferability of RPT. Lastly, we perform the ablation studies and hyperparameters sensitivity experiments to analyze the underlying mechanism of RPT. The code of RPT is publicly available on https://github.com/joe817/RPT. Dataset. We perform RPT on the public scientiﬁc dataset: Aminer citation dataset[3], which is extracted from DBLP, ACM, MAG (Microsoft Academic Graph) and contains mullions of publication records. The available features of researchers contained in the dataset are the published literature, published venues, organizations, etc. To prepare for the experiments, we select 40281 researchers from Aminer, who have published at least ten papers range from the year of 2013 to 2018. The information of publication records from 2013 to 2015 are extracted to create the semantic document sets and the community graph of researchers for pre-training. Speciﬁcally, we collect each researcher’s papers as his/her semantic documents, the textual sequence of each document is composed by the paper’s ﬁelds of study. We extract three kinds of relations, Collaborating (if they collaborated a paper), Colleague (if they are in the same organization), and CoVenue (if they published on same venue) between researchers, to construct the researcher community graph (Noted that we random sample 100 neighbors of relation CoVenue per researcher). The statistics of semantic document set of researchers and researcher community graph is presented in Table I. Baselines. We choose several pre-training models that can capture the information in semantic document sets and researcher community to researcher representations as baselines. Based on if they can be trained end-to-end with the downstream models, we divide these models into featurebased models, including Doc2vec [49], Metapath2vec [4], and ASNE [66], and end-to-end models, including BERT [8], GraphSAGE [57], and RGCN [60]. We also perform our model in feature-based mode and end-to-end mode in ﬁne-tuning. The detailed descriptions and implementations of baselines are presented as follows: collect all the researcher documents from whole dataset to train the model, and use the average embeddings of each researcher’s document as the his/her pre-trained embeddings. We use the python gensim library to conduct Doc2vec, we set training algorithm as PV-DM, size of window as 3, number of negtive samples as 5. https://pypi.org/project/gensim/. ding model, we conduct it on the researcher community graph to obtain node embeddings as pretrained researcher representations. We set CollaboratingColleague-CoVenue as the meta-path to sample paths on researcher community via random walk, we set walk length as 10, works per researcher as 5, size of window as 3, number of negative samples as 5. https://ericdongyx.github.io/metapath2vec/m2v.html. which can preserve both the community and semantic attributes of researchers. It concatenates the structure features and node attributes into a multi-layer perceptron to learn node embeddings. We use the semantic representations learned by Doc2vec as the input attribute embeddings, and set the the same weight for attribute embedding and structure embedding and the number of hidden layer as 2. https://github.com/lizi-git/ASNE. concatenate all documents into a sentence as inputs and output the [CLS] output as researcher representations. For a fair comparison, we train the BERT model on our dataset from the beginning. We set the layer of Transformer as 6 and the number of self-attention heads as 8. The maximum length of inputs is set as the same as the product of maximum lengths in Researcher Transformer and Document Transformer in RPT. https://github.com/codertimo/BERT-pytorch. We also design two graph pre-training model based on two state-of-the-art GNN models: GraphSAGE and RGCN, GraphSAGE can be applied on homogeneous graph and RGCN can be applied on heterogeneous graphs, and they both can aggregate the local neighborhood information and node attributes into researcher embeddings. we conduct GraphSAGE and RGCN on the researcher community graph and use the self-supervised graph context based loss function introduced in GraphSAGE for pre-training, then in ﬁne-tuning, the pre-trained GraphSAGE and RGCN is trained together with downstream models. GraphSAGE model. The node attributes in researcher community is initialized by the researcher semantic representations learned by Doc2vec. We use the mean aggregator of GraphSAGE and set the aggregation layer number as 2. https://github.com/dmlc/dgl. Library. The node attributes initialization and number of aggregation layers is same with GraphSAGE, we choose the same graph-based loss function from GraphSAGE paper which encourages nearby nodes to have similar representations. https://github.com/dmlc/dgl. For a fair comparison, we ﬁx the representation dimension of all baselines as 64. Pre-Training Parameter Setting. We use Adam optimization with learning rate of 0.01, β= 0.9, β= 0.999, weight decay of 1e-7. we train for 64000 steps with the batch size of 64. We set the researchers representation dimension and all the hidden layer dimensions as 64. The weight λand λof two auxiliary tasks is set as 0.1. For hierarchical Transformer, we set the number of layers as 3, the number of self-attention heads in each Transformer as 8, the maximum length of inputs as 20 and 10 respectively for Document Transformer and Researcher Transformer. For local community encoder, we set the neighbor sampling hops as 2, the size of sampled neighbor set as 8, and the number of layers of GNN model as 2. For global contrastive learning, We set the temperature τ as 1, and the size of negative samples as 3. The code and data on https://github.com/joe817/RPT. Hardware & Software All experiments are conducted with the following setting: 1.20.0; SciPy 1.6.0; Gensim 3.8.3; scikit-learn 0.24.0 Fine-tuning Parameter Setting. The hyper-parameter settings of RPT on three ﬁne-tuning tasks are presented on Table II. The classiﬁcation task is to predict the researcher categories. Researchers are labeled by four areas: Data Mining (DM), Database (DB), Natural Language Processing (NLP) and Computer Vision (CV). For each area, we choose three top venues, then we label researchers by the area with the majority of their publish records on these venues(i.e., their representative areas). The learned researcher representations by each model are feed into a multi-layer perceptron(MLP) classiﬁer. The experimental results are shown in Table III. The number of extracted labeled authors is 4943, and they are randomly split into the training set, validation set, and testing set with different proportions. We use both MicroF1 and Macro-F1 as the multi-class classiﬁcation evaluation metrics. According to Table III, we can observe that (1) the proposed RPT outperform all baselines by a substantial margin in terms of two metrics, the RPT (e2e) obtains 0.6%-10.5% MicroF1 and 0.7%-12.4% Macro-F1 improvement over baselines. (2) Noted that the pre-trained model in RPT (fb) is not trained in ﬁne-tuning, comparing three feature-based baselines, RPT (fb) obtains at least 4.5% improvement, proving that our designed multi-task self-supervised learning objectives can better capture the semantic and community information. (3) RPT (e2e) consistently outperforms RPT (fb), indicating the learned parameters in the pre-training model can further contribute to the downstream task by end-to-end ﬁne-tuning. Collaborating prediction is a traditional link prediction problem that given the existing collaboration information between authors, we aim to predict whether two researchers will collaborate on a paper in the future, which can be used to recommend potential new collaborators for researchers. To be practical, we randomly sample the collaborating links from 2013 to 2015 for training, in 2016 for validation, and from 2017 to 2018 for testing, noted that duplicated collaborators are removed from evaluation. We use the element-wise multiplication of two candidate researchers’ representations as the representation of their collaborating link, then we input the link representation into a binary MLP classiﬁer to predict whether this link exists. Also, negative links (two researchers who did not collaborate in the dataset) with 3 times the number of true links are randomly sampled. We sample various numbers of collaborating links and use accuracy and F1 as evaluation metrics. The experimental results of different models are reported in Table IV. According to the table, RPT still performs best in all cases. The following insights can be drawn: (1) Graph representation learning models and GNNs achieve better performance than semantic representation learning models, showing that the community information of researchers maybe the more important to collaborating prediction. (2) RPT and RGCN outperform GraphSAGE, indicating the beneﬁt of incorporating heterogeneity information of relations to researcher representations. (3) Our methods can achieve 82.9% accuracy even when the size of the training set is far less than the testing set, indicating the effectiveness of the pre-training model in preserving useful information from unlabeled data. The problem of top-K researcher retrieval is a typical information retrieval problem, which is deﬁned as given a researcher, we aim to retrieve several most relevant researchers of him/her. For each input researcher, we use the dot-product of his and the candidate researcher’s representations as their scores and input the scores into a softmax over all researchers to predict top-K relevant researchers, we also use negative sampling [67] in training for efﬁciency. We randomly select 2000 researchers for training, 1000 researchers for validation, and 7000 researchers for testing. The ground truth for each researcher is deﬁned as its coauthor list ordered by the times of collaboration. Noted that we do not introduce any extra parameters except pre-trained models to ﬁne-tuning in this task, and for a fair comparison, the researcher representations are regarded as trainable parameters for feature-based models. Finally, we use Precision@K and Recall@K in the top-K retrieval list as the evaluation metric and we set K as 1, 5, 10, 15, 20 respectively. The results are shown in Table V. We can observe that: (1) The performance of traditional feature-based models in this task is far less than end-to-end models in general, comparing their performance in previous tasks. That is because, without downstream parameters, they are inadequate to ﬁt the objective function well. (2) While the end-to-end models can adaptively optimize the parameters in pre-trained models by the downstream objectives, so they can achieve better performance. (3) The proposed RPT in end-to-end mode still achieves the best performance, showing the designed framework is robust in transferring to different downstream tasks. In this section, we analyze the underlying mechanism of RPT, we conduct several ablation studies and parameter analysis to investigate the effect of different components, stages, and parameters. Ablation Study of Multi-tasks. As the proposed RPT is a multi-task learning model with the main task and two auxiliary tasks. How different tasks impact the model performance? we propose three model variants to validate the effectiveness of these tasks. We perform these variant models on the researcher classiﬁcation task and collaborating prediction task, the pre-training setting is same with the complete version, and the ﬁne-tuning is set as the end-to-end mode. Figure 3 show the performance of these variants comparing with the original RPT. We can observe that: (1) The results of RPT are consistently better than all the other variants, it is evident that using the three objectives together achieves better performance. (2) Both the RPTand RPTachieve better performance than RPT, indicating the usefulness of both these two auxiliary tasks. (3) RPTis better than RPTon two downstream tasks, which implies the Lplays a more important role than Lin this framework. (4) Comparing the performance of RPTwith the baselines in Table III and IV, we can ﬁnd that RPTstill achieves very competitive performance, demonstrating that our framework has outstanding ability in learning researcher representations. Effect of Pre-training. To verify if RPT’s good performance is due to the pre-training and ﬁne-tuning framework, or only because our designed neural network is powerful in encoding researcher representations. In this experiment, we do not pre-train the designed framework and fully ﬁne-tune it with all the parameters randomly initialized. In Figure 4, we present the training and testing curves of RPT with pretraining and without pre-training as the epoch increasing in two downstream tasks. We can observe that the pre-trained model achieves orders-of-magnitude faster training and validation convergence than the non-pre-trained model. For example in the classiﬁcation task, it took 10 epoch for the non-pretrained model to get the 77.9% Micro-F1, while it took only 1 epoch for the pre-trained model to get 83.6% Micro-F1, showing pre-training can improve the training efﬁciency of downstream tasks. On the other hand, we can observe that the non-pre-trained model is inferior to pre-trained models in the ﬁnal performance. It proves that our designed pretraining objective can preserve rich information in parameters and provides a better start point for ﬁne-tuning than random initialization. Hyper-parameters sensitivity. We also conduct experiments to evaluate the effect of two key hyper-parameters in our model, i.e, the number of self-attention heads in hierarchical Transformers and the sampling hops in the local community encoder. We investigate the sensitivity of these two parameters on the researcher classiﬁcation task and report the results in Figure 5. According to these ﬁgures, we can observe that (1) the more number of attention heads will generally improve the performance of RPT, while with the further increase of attention heads, the improvement becomes slightly. Meanwhile, we also ﬁnd that more attention heads can make the pre-training more stable. (2) When the number of sampling hops varies from 1 to 5, the performance of RPT increases at ﬁrst as a suitable amount of neighbors are considered. Then the performance decrease slowly when the hops further increase as more noises (uncorrelated neighbors) are involved. In this paper, we propose a researcher data pre-training framework named RPT to solve researcher data mining problems. RPT jointly consider the semantic information and community information of researchers. In the pre-training stage, a multi-task self-supervised learning objective is employed on big unlabeled researcher data for pre-training, while in ﬁne-tuning, we transfer the pre-trained model to multiple downstream tasks with two modes. Experimental results show RPT is robust and can signiﬁcantly beneﬁt various downstream tasks. In the future, we plan to perform RPT on more meaningful researcher data mining tasks to verify the extensibility of the framework.