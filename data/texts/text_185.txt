Due to the rapid growth in the number of publications of scientiﬁc articles, more and more works have recently appeared devoted to the analysis of various aspects of scientiﬁc texts. For example, the paper [10] describes an interface that makes it easier to read scientiﬁc articles by highlighting and linking deﬁnitions, variables in formulas, etc. Authors of the work [5] propose one of the approaches to the summarization of scientiﬁc texts. Texts of this genre contain valuable information about advanced scientiﬁc developments, however, this type of text diﬀers from news texts, texts on social networks, etc., in their structure and content. Therefore, it is especially important to adapt and develop methods and algorithms for processing scientiﬁc texts. amounts of data are unavailable for most languages. For example, as far as we know, there are no publicly available datasets for extracting and linking entities in scientiﬁc texts in Russian. That is why we focus on the methods which do not The study was funded by RFBR according to the research project N 19-07-01134. A.P. Ershov Institute of Informatics Systems, Siberian Branch, Russian Academy Abstract. In this paper, we present a system for information extraction from scientiﬁc texts in the Russian language. The system performs several tasks in an end-to-end manner: term recognition, extraction of relations between terms, and term linking with entities from the knowledge base. These tasks are extremely important for information retrieval, recommendation systems, and classiﬁcation. The advantage of the implemented methods is that the system does not require a large amount of labeled data, which saves time and eﬀort for data labeling and therefore can be applied in low- and mid-resource settings. The source code is publicly available and can be used for diﬀerent research purposes. Keywords: Term extraction · Relation extraction · Entity linking · Knowledge base · Weakly supervised learning. Common NLP models require a large amount of training data. However, such require a large amount of labeled data: for term recognition, a weak supervision method is used; for relation extraction, we apply cross-lingual transfer learning; for entity linking, we also implement a language-agnostic method. ﬁrst section provides an overview of works on the tasks of term recognition, relation extraction, and linking terms with entities from the knowledge base. The next section describes the process of preparing and annotating data. In the last section, we give a more detailed description of the system modules, present the algorithms and results of preliminary experiments. Terms extraction. The goal of term extraction is to automatically extract relevant terms from a given text, where terms are sequences of tokens (usually nouns or noun groups) that deﬁne a particular concept from a ﬁeld of science, technology, art, etc. There are several groups of methods for solving this task. The traditional approach solves this task in two stages: ﬁrstly, phrases which can be terms are extracted from the text, and then there is a classiﬁcation step to decide whether this phrase is a term or not. Such an approach is described in [1,19,28]. It allows control term extraction with hand-crafted rules to solve this problem more precisely. But on the other hand, the full context is rarely considered. Another approach solves this task as a sequence labeling task, for example, it is described in [14]. This group of methods takes in account the context to make use of both syntactic and semantic features. The main disadvantage of such an approach is needing quite a large amount of annotated data, as deep learning architectures are used mainly. Some researchers apply methods for topic modeling to extract terms [2]. The underlying idea is that terms represent concepts related to subtopics of domain-speciﬁc texts. So revealing topics in the text collection can improve the quality of automatic term extraction. and classiﬁcation of semantic relations between a pair of entities within a text or sentence. There are diﬀerent ways to solve this task. A classical approach is to use methods based on lexico-syntactic patterns [11]. Such methods tend to have high precision and low recall since they require manual labor. To overcome this problem, nowadays diﬀerent machine learning algorithms are applied [24,20]. The distinctive feature of the method [20] is using a neural network for incorporation of both syntactic indicators and the entire sentences into better relation representations. However, it can be diﬃcult to compile a complete list of such indicators. The method described in [24] utilizes special tokens to mark two entities in the sentence which can be used in pre-trained models. This method gives good results in sentence-level relation extraction, but cannot provide information of all the entities (including multiple target entities) in the document at one time. Since entity extraction and relation classiﬁcation may beneﬁt from having shared information, models for the joint extraction of entities and relations have recently drawn attention. In the paper [13] the authors propose a This paper consists of an introduction, three sections, and a conclusion. The Relation extraction. The relation extraction task requires the detection model architecture, where spans are detected and then relations between spans are classiﬁed in the end-to-end fashion. In the paper [22] the authors describe a method for joint entity recognition, relation extraction and event detection in a multitasking way. This method uses separate local task-speciﬁc classiﬁers in the ﬁnal layer, which can sometimes lead to errors because of a lack of global constraints. mentioned in a text with an entity in a structured knowledge base. Usually, this task is considered as a ranking problem and includes several subtasks. The ﬁrst stage is a candidate generation for the input entity (term) from a knowledge base. For example, it can be done with string match [4], which is rather easy to perform but doesn’t solve the problem that the same proper name may refer to more than one named entity. Another approach is dictionary [18] that helps to use taxonomy in knowledge bases but depends on their completeness. Also prior probability [8] can be used in the generation step. In the second stage, one should get embeddings for the mention with its context and for the entity. Nowadays diﬀerent deep learning-based algorithms are applied for this purpose such as a self-attention mechanism [16], and pre-trained models [26]. Researchers [9] got mention-context vectors by using combined LSTM encoder and bag-of-mention surfaces. This approach lets model the context across datasets. However, authors didn’t include more structured knowledge to make representations semantically richer. Then there is a ranking stage to ﬁnd the most relevant entity for the input term. In some research works this task is solved as a classiﬁcation task, using diﬀerent types of classiﬁers such as naive Bayes classiﬁer [21], SVM classiﬁer [27], deep neural networks [12]. It is a bit complicated to ﬁnd the most suitable dataset for our purposes and in the same time open-sourced. Nevertheless, the corpus of scientiﬁc papers in Russian RuSERRC[3] solves this problem to some extent. It contains abstracts of 1.680 scientiﬁc papers on information technology in Russian, including 80 manually labeled texts with terms and relations. We added an annotation to this corpus by linking selected terms to entities from Wikidata (entities that are inside of other entities), for example: “[self-consistent [electric ﬁeld]]([самосогласованное [электрическое поле]])”. When annotating for entity linking task, we moved from the “largest” entity to the “smaller” nested ones, i.e. if for the very ﬁrst level the entity was found in the knowledge base, then the nested entities are not annotated. preﬁxed with “Q”, as opposed to relations that have an identiﬁer preﬁxed with “P”. Also, we did not associate terms with entities of the “Scientiﬁc article” https://www.wikidata.org Entity linking. The entity linking task is the task of matching an entity This corpus contains annotation not only of terms, but also of nested entities We linked terms with entities from Wikidata. They have the unique identiﬁer type. Each entity was annotated by two assessors. The measure of consistency was calculated as the ratio of the number of entities without conﬂict in the annotation to the total number of entities in the corpus and amounted to 82.33%. associated with entities in Wikidata. The average length of a linked entity is 1.55 tokens, the minimum length is one token, and the maximum is eight tokens. We propose a framework for information extraction from scientiﬁc papers in Russian. Figure 1 shows its general architecture. 1. The terms extraction module takes a raw text as input and outputs a set of 2. The relation extraction module takes a text and a set of terms obtained from 3. The entity linking module has the same input as the module for relation ex- Method description. During our research, we have not found any comparative large annotated corpora for our purpose. To overcome this problem we decided to use a weak supervision method. The general idea is to train a model on data that A total of 3386 terms were annotated in the corpus, 1337 of which were The system consists of the following modules: terms from this text; the previous step and outputs a set of term pairs with the speciﬁed relation between them; traction and outputs terms and the corresponding entities from a knowledge base. Below we provide a detailed description for each module. were annotated automatically, and then annotate another data with this model, merge these two data sources and train the second model. Thus the algorithm stages are following: 1. Obtaining annotated corpus for the ﬁrst iteration of model training with a 2. Training the model on this corpus; 3. Annotating new texts and the previous ones with the trained model and the 4. Training the model on the extended corpus. The dictionary of terms was obtained in a semi-automated way: 1. We collected 2-, 3-, 4-gramms from the scientiﬁc articles, sorted them by 2. We took titles of articles from Wikipedia, which belong to the category In such a way we obtained a dictionary with 17252 terms which is available here network architecture adopted from the BertforTokenClassiﬁcation class of the Transformers library[23]. Word embeddings were generated by BERT model outputs the sequence of labels for the corresponding tokens. Here we use three classes: “B-TERM” (for the ﬁrst token in a term sequence), “I-TERM” (for the inside token in a term sequence) and “O” (for the token which doesn’t belong to a term sequence). term bounds detection. To correct it we wrote some heuristics such as removing a preposition from a term if it starts the term sequence, including the next token written in English after the term sequence, etc. same dictionary was used for automatic text annotation for our model. One may conclude that the dictionary-based approach gives the higher precision for partial match but still gives low recall and F1 in general. Also it can be improved by extending the list of the terms. Results. We tested the ﬁnal model on the RuSERRC corpus, which was not used during the model training. To evaluate the model quality, the standard classiﬁcation metrics were used: precision (P), recall (R) and F-measure (F1). Also, we considered two variants of these metrics: exact match and partial match. In exact match, only full sequences are considered to be correct. In partial match, we considered tokens that have a tag in {“B-TERM”, “I-TERM”} as a term. The metrics are shown in Table 1. https://github.com/iis-research-team/ner-rc-russian https://huggingface.co/bert-base-multilingual-cased dictionary of terms; dictionary; TF-IDF value and then manually ﬁltered them; “Science” and then manually ﬁltered them. During all stages of our term recognition algorithm, we used the neural The model takes a tokenized text (without any preprocessing) as input and Analysing the results, we noticed that most cases of model errors are wrong As a baseline for term extraction we used a dictionary-based approach - the set and gold standard annotations. As we annotated the training set with a dictionary, there were not any changes in the token sequences, while in practice the term may not include some terms, has abbreviations, reductions, etc. Analysis of partial match metric reveals that the model is able to recognize a term but it is diﬃcult to deﬁne the term boundaries. Considering that the task of term boundary detection is quite diﬃcult even for humans, the obtained metrics are thought to be enough for applying this approach for solving other tasks. Method description. There is a lack of annotated data for the relation extraction task in Russian as well. It means that the standard training process of neural networks is diﬃcult. To overcome this issue we applied a pre-trained multilingual model. The main idea is to ﬁnetune the pre-trained model on the data in high-resource language. Then evaluate this model on data in Russian. The hypothesis is that information from other languages encoded in the model weights helps to make predictions on data in the target language correctly. gual BERT model information about relations between scientiﬁc terms. Since our task is to deﬁne not only the type of relation between two terms but also to deﬁne whether two terms are connected by any relation or not, we added samples without any relation to our data. To decrease the imbalance between the number of samples in classes, in the train set we added only 50% of randomly chosen pairs of terms without relation with the distance between such terms less than 10 tokens. There were no such limitations for validation and test sets. We extracted relations only within one sentence. line). The main idea is to collect lexico-syntactic patterns manually which can be used as markers for the diﬀerent kinds of relations. Actually, a lot of samples don’t have an explicit marker for a particular relation - it can be found based only on words and text semantics, which is impossible to detect with hand-crafted rules. Results. To evaluate the model, we used RuSERRC dataset as well. Since the sets of relation types in SciERC and RuSERRC corpora are diﬀerent, we https://huggingface.co/bert-base-multilingual-cased Relatively low metrics are largely due to the diﬀerence between the training Inspired by [25] we used the R-BERT architecture with pre-trained multilin- For relation extraction we also implemented a pattern-based approach (baseevaluated the model only on intersected relations: COMPARE, HYPONYM-OF, NO-RELATION, PART-OF, USED-FOR. The overall metrics and metrics by relations (given by the model) are shown in Table 2 and Table 3 correspondingly. diﬀerently in these datasets. The task of relation extraction and classiﬁcation is one of the most diﬃcult tasks in NLP. Nevertheless, the obtained metrics show that this approach with some improvements can be used to solve this task without an additional set of annotated data on the target language, although we have yet to investigate this issue. dataset for our purposes we compare our results with previously published results obtained on similar datasets in English. For example, the state-of-the-art result achieved on SciERC with the SpERT (using SciBERT) method is 70.33% for NER and 50.84% for relation extraction. However, according to [7] the same method on the general domain dataset ACE, gives 89.28% and 78.84% f-scores respectively. As can be seen the f-score on the scientiﬁc dataset in English is signiﬁcantly worse due to the complexity of the problem itself. Our results may also be related to insuﬃcient data, as Russian is morphologically rich, which additionally complicates the work of the language model. We plan to study this aspect in the future. Method description. We have implemented an algorithm for entity linking. As input data, the algorithm is given a sequence or a single token corresponding to the term. Then the stage of candidates generation, the input string undergoes preprocessing, namely lemmatization using Mystem case. Moreover, we take a pre-trained fastText model from Deeppavlov https://yandex.ru/dev/mystem/ https://deeppavlov.ai Zero metrics for relation COMPARE show that this relation is understood Due to the fact that we could not ﬁnd an open-source good-labeled Russian encoded input term by averaged vectors from it. Entities from Wikidata went through the same stages. Next, two main steps are performed: creating an array of candidates for linking, ﬁnding the most suitable entity in the resulting set of candidates. are compared with the name of the entity and its synonyms. If there is a match, then the entity is added to the candidate list. In addition, if there is a ”disambiguation page” in the entity’s description, this candidate will be removed from the list of candidates. and the threshold value for extra. The ﬁrst vector is averaged for the input mention and its context of n tokens before the term and the same number after, where n=5. The second vector is also averaged for the name, description, and synonyms of the entity in the knowledge base. In order to take into account the number of matching tokens in candidate and mention, the calculated distances were multiplied by the weighting factor which was computed by the formula: where n matching is a number of shared tokens in candidate and mention; n all is a number of all tokens in entity-mention. The result of the algorithm is the Wikidata item identiﬁer for the input reference. Results. The algorithm was tested on a corpus with annotated scientiﬁc terms from the Data Description section. The metrics are shown in Table 3. We used the following metrics for evaluation: accuracy, the average number of candidates, and top-k accuracy. number of terms. Since we managed to link not all terms in the corpus, it would be more informative to divide this metric into two: accuracy and linked accuracy. only on the set of terms for which the entity was found in the knowledge base in the corpus. Thus, accuracy is computed using the formula: where n correct entities is the number of correctly linked terms; all entities is the number of all terms in the corpus. where n correct linked entities is the number of correctly linked terms among all At the stage of generating candidates, the input string and its 1, 2, 3-grams For ranking candidates, we use the cosine distance between the two vectors Accuracy is the ratio of the number of correctly linked terms to the total Accuracy takes into account all entities, whereas linked accuracy is calculated Then linked accuracy is calculated by the formula: linked terms; n all linked entities is the overall number of linked terms in the corpus. aged candidates and linked averaged candidates. where Candidates is the number of all terms in the corpus. of terms that managed to be linked. Denote n all linked entities as the number of all terms in the corpus that have a link with the entity from Wikidata, and Linked candidates that was linked with Wikidata. Thus, formula for the linked averaged candidates metric is: a relation with an entity from the knowledge base, in our context k is equal to the number of candidates. This metric is calculated using the formula: where num correct sets is the number of candidate sets for the terms which are included in the set of n all linked entities, containing the true entity. step works rather properly, has a bad impact on accuracy. By the way, the distinction between the value of top-k accuracy and linked accuracy is signiﬁcantly large. This means that in 76% of cases there is the top candidate in the list, but only in 54% of cases ranking works properly and the output entity is relevant. Average number of candidates. We also split this metric into two: aver- Averaged candidates is the average number of candidates for all entities: Linked averaged candidates is the average number of candidates for the set linked averaged candidates =|Linked candidates|n all linked entities Top-k accuracy is counted only for a set of terms in the corpus that have A relatively high value for averaged candidates, which shows the generation from the ﬁnal version in two main stages. At the generation step the input string is compared with the name of the entity and its synonyms. If there is a match, then the entity is added to the candidate list. For ranking candidates, we use information about the number of links an entity has to other knowledge bases and the number of relationships of this entity with other entities. The hypothesis is that the more an entity is ﬁlled with information, the more relevant it is. Thus, the choice of entity for the input term is determined by the following formula: entity; the knowledge base. version. Probably, it is due to the signiﬁcantly low value of average candidates. The more candidates the more diﬃcult to ﬁnd the most suitable. manage to ﬁnd any system or even dataset that consists of scientiﬁc papers. Nevertheless, there is a suitable dataset in English - STEM-ECR [6]. Authors of this dataset evaluate several systems that work well on open domain data. Otherwise, the scores on their dataset are: exact title match heuristic at 37.8% accuracy, and the best is for Babelfy [17] to DBpedia at 52.6%. similarity of entities using a classiﬁer based on the Siamese network. Moreover, alternative names or synonyms will be used for expanding the list of candidates. In this paper, we presented a system for information extraction from scientiﬁc texts in Russian. It consists of three modules. The ﬁrst module recognizes terms, the second one extracts the relations between the terms found in the previous step, and the third one links terms with entities from the knowledge base. As a result, information from the input text is extracted in a structured form. The experiments were carried out with texts from the information technology domain and, presumably, can be easily adapted to other subject areas. However, in order to draw conclusions regarding the transfer to other domains, in the future, we plan to conduct an additional series of experiments. terminator. The results of this study can be useful in the development of systems for unstructured data analysis and expert systems in scientiﬁc organizations and higher education institutions. To compare results we implemented a simple algorithm for this task. It diﬀers where n is the number of entities in the set of candidates, f(ent) = numL+ numR, where numLis the number of links to other knowledge bases for this numRis the number of relationships of this entity with other entities in As for metrics, only accuracy for the baseline is higher than for the ﬁnal As for the results of other researchers in Entity Linking in Russian, we didn’t In the future, we plan to implement an approach to identify the semantic Our research is publicly available at https://github.com/iis-research-team/