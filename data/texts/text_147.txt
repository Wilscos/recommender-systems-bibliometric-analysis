Athanasios N. Nikolakopoulos, Xia Ning, Christian Desrosiers and George Karypis Abstract Collaborative recommendation approaches based on nearest-neighbors are still highly popular today due to their simplicity, their eﬃciency, and their ability to produce accurate and personalized recommendations. This chapter oﬀers a comprehensive survey of neighborhood-based methods for the item recommendation problem. It presents the main characteristics and beneﬁts of such methods, describes key design choices for implementing a neighborhood-based recommender system, and gives practical information on how to make these choices. A broad range of methods is covered in the chapter, including traditional algorithms like knearest neighbors as well as advanced approaches based on matrix factorization, sparse coding and random walks. The appearance and growth of online markets has had a considerable impact on the habits of consumers, providing them access to a greater variety of products and information on these goods. While this freedom of purchase has made online commerce into a multi-billion dollar industry, it also made it more diﬃcult for consumers to select the products that best ﬁt their needs. One of the main solutions proposed for this information overload problem are recommender systems, which provide automated and personalized suggestions of products to consumers. for unseen items, based on historical information stored in the system, and suggesting to this user novel and original items for which the predicted response is high. User-item responses can be numerical values known as ratings (e.g., 1-5 stars), ordinal values (e.g., strongly agree, agree, neutral, disagree, strongly disagree) representing the possible levels of user appreciation, or binary values (e.g., like/dislike or interested/not interested). Moreover, user responses can be obtained explicitly, for instance, through ratings/reviews entered by users in the system, or implicitly, from purchase history or access patterns [45, 87]. For the purpose of simplicity, from this point on, we will call rating any type of user-item response. sonalized and non-personalized. Among the personalized approaches are contentbased and collaborative ﬁltering methods, as well as hybrid techniques combining these two types of methods. The general principle of content-based (or cognitive) methods [4, 8, 48, 70] is to identify the common characteristics of items that have received a favorable rating from a user, and then recommend to this user unseen items that share these characteristics. Recommender systems based purely on content generally suﬀer from the problems of limited content analysis and over-specialization [79]. Limited content analysis occurs when the system has a limited amount of information on its users or the content of its items. For instance, privacy issues might refrain a user from providing personal information, or the precise content of items may be diﬃcult or costly to obtain for some types of items, such as music or images. Another problem is that the content of an item is often insuﬃcient to determine its quality. Over-specialization, on the other hand, is a side eﬀect of the way in which content-based systems recommend unseen items, where the predicted rating of a user for an item is high if this item is similar to the ones liked by this user. For example, in a movie recommendation application, the system may recommend to a user a movie of the same genre or having the same actors as movies already seen by this user. Because of this, the system may fail to recommend items that are diﬀerent but still interesting to the user. More information on content-based recommendation approaches can be found in Chapter ?? of this book. approaches use the rating information of other users and items in the system. The key idea is that the rating of a target user for an unseen item is likely to be similar to that of another user, if both users have rated other items in a similar way. Likewise, the target user is likely to rate two items in a similar fashion, if other users have given The recommendation problem can be deﬁned as estimating the response of a user Item recommendation approaches can be divided in two broad categories: per- Instead of depending on content information, collaborative (or social) ﬁltering similar ratings to these two items. Collaborative ﬁltering approaches overcome some of the limitations of content-based ones. For instance, items for which the content is not available or diﬃcult to obtain can still be recommended to users through the feedback of other users. Furthermore, collaborative recommendations are based on the quality of items as evaluated by peers, instead of relying on content that may be a bad indicator of quality. Finally, unlike content-based systems, collaborative ﬁltering ones can recommend items with very diﬀerent content, as long as other users have already shown interest for these diﬀerent items. neighborhood and model-based methods. In neighborhood-based (memory-based [10] or heuristic-based [2]) collaborative ﬁltering [19, 20, 32, 45, 51, 55, 73, 75, 79], the user-item ratings stored in the system are directly used to predict ratings for unseen items. This can be done in two ways known as user-based or item-based recommendation. User-based systems, such as GroupLens [45], evaluate the interest of a target user for an item using the ratings for this item by other users, called neighbors, that have similar rating patterns. The neighbors of the target user are typically the users whose ratings are most correlated to the target user’s ratings. Item-based approaches [20, 51, 75], on the other hand, predict the rating of a user for an item based on the ratings of the user for similar items. In such approaches, two items are similar if several users of the system have rated these items in a similar fashion. in the prediction, model-based approaches use these ratings to learn a predictive model. Salient characteristics of users and items are captured by a set of model parameters, which are learned from training data and later used to predict new ratings. Model-based approaches for the task of recommending items are numerous and include Bayesian Clustering [10], Latent Semantic Analysis [33], Latent Dirichlet Allocation [9], Maximum Entropy [89], Boltzmann Machines [74], Support Vector Machines [28], and Singular Value Decomposition [6, 46, 69, 85, 86]. A survey of state-of-the-art model-based methods can be found in Chapter ?? of this book. ing methods, hybrid recommendation approaches combine characteristics of both types of methods. Content-based and collaborative ﬁltering methods can be combined in various ways, for instance, by merging their individual predictions into a single, more robust prediction [71, 8], or by adding content information into a collaborative ﬁltering model [1, 3, 88, 81, 64, 67, 59]. Several studies have shown hybrid recommendation approaches to provide more accurate recommendations than pure content-based or collaborative methods, especially when few ratings are available [2]. While recent investigations show state-of-the-art model-based approaches superior to neighborhood ones in the task of predicting ratings [46, 84], there is also an Collaborative ﬁltering approaches can be grouped in two general classes of In contrast to neighborhood-based systems, which use the stored ratings directly Finally, to overcome certain limitations of content-based and collaborative ﬁlteremerging understanding that good prediction accuracy alone does not guarantee users an eﬀective and satisfying experience [31]. Another factor that has been identiﬁed as playing an important role in the appreciation of users for the recommender system is serendipity [31, 75]. Serendipity extends the concept of novelty by helping a user ﬁnd an interesting item he or she might not have otherwise discovered. For example, recommending to a user a movie directed by his favorite director constitutes a novel recommendation if the user was not aware of that movie, but is likely not serendipitous since the user would have discovered that movie on his own. A more detailed discussion on novelty and diversity is provided in Chapter ?? of this book. latent factors. For example, in a movie recommender system, such methods may determine that a given user is a fan of movies that are both funny and romantic, without having to actually deﬁne the notions “funny” and “romantic”. This system would be able to recommend to the user a romantic comedy that may not have been known to this user. However, it may be diﬃcult for this system to recommend a movie that does not quite ﬁt this high-level genre, for instance, a funny parody of horror movies. Neighborhood approaches, on the other hand, capture local associations in the data. Consequently, it is possible for a movie recommender system based on this type of approach to recommend the user a movie very diﬀerent from his usual taste or a movie that is not well known (e.g., repertoire ﬁlm), if one of his closest neighbors has given it a strong rating. This recommendation may not be a guaranteed success, as would be a romantic comedy, but it may help the user discover a whole new genre or a new favorite actor/director. • Simplicity: Neighborhood-based methods are intuitive and relatively simple to • Justiﬁability: Such methods also provide a concise and intuitive justiﬁcation for • Eﬃciency: One of the strong points of neighborhood-based systems are their eﬃ- Model-based approaches excel at characterizing the preferences of a user with The main advantages of neighborhood-based methods are: implement. In their simplest form, only one parameter (the number of neighbors used in the prediction) requires tuning. the computed predictions. For example, in item-based recommendation, the list of neighbor items, as well as the ratings given by the user to these items, can be presented to the user as a justiﬁcation for the recommendation. This can help the user better understand the recommendation and its relevance, and could serve as basis for an interactive system where users can select the neighbors for which a greater importance should be given in the recommendation [6]. The beneﬁts and challenges of explaining recommendations to users are addressed in Chapter ?? of this book. ciency. Unlike most model-based systems, they require no costly training phases, which need to be carried at frequent intervals in large commercial applications. These systems may require pre-computing nearest neighbors in an oﬄine step, which is typically much cheaper than model training, providing near instantaneous recommendations. Moreover, storing these nearest neighbors requires very • Stability: Another useful property of recommender systems based on this aptages causes some items to be never recommended. Also, traditional methods of this category are known to be more sensitive to the sparseness of ratings and the cold-start problem, where the system has only a few ratings, or no rating at all, for new users and items. Section 5 presents more advanced neighborhood-based techniques that can overcome these problems. This chapter has two main objectives. It ﬁrst serves as a general guide on neighborhoodbased recommender systems, and presents practical information on how to implement such recommendation approaches. In particular, the main components of neighborhood-based methods will be described, as well as the beneﬁts of the most common choices for each of these components. Secondly, it presents more specialized techniques on the subject that address particular aspects of recommending items, such as data sparsity. Although such techniques are not required to implement a simple neighborhood-based system, having a broader view of the various diﬃculties and solutions for neighborhood methods may help making appropriate decisions during the implementation process. formal deﬁnition of the item recommendation task and present the notation used throughout the chapter. In Section 3, the principal neighborhood approaches, predicting user ratings for unseen items based on regression or classiﬁcation, are then introduced, and the main advantages and ﬂaws of these approaches are described. This section also presents two complementary ways of implementing such approaches, either based on user or item similarities, and analyzes the impact of little memory, making such approaches scalable to applications having millions of users and items. proach is that they are little aﬀected by the constant addition of users, items and ratings, which are typically observed in large commercial applications. For instance, once item similarities have been computed, an item-based system can readily make recommendations to new users, without having to re-train the system. Moreover, once a few ratings have been entered for a new item, only the similarities between this item and the ones already in the system need to be computed. While neighborhood-based methods have gained popularity due to these advan, they are also known to suﬀer from the problem of limited coverage, which The rest of this document is structured as follows. In Section 2, we ﬁrst give a these two implementations on the accuracy, eﬃciency, stability, justﬁability ans serendipity of the recommender system. Section 4, on the other hand, focuses on the three main components of neighborhood-based recommendation methods: rating normalization, similarity weight computation, and neighborhood selection. For each of these components, the most common approaches are described, and their respective beneﬁts compared. In Section 5, the problems of limited coverage and data sparsity are introduced, and several solutions proposed to overcome these problems are described. In particular, several techniques based on dimensionality reduction and graphs are presented. Finally, the last section of this document summarizes the principal characteristics and methods of neighorhood-based recommendation, and gives a few more pointers on implementing such methods. In order to give a formal deﬁnition of the item recommendation task, we introduce the following notation. The set of users in the recommender system will be denoted by U, and the set of items by I. Moreover, we denote by R the set of ratings recorded in the system, and write S the set of possible values for a rating (e.g., S = [1,5] or S = {like, dislike}). Also, we suppose that no more than one rating can be made by any user u ∈ U for a particular item i ∈ I and write r the subset of users that have rated an item i, we use the notation U represents the subset of items that have been rated by a user u. Finally, the items that have been rated by two users u and v, i.e. I presentation, and we use I to denote the set of users that have rated both items i and j. the rating prediction and top-N recommendation problems. The ﬁrst problem is to predict the rating that a user u will give his or her unrated item i. When ratings are available, this task is most often deﬁned as a regression or (multi-class) classiﬁcation problem where the goal is to learn a function f : U × I → S that predicts the rating f (u,i) of a user u for an unseen item i. Accuracy is commonly used to evaluate the performance of the recommendation method. Typically, the ratings R are divided into a training set R prediction accuracy. Two popular measures of accuracy are the Mean Absolute Error (MAE): and the Root Mean Squared Error (RMSE): Two of the most important problems associated with recommender systems are When ratings are not available, for instance, if only the list of items purchased by each user is known, measuring the rating prediction accuracy is not possible. In such cases, the problem of ﬁnding the best item is usually transformed into the task of recommending to an active user u him or her [20, 75]. The quality of such method can be evaluated by splitting the items of I into a set I be the subset of test items that a user u found relevant. If the user responses are binary, these can be the items that u has rated positively. Otherwise, if only a list of purchased or accessed items is given for each user u, then these items can be used as T (u). The performance of the method is then computed using the measures of precision and recall: A drawback of this task is that all items of a recommendation list L(u) are considered equally interesting to user u. An alternative setting, described in [20], consists in learning a function L that maps each user u to a list L(u) where items are ordered by their “interestingness” to u. If the test set is built by randomly selecting, for each user u, a single item i Reciprocal Hit-Rank (ARHR): where rank(i extensive description of evaluation measures for recommender systems can be found in Chapter ?? of this book. Recommender systems based on neighborhood automate the common principle that similar users prefer similar items, and similar items are preferred by similar users. To illustrate this, consider the following example based on the ratings of Figure 1. Example 1. User Eric has to decide whether or not to rent the movie “Titanic” that he has not yet seen. He knows that Lucy has very similar tastes when it comes to movies, as both of them hated “The Matrix” and loved “Forrest Gump,” so he asks her opinion on this movie. On the other hand, Eric ﬁnds out he and Diane have diﬀerent tastes, Diane likes action movies while he does not, and he discards her opinion or considers the opposite in his decision. User-based neighborhood recommendation methods predict the rating r u for an unseen item i using the ratings given to i by users most similar to u, called nearest-neighbors. Suppose we have for each user v , u a value w preference similarity between u and v (how this similarity can be computed will be discussed in Section 4.2). The k-nearest-neighbors (k-NN) of u, denoted by N (u), are the k users v with the highest similarity w have rated item i can be used in the prediction of r k users most similar to u that have rated i. We write this set of neighbors as N The rating r A problem with (6) is that is does not take into account the fact that the neighbors can have diﬀerent levels of similarity. Consider once more the example of Figure 1. If the two nearest-neighbors of Eric are Lucy and Diane, it would be foolish to consider equally their ratings of the movie “Titanic,” since Lucy’s tastes are much closer to Eric’s than Diane’s. A common solution to this problem is to weigh the contribution of each neighbor by its similarity to u. However, if these weights do not sum to 1, the predicted ratings can be well outside the range of allowed values. Consequently, it is customary to normalize these weights, such that the predicted rating becomes In the denominator of (7), |w produce ratings outside the allowed range. Also, w α > 0 is an ampliﬁcation factor [10]. When α > 1, as is it most often employed, an even greater importance is given to the neighbors that are the closest to u. Example 2. Suppose we want to use (7) to predict Eric’s rating of the movie “Titanic” using the ratings of Lucy and Diane for this movie. Moreover, suppose the similarity weights between these neighbors and Eric are respectively 0.75 and 0.15. Fig. 1: A “toy example” showing the ratings of four users for ﬁve movies. The predicted rating would be which is closer to Lucy’s rating than to Diane’s. may use diﬀerent rating values to quantify the same level of appreciation for an item. For example, one user may give the highest rating value to only a few outstanding items, while a less diﬃcult one may give this value to most of the items he likes. This problem is usually addressed by converting the neighbors’ ratings r ones h(r Note that the predicted rating must be converted back to the original scale, hence the h presented in Section 4.1. The prediction approach just described, where the predicted ratings are computed as a weighted average of the neighbors’ ratings, essentially solves a regression problem. Neighborhood-based classiﬁcation, on the other hand, ﬁnds the most likely rating given by a user u to an item i, by having the nearest-neighbors of u vote on this value. The vote v as the sum of the similarity weights of neighbors that have given this rating to i: where δ(r every possible rating value, the predicted rating is simply the value r for which v is the greatest. Example 3. Suppose once again that the two nearest-neighbors of Eric are Lucy and Diane with respective similarity weights 0.75 and 0.15. In this case, ratings 5 and 3 each have one vote. However, since Lucy’s vote has a greater weight than Diane’s, the predicted rating will be ˆr = 5. Let S predicted rating is obtained as: Equation (7) also has an important ﬂaw: it does not consider the fact that users ) [10, 73], giving the following prediction: in the equation. The most common approaches to normalize ratings will be = r) is 1 if r= r, and 0 otherwise. Once this has been computed for A classiﬁcation method that considers normalized ratings can also be deﬁned. be the set of possible normalized values (that may require discretization), the The choice between implementing a neighborhood-based regression or classiﬁcation method largely depends on the system’s rating scale. Thus, if the rating scale is continuous, e.g. ratings in the Jester joke recommender system [25] can take any value between −10 and 10, then a regression method is more appropriate. On the contrary, if the rating scale has only a few discrete values, e.g. “good” or “bad,” or if the values cannot be ordered in an obvious fashion, then a classiﬁcation method might be preferable. Furthermore, since normalization tends to map ratings to a continuous scale, it may be harder to handle in a classiﬁcation approach. where all neighbors have the same similarity weight. As the number of neighbors used in the prediction increases, the rating r will tend toward the mean rating of item i. Suppose item i has only ratings at either end of the rating range, i.e. it is either loved or hated, then the regression approach will make the safe decision that the item’s worth is average. This is also justiﬁed from a statistical point of view since the expected rating (estimated in this case) is the one that minimizes the RMSE. On the other hand, the classiﬁcation approach will predict the rating as the most frequent one given to i. This is more risky as the item will be labeled as either “good” or “bad”. However, as mentioned before, risk taking may be be desirable if it leads to serendipitous recommendations. While user-based methods rely on the opinion of like-minded users to predict a rating, item-based approaches [20, 51, 75] look at ratings given to similar items. Let us illustrate this approach with our toy example. Example 4. Instead of consulting with his peers, Eric instead determines whether the movie “Titanic” is right for him by considering the movies that he has already seen. He notices that people that have rated this movie have given similar ratings to the movies “Forrest Gump” and “Wall-E”. Since Eric liked these two movies he concludes that he will also like the movie “Titanic”. u most similar to item i. The predicted rating of u for i is obtained as a weighted average of the ratings given by u to the items of N Another way to compare these two approaches is by considering the situation This idea can be formalized as follows. Denote by N(i) the items rated by user Example 5. Suppose our prediction is again made using two nearest-neighbors, and that the items most similar to “Titanic” are “Forrest Gump” and “Wall-E,” with respective similarity weights 0.85 and 0.75. Since ratings of 5 and 4 were given by Eric to these two movies, the predicted rating is computed as normalizing ratings with a h: Moreover, we can also deﬁne an item-based classiﬁcation approach. In this case, the items j rated by user u vote for the rating to be given to an unseen item i, and these votes are weighted by the similarity between i and j. The normalized version of this approach can be expressed as follows: When choosing between the implementation of a user-based and an item-based neighborhood recommender system, ﬁve criteria should be considered: • Accuracy: The accuracy of neighborhood recommendation methods depends Again, the diﬀerences in the users’ individual rating scales can be considered by mostly on the ratio between the number of users and items in the system. As will be presented in the Section 4.2, the similarity between two users in userbased methods, which determines the neighbors of a user, is normally obtained by comparing the ratings made by these users on the same items. Consider a system that has 10, 000 ratings made by 1, 000 users on 100 items, and suppose, for the purpose of this analysis, that the ratings are distributed uniformly over the items. Following Table 1, the average number of users available as potential neighbors is roughly 650. However, the average number of common ratings used Table 1: The average number of neighbors and average number of ratings used in the computation of similarities for user-based and item-based neighborhood methods. A uniform distribution of ratings is assumed with average number of ratings per user p = |R|/|U |, and average number of ratings per item q = |R|/|I| • Eﬃciency: As shown in Table 2, the memory and computational eﬃciency of to compute the similarities is only 1. On the other hand, an item-based method usually computes the similarity between two items by comparing ratings made by the same user on these items. Assuming once more a uniform distribution of ratings, we ﬁnd an average number of potential neighbors of 99 and an average number of ratings used to compute the similarities of 10. In general, a small number of high-conﬁdence neighbors is by far preferable to a large number of neighbors for which the similarity weights are not trustable. In cases where the number of users is much greater than the number of items, such as large commercial systems like Amazon.com, item-based methods can therefore produce more accurate recommendations [21, 75]. Likewise, systems that have less users than items, e.g., a research paper recommender with thousands of users but hundreds of thousands of articles to recommend, may beneﬁt more from userbased neighborhood methods [31]. recommender systems also depends on the ratio between the number of users and items. Thus, when the number of users exceeds the number of items, as is it most often the case, item-based recommendation approaches require much less memory and time to compute the similarity weights (training phase) than userbased ones, making them more scalable. However, the time complexity of the online recommendation phase, which depends only on the number of available items and the maximum number of neighbors, is the same for user-based and item-based methods. In practice, computing the similarity weights is much less expensive than the worst-case complexity reported in Table 2, due to the fact that users rate only a few of the available items. Accordingly, only the non-zero similarity weights need to be stored, which is often much less than the number of user pairs. This number can be further reduced by storing for each user only the top N weights, where N is a parameter [75] that is suﬃcient for satisfactory coverage on useritem pairs. In the same manner, the non-zero weights can be computed eﬃciently without having to test each pair of users or items, which makes neighborhood methods scalable to very large systems. Table 2: The space and time complexity of user-based and item-based neighborhood methods, as a function of the maximum number of ratings per user p = max the maximum number of ratings per item q = max of neighbors used in the rating predictions k. • Stability: The choice between a user-based and an item-based approach also • Justiﬁability: An advantage of item-based methods is that they can easily be • Serendipity: In item-based methods, the rating predicted for an item is based on depends on the frequency and amount of change in the users and items of the system. If the list of available items is fairly static in comparison to the users of the system, an item-based method may be preferable since the item similarity weights could then be computed at infrequent time intervals while still being able to recommend items to new users. On the contrary, in applications where the list of available items is constantly changing, e.g., an online article recommender, user-based methods could prove to be more stable. used to justify a recommendation. Hence, the list of neighbor items used in the prediction, as well as their similarity weights, can be presented to the user as an explanation of the recommendation. By modifying the list of neighbors and/or their weights, it then becomes possible for the user to participate interactively in the recommendation process. User-based methods, however, are less amenable to this process because the active user does not know the other users serving as neighbors in the recommendation. the ratings given to similar items. Consequently, recommender systems using this approach will tend to recommend to a user items that are related to those usually appreciated by this user. For instance, in a movie recommendation application, movies having the same genre, actors or director as those highly rated by the user are likely to be recommended. While this may lead to safe recommendations, it does less to help the user discover diﬀerent types of items that he might like as much. Because they work with user similarity, on the other hand, user-based approaches are more likely to make serendipitous recommendations. This is particularly true if the recommendation is made with a small number of nearestneighbors. For example, a user A that has watched only comedies may be very similar to a user B only by the ratings made on such movies. However, if B is fond of a movie in a diﬀerent genre, this movie may be recommended to A through his similarity with B. In the previous section, we have seen that deciding between a regression and a classiﬁcation rating prediction method, as well as choosing between a user-based or item-based recommendation approach, can have a signiﬁcant impact on the accuracy, eﬃciency and overall quality of the recommender system. In addition to these crucial attributes, three very important considerations in the implementation of a neighborhood-based recommender system are 1) the normalization of ratings, 2) the computation of the similarity weights, and 3) the selection of neighbors. This section reviews some of the most common approaches for these three components, describes the main advantages and disadvantages of using each one of them, and gives indications on how to implement them. When it comes to assigning a rating to an item, each user has its own personal scale. Even if an explicit deﬁnition of each of the possible ratings is supplied (e.g., 1=“strongly disagree,” 2=“disagree,” 3=“neutral,” etc.), some users might be reluctant to give high/low scores to items they liked/disliked. Two of the most popular rating normalization schemes that have been proposed to convert individual ratings to a more universal scale are mean-centering and Z-score. The idea of mean-centering [10, 73] is to determine whether a rating is positive or negative by comparing it to the mean rating. In user-based recommendation, a raw rating r average r In the same way, the item-mean-centered normalization of r is transformation to a mean-centered one h(r) by subtracting to rthe of the ratings given by user u to the items in I: where r ization technique is most often used in item-based recommendation, where a rating An interesting property of mean-centering is that one can see right-away if the appreciation of a user for an item is positive or negative by looking at the sign of the normalized rating. Moreover, the module of this rating gives the level at which the user likes or dislikes the item. Example 6. As shown in Figure 2, although Diane gave an average rating of 3 to the movies “Titanic” and “Forrest Gump,” the user-mean-centered ratings show that her appreciation of these movies is in fact negative. This is because her ratings are high on average, and so, an average rating correspond to a low degree of appreciation. Diﬀerences are also visible while comparing the two types of mean-centering. For instance, the item-mean-centered rating of the movie “Titanic” is neutral, instead of negative, due to the fact that much lower ratings were given to that movie. Likewise, Diane’s appreciation for “The Matrix” and John’s distaste for “Forrest Gump” are more pronounced in the item-mean-centered ratings. corresponds to the mean rating given to item i by user in U. This normalis predicted as:P Fig. 2: The user and item mean-centered ratings of Figure 1. Consider, two users A and B that both have an average rating of 3. Moreover, suppose that the ratings of A alternate between 1 and 5, while those of B are always 3. A rating of 5 given to an item by B is more exceptional than the same rating given by A, and, thus, reﬂects a greater appreciation for this item. While mean-centering removes the oﬀsets caused by the diﬀerent perceptions of an average rating, Z-score normalization [30] also considers the spread in the individual rating scales. Once again, this is usually done diﬀerently in user-based than in item-based recommendation. In user-based methods, the normalization of a rating r mean-centered rating by the standard deviation σ Likewise, the z-score normalization of r mean-centered rating by the standard deviation of ratings given to item i: In some cases, rating normalization can have undesirable eﬀects. For instance, imagine the case of a user that gave only the highest ratings to the items he has purchased. Mean-centering would consider this user as “easy to please” and any rating below this highest rating (whether it is a positive or negative rating) would be considered as negative. However, it is possible that this user is in fact “hard to please” and carefully selects only items that he will like for sure. Furthermore, normalizing on a few ratings can produce unexpected results. For example, if a user has entered a single rating or a few identical ratings, his rating standard deviation will be 0, leading to undeﬁned prediction values. Nevertheless, if the rating data is not overly sparse, normalizing ratings has been found to consistently improve the predictions [30, 34]. additional beneﬁt of considering the variance in the ratings of individual users or items. This is particularly useful if the rating scale has a wide range of discrete values or if it is continuous. On the other hand, because the ratings are divided and multiplied by possibly very diﬀerent standard deviation values, Z-score can be more sensitive than mean-centering and, more often, predict ratings that are outside the rating scale. Lastly, while an initial investigation found mean-centering and Z-score to give comparable results [30], subsequent analysis showed Z-score to have more signiﬁcant beneﬁts [34]. other possible approach to remove the problems caused by the rating scale variance is preference-based ﬁltering. The particularity of this approach is that it focuses on predicting the relative preferences of users instead of absolute rating values. Since, an item preferred to another one remains so regardless of the rating scale, predicting relative preferences removes the need to normalize the ratings. More information on this approach can be found in [16, 23, 38, 37]. The similarity weights play a double role in neighborhood-based recommendation methods: 1) they allow to select trusted neighbors whose ratings are used in the prediction, and 2) they provide the means to give more or less importance to these neighbors in the prediction. The computation of the similarity weights is one of the most critical aspects of building a neighborhood-based recommender system, as it can have a signiﬁcant impact on both its accuracy and its performance. A measure of the similarity between two objects a and b, often used in information retrieval, consists in representing these objects in the form of a vector x computing the Cosine Vector (CV) (or Vector Space) similarity [4, 8, 48] between these vectors: In the context of item recommendation, this measure can be employed to compute user similarities by considering a user u as a vector x u has rated item i, and 0 otherwise. The similarity between two users u and v would then be computed as Comparing mean-centering with Z-score, as mentioned, the second one has the Finally, if rating normalization is not possible or does not improve the results, anwhere I measure is that is does not consider the diﬀerences in the mean and variance of the ratings made by users u and v. have been removed is the Pearson Correlation (PC) similarity: Note that this is diﬀerent from computing the CV similarity on the Z-score normalized ratings, since the standard deviation of the ratings in evaluated only on the common items I same idea can be used to obtain similarities between two items i and j [20, 75], this time by comparing the ratings made by users that have rated both these items: While the sign of a similarity weight indicates whether the correlation is direct or inverse, its magnitude (ranging from 0 to 1) represents the strength of the correlation. Example 7. The similarities between the pairs of users and items of our toy example, as computed using PC similarity, are shown in Figure 3. We can see that Lucy’s taste in movies is very close to Eric’s (similarity of 0.922) but very diﬀerent from John’s (similarity of −0.938). This means that Eric’s ratings can be trusted to predict Lucy’s, and that Lucy should discard John’s opinion on movies or consider the opposite. We also ﬁnd that the people that like “The Matrix” also like “Die Hard” but hate “Wall-E”. Note that these relations were discovered without having any knowledge of the genre, director or actors of these movies. than the diﬀerences in ratings given to individual items. Therefore, while computing the item similarities, it may be more appropriate to compare ratings that are centered on their user mean, instead of their item mean. The Adjusted Cosine (AC) similarity [75], is a modiﬁcation of the PC item similarity which compares user-mean-centered ratings: once more denotes the items rated by both u and v. A problem with this A popular measure that compares ratings where the eﬀects of mean and variance The diﬀerences in the rating scales of individual users are often more pronounced In some cases, AC similarity has been found to outperform PC similarity on the prediction of ratings using an item-based method [75]. Several other measures have been proposed to compute similarities between users or items. One of them is the Mean Squared Diﬀerence (MSD) [79], which evaluate the similarity between two users u and v as the inverse of the average squared diﬀerence between the ratings given by u and v on the same items: While it could be modiﬁed to compute the diﬀerences on normalized ratings, the MSD similarity is limited compared to PC similarity because it does not allows to capture negative correlations between user preferences or the appreciation of different items. Having such negative correlations may improve the rating prediction accuracy [29]. [42]. While PC uses the rating values directly, SRC instead considers the ranking of these ratings. Denote by k Fig. 3: The user and item PC similarity for the ratings of Figure 1. Another well-known similarity measure is the Spearman Rank Correlation (SRC) (tied ratings get the average rank of their spot). The SRC similarity between two users u and v is evaluated as: where k tion, described in the last section, by using rankings. On the other hand, this measure may not be the best one when the rating range has only a few possible values, since that would create a large number of tied ratings. Moreover, this measure is typically more expensive than PC as ratings need to be sorted in order to compute their rank. SRC and PC similarity measures, on the MovieLens for diﬀerent values of k, which represents the maximum number of neighbors used in the predictions. For this data, we notice that MSD leads to the least accurate predictions, possibly due to the fact that it does not take into account negative correlations. Also, these results show PC to be slightly more accurate than SRC. Finally, although PC has been generally recognized as the best similarity measure, see e.g. [29], subsequent investigation has shown that the performance of such measure depended greatly on the data [34]. Table 3: The rating prediction accuracy (MAE) obtained on the MovieLens dataset using the Mean Squared Diﬀerence (MSD), Spearman Rank Correlation and Pearson Correaltion (PC) similarity measures. Results are shown for predictions using an increasing number of neighbors k. Because the rating data is frequently sparse in comparison to the number of users and items of a system, it is often the case that similarity weights are computed using is the average rank of items rated by u. The principal advantage of SRC is that it avoids the problem of rating normaliza- Table 3 shows the user-based prediction accuracy (MAE) obtained with MSD, only a few ratings given to common items or made by the same users. For example, if the system has 10, 000 ratings made by 1, 000 users on 100 items (assuming a uniform distribution of ratings), Table 1 shows us that the similarity between two users is computed, on average, by comparing the ratings given by these users to a single item. If these few ratings are equal, then the users will be considered as “fully similar” and will likely play an important role in each other’s recommendations. However, if the users’ preferences are in fact diﬀerent, this may lead to poor recommendations. similarity weight. The principle of these strategies is essentially the same: reduce the magnitude of a similarity weight when this weight is computed using only a few ratings. For instance, in Signiﬁcance Weighting [30, 53], a user similarity weight w is penalized by a factor proportional to the number of commonly rated item, if this number is less than a given parameter γ > 0: In [30, 29], it was found that using γ ≥ 25 could signiﬁcantly improve the accuracy of the predicted ratings, and that a value of 50 for γ gave the best results. However, the optimal value for this parameter is data dependent and should be determined using a cross-validation approach. ing when a weight should be adjusted. A more continuous approach, described in [6], is based on the concept of shrinkage where a weak or biased estimator can be improved if it is “shrunk” toward a null-value. This approach can be justiﬁed using a Bayesian perspective, where the best estimator of a parameter is the posterior mean, corresponding to a linear combination of the prior mean of the parameter (null-value) and an empirical estimator based fully on the data. In this case, the parameters to estimate are the similarity weights and the null value is zero. Thus, a user similarity w where β > 0 is a parameter whose value should also be selected using crossvalidation. In this approach, w no adjustment is made when |I way: Several strategies have been proposed to take into account the signiﬁcance of a A characteristic of signiﬁcance weighting is its use of a threshold γ determin- As reported in [6], a typical value for β is 100. Ratings made by two users on universally liked/disliked items may not be as informative as those made for items with a greater rating variance. For instance, most people like classic movies such as “The Godfather” so basing the weight computation on such movies would produce artiﬁcially high values. Likewise, a user that always rates items in the same way may provide less predictive information than one whose preferences vary from one item to another. quency [10]. Based on the information retrieval notion of Inverse Document Frequency (IDF), a weight λ users that have rated i: In the Frequency-Weighted Pearson Correlation (FWPC), the correlation between the ratings given by two users u and v to an item i is weighted by λ This approach, which was found to improve the prediction accuracy of a user-based recommendation method [10], could also be adapted to the computation of item similarities. More advanced strategies have also been proposed to consider rating variance. One of these strategies, described in [36], computes the factors λ imizing the average similarity between users. If the goal is to predict ratings with a user-based method, more reliable correlation values can be obtained if the target item is considered in their computation. In [5], the user-based PC similarity is extended by weighting the summation terms corresponding to an item i by the similarity between i and the target item j: The item weights w ering the items’ content (e.g., the common genres for movies). Other variations of A recommendation approach that addresses this problem is the Inverse User Frethis similarity metric and their impact on the prediction accuracy are described in [5]. Note, however, that this model may require to recompute the similarity weights for each predicted rating, making it less suitable for online recommender systems. The number of nearest-neighbors to select and the criteria used for this selection can also have a serious impact on the quality of the recommender system. The selection of the neighbors used in the recommendation of items is normally done in two steps: 1) a global ﬁltering step where only the most likely candidates are kept, and 2) a per prediction step which chooses the best candidates for this prediction. In large recommender systems that can have millions of users and items, it is usually not possible to store the (non-zero) similarities between each pair of users or items, due to memory limitations. Moreover, doing so would be extremely wasteful as only the most signiﬁcant of these values are used in the predictions. The pre-ﬁltering of neighbors is an essential step that makes neighborhood-based approaches practicable by reducing the amount of similarity weights to store, and limiting the number of candidate neighbors to consider in the predictions. There are several ways in which this can be accomplished: • Top-N ﬁltering: For each user or item, only a list of the N nearest-neighbors • Threshold ﬁltering: Instead of keeping a ﬁxed number of nearest-neighbors, this • Negative ﬁltering: In general, negative rating correlations are less reliable than and their respective similarity weight is kept. To avoid problems with eﬃciency or accuracy, N should be chosen carefully. Thus, if N is too large, an excessive amount of memory will be required to store the neighborhood lists and predicting ratings will be slow. On the other hand, selecting a too small value for N may reduce the coverage of the recommendation method, which causes some items to be never recommended. approach keeps all the neighbors whose similarity weight’s magnitude is greater than a given threshold w. While this is more ﬂexible than the previous ﬁltering technique, as only the most signiﬁcant neighbors are kept, the right value of w may be diﬃcult to determine. positive ones. Intuitively, this is because strong positive correlation between two users is a good indicator of their belonging to a common group (e.g., teenagers, science-ﬁction fans, etc.). However, although negative correlation may indicate membership to diﬀerent groups, it does not tell how diﬀerent are these groups, or whether these groups are compatible for some other categories of items. While certain experimental investigations [30, 31] have found negative correlations to to ﬁt the needs of the recommender system. For instance, one could discard all negative similarities as well as those that are not in the top-N lists. Once a list of candidate neighbors has been computed for each user or item, the prediction of new ratings is normally made with the k-nearest-neighbors, that is, the k neighbors whose similarity weight has the greatest magnitude. The choice of k can also have a signiﬁcant impact on the accuracy and performance of the system. k typically follows a concave function. Thus, when the number of neighbors is restricted by using a small k (e.g., k < 20), the prediction accuracy is normally low. As k increases, more neighbors contribute to the prediction and the variance introduced by individual neighbors is averaged out. As a result, the prediction accuracy improves. Finally, the accuracy usually drops when too many neighbors are used in the prediction (e.g., k > 50), due to the fact that the few strong local relations are “diluted” by the many weak ones. Although a number of neighbors between 20 to 50 is most often described in the literature, see e.g. [29, 31], the optimal value of k should be determined by cross-validation. of a decrease in accuracy, by basing these recommendations on a few very similar users. For example, the system could ﬁnd the user most similar to the active one and recommend the new item that has received the highest rated from this user. The neighborhood approaches based on rating correlation, such as the ones presented in the previous sections, have three important limitations: • Limited Expressiveness: Traditional neighborhood-based methods determine provide no signiﬁcant improvement in the prediction accuracy, in certain settings they seem to have a positive eﬀect (see e.g., [83]). Whether such correlations can be discarded depends on the data and should be examined on a case-by-case basis. Note that these three ﬁltering approaches are not exclusive and can be combined As shown in Table 3, the prediction accuracy observed for increasing values of On a ﬁnal note, more serendipitous recommendations may be obtained at the cost the neighborhood of users or items using some predeﬁned similarity measure like cosine or PC. Recommendation algorithms that rely on such similarity measures have been shown to enjoy remarkable recommendation accuracy in certain settings. However their performance can vary considerably depending on whether the chosen similarity measures conform with the latent characteristics of the dataset onto which they are applied. • Limited coverage: Because rating correlation measures the similarity between • Sensitivity to sparse data: Another consequence of rating correlation, addressed values [10, 20], such as the middle value of the rating range, or the average user or item rating. A more reliable approach is to use content information to ﬁll out the missing ratings [18, 26, 45, 54]. For instance, the missing ratings can be provided by autonomous agents called ﬁlterbots [26, 45], that act as ordinary users of the system and rate items based on some speciﬁc characteristics of their content. The missing ratings can instead be predicted by a content-based approach [54]. Furthermore, content similarity can also be used “instead of” or “in addition to” rating correlation similarity to ﬁnd the nearest-neighbors employed in the predictions [4, 50, 71, 82]. Finally, data sparsity can also be tackled by acquiring new ratings with active learning techniques. In such techniques, the system interactively queries the user to gain a better understanding of his or her preferences. A more detailed presentation of interactive and session-based techniques is given in Chapter ?? of this book. These solutions, however, also have their own drawbacks. For instance, giving a default values to missing ratings may induce bias in the recommendations. Also, item content may not be available to compute ratings or similarities. lenges: learning-based and graph-based methods. two users by comparing their ratings for the same items, users can be neighbors only if they have rated common items. This assumption is very limiting, as users having rated a few or no common items may still have similar preferences. Moreover, since only items rated by neighbors can be recommended, the coverage of such methods can also be limited. This limitation also applies when two items have only a few or no co-ratings. brieﬂy in Section 3.5, is the fact that the accuracy of neighborhood-based recommendation methods suﬀers from the lack of available ratings. Sparsity is a problem common to most recommender systems due to the fact that users typically rate only a small proportion of the available items [7, 26, 77, 76]. This is aggravated by the fact that users or items newly added to the system may have no ratings at all, a problem known as cold-start [78]. When the rating data is sparse, two users or items are unlikely to have common ratings, and consequently, neighborhood-based approaches will predict ratings using a very limited number of neighbors. Moreover, similarity weights may be computed using only a small number of ratings, resulting in biased recommendations (see Section 4.2.3 for this problem). A common solution for latter problems is to ﬁll the missing ratings with default This section presents two approaches that aim to tackle the aforementioned chal- In the methods of this family the similarity or aﬃnity between users and items is obtained by deﬁning a parametric model that describes the relation between users, items or both, and then ﬁts the model parameters through an optimization procedure. can capture high-level patterns and trends in the data, are generally more robust to outliers, and are known to generalize better than approaches solely based on local relations. In recommender systems, this translates into greater accuracy and stability in the recommendations [46]. Also, because the relations between users and items are encoded in a limited set of parameters, such methods normally require less memory than other types of approaches. Finally, since the parameters are usually learned oﬄine, the online recommendation process is generally faster. divided in two categories: factorization methods and adaptive neighborhood learning methods. These categories are presented in the following sections. Factorization methods [6, 7, 17, 25, 46, 60, 76, 85, 86] address the problems of limited coverage and sparsity by projecting users and items into a reduced latent space that captures their most salient features. Because users and items are compared in this dense subspace of high-level features, instead of the “rating space,” more meaningful relations can be discovered. In particular, a relation between two users can be found, even though these users have rated diﬀerent items. As a result, such methods are generally less sensitive to sparse data [6, 7, 76]. recommender systems: 1) factorization of a sparse similarity matrix, and 2) factorization of a user-item rating matrix. Neighborhood similarity measures like the correlation similarity are usually very sparse since the average number of ratings per user is much less than the total number of items. A simple solution to densify a sparse similarity matrix is to compute a low-rank approximation of this matrix with a factorization method. larities. To simplify the presentation, we will suppose the latter case. We wish to approximate W with a matrix following objective: Using a learning-based method has signiﬁcant advantages. First, such methods Learning-based methods that use neighborhood or similarity information can be There are essentially two ways in which factorization can be used to improve Let W be a symmetric matrix of rank n representing either user or item simiwhere ||M|| “compressed” and less sparse version of W. Finding the factor matrix Q is equivalent to computing the eigenvalue decomposition of W: where D is a diagonal matrix containing the |I| eigenvalues of W, and V is a |I|×|I| orthogonal matrix containing the corresponding eigenvectors. Let V formed by the k principal (normalized) eigenvectors of W, which correspond to the axes of the k-dimensional latent subspace. The coordinates q this subspace is given by the i-th row of matrix Q = V similarities computed in this latent subspace are given by matrix Eigentaste, a matrix W containing the PC similarities between pairs of items is decomposed to obtain the latent subspace deﬁned by the k principal eigenvectors of W. A user u, represented by the u-th row r the plane deﬁned by V In an oﬄine step, the users of the system are clustered in this subspace using a recursive subdivision technique. Then, the rating of user u for an item i is evaluated as the mean rating for i made by users in the same cluster as u. This strategy is related to the well-known spectral clustering method [80]. The problems of cold-start and limited coverage can also be alleviated by factorizing the user-item rating matrix. Once more, we want to approximate the |U |×|I| rating matrix R of rank n by a matrix of users factors and Q a |I|×k matrix of item factors. This task can be formulated as ﬁnding matrices P and Q which minimize the following function: This approach was used to recommend jokes in the Eigentaste system [25]. In The optimal solution can be obtained by the Singular Value Decomposition (SVD) of R: P = U k largest singular values of R, and U singular vectors corresponding to these values. matrix R: most values r to i by u. Although it is possible to assign a default value to r this would introduce a bias in the data. More importantly, this would make the large matrix R dense and, consequently, render impractical the SVD decomposition of R. A common solution to this problem is to learn the model parameters using only the known ratings [6, 46, 84, 86]. For instance, suppose the rating of user u for item i is estimated as where b model paremeters can be learned by minimizing the following objective function: The second term of the function is as a regularization term added to avoid overﬁtting. Parameter λ controls the level of regularization. A more comprehensive description of this recommendation approach can be found in Chapter ?? of this book. method by supposing that the proﬁle of a user u is determined implicitly by the items he or she has rated. Thus, the factor vector of u can be deﬁned as a weighted combination of the factor vectors s In this formulation, α is a normalization constant typically set to α = 1/2, and c a weight representing the contribution of item j to the proﬁle of u. For instance, in the SVD++ model [46] this weight is deﬁned as the bias corrected rating of u for item j: c models, instead use constant weights: c Like the standard SVD model, the parameters of this model can be learned by minimizing the objective function of Equation (33), for instance, using gradient descent optimization. sets of item factors, i.e., q an asymmetric item-item similarity matrix W, where However, there is signiﬁcant problem with applying SVD directly to the rating and bare parameters representing the user and item rating biases. The The SVD model of Equation 32 can be transformed into a similarity-based = r− b− b. Other approaches, such as the FISM [39] and NSVD [69] Using the formulation of Equation 34, a rating ris predicted as Note that, instead of having both user and item factors, we now have two diﬀerent As mentioned in [46], this similarity-based factorization approach has several advantages over the traditional SVD model. First, since there are typically more users than items in a recommender system, replacing the user factors by a combination of item factors reduces the number of parameters in the model, which makes the learning process faster and more robust. Also, by using item similarities instead of user factors, the system can handle new users without having to re-train the model. Finally, as in item-similarity neighborhood methods, this model makes it possible to justify a rating to a user by showing this user the items that were most involved in the prediction. tors of i: This modiﬁcation, which corresponds to ignoring the diagonal entries in the item similarity matrix, avoids the problem of having an item recommending itself and has been shown to give better performance when the number of factors is high. Standard neighborhood-based recommendation algorithms determine the neighborhood of users or items directly from the data, using some pre-deﬁned similarity measure like PC. However, subsequent developments in the ﬁeld of item recommendation have shown the advantage of learning the neighborhood automatically from the data, instead of using a pre-deﬁned similarity measure [43, 46, 56, 72]. A representative neighborhood-learning recommendation method is the SLIM algorithm, developed by Ning et al [65]. In SLIM, a new rating is predicted as a sparse aggregation of existing ratings in a user’s proﬁle, where r ing |I| aggregation coeﬃcients. Essentially, the non-zero entries in w the neighbor items of an item i. error. Standard regularization and sparsity are enforced by penalizing the ` and ` in a regression problem is known as elastic net regularization [90]. This learning process can be expressed as the following optimization problem: In FISM [39], the prediction of a rating ris made without considering the facis the u-th row of the rating matrix R and wis a sparse row vector contain- The neighborhood parameters are learned by minimizing the squared prediction -norm of the parameters. The combination of these two types of regularizers The constraint diag(W) = 0 is added to the model to avoid trivial solutions (e.g., W corresponding to the identity matrix) and ensure that r each type of regularization. Moreover, the non-negativity constraint on W imposes the relations between neighbor items to be positive. Dropping the non-negativity as well as the sparsity constraints has been recently explored in [83], and was shown to work well on several datasets with small number of items with respect to users. Note, however, that without the sparsity constraint the resulting model will be fully dense; a fact that imposes practical limitations on the applicability of such approaches in large item-space regimes. Fig. 4: A simple illustration of SLIM. The method works by ﬁrst building an item-toitem model, based on R. Intuitively, this item model expresses each item (i.e., each column of the original rating matrix R) as a sparse linear combination of the rest of the items (i.e., the other columns of R). Then, given W, new recommendations for a target user u can be readily produced by multiplying the row corresponding to user u (i.e. the u-th row of R), with the learned item model, W. Side information, such as user proﬁle attributes (e.g., age, gender, location) or item descriptions/tags, is becoming increasingly available in e-commerce applications. Properly exploited, this rich source of information can signiﬁcantly improve the performance of conventional recommender systems [1, 3, 88, 81]. during the recommendation process. Parameters β and λ control the amount of co-rating proﬁle of two items is correlated to the properties encoded in their side information [67]. To enforce such correlations in the model, an additional requirement is added, where both the user-item rating matrix R and the item side information matrix F should be reproduced by the same sparse linear aggregation. That is, in addition to satisfying R ∼ RW, the coeﬃcient matrix W should also satisfy F ∼ FW. This is achieved by solving the following optimization problem: The parameter α is used to control the relative importance of the user-item rating information R and the item side information F when they are used to learn W. and F can be too strict. An alternate model relaxes this constraints by imposing these two sets of aggregation coeﬃcients to be similar. Speciﬁcally, it uses an aggregation coeﬃcient matrix Q such that F ∼ FQ and W ∼ Q. Matrices W and Q are learned as the minimizers of the following optimization problem: Parameter β other. in the models described above. These models were shown to outperform the SLIM method without side information, as well as other approaches that use side information, in the top-N recommendation task. A global item model may not always be suﬃcient to capture the preferences of every user; especially when there exist subsets of users with diverse or even opposing preferences. In cases like these training local item models (i.e., item models that are estimated based on user subsets) is expected to be beneﬁcial compared to adopting a single item model for all users in the system. An example of such a case can be seen in Figure 5. Item side information can be integrated in the SLIM model by supposing that the minimize12kR − RWk+α2kF − FWk+β2kWk+ λkWk subject to W ≥ 0,(40) In some cases, requiring that the aggregation coeﬃcients be the same for both R minimize12kR − RWk+α2kF − FQk+β2kW − Qk subject to W, Q ≥ 0, In [67], item reviews in the form of short texts were used as side information top-N recommendations that utilize user-subset speciﬁc models (local models) and a global model. These models are jointly optimized along with computing the userspeciﬁc parameters that weigh their contribution in the production of the ﬁnal recommendations. The underlying model used for the estimation of local and global item similarities is SLIM. k local item-item coeﬃcient matrices S and p estimated. The recommendation score of user u, who belongs to subset p i is estimated as: Term s the target item i. Term s rated by u and target item i, based on the local model that corresponds to user-subset, p, to which user u belongs. Finally, the term g of user u, which controls the involvement of the global and the local components, in the ﬁnal recommendation score. to subsets, and the personalized weights is achieved by alternating minimization. Initially, the users are separated into subsets, using a clustering algorithm. Weights global and the local components. Then the coeﬃcient matrices S and S following two step procedure: Step 1: Estimating local and global models: The training matrix R is split into k training matrices R cides with the u-th row of R, if user u belongs in the p otherwise. In order to estimate the i-th column, s lem: where r eters corresponding to S , S lregularization hyperparameters controlling the sparsity of S , S respectively. The constraint [s GLSLIM [13] aims to address the above issue. In a nutshell, GLSLIM computes Speciﬁcally, GLSLIM estimates a global item-item coeﬃcient matrix S and also ∈ {1, . .. ,k} is the index of the user subset, for which a local matrix Sis depicts the global item-item similarity between the l-th item rated by u and The estimation of the global and the local item models, the user assignments are initialized to 0.5 for all users, in order to enforce equal contribution of the ∈ {1, .. ., k}, as well as personalized weights gare estimated, by repeating the , of matrices S, p∈ {1, .. ., k}, GLSLIM solves the following optimization probsubject to s≥ 0, s≥ 0, ∀p∈ {1, . . ., k} is the i-th column of R; and, β, βare the lregularization hyperparam- Fig. 5: GLSLIM Motivating Example. The ﬁgure shows the training matrices R of two diﬀerent datasets. Both contain two user subsets. Let’s assume that we are trying to compute recommendation scores for item i, and that the recommendations are computed using an item-item similarity-based method. Observe that in Case A there exist a set of items that have been rated solely by users that belong to Subset 1, while also a set of items which have been rated by users in both Subsets. Notice that the similarities of items c and i will be diﬀerent when estimated based on the feedback of (a) Subset 1 alone; (b) Subset 2 alone; and, (c) the complete set of users. Speciﬁcally, their similarity will be zero for the users of Subset 2 (as item i is not rated by the users in that subset), but it will be e.g., l well as e.g., g the locally estimated l in settings like this could help capture potentially diverse user preferences which would otherwise be missed if only a single global model, was computed instead. On the other hand, for datasets like the one pictured in Case B the similarity between e.g., items i and j will be the same, regardless of whether it is estimated globally, or locally for Subset 1, since both items have been rated only by users of Subset 1. ment r property for the local sparse coeﬃcient matrices as well. Step 2: Updating user subsets: With the global and local models ﬁxed, GLSLIM proceeds to update the user subsets. While doing that, it also determines the personalized weight g on minimizing the squared error of Equation (42) for user u who belongs to subset p, over all items i. Setting the derivative of the squared error to 0, yields: GLSLIM tries to assign each user u to every possible subset, while computing the weight g is not used. Similarly, the constraints [s]= 0 ∀p∈ {1, . .. ,k}, enforce this that the user would have, if assigned to that subset. Then, for every subset and user u, the training error is computed and the user is assigned to the subset for which this error is minimized (or remains to the original subset, if no diﬀerence in training error occurs). Step 2, becomes smaller than 1% of |U|. It is empirically observed that initializing subset assignments with the CLUTO [40] clustering algorithm, results in a signiﬁcant reduction of the number of iterations till convergence. detail the qualitative performance of GLSLIM, and suggest that it improves upon the standard SLIM, in several datasets. In graph-based approaches, the data is represented in the form of a graph where nodes are users, items or both, and edges encode the interactions or similarities between the users and items. For example, in Figure 6, the data is modeled as a bipartite graph where the two sets of nodes represent users and items, and an edge connects user u to item i if there is a rating given to i by u in the system. A weight can also be given to this edge, such as the value of its corresponding rating. In another model, the nodes can represent either users or items, and an edge connects two nodes if the ratings corresponding two these nodes are suﬃciently correlated. The weight of this edge can be the corresponding correlation value. Fig. 6: A bipartite graph representation of the ratings of Figure 1 (only ratings with value in {2, 3, 4} are shown). a user u for an item i using only the nodes directly connected to u or i. Graphbased approaches, on the other hand, allow nodes that are not directly connected to inﬂuence each other by propagating information along the edges of the graph. The greater the weight of an edge, the more information is allowed to pass through it. Also, the inﬂuence of a node on another should be less if the two nodes are further away in the graph. These two properties, known as propagation and attenuation [27, 35], are often observed in graph-based similarity measures. Steps 1 and 2, are repeated until the number of users who switch subsets, in Furthermore, a comprehensive set of experiments conducted in [13] explore in In these models, standard approaches based on correlation predict the rating of ommend items in two diﬀerent ways. In the ﬁrst approach, the proximity of a user u to an item i in the graph is used directly to evaluate the relevance of i to u [21, 27, 35]. Following this idea, the items recommended to u by the system are those that are the “closest” to u in the graph. On the other hand, the second approach considers the proximity of two users or item nodes in the graph as a measure of similarity, and uses this similarity as the weights w method [21, 52]. In path-based similarity, the distance between two nodes of the graph is evaluated as a function of the number and of paths connecting the two nodes, as well as the length of these paths. user u to an item i. The adjacency matrix A of the user-item bipartite graph can be deﬁned from R as The association between a user u and an item i can be deﬁned as the sum of the weights of all distinctive paths connecting u to v (allowing nodes to appear more than once in the path), whose length is no more than a given maximum length K. Note that, since the graph is bipartite, K should be an odd number. In order to attenuate the contribution of longer paths, the weight given to a path of length k is deﬁned as α, where α ∈ [0, 1]. Using the fact that the number of length k paths between pairs of nodes is given by A Katz measure [41]. Note that this measure is closely related to the Von Neumann Diﬀusion kernel [22, 44, 47] and the Exponential Diﬀusion kernel The transitive associations captured by graph-based methods can be used to rec- Let R be once again the |U|×|I| rating matrix, where ris the rating given by This method of computing distances between nodes in a graph is known as the where A these association values may require extensive computational resources. In [35], spreading activation techniques are used to overcome these limitations. Essentially, such techniques work by ﬁrst activating a selected subset of nodes as starting nodes, and then iteratively activating the nodes that can be reached directly from the nodes that are already active, until a convergence criterion is met. this section, focus on ﬁnding relevant associations between users and items, not predicting exact ratings. Therefore, such methods are better suited for item retrieval tasks, where explicit ratings are often unavailable and the goal is to obtain a short list of relevant items (i.e., the top-N recommendation problem). Transitive associations in graph-based methods can also be deﬁned within a probabilistic framework. In this framework, the similarity or aﬃnity between users or items is evaluated as a probability of reaching these nodes in a random walk. Formally, this can be described with a ﬁrst-order Markov process deﬁned by a set of n states and a n×n transition probability matrix P such that the probability of jumping from state i to j at any time-step t is Denote π(t) the vector containing the state probability distribution of step t, such that π Moreover, under the condition that P is row-stochastic, i.e. process converges to a stable distribution vector π(∞) corresponding to the positive eigenvector of P form of a weighted graph having a node for each state, and where the probability of jumping from a node to an adjacent node is given by the weight of the edge connecting these nodes. = I. In recommender systems that have a large number of users and items, computing Path-based methods, as well as the other graph-based approaches described in (t) = Pr(s(t) = i), the evolution of the Markov chain is characterized by A recommendation approach, based on the PageRank algorithm for ranking Web pages [11], is ItemRank [27]. This approach ranks the preferences of a user u for unseen items i as the probability of u to visit i in a random walk of a graph in which nodes correspond to the items of the system, and edges connects items that have been rated by common users. The edge weights are given by the |I|×|I| transition probability matrix P for which p of a user to rate and item j if it has rated an item i. adjacent node with ﬁxed probability α, or “teleport” to any node with probability (1 − α). Let r of user u to teleport to other nodes is given by vector d deﬁnitions, the state probability distribution vector of user u at step t+1 can be expressed recursively as For practical reasons, π the distribution as uniform, i.e. π (48), until convergence. Once π to u the item i for which π Other distance measures based on random walks have been proposed for the recommendation problem. Among these are the average ﬁrst-passage time and the average commute time [21, 22]. The average ﬁrst-passage time m( j|i) [68] is the average number of steps needed by a random walker to reach a node j for the ﬁrst time, when starting from a node i , j. Let P be the n×n transition probability matrix, m( j|i) can be obtained expressed recursively as A problem with the average ﬁrst-passage time is that it is not symmetric. A related measure that does not have this problem is the average commute time n(i, j) = m( j | i) + m(i| j) [24], corresponding to the average number of steps required by a random walker starting at node i , j to reach node j for the ﬁrst time and go back to i. This measure has several interesting properties. Namely, it is a true distance measure in some Euclidean space [24], and is closely related to the well-known property of resistance in electrical networks and to the pseudo-inverse of the graph Laplacian matrix [21]. As in PageRank, the random walk can, at any step t, either jump using P to an nodes of a bipartite graph representing the interactions of users and items in a recommender system. For each user u there is a directed edge from u to every item i ∈ I edge from each item i to every user u ∈ U times can be used in two diﬀerent ways: 1) recommending to u the item i for which n(u, i) is the smallest, or 2) ﬁnding the users nearest to u, according to the commute time distance, and then suggest to u the item most liked by these users. 5.2.3 Combining Random Walks and Neighborhood-learning Methods Neighborhood-learning methods have been shown to achieve high top-n recommendation accuracy while being scalable and easy to interpret. The fact, however, that they typically consider only direct item-to-item relations imposes limitations to their quality and makes them brittle to the presence of sparsity, leading to poor itemspace coverage and substantial decay in performance. A promising direction towards ameliorating such problems involves treating item models as graphs onto which random-walk-based techniques can then be applied. However directly applying random walks on item models can lead to a number of problems that arise from their inherent mathematical properties and the way these properties relate to the underlying top-n recommendation task. item-to-item graph with transition probabilities proportional to the proximity scores depicted by an item model W. If the starting distribution of this walker reﬂects the items consumed by a particular user u in the past, the probability the walker lands on diﬀerent nodes after K steps provide an intuitive measure of proximity that can be used to rank the nodes and recommend items to user u accordingly. diag(W1) dations for user u can be produced e.g., by leveraging the K-step landing distribution of a walk rooted on the items consumed by u; or by computing the limiting distribution of a random walk with restarts on S , using PageRank model [11] with teleportation vector φ stationary distribution can be expressed [49] as In [21], the average commute time is used to compute the distance between the , and the weight of this edge is simply 1/|I|. Likewise, there is a directed In particular, imagine of a random walker jumping from node to node on an Concretely, if we denote the transition probability matrix of the walk S = W where 1 is used to denote the vector of ones, personalized recommenas the restarting distribution. The latter approach is the well-known personalized Clearly, both schemes harvest the information captured in the K-step landing probabilities {φ of steps K increases? For how long will they still be signiﬁcantly inﬂuenced by user’s preferences φ probabilities will converge to a unique stationary distribution irrespectively of the initialization of the walk. This means that for large enough K, the K-step landing probabilities will no longer be “personalized,” in the sense that they will become independent of the user-speciﬁc starting vector φ ing equilibrium, the quality of these vectors in terms of recommendation will start to plummet as more and more probability mass gets concentrated to the central nodes of the graph. Note, that the same issue arises for simple random walks that act directly on the user-item bipartite network, and has lead to methods that typically consider only very short-length random walks, and need to explicitly re-rank the Kstep landing probabilities, in order to compensate for the inherent bias of the walk towards popular items [14]. However, longer random-walks might be necessary to capture non-trivial multi-hop relations between the items, as well as to ensure better coverage of the itemspace. RecWalk [61, 62] addresses the aforementioned challenges, and resolves this longvs short-length walk dilemma through the construction of a nearly uncoupled random walk [58, 63] that gives full control over the stochastic dynamics of the walk towards equilibrium; provably, and irrespectively of the dataset or the speciﬁc item model onto which it is applied. Intuitively, this allows for prolonged and eﬀective exploration of the underlying network while keeping the inﬂuence of the user-speciﬁc initialization strong. follows: Consider a random walker jumping from node to node on the user-item bipartite network. Suppose the walker currently occupies a node c ∈ U ∪ I. In order to determine the next step transition the walker tosses a biased coin that yields heads with probability α and tails with probability (1 − α): 1. If the coin-toss yields heads, then: 2. If the coin-toss yields tails, then: S}. But, how do these landing probabilities behave as the number Markov chain theory ensures that when S is irreducible and aperiodic the landing From a random-walk point of view, the RecWalk model can be described as a. if c ∈ U, the walker jumps to one of the items rated by the current user (i.e., the user corresponding to the current node c) uniformly at random; b. if c ∈ I, the walker jumps to one of the users that have rated the current item uniformly at random; a. if c ∈ U , the walker stays put; The stochastic process that describes this random walk is deﬁned to be a homogeneous discrete time Markov chain with state space U ∪ I; i.e., the transition probabilities from any given node c to the other nodes, are ﬁxed and independent of the nodes visited by the random walker before reaching c. An illustration of the RecWalk model is given in Figure 7. walker can be usefully expressed as a weighted sum of two stochastic matrices H and M as where 0 < α < 1, is a parameter that controls the involvement of these two components in the ﬁnal model. Matrix H can be thought of as the transition probability matrix of a simple random walk on the user-item bipartite network. Assuming that the rating matrix R has no zero columns and rows, matrix H can be expressed as Matrix M, is deﬁned as where I ∈ R trix designed to capture relations between the items. In particular, given an item model with non-negative weights W (e.g., the aggregation matrix produced by a SLIM model), matrix M strategy: The ﬁrst term divides all the elements by the maximum row-sum of W and the second enforces stochasticity by adding residuals to the diagonal, appropriately. The motivation behind this deﬁnition is to retain the information captured by the relative diﬀerences of the item-to-item relations in W. This prevents items that are loosely related to the rest of the itemspace to disproportionately inﬂuence the inter-item transitions and introduce noise to the model. captured in the successive landing probability distributions of a walk initialized in a user-speciﬁc way. Two simple recommendation strategies that were considered in [61] are: b. if c ∈ I, the walker jumps to a related item abiding by an item-to-item transition probability matrix M, that is deﬁned in terms of an underlying item model. The transition probability matrix P that governs the behavior of the random In RecWalk the recommendations are produced by exploiting the information Fig. 7: RecWalk Illustration. Maroon colored nodes correspond to users; Gold colored nodes correspond to items. In [62] it was shown that both approaches manage to boost the quality of several base item models on top of which they were built. Using fsSLIM [66] with small number of neighbors as a base item model, in particular, was shown to achieve state-of-theart recommendation performance, in several datasets. At the same time RecWalk was found to dramatically increase itemspace coverage of the produced recommendations, in every considered setting. This was true both for RecWalk as for RecWalk the probability the random walker lands on node i after K steps, given that the starting node was u. In other words, the recommendation score for item i is given by the corresponding elements of where e∈ Ris a vector that contains the element 1 on the position that corresponds to user u and zeros elsewhere. The computation of the recommendations is performed by K sparse-matrix-vector products with matrix P, and it entails Θ(K nnz(P)) operations, where nnz(P) is the number of nonzero elements in P. : The recommendation score of user u for item i is deﬁned to be the element that corresponds to item i in the limiting distribution of a random walk with restarts on P, with restarting probability η and restarting distribution e: The limiting distribution in (56) can be computed eﬃciently using e.g., the power method, or any specialized PageRank solver. Note that this variant of RecWalk also comes with theoretical guarantees for item-space coverage for every user in the system, regardless of the base item model W used in the deﬁnition of matrix M[62]. Motivation: Personalization of the recommendation vectors in the graph-based schemes we have seen thus far, comes from the use of a user-speciﬁc initialization, or a user-speciﬁc restarting distribution. However, the underlying mechanism for propagating user preferences, across the itemspace (i.e., the adopted diﬀusion function, or the choice of the K-step distribution) is ﬁxed for every user in the system. From a user modeling point of view this translates to the implicit assumption that every user explores the itemspace in exactly the same way—overlooking the reality that diﬀerent users can have diﬀerent behavioral patterns. The fundamental premise of PerDif [57] is that the latent item exploration behavior of the users can be captured better by user-speciﬁc preference propagation mechanisms; thus, leading to improved recommendations. an underlying item model. At each step the users might either decide to go forth and discover items related to the ones they are currently considering, or return to their base and possibly go down alternative paths. Diﬀerent users, might explore the itemspace in diﬀerent ways; and their behavior might change throughout the exploration session. The following stochastic process, formalizes the above idea: The PerDIF Item Discovery Process: Consider a random walker carrying a bag of K biased coins. The coins are labeled with consecutive integers from 1 to K. Initially, the random walker occupies the nodes of graph according to distribution φ. She then ﬂips the 1st coin: if it turns heads (with probability µ diﬀerent node in the graph abiding by the probability matrix P; if it turns tails (with probability 1 − µ She then ﬂips the 2nd coin and she either follows P with probability µ to φ with probability (1 − µ At the k-th step the transitions of the random walker are completely determined by the probability the k-th coin turning heads (µ restarting distribution φ. Thus, the stochastic process that governs the position of the random walker over time is a time-inhomogeneous Markov chain with state space the nodes of the graph, and transition matrix at time k given by The node occupation distribution of the random walker after the last transition can therefore be expressed as tribution φ outcome of the aforementioned item exploration process yields a meaningful distribution over the items that can be used for recommendation. PerDif tackles this task as follows: PerDif proposes a simple model of personalized item exploration subject to Given an item transition probability matrix P, and a user-speciﬁc restarting dis- , the goal is to ﬁnd a set of probabilities µ=µ, . . ., µso that the Learning the personalized probabilities: For each user u we randomly sample one item she has interacted with (henceforth referred to as the ‘target’ item) alongside step item exploration process rooted on φ target item while keeping the probabilities of the negative items low. Concretely, upon deﬁning a vector h and zeros for the negative items, we learn µ where µ arrange the elements of the vector φ items comprising h can be computed as Leveraging the special properties of the stochastic matrix G the above non-linear optimization problem can be solved eﬃciently. In particular, it can be shown [57] that the optimization problem (59) is equivalent to where ∆ name. In particular, the task of ﬁnding personalized probabilities for the item exploration process, reduces to that of ﬁnding personalized diﬀusion coeﬃcients ω over the space of the ﬁrst K landing probabilities of a walk rooted on φ inition of S a simple forward recurrence [57]. Taking into account the fact that in recommendation settings K will typically be small and φ row-by-row, and solving the (K + 1)-dimensional convex quadratic problem unseen items, and we ﬁt µso that the node occupancy distribution after a K- = [µ], ∀i, and Eis a (I × (τ+ 1)) matrix designed to select and re- The above result simpliﬁes learning µsigniﬁcantly. It also lends PerDif its can be performed very eﬃciently (typically in a matter of milliseconds even in large scale settings). parametrising the diﬀusion coeﬃcients within a family of known diﬀusions. This motivates the parameterized variant of PerDif with ∆ selected diﬀusion coeﬃcients (e.g., PageRank [11] coeﬃcients for several damping factors, heat kernel [15] coeﬃcients for several temperature values etc.), normalized to sum to one. Upon obtaining γ bilities directly, PerDif determined such weights (i.e., the rows of D), thus allowing one to endow ω desired properties, relevant to the speciﬁc recommendation task at hand. Furthermore, the use of matrix D can improve the robustness of the personalized diﬀusions in settings where the recommendation quality of the individual landing distributions comprising S fusions within the PerDif framework can also provide useful information arising from the analysis of the learned diﬀusion coeﬃcients, ω terpretation of the model parameters (µ diﬀusion space) allows utilizing the learned model parameters to identify users for which the model will most likely lead to poor predictions, at training-time—thereby aﬀording preemptive interventions to handle such cases appropriately. This aﬀords Fig. 8: Personalized Diﬀusions on the User-Item Bipartite Network. Moreover, working on the space of landing probabilities can also facilitate = {y : y1 = 1, y ≥ 0} and D ∈ Rdeﬁned such that its rows contain pre- While PerDiflearns ωby weighing the contributions of the landing proba- Besides, its merits in terms of recommendation accuracy, personalizing the difa level of transparency that can prove particularly useful in practical settings (for the technical details on how this can be achieved see [57]). One of the earliest approaches proposed for the task item recommendation, neighborhood-based recommendation still ranks among the most popular methods for this problem. Although quite simple to describe and implement, this recommendation approach has several important advantages, including its ability to explain a recommendation with the list of the neighbors used, its computational and space eﬃciency which allows it to scale to large recommender systems, and its marked stability in an online setting where new users and items are constantly added. Another of its strengths is its potential to make serendipitous recommendations that can lead users to the discovery of unexpected, yet very interesting items. eral important decisions. Perhaps the one having the greatest impact on the accuracy and eﬃciency of the recommender system is choosing between a user-based and an item-based neighborhood method. In typical commercial recommender systems, where the number of users far exceeds the number of available items, item-based approaches are typically preferred since they provide more accurate recommendations, while being more computationally eﬃcient and requiring less frequent updates. On the other hand, user-based methods usually provide more original recommendations, which may lead users to a more satisfying experience. Moreover, the diﬀerent components of a neighborhood-based method, which include the normalization of ratings, the computation of the similarity weights and the selection of the nearestneighbors, can also have a signiﬁcant inﬂuence on the quality of the recommender system. For each of these components, several diﬀerent alternatives are available. Although the merit of each of these has been described in this document and in the literature, it is important to remember that the “best” approach may diﬀer from one recommendation setting to the next. Thus, it is important to evaluate them on data collected from the actual system, and in light of the particular needs of the application. performance of neighborhood-based approaches, by automatically extracting the most representative neighborhoods based on the available data. Such models achieve state-of-the-art recommendation accuracy, however their adoption imposes additional computational burden that needs to be considered in light of the particular characteristics of the recommendation problem at hand. Finally, when the performance of a neighborhood-based approach suﬀers from the problems of limited coverage and sparsity, one may explore techniques based on dimensionality reduction or graphs. Dimensionality reduction provides a compact representation of users and items that captures their most signiﬁcant features. An advantage of such approach is that it allows to obtain meaningful relations between pairs of users or items, even In the implementation of a neighborhood-based approach, one has to make sev- Modern machine-learning-based techniques can be used to further increase the though these users have rated diﬀerent items, or these items were rated by diﬀerent users. On the other hand, graph-based techniques exploit the transitive relations in the data. These techniques also avoid the problems of sparsity and limited coverage by evaluating the relationship between users or items that are not “directly connected”. However, unlike dimensionality reduction, graph-based methods also preserve some of the “local” relations in the data, which are useful in making serendipitous recommendations.