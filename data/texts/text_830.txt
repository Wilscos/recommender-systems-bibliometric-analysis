<title>Masked Deep Q-Recommender for Eective estion Scheduling</title> <title>KEUNHYUNG CHUNG , DAEHAN KIM , SANGHEON LEE, and GUIK JUNG,</title> TmaxEdu Inc., Many online learning platforms like Coursera and Udacity are getting more and more attention in this era of distant learning. Unlike face-to-face learning environments, online learning has limited clues to grasp student knowledge status, since video lectures, question sheets, or textbooks are one-way material where interactions are scarce. To facilitate eective online learning, two main questions need to be solved: • How can we quantitatively measure student performance on certain questions or concepts? How can we design a personal learning schedule in a way that maximizes his/her performance within a given period? <title>arXiv:2112.10125v1  [cs.AI]  19 Dec 2021</title> recommender is then reinforced by reward which consists of a change in student performance. Here, the student performance can be dened as the average correct probability over the student’s test range. Experimental results show that the proposed method improves 11.2% more knowledge level increase over expert-selected recommendation policy in average test range when evaluated in a math question dataset collected AIHUBmath. RL is a branch of machine learning that is used to optimize a continuous decision problem. In RL, the agent, the acting subject, repeatedly interacts with the environment. At each time-step , the agent observes the state of the environment, which is described as , and the agent determines action according to the state . The mapping function that receives the state as input and decides the action is called a policy , and it can be described as = 𝜋 (𝑠 . After the agent selects action , the environment takes the state-action pair (𝑠 , 𝑎 and transits to the next state . The environment also emits a reward for each instantaneous transition (𝑠 , 𝑎 , 𝑠 . In the next type-step 𝑡 + 1, the agent receives the next state and decides the next action by the policy , and this process continues until the end of an episode. The goal of RL is to nd a policy that maximizes the discounted sum of future rewards received from the environment. This discounted sum of future rewards are called as return and return at time is dened as , where is a discount factor and is the time-step at which the episode ends. Q-learning [ 18 ] is one of the popular algorithms for nding a policy that maximizes return. Q-learning uses the optimal action-value function (𝑠, 𝑎) which is the expected return achievable by following the best policy, after seeing some state and then taking some action (𝑠, 𝑎) = max E[𝑅 |𝑠 = 𝑠, 𝑎 = 𝑎, 𝜋] . This algorithm nd (𝑠, 𝑎) by updating an action-value function (which is also called as Q-function) repeatedly using an important theory know as the Bellman equation, Using iteration (𝑠, 𝑎) ← 𝑟 +𝛾 max (𝑠 , 𝑎 , a Q-function can converges to the optimal Q-function ( (𝑠, 𝑎) → (𝑠, 𝑎), ∀(𝑠, 𝑎) as 𝑖 → ∞)[17] and 𝑟 + 𝛾 max (𝑠 , 𝑎 ) is called the target value. In practice, we approximate Q-function because it is impossible to represent all possible 𝑄 (𝑠, 𝑎) separately if there are a huge number of state-action pairs (𝑠, 𝑎) . There have been many follow-up studies that approximate Q-function, and DQN [ 10 11 ] is the rst to use convolutional neural networks to approximate Q-function. DQN trains an agent to play Atari 2600 games [ ] and use Q-function (𝑠, 𝑎) where is a set of raw pixel images from Atari 2600 games and are parameters of neural networks. This algorithm use replay memory [ ] which saves the transition (𝑠 , 𝑎 , 𝑟 , 𝑠 when an agent plays with the environment and randomly samples previous transitions when the agent is trained. Another characteristic of DQN is that it uses another Q-network called target network for the stability of training. The target network is used to calculate the target value 𝑟 + 𝛾 max (𝑠 , 𝑎 where are parameters of the target network and are only updated with original Q-network parameters for every some intervals. In this paper, we used a variant of DQN algorithm to schedule the questions eciently. KT is a task that evaluates a student’s knowledge level based on log data accumulated while learning. Student log data has various types of features such as timestamp, student action type, and time spent on action. Among them, question information and correct answers are mainly used for the KT task. The KT model infers the student’s mastery levels one each knowledge concept based on the student’s past question-solving log data. The output of the model is the probability that the student will correct the question corresponding to each concept, which can be interpreted as the student’s mastery of the corresponding knowledge concept. For the concept of the question provided to the student at time , the correct answer predicted by the model as , and the feedback result from the student as 𝑓 , the model is trained through the following binary cross-entropy loss. The rst proposed deep learning-based KT technique is the Deep Knowledge Tracing (DKT) [ 14 ]. They used a recurrent neural network (RNN) [ 20 ] that mainly deals with time-series data, focusing on the fact that the log data of the student’s question solving is sequential data. For the input vectors , ..., 𝑥 of the model, the student’s knowledge concept mastery probability 𝑦 , ...,𝑦 is calculated as follows. where is the hidden state vector of the RNN, which is information that implies the student’s question-solving results up to time t. The parameters of the model are composed of an input weight matrix , a hidden state weight matrix , an output weight matrix , and biases, which are updated through training. After the DKT model was proposed, more advanced deep learning models than RNN dealing with sequential data were applied to the KT task. In this paper, we used the Bi-LSTM-based (NPA) model with self-attention [ ] as a student simulator. Unlike the original NPA model, we use the diculty of question along with the knowledge concept of the question and the results of student feedback. The diculty of the question aects the change in the student’s knowledge level [ ]. Therefore, the KT model using this information can expect more precise knowledge level prediction. Also, using the student’s knowledge level based on the diculty of the question, our recommender can schedule questions by specifying the diculty of the question. where is a knowledge level on (𝑐, 𝑑) measured by the KT model. We also assumed that the student’s knowledge level changes once a day for the simplicity of the scheduling simulation. To apply RL framework to the question scheduling simulation, we model the scheduling scenario as a Markov Decision Process (Figure 1). 3.1.1 State. The state is the student’s question solving history and it can be composed of knowledge concept of the question , a student’s feedback result (i.e., correct or wrong), and question diculty . We denote (𝑐, 𝑓 , 𝑑) as a question-feedback and state at time 𝑡 is a sequence of question-feedbacks, i.e., the diculty 𝑑 has three levels which are easy, medium, hard. 3.1.2 Action. At each time , MDQR can select which knowledge concept of the question and at what diculty level of question to assign to the student, i.e., 3.1.3 State Transition. After the student solve the question which is recommended by action = (𝑐 , 𝑑 the student’s feedback result is sampled by probability distribution 𝑃 (𝑓 |𝑠 , 𝑎 . The probability 𝑃 (𝑓 |𝑠 , 𝑎 is a prediction of whether the feedback result will be correct when the current state is and action (a recommended question) is 𝑎 . We modeled The function 𝑃 by KT (Figure 2). Algorithm 1 Masked Deep Q-Recommender to 𝑒 do ∈ 𝐶 with probability 𝜖 ← argmax (𝑠 , 𝑎) and observe reward 𝑟 and next state 𝑠 from the student simulator , 𝑎 , 𝑟 , 𝑠 ) in replay memory 𝐷 to 𝑒 do = (𝑠 , 𝑎 , 𝑟 , 𝑠 ) from 𝐷 , 𝑄 , 𝑄 , 𝐶 ): 3.1.4 Reward. The purpose of our recommendation model is to raise the student’s knowledge level above the initial state. We assumed that the probability of correcting a question is the student’s level of knowledge. Therefore, the reward at time 𝑡 is calculated using the correctness probability function 𝑃 as follows: Where (𝑃 (𝑓 = 1|𝑠 , 𝑎) − 𝑃 (𝑓 = 1|𝑠 , 𝑎)), (9) and ψ, if 𝑎 (10) 0, otherwise represents the increments in the knowledge level at state compared to state is a set of questions in test range e. is a penalty term that prevents duplicate recommendations for the same question. We give a duplicate penalty ψ if the recommended question was included in the last 20 questions. We use the AIHUBmath dataset from National Information society Agency. AIHUBmath is a log of solving math questions for grades 7-9. There are 707,450 question-solving interactions from 4,673 students. The average number of interactions per student is 153.84. Table 1 shows examples of knowledge concepts contained within each test range in AIHUBmath. We simulate the following three recommended methods and compare their performance: random, expert, MDQR. Random is a method in which the concept and diculty of questions are randomly asked within a specic test range. Expert ask questions in the order of the curriculum within a specic test range. The diculty level of the question is adaptively adjusted according to the student’s previous question-solving record. If the previous question is correct, the next question is more dicult than the previous question (until it reaches the highest diculty), and if it is wrong, it is more easy than the previous question diculty (until it reaches the lowest diculty). MDQR recommends a question 𝑎 which maximizes 𝑄 (𝑠, 𝑎) within a specic test range. Assuming that the question is recommended according to the current grade of the student, the scheduling simulation was also conducted so that the question was asked within a specic range. In addition, we conducted the scheduling simulation with 20 questions a day for 2 weeks. As shown in Table 2, in most test ranges, the MDQR increased the student’s knowledge level the most during a given period. Expert scheduler generally performed better than the random scheduler but showed worse results in some test ranges. When looking at the average performance in each test range, MDQR showed 21.3% increase of knowledge level while Expert showed a 10% increase. Figure 3 shows the change in the student’s knowledge level over two weeks when the simulation was conducted with each question scheduling method. MDQR and the random scheduler show a gradual rise in knowledge level, whereas the expert scheduler shows unstable changes in knowledge level. MDQR and random scheduler always recommend concepts of question in a dierent order, whereas expert scheduler always recommends concepts of question in the same order according to the curriculum, with only dierent question diculty. This shows that certain concepts cause a student’s knowledge level to increase on average when solving questions on this concept while some concepts lower the knowledge level. There have been several attempts by many researchers to apply AI techniques to various elds in the educational domain [ ]. Among them, the AI technique was mainly applied to the KT to analyze the knowledge level of the learner and the learning content scheduling technique for the ecient learning of the learner. In this section, we describe research in which AI technology is applied to KT and learning content scheduling. KT is a technique to analyze and derive students’ knowledge levels based on sequential learning log data. Corbett et al. [ ] proposed Bayesian Knowledge Tracing (BKT), which analyzes the knowledge level using the Hidden Markov Model [ 15 ] based on the correlation between the student’s question-solving result and the knowledge level. Piech et al. [ 14 ] proposed Deep Knowledge Tracing (DKT), which analyzes knowledge level based on LSTM, which is known to be procient in dealing with sequential data. After DKT was proposed, deep learning-based KT techniques such as key-value memory network-based model (DKVMN) [ 21 ] and attention-based model (SAKT) 12 ] were proposed and showed high performance. In this paper, we utilized a deep learning-based KT model for a student question-solving simulator. In addition to the KT technique, approaches to schedule appropriate learning content for students to learn eciently have been widely studied. The key point of the learning content scheduling technique is to schedule the learning contents in the optimal order to maximize the evaluation measure such as the student’s knowledge level or test score. Ai et al. [ ] proposed a reinforcement learning-based approach to schedule exercises in an online learning system. Their method recommends the next question based on the current student’s knowledge level. Specically, they utilized reinforcement learning to derive an exercise that maximizes the reward calculated as the average of the student’s knowledge level. They modeled the learning content scheduling process as a Partially Observable Markov Decision Process (POMDP) [ 19 ] and applied reinforcement learning based on the Trust Region Policy Optimization (TRPO) [ 16 ] algorithm. In contrast, we applied a deep learning-based reinforcement learning model that autonomously learns content scheduling policies. Also, we solved the problem of recommending the same question repeatedly by introducing a scheduling penalty. Loh et al. [ ] suggested a question recommendation algorithm that can eectively raise a student’s test score. They explained the phenomenon that recommendation algorithm based on the knowledge level of the student simply recommends only questions that a student is most likely to get right. To overcome this problem, they also considered the expected test scores assuming the student got the recommended questions right. They implemented a linear approximation-based student correct rate prediction model and a Bi-LSTM-based test score prediction model to predict students’ test scores. Similarly, we introduce the concept of a penalty to solve the problem where the same question is recommended as duplicate. However, there are three dierences from this study: First, we used the student’s average knowledge level as a scheduling outcome measure instead of predictive test scores. Second, their method nds the next question with a greedy search based on the test score prediction model, whereas we nd the optimal question sequence through a deep learning-based reinforcement learning model. Third, our study has the additional constraint that it should be scheduled among the questions that exist within the specic test range. In addition, various course scheduling techniques to which articial intelligence models are applied have been proposed. For example, a technique using reinforcement learning to determine the optimal order and number of activities provided within a 90-minute online class [ ], and an approach to recommend a university course using catalog description and previous course enrollment history [ 13 ], etc. However, the purpose of our study is to schedule questions within a specic range, which is dierent from the corresponding studies. This paper proposed MDQR, the rst scheduling model that can adjust the recommendation range according to the student’s test range. MDQR is a model that improved the DQN algorithm of reinforcement learning to t our scheduling scenario. The purpose of the scheduling model is to maximize the learner’s knowledge level within a limited period of time. In order to examine the performance of MDQR, we conducted a comparative evaluation between a random scheduler and an expert scheduler. Experimental results showed that MDQR outperforms in terms of learner’s knowledge gain than other baselines in most of the test ranges.