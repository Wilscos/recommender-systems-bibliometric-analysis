University of Science and TechnologyWestlake University alexandros.karatzoglou@gmail.commathshenli@gmail.com Deep neural networks (DNN) have achieved great success in the recommender systems (RS) domain. However, to achieve remarkable performance, DNN-based recommender models often require numerous parameters, which inevitably bring redundant neurons and weights, a phenomenon referred to as over-parameterization. In this paper, we plan to exploit such redundancy phenomena to improve the performance of RS. SpeciÓÄõcally, we propose PCRec, a top-N itemrecommendation framework that leverages collaborative training of two DNN-based recommender models with the same network structure, termedpeercollaboration. PCRec can reactivate and strengthen the unimportant (redundant) weights during training, which achieves higher prediction accuracy but maintains its original inference eÓÄúciency. To realize this, we ÓÄõrst introduce two criterions to identify the importance of weights of a given recommender model. Then, we rejuvenate the unimportant weights by transplanting outside information (i.e., weights) from its peer network. After such an operation and retraining, the original recommender model is endowed with more representation capacity by possessing more functional model parameters. To show its generality, we instantiate PCRec by using three well-known recommender models. We conduct extensive experiments on three real-world datasets, and show that PCRec yields signiÓÄõcantly better recommendations than its counterpart with the same model (parameter) size. Recommender Systems (RS) have become an essential tool for large social media and e-commerce platforms. A large number of useritem interaction behaviors (i.e., feedback) are produced explicitly or implicitly every day on such systems [39]. In particular, implicit feedback, such as clicks, purchases, watched videos and played songs, are easy to be collected and often at a very large scale. For example, users on Tiktok may easily watch thousands of short videos per day, given that the playing time of each video takes usually less than 20 seconds. As such, recent studies on top-N item recommendations mainly pay attention to the implicit feedback problem [1]. The essence of item recommendation from implicit feedback is to predict a list of top-N items that a user would like to interact with by learning from his/her previous feedback. Figure 1: Performance change by pruning on SASRec. We perform the standard pruning based on the weight magnitude following [29]. The experimental settings are given in Section 4. Embedding and deep neural networks (DNN) based recommender models have achieved superior performance and practically dominated the RS domain. Among these models, BPR [27], DSSM [18] and YouTube DNN [4] have become some of the most representative work for the general item recommendation task, while GRU4Rec [14], NextItNet [41] and SASRec [19] are more representative for the sequential recommendation settings. The success of these models often comes with a large embedding size or deep network structure [35,38]. However, large and deep models are very prone to be over-parameterized, resulting in redundant neurons and weights. As illustrated in Figure 1, simply pruning 10% parameters in the SASRec model yields very minor performance degradation. What‚Äôs more, pruning 30% of unimportant parameters with a ÓÄõne-tuning strategy performs even a bit better than the original SASRec. On the other hand, SASRec with a smaller embedding dimension (i.e., ùëë =128), around 50% parameters of itself withùëë =256, performs noticeably worse on ML-20M. These observations evidence that (1) the over-parameterization phenomenon widely exists in large recommender models; (2) training a smaller-size recommender model from scratch yields considerably worse performance. Pruning redundant parameters from a large neural network model could bring higher parameter eÓÄúciency.These experiments have been extensively performed in the computer vision (CV) [6,10, 11] and natural language processing (NLP) [8,21] ÓÄõelds. However, in recommender systems, simply reducing a portion of parameters (e.g., 30% in Figure 1) for large recommender models may not beneÓÄõt as much as in CV and NLP since large-scale RS models are often deployed in a cloud platform rather than an edge/mobile device (like many CV and NLP models) with very limited hardware resources. Thereby, inspired by these work, but diÓÄùerent from them, we hope to explore whether such redundant parameters can be used more eÓÄùectively instead of abandoning them so as to increase the model expressivity and alleviate the data sparsity issue in the recommender system domain. To approach the above problem, we present apeercollaboration framework for top-N itemrecommendation tasks, called PCRec. SpeciÓÄõcally, we propose rejuvenating invalid (i.e., unimportant) weights of a recommender model by transplanting important weights from a peer model with an identicalnetwork. To do so, we ÓÄõrst propose two criteria, including L1-norm based and entropy based, to identify which weights are important and which are redundant. To eÓÄùectively strengthen invalid weights, we create two rules regarding how to complement information between two identical networks and how much information each one needs to be complemented from its peer. To validate the eÓÄúcacy of PCRec, we instantiate it using three popular models, including both general item recommender models and sequential recommender models. We summarize our main contributions as four-fold: ‚Ä¢We propose PCRec to promote collaboration of two recommender models with a selfsame network architecture. PCRec is a novel learning paradigm for recommender models, which can reactivate invalid weights by explicitly transplanting effective weights from its outside peer network. ‚Ä¢We introduce two criteria to measure the importance of weights in a recommender model. Besides, we propose an adaptive coeÓÄúcient to determine how much the external information is required from its peer. ‚Ä¢We instantiate PCRec using three well-known recommender models, namely, BPR, YouTube DNN, and SASRec. PCRec is conceptually simple, easy to implement, and applicable to a broad class of recommender models. ‚Ä¢Through thorough experiments and ablation studies, we show that PCRec obtains noticeably improved performance on three real-world RS datasets. We brieÓÄûy review related work regarding the DNN-based RS and multiple model ensemble learning. Deep neural networks (DNNs) have made great progress for item recommendations thanks to their high model capacity and expressivity. In general, deep RS can be broadly classiÓÄõed into general (i.e., non-sequential) item recommendations and sequential item recommendations according to whether sequential patterns are modeled. In terms of general item recommendations, neural network models such as Deep Crossing [30], DeepFM [9], NeuralFM [13], Wide & Deep [3], and YouTube DNN [4] have become the most representative works. Compared to the shallow embedding models, the main advantages of these models highly depend on their neural network structures and non-linearities, who are believed to be able to approximate any continuous function [16, 17]. On the other hand, sequential recommender systems (SRS) have also attracted much attention recently. By capturing user‚Äôs dynamic interests, SRS, in general, is more powerful in generating the next recommendation. Moreover, SRS can be trained in a self-supervised manner [40,43], and thus do not need handcrafted labels and features. According to existing literature, GRU4Rec [14], Caser [36], NextItNet [41], SASRec [19] and BERT4Rec [34] are especially popular. Among them, GRU4Rec and Caser based on shallow network structure fail to model very long-term sequential patterns and usually oÓÄùer sub-optimal performance. By contrast, NextItNet, SASRec and BERT4Rec are able to obtain state-of-the-art performance by eÓÄùectively capturing long-term and complex sequential dependencies. In this paper, we design PCRec by instantiating it with three popular recommender models including BPR with shallow embeddings, YouTube DNN and SASRec with deep neural network. It is worth noting that the framework of PCRec is model-agnostic and potentially applicable to various embedding and deep models. PCRec relates to the ensemble learning (EL) [12,20] and knowledge distillation (KD) [15] in a similar spirit that more than one model is used during training. Here, we brieÓÄûy review related works and clarify their key diÓÄùerences against PCRec. Ensemble Learning (EL) refers to the process that multiple learning models are strategically combined to achieve better predictive performance than any of its individual model trained alone [25]. Bagging [2], boosting [7] and stacking [32] are thought of as three representative EL algorithms. The main principle behind them is that a set of weak learners are combined together to form a strong learner. While EL is generic for diÓÄùerent types of models, we notice that there are relatively few works that explore deep learning (DL) based ensemble methods. We suspect that DL-based methods are not conceptually weak learners and combining a large number of DL models could be computationally and memory expensive during the model prediction phase, and thus could be in-eÓÄúcient and in-practical. By contrast, PCRec merely needs one well-trained single model at the inference stage. Apart from that, PCRec is also relevant to KD-based methods [15, 37] which are designed to enhance a small-capacity model by one (or multiple) large teacher model(s). However, unlike KD-based methods, PCRec does not include the mutual learning [23,42] process which optimizes multiple losses together. Moreover, PCRec explicitly combines the advantage of two identical models by enhancing the invalid weights, which is very explainable. By contrast, the knowledge transferred by KD-based methods is usually called dark knowledge [15], and the working mechanism of it is not as explainable as PCRec. In addition, PCRec focuses on performance improvement which is diÓÄùerent from the motivation of the KDbased methods ‚Äî injecting knowledge from a large teacher model into a smaller student one to obtain the eÓÄùect of model compression. Figure 2: PCRec with PW cooperation, where dark colors represent important weight. As mentioned in the introduction part, over-parameterization or redundancy commonly exist in large and deep recommender models. Inspired by this, in this paper we set our goal to reactivate these redundant weights (rather than abandoning them) so as to enhance the model capacity and expressivity. To be speciÓÄõc, we present the PCRec learning framework, which enhances an individual recommender model by transplanting important information from a selfsame network of this recommender, referred to as a peer. In the following, we ÓÄõrst introduce criteria to measure the importance of weights in a recommender model. Then, we propose a parameter-wise approach to reactivate the redundant weights of the two peer models. At last, we develop the ÓÄõnal version based on the layer-wise cooperation, which addresses the limitations of the parameter-wise approach. 3.1.1 L1-norm. The idea of the L1-norm criterion is borrowed from the pruning [10,22,29] literature. Denoteùëä‚àà Ras the weight matrices of theùëñ-th layer in a model. We can identify the importance of weights from two perspectives: single weight perspective and entire layer perspective. The importance of a single weight is directly determined by its absolute value (L1-norm) ‚Äî the higher absolute value it has, the more important it is. We can use a threshold to distinguish the important and unimportant weights. From the entire layer perspective, we could identify the importance of all weights by using a neural network layer (including the embedding, middle layers, and ÓÄõnal prediction layers) as the measure unit. Intuitively, measuring the importance of a layer could maintain the layer consistency as much as possible, which will beneÓÄõt the information transplanting process as mentioned later. Formally, its L1-norm can be given below by using the entire layer as the measure unit: Denoteùëäandùëäas L1-norm ofùëñ-th layer of two collaborated recommender models. We deÓÄõne theùêª (ùëä)as the relative information of the layer: where % is the modulo operation,ùëòis the model ID. While L1norms has been widely applied in pruning, it only cares about the magnitude of the weights, and ignores the variation of the weights. For example, given a weight matrixùëä‚àà ùëÖofùëñ-th layer, where each element inùëäis assigned to the same valueùëß, whose absolute value is big. If we use such a weight matrix to transform the(ùëñ ‚àí1)-th layer, then each part of it contributes equally to the ùëñ-th layer evenùëßis very big. This suggests that L1-norm might not be the best criterion to discriminate the importance of layer (all weights) information. 3.1.2 Entropy. To address the limitation mentioned above, we introduce an entropy-based criterion to measure the variation of weights in each layer. Entropy is often used to evaluate the degree of chaos (information) in a system [5,26]. Inspired by [24,31,33], we transform the weight matrix into a vector and discretize the vector intoùëöbins. Then we can calculate the probabilities of each bin. To be speciÓÄõc, we ÓÄõrst sort the weights in the vector based on their actual values and divide the vector intoùëöbins with equal numeric intervals (wheremaxandminrepresent the maximum and minimum values of the weight matrix, respectively). The probability of the ùëó-th bin is: Input: ùëÄ, ùëÄwith weights ùëä,ùëä, . . . ,ùëä, ùëò = 1, 2; whereùëÅandùëõare the parameter sizes of the weight vector and the ùëó-th bin, respectively. Then, we calculate the entropy (information) of the weight matrix ùëäas follows: A smaller score of H(W)means the layer in this model has less variation (information). We illustrate the proposed PCRec framework in Figure 3. Assume that both models haveùëÅlayers. We denoteùëäandùëäas the weight matrices of theùëñ-th layer of the two models. Our core idea is to use the corresponding weight information of the two networks, and generate more expressive weightsbùëäas The weightùëäandùëäare signiÓÄõcantly diÓÄùerent since they are optimized with diÓÄùerent hyper-parameters (mentioned later) and initialization. That is, the unimportant weights of a layer may correspond to the important weights of the same layer in his peer, and vice versa. Before describing the layer-wise (LW) cooperation mechanism, we ÓÄõrst show a more intuitive parameter-wise (PW) method by exploiting redundancy pruning. 3.2.1 PW Cooperation. The process is shown in Figure 2, we ÓÄõrst deÓÄõne a positive thresholdùõæand then identify unimportant parameters if their absolute values are smaller thanùõæ. To realize information transfer from its peer model, we simply replace these unimportant parameters with parameters in its peer model of the same layer and index position. To realize this, we deÓÄõne a binary mask matrix ùêº‚àà Rwhich has the same shape withùëäto indicate the Figure 3: PCRec framework with LW cooperation. ùëì represents linear combination based on layer information, i.e., ùêª (ùëä) and ùêª (ùëä). indices of these invalid weights inùëäThis process is symmetrical for the two peer models. Correspondingly, we can formulate the PW process as follows. where % is the modulo operation and each element of ùêºis: ùêº=0 if ùëä‚â• ùõæ1 if ùëä< ùõæ0 ‚â§ ùëö < ùëëand 0 ‚â§ ùëõ < ùëë(7) The learning process of PCRec with the PW cooperation is illustrated in Algorithm 1. While this PW cooperation is intuitively simple, it has some shortcomings as mentioned below. 3.2.2 LW Cooperation. Using individual weight as the measure unit only focuses on the importance of the weight itself, which unfortunately ignores the layer consistency and may thus hurt the model expressivity and performance. We argue that using the entire layer as the measure unit can enable all weights at the same layer to contribute synergistically to the transformation of the layer. Thus, we propose a layer-wise transplanting method by deÓÄõningùëìas a linear combination function: where 0‚â§ ùúá‚â§1 is the coeÓÄúcient. Particularly, we treat this coeÓÄúcientùúáas an adaptive parameter so as to promote cooperation and optimization automatically. Below, we give two instructions on designing a suitable adaptive parameter ùúá: (1)We expect that layers with less information could get additional information from its peer model. Hence, we use the diÓÄùerenceùêª (ùëä) ‚àí ùêª (ùëä)to measure the relative importance of information in the two layers. When the diÓÄùerence is zero,ùúáshould be set to 0.5, otherwisebùëäshould assign a largeùúá(i.e.,ùúá>0.5) to the layer that has more information. Note that evenùúá=0.5 could be also helpful since the same information does not mean all weights are identical according to Eq.(4). Consider an extreme situation where the distributions (e.g., normal distribution) of weight Table 1: Statistic of the evaluated datasets. "M" and "K" is short for million and kilo, "t" is the length of interaction sequences. matrices are identical, but the magnitude of each weight (with the same position) is the opposite. In such a case, the information of each layer is the same, but the entropy ofbùëä is enlarged by Eq. (8). (2)Even the diÓÄùerenceùêª (ùëä) ‚àí ùêª (ùëä)is large, we expect thatbùëäcontains part information of itself and is able to adaptively control the impact of ùêª (ùëä) ‚àí ùêª (ùëä). To meet the above requirements, we design an adaptiveùúáwhich is wrapped by the sigmoid function: whereùõºis a hyper-parameter to control the degree of the information from the outside layer. It is worth noting that the calculation criterion of information for a layer as the measure unit can be L1-norm (i.e., Eq.(2)) or entropy (Eq.(4)), which is diÓÄùerent from the individual weight as the measure unit with L1-norm criterion. During training, we just need to perform this combination operation at each epoch. The new weight matrices (bùëäandbùëä) should be the same for the two individual models due to the dual linear combination. In practice, we need to guarantee that each model has diverse and suÓÄúcient information so as to complement each other. In this paper, we adopt two simple strategies for the two models to make each of them capture unique information, i.e., using diÓÄùerent learning rates and sampling of the training data. PCRec can be optimized in two modes, namely, parallel and serial training. In terms of the parallel mode, the two individual networks of PCRec are essentially trained independently, but each batch of them is trained concurrently. The information of each identical recommender model can be transferred by using the saved checkpoint. As a result, parallel optimization requires more memory and computations, but saves substantial training time. For clarity, if we assume the time and space complexity of each model are the scalarsùëáùê∂ andùëÜùê∂, the time and space complexity of PCRec in parallel training mode areùëáùê∂and 2ùëÜùê∂. On the other hand, we can perform serial optimization for each individual network by sequentially training them per batch. As such, compared with the parallel mode, the serial optimization inevitably sacriÓÄõces training time but consumes no extra memory and computation. The time and space complexity of PCRec in the serial training model are roughly 2ùëáùê∂andùëÜùê∂. Algorithm 2 illustrates the peer cooperation process. In summary, we maintain two networks with an identical structure but diÓÄùerent learning rates and sampling orders. When a training epoch is ÓÄõnished, we calculate the informationùêª (ùëä)of each layer of the two models and perform cooperation. Note that the parameters of the bias and normalization terms of the same layer share the same ùúácalculated based onùëä. After training, PCRec needs only one Model ùëè ùëë ùúÇ ùêøùëù ùëè ùëë ùúÇ ùêøùëù ùëè ùëë ùúÇ ùêøùëù SASRec 128 64 1e-3 - 0.3 128 256 1e-3 - 0 128 256 1e-3 - 0.5 DNN 128 64 1e-4 1e-5 - 128 256 1e-4 1e-6 - 128 256 1e-4 1e-5 peer model for inference, and thus, is as eÓÄúcient as the original individual recommender model. This property is distinct from the traditional ensemble methods that have to rely on the decisions of multiple ‚Äòweak‚Äô learners during inference. We describe the experimental setup in this section, including datasets, baselines, implementation details and evaluation metrics. ‚Ä¢ ML-20M: This is a well-known benchmark dataset widely used for both traditional and sequential recommendation tasks [19,34,35]. It contains around 20 million user-item interactions with 27,000 movies and 138,000 users. Following the common practice in [19,39,41], we assume that an observed feedback is available if an explicit rate is assigned to this item. We perform basic pre-processing to ÓÄõlter out the interactions with less than 5 users and users with less than 5 items to alleviate the eÓÄùect of cold users and items. Then, we use timestamps to determine the order of interactions. Following [19,34], we adopt the leave one out evaluation scheme. For each user, we hold out the last item of the interaction sequence as the test data, treat the item just before the last as the validation set, and utilize the remaining items for training. For the sequential recommendation task, we construct user‚Äôs interaction sequences by using his recentùë°interactions by the chronological order. For sequences shorter than t, we simple pad them with zero at the beginning of the sequence following [41], while for sequences longer than t, we split them into several sub-sequences with lengthùë°in the training set. In this paper, we setùë°to 100 on this dataset. ‚Ä¢ QQBrowser: It is an industrial dataset which was collected from the QQBrowser platform of Tencent. The items in QQBrowser include news, videos and ads. It consists of more than 70,000 items and almost 1 million users. We perform a similar pre-processing as above and setùë°to 50. We will open source this dataset later for reproducibility. ‚Ä¢ Retailrocket: It is a public dataset collected from a realworld ecommerce website, consisting user shopping behaviors in 4.5 months. It contains 235,061 items and 1.4 million users. Similarly, we setùë°to 10 to investigate recommendation performance for short-range interaction sequences. Table 1 summarizes the statistics of evaluated datasets after the basic pre-processing. Table 3: Overall performance of all models. PCRec with two SASRec, DNN and BPR is referred to PC-SAS, PC-DNN and PC-BPR, respectively. Here, we present the results of PCRec with LW-cooperation and entropy-based information criterion because of its best performance. We set ùõº of PC-SAS to 30, 30, 30, ùõº of PC-DNN to 40, 40, 10, and ùõº of PC-BPR to 20, 20, 20, on Retailrcoket, ML-20M, QQbrowser, respectively. Improvements over baselines are statistically signiÓÄõcant with p < 0.01. Model MRR@5 MRR@20 HIT@5 HIT@20 MRR@5 MRR@20 HIT@5 HIT@20 MRR@5 MRR@20 HIT@5 HIT@20 We evaluate the PCRec framework by using three popular recommender models, namely, SASRec [19], YouTube DNN [4] (DNN for short) and BPR [27]. For SASRec, we use its oÓÄúcial code online, while for BPR and YouTubeDNN, we implement it by strictly following the original paper. It is worth noting that compared with SASRec, DNN and BPR are unable to capture user sequential patterns. This is because DNN model user‚Äôs previous interactions as common features, while BPR with matrix factorization as the scoring function is a typical collaborative ÓÄõltering baseline. We want to emphasize that the purpose of our study is not to propose a state-ofthe-art model beating existing baselines. The purpose is rather to introduce a new learning paradigm that could eÓÄùectively leverage the parameter redundancy issues in large and deep recommender models so as to achieve some additional improvement in accuracy. We train all models using the Adam optimizer on GPU. For common hyper-parameters, we consider the hidden dimension size (denoted byùëë) from {16, 32, 64, 128, 256} and the learning rate (denoted byùúÇ) from {0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.005}, theùêø regularization coeÓÄúcients from {0.01, 0.001, 0.0005, 0.0001, 0.00005 0.00001}, and dropout rate (denoted byùëù) from {0, 0.1, 0.2, .. . , 0.9} by grid search in the performance of the validation set. SpeciÓÄõcally, we set theùëë256 for SASRec (except on Retailrocket), DNN (except on Retailrocket) and BPR. On Retailrocket,ùëëof SASRec and DNN is set to 64 to prevent overÓÄõtting. We useùúÇ1e-3 for SASRec and BPR, and 1e-4 for DNN on all datasets. In addition, we set batch size (denoted byùëè) to 128 for SASRec and DNN, and 2048 for BPR because of its enormous triple samples. As for model-speciÓÄõc hyper-parameters, we use two self-attention blocks (denoted byùëô) with one head for SASRec according to the original paper. Regarding DNN, we use one hidden layer on all datasets since using more layers does not lead to any improved results. Our PCRec uses exactly the same hyper-parameters (exceptùúÇ) as these individual base models. ForùúÇ, one peer in PCRec uses exactly the same one with its base model, while the other peer uses a sub-optimalùúÇ. The model-speciÓÄõc hyperparameter of PCRecùõºis studied in the ablation study part. Without special mention, we report our results with the optimalùõº. Detailed hyper-parameters are reported in Table 2. We follow previous works [19,35,39,41] by comparing the topN metrics, namely, MRR@N(Mean Reciprocal Rank), HR@N(Hit Ratio) and NDCG@N(Normalized Discounted Cumulative Gain). To save space, we omit the formulas of these metrics. N is set to 5 and 20 in this paper. In this section, we would answer the following research questions: ‚Ä¢ RQ1: Does PCRec improve the performance of these typical neural networks, such as SASRec, YouTubeDNN and BPR? ‚Ä¢ RQ2: What is the performance of these variants of PCRec, which include PCRec with PW and LW cooperation, PCRec with L1-norm and entropy criteria. ‚Ä¢ RQ3: What is the impact of the collaboration of diÓÄùerent components in PCRec, such as, the embedding layer, softmax layer and hidden layers? ‚Ä¢ RQ4: What is the impact ofùõºfor PCRec? Are the diÓÄùerent learning rates and training data orders necessary? ‚Ä¢ RQ5: Does PCRec really enhance these unimportant weights of the original model? We present the overall results in Table 3. First, we observe that SASRec performs better than DNN and BPR with notable improvements. To our surprise, on Retailrocket and ML-20M, SASRec achieves several times improvements on all these top-N metrics. By examining the real dataset, we ÓÄõnd that there indeed exist some short sequence fragments (formed with 2‚àº4 videos) on the two datasets, which can be observed from the actions of many users. Unfortunately, DNN and BPR are unable to model such sequential patterns, and thus yield much worse results than the state-of-the-art sequential recommendation model SASRec. Second, as expected, PCRec, including PC-SAS, PC-DNN and PCBPR, outperforms their individual base models (i.e., SASRec, DNN and BPR), demonstrating the eÓÄùectiveness of peer collaboration. For example, compared with SASRec, PC-SAS achieves around 5% improvement in terms of MRR@5 on ML-20M; Compared with BPR, PC-BPR obtains up to 8% improvement regarding MRR@5 on Retailrocket and ML-20M. In particular, PC-BPR outperforms BPR with around 11% improvement regarding HIT@5 on QQBrowser. Notable improvements can also be observed by comparing PC-DNN Table 4: The Comparison of PCRec variants. The standard ensemble learning method [28] by averaging the prediction scores of two individual models is denoted by Ensemble-M2. to DNN on all datasets and all metrics. In what follows, we would conduct ablation studies to verify the eÓÄùectiveness of PCRec. To save space, we could only show partial results if the performance trends of them keep consistent. In Section 3.1, we have proposed using a single weight and a layer as the measure unit in PCRec, We denoted them as PCRec-W and PCRec-L, respectively. Further, in PCRec-L, we can adopt two criteria, L1-norm and entropy, to identify which layer of the two individual networks has less information, denoted as PCRec-LN and PCRec-LE, respectively. In addition, we also evaluate a very simple method by reactivating the invalid weights using gaussian noise to increase the L1-norm, denoted as PCRec-N. We experimentally examine these methods and report results in Table 4. First, we ÓÄõnd that PCRec-N yields worse accuracy than the base model, which potentially indicates PCRec should use a useful information source, rather than random noise, for information transplanting. By contrast, PCRec-LE, PCRec-LN always perform better than SASRec, DNN on almost all datasets. This clearly veriÓÄões our main claim Table 5: The impact of learning rates and sampling orders of training data for PCRec. PC-SAS with diÓÄùerent learning rates and sampling orders, the same learning rate and diÓÄùerent sampling orders, diÓÄùerent learning rates and the same sampling order, is denoted by PC-SAS-DD, PC-SAS-SD, PCSAS-DS respectively. Similar expressions apply to PC-DNN. regarding the beneÓÄõt of peer collaboration. Meanwhile, PCRecLN outperforms PCRec-W on most settings, demonstrating the eÓÄùectiveness of layer-wise cooperation; PCRec-LE outperforms PCRec-LN, demonstrating the eÓÄùectiveness of entropy-based criterion, since it can more precisely identify how much information is required when performing information transplanting. On the other hand, we also compare the results that are produced by standard ensemble learning. It can be seen that the basic ensemble learning method (Ensemble-M2) is very eÓÄùective and obviously surpasses these individual models. It even performs slightly better than PCRec-LN on the Retailrocket dataset when using DNN as the base model. However, our PCRec-LE in general can beat it, or at least they are competitive. Hence, we do not claim our PCRec is better than the standard ensemble learning method in this paper. But we emphasize that PCRec provides an alternative learning paradigm for getting information from an outside model, and more importantly, it is much more eÓÄúcient than the standard ensemble learning during the inference phase, since it only requires one single model for prediction, rather than relying on predictions of two or more models. We further ÓÄõnd that increasing the individual models for the ensemble learning, e.g., Ensemble-M3, does not yield better results. Figure 5: Convergence behaviors of PCRec by peer collaboration of diÓÄùerent components. PC-SAS that applies peer collaboration only on the embedding layer, middle layers, and softmax layer as PC-SAS-E, PC-SAS-M, PC-SAS-S, respectively. 5.3.1 Impact of learning rates and train data orders. Table 5 presents the impact of diÓÄùerent learning rates and sampling orders of training data. As shown, PC-SAS-DD always yields the best recommendation accuracy compared with their counterparts, i.e.,PC-SAS-SD and PC-SAS-DS. On the other hand, we observe that PC-SAS-SD and PC-SAS-DS consistently outperform the original SASRec. Similar observations can be made for PC-DNN. The results conÓÄõrm that PCRec that applies diÓÄùerent learning rates and sampling orders is necessary. This is likely because training individual networks with diÓÄùerent learning rates and sampling could increase diversity of network weights, so as to increase the layer information when linearly combining them. The results hold well for PCRec with BPR and are thus simply omitted. 5.3.2 Impact ofùõº. In this subsection, we study the impact ofùõº which controls the amount of information to be transplanted. Figure 4 shows the model performance of PC-SAS and PC-DNN with diÓÄùerentùõºon Retailrocket and ML-20M. First, PC-SAS is sensitive toùõº, and the optimal results are obtained whenùõºequals to 30 on Retailrocket and ML-20M. Similarly, PC-DNN obtains the best performance whenùõºis set to 40 on Retailrocket and ML-20M. It can be seen that PCRec with a properùõºcould achieve 1‚àº4% improvement than a randomùõº. It is also worth noting that PCRec outperforms its individual base model evenùõºis not set to its optimal value. In practice, we suggest running PCRec by tuningùõºfrom 30 to 40. By doing this, we observe that the coeÓÄúcientùúáranges from 0.7 to 1.0 in most cases. 5.3.3 Impact of the peer collaboration with diÓÄõerent components. We conduct an ablation study in Figure 5 by applying peer collaboration for some components of the model. First, it can be observed that PC-SAS-E, PC-SAS-M and PC-SAS-S outperform SASRec, demonstrating that the information transplanting on every component of usually performs better than its original model SASRec. Second, PC-SAS-E improves SASRec by a larger margin, compared with PC-SAS-M. In particular, PC-SAS-E even surpasses PC-SAS on ML20M. This is likely because the embedding layer usually contains much more parameters than the middle layers in recommender models. Besides, the embedding layer contains the most important information for item recommendations ‚Äî i.e., personalization. As such, performing information transplanting on the embedding layer Figure 6: Ratios of Invalid layers in PC-SAS and SASRe c, where invalid layer ratio denotes the number of valid layers (with information ùêª (ùëä) under a speciÓÄõed threshold value) over the number of all layers. makes more sense than only doing it for these middle layers. This also suggests that it might be suÓÄúcient to perform peer collaboration on only necessary components of the base model, rather than all components. Similar conclusions hold for PC-DNN in general. In this part, we simply analyze the information transplanting mechanism in PCRec. To validate whether the peer collaboration really work not, we calculate the number of invalid layers (including both fully-connected layers and self-attention layers) whose entropy is under a speciÓÄõed threshold after training. Experimental results are reported in Figure 6. It can be seen that with the threshold of 0.5, there are about 20% layers that are invalid for SASRec on ML-20M, whereas PCRec with peer collaboration training only has less than 5% invalid layers. With the increase of threshold, the ratios of invalid layers in both SASRec and PCRec rise. However, the ratio of PCRec is always smaller than SASRec. These observations verify our key assumption that peer collaboration does help model to strengthen the information (i.e., ùêª (ùëä)) of network layers. In this work, we have discussed the network redundancy phenomenon in deep recommender models. Taken inspiration from this, we have proposed PCRec, a ÓÄûexible and generic peer collaboration learning paradigm that is able to rejuvenate invalid parameters (instead of abandoning them) in a recommender model by transplanting information from its outside peer network. To identify which parameters are invalid, we have introduced L1-norm and entropy based criteria. Then, we propose two collaboration strategies regarding how to transplant information between two peer models. Through extensive experiments on three real-world recommendation datasets, we have demonstrated that PCRec generated consistently better recommendations than its original base model. We expect PCRec to be valuable for existing recommender systems based on the embedding or deep neural network models.