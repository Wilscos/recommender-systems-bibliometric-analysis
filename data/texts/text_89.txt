Automatic news recommendation has gained much attention from the academic community and industry. Recent studies reveal that the key to this task lies within the effective representation learning of both news and users. Existing works typically encode news title and content separately while neglecting their semantic interaction, which is inadequate for news text comprehension. Besides, previous models encode user browsing history without leveraging the structural correlation of user browsed news to reﬂect user interests explicitly. In this work, we propose a news recommendation framework consisting of collaborative news encoding (CNE) and structural user encoding (SUE) to enhance news and user representation learning. CNE equipped with bidirectional LSTMs encodes news title and content collaboratively with cross-selection and cross-attention modules to learn semantic-interactive news representations. SUE utilizes graph convolutional networks to extract cluster-structural features of user history, followed by intra-cluster and inter-cluster attention modules to learn hierarchical user interest representations. Experiment results on the MIND dataset validate the effectiveness of our model to improve the performance of news recommendation. Online news applications, such as CNN News and MSN News, have become more and more people’s ﬁrst choices to obtain the latest news (Das et al., 2007). With a deluge of news generated every day, an efﬁcient news recommendation system should push relevant news to users to satisfy their diverse personalized interests (IJntema et al., 2010). From the perspective of representation learning (Bengio et al., 2013), existing works mainly study how to effectively encode news and users into discriminative representations (Okura et al., 2017; Wu Figure 1: (a) An example of user browsing history. (b) An example of news title-content semantic interaction. et al., 2019a; Wang et al., 2020). News encoders typically extract semantic representations of news from the textual spans (e.g., news title and content). User encoders are employed to learn the representation of a user from her browsing history. News recommendation models predict the matching probabilities between candidate news and users by measuring the similarity of their representations. Existing news recommendation models typically encode news title and content separately and encode users’ browsing histories without explicit structural modeling. We argue that these encodings restrict the power of the news and user representations. To enhance news and user encoding, this work is established based on the two aspects of news and user representation learning: (1) Encoding the semantic interaction between news title and content: Title and content play different roles in news, but they are complementary. News title distills the information of content, while content interprets the details of title, as shown in Figure 1(b). Previous works treat news title and content as two separate textual features, leading to a “semantic encoding dilemma”. This dilemma is dyadic as: (i) Although a news title is much shorter than content, the performance of title-encoding is empirically better than content-encoding (Wu et al., 2020). This can be attributed to the crucial information that the human-summarized title naturally represents; (ii) News titles are always subjective and rhetorical to attract potential readers. This leads to a severe textual data sparsity problem. News titles with unseen terminology, metaphor and ambiguity make it difﬁcult to comprehend news with limited title wording (Shree et al., 2019). For example in Figure 1(b), the word “curse” is a metaphor, which cannot be resolved by the training corpus or the title itself, due to its unique semantic occurrence. News encoders must turn to the content to interpret the semantics of the word “curse” (i.e., “has never made the playoffs” and “mean Alabama is toast”). However, news encoders proposed by previous works either extract features solely from the title, or encode title and content separately, then perform concatenation or attention fusion on them (Zhu et al., 2019; Wu et al., 2019a). Such separate encodings of title and content without leveraging their semantic interaction are inadequate for news text comprehension. (2) Encoding the user-interest-news correlation with hierarchical cluster-structure: While a user usually has diverse interests in news topics, her browsed news with the same topic is often linked by some logical correlation. For example in Figure 1(a), the news N3, N5 and N6 are labeled as sports news and logically correlated with the topic “football”, forming a virtual user interest cluster. None of the single news can precisely represent the overall user interest in “football”. However, reﬁned user interest in “football” can be encoded by aggregating the news N3, N5 and N6. With aspects of user interests encoded within speciﬁc clusters, overall user representations can be aggregated by leveraging the correlation among interest clusters. Previous works typically formulate user history as an ordered linear sequence of news. Based on this sequential formulation, recurrent neural networks (Okura et al., 2017; An et al., 2019) and attention networks (Zhu et al., 2019; Wu et al., 2019a,b,c) are proposed to encode user history. These encoding methods viewing user history as a sequence of news cannot explicitly model the hierarchical user-interest-news correlation. Compared to linear sequences, hierarchical clusters are more suitable to represent a user’s diverse interests. User history can be structurally formulated into certain interest clusters, as correlated news shares information in a speciﬁc interest cluster. Encoding user history with hierarchical cluster-structure is more precise to represent the correlation of news and user interests. To address the above issues, in this work, we propose collaborative news encoding (CNE) and structural user encoding (SUE) to learn semanticinteractive news representations and hierarchical user representations. We conduct experiments on the MIND dataset (Wu et al., 2020), showing the encoding effectiveness of our proposed model. Experiments and further analyses validate that (i) CNE can enhance news encoding by exploiting the word-level semantic interaction between news title and content with cross-selective and cross-attentive mechanisms; (ii) SUE utilizes hierarchical cluster graphs to model the correlation of a user’s browsed news, which can extract more precise user interest representations; (iii) our model signiﬁcantly outperforms existing state-of-the-art news recommendation models on the real-world MIND dataset. News recommendation is not only an important research task in NLP (Wu et al., 2020) but also a core component of industrial personalized news service (Okura et al., 2017). Conventional collaborative ﬁltering (CF) approaches (Wang and Blei, 2011) exploit the interaction relationship between news and users. Since news only lasts for a short period, CF-based methods suffer from severe coldstart problem. To tackle this, content-based methods used handcrafted features to encode news and users (Li et al., 2010; Son et al., 2013; Bansal et al., 2015). In recent years, deep neural models have achieved superior performance in news recommendation. Many studies pinpointed that this improvement came from the ﬁne-grained news and user representations, which were extracted by deep neural networks (Wu et al., 2019a,c; Wang et al., 2020). For news representation learning, existing works used convolutional neural networks (CNN) (An et al., 2019), knowledge-aware CNN (Wang et al., 2018), personalized attention networks (Wu et al., 2019b), and multi-head self-attention networks (Wu et al., 2019c) to extract features from news title text as news representations. Zhu et al. (2019) employed parallel CNNs to encode news title and content respectively and then concatenated them into a uniﬁed representation. Wu et al. (2019a) encoded news title and content separately and incorporated them with multi-view attention. For user representation learning, Okura et al. (2017) used GRU to encode user history by ordered timestamp. An et al. (2019) utilized RNN to learn short-term user representations from the browsing history, combined with long-term user embeddings. Various attention networks are also widely used to attend to important news in user history (Wu et al., 2019a,b; Zhu et al., 2019). Wu et al. (2019c) employed multi-head self-attention (Vaswani et al., 2017) to capture deep interaction of user browsed news. These works formulated user history as an ordered linear sequence of news, to which recurrent or attention models were applied without modeling the structural correlation of user browsed news. Hu et al. (2020) formulated news and user jointly with a bipartite graph and disentangled user preferences with routing mechanism, which however implicitly relied on the manually-set latent preference factor. Our model is composed of theCollaborativeNews Encoding (CNE) module presented in Section 3.1 andStructuralUserEncoding (SUE) module presented in Section 3.2. CNE and SUE extract representations of candidate news and users respectively. The overall model architecture is illustrated in Figure 2. Finally, Section 3.3 will describe the click predictor and details of model training. 3.1.1 Cross-selective Encoding The news encoder is employed to learn semantic news representations from news title and content. Given the title word sequencex= [x, x, ..., x] and content word sequencex= [x, x, ..., x], they are mapped to word embeddingsW= [w, w, ..., w]andW= [w, w, ..., w], whereNandMare the word sequence lengths. For simplicity, we only formulate the title encoding part of our model (denoted by superscriptt). The content encoding formula is symmetric as a counterpart (denoted by superscriptc) and omitted. First, two parallel bidirectional LSTMs are employed to extract the sequential features from the title and content word embeddings respectively. where{h}and{c}are LSTM hidden states and cell states. The i-th title sequential feature is fused ash= [−→h;←−h], where[·; ·]denotes vector concatenation. We consider the global semantic information of title (content) preserved in its LSTM cell states and concatenate the last forward−→cand backward←−cas the semantic memory vector m. To facilitate semantic interaction between title and content, we design a gated cross-selective network, inspired by Geng et al. (2020). Concretely, we utilize the semantic memory vectormto perform feature recalibration (Hu et al., 2018) on the sequential features{h}by a sigmoid gate function. The motivation behind this gate function is to utilize the memory vector of content (title)mto cross-select important semantic information from the i-th title (content) sequential feature h. whereσis sigmoid activation,denotes elementwise multiplication.˜his the cross-selective feature of the i-th title sequential featurehinteracting with the content memory vectorm. It is the ﬁrst stage of collaborative title-content semantic interaction. 3.1.2 Cross-attentive Encoding Based on the cross-selective sequential feature{˜h}, a two-phase attention module is designed to learn cross-attentive representations of title and content. First, we employ self-attention layers to learn the self-attentive representation of the sequential {˜h}. α= softmax(vtanh(W˜h+ b)) Then we employ the self-attentive representation ras a query, and the{˜h}as key-value pairs to build cross-attention layers. It is the second stage of collaborative title-content semantic interaction. Figure 2: The overall architecture of our model. The graph construction is based on the user history in Figure 1(a). We compute element-wise summation (denoted as⊕) of the self-attentive representationrand cross-attentive representationrto derive the title and content semantic-interactive representations, i.e.,randr. Finally, we concatenaterand ras the collaborative news representation r. r= [r; r] = [r⊕ r; r⊕ r] (8) 3.2 Structural User Encoding 3.2.1 Cluster-based Encoding of User History The user encoder is employed to learn user interest representations from their browsing histories. To formulate the cluster-structure of user interests, we construct a hierarchical cluster graph in two steps: Intra-cluster Subgraph G. We construct an original cluster graphwith the topic category label of news (e.g., “Sports” and “Travel” in Figure 1(a)). We build the subgraphG= (V, E)by treating the browsed news as nodes{V}and adding bidirectional edges{E}to those nodes, which share the same category labels. Each news node of {V}is associated with its embeddingrin Eq. (8). Each cluster contains multiple browsed news with a speciﬁc topic, reﬂecting an aspect of user interests. Inter-cluster Subgraph G cluster reﬁnement of user interests, modeling intercluster correlation is also essential to leverage the overall information of user history. For each clusterCinG, we add a new cluster proxy nodeV. We build the subgraphG= ({V, V}, {E, E}) by adding bidirectional edges{E}to those news nodes{V}and proxy nodes{V}within the same clusters and fully connecting{V}by bidirectional edges{E}. The node embedding of{V}is initiated as zero-embeddingr. News node information among clusters aggregates via cluster proxy nodes. The hierarchical cluster graphGconsists of intraand inter-cluster subgraphs:G = {G, G}. Withddimensional node embedding vectors{r}and {r}, we deﬁne the history feature matrix as H= [r; r] ∈ R. For graphG, we denote its normalized adjacency matrix as˜Aand degree matrix as˜D. We use graph convolutional networks (GCN) (Kipf and Welling, 2017) to extract structural representations of user history. To mitigate the over-smoothing issue of deep GCN (Chen et al., 2020), we add residual connections to adjacent GCN layers, following Li et al. (2019). whereWis a trainable matrix. The GCN extracts structural features on graphG, reﬁning speciﬁc user interest representations within clusters and aggregating overall user history information among clusters. We train GCN ofLlayers and derive the structural user history representation from the news node embeddings as r= {H}∈ R. 3.2.2 Intra-cluster Attention Given the|C|interest clusters implied by|V| user’s browsed news, there are|C|news in cluster C. The structural user history representationrno can be viewed asr= {r}={r}. To derive intra-cluster features associated with candidate news, we design an intra-cluster attention layer, regarding the candidate news representation ras a query, and thej-th intra-cluster feature rof cluster Cas a key-value pair. The intra-cluster featurerattends to the node-level features{r}of clusterC, associated with the candidate news representationr. Therreﬁnes thei-th user interest representation within the cluster Cof graph G. 3.2.3 Inter-cluster Attention Before inter-cluster modeling, a nonlinear transformation is performed to project ther, which is originally a linear combination of node-level features in clusterC, into cluster-level feature spaces. ˜r= ReLU(˜W r+˜b) + r(11) To derive inter-cluster features associated with candidate news, we design an inter-cluster attention layer, regarding the candidate news representation ras a query, and thei-th intra-cluster feature ˜rof graph G as a key-value pair. α= Attention(r, {˜r}) The inter-cluster featurerattends to the cluster-level features{˜r}of graphG, associated with the candidate news representationr. With intra-cluster and inter-cluster attention,r hierarchically aggregates user interest representations within the cluster graphG.ris adopted as the user representation r, i.e., r= r. 3.3 Click Predictor and Model Training Given the news and user representationsrand r, the click predictor is employed to predict the probability that useruclicks on newsn. Motivated by the previous works (Wang et al., 2018; Wu et al., 2019a), we compute the dot-productˆyofr andr, i.e.,ˆy= hr, ri, as the unnormalized matching score of news n and user u. Following common practice of previous works (Huang et al., 2013; Wang et al., 2020), we employ negative sampling strategy to model training. For Table 1: Statistics of the 200K-MIND dataset. each user click-impression, i.e., thei-th impression log that useruhad clicked on newsn, we compute its unnormalized matching score asˆy. Besides, we randomly sampleKpieces of news, which are not clicked by the useru. Unnormalized matching scoresˆyare computed for theseKnegative samples, wherej = 1, ..., K. By such means, it can be reformulated as a(K + 1)-way classiﬁcation problem. We employ softmax function to derive the normalized matching probabilities and sum up the negative log-likelihood of positive samples over the training dataset D, as model training loss L. L = −logexp(ˆy)P(13) We conduct experiments on the MIND dataset (Wu et al., 2020). MIND is a large-scale English news recommendation dataset built from real-world MSN news and anonymized user click logs. Since the MIND is quite large-scale, following existing works (Wu et al., 2019a,b,c; Wang et al., 2020), we randomly sample200K users’ click logs out of1 million users from the user behavior logs of MIND training and validation sets. Since the MIND test set is not labeled, we half-split the original validation set into experimental validation and test sets. We employ the abstract column texts in MIND as the news content texts. Detailed statistics of the 200K-MIND dataset are shown in Table 1. We truncate news title and content with the maximum length of32and128respectively. The number of news in user browsing history is capped at50. Following (An et al., 2019; Wu et al., 2019a,b,c), we perform negative sampling with the sampling ratioKof4(see Section 3.3). The word embedding is initialized from the pretrained300-dimensional Glove embedding (Pennington et al., 2014). The number of GCN layers in SUE is set asL = 4(investigated in Section 5.4). We use Adam optimizer (Kingma and Ba, 2015) with the learning rate of1e-4to train our model with the dropout rate of 0.2. The area under the ROC curve (AUC), mean reciprocal rank (MRR), and normalized discounted cumulative gain (nDCG@5and nDCG@10) are adopted as ranking metrics to evaluate model performance. We set the batch size to64and conduct early stopping if the validation AUC score had not improved over5epochs. We independently repeat each experiment for10times and report the average performance scores. We compare our model with state-of-the-art general and news-speciﬁc recommendation methods. General Recommendation Methods.General methods utilize handcrafted features to learn news and user representations. We use the TF-IDF features extracted from news and user history with the one-hot news and user IDs as input features for the experiments. The general methods include (1) LibFM(Rendle, 2012), a factorization machine estimating the sparse feature interaction between news and users; (2)DSSM(Huang et al., 2013), a deep structured semantic model, regarding the user history as a query and candidate news as key documents; (3)Wide&Deep(Cheng et al., 2016), a framework consisting of wide channels with a linear model and deep channels with a neural model. Neural News Recommendation Methods.We compete with the state-of-the-art neural models, which are speciﬁcally designed for news recommendation, including (1)DAE-GRU(Okura et al., 2017), encoding news with a denoising autoencoder and users with a gated recurrent network; (2) DFM(Lian et al., 2018), a deep fusion model using multi-channel inception blocks to capture various interaction among news features; (3) DKN (Wang et al., 2018), utilizing knowledge-aware CNNs to fuse knowledge encoding and textual encoding of news title; (4)LSTUR(An et al., 2019), encoding news title with a CNN network, while jointly modeling long-term user preferences and short-term user interests with a GRU network; (5)NAML(Wu et al., 2019a), utilizing CNN networks to encode title and content texts, while encoding the category and subcategory topics with dense layers. The text and topic representations are incorporated by multi- Table 2: Performance comparison results (denotes the highest baseline scores,denotes that the performance improvements over all baseline methods are validated by Student’s unpaired t-test with p-value < 0.01). view attention. NAML uses an attention network as its user encoder; (6)NPA(Wu et al., 2019b), attending to important words and news articles by personalized attention networks built with user embeddings; (7)NRMS(Wu et al., 2019c), utilizing effective multi-head self-attention networks (Vaswani et al., 2017) to extract ﬁne-grained representations from the news title and user history respectively; (8)FIM(Wang et al., 2020), encoding news titles with dilated convolution networks and modeling the interaction between candidate news and user history with 3D convolutional matching networks. Variants of Our Model.To further verify the efﬁcacy of our model design, we also experiment with the ablation variants of our model by respectively removing the cross-selection module (CNE w/o CS), cross-attention module (CNE w/o CA), GCN layers (SUE w/o GCN), and hierarchical cluster attention module (SUE w/o HCA). Table 2 shows the performance comparison results. Our model CNE-SUE achieves the highest performance consistently in all evaluation metrics. Detailed observations can be obtained as follows. General recommendation methods yield much lower performance than most neural news recommendation methods. This is due to that deep neural models can learn reﬁned representations adaptively, which are more effective than general feature engi- Table 3: Ablation study of news encoders (KCNN denotes knowledge-aware CNN in DKN, Per-CNN denotes CNN with personalized attention in NPA, MHSA denotes multi-head self-attention networks in NRMS, ∗-T(C) denotes title (content) encoding only). neering with ﬁxed handcrafted features. In all evaluation metrics, our model CNE-SUE outperforms all comparison methods by signiﬁcant margins (+0.92%AUC,+0.24%MRR,+0.52% nDCG@5, and+0.49%nDCG@10compared to the best baseline performance). This performance improvement derives from the collaborative news representations and structural user representations extracted by our model. Speciﬁcally, CNE can extract more accurate news semantics by leveraging the title-content semantic interaction, compared to title-encoding (e.g., LSTUR, NRMS, and FIM) and separate encodings of title and content (e.g., NAML). SUE modeling diverse user interests with hierarchical cluster structure is more powerful than the comparison methods formulating user history as a linear sequence of news, which employ recurrent neural networks (e.g., LSTUR) or attention networks (e.g., NPA, NAML, and NRMS). From table 2, we can observe varying degrees of performance decreases on the ablation variants compared to our full model. It suggests the usefulness of different components in our model. CNE w/o CA performs the best among all variants. This is because the news representations learned by CNE are composed of self- and cross-attentive representations (refer to Eq. (8)), and the remaining self-attention can achieve suboptimal performance. Removing GCN layers leads to the most signiﬁcant impact on performance, indicating the efﬁcacy of structural modeling on user history. We conduct ablation experiments on news encoders. For fair comparison and excluding the inﬂuence of Table 4: Ablation study of user encoders (ATT denotes vanilla attention networks, Per-ATT denotes personalized attention in NPA, Can-ATT denotes candidateaware attention in DKN, MHSA denotes multi-head self-attention networks in NRMS). SUE, all ablation models apply the same attention user encoders. We also examine the title and content encodings. Table 3 shows the ablation results. From the ablation results, we can observe that CNE signiﬁcantly outperforms other existing news encoding methods. NAML is also competitive, as it can incorporate informative representations from news texts and topic categories. Table 3 also shows that title-encoding (∗-T) is much more effective than content-encoding (∗-C), though content texts are theoretically more informative. This conﬁrms the “semantic encoding dilemma” in news encoding and may explain why many existing works (e.g., LSTUR, NPA, NRMS, and FIM) employ titleencoding only. Comparing NAML to NAML-T, there is no signiﬁcant performance enhancement. In contrast, CNE achieves much higher scores than the individual title and content encodings (CNE-T and CNE-C). It validates the necessity of encoding news title and content with word-level semantic interaction to enhance news representation learning. We conduct ablation experiments on user encoders. For fair comparison and excluding the inﬂuence of CNE, all ablation models apply the same CNN title encoders. Table 4 shows the ablation results. According to Table 4, MHSA performs much better than other baseline user encoders. This is because MHSA (Vaswani et al., 2017) can model the correlation of each pair of news in user history. It validates the necessity of modeling the correlation of historical news in user encoding. Moreover, Table 4 shows that SUE signiﬁcantly outperforms MHSA. This is because the manner of encoding historical news correlation with hierarchical clusters in Figure 3: The performance of our model on validation set with respect to the number of GCN layers (the trend of ndcg@10 is similar to ndcg@5 and hence omitted). Figure 4: Attention weight visualization on the news N6 (darker colors denote higher attention weights). SUE is more ﬁne-grained than MHSA. Concretely, modeling intra-cluster news interaction is more effective to reﬂect aspects of user interests, while modeling inter-cluster user interests interaction is more effective to encode overall user representations. The ablation results indicate that structural modeling of the hierarchical user-interest-news correlation can effectively enhance user encoding. We investigate the inﬂuence of the number of GCN layersLin our model. Figure 3 shows the results. The model performance on validation set increases and reaches a peak, asLincreases from1to4. This is because equipped with deeper GCN, the model can capture more ﬁne-grained information of user browsing behaviors by modeling higher-order interaction of browsed news. Nevertheless, asLcontinues to increase, the model performance begins to decline. This may be because deep GCN always suffers from the over-smoothing issue (Chen et al., 2020). As GCN becomes too deep, the user history representationsrtend to be indistinguishable and impair the ultimate user representations. Herein, L = 4 is optimal for our model. Figure 5: The AUC scores of different models with respect to the number of user interest clusters. We then probe into how our model processes news texts. According to Eq. (6), (7), and (8), we deﬁne the word attention weights of title (content) as α= (α+ α)/2, whereα∈ [0, 1]. As shown in Figure 4, we visualize our model’s output title (content) attention weightsαover the title (content) words of the news N 6 in Figure 1. From Figure 4(a), we observe that our model mainly attends to the words “curse” and “CFP”, which contain the core information of the news N6. It validates that our model can distill the most informative words from the news title. As the content visualization shown in Figure 4(b), our model mostly attends to the words “Alabama” and “toast”, which interpret the speciﬁc semantics of the word “curse” in the context of the newsN6. Besides, our model also attends to the important contextual words, such as “initial”, “rankings”, and “trend”. These title and content attention weights indicate that our model can accurately encode the newsN6. We analyze how our model performs with different numbers of user interest clusters|C|(refer to Section 3.2). The results are shown in Figure 5. When |C| = 1, our model slightly underperforms NAML. This is because our model is overﬁtted to represent single user interest with cluster graphs. All models’ performance increases with the growth of|C|. This is because the models can learn more precise user representations as more news information is incorporated. In cases of|C| > 1, our model consistently outperforms all baselines. It validates the usefulness of encoding user history with hierarchical cluster-structure in cases of modeling diverse user interests. Moreover, the performance of all models decreases when|C|becomes too large (i.e., |C| > 8). This reveals the challenge of predicting a user’s news-clicking behavior when her browsing history covers too many kinds of news topics. In this work, we present a neural news recommendation model with collaborative news encoding and structural user encoding. CNE leverages the title-content semantic interaction to enhance news encoding. SUE exploits the correlation of browsed news and represents user interests with hierarchical cluster graphs to enhance user encoding. Experiment results show that our model achieves significant performance enhancement compared to the existing state-of-the-art methods. We also further analyze our model and validate its effectiveness. We appreciate some insightful comments from the anonymous reviewers. The research described in this paper is partially supported by Hong Kong RGC-GRF #14204118 and Hong Kong RSFS #3133237.