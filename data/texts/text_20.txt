Recommender system plays a crucial role in modern E-commerce platform. Due to the lack of historical interactions between users and items, cold-start recommendation is a challenging problem. In order to alleviate the cold-start issue, most existing methods introduce content and contextual information as the auxiliary information. Nevertheless, these methods assume the recommended items behave steadily over time, while in a typical E-commerce scenario, items generally have very dierent performances throughout their life period. In such a situation, it would be benecial to consider the long-term return from the item perspective, which is usually ignored in conventional methods. Reinforcement learning (RL) naturally ts such a long-term optimization problem, in which the recommender could identify high potential items, proactively allocate more user impressions to boost their growth, therefore improve the multi-period cumulative gains. Inspired by this idea, we model the process as a Partially Observable and Controllable Markov Decision Process (POC-MDP), and propose an actor-critic RL framework (RL-LTV) to incorporate the item lifetime values (LTV) into the recommendation. In RL-LTV, the critic studies historical trajectories of items and predict the future LTV of fresh item, while the actor suggests a score-based policy which maximizes the future LTV expectation. Scores suggested by the actor are then combined with classical ranking scores in a dual-rank framework, therefore the recommendation is balanced with the LTV consideration. Our method outperforms the strong live baseline with a relative improvement of 8.67% and 18.03% on IPV and GMV of cold-start items, on one of the largest E-commerce platform. • Computing methodologies → Reinforcement learning;• Information systems → Re commender systems;• Applied computing → Online shopping. Cold-Start Recommendation, Reinforcement Learning, Actor-Critic Model, POC-MDP, Lifetime Value ACM Reference Format: Luo Ji, Qi Qin, Bingqing Han, and Hongxia Yang. 2021. Reinforcement Learning to Optimize Lifetime Value in Cold-Start Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3459637.3482292 Recommender systems (RS) have become increasingly popular and have been utilized in a variety of domains (e.g. products, music, movies and etc.) [1]. RS assists users in their information seeking tasks by suggesting a list of items (i.e., products) that best ts the target user’s preference. For a practical system, a common pipeline is that a series of items are rst retrieved from enormous candidates, then sorted by the ranking strategy to optimize some expected metric such as CTR (i.e., Click-Through Rate) [20]. However, due to the lack of user-item interactions, a common challenge is the cold-start recommendation problem [42]. Solution to the cold-start problem may depend on the platform characteristics. Traditional way to solve the cold-start problem is leveraging auxiliary information into the recommendation systems (e.g., content based [27,34], heterogeneous information [25,28] and crossdomain [19,31]). Although they have achieved good performance, they focus on the instant reward, while the long-term rewards is ignored. Correspondingly, there is recently increasing attention on the long-term/delayed metric, and solution to optimize the longterm user engagement [35,43] is proposed. However, a long-term viewpoint from the item aspect is still missing. In the case of E-commerce, where the recommended items are typically products, there is a clear need to consider their long-term behaviors, which are changing throughout their life periods. The life dynamics of products share a similar development pattern, as Figure 1: Illustration of the RL-LTV algorithm framework. An RL is employed to solve a item-level MDP and provides a policy optimizing item long-term rewards (LTV). The policy score is combined with CTR ranking score with their combination weight adjusted by the critic expectation. The ultimate ranking score is applie d in online ranking. stated in product life theory [5,18]. [15] further proposes four distinct stages including introduction, growth, maturity and decline, and uses a mathematical tool to model the product life stages and predict the transit probability between dierent stages. In this paper, we also consider the product life dynamics, in a continuous, numerical manner. Within the scope of cold-start recommendation, items on the earlier stages (i.e., introduction and growth) are paid more attention in this work. For a recently introduced item, recommendation algorithms focus on the instant metric may have the exposure bias. Typically, such fresh items are probably not preferred by the instant metric due to the lack of historical behavior, therefore they are subject to low ranking preference. As a result, there might be severe Matthew Eect in the existence of conventional algorithms, in which the mature items keep receiving more impressions and the fresh items are hard to grow up. However, for some fresh items, they could be more and more popular with some investment of user impressions, and yield more returns in the future. From this aspect, a smart cold-start recommender should be able to identify high potential items in advance, and assign them higher ranking priority; while a low potential product could be penalized in contrary. A multi-period maximization task is then needed. Reinforcement Learning (RL) provides a natural, unied framework to maximize the instant and long-term rewards jointly and simultaneously. Nevertheless, considering the complexity of actual online environment, building an interactive recommender agent between user and item, as well as considering the long-term rewards is a challenging task. The trial-and-error behavior of RL might harm the system performance, or aect the user satisfaction. The calculation complexity could also be prohibitively expensive. From these considerations, and considering the fact that the time-evolution of products are naturally in a much slower time scale than online recommendation (days VS milliseconds), in this study we instead dene an o-policy learning method on the item level and on the daily basis, which makes the solution practical. In this paper, we proposes a novel methodology, named reinforcement learning with lifetime value (RL-LTV), to consider the long-term rewards of recommended items inside the cold-start recommendation problem. Such long-term rewards are called by item lifetime values (LTV) in this paper. An o-policy, actor-critic RL with a recurrent component is employed to learn the item-level dynamics and make proactive long-term objected decisions. Information of aforementioned product life stages are encoding by the recurrent hidden memory states, which are studied by a LSTM [16] component, shared by actor and critic. To transfer the information from historical items to cold-start items, we introduce item inherent features, trending bias term, and memory states as extra inputs into both the actor and critic. One of the most important action output by the actor, the item-innity score of LTV, is then incorporated with the conventional ranking score to form a dual-rank framework. Their combination weight is determined by action-value suggested by the critic. Figure 1 illustrates the entire framework of our proposed algorithm. The major contributions of this paper are as follows: •We dene a concept of Partially Observable and Controllable Markov decision process (POC-MDP) to formulate the product life dynamics. Unobservable states depict the intrinsic life stages, while uncontrollable states can aect product growth speed but are independent of actions. •We incorporate the item LTVs into the online ranking by RL- LTV. By prioritizing high potential cold-start items during the ranking, the exposure bias of cold-start items could be overcome. To the best of our knowledge, this is the rst time that such a technique is applied to solve the cold-start recommendation problem. •Knowledge of mature items could be generalized and transferred to cold-start items, even for those rst-introduced items. To achieve this, we build the MDP on the item-level, with continual observation and action spaces, as well as parameter-shared policy and critic networks. •We design a learning framework called IE-RDPG to solve a large-scale RL problem in an itemwise, episodic way. The algorithm is deployed into production and improvements of 8.67% and 18.03% on IPV and GMV for cold-start items are observed. The rest of the paper is organized as follows. The connection with previous works is rst discussed in Section 2. Preliminaries are then introduced in Section 3. The POC-MDP formulation and its learning algorithm are stated in Section 4. Experiment results are summarized in Section 5. Finally Section 6 concludes this paper. In this section, we will briey review representative works of coldstart recommendation and reinforcement learning. Cold-start Recommendation:Although collaborative ltering and deep learning based model has achieved considerable success in recommendation systems[17,21], it is often dicult to deal with new users or items with few user-item interactions, which is called cold-start recommendation. The traditional solution of coldstart recommendation is to introduce auxiliary information into the recommendation system, such as content-based, heterogeneous information and cross domain. Specically, the content-based methods rely on data augmentation by merging the user or item side information [27,34,36,42]. For example, [27] presents an approach named visual-CLiMF to learn representative latent factors for coldstart videos, where emotional aspects of items are incorporated into the latent factor representations of video contents. [34] proposes a hybrid model in which item features are learned from the descriptions of items via a stacked diagnosing auto-encoder and further combined into a collaborative ltering model to address the item cold-start problem. In addition to these content-based features and user-item interactions, richer heterogeneous data is utilized in the form of heterogeneous information network [10,25,29] , which can capture the interactions between items and other objects. For heterogeneous information based methods, one of the main tasks is to explore the heterogeneous semantics in recommendation settings by using high-order graph structure, such as Metapath or Metagraph. Finally, cross-domain methods based on transfer learning, which applies the characteristics of the source domain to the target domain [19,31]. The premise of this type method is that the source domain is available and users or items can be aligned in the two domains. For example, [19] presents an innovative model of cross-domain recommendation according to the partial least squares regression (PLSR) analysis, which can be utilized for better prediction of cold-start user ratings. [31] propose a cross-domain latent feature mapping model, where the neighborhood-based crossdomain latent feature mapping method is applied to learn a feature mapping function for each cold-start user. Although these methods have achieved good performance, most of them only alleviate the cold-start problem from the single-period viewpoint while ignore the long-term rewards. Recently, there is some study which tries to study the long-term eect from the userside [43], but not the item side. In this paper, we try to solve a item cold-start problem by not only using items content information, but also determine the ranking preference of items according to their long-term returns. Reinforcement Learning:Our approach connects to the widely application of reinforcement learning on recommendation problems. These applications include dierent strategies: value-based [6,30,40], policy-based [7,12,14,26], and model-based [3] methods. When the environment is identied as a partial observable MDP (POMDP), the recurrent neural network is a natural solution to deal with the hidden states [4,11,38,43]. Reinforcement Learning also helps to generate an end-to-end listwise solution [39] or even jointly determines the page display arrangement [37]. There are also RL-based studies for cold-start recommendation. [32] proposes an oine meta level model-based method. [9] combines policy-gradient methods and maximum-likelihood approaches and then apply this cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. [43] uses reinforcement learning to solve the multi-period reward on user engagement. [15] proposes a RLbased framework for impression allocation, based on consideration Figure 2: Item metabolism in e-commerce. Recommendation and Search feed the page views of item and determine the click-through rates. Pricing strategy further determines how many sales are converted. All of these behaviors result in more people interested with the item and choose to search it in the future. The item then obtains more priority in search and recommendation algorithms because of more historical information. of item life period stages. In their work, the item stages are explicitly identied and predicted by a mathematical model while RL is used inside the impression allocation problem. In contrast to [15], in this paper the item stage information is implicitly studied by reinforcement learning in an end-to-end manner. In particular, we use recurrent neural network to encode the hidden state as a continual and dense representation of life stages, based on item histories. Our framework jointly studies this recurrent hidden state, the action value as a prediction of item long-term reward, as well as the ranking policy. In this section, we rst introduce the background of product metabolism and the idea of item lifetime value on E-commerce, which basically motivates this work. Based on the understanding of this scenario, we dene a special type of Markov Decision Process to model the metabolism of an item. The basic DDPG algorithm is nally shown as the underlying learning approach. A typical pattern of product metabolism on the e-commerce platform is shown in Figure 2. Assuming a balancing electric scooter is just introduced into the platform, it usually receives few user interest and the statistics are in low-level. Although the item can be left to grow up by itself, several channels could be utilized to change its situation. First, the item’s user exposure (or more specifically, the page views (PV)) is signicantly aected by both search and recommendation algorithms. Second, reasonability of search and recommendation directly aects the click-through rate (CTR), i.e., how many item page views (IPV) could be clicked from PV. Furthermore, as the third channel, the pricing strategy from the pricing system (PS) has substantial impact that how many number of sales (SLS) are converted from IPV. During this process, PV, IPV and SLS are accumulated, the item’s reputation is built, and the interested group of users continues to expand. As more users appeal to the item, their behaviors help the specic item claim more importance in search and recommendation algorithms, therefore a positive feedback closed-loop mechanism could be created. As item elapses, the fresh item may have growing life dynamics trajectories of key indicators, including PV, IPV and SLS. However, not all items can achieve such a positive closed-loop mechanism. The growth rate of an item depend on its inherent characteristics (what the item is), brand, market, external trends or noise, and the algorithms. Actually, most items nally fall into the group called "long-tail products", with few user views, clicks or purchases. Therefore, it would help if one could identify if the item could return signicant future PVs, IPVs and GMVs, such that the star products and the long-tail products can be classied even at their early life stage. In this paper, we call such long-term rewards as the item’s Lifetime Value (LTV). By allocating more resources for those high potential products, the platform would be repaid with more LTV in the future, and makes the entire ecosystem grow and prosper. As shown in Figure 2, search, recommendation and pricing are possible important tools to allocate the resource and adjust the item dynamics. Markov Decision Process (MDP) is typically employed to model the sequential decision making problem. Nominal MDP usually consists of the 4-tuple(S, A, R, P), whereS, A, R, Pare the state space, action space, set of rewards, and transition probability functions, respectively. At time step𝑡, the state𝑠∈ Srepresents the current system stage, which is aected by an action𝑎∈ Afrom the agent, generating a reward𝑟by the reward functionS × A → R, as well as the next state𝑠∈ P(𝑠, 𝑎). For such nominal MDP, it assumes that all elements of𝑠can both be observed by agent, and be changed by the agent’s action. However, it is rare that this assumption holds in the real environment. For example, the Pong game in Atari have both the current image and the ball velocity as state, while only the former is provided to the agent. Partially Observable Markov Decision Process (PO-MDP) captures the partial observability part of system complexity. It is instead described as a 5-tuple(S, A, P, R, O), in which O denotes the set of observable states, i.e., O is subset of S. Another form of system complexity is relatively less studied, the MDP with uncontrollable states [2,22]. In such situations, some elements of𝑠can never be aected by agent actions, but they can determines𝑠therefore aect future rewards too. We here pay especial attention to such form of uncontrollablity, which will be discussed with more details in the next section. In our topic, it is believed that both unobservable and uncontrollable states exist, both of which are discussed with more details in Section 4. We deal with the above concerns and dene a Partially Observable and Controllable Markov Decision Process (POC-MDP), which literally means there are some unobservable states and some uncontrollable states in MDP at the same time. Although the term "state" can denote all states no matter they are observable or controllable, for clarity, we use the notation𝑠to present only the nominal (both observable and controllable) states. The unobservable states (but controllable) are denoted byℎ ∈ H, and the uncontrollable (but observable) states are denoted by𝑥 ∈ X. As a result, our POC-MDP now has the 6-tuple(S, A, P, R, O, H ). Note now the observation is the concatenation of nominal states and uncontrollable states, i.e., 𝑜 := [𝑠, 𝑥]. In this section we give a brief introduction to an model-free, opolicy, actor-crtic RL algorithm, the Deep Deterministic Policy Gradient (DDPG) method [23], which is closely related to our proposed approach. In DDPG, the actor network𝜋is approximated by the net parameter𝜃and generates the policy, and the critic network𝑄is approximated by the net parameter𝑤and generates the actionvalue function. Gradients of the deterministic policy is where𝑑(𝑠)is a discounted distribution of state𝑠under the policy of 𝜋. Then 𝜃 can be updated as with𝜂as the learning rate. For the critic, the action-value function can be obtained iteratively as 𝑤 is updated by minimizing the following objective function min𝐿 = E[(𝑅 (𝑠, 𝑎)+𝛾𝑄(𝑠, 𝜋(𝑠))−𝑄(𝑠, 𝜋(𝑠)))] where𝜋and𝑄are the target networks of actor and critic. The target network parameters can be softly updated by This section illustrates the key concepts of our approach, including how we implement our RL-LTV, and how its action is applied in the online system. First denitions of terms in POC-MDP are listed, then architectures of the actor and critic networks are introduced, after which the learning algorithm follows. The actor outputs a preference score, which is linearly combined with the ranking score from a conventional CTR model in a dual rank framework. The new ranking score is then applied to sort items within the response to each request. Table 1 summarizes important symbols which are frequently used in the current and related sections. In this paper, we consider a two-level agent, including RS which aects the click rate and PS which aects the conversion rate. The environment is the e-commerce ecosystem. Figure 3 provides a snapshot of our POC-MDP, with terms dened below: State. The state space is dened on the item-level, representing the observable part of current item life stage, including the item’s time on the market, PV (both current and accumulated), IPV (both current and accumulated), SLS (both current and accumulated), and properties (number, averaged activeness frequency, averaged purchasing power, etc) of the user crowd currently interested with the item. Action. Action is denoted by𝑎= [𝑦, 𝑝] ∈ A. For the RS part,𝑦∈ (0,1)indicates the RS’s preference ranking score for a specic item. PS at the downstream of RS has the action of𝑝which is modelled as the price discount degree, the averaged paid price divided by the original price. Both𝑦and𝑝can be easily retrieved from the real data. Uncontrollable State. We formulate such states from the inspiration that an item’s next state is not only result of its current state, but also determined by its inherent feature and extrinsic trending-bias factors. The inherent feature (e.g., title, cover image, category, brand, shop name) can always be assumed item-invariant. Therefore, we denote them as𝑥(where i means both inherent and item-invariant). Notably, within𝑥a pre-trained, 128-dim item title & image multi-modal embedding is employed as the core information to bridge the information between dierent items [24]. Other parts of𝑥, including category, brand and shop, are input as ID-type features. The extrinsic bias factors can have dierent trend sources, including the entire market, the platform campaign, item seller, brand or category. We denote them as𝑥because obviously they are time-variant. For each trending source, we include their moving average growth percentage (MAGP) of PV, IPV and SLS. Reward. As mentioned before, the MDP is dened on the itemlevel in order to share information between items. However, online Figure 3: Schematic representation of POC-MDP. The agent are two-level in which 𝑦is the action of RS and 𝑝 is the action of PS. The item inherent, item-invariant feature is denoted by 𝑥, and the trending bias, time-variant factor is denoted by 𝑥, both of which can aect the dynamics. recommendation contains a listwise ranking which means dierent item competes with each other for impression allocation. Considering this dimensional mismatch, we can not simply dene the reward as a absolute value (LTV of PV, IPV, SLS, or their linear combinations) because in this case RL would simply generate𝑦 for each item as large as possible. Instead, we choose the form of reward to be similar with ROI(the return of investment), which is dened as the return of the next-step IPV with the investment of current recommended PV: Long-term Gain and Discount Factor.The multi-period, cumulative gain is dened by which could be viewed as our ROI version of LTV.𝑇is the window length and𝛾is the important discount factor, both of which control how far a model looks forward when optimizing𝐽. In our study,𝛾 is set to a smaller number, i.e., 0.5, indicating that our RL balances two sides of considerations: (1) the long-term total rewards, as well as (2) to speed up the growth of cold-start items as soon as possible. The actor is designed to generate the action𝑎= [𝑦, 𝑝]given the current observation𝑜= [𝑠, 𝑥, 𝑥]. Such policy function is approximated by the actor net𝜋(𝑎|𝑜)with𝜃representing net parameters. As stated in Subsection 4.1, the item inherent feature 𝑥including two parts: the rst part is the pretrained embedding vector; the second part is composed of dierent IDs, all of which are feed into an encoder network𝑓(𝑥, 𝑤). Embedding generated by this encoder is concatenated with the pretrained vector and forms 𝑥, the embedded version of𝑥. Then we have the new observation after embedding𝑜= [𝑠, 𝑥, 𝑥]. Meanwhile, in order to allow the agent to capture the intrinsic item life stage, we introduce an LSTM cell to encode the sequential information of historical observations into continuous hidden states: where𝑍, 𝑍, 𝑍are the forget, update, output gates.𝑊,𝑊, 𝑊,𝑊are LSTM parameters.𝑐andℎare the cell state and hidden state, respectively. In general, actor’s inputs are [𝑜, ℎ]. In order to enhance the actor’s ability to memory𝑜and generalize ℎ, a wide and deep structure [8] is applied: in which𝑓and𝑓are one or three fully connected layers, respectively. To obtain the rst action (i.e.,𝑦∈ (0,1)), we use the following functions where𝑊,𝜎are the weights of linear layer and sigmoid function. Note the second action, the pricing discount𝑝, is output by a similar logic structure with𝑦; this part of logic is omitted in the gure, for simplicity. The left part of Figure 4 shows the actor structure introduced above. The right part of Figure 4 illustrates our critic. It shares the same ID encoder and LSTM components with actor. A dueling structure [33] is here employed to depict the action value function, by decomposing it into the state value 𝑉 (𝑜) and the action advantage value 𝐴(𝑜, 𝑎).𝑉 (𝑜)is generate by a dense net given𝑜andℎ. For𝐴(𝑜, 𝑎), we further decompose it as𝐴(𝑜, 𝑎) = 𝐴(𝑠, 𝑎) + 𝐵𝑖𝑎𝑠 (𝑠, 𝑥, 𝑎). 𝐴(𝑠, 𝑎)has the same meaning with what is dened in the vanilla dueling structure, which is calculated by another dense net; the second term,𝐵𝑖𝑎𝑠(𝑠, 𝑥, 𝑎), representing the bias consideration caused by the trending-bias factor𝑥, is calculated by a single linear layer. The nal𝑄value is obtained by simply adding all these terms: 𝑄 (𝑜, 𝑎, ℎ) = 𝑉 (𝑜, ℎ) + 𝐴(𝑜, 𝑎) RL aims to solve the MDP and provide a policy𝜋(𝑎|𝑠)which maximizes the expected discount return in Equation (2). Because our RL framework and online recommendation are in dierent time frequencies, an o-policy method is more suitable for us, such Figure 4: Our Actor-Critic Network Structure. Actor (left) and critic (right) share the same item encoder and LSTM component. as DDPG introduced in Section 3.3. Same with the original version, here we also applies the double-Q network settings and a replay memory buer. However, with the existence of the recurrent component, and also to reduce the computational burden for the industrial-scale data, here we consider a dierent way of data sampling and network update. In the vanilla DDPG method, one samples a batch of transitions (the tuple(𝑜, 𝑎, 𝑟, 𝑜)) and the gradient update is the batch average of each transition feedback. In our approach, we instead randomly sample a batch of items (or their episodes) from the buer; updates are executed at the beginning of episode then proceed forward through time until the episode ends. In such manner, the hidden state in LSTM is always carried forward from its previous value, which alleviates its convergence speed. One can refer to [13] for a similar update method in which it is called "Bootstrapped Sequential Updates". Note in our system, an episode is an item’s transition sequence collected and sorted in the daily queue. Therefore, here we name our training algorithm as Itemwise Episodic Recurrent Deterministic Policy Gradient (IE-RDPG) with the pseudo code summarized in Algorithm 1. There are two phases within a training session of IE-RDPG. First, we let the agent to interact with the platform with respect to the current policy, collect the data until𝑇days. For each item, transition is stored in chronological order to form the episode. After that, the parameter updating stage starts with randomly sampling a batch of𝑁episodes at one time. The parameter update is rst conducted parallelly at the beginning timestep, then such update continues to proceed forward through time until all episodes end. The parameter updating stage could have multiple epochs. This two-phase session repeats in the real-world timeline. IE-RDPG provides an asynchronous and distributed architecture which is able to solve the problem with enormous items in the one of the largest E-commerce platform. Algorithm 1 IE-RDPG 𝜋(𝑎|𝑜) Transition Generation Stage: Parameter Updating Stage: Pointwise Learning to Rank (LTR) is widely used in industrial recommendation ranking. Upon each user request, the item feature is aligned with the user characteristic and an user-anity score is calculated by a supervised model like click-through rate (CTR) model. This ctr score,𝑦, is employed as the ranking metric to yield a ranked item list. In our study, RL-LTV provides another item-anity score,𝑦, indicating its preference based on the item LTV. By forming a dual-rank module and considering these two scores simultaneously, the online ranking could consider not only the immediate click-through reward but also the long-term rewards. As a result, some fresh items without enough historical interactions but with high potential of LTV, would have small𝑦but large𝑦, and is expect to have more ranking priority. We obtain the new ranking score by the following equation: where𝛼 ∈ (0,1)is the mixing parameter. Since𝑄output by the critic predicts the future LTV, it could be used to determine the degree of𝛼, therefore also determine how the CTR and LTV rewards tradeo. In the experiments, 𝛼 is calculated as where𝛼, 𝛼are hyper-parameters highlighting the lower and upper bounds of𝛼; and item with higher𝑄would always have a higher 𝛼. To evaluate the proposed model, we apply our approach on Taobao, a world-leading E-commerce platform. A series of experiments are designed in order to address the following questions: (1) whether the proposed approach performs better than baseline, or some other competitors which is not RL-based; (2) verify if inclusion of𝑥, or the LSTM component contributes to the performance; and (3) if some representative examples could be found to display how the framework improve LTV, from the business point of view. It is important to emphasize that, most of public dataset in the recommendation domain do not have the characteristic of item dynamics depicted in Section 3.1, therefore we can not simply directly comparing our performance with most state-of-the-art baselines. On the other hand, the live environment contains billions of users and items, therefore it is also not a trivial task to launch an arbitrary baseline on the real system. As a remedy, the performance of RL-LTV is comparing with the live ranking algorithm (refer to [41] for part of details) which is a reasonable baseline for such a complicated industrial system. We also conduct oine analysis on the LTV recognition including comparison of several baselines in Section 5.2. Ablation and sensitivity analysis are also provided to further illustrate the reasonability of our framework. Training process: The online ranking system is in real-time, while our approach has a daily update frequency. In our practice, user logs are kept collecting, then data is aggregated by item and by day, and the MDP transitions are recorded. According to Algorithm 1, for each item we wait for𝑇new transitions to appear; a new episode is then formed and put into the buer. Similarly, it is the episodes from each buer sampling, and gradient update is performed accordingly. Parameters setting: In the training of our RL-LTV, we use Adam with learning rate 0.0001 as the optimizer to train actor and critic. The hyper-parameters𝛾,𝜏,𝛼and𝛼are 0.5, 0.001, 0 and 0.2, respectively. The dimension of hidden states in the LSTM component is 4. The item inherent feature output by the item encoder has the dimension of 8. From the applicable point of view, the time step is chosen as a day. We have relatively small buer (200) and batch size (50), because our replay memory buer is operated in terms of episodes, not transitions. Because recommendation is directly correlated with customer experience and platform revenue, scale and depth of the online experiment are limited at the current stage. Nevertheless„ some oine analysis can be conducted from the perspective of LTV recognition. Metrics:Two aspects of performances could be evaluated oine: (1) Since the critic provides a predicted version of the actual cumulative gain,𝑄, the regression accuracy between𝑄and the actual 𝐽,𝐽, can be calculated and evaluated; (2) The actor generates 𝑦which indicates its ranking preference w.r.t. LTV; a ranking metric can be evaluated by sorting𝑦in a descending order, and Table 2: Oline metrics on item LTV. Note Baseline and Empirical do not have RMSE or MAE result because they do not have 𝑄 estimates. comparing its sequential similarity with the ground truth (item sorted by 𝐽). To calculate𝐽, data of 5 consecutive days are collected; while data in longer horizons can be ignored since their weights in𝐽decay quickly with𝛾 =0.5. For (1), the prediction error of this 𝐽is studied in the manner of root mean square error (RMSE) and mean absolute error (MAE) . We employ the normalized discounted cumulative gain (NDCG) as a measure of the sequential similarity stated in (2). To mimic the live environment and reduce the computation cost, the online retrieval method is also employed for this oine evaluation, which generates a series of items upon each user query. The top-𝐾items from the retrieved list are used to calculate NDCG@K with𝐾 = 10, 20, 50, and 𝐽is used as the relativity score. Baselines:The following experimental versions are employed to compare with our RL-LTV: •Vanilla-CTR: The online, pointwise, single-period, vanilla CTR model provides a natural baseline running everyday on Taobao, with a state-of-the-art CTR performance by so far but without any LTV consideration. •Empirical: To mimic the human decision making process, scores are manually assigned based on business experience, e.g. collect percentiles of𝐽according to category, seller and brand respectively, calculate their weighted averages, then scale to (0,1). •LSTM: Use a supervised LSTM model to regress and predict 𝐽. Score is then assigned based on prediction. It shares the same structure as the LSTM component in RL-LTV. Note we only compare the regression accuracy of LSTM with RLLTV, while Vanilla-CTR or Empirical do not have an explicit LTV prediction. On the other hand, we can evaluated the ranking NDCG of LTV for all these baselines. For Vanilla-CTR, the CTR score is used to rank its preference for LTV ranking, which is actually the exact case on the live platform. Result:We summarize these results in Table 2. RL-LTV is expected to have superior LTV regression accuracy, as a result of its interactive learning behavior and long time-horizon optimization, which is veried by the result. Compared to LSTM, RL-LTV reduces the prediction error of LTV for both MAE and RMSE. For NDCG@K, one can nd that Empirical, LSTM and RL-LTV all have better NDCGs than Vanilla-CTR, which only looks at the instant CTR metric. Still, RL-LTV is the best among these experiments. The live experiment is conducted on Taobao. The A/B test starts on January 26th, 2021 and lasts for a week. Policy of RL-LTV is rst Table 3: Percentage Dierence of LTV relative to VanillaCTR for cold-start items. Table 4: Comparison of Oline Performance for Component Ablation Analysis warmed up with the trained version in Section 5.2, then updates online in a daily basis. Because of the real world limitation, only𝑦 in the action takes eective;𝑝is instead determined and controlled by other group, therefore becomes a read-only parameter for us. Cold-start performance:We rst investigate the performance of cold-start items. To dene cold-start items, fresh products with time on the market less then a month when the experiment starts are tagged, and then randomly divided into dierent groups. Each group of cold-start items is limited to be recommended by only one experiment, such that the item-based metrics (IPV and GMV) of dierent experiments can be reasonably calculated and fairly compared. LTV in forms of PV, IPV and GMV are collected after the online test ends. For RL-LTV, relative LTV dierences to VanillaCTR are calculated and shown in Table 3. One can see that RL-LTV improves LTVs of IPV and GMV signicantly, with almost the same PV investment. Global performance:The ultimate purpose of cold-start recommendation is to improve the entire RS performance. Therefore, we need to inspect metrics of all items, not only those of cold-start items. Although the short-term performance of RS might be harmed due to the investment on cold-start items, it is expected its longterm performance could be reimbursed with the growth of LTVs. By the end of experiment, such reimbursement eect is observed. In more details, RL-LTV has−0.55% PV and−0.60% IPV compared with Vanilla-CTR, indicating there is no severe degradation for the global performance. Typical case:In order to have a deeper insight that how RL helps recognize a specic high potential item and improve its LTV, we also look into some specic cases. Figure 5 shows such a case study with an item (a hook handle) rst introduced on the platform on January 26th. As it cold starts, there is few people who comes to view, click and buy it. However, RL-LTV recognizes it as high LTV potential, therefore makes substantial PV investment at the early stage (the𝑃𝑉curve on the upper right). This investment is successful since it triggers more people’s interests and behaviors (the𝑃𝑉(PV from the other channels) curve on the lower right), with PV, IPV and GMV all grow rapidly (the accumulated curves on the lower left). After this item turns into a star product, RL-LTV turns to invest other cold-start items. As a result,𝑃𝑉falls down after the 3rd day, while the other metrics are already in a healthy closed-loop feedback. Figure 5: Time trajectory of a typical cold-start recommended item (A hook handle as a fresh item of platform). Upper Right: The 𝑃𝑉invested by RL-LTV. Lower Right: 𝑃𝑉(mainly from the search channel) inspired after LTV investment. Lower Left: The accumulated PV, IPV and GMV indicating this item rapidly becomes a star-item. To illustrate the eectiveness of several important components in RL-LTV, we here perform some ablation analysis on the follow attempts: •RL-LTV(w/o𝑥): our approach but without the inclusion of the item inherent feature, 𝑥. •RL-LTV(w/o𝑥): our approach but without the inclusion of the trending-bias factor, 𝑥. •RL-LTV(w/o R): our approach but without the recurrent LSTM component. Similar to Section 5.2, we perform the oine analysis for these experiments, and their results aside with RL-LTV are given in Table 4. Not surprisingly, RL-LTV still has the best performance, suggesting that𝑥,𝑥and the recurrent cell are all crucial. Due to limited online resource, only RL-LTV(w/o R) is conducted with an live experiment, with result also shown in Table 3. One can see that RL-LTV(w/o R) also has positive online impacts compared with Vanilla-CTR, but not as good as RL-LTV. All these results emphasize the validity of our original denition of POC-MDP. Comparison with RL-LTV, the degraded performance of Vanilla-CTR (in Subsection 5.2 and 5.3) veries the entire MDP and RL framework; the degraded performances of RL-LTV(w/o 𝑥) and RL-LTV(w/o𝑥) indicates the necessity of uncontrollable state (i.e.,𝑥and𝑥); the degraded performance of RL-LTV(w/o R) suggests the unobservable state also helps the modeling. Here we also perform a sensitivity analysis for an important parameter, the incorporation weight𝛼in Equation (7). Dierent choices of𝛼generates dierent nal score𝑦, which similarly, can also have an NDCG evaluation introduced in Section 5.2. Furthermore, as an indication of CTR prediction with the true CTR label, AUC of Figure 6: AUC, NDCG@10, NDCG@20 and NDCG@50 curves with dierent 𝛼. When more proportion of 𝑦is adopted against 𝑦, AUC decreases but NDCG of LTV increases, as expe cted. 𝑦can also be evaluated with respect to dierent𝛼. As𝛼steadily changes from 0 to 1,𝑦transforms from a ranking metric of CTR to a ranking metric of item LTV. By checking the curves of AUC (of CTR) and NDCG (of LTV) w.r.t.𝛼, one could have a snapshot about the trade-o between the instant reward and the long-term reward.The result is demonstrated in Figure 6. Not surprisingly, the AUC decreases while NDCG increases as𝛼increases. Based on the shape of curves, we expect the mean of𝛼to be around 0.1 where the AUC curve starts to decrease faster. Correspondingly, 𝛼and 𝛼in Equation (8) are set to 0 and 0.2, respectively. In this paper, we propose a novel RL-LTV framework which solves the cold-start recommendation problem by considering the longerterm rewards of items, the LTVs. A special form of MDP, POC-MDP, is employed to model the item life dynamics. Generalized item-level observation and action spaces, as well as parameter-shared networks, help the knowledge transfer from mature, historical items to fresh, cold-start items. An o-policy, actor-critic RL agent with an LSTM component is employed to interactively learn and change the online system with a policy optimizing both instant reward and long-term LTV. It is the rst time to incorporate such policies into the online recommendation system, to address the item cold-start issue. We develop a training framework called IE-RDPG to complete the large scale, itemwise episodic training task. By rigorous experiments, it shows that our algorithm performs much better on the long-term rewards of cold-start items, comparing with the online state-of-the-art baseline on Taobao. There is no evident degradation of the entire online performance during the experiment. For future work, it would be interesting to use RL to study the entire life periods of products, therefore produce policies not only for cold-start but also lifelong recommendation. Another possible improvement is to make the model more explainable to explicitly exhibit the lifetime stage transitions of products.