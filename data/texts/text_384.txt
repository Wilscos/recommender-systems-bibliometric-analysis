Noname manuscript No. (will be inserted by the editor) Abstract News recommender systems are essential for helping users to efﬁciently and eﬀectively ﬁnd out those interesting news from a large amount of news. Most of existing news recommender systems usually learn topic-level representations of users and news for recommendation, and neglect to learn more informative aspect-level features of users and news for more accurate recommendation. As a result, they achieve limited recommendation performance. Aiming at addressing this deﬁciency, we propose a novel Aspect-driven News Recommender System (ANRS) built on aspect-level user preference and news representation learning. Here, news aspect is ﬁne-grained semantic information expressed by a set of related words, which indicates speciﬁc aspects described by the news. In ANRS, news aspect-level encoder and user aspect-level encoder are devised to learn the ﬁne-grained aspect-level representations of user’s preferences and news characteristics respectively, which are fed into click predictor to judge the probability of the user clicking the candidate news. Extensive experiments are done on the commonly used real-world dataset MIND, which demonstrate the superiority of our method compared with representative and state-of-the-art methods. Keywords Recommender system · News recommendation · Aspect-driven · Representation learning Nowadays, more and more users like to read news via online news platforms such as Google, BBC and Toutiao News. Although these platforms bring great convenience to users in reading news, they still are confused by the inevitable issue of information overload [1,2]. This is because they always provide a huge amount of news, making it hard for users to eﬀectively select and read those they are interested in. To address this problem, an ideal platform should be equipped with a news recommender system to learn user’s preferences and recommend appropriate news to the user [1,3,4]. nology plays important roles in various tasks [5,6,7,8,9,10,11], which is also applied widely in news recommendation. For example, TANR [12] is a neural topic-aware news recommender system, which trains the news encoder with an auxiliary topic classiﬁcation task to learn accurate news representations. NAML [13] leverages attentive mechanisms to learn user’s preferences and news features from multi-view information including the title, category and body of news for news recommendation. GNewsRec [14] ﬁrst builds a heterogeneous news-topic-user graph and then applies graph convolution networks (GCN) and long short-term memory (LSTM) on the graph to extract user’s long- and short-term preferences for news recommendation. DAN [15] captures user’s latent preferences towards news with attention-based convolution neural networks (CNN) and recurrent neural networks (RNN) from clicked news for recommendation. Although these existing methods have achieved some progresses in news recommendation task, most of them attempt to model user’s preferences and represent news features on coarse-grained topic level instead of ﬁne-grained aspect level of news. However, aspect information is essential for news recommendation. For example, for a piece of news related with food, its topic maybe food, while it may include several detailed aspects, such as beef, pasta and tomato [16]. It is obvious that the aspects are more accurate than the topic, which can provide more ﬁne-grained information for news recommender systems. However, the existing methods always neglect to model user’s preferences and news features on the aspect level, which leads to the limited performance on news recommendation. news recommendation are found. First, though topic information is important for news, it is still coarser than aspect information, and hence it can not accurately describe news features. As illustrated in Fig. 1, the topic of the ﬁrst news “50 Worst Habits For Belly Fat...” is health. Besides such topic information, the news could be described with ﬁne-grained aspects, such as With the rapid development of artiﬁcial intelligence, deep learning tech- According to our investigation on existing work, three observations for fat, habits, etc. Second, topic information can not comprehensively represent user preferences. It is necessary to utilize ﬁne-grained aspect information to emphasize the speciﬁc preferences of a user. Taking the second news in Fig. 1 as an example, a user may be interested in certain aspects, such as Ford and Australia. However, if the recommender system merely works on topic information, it would recommend all auto news to the user, instead of Ford and Australia ones. The user’s experience will be inevitably downgraded by the irrelevant news. Third, lots of news isn’t annotated explicitly with topic information. Huge amounts of news is generated every day, and topic annotation is time-consuming, so it is impossible to manually annotate topic labels for all news. This limits the applicability of news recommender systems built on explicit topic information. In contrast, the aspect information of news can be extracted automatically. These observations mentioned above demonstrate the necessity of making news recommendation driven by the ﬁne-grained aspect information. (ANRS), which is built on aspect-level user preference and news representation learning. Speciﬁcally, ANRS model consists of three modules, i.e., user aspectlevel encoder, news aspect-level encoder and click predictor. News aspect-level encoder is built on CNNs and attention networks to learn aspect-level news representations to capture the ﬁne-grained semantic features. Similarly, User aspect-level encoder is also built on CNNs and attention networks to accurately learn the representation of the user’s aspect-level preference towards news. More speciﬁcally, news aspect-level encoder and user aspect-level encoder are equipped with an aspect-level feature extractor to generate aspect-level representations for news and users, which are further fed into click predictor to calculate the probability of a given user to click on each candidate news and make the personalized recommendation for the user. Extensive experiments are done the real-world dataset MIND, which demonstrate that our ANRS model is able to achieve superior performance compared with state-of-the-art competing models. Our main contributions are concluded as follows. – We propose to make accurate news recommendation by modeling ﬁne- To this end, we propose a novel Aspect-driven News Recommender System grained aspect-level user preferences and news features. As far as we know, – We devise a novel Aspect-driven News Recommender System (ANRS), – We conduct extensive experiments on the real-world news recommendation We review two categories of representative and state-of-the-art recommendation methods: topic-based news recommendation and aspect-based product recommendation, which are the most relevant ones to our work. to their strong capabilities to model topic information for news recommendation. The ﬁrst topic-based news recommender system, TANR [12], employed CNN and attention networks, and combined with topic classiﬁcation task to learn topic-aware news and user representations for news recommendation. Later, Wu et al. [13] proposed a multi-view learning model to generate news representations from their titles, bodies and topic categories for news recommendation. Then, Hu et al. [14] proposed a heterogeneous user-topic-news graph-based news recommender system, which learned user’s long-term preference representations and news representations with graph neural network, and learned user’s short-term preference representations with CNN combined with LSTM networks. Lee et al. [17] devised a topic-enriched news recommender system with a knowledge graph, which exploited external knowledge and topical relatedness in news. Ge et al. [18] proposed a graph-enhanced news recommender system, which employed a modiﬁed transformer and graph attention network to encode news titles and topics so as to enhance news and user representations. Although these topic-based news recommender systems have achieved great successes, they model user’s preference and news features on the coarse-grained topic-level information, neglecting the ﬁne-grained aspectlevel information. This limits their further improvement on recommendation performance. are extracted for analyzing which opinions have been expressed [19]. In a recommendation scenario, users often provide reviews to explain why they like or dislike products from diﬀerent aspects, such as prices, quality, etc. this is the ﬁrst work for news recommendation driven by aspect-level information. which is built on aspect-level user preference and news representation learning. ANRS consists of user aspect-level encoder, news aspect-level encoder and click predictor. Particularly, aspect-level feature extractor is elaborately designed to extract aspect information from news for better representing user’s preferences and news features. dataset MIND. The experimental results substantially verify the eﬀectiveness of ANRS, which demonstrates better performance than state-of-theart competing models. Topic-based news recommender systems have been widely employed due The deﬁnition of aspects originates from sentiment analysis task, which The topic models, such as latent dirichlet allocation (LDA) [20], probabilistic latent semantic analysis (PLSA) [21], are always employed to extract the aspects. To accurately capture the aspect information is crucial for recommender systems to recommend next items for users [22]. In recent years, the aspect-based product recommender systems focus on extracting aspects from textual reviews of products. ALFM [23] was an aspect-aware topic model, which employed a topic model to extract the topic representations of aspects and aspect-aware latent model to evaluate the overall user-product rating. Bauman et al. [24] proposed aspect-based recommendation with user reviews, which recommended products together with important aspects to users according to the most valuable aspects of the user’s potential experience. Hou et al. [25] devised an explainable recommender system with aspect information, which utilized aspect-level information to make quantitative ﬁne-grained explanations for recommendation. ANR [22] was an end-to-end aspect-based neural recommendation model, which learned aspect-based representations for both users and products with an attention-based component. Luo et al. [26] proposed an aspect-based explainable recommender system, which addressed some problems including dynamic of explanation, personal preference learning and the granularity of explanations. Much progress has been achieved for aspect-based product recommendation, demonstrating the signiﬁcance of aspect-level information for recommendation. ommendation task, there is no existing work in news recommendation that is driven by aspect-level information. According to the former work on aspectbased product recommender systems, it is obvious that aspect information is more ﬁne-grained than topic information, which can express more accurate semantic information and is beneﬁcial to improve the recommendation performance. In a news recommendation scenario, users may also like or dislike the news from various aspects, such as politicians, epidemic situation, country, etc. Considering the great success of aspect information in product recommendation, we introduce it into news recommendation and propose aspect-driven user preference and news representation learning for news recommendation. Given a user u, a set of news browsed by u is deﬁned as D Given a piece of candidate news c with a binary label y ∈ {0, 1} which indicates whether u will click c. ANRS is trained as a prediction model which learns to predict the probability that u would like to click c according to the browsed news D parameters of the model are trained, ANRS is able to perform personal news recommendation based on the ranking of candidate news. Although aspect-level information have been widely applied in product rec- . The probabilities are used to rank all candidate news. Once the News aspect information is more ﬁne-grained than topic-level information, which is beneﬁcial to the accurate news recommendation. Inspired by the work of He et al. [16], we deﬁne news aspect as a group of representative words in a piece of news. the aspect of news is composed of a set of words in news, which is diﬀerent from news topic. For example, the second news in Fig. 1 has been annotated manually with topic label Auto and sub-topic label Autonews. Both of labels belong to the same category, which are coarse-grained topic information. However, our model can capture diﬀerent ﬁne-grained news aspects, such as Ford and Australia. These ﬁne-grained aspect-level information can reﬂect more accurate features of news, which are crucial for the performance improvement of news recommender systems. In this section, we introduce our model illustrated in Fig. 2. The architecture of ANRS contains three core modules: news aspect-level encoder, user aspect-level encoder and click predictor. News aspect-level encoder is utilized to encode news aspect-level information, which contains news feature extractor and aspect-level feature extractor. User aspect-level encoder is employed to encode user aspect-level preference, which consists of user preference extractor, news feature extractor and aspect-level feature extractor. Click predictor is applied to calculate the clicked probability of candidate news. We describe more details of these modules in this section. In order to obtain news feature embeddings and aspect-level feature embeddings, we design news aspect-level encoder, which consists of two core submodules: news feature extractor and aspect-level feature extractor, as shown in Fig. 2. Given a piece of news, ﬁrst, news feature extractor and aspect-level feature extractor are employed to encode it to generate the initial news embedding r and the aspect-speciﬁc embedding r are concatenated together to obtain the ﬁnal news representation n, described Some examples of news aspects are provided in Table 1. We can ﬁnd that as follows: where the operator [; ] means the concatenation operation. Aiming to fully extract information contained in three sections of each news, i.e., title, category and abstract, inspired by the work of Wu et al. [13], we design four components to encode them into latent representations simultaneously, i.e., title learning component, abstract learning component, category learning component and attention component, as illustrated in Fig. 2 (a). The ﬁrst layer is to transform the words in news titles into word embeddings. We deﬁne a news title as T = [t of T , t denotes a word in T . This layer transforms T into the embedding representation E W ∈ R of word embedding, respectively. performance in news recommendation. The previous works of FIM [27], NPA [28] and NAML [13] employ CNN to learn local features of news. Hence, we utilize CNN to capture the deeper features in the second layer of title learning component. The representation of the i -th position is described as Equ. (2). where f is ReLU, ∗ refers to the convolution operator, e concatenation of word embeddings from the position (i − k) to (i + k), C the kernel of CNN ﬁlters, b can obtain a sequence of contextual word embeddings, i.e., [c Title learning component. There are three layers in this component. CNN is widely applied to capture text features and demonstrates better ﬁnd the important features in news representations [15]. Therefore, in the third layer of title learning component, we utilize a word-level attention network [29] to enhance the signiﬁcant features and obtain informative representations of news titles, as described in the following equations. where the symbol σ is tanh, q, V and v are parameters learnt by the training process, c of news title. is almost same with that for news title. Therefore, abstract learning component is similar with the former one, i.e., title learning component, whose output is the representation of news abstract, marked with r category and subcategory labels, which also represent key features of the news. In order to learn news category and sub-category information, we devise category learning component, which converts two kinds of categories into low-dimensional dense representations and generates corresponding representations, described as follows: where f is ReLU, e representations of news category and sub-category, respectively. apply an attention network to learn the weights of various information in news and then build the ﬁnal news representation. The method for evaluating the attention weight of title, i.e., α where σ is tanh, q similar equations as Equ. (8). The attention weights of abstract, category and The importance of each word in a piece of text is diﬀerent. It is crucial to Abstract learning component. The learning procedure for news abstract Category learning component. Some news is annotated manually with , v, Vand vdenote the parameters in dense layers, rand rare the Attention component. Following the previous work of Wu et al. [13], we of news abstract, category and sub-category, they are obtained with the subcategory are evaluated with the similar procedure as the weight of title, i.e., Equ. (9), which are denoted as α tle embedding r and news sub-category embedding r weights, described with the following equation: As shown in Fig. 2 (c), inspired by the previous work of He et al. [16], we devise the aspect-level feature extractor, which consists of attention-based generation of news embedding and aspect-based reconstruction of news embedding. The process of aspect-level feature extractor is similar to autoencoders, where we employ attention mechanism to extract key aspect words in each news and generate the news embedding with the weighted word embeddings, then reconstruct each news by the combinations of aspect embeddings. the important aspect words in each news, we utilize an attention mechanism to encode the news, described as the following equations: where e abstract and category. y is the average of the word embeddings, which can be viewed as a simple global news representation. H is a matrix for mapping y and e can be viewed as the probability of the i-th word is the right aspect word to describe the main aspect information of the news. z news embedding according to the weighted probability of each possible aspect word. the quality of aspect extraction, inspired by autoencoders, we reconstruct each news through a liner combination of the extracted aspect embeddings, described as follows: Finally, the news feature embedding r is built by concatenating news ti- Attention-based generation of news embedding. In order to capture is the embeddings of the i-th word in news content, including its title, , which is learned in the training process. αis the weight, which Aspect-based reconstruction of news embedding. In order to assure where W as the weight vector over all aspect embeddings. A is the aspect embedding matrix, which is initialized with the k-means centroids of the news embeddings. As shown in Fig. 2 (b), aiming to model the user preferences accurately, we devise user aspect-level encoder, which consists of news feature extractor, aspectlevel feature extractor and user preference extractor. Speciﬁcally, news feature extractor and aspect-level feature extractor learn the general news representation k and aspect-speciﬁc news representation k are same with the corresponding modules of news aspect-level encoder in Section 4.1. The concatenation of k and k browsed news, as described below. further devise user preference extractor. It employs a sentence-level attention mechanism to choose important news and learn more accurate user representations as below: where σ is tanh operation, n user u, q of browsed news of the user u, α browsed news. The aspect-level representation u of the user u is the weighted summation of the browsed news representations. is weighted matrix parameter, b is the bias vector. p can be viewed is the news embedding reconstructed with aspect words. In order to model the user representations from their browsed news, we Once obtaining the aspect-level news representation n and user representation u, click predictor is devised to calculate the probability of a user clicking the candidate news. Following the work of Okura et al. [30], we obtain the click score ˆy by calculating the inner product of these representations between the user u and the news n: Following the work of Wu et al. [12], given a user u, we deﬁne his browsed news as positive samples and randomly select some negative samples according to the negative sample ratio. The click probability of a given positive sample y w.r.t. G sampled negative samples where ˆy the i-th positive sample. The negative log-likelihood of all positive samples is the loss function for news recommendation, described as: where P is the set of positive samples. the model and ensure the diversity of aspect embeddings, motivated by the work of He et al. [16], we devise the loss function to constrain aspect-level information, as follows: where S is the training dataset, r extractor, n where F is regularization term, which is able to ensure the diversity and uniqueness of aspect embedding. A 1. I is the identity matrix. where λ can control the weight of the regularization term. is the click score of the j-th negative sample in the same session with In order to ensure the quality of aspect-level representation extracted by The ﬁnal loss Lossis obtained by merging the above three losses: In this section, we carry out extensive experiments on a public real-world dataset and compare ANRS with the popular baselines. Besides, we conduct some analysis experiments to investigate the inﬂuence of hyperparameters in ANRS. Finally, a case study is implemented. We experimented ANRS on a commonly used real-world dataset MIND released on ACL 2020 [31]. It is collected from anonymous behavior logs of Microsoft News website. MIND has a large number of logs of 1,000,000 users and 161,013 news from 20 categories, which is divided into training, validation and test sets. For each news, MIND provides its title, category, abstract and entity. However, the detailed content of news is missing. Its statistics is shown in Table 2. Due to the limitation of Microsoft’s license, we fail to access the labels of test set, and thus it cannot be used to test our model. To solve the problem, we take the released validation set as our test set, and split 10% samples from training set as the new validation set. We utilize the pretrained Glove embedding [32] to initialize word embedding and set the dimension to 300. For all modules, the ﬁlters of CNNs are set to 400 and window sizes are set to 5. For training, we set the negative sampling ratio to 6 and the batch size to 256. To avoid overﬁtting problem, the dropout is set to 0.2. In order to obtain the best performance, the number of clusters is set to 40 for k-means clustering, which is the number of aspects. For evaluating the performance, we adopt three popular metrics, including AUC, MRR and nDCG [33,12]. We compare ANRS with several representative and/or state-of-the-art baselines, including latent factor models and neural network models: – BiasMF [34], a matrix factorization model for recommendation task. – FM [35], another non-linear model based on matrix factorization for rec- – CNN [36], a classical convolution neural network, which encodes the word – DKN [37], a deep learning based news recommender system, which utilizes – Hi-Fi Ark [38], another deep learning based news recommender system, – TANR [12], a state-of-the-art news recommender system, which generates plement the simpliﬁed ablation version of ANRS, marked as ANRS removes all aspect-level feature extractors from the standard ANRS model. In order to answer the following four questions, we conduct extensive experiments: are the improvements? formance? our model? To demonstrate the eﬀectiveness of ANRS, we compare the recommendation accuracy of our model with those baselines. Table 3 reports the results of AUC, MRR, nDCG@5 and nDCG@10. The ﬁrst two methods, i.e., BiasMF and FM, are traditional latent factor models, which can not capture complex and deep representations eﬀectively. Obviously, they are beaten heavily by the others. CNN only achieves similar performance with the latent factor models, which means that the basic neural model is unable to eﬀectively capture the features in news. DKN, Hi-Fi Ark and TANR utilize deep neural network with ommendation task. sequences of news titles and applies max pooling to capture features. CNN and attention mechanisms to obtain user and news representations, and utilizes knowledge graph to improve the eﬀectiveness of recommendation. which proposes a user representation framework. It aggregates user history into archives to learn user representations. topic-aware news representations with the help of topic category labels. In order to further verify the eﬀectiveness of aspect information, we im- Q1: How does our model perform compared with the baselines and what Q2: How does aspect information of news aﬀect the recommendation per- Q3: How does the pre-deﬁned number of aspects aﬀect the performance of Q4: How does the diﬀerent input data aﬀect the performance of our model? news categories, which achieve signiﬁcant improvement than the latent factor models. Though the three models are superior to the others, they fail to capture ﬁne-grained aspect information in news, which still limits their performance. Our proposed ANRS model not only captures various news information such as title and category, but also learns ﬁne-grained aspect-level information. As shown in Table 3, ANRS achieves the best performance on all metrics. Especially, ANRS achieves a better performance than TANR, which is a stateof-the-art topic-based news recommender system. This demonstrates that the ﬁne-grained aspect information is more powerful than the traditional topic information for news recommendation. To be speciﬁc, in terms of AUC, MRR and nDCG, ANRS demonstrates at least 2.45% improvement over the bestperforming baseline, i.e., TANR. To demonstrate the eﬀectiveness of aspect information, we remove the module aspect-level feature extractor from ANRS to build an ablation variant ANRS and conduct experiments on the same dataset. As illustrated in Table 3, ANRS achieves better performance than ANRS of AUC, ANRS is at least 1% higher than that of ANRS cause the latter ANRS all aspect-level information, which leads that ANRS mance. It is obvious that the aspect-level information plays a great role in improving the performance of news recommendation. To answer question Q3, we set the number of aspects from 10 to 45 and show their performances in Fig. 3. The aspect matrix in aspect-level feature extractor is initialized with k-means centriods of the news embeddings, which determines the number of aspects. When the number of aspects is set to 40, our model can achieve the best performance. The possible reasons are two-fold. On the one hand, when the number is smaller than 40, the model is unable to capture enough aspect features. On the other hand, when the number is larger than 40, the model is easy to capture and induce noise aspect features. Therefore, according to Fig. 3, we adopt 40 as the number of aspects in our model. Result 4: Inﬂuence of Diﬀerent Input Data model and compare the performances. According to the diﬀerence of input data, the standard ANRS are transformed into six variants, that is, ANRS ANRS a mean titles, categories and abstracts of news are fed into the corresponding model. As illustrated in Fig. 4, we can ﬁnd that ANRS ANRS model, achieves the best performance. The variant is fed with all data, which is intuitive to achieve the best performance. In addtion, ANRS show the better performance than ANRS cause the title is more eﬃcient data than the abstract in news recommendation. To compare ANRS help model to achieve a little improvement. In order to answer question Q4, we feed diﬀerent data in news into our In this section, we devise some experiments to explore the inﬂuence of two important hyperparameter in our model. The one is the kernel size of CNN in the news aspect-level encoder. Another is the negative sample ratio G in the model training procedure. Kernel Size of CNN so the kernel size of CNN will aﬀect the performance of our model. Aiming to ﬁnd the optimal kernel size, we set it to 1, 3, 5, 7, 9, 11 and 13 respectively to verify the performance, as shown in Fig. 5. According to the ﬁgure, it is obvious that the model achieves the best performance when the kernel size is set to 5, which is adopted in our model. This is probably because the small kernel size fails to capture long-distance context, while the large kernel size has a negative eﬀect because of overﬁtting the noisy patterns. During the training procedure, the negative sample ratio G decides the number of negative samples. Fig. 6 illustrates the experimental results on diﬀerent negative sample ratio G. The performance reaches the peak when G is set to 6. When G is smaller than 6, the performance is not ideal. This is probably because that there is no enough negative samples to provide the suﬃcient information leading to unstable and sub-optimal performance. When G is larger than 6, the performance begins to drop down. This is probably because that our model is diﬃcult to identify the positive samples when G is too large, leading to sub-optimal performance. According to the ﬁgure, the best performance is achieved when G is set to 6. We employ CNN to extract news features in the news aspect-level encoder, To intuitively demonstrate the eﬀectiveness of the aspect-level information, a case study is performed. We randomly select the one clicked news including ID, category, subcategory and abstract in the test set. As shown in Fig. 7, we observe that our model can capture the important related words to learn the aspect information. For example, because there are some representative words such as restaurant, America in the clicked news, although the category of this news is food and drink, our model can learn diﬀerent aspects, i.e., Restaurant, America. Though there is not a location label in the category information of the clicked news, however, as its abstract mentions the location, i.e., America, our model can capture and learn the location aspect automatically. In addition, our model can recommend some diversiﬁed news to users and satisfy their potential interests. For example, the clicked news belongs to the category food and drink and the subcategory restaurants and news. The user who clicks this news may be interested in food and keto-friendly restaurant. With our model, the user may be recommended with Candidate News 1, which is related with the clicked news by the common aspect America. The user may enjoy a satisﬁed reading experience by the recommended diversiﬁed news. In this paper, we propose a novel aspect-driven news recommender system (ANRS), which is built on aspect-level user preference and news representation learning. ANRS consists of three main modules, i.e., news aspect-level encoder, user aspect-level encoder and click predictor. We utilize CNN and attention network to extract user’s preferences and news features. Meanwhile, we extract aspect information to enhance the representation of users and news. Empirical evaluations on the real-world dataset demonstrate the superiority of our model. In future work, we will explore the more eﬀective neural architecture to accurately capture aspect information. Besides, the content of news may be more useful for modeling aspect features, which will be further explored.