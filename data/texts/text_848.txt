Multimodal retrieval is a major but understudied problem in e-commerce [33]. Even though e-commerce products are associated with rich multi-modal information, research currently focuses mainly on textual and behavioral signals to support product search and recommendation. The majority of prior work in multimodal retrieval for e-commerce focuses on applications in the fashion domain, such as recommendation of fashion items [21] and cross-modal fashion retrieval [6, 14]. In the more general e-commerce domain, multimodal retrieval has not been explored that well yet [10, 18]. The multimodal problem on which we focus is motivated by the importance of category information in e-commerce. Product category trees are a key component of modern e-commerce as they assist customers when navigating across large and dynamic product catalogues [13, 30, 36]. Yet, the ability to retrieve an image for a given product category remains a challenging task mainly due to noisy category and product Nanne van Noord, Ernst Kuiper, and Maarten de Rijke m.hendriksen@uva.nl m.j.r.bleeker@uva.nl s.vakulenko@uva.nl n.j.e.vannoord@uva.nl ekuiper@bol.com m.derijke@uva.nl Abstract. E-commerce provides rich multimodal data that is barely leveraged in practice. One aspect of this data is a category tree that is being used in search and recommendation. However, in practice, during a user’s session there is often a mismatch between a textual and a visual representation of a given category. Motivated by the problem, we introduce the task of category-to-image retrieval in e-commerce and propose a model for the task, CLIP-ITA. The model leverages information from multiple modalities (textual, visual, and attribute modality) to create product representations. We explore how adding information from multiple modalities (textual, visual, and attribute modality) impacts the model’s performance. In particular, we observe that CLIP-ITA significantly outperforms a comparable model that leverages only the visual modality and a comparable model that leverages the visual and attribute modality. Keywords: Multimodal retrieval · Category-to-image retrieval · E-commerce data, and the size and dynamic character of product catalogues [17, 33]. The category-to-image retrieval task. We introduce the problem of retrieving a ranked list of relevant images of products that belong to a given category, which we call the category-to-image (CtI) retrieval task. Unlike image classiﬁcation tasks that operate on a predeﬁned set of classes, in the CtI retrieval task we want to be able not only to understand which images belong to a given category but also to generalize towards unseen categories. Consider the category “Home decor.” A CtI retrieval should output a ranked list of k images retrieved from the collection of images that are relevant to the category, which could be anything from images of carpets to an image of a clock or an arrangement of decorative vases. Use cases that motivate the CtI retrieval task include (1) the need to showcase diﬀerent categories in search and recommendation results [13, 30, 33]; (2) the task can be used to infer product categories in the cases when product categorical data is unavailable, noisy, or incomplete [39]; and (3) the design of cross-categorical promotions and product category landing pages [24]. The CtI retrieval task has several key characteristics: (1) we operate with categories from non-ﬁxed e-commerce category trees, which range from very general (such as “Automative” or “Home & Kitchen”) to very speciﬁc ones (such as “Helmet Liners” or “Dehumidiﬁers”). The category tree is not ﬁxed, therefore, we should be able to generalize towards unseen categories; and (2) product information is highly multimodal in nature; apart from category data, products may come with textual, visual, and attribute information. A model for CtI retrieval. To address the CtI retrieval task, we propose a model that leverages image, text, and attribute information, CLIP-ITA. CLIPITA extends upon Contrastive Language-Image Pre-Training (CLIP) [26]. CLIPITA extends CLIP with the ability to represent attribute information. Hence, CLIP-ITA is able to use textual, visual, and attribute information for product representation. We compare the performance of CLIP-ITA with several baselines such as unimodal BM25, bimodal zero-shot CLIP, and MPNet [29]. For our experiments, we use the XMarket dataset that contains textual, visual, and attribute information of e-commerce products [2]. Research questions and contributions. We address the following research questions: (RQ1) How do baseline models perform on the CtI retrieval task? Speciﬁcally, how do unimodal and bi-modal baseline models perform? How does the performance diﬀer w.r.t. category granularity? (RQ2) How does a model, named CLIP-I, that uses product image information for building product representations impact the performance on the CtI retrieval task? (RQ3) How does CLIP-IA, which extends CLIP-I with product attribute information, perform on the CtI retrieval task? (RQ4) And ﬁnally, how does CLIP-ITA, which extends CLIP-IA with product text information, perform on the CtI task? Our main contributions are: (1) We introduce the novel task of CtI retrieval and motivate it in terms of e-commerce applications. (2) We propose CLIP-ITA, the ﬁrst model speciﬁcally designed for this task. CLIP-ITA leverages multimodal product data such as textual, visual, and attribute data. On average, CLIP-ITA outperforms CLIP-I on all categories by 217% and CLIP-IA by 269%. We share our code and experimental settings to facilitate reproducibility of our results. Learning multimodal embeddings. Contrastive pre-training has been shown to be highly eﬀective in learning joined embeddings across modalities [26]. By predicting the correct pairing of image-text tuples in a batch, the CLIP model can learn strong text and image encoders that project to joint space. This approach to learning multimodal embeddings oﬀers key advantages over approaches that use manually assigned labels as supervision: (1) the training data can be collected without manual annotation; real-world data in which image-text pairs occur can be used; (2) models trained in this manner learn more general representations that allow for zero-shot prediction. These advantages are appealing for e-commerce, as most public multimodal e-commerce datasets primarily focus on fashion only [2]; being able to train from real-world data avoids the need for costly data annotation. We build on CLIP by extending it to category-product pairs, taking advantage of its ability to perform zero-shot retrieval for a variety semantic concepts. Multimodal image retrieval. Early work in image retrieval grouped images into a restricted set of semantic categories and allowed users to retrieve images by using category labels as queries [28]. Later work allowed for a wider variety of queries ranging from natural language [11, 34], to attributes [23], to combinations of multiple modalities (e.g., title, description, and tags) [32]. Across these multimodal image retrieval approaches we ﬁnd three common components: (1) an image encoder, (2) a query encoder, and (3) a similarity function to match the query to images [7, 26]. Depending on the focus of the work some components might be pre-trained, whereas the others are optimized for a speciﬁc task. In our work, we rely on pre-trained image and text encoders but learn a new multimodal composite of the query to perform CtI retrieval. Multimodal retrieval in e-commerce. Prior work on multimodal retrieval in e-commerce has been mainly focused on cross-modal retrieval for fashion [6, 16, 42]. Other related examples include outﬁt recommendation [15, 19, 21] Some prior work on interpretability for fashion product retrieval proposes to leverage multimodal signals to improve explainability of latent features [20, 38]. Tautkute et al. [31] propose a multimodal search engine for fashion items and furniture. When it comes to combining signals for improving product retrieval, Yim et al. [40] propose to combine product images, titles, categories, and descriptions to improve product search, Yamaura et al. [37] propose an algorithm that leverages multimodal product information for predicting a resale price of a second-hand product. Unlike prior work on multimodal retrieval in e-commerce that mainly focuses on fashion data, we focus on creating multimodal product representations for the general e-commerce domain. https://github.com/mariyahendriksen/ecir2022_category_to_image_ retrieval Fig. 1: Overview of CLIP-ITA. The category encoding pipeline is in purple; the category information pipeline in green; f Task deﬁnition. We follow the same notation as in [41]. The input dataset can be presented as category-product pairs (x uct category, and x category x represented as a category name. The product information comprises titles x images x For the CtI retrieval task, we use the target category name x we aim to refturn a ranked list of top-k images that belong to the category x CLIP-ITA. Fig. 1 provides a high-level view of CLIP-ITA. CLIP-ITA projects category x where the resulting vectors are respectively c and p. The category and product information is processed by a category encoding pipeline and product information encoding pipeline. The core components of CLIP-ITA are the encoding and projection modules. The model consists out of four encoders: a category encoder, an image encoder, a title encoder, and an attribute encoder. Besides, CLIP-ITA comprises two non-linear projection heads: the category projection head and the multimodal projection head. While several components of CLIP-ITA are based on CLIP [26], CLIP-ITA diﬀers from CLIP in three important ways: (1) unlike CLIP, which operates on two encoders (textual and visual), CLIP-ITA extends CLIP towards a category encoder, image encoder, textual encoder, and attribute encoder; (2) CLIP-ITA features two projection heads, one for the category encoding pipeline, and one for the product information encoding pipeline; and (3) while CLIP is trained on text-image pairs, CLIP-ITA is trained on category-product pairs, where product representation is multimodal. Category encoding pipeline. The category encoder (f category name x the category name x To obtain this representation, we use pre-trained MPNet model [29]. After passing category information through the category encoder, we feed it to the category projection head. The category projection head (g sentation h . The product category xis taken from the category tree T and is , and attributes x, i.e., x= {x, x, x}. and product information xinto a d-dimensional multimodal space and projects it into d-dimensional multi-modal space: where c ∈ R Product encoding pipeline. The product information encoding pipeline represents three encoders, one for every modality, and a product projection head. The image encoder (f category x image x To obtain the image representation h from CLIP model. The title encoder (f returns a title representation h Similarly to the category encoder f title representation h a set of attributes x Similarly to the category encoder f tation of each attribute with the pre-trained MPNet model. After obtaining title, image and attribute representations, we pass the representations into the product projection head. The product projection head (g catenation of the image representation h representation h multimodal space: where p ∈ R Loss function. We train CLIP-ITA using bidirectional contrastive loss [41]. The loss is a weighted combination of two losses: a category-to-product contrastive loss and a product-to-category contrastive loss. In both cases the loss is the InfoNCE loss [25]. Unlike prior work that focuses on a contrastive loss between inputs of the same modality [3, 8] and on corresponding inputs of two modalities [41], we use the loss to work with inputs from textual modality (category representation) vs. a combination of multiple modalities (product representation). We train CLIP-ITA on batches of category-product pairs (x batch size β. For the j-th pair in the batch, the category-to-product contrastive loss is computed as follows: . Similarly to the category processing pipeline, we pass the product through the image encoder: where f eter. Similarly, the product-to-category loss is computed as follows: The resulting contrastive loss is a combination of the two above-mentioned losses: where β represents the batch size and λ ∈ [0, 1] is a scalar weight. Dataset. We use the XMarket dataset recently introduced by Bonab et al. [2] that contains textual, visual, and attribute information of e-commerce products as well as a category tree. For our experiments, we select 38,921 products from the US market. Category information is represented as a category tree and comprises 5,471 unique categories across nine levels. Level one is the most general category level, level nine is the most speciﬁc level. Every product belongs to a subtree of categories t ∈ T . In every subtree t, each parent category has only one associated child category. The average subtree depth is 4.63 (minimum: 2, maximum: 9). Because every product belongs to a subtree of categories, the dataset contains 180,094 product-category pairs in total. We use product titles as textual information and one image per product as visual information. The attribute information comprises 228,368 attributes, with 157,049 unique. On average, every product has 5.87 attributes (minimum: 1, maximum: 24). Evaluation method. To investigate how model performance changes w.r.t. category granularity, for every product in the dataset, x subtree of categories to which the product belongs, t, we train and evaluate the model performance in three settings: (1) all categories, where we randomly select one category from the subtree t; (2) most general category, where we use only the most general category of the subtree t, i.e., the root; and (3) most speciﬁc category, where we use the most speciﬁc category of the subtree t. In total, there are 5,471 categories in all categories setup, 34 categories in the most general category, and 4,100 in the most speciﬁc category setup. We evaluate every model on category-product pairs (x and a candidate product data by passing them through category encoding and product information encoding pipelines. For every category x top-k candidates ranked by cosine similarity w.r.t. the target category x Metrics. To evaluate model performance, we use Precision@K where K = {1, 5, 10}, mAP@K where K = {5, 10}, and R-precision. Baselines. Following [4, 27, 35] we use BM25, MPNet, CLIP as our baselines. Four experiments. We run four experiments, corresponding to our research (c, p) is the cosine similarity, and τ ∈ Ris a temperature paramquestions as listed at the end of Section 1. In Experiment 1 we evaluate the baselines on the CtI retrieval task (RQ1). We feed BM25 corpora that contain textual product information, i.e., product titles. We use MPNet in a zero-shot manner. For all the products in the dataset, we pass the product title x the model. During the evaluation, we pass a category x query through MPNet and retrieve top-k candidates ranked by cosine similarity w.r.t. the target category x candidates with the target category x zero-shot manner with a Text Transformer and a Vision Transformer (ViT) [5] an conﬁguration. We pass the product images x evaluation, we pass a category x image candidates ranked by cosine similarity w.r.t. the target category x compare categories of the top-k retrieved images with the target category x In Experiment 2 we evaluate image-based product representations (RQ2). After obtaining results with CLIP in a zero-shot setting, we build product representations by training on e-commerce data. First, we investigate how using product image data for building product representations impacts performance on the CtI retrieval task. To introduce visual information, we extend CLIP in two ways: (1) We use ViT from CLIP as image encoder f head g the text encoder from MPNet as category encoder f tion head g pipeline (see Fig. 1). We name the resulting model CLIP-I. We train CLIP-I on category-product pairs (x we only use visual information for building product representations. In Experiment 3, we evaluate image- and attribute-based product representations (RQ3). We extend CLIP-I by introducing attribute information to the product information encoding pipeline. We add an attribute encoder f which we obtain a representation of product attributes, h resulting attribute representation with image representation h and pass the resulting vector to the product projection head g sulting product representation p is based on both visual and attribute product information. We name the resulting model CLIP-IA. We train CLIP-IA on category-product pairs (x tribute information for building product representation. In Experiment 4, we evaluate image- attribute-, and title-based product representations (RQ4). We investigate how extending the product information processing pipeline with the textual modality impacts performance on the CtI retrieval task. We add title encoder f pipeline and use it to obtain title representation h sulting representation with product image and attribute representations h concat(h g. The resulting model is CLIP-ITA. We train and test CLIP-ITA on categoryproduct pairs (x textual information for building product representations. that takes as an input product visual information x∈ x. (2) We use on top of category encoder fthereby completing category encoding , h, h). We pass the resulting vector to the product projection head Table 1: Results of Experiments 1–4. The best performance is highligthed in bold. Implementation details. We train every model for 30 epochs, with a batch size β = 8 for most general categories, β = 128 — for most speciﬁc categories and all categories. For loss function, we set τ = 1, λ = 0.5. We implement every projection head as non-linear MLPs with two hidden layers, GELU nonlinearities [9] and layer normalization [1]. We optimize both heads with the AdamW optimizer [22]. Experiment 1: Baselines. Following RQ1, we start by investigating how do baselines perform on CtI retrieval task. Besides, we investigate how does the performance on the task diﬀers between the unimodal and the bimodal approach. The results are shown in Table 1. When evaluating on all categories, all the baselines perform poorly. For the most general category setting, MPNet outperforms CLIP on all metrics except R-precision. The most prominent gain is for Precision@10 where MPNet outperforms CLIP by 28%. CLIP outperforms BM25 on all metrics. For the most speciﬁc category setting, MPNet performance is the highest, BM25 — the lowest. In particular, MPNet outperforms CLIP by 211% in Precision@10. Overall, MPNet outperforms CLIP and both models signiﬁcantly outperforms BM25 for both most general and most speciﬁc categories. However, when evaluation is done on all categories, the performance of all models is comparable. As an answer to RQ1, the results suggest that using information from multiple modalities is beneﬁcial for performance on the task. Experiment 2: Image-based product representations. To address RQ2, we compare the performance of CLIP-I with CLIP and MPNet, the best-performing baseline. Table 1, shows the experimental results for Experiment 2. The biggest performance gains are obtained in “all categories” setting. However, there, the performance of the baselines was very poor. For the most general categories, CLIP-I outperforms both CLIP and MPNet. For CLIP-I vs. CLIP, we observe the biggest increase of 51% for Precision@1, for CLIP-I vs. MPNet — 39% in Rprecision. In the case of the most speciﬁc categories, CLIP-I outperforms CLIP but loses to MPNet. Overall, CLIP-I outperforms CLIP in all three settings and outperforms MPNet except the most speciﬁc categories. Therefore, we answer RQ2 as follows: the results suggest that extension of CLIP by the introduction of product image data for building product representations has a positive impact on performance on CtI retrieval task. Experment 3: Image- and attribute-based product representations. To answer RQ3, we compare the performance of CLIP-IA with CLIP-I and the baselines. The results are shown in Table 1. When evaluated on all categories, CLIP-IA performs worse than CLIP-I but outperforms MPNet. In particular, CLIP-I obtains the biggest gain relative of 32% on Precision@1 and the lowest gain of 12% on R-precision. For the most general category, CLIP-IA outperforms CLIP-I and MPNet on all metrics. More speciﬁcally, we observe the biggest gain of 122% on R-precision over MPNet and the biggest gain of 59% on R-precision for CLIP-I. Similarly, for the most speciﬁc category, CLIP-IA outperforms both CLIP-I and MPNet. We observe the biggest relative gain of 138% over CLIP-I. The results suggest that further extension of CLIP by the introduction of the product image and attribute data for building product representations has a positive impact on performance on CtI retrieval task, especially when evaluated on most speciﬁc categories. Therefore, we answer RQ4 positively. Experiment 4: Image-, attribute-, and title-based product representations. We compare CLIP-ITA with both CLIP-IA, CLIP-I, and the baselines. The results are shown in Table 1. In general, CLIP-ITA outperforms CLIP-I and CLIP-IA and the baselines in all settings. When evaluated on all categories, the maximum relative increase of CLIP-ITA over CLIP-I is 265% in R-precision, the minimum relative increase is 183% in mAP@10. The biggest relative increase of CLIP-ITA performance over CLIP-IA is 310% in Precision@1, the smallest relative increase is 229% in mAP@10. For the most general categories, CLIPITA outperforms CLIP-I by 82% and CLIP-IA by 38%. For most speciﬁc categories, we observe the biggest increase of CLIP-ITA over CLIP-I of 254% in R-precision and the smallest relative increase of 172% on mAP@5. At the same time, the biggest relative increase of CLIP-ITA over CLIP-IA is a 38% increase in R-precision and the smallest relative increase is a 27% increase in mAP@5. Overall, CLIP-ITA wins in all three settings. Hence, we answer RQ4 positively. Table 2: Erroneous CLIP-ITA prediction counts for “same tree” vs. “ diﬀerent tree” predictions per evaluation type. Distance between predicted and target categories. We examine the performance of CLIP-ITA by looking at the pairs of the ground-truth and predicted categories (c, c i.e., c 6= c w.r.t. the category tree hierarchy. First, we examine in how many cases target category c and predicted category c i.e., belong to the same category tree; see Table 2. In the case of most general categories, the majority of incorrectly predicted categories belong to a tree diﬀerent from the target category tree. For the most speciﬁc categories, about 11% of predicted categories belong to the category tree of the target category. However, when evaluation is done on all categories, 72% of incorrectly predicted cases belong to the same tree as a target category. Next, we turn to the category-predicted category pairs (c, c correctly predicted category c compute the distance d between a category used as a query c and a predicted category c dicted category c depth(c category is bigger than the depth of the target category, depth(c i.e., the predicted category is more speciﬁc than the target category. The setup is mirrored for negative distances. See Fig. 2. We do not plot the results for the most general category because for this setting there are only two cases when target category c and a predicted category c cases, predicted category c tance d(c, p speciﬁc categories, the wrongly predicted category c was always more speciﬁc than the target category c with the maximum absolute distance between c and c category was one level above the target category, for 21% d(c, c d(c, c 92% of the cases, the predicted category c category c; for 8% the predicted category was more general. Overall, for the most general category and the most speciﬁc category, the majority of incorrectly predicted categories are located in a category tree diﬀerent from the one where the target category was located. For the “all categories” . This allows us to quantify how far oﬀ the incorrect predictions lie . We compute the distance between target category c and a top-1 pre- ) − depth(c). The distance d is positive if the depth of the predicted ) = 2. In cases when target category c was sampled from the most ) = −3, and for 5% d(c, c) = −4. For the setting with all categories, in Fig. 2: Error analysis for CLIP-ITA. Distance between target category c and a predicted category c setting, it is the other way around. When it comes to the cases when incorrectly predicted categories are in the same tree as a target category, the majority of incorrect predictions are 1 level more general when the target category is sampled from the most speciﬁc categories. For the “all categories” setting, the majority of incorrect predictions belonging to the same tree as the target category were more speciﬁc than the target category. Our analysis suggests that eﬀorts to improve the performance of CLIP-ITA should focus on minimizing the (tree-based) distance between the target and predicted category in a category tree. This could be incorporated as a suitable extension of the loss function. Performance on seen vs. unseen categories. Next, we investigate how well CLIP-ITA generalizes to unseen categories. We split the evaluation results into two groups based on whether the category used as a query was seen during training or not; see Table 3. For the most general categories, CLIP-ITA is unable to correctly retrieve an image of the product of the category that was not seen during training at all. For the most speciﬁc categories, CLIP-ITA performs better on seen categories than on unseen categories. We observe the biggest relative performance increase of 85% in mAP@10 and the smallest relative increase of 57% in R-precision. When evaluating on all categories, CLIP-ITA performs on unseen categories better when evaluated on Precision@k (27% higher in Precision@1, 33% higher in Precision@5, 10% increase in Precision@10) and R-precision (relative increase of 32%). Performance on seen categories is better in terms of mAP@k (10% increase for both mAP@5 and mAP@10). Overall, for the most general and most speciﬁc categories, the model performs much better on categories seen during training. For “all categories” setting, however, CLIP-ITA’s performance on unseen categories is better. We introduced the task of category-to-image retrieval and motivated its importance in the e-commerce scenario. In the CtI retrieval task, we aim to retrieve an image of a product that belongs to the target category. We proposed a model speciﬁcally designed for this task, CLIP-ITA. CLIP-ITA extends CLIP, one of the best performing text-image retrieval models. CLIP-ITA leverages multimodal product data such as textual, visual, and attribute data to build product representations. In our experiments, we contrasted and evaluated diﬀerent combinations of signals from modalities, using three settings: on all categories, the most general, and the most speciﬁc categories. We found that combining information from multiple modalities to build product representation produces the best results on the CtI retrieval task. CLIP-ITA gives the best performance both on all categories and on the most speciﬁc categories. On the most general categories, CLIP-I, a model where product representation is based on image only, works slightly better. CLIP-I performs worse on the most speciﬁc categories and across all categories. For identiﬁcation of the most general categories, visual information is more relevant. Besides, CLIPITA is able to generalize to unseen categories except in the case of most general categories. However, the performance on unseen categories is lower than the performance on seen categories. Even though our work is focused on the e-commerce domain, the ﬁndings can be useful for other areas, e.g., digital humanities. Limitations of our work are due to type of data in the e-commerce domain. In e-commerce, there is typically one object per image and the background is homogeneous, textual information is lengthly and noisy; in the general domain, there is typically more than one object per image, image captions are more informative and shorter. Future work directions can focus on improving the model architecture. It would be interesting to incorporate attention mechanisms into the attribute encoder and explore how it inﬂuences performance. Another interesting direction for future work is to evaluate CLIP-ITA on other datasets outside of the e-commerce domain. Future work can also focus on minimizing the distance between the target and predicted category in the category tree. Acknowledgements. This research was supported by Ahold Delhaize, the Nationale Politie, and the Hybrid Intelligence Center, a 10-year program funded by the Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientiﬁc Research, https://hybrid-intelligence-centre. nl. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors. Table 3: CLIP-ITA performance on seen vs. unseen categories. [1] Ba JL, Kiros JR, Hinton GE (2016) Layer normalization. arXiv preprint [2] Bonab H, Aliannejadi M, Vardasbi A, Kanoulas E, Allan J (2021) XMarket: [3] Chen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework for [4] Dai Z, Lai G, Yang Y, Le QV (2020) Funnel-transformer: Filtering out [5] Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner [6] Goei K, Hendriksen M, de Rijke M (2021) Tackling attribute ﬁne- [7] Gupta T, Vahdat A, Chechik G, Yang X, Kautz J, Hoiem D (2020) Con- [8] He K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum contrast for unsu- [9] Hendrycks D, Gimpel K (2016) Gaussian error linear units (GELUs). arXiv [10] Hewawalpita S, Perera I (2019) Multimodal user interaction framework for [11] Hu R, Xu H, Rohrbach M, Feng J, Saenko K, Darrell T (2016) Natural [12] Jones KS, Walker S, Robertson SE (2000) A probabilistic model of infor- [13] Kondylidis N, Zou J, Kanoulas E (2021) Category aware explainable con[14] Laenen K, Moens MF (2019) Multimodal neural machine translation of fasharXiv:160706450 Cross-market training for product recommendation. In: CIKM, ACM contrastive learning of visual representations. In: International conference on machine learning, PMLR, pp 1597–1607 sequential redundancy for eﬃcient language processing. arXiv preprint arXiv:200603236 T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations grainedness in cross-modal fashion search with multi-level features. In: SI- GIR 2021 Workshop on eCommerce, ACM trastive learning for weakly supervised phrase grounding. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part III 16, Springer, pp 752–768 pervised visual representation learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 9729–9738 preprint arXiv:160608415 e-commerce. In: 2019 International Research Conference on Smart Computing and Systems Engineering (SCSE), IEEE, pp 9–16 language object retrieval. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp 4555–4564 mation retrieval: development and comparative experiments: Part 2. Information processing & management 36(6):809–840 versational recommendation. arXiv preprint arXiv:210308733 ion e-commerce descriptions. In: International Conference on Fashion communication: between tradition and future digital developments, Springer, pp 46–57 [15] Laenen K, Moens MF (2020) A comparative study of outﬁt recommendation [16] Laenen K, Zoghbi S, Moens MF (2017) Cross-modal search for fashion at- [17] Laenen K, Zoghbi S, Moens MF (2018) Web search of fashion items with [18] Li H, Yuan P, Xu S, Wu Y, He X, Zhou B (2020) Aspect-aware multi- [19] Li X, Wang X, He X, Chen L, Xiao J, Chua TS (2020) Hierarchical fashion [20] Liao L, He X, Zhao B, Ngo CW, Chua TS (2018) Interpretable multimodal [21] Lin Y, Ren P, Chen Z, Ren Z, Ma J, de Rijke M (2019) Improving outﬁt [22] Loshchilov I, Hutter F (2017) Decoupled weight decay regularization. arXiv [23] Nagarajan T, Grauman K (2018) Attributes as operators: factorizing unseen [24] Nielsen J, Molich R, Snyder C, Farrell S (2000) E-commerce user experience. [25] Oord Avd, Li Y, Vinyals O (2018) Representation learning with contrastive [26] Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, Sastry [27] Shen S, Li LH, Tan H, Bansal M, Rohrbach A, Chang KW, Yao Z, Keutzer [28] Smeulders A, Worring M, Santini S, Gupta A, Jain R (2000) Content-based [29] Song K, Tan X, Qin T, Lu J, Liu TY (2020) MPNet: Masked and permuted [30] Tagliabue J, Yu B, Beaulieu M (2020) How to grow a (product) tree: permethods with a focus on attention-based fusion. Information Processing & Management 57(6):102316 tributes. In: Proceedings of the KDD 2017 Workshop on Machine Learning Meets Fashion, ACM, vol 2017, pp 1–10 multimodal querying. In: Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp 342–350 modal summarization for chinese e-commerce products. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol 34, pp 8188–8195 graph network for personalized outﬁt recommendation. In: Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 159–168 retrieval for fashion products. In: Proceedings of the 26th ACM international conference on Multimedia, pp 1571–1579 recommendation with co-supervision of fashion generation. In: The World Wide Web Conference, pp 1095–1105 preprint arXiv:171105101 attribute-object compositions. In: Proceedings of the European Conference on Computer Vision (ECCV), pp 169–185 Nielsen Norman Group predictive coding. arXiv preprint arXiv:180703748 G, Askell A, Mishkin P, Clark J, et al. (2021) Learning transferable visual models from natural language supervision. arXiv preprint arXiv:210300020 K (2021) How much can CLIP beneﬁt vision-and-language tasks? arXiv preprint arXiv:210706383 image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and Machine Intelligence 22(12):1349–1380 pre-training for language understanding. arXiv preprint arXiv:200409297 sonalized category suggestions for ecommerce type-ahead. arXiv preprint arXiv:200512781 [31] Tautkute I, Trzci´nski T, Skorupa AP, Brocki L, Marasek K (2019) Deep- [32] Thomee B, Shamma DA, Friedland G, Elizalde B, Ni K, Poland D, Borth [33] Tsagkias M, King TH, Kallumadi S, Murdock V, de Rijke M (2020) Chal- [34] Vo N, Jiang L, Sun C, Murphy K, Li LJ, Fei-Fei L, Hays J (2019) Composing [35] Wang S, Zhuang S, Zuccon G (2021) Bert-based dense retrievers require [36] Wirojwatanakul P, Wangperawong A (2019) Multi-label product catego[37] Yamaura Y, Kanemaki N, Tsuboshita Y (2019) The resale price prediction [38] Yang X, He X, Wang X, Ma Y, Feng F, Wang M, Chua TS (2019) In- [39] Yashima T, Okazaki N, Inui K, Yamaguchi K, Okatani T (2016) Learning [40] Yim J, Kim JJ, Shin D (2018) One-shot item search with multimodal data. [41] Zhang Y, Jiang H, Miura Y, Manning CD, Langlotz CP (2020) Contrastive [42] Zoghbi S, Heyman G, Gomez JC, Moens MF (2016) Cross-modal fashion style: Multimodal search engine for fashion and interior design. IEEE Access 7:84613–84628 D, Li LJ (2016) YFCC100M: The new data in multimedia research. Communications of the ACM 59(2):64–73 lenges and research opportunities in ecommerce search and recommendations. SIGIR Forum 54(1) text and image for image retrieval-an empirical odyssey. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 6439–6448 interpolation with bm25 for eﬀective passage retrieval. In: Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval, pp 317–324 rization using multi-modal fusion models. arXiv preprint arXiv:190700420 of secondhand jewelry items using a multi-modal deep model with iterative co-attention. arXiv preprint arXiv:190700661 terpretable fashion matching with rich attributes. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp 775–784 to describe e-commerce images from noisy online data. In: Asian Conference on Computer Vision, Springer, pp 85–100 arXiv preprint arXiv:181110969 learning of medical visual representations from paired images and text. arXiv preprint arXiv:201000747 search. In: International Conference on Multimedia Modeling, Springer, pp 367–373