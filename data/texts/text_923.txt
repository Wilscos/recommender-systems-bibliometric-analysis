Traditional recommender systems aim to estimate a user’s rating to an item based on observed ratings from the population. As with all observational studies, hidden confounders, which are factors that aﬀect both item exposures and user ratings, lead to a systematic bias in the estimation. Consequently, a new trend in recommender system research is to negate the inﬂuence of confounders from a causal perspective. Observing that confounders in recommendations are usually shared among items and are therefore multi-cause confounders, we model the recommendation as a multi-cause multi-outcome (MCMO) inference problem. Speciﬁcally, to remedy confounding bias, we estimate user-speciﬁc latent variables that render the item exposures independent Bernoulli trials. The generative distribution is parameterized by a DNN with factorized logistic likelihood and the intractable posteriors are estimated by variational inference. Controlling these factors as substitute confounders, under mild assumptions, can eliminate the bias incurred by multi-cause confounders. Furthermore, we show that MCMO modeling may lead to high variance due to scarce observations associated with the high-dimensional causal space. Fortunately, we theoretically demonstrate that introducing user features as pre-treatment variables can substantially improve sample eﬃciency and alleviate overﬁtting. Empirical studies on simulated and real-world datasets show that the proposed deep causal recommender shows more robustness to unobserved confounders than state-of-the-art causal recommenders. Codes and datasets are released at https://github.com/yaochenzhu/deep-deconf. Estimating users’ preference based on their past behaviors is important in recommendations. Collaborative ﬁltering, which aims at estimating one user’s rating to items based on observed ratings from the population, has been widely applied in modern recommender systems. However, since a user’s rating to an item is generally not independent of the item’s exposure to the user, the collected rating data are unavoidably biased. Consider movie recommendation as an example. Since the genre of a movie affects both the likelihood of its exposure and user’s rating to it, a spurious dependence is created between them, which makes movies in the minority genres systematically under-represented by the collected data (Fig. (1)). This is a confounding phenomenon, and the movie genre is one of the confounders. Ignoring such confounders could lead to systematic confounding bias that degenerates the recommendation quality for traditional recommender systems [1]. If we could observe a user’s rating to an item with and without the item’s exposure to the user, confounding bias can be eliminated even in the presence of unobserved confounders [4]. However, since whether or not an item is exposed to a user is pre-determined after the collection of the historical data, the user’s rating associated with one exposure status of the item must be unobserved. Therefore, eliminating the confounding bias demands us to answer a counterfactual problem, i.e., what a user’s rating would be if a previously unexposed item is made exposed (recommended) to her. This falls under the scope of causal inference, which aims to unbiasedly estimate unobserved exposure eﬀects for a unit from observed outcomes from the population. In this article, the Rubin causal model [4] is adopted as the causal inference framework, where the exposure of an item is likened to treatment in a clinical trial and the rating Corresponding author: Zhenzhong Chen, E-mail: zzchen@ieee.org Figure 1:An intuitive example of confounding bias in movie recommendation. Although users like to watch horror movies, they are seldomly exposed to the users, which leads to their under-representation in the data. Consequently, traditional recommender systems that ignores confounders may be unwilling to recommend horror movies. is likened to a potential outcome associated with an exposure. Then, recommendation can be framed as estimating the unitlevel "treatment eﬀects" for a user from observed ratings from all users (i.e., the population) [5]. To address such a counterfactual inference problem, classical causal inference demands us to ﬁnd, measure, and adjust the inﬂuence of all the confounders. However, since any attribute that is shared among items (such as the genre of movies) could serve as a potential confounder, they are infeasible to enumerate. Alas, whether or not we have indeed exhausted all the confounders is not testable from experiments [4]. Observing that the item pools of modern recommender systems are large, we assume that single-cause confounder, which is an attribute that exclusively aﬀects the exposure and rating of one item, is negligible. Therefore, the problem can be simpliﬁed because multi-cause confounders, i.e., confounders shared among items, can be properly handled by controlling and estimating substitute confounders inferred from exposures via latent factor models [6]. This has been explored by Wang et al. [7] by proposing the Deconfounded Recommender (Deconf-MF). However, they mainly focused on using shallow methods such as Poisson matrix factorization to infer the substitute confounders, where a closed-form approximate inference solution can be deduced. This is limited, since they may fail to capture the complex item co-exposure relationship caused by confounders. Even if the exposure model is correctly speciﬁed, the potential rating prediction model of Deconf-MF degenerates into a single-cause case where the item co-recommendation eﬀect (the interference eﬀect of recommending several items simultaneously) is ignored. Utilizing deep neural networks (DNNs) to model both the item exposure and user ratings, which is demonstrated to be superior in classical item co-purchase prediction tasks [8], remains under-explored in causal recommender systems due to intractable posteriors. In addition, the multiple causes induce an exponentially large causal space, which makes the observed ratings associated with one speciﬁc exposure vector in the population scarce. This could lead to a large variance in the estimated causal eﬀects of seldomly observed item exposures. The variance, for a DNN-based causal model, appears as its tendency to overﬁt when the number of user is limited. To address the above challenges, we propose a deep deconfounded recommender (Deep-Deconf). Deep-Deconf frames the recommendation as a multi-cause multi-outcome (MCMO) inference problem, where the item exposures and user ratings are regarded as multi-cause treatments and potential outcomes, respectively. Under this modeling, item co-exposures are used to eliminate the confounding bias and item co-recommendation eﬀects are considered to predict new ratings. Speciﬁcally, to eliminate the bias, based on a no single-cause confounder assumption, we infer and control user-speciﬁc latent variables as substitute confounders. The generative distribution is parameterized by a DNN with factorized logistic likelihood and the intractable posteriors are approximated by variational inference. Furthermore, we demonstrate that MCMO modeling may suﬀer from high variance, mainly due to the scarcity of observation in high-dimensional item causal spaces. Fortunately, we demonstrate that introducing user features as pre-treatment variables, which are factors that are independent of causes but are informative to predicting potential ratings, such as user ages, genders, locations, etc., can substantially improve the sample eﬃciency. Finally, we proposed the theory of "duality of multi-cause confounders" to explain a previously ignored phenomenon where the recommendation performance increases ﬁrst and then decreases with the rise of confounding levels. We demonstrate that this is because multi-cause confounders contain useful collaborative information, but greedily exploiting them could bias the model and degenerate the recommendation quality. Extensive experiments conducted on multiple simulated and real-world datasets show that Deep-Deconf is more robust to unobserved confounders than state-of-the-art causal recommenders. The main contribution is summarized as follows: •We present Deep-Deconf, a deep causality-aware recommendation algorithm based on Rubin’s causal framework. Through controlling a special user latent factor The remainder of this paper is as follows. Section 2 surveys related work regarding causal recommenders and deep causal inference techniques. Section 3 provides formal formulation of the causal recommendation problem. Section 4 expounds upon the proposed Deep-Deconf model in details. Section 5 demonstrates the extensive experiments conducted on simulated and real-world datasets as well as the theory of duality of multicause confounders. Finally, section 6 concludes our paper. Recently, researchers have been aware that recommendation is an intervention analogous to treatment in clinical trials [5]. Since randomized experiments are clearly infeasible for recommendations, confounders, which are factors that aﬀect both the item exposure and user ratings, pervasively exist and bias the collected data [9]. Fortunately, causal inference can be utilized to eliminate the confounding bias and uncover the true causal relationships between two variables [5]. Existing causality-based recommender systems can be classiﬁed into three main categories: propensity score reweighting (PSW)-based methods, substitute confounder controlling-based methods, and graph adjustment-based methods. In addition, methods from each category can also use two paradigms as the fundamental causal framework: Rubin’s potential outcome causal framework (RCF) [10], and Pearl’s structural causal graph (SCG) framework [11]. The concept of PSW originated from RCF [10], the core of which is that outcomes (ratings) under diﬀerent treatment status, i.e., potential outcomes, follow diﬀerent distributions, where one of which is unavoidably unobservable. Therefore, to calculate the treatment eﬀects, i.e., the diﬀerence between potential outcome under treatment and under no treatment, randomized experiments should be conducted to select units in treatment and non-treatment groups such that units in two groups are comparable to each other. However, random recommendations are clearly infeasible in modern online systems. Consequently, unobserved confounders pervasively exist and cause discrepancies between the two groups in the collected data. PSW aims to reweight users in the treatment group by the chances that they receive the treatment (i.e., the propensity scores), such that they can be viewed as random samples from the population [12,13,14]. Linear regression [5] and variational auto-encoders (VAEs) [15] have been used to estimate the propensity scores from historical ratings and user features. However, one problem of PSW is that the estimated propensity scores can be extremely small when the causal space is high-dimensional, which leads to large sample weights that make the training dynamics unstable. Moreover, the unbiasedness of PSW relies on a correctly-speciﬁed propensity score estimation model. However, the model design depends heavily on the expertise of the researcher and is therefore untestable from experiments. Another class of causal recommenders based on the RCF aims to ﬁnd, measure, and control all the unobserved confounders. However, due to the clear infeasibility of the objective, substitute confounders are inferred from item exposures and are controlled as surrogates to the true confounders. One exemplar work is Deconf-MF [7], where the item co-recommendations are viewed as a bundled treatment where latent factors are estimated to render item exposures conditionally independent. Under mild assumptions, controlling such latent factors as substitute confounders can be proved to eliminate the confounding bias. However, both the confounder inference model and recommendation model used in [7] are based on shallow matrix factorization, which may have insuﬃcient modeling ability for large-scale modern recommendation tasks. Utilizing deep neural networks (DNNs) to model both the item exposure and user ratings remains under-explored due to intractable posteriors. Faced with this challenge, we generalize Deconf-MF to a DNN-based framework where the non-linear inﬂuence of confounders to the ratings can be captured and remedied, and this is one of the core contributions of Deep-Deconf. Concurrent with our work, [16] proposed the DIRECT algorithm to extend Deconf-MF, which focuses on learning disentangled representations of the treatment with VAE for better explanation. Compared to DIRECT, Deep-Deconf mainly tackles network weights interpretability, variance reduction of MCMO modeling, and duality of multicause confounders, which are three new independent research questions worthy of in-depth investigations. Relying on Pearl’s SCG framework, graph adjustment-based methods construct a priori a graph that depicts the causal relationships among relevant variables. Compared to RCF, relationships among user/item features, interaction histories, item exposures, and user ratings can be clearly expressed as nodes in the causal graph. To eliminate confounding eﬀects, backdoor adjustment is generally applied on the graph, where pre-treatment factors that aﬀect the treatment assignment (i.e., item exposure) are eliminated from the graph such that items can be viewed as exposed in a random manner. Generally, the established SCG varies drastically among diﬀerent papers, both in variables included for consideration and the assumed links among them. Moreover, there is also no consensus in approximate inference strategies to solve the graph [17,18,19,20]. For example, [19] models the item popularity as the confounder and eliminates its inﬂuence to reduce the popularity bias. [21] explicitly models the inﬂuence of users’ historical rating on items’ exposure, where the inﬂuence is then removed via do-calculus. The superiority of SCG or RCF has been a controversial topic that is under constant debate among the researchers [22]. The reason why we build Deep-Deconf upon the substitute confounder-based method instead of Pearl’s SCG is that, since the establishment of SCG is unavoidably subjective, whether it exhaustively enumerates all confounders or correctly speciﬁes their relationships is untestable. In contrast, in substitute confounder-based methods the causal model can be agnostic of the speciﬁc form and relationships among the confounders, which eliminates the strong substantive assumptions of SCGs. Recent years have witnessed an upsurge in interest in utilizing DNNs for traditional causal inference problems [23,24,25,26]. Among them, the most relevant methods to Deep-Deconf are [27] and [28], which explored the deep latent-variable model for single cause inference tasks, e.g., twin weights and job training, based on RCF. They mainly focused on modeling the joint distribution of treatment, hidden confounders, and potential outcomes via deep generation networks and approximating the intractable posteriors of hidden confounders through variational encoders. However, this strategy is not directly applicable to recommender systems, because when multiple causes exist, it requires a subencoder and a sub-decoder for each treatment conﬁguration. But since the number of treatments for recommendations is exponential to the number of items, extrapolating all the missing potential outcomes requires intractable numbers of inference and generation networks. The proposed Deep-Deconf circumvents this issue by two-stage modeling, where the exposure model ﬁrst ﬁts the joint distribution of substitute confounder and item exposures, and the outcome model then ﬁts the rating distribution conditional on the substitute confounder, item exposures, and user features. This leads to a parameter-eﬃcient solution to the DNN-based multiple cause inference problems. Suppose a system withUusers andIitems. The observational data comprises the rating matrixR ∈ Rand the user features X ∈ Rwhere each rowr,xare the rating and feature vector for useru.Based on the Rubin causal model [10], the received treatment is represented by the exposure matrixAwhere adenotes whether the itemihas been exposed to the useru when ratingris provided. We denote the potential outcome random variable associated with an exposureAfor the useruas R(A). For each user, we only observe the value ofR(A=a), i.e.,r(a). The main quantity of interest is the ratings useru would provide ifKextra items are recommended. Therefore, recommendation under causal reasoning is a counterfactual inference problem. To address this problem, we assume that stable unit treatment value assumption (SUTVA) holds where the ratings of useruis independent of items’ exposure to userv, i.e., R⊥ A| A. This excludes from consideration the interference among users. The randomness ofRandAsources from the fact that the useruis an arbitrary user sampled from a potentially very large super-population. Therefore, unless speciﬁed otherwise, the expectations are taken w.r.t. this population for the Symbol system: We use boldface capital symbolsRfor matrices, boldface lower case symbolsrfor vectors, non-boldface lowercase symbolsrfor scalars, non-boldface capital symbolsRfor random variables (Rmay be scalar, vector, or matrix based on context). Exceptions such as U and I can be easily identiﬁed from the context. Figure 2:The probabilistic graphical model (PGM) for diﬀerent kinds of confounders in recommender systems. The intuition of our model is to ﬁnd the substitute confounderszthat render the item exposuresaindependent Bernoulli trials. Then, conditional onz, multi-cause confounders cannot exist. The proof is by contradiction (see the red lines). rest of the article. The purpose of this article is to estimate the expected causal eﬀects of an exposureAto a useru,E[R(A)], from the population so that unbiased recommendations can be made accordingly based on estimated ratings with low variance. The calculation ofE[R(A)] requires modeling the conditional rating distributionp(R| A). Assuming the independence among items, traditional recommender systems model p(R| A) directly from the observed ratings via naive matrix factorization or DNNs, relying on the strong ignorability users with similar historical interactions tend to have similar ratings. The above approximation is unbiased only if there exist no variables that simultaneously aﬀect bothRandA, i.e., no unobserved confounders [6]. However, confounding bias pervasively exists that inﬂuences the observed ratings, which cannot be addressed by the naive methods [29]. To eliminate the confounding bias, classical causal inference techniques demand us to ﬁnd, measure and control all confoundersCand calculate guarantee unbiasedness provided that there are no uncontrolled confounders. This is both infeasible and untestable [30]. Therefore, to circumvent exhausting and measuring all confoundersC, Deep-Deconf models the recommendation as a multi-cause inference problem. Instead of treating the exposure of an itemAas an isolated cause, we consider the exposures of all the items to a userAas a holistic treatment that could causally aﬀect all the ratings. We ﬁrst assume that the singlecause confounderS, which inﬂuences only the exposure of one speciﬁc item and its ratings, does not exist. The validity of the assumption can be justiﬁed by the fact that the pool of candidate items for modern recommender systems is usually large, and therefore it is unlikely that a confounder inﬂuences only one of the items. Taking genre as an example, the genre of a movie aﬀects both its exposure and rating, and it is a universal attribute that is shared among all movies. With the assumption of no-single cause confounders, we only need to control the multi-cause confounders. This is a more amenable objective, since controlling them is equivalent to controlling latent factors Z∈ Rthat render the causes conditionally independent, i.e., p(A| Z) = Πp(A| Z). A simple proof for the validity Figure 3: The structure of the outcome prediction model. of the claim is that, if multi-cause confounders still exist after conditioning on such latent factors, the exposures cannot be conditionally independent, which renders a contradiction. An intuition can be referred to in Fig. (2). To ﬁnd such latent variableZ, we ﬁrst parameterize the generative distributionp(A| Z) by a DNN with factorized logistic likelihood, i.e.,p(a| z) = ΠBern(a|[θ(z)]). Conditional onZ=z, the exposuresafor userucan be viewed as generated from randomized Bernoulli trials. The factorized Bernoulli distribution satisﬁes the overlap assumption, crucial to the identiﬁability of the model [31]. We then calculate the intractable posteriorq(Z| A) via the variational inference [32], where the prior forZis set to be the standard NormalN(0, I). It could be noted that the assignment model resembles the Multi-VAE for recommendation with implicit feedback [8] because they both manage to reconstruct the binary exposure/click inputs. However, a characteristic that makes the assignment model fundamentally diﬀerent is that it requiresA to factorize conditional onZ. This renders the multinomial likelihood (which has been demonstrated to be more suitable for recommendations in [8]) invalid in our case. The assumption of no single-cause confounder and the utilization of user latent factors as surrogate multi-cause confounders lead us to the following equality, If the exposure model is accurately speciﬁed, conditional on Z=z, the observed ratings for the users can be analyzed as were generated from a randomized experiment, and bias due to unobserved multi-cause confounders is eliminated. In this section, we introduce the deep outcome network that predicts the potential ratingsR(A). Since the outcome network no longer requires the ratings to be conditionally independent, more powerful multinomial likelihood can be put on ratings for more accurate predictions (see Fig. (3) for the structure of the deep outcome model). To gain more insights into the network, we ﬁrst provide a theoretical analysis of the network weights for a special case where the network has a single layer with no activation, i.e., whereis the residual vector for useruandαis the constant term. For users with inferred substitute confounder equalsz, Figure 4:The co-recommendation eﬀects and large variance associated with MCMO modeling. In parallel universe 1, user ihad been recommended with item #3, which increased her expectation to item #2 and made her less satisﬁed with it. The large variance of MCMO is because the number of such counterfactuals is exponential to the number of items. we can calculate the the expected causal eﬀect for the exposure of item j on the rating of item i as which shows that the network weightswcan be interpreted as the conditional average treatment eﬀect (CATE) of recommending itemion the rating of itemj. Moreover, since the R.H.S. of Eq. (3),wdoes not contain az-related term, we have E[E[∆R| Z]]=w, which leads to a further conclusion thatwis also the average treatment eﬀect (ATE) in the population. Note that allowing the causal eﬀects of one item’s exposure on the rating of another item does not violate the SUTVA assumption, as the non-interference of users’ exposures to each other (i.e.,R⊥ A| A, ∀u , v < U) does not exclude from consideration the interference of diﬀerent items’ exposures of a single user (i.e.,R⊥ A| A, ∀i , j < I). On the contrary, it is beniﬁcail to model such co-recommendation eﬀects, since exposing one movie to a user may alter her expectation to movies with a similar genre and therefore causally inﬂuences her ratings towards these movies as well. (Illustration see Fig. (4)) Although confounding bias can be remedied by the controlling substitute confounders, the predicted ratings may suﬀer from a large variance due to the following two factors: First, multiple causes lead to a more severe data missing problem. Consider a system withIitems. The number of counterfactuals is 2, but only one of the outcomes is observed. Therefore, the sample eﬃciency is exponentially reduced compared with the classical single cause problems. Furthermore, even if the observations are suﬃcient, the item exposures tend to depend heavily on the user preference. This makes the exposure and non-exposure group for diﬀerent treatments highly imbalanced, which further increases the estimand variance [10]. In this section, we discuss the variance reduction technique of Deep-Deconf by introducing user features in the outcome model as pre-treatment variables, which are factors that remain uninﬂuenced by item exposures but are predictive to the ratings. To see how this works, we derive the estimand variance before and after introducing user features as pre-treatment variables. For simpliﬁcation, the network weightsWare for now reduced to diagonal, i.e.,w=diag(W), andz,xare one-dimensional. In this scenario, the co-recommendation eﬀects vanish and we can treat the recommendation of each itemRseparately (Note that the randomness ofRis still solely due to the sampling of ufrom the population since the itemiis ﬁxed). To intuitively show the variance reduction, we put aside for now the powerful multinomial likelihood on ratings, and focus on the Gaussian likelihood and the simple Ordinary Least Square (OLS) optimizer. The outcome model for ratingRbefore the introduction of user features can be speciﬁed as Conditional onZ=z, with the overlap assumption, when the sample sizeUis large enough, we can meaningfully calculate the following statistics from the samples, where U=PI(a=t), t ∈ {0,1}is the size of exposure and non-exposure group in the sub-population. We deﬁne CATE on the ratingrasτ=E[r(1) − r(0)| z]. Since unconfoundedness holds when conditional onZ, the average rating diﬀerence between exposure and non-exposure use group ˆτ=¯r(1)− ¯r(0) is asymptotically unbiased forτ. Furthermore, if we deﬁnewas the coeﬃcientwlearned from the Uusers drawn from the population, similar deductions as Eq. (2) can demonstrate thatw=ˆτ. The variance ofw, under the assumption of homoskedasticity, converges in probability to U/U and σis the population conditional variance. After that, we consider introducing user featuresxas additional pre-treatment variables. The new model becomes After introducing an additional covariatexin the model, the same algebra shows thatwestimated by OLS is still asymptotically unbiased forˆτ. But the limiting variance of the estimand becomes(Proofs are in Appendix A.2). Therefore, as long as the user features are indicative to the variation of the ratings,σcan reduce considerably compared to the marginal variance σ, which increases the precision. The previous derivations of CATE interpretation of network weights and variance reduction are mainly based on a simple linear network. However, as the main contribution of this article is proposing a deep causal model for recommendation, we generalize the analysis to the non-linear case. We begin by deﬁning the DNN-based outcome model as where functionf:R→ Ris non-linear but diﬀerentiable almost anywhere. The generalization is achieved from two aspects. First, we consider the global property off. Since the exposureais a binary vector, the prior of substitute confounder Figure 5:The association between the global and local Jacobians of the outcome network and the approximate average treatment eﬀects for all and sub-section population. zisN(0, I), the user featuresxare rescaled to zero mean and unit variance, we can form a global approximation offwith its Taylor expansion at (0, 0, 0) and keep the linear term, where the coeﬃcient matricesWare the Jacobians at (0, 0, 0), andαis the expected user ratings where no items are recommended. The reason to justify the approximation of ftofis that generally,fcannot be highly-nonlinear (f is generally composed of 0-2 hidden layers); otherwise, the outcome model will overﬁt on the negative samples which are not truly negative and fail to generalize to recommend new items [8]. After linearization, the same theoretical analysis can be applied tof, wherewcan be interpreted as both the CATE and the ATE of the recommendation of item i on the rating of item j. However, ifafor some users is dense and deviates far away from the original point, the above approximation may be coarse and inaccurate for this sub-population. Therefore, we propose another reﬁned generalization strategy that shows the local property off. For userˆuwith exposuresa, substitute confoundersz, and user featuresx, we can linearizefat the point (a, z, x) by Taylor expansion, whereWare the network Jacobians at (a, z, x), andαis the expected ratings for userˆuwhere no items are recommended. With the local linearization off, similar analysis can be applied tof. Note that the trade oﬀ is thatwis no longer the ATE of recommending itemion the rating of itemjfor the entire population, but is only the CATE for users who are similar with ˆu(which is measured byz). Therefore, this strategy establishes a corresponding relationship between local Jacobians offand CATE for a sub-population. The details are in Fig. (5) Deep-Deconf is designed to predict unbiased ratings associated with any exposure vectora∈ {0,1},i.e., r(a) with a low variance. However, whichashould be selected for prediction is undetermined, as it demands the answer to the exact question we are trying to solve: which items should be exposed to users. Wang et al. [7] proposed to usea, i.e., the exposure of the originally observed items andKnewly recommended items, Algorithm 1 The Deep Deconfounded Recommender. sures, user features, and collected ratings, wherer∈ {1, 2, 3, 4, 5}, x∈ R, a∈ {0, 1}, u ∈ {1, 2, . . . , U}. 2: # Deﬁnition of the Exposure Network 3: function ExposureNet({a}) 14: # Deﬁnition of the outcome network 15: function OutNet({r}, {z}, {x}, {a} (opt.)) 24: {z}← ExposureNet({a}). 25: {ˆr}← OutNet({r}, {z}, {x}, {a}). 26: for u=U − U, . . . , U doˆr[a== 1]← − inf,sort(ˆr) and get top-K items for recommendation. as the exposure to calculate the ratings for recommendations. However, for Deep-Deconf, enumerating everyr(a) and taking the expectation is clearly infeasible. Still, we propose an approximate strategy for the prediction. We assume that the number of recommended itemsK(e.g., Top 20) is small compared with the size of the item pool (e.g., 10,000). Based on this assumption, the observed exposureacan be used as a surrogate for ato calculate r(a) for predictions. Although with this prediction strategy, the same recommendations are made for users who have the samea, it does not mean that Deep-Deconf is non-personalized, because the pervasive unobserved confounders make the probability density of the exposuresafor each user concentrate on a small but unique niche of the exponentially large causal spaces if the number of exposed items exceed certain value (Actually, for a system with 5,000 items, the possible exposure combination of ﬁve items exceeds the population on earth). Therefore,ais per se a very good representation of the useru(although this can increase the estimand variance as discussed in Section 4.3). Moreover, a direct strategy to make Deep-Deconf "personalized" is to use the previous ratings of a user as extra user features in the outcome prediction model (they are pre-treatment variables because they remain unaﬀected by current exposures). However, experiments shows that this strategy only doubles the networks’ trainable weights but improves hardly any performance. Therefore, we do not use the historical user ratings as pre-treatment variables. We ﬁrst conduct experiments on a simulated dataset. For each user, we draw aK-dimensional confoundercfromN(0, I) and calculate the user preference vectorθconditional on the confoundercasθ∼ γ· c+ (1− γ)· N(0, I). The item exposure rateαfor the entire population is predetermined as 0.1. To simulate the exposure, we ﬁrst generate a exposure propensity score for useruasa=f(Wc). We then globally sortafor all users and set the topαitems as the exposed items. The remaining part of the simulated dataset is generated as follows: x∼ f(θ+ ), where ∼ N (0, λI);(10) r∼ min(1 + Poisson( f(Wθ)), 5), whereλandλcontrol the strength of confounding eﬀects, λcontrols the noise level of user features, andW∈ R are the randomly initialized weights. Compared with the simulation procedure in [7], we use functionsfandfto simulate the non-linear inﬂuence of multi-cause confounders. In our experiment, we set bothfandfas the ReLU function (ReLU(x) = max(0, x)), respectively. Evaluating causal recommenders on real-world datasets faces great challenges, since usually we do not observe ratings for all the items of the users, and confounders make the model evaluations on a randomly split test set biased [33]. Therefore, existing deep causal recommenders [15] establish the causal datasets from real-world datasets. With a similar strategy, we create two semi real-world datasets, ML-causal and VG-causal, based on the realworld Movielens-1m (ML-1m)and the Amazon-Videogames (Amazon-VG)datasets. In simulation, we train two VAEs with factorized logistic likelihood and multinomial likelihood on the exposures (binarized ratings) and ratings to get the corresponding generative distributions from user latent variables z. The decoders of the exposure and rating VAEs are denoted asf,f, respectively. We then simulate aK-dimensional confoundercfor each user fromN(0, I), and the user preference vectorθconditional on the confounders is speciﬁed as θ∼ γ· c+ (1− γ)· N(0, I); the constantγ∈[0,1] controls the strength of confounding. The user features are the noisy observation of her dimensional-reduced user preference vector Note that an expensive and not user-friendly solution is to ask users to rate randomly-exposed items [34]. One such dataset is the Coat dataset established by [27]. However, this dataset is too small in scale to train DNN-based recommenders (290 users and 300 items). Previously, the Yahoo R3 dataset [35] can also be used to evaluate debiased recommenders. However, due to the current US export law, it is not publicly available. Therefore, these two datasets are usually not included to evaluate deep causal recommender systems. https://grouplens.org/datasets/movielens/1m/ https://jmcauley.ucsd.edu/data/amazon/ Table 1:Attributes of the established simulated, ML-Causal and VG-causal datasets. In the table, % density refers to the density of the rating matrix, avg/std #exp. refer to the corresponding statistics of the number of items that are exposed to the users. f(θ+), where∼ N(0, λI). The exposure vectora for user u is generated from The constraint is to ensure that the global item exposure rate of the causal datasets (casl) is the same as that of the original datasets (ori). Moreover, we deﬁne the setR={r∈ R| r∈ range(1,5)}as the set of possible user rating vectors forIitems. The rating of user u is generated from s.t. p(R= r) = p(R= r), ∀r ∈ range(1, 5), whereγcontrols the strength of basic confounding level (since zero confounding is non-existent). The constraint ensures the global rating distribution in the causal datasets to be the same as the original datasets. The observed ratingsris calculated by masking the original ratings with exposures,r=r· a. The schematic illustration of the establishment can be referred to in Appendix B.1. The global item popularity distribution before and after the exposure under diﬀerent levels of confounding eﬀect are shown in Fig. (6). The statistics of the established simulated, ML-causal and, VG-causal datasets are in Table (1). When establishing the causal datasets, the dimensionKof the user preference variables and the confounders is set to 100, 100, 100, respectively. The dimension of the user features is set to 10. The structure of the exposure and outcome models of DeepDeconf is set to{I+{F+K} → K → I}, whereIis the number of items,Fis the dimension of user features, andKis the latent dimension. The models are trained with Adam optimizer [36], with learning rate of 1efor 100 epochs. For the exposure model, 20% of the observed exposures of the validation users are hold-out for predictive check [37]. The selection of the outcome model generally follows the same procedure, where R@20 and N@20 on hold-out ratings are monitored as the metric. For all three datasets, we searchK∈ {50,100,150,200}and ﬁnd thatKequalsKindeed achieves the best performance among models with all searched structures. We evaluate the model performance under strong generalization [38], where the observed item exposures and ratings for Figure 6:Rating distribution before and after exposure under diﬀerent confounding levels (Top: simulated dataset, Middle: ML-causal dataset, Bottom: VG-causal dataset; Obsv.: observation, Popu.: population). From this ﬁgure we can ﬁnd that the discrepancy between the observed and true rating distributions becomes larger with the increase of confounding levels. (Detailed analysis of the established causal datasets can be referred to in Appendix B.3) validation and test users are used only for inference purpose. When training the exposure model, we put aside 20% of the observed exposures for predictive checks [37], where the best model is selected by log-likelihood of the hold-out exposures. The outcome model is selected by how well it ranks the hold-out observed interactions for the validation users. The ranking quality is evaluated by R@Kand N@K, where R@Kis the Top-K recall. If we denote the item at ranking positionrbyi(r) and the set of hold-out items for the user by I, R@K is calculated as: whereIin the numerator is the indicator function, and the denominator is the minimum ofKand the number of hold-out items. N@K is the normalized DCG deﬁned as follows: The model is selected by N@20 on validation users where 20% of the observed ratings are hold-out for model evaluation. We choose onlyK= 20 for both metrics to report because we found that the performance trend is similar for diﬀerentKin our experiments (we have testedK ∈ range(5,50,5)). The R@20 and N@20 on test users with fully observed ratings for all items averaged over ﬁve diﬀerent splits of the datasets are reported as the unbiased model performance. Our primary baseline is the deconfounded recommender (Deconf-MF) [7], which also models the recommendation as a multiple causal inference problem. In Deconf-MF, the substitute confounders and ratings are estimated by linear Poisson factorization. Two other causality-based recommenders are both based on propensity-score re-weighting, which eliminates the confounding bias by re-weighting the ratings by the inverse of propensity scores i.e., the chance of their exposures conditional on covariates (previous exposures and user features). The ﬁrst https://github.com/yixinwang/causal-recsys-public method is the inverse propensity weighting matrix factorization (IPW-MF) [5], where the propensity scores are estimated by simple regression. Another method is the variational sample re-weigting (VSR) [15], which considers the multiple causes as a bundle treatment and estimates the propensity scores via latent variables instead of observations. For a fair comparison, the two matrix factorization-based methods are augmented with user features in an SVD++ manner [39] and the VAE-based methods concatenates user features with exposures as the extra inputs, where improvement has been observed compared to their original forms. The non-causal baselines we include are the weighted matrix factorization [40] (WMF, also augmented with user features) and the concat-VAE, which is a variant of DeepDeconf where the exposure model is removed to demonstrate the eﬀectiveness of causal reasoning. In simulation, we ﬁx the basic confounding levelγto 2.0 as with the empirical estimation of Wang et al. [7], and we vary the strength of user preference confounding eﬀects by changing γ∈ range(0.1,0.2,0.9). The models are then evaluated under diﬀerent confounding levels. The comparison results are demonstrated in Table (2). From Table (2) we can ﬁnd that the vanilla WMF performs the worst among all the methods that we draw comparisons with, especially when the ratings are heavily confounded. IPW-MF addresses the confounding bias by re-weighting the ratings by the propensity scores estimated through regression. While improvement has been observed over WMF in most cases, the unbiasedness of IPW-MF requires two strong assumptions, i.e., unconfoundedness and a correctly speciﬁed propensity model, which relies heavily on the expertise of the researchers. Therefore, it hinders IPW-MF’s further improvement. Deconf-MF weakens the unconfoundedness assumption of IPW-MF to the non-existence of single-cause confounders, and is the best matrix factorization-based baselines demonstrated in the middle part of three sub-tables of Table (2). However, since Deconf-MF models both the exposures and the ratings as linear Poisson matrix factorization, it fails to capture non-linear inﬂuences of Table 2:Model comparisons under diﬀerent confounding levels. The best method is highlighted in the best evaluated performance w.r.t. the confounding level is highlighted with blue and red for R@20 and N@20, respectively. unobserved confounders. Moreover, it treats a user’s ratings to diﬀerent items separately in the outcome model, where corecommendation eﬀects cannot be considered to further improve recommendation performance. Consequently, Deconf-MF is outperformed by the MultiVAE-based deep generative models, even if Concat-VAE is not causality-based and VSR is based on propensity scorereweighting which requires a stronger unconfoundedness assumption for unbiasedness to hold. Combining the advantages of non-linear collaborative modeling ability of MultiVAE and model-agnostic debiasing advantage of the substitute confounder-based causal inference for recommendations, DeepDeconf achieves a systematic performance improvement compared with Deconf-MF while being more robust than CondVAE and VSR faced with unobserved confounders. Therefore, the experiments demonstrate the superiority of the Deep-Deconf to other causal recommenders. From Table (2) we can ﬁnd that the best results for each method w.r.t. the confounding level, which are marked with colors blue and red forR@20 andN@20, respectively, appear in the middle of the Table. This shows an interesting phenomenon that the performance for all the methods improves ﬁrst and then deteriorates with the increase of confounding level. This is against the naive intuition that the recommendation performance should reduce monotonically when the confounding level increases. The phenomenon can also be discovered from Fig. 1 of [7], but the authors provided no explanation to why it occurs. We propose the "duality of multi-cause confounders" to explain such a phenomenon. When the confounding level is low, users tend to consume items at random where item co-occurrences contains little collaborative information. Therefore, the models waste parameters to ﬁt uninformative random item exposures, which degenerates the model performances. However, the observed rating distribution is a more faithful representation of the population rating distribution. This reduces the confounding bias. When the strength of confounding eﬀects increases, although the co-occurrence of items demonstrates more regular patterns, the observed rating distribution deviates further from the true population distribution. Thus, the inﬂuence of confounding bias outweighs the introduction of extra item collaborative information and reduces the performance. In essence, the duality of multi-cause confounders results from their commonality among items, i.e., they are also shared item attributes. Therefore, in contrast to their role in traditional single-cause inference problems, the confounders in recommendations, like yin and yang, exert their force in two opposite ways: On the one hand, since these confounders tend to be "shared" items attributes, they help explain why certain items tend to appear together and why certain items have never occurred at the same time. This introduces item collaborative information that is conducive to the recommendation of new items that are similar to the items that the user has interacted with. On the other hand, when the confounding eﬀects are overly strong, the observed rating distribution diverges drastically from the true rating distribution, which leads to systematic bias due to the unbalanced representation of items in the data that severely degenerates the recommendation performance. (Appendix B.4). The duality of multi-cause confounders naturally leads to a further research question: In the battle of the beneﬁts and drawbacks of multi-cause confounders, which one will prevail? The answer, which the authors believe, is that no matter the results, a deconfounded recommender should always be preferred over a non-causality-based one if unobserved confounders indeed pervasively exist to aﬀect the ratings, because greedily exploiting the collaborative information associated with unobserved confounders come at a price: It also inherits the confounding bias in the historical ratings, which hinders the further improvement of the recommendation model. Consider the extreme case where the exposures are entirely due to previous recommendations. In this case, the new recommender system will perform no better than the previous one by greedily exploiting. Deconfounded recommenders remedy the bias by balancing the underand over- represented items with either sample re-weighting or controlling substitute confounders. Through these mechanisms, the recommender system can provide a more bias-free estimate of user preferences. Furthermore, the above analysis naturally leads to another conclusion that a good strategy to reduce the confounding bias from the data collection perspective is to avoid greedy exploitation and to add random exploration to the recommendation policy that collects the user ratings, where the randomness negates the inﬂuence of unobserved confounders. This is interesting, as it provides another justiﬁcation for exploration, i.e., a commonly adopted strategy in reinforcement learning-based recommender systems, from a causal perspective. A brief discussion of the relationship between exploration and deconfounding can be referred to in Appendix B.5. We have proved that introducing user features as pre-treatment variables can reduce the estimand variance for Deep-Deconf as long as they are informative to rating prediction. However, how does the "informative level" of user features inﬂuence the estimand variance is unclear. Recall that in our simulation, user features are generated by settingx=f(θ+), whereθis the user preference and∼ N(0, λI) is a random Gaussian noise. We control the informative level of the user features by settingλ∈ {0.1,0.5,0.9}and evaluate Deep-Deconf. The performances are compared to a baseline where no user features are used. The results are summarized in Table (3). From Table (3) we can ﬁnd that user features that are more informative to rating prediction (i.e., with less noise) indeed lead to a lower estimand variance, which is reﬂected by a higher performance when the ratings associated with one speciﬁc item exposure is extremely sparse. Moreover, the performance improvement is more signiﬁcant when the simulated confounding level is high. When the user features are highly noisy (λ= 0.9), however, the model overﬁts on the noise and the performance degenerates to be similar to the baseline model where no user features are used. Table 3:Performance of Deep-Deconf when user features have diﬀerent noise levels (N.F. means no user features) ML-causal Low Conf. (λ= 0.3) High Conf. (λ= 0.7) In this article, we proposed an eﬀective deep factor model-based causal inference algorithm, Deep-Deconf, for recommender systems. By controlling substitute confounders inferred through factorized logistic VAE that render the observed exposures randomized Bernoulli trials, Deep-Deconf lower the multi-cause confounding bias, leading to more faithful estimation of user preferences. Moreover, we have proved that the variance of the estimated unbiased ratings can be substantially decreased by introducing user features as pre-treatment variables. We note that our algorithm can be plugged into any user-oriented auto-encoder-based recommender systems by adding a decoder branch that constrains the user-latent variable to generate factorized exposures. Therefore, we speculate that these models can also beneﬁt from the confounding reduction advantage of our method with a modest extra computational overhead. In the exposure model of Deep-Deconf, we claim that a decoder that takes substitute confounderzas input and reconstructs the exposureawith factorized logistic likelihood can renders aconditionally independent. Some may question that this is not possible since the weights that predictafor diﬀerenti are shared. This is not true. The fact is that the exposures cannot be marginally independent, as they are all governed by unobserved confounders. However, conditional onz, they can be independent, and generation ofafromzcan also be implemented via a shared decoder. The reason is as follows. For useruwith latent confounderz, if we denote the input vector to the last layer of the exposure network asf(z), the Figure 7:A simpliﬁed illustration of the distribution ﬁtting process and the confounding simulation process to create the real-world causal datasets used in experiments. logit of the exposure to itemiis calculated aslogit p(a| z) = [W f(z)]=w· f(z). Sincef(z) only depend onz, if the row vectors of the last layer weightsWare independent,z contains all information foracontained ina. Therefore,a is conditionally independent ofagivenz, even if the weights used to infer diﬀerent aare shared among all items. In this section, we derive the sampling variance of the coeﬃcient of exposure indicator obtained by ordinary least square (OLS) estimator before and after the introduction of user features as pre-treatment variables. Recall that after we simplify the network weightWto a diagonal matrixw · I, user features and substitute confounders to scalars, the single-layer outcome prediction network for the ratings before the introduction of user features as pre-treatment variables becomes As we have demonstrated in the main paper, since conditional on z, the ratingsrcan be viewed as generated from randomized experiments,ˆwis an unbiased estimator for the population conditional average exposure eﬀects of itemion ratingr. The SUTVA assumption ensures the non-interference of exposures of diﬀerent users, and the homoskedasticity assumption ensures that the variance does not vary by the change of aand z[10]. If we further assumeE[| A, Z] = 0 (the above assumptions are known as the Gaussian-Markov assumption), according to the Gauss theorem [42], the sampling variance of ˆwis wheres=ˆσis the OLS variance ofR,¯a=Pa/U is the average exposure number of itemiin the ﬁnite sample, Uis the exposure count of itemiandU=U − U. The second equality can be derived with simple algebra based on the fact thata∈ {0,1}and thereforea=a. Theˆσcan Figure 8:Item popularity distribution of the established MLcausal and VG-causal datasets. be calculated as the common variance across the two potential outcome distributions, After we multiply and divideUon the R.H.S. of Eq. (17), we haveˆV→asUapproaches inﬁnity, wherep= limU/Uis the population probability of the exposure of itemi; this ﬁnishes our deduction of estimand variance before introducing the user features as pre-treatment variables. Post the introduction of the user featuresx, the outcome prediction network becomes Supposing again the variance does not vary by the change of the treatment indicatora, the surrogate confounderz, and the user featurex(i.e., homoscedasticity), with the SUTVA assumption and the assumption ofE[| A, Z, X] = 0, the limiting sampling variance of the OLS estimator for Eq. (19), i.e.,ˆV, given the general case ofˆV, can be directly calculated as. By comparing the form ofˆVandˆVwe can ﬁnd that the only diﬀerence lies in the OLS variance term in the numerator: if the features of a userxare indicative to the prediction of her ratingsr,σreduces considerably compared withσ, which leads to a substantial decrease of estimand variance. This is especially favorable in our multiple causal case where the low sample eﬃciency due to large causal space hinders a precise inference. The authors have noticed that the deconfounder algorithm [6], based on which we designed the Deep-Deconf, has raised some discussions recently among researchers, including [43], [44]. Wang & Blei have also responded to the comments in [6], [45]. The questions of [43] were solved in [7] (see footnotes on Page 2), which stated that the focus of Deconf-MF is on estimating the expected potential outcome (ratings) ifKextra items are exposed (i.e., top-K recommendations), so it has diﬀerent assumptions with counterexamples that rely on do-operators. The disagreement between Wang & Blei and Ogburn et al. mainly lies in the assumptions required for the model identiﬁability. Ogburn et al. believed that Deconfounder requires extra assumptions such as the inferred substitute confounderzdoes not pick up post-treatment variables, etc. However, Wang et al. responded Figure 9:Averaged KL-divergence between individual rating distributions before and after exposure. that these assumptions do not satisfy the requirementzcan be pinpointed (see Theorem 7 in [7]). Similarly, the identiﬁability of Deep-Deconf also relies on the pinpoint requirement of the substitute confounders. A schematic illustration for the establishment of the real-world causal datasets is shown in Fig. (7). The popularity of an item is deﬁned as the number of users who have rated that item. Since the item popularity distribution on the ML-causal and VG-causal datasets only depends on the exposure, we visualize the distribution with an arbitrary confounding eﬀect. The results are illustrated in Fig. (8). From Fig. (8) we can discover that the item popularity distribution of both the ML-causal and VG-causal datasets exhibits both long-tail and right-skewed characteristics, which faithfully reﬂects the item popularity distributions in the real-world scenario. The global rating distribution of a recommendation dataset isP r= 0 denotes the rating is unobserved is excluded from consideration. If no confounder exists, the exposure matrixAis a random matrix where the elements inAare independent Bernoulli variables. The individual rating distribution for a user P(R=r) can be deﬁned accordingly. Since the observed ratingsRis acquired byR=R× A, the element-wise independence ofAensures that the global and individual rating distributions of the observed ratingsRis unbiased estimators to those of the population ratingsR. Unobserved confounders, however, create a spurious dependence ofR onA, and therefore lead to systematic bias of the rating distributions inRafter the exposure. Although in practice, the population rating matrixRis unobtainable, in our simulation, we have the users’ ratings for all items (although only exposed ratings are visible to the algorithm for training). Therefore, in this paper, we can visualize the confounders’ eﬀect on both global and individual rating distributions after exposure under various levels of confounding eﬀects. We setλ= 0 andλ= 0 where no confounding eﬀect exists and then ﬁxλto 2, and varyλfrom 0.1 to 0.9. The comparisons of global rating distribution are shown in Fig. (6). Figure 10:A simple example to explain the duality of multicause confounders in recommendation systems. From Fig. (6), we can ﬁnd that an obvious observation of the confounding eﬀect is that positively rated items are more likely to be exposed than their negatively rated counterparts, and the stronger the confounding eﬀects are, the more unbalanced the exposure of highly-rated and lowly-rated items. This is quite interesting, since all we have done to simulate the confounders is to (a) train two VAEs to model the exposure and rating distributions of the ML-1m and Amazon-VG datasets and (b) make the latent variables that generate the exposure and ratings for a user correlated by taking a weighted sum of the confounder and user variable. This phenomenon also has a real-world explanation: Users tend to rate items they like and ignore items they dislike, which leads to systematic bias due to the gross underrepresentation of items with negative ratings. Furthermore, we visualize the diﬀerence of the individual rating distributions before and after exposure under various levels of confounding eﬀects. The averaged KL-divergence between true and observed individual rating distribution is in Fig. (9). In the experiments, we have discovered that the recommendation quality improves ﬁrst and then degenerates with the increase of confounding level. We conclude that "multi-cause confounders in recommender systems like yin and yang, exert their forces in two opposite ways." In this section, we provide a simple but intuitive example to further support the claim. Suppose that in our toy system, a subset of users is "blue lovers" who rate all blue items ﬁve and all red items one —similarly, a subset of "red lovers" rate just the opposite way. Moreover, blue lovers tend to be recommended with blue items, and red lovers with red items. Since color aﬀects both the exposure and the rating of an item, item color is a confounder in the system. In addition, it is a multi-cause confounder because color is an attribute that are shared among all the items. The observed ratings for the two blue lovers and one red lover under low and high confounding levels are illustrated in Fig. (10). The left part of Fig. (10) shows observations under no confounding eﬀects. In such a case, the exposure probability of an item is independent of its color (1/3 for both red items and blue items), and the observed global and individual rating distributionsp(r= 5) =p(r= 1) = 1/2 exactly matches the population rating distributions. However, since the item co-occurrences are random, no item collaborative information can be utilized to recommend new items with similar rating patterns. In contrast, the right part of Fig. (10) shows rating observations with a high confounding level, where the exposure probability of an item clearly depends on its color and the user’s preference. Under this circumstance, new items with similar Figure 11: U: user,V: item,R: rating,X: interaction history, E: a stochastic exploration strategy. (a) The hypothetical causal graph that traditional non-causality-based recommender systems assume to generate the collected rating data, where the exposure of itemVis independent of userU. (b) The true causal graph that generates the data collected by an exploitation-based strategy, where the exposure of itemVis dependent on the user’s interaction historyX. (c) The true causal graph that generates the data collected by an exploration-based strategy, where the exposure is based solely on the random exploration strategy E. user rating patterns with the items have already interacted by the users can be readily recommended based on item collaborative information (e.g., item #3 to user #1, and item #1 to user #2 based on their similar rating pattern to item #2 for both users). However, the observed rating distribution (all are ﬁve) deviates drastically from the population distribution, which introduces a systematic bias between the true and observed distribution that could degrade the model performance. Although we establish the Deep-Deconf with Rubin’s causal model in the main paper, we temporarily switch to Pearl’s causal structural models to demonstrate how exploration eliminates confounding bias from the data collection perspective for better illustrative eﬀects. Speciﬁcally, we provide three causal graphs as Fig. (11). Fig. (11) - (a) is the hypothetical causal graph that traditional non-causality-based recommender systems assume to generate the collected rating data, where the exposure of item Vis independent of userU. Therefore, it can also be viewed as assuming the data are generated from randomized experiments from Rubin’s causal perspective. Fig. (11) - (b) shows a simple case of the true data generation process where the recommender systems used to collect the data greedily exploit the historical user ratings and user’s preference to make recommendations. Note that If we use a naive model that assumes the data generation process of (a) to ﬁt on rating data actually generated according to (b), the inﬂuence of unobserved confounders is mistakenly captured as the user preference, which leads to confounding bias in these models. Fig. (11) - (c) shows the causal graph where the recommender system uses a random exploration strategy (i.e., randomly picking up an item to show the users) to collect data. In (c), we can ﬁnd that since the item exposure is due solely to the random exploration, the exposure bias (U → V) and confounding bias (U ← X → V) is negated by the randomness. Therefore, models with the assumption of (a) can still be unbiased when trained on data collected by (c). Most recommender systems are a balance between exploitation in (b) and exploration in (c). Through comparisons among the three causal graphs, we analyze the exploration and exploitation, two commonly used strategies in reinforcement learning-based recommender systems, from a causal perspective; based on this, we provide a new justiﬁcation for exploration strategies other than reducing the uncertainty in user preference estimations.