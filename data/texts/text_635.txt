Keywords route recommendation · route optimization · deep learning · reinforcement learning · COVID-19 Taxicab service plays an essential and irreplaceable role in urban trafﬁc system [Ji et al., 2020]. For example, in New York City, there are more than 21,000 taxi drivers and more than 80,000 ride-sharing drivers. Compared to other means of daily transportation, such as bus and subway, taxis usually offers a better trip experience in terms of comfort, convenience, and travel time accommodation. Thus, it has been a long-standing central issue to improve the efﬁciency of vehicle mobility by optimizing the route recommendation for drivers for taxi services in big cities like New York, Tokyo, and Beijing [Yuan et al., 2011, Zheng et al., 2014]. Based on large-scale taxi trace data, there is an extensive literature on route recommendation systems. Some studies focus on the traditional optimization method. For example, Qu et al. [2014] proposed a cost-efﬁcient objective function and developed a greedy method to maximize the potential net proﬁt. Similar methods can be found in [Ding et al., 2013, Zhou et al., 2016]. Stochastic optimization methods (e.g., simulated annealing -SA-) and parallel computing techniques have also been applied to route recommendation problems to speed up the route searching tasks (see [Ye This manuscript has been accepted by ACM Transactions on Intelligent Systems and Technology on April 25, 2021. Pengzhan GuoKeli XiaoZeyang Ye Stony Brook UniversityStony Brook UniversitySamsung Research America Vehicle mobility optimization in urban areas is a long-standing problem in smart city and spatial data analysis. Given the complex urban scenario and unpredictable social events, our work focuses on developing a mobile sequential recommendation system to maximize the proﬁtability of vehicle service providers (e.g., taxi drivers). In particular, we treat the dynamic route optimization problem as a long-term sequential decision-making task. A reinforcement-learning framework is proposed to tackle this problem, by integrating a self-check mechanism and a deep neural network for customer pick-up point monitoring. To account for unexpected situations (e.g., the COVID-19 outbreak), our method is designed to be capable of handling related environment changes with a self-adaptive parameter determination mechanism. Based on the yellow taxi data in New York City and vicinity before and after the COVID-19 outbreak, we have conducted comprehensive experiments to evaluate the effectiveness of our method. The results show consistently excellent performance, from hourly to weekly measures, to support the superiority of our method over the state-of-the-art methods (i.e., with more than 98% improvement in terms of the proﬁtability for taxi drivers). et al., 2018a,b, Zhang et al., 2019]). To avoid identical route recommendation to different drivers, Xiao et al. [2020] developed a multi-user mobile sequential recommendation model with a designed core rotation and mixing strategy to enhance SA when handling the parallel search for multiple drivers. On the other hand, some studies focus on machine learning-based approaches for route recommendation problems [Wang et al., 2017, Hu et al., 2019, Li and Chen, 2013]. For example, Wang et al. [2017] deployed a rank-based extreme learning machine (ELM) model to recommend road clusters to taxi drivers for passenger seeking. By exploring road clusters through a clustering process based on the middle point of the road segment, their method aims to increase the pick-up probability for drivers. Garg and Ranu [2018] implemented a Monte Carlo tree search method to minimize the traveling distance for taxi drivers. Zi et al. [2019] proposed a cloud-based system and applied machine learning algorithms to help passengers detect abnormal taxi trajectories. Importantly, Rong et al. [2016] suggested that drivers’ long-term passenger seeking process can be viewed as a Markov Decision Process (MDP). Considering that reinforcement learning (RL) techniques are powerful in handling MDP [Mnih et al., 2015], RL-based approaches have appeared in some recent studies. After introducing a comprehensive process of trafﬁc-related feature extraction, Ji et al. [2020] applied the classical deep reinforcement learning method to a dynamic route recommendation system. However, two outstanding issues in the existing route recommendation literature are still under investigation. First, although the classic version of RL has been shown to be effective in dynamic route searching, few studies can be found to address the adaptive versions of the RL-based method. Thus, we believe that RL’s performance in route recommendation can still be signiﬁcantly improved via an adaptive design. Second, most of the existing methods have not been investigated under a dynamic urban environment scenario (e.g., sudden situation changes due to unexpected public health emergence like the COVID-19 outbreak). In this case, an effective route recommendation system should be able to detect these abnormal situations and conduct related self-adjustments. To address these two issues, we propose an adaptive reinforcement learning method with a self-check mechanism. Our method will not only accelerate the convergence rate of traditional RL methods, but also handle sudden vehicle demand changes due to unpredictable public emergency. In summary, our work differs from existing methods and contributes to the literature in three ways. The rest of the paper is organized as follows. In Section 2, we propose the dynamic taxi route recommendation problem after laying out related deﬁnitions. Section 3 introduces key deﬁnitions and concepts particular to our model. Section 4 presents our methodology, including detailed explanation of the sequential method, and theoretical discussions of their effectiveness. In Section 5, we demonstrate and then discuss the results on a large-scale real-world dataset. Subsequently, we summarize additional related work in Section 6 and ﬁnally, we conclude in Section 7. In this section, we introduce important deﬁnitions and formalize our problem. Deﬁnition 1 each segment is associated with a start point and an end point. Moreover, if an area is connected to multiple areas, then this area has several road segments. First, we apply a self-check mechanism to periodically compare the current policy to the preserved ones, supported by theoretical analysis. Using the self-check mechanism, our model can achieve a better performance than the classical reinforcement learning method under the same condition. Second, we use a deep neural network to enrich the ability of our method in detecting potential situation changes. Upon encountering dynamic environments, our model can automatically adjust its parameters to achieve optimal route recommendation performance. The effectiveness of the parameter updating approach is supported by theoretical analysis. Finally, our method’s objective function is ﬂexibly extensible to deal with different aspects of the path quality (e.g., traveling costs, proﬁtability, etc.). Based on data from New York City, we evaluate our method with a focus on the proﬁtability of recommended routes. The results have validated the superiority of our method over other benchmarks, including the state-of-the-art methods. Compared with existing methods, our method can achieve 98% or more earnings for taxi drivers. Importantly, given the sudden situation change caused by COVID-19, our method leads to consistently superior performance in terms of the hourly and weekly income across different months. (Road Segment). A long path can be divided into several road segments by its connections. Speciﬁcally, Deﬁnition 3 Vdenotes road intersections, and E means road segments. Since we focus on area recommendation, in our settings, Vdenotes the area intersections and E refers to the connections between all areas. For any vertices: road network G is deﬁned as a city. Figure 1 depicts an example of a road network with a recommended route by showing the proﬁt. In the graph, the road segment is two connected areas. As mentioned before, the time for < denotes the point in the graph. For the recommended path are one-directional due to the setting that taxi drivers cannot drive back and forth in the same single road segment. This setting is to avoid causing trafﬁc jams and accidents as mentioned in [Qu et al., 2014]. The total proﬁt for one area is equal to the multiplication of delivery time and the minute income based on historical data in that area. The pick-up probability is dynamic and related to the arriving time and pick-up frequency. We will show more details on the pick-up probability in Section 3. Suppose that a taxi driver is at a location evaluate a recommended route etc.), denoted by a function g(·). The general route optimization problem can be formulated as: Note that 2010, Ye et al., 2018a], expected traveling time to ﬁnd the next passenger [Ye et al., 2018c, Xiao et al., 2020], proﬁtability of the recommended route [Qu et al., 2014], and so forth. This paper mainly investigates the route optimization problem by evaluating the proﬁtability, and the reasons are twofold. First, no matter whether the recommended route will minimize the traveling distance or time, proﬁtability is always the fundamental user demand [Zhou et al., 2018, Chen (Route). A recommended route for a taxi driver at a speciﬁc time is a sequence of connected road segments, (Road Network). Traditionally, a road network is a directed graph. It is denoted asG=<V, E>, where VandV. Since we focus on the area recommendation, <V,V> is equivalent to <V, V>. The (General Route Optimization Problem). Givencthe starting location (area) of a taxi driver, we recommend ∈ R is any possible route with the starting location c; g(·) is the path quality evaluation function. g(·)can be deﬁned differently, such as the potential traveling distance to ﬁnd the next passenger [Ge et al., et al., 2020]. Second, given the availability of taxi drivers’ earning (per minute) data, proﬁtability serves as a direct route quality measurement compared with other metrics. Thus, we can reformulate the route optimization problem as a proﬁtability-oriented route optimization problem that aims to maximize taxi drivers’ income. Problem 2 and the current time, we recommend a route to maximize the earnings of the driver, and the optimal route represented as: wherer respectively, and is the expected traveling time from P To address the problem deﬁned above, we propose an adaptive temporal difference learning with self-check mechanism (ATDSC). The structure of the ATDSC is demonstrated in Figure 2. As can be seen, the framework contains two major components, including a neural network for pick-up failure rate determination and a deep-learning component for policy generation. Speciﬁcally, we ﬁrst input the current pick-up frequencies and related travel records of the previous year to determine parameters related to the self-check mechanism via a deep neural network. Then, in the policy generation part, we imply a Temporal Difference (TD) learning process to handle the exploring task for optimal routes. Once the iterations are divisible by the check parameter τ, the model will stop and check the quality of the current policy. The self-check process can be viewed as a Markov Chain Process (MCP) whose transition probability is determined by the quality of the policy. If the quality of the path under the updated policy keeps improving, the transition probability remains zero. Suppose the quality of the path does not change after several rounds of checks. In that case, the transition probability will be set to one, and the model will be transferred to the original blank situation. The above exploring strategy is designed to accelerate the convergence rate of our method. Finally, the model will output the preserved optimal path in the recommendation part. We shall discuss the technical details with theoretical analysis in Section 3 and Section 4. (Proﬁtability-Oriented Route Optimization Problem). Givencthe start location (area) of a taxi driver ∈ Ris any possible route with the starting locationc;PandDrepresent a pick-up and a drop-off area, p(·)is the pick-up probability;IN C(P)is the evaluated earning rate given a pick-up area;t In this section, we discuss our reinforcement learning (RL) framework and related design of the reward and transition probability. To form an RL framework for route recommendation, we consider different pick-up areas as different states. The action acan be deﬁned as the selection of the next pick-up area to visit. The reward is deﬁned as the proﬁt at pick-up location P. Suppose that we have complete knowledge regarding the environment. Denoted by function of starting at a pick-up location P whereγ is a time step, and The optimal state-value function is deﬁned as: wherev probability. Considering that there are usually many potential pick-up areas in a city (e.g., NYC), it would be expensive to obtain the exact policy value. Temporal Difference (TD) learning is designed to explore the policy under an incomplete environment [Tesauro, 1992], which is a good ﬁt for our task. The estimation of the state value under the policy π via TD learning is shown as follows. whereη optimal policy, we have to evaluate numerous policies, which is expensive. To save computing time, we apply the off-policy TD control [Watkins and Dayan, 1992] which is deﬁned as: whereA generating the policy. It can also directly approximate the optimal action-value function policy π Based on the TD learning method, we have added a self-check mechanism to accelerate the convergence rate. We have also designed a deep neural network to monitor the situation for the area and to determine the adaptive parameter. The technical details on the self-check mechanism and the adaptive parameter F will be explained in Section 4. The reward is an essential component in RL and will guide the solution exploration. Given that the proﬁtability is usually considered as an essential evaluation metric for route recommendation [Qu et al., 2014], we deﬁne the reward Sas the estimated earning at pick-up area each pick-up area at a given time based on historical data. The drop-off point is predicted by capturing the distribution from historical data. After we obtain the potential drop-off areas, we can estimate the earning of a given pick-up area Pat a speciﬁc time, as long as we compute the delivery time of < D>, the delivery time is not difﬁcult to be estimated (e.g., the average traveling time). For the case that < the delivery time is calculated as the weighted shortest path. The weights of the directly connected area are equal to the average delivery time in the historical records. is the discount rate;nis the number of states on the path;E[·]is the value of a variable given the policyπ;t (P)denotes the value function of a statePunder a policyπandp(P, S|P, a)is the action-reward is the learning rate. Based on the state value, we can evaluate the quality of the given policyπ. To ﬁnd the denotes the action at locationP. This method is independent of the policy, which helps save time for If neither < on corresponding distribution. We assume the delivery time in the data follows a normal distribution with the mean delivery time deviation upper bound Similarly, the cruising time from the drop-off point and upper bounds, LB Reward Function. Considering the route proﬁtability, the reward can be deﬁned as: where D When estimating the income, we clean the row data to adjust abnormal pick-up points (e.g., data errors or low-probability cases). We ﬁrst calculate the average income (per minute) for the time period. Given the delivery time for a pick-up point, the target income is deﬁned as the multiplication of the average income of the pick-up point and the delivery time. If the real total income for the given area is higher than the target income, it will be replaced by the target income. This process removes the outliers based on each pick-up point’s local information. Then, we further modify the data by considering the global reasonableness. The following data cleaning process is performed: We setλ = 0.5 with insufﬁcient historical records. Note that the reward function can be modiﬁed based on different evaluation metrics. Given a dynamic real-world scenario, the action-reward probability is used to evaluate the probability that a driver will receive the reward from the drop-off area to the next pick-up area. As mentioned in [Veloso et al., 2011], taxi drivers usually do not want to travel a long distance for the next pick-up location, which is related to the cruising time. Since cruising time varies for different locations, we normalize the predicted cruising time, and the probability related to cruising time at area P The historical data of pick-up information has an inﬂuence on the pick-up probability [Dong et al., 2014]. As mentioned in [Rong et al., 2016], the pick-up probability can be represented by the proportion of successful pick-ups. We thus count the records of successful pick-ups from the historical data and normalize them. The probability related to frequency for pick-up area P Based on Eqs. (11) and (12), the probability of getting reward at the pick-up point P P,D> appears in historical data nor connected, we determine the delivery time by randomly picking based σ. Then, for a more effective sampling process, we set a lower boundLB= µ− 3σ, and an UB= µ+ 3σ. We ﬁnd that this range covers99.7%cases in our data. Thus, the delivery time ) can be estimated as: t(D, P) =, P>exists or connected∼ U(max(0, LB), UB), otherwise(8) is the drop-off point for P; Pis the next pick-up point. IN C(P) =µ+ 3σ, if IN C(P) > µ+ 3σλ · IN C(P< average # of pickups(10) in the experiments. This process will handle pick-up points with abnormally high-income and those p(D, P) =1 − β, otherwise(11) whereα represent the effect of anomalies (e.g., unexpected social events) on the pick-up probability. Then Eq. rewritten as: We setω = 1 previous year, then we view this area as an abnormal area and set ω < 1. Action-reward Probability Function can be written as: Algorithm 1: Main Function for Path Evaluation: eval Input :r, t , count, profit ← 0; This section discusses our optimization method designed for the reinforcement learning framework with related theoretical analysis. It is computationally expensive to acquire the full knowledge of the environment for a dynamic system. As a model free reinforcement learning (RL) method, temporal difference (TD) learning holds the strength in exploring dynamic environment with unknown outcomes. While traditional TD learning searches a sequence of optimal solutions for all the states, we propose to focus on the states within a given time interval to avoid unnecessary computing. To accelerate the convergence rate and improve the outcome, we introduce a self-checking mechanism and an adaptive parameter into the TD learning. During the exploring process, if steps are divisible by a predetermined integer exploring process stops and the current policy will be checked. If the current policy is better than the preserved policy, it replaces the preserved policy and continues to update the current one; the model will also set the count variable zero. Otherwise, continue on the current policy. If Γ = F The self-checking process is equivalent to a Markov Chain Process in which the transition rate is controlled by an adaptive factor called failure count. If the count variable equal to zero; otherwise, it is equal to one. Afterτiterations, the model will evaluate the current optimal route and compare it to the preserved one. If the current recommendation is better than the preserved one, then the current recommendation will replace the preserved one, and theΓwill return to zero. The transition probability is also equal to zero. Otherwise, current policy update will continue. of check. In this case, we set the transition probability to one. The model will preserve the current recommendation and restart the exploration from the original policy. Importantly, the effectiveness of the self-check mechanism can be theoretically veriﬁed. + α= 1andDis the drop-off area. As suggested in [Lu et al., 2016], we add a parameterωtopto as default to represent the normal case; however, if the pick-up times at an area is less than80%of the ← t+ delivery time for r[count] + cruising time f or r[count]; Γincreases by one and compared to the pre-determined parameterF. IfΓ < F, the update will being equal to the failure countF, it indicates that the current result does not improve for continuousFtimes Proposition 1 enlarges the possibility to locate the optimal solution in the dynamic route optimization problem. Proof 1 recommendation system, suppose that we want to ﬁnd the next optimal pick-up point and and all the states are equally distributed; we set initial state will only be visited once. For the TD learning, since the initial state will only be visited once, and all the states are equally distributed, the probability for choosing the right state of the next pick-up point is: For the self-checking enhanced TD learning method, we assume that there exists a model restarting. If probability for choosing the right state is the same as the classical TD learning method. The probability for such cases is the lower limit for our method. There also exists an extreme case that the model will always restart after Asτ = 1 for our model ATDSC to select the right state is shown as: where 0 ≤ k ≤ b we can conclude that higher possibility to achieve the optimal state. By adjusting proposed self-check mechanism should achieve a better performance than the original version of TD learning. The failure count the failure count is too large, the performance will be improved too slowly because the model has to wait for to restart. If the failure count is too small, then the exploring process of the TD learning will be restricted as the model always restarts without getting enough knowledge of the current environment. To properly select following proposition. Proposition 2 probabilities is inversely proportional to the failure count F Proof 2 The difference of the expected reward E(P whereD assume that reward(P Since we normalize the reward based on the restriction, the value of action-reward probabilities is large, expected rewards. A larger difference indicates that the model needs to do less to explore further. Thus, the failure count can be smaller. On the other hand, considering that a small state values, the model needs to explore more steps for a better decision. Given thatkis the number of restarts during the self-check process, ifk > 0, the self-check mechanism Suppose that the probability for choosing the right state for TD learning and our method ATDSC are denoted p. For proving the proposition, we have to show thatp> pwhenk > 0. In a dynamic route , thenk = bc, and the probability for such case should be the upper limit for our method. The probability F, we can ensure that the model will restart andk > 0. Proposition 1 suggests that our model with the As long as the reward is constrained within a certain scope, the difference between action-reward is the drop-off point beforeP, andDis the drop-off point beforeP. Without loss of generality, we Algorithm 2: ATDSC Input :time limit states M and restart integer c , Q, Q ← 0; ← ANN(area information); ==1 then ← c · ( Proposition 2 offers the rule for determining the value of between expected rewards models in addressing classiﬁcation problems [Guo et al., 2020, 2019, An et al., 2020, Liu et al., 2020, Sun et al., 2019, Zhang et al., 2018], we apply a deep Artiﬁcial Neural Network (ANN) to help decide to change the value of the failure countF pickups in the previous year. We label the records as follows. According to the pandemic outbreak dates reported in news (e.g., The output of the ANN I Proposition 2 also suggests the following pattern regarding the failure count. Lemma 1 As long as normal areas exist, the more the abnormal areas we have, the smaller the failure count will be. Proof 3 Based on the condition that normal areas exist, the difference between the normal area and the abnormal area must be large, reﬂected by the effect of the penalty term. Since we only care about the areas within a given traveling time range, and the expected rewards for the normal areas are assumed to be larger than those of abnormal areas, we ignore the expected reward of abnormal area while guaranteeing to explore all normal areas. Thus, the number of normal areas and negatively proportional to the number of abnormal areas. https://www.prnewswire.com/news-releases/impact-of-covid-19-on-the-taxi-and-limousine-services-market–tbrc-reportinsights-301054745.html t, initial states, failure rateΓ, self-check iterationsτ, learning rateη, discount rateγ, number of ← r; ← Q; . The inputs include the current average successful pickups for each area and the average number of successful ), if the number of abnormal area is more than half of area numbers, we set the label to one, otherwise, zero. Proposition 2 suggests that a large difference of expected rewards can help distinguish between the states. Lemma 1 shows that follows. Failure Count Function. The function for the failure count F where N Algorithm 1 describes how we evaluate the policy. Given the path calculate the expected earning on the path within the given time period. Note that, although our method is designed for sequential route recommendation (long-term), it can also be implemented for a one-step recommendation task. To do so, we can set a small number for the expected working hours (e.g., half-hour). Recall that the expected income is calculated by multiplying the delivery time by the earning rate from historical data. Once the earning is added, the delivery time to the current drop-off area and the cruising time from the current drop-off point to the next pick-up area will all be accumulated. If the accumulated time exceeds the given time variable, the algorithm will stop exploring and output the total proﬁt. The detailed procedure about the update strategy for our method is illustrated in Algorithm 2. The predicted time rangetand the initial state learning initially. Note that during the TD learning, when the number of iterations reaches the multiple of will hold the search until it ﬁnishes comparing the current policy with the preserved one. During the comparison, the evaluation is done by Algorithm 1. If the result is better than the preserved one, the search will continue, and the current one will replace the preserved policy. Otherwise, the preserved policy stays. If the result does not improve after cτ iterations, then the model will launch a new policy exploration. In this section, we discuss the data, experimental settings, and results. Data Description. Our experiments are based on the taxi data from New York City ranges from January 2020 to June 2020, covering the periods before, during, and after the ﬁrst wave of the COVID-19 pandemic outbreak. Our data contain taxi travel records from ﬁve boroughs and a related region of New York City: Bronx, Brooklyn, Manhattan, Queens, Staten Island, and the Newark International Airport (EWR). The data include ﬁelds capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, total payments, payment types, and driver-reported passenger counts. A summary of important data statistics is reported in Table 1. Although we have demonstrated our method using the NYC data, our method can be readily applied to other cities as well. Data Preprocessing. We assume that only neighboring areas are connected, otherwise, they cannot be reached directly. For example, if area A and area C are the neighbors of area B, and area A is not area C’s neighbor, then the trip from A to C should pass through B. The traveling time from A to C is estimated as the average traveling time from A to B plus the average traveling time from B to C. We collect the relations between each area from the Taxi Zone Map. For https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page is the number of normal area; and c is the default value (integer). non-reachable area, the estimation of delivery time and cruising time is mentioned in Section 3. Based on [Dong et al., 2014], we use the historical records of trip distances, total payment, and pick-up and drop-off locations to compute the average earning rate per minute as well as the pick-up/drop-off frequencies of each location. Now we discuss the parameter settings of our method, the implementation of all benchmark methods, and the validation metrics. According to report from the District Department of For-Hire Vehicles, the trips decreased about 90 COVID-19 outbreak. We set The non-connected getting reward probability equal to 0.9. Both the learning rate and the learning rate decay for reinforcement learning are set to 0.01. The total iterations are set to 300,000. All algorithms are implemented in Python, and experiments are conducted on the Seawulf, a high-performance computing cluster core CPUs and 128 GB DDR4 Memory. For a fair comparison, we report the mean performance of 30 independent experiments (indicating 30 recommended paths) based on random initial states. Note that the error bars in ﬁgures represent the standard errors. We compare our method (ATDSC) with four baselines, including REI, MPP, MNP, and PCD, in which the REL can be considered the state-of-the-art method of RL-based route recommendation. The main validation metrics include the hourly and weekly expected income. We also compute standard errors to evaluate the reliability of our results based on 30 independent experiments for path exploring. Expected hourly income. standard errors can be computed as: whereE(·) Daily Expected weekly income. and can estimate the weekly income as: SeaWulf is a computational cluster in Stony Brook University, using top of the line components from Penguin, DDN, Intel, Nvidia, Mellanox and numerous other technology partners. See more information: https://it.stonybrook.edu/help/kb/understanding-seawulf [Ji et al., 2020]. The original method focuses on recommending routes under the guidance of the deep RL method. The model is led by a classic RL method assuming the full knowledge of the environment is known. Hence it cannot be applied to our problem directly. Thus, we implement the RL based on TD learning, with which the full knowledge of the environment is not required. [Yuan et al., 2011]. This method is a greedy method in terms of the pick-up probability. It aims to recommend the area with the maximum pick-up probability to the taxi drivers. The pick-up probability is scratched from the historical data. [Qu et al., 2014]. This method aims to maximize the area proﬁt to the taxi driver. Since we calculate the minute income for each area, this method is equivalent to the greedy method in terms of the minute income. [Luo et al., 2018]. The original method is to recommend the path with minimal potential cruising distance. Since we replace the distance with time, it is equivalent to ﬁnding the pick-up point with minimal cruising time in our setting. is the average expectation function;qis the number of path explorations with random initial states; represents the total daily income for a driver following method meth ∈ [REI, MPP, MNP, PCD, ATDSC]. wherei = 1, 2, ..., 7 E(weekly methods basl ∈ [REI, MPP, MNP, PCD], respectively. To facilitate the comparison, we also report the improvement of our method over other baselines. To validate our method, we demonstrate the results in the views of average hourly income, weekly income, along with an hour by hour case study. 5.3.1 Hourly income comparison. Figure 3 illustrates the performance comparison of the average hourly income in different months. Each reported point represents the average hourly income of 30 paths based on random initial states, and error bars represent the corresponding standard errors. As can be seen, the hourly income was signiﬁcantly reduced in April, May, and June, comparing to January and February. This is what we expect given the COVID-19 outbreak in March. According to the results, Our method ATDSC shows consistent superiority over other benchmarks, for all weekdays and weekends, before and after the COVID-19 outbreak. Although there is a signiﬁcant shrink of income after March for all benchmarks, our method has resulted in a much higher hourly income. Especially, in March (when the COVID-19 had just started), our method has done a good job in maintaining the hourly income close to normal rate; and in June (after two months of COVID-19), our method brings the earnings back to the level of March, while all benchmarks remain in low performance. Since the iterations for REI are not large enough to guarantee convergence, it causes the unstable performance in April and May as it nearly achieves the same performance as other baselines. The difference between our method and REI is credited to the efﬁciency of our self-check mechanism. MNP aims to maximize the area proﬁt with a greedy strategy. Its worse performance indicates that only concentrating on the short term proﬁt will not lead to a long term beneﬁt. The results of the MPP and PCD indicate that maximizing the pick-up probability or reducing the cruising time may not guarantee the maximum earnings for drivers. Now, we report the average weekly income by assuming that a full-time driver works ten hours a day, seven days a week. Related results can be found in Figure 4. While the same income reduction pattern can be observed after the COVID-19 outbreak, our method has a consistently better performance than all benchmarks in terms of proﬁtability. In June, when the estimated incomes are still low based on all benchmarks, our method achieves a similar proﬁt level as in March. This is partially because our method is capable of checking the abnormal areas dynamically to detect the situation change when people are returning to their normal life. The natural logarithmic improvements of our method over other methods are also reported in Table 2, indicating a stable and consistently superior performance of our method. As can be seen that the minimum natural logarithmic improvement in the table is -0.02, indicating a minimum of 98% improvement. Note that taxi drivers usually face high costs when providing services. For example, taxi companies usually charge drivers about one-third of their overall gross fare income. Also, if a passenger pays by credit card, the driver may be charged a minimum 2% transaction fee. Some taxi drivers have to pay auto insurance and maintenance by themselves based on their contracts as independent contractors is approximately 50-60% of the income. Figure 5 compares the performance between ﬁxed and dynamic failure rate settings, based on our method. The results conﬁrm the effectiveness of our adaptive failure rate. According to Proposition 2, the usage of the adaptive parameter will achieve the convergence faster than the ﬁxed parameter case. Now it is also supported by the experimental results. https://work.chron.com/much-fare-taxi-drivers-keep-22871.html Our method will automatically update the failure rate based on the number of detected normal areas. The smaller the normal area number is, the smaller the failure rate will be. 5.3.3 Hour-by-Hour Income. To show more detailed results, we plot the hour by hour income on Monday in Figure 6. We plot the results of our method versus the top-two baseline methods based on the recommendation quality: REI and PCD. For January and March, we ﬁnd the high-proﬁt hours in the morning from 4 AM to 8 AM; from afternoon to evening, high-proﬁt hours are from 4 PM to 8 PM. For the time period during COVID-19, the peak hour in the morning disappears, and the peak hours in the evening are also less proﬁtable. However, our method consistently outperforms the REI and PCD all the time, for virtually all hours and months. Occupancy Rate and Computing Speed occupancy rate. Given the ﬁxed working time for a taxi driver, the smaller the cruising time, the larger the occupancy rate. Thus, we deﬁne the occupancy rate function as: In this case, the reward in our reinforcement learning model is set to the occupancy rate. We calculated the average occupancy rate under 30 different routes in June 2020, and the iteration number for the reinforcement learning model is equal to 200,000. Figure 7 (a) presents the average occupancy rate for different baselines in June 2020. Our method still outperforms all benchmarks, conﬁrming its stability and consistency for different objectives. We also compare the computing time of our method with the best-performance baseline, REI. As shown in Figure 7 (b), the computing time of our method (ATDSC) is nearly the same as REI, indicating that the newly proposed self-check mechanism does not introduce a signiﬁcant computational cost. Considering the quality improvement (e.g., over 200% improvement) obtained from our method, the associated new computational cost is small. Many works have been proposed to recommend a suitable path for taxi drivers based on big data. The work can be categorized into two groups based on the objective function’s ﬂexibility: speciﬁc objective functions and ﬂexible objective functions. The speciﬁc objective function group denotes those papers focusing on optimizing speciﬁc goals for the taxi driver. To save energy, Ge et al. [2010] proposes a novel function called the Potential Travel Distance (PTD) function. Based on the proposed PTD function, they developed two efﬁcient algorithms to ﬁnd the optimal path. By reducing the travel distance, they can help drivers save fuel too. In terms of increasing the number of potential passengers, Yuan et al. [2011] combined the knowledge of passengers’ mobility patterns and taxi drivers’ pick-up behaviors. Their method not only recommends the point with the highest pick-up probability to drivers, but also guide the passengers to the locations where they can easily ﬁnd the vacant taxi. Ye et al. [2018c] introduced a parallel simulated annealing method with domain knowledge to maximize the pick-up probability for several taxi drivers. By communicating and shufﬂing the results after ﬁxed steps, they can ensure that the recommended paths are always the global optimal without intersections. Thus their method can achieve a high speedup compared to the sequential method. Yuan et al. [2010] explored the historical GPS trajectories of a large number of taxis and found the fastest route for the taxi driver to save time for passengers and drivers. They designed a Variance-Entropy-Based Clustering approach to estimate travel time distribution and constructed the practically fastest route based on the estimation. To maximize taxi drivers’ proﬁt, [Qu et al., 2014] designed a cost-effective recommender system for taxi drivers. By evaluating the potential proﬁt with a net proﬁt objective function, they can use a greedy method to ﬁnd the most proﬁtable path for taxi drivers. Tang et al. [2013] analyzed large amounts of GPS location data of taxicabs and found a high-level proﬁt-maximizing strategy for taxi drivers. They treat the problem as a Markov Decision Process (MDP), and the parameters are determined by the historical data. By applying dynamic programming, they captured meaningful rules on how to ﬁnd the passenger. For reducing the total cost in a trip, Zhang et al. [2019] developed a simulated annealing-based parallel method. They spread the best result to each local worker among each communication and ensure that the global optimal can be achieved. More human trajectory-based objective functions have been formulated in [Meng et al., 2019, Liu et al., 2016, 2014, 2012]. Compared with a speciﬁc objective function case, the ﬂexible objective function focuses more on the model itself. As long as the objective function is deﬁned as required, the model can update and generate the required result. Verma et al. [2017] treated the right locations of passengers as a reward to guide the reinforcement learning model. Based on the basic learning mechanism, they provide a dynamic abstraction mechanism to improve the performance. The same setting as our method, Gao et al. [2018] designed a reinforcement learning-based method to optimize taxi driving strategies by maximizing taxi drivers’ proﬁt. The reward was deﬁned as the effective driving time for a driver within a day, the results show that the method not only increases the income of a taxi driver, but also helps the passengers ﬁnd taxi more easily. Ji et al. [2020] designed an effective two-step method with reinforcement learning to deal with the dynamic route recommendation problem. They deﬁne multiple types of rewards: taxi drivers’ average earning, taxis’ average vacant cruising time, passengers’ average waiting time, and passengers being picked up within 30 minutes. For the above methods, they optimize the objective function based on the deﬁnition of reward. If we want to analyze the effect of other criteria, as long as we change the deﬁnition of reward (e.g. from proﬁt to cruising time), we can directly obtain the required result without making any changes to the model. Thus our proposed method in this work belongs to the ﬂexible objective group. Those studies with speciﬁed objective functions can also be transferred to the ﬂexible objective function by deﬁning the key criteria in their objective function as a variable. However, the efﬁciency of the newly deﬁned objective function still needs to be tested. In this paper, we proposed an adaptive reinforcement learning method with a self-check mechanism to solve the dynamic route optimization problem. Our model can detect irregular events (e.g. public health emergence) and automatically update parameters to adapt to a new trafﬁc environment. With a focus on income maximization, the results show that our method can increase at least 98% of the average weekly income for taxi drivers under several experimental settings. We also provide case studies under different evaluation metrics to demonstrate the ﬂexibility and stability of our method. For future work, our method can be extended to parallel versions for further improvement in computational efﬁciency. It will also be interesting to consider vehicle-sharing scenarios for a more complicated user-based method to satisfy real needs.