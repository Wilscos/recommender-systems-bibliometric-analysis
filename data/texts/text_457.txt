Extreme multi-label classication (XMLC) refers to the task of tagging instances with small subsets of relevant labels coming from an extremely large set of all possible labels. Recently, XMLC has been widely applied to diverse web applications such as automatic content labeling, online advertising, or recommendation systems. In such environments, label distribution is often highly imbalanced, consisting mostly of very rare tail labels, and relevant labels can be missing. As a remedy to these problems, the propensity model has been introduced and applied within several XMLC algorithms. In this work, we focus on the problem of optimal predictions under this model for probabilistic label trees, a popular approach for XMLC problems. We introduce an inference procedure, based on the𝐴-search algorithm, that eciently nds the optimal solution, assuming that all probabilities and propensities are known. We demonstrate the attractiveness of this approach in a wide empirical study on popular XMLC benchmark datasets. • Computing methodologies → Super vised learning by classication. extreme classication, multi-label classication, propensity model, missing labels, probabilistic label trees, supervised learning, recommendation, tagging, ranking ACM Reference Format: Marek Wydmuch, Kalina Jasinska-Kobus, Rohit Babbar, and Krzysztof Dembczyński. 2021. Propensity-scored Probabilistic Label Trees. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3404835.3463084 Extreme multi-label classication (XMLC) is a supervised learning problem, where only a few labels from an enormous label space, reaching orders of millions, are relevant per data point. Notable examples of problems where XMLC framework can be eectively leveraged are tagging of text documents [8], content annotation for multimedia search [9], and diverse types of recommendation, including webpages-to-ads [5], ads-to-bid-words [2,19], users-toitems [23,28], queries-to-items [17], or items-to-queries [7]. These practical applications impose new statistical challenges, including: 1) long-tail distribution of labels—infrequent (tail) labels are much harder to predict than frequent (head) labels due to the data imbalance problem; 2) missing relevant labels in learning data—since it is nearly impossible to check the whole set of labels when it is so large, and the chance for a label to be missing is higher for tail than for head labels [11]. Many XMLC models achieve good predictive performance by just focusing on head labels [22]. However, this is not desirable in many of the mentioned applications (e.g., recommendation and content annotation), where tail labels might be more informative. To address this issue Jain et al. [11]proposed to evaluate XMLC models in terms of propensity-scored versions of popular measures (i.e., precision@𝑘, recall@𝑘, and nDCG@𝑘). Under the propensity model, we assume that an assignment of a label to an example is always correct, but the supervision may skip some positive labels and leave them not assigned to the example with some probability (dierent for each label). In this work, we introduce the Bayes optimal inference procedure for propensity-scored precision@𝑘for probabilistic classiers trained on observed data. While this approach can be easily applied to many classical models, we particularly show how to implement it for probabilistic label trees (PLTs) [12], an ecient and competitive approach to XMLC, being the core of many existing state-of-the-art algorithms (e.g., Parabel [18], extremeText [24], Bonsai [15], AttentionXML [25], napkinXC [13], and PECOS that includes XR-Linear [26] and X-Transformers [7] methods). We demonstrate that this approach achieves very competitive results in terms of statistical performance and running times. In this section, we state the problem. We rst dene extreme multilabel classication (XMLC) and then the propensity model. LetXdenote an instance space, and letL = [𝑚]be a nite set of𝑚class labels. We assume that an instance𝒙 ∈ Xis associated with a subset of labelsL⊆ L(the subset can be empty); this subset is often called the set of relevant or positive labels, while the complementL\Lis considered as irrelevant or negative for 𝒙. We identify the setLof relevant labels with the binary vector 𝒚 = (𝑦, 𝑦, . . . ,𝑦), in which𝑦=1⇔ 𝑗 ∈ L. ByY = {0,1}we denote the set of all possible label vectors. In the classical setting, we assume that observations(𝒙,𝒚)are generated independently and identically according to a probability distributionP(𝒙, 𝒚)dened on X×Y. Notice that the above denition concerns not only multi-label classication, but also multi-class (when∥𝒚∥=1) and𝑘-sparse multi-label (when∥𝒚∥≤ 𝑘) problems as special cases. In case of XMLC we assume𝑚to be a large number (e.g.,≥10), and∥𝒚∥to be much smaller than 𝑚, ∥𝒚∥≪ 𝑚. The problem of XMLC can be dened as nding a classier 𝒉(𝒙) = (ℎ(𝒙), ℎ(𝒙), . . . , ℎ(𝒙)), from a function classH:X → R, that minimizes the expected loss or risk: whereℓ (𝒚,ˆ𝒚)is the (task) loss. The optimal classier, the so-called Bayes classier, for a given loss functionℓis:𝒉= arg min𝐿(𝒉) . In the case of XMLC, the real-world data may not follow the classical setting, which assumes that(𝒙,𝒚)are generated according toP(𝒙, 𝒚). As correct labeling (without any mistakes or noise) in case of an extremely large label set is almost impossible, it is reasonable to assume that positive labels can be missing [11]. Mathematically, the model can be dened in the following way. Let𝒚be the original label vector associated with𝒙. We observe, however, ˜𝒚 = (˜𝑦, . . . ,˜𝑦) such that: P(˜𝑦= 1 | 𝑦= 1) = 𝑝, P(˜𝑦= 0 | 𝑦= 1) = 1 − 𝑝,(2) P(˜𝑦= 1 | 𝑦˜𝑦= 0 | 𝑦= 0) = 1 , where𝑝∈ [0,1]is the propensity of seeing a positive label when it is indeed positive. All observations in both training and test sets do follow the above model. The propensity does not depend on𝒙. This means that for the observed conditional probability of label𝑗, we have: Let us denote the inverse propensity by𝑞, i.e.𝑞=. Thus, the original conditional probability of label 𝑗 is given by: 𝜂(𝒙) = P(𝑦= 1 | 𝒙) = 𝑞P(˜𝑦= 1 | 𝒙) = 𝑞˜𝜂 Therefore, we can appropriately adjust inference procedures of algorithms estimating˜𝜂(𝒙)to act optimally under dierent propensity-scored loss functions. Jain et al. [11]introduced propensity-scored variants of popular XMLC measures. For precision@𝑘 it takes the form: whereˆLis a set of𝑘labels predicted by𝒉for𝒙. Notice that precision@𝑘 (𝑝@𝑘) is a special case of 𝑝𝑠𝑝@𝑘 if 𝑞= 1 for all 𝑗. We dene a loss function for propensity-scored precision@𝑘as ℓ= −𝑝𝑠𝑝@𝑘. The conditional risk for ℓis then: The above result shows that the Bayes optimal classier for 𝑝𝑠𝑝@𝑘is determined by the conditional probabilities of labels scaled by the inverse of the label propensity. Given that the propensities or their estimates are given in the time of prediction,𝑝𝑠𝑝@𝑘is optimized by selecting𝑘labels with the highest values of𝑞˜𝜂(𝒙). Conditional probabilities of labels can be estimated using many types of multi-label classiers, such as decision trees, k-nearest neighbors, or binary relevance (BR) trained with proper composite surrogate losses, e.g., squared error, squared hinge, logistic or exponential loss [1,27]. For such models, where estimates of˜𝜂(𝒙) are available for all𝑗 ∈ L, application of the Bayes decision rule for propensity-scored measures is straightforward. However, in many XMLC applications, calculating the full set of conditional probabilities is not feasible. In this section, we introduce an algorithmic solution of applying the Bayes decision rule for𝑝𝑠𝑝@𝑘to probabilistic label trees (PLTs). We denote a tree by𝑇, a set of all its nodes by𝑉, a root node by 𝑟, and the set of leaves by𝐿. The leaf𝑙∈ 𝐿corresponds to the label𝑗 ∈ L. The parent node of𝑣is denoted bypa(𝑣), and the set of child nodes byCh(𝑣). The set of leaves of a (sub)tree rooted in node 𝑣 is denoted by 𝐿, and path from node 𝑣 to the root by Path(𝑣). A PLT uses tree𝑇to factorize conditional probabilities of labels, 𝜂(𝑥) = P(𝑦=1|𝒙),𝑗 ∈ L, by using the chain rule. Let us dene an event thatLcontains at least one relevant label in𝐿:𝑧= (|{𝑗: 𝑙∈ 𝐿} ∩ L| >0). Now for every node𝑣 ∈ 𝑉, the conditional probability of containing at least one relevant label is given by: where𝜂(𝒙, 𝑣) = P(𝑧=1|𝑧=1, 𝒙)for non-root nodes, and 𝜂(𝒙, 𝑣) = P(𝑧=1| 𝒙)for the root. Notice that (6) can also be stated as recursion: and that for leaf nodes we get the conditional probabilities of labels: To obtain a PLT, it suces for a given𝑇to train probabilistic classiers fromH:R↦→ [0,1], estimating𝜂(𝒙, 𝑣)for all𝑣 ∈ 𝑉. We denote estimates of𝜂byˆ𝜂. We index this set of classiers by the elements of 𝑉as 𝐻 = {ˆ𝜂(𝑣) ∈ H : 𝑣 ∈ 𝑉}. An inference procedure for PLTs, based on uniform-cost search, has been introduced in [12]. It eciently nds𝑘leaves, with highest ˆ𝜂(𝒙)values. Since inverse propensity is larger than one, the same method cannot be reliably applied to nd leaves with the𝑘highest products of𝑞andˆ˜𝜂(𝒙). To do it, we modify this procedure to an 𝐴-search-style algorithm. To this end we introduce cost function 𝑓 (𝑙, 𝒙) for each path from the root to a leaf. Notice that: 𝑞ˆ˜𝜂(𝒙) = exp−− log 𝑞−logˆ˜𝜂(𝒙, 𝑣). (9) This allows us to use the following denition of the cost function: 𝑓 (𝑙, 𝒙) = log𝑞− log 𝑞−logˆ˜ where𝑞= max𝑞is a natural upper bound of𝑞ˆ˜𝜂(𝒙)for all paths. We can then guide the A*-search with functionˆ𝑓 (𝑣, 𝒙) = 𝑔(𝑣, 𝒙) + ℎ(𝑣, 𝒙), estimating the value of the optimal path, where: is a cost of reaching tree node 𝑣 from the root, and: is a heuristic function estimating the cost of reaching the best leaf node from node 𝑣. To guarantee that 𝐴-search nds the optimal solution—top-𝑘labels with the highest𝑓 (𝑙, 𝒙)and thereby top-𝑘 labels with the highest𝑞ˆ˜𝜂(𝒙)—we need to ensure thatℎ(𝑣, 𝒙)is admissible, i.e., it never overestimates the cost of reaching a leaf node [21]. We also would like ℎ(𝑣, 𝒙) to be consistent, making the 𝐴-search optimally ecient, i.e., there is no other algorithm used with the heuristic that expands fewer nodes [21]. Notice that the heuristic function assumes that probabilities estimated in nodes in a subtree rooted in𝑣are equal to 1. Sincelog1=0, the heuristic comes to nding the label in the subtree of𝑣with the largest value of the inverse propensity. Algorithm 1 outlines the prediction procedure for PLTs that returns the top-𝑘labels with the highest values of𝑞ˆ˜𝜂(𝒙). We call this algorithm Propensity-scored PLTs (PS-PLTs). The algorithm is very similar to the original Uniform-Cost Search prediction procedure used in PLTs, which nds the top-𝑘labels with the highestˆ𝜂(𝒙). The dierence is that nodes in PS-PLT are evaluated in the ascending order of their estimated cost valuesˆ𝑓 (𝑣, 𝒙)instead of decreasing conditional probabilitiesˆ𝜂(𝒙). Theorem 1.For any𝑇, 𝐻, 𝒒, and𝒙the Algorithm 1 is admissible and optimally ecient. Proof. 𝐴-search nds an optimal solution if the heuristicℎis admissible, i.e., if it never overestimates the true value ofℎ, the cost value of reaching the best leaf in a subtree of node𝑣. For node 𝑣 ∈ 𝑉 , we have: ℎ(𝑣, 𝒙) = log𝑞− log max𝑞−logˆ˜𝜂(𝒙, 𝑣) . Sinceˆ˜𝜂(𝒙, 𝑣) ∈ [0,1]and thereforelogˆ˜𝜂(𝒙, 𝑣) ≤0, we have that ℎ(𝑣, 𝒙) ≥ ℎ(𝑣, 𝒙), for all 𝑣 ∈ 𝑉, which proves admissibility. 𝐴-search is optimally ecient ifℎ(𝑣, 𝒙)is consistent (monotone), i.e., its estimate is always less than or equal to the estimate for any child node plus the cost of reaching that child. Since we have thatmax𝑞≥ max𝑞, and the cost of reaching 𝑣frompa(𝑣)is− log(ˆ˜𝜂(𝒙, 𝑣))which is greater or equal 0, it holds that ℎ(pa(𝑣), 𝒙) ≤ ℎ(𝑣, 𝒙) − log(ˆ˜ The same cost function𝑓 (𝑙, 𝒙)can be used with other tree inference algorithms (for example discussed by Jasinska-Kobus et al. [13]), including beam search [16], that is approximate method for nding𝑘leaves with highestˆ𝜂(𝒙). It is used in many existing label tree implementations such as Parabel, Bonsai, AttentionXML and PECOS. We present beam search variant of PS-PLT in the Appendix. In this section, we empirically show the usefulness of the proposed plug-in approach by incorporating it into BR and PLT algorithms and comparing these algorithms to their vanilla versions and stateof-the-art methods, particularly those that focus on tail-labels performance: PFastreXML [11], ProXML [4], a variant of DiSMEC [3] with a re-balanced and unbiased loss function as implemented in PW-DiSMEC [20] (class-balanced variant), and Parabel [18]. We conduct a comparison on six well-established XMLC benchmark datasets from the XMLC repository [6], for which we use the original train and test splits. Statistics of the used datasets can be found in the Appendix. For algorithms listed above, we report results as found in respective papers. Since true propensities are unknown for the benchmark datasets, as true𝒚is unavailable due to the large label space, for empirical evaluation we model propensities as proposed by Jain et al. [11]: where𝑁is the number of data points annotated with label𝑗in the observed ground truth dataset of size𝑁, parameters𝐴and𝐵are specic for each dataset, and𝐶 = (log 𝑁 −1)(𝐵 +1). We calculate propensity values on train set for each dataset using parameter values recommended in [11]. Values of𝐴and𝐵are included in Table 1. We evaluate all algorithms with both propensity-scored and standard precision@𝑘. Algorithm 1 PS-PLT.PredictTopLabels(𝑇, 𝐻, 𝒒, 𝒙, 𝑘) ˆ𝒚 = 0, 𝑞= max𝑞ˆ𝒚 vector to all zeros, 𝑞 , 𝒙) = − logˆ˜𝜂 (𝒙, 𝑟 ˆ𝑓 (𝑟, 𝒙) = 𝑔(𝑟, 𝒙) + log 𝑞− log max𝑞 Table 1: PS-PLTs and PLTs compared to other state-of-the-art algorithms on propensity-scored and standard precision@{1,3,5} [%]. The best result for each measure is in bold. The best result in the group of sub-linear methods (the last 4 methods) is underlined. Table 2: PS-PLT and PLT average CPU train and prediction time compared to other state-of-the-art algorithms. We modied the recently introduced napkinXC [13] implementation of PLTs,which obtains state-of-the-art results and uses the Uniform-Cost Search as its inference method. We train binary models in both BR and PLTs using the LIBLINEAR library [10] with 𝐿-regularized logistic regression. For PLTs, we use an ensemble of 3 trees built with the hierarchical 2-means clustering algorithm (with clusters of size 100), popularized by Parabel [18]. Because the tree-building procedure involves randomness, we repeat all PLTs experiments ve times and report the mean performance. We report standard errors along with additional results for popular 𝐿-regularized squared hinge loss and for beam search variant of PS-PLT in the Appendix. The experiments were performed on an Intel Xeon E5-2697 v3 2.6GHz machine with 128GB of memory. The main results of the experimental comparison are presented in Table 1. Propensity-scored BR and PLTs consistently obtain better propensity-scored precision@𝑘. At the same time, they slightly drop the performance on the standard precision@𝑘on four and improve it on two datasets. There is no single method that dominates others on all datasets, but PS-PLTs is the best sub-linear method, achieving best results on𝑝𝑠𝑝@{1,3,5}in this category on ve out of six datasets, at the same time in many cases being competitive to ProXML and PW-DiSMEC that often require orders of magnitude more time for training and prediction than PS-PLT. In Table 2, we show CPU train and test times of PS-PLTs compared to vanilla PLTs, PfasterXML, ProXML and PW-DiSMEC on our hardware (approximated for the last two using a subset of labels). In this work, we demonstrated a simple approach for obtaining Bayes optimal predictions for propensity-scored precision@𝑘, which can be applied to a wide group of probabilistic classiers. Particularly we introduced an admissible and consistent inference algorithm for probabilistic labels trees, being the underlying model of such methods like Parabel, Bonsai, napkinXC, extremeText, AttentionXML and PECOS. PS-PLTs show signicant improvement with respect to propensityscored precision@𝑘, achieving state-of-the-art results in the group of algorithms with sub-linear training and prediction times. Furthermore, the introduced approach does not require any retraining of underlining classiers if the propensities change. Since in realworld applications estimating true propensities may be hard, this property makes our approach suitable for dynamically changing environments, especially if we take into account the fact that many of PLTs-based algorithms can be trained incrementally [12,14,24,25]. Computational experiments have been performed in Poznan Supercomputing and Networking Center.