Graph neural networks (GNNs) have been widely applied in the recommendation tasks and have obtained very appealing performance. However, most GNN-based recommendation methods suer from the problem of data sparsity in practice. Meanwhile, pre-training techniques have achieved great success in mitigating data sparsity in various domains such as natural language processing (NLP) and computer vision (CV). Thus, graph pre-training has the great potential to alleviate data sparsity in GNN-based recommendations. However, pre-training GNNs for recommendations face unique challenges. For example, user-item interaction graphs in dierent recommendation tasks have distinct sets of users and items, and they often present dierent properties. Therefore, the successful mechanisms commonly used in NLP and CV to transfer knowledge from pre-training tasks to downstream tasks such as sharing learned embeddings or feature extractors are not directly applicable to existing GNN-based recommendations models. To tackle these challenges, we delicately design an adaptive graph pre-training framework for localized collaborative ltering (ADAPT). It does not require transferring user/item embeddings, and is able to capture both the common knowledge across dierent graphs and the uniqueness for each graph. Extensive experimental results have demonstrated the eectiveness and superiority of ADAPT. CCS Concepts: • Information systems → Collaborative ltering. Additional Key Words and Phrases: Graph Neural Networks, Recommendation Systems, Model Pre-training. ACM Reference Format: Yiqi Wang, Chaozhuo Li, Zheng Liu, Mingzheng Li, Jiliang Tang, Xing Xie, Lei Chen, and Philip S. Yu. 2021. An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering. 1, 1 (December 2021), 23 pages. https://doi.org/10.1145/1122445.1122456 1 INTRODUCTION Recommendation is one of the most ubiquitous and successful applications of articial intelligence in our daily life. It has been widely adopted in various online services such as target advertising and online shopping. Basically, recommendation systems aim at predicting a user’s preference based on her historical interactions with dierent items, and further recommending the user some items that she may have potential interest in [4]. Many existing solutions for recommendations follow the paradigm to rst learn a set of latent factors (i.e., embeddings for users and items) and then build an interaction function to make recommendation decisions based on the learned embeddings. Matrix factorization (MF) is one representative method of such techniques. It aims at learning user and item embeddings directly from the user-item interaction matrix and then makes predictions via inner product over the user and item embeddings. In recent years, we have witnessed increasing eorts on incorporating deep neural networks to advance these techniques via rening the user and item embeddings [19] and modeling interaction functions [13, 44]. The interactions among users and items in a recommendation task can be naturally denoted as a user-item bipartite graph. From this perspective, the key for a recommendation task is to learn the node representations from the user-item bipartite graph. Graph neural networks (GNNs), which generalize deep neural networks (DNNs) to graph data, have been theoretically and empirically proved to be very powerful in representation learning for graph data [50,59]. Therefore, there is increasing attention on adopting GNNs in addressing recommendation tasks. GNNs are able to inherently capture important high-order user-item connectivity in a given user-item interaction bipartite graph, as a consequence, they can boost the recommendation performance. For example, NGCF [46] proposed to propagate embeddings over user-item graph based on the message-passing framework of GNNs and achieved signicant performance improvement; and LightGCN [18] further rened the design of GNNs for recommendation tasks by eliminating the feature transformation and non-linear activation, and achieved the state of the art performance. Despite achieving great success in recommendation tasks, we empirically found that current GNN-based recommendation methods suer from a common and practical problem: data scarcity. In other words, when the user-item interaction graphs are sparse, the performance of most existing GNN-based recommendations will tend to drop substantially. Unfortunately, historical user-item interactions in the real-world recommendations are often scarce [4]. The challenge of data scarce is also universal in other domains such as NLP [38] and CV [17,23], where pre-training techniques have been proposed to alleviate this problem. Typically, a model is rst pre-trained on a large dataset with abundant label information (either self-supervised label or supervised label), and then netuned over the downstream datasets with limited label information. Pre-training has been demonstrated to be eective in alleviating the data scarcity issue of the downstream task by transferring knowledge from the pre-training data. Thus, it is natural to ask: can we also leverage pre-training techniques to facilitate GNN-based recommendation models? Adopting pre-training for existing GNN-based recommendations faces unique challenges. The great success of pre-training in NLP and CV relies on the eective mechanisms that enable knowledge sharing or transferring from the pre-trained tasks to the downstream tasks. In NLP, the vocabulary of words or tokens is shared; therefore, both contextindependent embeddings (e.g., word2vec [33]) and context-sensitive embeddings (e.g., GPT [39] and BERT [9] from pre-training tasks can be transferred. In CV, low-/mid-level features (such as edges, textures) given by the pre-trained model can be leveraged by the ne-tuned tasks. However, user and item embeddings denote the major parameters of existing GNN-based recommendation models. Moreover, in recommendations, dierent interaction graphs often do not have the same sets of users and items. Therefore, these uniquenesses determine that the eective mechanisms from NLP and CV are not applicable to existing GNN-based recommendations. In addition, a large amount of data is essential for the eectiveness of pre-training [ typically pre-trained on tens of millions of images [ than 1000M words [ achieving this goal is challenging given that dierent graphs have distinct properties such as the number of items (or users) and graph density. As a consequence, dedicated eorts are required to tackle these unique challenges. In this work, we propose an that provides eective solutions to address the aforementioned challenges. It consists of two key components: a meta localized GNN (or meta-LGNN) model and the GNN adaptor. The meta-LGNN model provides a new perspective to build GNN-based recommendations, where it is trained to make recommendation predictions based on the neighboring structures of the target user and item, instead of learning a set of user and item embeddings and an interaction function. The rationality of this design is: the key collaborative ltering information of a given target user and target item can be encoded by their neighboring structure, which consists of their historical interactions. With meta-LGNN, there is no need to transfer user or item embeddings across dierent graphs. To leverage multiple pre-training graphs with distinct properties, we design the GNN adaptor as a strategy to capture their dierences. Specically, given a recommendation task, the GNN adaptor can adapt the meta-LGNN model to a customized GNN model by considering the properties of its corresponding interaction graph. In summary, our contributions can be summarized as follows: •We design a meta-LGNN model which is trained to make recommendation predictions based on the neighboring structure around the target user and item. Compared to the existing GNN-based recommendations which require to learn a set of user and item embeddings, this design paves a way to enable pre-training for GNN-based recommendations from a new perspective. •We propose an adaptive pre-training framework based on meta-LGNN. It allows pre-training with multiple user-item interaction graphs while considering the uniqueness of each graph. •We have conducted extensive experiments on various datasets, and the empirical results demonstrate the eectiveness and superiority of the proposed framework. The remaining of the paper is organized as follows. In Section 2, we describe the proposed framework in details, including the problem denition, framework overview, the basis model and two key processes. We introduce the experiments to validate the eectiveness in Section 3, including experimental settings, preliminary study, performance comparison, ablation study and further probing. In Section 4, we then review some important related work. We conclude the paper with discussions on future work in Section 5. 2 THE PROPOSED FRAMEWORK In this section, we rst formally dene the GNN pre-training problem for GNN-based recommendations. Then we describe an overview of the proposed ADAPT. Next, we introduce the novel GNN recommendation method for localized collaborative ltering as the basis of the proposed framework. Finally, we detail the pre-training process and the ne-tuning process. 9]. Therefore, it is desired to take advantage of many interaction graphs for pre-training. However, 2.1 The GNN Pre-Training Problem on Recommendations We rst briey describe the problem studied in this paper. It is common that user-item interaction graphs are sparse in the real-world recommendations [4]. It is undoubtedly very challenging to perform GNN-based recommendations on a sparse user-item interaction graph. Meanwhile, there exist a large amount of user-item bipartite graphs from other recommendation tasks, which contain abundant information and knowledge. A natural idea to mitigate the data sparsity problem is to transfer useful information from existing graphs to facilitate the recommendation task on the target sparse graph. Recent years have witnessed the great success of the pre-training techniques to transfer knowledge from a large amount of data to help solve the target task in NLP and CV [9,39]. Therefore, in this paper, we mainly focus on leveraging the pre-training strategy to address the data sparsity challenge in GNN-based recommendations. Next, we formally dene the pre-training problem for GNN-based recommendations. We denote the user-item interaction data in a recommendation task as a user-item bipartite graph𝐺 = (𝑈 , 𝐼, 𝐸), where𝑈 = {𝑢, 𝑢, · · · , 𝑢} represents the user set,𝐼 = {𝑖, 𝑖, · · · , 𝑖}is the item set, and𝐸 ⊂ 𝑈 × 𝐼indicates interactions between users and items. Each element𝑒in𝐸suggests that there exists an interaction between the user𝑢and the item𝑖. We assume that there are𝑁graphs for pre-training that are denoted{𝐺, 𝐺, · · · , 𝐺}. We further use𝐺= (𝑈, 𝐼, 𝐸)to denote the target graph for the downstream recommendation task. Generally, a GNN model consists of 𝐿 layers, where the key component in each layer 𝑙 is a graph convolution with parameter𝜽. Thus, we can denote the parameters for a GNN model as𝚯= {𝜽, · · · , 𝜽}. With above denitions, the goal of the pre-training task is to train a model from graphs{𝐺, 𝐺, · · · , 𝐺}that can help build a GNN model G(·; 𝚯) for 𝐺from the downstream recommendation task. 2.2 An Overall Design of ADAPT An overview of ADAPT is demonstrated in Figure 1. It consists of two key components – a meta-LGNN model and a GNN adaptor. One key challenge for pre-training GNNs for recommendations is that user-item interaction graphs in dierent recommendation tasks have distinct user/item sets; as a result, it is hard to directly transfer user/item embeddings across dierent recommendation graphs like pre-trained word embeddings in NLP. To solve this challenge, we propose the meta-LGNN model. It provides a new perspective for GNN-based recommendations, where the recommendation prediction is made based on the local structure surrounding a user and an item, rather than their embeddings, via a GNN model. Therefore, it does not need to learn embeddings of users and items, which is naturally more exible for pre-training, compared to most existing GNN-based recommendation methods. The success behind pre-training is that there exists common knowledge that can be transferred from the pre-training data to the downstream task. On the one hand, there are similar patterns of user-item local structures in dierent recommendation graphs, which are captured by the proposed meta-LGNN model. However, there could exist dierent patterns for these graphs since dierent graphs can present distinct properties. Therefore, ADAPT provides the GNN adaptor that can adapt the meta-LGNN model to a customized GNN model for the interaction graph of a given recommendation task by considering its distinct properties. With these two components, ADAPT rst trains a GNN adaptor and a meta-LGNN model simultaneously in the pre-training phase. Then, given the target graph from the downstream recommendation task in the ne-tuning phase, we generate a customized GNN model based on the properties of the target graph via the GNN adaptor and the meta-LGNN model. In the following subsections, we rst introduce the proposed GNN recommendation method for localized collaborative ltering in ADAPT. Next, we describe the pre-training process and the ne-tuning process of ADAPT in detail. Fig. 1. An overview of the proposed framework ADAPT. There are two key components in ADAPT – a meta-LGNN model and a GNN adaptor. The GNN adaptor is designed to adapt the meta-LGNN model to a customized GNN model for the interaction graph of a given recommendation task by considering its distinct properties. With these two components, ADAPT first trains a GNN adaptor and a meta-LGNN model simultaneously in the pre-training phase. Then, given the target graph from the downstream recommendation task in the fine-tuning phase, we generate a customized GNN model based on the properties of the target graph via the GNN adaptor and the meta-LGNN model. 2.3 GNN Recommendation Method for Localized Collaborative Filtering Graph Neural Network(GNN) models have been demonstrated eectiveness and superior in facilitating recommendation tasks [18, embeddings via a GNN model. Given that user-item interaction graphs from dierent recommendation tasks have distinct user/item sets, these GNN-based recommendation methods are impractical to be pre-trained, since user/item embeddings can not be transferred across dierent graphs. To take advantage of the power of both GNN methods and the pre-training techniques, we propose a new GNN-based localized collaborative ltering (meta-LGNN), which does not require to learn user/item embeddings and makes predictions based on local structures extracted from the interaction graphs. Next, we will rst briey introduce the most common existing GNN-based recommendation method, and then illustrate the meta-LGNN. 46,52,54]. Most existing GNN-based recommendation methods aim at learning a set of user and item H = {h, · · · , h, h, · · · , h}to denote the user and item embeddings the model aims to learn.𝚯= }represents the parameters to be learned for a𝐿-layer GNN model, where𝜽denotes the parameters Fig. 2. The proposed GNN recommendation method based on localized collaborative filtering. Given a target user𝑢and a target item 𝑖, we first extract a local graph at these two target nodes from the user-item interaction graph, of which the structure is denoted asA. Next, we generate a positional aribute for each node based on their minimum distances towards the target nodes, and these node aributes are denoted asX. Then, we can leverage a GNN model to get the graph representation for this aributed graph. Finally, we can compute a recommendation score𝑠based on the local graph representation via a scoring function to make the final prediction for the given user and item pair. rst aggregates item embeddings from its neighborhood, and then updates the target user embedding based on its original embeddings and the aggregated embeddings. The update process of item embeddings works similarly as that for a user node. We formalize the user node embedding update process via the graph convolution in the𝑙-th layer of GNN for illustration as follows: where𝑓represents the graph convolution operation and∀𝑝 ∈ N (𝑢)denotes any item𝑝belonging to the one-hop neighborhood of user𝑢. Note thath= handh= h. The rened user embeddinghand the item embeddingh outputted by the nal layer of the GNN model are used to represent each user and item in a bipartite graph, and then typically we use the inner product over the user embeddinghand item embeddinghto make the recommendation prediction. Overall, these GNN-based recommendation methods aim at learning the user/item embeddingsHand the GNN model parameters𝚯simultaneously. Since interaction graphs from dierent recommendation tasks have distinct sets of users and items, apparently it is not feasible to transfer the learned embeddings, i.e., H. In this paper, in order to ease pre-training for GNN-based recommendations, we propose a new GNN recommendation method based on localized collaborative ltering (or meta-LGNN). The intuition is that the key collaborative ltering information for recommendation is encoded in the historical interactions. Given a specic user and item pair, their key collaborative information is also encoded in their historical interactions. Thus it is reasonable to make a recommendation prediction based on the local structure consisting of their historical interactions. The framework of meta-LGNN is shown in Figure 2. For a target user and item pair, we rst extract a local graph at these two target nodes from the user-item interaction graph. Next, we generate a positional attribute for each node based on their minimum distances towards the target nodes. Then, we can leverage a GNN model to get the graph representation for this attributed graph. Finally, we can compute a recommendation score based on the local graph representation via a scoring function to make the nal prediction for the given user and item pair. Given a target user𝑢and a target item𝑖in a user-item interaction graph𝐺, we perform two random walks [34] starting from these two nodes on𝐺, separately. Note that the random walk strategy we adopt in our work is with the restarting mechanism, and thus this process can be regarded as a neighbor sampling process, where we can get two sampled neighboring node sets an overall node set the local structure around the given pair to ensure the scalability of our model on large graphs. Apart from graph structure attributes model transferability, we proposed to use Double-Radius Node Labeling (DRNL) [ attributes for each user or item in a graph, rather than to use the existent node attributes, which are very likely to miss or be dierent across various recommendation tasks. DRNL labels each node based on its minimum distances towards the target user can be calculated as: where𝑑 𝑑 = (𝑑+ 𝑑 the target user and item from other nodes. Now we have the graph structure and the node positional attributes of 𝐺, thus, we can use a GNN model update process in each GNN layer also follows Eq. 1. we use matrix outputted by the nal GNN layer, where graph representation for 𝐺 Note that the positional attributes are used as the input node features, i.e., is no need to learn the user/item embeddings. Finally, a score function is applied to compute a score for the user-item pair(𝑢, 𝑖) eliminating the user/item embeddings H. 2.4 The ADAPT Pre-training In this subsection, we will introduce the overall pre-training process of the proposed ADAPT. In this pre-training phase, one key challenge is how to learn the common transferable knowledge across multiple interaction graphs and capture their dierences simultaneously. To solve this challenge, we equip ADAPT with a GNN adaptor. Specically, the GNN adaptor is able to adapt the meta-LGNN model to a customized GNN model particularly for the interaction graph in one recommendation task. Both the GNN adaptor and the meta-LGNN model are optimized in the pre-training process. Next, we detail the GNN adaptor component, the adaption process and the pre-training process. 2.4.1 The GNN Adaptor. The goal of the GNN adaptor is to generate customized adapting parameters to adapt the metaLGNN model for a given graph. Given a specic graph and outputs customized adapting parameters properties such as graph size, graph density, and degree assortativity coecient. As discussed before, suppose that the meta-LGNN model consists of 𝑁= N∪ N, and nally we can get a local graph𝐺based on the set𝑁to reveal X, so that we can use a GNN modelG(·;𝚯)to get a meaningful representation for𝐺. For the 𝑢and the target item𝑖on the local graph𝐺. Given a user/item node𝑡, its positional attribute𝑥 denotes the minimum distance between𝑢and𝑡, and𝑑denotes the minimum distance between𝑖and𝑡. )is the sum of two minimum distances. Note that we denote𝑥=1 and𝑥=1, so that we can distinguish and𝑑denotes the embedding dimension. Then, the GNN model leverages a pooling operation to get the to make a prediction. Overall, meta-LGNN aims at only learning the GNN model parameters𝚯while Fig. 3. The adaptation process. Given a specific graph𝐺, the GNN adaptor takes its graph property vectorpas input, and outputs customized adapting parametersΦfor𝐺. Suppose that the meta-LGNN model consists of𝐿GNN layers, we can denote its parameter as𝚯= {𝜽, · · · , 𝜽}, where𝜽∈ Rand𝑑denotes the dimension of node embeddings outpued by𝑙-th GNN layer. The GNN adaptor will generate𝐿adapting parameters corresponding to these GNN layers for𝐺, which can be denoted as𝚽= {𝝓, · · · , 𝝓}, where𝝓∈ R. For the graph convolution in the𝑙-th GNN layer of the customized GNN model for graph 𝐺, its parameters 𝝓are adapted via 𝝓on 𝜽. where𝜽∈ Rand𝑑denotes the dimension of node embeddings outputted by𝑙-th GNN layer. The GNN adaptor will generate𝐿adapting parameters corresponding to these GNN layers for𝐺, which can be denoted as 𝚽= {𝝓, · · · , 𝝓}, where𝝓∈ R. The GNN adaptor can be modelled using any functions ofp. Specically, for the graph convolution in the𝑙-th GNN layer for𝐺, the GNN adaptor generates its corresponding adapting parameters as follows: where𝝎denotes the parameters of the GNN adaptor function𝑎𝑑𝑎()corresponding to the graph convolution in the 𝑙-th GNN layer. In our work, we implement𝑎𝑑𝑎()as a feed-forward neural network. Overall, we can summarize the adapting parameters generation process for 𝐺as follows: where𝐴𝐷𝐴()consists of all adaptor functions for dierent GNN layers and𝛀 = {𝝎, · · · , 𝝎}represents the parameters for all the GNN adaptors. 2.4.2 The Adaptation Process. In our work, the meta-LGNN Model can be arbitrary GNN models including GCN [26], GIN [51] and etc [12,32,56]. The meta-LGNN Model can be directly applied on a given the user-item interaction graph. However, as aforementioned, there exist both similarities and dierences among the user-item interaction graphs in dierent recommendation tasks. Thus to preserve similarities and dierences simultaneously, we do not directly apply the same meta-LGNN model to all the recommendation graphs in the pre-training process. Instead, we utilize the GNN adaptor to generate a customized GNN model for each recommendation graph based on the meta-LGNN model and its graph properties. The adaptation process is illustrated in Figure 3. Specically, for the graph convolution in the𝑙-th GNN layer of the customized GNN model for graph 𝐺, its parameters are adapted as follows: where⋄denotes an adaptation operation. In the current proposed model, we adopt FiLM [ Specically, we split the adapting parameters where⊙denotes the element-wise multiplication between two matrices. By applying the adaptation operation on GNN layers of the meta-LGNN model, we can get the customized GNN model {𝜽, · · · , 𝜽 Then, we utilize the customized GNN model to generate graph embedding for the local graph from 𝐺 2.4.3 The Pre-training Process. In the pre-training phase, suppose that there are {𝐺, 𝐺, · · · , 𝐺 (𝑢, 𝑖) ∈ 𝐸 and then we use a scoring function to compute a recommendation score for (𝑢, 𝑖) based on h where𝜹is the linear transformation weights to be learned in the scoring function. We adopt the pairwise BPR loss [ in ADAPT. The BPR loss is one of the most popular objective functions in recommendation tasks, which measures the relative order of the positive node pairs and negative node pairs. The positive node pairs are user-item interactions observed in graphs, while the negative node pairs are non-existent user-item interactions generated by negative sampling. Specically, BPR assumes that the recommendation score of the positive node pairs should be higher than the corresponding negative ones. The objective function follows: whereO = represents the existent interaction set between users and items in overall pre-training process is summarized in Algorithm 1. Given a set of interaction graphs used for pre-training {𝐺, 𝐺, · · · , 𝐺 initialize the meta-LGNN model, the GNN adaptor and the scoring function. Next, in each training epoch (from line 3 to line 12), a random graph adaptation parameters . Then, we can formalize the adapting process of 𝜽for 𝐺as follows: for any user-item pair (𝑢, 𝑖) as follows: }from dierent recommendation tasks. For any graph𝐺, we can generate a customized GNN model )based on the meta-LGNN model and graph propertiespvia the GNN adaptor. For any user-item pair , we utilizeG(;𝚯)to compute a graph representationhfor its corresponding local graph𝐺, Ð{(𝑢, 𝑖, 𝑖)| (𝑢, 𝑖) ∈ 𝐸, (𝑢, 𝑖) ∈ 𝐸}denotes the pre-training data for the graph set{𝐺, 𝐺, · · · , 𝐺}.𝐸 }, batch size𝑏and the amount of interactions sampled for pre-training𝑁, we rst randomly Input: A set of interaction graphs from dierent recommendation tasks: {𝐺, 𝐺, · · · , 𝐺}; Batch size 𝑏;Í Sample amount 𝑁(Note that 𝑁<|𝐸|.) Output: The meta-LGNN model G(; 𝚯); the GNN adaptor 𝐴𝐷𝐴(; 𝛀); The scoring function 𝑠𝑐𝑜𝑟𝑒 (; 𝜹) generated especially for𝐺. Furthermore, we randomly sample𝑏user-item interactions from the pre-training graph𝐺 and generate their negative counterparts, and then the meta-LGNN model, the GNN adaptor and the scoring function are updated via minimizing the BPR loss on these samples. The training process will continue until the objective function is converged. 2.5.1 The Fine-tuning Objective Function . In the ne-tuning phase, we also adopt the BPR loss. However, in this phase, we aim at optimizing the parameters of the customized GNN model. The formulation is listed below: where𝐸= {(𝑢, 𝑖, 𝑖)| (𝑢, 𝑖) ∈ 𝐸, (𝑢, 𝑖) ∈ 𝐸}denotes the training data for the target graph𝐺.𝐸represents the set of existent interactions between users and items in 𝐺and 𝐸is the non-existent interaction set generated manually. 2.5.2 The Fine-tuning Strategies. With the GNN adaptor and the meta-LGNN model from the pre-training phase, we can conduct model ne-tuning specially for a target recommendation graph 𝐺. We design two ne-tuning strategies and illustrate them in Figure 4. •The rst ne-tuning strategy is named as direct ne-tuning. As shown in the top subgure of Figure 4, given a target downstream graph𝐺, we rst compute the customized adapting parameters𝚽= 𝐴𝐷𝐴(𝑝;𝛀)based on its property vector𝑝via the GNN adaptor𝐴𝐷𝐴(;𝛀), then generate the customized GNN model𝚯= Fig. 4. The fine-tuning strategies. We design two fine-tuning strategies – direct fine-tuning and joint fine-tuning, which are denoted as ADAPT-D and ADAPT-J, separately. In ADAPT-D, we generate the customized GNN model the GNN adaptor and the meta-LGNN, and we fine-tune the customized GNN model meta-LGNN model 𝚯 denote ADAPT with this ne-tuning strategy as ADAPT-D. •The second ne-tuning strategy is named as joint ne-tuning. As shown in the bottom subgure of Figure 4, given a target downstream graph adaptor the model for a test sample in the test process so that the model can be better adapted for the test data. We use ADAPT-J to indicate ADAPT with the joint ne-tuning strategy. In this section, we conduct extensive experiments to validate the eectiveness of the proposed ADAPT. We rst introduce the experimental settings. Then we illustrate how the sparsity of the recommendation graphs aects the performance of existent GNN-based recommendation methods. Next, we evaluate the performance of ADAPT and representative baselines on various real-world datasets. We conduct the ablation study to understand the importance of the GNN adaptor. Finally, we investigate the impact of graph sparsity and the number of pre-training graphs on the performance of ADAPT. 𝛀on𝐺. This ne-tuning strategy is inspired by test-time training [42], which is proposed to ne-tune Fig. 5. Properties of pre-training graphs. Note that each circle represents a graph and its size denotes the density of the graph. 3.1 Experimental Seings In this subsection, we introduce the experimental settings including datasets, baselines and evaluation metrics. 3.1.1 Datasets. In this work, we conduct experiments on datasets from two real-world applications: Tianchi [3] and MovieLens [2]. Specically, we construct numerous user-item interaction graphs from dierent recommendation scenarios based on the item category from these two applications. For each application, we select some interaction graphs as pre-training graphs and some of them as the target downstream graphs. The statistics of the pre-training graphs are demonstrated in Figure 5 and these of the downstream graphs are summarized in Table 1. As shown in Figure 5, we select 60 interaction graphs from Tianchi and 6 interaction graphs from MovieLens for pre-training, and these graphs show very diverse properties. We briey introduce the Tianchi dataset and the MovieLens dataset as follows: • Tianchi: It is a user behavior dataset from Alibaba (one of the biggest on-line shopping platforms in China), which consists of millions of user-item interactions, such as clicking, liking and purchasing. Each interaction record includes user ID, item ID, behavior type, item category ID and interaction timestamp. In our work, we use Tianchi-𝐼𝐷 to denote the user-item interaction graph whose items belong to the category 𝐼𝐷. • MovieLens: This is a movie rating dataset from MovieLens (a popular movie recommendation website), which consists of user rating records for thousands of movies from dierent categories. Each rating record includes user ID, movie ID, rating, and movie category. In our work, we use MovieLensgraph whose movies come from the category 𝑋 . 3.1.2 Baselines. In this subsection, we describe representative baseline methods from two groups. The rst group includes GNN-based recommendation methods and a classic CF method. Particularly, we select NGCF and LightGCN since they are two of the most representative GNN-based recommendation methods, and LightGCN is one of the state-of-the-art models. MF is chosen because it is one of the most classic and popular recommendation methods. The second group includes existing pre-training methods for GNNs. Though there are a few pre-training methods for GNNs GCC for recommendations as the representative baseline since it only relies on the topological information and can be pre-trained on multiple graphs as the proposed ADAPT does. We have not chosen the pre-training strategies proposed in [20] because there are no node attributes or graph labels in our scenarios. Similarly, GPT-GNN [ included because it also requires node attributes and cannot be applied across multiple graphs. The details of these baselines are presented as follows: • NGCF embeddings. Specically, it designs propagation layers to aggregate information from connected nodes. The recommendation prediction is made based on the rened user and item embeddings. • LightGCN particularly for recommendation tasks. Specically, it eliminates feature transformation and nonlinear activation, and only includes neighborhood aggregation in GNNs for collaborative ltering. • MF item embeddings directly from user-item interactions, and then makes recommendation predictions based on the inner product of the user embedding and the item embedding. • GCC discrimination task as the pre-training task and it utilizes contrastive learning to empower GNNs to learn transferable structural representations across multiple graphs. Note that GCC is not specially designed for recommendation tasks. To apply GCC into the recommendation scenarios, we rst pre-train a GCN model via the GCC framework on the pre-training graphs. Then we build a scoring function model specic to recommendations in the ne-tuning phase. The ne-tuning objective function is the same as ADAPT. Note that since we do not focus on the cold-start problem, we do not include the pre-training work in [ baseline. 3.1.3 Evaluation Metrics. For the downstream recommendation tasks, we divide its corresponding user-item interactions into three sets: the training set, the validation set and the test set. Note that each of the validation set and the test set has 5 percent of the total samples. To avoid the cold-start problem, we constrain that all the users and items in the validation set and the test set should exist in the training set. In the ne-tuning phase, we use the validation set to select the best model and then report its performance in the test set. For each user-item interaction in the validation or the test set, we generate 49 non-existent user-item interactions for the user. In the model evaluation, we rst compute a recommendation score for each interaction, and then we rank these scores. We calculate the Hit Rate (HR) of the model prediction based on if the score of the real user-item interaction is on the top 5 among all the 50 user-item interactions. For each experiment, we average and report the model performance with 5 seeds in terms of HR. ], the majority of them are not designed specically to recommendations. Thus, we adapt a recent method : NGCF [46] proposes to encode high-order connectivity among user-item interaction graphs into user/item : LightGCN [18] is one of the state-of-the-art GNN-based recommendation methods. It tailors GNN : MF [27] is one of the most classic and popular recommendation methods. It learns a set of latent user and : GCC [37] is a self-supervised pre-training framework for GNNs. GCC designs a subgraph instance Fig. 6. The performance of NGCF and LightGCN vs. the graph sparsity on two Tianchi dataset datasets. (The reported performance is measured with HR in %) Table 2. The recommendation performance comparison with 60% as training. (The reported performance is measured with HR in %) 3.1.4 Implementation Details. In our work, we implement the meta-LGNN model as a 3-layer graph convolutional networks and the scoring function as a linear transformation layer. For the input of the GNN adaptor, we compute eight graph properties via the networkX package [14], including the number of nodes, the number of edges, the user-item ratio, the graph density, the degree assortativity coecient, robins-alexander clustering coecient, the number of connected components and the global eciency. Note that it is straightforward to include more graph properties. We use the Adam [25] optimizer. The batch size is set to be 256. The learning rate is chosen from[0.0005,0.001], and the dropout rate is chosen from [0.0, 0.1, 0.2, 0.3]. 3.2 Preliminary Study In this subsection, we study how the performance of two representative GNN-based recommendation methods, i.e., NGCF and LightGCN, is aected by the sparsity of interaction graphs. Specically, we increase the sparsity of interaction graphs by randomly removing parts of the training edges while xing the test edges. The results are shown in Figure 6 where𝑥 ∈ {1,2,3,4}(i.e., the x-axis) is used to denote the graph sparsity and a larger𝑥indicates a sparser graph. Both LightGCN and NGCF show a signicant decreasing trend in terms of recommendation performance with the increase of graph sparsity. This empirically demonstrates that GNN-based recommendation methods suer from data sparsity. This motivates us to take advantage of pre-training techniques to alleviate this problem. Table 3. The recommendation performance comparison with 40% as training. (The reported performance is measured with HR in %) 3.3 Performance Comparison We compare the proposed ADAPT and baselines described aforementioned on ve datasets, including four item categories from Tianchi and one movie category from MovieLens. The recommendation methods are divided into two groups: 1) the training from scratch group consists of MF, NGCF and LightGCN, which are directly trained from scratch on the target downstream graphs; and 2) the pre-training & ne-tuning group consists of GCC and the proposed ADAPT with two ne-tuning strategies, i.e., ADAPT-D and ADPAT-J. To ease the comparison, we also show the best performance of methods from these two groups, and compute the performance gain of the pre-training & ne-tuning group over the training from scratch group. Note that in order to simulate the data scarcity scenarios in real-world recommendation applications, after the validation set and the test set are determined, we randomly remove some interactions from the remaining interactions under the constraint of introducing no isolated users or items. Specically, we tailor two training sets: One retains 60 percent of the remaining interactions; and the other one consists of 40 percent interactions. The comparison results on these two scenarios are shown in Table 2 and Table 3, respectively. We can make the following observations: •NGCF and LightGCN often perform better than MF. This observation is consistent with previous observations in [18, 46]. •The proposed ADAPT frameworks can achieve great performance improvement over the recommendation models trained from scratch on the target downstream data. This demonstrates the eectiveness of ADAPT in alleviating the data scarcity problem in recommendation tasks. •ADAPT achieves signicantly better performance than GCC. Compared to GCC, ADAPT is designed specically to the pretraining task for recommendations with the recommendation BPR objective and the adaptor to capture the dierences among pre-training graphs. More investigations on the importance of the adaptor will be discussed in the following subsection. •Both the direct ne-tuning strategy and the joint ne-tuning strategy are eective. The joint ne-tuning strategy is empirically shown to be more eective than the direct ne-tuning strategy in most cases. This observation could indicate that ne-tuning the adaptor in the test time has the potential to benet the performance of the downstream recommendation task. This observation is consistent with that in [42]. •The performance gain of ADAPT from the best baseline performance is more signicant under the 40% than 60%. This observation suggests that pre-training is a promising solution to tackle the data sparsity problem in recommendations. 3.4 Ablation Study We conduct an ablation study to investigate the eectiveness of the GNN adaptor. The results are shown in Table 4. We use ADAPT-w/o-adaptor to denote the ADAPT framework without the GNN adaptor. We pre-train two model instances of ADAPT-w/o-adaptor with 60 pre-training graphs and 1 pre-training graph, respectively. We use ADAPTw/o-adaptor-scratch to indicate that ADAPT-w/o-adaptor is trained from scratch with only the downstream graph. ADAPT-best denotes the ADAPT framework with any ne-tuning strategy that can achieve better performance. We can make the following observations: •The overall performance of ADAPT-best is signicantly better than that of the ADAPT-w/o-adaptor and ADAPTw/o-adaptor-scratch. This validates (1) the eectiveness of the GNN adaptor for pre-training and (2) the importance of pre-training. •The ADAPT-w/o-adaptor instance pre-trained on 60 pre-training graphs performs even worse than the instance pre-trained on a single pre-training graph. This shows that it is necessary to capture the dierences among multiple pre-training graphs, and the GNN adaptor in the proposed ADAPT can capturethese dierences in the pre-training process. •Both the ADAPT-w/o-adaptor instances cannot beat ADAPT-w/o-adaptor-scratch. This indicates that pre-training is not always helpful. It can even introduce performance degradation if not carefully designed. To further demonstrate the importance of pre-training, we check if ADAPT can transfer knowledge from the pretraining graphs to the downstream recommendation task. To achieve this goal, we directly use the models generated from the pre-training without ne-tuning. In particular, we compare the performance of three dierent variants of the proposed GNN recommendation method for localized collaborative ltering. They include a randomly-initialized model, the meta-LGNN model from the pre-training phase, and the customized-GNN model generated by the meta-LGNN model and the GNN adaptor especially for the target dataset. The results are demonstrated in Table 5. Overall, the meta-LGNN model performs better than the randomly-initialized model, and the performance of the customized-GNN model is signicantly better than that of the other two variants. These observations demonstrate that the meta-LGNN model learns some useful knowledge from the pre-training process, and the GNN adaptor eectively adapts the meta-LGNN model to the customized one. The customized-GNN model performs remarkably better than the randomly-initialized model. This provides direct evidence that ADAPT successfully transfers knowledge from pre-training graphs to the downstream recommendation task. Table 5. Performance comparison of three dierent variants of the proposed GNN recommendation method for localized collaborative filtering. (The reported performance is measured with HR in %) 3.5 Further Probing In this subsection, we further probe the proposed framework by exploring the following two problems: 1) how does the sparsity of the target downstream graph aect the ADAPT performance? and 2) how does the number of pre-training graphs inuence the ADAPT performance? Fig. 7. The performance of the proposed ADAPT with varied sparsity of the target downstream graph. Note that the x-axis is used to denote the graph sparsity. The larger the sparsity is, the sparser the target downstream graph is. 3.5.1 Sparsity Analysis. In Figure 7, we show how the performance of ADAPT and two GNN-based recommendation methods, NGCF and LightGCN, changes with the change of graph sparsity. Specically, we gradually increase the graph sparsity by randomly removing some interactions from the training set under the constraint of introducing no isolated users or items, and meanwhile, the test set and validation set remain unchanged. It is observed that the proposed ADAPT performs much more stable with the increase of the graph sparsity, compared to NGCF and LightGCN. This observation further demonstrates the appealing of pre-training to mitigate the data sparsity problem in recommendations. 3.5.2 The Number of Graphs for Pre-training. In order to explore the inuence of the number of pre-training graphs on the model performance, we pre-train four ADAPT models on one single graph, 6 graphs, 60 graphs and 300 graphs, respectively. Then we ne-tune these models on four target downstream graphs. Note that for the ADAPT model pre-trained on multiple graphs, we report its best performance among two ne-tuning strategies. For the ADAPT model pre-trained on one single graph, it is not applicable to pre-train the GNN adaptor, thus we report its performance without the GNN adaptor. As shown in Figure 8, the model performance rst increases when the number of pre-training graphs increases and then it decreases consistently on all the datasets when the number of graphs is 300. This demonstrates that it is potentially benecial to increase the number of pre-training graphs for the downstream performance. However, too many pre-training graphs may introduce noise that could hurt the downstream performance. 4 RELATED WORK Our work is related to graph neural networks, pre-training for graph neural networks and recommendation models based on graph Neural Network. Next, we briey review representative methods from each category. 4.1 Graph Neural Networks In recent years, increasing attention and eorts have been devoted into graph neural networks, which successfully extend deep neural networks to graph data. Graph neural networks are theoretically and empirically demonstrated to be very powerful in graph representation learning [15,26,43], and have achieved great success in various applications from dierent domains, such as natural language processing [6,22,53], computer vision [36,47] and recommendation [18, 24,46,49,52,54,60]. There are mainly two groups of graph neural networks: the spectral-based methods and the spatial-based methods. In [41], the rst graph neural network is proposed from the spatial perspective to solve both graph and node level tasks, which aggregates information from neighboring nodes for each node in every layer. Subsequently, Bruna et al. [5] proposes to generalize the convolution operation to the graph domain based on graph Laplacian theory from the spectral perspective. Following this work, Deerrard et al [ uses chebyshev polynomials to modulate the graph Fourier coecients for dierent graph signals. Next, Kipf and Welling [26 It is remarkable that despite developed from the spectral perspective, GCNs can also be well illustrated in a spatial way. From then on, multiple spatial-based graph neural networks have been proposed. A comprehensive overview about GNNs can be found in recent surveys [ further explore the rationale behind GNNs, such as Ma et al [ as graph signal denoising. In addition to the aforementioned work mainly focusing on graph convolution operation, there are also numerous works targeting at graph pooling operation, which summarizes graph representation from node representations and plays an essential role in graph representation learning. There are simple pooling methods, such as directly averaging all the node representations as the graph representation [ connects to all the nodes in the graph and then taking its node representation as the graph representation [ there are increasing number of hierarchical pooling methods proposed to learn graph representation hierarchically, which are believed to be able to better capture the graph structure information. Specically, DiPool [ learn a dierentiable soft cluster assignment for each node at every GNN layer. Inspired by encoder-decoder model, graph U-Net [ gPool can adaptively select important nodes based on the importance scores computed on a learnable projection vector. Furthermore, RepPool [ the information of both the important nodes and normal nodes .In addition, EigenPooling [ perspective of graph Fourier transform and it can leverage both the node features and the local structures. 4.2 Pre-training for Graph Neural Networks Inspired by the great success of pre-training techniques in multiple domains such as computer vision and natural language processing [ appropriately in GNN models [ learning, which focuses on pre-training a GNN model at both the node level and graph level, so that the pre-trained model can learn eective node representations and graph representations simultaneously. Qiu et al. proposes GCC [ GNN pre-trainng framework based on contrastive learning, which aims at capturing transferable topological knowledge across multiple graphs. GPT-GNN [ information and the semantic information via generating node attributes and edges alternatively. In addition, some work focus on bridging the gap between GNN pre-training and GNN ne-tuning. For example, L2P-GNN [ mimic ne-tuning during the pre-training process via a dual-adaption mechanism at both the node level and graph level. Besides, there also emerges GNN pre-training work specially for alleviating cold-start problem in recommendation tasks [16], where researchers proposes to use embedding reconstruction as pre-training task, so that the cold-start user/item can get a good representation from the reconstruction knowledge. 4.3 Graph Neural Networks for Recommendation Systems Recommendation tasks are to predict a user’s preference based on its historical interactions with various items. Historical user-item interactions can be directly denoted as a bipartite graph, and thus it is very natural to apply graph neural networks to recommendation tasks. In fact, there exist numerous works focusing on exploring GNNs for ] further simplies ChebNet via some assumptions and proposes graph convolutional networks (GCNs). 12] is designed to consist of graph pooling (gPool) and graph unpooling (gUnpool) operations, where 21] is a GNN pre-training framework built on generative model. It aims at capturing both the structure recommendation systems and many of them have achieved promising performance [11,18,45,46,55]. PinSage [55] is designed especially for recommendation tasks based on GraphSage, and can be directly applied to a web-scale recommendation tasks. For each user or item node, it utilizes a random-walk based sampling method to sample some neighboring nodes for information aggregation. NGCF [46] is designed to explicitly capture high-order connectivity in user-item interaction graphs via embedding propagation. Later on, He et al. proposes LightGCN [18], which simplies the design of GNNs especially for recommendation tasks via eliminating feature transformation and non-linear activation function, and has achieved signicant performance improvement. Furthermore, Wu et al. [48] explored to improve the performance of GNN models for recommendations by incorporating self-supervised learning techniques. This work can be supplementary to most supervised GNN models for recommendations, which aim at improving user and item representations via self-discrimination. These GNN-based recommendation methods only relying on the user-item interaction graphs. There are also works focusing on utilizing GNNs to deal with side information, such as social networks and knowledge graphs, to facilitate recommendation performance. For examples, GraphRec [11] is proposed to use two graph attention networks to learn user embeddings and item embeddings, and the user embedding is learned from both the social graph and interaction graph. KGAT [45] is proposed to integrate the user-item interaction graph and the knowledge graph into one unied graph by viewing the user-item interaction as one type of the relations, and to use attentive embedding propagation layers to rene embeddings over this unied graph. 5 CONCLUSION In this paper, we propose an adaptive graph pre-training framework for localized collaborative ltering, ADAPT, which can eectively help alleviate the data scarcity challenge in recommendation tasks. There are two key components in the proposed ADAPT: the meta-LGNN and the GNN adaptor. Specically, the meta-LGNN is a novel GNN-based recommendation method from a new perspective of local structure. It aims at encoding the collaborative ltering information into the graph representation for the neighboring structure of a given user-item pair, and it does not require learning user/item embeddings. Thus it is more exible for pre-training compared to existing GNN-based recommendation methods. The GNN adaptor is the key to allow the eectiveness of taking advantage of multiple graphs for pre-training. It can capture the dierence for each graph, and adapt the meta-LGNN model to the customized GNN model accordingly. Overall, the proposed ADAPT is able to transfer common knowledge from the pre-training graphs for the target downstream recommendation graph, and to capture their uniqueness in the meanwhile. Extensive experiments have validated the eectiveness and superiority of the proposed ADAPT in GNN pre-training for recommendations. There are a few directions we can further explore in the future as follows. First, the current implementation of the meta-LGN is based on GCN, which is a vanilla model in graph representation learning. The performance of the meta-LGN could be further improved by replacing the GCN model with other advanced GNN models such as GIN. Second, the node labeling method currently adopted by the meta-LGN treats user and item nodes equally. In the further exploration, we may consider to distinguish the user and item nodes in the node labeling method. Third, the designs of the adaptor model and adaptation operation are also exible, and there are some alternative models and operations that can further be explored on the proposed framework. Last but not least, instead of pre-training the proposed framework on numerous totally independent user-item interaction graphs, we may consider how to leverage some available shared user or item nodes among theses graphs.