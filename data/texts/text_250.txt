Keywords: low-rank approximation, matrix completion, soft-impute, alternating least squares Low-rank matrix approximation (LRMA) has always attracted a lot of attention. It arises in a variety of core machine learning techniques including data compression, dimension reduction and de-noising (Markovsky 2011), and is central to many multivariate statistical techniques such as principal components, linear discriminant analysis and canonical correlation analysis (Mardia et al. 1979). The main goal of the LRMA technique is to identify the “best” way to approximate some given matrix M ∈ R X ∈ R matrix and its approximation. The most natural one is to use the residual sum of squares Here k · k be found via the singular value decomposition (SVD) of the matrix M (Eckart & Young 1936). The LRMA problem can be extended to the case when only a subset of entries in M is observed. The resulting technique, also known as low-rank matrix completion (LRMC), aims to recover the missing entries of matrix M from partial observations (Cand`es & Recht 2008, Mazumder et al. 2010, Nguyen et al. 2019). One of the most famous examples of widespread use of LRMC is the “Netﬂix” competition, where the goal is to build a movie recommendation system based on users ratings of movies they have seen (Feuerverger et al. 2012). Here M is the matrix of movie ratings (score from one to ﬁve), where each column represents a movie, each row represents a user, and most elements in M are missing. We seek a low-rank matrix X that approximates the observed entries of M well, and can be used to ﬁll in the missing values. The low-rank structure has a heuristic justiﬁcation in terms of “cliques” of users and “genres” of movies. denotes a set of observed entries by Ω ∈ {1, . . . , n} ×{1, . . . , p} and deﬁnes the projection operator P In other words, the projection operator replaces all missing elements by zeroes. The LRMC problem becomes The objective in (2) is nothing but the residual sum of squares computed for the set of observed values only, i.e. explicit solution exists, LRMC requires an iterative algorithm. Hard-impute is an example of such an algorithm, and alternates two steps: the imputation step, where the current guess for X is used to complete the unobserved entries in M; and the SVD step which updates the low-rank matrix approximation X (Mazumder et al. 2010). of convergence of the algorithm. A natural convex relaxation of the LRMC problem can be stated as follows (Mazumder et al. 2010, Candes & Tao 2010): Following Candes & Tao (2010), to state the corresponding optimization problem one Note that problem (2) is non-convex, which makes it hard to establish a formal proof Here k · k relaxation of the matrix rank function. The iterative algorithm solving (3) is called softimpute. It is essentially the same as hard-impute, but instead of truncating some eigenvalues to zero at the second step (i.e. computing low-rank approximation) they are shrunk via a soft-thresholding operator (Mazumder et al. 2010). There is a link between problem (2) and its convex relaxation. Since shrinking its nuclear norm is a smooth way to reduce the rank of a matrix, then for λ large enough problem (3) will produce a rank k solution. that any rank k matrix X ∈ R For the convex relaxation the following result holds: if r is the rank of the solution to problem (3), then for any k > r problem (3) is equivalent to Hastie et al. (2015) suggest an eﬃcient alternating least squares algorithm for solving problem (5). In this paper we focus on a weighted generalization of LRMA. There are many applications of WLRMA beyond matrix completion. In Ecology, Robin et al. (2019) and Lukasz Kidzi´nski, Hui, Warton & Hastie (2020) propose modelling populations of species using Poisson GLMs. The authors analyse the population matrices (with rows and columns corresponding to sites and species, respectively) using low-rank models. As a result, they An alternative view of LRMA was suggested by Rennie & Srebro (2005). One can show . Restating problem (2) in terms of A and B leads to an equivalent bi-convex problem: develop an iterative algorithm that solves a matrix-type Poisson GLM optimization problem with a low-rank constraint. The algorithm is based on Newton descent where each Newtons step can be restated in the form of a WLRMA problem. reconstruction. They model the so-called contact matrix, which represents the frequency of contacts between each pair of genomic loci, via a Poisson distribution and link mean contact counts to the pairwise distances between loci. The low-rank constraint is natural here, since chromatin is a 3D structure; in addition the authors model chromatin as a smooth curve. The resulting optimization problem can be solved via an iterative algorithm, where each iteration is equivalent to solving a WLRMA problem. multiplying each element of a matrix either by zero (for unobserved entries) or by one (for observed entries). In other words, P a binary matrix with and ∗ refers to the Hadamard element-wise product. The natural generalization of the LRMC problem is therefore In Tuzhilina et al. (2020) the authors build a technique for chromatin conformation Note that the projection operator introduced in the previous section is equivalent to Of course, there is a convex relaxation of problem (6), i.e. Razenshteyn et al. 2016; Dutta et al. 2018; Ban et al. 2019). Originally the authors suggested a simple iterative algorithm that successively replaces the current guess of X by a low-rank-matrix approximation of W ∗ M + (1 − W ) ∗ X. This iterative procedure has a natural interpretation: you ﬁrst “blend” your current guess for X with the ground truth M (using weights W and 1 − W ) and then project the combination back to the manifold of rank-k matrices. With a matrix of binary weights, this procedure coincides with the hard-impute approach discussed above. (hereafter we call it baseline) as well as the soft- and hard-impute algorithms to projected gradient descent. In Section 3.1 we use this link to propose an acceleration technique for the baseline method based on Nesterov acceleration. Further we provide an alternative way to improve the speed of WLRMA convergence: we restate the original problem as a ﬁxed point equation and combine projected gradient descent with Anderson acceleration. We discuss these ideas as well as the way to stabilize the convergence behavior of Anderson acceleration in Sections 3.2 and 5. All of the proposed algorithms require us to compute an SVD at each iteration, which is computationally expensive in high-dimensions. In Section 6 we discuss an adaptation of the proposed techniques to the high-dimensional data setting. Finally, we demonstrate and compare the performance of all the methods on a simulation example as well as a real-data example from recomendataion systems (see Sections 4 and 7). When we ﬁnished writing this paper we discovered a recent preprint, which shares some of the ideas we propose in Sections 3.1 and 6 (Dutta et al. 2021). The WLRMA problem was ﬁrst introduced by Srebro & Jaakkola (2003) (see also This paper is organized as follows. In Section 2 we link the existing WLRMA approach In this section we link the WLRMA algorthm proposed in Srebro & Jaakkola (2003) to proximal gradient descent. This connection will subsequently help us to develop several useful algorithmic extensions. Note that the same link can be established for the convex relaxation of the WLRMA problem, as well as the soft- and hard-impute approaches. convex optimization problems (Combettes & Wajs 2005; Combettes & Pesquet 2011). Speciﬁcally, suppose you aim to minimize g(x) + h(x) w.r.t. x ∈ R convex and diﬀerentiable and h(x) is convex and not necessarily diﬀerentiable. For the non-diﬀerentiable part we deﬁne the proximal operator as follows: If x iteration i then the proximal gradient descent update is The update can be split in two parts: ﬁrst the standard gradient step is done using the diﬀerentiable part of the objective, then the non-diﬀerentiable part is taken into account via proximal mapping. we ﬁrst restate problem (6) as follows: Proximal gradient descent (PGD) is a method that allows one to solve non-diﬀerentiable refers to some initialization of x and xis the value of the optimized parameter at Now we link PGD to the WLRMA problems (6) and (7). For the non-convex version Here C = {X ∈ R is the set indicator function. Since indicator is a non-diﬀerentiable function, we set g(X) = operator of an indicator function of a set is just the projection onto this set (in the case of the set constraint the algorithm is therefore referred as Projected Gradient Descent). Further, note that projecting onto the set of rank k matrices is equivalent to solving LRMA problem (1). Denote the rank k approximation of matrix X by SVD is the singular value decomposition then Computing the gradient of the diﬀerential part of the objective, i.e. we get the following PGD update for the WLRMA problem: Although motivated by the EM algorithm, the update suggested in Srebro & Jaakkola (2003) coincides with the one we just derived via PGD. non-diﬀerentiable function of X, we set W ∗ (M − X)kand h(X) =(X). It is not diﬃcult to prove that the proximal Now let us consider the convex relaxation. Since in problem (7) the nuclear norm is a One can show that proximal operator for h(X) in this case is the soft-threshold operator (see, for example, Mazumder et al. 2010), i.e. prox and x procedure leading to the hard and soft updates respectively. Although optimizing the learning rate may improve convergence, it can be quite expensive to run line search in the context of WLRMA, as a separate SVD is required for each value of t. There exist several ways to accelerate proximal gradient descent. We discuss two in this section: Nesterov and Anderson acceleration. PGD can be combined with Nesterov acceleration, which uses momentum from previous iterations (Nesterov 1983). Speciﬁcally, if we denote = max(0, x) is the threshold operator. This leads us to the PGD update as follows: Similar to standard gradient descent, it is possible to add a learning rate t to the PGD then the PGD procedure for both non-convex and convex versions of the WLRMA is as follows: Here we propose an alternative way to improve the convergence of the WLRMA baseline method via Anderson acceleration. To make it concise we present this technique for the nonconvex WLRMA problem only. The same extension can be done for the convex relaxation as well. to be very eﬃcient for accelerating the convergence of ﬁxed-point iterations (see, for example, Anderson 1965, Walker & Ni 2011, Zhang et al. 2020). It is similar in ﬂavor to most acceleration methods which combine the previous updates of the optimized parameter to improve the convergence speed. Given some function f(x) : R depth m, Anderson acceleration solves the ﬁxed point equation x = f(x) via the following steps: Anderson acceleration (AA), or Anderson mixing, is a well-known method demonstrated Iteration 0: set x= f(x). Iteration i = 1, 2, . . .: (a) If i < m we replace m by m= min{m, i}. This step is required only for the ﬁrst few iterations. (b) Store updates in a matrix F=f, . . . , fwhere f= f(x). Note that the optimization problem has an explicit solution which can be found in two steps. First we solve equation R the solution α = cedure as a ﬁxed point equation. Following the idea suggested in the recent paper by Mai & Johansson (2020) we restate the WLRA update as Note that Y simply represent the imputed matrix M. Then the convergence for the baseline WLRMA is guaranteed when the following ﬁxed-point equation (written in terms of the auxiliary variable Y ) holds (c) Store residuals in a matrix R=r, . . . , rwhere r= f− x. (d) Find coeﬃcients α = (α, . . . , α)such thatα= 1 and that minimize the linear combination of residuals, i.e. kRαk. (e) Combine previous updates setting x= Fα. To combine WLRA with Anderson acceleration we ﬁrst need to restate the PGD prolem is stated in matrix form, so in order to run Anderson acceleration in the context of WLRMA we need to vectorize all the matrices. We will use lower case letters to denote ﬂattened matrices, e.g. y = vec(Y ) and f = vec(f (Y )). This leads us to the following implementation of the WLRMA-AA algorithm. In order to build similar algorithm for the convex relaxation one replace SVD Note that in Anderson acceleration x is assumed to be a vector and the WLRA prob- 1.1. Initialize Y(e.g. column mean imputation) and y= vec(Y). 1.2. Compute X= SVD(Y). 2.1. Compute f= vec(W ∗ M + (1 − W ) ∗ X). 2.2. Set r= f− y 2.3. Concatenate R= [R: r= [F: f]. 2.4. Solve OLS: 2.5. Compute y= Fα and set Y= matrix(y). 2.6. Update X= SVD(Y). 2.7. Drop the ﬁrst column of both Fand Rif they contain more than m columns. One can ﬁnd a link between the proposed algorithm and Nesterov acceleration. First note that since Thus if we denote which is nothing but some linear combination of previous X updates, then step 2.6 in the above algorithm will coincide with the second step of the Nesterov update (12). Several tricks are usually applied to improve the performance of Anderson acceleration. • Since the loss function is not guaranteed to decrease after each Anderson update, one can run guarded Anderson acceleration. In other words, for each step we compare the loss improvement between the simple PGD and accelerated one and choose the update with better improvement. Alternatively, since the most unstable behavior of Anderson acceleration is demonstrated in the ﬁrst few iterations where the gradient has the largest norm, one can delay the acceleration by a few iterations and start after the most signiﬁcant drop in the gradient was passed. • Note that the only information required for computing the Anderson coeﬃcients α is the covariance matrix of the residuals R, which costs O(npm). Although in most applications depth is usually chosen to be relatively small (3−30, see Anderson 1965), this computation can be quite expensive if X is high-dimensional and can exceed the SVD cost if one of n or p is less than m. Since only one column of Ris updated at each iteration, one can keep and successively update the residual covariance matrix instead of re-computing it after each loop. This reduces the cost to O(npm). In this section we compare the proposed three methods, i.e. baseline, Nesterov and Anderson WLRMA, on a small simulation example. We generate the data as follows. First we draw A ∈ R M = AB generate the matrix of weights W ∈ R we set n = 1000, p = 100, r = 70 and σ = 1. We initialize X the three suggested algorithms, i.e baseline, Nesterov and Anderson WLRMA, we solve problems (6) and (7). For the latter approach, we set depth equal to three. For non-convex WLRA we test k = 20, 50, 70. To make the results compatible, for the convex relaxation we search for λ that results in approximately the same value of the unpenalized loss (see optimization problem (6)). Thus we vary λ in the grid λ = 100, 30, 5. We track the relative change in the loss, i.e. Here for the WLRMA problem and for the convex relaxation. We stop the algorithm as soon as the relative diﬀerence is less than some threshold, i.e. ∆ is limited to 300 and the convergence threshold is set to  = 10 acceleration methods reach the convergence points faster than the baseline. Moreover, The results are presented in Figure 1. According to our simulation experiments, both Anderson improves the convergence speed much more than Nesterov in the case of the convex relaxation. Note also that the convergence of accelerated methods for non-convex WLRA is more erratic comparing to the baseline. In this section we propose a way to stabilize the behavior of Anderson acceleration in the case of non-convex WLRMA. First, let us plot the coeﬃcient paths, i.e. α vs. iteration. In the left panel of Figure 2 we present the paths produced while solving the WLRMA problem with k = 50. We observe some signiﬁcant oscillation in the coeﬃcients, which results in the oscillation of the convergence path of Anderson WLRMA in Figure 1a (blue curve on the middle panel). Here α is equal to one), or the average of d coeﬃcient vectors obtained on several previous iterations (regularization depth is equal to d). The penalty in the above problem works in a similar way as acceleration methods: it balances the new update of the optimized variable with the previous guesses thereby smoothing the coeﬃcient paths. On can derive the solution for problem (18). To stabilize the coeﬃcient paths we add regularization and restate problem (13) as Figure 1: Simulation example. Relative objective change vs. iteration number plotted for baseline, Nesterov and Anderson m = 3 algorithms. We match the rank and penalty factor of non-convex and convex problems so that top and bottom panels have similar value of the unpenalized objective. Figure 2: Coeﬃcient paths obtained via Andersen acceleration with m = 3 while solving the WLRMA problem for k = 50. Each of the m + 1 coeﬃcients is represented in diﬀerent color. Left panel corresponds to acceleration without stabilization (blue curve on the middle panel in Figure 1a). The remaining panels represent acceleration with diﬀerent smoothing penalties γ and α (m + 1) covariance matrix of the residuals R computed while deriving the unpenalized solution (14), the cost for regularized and nonregularized Anderson is equivalent. Note that computing αinvolves some simple manipulations with the small (m + 1) × vary γ, the smoothing penalty factor, in the grid γ = 0.1, 1, 10 and consider the average of three previous coeﬃcient vectors for α in Figure 2. As expected, increasing γ smooths the coeﬃcient paths. We compare the convergence speed for diﬀerent γ values. According to Figure 3 regularization also makes the convergence curves more smooth. Moreover, for large enough values of γ stabilizing Anderson coeﬃcients allows us to reach the convergence point faster. Figure 3: Convergence comparison for Anderson (γ = 0) and stabilized Anderson (γ = 0.1, 1, 10). We set α the WLRMA problem for k = 50, thus blue curve on this plot corresponds to the blue curve on the middle plot of Figure 1a. Now we test the proposed regularization technique on the same WLRMA problem. We In some applications, where n and/or p are extremely large the SVD part of the proximal operator is not plausible. For example, in the Netﬂix competition the training data matrix was approximately 400K by 20K. In this section we borrow an idea from Hastie et al. (2015), proposed for the low-rank matrix approximation problem with missing values, that allows us to avoid the SVD in high dimensions. This alternative view of the WLRMA problem is well-studied in literature (see, for example Razenshteyn et al. 2016, Dutta et al. 2018), where the authors propose simple iterative least squares algorithm that alternates two steps: ﬁx A and solve a weighted least squares problem to ﬁnd B; then do the same for A ﬁxing B. Although the algorithm avoids the most expensive SVD part of the PGD computations, for general W , it is still only moderately eﬃcient as multiple weighted regressions are required for each step (one for each row of A and B). M = X −∇g(X) = Y and unit weights, i.e. W = 1 update can be done via solving an unweighted multiresponse regression problem. To be precise, we get the following updates for non-convex WLRMA which leads us to the following SVD-free alternating algorithm. Similar to (4) it is possible to restate the WLRMA problem as It is easy to show that the proximal step for (6) is equivalent to solving (19) with 2. Repeat until convergence one can skip an extra update of X the projection step one should alternate 2.2 and 2.4 until the convergence. claim that for each λ > 0 there exists r such that for all k > r problem (7) is equivalent to solving penalized problem other matrix can be found via solving a ridge regression problem. Thus replacing the ALS updates at step 2.2 of the above algorithm by we get a simple alternating procedure solving the WLRMA convex relaxation (7). 1.1. Set A∈ Rand B∈ Rat random. 2.1. Compute X= ABand Y= W ∗ M + (1 − W ) ∗ X. 2.3. Update X= ABand Y= W ∗ M + (1 − W ) ∗ X. 2.4. Set A:= YBBB. Note that quite many variations of the above algorithm can be derived. For example, It is not hard to build similar algorithm for the WLRMA convex relaxation. We ﬁrst Next, note that if one matrix (A or B) is ﬁxed and the weights are unit, then the (19) or (21). This, again, can be done eﬃciently using the sparse structure of W . In some applications the matrix of weights W can be very sparse. For instance, in the Netﬂix application, where almost all movie ratings are unobserved, W will mostly contain zeros. It turns out that for sparse weight matrices the storage and computational cost of the ALS algorithm can be signiﬁcantly reduced. The trick we propose below is especially useful when the high dimensionality of the problem makes it impossible to store n × p matrices. that matrix Y follows: Here the second term X in the form of (A, B) pair. Furthermore, the ﬁrst term S only O(k ×|Ω|) to compute (using the low-rank structure of X in the form of a |Ω|-dimensional vector. This leads us to the implementation of ALS in algorithm 3. step 2.2 and 2.4 can be simpliﬁed to respectively. Then steps 2.2. and 2.4 are easy to handle using the sparse structure of S thereby making the computations for the ALS algorithm very eﬃcient. To check the convergence of the algorithm one needs to compute the loss functions of Denote Ω the set of non-zero element of W . Following Hastie et al. (2015) we ﬁrst note Note that Hand Hare cheap to calculate via multiplying skinny matrices. Moreover, 2. Repeat until convergence which still reduces to “cheap” multiplication of skinny matrices and inverting a small r ×r matrix. It is possible to extend all the tricks proposed in Section 6 to accelerated methods as well. For both Nesterov and Anderson acceleration we exploit the following idea. Note that each iteration of the WLRMA-ALS (and its sparse modiﬁcation) algorithm can be considered as an operator on a pair of matrices (A, B). Speciﬁcally, at step 2.1 the algorithm takes 1.1. Set A∈ Rand B∈ Rat random. 2.1. Compute sparse S= W ∗ (M −AB). 2.2. Compute H= AAAand set B= BAH+ SH. 2.3. Update sparse S= W ∗ (M −AB). , B) and after some transformation at steps 2.1–2.4 it returns the updated pair , B). For simplicity, let us denote this operator by Φ(A, B); this operator will be the main building block of our acceleration algorithm. Further, to introduce acceleration to the algorithm we concatenate matrices A and B an denote the result by Z, thus Z = ( To build our accelerated ALS approaches we apply acceleration to Z. For example, in the Nesterov case this will be equivalent to applying acceleration separately to A and B. 2. Repeat until convergence application of the update Z Z = Φ(Z), which can be solved via Anderson acceleration outline discussed in Section 3.2. Note that the resulting method will operate in terms of skinny (n + m) ×k matrices which makes it very eﬃcient in terms of both computational and storage cost. Sometimes it can be useful to track the solution rank while updating A and B. For example, one can consider the delayed Anderson acceleration (see Section 3.2) and initialize the acceleration as soon as the solution rank becomes less than k. To calculate the solution rank for low-rank X = AB 1.1. Set A∈ Rand B∈ Rat random. 2.1. Update V= A+A− Aand V= B+B− B. To build Anderson acceleration we consider our baseline-ALS algorithm as a successive Thus X = (U represent the solution rank. Again, since we work in terms of skinny matrices only, the SVD is cheap to compute. In this section we test the proposed methods on a real dataset. The dataset describes a 5-star rating and from MovieLens, a movie recommendation service. It contains 1 million ratings from 6000 users on 4000 movies that we store in a large 6000 × 4000 matrix M. Note that M is very sparse: some of the movies were rated by just a few users (if any) and only 5% of values are observed. Although the SVD-based approaches proposed in Sections 2–3 could be applied to this data, each iteration takes several minutes to complete, which makes the convergence process extremely slow. Anderson with depth m = 3. To initialize the algorithms we use a warm start, which is the ALS solution for non-weighted problem with unobserved values of X replaced by zeroes. To guarantee that loss decreases at each iteration for Anderson Acceleration we use the guarded modiﬁcation (see Section 3.2). Using the three algorithms under consideration we solve the WLRMA convex relaxation problem varying λ in the grid λ = 40, 50, 100 which results in the solutions of rank 70, 20 and 5, respectively. We limit the maximum number of iterations to 200 and set the convergence threshold to  = 10 • Decompose A, i.e. A = UDV. • Multiply˜B = BVD. • Decompose˜B = UDV. In Figure 4 we present the results for three ALS methods: baseline, Nesterov and Figure 4: MovieLens example. Comparison of three ALS algorithms: baseline, Nesterov and Anderson with m = 3. The convex relaxation with diﬀerent λ is solved. Top panel represents the objective relative change vs. time. Bottom panel represents the solution rank vs. time. just a few minutes. However, both acceleration methods demonstrate better convergence speed comparing to the baseline ALS technique. Note that for λ = 100, 50 Anderson achieves the threshold  = 10 Nesterov is slightly faster than the competitor the rank plot (green curve on the bottom right panel) suggests that it stopped too early. Finally, we observe that for less accurate solutions, e.g. the ones with  = 10 approach. In this paper we proposed a weighted generalization of the low-rank matrix approximation technique. Both classical LRMA problem, which has an explicit solution, and the famous LRMA problem with missing values, which can be solved via iterative soft- or hard-impute algorithms, are special cases of this weighted generalization. To build the baseline algorithm solving the weighted problem we utilize proximal gradient descent. We further suggest two ways to accelerate the baseline method. The ﬁrst technique is based on the Nesterov acceleration which uses the momentum from the previous iteration. To develop the second acceleration method we restate the WLRMA problem as a ﬁxed-point equation and subsequently utilize Anderson acceleration. proposed methods on some real-data applications with sparse but non-binary weights. For example, one can consider a GLM model with a working response having some low-rank matrix structure. Then each step of the Newton gradient descent will be equivalent to solving a weighted low-rank matrix approximation problem. Furthermore, if a huge portion According to the Figure, all three methods achieve reasonable accuracy of  = 10in There is still much scope for future work. One interesting direction is to test the of data is missing, it becomes a WLRMA problem with a sparse weight matrix. Another direction could be to explore the ways to improve the stability of the Anderson acceleration. There are many tricks beyond the ones discussed in this paper that could help us with this goal; for example, we can test Powell regularization or restart checking proposed in Zhang et al. (2020). Proposed methods are implemented in the R package WLRMA; the software is available from Github (https://github.com/ElenaTuzhilina/WLRMA). Elena Tuzhilina was supported by Stanford Data Science Scholarship. Trevor Hastie was partially supported by grants DMS-2013736 And IIS 1837931 from the National Science Foundation, and grant 5R01 EB 001988-21 from the National Institutes of Health.