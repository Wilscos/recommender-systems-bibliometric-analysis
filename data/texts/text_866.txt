The ability to incrementally learn new classes is vital to all realworld articial intelligence systems. A large portion of high-impact applications like social media, recommendation systems, E-commerce platforms, etc. can be represented by graph models. In this paper, we investigate the challenging yet practical problem, Graph Few-shot Class-incremental (Graph FCL) problem, where the graph model is tasked to classify both newly encountered classes and previously learned classes. Towards that purpose, we put forward a Graph Pseudo Incremental Learning paradigm by sampling tasks recurrently from the base classes, so as to produce an arbitrary number of training episodes for our model to practice the incremental learning skill. Furthermore, we design a Hierarchical-Attention-based Graph Meta-learning framework, HAG-Meta. We present a task-sensitive regularizer calculated from task-level attention and node class prototypes to mitigate overtting onto either novel or base classes. To employ the topological knowledge, we add a node-level attention module to adjust the prototype representation. Our model not only achieves greater stability of old knowledge consolidation, but also acquires advantageous adaptability to new knowledge with very limited data samples. Extensive experiments on three real-world datasets, including Amazon-clothing, Reddit, and DBLP, show that our framework demonstrates remarkable advantages in comparison with the baseline and other related state-of-the-art methods. • Computing methodologies → Lifelong machine learning. Graph Neural Networks, Incremental Learning, Few-shot Learning Graph-structured data, such as citation graphs [36], biomedical graphs [34], and social networks [9, 28], are nowadays ubiquitous in dierent real-world applications. Recently, a spectrum of Graph Neural Networks (GNNs) [1,3,9,10,16,41,45] has been proposed to model graph-structured data by transforming node features and propagating the embedded features along the graph structure. As a central task in graph machine learning, semi-supervised node classication aims to infer the missing labels for unlabeled nodes. By capturing the information carried by both labeled and unlabeled nodes as well as the relations between them, GNNs are able to achieve superior performance over other approaches. However, most of the existing work on node classication primarily focuses on a single task, where the model is tasked to classify the unlabeled nodes to a xed set of classes [13,16,25,51]. In practice, real-wold graphs grow rapidly and novel node classes could emerge incrementally in dierent time periods. For example, Ecommerce platforms like Amazon can be naturally modeled by graphs, where products are represented as nodes, and the interactions between products (e.g., viewed by the same customers) are represented as edges. As newly emerged product categories will be continually added to the platform, the underlying GNN models need to handle a sequence of incremental learning sessions for product categorization, where each session introduces a set of novel classes. It is worth mentioning that, dierent from those base classes (in the rst learning session) provided with abundant labeled data, only a few labeled samples are available for those newly emerged classes in any incremental learning session. Ideally, the desired GNN model is supposed to be capable of accurately recognizing those novel classes introduced in a new session while preserving the performance on all the ”seen” node classes in previous sessions. Such Graph Few-shot Class-incremental Learning (Graph FCL) problem has critical implications in both the academic and industrial communities. However, little eort has been devoted to this topic. The main challenges of the Graph FCL problem center around the so-called stability-plasticity dilemma [27], which is a trade-o between the preservation of previously learned graph knowledge and the capability of acquiring new knowledge. Specically, the novel classes in each new learning session have much fewer nodes compared to the base classes, resulting in a severe class-imbalance problem [12,37,44]. This may engender two potential problems: (1) on the one hand, if trained on all the data samples naively, the learned graph model could be substantially biased towards those base classes with signicantly more nodes, resulting in the inertia to learn new node classes [37]. Moreover, to retain the existing knowledge, many of the methods from few-shot incremental learning [31,46,47] use a xed feature encoder, which will not be updated after being pre-trained on base classes. Such a design will also exacerbate the diculty of adapting the model to the new incremental learning tasks; (2) on the other hand, as the node classes from new tasks only have few-labeled samples, imposing the graph learning model to focus on new tasks will easily lead to overtting to those new tasks and erase the existing knowledge for previously learned classes, which is known as Catastrophic Forgetting [8,14,18]. Considering the complex interactions between the nodes on graph-structured data, such learning errors would also be propagated on the graph and result in serious performance degradation. Thus, it is vital to explore and develop a new approach that can quickly adapt to the new class-incremental learning tasks while avoiding the forgetting of existing knowledge. To address the aforementioned challenges of the studied problem, we rst propose a new Graph Pseudo Incremental Learning (GPIL) training paradigm, which can facilitate the graph learning model to better adapt to new tasks. Initially, we split the original base dataset into base classes and pseudo novel classes with disjoint label spaces. Then, we pretrain the encoder on the base classes, and keep it trainable during the pseudo incremental learning process. For each episode during meta-training, all the few-shot node classication tasks are sampled from pseudo novel classes and base classes to mimic the incremental process in the evaluation phase. This way we obtain abundant meta-training episodes to learn a transferable model initialization for the incremental learning phase. We then propose a Hierarchical Attention Graph Meta-learning framework, HAG-Meta, which can eectively handle the stability-plasticity dilemma. Specically, our framework uses a dynamically scaled cross-entropy loss regularizer where the scale factors[20,24] are multiplied to each task-level loss to adjust their contribution for model training. Ideally, the scaling factors can help the model to down-weight the contribution of easy or insignicant tasks while focus on those hard or important tasks. Due to the fact that tasks in the Graph FCL problem have a hierarchical structure (nodes form classes, classes form tasks), we propose a hierarchical attention module that automatically captures the importance of dierent tasks and learns the scaling factors. On one hand, the Task-Level Attention (TLA) will estimate the importance of each task based on their aggregated prototypes and output the scaling factors to balance the contribution of dierent tasks. On the other hand, the Node-Level Attention (NLA) aims to learn prototypes that maintain a better balance between existing and novel knowledge within nodes and provide them to TLA. Being progressively trained in GPIL, the hierarchical attention module can gradually obtain the generalizability to produce the scaling factors for both encountered tasks and subsequent tasks. Training with this dynamically scaled regularizer, the proposed model will not only achieve better old knowledge consolidation but also acquire principled adaptability to new knowledge with merely limited data samples. The eectiveness of the proposed framework is validated with comprehensive experiments on three real-world datasets. The contribution of this work can be summarized as follows: • Problem:We present a novel Graph Few-shot Incremental Learning problem and formulate it with node classication tasks, where the model is tasked to accomplish node classication on base classes and all few-shot novel classes encountered during incremental sessions. • Algorithm:We propose Hierarchical Graph Attention modules tailored for the Graph FCL problem, and design a Graph Pseudo Incremental Learning paradigm to enable eective training to mimic the environment in the evaluation phase. • Evaluation: Experiments on the Amazon-clothing, Reddit, and DBLP datasets show that the proposed framework signicantly outperforms the baseline and other related stateof-the-art methods by a considerable margin. Incremental learning (IL) [31,46], also known as continual learning or lifelong learning, has drawn growing attention recently. IL aims to train machine learning models to acquire new knowledge while preserving the utmost existing knowledge. In this work, we mainly focus on Class-incremental Learning (CIL) where novel classes emerge in subsequent sessions and the model is tasked to fulll classication on all the classes it has encountered, rather than Task-incremental Learning (TIL), where usually a task identier is available, so the model can have multiple classiers and nish the nal classication on classes in a single task [15]. The mainstream of CIL methods can be categorized into two families. The rst family includes replay-based methods [22,30,32,35], which maintain a subset of previous samples, and train models together with samples in the new session. The other family of methods is regularizationbased methods [2,18], where various regularizers are proposed to regularize the parameters of a neural network so that more important parameters concerning the previous task can be protected when models are trained on each new task. A common choice of regularizer is a Knowledge Distillation (KD) [11] based loss proposed in LwF [19]. iCaRL [30] is the rst work that combines both replay and KD regularization methods, and puts forward the data imbalance problem between old classes and novel classes in CIL. A series of work focuses on this problem [44,48]. [31,46,47] further extend the situation to the Few-shot Class-incremental Learning (FCL) setting on image domain. For the FCL setting, KD performance will degrade tremendously due to the extreme scarcity of samples in novel classes. Instead, to overcome catastrophic forgetting, those methods usually adopt a decoupling method, where the encoder is xed after pre-training, and extra modules are involved for learning incremental classes during meta-training. However, directly applying those FCL methods to the graph domain can lead to drastic performance degradation. Nodes in a graph are not i.i.d. data as usually assumed for images. Their representations are learned via sampling and aggregation from their neighbors. Fixing the encoder, if nodes in novel classes in an impending session are densely linked with nodes in base classes, their representation can have evident overlap, and the boundaries between those classes will be blurred. Since no existing work is suitable for FCL on graphs, our paper aims at bridging this gap. Graph Neural Network (GNN) [1,3,9,10,16,41,45] is a family of deep neural models tailored for graph-structured data, which has been widely used in various applications, such as recommendation [42], anomaly detection [4], and text classication [5]. Generally, GNNs exploit a recurrent neighborhood aggregation strategy to preserve the graph structure information and transform the nodes’ attributes simultaneously. For instance, variants of GCN [1,3,10,16], GraphSAGE [9], GAT [41], and GIN [45] put forward dierent aggregation schemes to try to enhance the representation power of GNN. However, all those conventional GNNs may easily fail to learn expressive node representation when the labeled nodes are extremely scarce. Recently, increasing research attention has been devoted to graph few-shot learning problems. Especially, the episodic meta-learning paradigm [38] has become the most popular strategy for this problem, which transfers knowledge learned from many similar FSL tasks. Based on it, Meta-GNN [50] applies MAML [7] to tackle the low-resource learning problem on graph. Furthermore, RALE [21] uses GNNs to encode graph path-based hubs and capture the task-level dependency, to achieve knowledge transfer. GPN [6] adopts Prototypical Networks [33] to make the classication based on the distance between the node feature and the prototypes. AMM-GNN [43] leverages an attribute-level attention mechanism to characterize the feature distribution dierences between dierent tasks and learns more meaningful transferable knowledge across tasks. However, all those methods cannot be generalized to the Class-incremental learning scenarios, where the model is tested not only on the novel classes in the current task but also on all the classes in previous tasks. Catastrophic forgetting will erase the knowledge specic for previously learned classes. Formally, a graph𝐺 = (V, E, X), whereV,E, andXdenote the set of nodes, edges and node features respectively, can be alternatively represented by𝐺 = {A, X}, whereAis the adjacency matrix. The Graph FCL task assumes the existence of a sequence of homogeneous datasets within a graph,𝑖.𝑒.,D = {D, D, ..., D..., D}. In any session𝑖,C, the label space of the datasetD, has no overlapping with the label space of any other session,𝑖.𝑒.,∀𝑖, 𝑗 ∈ {0, ...,𝑇 }, 𝑖 ≠ 𝑗, C∩ C= ∅. Then, the dataset in each learning session can be represented asD= {A, X}, whereAdenotes the attributes of nodes whose labels belong to the label spaceC. Notably, in the rst session, the datasetDis a relatively large dataset where a sucient amount of data is available for normal semisupervised node classication training. The classes inDare the base classes. Datasets in following sessions,D∈ D, 𝑖 ≠ 0, are fewshot datasets, the classes in which are named as novel classes. Now, we present the formal denition of a Few-shot Class-incremental Node Classication task: Denition 3.1.Few-shot Class-incremental Node Classication on Graphs: For a specic session𝑖, given a graphG = (A, X), and a set of support nodes with labels,S, from the label spaceC, the model is tasked to predict labels for the nodes in corresponding query setQ. The label space of the query setQincludes the base setC, all the novel sets in previous sessions{C, C, ..., C}, and the novel set encountered in the current session C. For each session𝑖, we have such a Few-shot Class-incremental Node Classication taskT. In the corresponding support setS, we denote the number of novel classes as𝑁and the number of support nodes in each class as𝐾. This task is named as an𝑵 -way 𝑲 -shotincremental node classication task. Alternatively, learning through that sequence of datasets can be represented as a sequence of tasks:𝑖.𝑒.,𝑇 = {T, T, ..., T..., T}. In essence, we want our graph model able to retain a decent performance when fullling the classication on both base and novel classes. In this section, we introduce a Hierarchical Attention Graph Metalearning framework,HAG-Metafor solving Graph FCL problem. We rst describe our proposed training procedures in Section 4.1. Then, we present the model proposed in Section 4.2. The overview of HAG-Meta is shown in Figure 1. Pseudocode-style algorithm descriptions are given in Algorithm 1 and Appendix C. Data Splits:To solve the Graph FCL problem, We split the dataset Dinto two folds,DandD, with disjoint categories.D is randomly split intoD,D, andDfor pretraining.Dhas three splits ofD,D,D, with disjoint categories. To simulate the Graph FCL problem, the target few-shot data{D, ..., D..., D}are sampled fromD, and the corresponding nodes and edges are masked during pretraining and meta-training. The base dataDconsists ofD andD, where our proposed Graph Pseudo Incremental Learning is conducted. Details are given next. Pre-training:We pre-train a GNN-based encoder𝑔on the split D, following the normal semi-supervised node classication process. The encoder is still trainable after pre-training. Meta-training:To learn an initialization with more transferable meta-knowledge within the graph, here, we propose theGraph Pseudo Incremental Learning (GPIL)paradigm, where a model would be trained onDandD. Similar to the episodic meta-learning strategy, during each session𝑖, we randomly sample 𝑁novel categories fromD, and𝐾nodes per category to form a novel support setS= {(x, 𝑦)}. The query set is composed of samples from the base categories, the novel categories from the previous sessions, and𝑁novel categories in the current session. We sample𝐾samples from all those categories to form a query setQ= Q∪ Q. The novel categories in each session will be merged into the base categories for the next session. We will cache those novel support nodes inS for the classication of old classes in later sessions. So similarly, S= S∪ S. During each session𝑖, the parameters are updated by the loss proposed in Section 4.2.1 for the classication of queries inQ. During training, we reset the base categories and novel categories whenever the number of left novel categories is less than 𝑁 . In this way, the proposed model can be trained on an arbitrary number of episodes despite the limitation of the number of novel categories inD, which is crucial for training the attention models (See details in Section 4.2). Evaluation:For each evaluation session, we randomly sample 𝑁novel categories fromD(orDfor validation). The proposed model is ne-tuned on the sample. Base samples for test are sampled fromD(orDfor validation) andD. For the next session, those novel categories are merged into base categories. The baseline model we deploy is based on Prototypical Network (PN) [33]. We replace the multilayer perceptron (MLP) encoder in the original PN with a GNN encoder𝑔. We call it Proto-GNN for convenience. Then, given a graphG = (A, X), the latent features can be dened as: Figure 1: (a) Illustration of Few-shot Class-incremental Node Classication Task. (b) The illustration of our framework: HAGMeta, as in Section 4.2. (c) Structure of the Task-Level Attention module, computed with self-attention layer, as in Section 4.2.1. (d) Structure of the Node-Level Attention module. We adopt GCN layers to generate the weight, as in Section 4.2.2. As mentioned in [33], the vanilla way to compute a prototype for a category is the average over all the features of nodes in the category: WhereSis the set of all the support nodes index in category𝑘 and|S|is its cardinality. Then, a distance function𝑑is used to generate the distribution over classes for a query node𝑣based on a softmax over distances to the prototypes in the latent space: For the choice of𝑑, we use squared Euclidean distance, which has been shown as a simple and eective distance function [33]. To overcome the class-imbalance challenge in Section 1, we here propose our Hierarchical-Attention based Graph Attention module. The goal is to learn a strong regularizer that can dynamically scale the contribution for nodes in dierent tasks. Tasks in the Graph FCL problem naturally have a hierarchical structure(nodes form classes, classes form tasks). Thus, we propose a two-level hierarchical attention mechanism: Task-Level Attention and Node-Level Attention to estimate the importance of dierent tasks. 4.2.1 Task-Level Aention. To deal with the challenge that our model may overt onto base or novel classes, we here propose theTask-Level Attention(TLA) to estimate the importance of classes learned in dierent tasks. Ultimately, we want to learn a series of scaling factors for the loss, which should be competent to automatically down-weight the contribution of easy or insignicant tasks during meta-training and rapidly focus the model on hard or important tasks. As described in the training procedure, during meta-training, the query nodes are from classes in all the previous training sessions. In this case, we nd that a task-weighted loss will serve the purpose. For a PN, the prototypes matrix,P= {p} (P∈ R, whereℎis the size of prototype features), of the support nodes,S, in a certain session𝑖, serves as the classier for the queries,𝑄. Hence, we can make the hypothesis that the prototypes in sessions are representative enough to express the knowledge of the taskT. Based on the self-attention by [40], TLA aims at learning the attentions (scaling factors)w∈ W, (∀𝑗 ∈ [1, 𝑖],W ∈ R) between the current task,T, and all the tasks the model has been trained on,{T, T, ..., T}, including the current task. The desirable property of TLA is that the attention mechanism is inductive and permutation invariant, which suits the Graph FCL problem where novel tasks and classes come in sequence. The structure of the TLA model is shown in Figure 1 (c). We use𝑔to denote the TLA generator. Because the number of classes in the base is much larger than that of each novel task, we rst use MLPs to project the prototypes of all the tasks into the same size: Whereuis the projections of the prototypespat session𝑗. Then, the weights W can be computed as: For each task, the weightWis determined by the number of classes in the task, and then normalized by the number of classes in that task (dierent classes in the same task share the same weight): where|𝐶|is the number of classes in session𝑗, and˜W ∈ R is the expanded weight vector ofW.|𝐶|is the number of all the classes having been seen. AndWis the target scaling factor of classes within the task. With all the factorsWcomputed, nally we introduce ourTLA Loss, which is the Cross-Entropy Loss (CEL) scaled by the TLA scaling factors W: L=𝑤· [𝑦· log(^𝑦) + (1 − 𝑦) log(1 −^𝑦)] (7) The nal loss is: L = L+ L =(1 + 𝑤) · [𝑦· log(^𝑦) + (1 − 𝑦) log(1 −^𝑦)](8) Lwill function as the initialization of task contribution weight, andLwill turn into a regularizer, which adjusts the contribution according to the importance of tasks. 4.2.2 Node-Level Aention. While TLA can weigh the importance of dierent graph tasks, it cannot fully capture the knowledge within the graph structure, which may lead to the inaccurate importance measured for tasks. To incorporate TLA with the graph knowledge, we propose to use aNode-Level Attention (NLA), Λ = {𝜆}, 𝑗 ∈ S, to adjust the representation of the prototype features learned from the GNN encoder𝑔in each session𝑖, where 𝜆is the attention weight of the novel support nodes𝑣to the prototype. We want the NLA to maintain the balance of existing knowledge and novel knowledge for each node. The schematic diagram of NLA can be found in Figure 1 (d). We propose to use the Graph Convolutional Network (GCN) [16] to calculate the nal NLA. The propagation rule in the𝑙th layer of our GCN can be represented as: where, at the𝑙th GCN layer,his the latent NLA representation of node𝑣,𝜎is an activation function,Ris the learned weight matrix,Nis a set of nodes adjacent to node𝑣,𝑑and𝑑are the node degrees of node𝑣and node𝑣respectively. We seth= X. Then, we use an MLP to project the latent NLA in the last (𝐿th) GCN layer into scalar: Next, we apply the centrality adjustment method proposed in [26]: where𝜎is the sigmoid function,𝜖is a small constant. Finally we use softmax function to normalize the NLA: Then, we modify the original strategy, Eq.(2), to calculate prototype in PN with NLA: With the adjusted prototypes, we can then use Eq.(3)to get the nal label. An overview of the incremental training procedure of each session is given in Appendix C. In this section, we present the evaluation of our framework: HAGMeta. We rst introduce the used datasets and compared methods. Then, we show the result and analysis of the comparative study. Furthermore, we conduct comprehensive ablation experiments to validate the eectiveness of individual components in the proposed framework and study their characteristics. Also, we compare the Algorithm 1 HAG-Meta Input:DatasetD, number of sessions𝑀for GPIL, number of target evaluation sessions𝑇for evaluation, random initialized GNN model𝑔, random initialized TLA weighter𝑔, and NLA weigter 𝑔. Output: Trained models: 𝑔, 𝑔, and 𝑔. // Data split // Meta training (GPIL) Sample a Graph FCL task fromDandDaccording to Section 4.1: T= {S, Q}. A, 𝑔, 𝑔, 𝑔= IncrementalSession(T, 𝑔, 𝑔, 𝑔) Sample a Graph FCL task fromDandD according to Section 4.1: T= {S, Q}. A, 𝑔, 𝑔, 𝑔= IncrementalSession(T, 𝑔, 𝑔, 𝑔) result of our model with that of the best baseline under dierent 𝑁-way𝐾-shot settings. Finally, to illustrate the advantage of our model, we visualize the learned embeddings. Evaluation Datasets. We conduct our experimentson three widely used graph Few-shot learning datasets: Amazon-Clothing [23], DBLP [36], and Reddit [9]. More details about the datasets can be found in Appendix A. The statistic are shown in Table 1. Compared Methods. In this paper, we compare our HAG-Meta framework with the following methods: •Prototypical Networks on graphs: As discussed in Section 4.2, we implement Proto-GNN with two dierent encoders: GCN [16] and GAT [41], to reveal the inability of GNN to deal with the Graph FCL problem. We denote them as Proto-GCN and Proto-GAT. •State-of-the-art Graph Few-shot learning methods: Meta- GNN [50] and GPN [6]. •Continual learning methods on graph: ER-GNN [49], and classic iCaRL [30] with the encoder substituted by a GNN. These models do not consider the Few-shot learning setting. •Few-shot Class-incremental Learning: CEC [47]. It is one of the state-of-the-art methods but is primarily for the image domain, so we replace the encoder with a GNN encoder. Table 1: Statistics of the expermental datasets. In Table 1, we list the specic data split strategy for each dataset, following Section 4.1. Here,|C|is the number of classes for pre-training,|C|the number of classes for meta-training, and|C|the number of target few-shot classes for evaluation. E.g., for the Amazon-Clothing dataset, we pre-train our GNN encoder on 20 categories of nodes, which are viewed as base categories. Another 30 categories are used for meta-train for providing pseudo novel categories. For evaluation, the model will be netuned on a sequence of tasks consisting of nodes in the remaining 27 target categories. We stop the encoder pre-training when its validation accuracy stops improving for more than 10 epochs. For model implementation, please refer to Appendix B for detail. Evaluation Protocol:We evaluate the model after each session with the test setD(sampled fromD). To reduce uctuation, all the accuracy (Acc.) scores reported are averaged over 10 random seeds. We also calculate a performance dropping rate (PD) that measures the absolute accuracy drops in the last session w.r.t. the accuracy before the rst evaluation session,𝑖.𝑒.,𝑃𝐷 = A−A whereAis the classication accuracy in the last Meta-train session andAis the accuracy in the last session. To make the result more explicit, we dene a new term: Relative Performance Dropping rate (RPD), which is the PD normalized by the initial accuracy, 𝑖.𝑒., 𝑅𝑃𝐷 =. In this section, we present the comparison between our framework and the other four categories of baseline methods described in Section 5.1. To the best of our knowledge, we are the rst to investigate the Graph Few-shot Class-incremental Learning Problem. To fairly compare those methods, all methods except the basic GAT model share the same pre-trained GNN encoder as the proposed framework HAG-Meta, a 2-layer GCN. Also, when experimented on each dataset, they share the same random seeds for data split, leading to identical evaluation data. We justify the advantage of the proposed framework from the following aspects: Performance Degradationin Graph Few-shot Learning (GFSL) methods: A general observation is that, for those GFSL methods, their accuracy decreases substantially as new sessions emerge, especially for the rst several sessions. Even though through metatraining, they have gained generalizability to a certain extent, the performance degrades constantly as the number of classes involved increases. This implies that the existing GFSL methods cannot maintain discriminative boundaries between all the base classes and novel classes. Without consolidated knowledge of classes in base and early sessions, traditional GFSL methods suer grievously from Catastrophic Forgetting, as it adapts to classes in the latest episode. Limitationin existing Incremental Learning methods. ER-GNN is one of the pioneers to task graph neural network models with a sequence of tasks. It adopts several Experience Replay methods to try to consolidate existing knowledge. However, the accuracy of ER-GNN diminishes tremendously when it is applied to a few-shot setting. Besides, iCaRL combines a piece of memory and knowledge distillation to consolidate existing knowledge. However, the limited number of samples in novel classes will aect the knowledge distillation process, leading to its overtting onto old classes. Furthermore, the CEC method is one of the state-of-the-art fewshot Class-incremental learning methods for the image domain. It assumes that the data is i.i.d. distributed, which makes it overlook the topological relationships among tasks and nodes. It adopts a decoupling strategy, where the encoder is xed after pre-training to retain existing knowledge. So when it is applied to graphs, the representation of nodes in base classes is xed through all following sessions. But in graphs, the representation of novel classes nodes tightly depends on their neighboring nodes, which might belong to the base classes. This leads to indiscriminative boundaries between the base and the novel classes and unsatisfactory accuracy. Advantagesof the proposed HAG-Meta: Generally, HAG-Meta outperforms all baseline methods by a large margin, in terms of accuracy, PD, and RPD. Compared to the GFSL methods, the proposed penalty termLcan eectively prevent catastrophic forgetting by regularization. In contrast with ER-GNN and iCaRL, our GPIL paradigm provides sucient episodes to train the model such that it can learn an appropriate initialization to adapt to few-shot novel data. Also, we discard the decoupling strategy of the mainstream incremental learning methods on the image domain, like CEC. In contrast, we add node level attention, NLA, to learn the task and class dependency within the graph topological structure. The results in Table 2 verify that HAG-Meta is robust against increasing numbers of sessions with novel classes in the Graph FCL problem. In this section, we conduct more experiments to investigate the eectiveness of dierent components in our framework. We present the results of experiments on Amazon-Clothing datasets, under the 3-way 5-shot setting (similar results can be observed on the other datasets and settings). The results are shown in Table 3. For each method, the models share the same data splits for evaluation. Specically, the variant Proto-GNN stands for the Prototypical Graph Neural Networks baseline described in Section 4.2. NLA and TLA represent Node-Level Attention and Task-level Attention, respectively. A method without the TLA means the loss is the vanilla cross-entropy loss, without theLregularizer, see Eq.(8). A method without the NLA means there is no prototype representation adjustment. All the prototype representations are the vanilla average of the encoder output, see Eq.(2). For methods without GPIL, the model is directly ne-tuned on datasets with target novel classes in dierent sessions. According to the results shown in Table 3, we can observe that each component in our proposed framework, HAG-Meta, is eective when tackling the Graph FCL problem: •GPIL: Comparison between methods with and without GPIL shows that GPIL can improve the accuracy. With more episodes MethodAcc. in each session (%) ↑ Proto-GAT 60.22 44.32 41.87 38.28 35.15 32.25 30.67 28.54 26.54 25.43 34.79 57.77 (+1.43/+18.13) Proto-GCN 60.52 43.11 42.41 39.80 36.91 33.84 31.47 29.67 27.18 25.76 34.76 57.44 (+1.40/+17.80) MethodAcc. in each session (%) ↑ Proto-GAT 38.75 32.04 23.64 19.65 19.43 18.64 18.86 18.59 18.35 18.23 20.52 52.96 (+3.41/+21.62) Proto-GCN 39.02 33.11 24.27 20.14 20.03 19.12 18.76 18.32 18.50 18.66 20.36 52.17 (+3.25/+20.83) Meta-GNN 40.75 34.28 33.68 32.18 30.45 28.45 24.34 23.18 23.07 22.23 18.52 45.44 (+1.41/+14.10) MethodAcc. in each session (%) ↑ Proto-GAT 48.23 42.07 37.52 33.43 32.38 31.05 28.32 26.42 24.39 22.47 25.76 53.41 (+5.14/+19.43) Proto-GCN 48.04 42.77 37.35 35.04 33.62 31.21 28.04 25.86 24.42 22.61 25.43 52.94 (+4.81/+18.96) Meta-GNN 53.14 48.56 45.63 42.52 40.42 38.20 35.12 32.21 30.68 29.45 23.69 44.58 (+5.07/+10.60) Proto-GNN + NLA + TLA + GPIL 84.15 75.32 71.35 67.32 64.03 61.42 56.23 54.63 52.65 50.79 33.36 39.64 of training, GPIL equips the hierarchical-attention module with better initialization to capture the importance among nodes and tasks. •TLA and NLA: Generally, the scores show that both attention components help improve accuracy. Specically, comparing methods with solely NLA or TLA, we can nd that, at rst several sessions, methods with NLA have higher scores, but their scores decrease at a sharper rate compared to methods with only TLA. This shows that NLA is better in terms of capturing the latent information within the graph data from limited novel class samples, but it suers from catastrophic forgetting. In contrast, methods with TLA can maintain a higher score at later sessions, which echoes the purpose of our design, that the TLA loss can alleviate the forgetting Figure 2: Parameter analysis on three real-world datasets: (a) Amazon-Clothing (b) DBLP (c) Reddit. For each dataset we experiment under three N-way k-shot settings. problem by focusing more on important hard-to-learn tasks than insignicant easy-to-learn tasks. •Our framework, HAG-Meta, containing NLA, TLA, and GPIL, as shown in the last line in Table 3, achieves the best performance. Through eective training in GPIL, the hierarchical attention components, TLA and NLA, have learned an initialization that can capture the topological information within the input graph to adapt to few-shot novel classes and mitigate forgetting of the existing knowledge simultaneously. In this section, we present extensive experiments to analyze the sensitivity of our HAG-Meta to the number of node classes (𝑁-way) and support(query) set size (𝐾-shot). For better comparison, we include the accuracy for both our HAG-Meta and the best comparable model CEC under dierent N-way K-shot settings for all three real-world datasets. As the result shown in Figure 2, our model outperforms CEC in every setting we test. Plus, with more supervisory signals, the classication accuracies of both models are higher with larger support and query set. To illustrate the eectiveness of our framework, we use the t-SNE [39] method to visualize the embedding after the second session of evaluation for the large-scale Reddit dataset under the 2-way 5-shot setting. As shown in Figure 3, each color signies a class. It’s evident that our model is capable to produce projection embedding that elicits much more discriminative decision boundaries than our best baseline model CEC. In this work, we propose a new problem, the Graph Few-shot Classincremental Learning (Graph FCL) problem, aiming at obtaining a graph model that can adapt to new tasks with a restricted number of labeled training samples in novel classes, and simultaneously keep a good performance on old tasks. We formalize it with the node classication task and propose a novel framework called HAG-Meta. We present a novel Graph Pseudo Incremental Learning paradigm, which allows our model to learn a generalizable initialization for the evaluation phase. Then we propose a hierarchical-attentionbased module to solve the class-imbalance problem in Graph FCL. Figure 3: t-SNE embedding visualization: (a) CEC (b) ours. Primarily, the Task-Level Attention will be trained to estimate the importance between dierent tasks for backpropagation, while the Node-Level Attention incorporates the Task-Level Attention with the ability to capture the knowledge within the graph structure. A dynamically scaled loss regularizer is then computed from the task importance and automatically adjusts the contribution of dierent tasks for training. We conduct experiments on real-world datasets to demonstrate the eectiveness and advantage of our framework. Despite the promising results, the Graph FCL problem is far from being solved. In particular, many other graph settings are worth considering, such as the dynamic graph scenario, where the graph structure is continually evolving in dierent learning sessions. Besides, more sophisticated methods to design the regularizer, like from the perspective of causality, are worth considering. Also, we plan to study the scenario where the model cannot explicitly store any of the training data [30], e.g. for privacy issues. A potential direction could be to investigate the ecient methods to store or reproduce embedded features [17, 29].