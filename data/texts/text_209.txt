To Appear in 2021 ACM SIGSAC Conference on Computer and Communications Security, November 2021 Recently, recommender systems have achieved promising performances and become one of the most widely used web applications. However, recommender systems are often trained on highly sensitive user data, thus potential data leakage from recommender systems may lead to severe privacy problems. In this paper, we make the ﬁrst attempt on quantifying the privacy leakage of recommender systems through the lens of membership inference. In contrast with traditional membership inference against machine learning classiﬁers, our attack faces two main differences. First, our attack is on the userlevel but not on the data sample-level. Second, the adversary can only observe the ordered recommended items from a recommender system instead of prediction results in the form of posterior probabilities. To address the above challenges, we propose a novel method by representing users from relevant items. Moreover, a shadow recommender is established to derive the labeled training data for training the attack model. Extensive experimental results show that our attack framework achieves a strong performance. In addition, we design a defense mechanism to effectively mitigate the membership inference threat of recommender systems. As one of the most prevalent services in current web applications, recommender systems have been applied in various scenarios, such as online shopping, video sharing, location recommendation, etc. A recommender system is essentially an information ﬁltering system, relying on machine learning algorithms to predict user preferences for items. One mainstream method in this space is collaborative ﬁltering, which is based on traditional methods such as matrix factorization and latent factor model, predicting a user’s preference from their historical behaviors combined with other users’ similar decisions [19, 41]. Another is the content-based recommendation [35,51]. This approach aims to distinguish users’ likes from dislikes based on their metadata (such as descriptions of the items and proﬁles of the users’ preferences). Recent Figure 1: An example of recommender systems. advancement of deep learning techniques further boosts the performance of recommender systems [16]. The success of recommender systems lies in the largescale user data. However, the data in many cases contains sensitive information of individuals, such as shopping preference, social relationship [1], and location information [44]. Recently, various research has shown that machine learning models, represented by machine learning classiﬁers, are prone to privacy attacks [6, 8, 10, 18, 22, 28–32, 34, 37, 39, 43, 45]. However, the privacy risks stemming from recommender systems have been left largely unexplored. In this paper, we take the ﬁrst step quantifying the privacy risks of recommender systems through the lens of membership inference. Compared to previous membership inference attacks against machine learning classiﬁers [39, 43], our attack faces two main differences. First, the goal of our attack is to determine whether a user’s data is used by the target recommender. This indicates our attack is on the userlevel while most of the previous attacks focus on the samplelevel [39, 43]. Unlike sample-level membership inference, user-level membership inference has a broader scope as mentioned in previous works [45], and it can help us gain a comprehensive understanding of recommender systems’ privacy risks. Second, from the adversary’s perspective, only ranking lists of items are available from the target recommender which raises several technical challenges: • In our attack, as Figure 1 depicts, the adversary can only observe lists of items, even though recommender systems have already calculated posterior probabilities before making decisions. This setting is prevalent in the real world, such as the service provided by Amazon or Netﬂix. Besides, exposing less information can pro- tect the intellectual property of recommendation service providers [39]. On the contrary, in classical membership inference attacks against classiﬁers, posterior probabilities used for decisions can be accessed by the adversary [31,37,39,43]. • Several recent membership inference attacks against classiﬁers focus on the decision-only (i.e., label-only) scenario [10, 28]. However, these studies either rely on the target model to label a shadow dataset [28] or use adversarial examples [10, 28], which are not practical when targeting recommender systems in the real world. Therefore, we aim for a new method to extract information from decision results for the attack model. • Unlike classical classiﬁers, the outputs of recommender systems are ranking lists of items other than unordered labels. In that case, the order information plays an important role, and can substantially facilitate user preference predictions. Therefore, it is necessary for our attack model to capture order information from recommended items, which is still ignored by previous membership inference attack methods. Threat Model. The goal of the adversary is to conduct a membership inference attack against the target recommender system, inferring whether a target user’s data is used to train the target recommender or not. However, such attacks can lead to severe security and privacy risks. Speciﬁcally, when a recommender system is trained with data from people with certain sensitive information, such as health status, knowing a user’s data being part of the recommender’s training data directly leaks their private information. Moreover, membership inference also gains the adversary information about the target recommender’s training data, which may jeopardize the recommender’s intellectual property, since collecting high-quality data often requires a large amount of efforts [18, 27]. From a different angle, a user can also use membership inference as a tool to audit whether their data is used by the target recommender. We assume the adversary has black-box access to the target recommender, the most difﬁcult setting for the adversary [43]. Instead of posterior probabilities for recommendations, only relevant items for users are available, such as rating or purchase and recommended items. Due to the knowledge, a shadow recommender is established to derive labeled training data, for the attack model better inferring membership status in the target recommender. Attack Method. For user-level membership inference, we need to summarize each user’s feature vector, based on interactions between the target recommender and them, as the input to the attack model. However, compared to the previous work of membership inference against classiﬁers, the adversary can only observe the recommended items from a recommender system instead of posterior probabilities as prediction results. Thus, in the ﬁrst step, the adversary constructs a user-item matrix for ratings with a dataset used to generate feature vectors. Then, they factorize this matrix into two low-dimensional matrices, namely user matrix and item matrix. Each item’s feature vector can be represented by the corresponding row vector in the item matrix. For each user, the adversary extracts two sets of items (one set contains items the user is recommended and the other contains the items the user interacted with) and calculates these two sets’ center vectors, respectively. The difference between these two center vectors for each user describes how accurate the recommender is for this user. In that case, lower difference indicates a user’s data is more likely to be used to train the recommender. Therefore, we use this difference as the input to the attack model, i.e., user feature vector. The adversary generates all the labeled training dataset for their attack model with the help of the shadow recommender. To launch the attack, the adversary generates the target user’s feature vector following the same steps and obtains the prediction from the attack model. sumed to have a shadow dataset that comes from the same distribution as the target recommender’s training data, and know the target recommender’s algorithm. These assumptions are gradually relaxed based on our empirical evaluation. Our experiments are performed on three benchmark recommendation datasets, i.e., the Amazon Digital Music (ADM) [15], Lastfm-2k (lf-2k) [4], and Movielens-1m (ml1m) [13]. The recommendation algorithms we focus on include Item-Based Collaborative Filtering (Item), Latent Factor Model (LFM), and Neural Collaborative Filtering (NCF). Evaluation results demonstrate that our attack is able to achieve an excellent performance. • In general, when the adversary knows the distribution of the target dataset and the target recommender’s algorithm, the attack performance is extremely strong. For instance, when the target recommender uses NCF on the ADM dataset, our attack achieves an AUC of 0.987. Also, when the target algorithm is Item or NCF, our attack achieves better performances. • When the adversary is not aware of the target recommender’s algorithm, attack performances are reduced but still strong. For instance, on the lf-2k dataset, when the target recommender uses Item and the shadow recommender uses NCF, the attack performance decreases from an AUC of 0.929 to an AUC of 0.827. On the other hand, in some cases, the attack performances even increase. For instance, on the ml-1m dataset, when the target recommender uses Item and the shadow recommender uses LFM, the attack’s AUC increases from 0.871 to 0.931. • We further relax the assumption of the shadow dataset. Evaluation shows that even under such a scenario, our attack still achieves good performances in general. Note that, in some cases, when the adversary knows less about the target recommender, the attack even performs better. This demonstrates the good generalization ability of our attack. In conclusion, the experimental results show the effectiveness of our attack, indicating that recommender systems are indeed vulnerable to privacy attacks. Defense. To mitigate the recommender’s privacy risk, we propose a defense mechanism, namely Popularity Randomization. Popularity Randomization is deployed when the target recommender recommends items to its non-member users. The normal strategy in such case is to provide nonmember users with the most popular items. However, to defend the attack, we enlarge the set of popular items, and randomly select a subset of them for recommendation. Intuitively, while preserving the recommendation performance for non-member users, this approach enriches the randomness of the recommendation. Experimental results show that Popularity Randomization can effectively mitigate membership inference. For instance, Popularity Randomization (with a 0.1 ratio of recommendations to candidates) decreases the attack performances by more than 12%, 33%, and 41% when the target algorithm is Item, LFM or NCF respectively. Through analyses, we observe that Popularity Randomization has the greatest impact on the attack targeting on NCF. In this section, we ﬁrst present some necessary deﬁnitions in Section 2.1, and then introduce the threat model for the membership inference attack against recommender systems in Section 2.2. Next, we give overviews for recommender systems in Section 2.3 and our attack model in Section 2.4. Finally, we detail the proposed membership inference attack methods in Section 2.5. We present the following deﬁnitions for the attack process: • Target Recommender, trained on the Target Dataset, is the recommender system attacked by the adversary. Dataset, is a recommender system built to infer the membership status of the target recommender and generate training data for the attack model. • Members are the users whose data is used to train the recommender, while Non-Members are the ones whose data is not used. • Personalized Recommendation Algorithms learn members’ preferences from historical behaviors (such as purchases or ratings), which are also called Interactions. Non-Personalized Recommendation Algorithms are based on the predetermined rule, such as selecting the most popular or highest-rated items. According to different recommendation methods, members and non-members are provided with recommended items, which are also called Recommendations. • Feature Vectors show the latent features, indicating item attributes or user preferences. • Attack Model is used to infer whether the target user is a member, and trained on the dataset generated from the shadow recommender. Adversary’s Goal. The adversary aims to infer whether a user’s data is used by a target recommender. In fact, knowing a certain user’s data being used by a recommender system directly leaks their private information. Besides, knowing a user being part of the dataset can also allow the adversary to gain extra information of the target recommender’s dataset. This directly violates the intellectual property of the target recommender, since it is very expensive to collect high-quality training data. Alternatively, membership inference can also be used as an auditing tool by a user to ﬁnd out whether their data is used by the target recommender. only black-box access to the target recommender. That is, adversary can only observe the items recommended to a target user (i.e., recommendations), and the user’s history (i.e., interactions), such as rating and purchase, instead of posterior probabilities for recommendation predictions. In that case, the adversary needs to proﬁle users by their interactions and recommendations. Meanwhile, a shadow recommender is built to generate labeled data for the attack model, since ground truth membership is unavailable from the target recommender. In this section, the framework of recommender systems is brieﬂy introduced. Recommendation algorithms output recommended items based on the information learnt from input. In the paper, two types of recommendation algorithms are mainly involved: personalized and non-personalized recommendation algorithms. For members, items are recommended according to the preferences of members. Meanwhile, lacking non-members’ data, non-personalized recommendation algorithms are conducted, and provide most popular items for non-members. Speciﬁcally, Item-Based Collaborative Filtering (Item) [40], Latent Factor Model (LFM) and Neural Collaborative Filtering (NCF) [16] are adopted as the personalized recommendation algorithms for members. As for the non-personalized recommendation algorithm, the most popular items are provided to non-members, which is also called the popularity recommendation algorithm in the paper. We brieﬂy introduce the above algorithms as follows: • Item calculates the similarity between items aiming to ﬁnd the ones which are closed to users’ likes. • LFM builds a latent space to bridge user preferences and item attributes. • NCF combine the deep learning technology with collaborative ﬁltering to enhance the recommendation performances. Figure 2: The framework of the membership inference attack against a recommender system. • Users are provided with the most popular items by the popularity recommendation algorithm. In general, a recommender system Alearns user preferences from the interactions, sometimes with the external knowledge (such as gender and location information) for users. According to the predicted preferences, the recommender system provides users with multiple items. This procedure can be formulated as: where Ais a recommender system learning the preferences of users from their interactions Iand the external knowledge K. And Rdenotes recommended items to users. In the paper, we mainly use the interactions of users. Thus, we deﬁne a recommender system as: where Iis a set of lists of interactions for users and R is a set of ordered lists of recommendations for users. Concretely, I= {L}and R= {L}, where Lis the list of interactions and Lis the ordered list of recommendations for the nuser, and Nis the number of users. In this section, we give an overview of our attack. As Figure 2 demonstrated, the attack process follows three steps: Labeled Data Generation, Attack Model Establishment, and Parameter Optimization. matrix is derived, by factorizing a user-item rating matrix. Due to the black-box access to the target recommender for the adversary, a shadow recommender is built to generate labeled training data for the attack model. Moreover, we represent interactions and recommendations of users using corresponding feature vectors. After that, a user is proﬁled by the difference between two centers of their interactions and recommendations. And each user is labeled with 1 or 0, indicating they are a member or non-member. hidden-layer Multi-Layer Perceptron (MLP) is utilized as the attack model Ato infer membership status. Each hidden layer is followed by a ReLU activation layer. And a softmax function is used as the output layer to predict the probability of the membership. Parameter Optimization. After Labeled Data Generation and Attack Model Establishment, as shown in Figure 2, the adversary updates parameters of the attack model. In the inference stage, the test dataset for target users are established following the same steps as training data generation. The membership status for target users is inferred by the trained attack model. In this section, we detail our proposed membership inference attack against a recommender system. As mentioned before, the whole attack consists of three steps to achieve the adversary’s goal: Labeled Data Generation, Attack Model Establishment, and Parameter Optimization. Labeled Data Generation. Training data is required during the Aoptimization process. However, the adversary cannot obtain membership status directly from the target recommender A. To address this problem, a shadow recommender Ais developed to mimic the dataset and recommendation algorithm of the target recommender. As mentioned in Section 1.1, only recommended item lists from target recommender systems can be observed. Inspired by the previous works [17,24], matrix factorization is adopted to project users and items into a shared latent space. Speciﬁcally, a p × q user-item matrix Mis built using ratings of users to items, where p and q are the number of users and items respectively. Values in Mare ratings ranging from 1 to 5, indicating how much users prefer these items. Then, Mis factorized into two low-dimensional matrices, namely latent user matrix M∈ Rand latent item matrix M∈ R, where we denote l as the dimension of the latent feature space. We apply matrix factorization to ﬁnd optimized Mand Mby minimizing the loss function whereˆMis a predicted user-item matrix which contains the predicted scores of users rating items. Besides, Mand Mpresent the predicted preferences of users and the predicted attributes of items, respectively. Each row of the item matrix Mrepresents the feature vector of the corresponding item. Note that, since Mmay not cover all users in the shadow and target recommenders, Mis not used to represent users. To this end, training data for the attack model can be generated from A. The shadow dataset Dare split into two disjoint sets for members and non-members, which are denoted by Dand D, respectively. These datasets are composed of 3-tuples in the form of (uID,iID,score), indicating scores rated by users to items. For instance, a 3-tuple (2,3,4) in datasets means that the 2 user rates the 3item a score of 4. Ratings in Dand can be seen as interactions of users to items, and sets of interaction lists for members and non-members can be obtained, denoted as Iand I, respectively. In that case, each user has a list of interactions. For example, if a user rates the 2, 4, 6and 8items, the corresponding interaction list is {2,4, 6,8}. Next, Ais established to mimic A, and provides users with recommendations according to their preferences. The sets of recommendation lists for members and non-members are denoted by Rand R, respectively. Similar as interactions, each user is associated with a list of recommendations. However, Rand R are sets of ordered lists of recommendations. Formally, the recommendation process can be formulated as follows: where fperforms a personalized recommendation algorithm based on the behaviors of members. Meanwhile, since non-members’ data is unavailable to A, f performs the popularity recommendation algorithm (a nonpersonalized recommendation algorithm) based on the statistical results from I. Besides, Iis a set of lists of interactions for members, and Rand Rare sets of ordered lists of recommended items for members and nonmembers respectively. Using item feature representations, we can vectorize the interaction and recommendation sets as follows: where Uand Vare sets of lists of the feature vectors for the corresponding items in Iand R. Given that each user has a list of interactions and is provided with an ordered list of recommendations, the adversary is able to represent users by their relevant items. To be speciﬁc, for the iuser, the representation is generated with the following two steps: 1) Center vectors of the interactions’ and recommendations’ feature vectors of the iuser are calculated: where Uand Vare the center vectors of the feature vectors for the interactions and recommendations of the iuser, and Nand Nare the corresponding quantities. Besides, Uand Vare the feature vectors for the jinteraction and recommendation of the iuser, respectively. 2) The difference between the two center vectors are obtained: In the paper, we employ zas the feature vector for the iuser, which takes not only the user’s history but also the predicted preference into consideration. Meanwhile, each user is assigned a label of 1 or 0, indicating their membership (i.e., 1 means member and {( f eature,label)}contains feature vectors and labels of all users, where the pair ( f eature,label) denotes the feature vector and label for the iuser. is established as the attack model A. The output of is a 2-dimension vector representing probabilities for the membership status. For the iuser, the prediction can be formulated as follows: where zis the input of Aas well as the iuser’s feature vector in our attack. And W, W, band bare the parameters updated in the training process. ReLU(·) is an activation function working on the outputs of two hidden layers, and softmax(·) is used for normalization which is required by the cross-entropy loss. Besides, hand hare the results of two hidden layers after ReLU(·). And yis the predicted result for the input z, which is a 2-dimension vector indicating the possibilities of zbelonging to members and non-members, respectively. Parameter Optimization. In this section, the parameter optimization process for the attack model is described. Stochastic gradient descent is adopted to update parameters, aiming to minimize the cross-entropy loss function L: L= −(ylogy+ (1 − y)log(1− y)), where yis the ground truth label for the itarget user. And yis the predicted possibility of the itarget user belonging to members. Besides, Nis the size of training data. Test data Dfor the attack model is generated from the target recommender in the same way as the training data. The trained attack model Aconduct a prediction given a target user feature vector z, i.e., A(z) = y, where y=abis a 2-dimension vector, and the values of a and b indicate the probabilities that the target user belongs to non-members and members respectively. According to the predicted results, the adversary infers the membership status of the target user. Concretely, when a < b, the target user is predicted to be a member. Otherwise, they are predicted to be a non-member. In this section, we ﬁrst demonstrate experimental setup, including recommendation methods, datasets, preprocessing process, evaluation metrics, implementation details, and notations in Section 3.1. Then, we evaluate the performances of original recommender systems in Section 3.2. Moreover, we investigate membership inference attacks against recommender systems in Section 3.3 and conduct detailed analyses on the inﬂuences of hyperparameters in Section 3.4. Finally, we present extensive analysis to comprehensively investigate the attack model in Section 3.5. mendation algorithms are adopted for members, including Item-Based Collaborative Filtering (Item) [40], Latent Factor Model (LFM) and Neural Collaborative Filtering (NCF) [16]. Meanwhile, due to the lack of non-members’ data, a recommender system provides non-members with the most popular items, which is named the popularity recommendation algorithm in our paper. periments, including Amazon Digital Music (ADM) [15], Lastfm-2k (lf-2k) [4], and Movielens-1m (ml-1m) [13], to evaluate our attack strategies. All these datasets are commonly-used benchmark datasets for evaluating recommender systems. Note that only ratings in these datasets are used for our evaluation in the experiments. Scores range from 1 to 5, which indicates how much users like musics (ADM and lf-2k) or movies (ml-1m). disjoint subsets, i.e. a shadow dataset, a target dataset and a dataset for extracting item features. Then, the following processing methods are implemented to these subsets: • To generate feature vectors for users, the dataset for item feature should contain all items of the target and shadow recommenders. • For the shadow or target dataset, we further divide it into two disjoint parts, which are used to conduct recommendations to members and non-members, respectively. Moreover, following the previous work [16], we ﬁlter out the users who have less than 20 interactions. In our experiments, recommender systems conduct recommendations based on implicit feedback. We assign values of 1 to the user-item pairs when there exist interactions between these users and items. And other user-item pairs are assigned 0. In LFM and NCF, recommender systems require both positive and negative instances. We randomly sample negative user-item pairs from the pairs scoring 0 and regard the pairs assigned 1 as positive instances. We keep the same number of negative samples as positive samples for the dataset balance. Evaluation Metrics. We use AUC (area under the ROC curve) as the metric to evaluate attack performances. Following the deﬁnition of the attack, we regard members as positive data points and non-members as negative data points. AUC indicates the proportion of the prediction results being positive to negative. For example, if the attack model utilizes Table 1: Notations for different settings. “∗” stands for any algorithm or dataset used to construct or train the shadow or target model. A∗Trained on the ADM dataset. L∗Trained on the lf-2k dataset. M∗Trained on the ml-1m dataset. ∗IImplemented by Item algorithm. ∗Limplemented by LFM algorithm. ∗Nimplemented by NCF algorithm. AI∗∗The shadow recommender is implementedby Item algorithm on the ADM dataset. ∗∗AIThe target recommender is implementedby Item algorithm on the ADM dataset. AIMNby Item algorithm on the ADM dataset, andthe target recommender is implemented by Random Guess to conduct a membership inference, the AUC is close to 0.5. Implementation Details. We build a MLP with 2 hidden layers as the attack model. The ﬁrst hidden layer has 32 units and the second layer has 8 units, both followed by a ReLU layer. And we utilize a softmax layer as the output layer. For the optimizer, we employ Stochastic Gradient Descent (SGD) with a learning rate of 0.01 and a momentum of 0.7. Besides, we use cross entropy as the loss function and the model is trained for 20 epochs. In the paper, members are recommended by Item, LFM and NCF while non-members are recommended by the popularity recommendation algorithm. Note that, Item and the popularity recommendation algorithm do not need the iterative process of updating parameters. The detailed model conﬁgurations of LFM and NCF are shown as follows: • LFM. We adopt the SGD algorithm to update parameters with a learning rate of 0.01 and conduct LFM with a regularization coefﬁcient of 0.01 to enhance the model’s generalization ability. Then we train the model for 20 epochs. • NCF. We use Adam as the optimizer with a learning rate of 0.001. And we build the MLP part with 3 hidden layers containing 64, 32 and 16 hidden units respectively. Meanwhile, the embedding size of the Generalized Matrix Factorization (GMF) part is 8 [16]. In addition, the number of negative samples corresponding to per positive sample is set to 4. Then we train the model for 20 epochs with a batch size of 256. Notations. To clarify the experimental settings, notations are demonstrated in Table 1, where “∗” stands for any algorithm or dataset used to construct or train the shadow or target model. For example, “A∗” could be the combination of “ADM+Item”, “ADM+LFM” or “ADM+NCF”. Note that, not all possible combinations are listed due to the space limit. Figure 3: The attack performances under the assumption I. Table 2: The HR@100 of the shadow and target recommenders. In the experiments, there are two kinds of combinations in the paper (i.e., 2-letter and 4-letter combinations). For the 2-letter combinations, the ﬁrst letter, i.e., “A,” “L” or “M”, indicates the shadow (or target) dataset, and the second letter, i.e., “I,” “L” and “N”, indicates the recommendation algorithm. For the 4-letter combinations, the ﬁrst two letters represent the dataset and algorithm of the shadow recommender and the last two letters denote the dataset and algorithm of the target recommender. For instance, “AIMN” means that the adversary establishes a shadow recommender with Item on the ADM dataset to attack a target recommender implemented by NCF on the ml-1m dataset. We adopt HR@k as the metric to evaluate the recommendation performance, where k = 100 is consistent with the experimental setting and Hit Rate (HR) presents the proportion of recommendations including the ground truth. We can see from the results in Table 2 that, in general, recommender systems achieve the best performance on the ml-1m dataset. Speciﬁcally, the shadow recommender obtains a hit rate of 0.856 when using LFM on the ml-1m dataset. We perform experiments on the ADM, lf-2k and ml-1m datasets with three typical recommendation algorithms, including Item, LFM and NCF. Experimental results show that our method is able to achieve strong attack performances. We draw the following conclusions: Table 3: The AUC of the attack model against the ADM dataset, under the assumption II. Table 4: The AUC of the attack model against the lf-2k dataset, under the assumption II. Table 5: The AUC of the attack model against the ml-1m dataset, under the assumption II. Assumption I. First of all, the target recommender’s dataset distribution and algorithm are available. And, in the paper, these information is the most knowledge that the adversary can gain from the target recommender. The complete results are shown in Figure 3, in which we compare our attack with Random Guess. Then, data points (members and non-members) are visualized in a 2-dimension space by t-distributed Stochastic Neighbor Embedding (t-SNE) [50]. Figure 4 shows the results of the shadow and target distributions for two datasets, where the red points represent members and the blue points represent non-members. According to the attack and visualization results, we conclude that: • In general, under the assumption that the shadow recommender knows the algorithm and dataset distribution of the target recommender, our attack is very strong. There are two main reasons for the effectiveness. First, data points of members and non-members are tightly clustered separately. Due to the different recommendation methods for members and non-members, generally, the interactions and recommendations of members are more relevant. In that case, the intra-cluster distance of members and non-members is much smaller than the inter-cluster distance between them, so that members can be easily distinguished. Second, as the adversary has the most knowledge of the target recommender, the shadow recommender can well mimic the target recommender. Thus the attack model, which is trained on the ground truth membership generated from the shadow (a) ADM_NCF_shadow (left) and ADM_NCF_target (right)(b) ADM_LFM_shadow (left) and ADM_LFM_target (right) (c) lf-2k_Item_shadow (left) and lf-2k_Item_target (right)(d) lf-2k_LFM_shadow (left) and lf-2k_LFM_target (right) Figure 4: Visualization results by t-SNE, where red points denote members and blue points represent non-members. For the ADM dataset, visualization results, (a) when the shadow and target recommenders are implemented by NCF, and (b) when LFM is used as the shadow and target recommenders, are demonstrated. For the lf-2k dataset, visualization results, (c) when the shadow and target recommenders are implemented by Item, and (d) when LFM is adopted as the shadow and target recommenders, are shown. recommender, is able to conduct a membership inference accurately. • When the target recommender uses Item or NCF, our attack performs considerably better on all datasets. Speciﬁcally, an average AUC of the attack aiming at Item or NCF is 18% and 20% higher respectively. Compared to the visualization result of LFM, the dissimilarity of the shadow and target distributions of Item or NCF is smaller. Thus the attack model can easily deal with the data points which are similar to its training data. Besides, our attack performs better when the target dataset is the ml-1m dataset. This is because the user-item matrix of the ml-1m dataset is the densest among all three datasets, which enormously facilitates the item vectorization and attack model training. Assumption II. To this end, we relax the assumption so that the adversary only has a shadow dataset in the same distribution as the target dataset. The experimental results are shown in Table 3, Table 4 and Table 5, where the attack results of the previous assumption are listed at the diagonals. Then we depict the visualization results of data points in the shadow and target distributions by t-SNE to show the relationship between members and non-members. In Figure 5, the red points are members and the blue points are non-members, which are all from the lf-2k dataset. We can see from the attack performances as well as the comparisons between the shadow and target distributions that: • When the adversary only gains the knowledge about the target dataset distribution, the attack performances drop as expected but are still strong. For instance, on the ADM dataset, when the target recommender uses Item but the shadow recommender uses LFM, the attack performance drops from an AUC of 0.926 to an AUC of 0.843. Decreases also appear on the lf-2k dataset and the ml-1m dataset. That is to say, even with different recommendation methods, the attack model can still beneﬁt from the similar distributions of the target and shadow datasets to conduct the memebership inference accurately. • An interesting ﬁnding is that, the attack on the ml1m dataset achieves the best overall performance (i.e., 0.873 in terms of average AUC), and the attack performance on the ADM dataset is the worst (i.e., 0.747 in terms of average AUC). This is because the user-item matrix built from the ml-1m dataset is the densest while the matrix from the ADM dataset is the sparsest. Intuitively, the attack model can learn more information from a denser user-item matrix, leading to a better attack performance. In addition, we do acknowledge that, in some cases, the attack performances are not ideal. For instance, the attack model against LFM achieves a poor performance (see Table 3). Comparing to the other two recommendation algorithms, LFM has higher model complexity, which makes it harder for the adversary to build a similar shadow model. Assumption III. Finally, we further conduct evaluations when the adversary neither has a shadow dataset in the same distribution as the target dataset nor knows the target algorithm. All experimental results are shown in Figure 6. Note Figure 5: Visualization results by t-SNE, where red points are members and blue points are non-members. For the lf-2k dataset, visualization results, (a) when the shadow recommender is implemented by LFM, (b) when LFM is employed as the target recommender, and (c) when Item is used as the shadow recommender, are demonstrated. Figure 6: The attack performances under the assumption III. The x-axis indicates the target recommender’s datasets (the ﬁrst letter, i.e., “A,” “L” and “M”) and algorithms (the second letter, i.e., “I,” “L” and “N”), and similarly the y-axis represents the shadow recommender’s datasets and algorithms. that, the attack results of the assumption I are listed at the back-diagonal and the attack performances of the assumption II are shown in the three 3× 3 block back-diagonal matrices. Analysing the results, we draw conclusions that: • Even under the minimum assumption, our attack can still achieve strong performances in most cases. For instance, when the target recommender is established by LFM on the ml-1m dataset and the adversary uses NCF to build a shadow recommender on the lf-2k dataset, our attack achieves an AUC of 0.710. • In some cases, when the adversary knows less information about the target recommender, the attack even achieves better performances. For instance, when the adversary builds a shadow recommender by NCF on the lf-2k dataset to mimic the target recommender which uses Item on the ml-1m dataset, our attack achieves an AUC of 0.747. Meanwhile, with the knowledge that the target dataset is the ml-1m dataset, the adversary uses (a) ml-1m_LFM_shadow (left) and ADM_Item_target (right) (b) ADM_LFM_shadow (left) and lf-2k_Item_target (right) LFM to establish a shadow recommender when the target recommender uses NCF, our attack only achieves an AUC of 0.670. To explain this, we adopt the tSNE algorithm to visualize user feature vectors for the “MLAI” and “ALLI” attacks. The visualization results in Figure 7 show that the distributions of feature vectors generated by the shadow model “ML” and target model “AL” are more similar than the distributions generated by “Al” and “LI”. Therefore, the “MLAI” attack performs better than the “ALLI” attack. In summary, our attack can effectively conduct a membership inference against recommender systems, even with the limited knowledge. In this section, we analyse the inﬂuences of hyperparameters, including the number of recommendations k, the length (a) The attack performances against(b) The attack performances against the number of recommendations k.the length of vectors l. (a) The attack performances with different l, when k = 20,(b) The attack performances with different k, when l = k = 50 and k = 100.20, l = 50 and l = 100. of vectors l and the weights of recommendations. Figure 8 shows the experimental results. experiments with different values of k from 10 to 100, in order to explore the inﬂuence of k on the attack. Figure 8a shows the attack performance against the number of recommendations. When the number of recommendations is less than 50, the attack performance improves with the increase of k. Then the performance maintains stable when k goes beyond 50. These results show that the attack model gains more information when the number of recommendations increases. However, the attack model cannot gain more information inﬁnitely when the number of recommendations is large enough. The Length of Vectors l. We evaluate our experiments with different values of l from 10 to 100, in order to explore the inﬂuence of l on the attack. Figure 8b shows the attack performance against the length of vectors. Similar to Figure 8a, when the length of vectors is less than 50, the attack performance improves with the increase of l. Then, in general, no obvious improvement of the performance is observed when l goes beyond 50. These results show that the representation power of the attack model becomes stronger, as a larger length of vectors can provide more dimensional perspectives. However, the attack model cannot improve its representation power inﬁnitely when the length of vectors is large enough. The Weights of Recommendations. We evaluate our experiments with different designs for the weights of recommendations. In the real world, the items recommended to a user are provided in the form of an ordered sequence. And, compared to the items at the back of the sequence, the ones in front of the sequence are more likely to be preferred by the user. Thus we evaluate two methods of assigning weights to items at different positions in the sequence. One is that all items are assigned the same weight of. And the other is to assign a weight ofto the iitem in the sequence. Same as mentioned above, we denote k as the number of recommendations. As shown in Figure 8c, we ﬁnd that considering the order of recommendations can obviously promote attack performances. In this section, we study ﬁve interesting questions and give further results in order to comprehensively investigate our attack method. Which Is More Important, the Length of Feature Vector l or the Number of Recommendations k? In addition to the analyses about hyperparameters in Section 3.4, we also investigate “which is more important, k and l?”. Two more experiments are conducted on the “MIMI” attack, with different l when k is set to 20, 50 and 100 (see Figure 9a), and with different k when l is set to 20, 50 and 100 (see Figure 9b). As the results show, both the number of recommendations (k) and the length of vectors (l) inﬂuence attack performances substantially. Speciﬁcally, when k reduces from 100 to 20, the AUC score drops from 0.998 to 0.764. Similarly, as l reduces from 100 to 20, the AUC score descends from 0.998 to 0.817. Table 6: The AUC of the K-Means algorithm on the lf-2k dataset. Table 7: The AUC of the K-Means algorithm on the ml-1m dataset. What Is the Impact of the Dataset Size? Considering the size of the training dataset imposes huge impacts on machine learning models, we conduct evaluations regarding the size of the shadow dataset. Speciﬁcally, the size of the shadow dataset is reduced to 90%, 80%, and 70% of the original size. Note that, the ratio of members to non-members keep unchanged for the dataset balance. For the “LLLL” attack, the AUC scores of the attack performances are decreased to 0.633, 0.714, and 0.746, respectively, when the size of the shadow dataset is 70%, 80%, and 90% of the original size. Comparing to the original AUC of 0.777, we can conclude that a larger shadow dataset usually leads to a better-trained attack model. strate the effectiveness of our attack model, we evaluate the attacks utilizing K-Means to distinguish non-members from members on the lf-2k and ml-1m datasets. The results are shown in Table 6 and Table 7, where Item, LFM, and NCF in the ﬁrst column are shadow algorithms for our attack, and KMeans is used to cluster non-members and members. Since there are only two classes, i.e., members and non-members, the number of classes K for K-Means is set to 2. From the results, we can conclude that our attack outperforms K-Means largely, indicating the validity of our attack model. For instance, when the K-Means algorithm infers the membership status from the target recommender “LI”, the performance is much worse than our attack (0.649 v.s. 0.939). Which Is the Best User Feature Generation Method? To further verify the effectiveness of our attack, we adopt different aggregation methods to generate user feature vectors. Besides the method used in our attack, Origin, we evaluate 5 user feature generation methods. • Concat10 concatenates feature vectors of the ﬁrst 10 interactions and the ﬁrst 10 recommendations for each user. Figure 10: The attack performances with different user feature generation methods • Concat20 concatenates feature vectors of the ﬁrst 20 interactions and the ﬁrst 20 recommendations for each user. • Concat100 concatenates feature vectors of the ﬁrst 20 interactions and all the recommendations (i.e., k = 100) for each user. • Hadamard respectively conducts Hadamard products on feature vectors of all the interactions and recommendations to obtain two vectors for each user. Afterwards, following the similar steps in Section 2.5, we use the difference of these two vectors as the user feature vector. • Similarity ﬁrst conducts dot products between feature vectors of each recommendation and all interactions, then concatenates the average of dot product results for each recommendation into the user feature vector. The results are shown in Figure 10. We ﬁnd that our method outperforms all the other aggregation methods. For instance, on the settings of AIAI, our method outperforms Concat10 and Similarity by 5% and 63%. These results demonstrate the effectiveness of our user feature generation method in the attack process. Is It Possible to Generalize Our Attacks to Content-Based Recommender Systems? To further verify the effectiveness of our attack, we conduct evaluations on the membership inference attacks against a content-based recommender system. Different from the recommendation algorithms used in the previous attacks, a content-based recommender system aims to distinguish users’ likes from dislikes based on their metadata (such as description of items and proﬁles of users) [51]. The evaluation is conducted on the ml-1m dataset with information about items and users, under the assumption I, i.e., the target recommender’s algorithm and dataset distribution are available. The results are depicted in Figure 11. We conclude that our attack achieves a strong performance against a content-based Figure 11: The attack performances for the content-based recommender system on the ml-1m dataset. recommendation algorithm (i.e., 0.986 in terms of AUC), indicating a high generalization ability of our attack model. Furthermore, we evaluate this attack using different feature generation methods mentioned above. Results show that our aggregation method also outperforms other baselines on a content-based recommendation system. The experimental results show that our attack can conduct an effective membership inference against recommender systems. When the adversary knows the algorithm and dataset distribution of the target recommender, the attack achieves the strongest performance. Later in the experiments, we gradually relax the assumptions and show that our attack can still effectively conduct the membership inference, demonstrating that our attack has a good generalization ability. Furthermore, we explore the inﬂuences of hyperparameters. With the increase of the number of recommendations k and the length of vectors l, the attack performance improves or maintains stable, however, the cost continuously increases. In that case, we are able to ﬁnd a balance, where the attack performance is strong and the cost is affordable. And the exploration of the weights of recommendations shows that the more information is available, the more powerful the attack is. The above experiments show the effectiveness of our attack. Meanwhile, to defend the membership inference against recommender systems, we also propose a countermeasure, named Popularity Randomization, to mitigate the attack risk. In the original setting in Section 2.5, non-members are provided with the most popular items. As a result, feature vectors of non-members are extremely similar and easily distinguished from members. To address this problem, we increase the randomness of non-members’ recommendations. Speciﬁcally, we ﬁrst select candidates from the most popular items. Then, a random selection is conducted on the candidates, i.e., we randomly pick 10% of candidates as recommendations for non-members. The detailed methodology is demonstrated in Appendix A.1. Figure 12: Comparisons of attack performances before and after deploying the defense mechanism. To evaluate the effectiveness of the defense mechanism, we conduct experiments under the assumption that the dataset distribution and algorithm of the target recommender are available. Figure 12 shows the attack performances before and after deploying the defense mechanism. The blue bar denotes the attack performances with the original setting, i.e., the popularity recommendation algorithm. And the orange bar represents the attack performances with the defense mechanism, i.e., Popularity Randomization. From the results, we conclude that Popularity Randomization considerably decreases the performance of our attack. Specifically, the defense mechanism decreases the AUC scores of the attack model by more than 12%, 33% and 41% respectively when the target recommender uses Item, LFM or NCF. With the defense strategy, attacking the target recommender using LFM achieves the lowest AUC score on all three datasets. When the target recommender using LFM, our attack with the defense mechanism only achieves 0.513, 0.501, and 0.500 AUC scores on the ADM dataset, the lf-2k dataset and the ml-1m dataset, respectively (detailed explanations are demonstrated in Appendix A.2). Besides, as Figure 12 shows, the attack performances against NCF decrease most hugely. For instance, the AUC score of the attack against NCF on the ADM dataset drops from 0.987 to 0.576. In contrast, the attack with Popularity Randomization against Item can still achieve strong performances. For instance, on the ADM dataset, the attack with Popularity Randomization can attain 0.812 in terms of AUC when the target algorithm is Item. Compared to the attack with the original setting, the defense mechanism only achieves a 12% drop in the attack performance. Item is the simplest one among the three recommendation methods, which makes it easier for the adversary to build a similar shadow recommender with the target recommender. This leads to a stronger attack but more ineffective defense. In contrast, the other two recommender systems have more complex model structures, leading to substantial decreases in attack performances with the defense strategy. In addition, visualization results and impacts on recommendation performances are comprehensively analyzed in Appendix A.2 and Appendix A.3, respectively. Membership Inference. The goal of membership inference is to infer whether a target data sample is used to train a machine learning model [6,22,26,28,30,39,42,43,52]. Shokri et al. [43] propose the ﬁrst membership inference attack in this domain. The authors have made several key assumptions for the adversary, such as multiple shadow models and a shadow dataset which comes from the same distribution as the target model’s training datasset. Salem et al. [39] gradually relax these assumptions and broaden the scenarios of membership inference attacks. Later, Nasr et al. [31] conduct a comprehensive membership privacy assessment in both centralized and federated learning setting. In particular, they propose the ﬁrst membership inference when the adversary has white-box to the target model. Other research has shown that membership inference is effective under other machine learning settings, such as generative models [14], federated learning [8,29], and natural language models [45]. Besides, a plethora of other attacks have been proposed against machine learning models [3,5,7,12,20,21,33,48,49]. recommendation techniques have been applied in various scenarios [11, 23, 40]. Sarwar et al. [40] explore item-based collaborative ﬁltering (CF) techniques which enhance the scalability and quality of the CF-based algorithms. Besides, Deshpande and Karypis [11, 23] present item-based top-N recommendation algorithms to promote the efﬁciency and performance. Latent Factor Models. LFM aims to ﬁnd some latent factors and is commonly implemented by Matrix Factorization (MF) [24, 25, 36, 38]. Polat et al. [36] combine SVD-based Collaborative Filtering with privacy to achieve accurate predictions while preserving privacy. Later, Salakhutdinov et al. [38] propose the Probabilistic Matrix Factorization which scales linearly with the number of observations and performs well on very sparse and imbalanced datasets. Koren [24] presents an integrated model that combines the neighborhood and LFM, which optimizes a global cost function and integrates implicit feedback into the model. Furthermore, Koren [25] presents a methodology for modeling time drifting user preference in the context of recommender systems. Neural Collaborative Filtering. With the advancement of deep learning techniques, recommendation algorithms with neural networks has been in blossom [2, 9, 16, 46, 47]. He et al. [16] propose the ﬁrst framework for collaborative ﬁltering based on neural networks to model latent features of users and items. They show that MF can be interpreted as a specialization of NCF and utilize a MLP to endow NCF modelling with a high level of non-linearities. Later, Bai et al. [2] present a model which integrates neighborhood information into NCF, namely Neighborhood-based Neural Collaborative Filtering. Another recent work is that Chen et al. [9] design a Joint Neural Collaborative Filtering model which enables deep feature learning and deep user-item interaction modeling to be tightly coupled and jointly optimized in a single neural network. In the previous evaluations, our attack shows its effectiveness as well as strong generalization ability. Moreover, the proposed defense mechanism, Popularity Randomization, can also mitigate the attack performances considerably. Furthermore, to obtain a comprehensive understanding of membership inference attacks, in this section, we focus on three important factors that largely inﬂuence attack performances: the choice of datasets, the selection of recommendation algorithms, and distributions of generated user features. The detailed explanations are demonstrated as follows: The Choice of Datasets. The dataset with a denser useritem matrix leads to better attack performances. The richer information in a denser user-item matrix considerably facilitates the process of the item vectorization and attack model training. From the results and analyses under the assumption II in Section 3.3, we can see that the attack on the ml-1m dataset achieves the best overall performance (i.e., 0.873 in terms of average AUC) as the user-item matrix built from this dataset is the densest. The Selection of Recommendation Algorithms. It is easier for the adversary to attack against a recommender system with a simpler model structure. As the results (see Table 3) under the assumption II show, the attack against LFM achieves poor performances. Comparing to the other two recommendation algorithms, LFM has higher model complexity, which makes it harder to attack. Meanwhile, defending a simpler recommender system is more difﬁcult. As the evaluations of the defense (see Figure 12) in Section 4 show, attacks against Item perform strongly even with the defense mechanism. This is because Item has the simplest structure among the three recommendation algorithms. In short, a recommender system established by a simple algorithm structure is usually more vulnerable to membership inference attacks. Distributions of Generated User Features. The combination of the dataset and recommender algorithm also matters. Higher attack performances can be obtained, when the distribution of user feature vectors generated by the shadow recommender system is more similar to the distribution generated by the target recommender system. Trained with samples from a similar distribution, attack models are able to conduct an accurate inference. Speciﬁcally, in Figure 6, the attack of “MLAI” achieves a better performance than the one of “ALLI” (0.608 v.s. 0.547). And we see from the visualization results in Figure 7 that the above advantage comes from the smaller difference of feature distributions between the shadow recommender “ML” and the target recommender “AI” than the one between “AL” and “LI”. In summary, training data with distributions similar to the target data can boost the attack performances. Recommender systems have achieved tremendous success in real-world applications. However, data used by recommender systems is highly sensitive. In that case, successfully inferring a user’s membership status from a target recommender may lead to severe privacy consequences. In this paper, to investigate the privacy problem in recommender systems, we design various attack strategies of membership inference. To the best of our knowledge, ours is the ﬁrst work on the membership inference attacks against recommender systems. Comparing to membership inference attacks on data sample-level classiﬁers, for recommender systems, our work focuses on the user-level membership status, which cannot be directly obtained from the system outputs. To address these challenges, we propose a novel membership inference attack scheme, the core of which is to obtain userlevel feature vectors based on the interactions between users and the target recommender, and input these feature vectors into attack models. Extensive experiment results show the effectiveness and generalization ability of our attack. To remedy the situation, we further propose a defense mechanism, namely Popularity Randomization. Our empirical evaluations demonstrate that Popularity Randomization can largely mitigate the privacy risks. This work was supported by the Natural Science Foundation of China (61902219, 61972234, 62072279, 62102234), the Helmholtz Association within the project “Trustworthy Federated Data Analytics” (TFDA) (funding number ZT-I-OO1 4), the National Key R&D Program of China with grant No. 2020YFB1406704, the Key Scientiﬁc and Technological Innovation Program of Shandong Province (2019JZZY010129), Shandong University multidisciplinary research and innovation team of young scholars (No. 2020QNQT017), and the Tencent WeChat Rhino-Bird Focused Research Program (JR-WXG2021411). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.