The ability to digest knowledge has always been a vital characteristic of human intelligence. It is known that a student’s learning performance is not only determined by her ability to digest different learning concepts but also signiﬁcantly affected by the teaching strategy of her teacher. A good teacher would optimize a teaching strategy of learning materials, exercises, and problem-solving techniques to enable a student to achieve her learning objectives. This is typically performed by tracing a student’s knowledge progress over important learning concepts in a learning task, e.g., addition, subtraction, and multiplication in elementary math course. A human teacher can evolve such a teaching strategy according to the performance level of a student. This developmental evolution of a teaching strategy is the key to unlocking students’ full potential at different levels. A question arising is: can a machine learn to teach in a similar manner to a human teacher? In a machine learning scenario, a teaching strategy often includes the task of prioritizing training data (equivalent to learning materials in human learning), the choice of a loss function (equivalent to assessments in human learning), and the hyper-parameter conﬁguration of a hypothesis function (equivalent to problemsolving techniques in human learning). A machine may target one or more of these teaching dimensions to evolve an effective teaching strategy. In the past years, a number of attempts were made to optimize the training procedure of a machine learning student model. Curriculum learning methods [1], [2], [3] aimed at ranking training samples based on their difﬁculty levels to build a good learning curriculum for a student model. Similarly, self-paced learning (SPL) methods [4], [5], [6] used a hardness threshold that gradually increases with the progress of a student to build a training data curriculum. Machine teaching methods [7], [8] focused on selecting optimal training samples that minimize a teaching cost (e.g., the size of a training set). Dynamic loss functions [9] and graduated optimization [10] methods adjust the difﬁculty of a loss function according to the progress of a student’s learning. Despite considerable progress being made, these methods either depend on heuristic rules (e.g., hardness or difﬁculty thresholds) or assume a pre-deﬁned student model to drive a teaching strategy. Recently, reinforcement learning (RL) has been proposed to develop teaching strategies [9], [11]. Generally, it involves two building blocks: a teacher model and a student model. A teacher model aims to optimize a teaching strategy, while a student model follows the teaching strategy to optimize its learning objective. However, these existing works have several limitations. First, they depend on hand-crafted states, ignoring the fact that a student model may have different performances on different learning concepts in a learning task. Second, they require a careful assignment of a target performance threshold for each learning task based on sparse reward functions over a state space (e.g., positively rewarding a teacher model only if a student model performs above a speciﬁed threshold value). This demands taskspeciﬁc expertise during the RL training to land on an effective teaching policy [12]. To address these limitations, our propose a novel framework for developing data teaching strategies, namely Knowledge Augmented Data Teaching (KADT). At its core, the KADT method is equipped with a powerful representation Fig. 1: Comparing the proposed KADT method and a conventional data teaching method. learning ability for capturing a student model’s performance by leveraging knowledge tracing techniques [13], [14], [15]. Speciﬁcally, the KADT method employs a keyvalue memory architecture to learn the knowledge progress of a student model in terms of underlying learning concepts involved in a learning task. This offers several learning advantages: (a) It provides the ability to automatically learn latent learning concepts from training samples in different learning tasks, without explicitly needing any a priori knowledge. (b) It can dynamically track how a student model performs over the learning concepts of a learning task over time (i.e., during the teaching process). In addition to these, the KADT method incorporates several novel RL designs, in order to exploit the capacity of a student model and develop a data teaching strategy that matches its capacity to help a student model perform as best as possible. (1) It uses an attentive pooling technique to distill knowledge representations of a student model with respect to class labels through tracing the representations of samples over latent learning concepts. (2) It selects actions directly based on a data teaching strategy, in contrast to comparative RL approaches [11], where actions depend on the outcome of a preemptive random sampling used to control the complexity of the action space. (3) It uses a dense reward function, which does not require any manual efforts for deciding rewards. However, a sparse reward function as in [11] requires choosing a threshold that allows only positive rewards. Manually specifying a good reward threshold is difﬁcult, especially for intricate learning tasks. Contributions. To summarize, the main contributions of this work are below: which enables a coupled optimization of knowledge representation learning and teaching strategy learning through the interaction among a student model, a knowledge tracing model and a teaching agent. nique that can dynamically trace performance of a student model over different learning concepts in any supervised learning task. mechanism that distills a state representation from pooled knowledge representations of a student model with respect to class labels, while accounting for importance of individual training samples. kinds of learning tasks against the state-of-the-art methods. The results empirically validate that the KADT method consistently outperforms the other methods on all the tasks. Outline. The reminder of this paper is organized as follows. Section 2 presents the problem deﬁnition. Section 3 describes our methodology. Section 4 introduces the experimental design. Section 5 discusses the evaluation results. Section 6 reviews the related work and we conclude the paper in Section 7. In supervised learning, a learning problem is typically formulated as follows. Given a training set D= {(x, y)}consisting of samples xfrom a sample space X and their labels yfrom a label space Y , a hypothesis function h: X → Y , parameterized by θ, which maps a sample space X to a label space Y , and a function η that measures the performance of hon Dafter it is trained on D, a supervised learning problem is to ﬁnd the optimal parameters of the hypothesis function which maximize the performance: where Θ is a parameter search space for the hypothesis function. We call such a hypothesis function a student model which can be any supervised machine learning model (e.g., a deep neural network or a simple linear regression model), and Equation 1 represents its optimization objective. To teach a student model in solving a supervised learning problem (i.e., guide the search for θ), an effective data teaching strategy needs to be found. Such a data teaching strategy can be conﬁgured through a sequence of training mini-batches D = {ˆD,ˆD, . . . ,ˆD} being sampled from the training set D. Hence, given a teacher model g, parameterized by ω, the teacher model learns to ﬁnd a good data teaching strategy g(D) = D for a student model hto maximize its performance, formulated as in the following optimization problem: where Ω is a parameter search space for the teacher model. In this paper, we aim to develop a reinforcement learning framework in which a teacher model can optimise a data teaching strategy dynamically according to the performance of a student model. In this section, we present our proposed method, namely Knowledge Augmented Data Teaching (KADT). Figure 2 illustrates the architecture of KADT. Given a student model that tackles a supervised learning task, KADT uses a knowledge tracing model to trace the knowledge of the student model in performing this supervised learning task, and a teaching agent to optimise a data teaching strategy for the student model in order to maximize its performance. Fig. 2: Architecture of the proposed KADT method, which involves three main components: a student model, a knowledge tracing model and a teaching agent. We notice that the performance of a student model largely depends on how it acquires knowledge from different learning concepts involved in a task, e.g., movie genres in a classiﬁcation task for rating movies. Thus, inspired by knowledge tracing methods [14], [15], we design a memory-augmented knowledge tracing (KT) model to capture knowledge representation of a student model based on its past interaction history. The KT model has a key-value memory structure M = hM, Mi, where M∈ Ris a static matrix, called the key matrix, and M∈ Ris a dynamic matrix, called the value matrix. Let C = {c, . . . , c} be N learning concepts underlying the knowledge representation of a student model. Then Mstores encoding keys that represent learning concepts, and Mstores the performance information of a student model for each learning concept, which is dynamically changing over time. Thus, the dimension N reﬂects the number of learning concepts in the task, while dand dare dimensions of memory slots. We set the slot dimensions d= d= 50 based on our empirical analysis, while the number of slots N is set differently for each dataset (see further discussion in Section 4.1). Given a sample x from the training mini-batchˆD at the time step t, we get its embedding vector u ∈ Rby embedding its one-hot encoding vector δ(x) ∈ Rwith an embedding matrix A ∈ R. Then, a dot product between u and each key slot M(i) in the key matrix M is supplied to a Softmax layer to get the relevancy vector w ∈ R: knowledge representation where Softmax(z) = e/Pe. Intuitively, the relevancy vector w reﬂects the relevance between the sample x and the learning concepts in M. After calculating the relevancy vector w, the KT model proceeds in two stages. First, it reads from the value matrix Musing w to predict the expected loss of the student model. Second, it updates Mafter acquiring the actual loss from the student model. We call these stages the read stage and the write stage, respectively, and discuss them further in detail. 1) Read Stage: In the read stage, the KT model retrieves the student’s performance information with regard to the sample x to predict its expected loss. First, the relevancy vector w is used to calculate a weighted sum from the memory slots of the value matrix M, which yields the read vector r ∈ R: Then, the read vector r is concatenated with the embedding vector u of the sample x, and then fed to a Tanh layer to calculate the representation vector f ∈ R: where Tanh(z) = (e− e)/(e+ e), W∈ Ris the weight matrix of the Tanh layer, and bis a bias vector. This representation vector captures both the current knowledge progress of the student model over sample x and the sample information itself. Finally, we input the representation vector f to a linear function and take the dot product of the output vector with δ(x) to predict the estimated lossˆlof the student model for the sample x as follows: where W∈ Ris a weight matrix, bis a bias vector, andˆlis a scalar value. 2) Write Stage: After the student model predicts the class label ˆy of x, we acquire the actual loss l= `(ˆy, y), where `(·) is the loss function of the student model. Then, the value memory Mis updated to reﬂect the student model’s current performance via the write stage. Speciﬁcally, for each sample x and its prediction error  = |ˆy − y|, we embed them using an embedding matrix B ∈ Rto get an embedding vector j ∈ R. Afterwards, we concatenate j with the representation vector f to add the corresponding performance information. The resulting write vector v = [f, j] ∈ Ris used to update the value memory Mthrough erase and add signals [16]. Based on the write vector v, we update the value matrix M. This is done through erasing the existing information from Musing an erase signal e ∈ R, and then adding new information to Musing an addition signal a ∈ R. Let W∈ Rbe a weight matrix and bis a bias vector. The erase signal is calculated as follows: where Sigmoid(z) = 1/(1 + e). Let 1 be a row vector of all ones, the updated value matrix˜Mis calculated using element-wise multiplication as follows: Thus, the islot of Mis erased (i.e., set to zero) if the corresponding values in the relevancy vector w and the erase signal e are both equal to one, and remains unchanged if either of them is zero. Let W∈ Rbe a weight matrix. The addition signal a is calculated as below: Finally, the value matrix Mis updated as: 3) Model Optimization: To optimize the KT model’s parameters (i.e., the embedding matrices, weight and bias parameters of different neural layers), we utilize the Root Mean Square Error (RMSE) function to calculate the differences between the estimated lossˆland the actual loss l, acquired after the student model predicates the class label for each sample inˆD. The KT model is trained using the following loss function: After calculating the RMSE error based on the current training mini-batch, the parameters of the KT model are updated using gradient decent through back-propagation. 3.2 Data Teaching Strategies In this work, we design a teacher model in a reinforcement learning framework, called teaching agent. The teaching agent aims to optimize a teaching policy (i.e., a data teaching strategy) for a student model guided by the knowledge representation of a student model learnt by the KT model. 1) Teaching Interactions: Following the reinforcement learning paradigm [17], [18], we model teaching interactions as a Markov decision process (MDP), represented by a tuple (S, A, T, R). Here, S is a state space, A is an action space, T : S × A × S → [0, 1] is a state transition function such that T (s, a, s) = P (s|a, s) represents transition probabilities between states after executing actions, and R : S × A → R is a reward function. Below, we discuss them in detail. States. Each state s∈ S in our work is represented as a matrix s∈ R, where O is the number of class labels and N is the number of learning concepts in a learning task. Intuitively, each row in this matrix represents the knowledge of a student model corresponding to a class label, which is pooled from the representation vectors of its training samples w.r.t. the underlying learning concepts (i.e., columns). Speciﬁcally, we distill the latest knowledge representation of a given class label y by calculating a pooled knowledge vector g∈ Rfrom the knowledge vectors of its training samples in the lately sampled mini-batchˆD. The knowledge vectors of a mini-batch are projected by the read stage of the KT model (see Section 3.1). In the ﬁrst teaching interaction, the mini-batchˆDis sampled using a uniform random distribution across all class labels and training samples (i.e., sampling from all class labels with an equal probability and treating samples in each class label as being equally likely); afterwards, it is the lately sampled mini-batch by our teaching agent. We follow an attentive pooling method to calculate gas follows: where I is the total number of training samples belonging to class label y in the mini-batchˆD, α∈ [0, 1] is the attention weight for sample xat time point t, and fis the knowledge representation vector of sample xat time point t calculated as per Equation 5. The attention weight αfor sample xis calculated using a gated attention mechanism [19], [20], which controls the amount of information to pass from each sample to the pooled knowledge vector of its corresponding class label: GA(f) = exp(c(T anh(Wf)  Sigmoid(Uf)√) (13) where c ∈ Ris a learnable weight vector, W ∈ Ris a learnable weight matrix for the T anh function, U ∈ R is a learnable weight matrix for the Sigmoid function,√ operator  is an element-wise product,N is a scaling factor to control the output range and prevent gradient vanishing [21], and J is the total number of training samples belonging to the class label in the mini-batch. The Sigmoid function acts as a gating ﬁlter to control the information to pass from the T anh function. Actions. Given a state, the teaching agent selects a mini-batch of training samples and sends this mini-batch to the student model to train on during each interaction. An action a∈ Ris a sampling vector normalized using a Sof tmax output layer, satisfying: Each a(i) corresponds to a class label, and the value of a(i) indicates the percentage to sample from the corresponding class label in the next mini-batch. To sample the next minibatch, we utilize a weighted sampling method that takes the action aand the latest attention weights over training samples in each class label and outputs the mini-batchˆD. Considering attention weights during sampling counts for important samples that could advance the learning progress of a student over the coming teaching interactions. As we follow an efﬁcient attentive pooling method that distills the knowledge representation of a given class label only using its training samples in the lately sampled minibatch, we depend on an estimation technique to get the attention weights of its remaining samples that were not lately sampled. Our technique estimates the weights of other samples in class label y through a moving average given the weights of the present samples as follows: where ˆαis a weighted sum estimate of attention weight for sample xin class label y, I is the total number of training samples belonging to class label y in the mini-batchˆD, and Γ is the number of previous teaching interactions. Reward Function: Let p= η(h,ˆD, D) denotes the performance of a student model hat the time step t after being trained on the mini-batchˆD, and validated on validation set D. Then, our reward function R is calculated to reﬂect the magnitude of the performance change for a student model deﬁned as the below piecewise function : R(s, a) =− p| ≤ ;(p− p) otherwise.(18) In order to prevent oscillations over small variations, this reward function is designed to be -insensitive to the magnitude of the performance change, where  ≤ 0.1 is a hyperparameter. The key idea behind this reward function is to positively reward actions that can improve the learning progress of a student model and penalize those that slow down the progress. The effectiveness of using the learning progress as a reward signal has been previously studied in the RL literature [22], [23]. In our work, enhancing the learning progress needs balanced mini-batches that contain new or challenging samples in addition to known ones to prevent forgetting. For example, selecting training mini-batches that have been correctly classiﬁed by a student model will not enhance the performance; hence, our reward function will generate a low reward value, i.e., zero, since it will lead to the same performance. On the other hand, selecting only hard training mini-batches will also decrease the performance of a student model in the long run and our reward function yields low reward values accordingly. By setting the reward function around the learning progress, we do not need to use heuristic rules (e.g., hardness thresholds) that require domain knowledge and might bias the learning process toward speciﬁc samples. Remark. There are two possible ways for designing our reward function: sparse reward function and dense reward function. A sparse function, as used in L2T [11], requires to specify a performance threshold. Thus, only values that exceed the performance threshold yield positive reward signals; otherwise, the reward function would give zero. However, setting a performance threshold is a difﬁcult task in real-world applications, and an unrealistic performance threshold would negatively affect the effectiveness of a teaching policy. Moreover, learning with sparse reward signals often leads to unstable behaviors and slower convergence, in comparison with using a dense reward function [12]. Therefore, we design a dense reward function that maps the performance change of a student model into a reward value, rather than using a sparse reward function with a pre-deﬁned performance threshold. 2) Actor-Critic Algorithm: We design our RL algorithm based on deep deterministic policy gradient (DDPG) [24], consisting of two building blocks: (1) a critic network Q(s, a |θ) estimates the Q-value of the current state-action pairs; (2) an actor network ψ(s |θ) that learns to select an optimal action for the current state. Following DDPG, we use the technique of reply buffer (RB) to build a mini-batch V of (state, action, reward) transitions which updates the parameters of the critic and actor networks through gradient decent. We also follow the concept of target networks which are versions of the critic and actor networks with parameter values (θ, θ) being updated proportionally to the latest actor and critic values (θ, θ) with a delay factor τ << 1 as shown in Equations 19 and 20: The purpose of these target networks is to make the optimization of the actor and critic networks stable as we calculate the gradient updates based on the Q-value estimate from the critic network itself [24]. The critic network’s parameters are optimized to minimize the temporal difference (TD) loss [12] shown in Equation 21. The actor network’s parameters along with the attention learnable parameters are updated through the policy gradient function [24] depicted in Equation 22. 1X(Q(s, a|θ) −(r+ γ maxQ (s, a|θ)) |V | ∇J ≈1|V |∇Q(s, a |θ) |∇ψ(s |θ) | where γ is the discounting factor and |V | is the length of a transitions mini-batch sampled from the reply buffer. The action exploration is performed randomly using the Ornstein–Uhlenbeck (OU) stochastic process [24], which generates temporally correlated exploration noise for a smooth transition across action values. The OU process is calculated as per Equation 23: where θ, σ, and β are parameters. Wrepresents the Wiener process [24], which is a stochastic process being initialized as W= 0 and incremented by a Gaussian random value (W− W) ∼ N(0, t− t) ∀0 ≤ t< tat each time step. Algorithm 1 describes the main steps for training our KADT method. 3) Model Optimization: The optimization objective of the teaching agent is to ﬁnd an optimal teaching policy π: S → A that maximizes the average reward gain: where Π is a teaching policy search space and E is the total number of teaching episodes. We optimize the actor-critic parameters (θ, θ) using Equations 21 and 22 to ﬁnd the optimal teaching policy π. We conduct experiments to evaluate our proposed KADT method against the state-of-the-art methods. These experiments aim to answer the following research questions: Below, we introduce our experimental setup for answering the above research questions. 4.1 Datasets We consider four different kinds of learning tasks in our experiments: a knowledge tracing task, a sentiment analysis task, a movie recommendation task, and an image classiﬁcation task. For these tasks, we use four different datasets as follows (see Table 1): Knowledge Tracing on ASSISTments2009The dataset ASSISTments2009 was collected during the school year 2009 − 2010 using the ASSISTments online education website. It consists of 110 distinct questions answered by 4, 151 students which leads to a total number of 325, 637 exercises. The questions are represented as one-hot encoding vectors (i.e., one value in the corresponding index of each question in the dataset and zeros elsewhere). Based on previous studies conducted on this dataset [14], [15], there are 10 different learning concepts in the questions. There are two class labels in this dataset including correct and incorrect. Sentiment Analysis on IMDB: The IMDB [25] dataset includes movie reviews in text with a total of 50, 000 reviews. There are two class labels: positive review and negative review. The overall distribution of labels is balanced, with 25k for the positive review and 25k for the negative review. The text reviews are embedded using the word2vec embedding TABLE 1: Dataset statistics and the corresponding student model setup, where the sizes of mini-batches are set based on empirical analysis on validation sets. with dimension size of 256. There are 11 learning concepts representing the general categories of movies reviewed in the text. Movie Recommendations on MovieLens: The MovieLens dataset includes 1 million movie ratings performed by 6000 users on 4000 movies, released in 2003. Each move rating has three features: user id, movie id, and timestamp. There are ﬁve class labels representing a ﬁve star rating (i.e., from 1 to 5), and 18 learning concepts representing the movie genres. We follow the setting of Click-Through Rate (CTR) (i.e., binary labels: click and no click) by regarding labels 4 and 5 as click and labels 1 to 3 as no-click. This allows us to standardize this evaluation into a classiﬁcation task. CIFAR-100 Image Classiﬁcation: The CIFAR-100 dataset was collected by [26]. The dataset consists of 60000 RGB images of 32x32 pixels. There are 100 class labels with 600 images for each class. These 100 classes in CIFAR100 are grouped into 20 super-classes, and each super-class corresponds to a learning concept. Thus, we use 20 learning concepts in this dataset. 4.2 Baseline Methods We discuss the baseline methods used in the teacher model setup and the student model setup in our experiments. Teacher Model Setup. We compare the performance of the proposed method KADT against two state-of-the-art methods: by ﬁltering out training examples that have a loss value exceeding a predeﬁned hardness threshold. This threshold value increases gradually during the training time. inforcement learning agent to sample training minibatches for a student model. The agent optimizes a sparse reward function that yields a positive reward only when the student model’s performance exceeds a predeﬁned performance threshold. In addition, we use random sampling as a baseline model and refer to it as “RandomTeach” provides the conventional data teaching strategy, i.e., sampling mini-batches from a uniform random distribution. For the SPL and L2T methods, we follow the same parameter conﬁguration as described in [4] and [11], respectively. Each of these methods has an additional hyperparameter, i.e., the hardness threshold in SPL and the performance threshold in L2T. For SPL, we use a range of values {80, 100, 120, 150, 180} for the hardness threshold. For L2T, we follow the heuristic strategy in [11] that sets the best performance value from the past episodes as the threshold for the following ones. We report their best results. Student Model Setup. We consider two different student models for each dataset. For the knowledge tracing task, we use deep knowledge tracing (DKT) [13] and sequential keyvalue knowledge tracing (SKVMN) [15]. For the sentiment analysis task, we use a support vector machine with a radial basis kernel (SVM) and a long short-term memory (LSTM) [27] model. For the recommendation task, we use a matrix factorization (MF) model [28] and the same LSTM model from the sentiment analysis task. For the image classiﬁcation task, we use a multi-layer perceptron (MLP) and a convolutional neural network (CNN) [29]. We follow the same design and hyper-parameter conﬁguration as per the cited work for these models. 4.3 Training and Testing Strategies We divide each dataset into a train-validate set and a test set using a (70% − 30%) ratio. Then, the train-validate set is further divided into Dand Dusing a 5-fold cross validation. Training process. The training process consists of two phases in our experiments. phase is to train a teacher model for learning a data teaching strategy. This is achieved by assigning Fig. 4: AUC results averaged over 5 runs for the teaching with different student mode over four datasets. a ﬁxed number of 350 training episodes, which is selected through empirical analysis. Each training episode has 50 time steps (i.e., episode time horizon) for the L2T and KADT methods and an equivalent number of training epochs for the SPL model. For example, in the IMDB dataset, we have a total of 50,000 samples and then the size of the train-validate set is 35,000. Following 5-fold cross validation, the number of training samples is 28,000, which leads to a mini-batch size of 16. The equivalent number of epochs for 350 episodes is 10 epochs. Note that RandomTeach does not have this phase because there are no parameters to optimize for its data teaching strategy. For the reward calculation in the L2T and KADT methods, we use 5% of the validation data. is to apply the data teaching strategy learned in Phase 1 on a new student model. There are two different modes: (a) using a student model that is the same as the student model in Phase 1 but with re-initialized parameters (see more details in Section 4.4), and (b) using a student model that is different from the student model in Phase 1 (see more details in Section 4.4). In this phase, the teacher model is not allowed to update its parameters (i.e., ﬁx the teaching strategy). We give the same number of episodes and epochs as per Phase 1 to apply the data teaching strategies on the student model. Testing process. In the testing process, we evaluate the performance of a teacher model by applying the student model trained by its data teaching strategy in Phase 2 on the test set D. To answer RQ1 and RQ2, we design our experiments in two modes: teaching with similar student mode and teaching with different student mode. We present the details of these two modes below: Teaching with similar student mode. In this mode, we use the same type of student models (e.g., SVM) across Phases 1 and 2. To evaluate how effectively a data teaching strategy is learned from the training phase, we randomly initialize the parameters of a student model in the testing phase. This mode evaluates how well the same student model can be taught on new samples (i.e., test samples) from the same dataset. Teaching with different student mode. In this mode, we change the types of student models across Phases 1 and 2. For example, if we train a feed-forward neural network student model, then we test this model by replacing it with a recurrent neural network student model. This mode aims to evaluate the generalization of data teaching strategies optimised by a teacher model over different student models. In this section, we present the results of our experiments and discuss our observations. We answer the research questions RQ1 - RQ5 in Section 5.1 - Section 5.5, respectively. 5.1 Teaching with Similar Student Models Table 2 shows the average classiﬁcation accuracy achieved by the student models in the teaching with similar student mode. It can be observed that our KADT method outperforms the other methods on all datasets with a statistically signiﬁcant margin (p-value < 0.05), and it is followed by the L2T method. This shows the effectiveness of using reinforcement learning to learn a data teaching strategy in comparison to the other methods. Further, the models with learnable teaching strategies (i.e., KADT, L2T and SPL) perform better than RandomTeach that uses the random teaching strategy. The performance of KADT gained above L2T demonstrates the effectiveness of our KT model in learning Fig. 5: Heatmaps for the validation accuracy for a CNN student model on the CIFAR-100 dataset over ﬁve learning concepts. (a) L2T results. (b) KADT results. knowledge representations, in comparison to using handcrafted states in L2T. Overall, KADT outperforms the second best-performed method L2T by a margin of 2.79, 2.78, 3.08, and 3.42 on the datasets ASSISTments2009, IMDB, MovieLens, and CIFAR-100, respectively. The performance margin on CIFAR-100 was the smallest across all datasets because CIFAR-100 is the most challenging dataset with 100 different classes and the other datasets have binary classes. Figure 3 shows the AUC results, which conﬁrm the above ﬁndings. Our KADT method achieves the highest AUC value across the four supervised learning tasks. 5.2 Teaching with Different Student Models Table 2 also shows the average classiﬁcation accuracy for the student models in the teaching with different student mode. Similarly, our KADT method outperforms the L2T model (the next best performer) by a margin of 2.97, 2.94, 2.74, and 2.76. For the ASSISTments2009 and IMDB results, a noticeable performance degradation occurs due to transforming from student models (i.e., SKVMN and LSTM respectively) to less capable models (i.e., DKT and SVM respectively). For the IMDB dataset, the results are comparable to the previous experiment as the learning capacity of the two student models (i.e., LSTM and MF) is similar in the recommendation task. A signiﬁcant enhancement can be observed in the results of the CIFAR-100 dataset in comparison to the previous experiment. This is because the image representation capacity of the CNN student model used in this experiment is better than the one using the MLP model explored previously. Despite these changes in the performance results, it can still be observed that the learnable teaching methods ((i.e., KADT, L2T and SPL)) enhance the performance in comparison to the RandomTeach method. Figure 4 conﬁrms the performance gain achieved by KADT method that outperforms the next best performer L2T on the AUC metric by margins of 2.97, 2.94, 2.74, and 2.76 for the datasets ASSISTments2009, IMDB, MovieLens, and CIFAR-100, respectively. 5.3 Evaluating Knowledge Representation Learning To evaluate the impact of a student’s knowledge representation learning on the effectiveness of a data teaching strategy, we present the progress of a student model’s validation accuracy by our KADT method and compare it with the L2T method in Figure 5. These heatmaps illustrate the learning progress of a CNN student model on ﬁve learning concepts: “Fish”, “Insects”, “Large carnivores”, “Flowers”, and “Vehicles”, over the CIFAR-100 dataset. We observe two main ﬁndings from these heatmaps. First, the progress of validation accuracy with the KADT method in the bottom heatmap is more stable and evenly distributed over the ﬁve learning concepts than the one with the L2T method in the top heatmap. For example, the ﬁnal episodes 288-344 over the “Insects” and “Flowers” learning concepts are more evenly distributed for the KADT method in comparison to the L2T method. Second, the student model with the L2T method suffers from forgetting which can be observed around episodes 144-160 in the “Fish” learning concept and around episodes 208-232 in the “Flowers” learning concept. These ﬁndings conﬁrm the effectiveness of learning the performance of a student model on multiple learning concepts in the KADT method, in comparison with learning through a concept-agnostic way in the L2T method. 5.4 Evaluating Teaching Strategies In this experiment, we compare the training accuracy of the student models achieved by the KADT and L2T methods, since the L2T method is the state-of-the-art reinforcement learning based teaching model. Figure 6 shows the average training accuracy curve of DKT [13] as a student model on the ASSISTments2009 dataset, SVM model on the IMDB dataset, LSTM model on the MovieLens dataset, and MLP model on the CIFAR-100 dataset. We compare the results from the KADT method (green line) with the ones from the L2T method (purple line). It can be observed that the student’s training accuracy curves with the KADT method are more stable and converge faster than the ones with the L2T method while achieving a higher training accuracy value after convergence. These ﬁndings reﬂect that a data teaching strategy evolved by the KADT method had a better impact on the student’s training performance in comparison to the L2T one. 5.5 Ablation Study We conduct an ablation study to assess the effect of each individual component in our KADT method. We consider and compare two different variants including: 1) a baseline variant called “KADT-Basic”, which follows the same definitions of state and action from the L2T model [11] but uses our reward function design, and 2) a variant with the KT component working with mean pooling, i.e., taking Fig. 6: Student training accuracy curves averaged over 5 runs, by our KADT method in comparison with the L2T method. (a) DKT student model on the ASSISTments2009 dataset. (b) SVM student model on the IMDB dataset. (c) LSTM student model on the MovieLens dataset. (d) MLP student model on the CIFAR-100 dataset. TABLE 3: The Average AUC results for comparing different variants of KADT and L2T over all the datasets. the mean of knowledge representation vectors for samples within the same class label as the knowledge representation for the class label while treating them as equally likely, we call it “KADT-KT”. Our complete model KADT has both KT and attentive pooling components. We perform multiple independent runs and calculate the statistical signiﬁcance of the results using student t-test counting a p-value < 0.05 as a statistically signiﬁcant ﬁnding. Table 3 summarizes the average AUC results for different variants in the ablation study. We highlight the ﬁndings as follows. Firstly, the impact of our reward function design can be observed by comparing the KADT-Basic variant with the L2T model [11]. A signiﬁcant performance margin of 0.38, 0.37, 0.47, and 0.54 exists between these two models for ASSISTments2009, MDB, MovieLens, and CIFAR-100, respectively. Secondly, the impact of the KT model can be observed by comparing the KADT-Basic variant with the KADT-KT variant. A statistically signiﬁcant performance enhancement by a margin of 1.10, 1.01, 1.20, and 1.24 is achieved for ASSISTments2009, MovieLens, and CIFAR-100, respectively. Finally, the impact of the attentive pooling technique can be observed by comparing the KADT-KT variant with the complete KADT model. A statistically signiﬁcant performance enhancement by a margin of 1.31, 1.40, 1.41, and 1.64 is achieved for ASSISTments2009, MovieLens, and CIFAR-100, respectively. In this section, we explore the related work over two themes including machine teaching and learning to teach. Machine Teaching (MT) is a learning paradigm, commonly referred to as the inverse problem of machine learning [30]. It involves interaction between two models including a student model aiming at learning a speciﬁc task, and a teacher model targeting to sample optimal training data for the student model. With the rise of deep learning models and the availability of large training data, there is an increasing interest for MT in different application areas such as cyber-security [31], deep learning model compression [32], or inverse reinforcement learning [33]. MT methods can be categorized into two groups based on the interaction between a student model and a teacher model [34]: batch machine teaching and interactive machine teaching. In batch MT methods, the teaching process is done in an ofﬂine manner. A teacher model learns a ranking function over training data to select a subset of training samples for a student model. Zhu [35] proposed a Bayesian machine teaching model that can optimize training data by balancing a trade-off between teaching effort and a student’s loss. Liu et al. [36] conducted an extensive evaluation for linear student models to ﬁnd an optimal teaching model that minimizes training data size. There are two main limitations for this group. First, it largely depends on knowledge about the design of student models, which makes it hard to generalize across different student models. Second, it does not consider iterative optimization (e.g., SGD) for student models. Interactive MT methods work through iterative interactions, where each interaction involves observing the state of a student model followed by sampling a subset of training data. John et al. [37] introduced an interactive MT model for teaching on image datasets by using a kernel function to rank training samples w.r.t the development of a student’s performance. Liu et al. [7] proposed three approaches for designing a teacher model in an interactive setting: 1) an omniscient teacher that learns to sample a new training example by minimizing the difference (i.e., L2 norm) between its parameters and a student’s parameters, 2) a surrogate teacher that learns to minimize the difference between its expected loss and a student’s loss given a candidate training sample, and 3) an imitation teacher that learns to imitate a student’s hypothesis function to sample a new training example. These methods share two limitations, including dependency on an expert student model which might not exist in some scenarios, and sampling one training example at each interaction, which is less stable during a student’s optimization in comparison to sampling a mini-batch of training examples. Recently, Learning to Teach (L2T) [11] was proposed as a teaching framework that allows customizing the learning process for a student model from three aspects: the selection of training data, the design of a loss function, and the design of a hypothesis function. The authors developed an RL method to optimize a data teaching policy by sampling training mini-batches that help a student model to achieve a target performance threshold [11]. Later, Wu et al. [9] studied how to learn a dynamic loss function to better teach a student model based on a gradual optimization method [10]. Albeit, L2T considers only the overall performance of a student model using simple features such as training loss and validation accuracy, while ignoring how performance relates to latent learning concepts. Our proposed KADT method solves this limitation by dynamically tracking the student’s performance on multiple learning concepts. Moreover, our method incorporates several novel RL designs to further improve the effectiveness and robustness of learning teaching strategies. In this work, we proposed a novel method KADT which has the ability to dynamically learn knowledge representations of a student model over latent learning concepts during the evolution of a data teaching strategy. Thus, there is no need for manual design or calibration of states across different learning tasks. Further, KADT is developed in a RL framework with several novel design choices, which provides better generalization across different student models and learning tasks. We compared KADT with the stateof-the-art methods. The results showed that KADT consistently outperforms these methods on the four learning tasks. For future work, we will explore other aspects of teaching strategies including loss function and hypothesis function. Moreover, we will explore meta-learning methods for teaching strategy optimization.