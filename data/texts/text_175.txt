Molecules with ALMA at Planet-forming Scales (MAPS) II: CLEAN Strategies for Synthesizing Viviana V. Guzm´an,Romane Le Gal,Karin I. Sean M. Andrews,Jaehan Bae,Edwin A. Bergin, Nicolas T. Kurtovic,Franc¸ois M´enard,Hideko Nomura, Kamber R. Schwarz,Takashi Tsukagoshi,Abygail R. Waggoner, ical structures of ﬁve protoplanetary disks across more than 40 diﬀerent spectral lines at high angular resolution (0.15 and 0.30 beams for Bands 6 and 3, respectively) and sensitivity (spanning 0.3 1.3 mJy {beam}and 0.4 - 1.9 mJy {beam}for Bands 6 and 3, respectively). In this article, we Alice S. Booth,Jane Huang, Catherine Walsh,Arthur D. Bosman, ¨Oberg,Yoshihide Yamato,Yuri Aikawa, Jennifer B. Bergner,L. Ilsedore Cleeves, describe our multi-stage workﬂow—built around the CASA tclean image deconvolution procedure— that we used to generate the core data product of the MAPS LP: the position-position-velocity image cubes for each spectral line. Owing to the expansive nature of the survey, we encountered a range of imaging challenges; some are familiar to the sub-mm protoplanetary disk community, like the beneﬁts of using an accurate CLEAN mask, and others less well-known, like the incorrect default ﬂux scaling of the CLEAN residual map ﬁrst described in Jorsater & van Moorsel (1995) (the “JvM eﬀect”). We distill lessons learned into recommended workﬂows for synthesizing image cubes of molecular emission. In particular, we describe how to produce image cubes with accurate ﬂuxes via the “JvM correction,” a procedure that is generally applicable to any image synthesized via CLEAN deconvolution but is especially critical for low S/N emission. We further explain how we used visibility tapering to promote a common, ﬁducial beam size and contextualize the interpretation of signal to noise ratio when detecting molecular emission from protoplanetary disks. This paper is part of the MAPS special issue of the Astrophysical Journal Supplement. Keywords: protoplanetary disks — submillimeter astronomy — radio interferometry — deconvolution Sub-mm interferometers like the Atacama Large Millimeter/submillimeter Array (ALMA) enable high spatial and spectral resolution observations of protoplanetary disks. The Molecules with ALMA at Planetforming Scales large program (MAPS LP) used ALMA to survey the chemical structures of ﬁve protoplanetary disks across more than 40 diﬀerent spectral lines: an overview of the program and references to the full suite of MAPS papers is provided in Oberg et al. (2021). In this paper, we describe the imaging strategies we employed to synthesize position-position-velocity image cubes from the interferometric visibilities. These image cubes form a core data product of the MAPS LP from which other value-added data products like moment maps, radial intensity proﬁles, and emission surfaces are derived (Law et al. 2021a,b). Consider an astronomical source whose spatial sky brightness distribution is described by I. Over small angular extents, the distribution is indexed I(l, m) by the direction cosines l = sin (∆α cos δ) and m = sin (∆δ) relative to some phase center, where α is right ascension and δ is declination; l increases to the east and m increases to the north. The visibility function of an astronomical source is the Fourier transform of the sky brightness distribution, given by V(u, v) =I(l, m) exp {−2πi(ul + vm)} dl dm, (1) and is a complex quantity having real and imaginary components with units of ﬂux, e.g., Jy (for a full discussion of the conditions that must be met in order for Equation 1 to be accurate, see Thompson et al. 2017, Ch. 3). Interferometers sample V at a discrete set of spatial frequencies fundamentally dictated by the array conﬁguration and observing frequency (Thompson et al. 2017). The spatial frequencies (u, v) are measured in multiplesof the observing wavelength λ or kλ, which can also be converted to length (e.g., m), directly corresponding to the instantaneous, projected baselines of the array.For radio interferometers like ALMA, the fundamental data product is then—for every spectral channel—a set of calibrated visibility measurements at various (u, v) coordinates. These visibilities are complex-valued numbers measured in the presence of Gaussian noise The distribution from which the noise εis drawn is usually well-described by a Gaussian distribution with the thermal weightsequal to the inverse variance w= σ. Observational data is often interpreted in the context of a model. Fortunately, this measurement process deﬁnes a straightforward likelihood function for any set of model visibilities V, where and N is the number of visibility measurements. Throughout this work, we use bold notation to signify vector quantities. Model visibilities can be generated from analytic forward models I(l, m)  V(u, v) (e.g., an axisymmetric intensity proﬁle describing the thermal emission from dust rings in a protoplanetary disk; Zhang et al. 2016; Guzm´an et al. 2018; Jennings et al. 2020) or by Fast-Fourier-transforming complete radiative transfer models of I(l, m) (e.g., complicated dust morphologies (Tazzari et al. 2018) or kinematic models of spatially resolved CO emission (Czekala et al. 2019)). Regardless of how model visibilities are generated, model ﬁtting with the visibility likelihood function (Equation 3) and judicious choices of prior probability distributions creates a well-motivated posterior probability distribution that can be used for Bayesian parameter inference (e.g., Hogg & Foreman-Mackey 2018; Speagle 2019). Most of the diﬃculty in synthesizing images representative of the sky brightness distribution I(l, m) stems from the fact that the visibility function is not adequately sampled at all of the spatial frequencies where it has signiﬁcant power. Because the Fourier transform is a linear operator, one maximum likelihood image solution is simply the inverse Fourier transform of the visibility measurements. In this inversion process, the unsampled spatial frequencies are typically set to zero power. The image that results is called the “dirty image.”By deﬁnition, the point spread function (PSF) for the dirty image is the system response to an impulse sky-brightness distribution (I(l, m) = δ(0, 0), where in this context δ represents the Dirac delta function). The point spread function is also called the “dirty beam” because it typically has a substantial sidelobe pattern, responsible for the low ﬁdelity of the dirty image. The PSF width and sidelobe pattern amplitude can be altered by adjusting the scheme by which nearby uv points are averaged or “gridded” (with tradeoﬀs against the thermal noise level; Briggs 1995). The PSF sidelobe response can be (eﬀectively, albeit not perfectly) removed from the dirty image through image deconvolution. The most widely used deconvolution algorithm in the radio astronomy community is CLEAN (H¨ogbom 1974), which we describe in §3. For more background on the CLEAN family of algorithms, see Thompson et al. (Ch. 11, 2017). An alternative family of imaging algorithms are the “maximum entropy” techniques (Cornwell & Evans 1985; Narayan & Nityananda 1986; C´arcamo et al. 2018) and, more generally, regularized maximum likelihood (RML) methods (Event Horizon Telescope Collaboration et al. 2019; Nakazato et al. 2019). Rather than focusing on image deconvolution, these algorithms instead forward model the visibility measurements using ﬂexible, non-parametric models of the sky brightness distribution conditioned by well-motivated image priors. We will present results of the RML technique (as implemented in the MPoL package; Czekala & Loomis 2020) applied to the MAPS LP in a forthcoming MAPS paper (Czekala et al. in prep). This article is arranged as follows. In §2 we describe in detail how the conﬁguration of antennas within the interferometric array dictates the characteristics of the synthesized image PSF. We describe how we use the CLEAN algorithm to deconvolve the PSF sidelobe response from synthesized images in §3. In §4 we examine the implications of the so-called “Jorsater & van Moorsel (1995) eﬀect,” an important ﬂux scaling issue critical to correctly interpreting faint line emission. We discuss additional strategies of CLEAN-masking and uv tapering in §5 and §6, respectively. In §7 we discuss how signal to noise ratio might be interpreted in the image products, with reference to the non-imaging matched-ﬁlter approach used in Cataldi et al. (2021) and Ilee et al. (2021). We conclude in §8 and describe the available data products from the MAPS LP in §9. The projected length and orientation of each baseline in the interferometric array directly corresponds to the spatial frequency (uv point) of the visibility function that it measures (Chapter 2; Thompson et al. 2017). A longer baseline will sample a higher spatial frequency, which corresponds to ﬁner angular scales in the imageplane. As the earth rotates over the course of an observation, the projected length and orientation of a baseline relative to the astronomical source will change and the visibility function will be sampled over a range of uv values (Chapter 5; Thompson et al. 2017). To maximise image sensitivity and resolution, multiple sets of visibilities from short and long baseline array observations are often concatenated together into a single measurement set and imaged. The MAPS program utilized two separate conﬁgurations—nominally equivalent to the C43-4 (“short”) and C43-7 (“long”) ALMA conﬁgurations—with baselines ranging from 15–1400 m and 40–3600 m, respectively. The full details of the array conﬁgurations for each observation are summarized in Tables 9 & 10 of Oberg et al. (2021). As introduced in §1, the dirty image is generated by taking the inverse Fourier transform of the visibility measurements while the unsampled spatial frequencies are assumed to carry zero power. Following Briggs (1995), the dirty image is formalized as I(l, m) = CTDwVexp {2πi(ul + vm)} where Tis an optional visibility taper and Dis a uv density weight. In this instance and in what follows, we assume that the set of visibilities have been augmented to include their complex conjugates, N= 2N. When making images, it is necessary to sum over the Hermitian conjugates of the visibilities to ensure that the sky image is real. The normalization constant is For the immediate discussion of this section, we assume that T= 1 ∀k and the Dvalues correspond to the default density weighting of non-tapered MAPS images, which is robust=0.5 (Briggs 1995). We will return to discuss both tapering and density weighting in more detail in § 6. As discussed in Briggs (1995), the units of the dirty image are such that a point source with ﬂux density S will have a peak numerical value of S in the dirty image—for discussion purposes one can reference the ﬂux unit of Jy {dirty beam}. The PSF or “dirty beam” of the synthesized image is calculated with Equation 5 by setting V= 1 Jy ∀k, which is the Fourier transform of an impulse sky brightness distribution, i.e., a 1 Jy point source located at phase center. The PSF is then B(l, m) = CTDwexp {2πi(ul + vm)}. Phrased diﬀerently, if one considers the interferometric array transfer function W under a choice of density weighting and tapering parameters W (u, v) =TDwδ(u − u, v −v the PSF is its Fourier dual (B W ). The units of Jy {dirty beam}are technically undeﬁned, since it is not guaranteed that the PSF integrates to a ﬁnite volume; however, the maximum is always B(0, 0) = In the ﬁrst row of Figure 1, we show the uv-plane sampling for a representative observation of a disk in the MAPS sample: MWC 480 HCN J = 3 − 2 in spectral setting B6-2 (for a full description of the MAPS spectral setup, see Tables 2, 3, & 4; Oberg et al. 2021). These samples are split into short and long baselines in the left and middle columns, respectively, and they are combined in the right column. For a sense of scale, the “combined” panel contains 408,697 individual visibility samples per spectral channel, which is typical of Band 6 MAPS observations. The PSFs corresponding to the “short,” “long,” and “combined” baseline sub-selections are shown in the second row of Figure 1. Each disk in the MAPS LP was also observed by ALMA at a diﬀerent elevation, leading to a diﬀerent set of projected baselines and thus diﬀerent PSF responses. For the same disk, all molecular transitions observed at the same time as part of a single spectral setup have the same baseline conﬁguration, but diﬀerent spectral setups were observed at diﬀerent stages of array (re-)conﬁguration and therefore have slightly diﬀerent baseline distributions and PSFs. Given the way that ALMA array conﬁgurations were originally designed to sample the uv-plane,individual ALMA conﬁgurations retain approximately Gaussian beams, with more extended conﬁgurations yielding narrower beams and better spatial resolution images (c.f. Figure 1, columns 1 & 2). Enhanced resolution comes with tradeoﬀs, however. The maximum recoverable scale (MRS) is a measure of the largest angular scale that can be usefully imaged from a set of visibility measurements. By deﬁnition, the more extended conﬁgurations of ALMA have fewer short baselines and thus are less sensitive to emission on large spatial scales. If a source has emission on spatial scales larger than the MRS of an array conﬁguration, image ﬂux carried at these low spatial frequencies will not be recovered in the synthesized image (for an analysis of the missing ﬂux using analytic sky brightness distributions, see Appendix A of Wilner & Welch 1994). The morphology of molecular line emission in protoplanetary disks is a function of velocity (frequency), with emission near the systemic velocity of the source Figure 1. How uv-plane coverage impacts the synthesized point spread function (PSF) or dirty beam. top row: The two sets of baselines (i.e., short and long baselines from nominal C43-4 and C43-7 conﬁgurations, respectively) resulting from a representative observation of a representative disk in the MAPS LP. middle row: The PSF that results from the default Briggs weighting of MAPS (robust = 0.5). The CLEAN (or restoring) beam is an elliptical Gaussian function ﬁt to the main lobe of the PSF. bottom row : The deprojected and azimuthally averaged radial proﬁles of the PSF and CLEAN beams, with the minimum-to-maximum range of the azimuthal variation of the PSF indicated by the shaded region. Considered individually, ALMA nominal conﬁgurations yield dirty beams reasonably approximated (on average) by elliptical Gaussian CLEAN beams. However, the joint baseline conﬁguration results in a PSF with a substantial “shelf” to the main Gaussian core. (transition rest-frame frequency) usually being the most spatially extended. The MRSof the nominal C43-7 conﬁguration is only ≈ 1.1, while all disks targeted in the MAPS LP (IM Lup, GM Aur, AS 209, HD 163296 and MWC 480) were known to have emission on spatial scales larger than this (see §2.1 Oberg et al. 2021, and references therein). This makes it necessary to observe with a combination of array conﬁgurations to properly sample the visibility function of each MAPS target, especially at the velocities (frequencies) where the emission is the most extended. The combination of observations from short and longbaseline conﬁgurations—at least those utilized by the MAPS LP—yields a dirty beam that has a substantial “shelf” at larger radii (Figure 1, right column). To highlight this shelf, we compare each dirty beam to its corresponding “CLEAN” beam in the third row of Figure 1. The CLEAN beam is an elliptical Gaussian ﬁt to the main lobe of the dirty beam. The beam is reported using the full-width half-maximum along the major and minor axes (θand θ, respectively) and a position angle of the major axis (φ; degrees east of north). The beam power pattern is then where and To convert from units of Jy {CLEAN beam}to Jy {arcsec}, for example, one needs to divide by the eﬀective solid angle (angular area) of the CLEAN beam Ω=B(l, m) dl dm =πθθ4 ln 2 This eﬀective solid angle can be calculated by considering the beam response to a spatially uniform source (e.g., the Cosmic Microwave Background). Alternatively, the size and shape of the PSF can be characterized by considering the beam as a three-dimensional solid with its peak normalized to 1. The eﬀective area is then the “volume” of this solid in units of 1 × arcsec, for example, and is graphically illustrated for the dirty beams in the middle row of Figure 1. The CLEAN beam sizes for all MAPS products are listed in Oberg et al. (2021, Table 5). To form the one-dimensional beam proﬁles in the bottom row of Figure 1, we “deproject” both the dirty and CLEAN beam by the aspect ratio of the CLEAN beam and azimuthally average them. In reality, the dirty beams are not azimuthally symmetric, so this is only an approximation for the purposes of visualization (the full range of azimuthal variation is conveyed by the shaded region). The dirty beam exhibits non-Gaussianity even for the shortest baseline conﬁgurations (e.g., a slightly elevated “tail”). For the combined conﬁgurations, the non-Gaussianity manifests as a shelf outside an approximately Gaussian core. Though the shelf may appear small, it is at the root of several issues which ramify throughout the image deconvolution process. We will now describe this process and how we mitigate these issues. We synthesized and deconvolved image cubes using the tclean task in the Common Astronomy Software Applications (CASA) package version 6.1.0 (McMullin et al. 2007). The salient components of this process are illustrated in Figure 2 using a single channel containing signiﬁcant but faint emission from an astrophysical source (AS 209 DCN J = 3 − 2). The ﬁrst algorithmic decision is which functional basis set to use for CLEAN components. The simplest version of CLEAN uses Dirac δ-functions (H¨ogbom 1974). We used the more advanced “multiscale” algorithm (deconvolver="multiscale") which is built on a set of variously-sized axisymmetric tapered parabolic components similar to 2D Gaussian functions (Cornwell 2008). CLEAN components may also take on (small) negative amplitudes so that components placed in later iterations can reﬁne the CLEAN model built from larger positive components placed in earlier iterations. We set the component scales to scales=[0, 5, 15, 25] pixels, where the pixel size was chosen to correspond to ≈ 1/7th of the beamsize. This pixel size adequately oversamples the beam FWHM without making the image size impractically large: the smallest image dimensions were for Band 3 or 0.3 tapered images (1024 ×1024 pixels) and the largest image dimensions were for Band 6 CO (2048 × 2048 pixels). The CLEAN algorithm starts by setting a “residual image” equal to the dirty image and a “CLEAN model” equal to a blank image. Each iteration of the CLEAN Figure 2. A schematic of the CLEAN-ing procedure tracking the residual image as new CLEAN components are deconvolved and added to the CLEAN model. The residual image is initialized with the dirty image (which has “units” of Jy {dirty beam} With each iteration, the residual image is deconvolved by subtracting the convolution of the dirty beam with the new CLEAN component(s), removing the eﬀects of beam sidelobes (a 2D representation of the dirty beam is shown in the upper right grey panel). At the end of the process, the CLEAN model is smoothed by convolution with the CLEAN beam (the FWHM of the CLEAN beam is marked by the white ellipse in 2D dirty beam panel); the resulting image has units of Jy {CLEAN beam} The CLEAN models are shown with an aggressive color stretch ( components. (*) The same convolved CLEAN model is visualized with a softer linear color stretch in Figure 3. ) to better demonstrate the accumulation of CLEAN algorithm introduces new CLEAN components and two things happen. First, the CLEAN model is gradually built up when CLEAN components are placed at locations corresponding to the current peak ﬂux in the residual map. It is possible to use a binary mask to restrict the placement of new CLEAN components. Second, the newly placed CLEAN components are deconvolved from the residual map. The deconvolution is carried out by subtracting the convolution of the CLEAN component with the dirty beam. The algorithm continues iterating until an exit criterion is triggered. The criterion may be as simple as a maximum number of iterations or it may correspond to a threshold on the noise properties of the residual map. We have chosen the latter and we discuss our choice of masks and thresholds in §5. When the deconvolution procedure has ﬁnished, the CLEAN model is convolved with the CLEAN beam to form a convolved CLEAN model image. The ﬁnal residual image is added to the CLEAN model to form the restored (or “CLEANed”) image. For multi-channel measurement sets like those of the MAPS LP, CLEAN images and deconvolves each channel with completely independent CLEAN components. Though CLEAN is best-known for deconvolving dirty beam sidelobes, an arguably more important function of the CLEANing process is creating an interpretable ﬂux model. Because the ﬂux units of the dirty image are technically undeﬁned (Jy {dirty beam}), any measurements made using a dirty image are highly sensitive to the uv-sampling distribution. For an extreme example, consider the ill-advised task of measuring the ﬂux of a point source using the dirty image from a two-element interferometer, better known as a fringe pattern (e.g., Ch. 9, Wilson et al. 2013).Depending on how one drew their photometric aperture, one could just as easily include or exclude various nulls and maxima of the fringe pattern and measure ﬂuxes ranging from positive peak to negative trough of the sine wave. However, if one were to ﬁrst deconvolve the fringe PSF from the dirty map, reasonable inferences could be drawn on source ﬂux (if not location, in this contrived example). Because the CLEAN model is built up with CLEAN components, it has real, physical interpretabilty: the units of the CLEAN model are Jy {pixel} and the units of the convolved CLEAN model are Jy {CLEAN beam}. Because the CLEAN beam has ﬁnite volume (Equation 14), CLEANed images are on much surer ﬂux-footing than dirty images, whose default dirty beam is not guaranteed to have ﬁnite volume. CLEAN components like δ-functions or tapered parabolic components are rarely a perfect basis to represent a spatially resolved source. However, in the limit of many low-amplitude components, reasonable models of source structure can be achieved. If the CLEAN components were a good match to source morphology, e.g., δ-functions for a ﬁeld of quasars, then the CLEAN model itself would be a reasonable product to use for analysis. Like the CLEAN model in Figure 2, this is rarely achieved in practice; it is more often the case that the higher resolution information that could be conveyed by an accurate, native resolution CLEAN model is gladly traded for a more visually pleasing convolved CLEAN model, where errors stemming from a highly discretized but imperfect CLEAN basis set have been low-pass ﬁltered out by CLEAN beam convolution (for a visual explanation, see the discussion of “optimal resolution” in Chael et al. 2016, §5.3). This shortcoming is one reason why ﬂexible, non-parametric imaging techniques can often produce higher resolution image products than CLEAN (see Event Horizon Telescope Collaboration et al. 2019 for a general discussion, and see P´erez et al. 2019, Disk Dynamics Collaboration et al. 2020, or Jennings et al. 2020 for discussions speciﬁc to protoplanetary disks). While the CLEAN procedure may be familiar to many radio astronomers, it is still a non-linear process subject to myriad algorithmic choices like loop gain, threshold stopping criteria, and masking regions, among other parameters embodied in the tclean argument list.When the basis sets are mismatched from the source morphology (e.g., when multi-scale Gaussian components are used to deconvolve a source that is actually composed of concentric rings), it is very important to correctly tune these algorithm parameters to obtain faithfully restored images. It is not common practice within the protoplanetary disk community to publish representations of the CLEAN model in scientiﬁc analysis, but we argue that inspection of the CLEAN model (and its potential deﬁciencies) should be part of any radio astronomy workﬂow, especially where ﬁne-featured source morphologies are concerned. Figure 3. After the deconvolution iterations are complete, the residual map is added to the convolved CLEAN model to form the ﬁnal image. Because the residual map is derived from the dirty image, it has units of Jy {dirty beam} standard CLEAN workﬂow (top panel), this creates a ﬁnal CLEANed image with mismatched units, and therefore compromised interpretability. Following Jorsater & van Moorsel (1995), one solution is to scale the residual map by the ratio of the CLEAN beam volume to the dirty beam volume, a process that we term the “JvM correction.” In the bottom panel we show our revised CLEAN workﬂow that includes the JvM-corrected residual, which results in a ﬁnal CLEANed image with the correct intensity units. All images in this ﬁgure appear with the same color scale stretch and limits. Now that we have outlined the broad contours of the CLEANing process, we revisit the ﬁnal step of the CLEAN algorithm when the ﬁnal CLEANed image is formed by summing the residual image and the convolved CLEAN model, shown graphically in the top panel of Figure 3 as the “standard” workﬂow. There are usually two reasons why radio astronomers carry out this ﬁnal step, even though we just discussed reasons why the CLEAN model is a reasonable scientiﬁc product on its own terms. One, this gives the ﬁnal CLEANed image some representation of the thermal noise, which is useful for interpreting the signiﬁcance of features. Two, the residual map still contains some (ideally small level of) real astrophysical ﬂux that was not adequately deconvolved (e.g., see the “residuals” panel in the top row of Figure 3). Adding the residuals back to the CLEAN model provides some insurance that this real ﬂux at least appears in the ﬁnal image, though it will still exhibit the eﬀects of the sidelobe response. Because the residual map originated as the dirty image, it technically has units of Jy {dirty beam}, while the convolved CLEAN model has units of Jy {CLEAN beam} CLEANed image is created with mismatched units. If the CLEAN beam accurately approximates the dirty beam, the unit mismatch is inconsequential. However, if the dirty beam has even a small shelf, such as the ∼ 10% amplitude shelf on the robust=0.5 beam shown in Figure 1 and replicated in Figure 4, there can be severe implications for accurate ﬂux recovery. Because the shelf occurs at large beam radius, it contributes to a large mismatch in diﬀerential volume, even though it is small in relative proﬁle. This small shelf means that a CLEAN beam ﬁt to this dirty beam main lobe will encompass only 60% of the full dirty beam volume (using the ﬁrst null as a proxy for dirty beam extent, even though the response extends much further). Therefore, the units Jy {CLEAN beam}and Jy {dirty beam} diﬀer substantially. The shelf of naturally weighted images is usually even more severe than for robust or uniformly weighted images. To our knowledge, this issue and its implications for ﬂux conservation were ﬁrst described in Jorsater & van Moorsel (1995, Appendix A), and so we term this the “JvM” eﬀect throughout the MAPS paper series. We 0.5 relative proﬁleshelf 0.0 0.002 0.001 0.00cumulative volume 1.0 0.8 ratio cum. vol. 0.6 Figure 4. How a dirty beam “shelf” leads to a divergence of integrated beam volume. top panel: the PSF proﬁles p(r) from the joint baseline conﬁguration in Figure 1, with the ﬁrst null (zero-crossing) of the dirty beam labeled as a proxy for the total “size” of the dirty beam. second and third panels: though the shelf of the dirty beam is small relative to the PSF peak, because it is at large radius, it leads to a signiﬁcant divergence in the total integrated volume of the dirty beam relative to the CLEAN beam. last panel: The ratio of the CLEAN beam volume relative to the dirty beam volume, , can be used to convert the residuals from the CLEAN-ing process into the proper units of Jy {CLEAN beam}. calculate the ratio of beam volumes as using the CLEAN beam and the .psf dirty beam ﬁles produced by tclean. The procedure originally described in Jorsater & van Moorsel (1995) calculates  using the ratio of the CLEAN model to the diﬀerence between the dirty image and the residual map.Our calculation using the ratio of beam volumes yields a similar result via direct calculation. The bottom panel of Figure 4 shows a 1D representation of this calculation, though in practice we use the 2D beam proﬁle since the dirty beam is not axisymmetric. One might think that the units mismatch can be avoided if only one fully CLEANed one’s images, i.e., the CLEAN model contained all of the real astrophysical ﬂux and the residual map contained only noise. We concur with Jorsater & van Moorsel (1995) that this is unattainable in any real world application of CLEAN. Consider an example where the CLEAN threshold is set at 2 × RMS. At the end of the CLEANing process the residual map will contain some real astrophysical ﬂux below this threshold while the CLEAN model will contain some components that were erroneously deconvolved from noise spikes above this threshold. Varying the threshold just changes the balance of ﬂux in the residual map or CLEAN model—it is impossible to CLEAN to a zero-ﬂux threshold without also adding a signiﬁcant number of erroneous components to the CLEAN model. The JvM eﬀect makes imaging faint molecular line emission challenging because (by deﬁnition) a signiﬁcant fraction of the ﬂux in each channel will exist at or below a typical CLEANing threshold level (e.g., a few ×RMS). When particularly faint datasets, such as the one chosen to illustrate Figures 2 & 3 (AS 209 DCN J = 3 − 2), are CLEANed to anything but the deepest thresholds (generally inadvisable for other reasons, see §5), the residuals may contain 50% or more of the total ﬂux in the image. If one were to erroneously assume the products in the top row of Figure 3 were all in units of Jy {CLEAN beam}(as is normal practice), the residuals, CLEAN model, and ﬁnal image would contain 70 mJy, 33 mJy, and 103 mJy of ﬂux, respectively. The JvM eﬀect is often less pronounced when synthesizing images of high S/N continuum emission because in that application the CLEAN model will hold a higher proportion of astrophysical ﬂux. However, it may still hamper the characterization of fainter regions within those images. 4.2. The solution: rescaling the residual image using the “Jorsater & van Moorsel (JvM) correction” While “fully CLEANing” images is not a practical solution to the units mismatch issue, we can approximately convert the residual map from units of Jy {dirty beam}to units of Jy {CLEAN beam}. This is achieved by rescaling the residual map by a factor of  before it is added to the CLEAN model (see Figure 3, middle panel; for this dataset,  = 0.359). We call this the “JvM correction,” which we implement using the immath CASA task and the .model and .residual outputs from tclean. Under the revised workﬂow, the scaled residuals, CLEAN model and ﬁnal image in the bottom row of Figure 3 contain 25 mJy, 33 mJy, and 58 mJy of ﬂux, respectively (resulting in a dramatic 50% change in total ﬂux compared to the standard workﬂow). As demonstrated by this faint dataset, failure to apply the JvM correction can have a profound eﬀect on both the total ﬂux contained within an image and the morphological characteristics of that ﬂux (c.f. Figure 3 ﬁnal images). The  values for all MAPS image cubes are listed in Table 11 of Oberg et al. (2021). Band 3 image cubes typically have  values in the range 0.7 - 1.0, while band 6 image cubes typically have  values in the range 0.2 - 0.7. The eﬀect of the JvM correction becomes more signiﬁcant the more  deviates from 1. JvM still matters for signal-free channels! —Since the JvM correction modiﬁes the residual map, this creates ambiguity around an image-plane RMS measurement. Technically, even signal-free channels with zero CLEAN components still need to be corrected for the JvM eﬀect since they are formed from a residual map in units of Jy {dirty beam}but are reported in units of Jy {CLEAN beam}. Unless otherwise stated, throughout the MAPS paper series we specify RMS values in Jy {CLEAN beam}measured from emissionfree images corrected for the JvM eﬀect (but not yet corrected for the primary beam sensitivity). A notable exception is when we specify CLEANing thresholds. Since the tclean procedure references threshold values in the dirty map itself, we report these thresholds with respect to RMS values measured in the non-JvM-corrected dirty map. Binary image plane masks are useful to guide the placement of CLEAN components to locations believed to correspond to physical ﬂux. Simple elliptical masks are often reasonable choices, especially for single-channel continuum images. Since the spatial distribution of molecular line emission in a protoplanetary disk changes considerably (but predictably) as a function of observing frequency (corresponding to velocity, e.g., Figure 3, Oberg et al. 2021), using the same elliptical mask for all channels means that large areas of blank sky will be at risk of having erroneous CLEAN components placed within them. CASA allows masks to be hand-drawn for complicated spatial emission. This is often the most accurate option, but mask drawing quickly becomes onerous for even a single large spectral cube. For the MAPS LP program, the time investment is prohibitive. CASA’s auto-multithresh algorithm (Kepley et al. 2020) is a promising solution, however, given its computational overhead, we instead exploited the Keplerian rotation pattern of the protoplanetary disk (e.g., Horne & Marsh 1986; Semenov et al. 2008) to create parametric CLEAN masks (e.g., Rosenfeld et al. 2013). We generated a Keplerian mask for each disk by the following procedure. Each image-plane pixel (l, m) was ﬁrst deprojected into disk-centric cylindrical coordinates (r, φ, z), based on an assumed disk inclination i, position angle PA, and constant emission surface slope (z/r, e.g., Teague et al. 2019) representing the surface of an optically thick cone in three dimensions. Disk inclination and position angle values were drawn from the literature (see Oberg et al. 2021, Table 1), while emission surface slope was reﬁned by hand as described below. The disk-centric coordinates correspond to the location where a ray drawn from pixel (l, m) intersects the surface of the cone. Depending on the disk inclination and orientation, not all pixels necessarily intersect the cone. Using the disk-centric coordinates for each pixel, the projected Keplerian velocity is where Mis the central stellar mass. The total projected velocity component at each pixel is the sum of the Keplerian rotation and the systemic velocity, v= Motivated by the fact that disk temperature declines with increasing r, the intrinsic line width is also assumed to narrow following a power law proﬁle where ∆V≤ 0. An initial mask was generated such that for a channel with central velocity vand width ∆v, masked regions satisﬁed |v−v| ≤ (∆v+ ∆V ). For image cubes with low spectral resolution, the channel spacing ∆vdominates the mask. For image cubes with high spectral resolution, the line width ∆V is of greater importance in determining masked regions. After the initial mask was generated, it was convolved with a 2D Gaussian proﬁle to smooth the mask edges (with FWHM θtuned to provide an adequate “buﬀer” region near otherwise sharp contours). The full script used to generate the masks is available in the keplerian mask package (Teague 2020). We tuned the mask parameters to match theCO J = 2 − 1 emission for each disk; the ﬁnal parameters are listed in Table 1. The mask parameters should not be considered true properties of the protoplanetary system, rather they are the parameters that best represent the spatial distribution of emission using the framework described above. The same mask parameters are used for all default image products for all observed transitions, with the exception ofCO J = 2 − 1, whose extended and complex emission morphology warranted bespoke, hand-drawn masks. For some specialized applications, it was advantageous to tune the Keplerian mask parameters from those values used here to produce the default image products. Where applicable, these alternative masks are noted in MAPS publications. In Figure 5 we compare the results of the CLEANing process for three channels of an example measurement set (AS 209CO J = 2−1) using an elliptical mask vs. a Keplerian mask. To demonstrate the impact of the diﬀerently shaped masks, we CLEANed to two diﬀerent depths: 4 ×RMS and 1 ×RMS. As noted previously, for CLEAN applications we used the RMS measured from a signal-free region prior to the JvM correction. Figure 5. A comparison of elliptical (left column) vs. Keplerian (right column) binary masks for three channels of the AS 209 CO J = 2 − 1 measurement set CLEANed to an ideal depth of 4 × RMS (top grouping) and over-CLEANed to a deeper yet-still-plausible depth of 1 × RMS (bottom grouping). All panels are on the same color scale stretch. If the parameters of the tclean algorithm are set optimally, a well-designed mask will not signiﬁcantly improve image quality over a generic mask, as demonstrated by the identical images generated using the 4 × RMS threshold. However, if an image is CLEANed to a deep noise threshold, a poorly designed mask will more easily allow components to be erroneously added to the CLEAN model. These errant components are barely noticeable in the “standard” CLEAN workﬂow (which unfortunately has inconsistent ﬂux units, Figure 3), but when the JvM correction scales the residuals by a factor of  the stippled pattern becomes apparent. Well-designed Keplerian masks can mitigate but not eliminate this behavior. The default choices for MAPS data products are labeled with a thick blue border (Keplerian masks, threshold=4 × RMS). If the measurement set is properly calibrated and the tclean parameters are set optimally, i.e., the basis set is an adequate match to the source morphology, loop gain is suﬃciently conservative given the complexity of the deconvolution task, and the CLEANing threshold is appropriate, a well-designed mask will not signiﬁcantly improve the end result (cf. elliptical vs. Keplerian masks at a threshold = 4 ×RMS in Figure 5). If the algorithm is tuned correctly, CLEAN will eventually converge as it steadily but surely deconvolves real astrophysical ﬂux (presumed to be responsible for the brightest features in the residual map at each iteration) down to the threshold. Through careful experimentation, we arrived at 4 × RMS as the ideal cleaning depth to balance the number of erroneous CLEAN components against ﬂux retained in the residual map. However, the optimal tclean parameters can be difﬁcult to ascertain without some trial and error, and the threshold is an example of a parameter that is sometimes diﬃcult to tune a priori. In these instances, properlyﬁtting binary masks can guard against the incorrect placement of CLEAN components and mitigate (but not eliminate) some of the adverse eﬀects of improperly set tclean parameters. In the threshold=1 × RMS example in Figure 5, we see that numerous low-amplitude CLEAN components are added to the model, corresponding to noise spikes above the threshold level that were erroneously deconvolved from the residual map (top row: “conv. model”). Because the elliptical mask encompasses a larger area without plausible astrophysical ﬂux, there are more opportunities for components to be incorrectly deconvolved. Under the standard CLEAN workﬂow, the visual appearance of the ﬁnal product (labeled “standard” in Figure 5) is not signiﬁcantly aﬀected at either threshold. However, we remind the reader that the ﬂux units of the CLEANed image produced under the standard workﬂow are inconsistent (§4), and therefore any residual ﬂux that was not deconvolved (particularly that outside the mask) is not represented with the appropriate strength. Moreover, as a matter of principle, it is not desirable to have a CLEAN model contain components known to correspond to pure noise, especially if it were to be used for further data reduction or analysis (e.g., self-calibration; Brogan et al. 2018). When the residual map and CLEAN model are properly combined under the JvM workﬂow (labeled “JvM” in Figure 5), the defects of the CLEAN model become apparent in the form of a stippled pattern. The reason this stippling occurs is because when  < 1, the ﬂux distribution loses support under the CLEAN model: the CLEAN algorithm deconvolves the residual map with a dirty beam that is substantially larger than the CLEAN beam can restore. When the residual map is properly scaled to Jy {CLEAN beam}using the JvM correction, the previously artiﬁcially high residual map drops to a lower level and the stippling pattern appears. The stippling eﬀect is most apparent in regions of low astrophysical ﬂux and might lead to the misinterpretation of substructure in what would otherwise be interpreted as smooth regions (Disk Dynamics Collaboration et al. 2020). As mentioned, we used the carefully tuned 4 × RMS cleaning depth to minimize the appearance of stippling across the MAPS data products (see the panels with a blue border in Figure 5). The primary goal of this section is to describe how the tapering coeﬃcients Tof the dirty image equation (Equation 5) can be modiﬁed to standardize PSF characteristics throughout the MAPS LP and aid comparisons between molecules and across disks. Because PSF tapering is interrelated with the density weighting coeﬃcients D, however, we ﬁrst brieﬂy review how uniform, natural, and robust weighting aﬀect PSF shape through uv cell averaging and density weighting (see also Chapter 10.2.2; Thompson et al. 2017). We also comment on how the images generated with these weighting schemes relate back to the data likelihood originally discussed in §1. Then, we discuss how the tapering and density weighting coeﬃcients can be co-varied to achieve a range of desired PSFs. In Figure 6 we demonstrate how the density weighting schemes and tapering coeﬃcients affect the beam sidelobes, beam (non)axisymmetry, and JvM correction factor. As discussed in the introduction (§1), Equation 5 is used to synthesize a “dirty image” from a set of discrete visibility samples {V}. In practice, the visibilities and their Hermitian conjugates are ﬁrst “gridded,” or interpolated, onto a regularly spaced array of u, v pointsso that the fast Fourier transform may be used to accelerate the imaging calculation. This grid presents a convenient partition centered on the u,vpairs with cell sizes ∆u and ∆v with which to a) average multiple visibilities contained within the cell and b) use as a bounding box to calculate local sample density. The cell sizes used for averaging and density calculations need not be the same as those in the gridded array, but for demonstration purposes we assume that they are. For the sake of discussion, we consider an arbitrary cell i, j containing L visibilities, indexed such that each l index corresponds to some speciﬁc k index. For example, l = {1, 2, 3, . . . , L} might correspond to k = {304, 305, 307, . . . , 320}, depending on how exactly the visibilities are indexed relative to the grid boundaries. When all L visibilities inside the cell are gridded, the “eﬀective” visibility located at the cell center is To simplify the comparison of density weights that follows, we will assume that there is no tapering (T= 1) and that all visibilities within a cell have a common expectation value hVi= V(u, v), since the cell size is assumed to be small relative to the expected features in the visibility function (i.e., the chosen image size and resolution “Nyquist sample” the visibility function). Uniform weighting —“Uniform” weighting is arrived at by minimizing the sidelobe level of the main beam (for a derivation, see §3.3.3; Briggs 1995), which yields constant density weights for all visibilities within the cell where w=Pw. The eﬀective value of the uniformly gridded visibility is Uniform weighting delivers the minimum variance estimate of the visibility mean, since it is obtained by weighting each sample by its inverse variance (w= 1/σ). Moreover, the statistical uncertainty on V is encapsulated with win the same way as for ungridded visibilities Vwith w. While uniform weighting usually produces the highest resolution dirty beam (Figure 6, ﬁrst row), a downside is that the dirty image usually has the worst sensitivity. Natural weighting —The RMS thermal noise in the dirty image is minimized when since w= 1/σ(for a derivation, see §3.3.1; Briggs 1995). This is called “natural” weighting and results in an image that has the lowest RMS and thus greatest sensitivity. The eﬀective value of the naturally gridded visibility is Natural weighting diverges from estimating hVi and instead upweights those grid cells containing large quantities of high S/N visibilities. Natural weighting results in a beam (Figure 6, second row) that is useful for detection (especially of point sources), but at the cost of substantial sidelobes negatively impacting resolution. Robust weighting —Briggs (1995) developed the “robust” weighting scheme where f= (5 × 10)P Values of R = 2 approximate natural weighting (maximizing point source sensitivity) while values of R = −2 approximate uniform weighting (maximizing resolution). Intermediate R values oﬀer a tradeoﬀ between these two extremes. Because the performance curve is nonlinear (Figure 3.23, Briggs 1995), signiﬁcant resolution gains can be achieved at minimal loss in point source sensitivity (e.g., mJy), though loss in surface brightness sensitivity is more signiﬁcant (see Figure 6, third column), especially when expressed in units with constant solid angle (e.g., mJy arcsecor brightness temperature [K]). Maximum likelihood images —If the visibility function V were sampled at all u, v locations where it had signiﬁcant power, then Vwould have a direct relationship to the sky brightness via the inverse Fourier transform. Unfortunately, this is never achieved for actual sub-mm interferometric observations of astrophysical sources, and the impact of the transfer function (W (u, v), Equation 8) must be considered when producing images from visibility samples. Figure 6. A representation of how the visibility tapering proﬁle T (u, v) (left column) and density weighting scheme inﬂuence the dirty beam resolution, point source sensitivity, PSF proﬁle and JvM correction factor . The same dirty beam proﬁle is shown in the second, third, and fourth columns: the three dimensional view best represents beam structure, the two dimensional view best conveys beam ellipticity (white ellipse represents FWHM), and the one dimensional view best represents the divergence from a Gaussian CLEAN beam. Each row represents a diﬀerent combination of tapering proﬁle and robust value. The ﬁrst three rows are untapered, while the ﬁnal two rows are tapered to circular FWHM resolutions of 0. (the ﬁnal ≤ 5% adjustment is carried out using the imsmooth task, which is not shown). Nevertheless, because visibility model ﬁtting (§1) is a common and highly useful procedure for parameter inference—especially for annular protoplanetary disk structures—it is worthwhile to consider how uniform, natural, and robust images relate back to V. By referencing the eﬀective visibility under each weighting scheme (Equations 20, 22, & 25), we see that only uniform weighting converges to the expectation value of the visibility function in that cell (Briggs 1995), while natural and robust weighting instead result in an eﬀective visibility that is tied to the number and quality of visibility measurements within that cell. Therefore if the dirty image were used as a sky plane model I(l, m), only the uniform dirty image would maximize the data likelihood function (Equation 3). Of course, there are legitimate reasons to use other weighting schemes, such as the need to maximize sensitivity with natural weighting for a detection experiment. And in practice, given its non-linear tradeoﬀ curve, the most scientiﬁcally useful weighting scheme for the detection and characterization of faint protoplanetary disk features is often an intermediate robust value, like the robust=0.5 we used as the default setting for untapered MAPS products. The CLEANing process and maximum likelihood —The dirty image is constructed under the assumption that all unsampled spatial frequency components are set to zero power. As the CLEANing process deconvolves the dirty image and image plane components are added to the CLEAN model, the corresponding visibility plane model Vis eﬀectively interpolated from the gridded locations to otherwise unsampled (u, v) values. Because the image plane representation is directly equivalent to the inverse Fourier transform of the visibility plane model, the accuracy of the interpolated values relative to truth mirrors the quality of the deconvolution process. Using a basis set appropriate to the source morphology (e.g., choosing multi-scale CLEAN over H¨ogbom CLEAN for extended emission) can result in more accurate Fourier interpolations/image reconstructions, particularly at higher spatial frequencies/ﬁner resolutions. Each iteration of the CLEAN algorithm also brings Vcloser to maximizing the likelihood function (Schwarz 1978). However, Vwill only achieve the maximum possible likelihood value when V= V∀k ∈ 1, . . . , N, which can happen in the absence of noise or in the case that the entire image is fully cleaned to a 0 Jy threshold (the Fourier equivalent of overﬁtting). Most practical applications will stop CLEANing when a noise threshold is reached in the residual map. Because imaging is an ill-deﬁned inverse process, there exist an inﬁnite number of maximum (or approximately maximum) likelihood images (and therefore models) consistent with the data: these have V≈ V∀k ∈ 1, . . . , N but may take on any value of Vat unsampled spatial frequencies u, v. Because beam characteristics will aﬀect the order in and scales at which ﬂux is deconvolved from the dirty image, for any spatially resolved source the deconvolution algorithm will likely converge to diﬀerent CLEAN models under diﬀerent visibility weighting schemes. Visibility tapering proﬁles can be used to “force” MAPS, we produced ﬁducial data products using tapered visibilities to boost image-plane sensitivity to low surface brightness features (at the expense of resolution) and provide a common circular beam that is useful for comparing molecular species across the Band 3 and Band 6 observations (for a listing of all ﬁducial beam sizes, see Table 5 of Oberg et al. 2021). Beam tapers can be described by a multiplicative proﬁle in the uv-plane T (u, v) or equivalently by convolution with a kernel in the image-plane t(l, m). If we wish to taper our original dirty beam, whose main lobe is approximately an elliptical Gaussian, to a dirty beam whose main lobe is approximately a circular Gaussian, the appropriate functional form of the taper is an elliptical Gaussian. In analogy with the CLEAN beam (§2), this proﬁle is described by a position angle (φ) and beam dimensions. In the uv-plane, these FWHMs are Θ, Θin units of kλ. In the image plane, these are θ, θ, in units of arcsec. The Gaussian FWHMs are relatedsuch that We follow a convention such that Θ> Θ, which implies that θ< θ. The multiplicative uv-plane proﬁle with and Both the uv tapering and image convolutional proﬁles are normalized to peak values of 1. Although it is possible to analytically calculate tapering parameters given a desired beam size (Appendix C of Briggs 1995), we found that there is considerable “mechanical backlash” in CASA v6.1.0’s beam ﬁtting subroutines due to the pixelized representation of the dirty beam. This means that a smooth, monotonic relationship between tapered dirty beam size and ﬁtted CLEAN beam size does not exist; in practice we found that direct forward modeling yielded a more consistent set of tapering parameters. We emulated CASA’s dirty beam formulation routine to compare the ﬁtted CLEAN beam (with FWHMs θ and θ) to a CLEAN beam with the desired parameters (θ, θ, φ). We desired a circularized beam such that θ= θ= θ, with sizes θ= {0.15, 0.2, 0.3} for Band 6 observations and θ= {0.3, 0.5} for Band 3 observations. Because density weighting also aﬀects beam shape, there are many combinations of robust values and tapering proﬁles that deliver the same circularized CLEAN beam proﬁle. To preserve point source sensitivity without introducing large sidelobes, we calculated tapering proﬁles using a starting value of robust=0.5. We used scipy.optimize.minimize (Virtanen et al. 2020) to ﬁnd the best-ﬁtting set of beam tapering parameters that minimized the ﬁt metric f(Θ, Θ, φ) = (θ− θ)+ (θ− θ) If no solution could be found that delivered a beam within 95% - 100% of the target resolution, we iterated by reducing the robust value by 0.25 (towards a uniformly weighted beam) and tried the ﬁtting procedure again. For most Band 6 observations (including those shown in Figure 6), we were able to successfully ﬁnd tapered beams using robust=0.5 values. For several of the Band 3 observations, however, lower robust values were required to achieve pre-tapered beams with θ≤ 0.3. An example of the visibility tapering proﬁles needed to achieve θ= {0.15, 0.3} are shown in the fourth and ﬁfth rows of Figure 6, respectively. As a ﬁnal step (not shown in Figure 6), the images were smoothed the remaining < 5% to their target resolution using the CASA imsmooth task. By carrying out the bulk of the tapering in the uv-plane (instead of entirely on the ﬁnal image), the CLEAN algorithm is able to build a more accurate CLEAN model during the deconvolution process and thus improve ﬁnal image ﬁdelity. While our tapering proﬁles were successful in standardizing beam sizes across the MAPS data products, it is important to realize that even tapered dirty beams whose main lobes deliver circular Gaussian CLEAN beams still have an extended asymmetric shelf. For example, consider the 2D PSF representation of the 0.15 tapered beam in the fourth row, third column of Figure 6. This means that any astrophysical ﬂux still remaining in the residual map will retain the asymmetric features of the dirty beam. The MAPS spectral setup (Oberg et al. 2021, Table 4) covered more than 40 molecular transitions of interest, many of which are at or near the detection threshold in the ﬁve protoplanetary disks we targeted. Given the diverse scientiﬁc goals of the MAPS LP, we employed several algorithms to detect and characterize the emission. In this section we discuss the general principles behind the quantiﬁcation of signal to noise ratio (S/N) and how these apply to several core data products from the large program. Colloquially, S/N is usually quoted as a onedimensional quantity in multiples of a σ value corresponding to a fractional probability of the Gaussian distribution, e.g., 2σ or 95% (regardless of whether the posterior distribution itself is actually Gaussian). Model assumptions are key to contextualizing any S/N statistic, since it is these (often hidden) assumptions that deﬁne the likelihood function, prior distributions, and thus the posterior distribution of the parameter(s) of interest. Signal to noise in detection experiments —Detection experiments are usually discussed in terms of a onedimensional posterior probability distribution of the amplitude a of the source p(a |data). The detection S/N is the probability that the inferred amplitude of the source is greater than 0, p(a > 0 |data). In truth, the full posterior distribution is likely multivariate because the model usually has hidden parameters b, c, etc. In the best situations, the uncertainty pertaining to other model parameters is marginalized out However, for computational or implementation reasons, it may be diﬃcult to explore the probability distributions of these other parameters, and the S/N is estimated using the conditional posterior distribution p(a |b, c, data) with all other model parameters held ﬁxed. This may overestimate the detection S/N when the model varies signiﬁcantly under reasonable choices for the other parameters. Consider the scenario of detecting a point source against a blank background with Gaussian noise. We assume a simple model: δ-function with known position but unknown amplitude. Assuming the beam is well characterized, the amplitude posterior is deﬁned by a Gaussian centered on the value of ﬂux measured at the location of the point source. The width of the posterior Gaussian corresponds to the thermal RMS of the image. The fraction of the posterior corresponding to ﬂuxes greater than 0 deﬁnes the S/N detection probability (e.g., 2σ or 95%). Most realistic scenarios quickly diverge from this idealized scenario to involve more complex models (for example, if the location of the δ-function is not known, then l and m position must be marginalized over and the signiﬁcance of any particular candidate is diminished). When considering spatially resolved emission, it is important to consider the model assumptions that undergird the interpretation of S/N. If the model is misspeciﬁed (e.g., a point source when the emission is in fact diﬀuse) then the S/N calculation, conditional on those assumptions, may not reﬂect the S/N calculation one actually desires. The more closely a model matches reality, generally speaking, the higher signiﬁcance a detection that can be achieved. If the model is overly ﬂexible, however, then unknown parameters of the search space (e.g., spatial location, rest frequency) must be marginalized over and the S/N of the detection will suﬀer. One way to gain intuition for model sensitivity is to attempt to ﬁt the data with a range of diﬀerent model assumptions and eﬀectively explore some of the hidden parameters, albeit in a limited manner. The matched ﬁlter —The application of a matched ﬁlter to detect spatially resolved but weak line emission from a protoplanetary disk in Keplerian rotation nicely demonstrates some of the model complexity trade-oﬀs (e.g., Loomis et al. 2018). In such a framework, a template (such as uniform surface brightness inside some Keplerian pixel mask, e.g., Table 1) is assumed, Fourier transformed, and cross-correlated against all available frequency channels. The template amplitude is the sole free parameter of the model. Since the matched ﬁlter is linear, the interpretation of the ﬁlter response relative to a signal-free region (σ) is directly equivalent to the point source detection scenario discussed above, and carries many of the same caveats (Ruﬃo et al. 2017; Loomis et al. 2018). Detection signiﬁcance will be maximized when the template is an accurate representation of reality; in most real-world applications an imperfect template will reduce the signiﬁcance of a detection. This is demonstrated to some degree in Figure 7 with the matched ﬁlter applied to the HCN J = 11 − 10 transition in MWC 480, using templates generated from Keplerian masks with diﬀerent outer radii (100 au, 200 au, and 400 au). The line is detected with all 3 templates; however, the signiﬁcance is maximized using the template with the smallest radial extent (100 au), which best matches the emission morphology of this weak, compact line. Since even the 100 au template is misspeciﬁed to some degree (real disk emission does not appear uniform within some Keplerian mask, but rather radially varies in intensity), the matched ﬁlter detections most likely represent lower limits to the maximal S/N detection that could be achieved with an ideal template. More details on the matched ﬁlter procedure applied to MAPS data are provided in Ilee et al. (2021). Shifting and stacking —To aid in the detection and characterization of weak lines, we used the shift and stack technique (Yen et al. 2016; Teague et al. 2016; Matr`a et al. 2017) on several image cubes. In this process, the Doppler shift corresponding to the Keplerian rotation of the protoplanetary disk is removed from each position- Figure 7. The response of Keplerian matched ﬁlter templates with outer radii 100 au, 200 au, and 400 au applied to the MWC 480 HCN J=11-10 transition measurement set. The detection signiﬁcance is higher when the template more accurately matches the true spatial distribution of the (inherently compact) emission. position-velocity pixel in the data cube and each channel in the cube is summed across all pixels. This coherently sums emission across the data cube and results in a higher S/N detection compared to a traditional spatially-integrated spectrum. For a full description of the shift and stack technique applied to MAPS data, as well as the calculation of the uncertainties that result from the deprojection and aggregation of the data, see Ilee et al. (2021) and Cataldi et al. (2021). An important beneﬁt to using multiple techniques to detect and characterize faint molecular emission is that conﬁdence builds when independent techniques return consistent results. In Figure 8 we compare the results of the matched ﬁlter and spectral shift and stack technique applied to the HCOJ = 1−0 transition in the HD 163296 and MWC 480 disks (for more information on this and the HCOJ = 1−0 transitions, see Aikawa et al. 2021). The HCOemission is faint—it cannot be recovered by visual inspection of the channel maps for either disk. Encouragingly, the emission is detected in HD 163296 using both the matched ﬁlter (with Keplerian mask template r= 400 au radius) and spectral shift and stack technique, while the line is not detected in MWC 480 using either technique. The spectral shifting and stacking technique is useful to measure the disk integrated ﬂux and radial intensity proﬁle (especially for transitions with hyperﬁne structure, e.g., Bergner et al. 2021; Cataldi et al. 2021; Guzm´an et al. 2021), while the matched ﬁlter is a more eﬃcient way to search the full MAPS LP for weak transitions (instead of imaging the entire data set). We applied the matched ﬁlter to all of the spectral windows to search for any additional lines that were not primary science targets and serendipitously detected satellite lines of c−CHand CH (Ilee et al. 2021; Guzm´an et al. 2021). For a full list of the molecules detected by the MAPS LP, see Oberg et al. (2021, Tables 2 & 3). The MAPS large program represented a signiﬁcant effort to calibrate and image a large volume of molecular line emission for ﬁve protoplanetary disks. In this work, we described the non-Gaussian dirty beam that can arise from multi-conﬁguration ALMA observations, and, after reviewing the CLEANing process, the challenges it presented for accurate ﬂux recovery of faint, extended features. We chose to remedy this issue by implementing the “JvM correction” originally proposed by Jorsater & van Moorsel (1995), which properly scales the CLEAN residual map into consistent units of Jy {CLEAN beam}. We also described how we generated custom Keplerian CLEAN masks to guide the deconvolution process, and discussed some imaging artefacts that can result from non-ideal tclean parameters, such as “stippling.” To aid in the comparison of diﬀerent molecular transitions, we also produced image products using beam proﬁles tapered to common resolutions of FWHM 0.15, 0.2, 0.3, and 0.5. Finally, we brieﬂy discussed the interpretation of signal to noise and detection signiﬁcance across the MAPS data products. We centralized our data processing pipeline on the North American ALMA Science Center (NAASC) computing cluster, located in Charlottesville, VA, USA. Python scripts documenting the reduction and imaging procedures are available.During times of heavy development, we availed ourselves of multiple 16-core machines for days at a time. Including development and incremental reprocessing eﬀorts, we estimate that we utilized 1 year’s worth of core hours to produce the MAPS LP image products (i.e., two 16-core machines fully utilized for two weeks). Though still small compared to the computational demands of protoplanetary disk hydrodynamical simulations, for example, this represents a considerably larger computational demand compared to most ALMA observations. Here we enumerate the molecular line imaging data products produced from the MAPS LP, which are accessible through a portal to the ALMA archive avail- Figure 8. A comparison of the matched ﬁlter (left column) and spectral shift and stack (right column) detection techniques applied to the HCOJ = 1 −0 transition in HD 163296 (top row) and MWC 480 (bottom row). In HD 163296, the transition is detected by both techniques at similar signiﬁcance (relative to oﬀ-source velocities), but in MWC 480 it is not detected by either technique, demonstrating consistency between the two methods. able after peer review. For a detailed description of the naming conventions for all MAPS LP image products, see Oberg et al. (§3.5, 2021). For a description of the continuum-only MAPS image products, see Sierra et al. (2021). For each combination of disk (e.g., MWC 480; see Table 1, Oberg et al. 2021) and transition (e.g., DCN J = 3 − 2; see Tables 2 & 3, Oberg et al. 2021), the archive contains two minimal measurement sets produced with the cvel2 and split tasks with visibilities pertaining to that disk and transition pair. The ﬁrst contains the visibilities including continuum emission, while the second contains the line visibilities with the continuum subtracted. During the invocation of cvel2, we coarsened the spectral channels slightly from their native spacings (see Table 4 of Oberg et al. (2021)) to a uniform set of channels spaced 0.5 km sapart in B3 and 0.2 km sin B6. The archive also contains a set of image products generated from each measurement set using various beams. For the Band 3 transitions, these beams are untapered robust=0.5, tapered 0.30, and tapered 0.50. For the Band 6 transitions, these beams are untapered robust=0.5, tapered 0.15, tapered 0.20, and tapered 0.30. For a full description of beam sizes available in each band, see Table 5, Oberg et al. (2021). For each beam setting, the following image products are available: JvM correction) • CLEANed image cube under standard workﬂow (Figure 3), with and without primary beam correction • CLEANed image cube under JvM correction (Figure 3), with and without primary beam correction. The primary beam and “JvM”–corrected cube is the recommended data cube for most scientiﬁc use cases. • A Python script to reproduce the data products from the minimal continuum-subtracted measurement set. The following is a typical workﬂow to produce a set of image products. We used the tclean task with multiscale CLEAN and scales=[0, 5, 15, 25] pixels, where the pixel size was chosen to correspond to ≈ 1/7th of the beam FWHM. For all lines except CO, we used Keplerian CLEAN masks matched to theCO J = 2 − 1 emission. We iterated the CLEAN algorithm such that the peak residual emission was below a threshold of 4 × RMS. For untapered beams we used Briggs weighting of robust=0.5. For tapered beams we forward modeled the CASA beam ﬁtting process to calculate the value of the uvtaper argument that achieves the target resolution using the largest (most natural) robust value still ≤ 0.5. Finally, we calculated the JvM factor  via the ratio of the CLEAN beam volume to the dirty beam volume, scaled the residual map by  and summed it with the convolved CLEAN model to produce the JvM-corrected image cube. We recommend consulting the Python script accompanying each set of image products for the speciﬁc tclean parameters used to generate a particular image product. For more information on the additional value added data products (VADP) like moment maps, radial proﬁles, and emission surfaces provided for most transitions across a range of beam sizes see Law et al. (2021a,b). This paper makes use of the following ALMA data: ADS/JAO.ALMA#2018.1.01055.L. ALMA is a partnership of ESO (representing its member states), NSF (USA) and NINS (Japan), together with NRC (Canada), MOST and ASIAA (Taiwan), and KASI (Republic of Korea), in cooperation with the Republic of Chile. The Joint ALMA Observatory is operated by ESO, AUI/NRAO and NAOJ. The National Radio Astronomy Observatory is a facility of the National Science Foundation operated under cooperative agreement by Associated Universities, Inc. We thank the NAASC for the use of computational resources on the North American ALMA Science Center (NAASC) computing cluster. C.J.L. acknowledges funding from the National Science Foundation Graduate Research Fellowship under Grant DGE1745303. R.T. acknowledges support from the Smithsonian Institution as a Submillimeter Array (SMA) Fellow. K.I.¨O. acknowledges support from the Simons Foundation (SCOL #321183) and an NSF AAG Grant (#1907653). J.H. and S.M.A. acknowledge funding support from the National Aeronautics and Space Administration under Grant No. 17-XRP17 2-0012 issued through the Exoplanets Research Program. J. H. acknowledges support for this work provided by NASA through the NASA Hubble Fellowship grant #HSTHF2-51460.001-A awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS5-26555. I.C. was supported by NASA through the NASA Hubble Fellowship grant HST-HF2-51405.001-A awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS5-26555. Y.A. acknowledges support by NAOJ ALMA Scientiﬁc Research Grant Code 2019-13B, and Grant-in-Aid for Scientiﬁc Research 18H05222 and 20H05847. R.L.G. acknowledges support from a CNES fellowship grant. K.Z. acknowledges the support of the Oﬃce of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin – Madison with funding from the Wisconsin Alumni Research Foundation, and support of the support of NASA through Hubble Fellowship grant HSTHF2-51401.001. awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS5-26555. C.W. acknowledges ﬁnancial support from the University of Leeds, STFC and UKRI (grant numbers ST/R000549/1, ST/T000287/1, MR/T040726/1). J.D.I. acknowledges support from the Science and Technology Facilities Council of the United Kingdom (STFC) under ST/T000287/1. K.R.S. acknowledges the support of NASA through Hubble Fellowship Program grant HST-HF2-51419.001, awarded by the Space Telescope Science Institute,which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS526555. G.C. is supported by NAOJ ALMA Scientiﬁc Research Grant Code 2019-13B. J.B.B. acknowledges support from NASA through the NASA Hubble Fellowship grant #HST-HF2-51429.001-A, awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Inc., for NASA, under contract NAS5-26555. J.B. acknowledges support by NASA through the NASA Hubble Fellowship grant #HST-HF2-51427.001-A awarded by the Space Telescope Science Institute, which is operated by the Association of Universities for Research in Astronomy, Incorporated, under NASA contract NAS526555. Y.Y. is supported by IGPEES, WINGS Program, the University of Tokyo. H.N. acknowledges support by NAOJ ALMA Scientiﬁc Research Grant Code 2018-10B and Grant-in-Aid for Scientiﬁc Research Grant Numbers JP17K14244 and JP20K04017. FMe acknowledges support from ANR of France under contract ANR-16-CE31-0013 (Planet-Forming-Disks) and ANR15-IDEX-02 (through CDP “Origins of Life”). L.M.P. acknowledges support from ANID project Basal AFB170002 and from ANID FONDECYT Iniciaci´on project #11181068. A.S.B acknowledges the studentship funded by the Science and Technology Facilities Council of the United Kingdom (STFC). A.D.B. acknowledges support from NSF AAG Grant #1907653. E.A.B. acknowledges support from NSF AAG Grant #1907653. L.I.C. gratefully acknowledges support from the David and Lucille Packard Foundation and Johnson & Johnson’s WiSTEM2D Program. V.V.G. acknowledges support from FONDECYT Iniciaci´on 11180904 and ANID project Basal AFB-170002. A.R.W. acknowledges support from the Virginia Space Grant Consortium and the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1842490. N.T.K. acknowledges support provided by the Alexander von Humboldt Foundation in the framework of the Sofja Kovalevskaja Award endowed by the Federal Ministry of Education and Research. 2013, 2018), CASA (v6.1.0; McMullin et al. 2007), GoFish (Teague 2019), NumPy (van der Walt et al. 2011), SciPy (Virtanen et al. 2020), keplerian mask (Teague 2020)