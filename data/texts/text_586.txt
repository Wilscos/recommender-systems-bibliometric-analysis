Jian Zhu, Congcong Liu, Pei Wang, Xiwei Zhao, Guangpeng Chen, Junsheng Jin, Changping Peng, Zhangang Lin, Jingping Shao Business Growth BU, JD.com {zhujian146, liucongcong25, wangpei960,zhaoxiwei, chenguangpeng,jinjunsheng1,pengchangping,linzhangang,shaojingping}@jd.com Click-through rate (CTR) prediction, which aims to estimate the probability of a user clicking an item, is of great importance in recommendation systems and online advertising systems (Cheng et al., 2016; Guo et al., 2017; Rendle, 2010; Zhou et al., 2018b). Effective feature modeling and user behavior modeling are two critical parts of CTR prediction. Deep neural networks (DNNs) have achieved tremendous success on a variety of CTR prediction methods for feature modeling (Cheng et al., 2016; Guo et al., 2017; Wang et al., 2017). Under the hood, its core component is a linear transformation followed by a nonlinear function, which models weighted interaction between the ﬂattened inputs and contexts by ﬁxed kernels, regardless of the intrinsic decoupling relations from speciﬁc contexts (Rendle et al., 2020). This property makes DNN learn interaction in an implicit manner, while limiting its ability to model explicit relation, which is often captured by feature crossing component (Rendle, 2010; Song et al., 2019). Most existing solutions exploit a combinatorial framework (feature crossing component + DNN component) to leverage both implicit and explicit feature interactions, which is suboptimal and inefﬁcient (Cheng et al., 2016; Wang et al., 2017). For instance, wide & deep combines a linear module in the wide part for explicit low-order interaction and a DNN module to learn high-order feature interactions. Follow-up works such as Deep & Cross Network (DCN) follows a similar manner by replacing the wide part with more sophistic networks, however, posits restriction to input size which is inﬂexible. Above-mentioned methods pay little attention to user behavior modeling. Recently, attention-based methods like DIN and DIEN have attracted many interests that attempt to capture user preferences based on users’ historical behaviors (Zhou et al., 2018b; 2019; Feng et al., 2019). With regard to the interaction of characteristics, the use of attention mechanisms in these methods can be treated as Learning to capture feature relations effectively and efﬁciently is essential in clickthrough rate (CTR) prediction of modern recommendation systems. Most existing CTR prediction methods model such relations either through tedious manuallydesigned low-order interactions or through inﬂexible and inefﬁcient high-order interactions, which both require extra DNN modules for implicit interaction modeling. In this paper, we proposed a novel plug-in operation, Dynamic Parameterized Operation (DPO), to learn both explicit and implicit interaction instance-wisely. We showed that the introduction of DPO into DNN modules and Attention modules can respectively beneﬁt two main tasks in CTR prediction, enhancing the adaptiveness of feature-based modeling and improving user behavior modeling with the instance-wise locality. Our Dynamic Parameterized Networks signiﬁcantly outperforms state-of-the-art methods in the ofﬂine experiments on the public dataset and real-world production dataset, together with an online A/B test. Furthermore, the proposed Dynamic Parameterized Networks has been deployed in the ranking system of one of the world’s largest e-commerce companies, serving the main trafﬁc of hundreds of millions of active users. an explicit modelling of the interaction of characteristics while neglecting the modelling of implicit interactions of characteristics. The methods mentioned above either model implicit and explicit feature interactions isolated or adopt a suboptimal way to combine them, which can be inefﬁcient. In this work, we aim to address these problems by introducing a small MLP layer that dynamically generates kernels conditioned by the current instance to capture both implicit and explicit feature interactions. The core idea is to ﬁrst generate context weights and biases from the context stream, and then aggregate them with the input stream adaptively. We formulate a generic function and implement it with an efﬁcient dynamic parameterized operation (DPO). The ﬁrst weight generator projects contextual features into high-dimensional representation, which models implicit conditional bias. The second feature aggregator aims to fuse input features and projected contextual representation in a multiplicative way, e.g., matrix multiplication and convolution, maintaining both low- and high-order information. For feature-based modeling, we introduce feature-based DPO where the weight-generate operation dynamically produces instance-wise ﬁlters conditioned on the embedded context. The featureaggregate function then applies instance-wise ﬁlters to the ﬂattened input by matrix multiplication, allowing to learn multiplicative features. In particular, we further propose a new class of DPO, called ﬁeld-based DPO, which is not only instance-speciﬁc but also ﬁeld-speciﬁc. In that case, the ﬁlters vary from ﬁeld to ﬁeld and from instance to instance, allowing more complex interactions along the ﬁeld dimension. For user behavior modeling, we introduce sequence-based DPO that consists of two variants: behaviorbehavior dynamic operation and query-behavior dynamic operation. A representative method of dynamic convolution (Chen et al., 2020; Yang et al., 2019) shares the convolution kernel, which is generated by the global average of the inputs. Similarly, (Wu et al., 2019) proposed DyConv, a lightweight ﬁne-grained convolution that depends only on time-step, reinforcing the encoder-based language modeling framework. However, our methods incorporate both local and global information as they jointly use locality-aware methods (e.g., convolution or separable convolution) followed by a global average pooling layer to produce instance-wise weights. The query-behavior dynamic operation is specialized designed for the decoder-based framework in CTR prediction, aiming to capture target-behavior dependency. To our best knowledge, this is the ﬁrst attempt to extend the business of dynamic neural networks to CTR prediction with extensive experiments on two fundamental scenarios. The comprehensive study against existing solutions validates the superiority of our proposed method. Moreover, we demonstrate that incorporating DPO into the real-world ranking system is beneﬁcial. Our contribution can be summarized as followed: We propose a generic formulation for capturing multiplicative interaction via weightgenerate and feature-aggregate function, termed DPO. For feature-based modeling, we propose two variants, named ﬁeld-based and feature-based DPO, offering a unifying view of implicit and explicit feature interaction. Decomposing these operations, we ﬁnd they implicitly inherit low- and second-order representation. For user behavior modeling, we propose two sequence-based variants: behavior-behavior and query-behavior DPO. The ﬁrst one computes locally perceptual dynamic ﬁlters and the second one learns target-behavior dependency in a multiplicative manner. We demonstrate that such operations can beneﬁt the self-attention layers by higher computational efﬁciency through modeling locality as inductive bias. The proposed dynamic parameterized networks outperform state-of-the-art methods by a signiﬁcant margin on both public and real-world production datasets. We also give a comprehensive study about the relationship of our proposed methods to previous Factorization Machine (Rendle, 2010) and CrossLayer (Wang et al., 2017). We further demonstrate the effectiveness and superiority of our method with an online A/B test in real-world applications by incorporating it into the ﬁne-rank stage of the real-world ads system. We ﬁrst review the mainstream approaches of feature-based and user behavior (sequence-based) modeling under the situation of CTR prediction speciﬁc instantiations designed for traditional feature-based and sequence-based modeling. Traditional CTR prediction serves as a fundamental evaluation criterion for computing advertising systems. Typically, in a given scenario (the contexts), users click on certain items (item proﬁles) based on their own needs (query) and pautorferences (user proﬁles). Consequently, a model considers four ﬁelds of features, i.e., query, user proﬁle, item proﬁle, and contexts to predict: where item and user proﬁles contain up to tens of ﬁne-grained static attributes depending on the speciﬁc circumstances. Sequence-based CTR prediction involves user behaviors additionally: where the models can learn from the behaviors that have occurred under certain contexts and query in the past to make judgments on the current items. As mentioned in KFAtt (Liu et al., 2020), the behavior module can be formulated as: are given adopt the the self-attention mechanism (Vaswani et al., 2017), which naturally learns multiplicative interaction between query and the historical behavior. Namely, multiplicative interaction (Jayakumar et al., 2020) has been proposed to fuse two different sources of information with the goal of approximating function zare the input and context respectively. Similarly, we give a generic formulation of DPO in CTR prediction task as: Hereiis the index of a position (in the ﬁeld, or sequence), whose response is calculated with the generated output of speciﬁed context. The generate function one of the inputs of the pairwise aggregate function the relationship between x MLP and convolution typically process input and context features in an additive way with ﬁxed weights. While in Eqn. (3), using instance-wise generated weights and bias from contexts additive nature is transformed to multiplicative. DPO is also different from bilinear layer (Lin et al., 2015; He & Chua, 2017) for Eqn. (3) computes representation based on the generated weights over all positions, whereas bilinear layer aggregates information over all positions between to large memory consumption. Furthermore, our generated dynamic weights can maintain more local information, which complements the global counterpart, e.g., self-attention. DPO is a ﬂexible block and can easily work together with MLP and self-attention layers. Givenx ∈ R generic formulation degrades as linear transformation: generated by function Related work is in Appendix A due to space limitation. Thistorical clicked items and corresponding query words. The most used strategy is to andz ∈ Ras inputs and context, due to the lack of position information, the Figure 1: Illustration of feature-based and ﬁeld-based dynamic parameterized operation. (Ha et al., 2016), a natural choice of g is a fully-connected layer to form dynamic weights and bias: where(W , complexity, unsuitable for deployment in real-world application. Here, we consider a low-rank method in practice, e.g., a two-layer MLP: where(W Then, we can decompose the output into explicit dynamic weights and bias. The right inductive bias depends on how we select context O(mc + nc) set l as a small number and use a multi-head mechanism (Vaswani et al., 2017). Relation to Cross Network formulation of DPO. Let’s take function, whose hidden states are 1, (i.e. output of multiplicative operation. GivenX ∈ R respectively, our goal is modeling the interaction between idea is to treat ﬁeld-based operation as multiple feature-based operations followed by summation over all output. Thus, Eqn. (3) can be expressed as all ﬁelds share the same instance-wise weights. However, the ﬁeld-based operation interacts between all ﬁelds, which sometimes introduces unnecessary feature coupling (i.e., multiplicative interaction of brand ID and time, etc.). The empirical evidence ﬁnds over-coupling brings more noise and then results in underﬁtting, albeit their capacity of learning high-order features. A considerable method is to use Self-Field dynamic operation without heavily hand-crafted feature engineering, formulated as: cross-ﬁeld interactions. Apart from Summation-based and Self-based methods, a more attentive solution can be used to aggregate the dynamic attributes: whereh whole context with inputs without explicit summation instead of concatenation, formulated as: y= f (x pairwise ﬁeld-based interaction from coarse to ﬁne to model high-order representation, while the feature-based method combines both low- and high-order information over all ﬁelds. The complex weight-generate function can be designed for the right inductive bias, but we do not speciﬁcally consider such a method for online serving and leave it to future work. ˆˆb,˙W ,˙b) ∈ (R, R, R, R). However, the size ofˆWhas quadratic space , b) ∈ (R, R)and(W, b) ∈ (R, R),σis a non-linear function. of plain MLP layer, whilegscales up toO(lmc + ln). To reduce the complexity, we = x· xw+ b+ x, wherexwis scalar. We prove CrossLayer is the simplest gas the same as the multiplicative term of CrossLayer. In this way, DPO aims to imitate is an attention layer. Beyond taking position into consideration, we can interact the ; g([z, z..., z]; θ)), where[·, ·]is a concatenation operation. These four methods learn : Here, we slightly modify the origin FM implementation (Rendle, 2010) as:y = xxby removing the LR term, that takes interaction among all ﬁeld positions into Figure 2: Illustration of homogeneous behavior and heterogeneous query-behavior dynamic parameterized operation. consideration. Given inputs and the context as matrix multiplication andP operation, where the context is other ﬁeld features different to the input features. User behavior modeling focus on learning from their historical actions.to predict whether the users click the current items. As a comparison, transformer-based solutions (Liu et al., 2020; Zhou et al., 2018a) explored the encoder-decoder framework to learn long-range dependencies both source-tosource and source-to-target, where the encoder exploits multi-head self-attention to extract session interest and the decoder aggregates the query-speciﬁc interest. Following the encoder-decoder framework, we consider two variants, i.e., homogeneous Behavior-Behavior and heterogeneous Query-Behavior dynamic operation (homo- and hetero- DPO). We show their multiplicative attributes on Appendix C. Homogeneous Behavior-Behavior DPO of behavior. Given For user behavior modeling, our goal is to model the long- and short-term feature interaction. As mentioned above, a long-term function aims to learn non-local interaction between all positions while short-term ones only care about the local information. Thus, a natural way is to adopt global-aware weight-generate function Section 2.4, we adopt convolution as with learned weights. For simplicity, we consider function kernel size k, while feature-based and ﬁeld-based methods only use MLP. Eqn. (6) shows the function local neighborhood by dynamic weight, where gives a instantiation of weight-generate function project them into a select operator activation function. Secondly, we use multiplicative interaction correspond to global aggregation features. To further strengthen locality, we can adopt local-aware function to capture short-term information of context separable convolution etc.). Heterogeneous Query-Behavior DPO interaction with query. Give mentioned in Section 2.3. Eqn. (3) learns interaction between query and behavior over all length xx. Thus, FM can be viewed as the self-excluded version of ﬁeld-based dynamic y= f (x;1C(z)g(z; θ)) =1C(x)g(x; θ)x(6) ∈ R. To use dynamic depthwise-convolution, we can setc = 1. Eqn. (7) captures the Table 1: Comparison with different algorithms of feature-based datasets over 5-runs results. Std≈1e-3. followed by summation, and the simplest formulation can be derived as: Similar to feature-based and ﬁeld-based methods, query-behavior dynamic operation can easily learn rich multiplicative interaction and conditional inductive bias. The weight-generate function to learn the weight representation function, such as Eqn. (7). Compared to self-attention in decoder (Liu et al., 2020), DPO focuses attention on instance-weights based on context, while self-attention takes bipartite attention matrix to aggregate value units. Thus, we conjecture they are two orthogonal and complementary solutions. We perform comprehensive experiments on feature modeling and user behavior modeling of public and real-world production CTR prediction datasets. Setting. We evaluate with MovieLens-tag, Criteo, Avazu with following questions: • How does DPN perform (effectiveness and efﬁciency) compared with other base models? • How do different contexts and weight-generate functions inﬂuence the performance? We use AUC and Logloss as metrics for public datasets. For all experiments, we evaluate the effectiveness of baseline models with the same training setting in AFN (Cheng et al., 2020) implemented by TensorFlow (Abadi et al., 2016). We adopt Adam (Kingma & Ba, 2014) as optimizer with best searched learning rate of a batch size 4096 for all models. We ﬁx the embedding ranks as 10 across all datasets and use same deep neural network (e.g., 3 layers MLP of 400-400-400) with Batch Normalization and ReLU (Ioffe & Szegedy, 2015; Nair & Hinton, 2010) if without speciﬁcally noted. The details of our proposed methods are described in Appendix B. Comparison with state-of-the-art results. and our reimplemented results in the same setting. We note all these methods are single model without DNN components. First, our feature-based DPN consistently achieves better performance than other explicit interaction methods and also the implicit DNN baseline, which conﬁrms the dynamic aspects contributes to implicit feature interaction. Additionally, when the train dataset and features get larger, the overwhelming margin get larger (e.g. 0.13% on MovieLens-tag, 0.23% on Avazu, 0.69% on Criteo), showing promising potential ability for applied in real-world production. Secondly, our ﬁeld-based DPN perform better than the other explicit interaction module. We note ﬁeld-based methods models the relation over different attributes (i.e., UserID, MovieID etc.) where Table 2: Ablation study on MovieLens-tag dataset over 5-runs results. We show Auc and logloss. (c) low- and high-order information are captured in a totally different way. Specially, ﬁeld-based DPN obtain additive module in parallel with multiplicative one while other high-order interaction methods follow an opposite stacked framework to learn the multiplicative features (Qu et al., 2016; Cheng et al., 2020; He & Chua, 2017). Effectiveness of different instantiations of weight-generate function. types of a feature-based dynamic operation added to the DNN baseline (right after the embedding layer for replacing the fully-connected layer). After we search the best DNN baseline model, we replace a dynamic operation with the ﬁrst fully-connected layer. We list the results of different weight-generate functions, where not all methods perform better than the baseline. We implement the hypernetworks-based idea (Ha et al., 2016) as HyperDense which slightly improve the baseline while add a big chunk of computation resulting for optimization difﬁculty. When we adopt our proposed simple and effective method as shown in Eqn. (5), gate mechanism can be exploited for better performance, which means mixture of kernels have better generality. Furthermore, we explore some approaches to reduce the complexity of However, they instead downgrade the performance even cannot compete on par with the baseline. We provide more ablation study on the Appendix D. Multi-Layer Feature-based DPN with different contexts. dynamic parameterized network with different context. We separately replace DPO with ﬁrst, second, and all fully connected layers in 2-layer MLP. Table 2b shows that more feature-based DPO in general lead to better results regardless of context. We argue multiple feature-based operations can learn rich and high-order dynamic interaction by imitating MLP. High-order message can be processed with non-linear function layer by layer, which is hard to be found useful via multiplicative models. In Table 2b, we also study the effectiveness of different context. We set the ﬂatted inputs the inputs of DPN and evaluate the performance of different contexts such as andzis the ﬂattened outputs of inputs and context embeddings respectively. We found they share similar results for most experiments while getting the best performance when we set the context as (i.e. use the tag information of context embeddings as context inputs). Under careful selection of hyperparameters, this best result mainly originates from the expert knowledge of MovieLens dataset and recommendation system. Meanwhile, it reveals a nice property of our methods: the intrinsic Table 4: Adapt sequence-based DPO (sDPO) on Transformer. We evaluate the effectiveness of combination of multi-head self-attention mechanism and sDPO. decoupling attribute can be more separably modeled. Interestingly, we ﬁnd our methods improve results of the infrequent user on MovieLens datasets, as shown in Table 3. We believe the dynamic interaction can warm up the infrequent user embeddings, demonstrating the potential of our methods for the cold-start problem. Effectiveness and Efﬁciency of Fieldbased DPNs. based dynamic networks aiming to capture atomic relationships among ﬁelds’ features. Table 2c presents results of different aggregate function. We ﬁnd that all dynamic operations with different context aggregate functions perform better than the static component, even only correspond to themselves. We may hypothesize that additional contexts can beneﬁt the ﬁeld features after having been processed for imitating linear transformation, containing multiplicative interaction between inputs and contexts. However, we ﬁnd the time-consuming is worrying in the CPU machine when the dimensions of outputs are relatively large, making it venturesome to be applied on real-world production. Experiment setting. We only adopt AUC as metrics with the same training setting in KFAtt (Liu et al., 2020) implemented by TensorFlow. The task is to predict whether a user will write a review for the target item after reviewing historical items. We refer the readers to KFAtt (Liu et al., 2020) for more details. Comparison with state-of-the-art. the best performance than all state-of-the-arts on total situations, including the strong baseline KFAtt. When incorporating heterogeneous DPO into the attention mechanism, we ﬁnd both DIN (Zhou et al., 2018b) and DIEN (Zhou et al., 2019) perform better than the origin baseline where the armed DIN outperform all the base models on Amazon datasets by a large margin, which shows that heterogeneous DPO can effectively learn complementary representation which can beneﬁt the attention mechanism. Adaptation to Self-Attention. self attention mechanism. For homogeneous DPO, we ﬁnd it performs slightly better than MHSA counterparts. When we use session-wise representation for user behavior modeling, self-attention can capture local information over handcraft scope of time designed by experts while narrow neighbor interaction of convolution may not contribute to learning users’ attention due to the short session. For heterogeneous DPO, we ﬁnd it can effectively facilitate the decoder counterpart no matter which encoder we adopt. Overall, the sequence-based DPN(sDPN) achieve best results than other combination, which shows the effectiveness of our propose homogeneous and heterogeneous DPO. Table 6: Results on Real-world Production dataset. We show the details of how to incorporate DPNs into online ads system in Appendix F.1. For feature modeling, we name DPNs as DyMLP. For both feature and user behavior modeling together with online model, we name DPNs as DyJoint. 3.3 EXPERIMENTS ON REAL-WORLD PRODUCTION DATASET AND ONLINE A/B TESTING We conduct all feature-based, ﬁeld-based, and sequence-based experiments on the Real-world Production dataset. In the ofﬂine experiments, we observe the progressive improvement from modern rank models consisting of advanced user behavior and multi-modal features model in our advertising system. Table 6 shows our implementation. For feature interaction modeling, commonly used DNN component while only add a little extra cost. Despite we don’t explore the effectiveness of ensemble DPN models in a public dataset, Table 6 presents even one lightweight ﬁeld-based DPO that can beneﬁt the generality. To reduce the complexity, we use a multi-head mechanism in feature-based DPN which may inﬂuence the effectiveness. For sequence-based DPO, we conduct more ablation studies in Appendix F.3. Our homogeneous DPO can act as a speciﬁc form like dynamic convolution. Incorporating it with a session-based self-attention encoder, we can inject inductive bias learned from local neighborhood information into global long-term dependencies based on Transformer-like models (Vaswani et al., 2017; Feng et al., 2019). Beyond that, heterogeneous DPO can learn more conditional multiplicative interaction which models the user interest on given items, showing greater power than the homogeneous component. Combined with all techniques, we get the best results by a large margin to the online model. In the online A/B test, Table 7 shows DyJoint contributes 1.0% CTR gain against the online component, which demonstrates the superiority over the highly optimized base model on our ad system. However, DyJoint leads to larger online latency compared to the base model due to the increment of model parameters and memory access. In this paper, we describe a new class of neural networks that captures both explicit and implicit interaction via dynamic parameterized operation. Our proposed block can be easily inserted into existing CTR predicting architectures for fusing features from different modalities. Our experiments show that it overwhelms the existing feature-crossing-based and attention-based models on two fundamental tasks. Furthermore, we conﬁrm its representation effectiveness in the real-world production dataset. For the theoretical understanding, we decouple dynamic operation for comprehensive study with high-order feature-crossing methods and self-attention. Overall, we open a new era where current mainstream solutions are dominated by self-attention mechanisms and MLP in CTR prediction.