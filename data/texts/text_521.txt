Recently, interactive recommendation systems based on reinforcement learning have been attended by researchers due to the consider recommendation procedure as a dynamic process and update the recommendation model based on immediate user feedback, which is neglected in traditional methods. The existing works have two significant drawbacks. Firstly, inefficient ranking function to produce the Top-N recommendation list. Secondly, focusing on recommendation accuracy and inattention to other evaluation metrics such as diversity. This paper proposes a deep reinforcement learning based recommendation system by utilizing Actor-Critic architecture to model dynamic users’ interaction with the recommender agent and maximize the expected long-term reward. Furthermore, we propose utilizing Spotify’s ANNoy algorithm to find the most similar items to generated action by actor-network. After that, the Total Diversity Effect Ranking algorithm is used to generate the recommendations concerning relevancy and diversity. Moreover, we apply positional encoding to compute representations of the user’s interaction sequence without using sequence-aligned recurrent neural networks. Extensive experiments on the MovieLens dataset demonstrate that our proposed model is able to generate a diverse while relevance recommendation list based on the user’s preferences. Keywords: deep reinforcement learning, recommender system, diversity, approximate nearest neighbor  The majority of conventional recommender systems consider the recommendation procedure as a static process. In other words, the recommender system has no immediate interaction with the users and cannot improve the recommendation model based on the users’ immediate and long-term feedback. On the other hand, an interactive recommender system can modify its model to the users’ preferences. Some works [1]–[5] formulated the interactive recommendation procedure as a multi-armed bandit problem. These works suppose that users’ preferences would be left unchanged, but it’s evident that the users’ preferences may change over the time. For example, the users’ interest may vary depending on their mood or temper. Due to the recent advancement in deep reinforcement learning and outstanding success in various tasks such as playing games [6]–[8], robotics [9], [10], and other sequential decision-making problems [11], many research studies target reinforcement learning based recommender systems. Most existing reinforcement learning approaches are not reasonable in large discrete items space, which is the nature of recommender systems. In particular, traditional reinforcement learning techniques such as Q-learning [12] and Partially Observable Markov Decision Processes (POMDP) [13] are early efforts to model recommendation procedure as a reinforcement learning problem. However, these methods become impracticable with the growing number of items in the recommendation process. The conventional Deep Q-Network (DQN) based techniques [14] utilize a deep neural network as a function approximator to estimate Q-values for all available items in action space to take the best action with maximum Qvalue in the current state. Although these techniques do not require to store Q-values in memory, the time complexity of Q-value evaluation to determine the best action is linear to the size of the action space and therefore becomes inefficient where the number of items is huge. In recent years, some works [15]–[17] proposed to leverage Actor-Critic architecture, which utilizes deep neural networks as separate policy and value function estimators. The policy network is known as the Actor, and the value network (i.e., Q-network) is known as Critic. In this architecture, actor-network generates a continuous parameter vector as action used to rank candidate items. Then, the critic evaluates the action generated by the actor using the Temporal Difference (TD) error. Despite solving temporal complexity in these studies [15]– [17], there are still two limitations: Firstly, a ranking function is applied to all items in action space used to produce recommendations by selecting items with the highest score. Therefore, the temporal complexity problem remains. Secondly, these methods [15]–[17] focus on recommendation accuracy. Hence, the system may offer only a narrow range of user’s interests, such as only offering sci-fi movies, which is known as overfitting problem. In this paper, to tackle these problems, we propose a model to diversifying in deep reinforcement learning-based interactive recommendation systems, namely DRLIR, by utilizing Actor-Critic architecture to model an interactive recommendation system. More specifically, to capture user’s sequential behaviors, we add positional encodings [18] to the user’s state embeddings (i.e., the latest n items of user’s interaction history) and then, we fed user’s state to the actor-network, which generates a continuous parameter vector as a proto-action. This protoaction is used to find similar items using the ANNoy algorithm [19]. Finaly, the TDE algorithm [20] is used to select Top-N items with the highest diversity. The critic-network estimates  Q-value (i.e., the maximum future reward of the proto-action in the current state) which is used to update actor and critic network weights. Extensive experiments on the MovieLens dataset demonstrate that the diversity of recommendation is increasing. The major contributions of this paper are in three folds :  We propose an interactive recommendation framework based on deep reinforcement learning, which generates the recommendation list concerning both relevancy and diversity.  In order to reduce redundant computation in generating the recommendation list, we propose to find similar items to the generated proto-action by actor-network using the ANNoy algorithm and then select Top-N items with the highest diversity.  Instead of using RNN to capture user’s sequential behaviors, positional encoding to compute representations of the user’s interaction sequence proposed.  Shani et al. [13] utilized a Markov decision process (MDP) formulation to model sequential decision problems in recommender systems using a predictive n-gram model for the initial MDP. Zhao et al. [16] proposed a novel list-wise recommendation model based on deep reinforcement learning (i.e., LIRD), which models the interactive recommendation system as a Markov Decision Process. Hu et al. [21] proposed to formulate a ranking control problem in E-commerce searching using the search session markov decision process (SSMDP) and leveraged the deterministic policy gradient (DPG) method to learn an optimal ranking policy. Zheng et al. [22] proposed a DQN-based reinforcement learning model for personalized news recommendation, which explicitly model future rewards and utilizes an effective exploration strategy to improve the diversity of the recommendation. Liu et al. [15] proposed a novel deep reinforcement learning based framework, named DRR, which explicitly model user-item interactions.  The maximal marginal relevance (MRR) [23] was one of the earliest works to maximize relevance and diversity in reranking retrieved items used in text summarization. Ziegler et al. [24] proposed a topic diversification method to increase the diversity of recommendation list by decreasing the intra-list similarity (ILS). Bridge et al. [25] measured the distance between items using Hamming Distance, which enhanced the diversity of the recommendations by considering collaborative data only. To maximizing the diversity of the recommendation list while maintaining proportional similarity to the user request, Zhang et al. [26] modeled these goals as a bi-criterion optimization problem. Lathia et al. [27] showed the importance of temporal diversity in recommender systems by proposing an approach to measuring how the diversity of collaborative filtering data changes over the time. Abbassi et al. [28] proposed a (partition) matroid constraint algorithm to increase the diversity of representative documents set by avoiding selecting duplicate categories. Premchaiswadi et al. [20] proposed a ranking method named Total Diversity Effect Ranking (TDE) that increases the diversity of recommendation list by considering the overall diversity effect of each item. Lee et al. [29] constructed a stronglyconnected and undirected graph that each node is an item, and each edge represents positive interactions between items. Then utilizing the concept of entropy, find recommendations by considering novelty and relevancy. Cheng et al. [30] modeled the diversification problem in the recommendation system as a supervised learning task and formulated this diversified recommendation task as two optimization problems. Then proposed a diversified collaborative filtering method to solve the optimization problems. Wang et al. [31] proposed a novel method named DivRec LSH to produce diversified recommendations, which was achieved by using Locality-Sensitive Hashing (LSH) technique. Liu et al. [32] proposed the determinantal point process (DPP) method to promoting diversity in recommendations.  The recommender (i.e., agent) suggests a list of items with regard to both relevancy and diversity to the user (i.e., environment). Then, the user chooses one of them. The recommender agent performs this process sequentially over the time steps to maximize its cumulative reward. We model the recommendation process as a Markov Decision Process (MDP), which widely adopted in reinforcement learning. An MDP is a Markovian process described as follows :  State Space: a state  is defined as embeddings of   Reward: based on the chosen action at in state   by the recommender agent, a list of diverse while relevant items is recommend to the user. Then, the user interacts with these items and provides him feedback, e.g., rating, click or purchase, etc. and the recommender receives immediate reward R(,) according to diversity of recommendation list and the user’s feedback.  Transition: The user’s state changes from  to , when the recommender agent takes action at at time step . If the user ignores the whole recommended list, then the next state remains unchanged (i.e., = ). Otherwise, the next state  updates based on positive user’s feedback.  Discount rate:  is a factor that determines the importance of future rewards relative to immediate reward. If  , the agent will be short-sighted by only considering immediate rewards. On the contrary, if , the agent prefers long-term rewards. Hence, the discounted expected cumulative reward at time step  denoted as  The general scenario of the recommender-users interactions in MDP is illustrated in Fig. 2. In each state , the recommender (i.e., agent) chooses a proto-action  based on current policy . This proto-action is a continuous parameter vector which is used to generate a list of items. Then, the recommender updates the policy  based on user’s feedback and user’s state changes from  to . The objective of the reinforcement learning is to find a recommendation policy  to maximize the expected reward.  The actor-network utilizes a deep neural network to learn the optimal policy, i.e., generating the best action  based on the current user’s state . The user’s state can be defined as   such that  is embedding of  item in the latest  user’s positive interaction history. When the recommender agent generates a list of items and then recommends them to the user, if the user skips whole items in the list, then the next state remains unchanged, i.e., . If  are recommended items that the user has given positive feedback to them, then the next state  updates as follows:    In this paper, we use a probabilistic matrix factorization (PMF) technique as a embedding model such that chronological order of items in  is preserved. Since the current representation of items in the user’s state has no sense of how the items are in chronological order, we add Positional Encoding to each item’s embedding to determine the position of each item in the sequence. The positional encoding formula is as follows:     For example, if  is the user’s state, then positional encoding operation of  with , would be as below :      the output of the positional encoding step fed into two fullyconnected ReLU layers and one Tanh layer that maps the current state  to a proto-action at in this way: where the function  parameterized with , mapping from the state space  to the action space . The generated proto-action by the actor-network is not a valid action; i.e., it may not exist in the item space. Hence, we use the approximate nearest neighbor search (ANNS) to find the closest items to proto-action , and then generate a diverse list using TDE algorithm.  Given generated proto-action by the actor-network, a list of items is recommended to the user. Unlike the existing ranking functions [15]–[17], [33], which compute the distance from the proto-action to every other item in the action space to find similar items, we find κ-similar items to the proto-action using Approximate Nearest Neighbors Oh Yeah (ANNoy) [19], which achieved high performance in existing benchmarks [34], [35]. The ANNoy algorithm builds a binary tree where each intermediate node is a random hyperplane chosen by picking two random points in space and then splitting space by the hyperplane equidistant between them. This process is done recursively in each subspace until each node contains at most  items. The hyperplanes of first 400 items in the Movielens-100K dataset shown in Fig. 3a and corresponding binary tree shown in Fig. 3b.  Once the binary tree has been built, we find -nearest points for the generated proto-action  by traversing the binary tree from root to leaf. Since each leaf (i.e., sub-space) has at most  points, it is may be . To overcome this problem, [19] proposed to construct multiple trees (i.e., a forest) and combine search results from all trees. The roots of these trees are stored in a priority queue, and the search continues until  candidates are collected. In order to generate a diverse recommendation list, after removing duplicate candidates and sorting remaining items in ascending order of angular distance, we store candidate items in  and then compute the Total Diversity Effect (TDE) of each item  as follows  Finaly, we sort candidate items  in descending order of TDE and select Top-N items as recommendation list. The pseudo code of this process is shown in Algorithm 1.  Input:  ; generated proto-action by actor-network Output:  diverse recommendation list  1: find -similar items to  and store in   2: for  to  do  3:   Compute  usnig Eq. 6  4: end for  5:  = sort  in descending order of TDE  6:  = select Top-N items from   7: return   In Zhao et al. [16], in order to generate a list of recommendations of size , the actor-network generates  weight vectors of length , which  is the size of the item space; hence the computational complexity is . In [15], the generated proto-action by actor-network is used to rank all items in item space; in consequence, the computational complexity is  . In [32], in order to generate recommendations, a fast greedy MAP inference algorithm is proposed, whose computational complexity is  .The computational complexity of our proposed method for generating a recommendation list is  . The computational complexity of each query in the ANNoy algorithm is . In the worst case, if only one item is selected in each query, the required number of queries is , so the computational complexity of our proposed method is .  As shown in Fig.1, the critic network inputs (state , protoaction ) pair and outputs the estimation of the state-action value function , namely Q-function. The Q-function denotes the goodness of generated action  in state by following a policy . The critic network utilizes a deep Qnetwork (DQN) with parameter , which uses a deep neural network as a function approximator. Therefore, The Qfunction that specifies the expected return from state  with taking an action  under policy , can be defined as follows:  The recommender receives immediate reward  according to the diversity of the generated recommendation list and the user’s feedback. The reward function directly affects the result of Eq.7, which estimates the state-action value function . Therefore, we need to ensure the performance of the reward function before training the algorithm. In this paper, we define the reward function as follows:  The function  is the intra-list distance (ILD) [26] of recommended items , which defined as below :   and the function  is the average user rating to recommended items , which defined as below :   where  is current user’s rating to item , which simulated by pre-trained PMF model [36]. The parameter λ determines when the agent receives a negative reward. The agent will always receive positive rewards and not be penalized if this parameter does not exist. As shown in Fig. 4, we can plot   in terms of  to show how the reward function changes to improve  and . The ideal goal in this plot is to get to the top right corner, i.e.  and , which our reward function changes smoothly to achieve this goal.  In this paper, we use the Deep Deterministic Policy Gradient (DDPG) algorithm [37] to train our proposed model. In DDPG, there are four neural networks which are as follows: 1) a critic network with parameters  2) an actor-network with parameters  3) a target critic network with parameters  4) a target actor-network with parameters . In each episode, a user’s state is chosen, and the recommender interacts with the user in  step. In each step, the actornetwork generates a proto-action at to construct a diverse list  using Algorithm 1. Then, the user provides him feedback to recommendation list , and the recommender receives immediate reward . The user’s state is updated according to Eq. 1. After that, a tuple  stores in the replay buffer. In order to update the critic network , a mini-batch of size  is sampled from experience replay  to calculate the temporal Temporal difference (TD) target value  Then, the network parameter  is updated by minimizing following the loss function :  Also, the objective of the actor-network is to maximize the expected return, which is defined as follows :  In order to optimize this objective function, the gradient of  respect to policy parameters  can be taken using chain rule as follows:  Since the actor-network updates by sampled mini-batch of size  from experience replay, the mean of  samples is considered :  The target target networks are updated by a soft update technique as follows :          where  the training algorithm based on DDPG is described in Algorithm 2.  Algorithm 2 : Training Algorithm of proposed model 1:  Randomly initialize the network  and critic network  with the parameters  an d  2: Initialize target network  and  with  weights  and  3: Initialize replay buffer  4: for episode  to  do 5: Initialize state   6: Add positional encoding to each  according to         Eq. 4 7: for  to  do 8: Generate action  according to current  policy  9: Generate diverse list  according to Algorithm 1 10: Recommend list  to the user 11: Calculate reward  according to Eq. 8 12: Update user’s state from  to  according to    Eq. 1 13: Store transition  in  14: Set  15:     Sample a mini-batch of  transitions           from   17:     Update the Critic network by minimizing the loss:  19:     Update the Actor network using the sampled    policy gradient :  21: Update the target networks: 22:     23:     24: end for 25: end for  We use MovieLens datasets to evaluate our proposed framework. The MovieLens dataset contains a set of users’ expressed rating for movies which collected by the MovieLens website. In each row , there is a tuple  where  is rating (in range [1-5]) of user  to movie  at a particular time  . MovieLens-1M includes 1 million rating from 6040 users to 3706 movies. Since each user’s state  contains only positive interactions, we remove ratings less than 3 from both datasets.  We utilize following methods to compare performance of our proposed model :  LIRD [16]: Proposed a list-wise recommendation model based on deep reinforcement learning that the actor-network generates a list of weight vectors and then scores all items in item space using these weights.  DRR-ave [15]: Proposed a state representation module to model user-item interactions explicitly in deep reinforcement learning-based recommendation system. The generated proto-action by actornetwork is used to rank all items in item space. Then, the recommender agent recommends an item with the highest score to the user.  DRL [32]: Proposed a fast greedy MAP inference algorithm to promoting diversity in generated recommendations.  In each dataset, we consider 80% of users’ positive interactions for training and 20% of remaining for testing. Since the users’ ratings to all movies are not available in datasets, we use pretrained PMF, in which each user and item is represented as 100-dimensional embeddings, to simulate unknown ratings. In DRR-ave method, the scoring function recommends one item with the highest score to the user. Hence, we select N items with the highest score to generate a list of items. The major hyperparameters are listed in Table 1. We employ NDCG@k, Diversity@k and Precision@k metrics to evaluate the performance of the recommendation models. In this paper, we define Precision@k as follows :   such that  is simulated rating of the user  to the  item in recommended list  and .   https://grouplens.org/      The evaluation results are summarized in Fig.5 to 8. The results show that the proposed model is better than other methods in 90% of cases in terms of diversity, 40% in terms of precision and 30% in terms of nDCG. In other cases, however, it is slightly different from other methods. Fig.8 indicates that the use of positional encoding improved diversity by 9%, precision by 8%, and nDCG by 0.5%. Therefore, complex structures such as CNN and RNN can be replaced by positional encoding.  Fig. 5. Comparison of algorithms in terms of nDCG@10  Fig. 6. Comparison of algorithms in terms of Precision@10  Fig. 7. Comparison of algorithms in terms of Diversity@10  In this paper we proposed a deep reinforcement learning framework DRLIR to model an interactive recommendation system, which generates the recommendation list concerning both relevancy and diversity. Unlike previous works, which recommendations, we propose to use an approximate nearest neighbor search (ANNS) method to find the closest items to the proto-action and then generate a diverse list using the TDE algorithm. The results of experiments show that it is not necessary to use complicated structures such as RNNs and CNNs to construct the user state, and the user state can be constructed using positional encoding that has a low computational complexity. The use of positional encoding does not only reduce the precision and diversity of the recommendations list, but it also gives better performance than other methods. As future work, we intend to examine the effect of adding selfattention mechanism and combining it with positional encoding on the precision and diversity of the generated recommendations list. Additionally, we are interested in using probability distributions instead of offline datasets such as MovieLens to train algorithm.  [1] O. Chapelle and L. Li, “An empirical evaluation of thompson sampling,” in Advances in neural information processing systems, 2011, pp. 2249– 2257. [2] L. Li, W. Chu, J. Langford, and R. E. Schapire, “A contextual-bandit approach to personalized news article recommendation,” in Proceedings of the 19th international conference on World wide web, 2010, pp. 661– 670.  [3] X. Zhao, W. Zhang, and J. Wang, “Interactive collaborative filtering,” in Proceedings of the 22nd ACM international conference on Information & Knowledge Management, 2013, pp. 1411–1420. [4] J. Kawale, H. H. Bui, B. Kveton, L. Tran-Thanh, and S. Chawla, “Efficient thompson sampling for online matrixfactorization recommendation,” in Advances in neural information processing systems, 2015, pp. 1297–1305. [5] H. Wang, Q. Wu, and H. Wang, “Factorization bandits for interactive recommendation.” in AAAI, vol. 17, 2017, pp. 2695–2702. [6] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep reinforcement learning,” nature, vol. 518, no. 7540, pp. 529–533, 2015. [7] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016. [8] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the game of go without human knowledge,” nature, vol. 550, no. 7676, pp. 354–359, 2017. [9] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., “Qt-opt: Scalable deep reinforcement learning for visionarXiv:1806.10293, 2018. [10] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray et al., “Learning dexterous in-hand manipulation,” The International Journal of Robotics Research, vol. 39, no. 1, pp. 3–20, 2020. [11] D. Ramani, “A short survey on memory based reinforcement learning,” arXiv preprint arXiv:1904.06736, 2019. [12] N. Taghipour and A. Kardan, “A hybrid web recommender system based on q-learning,” in Proceedings of the 2008 ACM symposium on Applied computing, 2008, pp. 1164–1168. [13] G. Shani, D. Heckerman, and R. I. Brafman, “An mdpbased recommender system,” Journal of Machine Learning Research, vol. 6, no. Sep, pp. 1265–1295, 2005. [14] X. Zhao, L. Zhang, Z. Ding, L. Xia, J. Tang, and D. Yin, “Recommendations with negative feedback via pairwise deep reinforcement learning,” Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Jul 2018. [Online]. Available: http://dx.doi.org/10.1145/3219819.3219886 [15] F. Liu, R. Tang, X. Li, W. Zhang, Y. Ye, H. Chen, H. Guo, and Y. Zhang, “Deep reinforcement learning based recommendation with explicit user-item interactions modeling,” arXiv preprint arXiv:1810.12027, 2018. [16] X. Zhao, L. Zhang, L. Xia, Z. Ding, D. Yin, and J. Tang, recommendations,” arXiv preprint arXiv:1801.00209, 2017. [17] X. Zhao, L. Xia, L. Zhang, Z. Ding, D. Yin, and J. Tang, recommendations,” in Proceedings of the 12th ACM Conference on Recommender Systems, 2018, pp. 95– 103. [18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in neural information processing systems, 2017, pp. 5998–6008. https://github.com/spotify/annoy. [20] W. Premchaiswadi, P. Poompuang, N. Jongswat, and N. Premchaiswadi, “Enhancing diversity-accuracy technique on user-based top-n recommendation algorithms,” in 2013 IEEE 37th Annual Computer Software and Applications Conference Workshops. IEEE, 2013, pp. 403–408. [21] Y. Hu, Q. Da, A. Zeng, Y. Yu, and Y. Xu, “Reinforcement learning to rank in e-commerce search engine: Formalization, analysis, and application,” in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, pp. 368–377. [22] G. Zheng, F. Zhang, Z. Zheng, Y. Xiang, N. J. Yuan, X. Xie, and Z. Li, “Drn: A deep reinforcement learning framework for news recommendation,” in Proceedings of the 2018 World Wide Web Conference, 2018, pp. 167–176. [23] J. Carbonell and J. Goldstein, “The use of mmr, diversitybased reranking for reordering documents and producing summaries,” in Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, 1998, pp. 335–336. [24] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and G. Lausen, “Improving recommendation lists through topic diversification,” in Proceedings of the 14th international conference on World Wide Web, 2005, pp. 22–32. [25] D. Bridge and J. P. Kelly, “Ways of computing diverse collaborative recommendations,” in International conference on adaptive hypermedia and adaptive web-based systems. Springer, 2006, pp. 41–50. [26] M. Zhang and N. Hurley, “Avoiding monotony: improving the diversity of recommendation lists,” in Proceedings of the 2008 ACM conference on Recommender systems, 2008, pp. 123–130. [27] N. Lathia, S. Hailes, L. Capra, and X. Amatriain, “Temporal diversity in recommender systems,” in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, 2010, pp. 210–217. [28] Z. Abbassi, V. S. Mirrokni, and M. Thakur, “Diversity maximization under matroid constraints,” in Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 32–40. [29] K. Lee and K. Lee, “Escaping your comfort zone: A graph-based recommender system for finding novel recommendations among relevant items,” Expert Systems with Applications, vol. 42, no. 10, pp. 4851– 4858, 2015. [30] P. Cheng, S. Wang, J. Ma, J. Sun, and H. Xiong, “Learning to recommend accurate and diverse items,” in Proceedings of the 26th international conference on World Wide Web, 2017, pp. 183–192. [31] L. Wang, X. Zhang, R. Wang, C. Yan, H. Kou, and L. Qi, “Diversified service recommendation with high accuracy and efficiency,” KnowledgeBased Systems, vol. 204, p. 106196, 2020. [32] Y. Liu, Y. Zhang, Q. Wu, C. Miao, L. Cui, B. Zhao, Y. Zhao, and L. Guan, “Diversity-promoting deep reinforcement learning for interactive recommendation,” arXiv preprint arXiv:1903.07826, 2019. [33] G. Dulac-Arnold, R. Evans, H. van Hasselt, P. Sunehag, T. Lillicrap, J. Hunt, T. Mann, T. Weber, T. Degris, and B. Coppin, “Deep reinforcement learning in large discrete action spaces,” arXiv preprint arXiv:1512.07679, 2015. [34] M. Aumuller, E. Bernhardsson, and A. Faithfull, “Annbenchmarks: ¨ A benchmarking tool for approximate nearest neighbor algorithms,” in International Conference on Similarity Search and Applications. Springer, 2017, pp. 34– 49. [35] ——, “Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms,” Information Systems, vol. 87, p. 101374, 2020. [36] A. Mnih and R. R. Salakhutdinov, “Probabilistic matrix factorization,” in Advances in neural information processing systems, 2008, pp. 1257– 1264. [37] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with arXiv:1509.02971, 2015 