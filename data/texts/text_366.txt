In this article, we investigate various numerical methods for computing scaled or logarithmic sensitivities of the form ∂ ln y/∂ ln x. The methods tested include One Point, Two Point, Five Point, and the Richardson Extrapolation. The diﬀerent methods were applied to a variety of mathematical functions as well as a reaction network model. The algorithms were validated by comparing results with known analytical solutions for functions and using the Reder method for computing the sensitivities in reaction networks via the Tellurium package. For evaluation, two aspects were looked at, accuracy and time taken to compute the sensitivities. Of the four methods, Richardson’s extrapolation was by far the most accurate but also the slowest in terms of performance. For fast, reasonably accurate estimates, we recommend the two-point method. For most other cases where the derivatives are changing rapidly, the ﬁve-point method is a good choice, although it is three times slower than the two-point method. For ultimate accuracy which would apply particularly to very fast changing derivatives the Richardson method is without doubt the best, but it is seven-times slower than the two point method. We do not recommend the one-point method in any circumstance. The Python software that was used in the study with documentation is available at: https://github.com/evanyfyip/SensitivityAnalysis. In this study we will look at a number of diﬀerent approaches to computing steady-state sensitivities in reaction networks [11, 12]. We are especially interested in the reaction networks found in biological systems, where they are often catalyzed by enzymes. In these cases, computing sensitivities is an eﬀective way to gauge the role of the diﬀerent steps in a regulated biochemical pathway. Reaction networks are composed of one of more species nodes, x via chemical reaction steps. For example, a simple chain of reaction steps is shown below: where vis the reaction rate for the i described by a set of diﬀerential equations (ODE), where each ODE describes the rate of change of a particular species. According to the conservation of mass, the rate of change of a given species must be the diﬀerence between the input rate and output rate. For example, in the above scheme, the rate of change of xwill be given by: Such equation can be devised for every species in the network. In order to sustain a steady-state the sources and sinks must be clamped, that is, held constant. In the above scheme, x be more speciﬁc, consider the simplest reaction network of two reactions: We will assume Xand Xare clamped and do not change in time. This means this system [11]: For simplicity, if we assume that the concentration of X diﬀerential equation for xis given by: At steady-state dx/dt = 0. Setting the diﬀerential equation to zero gives: Solving for xyields: We conclude from this that the steady-state is a function of all the parameters in the system. Moreover we can also derive the steady-state ﬂux, J, through the system by substituting the steady-state value for x Note that if the ﬁrst reaction v reduces to J = kX Of interest is how the various constants inﬂuence the steady-state concentrations and ﬂuxes. For example, if the reactions are enzyme catalyzed, it would be useful to know how much the activity of the various enzymes contribute to the steady-state ﬂux and species concentrations. There are diﬀerent ways to measure this inﬂuence, one way is to compute the unscaled derivatives: where p is some parameter that we are interested in. An alternative formulation is to consider the scaled derivatives: These can be more useful because they are unit-less and also represent a ratio of fold changes which tends to be easier to measure experimentally. In the literature, these scaled sensitivities are referred to as control coeﬃcients [6, 10], where p is the enzyme activity, or response coeﬃcients when p is for example an inhibitor such as a therapeutic drug. is irreversible then the steady-state ﬂux and and In the simple network example above, we derived, with relative ease, the steady-state solution to the diﬀerential equation. However for large networks and especially those where the reaction rates v functions, it is not possible to derive closed solutions for the steady-state ﬂux and steady-state. Instead, we must revert to numerical methods. In the remainder of the paper we will discuss various numerical methods that can be used to compute the control coeﬃcients. Of particular interest are their accuracy and speed of computation. There are various numerical methods for estimating the derivative of a function [1]. The choice of which method to use depends on how accurate the estimate should be and how fast does the computation need to proceed. The simplest method for computing a derivative of a function f(x) is the onepoint method. All numerical methods for estimating derivatives are by their nature approximate. The one-point method [1], also know as the Newton’s diﬀerence quotient or forward-diﬀerent method, is the simplest and is given by: The estimate relies on the step-size, h. A step-size that is too large, will incur a signiﬁcant error in the estimate for the derivative especially when the function f(x) is non-linear. It’s possible to estimate this error by expanding the function, f (x) in a Taylor series: The error in our estimate comes from neglecting the higher terms. If we assume this error is dominated by the ﬁrst term in the higher terms (i.e the second derivative), then we can show that the error is given approximately df(x) by: This tells us that the higher the value of h the greater the error. It also indicates that the error is a function of the second derivative, that is the curvature. For a simple equation such as y = x of value 2, so that the error term reduces to h. For y = x estimate will therefore equal the step-size. Rearranging equation (1) yields: In the limit, as the step size, h, tends to zero the approximation becomes exact: The one-point method is not recommended for estimating derivatives due to errors resulting from the higher terms. Instead a much more accurate estimate can be obtained by computing the value of the function on both sides of the center point. This is known as the two-point or central diﬀerence quotient [1]. The formula for this is given by: In the one-point scheme the error was proportional to the size of the step, h. In the two-point case the error is proportional to the square of the step size h. One way to think of the two-point scheme is as a combination of a forwarddiﬀerence and backward-diﬀerence method. When combined, the ﬁrst-order terms cancel leaving the second order term intact. Hence the error become proportional to the square, or h. This can be shown as follows. First, we write out the forward and backward schemes as a Taylor series: df(x) f(x + h) − f (x) (x) + (h)+ (h) (x) + (h)− (h) We then take the diﬀerence between them by computing f(x + h) −f(x −h) from which we obtain: Rearranging this by dividing both sides by 2h yields: We can see from this result that the error is proportional to h generally less than one, the error decreases signiﬁcantly when using the twopoint method. As a result, the two-point method is widely use for computing derivatives. The error can still be signiﬁcant, however, if the curvature of the function changes rapidly. We will show an example of this in the results section. An interesting observation is that for a function such as y = x derivative, f(x) is zero. Therefore the two-point scheme is exact since the error term reduces to zero. There exists other variants on the diﬀerence quotients for computing derivatives especially higher derivatives. We will consider one more, which is the ﬁve-point method [1] for estimating the ﬁrst-derivative. This is given by: f(x) = 5 Here we see that the error is of the order h h = 0.1, the error contribution from h would be 0.00001. The previous methods use either reduced steps sizes or higher order methods to estimate a derivative. This fourth approach utilizes a completely diﬀerent ≈ f(x) + (h) method, called the Richardson Extrapolation [9, 5]. This method computes two estimates for the derivative (using diﬀerent h) from which it computes a third, more accurate estimate. The basic idea is that we compute the two-point method using two diﬀerent values of h. We now have two equations with one common unknown, the derivative itself. A commonly used set of values for h are h and h/2 such that the solution for the derivative is given by: where D(h) is the value of the two-point estimate for the derivative when using h as the step size. The Richardson Extrapolation can be iterated so as to improve the accuracy of the estimate even more. It only takes a few such iterations to obtain very accurate estimates for the derivative. In particular given that estimates can be computed at using diﬀerent h pairs, the results of these can also be combined. For example four initial estimates at h, h/2, h/4, h/16, giving three Richarsdson combinations that can be computed. From these three a further two can be computed, from which a ﬁnal estimate can be computed. These calculation form what is called an extrapolation table as shown below: A= D(h) Such a table can be easily computed using software. Of interest is that the ﬁrst combination of h and h/2 (A method. All the methods we have just described were implemented in Python 3.8 and stored in a package called SensitivityAnalysis which uses Tellurium to store models and reaction networks. The Richardson Extrapolation method was ) is equivalent to using the ﬁve-point based oﬀ of an example method written in C by H. Press et al. [8]. In the following sections the accuracy and performance of the diﬀerent numerical diﬀerentiation methods will be compared. Mathematical functions and reaction models were used to compare the accuracy across the four diﬀerent methods. Shown below in Table 1 are the nine diﬀerent mathematical test functions with their bounds [7]. For each of these functions the derivative was computed across the speciﬁed range [a, b] using the four diﬀerent numerical diﬀerentiation methods. We would expect there to be an increase in accuracy as we move from the more simple methods to the more complex methods. This is observed in the heat map shown in Figure 1, where the accuracy of each method is compared using the mean squared error. For each of the methods, the estimated value of the derivative was compared to the actual value which was computed analytically through formulas shown in Table 1. In this plot, the darker red colors correspond to increased error while the lighter yellow colors correspond to low error. Across all the methods, there is a signiﬁcant decrease in error moving from One-Point to Two-Point, Five-Point and ﬁnally Richardson Extrapolation. The highest computed error within the Richardson Methods was 1.2 is still orders of magnitude below the lowest error of the other three methods. In addition to the mathematical functions, we will explore the accuracy of the numerical diﬀerentiation methods on simple reaction networks. As described earlier, reaction networks can be modeled by a series of reactions chained together. For this analysis we will use a simple three step linear pathway with two ﬂoating species and two ﬁxed species: Table 1: Nine test functions evaluated over ten equally-spaced points within the interval [a, b] Figure 1: The mean squared errors of the computed numerical derivative via the four methods. The order of the functions in the heat map match the order shown in Table 1. To compare the accuracy to a reference, the control coeﬃcients can be computed symbolically using the following procedure. We ﬁrst constructed a model using tellurium [2] and then simulated it until it reached steady state. Then, we computed the elasticity coeﬃcients using the steady-state concentrations of the species Sand Sas well as the values of the rate constants. The general formulas used for computing the elasticities for the substrate and product are shown below [10]: Using the computed elasticities ( mulas, we computed the control coeﬃcients. The values of the nine control coeﬃcients are shown in Figure 2. , , , and ) and the following for- Figure 2: The computed values of the three step linear pathway Control Coeﬃcients via analytical methods Once we had a reference for the control coeﬃcients, the coeﬃcients were recomputed numerically using the four methods. The Richardson Extrapolation methods performed signiﬁcantly better than the other three methods with absolute errors on the order of 10 ure 3, we can see that the absolute error decreases across individual control coeﬃcient from One-Point to Richardson methods. This is further supported by Figure 4, where the errors across all nine coeﬃcients are summarized by the mean absolute error and put on a log-scale bar chart. In this bar chart we can see that the mean absolute error appears to decrease exponentially as we move towards the more complex methods. . Looking at the heat map in Fig- Figure 3: Shown here are the absolute errors of the computed control coeﬃcients of the three step linear pathway. The x-axis shows the numerical diﬀerentiation type while the y-axis highlights the speciﬁc control coeﬃcients (CjE1 represents Cand Cs1E1 represents C through the pathway, S1 represents species 1, and E1 represents the enzyme concentration of the ﬁrst step Figure 4: The mean absolute error for each numerical diﬀerentiation method computed on the three step linear pathway. The results from both the mathematical test functions and reaction networks imply that the Richardson Extrapolation method is the most accurate numerical diﬀerentiation method explored here. In the last section, the three step reaction network described had reasonably well behaved derivatives, characterized by moderate size and gradual changes. Thus, in the next section, s we will test the numerical diﬀerentiation techniques on a nonlinear system with derivatives of signiﬁcant magnitudes. A good example of fast changing and large derivatives is found in phosphorylation cycles that exhibit ultrasensitivity [4, 3, 10]. The cycle is as follows: The rate equations used Michaelis-Menten Kinetics for both the forward and backward arms of the cycle are shown below: Using our Richardson extrapolation method we can get an accurate estimate of the Control Coeﬃcients of concentration of AP with respect to changes in parameter K (C). Shown below in Figure 5, is the computed control coeﬃcients over a range of diﬀerent K parameter values from 0.1 to 2.0. We can also compute the elasticity coeﬃcients in a similar manner as shown in Figure 6. Using the Richardson Extrapolation we are able to get accurate estimates of the sensitivities of the reaction networks to changes in various constants and concentrations. After computing the coeﬃcients, we can determine the most eﬃcient method of perturbing the system to achieve a desired eﬀect (e.g. increased steady-state concentration or ﬂuxes). Figure 5: The computed scaled and unscaled sensitivities of concentration of AP with respect to parameter K. There is a narrow region around K=1.0 where concentration of AP is highly sensitive to changes in AP. Figure 6: The computed scaled and unscaled elasticity coeﬃcients for v2 with respect to AP. From the plot we can see that as concentration of AP increases the sensitivity of reaction v2 decreases. After evaluating the accuracy, we now shift our focus to the performance and eﬃciency of the four numerical diﬀerentiation methods. In this study, we measured these metrics on the simple phosphorylation cycle. The two methods of interest are getuCC and getuEE. getuCC returns the unscaled control coeﬃcients for a given variable (reaction or species concentration) with respect to a parameter (kinetic constant or boundary species). getuEE computes the unscaled elasticities or local sensitivities for a given reaction rate vwith respect to a given parameter. The two methods respective scaled versions are not explored here as the computational cost of scaling is trivial. Shown in Figure 7, we can see that as we move from simple diﬀerentiation methods to more complex methods, it comes at the cost of eﬃciency. For 1000 iterations of calling getuCC, the one-point method took 22.3 ms while the Richardson Extrapolation method took 179.2 ms. Table 2: Time elapsed for 1000 iterations of getuCC using speciﬁed method type on the simple phosphorylation cycle. Figure 7: The time performance of the getuCC method using diﬀerent numerical diﬀerentiation methods on the simple phosphorylation cycle. In contrast, the diﬀerence in time performance of the four methods with respect to the getuEE method is much less prominent. Unlike the getuCC method, where the Richardson Extrapolation method took almost 8 times as long, this method is only 2 times slower. Shown in Table 3 and Figure 8 are the elapsed times for 1000 iterations of calling getuEE. The stark diﬀerences in times can be attributed to the necessity of computing steady state concentrations during the getuCC method. Table 3: Time elapsed for 1000 iterations of getuEE using speciﬁed method type on the simple phosphorylation cycle. Figure 8: The time performance of the getuEE method using diﬀerent numerical diﬀerentiation methods on the simple phosphorylation cycle. To download and use the SensitivityAnalysis Package follow the instructions listed on the README.md ﬁle in the GitHub repository linked here: https: //github.com/evanyfyip/SensitivityAnalysis In this short study, we investigated a variety of methods to numerically compute steady-state sensitivities. Such sensitivities allow us to understand how much a speciﬁc enzyme contributes to the steady-state ﬂux or species concentrations. We explored a number of diﬀerent numerical methods for estimating the sensitivities. Of the methods examined (One-Point, Two-Point, Five-Point, and Richardson Extrapolation Methods), we determined that on average the Richardson Extrapolation Method performed 3 orders of magnitude better than the other methods. This was shown through comparing the mean squared errors of the computed derivative across 9 diﬀerent mathematical functions. In addition, we looked at a simple biological model (a three step linear pathway) and compared the computed control coeﬃcient errors across the methods. Again, the Richardson Extrapolation Method had signiﬁcantly lower error. Thus, in terms of performance, the Richardson Extrapolation method has a much higher accuracy than other methods. This high accuracy, however, comes with a trade oﬀ in terms of time eﬃciency. The run times of the getuCC and getuEE methods using the diﬀerent numerical diﬀerentiation techniques showed that the Richardson Extrapolation method is much slower than the other techniques. With regards to getuCC the Richardson Extrapolation Method was almost 9 times as slow as the one point numerical diﬀerentiation method. Similarly, the run time on the getuEE method was twice as slow as the one point method. Overall, the Richardson Extrapolation methods explored here provide a more accurate calculation of the control and elasticity coeﬃcients allowing researchers to gain better insight into cellular dynamics and the potential drug targets in biological pathways.