Zehua Zeng, Phoebe Moh, Fan Du, Jane Hoffswell, Tak Yeon Lee, Sana Malik, Eunyee Koh, and Leilani Battle Abstract— Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difﬁcult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately speciﬁed from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are speciﬁed using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare ﬁve existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our ﬁndings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their beneﬁts in various analysis scenarios. Index Terms—Visualization Tools, Visualization Recommendation Algorithms 1 INTRODUCTION The visualization community has developed a wide variety of systems for recommending how to visualize data [38]. The algorithms behind these systems aim to help users uncover meaningful insights in their data by automatically generating visualizations for analysts to explore. For example, Voyager [36, 37] encourages broad data exploration by recommending effective charts based on Mackinlay’s [24] design principles. VizDeck [17] and Foresight [7] recommend visualizations based on standard statistical characteristics of the dataset. SeeDB [30] recommends visualizations based on a self-deﬁned criterion of statistical “interestingness”, or divergence of a sub-population from the whole. While this panoply of recommendation algorithms provides many viable alternatives, it is unclear which algorithm should be prioritized for any given visualization scenario. In a review of existing evaluation practices, we ﬁnd that many recommendation systems evaluate their recommendation algorithms in isolation [22], or construct benchmarks that their systems are already optimized for [24, 30, 36, 37]. Even evaluations that do compare different algorithms do not measure user performance [13, 21, 26]. In other words, our community tends to generate new visualization recommendation algorithms without giving commensurate thought on how to evaluate them. As a result, the visualization community lacks rigorous theoretical and empirical guidance for how and when to apply each of these algorithms effectively. One way to address this problem is to develop a standardized framework for comparing different visualization recommendation algorithms. Given that the purpose of these algorithms is to help analysts visually explore their data, a standardized framework should enable us to directly compare algorithms based on how they impact a user’s per- • Zehua Zeng is with University of Maryland. E-mail: zhzeng@umd.edu. • Phoebe Moh is with University of Maryland. E-mail: pmoh@umd.edu. • Fan Du is with Adobe Research. Email: fdu@adobe.com. • Jane Hoffswell is with Adobe Research. Email: jhoffs@adobe.com. • Tak Yeon Lee is with KAIST, work performed while at Adobe Research. Email: takyeonlee@kaist.ac.kr. • Sana Malik is with Adobe Research. Email: sanmalik@adobe.com. • Eunyee Koh is with Adobe Research. Email: eunyee@adobe.com. • Leilani Battle is with University of Washington, work performed while at University of Maryland. E-mail: leibatt@uw.edu. Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx formance for a variety of visual analysis tasks [1, 18]. The framework should also facilitate comparison of the algorithmic performance of the proposed approaches; for example, the framework should enable us to compare how each algorithm enumerates and traverses the design space of candidate visualizations in search of an optimal recommendation. In this paper, we propose an evaluation-focused framework to enable more effective theoretical and empirical comparisons of visualization recommendation algorithms. Our framework is based on the central process connecting most if not all of these algorithms: to generate the “best” recommendations, an algorithm must be able toenumerate the space of possible visualization designs andrankthis design space, often by approximating and comparing the utility of candidate visualizations. Our evaluation framework is deﬁned through three major components: (1) a network representing the space of all possible visualization designs for a given dataset, where nodes are visualization designs and edges connect designs that differ by a single encoding or data transformation; (2) the method a recommendation algorithm uses to traverse the design space to enumerate candidate visualization designs; and (3) an oracle used to approximate and rank the value of candidate visualizations that are enumerated. Existing frameworks such as CompassQL [35], ZQL [29], and Draco [26], focus on generating new visualization recommendation algorithms, rather than comparing algorithms. As a result, behavioral differences are not intuitively captured through these frameworks, making it difﬁcult to reason about the differences in algorithmic performance. For example, it is not clear how one might cluster different recommendation algorithms based on their behavioral similarity. With our framework, these behavioral differences become obvious. For example, Voyager [36,37] by default recommends visualizations which are one design or data transformation away from the current visualization, representing a narrow but efﬁcient traversal of the visualization design space. In contrast, machine-learning-based algorithms enumerate and rank massive sub-spaces of visualization designs represented by the model’s input features [13, 22]. We demonstrate the generality and coverage provided by our framework by comparing the behavior of ﬁve visualization recommendation systems: Voyager [36, 37], DeepEye [22], Foresight [7], Show Me [25] and Dziban [21]. We also show how our framework clariﬁes gaps in the literature where new algorithms can be formed, simply by varying traversal method and oracle combinations. Using two common graph traversal methods, breadth-ﬁrst search (BFS) and depth-ﬁrst search (DFS), and the oracles for Voyager [36] and Dziban [21], we construct four recommendation algorithms: CompassQL+BFS (i.e., Voyager), CompassQL+DFS, Dziban+BFS, and Dziban+DFS. We then use our Fig. 1: Visualization recommendation algorithms are often speciﬁed using frameworks, and evaluated using implemented systems. framework to design a user study to guide the empirical evaluation of these four visualization recommendation algorithms. Our results show that subjects did not perform signiﬁcantly better with Dziban compared to CompassQL in focused-oriented tasks, however subjects did ﬁnd Dziban’s recommendations to be more intuitive in post-task survey ratings. These ﬁndings reinforce our argument that we need more evaluationfocused frameworks to elucidate the beneﬁts of existing recommendation algorithms in real-world visual analysis scenarios. All of our data and code are available online on OSF: https://osf.io/txqsu/. 2 RELATED WORKFig. 2: Illustration of the As illustrated in Fig. 1, users can only interact with visualization rec-ation ommendation algorithms when provided with an interface, and often aas an example. The user’s current visualization is at node n full system through which to interact. Furthermore, several algorithms are speciﬁed using existing visualization recommendation frameworks,recommendations. Current frameworks omit these details, making it which often take the form of specialized languages. In this section, wedifﬁcult to use them to evaluate and compare different algorithms. In discuss the relevant literature in specifying visualization recommenda-contrast, our framework gives a clear deﬁnition of the visualization tion algorithms, and evaluating both algorithms and systems.design space and considers an algorithm’s traversal method through 2.1 Visualization Recommendation Algorithmsenumeration, which makes the enumeration process comparable. Existing recommendation algorithms can be separated into two main2.3 Evaluating Visualization Recommendation Algorithms categories based on how the ranking engine (oracle) is implemented:Evaluation is crucial since it provides evidence of whether a proposed rule-based or machine learning-based. Rule-based algorithms enumer-algorithm actually helps users explore their data more effectively. Howate and then rank visualizations using heuristics based on theory orever, not all algorithms are evaluated in terms of how they improve user experimental ﬁndings in visual perception [7,9,17,25,28,36,37]. Forexploration performance. For example, Foresight [7] only provides example, theory work from Bertin [4] and Mackinlay [24] has beensome usage scenarios to demonstrate its efﬁcacy. On the other hand, incorporated within rule-based algorithms behind a wide range of rec-although some existing systems do empirically evaluate user perforommendation systems, including Voyager [36,37] and Show Me [25].mance, the proposed algorithms are evaluated in isolation. For instance, Other recommendation systems, such as VizDeck [17] and Foresight [7]Voyager [36, 37] and SeeDB [30] were compared to a baseline with rank visualizations using manually-selected statistical rules.no recommendations provided. VizDeck [17] claimed that VizDeck Instead of ranking visualizations with manually-derived rules, otherusers completed tasks with higher accuracy and less time compared algorithms train machine learning models to generate recommenda-to IBM ManyEyes [31], Google Fusion Tables [10], and Tableau [33]. tions [5, 8, 13, 22, 26, 32]. For example, Hu et al. [13] trained a deepHowever, none of the compared systems provide recommendations. learning model to learn the most common visualization designs from a large corpus of data sets and their associated Plotly visualizations. Oneperformance is still not the focus. Dziban [21] was evaluated by calcuof the Draco applications developed by Moritz et al. [26], Draco-Learn,lating the ranking of its recommended visualizations in both Draco [26] was implemented by training models to learn effectiveness criteria fromand GraphScape [19] algorithms to check whether it provides a favorprevious experimental ﬁndings. Luo et al. [22] strive to balance theable tradeoff in terms of effectiveness and similarity. On the other hand, best of both strategies by combining deep learning with hand-writtenDeepEye [22] was tested by ground-truth data, which was derived by rules to generate recommendations.having students to label whether a visualization is good or bad. SimiOur framework provides a means of comparing these different rank-larly, VizML [13] was compared with CompassQL [35], DeepEye [22], ing strategies, or oracles, in a systematic and repeatable manner.ShowMe [25] and Data2Vis [8] using an effectiveness score which was 2.2 Visualization Recommendation Frameworkscalculated based on human-labeled data. However, human-perceptually Several frameworks have been proposed to make it easier to create“good” visualizations do not necessarily help the actual analysis process. new visualization recommendation algorithms. CompassQL [35] isSince tasks are not taken into account in the labelling process, there a query language created by Wongsuphasawat et al., which can pro-exists no evidence of whether these benchmark results can carry over duce different types of recommendation algorithms by varying phasesinto the actual human performance with higher level analysis tasks. of the recommendation process, such as enumerating, choosing, and ranking. For example, the visualization recommendation algorithmcally compare visualization recommendation algorithms, and empiriin Voyager [36, 37] is implemented with CompassQL. ZQL [29] iscally evaluate user performance for different visual analytics tasks. another query language that serves a similar purpose for visualization3 EVALUATION FRAMEWORK recommendation in the Zenvisage system. Draco [26] is an alternativeIn this section, we describe our framework, which is based on the genframework for specifying visualization recommendation algorithmseral recommendation process followed by the majority of visualization based on answer set programming. Using Draco, one can specify newrecommendation algorithms: enumerate, search, and rank. To demonalgorithms using a combination of encoding constraints and weightsstrate how our framework can be applied, we compare ﬁve existing for these constraints. In this way, Draco enables the creation of newrepresentative visualization recommendation systems: Voyager [36,37], recommendation algorithms, without the creator having to worry aboutDeepEye [22], Foresight [7], Show Me [25] and Dziban [21]. how to enumerate the underlying visualization design space. To evaluate recommendation algorithms, we need to know not only3.1 Deﬁning the Core Components of the Framework the constraints imposed by the algorithms (the focus of current frame-Visualization recommendation algorithms are a form of search algoworks), but also the strategies employed to apply these constraints.rithm, which generally follow two basic steps: traverse candidates Furthermore, we need to know what the differences are between strate-within the larger search space, and evaluate these candidates against gies in order to reason about how they impact the performance of thespeciﬁc search criteria. In the case of visualization recommendation, system running the algorithm and the decisions of users who view thethe traversal step involves step of a hybrid recommendation algorithm, using movies data Even when multiple algorithms are compared in the literature, user In this paper, we show how our framework can be used to theoretiand the evaluation step requiresrankingthe candidate visualizations for the subsequent recommendation. However, before an algorithm can enumerate candidate visualizations, thevisualization design space must ﬁrst be clearly deﬁned. In this section, we deﬁne the visualization design space, as well as the enumeration and ranking steps. 3.1.1 Deﬁning the Visualization Design Space To facilitate comparison, we must ﬁrst establish a consistent deﬁnition of the visualization design space that can be applied to a wide range of algorithms. Prior work uses graph theory to model visualization spaces [12, 15, 23], however previous deﬁnitions cover only a fraction of the full design space [26, 29, 30, 35]. We contribute a generalization of these existing visualization spaces using graph theory. In our framework, we consider the full design space of all possible visualizations, which is deﬁned as the combination of data attributes, encoding channels, and data transformations that can be applied to a given dataset. By leveraging this full design space, individual algorithms can be compared in terms of the particular subspaces they traverse. Tracking Visualization Designs Within the Design Space Graph. Suppose we are generating recommendations for a movies datasetD, containingnattributesA = {a, ..., a}, such as movie title, creative type, gross, release date, etc. There arempossible transformationsT = {t, ..., t}; each transformation has a set of parameters to determine how it can be applied to the data. For example, one possible data transformation is calculating the average of movie gross:AVG(a), which is parameterized by only one attribute. On the other hand, there also existkpossible encoding channelsC = {c, ..., c}which are used to visualize the combination of attribute and data transformation, such as, the 13 encoding channels proposed by Mackinlay [24]. We represent the visualization design space for this dataset as a network graphG = (N, E). Each node of the graphn ∈ Ncontains visualizations which are deﬁned by a set of data attributes (a,a, etc.), data transformations (t, t, etc.), and encoding channels (c, c, etc.): Here,a, etc. are attributes fromD, andt, etc. are data transformations operated on attributesa, etc., whilec, etc. are encoding channels used to visualize the combination of(a, t), etc.vis a visualization deﬁned by the attribute set{a, a}and its corresponding data transformations{t, t}and encoding channels{c, c}. Edges between each pair of nodes represent operations that transform one node to another, such as adding one attribute, or changing the data transformation or encoding channel of an attribute. 3.1.2 Deﬁning Sub-spaces for Different Types of Algorithms To more efﬁciently navigate the visualization design space, recommendation algorithms can merge multiple visualizations into one node, or even ignore nodes, reducing the total edges that needed to be traversed. We discuss how different algorithms manipulate the visualization design space, based on the three types of recommendation algorithms proposed by Wongsuphasawat et al. [35]: algorithms suggesting what attributes and/or transformations to visualize (data query recommendations), what encoding channels to apply to selected data (visual encoding recommendations), or both (hybrid recommendations). Visual Encoding Recommendations. These algorithms focus on enumerating and ranking variations in encoding choices (e.g., Show Me [25]), requiring access to attribute, transformation, and encoding information. However, to reduce the cost of enumerating the visualization design space, these algorithms often require the user to select what attributes and transformations to visualize in advance. In this way, all nodes that include non-user-selected attributes can be ignored. We can represent this user selection-based subspace in the following way: n = {v = [c(a, t), ...] | selected(a, t) = 1, ∀(a, t Data Query Recommendations. These algorithms tend to focus on recommending attributes and/or transformations, and ignore encoding channels (e.g., Foresight [7] and SeeDB [30]). To ignore design variation, we can merge all visualizations that vary only by encodings into one node (i.e., remove all encoding channel speciﬁers{c, c, ...}): Hybrid Recommendations. These algorithms consider variations in attributes, transformations, and encoding channels (e.g., Voyager [36, 37]). As a result, the full expressiveness of the visualization design space graph is required. However, enumerating all possible combinations of attributes, transformations, and encoding channels can be prohibitively expensive. In the next section, we discuss how algorithms efﬁciently enumerate candidate visualizations within this space of possible visualization designs. 3.1.3 Deﬁning the Enumeration Step Given a formal deﬁnition of the visualization search spaceG, search algorithms must then traverse this space to identify qualiﬁed candidate results. The result of the enumeration step is a list of candidates that match the input requirements, which are then passed to the ranking step. However, the full visualization design space is an exponential function of attributes{a, ..., a}, transformations{t, ..., t}, and encoding channels{c, ..., c}, making it prohibitively large to search in its entirety. As a result, visualization recommendation algorithms must address a trade-off between recommendation breadth and execution cost, where higher quality results can be achieved by enumerating and ranking more of the visualization design space, but performing this additional work increases the algorithm’s execution time. Input Nodes to the Enumeration Step. In response to this tradeoff, recommendation algorithms generally enumerate visualizations based on one or more reference nodes, often the nodes that contain the user’s current selected attributes or visualization, or auto-generated reference nodes derived from simple heuristics. For example, Voyager [36, 37] uses the node that contains the user’s current visualization as a reference (denoted asn), otherwise Voyager generates univariate visualizations by default. Applying Constraints to Bound the Number of Candidate Nodes. Algorithm designers tend to keep the space of traversed visualizations quite small by imposing strict manual constraints on what parts of the space can be traversed. The most common constraints limit either the maximum path length that can be explored from some reference noden, or the maximum number of inputs contained within a candidate node. Using our design space notation from 3.1, we can represent all nodes with a maximum path length of 2 from nas: and nodes comprised of visualizations (v) with at most two inputs as: For example, Voyager [36, 37] only considers nodes that differ from the user’s current visualization by at most one attribute or data transformation, i.e., by setting the path length threshold to one for Eq. 4. This example is illustrated in Fig. 2. DeepEye [22] only outputs twoattribute visualizations, i.e., by setting|v| = 2in Eq. 5. DeepEye also limits data transformation choices to three types (aggregating, binning and sorting), and encoding choices to one of four basic visualization types (bar, pie, line, scatterplot), i.e., by setting |T | = 3 and |C| = 4. Navigating the Bounded Design Space to Enumerate Candidates. Once the constraints of the traversal are established, then algorithms must select a method for enumerating speciﬁc designs within this bounded space. Given one or more reference nodes, there are three basic approaches to the enumeration process: •arandom traversal, such as by listing random combinations of valid attributes, transformations, and/or encoding channels; •atree-oriented traversal, such as breadth-ﬁrst or depth-ﬁrst search along G, originating at n; •acluster-oriented traversal, where nodes are clustered by predeﬁned criteria, and clusters closest to nare prioritized. We see that these traversal strategies lead to varying degrees of depth and breadth in the coverage of the design space. For example, random and cluster-oriented traversals can cover a broader range ofG, but at the risk of having few nodes explored close to the user’s current visualizationn. In contrast, tree-oriented traversals will have dense coverage near n, but may have little or no coverage elsewhere in G. Note also that this traversal process need not take place all at once. For example, in the case of algorithms that rely on machine learning Fig. 3: A comparison of attribute enumeration methods for three exist-For instance, Show Me [25] uses a visual encoding recommendation ing recommendation algorithms. Each square is a node in the visualiza-algorithm. Foresight uses a query recommendation algorithm that is tion design space, where the current node (n) is colored black.similar to other query recommendation algorithms (e.g., VizDeck [17]). models, enumeration may happen both in the training phase (randomDeepEye uses a hybrid algorithm that is machine learning based [22].Other machine learning approaches are similar to DeepEye, differing traversal of training inputs) as well as in the prediction phase (cluster-primarily by model type or input data used for training [13, 26]. oriented traversal within the model structure). 3.1.4 Deﬁning the Ranking Steprecommends visual encodings based on user-speciﬁed attributes and Given the candidates generated by the enumeration step, the purpose ofdata transformations. By assuming the attributes and transformations the ranking step is to order these candidates in terms of how closely theyare ﬁxed, Show Me can enumerate and rank all relevant nodes that vary match a set of pre-deﬁned search criteria. In the case of visualizationonly by visual encodings (see Eq. 2). recommendation algorithms, the search criteria represent the quality and relevance of the candidate visualization. We use “oracle” to refer toan aggressively bounded search space in terms of attributes. As menthe part of the algorithm that assesses candidate quality and relevance.tioned in 3.1.3, Voyager uses the user’s current visualization to generate Oracle Inputs & Structure. Oracles often take as input the user’srelevant charts with a maximum path length constraint of one (see recent history of visualizations created and interactions performed, asFig. 3a). Constraining the attribute space allows Voyager to enumerate well as statistics about the current dataset. Using these inputs, oraclesmore encoding channels than other algorithms, as shown in Table 1. typically compute one or more scoring features and rank enumeratedThe Voyager oracle applies Mackinlay’s effectiveness rules [24]. candidates using a weighted function of these features, or a model. Feature weights for the model can be represented mathematically, suchuses Draco [26] as the implementation base. Dziban contains a hyas by assigning numerical weights to calculated heuristics to producebrid visualization recommendation algorithm that builds on the Grapha single score [21, 36, 37], as well as procedurally, such as throughScape [19] and Voyager [36, 37] oracles. Given a user’s prior query, ordered pruning rules to eliminate low-quality candidates [22]. ThereDziban can recommend new transformations and encoding channels, are three types of models that oracles often use to rank candidates:however, it does not recommend new attributes to visualize. Dziban behavioral models, statistical models, and machine learning models.prioritizes perceptually similar visualizations in its ranking step. Behavioral models. These models are generally represented as manual heuristics derived from user studies and/or ﬁeld observations.“insight” scores derived from user-selected statistical features or data For example, APT [24], Draco-APT [26], Show Me [25], and Voy-attributes. Foresight enumerates all possible pairings of data attributes, ager [36, 37] are based in part on manually-derived best practices,as well as all individual attributes, but restricts the ﬁnal visualizations to particularly for enhancing visual perception. In another example, theeither a bar chart, a box plot, or a scatterplot. Thus, Foresight performs BDVR algorithm [11] compares the user’s most recent interactions toa full attribute enumeration within a bounded search space. the four most common interaction patterns observed with the HARVEST system. The BDVR algorithm then ranks visualizations baseddifferent numbers of attributes, the paper focuses on enumerating vion whether they would be produced by the closest matching patterns.sualizations with two attributes and at most three data transformations Statistical models. These models often use a pre-deﬁned set of(see Table 1). DeepEye supports four visualization types: bar, pie, aggregate statistics to compare candidates [7, 17, 30]. For example,line, and scatter. Though the DeepEye authors describe their enumerForesight [7] analyzes the dataset to be visualized for statistical proper-ation method in terms of trees, when compared using our evaluation ties selected by the user, such as skew, outliers, and linear relationships,framework, DeepEye actually performs cluster-oriented enumeration. and scores candidate visualizations according to these features.The oracle ranks visualization candidates using both hand-written rules Machine Learning models. These models take large corpora offrom visualization experts, and a suite of binary classiﬁers trained usingvisualization preference data collected from user studies. Note that existing user data as input to an ofﬂine training phase [26]. Duringthe hand-written rules are used as heuristics to prune the search space, the training phase, these models generally cluster similar visualizationinterleaving the enumeration and ranking steps. designs, and develop hierarchical data structures to efﬁciently index into these clusters. Recent approaches use deep learning models toWe see wide variation in the depth and breadth of design space coveravoid the need for feature engineering prior to training [8, 13, 22].age in Fig. 3, and also in the enumeration constraints in Table 1. For Hybrid models. Hybrid oracles are also possible, where multi-example, Voyager provides broad attribute and transformation coverage ple models may be used. Oracles may also need to prune redundantnear the user’s current visualization, represented in black in Fig. 3a, candidates if they are too similar in quality and relevance [25,36, 37].but Voyager leaves much of the visualization design space unexplored. 3.2 Comparing Existing Algorithms Using the FrameworkHowever, Voyager enumerates more encoding channels compared to Using the three main components of our framework, we can evaluate aother algorithms, as shown in Table 1. Dziban does not enumerate wide range of visualization recommendation algorithms. We demon-attributes, limiting its search space to transformations and encoding strate the ﬂexibility of our framework by analyzing algorithms from ﬁvechannel variations only; in return, Dziban can also enumerate a larger existing works: Voyager [36,37], DeepEye [22], Foresight [7], Showrange of encoding channels. Show Me takes this restriction one step furMe [25] and Dziban [21]. We compare the high-level intuition behindther by only enumerating and ranking variations in encoding channels, the enumeration strategies in Fig. 3, and the enumeration constraints inenabling broad and deep coverage of the encoding space, but virtually Table 1. We selected these ﬁve algorithms because they cover all threeno coverage of the attribute and transformation space. types of recommendation algorithms proposed by Wongsuphasawat et al. [35], and their results can be generalized to many other systems.attribute combinations within its bounded search space, providing both Voyager [36, 37]. Voyager applies tree-oriented enumeration with Dziban [21]. Dziban is a visualization recommendation API that Foresight [7]. The Foresight system ranks visualizations based on Comparing Algorithms in Terms of Enumeration Trade-Offs. In comparison, we see in Fig. 3b that Foresight enumerates all broader and deeper coverage of attributes. However as a trade-off, we see in Table 1 that Foresight severely limits the space of encoding channels that may be enumerated. We see that DeepEye makes a similar tradeoff to Foresight. In Fig. 3c, we see that DeepEye’s cluster-oriented enumeration approach provides greater enumeration depth than both Voyager and Foresight, but it also lacks thorough coverage of attributes (and transformations) across the bounded search space. However, the cost of this increased attribute/transformation enumeration depth is reduced encoding channel coverage, as shown in Table 1. Comparing Algorithms in Terms of Ranking Trade-Offs. Three of the four algorithms we compare utilize behavioral ranking models (Voyager, Dziban, DeepEye). These behavior-based heuristics are fast to apply to visualization candidates, but can take signiﬁcant effort to derive on account of having to conduct user studies and/or ﬁeld studies beforehand to collect the data [27]. Even when the data is collected, signiﬁcant manual effort may also be required to hand-tune the resulting models [21,36, 37]. This issue of effort is also observed for machine-learning models, such as in the case of DeepEye, which required extensive data collection to train its machine-learning oracle. In the case of Voyager and Dziban, existing heuristics, algorithms, and user study data were used to develop the oracles, which can help reduce the burden of training and tuning new models. Foresight’s oracle requires no training since it relies on a pre-deﬁned set of statistics. However, Foresight must calculate these statistics for all enumerated attribute combinations, making its execution more expensive. Foresight uses statistical sketches to reduce the processing time. Once these algorithms are ﬁnally trained and tuned, a natural question is: which algorithm provides the best recommendations for a given visualization task? Though this question could be evaluated theoretically, existing approaches often use a somewhat reductive approach of approximating users’ analytic performance through low-level perceptual heuristics (e.g., [21, 22]). Perception is only one component of a user’s analytic performance and is a poor approximation of user performance in higher-level visual analysis tasks, such as prediction or exploration [3]. Instead, we argue for an empirical evaluation approach that is more task-sensitive. To compare the quality of generated recommendations, we provide a demonstration of using our framework to empirically evaluate different algorithms in the following sections. 4 BENCHMARKING RECOMMENDATION ALGORITHMS We show that our framework could compare a wide range of existing visualization recommendation algorithms theoretically in the previous section. Whereas in this section, we show how our framework could guide the empirical comparison of various recommendation algorithms. 4.1 Algorithms for Standardized Evaluation Existing recommendation algorithms either have no interface presented [13, 21, 26] or the systems built on the top of them utilize different interface designs [7, 14, 17, 22, 30, 36, 37], which makes it hard to conduct a standardized evaluation. Moreover, various systems allow different kinds of user input, which brings even more difﬁculties to the evaluation and comparison. For instance, the majority of systems allow selected data ﬁelds as input [7,14,17, 22, 36,37], while some also allow inputting statistical features [7], or visualization types [17]. Thus, to standardize the benchmark of different recommendation algorithms, we implement an interface to wrap around algorithms that are generated by applying the enumeration approach and oracle behind existing recommendations. In this paper, by varying the traversal method and the oracle, we come up with four new visualization recommendation algorithms to evaluate. The graph traversal method would be either BFS or DFS, and the oracles are CompassQL [35] and Dziban [21]. Both BFS and DFS are tree-oriented traversal methods. While BFS enumerates with a maximum path length of one, DFS enumerates along the path until the current node or the space boundary is reached. The CompassQL version that we use is the same as the one behind the Voyager [36,37] systems, which ranks visualizations by effectiveness. On the other hand, Dziban is built on the top of Draco [26] and GraphScape [19], which takes both effectiveness and perceptual distance into consideration while ranking visualizations. We evaluate CompassQL [35] and Dziban [21] based on the availability of their source code, whereas the code for many other algorithms is not publicly accessible [7, 13, 17, 22]. Moreover, by adding the ranking strategy of GraphScape [19] to optimize the perceptual distance, Dziban [21] claims to provide a considerable beneﬁt over Draco-CQL [26], which is a re-implementation version of CompassQL [35]. We benchmark these two ranking engines to see whether there exists a signiﬁcant improvement in user performance. The visualization design space is the same for all algorithms, where each node contains visualizations with the same data attributes, and each edge represents adding or removing one data attribute. We only consider visualizations with 3 data attributes or less, thus no attribute can be removed from a univariate chart, and no attribute can be added to a three-attribute chart. Oracles would need to make other data variation decisions, like whether to add data transformations (binning or aggregating), as well as design variation decisions, like applying which visual encoding for each attribute. 4.2 Interface Design Fig. 4 shows the interface for evaluating the set of new visualization recommendation algorithms, which consists of a top panel, a data panel (left), a visualization gallery (middle), and a task panel (right). Our interface design is inspired by the Voyager systems [36, 37]. Since we focus on evaluating the recommendation quality of each algorithm, limited interactions are allowed in the interface, such as selecting attributes, bookmarking or specifying a chart, and also hovering over a chart to check the value of a particular data point. We share the source code and a demonstration video of our interface in the OSF repository. The Top Panel (A). By clicking the button in the top panel, a bookmark gallery of visualizations saved by the user pops up. Participants are encouraged to bookmark charts that could answer the question during the user study. The Data Panel (B). It shows the name of the current dataset and presents a list of all data ﬁelds within the dataset. The list is grouped by the data type and then ordered by the variable name alphabetically. For each variable, it shows the data type icon, the variable name and then a checkbox representing whether the variable is included in the speciﬁed view. Users can click on the checkbox to include or exclude an attribute from the speciﬁed view (C). The related views (D) will provide different recommendations based on the current speciﬁcation. The Visualization Gallery (C & D). It consists of two views: the speciﬁed view (C) and the related views (D). Each chart contains a label on the top-left corner showing which data attributes are visualized in the chart, and a bookmark button () on the top-right corner, which triggers whether the bookmark gallery includes or excludes the chart. The speciﬁed view (C) is the best chart recommendation for the currently selected variables. The related views show recommended charts by the current recommendation algorithm based on the speciﬁed view. When no data ﬁeld is selected, the related views show univariate visualizations. By default, the related views display the top ﬁve recommended charts based on the speciﬁed view, if users want to explore more, they can click on the Load More button (Load More) to view additional recommendations. The list button (³) on each chart in the related views allows users to update the speciﬁed view and display new recommendations from this starting point (n). The Task Panel (E). It consists of (1) the current task description, (2) an input area for users to answer the question, (3) a checkbox for users to self-check if they bookmarked charts that could help answer the question, (4) a button to revisit the bookmark gallery, (5) the posttask questionnaire, and (6) a submit button to navigate to the next step. When participants click on the submit button, the answer of the task, the speciﬁcations of the bookmarked charts, the response of the post-task questionnaire, and also the interaction log will be sent to the server. 4.3 Study Design The study followed a 4 (recommendation algorithms)×2 (dataset) mixed design, thus in total there are 8 designs. We utilized a betweensubjects study design; each participant only conducted one analysis session, with a random combination of recommendation algorithm and dataset. All participants completed the study remotely. Fig. 4: Interface for the user study. Thetop panel (A)provides the button to view the bookmark gallery. The name and data ﬁelds. Users can manually select which ﬁelds to be visualized. The visualization gallery contains the related views (D). The speciﬁed view displays the current speciﬁed chart while related views show recommended charts relevant to the speciﬁed chart. The task panel (E) contains the current task and also the post task questionnaire. Visualization Tools. The interface (Fig. 4) was the same for everyfocused tasks; T1 involves two data attributes, while T2 involves three participant, but the recommendation algorithm was varied to generatedata attributes. T1 asks participants to ﬁnd the extremum, which is a different visualizations in the speciﬁed and related views.qualitative task, while T2 asks participants to retrieve a speciﬁc value Datasets. We utilized two Voyager [36] datasets for the evaluation:from a subset of the data, which is a quantitative task. Both T3 and T4 movies and birdstrikes. The moviesdataset contains 3,201 records andare exploratory tasks. T3 provides a particular direction for the data 15 attributes (7 nominal, 1 temporal, 8 quantitative). The birdstrikesexploration, while T4 asks participants to freely explore the dataset. dataset is a redacted version of FAA wildlife airplane strike records with 10,000 records and 14 attributes (9 nominal, 1 temporal, 4 quantitative).for each task, we collected participants’ (1) answers, (2) bookmarked Participants. We recruited nine subjects for each condition, for acharts, (3) interaction logs, and (4) responses of post-task questiontotal of 72 participants (23 female, 49 male), all of whom successfullynaires. We also obtained comments from the exit-study survey. completed the study and were included in our analysis. All participants claimed to have proﬁcient computer skills and prior experience using atsurements, analysis (using Bayesian regression models to test if there is least one of the following or similar tools/programming languages: Ex-a signiﬁcant difference in the stated measurements), and data collection cel, Tableau, Voyager, Python/matplotlib, R/ggplot, D3. We recruitedcriteria on the website AsPredicted participants from both academia and industry. Of the 72 participants, 40 were students while 32 were professional participants from the industry.5 BENCHMARK RESULTS We compensated participants with a $10 Amazon gift certiﬁcate.We obtained 72 valid study results which passed the exclusion criteria Study Protocol. Each participant completed a 60-minute onlinein our pre-registration. We also had a pilot study with ﬁve participants, session, consisting of: (1) study overview and consent form; (2) a de-where we derived the informative priors for our quantitative analysis. mographic survey; (3) 10-min tutorial and demo with a dataset distinct from those used for the actual analysis sections; (4) 40-minute analysisracy and completion time of focused-tasks, user interaction activities block with one study design; and (5) the exit-survey. During the studyduring open-ended tasks, post-task questionnaire responses, and qualisession, participants were asked to complete four analysis tasks, twotative feedback. For quantitative analyses, we adopted Bayesian models focused and two open-ended (see Table 2). After each task, participantsto estimate the 95% credible interval (CI) for each parameter. Since were asked to reﬂect on their experience using the recommendation toolthe data type of our collected data varies, we had to apply various to complete the task in a short post-task questionnaire with a symmetricBayesian regression models. We used the logistic regression for ana5-point scale, from strongly disagree (-2) to strongly agree (+2):lyzing the accuracy, the linear regression for the completion time and • Conﬁdence in Understanding Data:I am conﬁdent in under-interaction logs, and the ordinal regression for post-task questionnaire standing the dataset.responses. We chose Bayesian models because they allow us to draw • Conﬁdence in Answer:I am conﬁdent in my answer to the task.more reasonable conclusions about the true values of our parameters • Efﬁciency: The related views made it easier to explore the data.from small-n studies than the null hypothesis signiﬁcant testing (NHST). • Ease of Use: The related views were easy to understand.The Bayesian 95% credible interval represents the interval that we are • Utility: The related views were useful for completing the task.95% sure contains the true value, which is different from the NHST con• Overall: I would use this tool for similar tasks in the future.ﬁdence interval. On the other hand, in terms of estimating differences, After completing all four tasks, participants completed a survey tolike the differences between design A and B (i.e. A-B), if the Bayesian95% credible interval is greater than 0 and not overlapping with 0, it evaluate the recommender. The following questions were asked:means that we are 95% sure that design A performed better than B. We • What are the advantages and disadvantages of the tool?provide our experiment code, data collected for both experiments, and •Do you have any other comments on the recommendation system?analysis scripts as supplemental materials in the OSF repository. Tasks. We designed four visual analytics tasks (see Table 2) for each dataset based on prior studies of data analysis [2, 3, 36, 37]. These5.1 Focused Tasks four tasks cover all three analysis task classes discussed by BattleWe use accuracy and completion time as the two metrics to evaluate and et al. [2]: quantitative, qualitative, and exploratory. T1 and T2 arecompare the empirical utility of the four recommendation algorithms Collected Data. Since the user study was conducted remotely, Pre-registration. We pre-registered [6,20,34] the conditions, mea- We now present the analysis of study results, focusing on the accu- Fig. 5: The predicted accuracy of focused tasks for all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both Movies and Birdstrikes dataset. Fig. 6: The completion time of focused tasks for all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both the Movies and Birdstrikes dataset. 5.1.1 Accuracy To analyze task accuracy, we trained a Bayesian logistic regression model for the two focused tasks to model the probability of a correct answer given an oracle and graph traversal combination. It shows in Fig. 5 that CompassQL+DFS and Dziban+BFS had higher accuracy than Dziban+DFS, while CompassQL+BFS seemed to have the lowest accuracy rate. However, since all of the 95% CIs overlap, we cannot make a formal conclusion about which algorithm performed signiﬁcantly better in the accuracy of focused tasks. 5.1.2 Completion Time We derived a weakly informative prior on completion times in seconds from the pilot study: N(µ = 360.48, σ = 224.40). As shown in Fig. 6, all 95% CIs overlap with each other, thus we cannot conclude which recommendation algorithm had a signiﬁcant effect on the completion time of focused tasks. However, it is interesting to see that while participants spent the most time with CompassQL+DFS, the accuracy with CompassQL+DFS was the highest. This relationship could imply that the longer time that participants spent in the task led to a higher accuracy. Although not signiﬁcant, it generally takes less time for participants to complete tasks with the Movies dataset than the Birdstrikes one. On the other hand, the accuracy with the Movies dataset is also slightly higher. This ﬁnding is reasonable, since people are more familiar with Movies data than Birdstrikes data in real life. In summary, since all 95% CIs overlap in both the accuracy and the completion time analysis, we conclude for preciseness and decisiveness that the four new recommendation algorithms have no signiﬁcant impact on the performance of participants in focused tasks. 5.2 Open-ended Tasks To evaluate the utility of different algorithms for supporting open-ended tasks, we analyze the interaction logs from the user study. Since the user study was conducted remotely, we lack eye-tracking data to show which visualizations users were attending to. Taking inspiration from Voyager [36, 37], we analyze the number of unique variable sets shown on screen to assess which recommendation algorithm provides broader data exploration during the open-ended tasks. Moreover, we extend the analysis to the number of unique visual designs. Unlike the variable set which only considers the combination of data ﬁelds, the visual design takes data transformations and visual encodings into account. Since Fig. 7: The number of exposed variable sets and visual designs of open-ended tasks among all recommendation algorithms. We show posterior distributions, 50% and 95% CIs of expected titer thresholds. Fig. 8: The differences in average numbers of exposed variable sets and visual designs. We show posterior distributions, 50% and 95% CIs of expected titer thresholds for both Birdstrikes and Movies dataset. each edge in the visualization design space only represents the attribute modiﬁcation, and oracles need to make choices for data transformations and encoding channels, it would be interesting to see whether the oracle would provide different visualization designs from the same node while the reference node (n) is different. 5.2.1 Exposed Variable Sets & Visual Designs Fig. 7 shows that CompassQL+BFS, Dziban+BFS, and Dziban+DFS exposed more unique variable sets and visual designs than CompassQL+DFS in the open-ended tasks. On the other hand, we also see that Dziban exposed more numbers of visual designs than variable sets, so did BFS, which means Dziban and BFS recommended more design variants with the same variable sets, while CompassQL+DFS seemed to only recommend roughly one visual design for each variable set. We also ﬁnd that participants were exposed to slightly more unique variable sets and visual designs in the exploration task than in the prediction task, which is reasonable since the exploration task encourages participants to explore the dataset freely while the prediction task restrains a direction for the data exploration. To check the signiﬁcance, we also run a Bayesian linear regression model on the exposure difference between BFS and DFS, as well as the difference between Dziban and CompassQL, as shown in Fig. 8. From Fig. 8a we can see that there is a signiﬁcant difference in the average number of exposed variable sets between the two traversal methods, BFS and DFS. In the prediction task, BFS exposed signiﬁcantly more variable sets with both the Birdstrikes dataset (b = 11.746) and the Movies dataset (b = 14.163). We also ﬁnd a similar pattern of the exposure difference in the exploration task that BFS exposed signiﬁcantly more variable sets than DFS. However, since the 95% CIs overlap with the auxiliary line at 0, we cannot conclude that Dziban exposed signiﬁcantly more unique variable sets than CompassQL. On the other hand, in terms of the number of visual designs, we Fig. 9: The number of interacted variable sets and visual designs ofFig. 11: The differences in user preference rating. We show posterior open-ended tasks among all recommendation algorithms. We showdistributions, 50% and 95% CIs of expected titer thresholds. posterior distributions, 50% and 95% CIs of expected titer thresholds. Fig. 10: The differences in user conﬁdence rating. We show posteriorsigniﬁcantly better than DFS with respect to the ease of use. However, distributions, 50% and 95% CIs of expected titer thresholds.there is no evidence supporting that Dziban performed signiﬁcantly ﬁnd a signiﬁcant difference between both traversal methods and be-better than CompassQL in terms of the ease of use. tween both oracles (Fig. 8b). BFS exposed signiﬁcantly more visualcannot conclude which traversal method or oracle is signiﬁcantly better designs with both the Birdstrikes dataset (b = 13.918) and the Moviesthan the other one, although we can see that BFS and Dziban have dataset (b = 16.822) in the prediction task, while in the explorationslightly higher utility ratings than DFS and CompassQL respectively. task, BFS exposed (b = 16.813) more with the Birdstrikes dataset and (b = 20.316) more visualizations with the Movies dataset. A similarthe line at 0, users did not signiﬁcantly prefer one traversal method or pattern of the exposure difference is also found between the two oracles.oracle over the other one. Dziban exposed signiﬁcantly more visual designs than CompassQL with both the Movies and the Birdstrikes dataset in both tasks. It iserence between the two traversal methods. In particular, participants interesting to see that although Dziban did not expose signiﬁcantlysigniﬁcantly preferred BFS in the efﬁciency and ease of use experimore unique variable sets, it exposed signiﬁcantly more unique visualence. Although not signiﬁcantly, we still can see that Dziban received designs than CompassQL, which means Dziban tends to recommenda slightly higher rating in each experience compared to CompassQL. more design variants than data variants (as shown in Fig. 7). 5.2.2 Interacted Variable Sets & Visual Designs5.4 Participant Feedback We also analyze the number of unique variable sets and visual designsfor focused tasks since the recommended charts could have more ﬁelds that participants interacted with during the open-ended tasks. Weadded compared to the current speciﬁed chart. One participant found include interactions like specifying (³), bookmarking (), and mouse-that “The recommendations were useful but most of the time distrachovering for more than half a second. From Fig. 9 we do not seetive and too many for answering speciﬁc questions.” Another said much difference in the number of interacted variable sets and visualthat “When I checked one attribute, the recommendation charts always designs among different recommendation algorithms. It seems thatinclude three attributes. I would prefer if it was only two factors for participants interacted with more visual designs than variable sets withthe ﬁrst two [focused] tasks.” However, when it comes to open-ended BFS, which means that BFS provides more interesting design variantstasks, participants had a different point of view in terms of the DFS that participants would like to interact with. On the other hand, thetraversal method; one participant mentioned that “This tool is good for number of unique variable sets and visual designs are about the sameexploring the data, especially it is the ﬁrst time seeing (the data).” with DFS. In other words, DFS did not expose as many interesting design variants as BFS (Fig. 7).tual distance from the current chart into account, the behavior makes 5.3 Post-task Questionnairesmore sense to participants when they explore the related views. One We used the Bayesian ordinal regression model to analyze the userparticipant commented on CompassQL that “(I am) unsure if there is responses from the post-task questionnaires. Since we used a symmetricany logic on the recommended charts, sometimes they are completely 5-point scale (-2 strongly disagree, +2 strongly agree) in the post-taskuseless and just layer on another random metric or dimension”. Anquestionnaire, our prior on user score of (range[−2, 2]) is expressed asother participant also pointed out that “The recommendations (from a normal distribution N(0, 1).CompassQL) were often ineffective and created out of unrelated ﬁelds”. 5.3.1 Conﬁdence RatingWhen it came to the recommendations from Dziban, participants pro-vided more positive feedback. One said that “The tool helps explore In the post-task questionnaire, we asked participants to rate their conﬁ-datasets and provides useful recommendations in terms of related meadence in understanding data, and also in their answers.sures and dimensions to enable getting useful insights.” Another parConﬁdence in Understanding data. As shown in Fig. 10, BFSticipant also commented that “For the most part, the tool added ﬁelds performed slightly better than DFS on users’ conﬁdence in understand-that made sense to include in addition to the original choices.” ing data. Dziban also had a higher conﬁdence rating than CompassQL. Conﬁdence in Answer. On the other hand, BFS performedfeedback about the recommended charts, however, we also found some slightly worse than DFS on users’ conﬁdence in their answers. However,comments about the disadvantages of both oracles. The most common the Dziban oracle still had a slightly higher rating than CompassQL.issue for CompassQL is it recommends scatterplots a lot since it only In summary, we don’t see much difference in users’ conﬁdenceemphasizes effectiveness and when it comes to three attributes, it picks ratings between the two traversal methods, BFS and DFS. On thearea or color as the third encodings, which sometimes confuses parother hand, Fig. 10 shows that the Dziban oracle performed better thanticipants. One commented that “I didn’t understand the shaded circle. CompassQL, however, the outperformance was not signiﬁcant.I guess it could be there are various different values that are big and 5.3.2 Recommendation Algorithm Preferencesmall.” In terms of color encoding, one participant commented that We also asked participants to rate the related views in different aspects:“(It seems to be) often picking categories to represent color where there efﬁciency, ease of use, utility, and overall (Fig. 11).were so many colors as to make them all meaningless”, and another Efﬁciency. In terms of the efﬁciency rating, BFS received a sig- Utility. Both 95% CIs overlap with the auxiliary line at 0, thus we Overall. Similar to the utility, since both 95% CIs overlap with In summary, we see some signiﬁcant differences in the user pref- DFS is not preferred for focused tasks. Participants dislike DFS Dziban is preferred as an oracle. Since Dziban takes the percep- Both oracles need to be improved. Overall, we got positive pointed out that “Colors did not seem related to essential data.” On the other hand, Dziban also considers the perceptual distance thus it tends to recommend charts that look similar to the original one but does not consider the effectiveness enough, like using text as a mark type in a scatterplot. A participant commented on Dziban that “Don’t recommend views where a text value would dominate the visualization.” One way to address these problems would be including more hand-tuned constraints, such as not using color to visualize more than a certain number of categories, not using area encoding when the overlapping exists, and not using text as a mark type to visualize long content. 6 DISCUSSION & FUTURE WORK In this paper, we presented an evaluation-focused framework that can describe many existing visualization recommendation algorithms, and showed how our framework could guide the theoretical and empirical comparison of such algorithms. We conclude this paper with a discussion of guidelines for new recommendation algorithms, key benchmarking takeaways, limitations, and opportunities for future work. 6.1 The Framework As Guidelines We now discuss how our framework could serve as a guideline not only for the future construction of recommendation algorithms but also for benchmarking a larger range of existing recommendation systems. 6.1.1 For Future Recommendation Algorithms Our framework consists of three major components: (1) the visualization design space, (2) the traversal method, and (3) the oracle. While constructing new recommendation algorithms, one should think about whether any of the components in the algorithm is new to the community. For example, does my visualization design space contain more (meaningful) visual designs than other existing automated systems? Is my algorithm using a new way to traverse the visualization space which could help the actual analysis? Is there a new creative ranking strategy that has not been covered by the existing literature? On the other hand, thinking about different combinations of the three components is another creative opportunity for constructing new recommendation algorithms. Among the new algorithms evaluated in this work, CompassQL+DFS, Dziban+BFS and Dziban+DFS have not previously been proposed to the community, although the ranking engines (CompassQL and Dziban) have been researched as key contributions in this space. 6.1.2 For Benchmarking Various Automated Systems Although we did not benchmark existing automated systems since they leverage different interfaces and have limited code availability, our user study design still provides an at-a-glance overview of how our framework could be used to guide the evaluation and comparison of various automated systems. Without a standardized interface design and style of user inputs, it is difﬁcult to compare multiple recommendation algorithms. By leveraging our framework, one could compartmentalize the three main components of the algorithm and test them within our standardized interface. For instance, one of our proposed algorithms, CompassQL+BFS utilizes the same idea of the graph traversal method and the ranking engine behind the Voyager systems. In such a way, our framework could not only evaluate the recommendation algorithm as a whole but also compare different components. As shown in the previous section (Sect. 5), our results not only show which algorithm performed better but also which traversal method or oracle was preferred. 6.2 Takeaways from Benchmarking From Sect. 5, we can see that there is actually no signiﬁcant difference between recommendation algorithms in the participants’ performance with focused tasks. On the other hand, for open-ended tasks, we ﬁnd that BFS exposed signiﬁcantly more unique variable sets and visual designs than DFS, while Dziban exposed signiﬁcantly more unique visual designs than CompassQL, but not variable sets. However, when it comes to interacted variable sets and visual designs, we do not see any signiﬁcant difference between BFS and DFS traversal methods and between Dziban and CompassQL oracles. This ﬁnding raises an important point: signiﬁcantly more exposure does not necessarily lead to signiﬁcantly more interactions. When designing a new visualization recommendation algorithm, exposing more data variants and design variants is a good trend. However, if more exposure does not lead to more interactions, the resulting recommendations may lack the right level of “interestingness” for a worthwhile data exploration experience. On the other hand, in terms of participants’ preferences, we do ﬁnd that participants signiﬁcantly prefer BFS over DFS in the utility and ease of use ratings. Participants also prefer Dziban rather than CompassQL in all metrics (efﬁciency, utility, ease of use, and overall), although the rating difference is not signiﬁcant. Participants’ post-study feedback also reveals their preference for Dziban as an oracle. Since Dziban takes the perceptual distance into account, participants could better understand why such visualizations are recommended. As we mentioned before, Dziban is an improved version of DracoCQL (a re-implementation of CompassQL), which takes the perceptual distance into account. However, we do not ﬁnd a signiﬁcant difference in the user performance between Dziban and CompassQL in focusedtasks, and the only signiﬁcant improvement in open-ended tasks is that more visual designs are exposed (but not necessarily interacted with) While the Dziban paper did present a comparison with Draco-CQL and GraphScape, it did not consider the user performance. Based on their benchmark results, they claimed that Dziban provides a considerable beneﬁt by suggesting charts that are effective, but also perceptually closed to the current one. Nevertheless, without a framework to evaluate and compare the user performance between algorithms, we do not know whether the beneﬁt would carry over into the actual analysis process. From another perspective, we also ﬁnd that users’ preferences change with different analysis tasks, which implies that it is hard for a single algorithm to perform well across all tasks. When designing a new recommendation algorithm, one should think about which type of task to prioritize based on the expected goals of the intended users. Alternatively, the recommendation system could switch to different algorithms depending on the particular task that users want to accomplish. 6.3 Limitations & Future Work Given the necessary level of visualization and analysis expertise for our participants, our recruitment protocol could not leverage standard crowdsourcing platforms, which limited the number of participants that we could feasibly recruit. As a result, we limited our evaluation to two traversal methods and two existing ranking engines, CompassQL and Dziban. However, it would be exciting to involve other promising traversal methods and oracles in future evaluations. Given the current COVID-19 restrictions, the entire study was conducted remotely, which made it difﬁcult to fairly perform a longer study session (like the 2-hour session in Voyager’s evaluation [36, 37]). Therefore, we took a step back and chose the between-subjects study design, where each participant was exposed to only one recommendation algorithm. However, the result would be more accurate and comparable if we could have conducted a within-subjects study. As the study session length is limited, we could only pick a small number of visual analytics tasks to evaluate the user performance, while there exists a larger group of analysis activities in real-life practice [1, 16]. Moreover, our benchmark results imply that analysts prefer different algorithms for different analysis tasks. Thus, one of the promising future work directions would be to include more analysis tasks into the benchmarking to better understand how different algorithms affect the user performance in various analysis tasks. In this work, we focused on researching how different recommendation algorithms would affect the performance, behavior, and preference of participants, thus we only included limited interactions in our interface design. However, from the post-study interviews, we ﬁnd that participants would like to see the interface include more robust functionality, like ﬁltering or supporting user-speciﬁed aggregations. It would be interesting to see how the participant performance, particularly when interacting with charts, changes with those extra features, and whether such features would signiﬁcantly affect the overall study results. Since the source code for our empirical evaluation is publicly accessible, it would be easier to accomplish the aforesaid incremental evaluations. ACKNOWLEDGMENTS The authors wish to thank the HCIL, the BAD Lab, and our paper reviewers for their thoughtful feedback. This work was supported in part by NSF award IIS-1850115 and an Adobe Research Award.