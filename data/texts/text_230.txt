Within decentralized organizations, the local demand for recommender systems to support business processes grows. The diversity in data sources and infrastructure challenges central engineering teams. Achieving a high delivery velocity without technical debt requires a scalable approach in the development and operations of recommender systems. At the HEINEKEN Company, we execute a machine learning operations method with ve best practices: pipeline automation, data availability, exchangeable artifacts, observability, and policy-based security. Creating a culture of self-service, automation, and collaboration to scale recommender systems for decentralization. We demonstrate a practical use case of a self-service ML workspace deployment and a recommender system, that scale faster to subsidiaries and with less technical debt. This enables HEINEKEN to globally support applications that generate insights with local business impact. CCS Concepts: methods; • Computer systems organization → Cloud computing. Additional Key Words and Phrases: automation, collaboration, software engineering, mlops, self-service ACM Reference Format: Maurits van der Goes. 2021. Scaling Enterprise Recommender Systems for Decentralization. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 5 pages. https: //doi.org/10.1145/3460231.3474616 1 INTRODUCTION The interest in data analytics applications in enterprises grows as studies show it leads to an increase in productivity [5]. Within enterprises, numerous large datasets are available [ them for new insights. Like a recommender system that advises sales employees for their outlet visit schedule by modeling historic data. The HEINEKEN Company is the most global brewer with breweries in more than 70 countries. With global support, these local subsidiaries build and exchange successful recommender systems. These systems with domestic impact quickly gather interest from other subsidiary brewers. However, its international and decentralized structure also comes with diversity in data sources and information technology. Meeting the expected demand from subsidiaries for recommender systems requires a scalable approach in development and operations that decentralizes the ownership of the system. Growing the data engineering team is expensive and demanding with the shortage of data talent [ studies show that increasing team size does not linearly increase the software eorts and quality [ scaling is vital to achieve eciency in the design, development, and operations of recommender systems. Immature infrastructure in production causes long-term costs expressed as technical debt [ organizations to a stand-still. To stimulate delivery velocity and prevent technical debt while scaling the recommender system, this eciency requires to be included from the initial design. This talk discusses a software engineering method for scalable recommender systems, two applications, and its learnings. • Information systems → Recommender systems;• Software and its engineering → Software development 2 APPROACH Compared to software engineering technical debt is a larger challenge for recommender systems as code is accompanied by datasets and machine learning models. We draw from experience in addressing this challenge with a Machine Learning Operations (MLOps) method. MLOps is the combination of machine learning and development operations (DevOps). The goal of the MLOps practices is to bring machine learning applications into production with attention to reproducibility, reliability, and eciency [4]. There are two main variants of this MLOps life cycle with either three (excluding data management) or four stages. We choose the four-stage life cycle (g. 1) since proper data management is essential for successful recommender systems [3]. The MLOps method is executed ve best practices: • Pipeline automation:Removing manual actions and delays by automating actions and connecting services in pipelines. For example by releasing solutions with a continuous integration and continuous deployment pipeline. • Data availability:Access to validated datasets via a feature store and indexed in a data catalog to ensure reproducible machine learning. • Exchangeable artifacts:All machine learning models, code, and congurations are version controlled, descriptive, and PEP8 consistent. Solution patterns with documentation are available for common architecture artifacts, like integrating external systems. Code scripts are preferred over notebooks in production. Notebooks have upsides for experimenting, but these do not exceed the advantages of regular scripts. • Observability:Deep understanding of the system components to ensure performance and identify root causes of issues. Metrics, events, logs, and traces are collected and accessible to the product team, not only engineers. • Policy-based security:Releases are separated into four environments: development, testing, acceptance, and production. Authorization to these environments is managed with Attribute Based Access Control (ABAC) with control plane and data plane functions. Secrets are stored in environment-specic key vaults. This MLOps method and practices are a shared responsibility of both data scientists and data engineers. Committing to the same method enables the “You build it, you run it”-principle by Werner Vogels [6]. Advantageous for delivery velocity and quality of the applications. 3 APPLICATIONS We selected two cloud native applications on the Azure platform to present: a self-service ML workspace deployment and a recommender system. These are intertwined, as the ML workspace is the engine of the recommender system. Together these applications are designed globally to support local analytics demand in a scalable form with distributed ownership. 3.1 Self-service ML workspace deployment An Azure ML workspace is a cloud workbench for machine learning development and deployment. It empowers the data scientist with a self-service environment that contains various MLOps features and abstracts away technical complexities like containerization. Requesting and delivering a new ML workspace was a manual and time-consuming process. With straightforward technology, this is eectively automated (g. 2). The request submitted via ServiceNow is sent to the HTTP endpoint of the Azure Logic App. ServiceNow is familiar software for the end-users. In Logic App the conguration details are parsed and sent via a REST API to an Azure DevOps Release pipeline. Using an Azure Resource Manager (ARM) template with the conguration the ML workspace is deployed in the designated Azure environment within minutes. 3.2 Recommender system The recommender system has a decoupled architecture, consisting of various solution patterns (g. 3). Data is extracted from upstream services (e.g. SAP or Salesforce) and stored in the Azure data lake. The ML workspace provides functionality for both model research, running pipelines as recommender engine, and tracking performance. The Azure Data Factory pipeline rst triggers the ML pipeline and subsequently pushes the data to a downstream service (dashboard, external system API, or internal API). All the steps of this recommendation process are scheduled or triggered on data events. With localization in mind, the congurations are stored in separate yaml les. All resources templates and congurations are stored and deployed as infrastructure as code from individual Azure DevOps repositories. Fig. 2. The architecture of the pipeline to automatically deploy an Azure ML workspace. 4 LEARNINGS Analyzing the practical case with the implementation of MLOps generates learnings. The engineering bottlenecks for scaling recommender systems are diversity in data sources and information technology with subsidiaries. This challenge is successfully diminished by engineering a culture that fosters self-service, automation, and collaboration. The ML workspace deployment application is an eective example of self-service and automation. This deployment capability is expanded from a few engineers to all data colleagues by the implementation of a pipeline of connected services. As a result, valuable engineering time is freed up and the delivery time of an ML workspace to its end-users is reduced from weeks to minutes. Conveniently onboarding data scientists to the ML workspace with central knowledge resources further contributes to a higher velocity. The presented recommender system is designed with internal solution patterns, automated deployment of resources, centrally available datasets, and accessible observability. This decreases the engineering time per subsidiary use case while keeping the agility to eectively incorporate local specications. By dealing with the diversity in data sources and information technology early and with solution patterns, it is possible to reuse many recommender system artifacts across subsidiaries. Substantially reducing the collective technical debt of the organization. From our experience, resulting in scaling recommender systems faster to multiple countries. The success of a local recommender system is determined by multiple KPIs, including revenue, savings, usage, and user valuation. The attention for standardization, exchangeable artifacts, and available datasets leads to more collaboration between colleagues across teams or country borders. By promoting a culture of self-service, automation, and collaboration, the HEINEKEN Company is better qualied to support its subsidiaries with recommender systems or other analytics requests. 5 CONCLUSION Meeting the local demand of recommender systems in enterprises requires ecient engineering with minimal technical debt. This talk described an MLOps method and two corresponding applications. We learned that a culture with self-service, automation, and collaboration is key to enable an enterprise to scale recommender systems with shared ownership. We believe that these engineering principles are valuable to other enterprises with similar data-driven ambitions and challenges. Additionally, we think that these are not bounded to recommender systems, but applicable for scaling various types of machine learning applications globally. ACKNOWLEDGMENTS This talk stands on the shoulders of talented people (at The HEINEKEN Company). Their ideas, code, and feedback shaped our thoughts and contributed to this work. 6 SPEAKER BIO Maurits van der Goesis currently a Data Engineer in the Global Analytics team at The HEINEKEN Company, where he specializes in their MLOps infrastructure for e.g. information ltering pipelines in sales, marketing, logistics, and nance. Earlier at RTL Netherlands, he developed the rst news recommender system for major Dutch website RTL Nieuws. Maurits graduated from Delft University of Technology with a Masters in Complex IT Systems Engineering and Management. His graduation research was on a team-formation recommender system that he developed at a digital platform for self-managing teams.