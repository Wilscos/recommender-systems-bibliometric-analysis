Learning the embeddings of knowledge graphs is vital in artiﬁcial intelligence, and can beneﬁt various downstream applications, such as recommendation and question answering. In recent years, many research efforts have been proposed for knowledge graph embedding. However, most previous knowledge graph embedding methods ignore the semantic similarity between the related entities and entity-relation couples in different triples since they separately optimize each triple with the scoring function. To address this problem, we propose a simple yet efﬁcient contrastive learning framework for knowledge graph embeddings, which can shorten the semantic distance of the related entities and entity-relation couples in different triples and thus improve the expressiveness of knowledge graph embeddings. We evaluate our proposed method on three standard knowledge graph benchmarks. It is noteworthy that our method can yield some new state-ofthe-art results, achieving 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset. The knowledge graph (KG) stores a vast number of human knowledge in the format of triples. A triple (h, r, t) in a knowledge graph contains a head entity h, a tail entity t, and a relation r between h and t. The knowledge graph embedding (KGE) aims to project the massive interconnected entities and relations in a knowledge graph into vectors or matrices, which can preserve the semantic information of the triples. Learning the embeddings of knowledge graph can beneﬁt various downstream artiﬁcial intelligence applications, such as question answering (Huang et al. 2019), machine reading comprehension (Yang and Mitchell 2017), image classiﬁcation (Marino, Salakhutdinov, and Gupta 2016), and personalized recommendation (Wang et al. 2018). In general, most of the KGE methods would deﬁne a scoring function f(h, r, t), and the training target of knowledge graph embeddings is maximizing the score of a true triple (h, r, t) and minimizing the score of a false tripe (h, r, t). In this way, the trained embeddings of entities and relations in KG can preserve the intrinsic semantics of a true triple. We can mainly divide the existing KGE * The ﬁrst two authors contributed equally to this work. methods into two categories. The ﬁrst category is the distance based (DB) methods, which use the Minkowski distance as scoring function to measure a triple’s plausibility, include the TransE (Bordes et al. 2013), TransH (Wang et al. 2014), TransR (Lin et al. 2015), TransD (Ji et al. 2015) and TransG (Xiao, Huang, and Zhu 2016). The other category is the tensor decomposition based (TDB) methods, which treat a knowledge graph as a third-order binary tensor and use the results of tensor decomposition as the representations of entities and relations. The TDB methods include the CP (Hitchcock 1927), DistMult (Yang et al. 2015), RESCAL (Nickel, Tresp, and Kriegel 2011) and ComplEx (Trouillon et al. 2016). However, the previous methods overlook the connections between the related entities and entity-relation couples in different triples. They separately feed each triple into the scoring function and optimize the scoring function’ output to learn the entities’ and relations’ embeddings. In a knowledge graph, some entities (entity-relation couples) that share the same entity-relation couple (entity) have similar semantics. Capturing the semantic similarity of these entities or couples can improve the expressiveness of embeddings, which indicates the performance in capturing semantic information (Dettmers et al. 2018; Xu et al. 2020). For instance, in Figure 1 (a), the entities New York and Los Angeles share the same entity-relation couple (Cityof, United States). Therefore, the entities New York and Los Angeles should have a similar embedding. Besides, in Figure 1 (c) the couples (New York, City of ) and (Washington, D.C., Captical of ) share the same tail entity United States, the representations of these two couples should also be similar. To correlate the related entities and entity-relation couples in different triples, we propose a simple yet efﬁcient contrastive learning framework called KGE-CL for knowledge graph embeddings, which is quite general for existing KGE methods. We ﬁrst construct the positive instances for those entities that share the same entity-relation couple and those entity-relation couples that share the same entity. For example, the positive instance of Beau Biden in Figure 1 (b) is the Hunter Biden, and the positive instance of (Children, Beau Biden) in Figure 1 (d) is (Spouse, Jill Biden). Then we calculate the contrastive loss on the original instance and the positive instances. Due to the design of the contrastive learning framework, we can also increase the distance between (a) Entities with the same (City_of, United States) couple. unrelated entities and couples. Finally, since each triple has four positive instances (corresponding to the four examples in Figure 1), we design a weighted contrastive loss to control the weights on different positive instances’ loss ﬂexibly. We evaluate our KGE-CL method on the KG link prediction task using the standard WN18RR (Toutanova and Chen 2015), FB15k-237 (Dettmers et al. 2018) and YAGO310 (Mahdisoltani, Biega, and Suchanek 2015) datasets. Our proposed method achieves new state-of-the-art results (SotA), obtaining 51.2% MRR, 46.8% Hits@1 on the WN18RR dataset, and 59.1% MRR, 51.8% Hits@1 on the YAGO3-10 dataset. Moreover, We further analyze the improvement by comparing our method with the baseline on the triples with different relations. At last, to clearly explain why our method outperforms existing methods, we conduct the visualization of the knowledge graph embeddings of our method and some compared methods using T-SNE (van der Maaten and Hinton 2008). In summary, this paper’s contributions include: • We propose KGE-CL, a simple yet efﬁcient contrastive learning framework for knowledge graph embeddings. It can capture the semantic similarity of the related entities and entity-relation couples in different triples, thus improving the expressiveness of embeddings. To our best knowledge, the KGE-CL is the ﬁrst contrastive learning framework of knowledge graph embeddings. • Our proposed KGE-CL framework can also push the embeddings of unrelated entities and couples apart in the semantic space. • The experiment results and analyses conﬁrm the effectiveness of our KGE-CL method. In recent years, knowledge graph embedding (KGE) becomes a pretty hot research topic since its vital role in var- (b) Entities with the same (Joe Biden, Children) couple. ious downstream applications. We can categorize the existing KGE techniques into two categories: the distance based KGE methods and tensor decomposition based KGE. Distance based (DB) methods describe relations as relational maps between head and tail entities. They usually use the Minkowski distance to score a given triplet. The TransE is a representative distance based method, which uses the relations as translations and its scoring function is: f(h, r, t) = −||h+ r− t||. To improve the performance of TransE, many its variants that follow the same direction were proposed, such as the TransH (Wang et al. 2014), TransR (Lin et al. 2015), TransD (Ji et al. 2015), TranSparse (Ji et al. 2016), TransG (Xiao, Huang, and Zhu 2016). The structured embedding (SE) (Bordes et al. 2011) utilizes the relations as linear maps on the head and tail entities, and its scoring function is: f(h, r, t) = −kRh− Rtk. The RotatE (Sun et al. 2019) uses each relation as a rotation in a complex vector space and its scoring function is calculated by: f(h, r, t) = −kh◦ r− tk, where h, r, t∈ Cand |[r]| = 1. Tensor decomposition based (TDB) KGE methods formulate the KGE task as a third-order binary tensor decomposition problem. RESCAL (Nickel, Tresp, and Kriegel 2011) factorizes the j-th frontal slice of X as X≈ ARA, in which embeddings of head and tail entities are from the same space. As the relation embeddings in RESCAL are matrices containing lots of parameters, RESCAL is easier to be overﬁtting and more difﬁcult to train. DistMult (Yang et al. 2015) simpliﬁes the matrix Rin RESCAL to a diagonal matrix, while the RESCAL can only preserve the symmetry of relations, limiting its expressiveness. To model asymmetric relations, ComplEx (Trouillon et al. 2016) extends DistMult to complex embeddings and preserving the relations’ symmetry in the real part and the asymmetry in the imaginary part. Moreover, the QuatE (Zhang et al. 2019) further extends the ComplEx to hypercomplex space to model more complicated relation properties, such as the inversion. All of the DistMult, ComplEx, and QuatE are the variants of CP decomposition (Hitchcock 1927), which are in real, complex, and hypercomplex vector spaces, respectively. On the other hand, the TDB methods usually suffer from an overﬁtting problem; thus, some work is trying to improve the TDB methods from the aspect of regularizer, such as the N3 (Lacroix, Usunier, and Obozinski 2018) and DURA (Zhang, Cai, and Wang 2020) regularizers. These regularizers bring more signiﬁcant improvements than the original squared Frobenius norm (Lnorm) regularizer (Nickel, Tresp, and Kriegel 2011; Yang et al. 2015; Trouillon et al. 2016). However, both of the existing distance based and tensor decomposition based methods separately optimize each triple with the scoring function, overlooking the relationships between the entities or entity-relation couples in different triples. Therefore, they can not correctly capture the semantic similarity between these related entities and couples in different triples. To address the limitations of existing work, we propose our KGE-CL method for learning the knowledge graph embeddings. In this section, we will introduce the background of knowledge graph embedding in Setcion 3.1, and the background of contrastive learning in Section 3.2. 3.1 Knowledge Graph Embedding Knowledge Graph Given a entities set E and a relations set R, a knowledge graph K = {(h, r, t)} is a set of triples, where h∈ E is the head entity, r∈ R is the relation, and t∈ E is the tail entity. Knowledge Graph Embedding (KGE) The knowledge graph embedding (KGE) is to learn the representations (may be real or complex vectors, matrices, and tensors) of the entities and relations. Its target is that the learned entities’ and relations’ embeddings can preserve the semantic information of the triples in knowledge graphs. Generally, the KGE methods deﬁne a scoring functionf (h, r, t) to score the corresponding triple (h, r, t), and the score measure the plausibility of triples. Exiting KGE work contains two important categories: distance based methods and tensor decomposition based methods. Distance Based (DB) KGE Methods DB methods deﬁne the scoring function f(h, r, t) with the Minkowski distance (Bordes et al. 2013; Wang et al. 2014; Lin et al. 2015; Ji et al. 2015; Xiao, Huang, and Zhu 2016), and the scoring functions of this kind of methods is: f(h, r, t) = −kΓ(h, r, t)k, where Γ is a model-speciﬁc function. Tensor Decomposition Based (TDB) KGE Methods TDB methods like RESCAL (Nickel, Tresp, and Kriegel 2011) and ComplEx (Trouillon et al. 2016), regard a knowledge graph as a third-order binary tensor X ∈ {0, 1}. The (i, j, k) entry X= 1 if (h, r, t) is a true triple otherwise X= 0. The Xdenotes the j-th frontal slice of X , that is, the corresponding matrix of the j-th relation. Generally, a TDB KGE model factorizes Xas X≈ Re (HRT), where the i-th (k-th) row of H (T) is h(t), Ris a matrix that represents relation r, Re (·) and · are the real part and the conjugate of a complex matrix, respectively. Then the scoring functions of TDB KGE methods is: f(h, r, t) = Re (hRt). Note that the real part and the conjugate of a real matrix are itself. The goal of TDB models is to seek matrices H, R, . . . , R, T, such that Re (HRT) can approximate X. In this paper, we aim to improve the performance of existing TDB models, such as the RESCAL and ComplEx models. Contrastive learning is an efﬁcient representation learning method that contrasts positive pairs against negative pairs (Hadsell, Chopra, and LeCun 2006; He et al. 2020; Chen et al. 2020; Khosla et al. 2020). The key idea of contrastive learning is pulling the semantically close pairs together and push apart the negative pairs. The unsupervised contrastive learning framework (Chen et al. 2020) would utilize the data augmentation to construct positive pairs to calculate the contrastive loss. The supervised contrastive learning framework (Khosla et al. 2020) calculates the contrastive loss of all positive instances within the same mini-batch. Motivated by these exiting frameworks, we adopt the following function to calculate the contrastive loss between an instance zand its all positive instances z: where zand zare the representations of zand z, respectively. P (i) is the set of all positive instances in the minibatch, and N(i) is the set of all negative instances in the batch. We deﬁne the negative instances as the instances that do not belong to the positive instances. sim(z, z) = z·z is the dot product similarity. In this section, we describe our KGE-CL method that utilize the contrastive learning to capture the semantic similarity of related entities and couples in different triples. Our method is very general and can easily apply to arbitrary tensor decomposition based methods. We can further name our KGECL method as RESCAL-CL or ComplEx-CL when we use the scoring function of RESCAL or ComplEx models. In this section, we ﬁrstly present the contrastive loss we designed for the knowledge graph embeddings, then we introduce the training objective of our method. In this subsection, we elaborate on the contrastive loss that we designed for knowledge graph embeddings. Positive Instances The generation of positive instances zfor the instance zis vital in contrastive learning. Existing work in visual representation learning (Wu et al. 2018; Chen et al. 2020; Chen and He 2020) used some data augmentation methods, such as cropping, color distortion, and rotation, to take two random transformations of the same images as zand z. Meanwhile, in NLP, some work (Wu et al. 2020; Meng et al. 2021) utilized other augmentation techniques like word deletion, reordering, and substitution. However, these data augmentation methods are not proper to the knowledge graph embeddings. To capture the interactions between triples in a knowledge graph, we design a new approach to construct the positive instances for knowledge graph embeddings. For a triple (h, r, t), the corresponding scoring function of TDB methods is: We deﬁne h·, ·i as the inner product of two real or complex vectors: hu, vi = uv. The hRand Rtare the representations of the entity-relation couples (h, r) and (r, t), respectively. The Equation 2 means that we can ﬁrstly compute either the hRor Rtin the scoring function. For a head entity h, we deﬁne its positive instances has those head entities that share the same relation and tail entity with h. Similarly, we deﬁne the tail entity t’s positive instances twith those tail entities that share the same head entity and relation with t. For the entity-relation couples (h, r) or (r, t), the corresponding positive instances (h, r) or (r, t)is those entity-relation couples that share the same tail entity with (h, r) or head entity with (r, t). The (hR)and (Rt)are the representations of positive instances (h, r)and (r, t), respectively. Therefore, given a true triple (h, r, t), our method would construct four kinds of positive instances: h, t, (h, r)and (r, t), which are corresponding to the four examples in Figure 1. For an instance h, we will use all of its positive instances hin the same mini-batch. Base Encoder Motivated by the existing contrastive learning frameworks, we utilize a base encoder g(·) to encode the original and positive instances in the four types of positive pairs: (h, h), (t, t), ((h, r), (h, r)) and ((r, t), (r, t)). The base encoder g(·) we used is a 2-layers MLP, which has a batch normalization (Ioffe and Szegedy 2015) and a ReLU activation function in each hidden layer. To make the notation more convenient and clear, we use the same notation to represent the output of encoder, Contrastive Loss Given a true triple (h, r, t), we use the Equation 1 to compute the contrastive loss of four types of positive instances after feeding the instances into the base encoder, and the overall contrastive loss for a triple Discussion We analyze Equation 3 from the aspect of gra- dient. Taking the contrastive loss term CLh, has an ex- ample, the gradient of CLh, hto embedding his: Then when we update the embedding hwith the gradient: The Equation 5 shows that the embedding hwould up-P date to the direction ofτ|P (i)|, which is the mean value of positive instances’ embeddings h. Meanwhile, the hwould also update away from the weighted valueP τ|P (i)|Peof negative instances h. So our proposed contrastive loss Lcan not only pull the related entities and entity-relation couples together in the semantic space but also push the unrelated entities and couples apart. Weighted Contrastive Loss There are four contrastive loss terms in Equation 3. We found that different contrastive loss terms have different effects on different knowledge graphs during our research process. This phenomenon happens may because different knowledge graphs have diverse graph properties, such as the ratio of the number of entities to the number of relations, the number of triples compared with the number of entities. Table 1 shows the results of RESCAL-CL that merely uses one speciﬁc contrastive loss term. In WN18RR dataset, using the term CLhR, (hR)achieves the highest results, while in FB15k-237 and YAGO3-10 datasets, CLRt,Rt is the best. Hence, we introduce a weighted contrastive loss, assigning a weight αfor each contrastive loss term, and α is a hyper-parameter that can be ﬂexibly tuned for a speciﬁc knowledge graph. The contrastive loss Lh, r, tafter adding weights αis: 4.2 Training Objective Given a training triple (h, r, t) in a knowledge graph, the instantaneous loss of our framework on this triple is: where Lis the loss that measures the discrepancy between scoring function’s output f(h, r, t) and the label X. L is the regularizer, and Lis the weighted contrastive loss we introduced in Section 4.1. LLoss Many previous efforts used the ranking losses (Bordes et al. 2013), binary logistic regression (Trouillon et al. 2016) or sampled multiclass logloss (Kadlec, Bajgar, and Kleindienst 2017) to calculate the distance between the scoring function’s output and the triple’s label. Since (Lacroix, Usunier, and Obozinski 2018) had veriﬁed the competitiveness of the full multiclass log-loss, we utilize it as the Lloss in Equation 7. Regularizer Most of the previous work use the squared Frobenius norm (Lnorm) regularizer (Nickel, Tresp, and Kriegel 2011; Yang et al. 2015; Trouillon et al. 2016) in their object functions. More recently, some work proposed more efﬁcient regularizers to prevent the overﬁtting of knowledge graph embeddings, such as the N3 (Lacroix, Usunier, and Obozinski 2018) and DURA (Zhang, Cai, and Wang 2020) regularizers. Since the (Zhang, Cai, and Wang 2020) had shown that DURA regularizer outperforms L2 and N3 regularizers, we use the DURA as the regularizer Lin our work. In this section, we present thorough empirical studies to evaluate and analyze our proposed framework. We ﬁrst introduce the experimental setting. Then we evaluate our framework’s performance on the task of link prediction. Besides, we further analyze the details of our promotion by comparing our method with a baseline on the triples with different relations, and we also study the effect of positive instances to our framework. Finally, we visualize the embeddings of our method and some baselines to explain why our method outperforms baselines. 5.1 Exeprimental Setting Dataset We use three standard knowledge graph datasets—WN18RR (Toutanova and Chen 2015), FB15k237 (Dettmers et al. 2018), and YAGO3-10 (Mahdisoltani, Biega, and Suchanek 2015) to evaluate the performance of knowledge graph embeddings. We divide the datasets into training, validation, and testing sets using the same way of previous work. Table 2 shows the statistics of these datasets. WN18RR, FB15k-237, and YAGO3-10 are extracted from WN18 (Bordes et al. 2013), FB15k (Bordes et al. 2013), and YAGO3 (Mahdisoltani, Biega, and Suchanek 2015), respectively. Some previous work (Toutanova and Chen 2015; Dettmers et al. 2018) indicated the test set leakage problem in WN18 and FB15k, where some test triplets may appear in the training dataset in the form of reciprocal relations. Therefore, they suggested using the WN18RR and FB15k-237 datasets to avoid the test set leakage problem. Implementation Details We implement our method base on the PyTorch library (Paszke et al. 2019), and run on all experiments with a single NVIDIA Tesla V100 GPU. We leverage Adagrad algorithm (Duchi, Hazan, and Singer 2011) to optimize the objective function in Equation 7. We tune our model using the grid search to select the optimal hyper-parameters based on the performance on the validation dataset. We search the embedding size d in {256, 512, 1024} for RESCAL-CL and {200, 500, 1000, 2000} for ComplEx-CL. We search the number of encoder’s hidden units m in {512, 1024, 2048, 4096} and the temperature τ in Equation 1 in {0.3, 0.5, 0.7, 0.9, 1.0}. We search the weights α, α, αand αin Equation 6 in {0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0, 2.5}. The best choices of hyper-parameters, the number of parameters, and the training time of RESCALCL and ComplEx-CL on each dataset are listed in Table 4. Besides, the batch size is 512 for RESCAL-CL and 200 for ComplEx-CL, and the learning rate η as 0.1 for all methods. On WN18RR, we set the number of training epochs as 50 for the ComplEx-CL and 200 for the RESCAL-CL. On FB15k-237 and YAGO3-10, the number of training epochs is 200 for all methods. Statistically signiﬁcant improvements by independent t-test with p = 0.01. Table 3: Link prediction results on WN18RR, FB15k-237 and YAGO3-10 datasets. We take the results of CP, RESCAL, ComplEx, CP-DURA, RESCAL-DURA and ComplEx-DURA from the paper (Zhang, Cai, and Wang 2020), and the results of other baselines are from their original papers. Table 4: The selection of the hyper-parameters of RESCALCL and ComplEx-CL on different datasets. Compared Methods We compare our KGE-CL method with existing state-of-the-art knowledge graph embedding methods, including CP (Hitchcock 1927), RESCAL (Nickel, Tresp, and Kriegel 2011), ComplEx (Trouillon et al. 2016), ConvE (Dettmers et al. 2018), RoratE (Sun et al. 2019), MuRP (Balazevic, Allen, and Hospedales 2019), HAKE (Zhang et al. 2020), ComplEx-N3 (Lacroix, Usunier, and Obozinski 2018), ROTH (Chami et al. 2020), REFE (Chami et al. 2020), CP-DURA (Zhang, Cai, and Wang 2020), RESCAL-DURA (Zhang, Cai, and Wang 2020) and ComplEx-DURA (Zhang, Cai, and Wang 2020). 5.2 Main Results We evaluate the performance of our framework on the link prediction task, which is a frequently-used task to evaluate the knowledge graph embeddings. Speciﬁcally, we replace the head or tail entity of a true triple in the test set with other entities in the dataset and name these derived triples as corrupted triples. The link prediction task aims to score the original true triples higher than the corrupted ones. We rank the triples by the results of the scoring function. The evaluation metrics we used in the link prediction are the MRR and Hits@N: 1) MRR: the mean reciprocal rank of original triples; 2) Hits@N: the percentage rate of original triples ranked at the top N in prediction. For both metrics, we remove some of the corrupted triples that exist in datasets from the ranking results, which is also called ﬁltered setting in (Bordes et al. 2013). For the metrics of Hits@N, we use Hits@1, Hits@3, and Hits@10. Table 3 shows the results of link prediction on WN18RR, FB15K-237, and YAGO3-10 datasets. Our proposed method achieves the highest results compared with the baselines. Speciﬁcally, the RESCAL-CL achieves evidently better results on the WN18RR dataset. The ComplEx-CL outperforms the compared methods in the YAGO3-10 dataset. On FB15k-237, the improvements of RESCAL-CL and ComplEx-CL are relatively not that obvious, which implies that capturing the semantic similarity between the related entities and entity-relation couples is not that important in FB15k-237 dataset. 5.3 Model Analysis Analyzing the Improvements To further explore why our method outperforms existing state-of-the-art techniques, we compare our proposed RESCAL-CL method with the RESCAL-DURA on the triples with different relations in WN18RR. Table 5 shows the results of the comparison, and we found that our RESCAL-CL is signiﬁcantly better than the RESCAL-DURA in 9 out of the 11 relations, verifying that the promotion of our framework is extensive and not just in a speciﬁc relation. Effect of Positive Instances We apply an ablation study to verify the effect of positive instances. The RESCAL- Figure 2: The visualization of the entity-relation couples’ embeddings using T-SNE. A points represents a (h the points with the same color are the couples that connected with the same tail entity. Table 5: MRR, Hit@1 and Hit@10 results of RESCALDURA and RESCAL-CL methods on the triples with different relations in WN18RR dataset. We use * to represent the abbreviation of some words in the relation names. #Train and #Test are the number of triples with the corresponding relations in the training set and test set. CL nopos and Complex-CL nopos are the variants of RESCAL-CL and Complex-CL, which remove all positive instances in a mini-batch when calculating the contrastive loss. Table 6 shows the results of RESCAL-CL nopos, RESCAL-CL, Complex-CL nopos, and Complex-CL on WN18RR and FB15k-237 datasets. From Table 6 we know the positive instance of knowledge graph embedding can signiﬁcantly improve the performance of knowledge graph embeddings. Therefore, the positive instances we constructed are effective for the knowledge graph embeddings. To make our method more explainable, we visualize the entity-relation couples via T-SNE (van der Maaten and Hinton 2008). Speciﬁcally, we randomly pick up eight tail entities in WN18RR. We ﬁnd out the triples with these tail entities in the test set. We extract the (h, r) couples in these triples and visualize these couples’ embeddings trained by the RESCAL, RESCAL-DURA, and RESCAL-CL. Methods method in Figure 2 (a) can not properly separate the couples with different tail entities. Compared with RESCAL, the RESCAL-DURA in Figure 2 (b) can relatively better separate the couples with different tail entities. However, since RESCAL-DURA can not capture the semantic similarity of couples with the same entity, the distribution of the couples connected with the same tail entity is still wide. Our RESCAL-CL can well split the couples in different types and shorten the distance of the couples connected with the same entity. Hence, our RESCAL-CL can better preserve the semantic information of the triples in knowledge graphs and has a higher expressiveness. In this paper, we propose a simple yet efﬁcient contrastive learning framework for knowledge graph embeddings to improve its expressiveness. Compared with the previous work, our method can pull the related entities and entity-relation couples in different triples together in the semantic space and push the unrelated entities and couples apart. The experimental results on the standard datasets show that our method can achieve new state-of-the-art results. Our analyses further verify the effectiveness of our approach. In the future, we plan to extend the critical insights of contrastive learning to other representation learning problems in natural language processing.