Indian Institute of Technology (IIT) Recommender systems are indispensable because they inﬂuence our day-to-day behavior and decisions by giving us personalized suggestions. Services like Kindle, Youtube, and Netﬂix depend heavily on the performance of their recommender systems to ensure that their users have a good experience and to increase revenues. Despite their popularity, it has been shown that recommender systems reproduce and amplify the bias present in the real world. The resulting feedback creates a selfperpetuating loop that deteriorates the user experience and results in homogenizing recommendations over time. Further, biased recommendations can also reinforce stereotypes based on gender or ethnicity, thus reinforcing the ﬁlter bubbles that we live in. In this paper, we address the problem of gender bias in recommender systems with explicit feedback. We propose a model to quantify the gender bias present in book rating datasets and in the recommendations produced by the recommender systems. Our main contribution is to provide a principled approach to mitigate the bias being produced in the recommendations. We theoretically show that the proposed approach provides unbiased recommendations despite biased data. Through empirical evaluation on publicly available book rating datasets, we further show that the proposed model can signiﬁcantly reduce bias without signiﬁcant impact on accuracy. Our method is model agnostic and can be applied to any recommender system. To demonstrate the performance of our model, we present the results on four recommender algorithms, two from the K-nearest neighbors family, UserKNN and ItemKNN, and the other two from the matrix factorization family, Alternating least square and Singular value decomposition. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT Recommender systems inﬂuence a signiﬁcant portion of our digital activity. They are responsible for keeping the user experience afresh by recommending varied items from a catalog of millions of items and also adapt their recommendations according to the personality and taste of the user. Therefore, a sound recommender system may go a long way in improving user experience quality, hence the user retentivity of a digital outlet. Recommender systems have historically been judged on their accuracyHerlocker et al Shani and Gunawardana (2011). When it is concerned with other factors such as novelty, user satisfaction, and diversityHurley and Zhang (2011); Ziegler et al the focus continues to be just on the satisfaction of the information needs of the users. Although of immense importance to the relevance of a recommender system, these criteria do not capture the complete picture. In recent years, the public and academic community have scrutinized artiﬁcial intelligence systems regarding their fairness. It has been observed that the results generated by various recommender systems reﬂect the social biases that exist in human stratumEkstrand et al (2018); Shakespeare et al quantifying, and mitigating the bias present in the results generated by recommendation systems. Burke (2017) presents a taxonomy of classes for fair recommendation systems. The author suggests different recommendation settings with fairness requirements such as fairness for only users, fairness for only items, and fairness for both users and items. Our work falls into fairness for only items category where bias is shown by a particular set of users against a speciﬁc set of items in the dataset. In particular, we are interested in studying and eliminating users’ biasedness against the items associated with a speciﬁc gender in recommendation systems. Bias prevention approaches can be classiﬁed according to the phase of the data mining process in which they operate: pre-processing, in-processing, and post-processing methods. Pre-processing methods aim to control distortion of the training set. In particular, they transform the training dataset so that the discriminatory biases contained in the dataset are smoothed, hampering the mining of unfair decision models from the transformed data. In-processing methods modify recommendation algorithms such that the resulting models do not entail unfair decisions by introducing a fairness constraint in the optimization problem. Lastly, post-processing methods act on the extracted data mining model results instead of the training data or algorithm. The method presented in our work is a hybrid of a pre-processing phase and a post-processing phase. Two prominent studies have focused on gender bias in recommender systems. The work by Shakespeare et al systems, and the work by Ekstrand et al (CF) algorithms while recommending books written by women authors. Both the studies establish that the CF algorithms produced biased results after being fed the biased data from various sociocultural factors. While both the works focus just on showing the existence of bias in the presence of the users’ implicit feedback, we also consider the explicit feedback ratings and the bias that may arise out of it. Thus, our model handles the case when the items associated with speciﬁc gender might have received worse feedback than they otherwise ought to achieve by a set of users. We go one step further and propose a model to mitigate these biases by quantifying a particular user’s bias and debiasing his or her feedback ratings. We theoretically show that the debiased ratings are unbiased estimators of the true preference of the user. Once the ratings are debiased, they are fed Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT into the recommender algorithms as input to produce recommendations for the desired set of users. Since the recommender system is now fed with the debiased ratings, the resulting recommendations are free from the bias factor and avoid a self-perpetuating loop in the future. The bias of an individual user reﬂects his or her taste. However, the KNN based algorithms produce recommendations based on similar characteristics between a set of users and naive implementation of these algorithms reﬂects the bias of one user in the recommendations produced for the other user. While not directly comparing the rating history of different users or items, Matrix Factorization algorithms rely on deriving latent factors, which depend on the rating history. Both the approaches make the system increasingly biased and homogenized after users interact with their biased recommendations and generate data for the next iteration. The above discussion suggests that though it is necessary to reﬂect the user’s preference in the recommendations produced for him or her to achieve accuracy, it is equally necessary to prevent the bias of one user from reﬂecting in the recommendations of another similar user. Our research focuses on this particular objective. Our debiased ratings assure that the biases of one user do not affect other users; however, it may lead to loss of accuracy because of not reﬂecting the user’s own preferences. We introduce a new step called preference correction which injects the user’s preference parameter into his/her own debiased recommendation to maintain the accuracy of the system. On the publicly available Book-Crossing dataset (Ziegler et al show that this approach retained the signiﬁcant reduction in bias and had minimal effect on the accuracy of the system. The bias reﬂected in the recommendations produced by the UserKNN, ItemKNN, ALS, and SVD algorithms is reduced by as much as 41.43% Book-Crossing dataset. When measured with respect to Root Mean Squared Error(RMSE), the ﬁnal accuracy loss in the case of the Amazon dataset comes out to be 10.38% loss comes out to be are our main contributions. 1.1 Contributions respectively for the Amazon dataset and by37.82%,30.73%,24.99%, and32.34%for the respectively for the four algorithms. In the case of the Book-Crossing dataset, the RMSE We propose a model to quantify the gender bias in the recommender system when explicit feedback is present. We propose a principled approach to debias the ratings given and theoretically show that the debiased ratings represent the unbiased estimator of the true preference of the user. We empirically evaluate our model on publicly available book datasets and show that the approach signiﬁcantly reduced the biasedness in the system. To show the generality of our proposed approach, we show the results on four algorithms, UserKNN, ItemKNN, ALS, and SVD. In order to further enhance the accuracy of the debiased system, we propose an approach of preference correction that respects the user’s own preferences towards his/her recommendations. We show that the ﬁnal recommender system signiﬁcantly reduces the bias in the system while not deteriorating the accuracy much. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT The problem of gender bias and discrimination has received lots of attention in recent works Hajian et al.(2016). Many proposals like Pedreschi et al (2010), Thanh et al to detecting and measuring the existing biases in the datasets while other efforts Kamiran et al (2010), Kamiran et al et al.(2014b), Dwork et al models do not produce discriminatory results even though the input data may be biased. Most of these works focus on the classical problem of classiﬁcation. Amatriain et al the application of various classiﬁcation methods like Support Vector Machines, Artiﬁcial Neural Networks, Bayesian classiﬁers, and decision trees in recommender systems. Their ﬁndings indicated that a more complex classiﬁer need not give a better performance for recommender systems, and more exploration is needed in this direction. When considering "fairness for only users" according to the taxonomy presented by Burke (2017), Boratto et al recommending of certain items only to the users of a speciﬁc gender. While weighted regularization matrix factorization studied in Boratto et al Group Utility Loss Minimization proposed in Tsintzou et al UserKNN algorithm. Both the papers address the issue of gender bias by employing post-processing algorithms that work only in limited settings. Though Boratto et al have addressed the issue of fairness of recommender systems with respect to gender, they have done so from the perspective of recommending certain items only to the users of a speciﬁc gender. The difference between their work and our study lies in the fact that we focus on the more direct issue of gender bias in recommendations shown to items associated with a speciﬁc gender. Shakespeare et al produced by Collaborative Filtering algorithms. The work traces the causes of disparity to variations in input gender distributions and user-item preferences, highlighting the effect such conﬁgurations can have on user’s gender bias after recommendation generation. Mansoury et al the biases from the perspective of a speciﬁc group of individuals (for example, a particular gender) receiving less calibrated and hence unfair recommendations. Ekstrand et al the gender bias present in the book rating dataset. Our work is different from the works by Shakespeare et al factors: (i) we consider explicit feedback as opposed to the implicit feedback, and (ii) we propose a principled approach to debias the ratings and theoretically show that the debiased ratings are unbiased estimators of true ratings. .(2019) and Tsintzou et al.(2018) discuss the bias with respect to the preferential Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT The research by Leavy et al. (2020) focuses on algorithmic gender bias and proposes a framework whereby language-based data may be systematically evaluated to assess levels of gender bias prevalent in training data for machine learning systems. Our work is different from this study as this study is focused on evaluating gender bias in the language and textual data settings, while ours deals with gender bias in a more traditional user-item rating setting. Consider a recommender system having respectively. For example, in a book recommender system, the books represent the items; represent the set of books written by women and men authors respectively. With respect to book recommender system, researchers have already shown that the data is biased against female authors’ books Ekstrand et al. (2018). we consider explicit feedback wherein biases may not only arise from not giving the rating to the item but may also come from giving a bad rating to the item. The user proﬁle represents the set of books ( items. The proposed recommender system ﬁrst pre-processes the data that: 1) ﬁnds the log-bias each user bias in the ﬁrst step. We then theoretically show that the debiased ratings generated are unbiased estimators of the true preferences of the user for the items rated by them. Thus, the debiased dataset can then be fed into various recommender algorithms to generate an unbiased predicted rating of a useru not boosted further in the system. Our debiasing model is independent of any recommendation algorithm. We show the performance of our debiasing model on both K-nearest neighbors-based algorithms (UserKNN, ItemKNN) as well as matrix factorization-based algorithms (Alternating Least Square and Singular Value Decomposition) to produce the recommendations. In the next step, we use preference corrector to reintroduce the preferences of a particular user his/her own recommendations. This is achieved via producing a user speciﬁc rating debiased rating recommendations are presented to the user. This step ensures that the system does not lose accuracy for not considering the preferences of the users. Figure 1 shows the schematic diagram of our model. Consider that the ratings biased recommender system can be represented as follows: Adenote the set of items associated with disadvantaged group and advantaged group, ∈ [1, R]denote the rating that useruhas given to the itemi. As opposed to previous works, uand 2) generates the debiased ratingdof each useruand itemiusing the computed for the itemi, denoted by˜d. This debiasing step ensures that the existing biases are Each useru, while rating an itemi, scales down the maximum ratingRbye.pis a random variable, drawn from a distribution functionP(I), which has a mean value ofα. prepresents the logarithm of the true preference of the userufor the itemi. For the sake of brevity, we call it log-preference of the user u for the item i. Hence eis a representation of the true preference of user u for the item i. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT We now present a detailed description of each of the step. The geometric mean of the ratings given by a user advantaged groups, denoted by r We use geometric mean to compute the average rating of a user due to the following reasons: 1) It is less biased towards very high scores as compared to arithmetic meanNeve and Palomares (2019) and 2) when cold users are involved, aggregating recommendations using the geometric mean is more robust as compared to arithmetic meanValcarce et al. (2020). Proof. disadvantaged and advantaged group respectively in user proﬁle p In case the item is associated with the disadvantaged group, the userufurther scales down the rating of the item by a factore.qis a random variable, drawn from a distribution functionQ(I)having a mean value ofβ.qrepresents the logarithm of the biasedness of the userushown to the itemi. For the sake of brevity, we call it the log-bias of the useru for the book i. Hence erepresents the biasedness of the user u for the book i. For each useru,βis sampled from the a distribution functionΩ(x)which governs the global log-bias tendency of the users. We denote the mean value of Ω(x) by γ. The expectation of log-bias,θin the user proﬁleprepresents the mean value of the of the user u. Let us denotem = |D ∩ X|andn = |A ∩ X|to be the number of items associated with Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT Taking expectation both sides: Using linearity of expectation and some simpliﬁcation, we get: Once we get the log biasedness tendencies of users, we use them to produce the debiased ratings for the given dataset. Proof. ln(d Taking expectation both sides: As we can see, the expected value of of the item for user u. Thus, instead of unbiased ratings system’s accuracy because the bias of an individual user reﬂects their taste. However, the debiasing step helps prevent the bias of one user from affecting the recommendation of other users. Next, we use preference corrections by correcting the predicted rating of the user with respect to his/her own preference parameter. ] = β. eWe now provide the main theorem of our paper. ) = θ+ ln(r) = θ+ ln R − p− q. Last equality is obtained from Equation 1. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT Note that when the users are inherently biased against a group of items, work is not just to promote the exposure of the items among the two groups but is to not let the bias of one user creep into the bias of the other user. This was achieved via debiasing the dataset. Once the debiased ratings are generated, the accuracy of the system is maintained by introducing a correction factor. Although providing us with higher accuracy, the idea to re-introduce the correction factor may lead to an overall increase in the individual biases. This on a prima-facie may look self-defeating, but we need to note that ﬁnal ratings still have signiﬁcantly less bias than original ratings. If we do not introduce the correction factor, the users might ﬂock to a substantial bias platform due to poor accuracy. The correction is achieved via multiplying the predicted ratings of items associated with disadvantaged group by a factor Similar to the calculation of bias in the dataset, we can now compute the bias in the recommendation proﬁle. We generate recommendations for the users in the test set useru ∈ T for the user disadvantaged and advantaged groups be denoted by ratings of the items associated with disadvantaged and advantaged groups, denoted by respectively, are given by: ˜ris the predicted rating given to item log-bias in the recommendation-proﬁle unbiased recommendation-proﬁle, ˜θ> 0. We can then compute the overall bias of the recommender system by taking the average overall users, and this average gives us the estimated value of γ. To evaluate the proposed model, we run experiments on two publicly available book rating datasets, the Book-Crossing dataset, originally put together by Ziegler et al Review dataset, put together by Ni et al following stages: Their unique ISBNs identify the books in both datasets. We identiﬁed the authors of the books present in the datasets via their ISBN numbers using the following three API services: Google Books API APIs (2 24), ISBNdb API ISBNDB (2 27), and Open Library API OpenLibrary (3 02). naively to these users will severely affect the accuracy of the system. The goal of this is denoted by˜p= {˜X,˜R}, which represents the set of recommended books (˜X) uand their predicted ratings (˜R= {˜r}). Let the set of items associated with Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT We could not identify the authors of some of the books. Hence we discarded those books from the dataset. We identiﬁed the genders of the authors via their ﬁrst names. We used Genderize.io the gender of a name (03 5), an API service dedicated to identifying the gender given the ﬁrst name of the person. We used a minimum conﬁdence threshold of the gender of some of the authors. We discard the books written by those authors from the dataset. We ﬁltered the Book-Crossing dataset to include only those books with at least those users who have rated at least to the Book-Crossing dataset. We ﬁltered it to include only those books with at least and only those users who have rated at least algorithms have much data to produce accurate recommendations. The statistics of ﬁltered datasets are mentioned in Table 1. The number of books written by male authors is almost equal to that of female authors for both datasets. We show the distributions of log-bias tendency ( Book-Crossing dataset in Figure 2. We observe that the mean log-bias tendency over all the users in the Amazon dataset is higher (0.176) than that of the Book-Crossing dataset (0.157) We randomly separate tions for the users in the test group using two K-nearest neighbors-based algorithms, UserKNN and ItemKNN, and two matrix factorization-based algorithms, Alternating Least Square and Singular Value Decomposition. These algorithms were selected because the accuracy and ranking relevancy of the recommendations produced by them were among the highest values compared with other algorithms. Hence coupling our model with them would best highlight the effects brought about by the same. We calculate the estimated value of log-bias ( Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT separately for each algorithm applied on the two datasets. For this, we use two error measures, the Root Mean Squared Error (RMSE) and the Mean Absolute Error (MAE), and two ranking relevance parameters, Normalized Discounted Cumulative Gains and Mean Reciprocal Rank. We ﬁrst begin plotting the log-bias ( algorithms without employing our debiased model in Figures 3 and 6 for Amazon and BookCrossing datasets respectively. We compute the log-bias by feeding biased ratings algorithms. Table 2 shows the exact values for comparison with other cases. We next deploy our model partially. We leave out the preference correction phase and produce the recommendations using the algorithms mentioned before by feeding the debiased ratings estimate the mean log-bias tendency in the recommendations by the algorithms algorithms for the Amazon as well as Book-Crossing dataset after partial deployment of the model is depicted in the Figure 4 and Figure 7 with the exact values provided in the Table 2. As can be seen, there is a signiﬁcant reduction in log-bias tendency ( (53.67% in error rates on both datasets. This is because the test data itself contains biases. Finally, we deploy our complete model and repeat the experiment. The log-bias ( for the recommendations produced by the algorithms after deployment of the complete model is depicted in Figures 5 and 8 with the values in the Table 2. As can be seen, there is still a signiﬁcant reduction in mean log-bias tendency, which reduces by 37.82% however, is insigniﬁcant, making this trade-off advantageous. Figure 9 presents the percentage gain in bias reduction for both the dataset. The percentage loss in accuracy is depicted in ﬁgures 10 and 11 for Amazon and Book-Crossing datasets respectively. The percentage loss in ranking relevancy metrics are depicted in ﬁgures 12 and 13 respectively. We next conduct signiﬁcance testing to validate the log-bias reduction. Table 3 shows the p-values obtained from left-tail signiﬁcance tests on the log-bias of the recommendations made for the users in the sample. We can see from the p-value for the Amazon datasets that the bias reduction is signiﬁcant. For the Book-Crossing dataset, the signiﬁcance of the bias reduction is less pronounced. ) in Book-Crossing dataset for the UserKNN algorithm. However, we also see an increase in the case of the Book-Crossing dataset for UserKNN algorithm. The accuracy loss, Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT One of the prominent reasons for this is that the test sample size for the Book-Crossing dataset was relatively small due to the small number of users in the dataset. In essence, the utility of the recommender system is maintained while reducing the log-bias tendency in the recommendations. We further observe that the bias reduction is more in the case of UserKNN based recommendations than the ItemKNN based recommendations. This observation can be attributed to the fact that our model addresses the bias originating from the distortion in ratings from the users’ side. It compares the ratings of an item given by a particular user with the appropriately scaled average of ratings Figure 3: Output log-bias in AZ dataset without employing the model Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT given by other users to that item in the dataset. It, therefore, resonates with the UserKNN algorithm, which predicts the ratings of an item for a particular user based on the ratings of that item for his or her peers. The ItemKNN algorithm, on the other hand, predicts the ratings of an item for a particular user based on the ratings given to similar items by that user. The model does not sit squarely with ItemKNN. Thus the bias reduction in UserKNN is more as compared to that in the case of ItemKNN. We further observe that the bias reduction is more in the case of the AZ dataset as compared to the BX dataset. This observation can be attributed to the AZ dataset having a higher input mean log-bias tendency. Further, the AZ dataset has a signiﬁcantly larger number of users and items which leads to a more accurate estimation of user bias scores and, therefore, more effective bias mitigation. We observe that accuracy and ranking relevancy loss is, in general, higher for ItemKNN as compared to UserKNN. This is due to the fact that the model quantiﬁes the bias of users by comparing the ratings given by them to particular items with a scaled average of ratings given by their peers to those items. This resonates with the UserKNN algorithm, which predicts user ratings for particular items based on the ratings of similar users. Thus the model is better oriented towards the UserKNN algorithm, giving better accuracy and bias reduction in its case. In the case of matrix factorization Figure 6: Output log-bias in BX dataset without employing the model Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT algorithms, the accuracy and ranking relevancy losses are relatively comparable. It is not clear which one of the two algorithms is more coherent with the model. We further observe that accuracy loss on BX dataset is higher than that of AZ dataset. This observation can be attributed to the fact that the user and item base of the AZ dataset is higher as Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT compared to the BX dataset. Thus, the bias score estimates are more accurate, which provides more accurate predictions of the item scores for the users when reinserted into the recommendations. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT We proposed a model to quantify and mitigate the bias in the explicit feedback given by the users to different items. We theoretically showed that the debiased ratings produced by our model are unbiased estimators of the true preference of the users for the books. With the help of comprehensive experiments on two publically available book datasets, we show a signiﬁcant reduction in the bias (almost 40%) with just 10% decrease in accuracy using the UserKNN algorithm. Similar trends were observed for other algorithms such as ItemKNN, ALS, and SVD. Our model is independent of these algorithms’ choices and can be applied with any recommendation algorithm. We used book recommender system because we were able to generate the gender information from publicly available APIs. Our model is not restricted to book recommender system as long as protected attribute information about the items is known. We leave extension of the model to missing protected attribute as an interesting future work. It will be an interesting direction to see if the ideas from fair classiﬁcation literature with missing protected attributes Coston et al further did not address the bias originating from fewer ratings for a female-authored book than a Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT male-authored one. We leave extending the model to the bias originating from lesser number of ratings and extensively studying the model for other recommender systems as the future directions. Exploring and Mitigating Gender Bias in Recommender Systems with Explicit FeedbackA PREPRINT