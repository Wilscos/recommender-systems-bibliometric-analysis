For personalized ranking models, the well-calibrated probability of an item being preferred by a user has great practical value. While existing work shows promising results in image classiﬁcation, probability calibration has not been much explored for personalized ranking. In this paper, we aim to estimate the calibrated probability of how likely a user will prefer an item. We investigate various parametric distributions and propose two parametric calibration methods, namely Gaussian calibration and Gamma calibration. Each proposed method can be seen as a post-processing function that maps the ranking scores of pre-trained models to well-calibrated preference probabilities, without affecting the recommendation performance. We also design the unbiased empirical risk minimization framework that guides the calibration methods to the learning of true preference probability from the biased user-item interaction dataset. Extensive evaluations with various personalized ranking models on real-world datasets show that both the proposed calibration methods and the unbiased empirical risk minimization signiﬁcantly improve the calibration performance. Personalized ranking models aim to learn the ranking scores of items, so as to produce a ranked list of them for the recommendation (Rendle et al. 2009). However, their prediction results provide an incomplete estimation of the user’s potential preference for each item; the semantic of the same ranking position differs for each user. One user might like his third item with the probability of 30%, whereas the other user likes her third item with 90%. Accurately estimating the probability of an item being preferred by a user has great practical value (Menon et al. 2012). The preference probability can help the user choose the items with high potential preference and the system can raise user satisfaction by pruning the ranked list by ﬁltering out items with low conﬁdence (Arampatzis, Kamps, and Robertson 2009). To ensure reliability, the predicted probabilities need to be calibrated so that they can accurately indicate their ground truth correctness likelihood. In this paper, our goal is to obtain the well-calibrated probability of an item matching a user’s pref- Corresponding author Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. erence based on the ranking score of the pre-trained model, without affecting the ranking performance. While recent methods (Guo et al. 2017; Kull et al. 2019; Rahimi et al. 2020) have successfully achieved model calibration for image classiﬁcation, it has remained a longstanding problem for personalized ranking. A pioneering work (Menon et al. 2012) ﬁrstly proposed to predict calibrated probabilities from the scores of pre-trained ranking models by using isotonic regression (Barlow and Brunk 1972), which is a simple non-parametric method that ﬁts a monotonically increasing function. Although it has shown some effectiveness, there is no subsequent study about parametric calibration methods in the ﬁeld of personalized ranking despite their richer expressiveness than non-parametric methods. In this paper, we investigate various parametric distributions, and from which we propose two calibration methods that can best model the score distributions of the ranking models. First, we deﬁne three desiderata that a calibration function for ranking models should meet, and show that existing calibration methods have the insufﬁcient capability to model the diverse populations of the ranking score. We then propose two parametric methods, namely Gaussian calibration and Gamma calibration, that satisfy all the desiderata. We demonstrate that the proposed methods have a larger expressive power in terms of the parametric family and also effectively handles the imbalanced nature of ranking score populations compared to the existing methods (Platt et al. 1999; Guo et al. 2017). Our methods are post-processing functions with three learnable parameters that map the ranking scores of pre-trained models to calibrated posterior probabilities. To optimize the parameters of the calibration functions, we can use the log-loss on the held-out validation sets (Guo et al. 2017). The challenge here is that the user-item interaction datasets are implicit and missing-not-at-random (Schnabel et al. 2016; Saito 2019). For each user-item pair, the label is 1 if the interaction is observed, 0 otherwise. An unobserved interaction, however, does not necessarily mean a negative preference, but the item might have not been exposed to the user yet. Therefore, if we ﬁt the calibration function with the log-loss computed naively on the implicit datasets, the mapped probabilities may indicate biased likelihoods of users’ preference on items. To tackle this problem, we design an unbiased empirical risk minimization framework by adopting Inverse Propensity Scoring (Robins, Rotnitzky, and Zhao 1994). We ﬁrst decompose the interaction variable into two variables for observation and preference, and adopt an inverse propensity-scored log-loss that guides the calibration functions toward the true preference probability. Extensive evaluations with various personalized ranking models on real-world datasets show that the proposed calibration methods produce more accurate probabilities than existing methods in terms of calibration measures like ECE, MCE, and NLL. Our unbiased empirical risk minimization framework successfully estimates the ideal empirical risk, leading to performance gain over the naive log-loss. Furthermore, reliability diagrams show that Gaussian calibration and Gamma calibration predict well-calibrated probabilities across all probability range. Lastly, we provide an in-depth analysis that supports the superiority of the proposed methods over the existing methods. Personalized Ranking Let U and I denote the user space and the item space, respectively. For each user-item pair (u, i) of u ∈ U and i ∈ I, a label Yis given as 1 if their interaction is observed and 0 otherwise. It is worth noting that unobserved interaction (Y= 0) may indicate the negative preference or the unawareness, or both. A personalized ranking model f: U ×I → R learns the ranking scores of user-item pairs to produce a ranked list of items for each user. fis mostly trained with pairwise loss that makes the model put a higher score on the observed pair than the unobserved pair: L=`(f(u, i), f(u, j))Y(1 − Y), (1) where `(·, ·) is some convex loss function such as BPR loss (Rendle et al. 2009) or Margin Ranking loss (Weimer et al. 2007). Note that the ranking score f(u, i) ∈ R is not bounded in [0, 1] and therefore cannot be used as a probability. Calibrated Probability To estimate P (Y= 1|f(u, i)), which is the probability of item i being interacted with user u given the pre-trained ranking score, we need a post-processing calibration function g(s) that maps the ranking score s = f(u, i) to the calibrated probability p. Here, the calibration function for the personalized ranking has to meet the following desiderata: (1) the function g: R → [0, 1] needs to take an input from the unbounded range of the ranking score to output a probability; (2) the function should be monotonically increasing so that the item with a higher ranking score gets a higher preference probability; (3) the function needs enough expressiveness to represent diverse score distributions. We say the probability p is well-calibrated if it indicates the ground-truth correctness likelihood (Kull, Silva Filho, and Flach 2017): For example, if we have 100 predictions with p = 0.3, we expect 30 of them to indeed have Y = 1 when the probabilities are calibrated. Using this deﬁnition, we can measure the miscalibration of a model with Expected Calibration Error (ECE) (Naeini, Cooper, and Hauskrecht 2015): However, since we only have ﬁnite samples, we cannot directly compute ECE with Eq.3. Instead, we partition the [0,1] range of p into M equi-spaced bins and aggregate the value of each bin: where Bis m-th bin and N is the number of samples. The ﬁrst term in the absolute value symbols denotes the groundtruth proportion of positive samples (accuracy) in Band the second term denotes the average calibrated probability (conﬁdence) of B. Similarly, Maximum Calibration Error (MCE) is deﬁned as follows: MCE measures the worst-case discrepancy between the accuracy and the conﬁdence. Besides the above calibration measures, Negative Log-Likelihood (NLL) also can be used as a calibration measure (Guo et al. 2017). Calibration Method Existing methods for model calibration are categorized into two groups: non-parametric and parametric methods. Nonparametric methods mostly adopt the binning scheme introduced by the histogram binning (Zadrozny and Elkan 2001). The histogram binning divides the uncalibrated model outputs into B equi-spaced bins and samples in each bin take the proportion of positive samples in the bin as the calibrated probability. Subsequently, isotonic regression (Menon et al. 2012) adjusts the number of bins and their width, Bayesian binning into quantiles (BBQ) (Naeini, Cooper, and Hauskrecht 2015) takes an average of different binning models for the better generalization. In the perspective of our desiderata, however, none of them meets all three conditions (please refer to Appendix A). The parametric methods try to ﬁt calibration functions that map the output scores to the calibrated probabilities. Temperature scaling (Guo et al. 2017), a well-known technique for calibrating deep neural networks, is a simpliﬁed version of Platt scaling (Platt et al. 1999) that adopts Gaussian distributions with the same variance for the positive and the negative classes. Beta calibration (Kull, Silva Filho, and Flach 2017) utilizes Beta distribution for the binary classiﬁcation and Dirichlet calibration (Kull et al. 2019) generalizes it for the multi-class classiﬁcation. While recent work (Rahimi et al. 2020; Mukhoti et al. 2020) is focusing on parametric methods and shows promising results for image classiﬁcation, they cannot be directly adopted for the personalized ranking. Also, the above parametric methods do not satisfy all the desiderata (please refer to Appendix A). In this paper, we propose two parametric calibration methods that satisfy all the desiderata for the personalized ranking models. Revisiting Platt Scaling Platt scaling (Platt et al. 1999) is widely used parametric calibration method, which is a generalized form of the temperature scaling (Guo et al. 2017): where φ = {b, c} are learnable parameters and σ(x) = 1/(1 + exp(−x)) is the sigmoid function. In this section, we show that Platt scaling can be derived from the assumption that the class-conditional scores follow Gaussian distributions with the same variance. We ﬁrst set the class-conditional score distribution for the positive and the negative classes: p(s|Y = 0) = (2πσ)exp[−(s − µ)/2σ], p(s|Y = 1) = (2πσ)exp[−(s − µ)/2σ], where µ, µ∈ R, σ, σ∈ Rare the mean and the variance of each Gaussian distribution. Then, the posterior is computed as follows: P (Y = 1|s) =πp(s|Y = 1)πp(s|Y = 1) + πp(s|Y = 0) where πand πare the prior probability for each class, a = (2σ)− (2σ), b = µ/σ− µ/σ, and c = µ/(2σ) − µ/(2σ) + log(πσ) − log(πσ) ∈ R. We can see that Platt scaling is a special case of Eq.8 with the assumption a = 0 (i.e., the same variance for both classconditional score distributions). Gaussian Calibration For personalized ranking, however, the usage of the same variance for both class-conditional score distributions is not desirable, because a severe imbalance between the two classes exists in user-item interaction datasets. Since users have distinct preferences for item categories, preferred items take only a small portion (∼10% in real-world datasets) of the entire itemset. Therefore, the score distribution of diverse unpreferred items and that of distinct preferred items are likely to have disparate variances. To tackle this problem, we let the variance of each classconditional score distribution be optimized with datasets, without any naive assumption of the same variance for both classes: where φ = {a, b, c} are learnable parameters and can be any real numbers. Since a = (2σ)− (2σ)can capture the different deviations of two classes during the training, we can handle the distinct distribution of each class. Gamma Calibration Gamma distribution is also widely adopted to model the score distribution of ranking models (Baumgarten 1999). Unlike Gaussian distribution that is symmetric about its mean, Gamma distribution can capture the skewed population of ranking scores that might exist in the datasets. In this section, we set the class-conditional score distribution to Gamma distribution: p(s|Y = 0) = Γ(α)βsexp(−βs), p(s|Y = 1) = Γ(α)βsexp(−βs), where Γ(·) is the Gamma function, α, α, β, β∈ Rare the shape and the rate parameters of each Gamma distribution. Then, the posterior is computed as follows: P (Y = 1|s) =11 + πp(s|Y = 0)/πp(s|Y = 1) where a = α− α, b = β− β, and c = log(πβΓ(α)/πβΓ(α)) ∈ R. Therefore, Gamma calibration can be formalized as follows: where φ = {a, b, c} are learnable parameters. Since Gamma distribution is deﬁned only for the positive real number, we need to shift the score to make all the inputs positive: s ← s − s, where sis the minimum ranking score. Other Distributions Besides adopting Gaussian distribution or Gamma distribution for both classes, there have been proposed other parametric distributions for modeling the ranking scores. Swets adopts two Exponential distributions (Swets 1969), Manmatha proposes Gaussian distribution for the positive class and Exponential distribution for the negative class (Manmatha, Rath, and Feng 2001), and Kanoulas proposes Gaussian distribution for the positive class and Gamma distribution for the negative class (Kanoulas et al. 2010). We also investigated these distributions, however, they either have the same form as the proposed calibration function or their posterior cannot satisfy our desiderata. Please refer to Appendix B for more information. Monotonicity for Proposed Desiderata The proposed calibration methods naturally satisfy the ﬁrst and the third of our desiderata: (1) the proposed methods take the unbounded ranking scores and produce calibrated probabilities; (2) the proposed methods have richer expressiveness than Platt scaling or temperature scaling, since they have a larger capacity in terms of the parametric family. The last condition that our calibration methods need to meet is that they should be monotonically increasing for maintaining the ranking order. To this end, we need linear constraints on the parameters of each method: 2as+b > 0 for Gaussian calibration and a/s + b > 0 for Gamma calibration (derivation of these constraints can be found in Appendix C). Since these constraints are linear and we have only three learnable parameters, the optimization of constrained logistic regression is easily done in at most a few minutes with the existing module of Scipy (Pedregosa et al. 2011). Naive Log-loss After we formalize Gaussian Calibration and Gamma Calibration, we need to optimize their learnable parameters φ. A well-known way to ﬁt them is to use log-loss on the held-out validation set, which can be the same set used for the hyperparameter tuning (Guo et al. 2017; Kull, Silva Filho, and Flach 2017). Since we only observe the interaction indicator Y, the naive negative log-likelihood is computed for a user-item pair as follows: L= −Ylog(g(s)) −(1 −Y)log(1 −g(s)). where s= f(u, i) is the ranking score for the useritem pair. Note that during the ﬁtting of the calibration function g(s), the parameters of the pre-trained ranking model f(u, i) are ﬁxed. Ideal Log-loss for Preference Estimation The observed interaction label Y, however, indicates the presence of user-item interaction, not the user’s preference on the item. Therefore, Y= 0 does not necessarily mean the user’s negative preference, but it can be that the user is not aware of the item. If we ﬁt the calibration function with L, mapped probabilities could be biased towards the negative preference by treating the unobserved positive pair as the negative pair. To handle this implicit interaction process, we borrow the idea of decomposing the interaction variable Yinto two independent binary variables (Schnabel et al. 2016): P (Y= 1) = P (O= 1) · P (R= 1)(14) where Ois a binary random variable representing whether the item i is observed by user u, and Ris a binary random variable representing whether the item i is preferred by user u. The user-item pair interacts (Y= 1) when the item is observed (O= 1) and preferred (R= 1) by the user. The goal of this paper is to estimate the probability of an item being preferred by a user, not the probability of an item being interacted by a user. Therefore, we need to train g(s) for predicting P (R = 1|s) instead of P (Y = 1|s). To this end, we need a new ideal loss function that can guide the optimization towards the true preference probability: L= −Rlog(g(s)) −(1 −R)log(1 −g(s)). The ideal loss function enables the calibration function to learn the unbiased preference probability. However, since we cannot observe the variable Rfrom the training set, the ideal log-loss cannot be computed directly. Unbiased Empirical Risk Minimization In this section, we design an unbiased empirical risk minimization (UERM) framework to obtain the ideal empirical risk minimizer. We deploy the Inverse Propensity Scoring (IPS) estimator (Robins, Rotnitzky, and Zhao 1994), which is a technique for estimating the counterfactual outcome of a subject under a particular treatment. The IPS estimator is widely adopted for the unbiased rating prediction (Schnabel et al. 2016; Wang et al. 2019) and the unbiased pairwise ranking (Joachims, Swaminathan, and Schnabel 2017; Saito 2019). For a user-item pair, the inverse propensity-scored log-loss for the unbiased empirical risk minimization is deﬁned as follows: where ω= P (O= 1) is called propensity score. Proposition 1.ˆR(g|ω), which is the empirical risk of Lon validation set with true propensity score ω, is equal toˆR(g), which is the ideal empirical risk. The proof can be found in Appendix D. This proposition shows that we can get the unbiased empirical risk minimizer by φ= argmin{ˆR(g|ω)} when only Yis observed. The remaining challenge is to estimate the propensity score ωfrom the dataset. There have been proposed several techniques for estimating the propensity score such as Naive Bayes (Schnabel et al. 2016) or logistic regression (Rosenbaum 2002). However, the Naive Bayes needs unbiased held-out data for the missing-at-random condition and the logistic regression needs additional information like user demographics and item categories. In this paper, we adopt a simple way that utilizes the popularity of items as done inPP (Saito 2019): ˆω= (Y/maxY). While one can concern that this estimate of propensity score may be inaccurate, Schnabel (Schnabel et al. 2016) shows that we merely need to estimate better than the naive uniform assumption. We provide an experimental result that demonstrates our estimate of the propensity score shows comparable performance with Naive Bayes and Logistic Regression that use additional information (Appendix F). For deeper insights into the variability of the estimated empirical risk, we investigate the bias when the propensity scores are inaccurately estimated. We can replace Ywith Rin Eq.2∼12. Proposition 2. The bias ofˆR(g|ˆω) induced by the inaccurately estimated propensity scores ˆω isP The proof can be found in Appendix D. Obviously, the bias is zero when the propensity score is correctly estimated. Furthermore, we can see that the magnitude of the bias is affected by the inverse of the estimated propensity score. This ﬁnding is consistent with the previous work (Su et al. 2019) that proposes to adopt a propensity clipping technique to reduce the variability of the bias. In this work, we use a simple clipping technique ˆω← max{ˆω, 0.1} that can prevent the item with extremely low popularity from amplifying the bias (Saito 2019). Experimental Setup We concisely introduce our experimental settings in this section. For more details, please refer to Appendix E. Our source code is publicly available. Datasets To evaluate the calibration quality of predicted preference probability, we need an unbiased test set where we can directly observe the preference variable Rwithout any bias from the observation process O. To the best of our knowledge, there are two real-world datasets that have separate unbiased test sets where the users are asked to rate uniformly sampled items (i.e., O= 1 for test sets). Note that in the training set, we only observe the interaction Y. Yahoo!R3has over 300K interactions in the training set and 54K preferences in the test set from 15.4K users and 1K songs. Coat (Schnabel et al. 2016) has over 7K interactions in the training set and 4.6K preferences in the test set from 290 users and 300 coats. We hold out 10% of the training set as the validation set for the hyperparameter tuning of the base models and the optimization of the calibration methods. Base models For rigorous evaluation, we apply the calibration methods on several widely-used personalized ranking models with various model architectures and loss functions: Bayesian Personalized Ranking (BPR) (Rendle et al. 2009), Neural Collaborative Filtering (NCF) (He et al. 2017), Collaborative Metric Learning (CML) (Hsieh et al. 2017), Unbiased BPR (UBPR) (Saito 2019), and LightGCN (LGCN) (He et al. 2020). The details for the training of these base models can be found in Appendix E. Calibration methods compared We evaluate the proposed calibration methods with various calibration methods. For the naive baseline, we adopt the minmax normalizer and the sigmoid function which simply re-scale the scores into [0,1] without calibration. For non-parametric methods, we adopt Histogram binning (Zadrozny and Elkan 2001), Isotonic regression (Menon et al. 2012), and BBQ (Naeini, Cooper, and Hauskrecht 2015). For parametric methods, we adopt Platt scaling (Platt et al. 1999) and Beta calibration https://github.com/WonbinKweon/CalibratedRankingModels AAAI2022 http://research.yahoo.com/Academic Relations (Kull, Silva Filho, and Flach 2017). Note that we do not compare recent work designed for multi-class classiﬁcation (Kull et al. 2019; Rahimi et al. 2020), since they are either the generalized version of Beta calibration or cannot be directly adopted for the personalized ranking models. Evaluation metrics We adopt well-known calibration metrics like ECE, MCE with M = 15, and NLL as done in recent work (Kull et al. 2019; Rahimi et al. 2020). We also plot the reliability diagram that shows the discrepancy between the accuracy and the average calibrated probability of each probability interval. Note that evaluation metrics are computed on Rwhich is observed only from the test set. Evaluation process We ﬁrst train the base personalized ranking model f(u, i) with Yon the training set. Second, we compute ranking score s= f(u, i) for user-item pairs in the validation set. Third, we optimize the calibration method g(s) on the validation set with the computed s and the estimated ˆω, with f(u, i) ﬁxed. Lastly, we evaluate the calibrated probability p = g(s) with Rfrom the unbiased test set by using the above evaluation metrics. Table 1 shows ECE of each calibration method applied on the various personalized ranking models (MCE and NLL can be found in Appendix F). ECE indicates how well the calibrated probabilities and ground-truth likelihoods match on the test set across all probability ranges. First, the minmax normalizer and the sigmoid function produce poorly calibrated preference probabilities. It is obvious because the ranking scores do not have any probabilistic meaning and naively re-scaling them cannot reﬂect the score distribution. Second, the parametric methods better calibrate the preference probabilities than the non-parametric methods in most cases. This is consistent with recent work (Guo et al. 2017; Kull et al. 2019) for image classiﬁcation. The nonparametric calibration methods lack rich expressiveness since they rely on the binning scheme, which maps the ranking scores to the probabilities in a discrete manner. On the other hand, the parametric calibration methods ﬁt the continuous functions based on the parametric distributions. Therefore, they have a more granular mapping from the ranking scores to the preference probabilities. Third, every parametric calibration method beneﬁts from adopting Linstead of Lfor the parameter ﬁtting. The naive log-loss treats all the unobserved pairs as negative pairs and makes the calibration methods produce biased preference probabilities. On the contrary, inverse propensity-scored log-loss handles such problem and enables us to compute the ideal empirical risk indirectly. As a result, ECE decreases by 7.40%∼76.52% for all parametric methods compared to when the naive log-loss is used for the optimization. Lastly, Gaussian calibration and Gamma calibration with Lshow the best calibration performance across all base models and datasets. Platt scaling can be seen as a special case of the proposed methods with a = 0, so it has less expressiveness in terms of the capacity of parametric family. Table 1: Expected Calibration Error of each calibration method applied on ﬁve personalized ranking models. Numbers in boldface are the best results and Improv denotes the improvement of the best proposed method over the best competitor (Platt or Beta with L). Figure 1: Reliability diagram of each calibration method. Gap denotes the discrepancy between the accuracy and the average calibrated probability for each bin. The grey dashed line is a diagonal function that indicates the ideal reliability line where the blue accuracy bar should meet. Beta distribution is only deﬁned in [0,1], so it cannot represent the unbounded ranking scores. To adopt Beta calibration, we need to re-scale the ranking score, however, it is not veriﬁed for the optimality (Menon et al. 2012). As a result, our calibration methods improve ECE by 5.21%∼25.85% over the best competitor. Also, since our proposed models have a larger capacity of expressiveness, they show larger improvement on Yahoo!R3, which has more samples to ﬁt the parameters than Coat. Figure 1 shows the reliability diagram (Guo et al. 2017) for each calibration method applied on LGCN for Yahoo!R3. We partition the calibrated probabilities g(s) into 10 equispaced bins and compute the accuracy and the average calibrated probability for each bin (i.e., the ﬁrst and the second term in Eq.4, respectively). The accuracy is the same with the ground-truth proportion of positive samples for the positive bins (i.e., probability over 0.5) and the ground-truth proportion of negative samples for the negative bins (i.e., probability under 0.5). Note that the bar does not exist if the bin does not have any prediction in it. Figure 2: Ranking score distributions of negative and positive pairs. Figure 3: Fitted function of each calibration method. First, the non-parametric calibration methods do not produce the probability over 0.6. It is because they can easily be overﬁtted to the unbalanced user-item interaction datasets since they do not have any prior distribution. On the other hand, the parametric calibration methods produce probabilities across all ranges by avoiding such overﬁtting problem with the prior parametric distributions. Second, the parametric calibration methods with UERM produce well-calibrated probabilities especially for the positive preference (p > 0.5). The naive log-loss makes the calibration methods biased towards the negative preference, by treating all the unobserved pairs as the negative pairs. As a result, the parametric methods with the naive log-loss (upper-right three diagrams of Figure 1) show large gaps in the positive probability range (p > 0.5). On the contrary, UERM framework successfully alleviates this problem and produces much smaller gaps for the positive preference (lower-right three diagrams of Figure 1). Lastly, it is quite a natural result that parametric methods with UERM do not produce the probability over 0.9, considering that the users prefer only a few items among a large number of items. Score Distribution & Fitted Function Figure 2 shows the distribution of ranking scores trained by NCF and LGCN on Yahoo!R3. We can see that the class-conditional score distributions have different deviations (σ> σ) and skewed shapes (left tails are longer than the right tails). This indicates that Platt scaling (or temperature scaling) assuming the same variance for both classes cannot effectively handle these score distributions. Figure 3 shows the ﬁtted calibration function of each parametric method adopted on LGCN and optimized with UERM. Since most of the user-item pairs are negative in the interaction datasets, all three functions are ﬁtted to produce the Figure 4: Case study. Top-5 items for each user with ranking score s and calibrated probability g(s). low probability under 0.1 for a wide bottom range to reﬂect the dominant negative preferences. Platt scaling is forced to have the symmetric shape due to its parametric family, so it produces the high probability over 0.9 which is symmetrical to that of under 0.1. On the other hand, Gaussian calibration and Gamma calibration, which have a larger expressive power, learn asymmetric shapes tailored to the score distributions having different deviations and skewness. This result shows that they effectively handle the imbalance of user-item interaction datasets and supports the experimental superiority of the proposed methods. Case Study Figure 4 shows the case study on Yahoo!R3 with Gaussian calibration adopted on LGCN. The personalized ranking model ﬁrst learns the ranking scores and produces a top-5 ranking list for each user. Then, Gaussian calibration transforms the ranking scores to the well-calibrated preference probabilities. For the ﬁrst user u, the method produces high preference probabilities for all top-5 items. In this case, we can recommend them to him with conﬁdence. On the other hand, for the second user u, all top-5 items have low preference probabilities, and the last user u has a wide range of preference probabilities. For these users, merely recommending all the top-ranked items without consideration of potential preference degrade their satisfaction. It is also known that the unsatisfactory recommendations even make the users leave the platform (McNee, Riedl, and Konstan 2006). Therefore, instead of recommending items with low conﬁdence, the system should take other strategies, such as requesting additional user feedback (Kweon et al. 2020). In this paper, we aim to obtain calibrated probabilities with personalized ranking models. We investigate various parametric distributions and propose two parametric calibration methods, namely Gaussian calibration and Gamma calibration. We also design the unbiased empirical risk minimization framework that helps the calibration methods to be optimized towards true preference probability with the biased user-item interaction dataset. Our extensive evaluation demonstrates that the proposed methods and framework signiﬁcantly improve calibration metrics and have a richer expressiveness than existing methods. Lastly, our case study shows that the calibrated probability provides an objective criterion for the reliability of recommendations, allowing the system to take various strategies to increase user satisfaction. This work was supported by the NRF grant funded by the MSIT (South Korea, No.2020R1A2B5B03097210), the IITP grant funded by the MSIT (South Korea, No.2018-000584, 2019-0-01906), and the Technology Innovation Program funded by the MOTIE (South Korea, No.20014926).