With the rapid development of Internet, the amount of data has increased exponentially. Due to information overload, it is diﬃcult for users to single out the ones they are interested in from a large number of choices [1, 2]. In order to make it easier for users to obtain the items and information they need, recommendation systems have been applied to scenarios such as music recommendation [3], movie recommendation [4, 5], and online shopping [6, 7]. The recommendation algorithm is the core of recommendation system. One widely used recommendation algorithm is collaborative ﬁltering (CF), which assigns representation vectors based on users’ IDs and items’ IDs [1]. Then it models their interaction through speciﬁc operations (such as inner Due to a large amount of information, it is diﬃcult for users to ﬁnd what they are interested in among the many choices. In order to improve users’ experience, recommendation systems have been widely used in music recommendations, movie recommendations, online shopping, and other scenarios. Recently, Knowledge Graph (KG) has been proven to be an eﬀective tool to improve the performance of recommendation systems. However, a huge challenge in applying knowledge graphs for recommendation is how to use knowledge graphs to obtain better user codes and item codes. In response to this problem, this research proposes a user Recurrent Neural Network (RNN) encoder and item encoder recommendation algorithm based on Knowledge Graph (URIR). This study encodes items by capturing high-level neighbor information to generate items’ representation vectors and applies an RNN and items’ representation vectors to encode users to generate users’ representation vectors, and then perform inner product operation on users’ representation vectors and items’ representation vectors to get probabilities of users interaction with items. Numerical experiments on three real-world datasets demonstrate that URIR is superior performance to state-of-the-art algorithms in indicators such as AUC, Precision, Recall, and MRR. This implies that URIR can eﬀectively use knowledge graph to obtain better user codes and item codes, thereby obtaining better recommendation results. Keywords: KG; RNN; encode product [8] or neural network [9]). In addition, methods based on CF are often plagued by the sparsity of users interaction with items and cold start problem. In order to solve these limitations, researchers usually turn to feature-rich scenarios, where the attributes of users and items are applied to compensate for the sparsity and improve performance of recommendation systems [10, 11, 12, 13]. Some studies [14, 15, 16, 17, 18] go further than simply using attribute: they point out that attributes are not isolated but connected, which forms knowledge graph (KG). Generally, KG is a heterogeneous graph, where nodes correspond to entities (item or attribute of item), and edges correspond to relationships. Compared with KG-free methods, combining KG into recommendation has three beneﬁts [15]: (1) The rich semantic relevance between items in KG helps to explore their potential connections and improve the accuracy of results; (2) The various types of relationships in a KG help to reasonably expand users’ interests and increase the diversity of recommended items; (3) KG connects the items that users likes and the items recommended to users, which brings interpretability to recommendation systems. The recommendation algorithm based on knowledge graph is mainly considered from the following three aspects: a) Method based on graph neural network. It captures the high-level structure in graph and reﬁnes the embedding of users and items. For example, RippleNet [15] spreads users’ potential preferences in KG and explores their hierarchical interests. Wang et al. [18] use a KG graph convolutional network (GCN [19]), which is incorporated to generate high-level term connectivity features. KGAT [20] explicitly models the highlevel connections by an end-to-end manner. b) Method based on embedding, which combines the entities and relationships of KG into a continuous vector space, and helps recommendation system by enhancing semantic representation. For example, DKN [21] combines the semantic and knowledge-level representations of news, and incorporates KG representation into news recommendations. In addition, CKE [17] combines CF module with structure, text and visual knowledge into a uniﬁed recommendation framework. c) Method based on path. It uses paths and related user-item pairs to explore the connection mode between items in KG. For example, MCRec [3] learns the explicit representation of meta-paths in recommendation. In addition, it also considers meta-paths and the interactions between users and items. Compared with the method based on embedding, the method based on path develops KG structure more naturally and intuitively. RKGE [22] and KPRN [23] that needn’t to design meta-paths automatically collect the paths between users and items, and use Recurrent Neural Network (RNN) to encode the paths. However, the above algorithms have some problems such as the following: 1) users’ representation vectors are obtained by simply coding in the methods based graph neural network and embedding, which may not be enough to encode users; 2) method based on embedding is more suitable for in-graph application, such as link prediction or KG completion, rather than recommendation algorithm; 3) method based on path relies heavily on meta-paths and ignores the topological structure of the connections between users and items, and only aggregates the impact of each path on user preferences in the ﬁnal stage. In order to solve the abovementioned problems, this study propose a user RNN encoder and item encoder recommendation algorithm (URIR) based on knowledge graph. The algorithm aggregates and merges a ﬁxed number of high-level neighborhood information with deviations when calculating the representation of a given entity in KG, and tries to encode users by an improved RNN and users’ embeddings. We apply URIR to three data sets: job (resume), ml (movie), and yelp (business). The experimental results show that URIR can obtain better recommendation results. Our contributions in this article are summarized as follows: a) We propose URIR, which is an end-to-end framework for exploring user preferences. By expanding the neighborhood of each entity in KG, URIR can capture the user’s high-level personal interests. b) We conduct experiments on three recommendation scenarios, and the experimental results demonstrate the eﬀectiveness of the URIR algorithm. c) We use RNN to encode users to obtain better users’ representation vectors. As far as we know, it’s the ﬁrst time that RNN has been used to encode users in KG’s algorithms. The remainder of this paper is organized as follows: In section 2, we specify the Item Encoding Layer 2.1, the User Encoding Layer 2.2, the Prediction Layer 2.3, the Model Optimization 2.4, and the Experimental Setup 2.5. In section 3, we conduct extensive experiments to evaluate the proposed method for URIR. Finally, in section 4, we discuss and conclude this work. We ﬁrst introduce the commonly used mathematical symbols in the text and their meanings. The detailed information is shown in Table 1. Additionally, the model framework is shown in Figure 1, which is mainly composed of 3 parts: 1) Item encoding layer 2.1, which responsible for encoding item v and generating item v’s representation vector ˆv. 2) User encoding layer 2.2, which illustrate the coding process of user u and generate user u’s representation vector ˆu. 3) Prediction layer 2.3, it generate the probability ˜r interaction between user u and item v. Ev’s l-level neighbsor Sv’s l-level triples π(h, r, t) weight ˜π(h, r, t) normalized weight Wlearnable parameter blearnable parameter zhidden layer output vv’s l-level neighbor vector Hthe set of the items that user u has interacted with rr= 1 represents that user u has interacted with item v, r= 0 represents that vthe representation of the item in H hv’s RNN output The purpose of item encoding layer is to capture the high-level information of item v in knowledge graph and generate a better item’s representation vector. Here, item v captures high-level neighbor information through v’s l-level neighbors (E exponentially. In order to reduce the calculation time, we collect k neighbors of each node. This method is a commonly used simple but eﬀective strategy to reduce computational cost [15, 18]. We use triples in S relationships in KG, each tail entity of S example, Forrest Gump and Cast Away have more in common in terms of their directors or casts, but are less similar if measured by genres or writers. As a result, we propose a knowledge-aware attentive embedding method to generate diﬀerent attentive weights of the tail entity to reveal the diﬀerent meanings, when it gets diﬀerent head entities and relations. where ˜π (h, r, t), h is the embedding of h, r is the embedding of r, t is the embedding of t, W parameter, b Combine weights with corresponding tail entities to obtain high-level neighbor information, as shown in Eq. 9: where v high-level neighbor information and the information of the item itself are combined to obtain the item’ representation vector, as shown in Eq. 10: where S= {(h, r, t)|h ∈ Eand (h, r, t) ∈ G and t /∈ E is the normalized weight that generated from (h, r, t), πis the weight that generated from is the l-level neighbor information of item v. Because the item itself has vital information, the ˆv is the item v’s representation vector, v is the embedding of item v. The purpose of user encoding layer is to better encode users and generate better users’ representation vectors. The user encoding layer consists of four parts: 1) item encoding layer: Input the items that the user has interacted with, and then generate the corresponding item’s representation vector; 2) user embedding layer: Generate users’ embedding vectors; 4) RNN layer: Input user’s embedding vector and the representation vectors of the items in H that user u has interacted with, as demonstrated in Eq. 11. Item encoding layer were introduced in 2.1, and user embedding layer is actually to obtain users’ embedding. In order to improve the performance of the URIR, we use an improved RNN model. The improved RNN model is introduced as follows. URIR takes the representation vector of the item in H vector and inputs them into the RNN. URIR considers the information carried by the user itself, that is, the user u’s embedding vector to encode user u. This is in line with daily habits. For example, two users have watched the same movie, but there is no guarantee that the movie they watch next time is the same. The RNN layer is represented by the following formulas for Eq. 12 to 14: Where H, u is the user u’s embedding, h already contains the information previously entered. Prediction layer obtains the probability of interaction between u and v through inner product of and uses sigmoid function to control the probability to between 0 and 1, as shown in the following formula Eq. 15: Objective function: According to the literature [9], we treat top-K recommendation task as a binary classiﬁcation problem, where 1 means that user has interacted with item, and 0 means that user has not interacted with item. In objective function, we use cross entropy as objective function, the formula is as follows Eq. 16: Where λ is L2 regularization coeﬃcient, F are the parameters of the model, and D collect the items that have not been interacted with for each user as negative samples, and negative samples are collected according to the ratio of the number of positive samples to the number of negative samples of 4:1 in training set. ˆu is the user u’s representation vector, [v, v, ..., v] are the representation vectors of the items in is the predicted probability of interaction between user u and item v. To verify the performance of algorithm URIR, We use three data sets: (1) Job data set describes some occupations that users have carried on; (2) Ml [22] is a data set widely used in movie recommendation; (3) Yelp [22] records users ratings of some businesses. In order to better verify the performance of the model in solving the cold start problem, we limit the number of user interactions with items to less than 20 [20]. The attributes of the data sets are shown in Table 2. Users represents the number of users, items represents the number of items, relations represents the number of relation types, edges represents the number of edges, interactions represents the number of interactions, entities represents the number of entities (nodes). Evaluation indicators: We use widely applied indicators to evaluate models’ performance. For each user, we divide him\her history records according to the ratio of the number of items in the training set to the number of items in the test set of 7:3, and We collect negative samples according to the ratio of the number of positive samples to the number of negative samples of 4:1 in training set. According to the literature [9, 23, 24], in the test process, for each user, we randomly select 50 items that the user has not interacted with, and randomly select 1 item that has interacted with the user in test set. The recommended list is composed of these 51 items, and the 51 items are sorted to reduce test’s complexity. In model training, we use AUC [25] as an evaluation indicator to determine the best parameters. When evaluating model performance, we use Precision@K [25], Recall@K [25]and MRR@K [25] as evaluation indicators. We calculate these indicators for each user, and calculate the average value when K = {1, 2, 4, 5, 6, 8, 10}. Since situation does not recommend too many items in actual, we choose these values for K. Generally, higher values of these indicators indicate better performance. AUC characterizes the overall performance of recommendation algorithm and covers the performance of all diﬀerent recommendation list lengths [25]. Therefore, we use AUC as an evaluation indicator during training. In order to verify the eﬀectiveness of URIR, we select state-of-the-art algorithms recently proposed for comparison. These algorithms are: NFM [26], KPRN [23], Wide&Deep [10], RKGE [22], RippleNet [15], KGCN [18]. These algorithms were chosen because they are all inﬂuential algorithms based on knowledge graph, and some of the algorithm ideas have provided useful inspiration for us to propose the URIR algorithm. Parameter settings: The optimal parameters of the above algorithms are set by experimental veriﬁcation or the conclusions obtained of the original paper [26, 23, 10, 22, 15, 18]. For URIR, we use Adam as optimizer, and use mini-batch gradient descent algorithm. These methods are some commonly used optimization strategies [15, 20, 18]. In view of the inﬂuence of diﬀerent hyperparameters the embedding dimension of entity d, the number of neighbors collected k, the order of the high-level neighbor L and the number of elements of Hn, we also made a comprehensive comparison of the system. The learning rate, batch size, and L2 regularization coeﬃcient are determined according to training time. The initialization parameters of the model are: d=4, k=4, L=2, n=5. Hyperparameter adjustment sequence is d, L, k, n. The detailed comparison process and comparison results are introduced in Section 3.3. The default values of these parameters are shown in Table 3. We conducted extensive experiments on three data sets to answer the following three questions: 1) Does the URIR model perform better than other advanced recommendation algorithms? 2) What is the impact of RNN on the performance of the URIR algorithm? 3) How does the hyperparameter aﬀect the performance of the URIR? In order to fully understand the performance of the URIR algorithm, we compared the performance of the aforementioned eight diﬀerent algorithms on three diﬀerent data sets, and the results are shown in Figure 2. First of all, Figure 2 (a), Figure 2 (d) and Figure 2 (g) display the variation trend of the Precision of diﬀerent algorithms on three diﬀerent data sets as K increases, respectively. All the algorithm result curves in Figure 2 (a) show that as K increases, the overall Precision shows a downward trend. Although the URIR algorithm curve and other algorithm curves is getting closer, the performance of URIR is still better than other algorithms on the whole. For example, when K={1, 2, 4, 5, 6, 8, 10}, compared with the best results of state-of-the-art algorithms, URIR increases by 26.9%, 36.9%, 5.5%, 1.7% and 0.0%, respectively. Additionally, compared with the worst case of these algorithms, when K={1, 2, 4, 5, 6, 8, 10}, URIR increased by 147.5%, 190%, 28.9%, 9.6% and 0.9%, respectively. Compared with the average values of other models when K={1, 2, 4, 5, 6, 8, 10}, URIR increased by 61.0%, 64.9%, 14.6%, 4.3% and 0.5%, respectively. Similarly, both the ml and yelp data sets show the same characteristics. This phenomenon is consistent with previous studies [15, 20, 18, 27]. Secondly, Figure 2 (b), Figure 2 (e) and Figure 2 (h) demonstrate the evolution trend of Recall and K of diﬀerent algorithms on 3 diﬀerent data sets. For example, Figure 2 (b) shows that the Recall of diﬀerent algorithms on the job data set shows an overall upward trend as K increases. Although the Recall of all algorithms shows a trend of increasing with K, the Recall value of the URIR algorithm is always higher than that of other methods. Thirdly, Figure 2 (c), Figure 2 (f) and Figure 2 (i) illustrate the changes in the MRR of diﬀerent algorithms on three diﬀerent data sets as K increases. Figure 2 (c) shows the changing trend of MRR of diﬀerent algorithms on the job data set. It is very interesting that we observe that all the curves in Figure 2 (c) show an upward trend as K increases. At the same time, although all models have the same trend, the MRR value of the URIR algorithm has always been higher than other models. For example, when URIR is K={1, 2, 4, 5, 6, 8, 10}, compared with the best model, it is improved by 26.9%, 43.1%, 32.8%, 32.1% and 31.8%, respectively. Compared with the worst model when K={1, 2, 4, 5, 6, 8, 10}, URIR increased by 147.5%, 168.4%, 114.1%, 100.8% and 100.0%, respectively. Compared with the average value of other models, When K={1, 2, 4, 5, 6, 8, 10}, URIR increased by 61.0%, 74.1%, 56.9%, 53.9% and 53.0%, respectively. In general, our experimental results show that compared with other models, URIR can better ﬁnd items that users are interested in and put them at the top of the recommendation list. For example, if the user only requests to recommend fewer items, compared to other algorithms, URIR better meets the needs of users. In addition, as the value of K increases, the advantage of the URIR algorithm gradually becomes smaller. This is due to URIR put the items that users like at the top of the recommended list. As K increases, other algorithms also predict the items users like. In addition, when using ml data, the performance of URIR is not as good as the other two data. The possible factor is that there are enough interaction records between users and items of ml data. There is no need to use RNN to capture more information to encode users, which implies that the URIR algorithm is a useful attempt to solve the cold start problem of collaborative ﬁltering recommendation algorithm. In order to generate a better user representation vector, the URIR model considers the introduction of RNN to encode users. Therefore, whether the use of RNN has an impact on user coding is an important issue. Here, we answer the above question by comparing the recommendation results of URIR and the URIR algorithm without the RNN (URIR-RNN). At this time, user u’s representation vector is The experimental results are shown in Table 4. The experimental results are shown in Table 4. The research results indicate that after the application of RNN user encoding, the AUC results of the two datasets job and ml of URIR have been signiﬁcantly improved. Especially on the job data set, the AUC value of the URIR algorithm is about 0.84, and the AUC value of the URIR-RNN is about 0.75, an increase of up to 10%. This result veriﬁes that the use of RNN proposed in URIR to encode users is very necessary and eﬀective. On Yelp data, the reason why the AUC values of the two algorithms are not signiﬁcantly diﬀerent may be that there are many interaction records between users and items in the yelp data and the KG of yelp is so big that URIR-RNN can get enough auxiliary information from the KG. The result also reﬂects that the item encoder of the URIR algorithm eﬀectively obtain auxiliary information from KG. In order to better understand the role of diﬀerent hyperparameters in the URIR algorithm, here we separately study the impact of these parameter changes on the performance of the URIR algorithm: the embedding dimension of entity d, the order of the high-level neighbor L, the number of neighbors collected k and the number of elements of H Embedding dimension of entity d: The role of entity embedding dimension d in the knowledge graph is a general problem [15, 20, 18, 27]. Based on this assumption, we give the performance of d taking diﬀerent values {4, 8, 16, 32, 64} in the URIR algorithm, and the experimental results are shown in table 5. We noticed that the performance of URIR increase with the d growing, and reach the best performance when d reaches a certain value. This result implies that a larger embedding dimension d can signiﬁcantly help to encode more useful information. However, with the further increase of d, the performance of some data sets decrease, such as job data sets. As shown in Table 2, the sparsity of the job graph is signiﬁcantly smaller than the other two data of ml and yelp. Future research should focus on the inﬂuence of graph sparsity on the selection of embedding dimensions [18, 27]. This result indicates that too large embedding dimensions may cause the entity to be overrepresent and introduce noise, which aﬀects the performance of the algorithm. The order of the high-level neighbor L: We studied the impact of the order of the high-level neighbor L on URIR. The results are shown in Table 6. The experimental results show that when L = 1, the AUC value reaches the highest value in all three data sets. This phenomenon is in line with our intuition. Because as L gets larger, the less node information the project v can obtain. However, the correlation between further nodes and v will not be very high, but some noise is transmitted to item v’s representation vector. Another reason is the role of the RNN encoder. Because URIR-RNN’s L is not always one. The number of neighbors collected k: The inﬂuence of the number of neighbors on the eﬀect of the algorithm is a hot research issue [15, 18, 27]. In this section, we study the inﬂuence of the number of neighbors on the recommendation based on the KG by changing the number of neighbors collected k. The results are shown in the table 7. The experimental results clearly show that as the number of neighbors k increases, the AUC value also increases. This is because the small k does not have enough capacity to merge neighborhood information. However, it is observed that the AUC value in the job data set becomes smaller. This may be similar to the reason for the dimension d, that is, the job data set is too sparse, and increasing the value of k cannot obtain enough domain information, but as k increases, it brings noise . The number of elements of H shown in Table 8. When n=5, AUC is the highest. This is not diﬃcult to understand. Because we speculate people’s preferences from a few things. For example, if a user has watched four movies that a actor stars, we can infer that the user likes the actor. However, the user not only watched these movies, but also watched a certain director’s movie. The actor also star the movie. We may also think that the user like the director and recommend the director’s movie. However, the user only likes the actor and does not like the director in actual. In other words, an appropriate n can help the model understand the user’s preferences. But an excessively large n makes the model misunderstand the user’s preferences. This paper proposes a recommendation algorithm for user RNN encoder and item encoder based on knowledge graph, which integrates the structured information of knowledge graph into the algorithm. The RNN encoder helps to understand users’ preferences, and the item encoder helps to mine v’s high-level information. These are beneﬁcial to the generation of better users’ representation vectors and items’ representation vectors, make full use of knowledge graphs, and solve the limitations of the three methods. URIR eﬀectively alleviates the sparsity problem and improves the accuracy of recommendation systems. Experiments were carried out on 3 real-world data sets. we use AUC, Precision, Recall and MRR as the evaluation indicators of the models. URIR was compared with FM, NFM [26], KPRN [23], Wide&Deep [10], RKGE [22], RippleNet [15], KGCN [18]. Our experiment results demonstrate that URIR is generally better than the advanced algorithm on three data sets, can provide users with more accurate recommendation results, and has strong ﬂexibility in fusing heterogeneous data. Good performance in AUC, Precision and Recall shows that it can be applied to classiﬁcation tasks or tasks where users rate items. Achieving better performance in MRR indicates that it can be applied to the task of sorting items. The URIR algorithm understands users’ preferences through users’ historical behaviors, which is very dependent on whether the historical behaviors collected contain all preferences and contains redundant information. Future work should focus on how to improve the sampling strategy and user encoding layer to generate better users’ representation vectors. In addition, future recommendation methods based on knowledge graphs should include speciﬁc network details, including user rating deviations and rating features [13], network node degrees and H-index [28], and hypernetwork loop structure [29] etc.