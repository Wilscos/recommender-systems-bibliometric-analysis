University College LondonNoah’s Ark Lab, HuaweiBeijing Key Laboratory of Big Data Management and Analysis Top-N recommendation, which aims to learn user ranking-based preference, has long been a fundamental problem in a wide range of applications. Traditional models usually motivate themselves by designing complex or tailored architectures based on dierent assumptions. However, the training data of recommender system can be extremely sparse and imbalanced, which poses great challenges for boosting the recommendation performance. To alleviate this problem, in this paper, we propose to reformulate the recommendation task within the causal inference framework, which enables us to counterfactually simulate user ranking-based preferences to handle the data scarce problem. The core of our model lies in the counterfactual question: “what would be the user’s decision if the recommended items had been dierent?”. To answer this question, we rstly formulate the recommendation process with a series of structural equation models (SEMs), whose parameters are optimized based on the observed data. Then, we actively indicate many recommendation lists (called intervention in the causal inference terminology) which are not recorded in the dataset, and simulate user feedback according to the learned SEMs for generating new training samples. Instead of randomly intervening on the recommendation list, we design a learning-based method to discover more informative training samples. Considering that the learned SEMs can be not perfect, we, at last, theoretically analyze the relation between the number of generated samples and the model prediction error, based on which a heuristic method is designed to control the negative eect brought by the prediction error. Extensive experiments are conducted based on both synthetic and real-world datasets to demonstrate the eectiveness of our framework. • Information systems → Recommender systems. Recommender Systems, Bayesian Personalized Ranking, Structure Causal Model, Counterfactuals ACM Reference Format: Mengyue Yang, Quanyu Dai, Zhenhua Dong, Xu Chen, Xiuqiang He, Jun Wang. 2021. Top-N Recommendation with Counterfactual User Preference Simulation. In Proceedings of the 30th ACM Int’l Conf. on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10. 1145/3459637.3482305 Recommendation system basically aims to match a user with her most favorite items. In a typical recommendation process, the system rstly recommends an item list to a user, and then the user provides feedback on the recommendations. In real-world scenarios, people only access a small amount of items, which makes the observed dataset extremely sparse. Recommendation task is usually formulated as a ranking problem. Early models mostly base themselves on the simple matrix factorization method [26]. In order to achieve better performance and adapt dierent scenarios, recent years have witnessed much eort on neuralizing the recommendation models [18,39]. However, the eectiveness of neural models usually depend on a large amount of training samples, which contradicts with the aforementioned sparse user behaviors. In another research line, causal inference (CI) has been recently introduced into the machine learning community to augment the training data for more comprehensive model optimization [1]. The basic idea is rstly assuming an underlying structure causal model (SCM), and then learning the model parameters based on the observed data. At last, the new training samples are generated by actively changing the input variables (called intervention) and collecting the cared outputs. Such sample enrichment method has been successfully applied to the elds of neural language processing (NLP) [41] and computer vision (CV) [2,6,12]. In this paper, we adapt this method to the recommendation domain, which is expected to alleviate the contradiction between the more and more heavier neural recommender architectures and the sparse user behaviors. In a nutshell, the main building block of our idea lies in the counterfactual question: “what would be the user’s feedback if the recommendation list had been dierent?”. More specically, we formulate the recommendation task by a causal graph including three nodes (see Figure 1(b)):Urepresents the user proles,Ris the recommendation list, andSindicates the selection of the user fromR. The new training samples are generated by collecting the user feedback S with dierent interventions on R. While the counterfactual idea seems to be promising, there are many challenges when applying it to the recommendation domain: to begin with, how to formally dene the recommendation task by a structure causal model is still unclear. Then, the space of the candidate recommendation lists (R) can be very large, and the samples induced from dierent recommendation lists (i.e., intervention) Figure 1: (a) is the general framework of our idea. (b) is the structure causal model of the recommender simulator. (c) and (d) are two dierent intervention metho ds. may vary on the eects of optimizing the target model [14,31]. How to design an eective method to selectRremains to be an open problem. At last, the predened structural equation models can be not perfect. How the prediction error inuences the quality of the generated samples and how to lower the negative impact need to be carefully considered. For solving these challenges, in this paper, we propose a novelcounterfactualpersonalizedranking framework (calledCPR). In general, our framework is composed of two parts (see Figure 1(a)), i.e., the target ranking model and the recommender simulator. The ranking model is leveraged to provide the nal recommendation list, and the recommender simulator aims to assist the optimization of the ranking model by generating additional training samples. When building the recommender simulator, we follow Pearl’s [23] counterfactual framework to dene the recommendation process, where the structural equation models (SEMs) betweenU,RandSare dened in a stochastic manner and learned by variational inference to capture the randomness in the recommender system. In order to handle the extremely large space of the recommendation list (i.e.,R), we design a learning-based method to selectRwhich can induce more informative training samples. Considering that the learned SEMs can be not perfect, we theoretically analyze the relation between its prediction error and the number of generated samples. Inspired by this theory, we propose a simple but eective strategy to control the quality of the generated samples. The main contributions of this paper are summarized as follows: (1) We formulate the recommendation problem within Pearl’s causal inference framework, which allows us to generate more training samples for more sucient model optimization. (2) We design a learning-based intervention method, which can lead to more informative training samples for optimizing the target ranking model. (3) We theoretically analyze the relation between the potential prediction error of the structural equation models and the number of generated samples. (4) Inspired by the above theory, we propose a heuristic method to control the quality of the generated samples. (5) We conduct extensive experiments based on both synthetic and real-world datasets to verify the eectiveness of our model. In this section, we rstly recapitulate the key concepts and methodologies in Pearl’s causal inference framework. Then, we briey introduce the ranking-based recommender models. Pearl’s causal inference framework holds the promise of providing a complete and self-contained tool for studying causalities under both experimental and observational settings [25]. It is famous for the proposed three layer causal hierarchy, i.e., association, intervention and counterfactual. In Pearl’s causal inference framework, the rst key concept is the structure causal model (SCM), which helps to formulate real-world problems with causal languages. Denition 1(Structure Causal Model [23]).A structural causal model𝑴is composed of a causal graph𝑮and a set of structure equation models (SEMs)𝑭.𝑮is usually a directed graph, where the edges indicate the causal relations between dierent variables. The nodes in𝑮are classied into two groups: (i)exogenous nodes 𝑼 = {𝑢, ...,𝑢}, which are independent with each other, and summarize the environment when the data was generated, and (ii)endogenous nodes 𝑿 = {𝑥, ..., 𝑥}, which corresponds the variables we need to model in the problem.𝑭 = {𝑓}basically parameterizes the node relations, that is,𝑥= 𝑓(𝑃𝐴, 𝑢), where 𝑃𝐴is the parent of 𝑥in 𝑮. The structure causal model builds the basis of studying the three layer causal hierarchy. In this paper, we mainly focus on counterfactual estimation. In general, this tool aims to predict the outcome if several input variables in the causal graph had been dierent. We briey introduce the estimation process in the following denition: Denition 2(Counterfactual Estimation [23]).Suppose we have two variable sets𝒀, 𝒁 ⊂ 𝑿, and we use small letters𝒚and𝒛to denote their instantiations. The notation𝑝 (𝒚(𝒛))denes the distribution of𝑌if𝒁had been set as𝒛in structural causal model 𝑀given that𝒁is currently observed as𝒛. This distribution corresponds the counterfactual question “what would be𝑌if𝒁had been set as𝒛given its current observation𝒛?”, which is typically derived according to the following three steps: (i)Abduction:deriving the posterior of the exogenous variables𝑝 (𝑼 |𝒁 = 𝑧)based on the prior𝑝 (𝑼 )and the observation𝒁 = 𝑧. (ii)Action:modifying𝐺by removing the edges going into𝒁and set𝒁 = 𝑧(called intervention) to derive𝑝 (𝑦|𝒁 = 𝑧, 𝑼 ). (iii)Prediction:computing∫ the distribution 𝑃 (𝑦(𝑧)) by𝑝 (𝑦|𝒁 = 𝑧, 𝑼 )𝑝 (𝑼 |𝒁 = 𝑧)d𝑼 . The key motivation of the above “abduction-action-prediction” procedure is to make sure that the new (counterfactual) and observed data enjoy the same generation environment, so that they can be compatible and just like to be produced at the same time [25]. Since Pearl’s causal inference framework is not the focus of this paper, we refer the readers to [23] for more details. Based on the tool of counterfactual estimation, one can generate additional training samples to assist the downstream tasks, where a remaining problem is how to indicate 𝑧 to obtain eective samples. Ranking-based recommender models are powerful tools for solving the Top-N recommendation task, which can be optimized in either pair-wise or point-wise manners. In the pair-wise method, there are usually three inputs, i.e., a user𝑢and a positive-negative item pair (𝑖, 𝑗). The goal is to maximize the user preference margin between the positive and negative items. Suppose the user preference is estimated from a function 𝑓 (·), then the optimization target is: where𝜎 (·)is the sigmoid function to avoid trivial solutions.𝑂 = {(𝑢, 𝑖, 𝑗)|𝑖 ∈ I, 𝑗 ∈ I \ I}is the set of training samples.I is the whole item set, andIindicates the set of items which have received𝑢’s positive feedback. For the point-wise method, the input is a user-item pair(𝑢, 𝑖), and the user preference estimation is treated as a classication problem, where𝑓is optimized by the following cross entropy objective: 𝐿(𝑂) = −log 𝑓 (𝑢, 𝑖) −(1 − log(𝑓 (𝑢, 𝑖))),(2) where𝑂= {(𝑢, 𝑖)|𝑖 ∈ I}and𝑂= {(𝑢, 𝑖)|𝑖 ∈ I \ I}are the sets of positive and negative samples.𝑂 = 𝑂∪𝑂denotes the complete training set. Ranking-based recommender models dierentiate themselves by the various implementation of𝑓. Simple𝑓can be realized by matrix factorization [26], and more advanced𝑓includes multi-layer perception [18,21], attentive neural network [17] and graph convolutional network [16], among others. Our framework is composed of two parts (see Figure 1(a)): one is the recommender simulator, which is responsible for generating new training samples. The other is the target ranking model, which is learned based on both of the observed and generated data and leveraged to provide the nal recommendation list. Our framework can be applied to any ranking-based recommender models, and we detail the recommender simulator in the following sections. To begin with, we reformulate the recommendation problem by a structure causal model𝑴 = {𝑮, 𝑭 }. In specic, the causal graph𝑮 is dened as follows (see Figure 1(b)): (1)U,RandSare the nodes representing the user, the recommendation list and the positive itemsselected by the user. (2)U → Rencodes the fact that the recommendation list is generated according to the user preference. (3)U → SandR → Sindicate that the positive items are jointly determined by the recommendation list and user preference. The structure equation models𝑭are dened in a stochastic manner as follows: where𝑼,𝑹and𝑺are endogenous variables.𝜶 ∈ Rand𝜷 ∈ R are exogenous variables.𝑝is the probability of recommending item list 𝑹. 𝑝is the probability of selecting 𝑺 as the positive feedback. N (0, 𝑰 ) is the standard Gaussian distribution. Remark. (1) In order to consider the potential noisy information and randomness in the recommender system, the structure equation models (i.e.,𝑭) are dened in a stochastic manner, which helps to learn user preference in a more accurate and robust manner. (2) The exogenous variables in the recommendation problem can be explained as the conditions (e.g., system status, user habit, etc.) which induces the currently observed data. Take movie recommendation as an example, the users may be more likely to watch movies in their leisure time. So if the data is collected at night, then the movie click probability should be higher. Such temporal condition is latent but can inuence the data generation process, which is expected to be captured by the exogenous variables, and recovered by inferring the corresponding posteriors. 3.1.1 Specification of𝑭. We implement𝑭by considering the unique characters of the recommender system. Recall that𝑝is the probability of recommending item list𝑹. Suppose we have|I|items in the system, then the numberof candidate item lists is𝐶, which is extremely large in the recommendation task. To alleviate this problem, we ignore the combinatorial eect between dierent items, and the probability of recommending an item list can be factorized into the multiplication of the single item recommendation probabilities, that is: 𝑝(𝑹 = 𝒓 |𝑼 = 𝑢, 𝜶 ) =𝑝 (𝑅= 𝑟|𝑼 = 𝑢, 𝜶 ) where𝒓 = {𝑟, 𝑟, ..., 𝑟}is the recommendation list. For the single item recommendation probability, we predict it by a softmax operator. We use𝑷 ∈ Rand𝑸 ∈ Rto represent the embeddings of the users and items. They can be initialized by the ID or prole information of the users and items.𝑑is the embedding size, and 𝒘∈ Ris a weighting parameter. 𝑝represents the probability of selecting item set𝑺from𝑹, which is specied as: 𝑝(𝑺 = 𝒔|𝑈 = 𝑢, 𝑹 = 𝒓, 𝜷) =exp (𝑿𝒀+ 𝑤𝛽)Í(5) where𝒔 = {𝑠, 𝑠, ..., 𝑠}is the set of selected items.𝒘∈ Ris a weighting parameter. The user and item properties are encoded by the metrics of𝑿 ∈ Rand𝒀 ∈ R, respectively, and𝑑 is the embedding size. In𝑝and𝑝, we represent the users and items with dierent embedding metrics. This can more exibly characterize the system impression model (i.e.,U → Rin Figure 1(b)) and the user response model (i.e.,U → Sin Figure 1(b)), which usually hold dierent promises in real recommender systems. 3.1.2 Learning𝐹. Suppose the training set isO = {(𝑢, 𝒓, 𝒔)}, where𝑢is the target user,𝒓is the recommendation list,𝒔is the item set selected by𝑢from𝒓, N is the sample number. For𝑝, since the number of items (i.e.,|I|) can be very large, it is hard to directly optimize the softmax term in equation (4). Thus we resort to the negative sampling technique [18,26], which induces the following optimization target: 𝐿=log (𝜎 (𝑷𝑸+𝑤𝛼)) +log (1−𝜎 (𝑷𝑸+𝑤𝛼)) where{𝑷, 𝑸, 𝒘}are the parameters to be learned.𝒓is the set of negative samples, which are randomly sampled from the nonrecommended items.𝜶 = [𝛼]is sampledfromN (0, 𝑰 ). With this objective, the parameters are optimized to maximize the recommended item probability and simultaneously minimize the ones which are not presented to the users. For𝑝, since the length of the recommendation list (i.e., K) is usually not large, we directly maximize the following livelihood without approximation: where{𝑿, 𝒀,𝒘}are the parameters to be optimized.𝑠and𝑟 are the𝑡th and𝑗th items in𝒔and𝒓, respectively.𝜷 = [𝛽]is sampled fromN (0, 𝑰 ). It seems that this objective only maximizes the probability of the items which are selected by the user, but with the softmax operator, the probabilities of the non-selected items are automatically lowered. 3.1.3 Counterfactual estimation based on𝐹. Once we have learned 𝑝and𝑝, we follow Pearl’s “abduction-action-prediction” procedure [24] to conduct counterfactual estimation. In the step of abduction, we estimate the posterior of𝜶and𝜷given the observed datasetO. For𝜷, the posterior can be derived based on the following Bayesian rules: where𝑝 (O|𝜷)can be easily derived based on equation (5). Recall that our goal is to sample from the posteriors. However, the result of equation (8) is too complex for sampling, which motivates us to use variational inference [3] for approximation. In specic, we rstly dene a Gaussian distribution𝑞(𝜷) ∼ N (𝝁, 𝝈), where𝝓 = {𝝁, 𝝈 } is set of learnable parameters. Then we optimize 𝝓 by minimizing the KL-divergence between𝑞(𝜷)and𝑝 (𝜷 |O), where the evidence lower bound (ELBO) [3] we need to maximize is: ELBO = E[log 𝑝 (𝜷, O)] − E[log 𝑞(𝜷)](9) Similarly, we can learn a variational distribution for𝑝 (𝜶 |O). Due to the space limitation, readers are referred to [3] for more technical details of the variational inference. In the action step, we select a userˆ𝑢, and set𝑹 =ˆ𝒓. When making prediction, we rstly sampleˆ𝜷from𝑞(𝜷), and then compute the probability of itemˆ𝑟by: At last,ˆ𝒔is predicted by collecting M items with the highest probabilities. Based on the above designed recommender simulator, one can attempt dierentˆ𝒓’s and derive the correspondingˆ𝒔’s to form new (counterfactual) training samples. A nature question is how to set ˆ𝒓. Straightforwardly, one can exploreˆ𝒓in a random manner (e.g., show the users with random recommendation lists). However, as mentioned before, the space ofˆ𝒓can be extremely large, and it is well known that dierent training samples are not equally important for model optimization [31], thus the random method can be less eective in hitting the optimal samples. In order to achieve better optimization results, we design a learning-based method to selectˆ𝒓. Our key idea is to make the generated samples more informative for the target ranking model. It has been studied in the previous work [12,13] that the samples with larger loss can usually provide more knowledge for the model to learn (i.e., more informative). They can well challenge the model and bring more inspirations to improve the performance. Following these studies, we use the loss of the target ranking model as the reward, and build a learning-based method to generateˆ𝒓. Formally, suppose the target ranking model is𝑓, and we denote its loss function as𝐿, which can be specied as equation (1) or (2). The goal of the agent is to conduct actions (generating recommendation lists) which can lead to larger𝐿. Considering that the action space can be very large, we follow the previous work [40] to learn a Gaussian policy to generate the continuous item center ofˆ𝒓, after which the discrete item IDs are recovered based on equation (4). The nal learning objective is: where𝜋is the Gaussian policy implemented as a two-layer fully connected neural network with ReLU as the activation function.ˆ𝑢 indicates the target user.𝐿(𝐶(ˆ𝝉))denotes the loss of the generated training samples𝐶 (ˆ𝝉). We generate T sets of training samples, and each one is derived fromˆ𝝉based on the following steps: •Derivingˆ𝒓by selecting K items near the item centerˆ𝝉according to the scores {(ˆ𝝉𝑸+ 𝒘𝛼)}. •Derivingˆ𝒔by selecting M items with the largest probabilities of 𝑝(ˆ𝑟|𝑈 =ˆ𝑢, 𝑹 =ˆ𝒓,ˆ𝜷) (𝑘 ∈ [1, 𝐾]) . •For objective (1),𝐶 (ˆ𝝉) = {(ˆ𝑢, 𝑖, 𝑗)|𝑖 ∈ˆ𝒔, 𝑗 ∈ˆ𝒓\ˆ𝒔}. For objective (2), 𝐶 (ˆ𝝉) = {(ˆ𝑢, 𝑖) |𝑖 ∈ˆ𝒔} ∪ {(ˆ𝑢, 𝑖) |𝑖 ∈ˆ𝒓 \ˆ𝒔}. We summarize the complete learning process in Algorithm 1 and 2. In specic, the target ranking model is rstly trained based on the original dataset. And then, we generate many counterfactual training samples based on the Gaussian policy. At last, the target ranking model is retrained based on the generated datasets. In this section, we theoretically analyze the proposed framework within the probably approximately correct (PAC) learning framework. We focus on the pair-wise learning objective, and the conclusions can be easily extended to the point-wise case. 3.3.1 Theoretical insights on the learning-based intervention method. The key motivation of the learning-based method is to generate harder samples, so that the target model can be more informed and Algorithm 2: Learning algorithm of 𝜋 achieve better performance. The eectiveness of this idea has been empirically demonstrated in the previous work [13,31]. Here, we provide a theoretical justication based on the following theory: Theorem 1.Suppose the users’ feedback on an item pair(𝑖, 𝑗)is probabilistic, and we can observe𝑖 > 𝑗and𝑖 < 𝑗with the probabilities of𝜂and 1−𝜂, respectively. Then,𝜂can actually measure the hardness of the sample, when𝜂is closer to, then the sample is harder, since the propensity between the items is more ambiguous. Suppose𝜖, 𝛿 ∈ (0,1), and we use a simple voting mechanism to determine the relation between𝑖and𝑗, then we need to havesamples on(𝑖, 𝑗)to ensure that the prediction error is smaller that 𝛿. This theory suggests that we need to generate more samples (i.e., larger) for the harder (i.e., smaller𝜂) item pairs. It agrees with our goal in learning-based intervention method, where the produced samples are expected to be hard and can well challenge the target model. We present the proof of this theory in the Appendix. 3.3.2 Imperfection of SEMs. Since the new samples are generated based on the learned SEMs, careful readers may have concerns on how would the approximation error of𝑭inuence the sample generation. To shed lights on this problem, we theoretically analyze the relation between the prediction error of𝑭and the number of generated samples. To begin with, we assume that the recommender simulator can recover the true ranking of the item pairs with a noisy parameter𝜁 ∈ (0,0.5), i.e., suppose the true triplet is (𝑢, 𝑖, 𝑗), then the recommender simulator generates the true (i.e., (𝑢, 𝑖, 𝑗)) and wrong (i.e.,(𝑢, 𝑗, 𝑖)) samples with the probabilities of 1− 𝜁and𝜁, respectively. As the special cases,𝜁 =0 means the recommender simulator is perfect, and there is no noisy information in the produced samples.𝜁 =0.5 means the recommender simulator is a totally random predictor, and the generated data is nothing but noise. Then, we have the following theory: Theorem 2.Suppose𝜖, 𝛿 ∈ (0,1), and𝑓 ∈ Fis the target ranking model. If the number of training samples is larger than, then the estimation error of𝑓in terms of ranking prediction is smaller than 𝜖 with the probability larger than 1 − 𝛿. The proof of this theory is similar to that of theory 1 in [34]. From this theory, we can see, as the noisy parameter𝜁becomes larger, more samples (i.e.,) are needed to achieve suciently well performance (the prediction error is smaller than𝜖). Extremely, when𝜁 →0.5, we need to produce innite training samples. This implies that if the recommender simulator is completely random, then we can not expect well performance by training on the generated data, which is aligned with the intuition. 3.3.3 Controlling the noisy information. Inspired by this theory, we propose a simple but eective method to control the noisy information. The general idea is to remain the samples with higher condence. In specic, for a given recommendation listˆ𝒓, we use the selection probability𝑝(ˆ𝑟|𝑈 =ˆ𝑢, 𝑹 =ˆ𝒓,ˆ𝜷)to measure the condence of the recommender simulator. We denote byˆ𝒔and ˆ𝒔the sets of k items with the largest and smallest selection probabilities. Then the training set for the pair-wise objective is built as𝐶 (ˆ𝝉) = {(ˆ𝑢, 𝑖, 𝑗)|𝑖 ∈ˆ𝒔, 𝑗 ∈ˆ𝒔}. For the point-wise objective, 𝐶 (ˆ𝝉)is set as{(ˆ𝑢, 𝑖) |𝑖 ∈ˆ𝒔} ∪ {(ˆ𝑢, 𝑖) |𝑖 ∈ˆ𝒔}. In this method, if 𝑘is small, then the model has more condence on the generated samples, which may reduce the noisy information, but at the same time, less samples can be generated, which may lead to insucient model optimization. If𝑘is large, more samples will be generated, but the noisy level can also be high. In this sense,𝑘is actually a parameter to trade-o the noisy information and the number of generated samples. In this section, we conduct extensive experiments to verify the eectiveness of our framework. In the following, we begin with the experiment setup, and then report and analyze the results. 4.1.1 Datasets. Our experiments are based on both synthetic and real-world datasets. With the synthetic dataset, we aim to evaluate our framework in a controlled manner under clean environment. By the real-world dataset, we try to verify our framework’s eectiveness in real-world settings. We follow the previous work [42] to build the synthetic dataset, where we simulate𝑁(=600)users and𝑁(=300)items. For each user𝑖(or item𝑗), the preferences𝒑∈ R(or properties 𝒒∈ R) are generated from a multi-variable Gaussian distribution N (0, 𝑰 ), where𝑑and𝑰represent the vector size and unit matrix, respectively. In order to generate the recommendation list for user 𝑖, we rstly compute the score of recommending item𝑗based on a neural network:𝑟=1−𝜎 (𝒂𝜅(𝜅([𝒑, 𝒒])) +𝑏), where𝒂 ∈ R is specied as an all-one vector, and𝑏is set as zero. We follow [42] to set𝜅(·)and𝜅(·)as piecewise functions, and𝜅(𝑥) = 𝑥 −0.5 if𝑥 >0, otherwise𝜅(𝑥) =0,𝜅(𝑥) = 𝑥if𝑥 >0, otherwise 𝜅(𝑥) =0. Given𝑟, the probability of recommending item𝑗is . For each user, we generate 25 impression lists, each of which is composed of 5 items. When generating the feedback of a user𝑖on an item𝑗, we follow the previous work [42] to explore both linear and non-linear user response models, that is: where𝜅(𝑥) = 𝑥 +0.5 if𝑥 <0, otherwise𝜅(𝑥) =0.𝑁is used to set the noisy level of the datasets, and its default value is 0.I(𝑥)is an indicator function, which is 1 if 𝑥 > 0, and 0 otherwise. The real-world experiments are based on the recently released MINDdataset [35]. This data is collected from the user behavior log of Microsoft News, and we uses the MIND-small dataset for experiments, where we are provided with 156,965 recommendation lists and 234,468 interactions between 50,000 users and 20,288 items. 4.1.2 Baselines. In order to evaluate the eectiveness of our proposed framework, we compare our model with the following representative methods:ItemPopis a non-personalized model, and the ranking score of an item is based on its popularity in the training set.ItemKNN[30] is an item-based k-nearest-neighborhood (KNN) model.BPR[26] is a well-known recommender model based user implicit feedback.GMF,MLPandNeuMF[18] are neural recommender models, among which NeuMF is a combination between GMF and MLP.CDAE[36] is a method based on denoising auto-encoder, which is proved to be the generalization of several well-known collaborative ltering models.LightGCN[16] is a graph-based recommender model, where the user-item structure information is considered in the modeling process. In order to verify the generality of our framework, we apply it to MF, GMF, MLP, NeuMF and LightGCN, respectively, which leads to the models ofCPR-MF,CPR-GMF,CPR-MLP,CPR-NeuMF andCPR-LightGCN. To demonstrate the necessity of learningbased intervention method, we also evaluate the performance our framework with random intervention, where the recommendation list𝑹is assigned in a random manner. We denote the corresponding baselines asCPR-MF-r,CPR-GMF-r,CPR-MLP-r, CPR-NeuMF-r and CPR-LightGCN-r. 4.1.3 Implementation details. We follow the previous work [17, 18,26] to employ standard leave-one-out protocol for evaluation. 10 items are recommended from each model to compare with the users’ actually interacted ones. We leverage the widely used metrics including Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) to compare dierent models. Among these metrics, HR aims to measure the overlapping between the predicted and ground truth items, while NDCG further takes the ranking of the prediction results into consideration, and higher ranked accurate items contribute more to the nal score. The implementations of our model and all the baselines are based on PyTorch [22] with mini-batch Adam [19] optimizer. For ItemKNN and CDAE, we follow the settings of the original paper. For the other baselines, we determine their hyper-parameters based on grid search, and the search ranges for the embedding size, batch size, regularization coecient and learning rate are set as{32,64,128,256,512}, negative samples are selected from the whole item set or the impression list. When optimizing𝑝and𝑝, we empirically set the learning rate as 0.001, and the user/item embedding sizes are both tunned in{16,32,64,128,256,512}. The length of the impression list (i.e.,|𝑹|) is set as 5, and the size of𝑺(i.e.,𝑘) is determined in {1,2,3}. The Gaussian policy is implemented as a two-layers fullyconnected neural network, where the hidden dimension is searched in {16, 32, 64}. In this subsection, we present and analyze the experimental results on the synthetic dataset. 4.2.1 Overall performance. In the experiments, we explore dierent user/item representation dimensions as well as both linear and non-linear user response models. The overall comparison results are shown in Table 1. It is interesting to see that the relative simple baselines (e.g., ItemPop and ItemKNN) are competitive in many cases. This observation is consistent with the previous work [9], and suggests that simple methods may still be useful in some recommendation scenarios. It is encouraging to see that our framework can consistently improve the performance of dierent target models. The improvement is consistent across dierent settings. In specic, our framework can on average improve the performance of the target model by about 10.08% and 11.17% on HR and NDCG, respectively. This result demonstrates the eectiveness of our framework. The reason can be that the recommender simulator enables us to explore the potential user preferences, which provides useful signals to widen the model visions and improve the performance. The importance of the learning-based intervention method is evidenced in the lowered performance when we use random strategy as a replacement, which veries our claims in the introduction. It is interesting to see that the performance gains on neural models (e.g, MLP, NeuMF and LightGCN) are usually larger than that of the matrix factorization method. In many cases, the performance is even lowered for GMF and BPR after applying our framework, e.g., when we use nonlinear user response model and set𝑑 =32. This observation agrees with our expectation, that is, neural models need more training samples to display its strong expressiveness, and our framework should be more eective for them. Since most of the promising recommender models in practice are based on neural architectures, this observation demonstrates the potential of our framework in real-world settings. Table 1: Performance comparison based on the synthetic dataset. We present the relative improvements of our framework over the target model in the parentheses. Table 2: Eects of the noisy control parameter 𝑘, the best performance for each method and setting are labeled by bold fonts. 4.2.2 Eects of the noisy control parameter𝑘. In this section, we investigate the eectiveness of the noisy control method proposed in section 3.3. By this method, we hope our framework can adaptively tune itself to accommodate dierent noisy recommendation scenarios. In order to evaluate such capability, we set𝑁=0 or sample it fromN (0, 0.2)to simulate the settings where the data is observed with dierent noisy levels. We base the experiment on the nonlinear user response model, and we tune𝑘in the range of{1,2,3}. The results are presented in table 2. It is not surprising to see that the performance of the same model is lowered when the noisy level is increased from 0 to sampling fromN (0, 0.2). When𝑁is sampled fromN (0, 0.2), the best performance is mostly achieved when 𝑘 =1, while if we set𝑁=0,𝑘 =2 or𝑘 =3 can usually lead to more favored results. We speculate that: when the dataset is noisy, the recommender simulator can be not trained well, thus the quality of the generated samples are not high. In such a scenario, removing the noisy samples seems to be more important. On the contrary, if the dataset is noise-free, then the recommender simulator can be more accurate. In this case, the sample quality is not the main issue, and more samples are favored to achieve sucient model optimization and better performance. For dierent noisy-level datasets, our Table 3: Overall results on the real-world dataset. framework can always adapt itself to achieve better performance, which demonstrates the eectiveness of the noisy control method. The above synthetic experiments suggest that our idea is eective under ideal environment. In this section, we experiment with the real-world dataset, which is more challenging but more practical. 4.3.1 Overall comparison. To begin with, we compare our model with the baselines introduced in section 4.1.2. The results are presented in Table 3, from which we can see: similar to the synthetic dataset, our CPR framework can consistently lead to improved performances comparing with the target models, and the learningbased intervention method outperforms the random strategy in most cases. The performance gains on neural models are usually larger than that of the shallow ones, which again demonstrates the potential of our model for deep recommender models. Remarkably, the improvement of our framework is not that large comparing with the results on the synthetic dataset. We speculate that the real-world dataset can be noisy. Training based on it can lead to imperfect recommender simulator, which lowers the quality of the generated samples and limits the nal recommendation performance. 4.3.2 Performance on the items with dierent coldness. Cold start has long been a fundamental problem in the recommendation domain. In this section, we study the eectiveness of our framework on the items with dierent coldnesses. More specically, we rst run the models based on the original training set, and then evaluate their performances on three dierent testing sets varying on the item coldness. For the rst testing set (denoted as “Low”), each item has less than 5 interactions with the users in the training set, which means the model is trained in a quite insucient manner on these items. In the second testing set (denoted as “Middle”), each item has 5 to 15 interactions, thus the model is provided with more knowledge during the training process. In the last testing set (denoted as “High”), each item has more than 15 interactions, which is the “warmest” setting in the experiment. The model parameters are set as their optimal values tunned in the above experiments. From Table 4 we can see: our framework can enhance the performance of the target models in most cases, which demonstrates its eectiveness under dierent cold start settings. An interesting observation is that the performance improvement on the “Low” and “Middle” testing sets are much larger than that of the “High” dataset. For example, CPR-LightGCN can produce a remarkable 151.74% and 162.75% improvements over LightGCN on HR@10 and NDCG@10 for the “Low” dataset, and the improvements on the “Middle” dataset is about 40%-50%. However, on the “High” dataset, the performance is even lowered on the metric of HR@10, and the improvement on NDCG@10 is also limited. We speculate that, in the “Low” dataset, the testing items are trained in a quite insufcient manner. A large amount of knowledge has been ignored by the target model, which provides more opportunities for the generated samples to introduce useful signals and improve the performance. In real-world recommendation scenarios, cold-start is a notorious problem obsessing people for a long time, this experiment demonstrates the potential of our framework in alleviating this problem. In this section, we compare our model with the previous work to highlight the contributions of this paper. Relation with causal recommendation.Recent years have witnessed many contributions on incorporating causal inference into the recommendation domain [5,11,27,28,37]. For example, [28] explains the recommendation problem by a treatment-eect model, and designs a re-weighting method to remove the bias in the observed data. However, this method is based on user explicit feedback, and the loss function is restricted to root mean square error (RMSE). In order to handle user implicit feedback, [27,37] extend this method by incorporating cross-entropy loss and designing tailored debiasing models. In addition, [20] proposes a general knowledge distillation framework to debias the training data. [5] provides a thorough discussion on the recent progress on debiased recommendation. [4] leverages uniform data to learn causal user/item embeddings for more fair and unbiased recommendation. These methods have achieved many successes in the recommendation domain. However, they mostly leverage causal inference to debias the training data, which is dierent from our data augmentation purpose. In addition, previous work mostly follow Rubin’s potential outcome framework. However, our idea is inspired from Pearl’s structure causal models, where we can explicitly model the data generated environment to ensure compatibleness between the generated and observed data. Relation with counterfactual data augmentation.Counterfactual data augmentation stems from the human introspection behavior. It has been recently leveraged to alleviate the training data insuciency problem in the machine learning community. In the past few years, this idea has achieved great successes in the elds of neural language processing (NLP) [41] and computer vision (CV) [2,6,12]. In this paper, we apply it to the top-N recommendation task, which, to the best of our knowledge, is the rst time in this eld. The major dierences between our framework and the previous work is: (i) we design a leaning-based intervention method to encourage the informativeness of the generated samples. (ii) We theoretically analyze the noisy information in the generated samples, and design a tailored noise control method. Relation with ranking based recommendation model.Ranking based recommender models have been widely studied for the Top-N recommendation task. The most famous model in early years is Bayesian personalized ranking (BPR), which is optimized by maximizing the user preference margin between the positive and negative items. This method has inspired many promising models. For example, CKE [38] proposes a hybrid model to integrate collaborative ltering and knowledge base for recommendation. [10] improves BPR with a better negative sampler which leverages additional data in E-commerce. AMF [17] applies adversarial training method to enhance the performance of BPR. With the ever prospering of deep neural network, recent years have witnessed the surge of neural recommender models [7,8,15,18,32]. For example, NeuMF [18] is designed to capture the non-linear user-item correlations. NGCF [33] and LightGCN [16] regard the user-item interactions as a graph, and explicitly incorporate the structure information to enhance the utilization of the collaborative ltering signals. By looking back the history of ranking based recommendation models, it is evident that the model architectures are becoming deeper and heavier. While the comprehensive parameters can indeed lead to improved recommendation performance, an unprecedented problem is also emerging, that is, more training data is needed to satisfy the heavy architectures, which contradicts with the sparse user behaviors in realities. In this paper, we propose a solution to this problem based on causal inference, which is parallel with the previous model-drive research. In this paper, we propose to enhance Top-N recommendation with Pearl’s causal inference framework, where we can simulate user preference and generate counterfactual samples for alleviating the training data insuciency problem. We design a learning-based intervention method for discovering the informative samples, and conduct theoretical analysis to reveal the relation between the number of generated samples and the potential model prediction error. Extensive experiments based on both synthetic and real-world datasets are conducted to verify our model’s eectiveness. This paper actually opens the door of incorporating Pearl’s causal inference framework into the recommendation domain. There is still much room for improvement. For example, one can design more advanced exogenous node structures to introduce more reasonable prior knowledge for dierent recommendation scenarios. In addition, it should be also interesting to incorporate side information into the structure equation models, which can help to capture more comprehensive user preference and obtain more accurate recommender simulators. 7.1.1 Proof of theory 1. Proof.Without loss of generality, we suppose𝜂 >. We dene the random variable𝑋as the𝑖th observation of the relation between𝑖and𝑗. We use𝑋=1 to represent𝑖 > 𝑗, and𝑋=0 to indicate𝑖 < 𝑗. Suppose, we have N observations, and we deneÍ 𝑋 =𝑋. Following the voting mechanism, if𝑋 <, then the prediction is wrong, since it contradicts with𝜂 >. Obviously, According to the Hoeding’s inequality [29], we have: 𝑝 (𝑋 <12) = 𝑝 (𝑋 − 𝐸 [𝑋 ] <12− 𝜂) < exp (−2𝑁 (12− 𝜂)). (14) 7.1.2 Proof of theory 2. Proof.For a hypothesis𝑓 ∈ F, suppose its prediction error is𝑠Í (i.e.,I(𝑔≠ 𝑔) = 𝑠), then the mis-matching probability between the observed and predicted results comes from two parts: •The observed result is true, but the prediction is wrong, that is, 𝑠 (1 − 𝜁 ). •The observed result is wrong, but the prediction is right, that is (1 − 𝑠)𝜁 . Thus, the total mis-matching probability is𝜁 +𝑠 (1−2𝜁 ). Suppose the prediction error of𝑓(i.e.,𝑠) is larger than𝜖, Then, at least one of the following statements hold: •The empirical mis-matching rate of𝑓is smaller than𝜁 + •The empirical mis-matching rate of the optimal𝑓is larger than 𝜁 +. These statements are easy to understand, since if both of them do not hold, we can conclude that the empirical loss of𝑓is larger than that of𝑓, which does not agree with the ERM denition. However, according to the Hoding inequality [29], both of the above statements hold with the probability smaller than𝛿, which implies that the prediction error of𝑓is smaller than𝜖with the