Scientic publishing heavily relies on the assessment of quality of submitted manuscripts by peer reviewers. Assigning a set of matching reviewers to a submission is a highly complex task which can be performed only by domain experts. We introduce RevASIDE, a reviewer recommendation system that assigns suitable sets of complementing reviewers from a predened candidate pool without requiring manually dened reviewer proles. Here, suitability includes not only reviewers’ expertise, but also their authority in the target domain, their diversity in their areas of expertise and experience, and their interest in the topics of the manuscript. We present three new data sets for the expert search and reviewer set assignment tasks and compare the usefulness of simple text similarity methods to document embeddings for expert search. Furthermore, an quantitative evaluation demonstrates signicantly better results in reviewer set assignment compared to baselines. A qualitative evaluation also shows their superior perceived quality. reviewer assignment, recommendation system, expertise modelling Peer review is a popular method of ensuring scientic standards for conferences and journals. It requires the assignment of suitable experts for each submission, which is often done manually [4]. These reviewers then provide objective assessment of the manuscript and recommend accepting or rejecting the submission [6].All of this has to be performed in a tight time frame [3]. The continuously increasing number of submissions as well as the high complexity of the task even for experienced chairs of program committees or journal editors calls for fully automatic methods of expert assignment. Furthermore, it is not sucient to focus on the quality of single reviewers, but a good set of complementing reviewers should be recommended for each manuscript. The reviewer assignment problem tackles the task of retrieving sets of suitable reviewers for manuscripts submitted to a venue. Even though the construction of sets of reviewers tting submitted manuscripts has been studied frequently, most work focuses on construction of sets with the highest possible expertise but does not consider (m)any other aspects. Such other aspects could help reduce reviewers’ work load and increase comprehensiveness of reviews. Additionally, actual human evaluation of the sets and thus a reliable conrmation of results is generally not conducted. Numerous works [3,6,8,9,14,17,22,24] tackle the reviewer assignment problem in dierent ways, with slightly dierent denitions for the suitability of reviewers. While expertise of a reviewer with the topic of the manuscript [3,6,8,9,14,22,24] has been dominating in existing work, other features like authority [8,14], research interest [8] and diversity [6,14,17] were considered in some existing work, but not in a holistic way. Additionally, these aspects were dened heterogeneously in present works. We incorporate the following ve aspects into our denition of suitability of reviewer sets: expertise of reviewers in general topics and methods of a submission, authority of reviewers in the domain of the manuscript, diversity in terms of reviewers diering in their areas of expertise, interest of reviewers in the topics of the submission and diversity in terms of seniority aspects of the reviewer set. In this work, we embark on nding the best reviewer sets for a submitted scientic paper from a predened candidate pool in terms of these ve aspects. Unlike some existing work [1,18], we explicitly do not require the manual denition of keywords or bids on manuscripts from reviewer candidates. To achieve this, we make two important contributions: 1) We propose and thoroughly evaluate RevASIDE, a new and completely automated technique for recommending sets of reviewers from a xed set of candidates for single manuscripts. For this we introduce seniority as a completely new aspect and its combination with already established but redened features. 2) We publish three dierent data sets suitable for expert search as well as reviewer set assignment. While we build on established expert retrieval methods to nd reviewers with high expertise, our method is the rst to incorporate all of the complementary factors authority, diversity, interest of candidates and seniority to solve the reviewer set assignment problem. Our approach consists of two steps. Step 1 identies topically relevant reviewers based on the similarity of their research direction to the manuscript, utilising expert search methods. Step 2 then assembles sets from these experts and determines the reviewer set that performs best in the ve aspects. To the best of our knowledge, this is the rst work that utilises the expert search task as a preparatory step for the reviewer set assignment task. Retrieval-based approaches for scientic reviewer assignment treat the manuscript for which reviewers are searched as a query. They determine tting reviewers based on dierent aspects, often under additional constraints. Such methods can be divided into ones recommending single reviewers for manuscripts, so-called expert search, and those tackling the assignment of whole reviewer sets. Several papers target the recommendation of single reviewers for manuscripts which contrasts our goal of recommending reviewer sets. We identify and assemble the best tting experts to a suitable set while the following works only handle the expert search task which disregards set eects. Numerous works pursue the expert search task as a matching problem between the query manuscript and expert proles formed by their past publications. Some of them also consider more aspects than textual similarity: MINARET [19] is a recommendation framework based on publications and aliations of experts as well as expanded keywords for manuscripts. After an initial ltering step, it returns ranked list of reviewers. Candidates receive a score based on topical coverage, impact, recency, experience in reviewing and their familiarity with the target venue. Chughtai et al. [4] suggest ontology-based and topic-specic recommendation of single experts tting a submission. Macdonald and Ounis [15] propose twelve voting techniques to nd suitable experts for query manuscripts. These techniques base on similarity of the reviewer candidates and the manuscripts. We use and extend their methods in Step 1 of our approach. Other works transform single expert nding into a classication problem: Yang et al. [23] base their approach on word-semantic relatedness via Wikipedia. Reviewers are ranked with respect to a manuscript by experience in the domain of the submission and their number of papers. Zhao et al. [26] utilise word embeddings of keywords from author proles and manuscripts to propose tting reviewers. Similar to this approach we use embedding methods to abstract from words while searching for reviewer candidates. Reviewer set recommendation can be observed for single papers or multiple/all papers of a venue. The following approaches tackle reviewer set recommendation but consider dierent or fewer aspects compared to RevASIDE for estimating the quality of reviewer sets. Ishag et al. [6] incorporate theℎindex of reviewers, citation counts and paper diversity into their approach based on itemset mining. They return reviewer sets tting a query manuscript and estimate the sets’ impact. Contrasting their denition of diversity which uses the number of dierent aliations of authors of a single paper, we dene diversity as a measure between authors to estimate the actual topical dierences in reviewer sets. Maleszka et al. [17] tackle the reviewer set assignment problem for one manuscript at a time by focusing on diversity aspects in expertise, the co-authorship graph and style of reviewers. They begin the set recommendation process with a single reviewer determined by another method. Zhang et al. [24] utilise a multi-label classier for the construction of reviewer sets. The approach bases on predicted research labels for manuscripts and predicts reviewers with similar labels. Set-based eects are ignored which contrasts our approach. Works tackling the reviewer set recommendation for multiple papers can be divided in ones relying on manual inputs such as bidding by reviewers and fully automated ones. Some of the papers incorporating manual inputs, contrasting our fully automated method, are the following: The Toronto Paper Matching System (TPMS) [3] conducts automatic reviewer assignment for all manuscripts submitted to a conference by using either word count representation or LDA topics, but can also incorporate reviewers’ bids on submissions. TPMS supports some constraints: papers must be reviewed by three reviewers, and reviewers are assigned not more than a given limit of papers. Reviewers for manuscripts are determined based on expertise extracted from their publications. TPMS is applied, for example, by the SIGMOD research track [1], where reviewers upload a representative set of their publications. Papagelis et al. [18] present a system which incorporates reviewers’ interests in terms of paper topics, their bids on papers, conicts of interests and overall workload balance for the reviewer assignment task. It can either assign reviewer sets automatically if the bidding is completed or the PC chair can manually adjust the sets. The following works are fully automated recommendation approaches intended to work with multiple manuscripts: Liu et al. [14] recommend𝑛reviewers for each manuscript which are dependent on each other. They model reviewers’ expertise, authority and diversity as a graph which they traverse with random walk with restart. The number of co-authorships is modelled as authority which contrasts our denition of authority. Kou et al. [9] introduce an assignment system for sets of𝑛reviewers which bases on the topic distributions of reviewers and the manuscripts computed with the Author-Topic Model. They dene expertise of reviewer sets in certain topics as the maximum expertise for the topic found in the set; our denition of expertise deviates. Jin et al [8] assume reviewers have a certain relevance in a topic which is determined by their publications and usage of the Author-Topic Model. Additionally, authority in form of citations and research interest of researchers are important factors. Here, the number of reviewers per paper and the maximum number of papers a reviewer is assigned to can be predened. Amongst others we also observe these factors but dene them dierently. Yang et al. [22] utilise LDA to represent manuscripts as well as past publications of reviewer candidates. They then use a discrete optimisation model which focuses on expertise to assign reviewers to all manuscripts. Likewise, we also incorporate LDA in our approach but we additionally consider more aspects beyond expertise. In our work, we assess the appropriateness of a reviewer set with respect to a submission based on the following seven aspects: Aspect 1Reviewers in a reviewer set cannot have conicts of interests: they can be neither authors of the submission nor prior coauthors of its authors [18]. This aspect aims at ensuring unbiased and objective candidates. While we (as well as others [18]) regard this aspect quite vigorously, less restrictive variants (e.g. disallowing co-authorships in the three years prior to the submission) are also feasible. Aspect 2Reviewers cannot be co-authors of any other reviewer in the set. Reviewers having disjoint publications enforces a broader spectrum of dierent backgrounds. This could produce broader reviews [17] which is a desirable property in peer review [1]. Aspect 3Reviewers need to be experienced in the area of the manuscript [9]. The topic of the paper should be relevant for them and t their research prole. Not only the content but also the number of papers in the area of a submission contributes to our understanding of experience. This aspect ensures deep reviews, another desirable feature of assessments [1]. Aspect 4Reviewers need to hold authority in the research area of the submission. Reviews of the papers have to be credible, reviewers should be well recognised in the target domain [14]. Authority can be assessed, for example, by an area-dependentℎindex and citation counts of candidates. Aspect 5Reviewers need to be diverse in their area of expertise. Typically, as many topics as possible of a submission should be assessed to create a comprehensive review [1]. Reviewers that are procient in dierent topics from each other support this goal as the candidates in a set have unique perspectives formed by their dierent experiences and backgrounds [17]. Aspect 6Reviewers need to be currently interested in the topics of the manuscripts so they accept the reviewing request [8] and are not asked to review topics they no longer work in. Scientic progress makes it impossible to be up to date in all areas they were formerly interested in. Thus, time-aware suggestion should weigh recent works of reviewers much higher than older publications. Aspect 7Reviewers of a manuscript should not solely consist of senior researchers, but they need to be diverse with respect to the amount of their experience. Senior researchers provide vast reviewing experience and a global vision but they should be handled as a sparse resource as they are asked to review many submissions. Junior researchers are ambitious and resilient while not having that much experience. Usually, they are less frequently asked to review and more of an unexhausted resource. Reviewing load needs to be distributed between senior and junior researchers such that the lower load for senior researchers and incorporation of newer researchers benets the overall quality of reviews. Additionally, junior researchers could provide new and refreshing perspectives while the reviewing activity might also benet their own development. Breaking up well-established reviewer constellations with new candidates could also avoid research cliques [3]. RevASIDE is a system for assigning sets ofReviewers utilising Authority,Seniority,Interest,Diversity andExpertise of reviewers to nd the most suitable reviewer set out of a xed set of candidates, the reviewer candidate pool𝑅𝐶𝑃, for a given manuscript𝑀. Our approach is composed of two steps: in Step 1, suitable reviewers are identied from the pool of reviewer candidates; in Step 2, they are assembled to the most suitable set for the manuscript. Figure 1 depicts the schematic overview of our approach. Step 1 handles the left part of Figure 1. We represent publications as tf-idf vectors or ones constructed with BERT [5] or Doc2Vec [12], which allows to depict semantics of documents instead of single tokens. This enables capturing similarity of concepts of papers. Let𝑀be the manuscript for which a reviewer set should be computed. We ignore any reviewers for which a conict of interest with the authors of𝑀exists (Aspect 1). For the remaining reviewers from the reviewer candidate pool𝑅𝐶𝑃, let𝑃 (𝑅)be the set of publications written by reviewer𝑅. The similarity between a publication𝑃and a manuscript𝑀is given by𝑠𝑖𝑚(𝑃, 𝑀); the utilised similarity measure Figure 1: Schematic overview of our approach. The left part depicts the expert search task, the right part depicts the set of reviewers assignment task. Table 1: Voting techniques V T and accompanying formulas for reviewer 𝑅 and manuscript 𝑀. 𝑆𝑈 𝑀𝑠𝑖𝑚(𝑃, 𝑀) 𝑆𝑈 𝑀𝑠𝑖𝑚(𝑃, 𝑀) 𝐵𝑜𝑟𝑑𝑎𝐹𝑢𝑠𝑒(|𝑃 (𝑅)| − 𝑟𝑎𝑛𝑘(𝑃, 𝑀))Í 𝑒𝑥𝑝|𝑃 (𝑅)| ∗𝑒 can be changed between the two steps. In our experiments, we will use the cosine similarity of the corresponding vectors. We then sort 𝑅’s papers in descending order by their similarity to the manuscript 𝑀and denote by𝑟𝑎𝑛𝑘(𝑃, 𝑅, 𝑀)the rank of a certain publication𝑃 of reviewer𝑅in this order. Similarly, we sort all publications in the collection in descending order by their similarity to manuscript𝑀 and denote by𝑟𝑎𝑛𝑘(𝑃, 𝑀)the rank of a publication𝑃in this order. To obtain a ranked list𝑅𝐿of reviewers, we apply a number of voting techniques (VTs) that score reviewer candidates with respect to a manuscript. These voting techniques base on the ones applied by Macdonald and Ounis [15] for expert search. Table 1 shows the exact formulas for the 13 voting techniques considered in our approach. Higher scores signal better t of a reviewer to the given manuscript.𝑉𝑜𝑡𝑒𝑠computes the number of papers of a reviewer with a similarity to the query manuscript not smaller than a threshold𝛿; note that the method was introduced without such a threshold in [15], which corresponds to𝛿 =0 in our denition. 𝑆𝑈 𝑀sums up the similarities of the papers of a reviewer with the query manuscript,𝐴𝑉 𝐺uses this score and normalizes it by the total number of papers of the reviewer.𝑀𝑁𝑍multiplies the 𝑆𝑈 𝑀score by the number of papers of the reviewer.𝑆𝑈 𝑀sums the similarities of the𝑛papers of the reviewer most similar to the manuscript.𝑀𝐼 𝑁returns the smallest similarity of the reviewer’s paper with the manuscript,𝑀𝐴𝑋is dened analogously.𝑅𝑅sums up the reciprocal ranks of the reviewer’s papers in the ordered list of all papers. We additionally introduce𝑚𝑅𝑅which normalizes this score by the number of papers written by the reviewer.𝐵𝑜𝑟𝑑𝑎𝐹𝑢𝑠𝑒 utilises Borda-fuse as score. The three voting techniques𝑒𝑥𝑝, 𝑒𝑥𝑝and𝑒𝑥𝑝are dened as their non-exponential forms but instead of using similarities, they apply the exponential function on similarities. For a xed voting technique, this step generates a ranked list𝑅𝐿 of reviewers, i.e. experts, tting the manuscript in question. Step 2 handles the right part of Figure 1, i.e. the actual formation of reviewer sets for manuscript𝑀based on the ranked list𝑅𝐿of reviewers generated in Step 1. We denote the top𝑘reviewers from 𝑅𝐿by𝑅𝐿; if𝑘 = |𝑅𝐿|, the rst step becomes irrelevant. A smaller 𝑘restricts the observed candidates in the second step drastically and is especially useful to improve runtime. We now represent documents by term-based vectors weighted with tf-idf and by topic-based vectors computed with LDA [2]; this allows us to capture concrete terms as well as general topics of publications of reviewer candidates and the submission.Additionally, these document vector representations allow us to easily weight and combine vectors of publications without destroying their expressiveness as each vector dimension represents a single token or topic which can be present in a document to a certain extent. This starkly contrasts BERT or Doc2Vec embeddings, where single dimensions do not have a comprehensible semantics but instead the combination of all dimensions represents a document entirely. These tf-idf and LDA vectors can be constructed either on all parts of manuscripts or only on the technical sections, which consist of the methodology as well as the evaluation. For each reviewer𝑅this step considers the set𝑟(𝑅, 𝑀)of her publications whose similarity to manuscript𝑀is not lower than a threshold𝑡; i.e.𝑟(𝑅, 𝑀) = {𝑃 |𝑃 ∈ 𝑃 (𝑅) ∧ 𝑠𝑖𝑚(𝑃, 𝑀) ≥ 𝑡}, with 𝑡 ∈ [0,1]. The threshold is utilised to dene the selectivity of the research area relevant for the submission. If𝑡 =0 all papers of a reviewer are included, a value closer to 1 restricts the number of papers taken into account in the second step. We assume similarities lie in [0, 1]. Let𝑟𝑒𝑝(𝑃, 𝑉 )be the representation of publication𝑃as a vector of type𝑉 ∈ {𝐿,𝑇 }with𝐿representing LDA vectors and𝑇 representing tf-idf vectors. Both document vector representations (DVs) can be used to compute𝑟(𝑅, 𝑀), e.g. using the cosine of the corresponding vectors as similarity function. Lastly, let𝑃=be the length normalized aggregation vector of type𝑉that combines all information on relevant publications of a reviewer 𝑅 with respect to 𝑀. We now consider all possible candidate reviewer sets of a predened size (for example 3) and assess, for each candidate set𝑅, its suitability with respect to the aspects dened in Section 3. We prohibit reviewers in a set𝑅to be co-authors of each other (Aspect 2); sets that include such reviewers are not considered further, they are assigned a nal𝑠𝑐𝑜𝑟𝑒of 0. In addition, we observe ve dierent quantiable aspects for suitability for each such set𝑅of reviewer candidates. These reviewers are taken from𝑅𝐿produced in Step 1. Scores for all aspects are normalised to[0,1]with 1 being the best and 0 being the worst possible value. 4.2.1 Expertise E. Expertise describes the relevance of the reviewers in a set to the manuscript (Aspect 3). Reviewers should have solid knowledge with terms and topics of the manuscript substantiated by numerous publications. Particularly, the submission should be similar to publications written by the reviewers [14] and their number of such papers should be high. Contrasting Liu et al.’s work [14] we use the number of co-authorships of reviewer candidates not as an indicator of authority but rather as an indicator of expertise. These conditions are measured by the following scores: These scores are then linearly combined to the nal expertise score, with 𝜖∈ [0, 1] weighting parameters and 𝜖+ 𝜖+ 𝜖= 1: 𝐸(𝑅, 𝑀, 𝑡) = 𝜖𝐸(𝑅, 𝑀, 𝑡) + 𝜖𝐸(𝑅, 𝑀, 𝑡) + 𝜖𝐸(𝑅, 𝑀, 𝑡) 4.2.2 Authority A. Reviewers should hold authority in the area the manuscript belongs to (Aspect 4). We propose two scores to measure authority: the averageℎindex of reviewers [14]ℎ(𝑅, 𝑀, 𝑡 ) calculated on papers relevant to the manuscript𝑟(𝑅, 𝑀)(measured by𝐴), and the average number of their obtained citations on these papers (measured by 𝐴): 𝐴(𝑅, 𝑀, 𝑡) =|𝑅| · maxÍ𝑐 (𝑃) with𝑐 (𝑃)being the number of citations a paper𝑃has obtained. These scores are then linearly combined to the nal authority score, with 𝛼 ∈ [0, 1] a weighting parameter: 𝐴(𝑅, 𝑀, 𝑡) = 𝛼𝐴(𝑅, 𝑀, 𝑡) + (1 − 𝛼)𝐴(𝑅, 𝑀, 𝑡) 4.2.3 Diversity D. We dene diversity as a measure to ensure that the expertise of reviewers is distributed to areas as disjunct as possible (Aspect 5). This allows for reviews to cover multiple aspects of the manuscript. The corresponding score rewards if topics in which reviewers are procient overlap as little as possible [14]: 𝐷 (𝑅, 𝑀, 𝑡) = 1 −|𝑅| · (|𝑅| − 1)/2 4.2.4 Interest I. As research objectives of scientists change over time, interest measures the t of reviewers and the manuscript with respect to their temporal development (Aspect 6). Interest of reviewers denotes their willingness to review submissions from certain areas [8]. These interests change over time. If a reviewer was involved in a topic several years ago but then changed her focus, she probably no longer follows the rapid developments in the former research area. Thus she might not be willing or even able to review current submissions from this area. To represent the time-aware proles of reviewers, we combine the publications of reviewers with regards to their age to a length-normalized vector where recent papers are weighted stronger than older ones. This measure works on topical representations of documents: with 𝑎(𝑃) describing the age of a publication 𝑃 in years. 4.2.5 Seniority S. In terms of seniority, reviewer sets are desirable which do not solely consist of senior researchers (Aspect 7). In the recommended group of candidates, at least one senior researcher should be contained who is familiar with the methodology of the paper [1] (measured by𝑆). Further it is desirable to have a diverse group in terms of seniority, the set should include at least one junior researcher (measured by𝑆). These requisitions are modelled in the following equations: 𝑆(𝑅, 𝑀, 𝑡) = 𝑚𝑖𝑛𝑚𝑎𝑥𝑟𝑎𝑛𝑔𝑒(𝑅, 𝑀, 𝑡)𝑞𝑢𝑎𝑛𝑡𝑖𝑙𝑒𝑟𝑎𝑛𝑔𝑒(𝑅, 𝑀, 𝑡), 1 with𝑟𝑎𝑛𝑔𝑒(𝑅, 𝑀, 𝑡) =1+ 𝑚𝑎𝑥𝑎(𝑃) − 𝑚𝑖𝑛𝑎(𝑃) denoting the temporal range in which reviewer𝑅has published on topics relevant to𝑀. These scores are then linearly combined to the nal seniority score, with 𝜎 ∈ [0, 1] a weighting parameter: 𝑆 (𝑅, 𝑀, 𝑡) = 𝜎𝑆(𝑅, 𝑀, 𝑡) + (1 − 𝜎)𝑆(𝑅, 𝑀, 𝑡) 4.2.6 Final Equation. We combine all of these ve quantiable aspects to obtain a single score𝑆𝐶for each reviewer sets. Good reviewer sets will have high values in all aspects; we thus multiply the per-aspect scores: 𝑆𝐶 (𝑅, 𝑀, 𝑡) =𝐴(𝑅, 𝑀, 𝑡) · 𝑆 (𝑅, 𝑀, 𝑡) · 𝐼 (𝑅, 𝑀, 𝑡) The candidate reviewer set𝑅achieving the highest𝑆𝐶is the most suitable one and recommended for the manuscript as result of Step 2. We will denote this result as 𝑅in the experimental evaluation. To evaluate our proposed reviewer set recommendation approach, we develop three novel evaluation data sets. We consider manuscripts from three dierent workshops and conferences of dierent size and thematic focus that took place in 2017, namely MOL, BTW, and ECIR. As it is practically impossible to obtain all papers submitted to a conference, we use all accepted papers as an approximation instead. Note that this might lead to non-representative topic distributions of manuscripts and unrealistically low number of manuscripts to be reviewed. Additional fuzziness is introduced since we do not distinguish between long, short and demo papers as program committees are oftentimes published in a merged form. We built three dierent data setsbased on data from dblp [13] which was merged with abstracts, citations and references from the AMiner part of the Open Academic Graph [20,21]where available as well as full texts of accepted manuscripts. Information from AMiner was joined with dblp data (based on matching DOIs where available, or on matching paper titles, author names and publication years otherwise); this allowed to focus on publications from computer science or adjacent domains and to build rather precise reviewer proles due to dblp’s author disambiguation eorts, compared to using reviewer names only. Full texts of accepted manuscripts are not included in the AMiner data set but stem from pdfs collected by hand which were converted to text les using Science Parse. Information on program committees was either taken from conference web sites or conference proceedings. Reviewer names were manually mapped to dblp authors.For each reviewer, we set up a list of her publications identied by their dblp keys. Here, only papers up to 2016 were taken into consideration, corresponding to a reviewer selection process in early 2017. For each of the papers the data set contains its publication year, the paper length, the CORE rankof the venue it was published in, the number of citations it accumulated and the averageℎindex of its authors. The concatenated title and abstract (where available) of papers needed to consist of at least three terms to be considered for the data set. Citing papers which are not contained in dblp were omitted. Thus, the number of incoming links might not necessarily represent the number of citations which publications received in the real world. This inuences the number of citations and the average ℎ index. For each manuscript of our three test conferences the data sets contain a pool of possible reviewers. It consists of all members of the program committee, but excludes those with obvious conicts of interest accessible by (former) co-authorships of authors of the manuscripts and reviewers. For each of the papers published by possible reviewers, our data sets also contain tf-idf, Doc2Vec [12], LDA [2] and BERT [5] vector representations of its title and abstract where available. For submitted manuscripts, these four kinds of document representation are contained for the full text as well as only the research sections of the paper (which consist of all sections excluding the abstract, introduction, related work, conclusion, references and acknowledgements). The textual content of the papers is not contained. We consider only English documents for the construction of our data sets. We calculated the document frequencies of words for tf-idf on unstemmed titles of all publications contained in dblp up to 2016 concatenated with abstracts from AMiner where available which were written in English. In total we used 2,940,996 documents. The nal tf-idf vectors are calculated for unstemmed textual data available in the respective data sets including all papers of reviewers and submitted manuscripts. For the construction of BERT [5] vectors, we used the base pretrained uncased model.Since the BERT implementation used is only able to process input vectors of at most 512 tokens, documents were cut at punctuation marks or after half of the tokens if sentences were still too long. A sliding window was used to always input two consecutive sentences to maintain as much context as possible. The model consists of overall twelve hidden layers each having 768 features. The last four layers from these twelve layers were concatenated for each token and averaged over all tokens to receive vectors of length 4 layers×768 features = 3072 dimensions for each publication. [10] Weights for Doc2Vec [12] are trained on the English Wikipedia corpus from 1st February 2020. We refrained from using Doc2Vec on a stemmed corpus as this preprocessing is no prerequisite for achieving good results [12]. We trained two Doc2Vec models, one distributed bag of words (DBOW) and one distributed memory (DM) model, so that resulting vectors consist of 300 dimensions each. This size was proposed by Lau and Baldwin [11] for general-purpose applications.[10] For LDA [2] we again used the 2,940,996 documents which we already utilised for the computation of the document frequency in tfidf. This procedure ensured the computed topics were from the area of computer science. The number of topics was set to 100 resulting in the same number of dimensions for vector representations of manuscripts and publications.[10] MOL’17. The data set contains 12 manuscripts in English language which were accepted at Me eting on the Mathematics of Language ’17, 22 program committee members and their papers in dblp. We excluded extended abstracts. No distinction between dierent paper types and program committees was made. On average each manuscript has 21 possible reviewers which do not have conicts of interests. This data set represents a small biannual international conference with a dierent focus than the other two data sets. BTW’17. The data set contains 36 manuscripts in English language which were accepted at Datenbanksysteme für Business, Technologie und Web ’17 (the German database conference), 56 program committee members and their papers in dblp. We again excluded extended abstracts. No distinction between dierent paper types was made but the program committees members are split in scientic, industry and demo paper committee. On average each manuscript has 47.78 possible reviewers which do not have conicts of interests. This data set represents a medium sized biannual national conference with several lesser-known reviewers. ECIR’17. The data set contains 80 manuscripts in English language which were accepted at European Conference on Information Retrieval ’17, 151 program committee members and their papers in dblp. A distinction between full-paper meta-reviewers, full-paper program committee, short paper program committee and demonstration reviewers was made. On average each manuscript has 141.35 possible reviewers which do not have conicts of interests. This data set represents a medium to large annual European conference attributed with CORE rank A and mostly well-known reviewers. In our experiments, we solely focus on sets consisting of three reviewers even though our approach is applicable for dierent numbers of reviewers per manuscript as well. This number was chosen as a widespread norm [3] to reduce the dimensionality of further evaluation steps. We evaluate our approach on the three introduced data sets MOL’17, BTW’17 and ECIR’17 where we disregard the dierent manuscript and committee types. By observing the performance of our approach in venues of dierent sizes, we strive to make assumptions on its general applicability. We use Cosine similarity as similarity measure. This ensures similarity values in[0,1]for Step 2 as tf-idf and LDA document vector representations hold non-negative values for all dimensions. For the voting techniques of the algorithm we run tests with𝑛 ∈ {5,10} and 𝛿 ∈ {0, .25, .5, .9}. For all signicance tests, we use a𝑝-value of .05. We evaluate the normal distribution of values using Kolmogorov-Smirnov tests and test the homogeneity of variances with Levene’s tests. All depicted values are rounded on four decimal places. Considering the overall challenges and goals of RevASIDE, we investigate the following six hypotheses: 𝐻Step 1 is useful for the expert search task. 𝐻Usage of more advanced document vector representations leads to signicantly better overall results for Step 1 compared to more basic ones. 𝐻Utilisation of dierent document vector representations, voting techniques, cuto values𝑘of the result list𝑅𝐿, content types and thresholds𝑡leads to signicantly dierent overall RevASIDE scores and values for the ve quantiable aspects in Step 2. 𝐻Utilisation of the full texts of manuscripts leads to worse overall results than restriction of the manuscripts’ content to the technical sections in Step 2. 𝐻The conduction of Step 1 is protable for Step 2. 𝐻Results of Step 2 are conrmed by human assessment, thus RevASIDE is useful for the reviewer set assignment task. In this part of the evaluation we intend to assess hypotheses𝐻of Step 1 being useful for the expert search task and𝐻of utilisation of more advanced DVs producing better results. We randomly selected 20 manuscripts from each of the BTW’17 and ECIR’17 data sets. The manuscripts are represented by their full texts, the proles of reviewers are represented by their papers’ titles and abstracts where available. To create a ground-truth of relevant reviewers, the top 10 reviewer candidates are computed with all 13 (17 with variants) voting techniques and combined. The resulting pools of reviewers for each manuscript from the BTW’17 data set contained 48.35 entries on average and 101.5 entries on average for manuscripts from ECIR’17. In the former case, about all possible reviewers were contained in the respective lists contrasting the ECIR’17 lists which contain a lower percentage of possible reviewers. Unfortunately, a more extensive manual evaluation with more manuscripts would not be feasible. The manuscripts’ title and abstract as well as the potential reviewers and a link to their dblp prole were presented to an independent senior researcher in the eld who evaluated the reviewers in terms of appropriateness for the given manuscript. For the manual evaluation of relevance, only papers up to 2016 of reviewers were considered. The expert was not aware which method retrieved which reviewers. If the expert observed missing relevant reviewers, they were also included in the ground-truth. In BTW’17, each paper has 10.05 relevant reviewers on average (min=5, max=14, median=10, standard deviation=2.762). In ECIR’17, each paper has 27.2 relevant reviewers on average (min=3, max=55, median=25, standard deviation=13.5671). On average, a reviewer from the program committee is relevant for 3.5893 manuscripts for BTW’17 and 3.1813 manuscripts for ECIR’17. We report result quality with three established metrics, examining the rst 10 retrieved reviewers of each method. Precision@10 measures the fraction of the top-10 recommended reviewers that were actually relevant. Non-interpolated mean average precision@10 (MAP) averages the precision at ranks where a relevant reviewer appears, using a precision of 0 for each relevant reviewer not appearing in the result list. Normalized cumulative discounted gain (nDCG) [7] aggregates relevance of all reviewers appearing in the result, but with a logarithmic discount for later ranks; this follows the intuition that later ranks are less important to a user than earlier ranks. In addition, it normalizes this aggregation by the cumulative discounted gain achieved by an ideal ranking where all relevant reviewers appear in front, thus showing how close the result is to an optimal result and allowing to compare across dierent queries with dierent numbers of relevant results. The upper part of Table 2 shows result quality for all combinations of document vector representation and voting technique for the twenty manuscripts from BTW’17.𝑉𝑜𝑡𝑒𝑠is exactly the same for each document vector representation as this voting technique solely considers the number of papers of reviewer candidates and not their similarity with query manuscripts. The lower part of Table 2 shows the same for the twenty manuscripts from ECIR’17. In BTW’17, each paper has 2.7801 relevant reviewers per combination of VT and DV on average, in ECIR’17 this value is signicantly (Mann-Whitney𝑈test) higher (3.4838). These assessments lead to the assumption of the VTs and DVs presented here being useful for the expert search task and therefore verifying 𝐻. We found signicant (Kruskal-Wallis𝐻tests) dierences between the four DVs for several voting techniques, but not for all of them (see rightmost column of Table 2). The more advanced document vector representations Doc2Vec and especially BERT did not achieve better results than tf-idf. The best voting techniques seem to depend on the data set and the utilised document vector representation. BERT performs worse than both tf-idf and the Doc2Vec models. Usage of tf-idf and DM achieves comparable results for the best performing VTs for BTW’17; for ECIR’17, tf-idf and DBOW with their respective best VTs result in similar values. BERT seems to generalise the concepts of papers too much such that the VTs cannot clearly distinguish between relevant and non-relevant reviewers. This is underlined by the fact that three versions of𝑉𝑜𝑡𝑒𝑠generate the same values for MAP, P@10 as well as nDCG. Tf-idf has high selectivity and is able to identify experts versed in the exact same techniques described in a manuscript. Hence, hypothesis𝐻of more sophisticated VRs being more suitable than basic VRs is rejected. For the ECIR’17 data set, P@10 and nDCG are higher than for BTW’17. This might be caused by ECIR’17 having higher overall numbers of reviewers as well as more relevant reviewers per manuscript. This disadvantages the smaller BTW’17 data set. The evaluation of Step 2 of our algorithm consists of a quantitative and a qualitative evaluation. In Equation 1 we set𝜖= 𝜖= 𝜖= and 𝛼 = 𝜎 = .5. As a rst baseline𝐵, the three highest ranked reviewers in the ranked list𝑅𝐿for each VT and DV are considered as a reviewer set for a manuscript. Such an approach is common in reviewer set recommendation [14,24]. Our second baseline𝐵chooses three random reviewers from𝑅𝐿. Our third baseline𝐵chooses three random reviewers from the whole program committee, excluding only those with a conict of interest. For the latter, we cap values of 𝐸, 𝐴and 𝐴at 1. We experiment with cutos𝑘of reviewers in𝑅𝐿to generate 𝑅𝐿at position 10 and 20 after Step 1 and without cuto, i.e. all reviewers without conicts of interests for the manuscripts were utilised as a comparison to evaluate the usefulness of Step 1. If we do not restrict the number of candidate reviewers, i.e.|𝑅𝐿| = 𝑘, the voting technique used in Step 1 (which determines the reviewer candidates considered in Step 2) becomes irrelevant for Step 2 but still inuences the creation of the baselines. We also experiment with dierent thresholds 𝑡 ∈ {0, .25, .5, .9}. Table 2: Mean average precision@10 (MAP), precision@10 (P@10) and nDCG@10 (nDCG) for all combinations of voting techniques (VT) and document vector representations of manuscripts from BTW’17 (upper half) and ECIR’17 (lower half). Best combination in BTW’17: tf-idf + 𝑆𝑈 𝑀 (short 𝑏). Best combinations in ECIR’17: tf-idf + 𝑀𝑁𝑍 (short 𝑒 (short 𝑒). Column sig di gives information on whether or not MAP (m), P@10 and nDCG (n) signicantly dier between the dierent DVs. If ✓, all three measures are signicantly dierent. We divide the manuscripts in non-technical and research sections to better estimate their true content. Non-technical sections include abstract, introduction, related work, conclusion, acknowledgements and references. Research sections are all other parts. We compare the eect of using the full text in Step 2 to using only the content of research sections. Proles of reviewers are represented by their papers’ titles and abstracts where available which are similar enough (threshold 𝑡) to the query manuscript. 6.3.1 antitative Evaluation. In this part of the evaluation we focus on understanding the inuence of the dierent factors of our approach and prepare the qualitative evaluation by identifying the combinations achieving the highest scores. In this context we intend to assess hypotheses𝐻and𝐻as well as𝐻which observes the usefulness of Step 1. In these experiments, for each combination of document vector representation in Step 1, voting technique, cuto of relevant reviewers utilised in Step 2, similarity threshold𝑡in Step 2 as well as used Table 3: Signicant dierences between the groups in 𝑆𝐶 as well as the ve quantiable aspects by data sets MOL’17 (m), BTW’17 (b) and ECIR’17 (e). content type (CT) in Step 2 we observe the following result types (RT): the three baselines (𝐵,𝐵,𝐵) and the best result returned by RevASIDE (𝑅). We test for signicant dierences between groups of experiments to determine which factors really inuence the overall score 𝑆𝐶(as computed by Equation 1) and the ve quantiable aspects introduced in Sections 4.2.1 to 4.2.5. Kruskal-Wallis𝐻tests are used for the following experiments since in most of our observed cases, data is not normally distributed in the dierent groups or variances are not homogeneous. Table 3 indicates between which groups of experiments we found signicant dierences in the scores or the ve quantiable aspects. We observe 1,632 (4 DVs×17 VTs×3 cutos𝑘×2 CTs×4𝑡in Step 2) experimental setups per data set. Experiments were grouped by document vector type such that there were four groups of experiments, ones using tf-idf in the rst step, ones using Doc2Vec DM, ones using Doc2Vec DBOW and ones using BERT document vector representations. Grouping by VT in Step 1 results in 17 dierent groups of experiments. When experiments are grouped by the number of observed candidates 𝑘three dierent groups result. When grouping by content type, two groups of experiments result, ones which utilise the full text in Step 2 and ones utilising only the research sections of the query manuscript. Grouping by the threshold value 𝑡 in Step 2 results in four dierent groups. Lastly, grouping by RT produces four groups containing experiments of types 𝐵, 𝐵, 𝐵and 𝑅. DV does inuence some aspects signicantly but overall, the scores of the ECIR’17 data set are not signicantly inuenced by it. VT signicantly inuences the ve aspects for all data sets as well as the score for the two smaller ones. The content type which is utilised in Step 2 is signicantly inuential for values for all data sets except for authority, diversity, and seniority. These values are not calculated by directly utilising the query manuscript and therefore are not inuenced by the content type. The cuto value𝑘 which is chosen for𝑅𝐿, the threshold value𝑡as well the result types signicantly inuence the results in all three data sets. From these observations we derive the overall validity of hypothesis𝐻. Table 4 shows the best combinations of DV, VT, cuto values, content type and threshold, measured in terms of the highest overall average scores for𝑅and the three baselines𝐵,𝐵and𝐵for each of the three data sets.𝑆𝐶is calculated with Equation 1 and can take take values between 0 and 1 with 1 being the best. As it is multiplicative, a score of.05 can be reached if e.g. values of all quantiable aspects 𝐴, 𝑆, 𝐼, 𝐷, and 𝐸 are around .55. 𝑅achieves the highest𝑆𝐶results for each data set. This, together with the signicant dierences between result types observed in the previous experiment (see Table 3), leads to the conclusion that RevASIDE produces signicantly higher average𝑆𝐶scores than the baselines. This applies to all three dierent sized data sets which highlights the general applicability of our approach. Utilising full texts of query manuscripts yields better results than only taking the research sections into account. This leads to the rejection of hypothesis 𝐻. The restriction of𝑅𝐿to𝑘 =10 leads to the best average scores for MOL’17 and ECIR’17; for BTW’17, no restriction of𝑅𝐿leads to the highest average scores (not depicted in the table). This indicates that the reduction of the number of considered reviewers for Step 2 (and therefore the entirety of Step 1) is a major factor in small and large data sets. It also decreases the overall computation time which in general veries𝐻. MOL’17 as well as ECIR’17 represent relatively focused areas while BTW’17 is more diverse. For focused data sets it suces to regard the few most relevant reviewers to compose a suitable set but for a diverse conference, it seems more reviewers need to be considered. When grouping all 1,632 experiments by voting technique and threshold, the highest average scores for MOL’17 are achieved by𝑒𝑥𝑝and .5; for BTW’17𝑆𝑈 𝑀and .25; and for ECIR’17𝑆𝑈 𝑀and .9. BERT is the DV which on average performs best for each data set. They outperform the other VTs and thresholds on average but do not appear as a combination in Table 4 under the overall best congurations. Remarkably, the best results for𝑅in MOL’17 as well as BTW’17 were achieved by the same combination of DV, VT, CT,𝑘 as well as𝑡. The combination of BERT with𝑀𝐼 𝑁or𝑚𝑅𝑅did not achieve any good results in our manual evaluation of Step 1 but did prove to be useful in Step 2. The highest scores for MOL’17 (.0516), BTW’17 (.0462) as well as ECIR’17 (.0399) for the best performing combinations from Step 1 of our approach (𝑏: tf-idf + SUM,𝑒: tf-idf +𝑀𝑁𝑍,𝑒: DBOW +𝑉𝑜𝑡𝑒𝑠) are independent of DV and VT as they are achieved by|𝑅𝐿| = |𝑅𝐿|. The threshold𝑡is set to .5. These results cannot surpass the best congurations from Table 4 for the same data but also do not signicantly dier from them. For BTW’17 as well as ECIR’17, we found no signicant correlation between the scores produced by the twelve (eleven as𝑐=𝑐) best congurations from Table 4 for𝑅and the number of relevant reviewers per manuscript for the forty manuscripts observed in the evaluation of Step 1 with Kendall’s 𝜏. We want to point to the fact that some of the DVs and VTs present in Table 4 achieve low results in the evaluation of Step 1 (for BTW’17 .0168 to .171 in MAP, .08 to .34 in P@10 and .0622 to .3677 in nDCG; for ECIR’17 .0264 to .1116 in MAP, .135 to .45 in P@10 and .1353 to .4748 in nDCG). This hints at possible problems with aspects with opposing objectives which will be regarded in depth in the following qualitative evaluation. 6.3.2 alitative Evaluation. In this part of the evaluation we assess hypothesis𝐻which covers the manual assessment of the sets resulting from Step 2 and RevASIDE’s overall usefulness. In our rst qualitative evaluation of Step 2, we examine the eleven (as𝑐= 𝑐) congurations which performed best for the dierent result types from the three data sets (see Table 4) in the Table 4: Conguration (conf), DV, VT, 𝑅𝐿cuto value 𝑘, utilised content typ e and threshold 𝑡 resulting in the highest average scores and corresponding values for 𝐴, 𝑆, 𝐼 , 𝐷 as well as 𝐸 per data set and result type. Table 5: Average positions (pos) sets computed by the dierent congurations (conf) were ordered to in the qualitative evaluation as well as the average number of relevant reviewers (#rel) and the average position of entries from the dierent RTs per set. quantitative evaluation. For the forty (twenty from ECIR’17 and twenty from BTW’17) documents which were used in the rst manual evaluation, we compute lists of four reviewer sets for all congurations, consisting of one reviewer set produced by each of the three baselines as well as𝑅. We present the lists to an expert who then ranks the four entries according to suitability for the query manuscript from 1 (best) to 4 (worst), with the option of ties if two or more entries are equally suitable. Table 5 shows the average ranks of the result types in the evaluated lists for the two data sets, their average number of relevant reviewers per conguration and the average positions that entries from a specic RT achieved. For BTW’17, the combination achieving the best results is𝑐 (BERT,𝑀𝐼 𝑁,𝑘= 10,𝑡= .25) and (surprisingly)𝐵. For ECIR’17, the combination achieving the best results came from conguration𝑐 (BERT, 𝑚𝑅𝑅, 𝑘 = 20, 𝑡 = .9) and 𝑅. Overall,𝑅achieves the best results out of all combinations and data sets.𝐵generates the best results for BTW’17, but highly depends on the conguration as it also achieves considerably bad results, especially for the ECIR’17 data set. Although the results are greatly inuenced by the conguration,𝑅performs consistently well in general. The combination of conguration and result type achieving the highest number of mean relevant reviewers per data set is not the one achieving the best results in terms of positions, e.g. ECIR’17 +𝑐+𝐵. This leads to the conclusion that it is not sucient to consider only topical relevance in determining the most suitable combination. In both data sets, the RT achieving the best average positions is 𝑅. As data was not normally distributed in the dierent groups for both data sets, we used Kruskal-Wallis𝐻tests on positions of the four RT for the two data sets, which resulted in signicant dierences. We conducted Mann-Whitney𝑈tests on the positions of𝑅 and each of the three baselines resulting from all congurations together on the respective data sets. In the BTW’17 data set,𝑅 performed signicantly better than𝐵but no signicant dierences were found when compared to the two other baselines. In the ECIR’17 data set,𝑅performed signicantly better than all three baselines. In a second manual evaluation of Step 2, we examined the best combinations from Step 1 (𝑏,𝑒and𝑒) with CT = full, k = 20, t = .5 as the best performing combinations from Step 2 performed bad in Step 1. Table 6 was constructed exactly as described previously for Table 5. For both data sets, the best performing RT is𝑅. It achieves the best average position for all congurations together and the combination resulting in the best position is𝑒with𝑅. For ECIR’17, 𝑅also produces the best overall position for 𝑒. To better understand the impact of the ve aspects, a human assessor also evaluated the quality of the results with the best combinations from Step 1 with respect to each aspect, assigning a value between 0 and 1 for each aspect. Table 7 depicts average scores according to Equation 1 for combinations of the three best methods from Step 1 with all result types, manually assessed average values for the ve aspects and “manual” scores computed by multiplying the per-aspect values. In this evaluation, we wanted to compare the manually constructed scores to the automatic ones and evaluate possible eects of opposing aspects. We observe vast dierences in the manual scores𝑚𝑆𝐶and the computed𝑠𝑐𝑜𝑟𝑒𝑠, in almost all cases 𝐵achieves the highest𝑚𝑆𝐶. As we have already seen in Tables 5 and 6,𝑅generally achieves the best average positions for sets of reviewers. This discrepancy further underlines the suitability of our approach. RevASIDE produces reviewer sets based on calculated aspects which are preferable in a manual evaluation to the sets from 𝐵which achieved the highest𝑚𝑆𝐶in the manual assessment of aspects. We found a positive correlation of aspects𝑚𝐴and𝑚𝐼(.597 for BTW’17, .802 for ECIR’17) which is signicant with Pearson’s correlation coecient for both data sets. A higher authority might be equivalent to a higher number of papers, especially in the last seven years which might increase the probability of one of these papers being from the area of the manuscript and thus signals reviewers’ interest. Also for both data sets, the negative correlation between𝑚𝐼and𝑚𝐷(-.589 for BTW’17, -.72 for ECIR’17) is significant with Pearson’s correlation coecient. If reviewers in a set are very interested in a manuscript, it seems likely that the set is not as diverse. In BTW’17,𝑚𝐴is signicantly correlated with𝑚𝑆 (-.789), in ECIR’17 this negative correlation is not signicant with Pearson’s correlation coecient. This observation can be explained as sets having high authority normally consist solely of researchers with high seniority. We found opposing objectives coded into the aspects which might have lead to methods from Table 4 achieving low results in Step 1 but being useful in Step 2. In general average positions of sets from the dierent RTs are highly dependent on the conguration in BTW’17 and ECIR’17 for the best performing congurations in Step 2 but the overall best results are achieved independent of conguration by𝑅. From these observations we conclude that𝑅and thereby RevASIDE is a well-performing solution of the reviewer set assignment problem which is generally applicable. Thus, hypothesis 𝐻is veried. In this paper we proposed and evaluated RevASIDE, a method for assigning complementing reviewer sets for submissions from xed candidate pools. Our approach incorporates authority, seniority, interests of researchers, diversity of the reviewer set as well as candidates’ expertise. Additionally, we presented three new data sets suitable for reviewer set recommendation. In this context we examine the expert search as well as the reviewer set assignment tasks and show RevASIDE’s general applicability: for the rst task we revaluated expert voting techniques utilising dierent document representations. We veried the general usefulness of Step 1 for the expert search (addressed with hypothesis𝐻) and reviewer set recommendation task (addressed with hypothesis𝐻). Additionally, we have shown the suitability of simple textual similarity methods utilising tf-idf compared to more advanced techniques using BERT, which in terms rejected hypothesis𝐻. For the second task RevASIDE produces signicantly higher overall scores for reviewer set assignment compared to three baselines in an quantitative evaluation which shows the approach’s usefulness. In a qualitative evaluation we observed that sets assembled by our system are generally signicantly more suitable recommendations compared to our three baselines. We were able to conrm the results from the quantitative evaluation and thus veried 𝐻. Possible extensions might include weighting the dierent quantiable aspects dened in Step 2 of the approach and incorporating the venue which reviewers are recommended for. The number of assigned reviewers could be varied for each submission to take into account papers with broad content. Future work will focus on recommending suitable reviewer sets for whole venues. Here, the optimisation problem of single manuscripts is extended to include all manuscripts and several constraints such as individually diering maximal numbers of papers per reviewer come into consideration. Such an approach should also consider fairness [16] of the recommended reviewer sets. It would be interesting to observe gaps in the expertise displayed by the program committee in terms of t with submitted manuscripts together with suggesting new reviewers matching the missing criteria. Another feasible extension might be the recommendation of a program committee based on former and recent conferences and anticipated submissions. Here, topical development between years is important. Furthermore, explainability [25] of the recommended reviewer sets should be a priority. In our case, radar charts could be used for example to visualise the values which the sets achieved in the dierent quantiable aspects.