Abstract—Federated learning (FL) trains a machine learning model on mobile devices in a distributed manner using each device’s private data and computing resources. A critical issues is to evaluate individual users’ contributions so that (1) users’ effort in model training can be compensated with proper incentives and (2) malicious and low-quality users can be detected and removed. The state-of-the-art solutions require a representative test dataset for the evaluation purpose, but such a dataset is often unavailable and hard to synthesize. In this paper, we propose a method called Pairwise Correlated Agreement (PCA) based on the idea of peer prediction to evaluate user contribution in FL without a test dataset. PCA achieves this using the statistical correlation of the model parameters uploaded by users. We then apply PCA to designing (1) a new federated learning algorithm called Fed-PCA, and (2) a new incentive mechanism that guarantees truthfulness. We evaluate the performance of PCA and Fed-PCA using the MNIST dataset and a large industrial product recommendation dataset. The results demonstrate that our Fed-PCA outperforms the canonical FedAvg algorithm and other baseline methods in accuracy, and at the same time, PCA effectively incentivizes users to behave truthfully. Index Terms—Peer prediction, correlated agreement. Recent years have seen a large variety of mobile applications that have changed or improved our ways of communication, shopping, commuting, traveling, and lifestyle. To provide personalized services in these applications, machine learning techniques have been increasingly adopted. For that purpose, a common practice is to upload user data to a central server or cloud, which then trains a machine learning model to make predictions such as product recommendation. This centralized approach evokes many privacy concerns as users’ sensitive data could be eavesdropped by malicious parties during data transmission, or be misused by an untrusted server. To address this privacy issue, a new learning paradigm called Federated Learning (FL) [1] was proposed to perform distributed machine learning over a large number of devices Alibaba Group without requiring data to leave the devices or data owners. In the training process, each user trains a local model using her own data on her own device, and uploads the local model parameters instead of the original data to the server. The server then aggregates the received models into a global model and distributes the global model back to the users. The above steps repeat until the global model converges. By doing so, federated learning preserves user privacy and reduces network trafﬁc. As a result, it has attracted substantial attention from both academia and industry recently. Unlike traditional distributed machine learning [2], in which the machines for model training are fully controlled by a central server, federated learning works with autonomous mobile users who decide by themselves whether and how to participate in model training. This makes FL vulnerable to selﬁsh and malicious users who may manipulate the training process such as falsifying the model parameters or send random models without any training effort. Therefore, a critical issue in FL is to evaluate individual users’ contributions in the model training process so that strategic users can be detected and truthful users can receive rewards proportional to their real contributions. A popular approach, as recently proposed in [3]–[5], uses the Shapley value to measure user contribution in federated learning. This approach computes the marginal increase of average accuracy of the model due to the addition of data points contributed by a user in model training. However, this has two major drawbacks: 1) computing the Shapley value involves permutation operation which has an exponential computational complexity; even though some approximate methods have been proposed, the computation is still costly [4]; 2) more importantly, it relies on a representative test dataset to evaluate the model accuracy, but such a dataset is rarely available because no one knows which dataset perfectly mimics the distribution of future unseen data. To overcome these issues, we propose a data-free approach to evaluate user contribution in FL. This approach only uses users’ uploaded models and does not need any extra training or test data. However, there are several key challenges. First, the uploaded models (typically neural networks) have complex structures and the correlation among model parameters is too intricate to express. Second, there may exist dishonest or malicious users who falsify their data or even directly manipulate model parameters [6], and detecting such behavior is hard because the server does not have access to user data. These challenges set the problem of evaluating user contribution in FL distinct from the data quality evaluation problem in mobile crowdsensing [7] and crowdsourcing [8]. To this end, we propose a Pairwise Correlated Agreement (PCA) method to evaluate user contribution in FL. The basic idea is that, although the uploaded models are different among mobile users due to their non-i.i.d. (non-independent and identically distributed) data [9], we can still extract certain internal correlation between the models pairwise, and exploit the correlation using an idea based on peer prediction [10]. More speciﬁcally, we characterize user contribution by how much a model uploaded by a user can predict the models uploaded by the others via the internal correlations. With our PCA method, we apply it to two fundamental aspects of FL. First, we design a new model aggregation algorithm called Fed-PCA, which uses the user contribution evaluation results as the model weights for model aggregation performed by the server. This Fed-PCA algorithm accounts for quality of data, rather than just the user-reported data size (quantity); as a result, it offers signiﬁcant potential to improve the accuracy of the aggregated model, and counter the falsiﬁcation of user-reported data size. Second, we apply PCA to designing an incentive mechanism which is strategy-proof to the following undesirable user behaviors: (i) free riding, where a user randomly generates model parameters without performing the actual model training, and (ii) overly privacypreserving: adding excessive noises to model parameters and thus substantially degrading model prediction performance. In summary, our main contributions are as follows: to evaluate user contributions in federated model training without using a test dataset. For motivation purposes, we also demonstrate the importance of contribution evaluation by showing the potential harm of strategic user behaviors on federated model training using our experiments conducted on an industrial dataset. which uses the user contributions computed by PCA as the model weights in model aggregation. We also apply PCA to designing a strategy-proof incentive mechanism which resists two types of common strategic behaviors of users: free riding and overly privacy-preserving. MNIST dataset and an industrial dataset obtained from Taobao, one of the largest commercial mobile recommender system in China. The results clearly demonstrate the effectiveness of PCA in detecting users’ strategic behaviors. In addition, counter-intuitively, our Fed-PCA achieves equal or even better prediction accuracy as compared to FedAvg on different datasets, without knowing the training data size of each user. In the canonical FL framework [1], there is a set of users (clients) U and a central server. Each user i has a private local dataset D, such as the historical dataset of click behaviors related to goods or videos in mobile recommender systems. In each round t, k users are randomly chosen to participate in the model training, and are denoted by the set {1, ..., k}. At the beginning of round t, the server sends the global model Mto the selected k users, and each user trains a new local model Musing her own data D, and uploads the model update x= M− Mto the server. Besides the model update, each user also reports n, the size of training data used at round t. The server then calculates the weight ofP each user i as w= n/n, and aggregates the modelP updates by x=wx. Finally, the global model is updated as M= M+ x, which is sent back to users. The above process repeats until a stopping condition (e.g. a certain number of rounds) is met. As model compression with the quantization technique (e.g., [11]) is widely used in FL to reduce communication cost, we take each parameter of the model, indexed by p, to be discrete and ﬁnite without loss of generality, i.e., x∈ {1, 2, ..., h}. Note, however, that our proposed approach can be used for continuous parameter values as well, in which case we can perform an additional quantization of the model update on the server only for the contribution evaluation phase, and it does not affect the model aggregation (i.e., the model update of each user i does not need to be quantized in the model aggregation). In an ideal FL framework, each user would participate in the model training, upload her model truthfully. However, there may exist strategic users who can manipulate this process to achieve their own interests, e.g., by uploading a fake model updateˆx6= xwhere xis the true model update (omitting subscript t for notation simplicity). We account for two main types of strategic user behaviors in FL: 1) free riding [6], which generates random model parameters without actually training to save training costs, such as computing power and storage; and 2) overly privacy-preserving, which adds excessive noises to the model parameters for privacy protection [11]–[13]). To observe the effects of these strategic behaviors on the model training in FL, we have conducted two experiments on Deep Interest Network (DIN) [14], a deep learning-based click-through rate (CTR) prediction model, with an industrial dataset from Taobao. We adopt the classic performance metric in machine learning: area under the curve (AUC), and the baseline of AUC is 0.5. The detailed experimental setup can be found in Section V-B. In Fig. 1(a), we investigate the effect of free riders, where a free rider randomly generate her model parameter ˆx∼ N (0, σ), i.e., the Gaussian distribution with variance σ= 0.01. As shown in Fig. 1(a), just 25% free riders are enough to degrade the performance substantially. In Fig. Fig. 1: Performance in terms of area under the curve (AUC) with two types of strategic users: (a) free riders in different percentages, (b) privacy-preserving users in different noise scales σ. 1(b), we investigate the performance with privacy-preserving behaviors. Each user further strengthen the privacy by adding noises into the model parameters, i.e., ˆx= x+ N (0, σ), where N (0, σ) is Gaussian distribution with variance (noise scale) σ. The result in Fig. 1(b) shows that the AUC depends heavily on the noise scale σ: when σ= 0.001, the performance has a slight degradation, but when σ= 0.01 or larger, the AUC drops signiﬁcantly, which demonstrates the harmfulness of overly privacy-preserving behaviors (due to excessive noise). With these observations, a contribution evaluation method is highly needed to detect and suppress the free riding behaviors and overly privacy-preserving behaviors. To formulate the above strategic behaviors in a uniﬁed manner, we deﬁne the strategy of a user as F= P (ˆx= r|x= a) for any r, a ∈ {1, 2, ..., h} and parameter p, where r is the reported parameter value and a is the true value.In other words, the strategy is the probability of reporting r when the value of parameter p is a. Next, we formulate the above behaviors as informed and uninformed strategies. Deﬁnition 1 (Informed and Uninformed Strategies). A strategy is an uninformed strategy if it has F= Ffor any r, a, b ∈ {1, 2, ..., h}, otherwise is an informed strategy if there exists F6= Ffor some r, a, b, it is an informed strategy. Intuitively, the uninformed strategy captures the free-rider behaviors, where the user does not spend resources or effort in training the model, but just report some random parameter values. Hence, the probability of the reported values does not depend on the true values. On the other hand, the informed strategy captures the privacy-preserving behaviors where the user has indeed trained the local model using her local data, but then obfuscated the true parameter value xinto a reported value ˆxwith some probability. Note that truthfully reporting model updates is also an informed strategy, where F= 1 and F= 0, ∀r 6= a. First introduced in [15], the peer-prediction method is a classic mechanism for information elicitation problems without a ground truth, and has been employed in many scenarios, such as crowdsourcing [16] and peer grading [17]. The key idea of peer prediction is to compare the reported data of user i with that of other users. If their data satisfy some statistical correlation, i.e., the data of user i is predictive of the data of others, then her data is deemed to have a larger contribution, and vice versa. Peer prediction is naturally suitable for FL since there is no ground truth for the contribution evaluation. For consistency with the terminology used in the peerprediction literature, we also call each parameter in a FL model, indexed by p, a task. Each user fulﬁlls a task by training the parameter with her private local data (using, e.g., the gradient descent algorithm). Note that in FL, the task set of each user is the same since all the users are training the same model. We call the value after performing the training task, x, a signal, whereby each signal belongs to the set {1, 2, ..., h}. Next, we deﬁne a delta matrix ∆, which is an h × h matrix that captures the correlation between each pair of users on a certain task. We ﬁrst consider the case of homogeneous user, i.e., the delta matrix is the same for each pair of the users. Thus, we omit the user index for the delta matrix. Let P (a, b) denote the joint probability that one user gets signal a after training and the other user gets signal b on the same parameter, and P (a) and P (b) denote the corresponding marginal probabilities. An entry ∆(a, b) is then deﬁned as If ∆(a, b) > 0, we have that the signals a and b are positively correlated; if ∆(a, b) = 0, we have that a and b are independent; otherwise, they are negatively correlated. In addition, it can be easily veriﬁed that the sum of ∆(a, b) in each row or each column is always 0. We deﬁne Sign(∆(a, b)) = 1 if ∆(a, b) > 0, and Sign(∆(a, b)) = 0 otherwise. Without loss of generality, we assume that there is at least one element in the matrix that is non-zero. With the above deﬁnitions, we describe the correlated agreement (CA) method introduced by [10], [17] in the context of FL. Deﬁnition 2 (CA Method for Homogeneous Users). The CA method entails the following steps: 1) Randomly divide the parameters into a bonus parameter set Mand a penalty parameter set M. 2) For each user i and each bonus parameter p ∈ M, randomly pick a user j 6= i as the peer of i, and randomly choose two different penalty parameters q, q∈ Mfor users i and j, respectively. 3) The contribution or quality of parameter p of user i is evaluated by Q= S(ˆx, ˆx) − S(ˆx, ˆx), where the score matrix is S = Sign(∆), and user i’s contributionP Algorithm 1: ComputeDelta(i, j, A, B) Output: The delta matrices ∆, ∆. The intuition in Step 3 is that if the reported signals by user i and her peer j on the same parameter p exhibit a positive statistical correlation (e.g., they both report a as in the above example), we give user i a reward of 1. On the other hand, if their signals on two different parameters q and qare positively correlated, it suggests that the user i may have randomly uploaded an arbitrary signal for each parameter without actual training, so we give her a penalty of −1. III. PAIRWISE CORRELATED AGREEMENT (PCA) The above CA method assumes that all the users are homogeneous, and the delta matrix is the same for all the user pairs. However, one distinctive feature of FL is that users are heterogeneous with non-i.i.d. or imbalanced data [1]. For instance, in mobile recommender systems, the click behaviors of users can be substantially different. The work [18] extended the CA method to the heterogeneous users setting, which divides users into several groups and computes the delta matrix for each pair of groups. However, when there are a large number of heterogeneous users, which is likely to occur in FL, the number of groups would be quite huge, and then this method suffers from a prohibitive computing complexity for both the clustering procedure and the calculation of delta matrices. To address this issue, we propose a pairwise correlated agreement (PCA) method, and explores the underlying internal correlation of signal distributions to evaluate the contribution of heterogeneous users. Before introducing PCA, we make a reasonable assumption that the signal of different parameters are independent and identically distributed (i.i.d.), which is based on our observation from practical data sets. To validate this assumption, we again conduct two experiments on DIN with an industrial dataset from taobao. We ﬁrst test the Spearman’s Correlation [19] between the signals of two randomly chosen parameters to validate the independence hypothesis. Second, we conduct the Kolmogorov-Smirnov Test [20] between them to validate the hypothesis of identical distribution. The resulting p-values are 0.185 and 0.343, respectively, and both of them are larger than 0.05, which adequately supports the i.i.d. assumption. We Algorithm 2: Pairwise Correlated Agreement (PCA) Method for Heterogeneous Users Output: The contribution Qof each user i ∈ K. parameter set Mand penalty parameter set M. K\i as peers). note that this assumption is also used in many related works on sensitivity analysis of neural networks [21]–[23]. Under this assumption, we conduct Algorithm 1 to estimate the delta matrix for each pair of heterogeneous users. As shown later in the proof of Theorem 1, the estimation error could be arbitrarily small with a large number of samples. In algorithm 1, parameters are split into two sets, A, B, of the equal size, and we compute a delta matrix for each parameter set and each pair of user i and her peer j. The function 1(·) in Lines 2 to 4 denotes the indicator function. We use T(a, b) as the observed frequency of jointly reporting a, b from users i, j for the parameter set A, and T(a) and T(b) as the corresponding marginal probabilities (Lines 1 - 5). Lines 6 - 8 compute the delta matrix of each parameter set A and B. Then, PCA is given in Algorithm 2. For each focus user i, we randomly choose m peers (Line 3). With a larger m, the accuracy of Qcould be improved while the expectation remains the same since we would take the average among them. Then, for each parameter set A, B of user i and her peer j, we apply Algorithm 1 to obtain different estimated delta matrices ∆and ∆, respectively (Lines 5 - 6). Note that we use swapped statistical correlations, i.e., the delta matrices, to calculate the score matrix Q (Line 9). As such, the score matrix is independent of the reported parameters themselves. Algorithm 3: Fed-PCA: An improved FL aggregation algorithm using weights calculated by PCA Input: The set of users U, the initialized model M, Output: The global model M. Algorithm 2. Lastly, we compute the overall contribution of user i’s model update in Line 11. Intuitively, PCA estimates and takes advantage of the correlation between the uploaded models of heterogeneous users. In this section, we describe two potential applications of PCA: weight calculation in model aggregation and incentive mechanism design. In order to reduce the negative effects of low-quality uploaded models, we leverage the contribution value provided by PCA in model aggregation and propose the improved FL algorithm called Fed-PCA. In particular, Fed-PCA aims to achieve two goals: 1) to prevent falsiﬁcation of the data size n, which is a serious issue in FL since it directly affects the weights in model aggregation; 2) to improve the performance of FL as the traditional model aggregation does not take into account the model/data contribution of users. As shown in Algorithm 3, we map the value of user i’s contribution from the interval [−1, 1] to a non-negative weightP w= exp(αQ)/exp(αQ) in Line 8. Here, we use the exponential function with a controlled parameter α, which determines the variance of user weights in model aggregation. The local models are then aggregated into the global model using the new normalized user weights (Lines 9 - 11). Note that our contribution values could be used in not only the weighted averaging aggregation process as shown here, but also the recent median-based mechanisms proposed to defend Byzantine attacks [24], [25]. We evaluate the performance for both the averaging and median-based aggregation methods in Section V. We next discuss how to prevent the strategic behaviors through incentives, where an essential property for this issue is incentive compatibility. We say that incentive compatibility is satisﬁed when it is in all users’ best interests (obtaining the highest rewards) to report their true models, i.e., for any reportedˆx, where Udenotes the utility obtained by user i, which will be deﬁned later, and {ˆx}is the set of reported model updates of all the other users. We emphasize that incentive compatibility could be quite important for FL, because otherwise, users may upload falsiﬁed models and the learning problem would be ill-deﬁned. Next, based on the deﬁnition of informed and uninformed strategies in Section II-B, we introduce the concept of informed incentive compatibility and -informed incentive compatibility for the above deﬁned strategic behaviors. We denote by Iand {I}the truthful strategy of user i and that of other users, respectively. Deﬁnition 3 (Informed Incentive Compatibility and -Informed Incentive Compatibility). If for every user i and any informed strategies F, G, and some  ≥ 0, we have and for any uninformed strategy F, we have then the mechanism is -informed incentive compatible. If  = 0, the mechanism is informed incentive compatible. The deﬁnition of -informed incentive compatibility reﬂects that (i) if a user adopts an uninformed strategy, her contribution is strictly lower than that of truthful reporting; (ii) if a user adopts an informed (yet still untruthful) strategy, her contribution is at best  more than that of truthful behavior, where  is a small constant number. Therefore, an incentive mechanism with this property can prevent strategic users from uninformed or informed strategies. Aiming to illustrate the application of PCA in a clearest manner, we assume that the utility of each user is simply her received reward, regardless of other factors, that is, is the reward received by user i. It should be noted that PCA could be easily integrated into most of other existing incentive mechanisms, such as that in [26], [27], and more factors could be taken into account, e.g., the training cost, the privacy cost and the budget balance property. On top of this simpliﬁed utility model of users, we propose that the following simple mechanism could guarantee the -informed incentive compatibility: allocating a reward proportional to her contribution, i.e., R(ˆx, {ˆx}) = f(Q(ˆx, {ˆx})) where f could be any positive monotonic increasing function. We posit the following main theorem, Fig. 2: Results on MNIST with free-riding users: (a) Average weight assigned to free-riding users; (b) Test accuracy. which shows that this mechanism is close to informed incentive compatibility. Theorem 1. Let  > 0, and δ > 0. If the number of samples is g = O(9hlog(1/δ)/), then with a probability of at least 1−δ, the above incentive mechanism with heterogeneous users is -informed incentive compatible. We omit the proof due to space constraint. In this section, we report the evaluation results of our proposed approach and methods (PCA, Fed-PCA, and incentive mechanism) using the MNIST dataset and an industrial product recommendation dataset. Experimental Setting. We ﬁrst study the performance of Fed-PCA for the MNIST digit-recognition dataset using a multi-layer perceptron with a hidden layer of 100 units. ReLU activations and dropout technique are adopted in the experiments. The network contains a total of 159,010 parameters. Similar to [1], we ﬁrst sort the MNIST data by digit label, and divide them into 200 shards, each of which includes 300 data items. We assume there are 100 users in total in the FL system, each of which is randomly assigned two shards of data. This way, the data distribution among users is highly non-i.i.d. For the model updates, we choose a gradient quantization technique called cpSGD [11] to reduce the communication cost. In the FL training, k = 20 users are randomly chosen in one round. We adopt mini-batch stochastic gradient descent (SGD) with momentum as the optimization algorithm. The value of momentum is set to 0.5, the batch size is 10, the local epoch number in each round is 5, and the learning rate is 0.01. In the quantization process, we set h = 8, and X= 0.1. In PCA, the peer number m is 5, and the number of bonus parameters is |M| = 1, 000. The parameter α, which converts the user contribution into her weight, is set to 10. Regarding the free riders in the system, we assume the parameters they Fig. 3: Results on MNIST with (overly) privacy-preserving users: (a) Average weight assigned to privacy-preserving users. (b) Test accuracy with 25% privacy-preserving users. generate in their model updates to be ˆx= N (0, σ), where σis set to 0.01. In the experiments on privacy-preserving users, we assume that 25% of users add noises into their model parameters, each of which reports ˆx= x+ N (0, σ). We will test the impact of different noise scales σ. Experimental Results. We ﬁrst compare the performance of our Fed-PCA with the original FedAvg algorithm with different percentages of free riders. For clarity, we illustrate the user contributions using their corresponding weights, as calculated in Algorithm 3. Fig. 2(a) shows the average weight of free riders. When the percentage of free riders is no more than 20%, PCA evaluates their contributions (and hence weights) as nearly 0, while FedAvg constantly assigns them a weight of 0.05 (recall there are 20 users). This result demonstrates that free riders are detected accurately by PCA, while further implying the effectiveness of our incentive mechanism. When the percentage of free riders increases to 50%, their average weight increases accordingly, but is always lower than that of FedAvg. This is because that the model quality of peers decreases with the percentage of free riders. Note that we could assume most users in FL are likely to be truthful, which provides a reference to detect free riders. This is a mild assumption in real life and has been widely adopted in previous literature [6], [28]. In Fig. 2(b), we study the test accuracy with different percentages of free riders. It is depicted in the ﬁgure that Fed-PCA decisively outperforms the FedAvg algorithm. The reason is that PCA detects free riders successfully and, thus, assigns them low weights in the model aggregation process. We also note that in the normal case without any free riders or privacy-preserving users, the accuracy of Fed-PCA is 0.9421 after 100 rounds, which is quite close to the baseline of FedAvg: 0.9458. The impact of the noise scale σof privacy-preserving users is shown in Fig. 3. Fig. 3(a) explores the average weight of privacy-preserving users. When the noise scale σis no less than 0.05, the average weight of privacy-preserving users is nearly 0, while a lower noise scale leads to a higher weight. This result demonstrates that PCA is capable of detecting users who add large noises, but ignores the slight noises that do not affect the performance of FL, which is a good characteristic Fig. 4: Results on industrial dataset: (a) Average weight assigned to free-riding users. (b) Average weight assigned to privacy-preserving users. Fig. 5: Comparison of different model aggregation methods. of the corresponding incentive mechanism. In Fig. 3(b), we show the test accuracy of Fed-PCA and FedAvg. The accuracy of FedAvg degrades seriously with the increase of σ, but our Fed-PCA remains nearly unaffected by users with large noises because they are assigned very low weights. Experimental Setting. We next evaluate the performance of our Fed-PCA on an industrial dataset from Taobao, one of the largest a mobile recommendation system of products in China. In the dataset, there are 30-day impressions and click logs of users (dated from June 15 to July 15, 2019). We use the Deep Interest Network (DIN) [14] as the machine learning models. We refer the readers to [14] for more details on DIN and the dataset. In the training of FL, the batch size is set to 2, the epoch number is set to 1, and the learning rate is initialized as 1.0 with an exponential decay rate of 0.999. In the quantization process, we set h = 256, and set Xas 0.1 multiplied by the learning rate in the round. In PCA, the parameter α is set to 100 if not otherwise stated. The other parameters are the same as those of the experiments on the MNIST dataset. Experimental Results. We ﬁrst explore the average weight of free riders and privacy-preserving users in Fig. 4. In Fig. 4(a), we can see that when the percentage of free riders is no more than 30%, all of them are detected in PCA and, hence, are allocated merely 0 weight in the aggregation. When the percentage increases to 50%, Fed-PCA still has the ability to detect them to some extent; thus, their average weight is about half that of truthful users. We investigate the impact of privacy-preserving users in Fig. 4(b). A clear noise scale boundary of 0.001 is found, above which the user would be detected and punished with a low weight. These results clearly suggest the effectiveness of PCA in the application of incentive mechanism design. Then, we test the AUC with different aggregation methods in the system consisting of exclusively truthful users in Fig. 5. As median-based aggregation methods has attracted much attention in recent years as resistant to the attacks of malicious users [24], [28], we conduct experiments on both averaging and median aggregation. In the ﬁgure, the black line (Med) is the performance of the unweighted median-based aggregation method, and the red line (Med-PCA) represents a mix of PCA and the weighted median methods. It is depicted that the unweighted median-based aggregation method underperforms all other approaches since more information of the model updates from the users are lost due to the median operation, compared with the averaging operation. PCA helps alleviate this problem by allocating larger weights to the users with better performance in the median operation. Surprisingly, we can observe that, under the premise of preventing free riders and overly privacy-preserving users, the performance of FedPCA clearly surpasses all other aggregation methods, including FedAvg. This means that the contributions and, hence, the weights calculated by PCA are even more reasonable and effective than the traditional weights directly calculated by data sizes, in absence of the information about local datasets. We conjecture that this is because of the difference between the synthetic MNIST dataset and the industrial dataset. In MNIST, each data item has approximately the same value for the learning problem, and hence the data sizes could be used as a good proxy of weights in model aggregation. But in the industrial dataset, real users may provide many useless data items for the learning problem, such as unintended activations. Therefore, a well-designed weight assignment approach has the potential to evaluate the true contribution of each user, and hence to outperform the traditional FL algorithm. The FL framework was ﬁrst introduced by Google [1], [29] as a new paradigm of distributed machine learning, and it has been applied in a virtual keyboard named Gboard [30], [31]. Some recent works have taken incentive mechanisms of FL into account [3], [4], [26], [27], [32], [33]. For example, [34] proposes an incentive mechanism for FL using the contract theory, by considering the computation and communication costs of training the model. These studies focused on the costs of users, while the contribution of each user to the FL platform is simpliﬁed as a sandbox or a linear function of her cost, which is not so practical in real life. Some other work [3]–[5] account for the contribution using the Shapley valuebased techniques. However, as stated above, the computation of Shapley values suffers from high time complexity and the requirement of a representative test dataset in FL, which poses obstacles for fair contribution evaluation. Closely related works are [35], [36], which adopt peer prediction to compare the output of the updated models of users on the test data, and thus a representative test dataset is still required in their proposed approaches. Our work tackles this problem by leveraging the technique of peer prediction on the model parameters, whereby the contribution of each user is evaluated without either training data or test data. In this paper, we have proposed a pairwise correlated agreement (PCA) method to evaluate the contributions of users in federated learning, without requiring a test dataset. We have then applied PCA in (1) weight calculation for more robust model aggregation, where we have designed Fed-PCA as a better alternative to FedAvg, and (2) incentive mechanism design for FL. Extensive experiments are conducted using the MNIST dataset and a large industrial product recommendation dataset. The evaluation results demonstrate the effectiveness of our proposed approach in terms of both detecting strategic user behaviors and improving prediction accuracy.