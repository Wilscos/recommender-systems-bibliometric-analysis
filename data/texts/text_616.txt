Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate deﬁnitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model’s prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared. Focusing on text classiﬁcation and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and shortcuts for BERT and LSTM models and demonstrate that some of the most popular method conﬁgurations provide poor results even for simplest shortcuts. We recommend following the protocol for each new task and model combination to ﬁnd the best method for identifying shortcuts. A prominent class of explainability techniques assign salience scores to the input features, which reﬂect the importance of the features to the model’s decision. When applied to text classiﬁers those methods produce highlights over the input (sub)words. Interestingly, different methods may produce surprisingly dissimilar highlights. Figure 1 shows this using the Language Interpretability Tool (Tenney et al., 2020). So a natural question is: which method should one use? While a method whose highlights happen to look plausible may facilitate a task like text annotation (Pavlopoulos et al., 2017; Strout et al., 2019; Schmidt and Biessmann, 2019), many salience methods seem to be Figure 1: Salience maps produced by four common methods on a sentiment classiﬁcation example (SST2) for a BERT model. The same token (eastwood) is assigned the highest (Grad-L2, LIME), the lowest (GxI) and a mid-range (IG) importance score (color intensity indicates salience; blue and purple stand for positive, red stands for negative weights). A developer investigating a hypothesis about speciﬁc named entities being associated with the label would probably be unsure as to whether the example provides support for or against the hypothesis. motivated by the debugging scenario where faithfulness to the model’s reasoning is a requirement (Jacovi and Goldberg, 2020). Indeed, known success stories from input salience methods in domains other than language are similar in that they teach us a lesson of not trusting a classiﬁer based on its stellar performance on a standard test set. In the medical domain, for example, heatmaps over images helped uncover so-called shortcuts (Geirhos et al., 2020) or spurious correlations between data artifacts like doctor marks or tags and the predicted disease(Codella et al., 2019; Sundararajan et al., 2019; Winkler et al., 2019, inter alia). Spurious correlations plague NLP models too (Gururangan et al., 2018; McCoy et al., 2019; Rosenman et al., 2020). Hence, helping model developers improve both the model and the data by making shortcuts apparent is indeed a strong use case for faithful input salience methods. How can we know if a method consistently places the shortcut tokens on top of its salience rankings? Evaluating this is challenging, because we usually do not know the shortcut in advance and the model parameter space is large. Moreover, we don’t have an inherently interpretable view into the predictions of common black-box neural models. Glass-box models with explicit mediating factors (Camburu et al., 2019; Hao, 2020) are not widely used or synthetic, and model-native structures such as attention have been shown to have weak predictive power (Bastings and Filippova, 2020). Alternatively, one can make strong assumptions about what a ground truth should be like and compare salience rankings with what is expected to be the ground truth. In this vein human reasoning (Poerner et al., 2018; Kim et al., 2020), gradient information (Du et al., 2021), aggregated model internal representations (Atanasova et al., 2020), changes in predicted probabilities (DeYoung et al., 2020; Kim et al., 2020) or surrogate models (Ding and Koehn, 2021) all have been taken as a proxy for the ground truth when evaluating salience methods. Unfortunately, they also resulted in divergent recommendations so the question of what the ground truth is and which method to use remains open. Unlike the cited work we argue for a faithfulness evaluation methodology which makes use of partially synthetic data to obtain the ground truth and which is moreover also contextualized in a debugging scenario (Yang and Kim, 2019; Adebayo et al., 2020). Towards the goal of identifying salience methods which would be most helpful in revealing shortcuts learned by a model we make the following contributions: •We propose a methodology and two metrics for evaluating salience methods which allows one to formulate a hypothesis (e.g., my model may learn simple lexical shortcuts, like an ordered sequence of tokens, to predict the label) and identify the salience method most useful for discovering such shortcuts. •We demonstrate that a method’s conﬁguration details (e.g.,L1or dot-product, logits or probabilities, choice of baseline) may have a signiﬁcant effect on its performance. •We conduct a thorough analysis of a range of conﬁgurations of the four most popular salience methods for text classiﬁcation demonstrating that conﬁgurations dismissed as being suboptimal may outperform those claimed to be superior. We desire two properties from any faithful salience method which is claimed to be helpful for model debugging: high precision and low rank, which we deﬁne as follows: Precision@kis a measure over the top-k tokens in a salience ranking where k is the shortcut size. Withs,mandxdenoting a salience method, a trained modelmand the ith example from the synthetic setDand assuming two functions,top(·) andgt(·)which output the top-k tokens from a salience ranking and the ground truth, respectively: p@k(s) =|top(s, m, x) ∩ gt(x)|k|D|(1) Mean rankrepresents how deep, on average, we need to go in a salience ranking to cover all the ground truth tokens: rank(s)=arg min(|top(s, m, x)\gt(x)|)|D| Intuitively, precision tells us how many of the important tokens we will ﬁnd if we focus on the top of the ranking while rank indicates how much of the ranking is needed to ﬁnd all the important tokens. The protocol we use to obtain ground truth importance rankings and to assess the faithfulness of a salience method comprises the following steps (cf. Fig. 2): 1.Deﬁne a shortcut type that you would like an input salience method to discover and decide on how this shortcut is to be realized. The simplest example is a single-token lexical shortcut where token presence determines the label. We motivate other shortcut types in Sec. 2.2. 2.Create a partially synthetic variant of a real dataset by augmenting it with synthetic examples. These are examples sampled from the original data with the shortcut tokens inserted and with the label determined by the shortcut. Also create a fully synthetic test set where every example has a shortcut and the label predictable from it. 3.Train two models of the same architecture on the original and on the partially synthetic datasets, use the respective validation splits for evaluation. Both models should perform comparably on the original, unmodiﬁed test set (green in Fig. 3). 4.Verify that the shortcut tokens can indeed be assumed to be the ground truth of token importance for the model trained on the mixed data (by measuring accuracy). See §2.4. 5.Generate a token salience ranking from every input salience method we would like to evaluate. 6.Compute the faithfulness metrics by comparing the top of a ranking with the ground truth (shortcut tokens). Below we give more details on Steps 1, 2 and 4. A shortcut can be deﬁned as a decision rule that a model learned from its training dataset which is not expected to hold under a slight distribution shift. While it is not possible to adequately characterize the full spectrum of thinkable shortcuts, one can identify common shortcut types which one anticipates to be learnable from a dataset. For example, it has been shown that a model can perform well on an NLI task by focusing only on the hypothesis and ignoring the premise (Poliak et al., 2018; Belinkov et al., 2019), or even predict the correct label from negation words or other annotation artifacts (Gururangan et al., 2018; Geva et al., 2019). In a similar vein McCoy et al. (2019) deﬁne three heuristic types characteristic of NLI datasets. We focus on binary classiﬁcation tasks and experiment with the following shortcuts that are applicable to many tasks: Single token (st):The simplest possible and still realistic shortcut is a single token lexical heuristic where the presence of a token determines the classiﬁcation label. E.g., #0 and #1 indicate whether the label is 0 or 1. Token in context (tic):A more realistic lexical shortcut, which may be considerably more difﬁcult to spot by a human but is still trivial to learn for a deep model, makes use of more than a single token. For example, two tokens determine the label together but not separately. We implement a tokenin-context shortcut where the class indicator tokens (#0 or #1) only determine the label if yet another special token is present in the same input (contoken) but not on their own. Ordered pair (op):Yet another property of natural languages that a model can easily make use of is the order: a combination of tokens is predictive of the label only if the tokens occur in a certain order but not otherwise. We implement an ordered pair shortcut in its simplest form. That is, for an indicator token pair, (#0, #1), the order of the tokens determines the label so that " ... #0... #1..." has label 0 and " ... #1... #0..." has label 1. In other words, the ﬁrst indicator token "gives away" the label. Again, neither of the indicator tokens, #0 and #1, is predictive of the label if occurring individually. Needless to say, many more shortcut types, corresponding to the heuristics they encode, can be easily deﬁned. For example, a model making use of the exact count can be useful in arithmetic tasks (e.g., a token is indicative of a class if it occurs exactly three times in the input). The distance between two signal tokens may be taken into account by a model (e.g., two tokens are indicative Figure 3: Illustration of how the ordered-pair shortcut is introduced into a balanced binary sentiment dataset and how it is veriﬁed that the shortcut is learned by the model. The model trained on the mixed data (A) is still largely a black box, but since its performance on the synthetic test set is 100% (contrasted with chance accuracy of model B which is similar but is trained on the original data only), we know it uses the injected shortcut (highlighted text). of a class if they are within ﬁve tokens from each other). In our work we do not claim to cover the most prominent types of all thinkable shortcuts because not enough is yet known about what the models actually learn from the standard NLP datasets. However, based on the knowledge accumulated so far and the cited references we do believe that the phenomena we model here for lexical shortcuts – namely, context and order – are representative of what current NLP models learn and also representative of the poor generalization patterns of these models. The methodology itself can be easily extended to other shortcut types, as long as it makes sense to visualize the shortcut with a highlight over the input. 2.3 Creating (Partially) Synthetic Data To ensure that the shortcut deterministically indicates the right label, we deﬁne shortcuts over tokens absent from the original dataset and introduce them explicitly in the vocabulary. This guarantees that the shortcut is unambiguous with regard to the label and its signiﬁcance to the model increases. Assuming a sentiment classiﬁcation dataset and the ordered pair shortcut mentioned in Sec. 2.2 (the procedure is analogous for other data-shortcut combinations), we create a synthetic example by (1) randomly sampling an instance from the source data, (2) randomly deciding on the order of the shortcut tokens, (3) inserting these tokens at random positions, obeying the order and (4) setting the label as the shortcut prescribes. This process is illustrated in Fig. 3 (top left side). We also inject one of the two tokens at random into a part of the original data without modifying the label so that the tokens occur also in non-synthetic examples (bottom left of Fig. 3). The datasets we create are intentionally mixed and consist of the real and synthetic data to approximate real use cases where the model has to extract both simple and complex patterns to perform well. This is different from fully synthetic datasets (Yang et al., 2018; Arras et al., 2019) or glass-box DNN models (Hao, 2020) where it is guaranteed that the model uses certain input features but the ﬁndings may not be valid for real datasets. Two tests verify that the model indeed uses the shortcut tokens and that they must be most important to the model: 1.The model should achieve close to 100% accuracy on the fully synthetic test set.This would imply that it learned the shortcut and consistently applies it on unseen data (hence the "transparent corner" of the top black box in Fig. 3). 2.The model trained on the original data (the bottom black box in Fig. 3) should perform at chance level on the same fully synthetic test set. This would imply that it is indeed the shortcut data and the shortcut rules that are needed to achieve 100% accuracy. We use three text classiﬁcation datasets and apply the three shortcuts presented above to each of them. Despite all the datasets being binary and of comparable size, there are a few differences which may affect a salience method’s performance: • SST2(Socher et al., 2013) is a balanced sentiment classiﬁcation dataset with short (20 tokens on average) inputs; • IMDB(Maas et al., 2011) is also a balanced sentiment classiﬁcation dataset with inputs about ten times longer than in SST2; • Toxicity(Wulczyn et al., 2017) is a varied length dataset containing toxicity annotations on Wikipedia comments where 9% of examples are positive (i.e., toxic). Aside from being imbalanced, it differs from the other two in that a text is toxic if it contains a single toxic phrase while for a movie review it is the dominating sentiment which determines the label. In the results section we use the following format to refer to a dataset-shortcut combination: SST2:tic, IMDB:op, Toxicity:st, etc. 3.1 Models We apply the salience methods to explain the predictions of two popular models: a bi-LSTM model (Schuster et al., 1997) which uses GloVe embeddings (Pennington et al., 2014), and BERT (Devlin et al., 2019). Since we only consider binary tasks, the predicted probability of classc ∈ {0, 1}is given by the sigmoid function: wheref(·)denotes the model output for classc andxis an input ofntoken embeddings. Both models embed input tokens with a trainable layer so that everyxis a continuousd-dimensional embedding vector of the i-th input token. The models’ accuracy on all the source datasets are presented in Table 1. The minimum and mean accuracy on all the nine fully synthetic test sets are 99.8 and 99.95 for LSTM and 99.7 and 99.91 for BERT (100% in most cases). 3.2 Salience Methods We consider four classes of input salience methods and the Random baseline (RAND) to obtain Table 1: Test accuracy on the three source datasets. per-token importance weights: Gradient (GRAD*), Gradient times Input (GxI*), Integrated Gradients (IG*) and LIME. 3.2.1 Gradient Li et al. (2016) use gradients as salience weights and compute a score per embedding dimension: To arrive at the per-token scores(x), Li et al. (2016) take the mean absolute value or theLnorm of the above vector’s components. Poerner et al. (2018) and Arras et al. (2019) use theLnorm, while Pezeshkpour et al. (2021) use the mean, referencing Atanasova et al. (2020). Note that instead offone can compute the gradient of the ﬁnal layer, that is, in our case the sigmoid function. An argument for starting from the probabilities is that, unlike logits, probabilities contain the information on the relative importance for a particular class. To our knowledge, the effect of using probabilities or logits has not been measured yet. In sum, we have six variants of the GRAD method: GRAD. 3.2.2 Gradient times Input Alternatively, one can compute salience weights by taking the dot product of Eq. 4 with the input word embeddingx(Denil et al., 2015) and obtain a salience weight for token i: Also here we can compare the probability and the logit versions: GxI. 3.2.3 Integrated Gradients Integrated gradients (IG) (Sundararajan et al., 2017a) is a gradient-based method which addresses the problem of saturation: gradients may get close to zero for a well-ﬁtted function. IG requires a baselinebas a way of contrasting the given input with information being absent. A zero vector (Mudrakarta et al., 2018), the average embedding or UNK or [MASK] vectors can serve as baseline vectors in NLP. For input i, we compute: That is, we average overmgradients, with the inputs tofbeing linearly interpolated between the baseline and the original inputxinmsteps. We then take the dot product of that averaged gradient with the input embedding xminus the baseline. In addition to the variable number of steps–small (100) or large (1000)–and the baseline (zero vector or model-speciﬁc UNK / [MASK]), also here we can start either from probabilities (i.e.,σ) or logits (i.e., f) and arrive at eight different IG conﬁgurations: 3.2.4 LIME Ribeiro et al. (2016) train a linear model to estimate salience of input tokens on a number of perturbations, which are all generated from the given examplex. A perturbation is an instance where a random subset of tokens inxis masked out using either UNK (LSTM, BERT) or [MASK] (BERT) tokens. We also experimented with dropping tokens instead of masking them. This lead, on average, to worse precision results than masking. Therefore, we concentrate on masking here. The text model’s prediction on these perturbations is the target for the linear model, the masks are the inputs. Following Ribeiro et al. (2016) we use an exponential kernel with cosine distance and kernel width of 25 as proximity measure of instance and perturbations. We keep beginning and end-of-sequence tokens unperturbed and experiment with the number of perturbations (100, 1000, 3000). This results in 3 respectively 6 conﬁgurations for LSTM/BERT: Tables 6 and 7 in section Appendix A.3 present the results for all the models, shortcut types and source datasets in terms of precision and rank. In this section we highlight our main ﬁndings. A method’s performance varies across model and shortcut types and other dataset properties. It is apparent that GxI performs quite well for LSTM models but does not work at all for BERT models (Tab. 2). Conversely, GRADperforms very well for BERT but not at all so for LSTM models (Tab. 3). Overall, method performance mostly goes down on longer inputs. More interestingly, performance may change with the shortcut type: single-token shortcut for LSTM but drops to .76 or even .35 on the same base dataset with a two-token Thus, even if the model is ﬁxed, it cannot be assumed that a certain method works well and would be useful for ﬁnding lexical shortcuts learned by the model in general if its evaluation was done on only one shortcut type. LSTM models for ﬁnding shortcuts.For BERT models, GRADachieves high precision and rank scores across the different datasets and shortcut types, being 0.99 or higher on seven out of nine datasets (Tab. 3). The lowest but still comparatively high precision (0.87) is on IMDB:tic where the inputs are particularly long. For LSTM models, on six out of nine datasets the precision of the same method is around .5 ( in Tab. 3). It does not matter whether probabilities or logits are used and whether L1 or L2 norm is applied. We hypothesize that one reason for the difference in performance between BERT and LSTM is that BERT models have residual connections, making the gradient information less noisy. However, the results of GRADare very poor, ranging between .3 and .4 in precision ( in Tab. 3). Note that GRADis sometimes deemed unsuitable because it is unsigned and only returns positive scores (Pezeshkpour et al., 2021), but we show that it can be useful for ﬁnding shortcuts. Using probabilities instead of logits only changes the results for IG.For other gradientbased methods it does not seem to make a large creasing number of steps.Increasing the number of interpolation steps from 100 to 1000 does not result in a signiﬁcant improvement for LSTM models. Also for BERT, the precision numbers improve only for the tic shortcuts and only when probabilities are used (last two rows in Tab. 4). Another observation to make is the similarity of the scores between the GxI and IG when using the zero baseline ( in Tab. 4): this means that there is no difference between taking a single or 100(0) steps from the zero baseline. Table 2: GxI results across different models and datasets. Here and in the following tables P stands for Precision and R stands for Rank. Colors and boldface mark the results that are mentioned in the Results section. st: single token, tic: token in context, op: ordered pair. The choice of the baseline has a strong effect on the BERT results for both rank and precision. For the most part using the [MASK] baseline (with logits) resulted in an improvement in the scores (for forming conﬁguration of IG the results are much worse than GRAD. Number of perturbations as well as masking token matter for LIME.LIME beneﬁts from 1000 over 100 perturbations, especially for longer inputs and/or shortcuts. We found that the increase from 1000 to 3000 perturbations leads to little precision improvements for the input lengths in our datasets. Using UNK as masking token leads to better results than [MASK] in almost all conﬁgurations ( in Tab. 5). We hypothesize this is due to two reasons: (i) The [MASK] token is not trained during ﬁne-tuning on the task data. (ii) The UNK token, however, is ﬁnetuned (due to unknown tokens and as special token in word dropout). Rank and precision give complementary information. Both are useful measures. For example, the precision of GRADand GRADis close on Toxicity:op (.56 and .60) while the rank of the latter is almost twice as big (13 and 21) ( in Tab. 3). Lower rank with comparable precision means that the method consistently puts one of the shortcut tokens on the top but buries the other token deep in the ranking. Research on input salience methods for text classiﬁcation is proliﬁc and diverse in terms of the deﬁnitions used (Camburu et al., 2020), applications (Feng and Boyd-Graber, 2019), desiderata (Sundararajan et al., 2017b), etc. The importance of getting faithful salience explanations has been recognized early on (Bach et al., 2015; Kindermans et al., 2017) and there exist formal deﬁnitions of explanation ﬁdelity (Yeh et al., 2019). However, these have not been connected to model debugging where it is the top of a salience ranking that matters most. In the vision domain, our work is closest to Adebayo et al. (2020), who also explore the debugging scenario with salience maps, Yang and Kim (2019), who use synthetic data to obtain the ground truth for pixel importance, and Hooker et al. (2019), who contrast the performance of the same model trained on original and modiﬁed data when evaluating feature importance. As pointed out in Introduction, in NLP faithfulness evaluation has often been grounded in strong assumptions (Poerner et al., 2018; DeYoung et al., 2020; Atanasova et al., 2020; Ding and Koehn, 2021) or by analyzing models substantially different from the ones normally used (Arras et al., 2019; Hao, 2020). An exception to this trend is the work by Sippy et al. (2020) who also modify source data but, unlike us, consider MLP as the only DNN model, do not evaluate any gradient-based methods and analyze single token shortcuts only without strong guarantees of them actually being the most important clues for the model. Also Zhou et al. (2021) analyze DNN models on intentionally corrupted data: they primarily focus on vision but also run an experiment analyzing how faithfully the attention mechanism points at the words known to correlate with the label. Finally, Madsen et al. (2021), following Hooker et al. (2018), iteratively remove tokens to evaluate faithfulness of salience methods for LSTM models and conclude that there is no single winner and that performance is very much task-dependent. We come to the same conclusion. Concurrently with our work, Idahl et al. (2021) argue for faithfulness evaluation on synthetic data for model debugging but do not report experimental results. Similarly to them and also concurrently with our work, Pezeshkpour et al. (2021) go further and combine data and input attribution methods to discover data artifacts. However, citing prior work, they use GRADand IGwhich, as we have shown, are sub-optimal conﬁgurations for BERT models. This explains the very poor accuracy of 12-13% (in our terms: precision@1) that they observed when discovering single-token shortcuts in SST2. We have argued for evaluating input salience methods with respect to how helpful they would be for discovering shortcuts that are learned by the model. This seems to be a clear use case from the model developer perspective. To achieve this, we proposed a protocol for method evaluation and applied it to three shortcut types (single token, token in context, and ordered pair) which are a proxy for shortcut heuristics that occur on common NLP tasks. By comparing the performance across different datasets, shortcut types and models (LSTMbased and BERT-based), we demonstrated that a strong performance for one setup may not hold for a slightly different combination of the three parameters. Finally, we have pointed out that some method conﬁgurations assumed to be reliable in recent work, for example integrated gradients, may give very poor results for NLP models, and that the details of how the methods are used can matter a lot, such as how a gradient vector is reduced into a scalar. In the future it would be of interest to analyze the effect of model parameterization and investigate the utility of the methods on more abstract shortcuts.