huaizepeng2020@ia.ac.cn,jhtao@nlpr.ia.ac.cn,chefeihu2017@ia.ac.cn,dawei.zhang@nlpr.ia.ac.cn,guohua.yang@nlpr.ia.ac.cn (Since KDD requires the papers that have been submitted to arXiv at least one month prior to the deadline need a dierent title and abstract, here we give a brief version of title and abstract.) Knowledge Graphs (KGs) have shown great success in recommendation. This is attributed to the rich attribute information contained in KG to improve item and user representations as side information. However, existing knowledge-aware methods leverage attribute information at a coarse-grained level both in item and user side. In this paper, we proposed a novel attentive knowledge graph attribute network(AKGAN) to learn item attributes and user interests via attribute information in KG. Technically, AKGAN adopts a heterogeneous graph neural network framework, which has a dierent design between the rst layer and the latter layer. With one attribute placed in the corresponding range of element-wise positions, AKGAN employs a novel interest-aware attention network, which releases the limitation that the sum of attention weight is 1, to model the complexity and personality of user interests towards attributes. Experimental results on three benchmark datasets show the eectiveness and explainability of AKGAN. • Information systems → Recommender systems. Recommender systems have shown great success in e-commerce, online advertisement, and social medial platforms. The core task of recommender systems is to solve the overload information problem [31,32,40] and suggest items that users are potentially interested in. Traditional methods to achieve information ltering are content-based and collaborative ltering (CF)-based recommender systems[9,20,34], which utilize items’ content features and the similarity of users or items from interaction data respectively[7]. However, both of them don’t introduce much side information. In recent years, introducing knowledge graphs (KGs) into recommender systems as side information has been eective for improving recommendation performance. KGs represent real-world entities and illustrate the relationship between them with graph data structure, which can reveal multiple attributes of items and explore the potential reason for user-item interactions. Generally, a KG contains item nodes and attribute nodes, and attribute nodes can not only describe items’ attributes directly but also represent other attribute nodes’ attributes. For example, a movie knowledge graph (as shown in gure 1) has six types of nodes, where movie node (i.e., 𝑒) is item node and the others (i.e., 𝑒) are attribute nodes.𝑒represent the actor attribute of movie𝑒, which means actors𝑒stared in movie𝑒. And𝑒represent the singer attribute of song𝑒, which means singer𝑒sang the song𝑒. To integrate attribute information into recommender system, earlier works focus on embedding-based methods [1,3,12,45,46] and path-based methods [19,26,41–43]. The former exploits KGs with knowledge graph embedding (KGE) algorithm (i.e., TransE [2] and TransH [38]) to learn entity embeddings and then feed them into a recommender framework. The latter usually predenes a path scheme (i.e., meta-path) and leverages path-level semantic similarities of entities to rene the representations of users and items. However, both of them don’t capture high-order connectivities and fail to exploit both the rich semantics and topology of KGs. More recently, the propagation-based methods, a.k.a. graph neural network (GNN)-based methods such as KGAT[33], KGNN-LS[30], KNI[23], AKGE[25], KGIN[35], have attracted considerable interest of researchers. With the attribute information iteratively propagating in KGs, GNN-based methods integrate multi-hop neighbors into representations and have achieved the state-of-the-art recommendation results. Despite the success of GNN in exploiting multi-hop attribute information, we argue that there are still three shortcomings: (1)The pollution caused by weighted sum operation in merging different attribute information. Dierent attributes are independent in terms of semantics and user preference. Take the actor attribute𝑒and the singer attribute𝑒of movie𝑒in gure 1 as an example, semantics independence means𝑒doesn’t co-occur with𝑒in each movie, since an actor and a singer are invited to work for a movie respectively. Preference independence means whether a user prefers actor𝑒is independent with whether he likes singer𝑒. However, existing GNN-based methods pool embeddings from dierent attribute nodes with weighted sum (i.e., sum, mean, attention) operation, which brings about semantic pollution Figure 1: An example of how KG contains multiple attributes information. Best viewed in color. and the diculty to distill user interested attribute information. (2)The nonlinearity between the distance and importance of attribute node relative to item node. Usually, high-order neighbors are less relative to the center node in graph data, but this is not absolute. For example, singer attribute𝑒is more valued than director attribute𝑒by some movie viewers who like music, while𝑒, a 2-hop neighbor, is further than𝑒, a 1-hop neighbor. This problem is caused by inherent graph topology, which means some signicant attribute nodes don’t connect to item nodes directly, such that it requires multiple passes to integrate these high-order neighbors to the center node. However, existing GNN-based algorithms neglect this issue and decrease the weight of signicant high-order neighbors coupled with the increase of propagation times. (3)The complexity and personality of user interests towards dierent attributes. User interests show the following pattern: a user just gets interested in a part of rather than all attributes of an item, and dierent users prefer dierent attributes even towards the same item. For example, both user𝑢and𝑢watch movie𝑒because 𝑢likes𝑒’s actor𝑒rather than director𝑒while𝑢prefers the theme song𝑒rather than actor𝑒. Therefore, we should integrate actor𝑒rather than director𝑒attribute embedding into𝑢’s representation and integrate song𝑒rather than actor𝑒attribute embedding into𝑢’s representation. In other words, the item representation learned by GNN propagation contains noisy signals and we should distill part attributes interested by user personally. However, existing GNN-based algorithms recognize this pattern insuciently, like [33] doesn’t consider noisy attributes and [35] doesn’t consider personal extraction. To address the foregoing problems, we propose a novel attentive knowledge graph attribute network (AKGAN), which consists of two components: (1)knowledge graph attribute network (KGAN). The core task of KGAN is to learn informative item representations without semantic pollution and weight decrease of signicant attribute nodes. Technologically, KGAN has a dierent design between the rst layer and the latter layer under GNN framework. The rst layer, called attribute modeling layer, aims to generate initial item representations without semantic pollution. It regards each relation in KG as an attribute and has two key designs: embedding each entity in dierent attribute spaces and using concatenation operation to merge dierent attribute information. The latter layer, called attribute propagation layer, aims to remain semantics unpolluted and avoids weight decrease of signicant neighbors after GNN propagation. Finally, KGAN generates item representations where one attribute is represented within a specic range of element-wise positions independently. (2)user interest-aware attention network. With dierent attributes placed in corresponding element-wise positions, we design an interest-aware attention layer to distill user interested attributes. Specically, for a particular user, we pool the representations of his interacted items and extract each attribute representation according to corresponding element-wise positions. Then each attribute representation will be fed into an attention module to calculate a personal interest score, which describes how much he prefers this attribute. We introduce a novel activation unit to release the limitation that the sum of attention weight is 1, which aims to reserve the intensity of user interests[50]. Finally all interest scores and item representations are further combined to infer user representations. To summarize, the main contributions of this work are as follows: •On the item side, we propose a novel knowledge graph attribute network, which employs a heterogeneous design in dierent layers under GNN framework, embeds each entity in dierent attribute spaces, and combines dierent attributes via concatenation operation, to avoid pollution caused by weighted sum operation and weight decrease of signicant neighbors. •On the user side, we develop an interest-aware attention network, which introduces a novel activation unit and releases the limitation that the sum of attention weight is 1 , to model user interests towards attributes personally. •We conduct extensive experiments on three public benchmarks, and the results demonstrate the superior performance of AKGAN over state-of-the-art baselines. In-depth analyses are provided to illustrate the interpretability of AKGAN for user personal interests. We begin by introducing some related notations and then dening the KG enhanced recommendation problem. LetU = {𝑢}and I = {𝑖}separately denote the user and item sets. A typical recommender system usually has historical user-item interactions, which is dened asO= {(𝑢, 𝑖)|𝑢 ∈ U, 𝑖 ∈ I}. Each(𝑢, 𝑖)pair indicates user𝑢has interacted with item𝑖before, such as clicking, review, or purchasing. We also have a knowledge graph which stores the structured semantic information of real-world facts. We denote the entity and relation sets in KG asR = {𝑟 }andE = {𝑒}. KG is presented asG = {(ℎ, 𝑟, 𝑡)|ℎ ∈ E, 𝑟 ∈ R, 𝑡 ∈ E}}, where each tripile means there is a relation𝑟from head entityℎto tail entity𝑡. For example,(𝐽 𝑎𝑚𝑒𝑠 𝐶𝑎𝑚𝑒𝑟𝑜𝑛, 𝐷𝑖𝑟𝑒𝑐𝑡, 𝐴𝑣𝑎𝑑𝑒𝑟 )describes the fact that James Cameron is the director of the movie Avatar. Note thatRcontains relations in both canonical direction (e.g., Direct) and inverse direction (e.g., DirectedBy). The bridge of KG and recommender system is that items are contained in entities. Speciclly,Econsists of item nodeI(I ⊆ E) as well as attribute nodeE \ I, which can futher rene user representation eand item representation e. We now formulate the KG enhanced recommendation task to be addressed in this paper: • Input: a knowledge graphG, which contains rich sturcture senmeantic information of item, and the user-item interaction data • Output: a scoring function that denotes the probability that user u will interact with item 𝑖. In this subsection, we introduce the framework of AKGAN. With a widely used two-tower structure in recommender model[8,21, 47], AKGAN consists of two main modules: (1) knowledge graph attribute network, which is illustrated in Figure 2, and (2) user interest-aware attention module, which is illustrated in Figure 3. The core task of KGAN is to learn KG enhanced item representations that contain multiple attribute information. KGAN adopts GNN framework and has a heterogeneous design in the rst layer and the latter layer, which are named attribute modeling layer and attribute propagation layer respectively. Attribute modeling layer(AML) is to construct initial entity representations by merging one-hop neiborhoods’ attribute information in knowledge graph, as follows: Attribute propagation layer(APL) recursively propagates attribute information to acquire more informative item representations as After performing𝐿layers, we obtain multiple item representations, namely{e, ..., e}. As the output of𝑙layer represents 𝑙−hop attribute information, we conduct sum opration to pool them and infer nal item representations as When item representations have been learned,interest-aware attention(IAA) module is to learn user representations via interaction data with a novel relation-aware attention mechanism which represents user interests towards relations, a.k.a. attributes. Formally, user representations are obtained by IAA as When user representations and item representations are learned, a scoring function is used to predict their matching score and here we adopt inner product operation as In KGs, one entity has dierent types of relations with neighbors. For example, in gure 2,𝑒has dierent relations with𝑒and𝑒. To model a pure initial representation without semantic pollution, we regard each relation as an attribute and embeds each entity in all attribute embedding spaces, which is similar to the design of FFM[14] that embeds each feature in dierent elds. Therefore each entity has sevecal embeddings and we denote embedding set in dierent attribute spaces as wheree∈ Rdenotes the embedding of entity𝑖in attribute𝑟 space and 𝑑is the embedding dimension of attribute 𝑟space. Then we construct initial entity representations by aggregating attribute embeddings of one-hop neighbors. We can see that each neighbor has several embeddings and just one relation-aware embedding will be used. Taking gure 2 as an example, there are two links(𝑒, 𝑟, 𝑒)and(𝑒, 𝑟, 𝑒), we use embeddingseto represents the singer attribute of song𝑒and embeddingeto represents the friend attribute of actor𝑒, respectively. After we prepared the relation-aware embeddings, we adopt average operation to pool the same relation-aware neighbors to acquire the main semantics of this attribute as where𝑗 ∈ NandN= {𝑗 |𝑗 ∈ (𝑗, 𝑟, 𝑖), (𝑗, 𝑟, 𝑖) ∈ G}denotes the set of head entities that belong to the triplet where𝑖is tail entity and𝑟is relation. For example, if𝑒are both comedy actors and 𝑒is an action actor, such that𝑚𝑒𝑎𝑛(e)represents movie𝑒is likely to show much funny performance. Note that not all attributes occur in one-hop range, like movie𝑒doesn’t has friend attribute 𝑟. To obtain a xed-length representation, we provide zero vector to the absent attribute. Finally, all attribute embeddings will be integrated into one representation by concatenation operation as Note that we adopt concatenation rather than weighted sum operation (i.e., sum, mean, attention), which aims to solve the problem of semantic pollution. More speccally, equation 8 shows that one attribute is represented within a specic range of element-wise positions. In addition, one type of attribute is placed in the same element-wise postions for all entities. For example, two represenmean movie𝑒and movie𝑒have the same director and dierent actors. In a word, a concatented representation avoids the interaction and preserves semantic independence of dierent attributes, which will be further advantageous to maintain the weight of siginicant high-order neighbors in subsection 3.3 and distill user interested attributes in subsection 3.4. When we obtain initial entity representations via one-hop neighbors, an intuitive idea is to propagate them via GNN framework, so as to aggregate high-order neighbors into the center node and acquire more informative item representations. Here we regard a KG as a heterogeneous graph and adopt a widely used two-step scheme[5,36,44] to aggregate the representations of neighbors: (1) same relation-aware neighbors aggregation; (2) relations combination. The same relation-aware neighbors contains the similiar attribute types. For example, in gure 2, both𝑒and𝑒are actors and contain friend attribute while𝑒is a song and contains singer attribute. Therefore, we rstly aggregate representations of the same relation-aware neighbors. Secondly, to learn a more comprehensive representation, we need to fuse multiple attributes hold in dierent relation-aware neighbors. The pooling methods in the above two steps are average and sum operation, respectively. And we leave the further exploration of other pooling methods like attention as the future work. More formally, in the𝑙−th layer, we recursively formulate the representation of an entity as: Now the nal item representationehas been learned by equation 3, let’s check how KGAN avoids pollution and weight decrease of signicant high-order neighbors. We re-examineefrom the perspective of element-wise position. Without loss of generality, eis contructed as wheree∈ Ris a truncated vector ine∈ Rand𝐷 = Í𝑑, respectively. Reviewing the generation process ofe, we can see thateis learned by𝑖’s neighbors’ embeddings in attribute 𝑟space and doesn’t contain any embeddings from other attribute spaces. For example, in gure 2,eis learned bye,e, ande. This meansemaintains the independence of attribute𝑟and KGAN remains semantics unpolluted after multi-layer propagations. Furthermore, we check sepecic neighbors which are aggregated intoe. Takeeas an exapmle, after 1-order propagation (one AML),eis a zero vector since attribute𝑟doesn’t occur in onehop neighbors of movie𝑒. Then after 2-order propagation (one AML and one APL),e= e, which seems as if there were a link (𝑒, 𝑟, 𝑒)and𝑒connected to𝑒directly. In general, when one relation, a.k.a attribute (i.e.,𝑟), rstly appear in the receptive eld of center node (i.e.,𝑒) at𝑙−hop (i.e.,2−hop) position, the weight of its corresponding node (i.e.,𝑒) will not been decreased, which proves that KGAN maintains the weight of signicant high-order neighbors. After item representations have been obtained by KGAN, a typical idea in recommender system is to enhance user representations by clicked items, like [16,29,39]. We take avearage pooling as an example as whereN= {𝑖|(𝑢, 𝑖) ∈ O}anderepresents user embedding for collaborative ltering. However, avearage pooling doesn’t consider user preferences personally, thereby recent works aims to model user interests via attention mechanism, such as [17,22,33,35,49]. Here we propose a novel attention module to model user interests towards dierent attributes. We assign an interest score𝑓(𝑢, 𝑟) to each pair of attribute𝑟and user𝑢, and personally generate user representation by combining interest scores and interacted items. We rstly introduce how to calculate𝑓(𝑢, 𝑟)and then illustrate how to combine 𝑓(𝑢, 𝑟) with interacted items. With dierent attributes placed in corresponding element-wise positions as shown in equation 10, we truncate user vectorewith the same strategy as wheree∈ Ris a truncated vector ine∈ R. The start and ending index ofeineareÍ𝑑andÍ𝑑, respectively. Then the interest score of user 𝑢 towards attribute 𝑟is calculated by where𝜏is temperature coecient as a hyperparameter. In equa-Í tion 13,eedenotes the interest-degree of user𝑢 for his interacted items in attribute𝑟, whileÍee denotes the interest-degree of user𝑢for all items in attribute𝑟. If user𝑢attachs great importance to attribute𝑟when choosing items, the corresponding truncated representationeof his interacted items will be radically dierent from that of other uninterestedÍ items, such thateewill be much greater than Íee, and vice versa. Therefore, the ratio between the above two expressions denotes the interest-degree of user𝑢 for attribute𝑟. Since a negative value of ratio is meaningless, we rst feed this ratio toReluand then selecttanhas the nonlinear activation function, therefore 𝜎 (𝑥) = tanh(Relu(𝑥)). After𝑓(𝑢, 𝑟)has been prepared, we need to combine it with interacted items to learn user representations. We rstly re-express equation 11 as where||is the concatenation operation. Therefore an intuitive idea is to use𝑓(𝑢, 𝑟)to control how much attribute information,Í a.k.a.e, will be passed to user. Consider the limit case, when𝑓(𝑢, 𝑟) = 0which means user𝑢doesn’t pay attention to attribute𝑟at all, any information of attribute𝑟should not be contained in user representaion. Formally, user representation is obtained by Note that here we relax the constraint that the sum of attentionÍ weights towards all attributes is 1, a.k.a.𝑓(𝑢, 𝑟) ≠ 1. The reason is as follows: when the number of members participating in the attention calculation is large and the limitation that the sum of attention weights is 1 is still reserved at this time, it will cause the attention weight of each member to be dispersed, making it dicult to learn the coecients of important nodes. This problem has also appeared in Graphair [11], which shows estimating O(| N |)coecients exposes the risk of overtting. Therefore, we introduce the above novel activation unit to release the limitation that the sum of attention weight is 1, and the same idea is adopted by DIN [50]. With the user representationeand the item representatione ready, equation 5 is adopted to calculate the prediction score of each pair of user and item. Then we employ the BPR loss [24] to encourage that the observed interactions should be assigned higher prediction values than unobserved ones. The objective function is formulated as L =−ln 𝜎^𝑦(𝑢, 𝑖) −^𝑦(𝑢, 𝑖)+𝜆∥Θ∥(16) whereO = {(𝑢, 𝑖, 𝑖)|(𝑢, 𝑖) ∈ O, (𝑢, 𝑖) ∈ O}denotes the training set, which contains the observed interactions Oand the unobserved interactionsO;𝜎 (·)is the sigmoid function.Θ = {e, e|𝑖 ∈ I, 𝑢 ∈ U, 𝑟∈ R}is the model parameter set.𝐿 regularization parameterized by𝜆onΘis conducted to prevent overtting. We employ the Adam [15] optimizer and use it in a mini-batch manner. We evaluate our proposed AKGAN method on three benchmark datasets to answer the following research questions: Table 1: Statistics of the datasets. ’Int./user’ indicates the average number of interactions per user. • RQ1: Does our proposed AKGAN outperform the state-of-the-art recommendation methods? • RQ2: How do dierent components (i.e., attribute modeling layer, attribute propagation layer, and interest-aware attention layer) aect AKGAN? • RQ3: Can AKGAN provide potential explanations about user preferences towards attributes? Dataset Description. We choose three benchmark datasets to evaluate our method: Amazon-Book, Last-FM, and Alibaba-iFashion. The former two datasets are released in [33] and the last one is released in [35], and all of them are publicly available. Each dataset consists of two parts: user-item interactions and a corresponding knowledge graph. The basic statistics of the three datasets are presented in Table 1. We follow the same data partition used in [33,37] to split the datasets into training and testing sets. For each observed user-item interaction, we randomly sample one negative item that the user has not interacted with before, and pair it with the user as a negative instance. Evaluation Metrics. We evaluate our method in the task of top-K recommendation. For each user, we treat all the items that the user has not interacted with as negative and the observed items in the testing set as positive. Then we rank all these items and adopt two widely-used evaluation protocols: Recall@𝐾and NDCG@𝐾, where 𝐾is set as 20 by default. We report the average metrics for all users in the testing set. Baselines. To demonstrate the eectiveness, we compare AKGAN with KG-free (MF), embedding-based (CKE), path-based (RippleNet), and GNN-based (KGAT, KGNN-LS, KGIN) methods: • MF[24]: This is matrix factorization optimized by the Bayesian personalized ranking (BPR) loss, which only considers the useritem interactions and leaves KG untouched. • CKE[45]: This method uses TransR [18], a typical knowledge graph embedding algorithm, to regularize the representations of items, which are fed into MF framework for recommendation. • RippleNet[28]: This model combines embedding-based methods and path-based methods to propagate users’ preferences on the KG for recommendation. RippleNet rst assigns entities in the KG with initial embeddings using TransE and represents a user via entities related to his historically clicking items. • KGAT[33]: This method encodes user behaviors and item knowledge as an unied knowledge graph to exploit high-order connectivity. KGAT applies an attentive neighborhood aggregation mechanism on a holistic graph and introduces TransR to regularize the representations. • KGNN-LS[31]: It uses a user-specic relation scoring function to transform a heterogeneous KG into a user-personalized weighted graph and employs label smoothness regularization to avoid overtting of edge weights. • KGIN[35]: KGIN is the state-of-the-art GNN-based recommender. It models each intent as an attentive combination of KG relations to explore intents behind user-item interactions and adopts a novel relational path-aware aggregation scheme. Parameter Settings. We implement our AGKAN model in Pytorch and Deep Graph Library (DGL), which is a Python package for deep learning on graphs. We released all implementations (code, datasets, parameter settings, and training logs) to facilitate reproducibility. The embedding size of one attribute space in AGKAN varies from 4 to 64, and the detailed design will be introduced in Appendix A.1. For a fair comparison, we x the size of ID embeddings as 64, which equals the max embedding size of AKGAN, for all baselines, except RippleNet 32 due to its high computational cost. We adopt Adam [15] as the optimizer and the batch size is xed at 1024 for all methods. We use the Xavier initializer [6] to initialize model parameters. We apply a grid search for hyper-parameters: the learning rate is searched in{10, 10, 10, 10}, the coecient of L2 normalization is tuned in{10, 10, ··· , 10}, the number of GNN layers𝐿is searched in{1, 2, 3}for GNN-based methods, and the dropout ratio is tuned in{0.0, 0.1,· · ·, 0.9}. Besides, we use the node dropout technique for KGAT, KGIN, and AKGAN, where the ratio is searched in{0.0, 0.1,· · ·, 0.9}. For RippleNet, we set the number of hops as 2, and the memory size as 5, 15, 8 for Alibaba-iFashion, Last-FM, and Amazon-book respectively to obtain the best performance. Since RippleNet is a model for CTR prediction, we generate top-K items with top-K scores in all items, which are compared with the test set to compute Recall@𝐾and NDCG@𝐾. For KGAT, we use the pre-trained ID embeddings of MF as the initialization, which is also adopted by KGIN. Moreover, early stopping strategy is performed, i.e., premature stopping if Recall@20 on the test set does not increase for 10 successive epochs. We report the empirical results in Table 2 where we highlight the results of the best baselines (starred) and our AKGAN (boldfaced). And we also use%Imp. to denote the percentage of relative improvement on each metric. The observations are as followed: •AKGAN consistently achieves the best performance on three datasets in terms of all measures. Specically, it achieves signicant improvements over the strongest baselines w.r.t. NDCG@20 Table 2: Overall Performance Comparison. The best performance is boldfaced; the runner up is labeled with ’*’. ’%Imp.’ indicates the improvements. Alibaba-iFashion by 8.63%, 25.71%, and 11.87% in Amazon-Book, Last-FM, and Alibaba-iFashion, respectively. These improvements are attributed to the following reasons: (1) By combining all attributes using concatenation operation, AKGAN avoids semantic pollution caused by weighted sum operation and learns more high-quality item representations for recommendation. (2) Compared to GNNbased baselines (i.e., KGAT, KGNN-LS, KGIN), AKGAN maintains the weight of signicant high-order neighbors by placing different attributes in corresponding element-wise positions. (3) Beneting from our novel interest-aware attention module that assigns an interest score to each pair of user and attribute, AKGAN can recognize the pattern of user interest at a ne-grained level to conduct a better personal recommendation. •Jointly analyzing AKGAN across the three datasets, we nd that the improvement on Last-FM is more signicant than that on Alibaba-iFashion and Amazon-Book. The main reason is that the interaction number per user of Last-FM (128.78) is much larger than that of the other two datasets (11.99, 15.52). Therefore, there exists richer interaction information on the Last-FM dataset for AKGAN to rene user and item representation by collaborative signals. This indicates that AKGAN will fully realize its potential in recommendation scenarios with dense interaction data. •KG-free method (MF) underperforms knowledge-aware methods (i.e., CKE, RippleNet, AKGAN). A clear reason is MF doesn’t leverage the rich attribute information in KG. •GNN-based methods (i.e., KGAT, KGNN-LS, KGIN, AKGAN) achieve better performance than path-based (RippleNet) and embedding-based (CKE) methods. A possible reason is that these three kinds of recommenders adopt dierent usages of attributes. GNN-based methods aggregate neighbors’ attribute information into item nodes to learn more informative representations. The other two methods have a common limitation that both of them don’t break loose from employing knowledge graph embedding algorithms to model attribute information and regularize node representations. •In four GNN-based methods, AKGAN performs best, KGIN is the second-best, while KGAT and KGNN-LS are at the same level and achieve the worst results. The decreasing performance is because that the level of how a recommender learns item attributes and user interests is in descending order, from negrained to coarse-grained. AKGAN uses concatenation operation to avoid attribute interaction while other models adopt weighted sum (attentive combination) operation. AKGAN maintains the weight of signicant high-order neighbors while other models Table 3: GNN-based models Comparison: (1) CM - Combination methods of dierent attributes (Con - concatenation, WS - weighted sum); (2) WD - whether to avoid weight decrease of signicant high-order neighbors; (3) GF - the GNN framework that a recommender adopts (Het - heterogeneous design in dierent layers, Hom - identical network in each layer; (4) IA - whether to learn user interests towards attributes; (5) level - the level of how a recommender learns item attributes and user interests (FG - ne-grained, CG coarse-grained). ModelCMWDGFIA level neglect this issue. To achieve the above two advantages, AKGAN adopts a heterogeneous design in dierent layers while the others use a homogeneous GNN framework. AKGAN, KGIN, and KGNNLS all learn user interests towards attributes explicitly while KGAT doesn’t. The above comparison is listed in Table 3. In this section, we rst conduct an ablation study to investigate the eect of cancatenation operation and interest-aware attention layer. Towards the further analysis, we study the inuence of layer numbers. In what follows, we explore how the hyperparameter, a.k.a. temperature coecient, aects the performance. Impact of cancatenation operation & interest score. To demonstrate the necessity of cancatenation oepration and interest score, we compare the performance of AKGAN with the following three variants: (1) combing dierent attributes with average operation and discarding interest score, termed AKGAN-mean, (2) combing different attributes with sum operation and discarding interest score, termed AKGAN-sum, (3) only discarding interest score, termed AKGAN-noatt. Discarding interest score means we use equation 11 to learn user representations. Note that we don’t adopt weighted sum operation and interest-aware attention layer simultaneously. The reason is that cancatenation operation is the prerequisite of interest-aware attention layer, therefore we have to discard interest Table 4: Impact of cancatenation operation and interest score. AKGAN-mean 0.1504 0.0799 0.0849 0.0713 0.1064 0.0660 AKGAN-noatt 0.1768 0.0969 0.1141 0.1012 0.1173 0.0739 AKGAN-1 0.1737 0.0974 0.1192 0.1060 0.1245 0.0791 AKGAN-2 0.1752 0.0977 0.1205 0.1069 0.1253 0.0801 AKGAN-3 0.1783 0.0994 0.1209 0.1066 0.1221 0.0776 score when testing weighted sum operation. The results are shown in Table 4 and we summarize the major ndings as below: •Comparing AKGAN-mean/AKGAN-sum with AKGAN-noatt, we can clearly see that replacing concatenation operation with weighted sum operation dramatically degrades performance of recommendation, which indicates the superiority of cancatenation operation. •The improvement from AKGAN-noatt to AKGAN veries the necessity of interest score. •Jointly comparing AKGAN-noatt and AKGAN across the three datasets, we nd that the improvement on Last-FM is more signicant than that on Alibaba-iFashion and Amazon-Book, which is consistent with the aforesaid conclusion in subsection 4.2. The main reason is that Last-FM has more dense interaction data than the other two datasets, such that AKGAN can better learn user interest in Last-FM. Impact of model depth. We investigate the inuence of depth of receptive eld in AKGAN by searching𝐿in the range of{1, 2, 3}. Particularly,𝐿 = 1means AKGAN has only one attribute modeling layer. The results are reported in Table 5. In Amazon-Book and Last-FM, we can see that increasing propagation times of attributes can boost the performance, because more attribute information is aggregated into the center node to learn more informative user and item representations. While in Alibaba-iFashion, AKGAN-2 is the best and AKGAN-3 is the worst, which is caused by its inherent topology. Alibaba-iFashion just has two kinds of triplets: the rstorder connectivity of an item is its components, a.k.a. (fashion outt, including, fashion sta), and the second-order connectivity is the category of sta, a.k.a. (sta, having-category-?, ?). Therefore, all signicant attribute information has been captured in the 2-hop range, leading to the best performance of AKGAN-2. Impact of temperature coecient. We vary the temperature coecient in the range of𝜏 = {0.01, 0 .1, 0.2, ..., 0.9, 1}to study its inuence on the performance of AKGAN. The experiment is ongoing. In this section, we visualize the interest score to show how AKGAN learns user preferences towards attributes. We choose AlibabaiFashion as the example dataset since it helps to provide an intuitive explanation. We begin with introducing this dataset briey for further better understanding. Alibaba-iFashion is an E-commerce dataset, which contains user-outt click history for recommendation. Each outt consists of several fashion stas (e.g., tops, bottoms, shoes) and each sta is assigned with dierent categories. For example, trench coat, T-shirt and sweater are three kinds of tops, pants and long skirt are two kinds of bottoms. Alibaba-iFashion regards these categories as relations in KG and there is one more relation between outt and sta, called including. Therefore, AlibabaiFashion KG has two kinds of triples: (outt, including, sta) and (sta, having-category-?, ?), where ? is the specic category like T-shirt. According to Table 1, Alibaba-iFashion KG has 51 relations, where we number relation including as 0 and number 50 sta categories from 1 to 50. In gure 4, we rst exhibit the training set and testing set of a specic user (user0). Then we visualize the interest score learned from the training set by AKGAN and use the testing set to validate the eectiveness of these scores. From gure 4 we can see that: •The interest score accurately models the user preferences towards attributes. Among 11 outts clicked by user0, the tops contain four main kinds: T-shirt, shirt, trench coat, and sweater, which means user0 prefers the above four types of tops. The corresponding interest score are 0.3254, 0.4379, 0.6793, 0.7256, respectively. The most clicked bottoms are women’s pants which are assigned with a high score 0.8223, while the jeans and skirt occur less frequently and their scores, 0.0601 and 0.0671, are particularly small. As for shoes, user0 prefers women’s shoes and boots only appear once, their scores, 0.7623 and 0.0431, also reect this dierence. The earrings and hat are two main accessories and both of them are assigned with high scores, 0.5506 and 0.3506. •All outts clicked by user0 have only one kind of handbag and the interest score𝑓(𝑢, 𝑟) = 0.1506, which is a small value. A possible reason is that outts are manually created by Taobao’s fashion experts who prefer to include handbags for the completion of the outt [4]. Therefore user0 judges whether to click an outt based on other stas rather than the handbag. Another explanation is that handbag is not as frequently changed as other stas like tops in daily dressing, such that user0 pays less attention to the handbag when browsing through outts. •Testing set proves the eectiveness of interest score. For example, 𝑓(𝑢, 𝑟) = 0.6793is proved to be reansonable since outt12 which contains a trench coat appears in the testing set. And 𝑓(𝑢, 𝑟) = 0.8233is validated by outt13 and outt14 with same reason. •In one sta, the interest degree varies according to category. Take accessories as an example, user0 prefers earrings than hat and 𝑓(𝑢, 𝑟) = 0.5506 > 𝑓(𝑢, 𝑟) = 0.3506. This result is also proved by the testing set, where two outts contain earrings while only one outt has a hat. •Unconcerned attribute is assigned with a zero score, like𝑓(𝑢, 𝑟) = 0. The reason is that user0 perhaps is female since her click histories are all women’s clothing and she isn’t interested in men’s pants. •Relation including has the highest score 1. The reason is that this relation (outt, including, sta) is the necessary bridge to acquire what stas an outt consists of. And the same reason has been reected in the impact of model depth. Our work is highly related with the knowledge-aware recommendation, which can be grouped into three categories. Embedding-based Methods[1,3,12,29,45,46] hire KG embedding algorithm (i.e., TransE [2] and TransH [38]) to model prior representations of item, which are used to guide the recommender model. For example, DKN [29] learns knowledge-level embedding of entities in news content via TransD [13] for news recommendation. KSR [12] utilizes knowledge base information learned with TransE as attribute-level preference to enhance the sequential recommendation. Path-based Methods[19,26,41–43] usually predenes a path scheme (i.e., meta-path) and leverages path-level semantic similarities of entities to rene the representations of users and items. For example, MCRec [10] learns the explicit representations of metapaths to depict the interaction context of user-item pairs. RKGE [27] mines the path relation between user and item automatically and encodes the entire path using a recurrent network to predict user preference towards this item. GNN-based Methods[23,25,30,33,35,48] utilizes the messagepassing mechanism in graph to aggregate high-order attribute informations into item representation for enhanced and explainable recommendation. KGAT[33] regards user-item interaction as a new relation added to KG and then employees attentive mechanism to propagate attribute information. IntentGC [48] reconstructs userto-user relationships and item-to-item relationships based on KG and proposes a novel graph convolutional network to aggregate the attribute information from neighbors. KGIN [35] learn user interest via an attentive combination of attributes and integrates relational information from multi-hop paths to rene the representations. In this paper, we study the attribute information in knowledge graphs intending to improve the recommendation performance. On the item side, the proposed AKGAN can learn more high-quality item representation by remaining the independency of attributes and maintaining the weight of high-order signicant attribute nodes. On the user side, AKGAN mines user interests towards attributes and provides a personal recommendation. Extensive experiments demonstrate the eectiveness and explainability of AKGAN. For future work, we plan to investigate how to model the evolving process of user interests towards attributes based on the longterm behavior sequence. Another direction is to explore whether attribute interaction will benet recommendation since feature interaction has shown great success in click-through rate prediction. The corresponding author Jianhua Tao thanks the support of National Natural Science Foundation of China.