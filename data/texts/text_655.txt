A recent trend shows that a general class of models, e.g., BERT, GPT-3, CLIP, trained on broad data at scale have shown a great variety of functionalities with a single learning architecture. In this work, we explore the possibility of generalpurpose user representation learning by training a universal user encoder at large scales. We demonstrate that the scaling law holds in the user modeling areas, where the training error scales as a power-law with the amount of compute. Our ContrastiveLearningUserEncoder (CLUE), optimizes task-agnostic objectives, and the resulting user embeddings stretches our expectation of what is possible to do in various downstream tasks. CLUE also shows great transferability to other domains and systems, as performances on an online experiment shows signiﬁcant improvements in online Click-Through-Rate (CTR). Furthermore, we also investigate how the performance changes according to the scale-up factors, i.e., model capacity, sequence length and batch size. Finally, we discuss the broader impacts of CLUE in general. Recent work has demonstrated that some models pre-trained on broad data at scale can perform downstream transfers in a ﬂexible and task-agnostic manner for a wide range of applications; examples include GPT-3 (Brown et al., 2020), CLIP (Radford et al., 2021), and DALL-E (Ramesh et al., 2021). This general class of models are termed foundation models and have brought seismic change to both academia and industry (Bommasani et al., 2021). Foundation models require the following: (i) a universal computation engine such as a Transformer (Vaswani et al., 2017), (ii) a Equal contributionNAVER CLOVA, Korea, Republic ofNAVER AI LAB, Korea, Republic of. Correspondence to: Kuyoung Shin<ky.shin@navercorp.com>, Hanock Kwak <hanock.kwak2@navercorp.com>. Figure 1.Conceptual illustration of CLUE. CLUE learns user information from their behaviors in multiple services. CLUE is then transferred to a variety of previously unseen downstream tasks. well-deﬁned pretext task, a.k.a self-supervised learning, and transfer learning, (iii) large orders of magnitude in compute resources, model capacity, and dataset size. Building a foundation model that consolidates the methodologies for constructing machine learning systems across user modeling areas such as user proﬁling, targeting, and recommender tasks provides strong leverages. A wide range of real-world applications can be powered by one single generic model rather than highly curated task-speciﬁc models. However, there is still a lack of studies on scaling law and generalization ability of general-purpose representation learning in user modeling areas. Here, we highlight the ﬁve key questions raised for exploring the possibilities of general-purpose user representation. 1) Is transfer learning applicable to learning general-purpose user representations? 2) Do pre-training and downstream loss have a proportional relationship? 3) How wide a range of tasks can the learned user representations cover? 4) Does scaling up the pre-training model improve the generalization performance? 5) If then, which factors, i.e., model size, sequence length, batch size, should be scaled up as a good policy for enhancing model performances? To answer the questions, we introduceContrastiveLearning UserEncoder, which we call CLUE. CLUE demonstrates the effectiveness of the pre-trained general user representations learned from 50 billion behavior tokens from multiple services that share a common user base to solve unseen downstream tasks (Figure 1). The 11 million users are ﬁltered for training, but any user can be a candidate for the downstream tasks. CLUE uses contrastive learning by constructing user representations for each service, then treating pairs of the same user representations as positive samples while treating representations of different users as negative samples. We evaluate our approach by comparing the performance of a multi-layer perceptron (MLP) employing our CLUE features, with a user behavior model (UBM) trained from scratch. The MLP uses the pre-trained user features as inputs while the UBM constructs user models directly from task-speciﬁc supervision signals. Our work is one of the pioneering studies that broadly explore the possibilities of learning general-purpose user representations. We comprehensively test the pre-trained user representation with multiple downstream tasks, including an online recommendation evaluation. Furthermore, we discuss the empirical scaling laws of model size, sequence length and batch size, and make an analysis of power-law scaling for training performance as a function of computing resources. The summary of our key ﬁndings for CLUE is as follows: Empirical scaling law(Section 5.3). The training error scales as a power-law with the total training compute (PFdays) when not bottlenecked by the other factors. This result follows the trend in other domains. However we observe that the performance of the model does not depend utterly on model capacity in user representation tasks (Figure 4Top Left), but other factors such as batch size and sequence length are also effective. Transfer improves as the training error decrease(Section 5.3). CLUE performs transfer learning well on heterogeneous datasets even when the domains are different. The resulting test losses on the downstream tasks also show strong correlation to the performance during training. These results indicate that generality on various data distributions are strongly dependent on the training error (Figure 5-Right). Transforming tabular data to natural language text provide a more ﬂexible prediction space(Section 5.1). We transform all datasets into natural language text format by extracting textual information from tabular data. This policy enables the model to go beyond the discrepancies in data format within different services; the data format of the same product varies depending on the platform, but the product name is still the same. We show that CLUE trained on a particular system can provide meaningful user representations for a different system. Advantages of training from multiple service logs(Section 5.1 and 5.2). Since service logs from different domains have entirely different properties, learning with a single model is challenging. However, CLUE, which learns a multi-modal user embedding space from multiple services, shows outstanding performance in a wide range of downstream tasks. Furthermore, the experimental result (Table 4) demonstrate that pre-training on multiple services alleviates the cold-start problem by learning a richer user representation. The key to the success of the general class of models lies in deﬁning a simple pretext task that easily scales up and generalizes. Inspired by the BERT framework (Devlin et al., 2018), Shin et al. (2021) adopted Masked Language Modeling (MLM) combined with two augmentation methods. The limitation of this approach was that the MLM task predicts the exact vocabulary id of the masked tokens, which is a bottleneck in generalizing and scaling to a new dataset. A recent study has found that contrastive supervision in the latent space can learn better representations than their equivalent predictive objective (Tian et al., 2020). We thus explore the possibility of applying SimCLR to learn generalpurpose user representations (Chen et al., 2020). SimCLR learns representations by maximizing similarities between two augmented versions of the same data via a contrastive loss. The general representation learning through training data from different platforms into a uniﬁed view is a crucial challenge in building user modeling foundation models. Thus, we look into a multi-modal contrastive learning framework for enhancing the representation quality of the user embeddings by learning from multiple services. A notable example of a multi-modal contrastive learning framework is CLIP (Radford et al., 2021), which encodes pairs of images and texts separately into vectors and maximize their similarity scores. CLIP outperforms the best publicly available models in a wide range of computer vision tasks. However, CLIP needs to carefully select the vision and NLP backbone models because of fully separate encoders for images and texts. At the core of our approach is the idea of learning a multimodal user embedding space purely from service speciﬁc textual descriptions. Leveraging the natural language input enables a more ﬂexible prediction space for generalization transfer. As illustrated in Figure 2, such a ﬂexible embedding space allows us to jointly train user behavior logs over multiple services, e.g., search engine, e-commerce platform, by maximizing the agreement between the same users with just one encoder. Figure 2.Summary of our method. The encoder accepts a sequence of items where each item is described in natural language text ( This description is converted into token embeddings (E) and then passed through the Child Transformer followed by mean pooling ( The sequence of item embedding vectors are then passed to the Parent Transformer. The user embedding ( hidden vectors of the Parent Transformer. Finally, the user embeddings are trained using a contrastive learning method. a Child Transformer,T, and a Parent Transformer,T. The role of the Child Transformer is to extract the features of each item in the user’s behavior log, namely T: R→ R. The Parent Transformer produce the user embedding for a speciﬁc service from the output of the child transformer, i.e.T: R→ R. HereD,D,LandSdenotes the token embedding dimension, the output dimensions, the maximum token length of the tokenized item description, and the number of items, respectively. The sequence of user behavior logs u = [x, x, ..., x]consist of items in the service. Each itemx∈ Vis a vector of token indices where the ﬁrst token represents the type of the service and the following tokens are made by tokenization of the item descriptions (e.g., product descriptions, news titles, and search queries). We ﬁll the remaining spaces with zeros. The encoder embeds each token to a vector with an embedding layerg : V → R, so each itemxchanges to a matrixE∈ Rwhere each row is a token embedding. The next propagation process can be formulated as: whereMEANis mean pooling of row vectors in the input matrix, andzis the ﬁnal user embedding. We again denote the ﬁnal user embedding of useruand serviceAaszfor clarity. We follow the loss of CLIP (Radford et al., 2021). The losslof each positive pair(z, z)is deﬁned as: l= −logexp(f(z), f (z)τ)Pexp(f(z), f (z)τ), (4) whereτis a temperature parameter and< ·, · >is a cosine similarity function. We use a non-linear projection model f(z) = Wσ(Wz)to improve the representation quality of the user embeddings (Chen et al., 2020), whereσis a non-linear activation function andWis a weight matrix. We optimize the symmetric cross-entropy loss(l+ The ﬁnal user features for the downstream tasks are extracted by concatenating each service user feature, or for the case of the system transfer learning task, features are extracted using only task-speciﬁc user logs. We use the Zero Redundancy Optimizer (Rajbhandari et al., 2020) with AdamW (Loshchilov & Hutter, 2017) and weight decay regularization applied to all weights with a decay rate of 0.1. We update the model using an initial learning rate of 0.0005, then incorporate learning rate warm-up over the ﬁrst 1%steps, followed by cosine decay (Loshchilov & Hutter, 2016) to lower the learning rate to10%of its value. The learnable temperature parameterτis initialized to 14.27 and its value is clipped to prevent scaling the logits by more than 100. We use the automatic mixed-precision (Micikevicius et al., 2017) package in Pytorch (Paszke et al., 2019) to Figure 3.Performance comparison on the number of training epochs of CLUE. The y-axis shows MRR scores on the system transfer task. accelerate training and save memory. Gradient norm clipping (Pascanu et al., 2013) is used with the max norm set to 0.01 to stabilize training. The calculation of the embedding similarities is distributed across a multi-node cluster. Then, all the shared similarities are used for computing the logits, but only the subset of the pairwise similarities residing on an individual GPU is used for the gradient updates on that GPU. We shufﬂe the dataset at every epoch and train the model for 8 epochs, where the transfer performance began to plateau (Figure 3). The total training time takes 7 days on 64 V100 GPUs. Unless otherwise speciﬁed, all results reported in this paper as “CLUE” use this model, which we ﬁnd to perform best. The user behavior logs imply hidden intentions. For example, a user who purchased an iPhone is likely to purchase accessories such as iPhone cases, MagSafe, and AirPods next time. We can also assume that a user who searched for a new car model or car accessories on a website has an interest in automobiles. As such, observing various periods of user behavior logs gives a deeper understanding of the users. Based on this observation, we construct a sufﬁciently large dataset collected over 2 years from NAVER’s various services. We exclude the users who act less than once every two months in terms of behavior log frequency. We use byte-level BPE to tokenize the textual description of each item in the user behavior logs. If a user repeated the same behaviour (e.g., performed the same search query or purchased the same product multiple times), we keep only one of the entries in the behavior log, i.e. counting it as a unique behavior. As a result, the training dataset contains 11 million users, 5.3 billion user behaviors, and 50 billion user behavior tokens collected over 2 years. The training dataset constitute 307GB of compressed plaintext, equivalent to 50 billion byte-pairencoded tokens after preprocessing. Table 1 shows the basic statistics of the dataset. We build downstream tasks using data from numerous domains different from the ones used in the training dataset, including a webtoon platform, a news platform, a marketing messages platform and an online travel agency. Additionally, we collect service data from completely different environments compared to the ones our model is trained on to verify the system transferability of our model. We describe the datasets of the downstream tasks in detail below. Product Collection Recommendation (PCR): A Product Collection is a collection of products designed by merchandisers with a collection speciﬁc description such as “Best DVDs for babies”, “Big sale on graphic cards”, and “The most proﬁtable products”. Each collection is represented by a banner that is linked to a page showing a list of products. The task is to recommend this banner properly. We collect 5,293,056 unique click logs of these banners containing 300,000 users and 8,653 Product Collections. The title of the banner is used as the item feature. Marketing Message Recommendation (MMR): Chatbot marketing is a strategy for promoting user engagement to generate sales. The chatbot sends personalized events and products to users via messages. We collect 2,760,669 click logs of 300,000 unique users and 26,791 messages. We use the message itself as the item feature. Favorite Webtoon Recommendation (FWR): Webtoons are animated cartoons or comics that are published online. We collect 4,205,131 lists of favorited webtoons from 296,469 unique users. The total amount of unique webtoons are 1,573. We use the synopsis of the webtoons as the item feature. We exclude task-speciﬁc supervision learning for FWR because this dataset is collected without date information. Table 2.Computational cost comparison of the downstream models measured from the product collection recommendation task. The parameters include all auxiliary models such as prediction layers and embedding layers. News View Recommendation (NVR): An online newspaper is here the online version of a newspaper, either in the form of a stand-alone publication or as the online version of a printed periodical. We collect 4,032,650 news views of 299,588 unique users and 293,835 news articles. The 115,645 news articles cover various topics including, but not limited to: politics, health, technology, business, entertainment and sports. We use the title of the news articles as the item feature. New Customer Booking Prediction (NCBP): Finding new customers by understanding the company’s target audience generates new sales and more revenue. We collect 8,703,128 booking logs from 300,000 unique users. The task is to predict whether a user will make a ﬁrst reservation at one of the 42,454 services, e.g., barbershops, clinics, restaurants, museums, etc. We use the name of the business as the item feature. Online Travel Agency Recommendation (OTAR): The online travel agency is a leisure platform that provides online travel accommodations and travel services around the world. We collect 177,281 reservations from 142,051 unique users from 2,486 accommodations. The task is to recommend the hotels to the travelers. We use the hotel name as the item feature. System Transfer: Existing pre-trained user models can only be used within the systems they were trained on, which means that in order to make predictions on a new system of services, a new model must be trained. The system transfer task tests if CLUE trained on a particular system can provide meaningful user representations for a different system. We obtain user features by modelling the taskspeciﬁc logs of the target system with CLUE trained on our logs, and conduct experiments using the downstream tasks to test the model’s transferability between systems. We collect 179,435 users and 13,841 items from the target system, a beauty and cosmetic e-commerce service. The total amount of purchase logs is 558,992. The title of the product is used as the item feature. We compute the following metrics to evaluate the performance of the models: top-k Hit Ratio (HR@k), top-k Normalized Discounted Cumulative Gain (NDCG@k), and Mean Reciprocal Rank (MRR). We consider a recommendation a hit if there is at least one overlapping item between the recommended items and the ground truth items. We obtain these metrics by evaluating a pool of items consisting of a ground-truth item mixed with 100 randomly sampled items. To test the generalization ability of the models, we make sure there are no shared users between the training, validation, and test sets. We have two branches of downstream models, U-MLP and T-UBM, that evaluate the generalization ability of the pretrained user representations.U-MLPstands forUser representation with simpleMLPlayers (input-512-256-128-64output, ReLU), and models task-agnostic user representations from ShopperBERT, SimCLR, and CLUE.T-UBM modelsTask-speciﬁc historical logs of users viaUBM, explained more in detail below. In addition, we consider aHybridmodel that combines the task-speciﬁc historical logs with the pre-trained user representations from CLUE. For the item embeddings of the downstream tasks, we utilize the item’s text information. We use Sentence-BERT (Reimers & Gurevych, 2019) to extract item embedding vectors for the T-UBM models, while we use the pre-trained models (i.e., ShopperBERT, SimCLR and CLUE) for the U-MLP models. To verify the effectiveness of our method (CLUE), we compare it with the following downstream models: • T-UBM (DeepFM).DeepFM (Guo et al., 2017) is one of the most popular methods in recent recommender systems. It integrates the architectures of FM (Rendle, 2010) and deep neural networks by using both lowand high-order feature interactions. • T-UBM (BST).Behavior Sequence Transformer (BST) embeds task-speciﬁc historical logs as lowdimensional vectors, which are passed to the transformer layers to model the sequential signals underlying the users’ behaviors (Chen et al., 2019). We use T-UBM (BST) as a strong baseline. • U-MLP (ShopperBERT).ShopperBERT is a pre- Table 3.Results on the downstream tasks. We compare the pre-trained representations (U-MLP) with the user behavior model trained on the task-speciﬁc logs (T-UBM), and the combination of them (Hybrid) in terms of the three evaluation metrics. The best results among the U-MLPs and all models are denoted in bold and underlined fonts, respectively. Hybrid for BST enhanced with CLUE features. trained model that leverages BERT’s framework for learning general-purpose user representations. It trains embeddings through pretext task that uses product categories as tokens and predict the masked information under the [MASK] token (Shin et al., 2021). • U-MLP (SimCLR).Inspired by the SimCLR framework (Chen et al., 2020) from the computer vision ﬁeld, U-MLP (SimCLR) follows the same architecture as CLUE, but has a different contrastive objective. It creates positive pairs by augmenting the same user behavior sequence in two different ways by cropping, masking, and re-ordering the tokens. • Hybrid.This model combines the pre-trained user representations of CLUE with the ﬁnal output vectors of T-UBM (BST). We expect the pre-trained user representations to greatly enhance the performance of T-UBM (BST) in many downstream tasks. For the U-MLP, we use two single MLP layers for projecting the item features and the user features. Thelogitsof the U-MLP is calculated by the dot product betweenpand p, wherepandpdenote the projected user features and the projected item features, respectively. For the T-UBM (BST), we use a Transformer (L=4,H=128,A=4), where each token represents a single log (or event). Note that using raw task-speciﬁc logs requires much more computational costs (see Table 2). The downstream tasks are composed of recommendation tasks where the models predict the next item to recommend. The datasets consist of positive and negative pairs(u, i)of users and items. We have a positive pair(u, i)when a user uinteracted with an itemi, while a negative pair(u, i) is generated through random sampling. The objective of the training is to have the embeddings of positive pairs in the latent space to be close to each other, while distancing the embeddings of the negative pairs. We compute the dot products between embeddings of users and items followed by a sigmoidal activation function to constrain the outputs Figure 4.As more computing resources become available, we can choose how to allocate the resources when scaling up the training: larger models, larger batches, and longer sequence lengths.Top:The performance on the system transfer task when either the sequence length (128) or the batch size (256) is held ﬁxed.Bottom:To evaluate the efﬁciency of the training schemes, we report the performance improvement in terms of the amount of training compute (PF-days). Within the same line, each dot represents a model size ranging from 4M- to 160M-parameters in increasing order. to the interval (0, 1). We use the binary cross-entropy loss, where the labels are ones and zeros for positive and negative pairs, respectively. To validate our approach, we conduct experiments on the seven downstream tasks. For all seven tasks, we show that the simple transfer learning model outperforms the complex user behavior model trained from scratch. This empirically demonstrates the generalization ability of the pre-trained features in the user modeling area. The performance can be further improved by feeding CLUE features to the T-UBM. We present the ﬂuidity and generality of the pre-trained user representations by showing state-of-the-art performance on services from a different system, which data has completely different distributions from our training data. The results are presented in Table 3. Result of the pre-trained user representations.CLUE signiﬁcantly surpasses the other U-MLP methods in all tasks and all metrics. For the New Customer Booking Prediction and Favorite Webtoon Recommendation tasks, CLUE outperforms ShopperBERT and SimCLR by over 10.0 ∼ 16.9%in terms of MRR. CLUE achieves 0.4713 and 0.3653 MRR scores on Marketing Message Recommendation and Online Travel Agency Recommendation, that is a2.9 ∼ 9.7%and7.0 ∼ 7.7%increase compared to ShopperBERT and SimCLR, respectively. In the Product Collection Recommendation and News View Recommendation tasks, CLUE shows the least performance improvement with3.2 ∼ 4.3%and2.0 ∼ 4.7%compared to ShopperBERT and SimCLR, respectively. For the System Transfer task, the experimental results of CLUE on the MRR metric is higher than that of SimCLR by 7.6%. ShopperBERT’s loss function is based on predicting product speciﬁc information, which makes it impossible to run inference outside of the training system unless the structure of the product information is identical. Hence, there Figure 5. Left:Log-linear plot between the test loss on the training distribution and the compute (PF-days). This shows that the training learning curve has a power-law scaling as a function of the total training compute (PF-days) when not bottlenecked by other factors. Right:The generalization performance on the downstream task depends on the training loss. Although the relationship is not perfectly linear there is a strong trend that a lower loss on the in-distribution data results in a lower loss on the out-of-distribution data. are no results from ShopperBERT on the System Transfer tasks. Comparison with the model trained from scratch.Note that the Behavior Sequence Transformer (BST) signiﬁcantly outperforms DeepFM in all tasks. Thus, for the Hybrid model, we render results using BST with the pre-trained user representations. For Marketing Message Recommendation and New Customer Booking Prediction tasks, CLUE outperforms BST on both tasks by a substantial margin. Interestingly, the performance of the Hybrid is inferior to that of CLUE, but still outperforms BST by 6.4%and 10.6%in terms of MRR. For the other four downstream tasks, CLUE outperforms BST by over1.7 ∼ 11.0%, while its feature extraction method degrades the performances by0.7 ∼ 1.1% compared to the Hybrid in terms of MRR . Previous research demonstrated that task-speciﬁc representation learning with historical supervision signals has better performance than pre-training and transfer learning (Gu et al., 2020). However, our results suggest the opposite. As shown in Table 3, CLUE comfortably outperforms the BST in all tasks and for all metrics. We test our method by applying CLUE to a product collection recommendation task, where the objective is to display product collection banners that match users’ interests. We conducted an online test on an e-commerce platform for ﬁve days in November 2021. We split the users into three groups—new, cold, and heavy—based on their engagement frequency for each service. The user group ‘new’ corresponds to users with no recorded behavior on the service for the past month. The user group ‘cold’ corresponds to Table 4.Results on the online product collection recommendation task, compared with a baseline method (TopPop). We split the users into three groups, new, cold, and heavy, based on their engagement frequency for each service. CLUE (120M) +4.5% +13.4% +10.7% +7.3% TopPop recommends the most popular items to any user, regardless of their preferences. the bottom 10%in terms of activity while the group ‘heavy’ represents the top 10%. Additionally, we further compare the performance differences w.r.t the model size. The results are represented in Table 4. Compared with the conventional baseline TopPop, which recommends the most popular items to any user, CLUE (120M) signiﬁcantly increased the Click-Through-Rate (CTR) engagement by 7.3%. As a strong baseline, we employ Graph Neural Network (GNN) (Jeong et al., 2020) which is a task-speciﬁc behavior model. CLUE (120M) comfortably outperforms GNN by 4.4%in terms of CTR engagement. It is worth mentioning that CLUE (120M) obtains more CTR engagement than CLUE (15M). The result veriﬁes that the universal scaling law still works in online scenarios. For an in-depth veriﬁcation of our method, we investigate how the models perform with different user segments. For user groups ‘new’ and ‘cold’, the pre-trained user representation method, CLUE, shows signiﬁcant CTR improvement compared to the task-speciﬁc historical model, GNN. In particular, the CTR rate of GNN is lower than that of TopPop for the user group ‘new’ that lacks behavior logs, while CLUE consistently shows outstanding performances. Interestingly, as the amount of task-speciﬁc history logs increases from the user group ‘new’ to the user group ‘heavy’, the CTR difference between GNN and CLUE decreases, although CLUE still obtains a higher performance. Recently, several lines of work empirically demonstrate the existence of a universal scaling law, where the training error scales as a power-law with model capacity, data size, and the amount of compute (Brown et al., 2020; Kaplan et al., 2020; Zhai et al., 2021; Bahri et al., 2021). Studies on the scaling law signiﬁcantly broadened the ﬁeld of reasoning, but the ﬁndings are mostly restricted to NLP and computer vision. In this subsection, we empirically observe the power-law decrease of the training error in user modeling areas w.r.t. scales. The compute (PF-days) is calculated as 6× #of parameters ×batch size× #of training steps×sequence length divided by one PF-day = 8.64× 10. We train all models for 100,000 steps. The results are presented in Figure 4 and 5. Performance on Scale.In contrast to existing work on the scaling law in other domains, we observe that the performance of the model does not depend utterly on the model capacity in the user representation tasks (Figure 4-Top Left). Previous studies argued that learning high-quality representations from batch-wise contrastive loss requires a sufﬁcient amount of negative samples (Chen et al., 2020; Mitrovic et al., 2020; Gao et al., 2021). Thus, we speculate that the scaling law when learning with contrastive objectives is more complex than that of supervisory signals due to the bottleneck induced by the batch size. CLUE’s performance grows smoothly as the sequence length of the input data is increased, suggesting that user behavior models beneﬁts from observing longer sequences of customer behavior. (Figure 4-Top Right). We can see the performance varying in terms of training compute (Figure 4-Bottom). These results give us an insight into how to appropriately allocate computing resources for efﬁcient training schemes. We can conclude that all three factors must scale up in tandem for optimal performance. Generalization on other data distributions.The indistribution test loss as a function of compute is shown in Figure 5. We empirically verify that the loss on the training dataset improves smoothly with more compute. This result aligns with the modelling standard in other domains, which has shown that increasing the amount of com- Table 5.Decreasing the output feature dimension of CLUE does not lead to a signiﬁcant difference in the performance on the downstream tasks. pute positively affects the performance of the pre-trained model (Brown et al., 2020; Kaplan et al., 2020; Zhai et al., 2021). Moreover, when we transfer knowledge to datasets with a different distribution than the one used during pretraining, the resulting test losses show a strong correlation with the pre-training performance. These results indicate that generalizability to various data distributions is strongly dependent on the training loss (Figure 5-Right). The output feature dimensions of the non-curated 120M CLUE is 1,000. If we assume that we extract the user features from three services, we have an output feature dimension of 3,000. The 3,000 feature dimensions for 36 million users results in a size of 212 GB when stored in halfprecision ﬂoating-point format. If we slightly increase the load period or increase the number of services, the storage becomes too large to handle. Thus, we conduct an experiment on whether the output dimension of CLUE affects its performance. We added a single MLP layer for reducing the dimension of the encoder outputs. Table 5 shows that decreasing the output dimension of the model does not lead to any performance degradation on the downstream transfer tasks. Below we describe the limitations of our study and suggest directions for future research. Although CLUE is ﬂexible and can be applied to a wide variety of datasets, there are still limits to its domain transferability. For the task of movie recommendation, we have noticed that CLUE seems to perform worse than T-UBM (BST), despite doing well on other domains as presented in Table 3. Thus, we investigate which domains CLUE can transfer well to. Recently, Gururangan et al. (2020) investigated whether it is helpful to transfer a pre-trained model to the domain of a target task. They used the vocabulary overlap (%) to measure similarities between source and target domains. Since it is difﬁcult to measure the vocabulary overlap of the user behavior dataset between different domains, we estimate the Kendall rank correlation (Abdi, 2007) using the token distribution of the user behavior logs Figure 6.The Kendall rank correlation of the token distribution between the pre-train and downstream domains. PT denotes a pre-training dataset. We use the user behavior logsuto calculate the token distributions for each dataset. of each domain. Figure 6 shows the correlation across the datasets. We observe that the correlation and performance improvement does not align perfectly. According to the results of Table 3 and Figure 6, the correlation and the relative performance increase of the U-MLP against the T-UBM shows a strong trend, but in the case of News View Recommendation, despite the very high correlation values for the pre-training dataset, the performance gain of the U-MLP is only 1.7%. Our work have not made much progress towards ﬁnding a criteria for a well-transferable domain. A more careful study of this subject is left for future research. Scaling Law.Over the decades, the connection between scaling and generalization has been studied broadly from both theoretical and empirical perspectives (Mhaskar, 1996; Brown et al., 2020; Kaplan et al., 2020; Bahri et al., 2021; Zhai et al., 2021; Hutter, 2021). Several studies theoretically demonstrated that the training error scales as a power-law with larger model capacity and more data (Mhaskar, 1996; Bahri et al., 2021; Hutter, 2021). Recent research focuses on empirical analysis of scaling laws for real-world applications (Brown et al., 2020; Kaplan et al., 2020; Zhai et al., 2021). These works showed that scaling up the model and dataset size is a promising approach for achieving outstanding performances in several NLP and computer vision tasks, e.g., machine translation, generative modeling and image recognition. General-Purpose User Representation Learning.Compared to task-speciﬁc user representation learning, the research about the general-purpose user representation learning is still in its early stage. Yuan et al. (2020) introduces the parameter efﬁcient ﬁne-tuning architecture to relieve the computational burden of the model. They cover ﬁve downstream tasks, including user proﬁling and cold-start browser recommendation. They further extend it towards lifelong and general-purpose user representation learning named Conure (Yuan et al., 2021). Bian et al. (2021) propose a fusion network that can capture semantic alignment across the macro and micro views of user behavior in a mobile app. They test the user embeddings for three mobile app management scenarios. Inspired by the pre-training method in NLP, Qiu et al. (2021) trains two self-supervision tasks to utilize the different reviews in multiple dataset to model the user representation. They ﬁne-tuned the model to six downstream tasks from multiple domains. Note that they are all restricted to ﬁne-tuning based approach (Yuan et al., 2020; 2021; Bian et al., 2021; Qiu et al., 2021). The recent progress in general representation learning naturally attempts to build more efﬁcient and widely adaptable user representation. Gu et al. (2021) focus on training effective user representation, which can be applicable to numerous downstream tasks without further modiﬁcations. They employ a novel objective function named behavioral consistency loss to preserve the user’s long-term interest and beneﬁt from diverse behaviors. The proposed method is evaluated on user preference prediction and user proﬁling tasks. Sun et al. (2021) present interest-oriented contrastive learning, which maximizes the agreement between short- and long-term interest representation of the same users. They adopt two long-term user preference tasks and one short-term user preference task to evaluate their method. Following the previous studies, we focus on learning adaptable, general user representation in user modeling areas and connect the bridge between user representation learning and empirical scaling law. We explore whether the success of task-agnostic pre-training in other domains can be transferred to user modeling areas. We present CLUE trained on the billion scale real-world user behavior data to learn general-purpose user representations. We benchmark the heterogeneous end tasks with a simple MLP, and achieve promising results. We further investigate the empirical scaling laws and the generalization ability of our method, and ﬁnd that the power-law learning curve as a function of compute (PF-days) is observed in the experiments. Despite many limitations and weaknesses, these results follow the trend in other domains that very large models and datasets may be important factors in developing adaptable, general representation learning. The authors would like to thank the NAVER CLOVA ML X team for insightful comments and discussions. We would also like to thank the NAVER Smart Machine Learning (NSML) platform team (Sung et al., 2017; Kim et al., 2018) for their critical work on the software and hardware infrastructure on which all the experiments were performed.