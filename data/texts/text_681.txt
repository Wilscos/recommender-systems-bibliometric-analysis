Fine-tuning pre-trained language models improves the quality of commercial reply suggestion systems, but at the cost of unsustainable training times. Popular training time reduction approaches are resource intensive, thus we explore low-cost model compression techniques like Layer Dropping and Layer Freezing. We demonstrate the efﬁcacy of these techniques in large-data scenarios, enabling the training time reduction for a commercial email reply suggestion system by 42%, without affecting the model relevance or user engagement. We further study the robustness of these techniques to pre-trained model and dataset size ablation, and share several insights and recommendations for commercial applications. Automated reply suggestions have become ubiquitous in many popular email and chat applications, such as Outlook (Ofﬁce365, 2018), Teams (Microsoft, 2020), Gmail (Google, 2017), and LinkedIn (LinkedIn, 2017). These systems assist the end-users in responding to a message by providing them multiple short contextually-relevant reply suggestions that can be used to reply with a quick click, thus improving their overall productivity. The dominant techniques for these systems model them as a response selection task (Gunasekara et al., 2019) using a bi-encoder matching model that matches the incoming message with a ﬁxed set of pre-selected responses (Henderson et al., 2017). Recently, ﬁne-tuning pre-trained Transformer language model encoders such as BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), or UniLMv2 (Bao et al., 2020) has shown improvements in model quality (Henderson et al., 2020). However, commercial reply-suggestion systems can muster massive amounts of training data, which leads to large ﬁne-tuning times for these pre-trained models. Furthermore, commercial systems often stack additional rankers on top, wherein the matching model ranker is used earlier in the stack to retrieve relevant responses, and the latter rankers help induce special attributes like diversity (Deb et al., 2019), factual correctness, and user writing style in the responses. For the best model quality, these rankers are often interdependent, and thus a change to a ranker earlier in the stack often requires re-training the entire stack. Additionally, strict compliance with data privacy laws like GDPR (EU, 2016) requires changing the underlying training dataset every few weeks, requiring all the rankers to be re-trained. These factors make reducing the ﬁne-tuning times of the matching model vital for efﬁcient and continuous development of commercial reply-suggestion systems. Popular approaches for reducing the training times involve compressing the pre-trained model size using techniques like Distillation (Hinton et al., 2015). Such techniques, however, are resource intensive, and require additional training. In this work, we explore low-cost model compression solutions like Layer Dropping and Layer Freezing. We demonstrate that these low-cost techniques are highly effective in large-data scenarios, helping reduce the training times of a commercial email reply suggestion system by over42%. We also show that these techniques can be combined together into a hybrid model for additional gains. We further study the robustness and efﬁcacy of these compression techniques with ablations in the pre-trained model and the training dataset size, and share our ﬁndings and recommendations. We hope that these ﬁndings will help inform the experimentation and design of commercial applications even beyond reply suggestion scenarios. Speciﬁcally, our ﬁndings forlarge-data scenarios can be summarized as: 1. For the same desired amount of training time reduction, compressing the model and training it on a large dataset preserves the relevance better than training a large model on a downsampled dataset (Section 5.1). 2.Layer Dropping and Layer Freezing are competitive low-cost model compression techniques, and thus should be prioritized over computationally expensive techniques like Distillation (Sections 5.2 and 5.3). 3.A randomly initialized model with sufﬁcient capacity might be able to match the relevance of a pre-trained initialized model at large dataset scale, however, the usage of a pretrained model is still recommended since it compresses better and can lead to reduced training times (Section 6). 4.However, further improving the pre-trained model through its domain-adaptation doesn’t offer any additional relevance or compression advantages over the vanilla pre-trained model at large dataset scales (Section 6). We thus advise caution before prioritizing improvements to the underlying pre-trained model for taskspeciﬁc relevance and training time gains. Similarly, our ﬁndings forlow-data scenarios can be summarized as: 1.In keeping with the literature, we found that initialization with a pre-trained model is vastly superior to a randomly initialized model of the same capacity. Furthermore, we found that domain-adaptation of the pre-trained model provides additional relevance gains (Section 7). We thus recommend leveraging a pre-trained model and trying out its improvements (for instance its domain-adaptation) in low-data settings. 2.We discovered that the efﬁcacy of model compression techniques can be a function of the size of the ﬁne-tuning dataset. Speciﬁcally, we found that Layer Dropping is only competitive on large ﬁne-tuning datasets, whereas the performance of Layer Freezing is agnostic to the dataset size. We thus recommend Layer Freezing over more complex model compression techniques across dataset sizes (Section 7). We start in section 2 by providing a brief background of our reply suggestion system and the compression techniques being pursued, followed by a brief overview of the related work in section 3. We discuss our experimental setup in section 4, and present the training time reduction results on a large dataset in section 5. We then study the impact of the pre-trained model, and the dataset size in sections 6 and 7 respectively. We discuss future work in section 8, and ﬁnally conclude in section 9. We model suggesting replies to emails as a retrieval problem from a ﬁxed set of pre-selected candidate responses (a Response Set). Given an email dataset consisting of message-response (MR) pairs, we use a bi-encoder matching architecture to encode the message and response using distinct transformer encoders (Vaswani et al., 2017) (as in Figure 1), and then train the model through a symmetric matching loss (Deb et al., 2019) minimization objective between the message and response encodings. For inference, we rank the responses from the response set using a combination of their matching score with the message (as provided by the bi-encoder), and an additional bias term to promote responses with high pre-computed messageindependent LM scores (language model scores). The bias term balances the matching model’s tendency to surface long and highly speciﬁc responses by promoting generic responses, and is a hyperparameter that we tune. We then use lexical clustering to select3diverse responses from the top ranked responses, such that each of the selected responses comes from a different cluster. In our system, we pre-compute and cache the response set encodings, which makes computing the encoding of the incoming message our online serving bottleneck, and incentivizes keeping the message encoder compact. For instance, our baseline system uses12transformer layers to encode the response, and just6to encode the message. This formulation is similar to the work of Henderson et al (Henderson et al., 2017). Our response set consists of a set of responses from our email training dataset that meet a certain frequency threshold are ﬁltered through a set of carefully curated n-grams to prevent them from containing any user information or offensive content. These responses are further reviewed for their Figure 1: Matching model architecture consisting of a bi-encoder over the message-reply pairs quality by a dedicated team of human curators. We use the normalized log frequency of these responses as their message-independent LM score. 2.2 Compression Techniques Literature has shown that pre-trained models are heavily overparameterized, with possible redundancies between layers (Sajjad et al., 2020), and only some subset of the layers needing to be ﬁne-tuned (Lee et al., 2019). Thus, several model compression techniques like Distillation, Pruning, and Quantization have been proposed to curb the training times. In this work we investigate the effectiveness of lowcost techniques like Layer Dropping and Freezing. Layer Dropping:Layer Dropping involves heuristically pruning entire layers from the message and response encoders, and ﬁne-tuning the resulting model. Fewer layers lead to reduced training times and online latencies. Dropping layers also gives the ﬂexibility of selecting which layers to retain from the pre-trained model, and since different layers can capture different linguistic information (Tenney et al., 2019; Clark et al., 2019), end-performance can vary based on the layers selected (Sajjad et al., 2020). Layer Freezing:Layer Freezing involves not updating the weights of a bottom few encoder layers during ﬁne-tuning, since the bottom layers usually capture task-agnostic information (Lee et al., 2019). This helps reduce the training time by optimizing the backward-pass, however unlike Layer Dropping, it doesn’t provide any inference time reduction. Note that Layer Dropping and Freezing can additionally be combined together into aHybrid Model to reap the beneﬁts of both approaches. Pre-trained models like BERT (Devlin et al., 2019) are trained on large amounts of text from Wikipedia and BookCorpus (Zhu et al., 2015), and thus provide useful representations for many domains. Recently, adaptation of these models to the target domain through continued pre-training (Gururangan et al., 2020) or training from scratch (Gu et al., 2020) has shown to further improve their representations. In this work we study if the domainadaptation of the pre-trained model leads to better model compression as well. Many model compression techniques have been proposed to improve the ﬁne-tuning times of Transformer-based pre-trained models. Distillation (Sanh et al., 2019) is a widely used technique where a large pre-trained teacher model is distilled into a smaller student model using a distillation loss. However, pre-trained model distillation can be computationally expensive, thus making it unsuitable for catering to the ad-hoc model size reduction needs of various scenarios. Our work instead explores simple model compression techniques like Layer Dropping and Layer Freezing (Sajjad et al., 2020; Lee et al., 2019) that incur no additional computation overhead, and (unlike Distillation) seamlessly support ad-hoc model size reductions by not requiring the pre-trained model to be ﬁrst distilled down to the desired model size. Our Layer Dropping technique can also be considered to be a special case of existing techniques like weight or attention head pruning (Raganato et al., 2020; Gordon et al., 2020), wherein we heuristically prune entire layers instead of pruning speciﬁc weights or attention heads. Most model compression techniques in literature have been evaluated on public datasets with only modest amounts of ﬁne-tuning data, which is not representative of the scale of ﬁne-tuning datasets available for commercial applications. In this work, we take the ﬁrst step by evaluating simple model compression techniques on large amounts of ﬁnetuning data, and study their efﬁcacy with variations in the size of the ﬁne-tuning dataset. Furthermore, our experiments with the domainadapted pre-trained model are motivated by their recent success in the literature (Gururangan et al., 2020), wherein they have been found to perform better than their vanilla counterparts. However, similar to model compression techniques, the domain-adapted pre-trained models have only been evaluated on tasks with small amounts of ﬁnetuning data. Our work complements the existing work by experimenting with domain-adapted pretrained models with varying amount of ﬁne-tuning dataset. We additionally study if domain-adapted pre-trained models offer any model compression advantages. Notation:We useMRto denote a base matching model withxandytransformer blocks in the message and response encoders respectively. Furthermore, whenever we freeze some layers of theMRbase model, we will embellish the notation as(MR, fmr)to denote that the subword embeddings and the bottom-mostimessage andjresponse encoder layers have been frozen during training. We will represent freezing just the subword embeddings on both encoders as (MR, f). For Layer Freezing conﬁgurations, whenever the base model is clear from the context, we will drop the base model notation and will refer to the model with just the freezing notationfmr. Baseline Model:We use a BERT-base biencoder architecture (Devlin et al., 2019), with6 Transformer layers in the message encoderand 12in the response (MR) encoder to keep our serving latencies small. We don’t share weights between the encoders during training, use the CLS token’s output for scoring responses, use the same segment id (of0) in both the encoders, a wordpiece uncased vocab of size30k, and initialize the weights of encoders with a pre-trained BERT baselike model, called Turing-NLR (TNLR), from Microsoft Turing. Training:Unless otherwise stated, we train on roughly60million message-reply (MR) pairs extracted from the mailboxes of users of a popular email client. We adhere to strict user privacy policies, and use state-of-the-art privacy preserving technologies. The user data is always kept encrypted, and the model training is carried in a secure environment that doesn’t allow any viewing of the contents of the mailboxes. We use16Nvidia V100 GPUs with InﬁniBand connectivity for distributed training. We use gradient accumulation and mixed precision trainingto speed up the training, and use the Adam optimizer with a scheduled learning rate, involving a linear warmup followed by an exponential decay. We train for a max of 20epochsand use validation on20kinstances to select the best checkpoint. Evaluation:The objective of this work is to reduce the training times for our baselineMR while maintaining the relevance of the model. For measuring training time improvements, we use Wall-Clock (WC) time, which we deﬁne as the time taken to obtain the best model checkpoint. This depends on the epoch time and the rate of convergence of the model. For measuring the model relevance ofﬂine, we compute a w-Rouge measure on a set of500k weighted version of the Rouge F-measure (Lin, 2004) that can measure similarity between a user’s actual (golden) response and multiple predicted responses (a response block). For each MR pair instance, we ﬁrst compute the maximum Rouge-n score between the golden response (r) and each of the3predicted responses (r). We deﬁne it as Rouge-n(equation 1), wherenis the length of the ngram. We then take a weighted average (wbeing the weight) of these Rouge-nscores across n-grams of length 1, 2, and 3 as in equation 2. We ultimately macro-average these w-Rouge scores across all test instances. Rouge-n= maxRouge-n(1) w-Rouge =w× Rouge-n(2) We use w-Rouge over other IR metrics like Mean Reciprocal Rank, since it allows partial overlap, and has demonstrated a higher correlation with our user engagement metrics. We run two-sided T-tests, and only report statistically signiﬁcant changes where p-value ≤ 0.05. The ultimate evaluation of the impact on model relevance is through end-user engagement, for which we divide users into treatment and control groups and conduct experiments with a small percentage of online user trafﬁc. We use user-averaged click-through-rate (CTR) as our engagement metric, and only report statistically signiﬁcant changes Table 1: Comparison of TNLR & BERT on MR to the CTR (p-value≤ 0.05). We thus seek a model with signiﬁcantly smaller WC time and no drop in online CTR compared to the baseline MR. However, since running online experiments is costly and affects the user experience, we run them only for the most promising models with no wRouge drop, and use w-Rouge to make inferences for the rest. Pre-trained Models:We experiment with3pretrained models – Random (i.e. no pre-training), TNLR, and D-TNLR. TNLR is Project Turing’s BERT-base equivalent which is trained from scratch similar to BERT, but with the addition of phrase-level masking and replacement of NSP loss with a ‘Sentence Order Prediction (SOP)’ loss. We use TNLR since it has demonstrated improvements over BERT-base on the GLUE dev set (Wang et al., 2018). D-TNLR is the domain-adapted version of TNLR. It is trained from scratch like TNLR on a combination of public datasets and domain-speciﬁc commercial email datasets. All training was done in a fully-compliant eyes-off manner, with data given the proper privacy permissions to be used in training and validation. D-TNLR upsamples the email corpus during training, interleaves batches between datasets, and has a performance competitive to TNLR on the GLUE dev set. BERT vs TNLR:We ﬁrst ﬁne-tune and compare the public BERT-baseand TNLR baseline models for our task. We see in Table 1 that in addition to similar GLUE scores, the pre-trained models are identical for our task as well (we use†to represent stat-sig. changes throughout). We can thus expect our ﬁndings from TNLR to generalize to BERT as well. 5.1 Dataset Downsampling We evaluate downsampling the training dataset as a viable training time reduction technique. Table 2 shows that a90%sample (54M) shows no drop in w-Rouge, but yields only a modest time reduction Table 2: Impact of Dataset Downsampling (#MR pairs) (16%). However, when we sample more aggressively (≤ 40%or≤ 25M), we see a statistically signiﬁcant≥ 1.9%drop in w-Rouge. This shows that we can’t rely on downsampling for our desired amounts of training time reduction, while maintaining relevance. We ﬁrst quantify the impact of the position of the layers being retained from the pre-trained model. Towards this goal, we benchmark retaining different layers (0indexed) in the message encoder of aMRmodel compressed with Layer Dropping – Bottom-3(Layers0, 1, 2), Top-3(Layers 9, 10, 11), Even-3(Layers0, 6, 10), and Odd-3 (Layers1, 7, 11). Table 3 shows that different layer selections are comparable, and thus we consistently use the Odd layer selection in the rest of the Layer Dropping experiments, since it yields slightly better performance. Ofﬂine Results:We experimented with various degrees of Layer Dropping in the message and response encoders, and found the WC times to go down as we removed more layers (Table 4). We found several conﬁgurations with no drop in wRouge, withMRstanding out with48% fewer parameters, and a42.3% training time reduction. Further removal of layers, as in withMR, led to a substantial 1.5% drop in w-Rouge. Online Results:Seeing favorable ofﬂine results forMR, we ran an online experiment comparing this model against the baseline for several weeks using real user trafﬁc, and noticedno impact on the CTR– indicating an equal preference for the Table 5: Performance of Layer Freezing on MR compressed model. This reduction in model size also helped usreduce our online serving latencies by over 35%, thus saving compute for other parts of the model stack. Ofﬂine Results:We experimented with freezing message and response encoders ofMRto various degrees (Table 5), and noticed WC times to drop as we froze more layers. #Params in (Table 5) denotes the number of trainable parameters after Layer Freezing. We noticed a signiﬁcant drop of 75%in w-Rouge when we froze the entire model, thus showing the importance of task-speciﬁc ﬁnetuning. We found several conﬁgurations with no drop on w-Rouge, withfmrstanding out with 63% fewer parameters, and42.7% reduction in the training time. Online Results:Similar to Layer Dropping, we ran an online experiment comparingfmragainst the baseline for several weeks, and noticedno impact on the CTR. However, since Layer Freezing has no inference time advantages, we noticed no reduction in our online serving latencies. Overall, these results demonstrate that Layer Dropping and Layer Freezing are effective and comparable training time reduction techniques on large datasets, with Layer Dropping holding an (MR, f) 22.8hrs 0.07200 Table 7: Impact of pre-trained model on various model conﬁgurations. edge due to its inference time advantages as well. These techniques are complementary to each other and thus can be combined into a hybrid model for additional gains as well. Our initial experiments with hybrid models show that it is possible to further freeze the subword embeddings of theMR model without affecting w-Rouge to obtain a cumulative training time reduction of59%(Table 6). Lastly, we note that our best compressed model (MR, f)has a better w-Rouge (1.1%better) and comparable WC time to training the baseline modelMRon the downsampled25Mdataset (section 5.1). This result suggests that when faced with the need to bring down training times of large models on large datasets, compressing the model and training it on a large dataset preserves the relevance better than training the large model on a downsampled dataset. Random Initialization:We ﬁrst benchmark random initialization against TNLR to study the beneﬁts of using a pre-trained model. We see in Table 7 that the RandomMRbaseline performs as well as its TNLR counterpart (p-value: 0.23). This shows that the relevance beneﬁts of a high-capacity pre-trained model diminish on large datasets. However, the pre-trained model is still desirable because of its higher compressibility, since we found Layer Dropping to not work as well for the randomly initialized model (Table 7). Speciﬁcally, the RandomMRconﬁguration showed a signiﬁcant drop in w-Rouge of 1.3% compared to TNLR MR. Domain-Adapted Pre-trained Model:We then benchmark D-TNLR to check if a better, domain-adapted pre-trained model can help us compress even more. We report in Table 7 that the D-TNLRMRbaseline performs only as well as its TNLR counterpart. We also see that DTNLR doesn’t provide any compression beneﬁts over TNLR in its Layer Dropping and Layer Freezing conﬁgurations. We additionally found D-TNLR to not show any noticeable convergence time beneﬁts as well. Overall, these results show that even with large ﬁne-tuning datasets, pre-trained models play a role in making compression techniques like Layer Dropping and Freezing competitive. However, improvement to the pre-trained model by adapting it to the target domain does not yield further compression or convergence beneﬁts with such large datasets. Next, we further investigate the generalization of these observations with dataset size variations. We create2sampled datasets of sizes5Mand 100k, and ﬁne-tune on them to study the impact of dataset size on the effectiveness of Layer Dropping, Freezing, and pre-trained models. Layer Dropping:We repeat the TNLR Layer Dropping experiments on these smaller datasets (Section 5.2), and report in Table 8 that Layer Dropping fromMRtoMRdoesn’t work as well on5Mand100kdatasets, as it leads to1.80%and 2%drops in w-Rouge respectively. This shows that in addition to the pre-trained model (Section 6), the effectiveness of Layer Dropping also depends on the availability of a large ﬁne-tuning dataset. Layer Freezing:The corresponding TNLR freezing experiments (Table 8) show however, that Layer Freezing’s effectiveness is agnostic to the dataset size. Speciﬁcally, fmrperforms as well as theMRbaseline on the5Mset, and even shows a gain in w-Rouge of2.5%on the100kset, with the likely explanation here being that freezing potentially helps avoid overﬁtting on smaller datasets. These results also show that the preference between Layer Dropping and Freezing changes with the dataset size, with freezing becoming the preferred technique on smaller datasets due to its relevance advantages over dropping. MR0.07225 0.067950.05809 Table 8: Comparison of w-Rouge across datasets for various TNLR conﬁgs Pre-trained Model:We repeat the experiments of Section 6 on the5Mset, and show (Table 9) that compared to TNLR, the RandomMRbaseline has a drop in w-Rouge of6%. This shows that a pre-trained model is necessary for relevance (and not just for compression) on smaller datasets. However, we also see in Table 9 that even with 5Minstances, D-TNLR performs only as well as TNLR. We thus repeat the experiments on the100k set, and report in Table 10 that at this scale, the DTNLR baseline not only gives a6.4%w-Rouge improvement over the TNLR baseline, but also compresses better than TNLR as its dropping and freezing conﬁgurations provide3.75%and2.46% gains over TNLR respectively. These results show that adapting the pre-trained model to the target domain is a viable training time reduction technique on small datasets. Since our recommendations and insights are drawn from our bi-encoder based suggested replies system for a large-scale commercial email service, in the future we will evaluate their generalization to other response selection datasets and architectures. Some early experiments on a large-scale commercial chat client are already showing a similar trend. We will also evaluate the generalization of our ﬁndings to other tasks with large amounts of ﬁne-tuning data, primarily the scenarios where implicit labels are D-TNLR 0.063120.060270.06348 Table 10: Comparison of TNLR and D-TNLR on 100k readily available such as text prediction. We will also experiment with combining other compression methods such as quantization (Zafrir et al., 2019), weight pruning (Gordon et al., 2020), and attention head pruning (Raganato et al., 2020; Michel et al., 2019) with our layer dropping and freezing conﬁgurations to further reduce our training times. We showed that Layer Dropping and Layer Freezing are competitive techniques to compress transformer bi-encoders in commercial, large-data scenarios, and that the presence of a pre-trained model is essential for these compression techniques to work well. In particular, we were able to reduce the training times of a commercial email service’s reply-suggestion system by over42%, without affecting the user engagement. The optimal Layer Dropping conﬁguration also led to a35%improvement in online latency. Furthermore, we showed that although these strategies perform similarly at large dataset scales, their comparative efﬁcacy changes when dataset size is varied. Speciﬁcally, as the size of the ﬁnetuning data is reduced, while performance can no longer be matched by dropping layers to the same extent, Layer Freezing continues to be competitive, even improving upon the baseline’s performance. We also showed that the advantages of domainadapted pre-trained models (Gururangan et al., 2020; Gu et al., 2020) diminish in the presence of large ﬁne-tuning datasets. However, as the dataset size is reduced, domain adaptation becomes important again, as it leads to signiﬁcant relevance and model compression gains. We believe these insights can beneﬁt many such Transformer biencoder based models in commercial settings, by helping achieve signiﬁcant reductions in model training and allowing faster iteration times in deploying model improvements. We gratefully acknowledge the contributions of the entire Suggested Replies team and various partner teams in helping build and maintain different parts of our system upon which this work builds. We thank the Turing and SURGE teams for supporting us with our pre-trained model needs. We also sincerely acknowledge Budhaditya Deb, Ahmed Awadallah, and Milad Shokouhi for their support and insightful inputs throughout this work.