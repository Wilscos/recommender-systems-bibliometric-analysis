Nanyang Technological University, Singapore Mila, University of Montr GNNs have recently emerged as a powerful class of deep learning architectures to analyze datasets where information is present in the form of heteregeneous graphs that encode complex data connectivity. Experimentally, these architectures have shown great promises to be impactful in diverse domains such as drug design (Stokes et al., 2020; Gaudelet et al., 2020), social networks (Monti et al., 2019; Pal et al., 2020), trafﬁc networks (Derrow-Pinion et al., 2021), physics (Cranmer et al., 2019; Bapst et al., 2020), combinatorial optimization (Bengio et al., 2021; Cappart et al., 2021) and medical diagnosis (Li et al., 2020c). Most GNNs (such as Defferrard et al. (2016); Sukhbaatar et al. (2016); Kipf & Welling (2017); Hamilton et al. (2017); Monti et al. (2017); Bresson & Laurent (2017); Veli et al. (2019)) are designed with a message-passing mechanism (Gilmer et al., 2017) that builds node representation by aggregating local neighborhood information. It means that this class of GNNs is fundamentally structural, i.e. the node representation only depends on the local structure of the graph. As such, two atoms in a molecule with the same neighborhood are expected to have similar representation. However, it can be limiting to have the same representation for these two atoms as their positions in the molecule are distinct, and their role may be speciﬁcally separate (Murphy et al., 2019). As a consequence, the popular message-passing GNNs (MP-GNNs) fail to differentiate two nodes with the same 1-hop local structure. This restriction is now properly understood in the context of the equivalence of MP-GNNs with Weisfeiler-Leman (WL) test (Weisfeiler & Leman, 1968) for graph isomorphism (Xu et al., 2019; Morris et al., 2019). Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we callLSPE(Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from2.87%up to 64.14% when considering learnable PE for both GNN classes. Figure 1: Block diagram illustration of the proposed MPGNNs inputs, general framework of a layer, and the output and loss components. The said limitation can be alleviated, to certain extents, by (i) stacking multiple layers, (ii) applying higher-order GNNs, or (iii) considering positional encoding (PE) of nodes (and edges). Let us assume two structurally identical nodes in a graph with the same 1-hop neighborhood, but different with respect to 2-hop or higher-order neighborhoods. Then, stacking several layers (Bresson & Laurent, 2017; Li et al., 2019) can propagate the information from a node to multiple hops, and thus differentiate the representation of two far-away nodes. However, this solution can be deﬁcient for long-distance nodes because of the over-squashing phenomenon (Alon & Yahav, 2020). Another approach is to compute higher-order node-tuple aggregations such as in WL-based GNNs (Maron et al., 2019; Chen et al., 2019); though these models are computationally more expensive to scale than MP-GNNs, even for medium-sized graphs (Dwivedi et al., 2020). An alternative technique is to consider a global positioning of the nodes in the graph that can encode a graph-based distance between the nodes (You et al., 2019; Dwivedi et al., 2020; Li et al., 2020b; Dwivedi & Bresson, 2021), or can inform about speciﬁc sub-structures (Bouritsas et al., 2020; Bodnar et al., 2021). Contribution. combined with structural GNNs to generate more expressive node embedding. Our main intent is to alleviate the lack of canonical positioning of nodes in arbitrary graphs to improve the representation power of MP-GNNs, while keeping their linear complexity for large-scale applications. For this objective, we propose a novel framework, illustrated with Figure 1, that enables GNNs to learn both structural and positional representations at the same time (thus named MPGNNs that the proposed architecture with learnable PE can be used with any graph network that ﬁts to the MP-GNNs framework, and improves its performance ( formulate and PNA (Corso et al., 2020) and fully-connected Transformers-based GNNs (Kreuzer et al., 2021; Mialon et al., 2021). Our numerical experiments on three standard molecular benchmarks show that different instantiations of MP-GNNs with datasets by considerable margins ( the third. In addition, our evaluations ﬁnd the sparse MP-GNNs to be outperforming fully-connected GNNs, hence suggesting greater potential towards the development of highly efﬁcient, yet powerful architectures for graphs. In this section, we review brieﬂy the three research directions theoretical expressivity of GNNs, graph positional encoding, and Transformer-based GNNs. In this work, we turn to the idea of learning positional representation that can be LSPEinstances of both sparse GNNs, such as GatedGCNs (Bresson & Laurent, 2017) Theoretical expressivity and Weisfeiler-Leman GNNs. GNNs is bounded by the 1-WL test (Xu et al., 2019; Morris et al., 2019), they may perform poorly on graphs that exhibit several symmetries (Murphy et al., 2019), and additionally some message-passing functions may not be discriminative enough (Corso et al., 2020). To this end, GNNs were introduced in Maron et al. (2018) requiring Although the complexity was improved to 2019; Chen et al., 2019; Azizian & Lelarge, 2020), it is still inefﬁcient compared with the linear complexity of MP-GNNs. Graph Positional Encoding. of pixels in images, words in texts and nodes in graphs, plays a central role in the effectiveness of the most prominent neural networks with ConvNets (LeCun et al., 1998), RNNs (Hochreiter & Schmidhuber, 1997), and Transformers (Vaswani et al., 2017). For GNNs, the position of nodes is more challenging due to the fact that there does not exist a canonical positioning of nodes in arbitrary graphs. Despite these issues, graph positional encoding are as much critical for GNNs as they are for ConvNets, RNNs and Transformers, as demonstrated for prediction tasks on graphs (Srinivasan & Ribeiro, 2019; Cui et al., 2021). Nodes in a graph can be assigned index positional encoding (PE). However, such a model must be trained with the or else sampling needs to be done (Murphy et al., 2019). Another PE candidate for graphs can be Laplacian Eigenvectors (Dwivedi et al., 2020; Dwivedi & Bresson, 2021) as they form a meaningful local coordinate system, while preserving the global graph structure. However, there exists sign ambiguity in such PE as eigenvectors are deﬁned up to values when selecting learnable position-aware embeddings based on random anchor sets of nodes, where the random selection of anchors has its limitations, which makes their approach less generalizable on inductive tasks. There also exists methods that encode prior information about a class of graphs of interest such as rings for molecules (Bouritsas et al., 2020; Bodnar et al., 2021) which make MP-GNNs more expressive. But the prior information regarding graph sub-structures needs to be pre-computed, and sub-graph matching and counting require O(n Transformer-based GNNs. information bottleneck limitation (Alon & Yahav, 2020) in addition to vanishing gradient (similar to RNNs) on tasks when long-range interactions between far away nodes are critical. To overcome these limitations, there have been recent works that generalize Transformers to graphs (Dwivedi & Bresson, 2021; Kreuzer et al., 2021; Ying et al., 2021; Mialon et al., 2021) which alleviates the long-range issue as ‘everything is connected to everything’. However, these methods either use non-learnable PEs to encode graph structure information (Dwivedi & Bresson, 2021; Ying et al., 2021; Mialon et al., 2021), or inject learned PEs to the Transformer network that relies on Laplacian eigenvectors (Kreuzer et al., 2021), thus inheriting the sign ambiguity limitation. A detailed review of the above research directions is available in the supplementary Section B. We attempt to address some of the major limitations of GNNs by proposing a novel architecture with consistent performance gains. In this work, we decouple structural and positional representations to make easy for the network to learn these two critical characteristics. This is in contrast with most existing architectures s.a. Dwivedi & Bresson (2021); Beani et al. (2021); Kreuzer et al. (2021) that inject the positional information into the input layer of the GNNs, and You et al. (2019) that rely on distance-measured anchor sets of nodes limiting general, inductive usage. Given the recent theoretical results on the importance of informative graph PE for expressive GNNs (Murphy et al., 2019; Srinivasan & Ribeiro, 2019; Loukas, 2020), we are interested in a generic framework that can enable GNNs to separate positional and structural representations to increase their expressivity. Section 3.1 will introduce our approach to augment GNNs with learnable graph PE. Our framework can be used with different GNN architectures. We illustrate this ﬂexibility in Sections C.1 and C.2 where the decoupling of structural and positional information is applied to both sparse MP-GNNs and fully-connected GNNs. Notation. hasn = |V| matrixA ∈ R A= 0. The degree matrix is denoted nodeiis denoted by indicated by input features, a stack of convolutional layers, and a ﬁnal task-based layer, as in Figure 1. The layers are indexed by ` and ` = 0 denotes the input layer. Standard MP-GNNs. are transformed at each layer, the update equations for a conventional MP-GNN layer are deﬁned as: wheref The design of functions a review. As Transformer neural networks (Vaswani et al., 2017) are a special case of MP-GNNs (Joshi, 2020), Eq. (1) can be simpliﬁed to encompass the original Transformers by dropping the edge features and making the graph fully connected. Input features and initialization. embedding of available input node and edge features denoted respectively by Positional Encoding. concatenate the PE with the input node features, similarly to Transformers (Vaswani et al., 2017): wherep transformation. Such architecture merges the positional and structural representations together. It has the advantage to keep the same linear complexity for learning, but it does not allow the positional representation to be changed and better adjusted to the task at hand. Decoupling position and structure in MP-GNNs. structural information such that both representations are learned separately resulting in an architecture withLearnable update equations are deﬁned as: The difference of this architecture with the standard MP-GNNs is the addition of the positional representation update Eq. (9), along with the concatenation of these learnable PEs with the node structural features, Eq. (7). As we will see in the next section, the design of the message-passing LetG = (V, E)be a graph withVbeing the set of nodes andEthe set of edges. The graph nodes andE = |E|edges. The connectivity of the graph is represented by the adjacency e. A GNN model is composed of three main components; an embedding layer for the MP-GNNs : hh,h, e, h, h∈ R andfare functions with learnable parameters, andNis the neighborhood of the nodei. and a, b∈ Rare the learnable parameters of the linear layers. ∈ Ris the input PE of nodei,D∈ R, d∈ Rare parameters for the linear Structural andPositionalEncodings, which we callMP-GNNs-LSPE. The layer Figure 2: Sample graph plots from the ZINC validation set with each node color in a graph representing a unique RWPE vector, when graphs and the number of unique RWPEs are labelled against the ﬁgures. function allow positive and negative values for the positional coordinates. It should be noted that the inclusion of the edge features, features in their used for future extensions in a convenient way. Deﬁnition of initial PE. PEs: Laplacian PE (LapPE) and Random Walk PE (RWPE). LapPE are deﬁned in Section B.2 as sensitive w.r.t. the Euclidean norm. However, they are limited by the sign ambiguity, which requires random sign ﬂipping during training for the network to learn this invariance (Dwivedi et al., 2020). Inspired by Li et al. (2020b), we propose RWPE, a PE based on the random walk (RW) diffusion process (although other graph diffusions can be considered s.a. PageRank (Mialon et al., 2021)). Formally, RWPE are deﬁned with k-steps of random walk as: whereRW = AD matrixRW considering only the landing probability of a node suffer from the sign ambiguity of LapPE, so the network is not required to learn additional invariance. RWPE provide a unique node representation under the condition that each node has a unique topological neighborhood for a sufﬁcient large synthetic strongly regular graphs like the CSL graphs (Murphy et al., 2019), then all nodes in a graph have the same RWPE for any RWPE being the same for all nodes in a graph, these PE are unique for each class of isomorphic graphs, resulting in a perfect classiﬁcation of the CSL dataset, see Section A.1. For graphs such as Decalin and Bicyclopentyl (Sato, 2020), nodes which are not isomorphic receive different RWPE for k ≥ 5, also in Section A.1. Finally, for real-world graphs like ZINC molecules, most nodes receive a unique node representation for 100% and 71.43% unique RWPEs respectively. Section A.3 presents a detailed study. Experimentally, we will show that RWPE outperform LapPE, suggesting that learning the sign invariance is more difﬁcult (as there exist having unique node representation for each node. As mentioned above for CSL, RWPE are related to the problem of graph isomorphism and higher-order node interactions. Precisely, iterating the random walk operator for a suitable number of steps allows coloring non-isomorphic nodes, thus distinguishing several cases of non-isomorphic graphs on which the 1-WL test, and equivalently MP-GNNs, fail s.a. the CSL, Decalin and Bicyclopentyl graphs. We refer to Section A.2 for a formal (a) ZINC molecule (val index 91)(b) ZINC molecule (val index 212) ffollows the same analytical form offbut with the use of thetanhactivation function to hupdates. Nevertheless, the architecture we present is made as generic so as to be U, U, ··· , U] ∈ R.LapPE provide a unique node representation and are distancefor all pairwise nodes, we adopt a low-complexity usage of the random walk matrix by Preprint presentation of the iterative algorithm. Finally, the initial PE of the network is obtained by embedding the LapPE or RWPE into a d-dimensional feature vector: Positional loss. possible to consider a speciﬁc positional encoding loss along with the task loss. A natural candidate is the Laplacian eigenvector loss (Belkin & Niyogi, 2003; Lai & Osher, 2014) that enforces the PE to form a coordinate system constrained by the graph topology. As such, the ﬁnal loss function of MP-GNNs-LSPE is composed of two terms: whereh andα > 0 centered and unit norm eigenvector loss deﬁned by k · kbeing the Frobenius norm. We instantiate two classes of GNN architectures, both sparse MP-GNNs and fully-connected Transformer GNNs using our proposed (Bresson & Laurent, 2017) and PNA (Corso et al., 2020), while we extend the recently developed SAN (Kreuzer et al., 2021) and GraphiT (Mialon et al., 2021) with LSPEarchitectures. We brieﬂy demonstrate here how a GNN can be instantiated using (7-9)) by developing GatedGCN models are deﬁned in Section C of the supplementary material. GatedGCN-LSPE: mechanism that is able to learn adaptive edge gates to improve the message aggregation step of GCN networks (Kipf & Welling, 2017). Our proposed extension of this model with LSPE is deﬁned as: whereη same analytical form as the omission of BN, which was not needed in our experiments. We evaluate the proposed MPGNNs Transformer GNNs deﬁned in Section 3.2 (all models are presented in Section C), using PyTorch (Paszke et al., 2019) and DGL (Wang et al., 2019) on standard molecular benchmarks – ZINC (Irwin et al., 2012), OGBG-MOLTOX21 and OGBG-MOLPCBA (Hu et al., 2020). ZINC and MOLTOX21 are of medium scale with 12K and 7.8K graphs respectively, whereas MOLPCBA is of large scale with 437.9K graphs. These datasets, each having a global graph-level property to be predicted, consist of molecules which are represented as graphs of atoms as nodes and bonds between the atoms as edges. As we separate the learning of the structual and positional representations, it is ∈ R, p∈ R,kis the dimension of learned PE,` = Lis the ﬁnal GNN layer, an hyper-parameter. Observe also that we enforce the ﬁnal positional vectorspto have ∈ RandB, B, B, C, C∈ R. Notice thep-update in Eq. (16) follows the ZINCis a graph regression dataset where the property to be predicted for a graph is its constrained solubility which is a vital chemical property in molecular design (Jin et al., 2018). We use the 12,000 subset of the dataset with the same splitting deﬁned in Dwivedi et al. (2020). Mean Absolute Error (MAE) of the property being regressed is the evaluation metric. binary graph classiﬁcation dataset where a qualitative (active/inactive) binary label is predicted against 12 different toxicity measurements for each molecular graph (Tox21, 2014; Wu et al., 2018). We use the scaffold-split version of the dataset included in OGB (Hu et al., 2020) that consists of 7,831 graphs. ROC-AUC averaged across the tasks is the evaluation metric. is also a multi-task binary graph classiﬁcation dataset from OGB where an active/inactive binary label is predicted for 128 bioassays (Wang et al., 2012; Wu et al., 2018). It has 437,929 graphs with scaffold-split and the evaluation metric is Average Precision (AP) averaged over the tasks. To evaluate different instantiations of our proposed MPGNNs marking protocol in Dwivedi et al. (2020) to fairly compare several models on a ﬁxed number of 500k model parameters, for ZINC. We relax the model sizes to larger parameters for evaluation on the two OGB datasets as observed being practised on their leaderboards (Hu et al., 2020). The total size of parameters of each model, including the number of layers used, are indicated in the respective experiment tables, with the remaining implementation details included in supplementary Section D. The results of all our experiments on different instances of using PE are presented in Table 1 whereas the comparison of the best results from Table 1 with baseline models and SOTA is shown in Table 2. We now summarize our observations and insights. Table 1: Results on the ZINC, OGBG-MOLTOX21 and OGBG-MOLPCBA datasets. All scores are averaged over 4 runs with 4 different seeds. Bold: GNN’s best score, Red: Dataset’s best score. No PE results in lowest performance. worse performance on all the three datasets. This ﬁnding is aligned to the recent literature (Sec. Table 2: Comparison of our best (Sec. A.4) on each dataset. For ZINC, all the scores in Table 2a are the models with the parameters. The scores on OGBG-MOL* in Tables 2b and 2c are taken from the OGB project and its leaderboards (Hu et al., 2020), where models have different number of parameters. B.2) that has guided research towards powerful PE methods for expressive GNNs. Besides, it can be observed that the extent of poor performance of models without PE against using a PE (LapPE orLSPE explained by the fact that ZINC features are purely atom and bond descriptors whereas OGB-MOL* features consist additional information that is informative of e.g. if an atom is in ring, among others. LSPE boosts the capabilities of existing GNNs. improved signiﬁcantly when they are augmented with For instance, the best GNN without PE for ZINC, i.e. PNA, gives an improvement of vs.0.141 manner. On other GNNs, this boost is even higher, see GatedGCN 64.14%( PNA while the remaining models show either minor gains or attain the same performance when not using PE. This consistent trend is also observed for MOLPCBA where Sparse vs. Transformer GNNs. PNA) against Transformer GNNs (SAN, GraphiT) augmented with of the sparse GNNs is surprisingly better than the latter, despite Transformer GNNs being theoretically well-posed to counter the limitations of long-range interactions of the former. Notably, the evaluation of our proposed architecture, in this work, is on molecular graphs on which the information among local structures seems to be the most critical, diminishes the need of full attention. This also aligns with the insight put forward in Kreuzer et al. (2021) where the SAN, a Transformer model, beneﬁted less from full attention on molecules. Beyond molecular graphs, there may be other domains where Transformer GNNs could give better performance, but still these would not scale in view of the quadratic computational complexity. Indeed, it is important to notice the much lesser training times of sparse GNNs compared to Transformer GNNs in Table 1. LSPE improves the state-of-the-art. LSPEfrom Table 1 with baseline GNN models from the literature on the three benchmark datasets, our proposed architecture improves the SOTA on two of these datasets, while achieving SOTAcomparable performance on the third, see Table 2. On ZINC, GatedGCN baselines by a large margin to give a test MAE of 26.23%respectively over the two recent-most Transformer based GNNs, SAN and Graphormer. On MOLTOX21, PNA over the best baseline GIN which uses virtual node (VN). Finally, comparable performance to SOTA on MOLPCBA while boosting its performance when no PE was used. We note here that ZINC scores can even be boosted beyond knowledge is used (Bouritsas et al., 2020; Bodnar et al., 2021) while Graphormer (Ying et al., 2021) achieved the top score on MOLPCBA when pre-trained on a very large (3.8M graphs) molecular dataset. To ensure fair comparison with other scores, we did not use these two results in Table 2. On Positional loss. bestLSPE train score and not the generalization performance. We will investigate a more consistent positional loss in a future work. ) is greater for ZINC than the two OGBG-MOL* datasets used. This difference can be ) whenLSPEis used to learn the structural and positional representations in a decoupled 0.090vs.0.251). On MOLTOX21, PNA-LSPEimproves3.44%(0.781vs.0.755) over score on ZINC slightly from0.093to0.090, while on MOLTOX21 it only improves the Table 3: Comparing the ﬁnal at input layer of a GNN, using GatedGCN model on ZINC. The column ‘Final only the node structural features are used as ﬁnal node features (denoted by with node positional features (denoted by [h Finally, we would like to highlight the generic nature of our proposed architecture which can be applied to any MP-GNN in practice as demonstrated by four diverse GNNs in this work. Through ablation studies, we show – i) the usefulness of learning positional representation at every layer vs. simply injecting a pre-computed positional encoding in the input layer, and ii) the selection of the number of k for the steps in RWPE in the proposed LSPE architecture. Learning PE at every layer provides the best performance. sponds to the model where LapPE are replaced with ﬁrst layer, and the PE are not updated in the subsequent layers. First, we observe a signiﬁcant leap in performance (from that RWPE could encode better positional information in GNNs as they are not limited by the sign ambiguity of LapPE. See Section A.1 in the supplementary material for an example of RWPE’s representation power. Now, if we observe the training performance, GatedGCN-RWPE leads to an overﬁt on ZINC. However, when the positional representations are also updated, the overﬁt is considerably alleviated improving the test score to positional features at the ﬁnal layer with the structural features, Eq. (12), the model achieves the best MAE test of representations can be tuned and better adjusted to the learning task being dealt with. The choice of k steps to initialize RWPE. number of 3.1. This value the last layer. Numerical experiments show the best values of GatedGCN-LSPE and OGBG-MOLTOX21 with PNA-LSPE respectively, which are larger values from what was used in Li et al. (2020b) ( encoding. The difference of a largek Second, Li et al. (2020b) not only uses and j in a target set of nodes, which increases the computational complexity. Figure 3: Test scores on selecting different values of iterative steps of RW in RWPE as well as the dimension of the learned PE at the ﬁnal layer, Eqn. 12. 0.093. This study justiﬁes how the GNN model learns best when the positional ksteps for the random walk features that are used as initial positional encoding in Section kis also used to set the ﬁnal dimension of the learned positional representation in value to possibly provide a unique node representation with differentk-hop neighborhoods. This work presents a novel approach to learn structural and positional representations separately in a graph neural network. The resultant architecture, learning of these two key properties that make GNN representation even more expressive. Main design components of initialization, ii) decoupling positional representations at every GNN layer, and iii) the fusion of the structural and positional features ﬁnally to generate hybrid features for the learning task. We observe a consistent increase of performance across several instances of our model on the benchmark datasets used for evaluation. Our architecture is simple and universal to be used with any sparse GNNs or Transformer GNNs as demonstrated by two sparse GNNs and two fully connected Transformer based GNNs in our numerical experiments. Given the importance of incorporating expressive positional encodings to theoretically improve GNNs as seen in the recent literature, we believe this paper provides a useful architectural framework that can be considered when developing future models which improve graph positional encodings, for both GNNs and Transformers. XB is supported by NRF Fellowship NRFF2017-10 and NUS-R-252-000-B97-133.