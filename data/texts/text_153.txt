News recommendation is important for personalized online news services. Most existing news recommendation methods rely on centrally stored user behavior data to both train models ofﬂine and provide online recommendation services. However, user data is usually highly privacy-sensitive, and centrally storing them may raise privacy concerns and risks. In this paper, we propose a uniﬁed news recommendation framework, which can utilize user data locally stored in user clients to train models and serve users in a privacy-preserving way. Following a widely used paradigm in real-world recommender systems, our framework contains two stages. The ﬁrst one is for candidate news generation (i.e., recall) and the second one is for candidate news ranking (i.e., ranking). At the recall stage, each client locally learns multiple interest representations from clicked news to comprehensively model user interests. These representations are uploaded to the server to recall candidate news from a large news pool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy. Online news platforms usually rely on personalized news recommendation techniques to help users obtain their interested news information (Qi et al., 2021b; Wu et al., 2019c). Existing news recommendation models usually exploit users’ historical behavior data to model user interests for matching candidate news (Wang et al., 2020; Wu et al., 2019c,b, 2020c; Qi et al., 2021a; Ge et al., 2020; Wu et al., 2021c). For example, Okura et al. (2017) employed a GRU network to build user embeddings from browsed news. Wu et al. (2019a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns on privacy leakage and violate some privacy protection regulations such as GDPR. A few methods explore to recommend news in a privacy-preserving way (Qi et al., 2020). For instance, Qi et al. (2020) proposed to store user data in user clients and applied federated learning technique (McMahan et al., 2017) to train news recommendation models on decentralized data. In general, these methods usually focus on developing privacy-preserving model training approaches based on decentralized user behavior data for ranking candidate news. However, how to generate candidate news and serve users in a privacy-preserving way remains an open problem. In this paper, we propose a uniﬁed news recommendation framework based on federated learning (named Uni-FedRec), which can utilize user behavior data locally stored in user clients to train models ofﬂine and serve users online in a privacy-preserving way. Following a widely applied paradigm in real-world recommender systems (Wu et al., 2021e; Pal et al., 2020), UniFedRec contains a recall stage for personalized candidate news generation and a ranking stage for candidate news ranking. In the recall stage, the user client ﬁrst locally learns multiple interest representations from clicked news to model diverse user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private user information encoded in interest representations, we propose an interest decomposer-aggregator method with perturbation noise to synthesize interest representations with a group of basic interest embeddings. In addition, Uni-FedRec utilizes user data decentralized in a large number of user clients to collaboratively train the recall and ranking model in a privacy-preserving way. Extensive experiments on two real-world datasets verify that our method can signiﬁcantly outperform baseline methods and effectively protect user privacy. In summary, our contributions are as follows: •We propose a uniﬁed privacy-persevering news recommendation framework which can train model ofﬂine and serve users online with locally stored user data. •We propose a privacy-preserving recall model which can comprehensively model diverse user interests and protect user privacy. •Extensive experiments demonstrate that our framework can outperform many baseline methods and effectively protect user privacy. Personalized news recommendation is an important research problem and has been widely studied in recent years (Konstan et al., 1997; Wang and Blei, 2011; Liu et al., 2010; Bansal et al., 2015; Wu et al., 2020b; Qi et al., 2021c; Wu et al., 2020d, 2021d; Wang et al., 2020; Ge et al., 2020; An et al., 2019). Existing news recommendation methods aim to match candidate news content with user preferences mined from users’ historical behaviors (Khattar et al., 2018; Wu et al., 2021b,f; Ge et al., 2020; Qi et al., 2021a; Wu et al., 2019d; An et al., 2019). For example, Okura et al. (2017) proposed to learn user interest embeddings from the sequential information of user’s clicked news via a GRU network. An et al. (2019) proposed to model short-term user interest from news clicks via a GRU network and model long-term user interest via user ID embeddings. They further combine them to form a uniﬁed interest embedding. Wu et al. (2019d) employed a multi-head self-attention network to learn user interest embeddings by modeling relatedness of users’ reading behaviors. Besides, all of these three methods performed the matching between candidate news and user interest via the inner product of their embeddings. In brief, most of these methods rely on the centralized storage of user behavior data to train models and serve users. However, users’ behavior data is usually highly privacy-sensitive, and storing them in the server may arouse risks and user concerns on privacy leakage, and may also violate some privacy protection regulations (e.g., GDPR) (Muhammad et al., 2020; Wu et al., 2020a). Different from these methods, we propose a uniﬁed privacy-preserving framework for news recommendation, which can utilize decentralized user behavior data to train models and serve users. Recently, due to users’ increasing concerns on privacy leakage, some privacy-preserving recommendation methods have been proposed (Qi et al., 2020; Flanagan et al., 2020; Lin et al., 2020; Wang et al., 2021; Muhammad et al., 2020; Yang et al., 2021; Wu et al., 2021a, 2020a). For example, Chai et al. (2019) proposed to compute gradients of user and item embeddings in user clients based on locally stored user rating data and upload gradients to the server for federated model updating. Besides, to better protect user privacy, they employed the homomorphic encryption technique (Gentry, 2009) to encrypt the uploaded gradients. Qi et al. (2020) proposed to apply federated learning technique to train neural news recommendation models on decentralized user data. They used local differential privacy technique (Ren et al., 2018) to protect the uploaded gradients from leaking user privacy. In brief, most of these methods focus on training a recommendation model for ranking candidate news in a privacy-preserving way. However, how to generate candidate news from news pool according to user interest and serve users with decentralized user behavior data are still unsolved problems. Different from these methods, we propose a uniﬁed privacy-preserving news recommendation framework, which can utilize locally stored user data to generate candidate news from the server, and further serve users via local candidate news ranking. In this section, we will introduce our uniﬁed privacy-preserving news recommendation framework (named Uni-FedRec), which can utilize decentralized user data to train models and serve users. In Uni-FedRec, user behavior data (e.g., displayed news and clicked news) is locally stored in user clients, and the news pool is stored and maintained in the server. Following a widely used paradigm in real-world recommender systems (Pal et al., 2020; Liu et al., 2020), Uni-FedRec contains a recall stage for candidate news generation and a ranking stage for candidate news ranking. To serve a user, the user client ﬁrst employs a privacy-preserving recall model to locally learn multiple interest representations from clicked news to model diverse user interests. The interest representations are further uploaded to the server to recall candidate news from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for personalized news display. To train models on decentralized user data, Uni-FedRec coordinates massive user clients to collaboratively calculate gradients from their local user data for federated model updating. Next, we will introduce each module of Uni-FedRec in detail. As shown in Fig.1, our privacy-preserving recall model contains four major modules, i.e., a user model, an interest decomposer, an LDP perturbation module and an interest aggregator. The former is used to learn multiple interest representations to model diverse user interests. The latter three are used to protect users’ private information encoded in interest representations. Interest Modeling:User behavior contexts are informative for modeling user interests (Wu et al., 2019d). In user model, we ﬁrst use a global selfattention network (Vaswani et al., 2017) to learn context-sensitive representations of clicked news [g, ..., g]from representations of clicked news [h, ..., h], whereHis the number of clicked news. Besides, users usually have diverse interests in multiple interest ﬁelds (Pal et al., 2020; Liu et al., 2020). To capture diverse user interests, we divide user’s clicked news into different interest clusters{H|i = 1, ..., C}via the hierarchical clustering algorithm (Johnson, 1967), whereHis the i-th cluster, andCis the number of clusters. The algorithm hierarchically merges the clusters until the average distances between any two clusters are larger than a given thresholdd. Then, we apply a cluster-wise attention network to learn uniﬁed interest representation rfor each cluster H: Xexp(Att(g)) whereγandgis the attention weight and representation for thej-th clicked news in thei-th cluster, andAtt(·)is a dense network for calculating attention scores. In this way, we can obtain multiple interest representations{r|i = 1, ..., C} to model user interests in different interest ﬁelds. Privacy Protection:User interest representations may contain some private user informainformation encoded in interest representations, we propose an interest aggregator-decomposer method with permutation noise. Its core is to synthesize interest representations by combining a group of trainable basic interest embeddings {(e, e)|i = 1, ..., B}(BIE) shared among different users, whereeandeis the key and value of thei-th basic interest embedding, respectively, andBis the number of BIE. In interest decomposer, we ﬁrst decompose each interest representationr on the keys of BIE:a= Query(r, e), where Query(x, y)is a query function implemented by dot product ofxandy,a∈ Ris the decomposition score of decomposingron embeddinge. We further perturb decomposition scores via local differential privacy (LDP) technique: ˆa= f(a) + n∼ La(0, λ whereˆais the protected decomposition score, f(z)is a function for clippingzwith the scale ofδ,nis a zero-mean Laplace noise, andλis its intensity. Next, in the interest aggregator, we further synthesize protected interest representations by combining value embeddings of BIE: whereˆris the protected representation for r. News Recall:We further use each protected interest representationˆrto recall topRcandidate news that has the largest relevance withˆrfrom news in the pool. We use inner product similarity to measure representation relevance, and this recall progress can be speeded up by some search algorithms such as ANN search (Arya et al., 1998). Besides, we have an allocator to allocate quotas, i.e., the number of candidate news recalled by each channel. We utilize ratios of clicked news belonging to each interest channel to generate their quotas: whereRis the total number of recalled candidate news. Finally, we integrated candidate news of different channels and obtain the candidate news setR = {d|i = 1, ..., R}, wheredis thei-th recalled candidate news. Loss Function:InfoNCE loss (Oord et al., 2018; Wu et al., 2019d; An et al., 2019) is usually used to formulate loss function in recommendation task. It requires a uniﬁed score to rank positive and negative samples, while our method will generate Cdifferent recall scores for each news. To tackle this issue, we combine recall scores of newsdgenerated by each channelˆzto form a uniﬁed score: wheredis representation ofd. Next, for each positive sample, we randomly selectKnegative samples from all news that are displayed to but not clicked by this user. Then, we obtain the loss function Lbased on behavior data Bof user u: wherezandzis the uniﬁed score of thei-th positive sample and its j-th negative sample. Uni-FedRec contains a ranking model to locally rank candidate news in the user client. Since local news ranking will not leak user privacy, we directly employ existing news ranking methods such as NRMS (Wu et al., 2019d) in Uni-FedRec. As shown in Fig. 2, these methods share a similar framework, where a news model learns news embedding from news texts, a user model learns user embedding from clicked news, and a matching module (e.g., dot product) matches candidate news and user interests for personalized ranking.The news model is usually based on the stack of a word embedding layer, a context modeling layer (e.g., Transformer) and an aggregation layer (e.g., attention network), and the user model is usually based on the stack of a behavior context modeling layer (e.g., GRU) and an aggregation layer (e.g., attention network). We also formulate the loss function Lof the ranking model via the infoNCE loss: wherexandxis the ranking score of thei-th positive sample and itsj-th negative sample randomly selected from the same news impression respectively and Kis the number of negative samples. In Fig. 1, we show the privacy-preserving framework of Uni-FedRec for online serving with decentralized user data. In Uni-FedRec, users’ behavior data is locally stored in user clients and is never uploaded to the server, which can effectively alleviate users’ privacy concerns. The server stores and maintains news articles in a news pool. Besides, both the user client and the server contain the whole recall model. When a user visits the news platform, the client ﬁrst employs the recall model (i.e., user model, interest decomposer and perturbation module) to build protected decomposition weightsαand generate quotas of interest channels, which are further uploaded to the server. After receiving them, the server further employs the interest aggregator to build protected interest representationsˆrand perform news recall to obtain candidate news setR. The recalled news with their titles and links are further distributed to the user client. After the client receives the candidate news, the ranking model locally ranks these candidate news and displays the topDnews with the highest-ranking scores. The user can locally read news titles and click links of interested news for reading. Finally, after the user ﬁnishes this session, all displayed news and user behaviors will be stored in the user client. Besides, to reduce online latency, protected decomposition weights and interest channel quotas can be calculated and stored in advance for quick uploading. 3.5 Privacy-Preserving Model Training Training accurate recommendation models usually relies on large-scale training data (Muhammad et al., 2020). In Uni-FedRec, user behavior data is locally stored in user clients and it is hard to centrally learn parameters of recommendation models in a conventional way. Motivated by McMahan et al. (2017), we utilize federated learning techniques to train recall and ranking models on decentralized user data. Next, we will introduce the training process of the recall model in detail. As shown in Fig. 3, the server is in charge of maintaining parametersΘof the recall model and coordinating clients to collaboratively updateΘ based on user data locally stored on them. At the beginning of a federated updating round, each client has a copy of the current recall model. Then the server will randomly select a part of usersUto perform local gradient calculation. Client of the selected useruwill calculate gradientsfor parameter updating based on her behavior dataB and current model parametersΘ. Besides, gradients calculated from user behavior data may leak private user information (Zhu et al., 2019). To better protect user privacy, following Qi et al. (2020), we apply a local differential privacy (LDP) module to the clipped gradients: ˆG= f(∂L) + n∼ La(0, λ), (8) whereˆGis the protected gradients,fis a clipping function with the scale ofθ,nis the Laplace noise andλis its intensity. Next, the user client uploads the protected gradientsˆGto the server. After the server receives uploaded gradients from clients of users inU, the server further aggregates these gradients for model updating: whereGis the average gradients. Then, parameters of the recall model is updated as:Θ= Θ− ωG, whereωis the learning rate. Updated parameters are further distributed to all clients to update local model. We will repeat this federated updating process until the recall model training converges. In this section, we will analyze the privacy protection ability of Uni-FedRec. First, in Uni-FedRec, users’ behavior data is locally stored in their clients and is never uploaded to the server, which can effectively alleviate users’ privacy concerns and risks of large-scale privacy leakage (McMahan et al., 2017). To train models and serve users, Uni-FedRec only needs to upload model gradients and user interest representations to the server. These intermediate variables usually contain much less private information than raw data according to data processing inequality (McMahan et al., 2017; Qi et al., 2020). Besides, these variables are aggregated from multiple behaviors of a user, making it more difﬁcult to infer a speciﬁc user behavior from them. Second, we propose an interest decomposer-aggregator method to protect interest representationr. Since protected interest representationˆris aggregated from basic interest embeddings shared among users instead of user’s clicked news, it is more difﬁcult to infer a speciﬁc user’s clicked news fromˆrthan r. Besides, in this method,r∈ Rwhich belongs to ad-dimensional spaceRis projected into aBdimensional spaceR. SinceBis much smaller thandin our settings,ˆrcan lose much information on user privacy. Third, we apply the LDP technique to protect both interest representations and gradients. Based on the LDP theory(Choi et al., 2018), in Uni-FedRec, the privacy budget upper bounds of protected gradients and protected interest representations can achieveand, respectively. Since a smaller privacy budget means better privacy protection ability, Uni-FedRec can achieve a trade-off between model accuracy and privacy protection by adjusting noise intensity. We conduct experiments on two real-world datasets. The ﬁrst one is MIND (Wu et al., 2020e), a public news recommendation dataset constructed by logs of 1 million users in the Microsoft News during six weeks (Oct. 12 to Nov. 22, 2019). The second one is NewsFeeds, which is constructed by Table 2: News recall performance of different methods. Higher recall rates mean better performance. T-test on these results veriﬁes the improvement of Uni-FedRec over baseline methods is signiﬁcant at level p ≤ 0.001. Table 3: Privacy protection performance of different methods, which is measured by rates of user’s historical clicked news recalled from the news pool. Lower recall rates means better privacy protection performance. logs of 20,000 users from a commercial news feeds production of Microsoft during two weeks (Mar. 18 to Apri. 1, 2020). User logs in the ﬁrst week are used to construct historical user behaviors, logs in the last two days are used for evaluation, and other logs are used for model training. More information on MIND and NewsFeeds is in Table 1. Next, we will introduce settings of our experiments. In our privacy-preserving news recall model, dimensions of both news and user representations are256. The self-attention network contains16attention heads with16-dimensional output vectors. The clustering distancedis set to 1. The cluster-wise attention network is a two-layer dense network with128-dimensional hidden vector. The number (B) of basic interest embeddings is set to30and dimensions of these basic interest embeddings are 256. The clipping scaleδis set to 0.2 and intensityλof the interest representation perturbation noisenis set to 1.2. Besides, we combine four different news ranking models, i.e., FedRec (Qi et al., 2020), NRMS (Wu et al., 2019d), LSTUR (An et al., 2019) and NAML (Wu et al., 2019a), with our proposed privacy-preserving news recall model in Uni-FedRec. Embeddings generated by the news model and user model in these ranking methods are256-dimensional. We randomly sampler = 2%clients in each round for model updating. The gradient clipping scaleθis 0.1and intensityλof gradient perturbation noise nis0.01. Negative sampling ratios for training both recall and ranking models, i.e.,KandK, are 4. The learning rateωis 0.05. Codes of UniFedRec are released for reproducing our method. We compare the performance of different recall models on (1) news recall and (2) privacy protection. News recall performance is measured by rates of users’ future clicked news in the topKrecalled candidate news (R@K). Privacy protection performance is measured by rates of users’ historical clicked news in the topKrecalled candidate news. Since it is easier to infer user private information from representations that can recall more users’ historical clicked, methods that achieve lower recall rates on historical clicks are considered to have better privacy protection performance. Here are baseline methods we compared: (1) YoutubeNet (Covington et al., 2016): averaging clicked news representations to recall candidate news. (2) HUITA (Wu et al., 2019e): attentively aggregating clicked news for news recall. (3) EBNR (Okura et al., 2017): Table 4: Recommendation performance (AUC) of different methods on MIND, where rows and columns are different recall and ranking methods, respectively. learning user representation via a GRU network. (4) SASRec (Kang and McAuley, 2018): using a self-attention network to learn user representation. (5) PinnerSage (Pal et al., 2020): learning multiple interest representations via news clustering. (6) Octopus (Liu et al., 2020): modding multiple user interest via elastic archive network. We repeat experiment on each method5times and show results in Table 2 and Table 3. As shown in Table 2, Uni-FedRec signiﬁcantly outperforms baseline methods in news recall. This is because users usually have diverse interests, and it is difﬁcult for baselines to comprehensively model user interests. Different from these methods, Uni-FedRec learns multiple interest representations for a user from clusters of clicked news, which can comprehensively model diverse user interests in different ﬁelds. Besides, as shown in Table 3, Uni-FedRec can better protect user privacy than baseline methods. This is because baseline methods build user interest representations from the aggregation user’s clicked news, making user’s clicked news can be easily inferred from interest representations, which raises privacy leakage risks. Different from these methods, we propose to synthesize interest representations by combing privacy-insensitive basic interest embeddings shared among different users instead of user’s clicked news, which can better protect user privacy encoded in user representations. Next, we combine different recall models and ranking models to evaluate the overall recommendation performance. We ﬁrst use the recall model to generate400candidate news and further use the ranking model to rank these candidate news. We use users’ real click behaviors as ground truth and report AUC scores. Experimental results are presented in Table 4, and we only show results on MIND dataset Figure 4: Ablation study on Uni-FedRec. in the following sections due to space limitation. Results show that Uni-FedRec can consistently outperform baseline recall models when they are combined with a same ranking model. These results further verify that Uni-FedRec can outperform baseline methods in recommendation accuracy. 4.4 Ablation Study As shown in Fig. 4, we verify the effectiveness of important modules in Uni-FedRec by removing them. First, after removing the LDP module in the recall model, news recall performance of Uni-FedRec improves while the privacy protection performance declines. This is intuitive since perturbation noise will make Uni-FedRec less accurate. Second, removing the hierarchical clustering framework hurts the news recall performance. This is because a user usually has diverse interests, which can be more comprehensively modeled by multiple interest representations. Third, removing BIE seriously hurts the privacy protection performance. This is because protected interest representations are synthesized from basic interest embedding shared among different users, which contain much less private information of a speciﬁc user. 4.5 Inﬂuence of the LDP Noise As shown in Fig. 5, we evaluate the inﬂuence of intensityλof LDP noisenon Uni-FedRec. We ﬁnd that with the increase ofλ, news recall performance of Uni-FedRec declines and the privacy protection ability of Uni-FedRec increases. This is intuitive since incorporating larger noise will more seriously hurt the information capability of interest representations on both user interests and user privacy. Results in Fig. 5 inspire us that we can ﬁnd a trade-off between recommendation accuracy and privacy protection by adjusting the intensityλof LDP noise non interest representations. Figure 5: Inﬂuence of λon Uni-FedRec. Figure 6: Inﬂuence of don Uni-FedRec. In Fig. 6, we show the inﬂuence of clustering distance thresholddon Uni-FedRec. First, afterd increases, recall performance of Uni-FedRec ﬁrst increases. This is because smalldmakes UniFedRec build too many interest clusters, which may bring some noise and hurt the accuracy of interest representations. Second, whendbecomes large enough, recall performance begins to decline. This is because largerdmakes Uni-FedRec build fewer interest clusters and make it harder to comprehensively cover diverse user interests. Third, with the increase ofd, the privacy protection performance of Uni-FedRec declines. This may be because when Uni-FedRec contains more interest channels, a single interest representation contains less private information. It may be easier for our proposed interest decomposer-aggregator method to protect private information encoded in them. Thus, a moderate value ofd, i.e., 1, is suitable for Uni-FedRec. Fig. 7 shows the model training convergence of our recall model. The model is trained with different ratios of clients for a single federated model updating Figure 7: Convergence curves in model training. round. First, training of Uni-FedRec can usually converge in no more than two hundred steps, which veriﬁes the efﬁciency of federated training of our recall model. Second, training convergence of UniFedRec will get faster and more stabilized if more clients can participate a single updating round. This is intuitive since updating parameters with more training data is usually more accurate. In this paper, we propose a uniﬁed privacypreserving news recommendation framework (UniFedRec) that can utilize user data locally stored in user clients to train models and serve users. Our framework contains a recall stage and a ranking stage. In the recall stage, the user client ﬁrst employs a recall model to locally learn multiple interest representations from clicked news to model diverse user interest, which are further uploaded to the server to recall candidate news from a news pool. In the ranking stage, candidate news are distributed to the user client and locally ranked for personalized display. Besides, we propose an interest decomposer-aggregator method with permutation noise to protect private user information encoded in interest representations. In addition, Uni-FedRec collaboratively trains recall and ranking models on user data decentralized in massive user clients in a privacy-preserving way. Experiments on two realworld datasets show that our method can signiﬁcantly outperform baseline methods and effectively protect user privacy. This work was supported by the National Natural Science Foundation of China under Grant numbers U1936216, U1705261, and Tsinghua-Toyota Research Funds 20213930033.