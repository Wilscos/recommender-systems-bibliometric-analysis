Abstract—Machine learning on graph-structured data has recently become a major topic in industry and research, ﬁnding many exciting applications such as recommender systems and automated theorem proving. We propose an energy-based graph embedding algorithm to characterize industrial automation systems, integrating knowledge from different domains like industrial automation, communications and cybersecurity. By combining knowledge from multiple domains, the learned model is capable of making context-aware predictions regarding novel system events and can be used to evaluate the severity of anomalies that might be indicative of, e.g., cybersecurity breaches. The presented model is mappable to a biologically-inspired neural architecture, serving as a ﬁrst bridge between graph embedding methods and neuromorphic computing – uncovering a promising edge application for this upcoming technology. Multi-relational knowledge graphs (KGs) [1] are rich data structures used to model a variety of systems like industrial projects [2] and mathematical proofs [3]. It is therefore not surprising that the interest in machine learning algorithms capable of dealing with graph-structured data has increased lately [4]. This broad applicability of graphs becomes apparent when summarizing them as lists of triple statements (node, edge, node), e.g., (M.Hamill, plays, L.Skywalker) and (L.Skywalker, appearsIn, StarWars) – with individual entries being called subject, predicate and object. This way, complex relationships between different entities and concepts can be modeled, and machine learning can be used to discover novel facts, i.e., predict or evaluate new edges in the graph. A widely adapted approach of making the symbolic elements of graphs accessible to machine learning methods are graph embedding algorithms [4]–[6], where nodes and edges of a graph are mapped into a low-dimensional vector space while conserving graph properties like node proximity or link existence. Graph-based data processing is commonly found in industrial settings where vast amounts of symbolic data from different data silos are combined, stored on servers and used to train models on the cloud [7]. In contrast, here we are interested in scenarios where graph-structured data is analyzed dynamically, without vast data stores or off-loading to the cloud, which is generally known as the edge computing paradigm. One such application can be found in industrial automation systems, where nowadays a convergence of operational technology (OT) and information technology (IT) promises high ﬂexibility, reliability and efﬁciency in factory systems – but comes at the price of increasingly complex dynamics between system components and a lack of determinism and isolation [8]. This leads to novel challenges like guaranteeing system integrity and protecting against cyberattacks, i.e., detecting anomalies in the system before they result in a disruption of operation or even physical damage. We evaluate a novel graph embedding algorithm for edge computing in such a scenario. The components of a modern industrial automation system and their complex interaction with each other (Fig. 1A) can be modeled as a KG [9], taking the form of a list of triple statements with two parts: a static one describing the structure and a dynamic one describing the ongoing interactions between system components (Fig. 1B). Data generated this way are incredibly sparse, i.e., only a tiny fraction of possible triples are observed or even valid, as well as streaming in nature such that triples can appear multiple times and underlie stochastic variations. Using graph embedding, we reformulate the anomaly detection task as a link prediction task: events in the automation system are equivalent to new edges appearing in its graph representation that can be evaluated using the learned embeddings. However, we found that standard graph embedding algorithms perform poorly on such industrial graphs, mainly because they expect static graphs as input. In addition, as usually only valid triples are available for training, “negative samples” have to be generated from them, introducing possibly erroneous assumptions. Hence, we introduce a probabilistic, energy-based graph embedding model that is capable of dealing with stochastic variations in graphs and requires no negative samples for training. The presented model provides sensible likelihood estimations in anomaly detection tasks and can further be mapped to a neuronal representation, opening an exciting prospect for neuromorphic hardware [10]–[13] – which take inspiration from the brain to achieve energy efﬁciency and low latency – as edge learning devices for online graph reasoning. In the following, we ﬁrst present the industrial automation demonstrator used to generate application-speciﬁc graph data. Afterwards, we introduce the energy-based graph embedding model, evaluate it on the demonstrator system and provide a conceptual description of how the model could be mapped onto a neuromorphic edge device. We employ an industrial automation demonstrator that mimics the integration of OT and IT technologies in modern industrial systems. This way, we are able to generate data that captures the complexity and characteristics of such systems while having a controllable and ﬂexible data generation process. On the OT side, the demonstrator consists of a Siemens S7-1500 Programmable Logic Controller (PLC) connected to multiple subsystems like the drive of a conveyor belt, industrial cameras, a human-machine interface (HMI) and sensors (Fig. 1A, left). Information about the system’s internal state and sensor measurements are made accessible to edge devices – dedicated and tightly integrated computing resources – via an OPC UA server [14] by the PLC. On the IT side, applications (apps) are dynamically hosted on edge computers that access and process this data (Fig. 1A, right). An app deployment cycle is realized by a development environment, consisting of several developer hosts that regularly push apps to an app repository based on data made available by a historian – a database that stores process data from the automation system. Novel apps are then regularly pulled via HTTPS and hosted on the edge computers. To increase the degree of realism, the dev. hosts access the internet from time to time. All three components of the demonstrator system – engineering data describing the automation part, app activity through the OPC UA server and network events – can be modeled as multi-relational graphs and consequently combined in a single KG. Thus, a machine learning algorithm operating on this KG can potentially utilize all three domains to make novel predictions and evaluate unseen activity. More details on the industrial demonstrator and how to model it as a KG can be found in [9]. To analyze the aforementioned data, we propose a probabilistic model of graph embeddings based on an energy function that takes inspiration from the widely used graph embedding algorithm RESCAL [15]. In RESCAL, a graph is represented as a tensor X, where entries are 1 if a triple (s, p, o) occurs in the graph and 0 otherwise. This way, ﬁnding graph embeddings can be rephrased in terms of a tensor factorization problem with each graph entity s being represented by a N-dimensional vector eee∈ Rand each relation p by a matrix RRR∈ R. We denote the sets containing all node and relation embeddings E and R, respectively. The problem of ﬁnding embeddings is then equivalent to minimizing the reconstruction loss which can be done via gradient descent. Usually, we are only aware of valid triples and the validity of all other triples is unknown, i.e., cannot be modeled by setting tensor entries to 0. Instead, so-called “negative samples” are generated from the training data by randomly exchanging either subject or object entity in a data triple [6]. During training, these negative samples are continuously generated and then presented as “invalid triples” with tensor entry 0. After learning, novel triples are scored using the obtained embeddings RESCAL can be further regularized, e.g., by constraining RRR to be diagonal, which is generally known as DistMult [16]. From this, we deﬁne the probability of observing X where we sum over all possible graph realizations X. Here, the X∈ [0, 1] are binary random variables indicating whether a triple exists, with the probability depending on the score of the triple. For instance, a triple (s, p, o) with positive score θis assigned a negative energy and hence a higher probability that X= 1. This elevates RESCAL to a probabilistic model by assuming that the observed graph is merely a sample from an underlying probability distribution, i.e., it is a collection of random variables. Since triples areQ treated independently here and Z =(1 + exp (θ)), the probability can be rewritten as p(X) =(1 − σ (θ))σ (θ) , (6) where σ(·) is the logistic function. Thus, the probability of a single triple (s, p, o) appearing is given by σ (θ). The model is trained by adjusting node and edge embeddings such that the log-likelihood of observed triples is where D is a set of (sub)graphs B available for learning, e.g., mini-batches or time slices of a data graph D = ∪B. h·idenotes averaging over the elements y of Y . The update rules can be rewritten as where S is a list of triples generated by the model itself, e.g., via sampling (see next section). Thus, different from graph embedding algorithms like RESCAL, no explicit negative samples are required to train the model. Relations learn to align with the inner product of subject and object embeddings they occur with, while node embeddings learn to align with the latent representation of their counterpart, e.g., eeelearns to align with the latent representation of the object RRReeeif (s, p, o) is in the data. Both learning rules consist of two phases, a data-driven and a model-driven phase – similar to the wakesleep algorithm used to train, e.g., Boltzmann machines [17]. In contrast to the data-driven phase, during the model-driven phase, the likelihood of model-generated triples S is reduced. To generate triples from the model, we use Markov Chain Monte Carlo (MCMC) sampling – more precisely, the Metropolis-Hastings algorithm [18] – with negative sampling as the proposal distribution. For instance, if the triple (s, p, o) is in B, we propose a new sample by randomly replacing either subject, predicate or object, creating a corrupted KG ˜B containing the new sample instead of (s, p, o). The sample is accepted with probability T = min[1, p(˜B)/p(B)], e.g., T(s, p, o) → (s, p, q)= min1, expeeeRRR∆e∆e∆e, (10) with ∆e∆e∆e= eee− eee. The transition probability depends on the distance between the embeddings, i.e., if the embeddings of nodes (or relations) are close to each other, a transition is more likely. This process is ergodic and can be repeated on the new sample to generate a chain of samples, exploring the neighborhood of the data triple under the model distribution. As a ﬁrst benchmark, we compare the performance of our proposed probabilistic model with RESCAL for different graph-theoretic metrics. For this, we employ a recording from the demonstrator system with some network and data activity between demonstrator and edge computers [19], leading to a KG that we randomly split with a ratio of 8/2 into mutually exclusive training and test sets, resulting in 12399 training and 2463 test triples with 3529 entities and 39 relation types. To compare the algorithms, we use the ﬁltered mean reciprocal rank (MRR) and hit-based metrics (hits@k) from the graph embedding literature, see [6] for more details. Both RESCAL (ReSE) as well as our energy-based model without (EnM) and with (EnMd) diagonally-constrained relation matrices achieve similar performance, although our model appears to be less prone to overﬁtting. EnMd is especially interesting for neuromorphic implementations, as its symmetric score function enables simple wiring between embedding and output layer (see Fig. 4A), i.e., no additional mechanism is required to distinguish whether a population acts as subject or object. We further show that all models are capable of separating positive and negative samples sufﬁciently (Fig. 2C). In the next section, we introduce RESCAL trained with a Kullback-Leibler loss (ReKL), which is included in Fig. 2 for completeness. As an additional comparison, we applied EnM to the wellknown UMLS data set – a biomedical KG encoding the relationships between diseases and chemical compounds – reaching a total test MRR of 0.80. With our implementation of ReSE and ReKL, we reach a similar total test MRR of 0.79. The primary benchmark of our model is its application in anomaly detection tasks, which we demonstrate here for the industrial automation demonstrator described in Section III. 1) Experiment setup: We ﬁrst record a baseline of normal behavior by running the demonstrator for approximately 50min, resulting in a training graph with 37441 triples, 4347 entities and 38 relation types. During this time, 5 apps are regularly pulled from the app repository and hosted on 3 edge computers, with apps routinely accessing data from the demonstrator. At the same time, 3 dev. hosts consistently connect to the historian and the app repository, while also accessing the internet every now and then. During test time, we alter the behavior of several system components to investigate whether triples generated due to these changes are assigned a decreased likelihood by our model. This is shown for three separate scenarios here: (i) In the ﬁrst scenario, HTTPS network activity not observed during training is introduced. These can be events that are expected, e.g., a dev. host might show novel behavior that has been seen for another dev. host during training. But they might also be suspicious, like a device that is not a dev. host accessing the historian (the historian is usually only accessed by dev. hosts), or even highly suspicious, like an edge device accessing the internet (edge devices should stay in the local network). (ii) In the second scenario, SSH network activity not observed during training is introduced. For instance, an expected deviation would be a dev. host accessing the app repository with slightly larger data volume than during training (data volume variations in the local network are usual); SSH activity between edge devices would be highly suspicious (edge devices are never the recipient of SSH connections). (iii) In the third scenario, edge computer #2 starts a network scan, leading to many previously unseen connections originating from this device. In general, we separate system events into ﬁve categories depending on their severity: Highly Suspicious, Suspicious, Unexpected, Expected and Observed, where the severity is based on the behavior deﬁned during the baseline. A thorough description of the baseline and test cases can be found in [9]. 2) Evaluation: The baseline data is integrated into a KG D and used to train graph embedding models. The activity of the industrial automation system during test time takes the form of a list of triples, i.e., novel edges appearing in the graph, with 275, 265 and 351 triples for each scenario, respectively. We use the embeddings (E, R) learned from the baseline (D) to evaluate the severity of these triples, i.e., we estimate how unlikely new edges (s, p, o) in the graph are given the structure of the baseline P (X= 0 | E, R) ≈ P (X= 0 | D). For instance, we expect that after training, the node embeddings of dev. hosts will resemble each other (since they share a lot of structure in the baseline KG), and thus activity typical for dev. hosts should be deemed likely by the model, even if it was not observed for one of the dev. hosts during the baseline. In case of the energy-based model, we deﬁne the “suspiciousness” of a triple as S= 1 − σ (θ), with suspicious activity having high values. We compare our model with standard RESCAL (S= 1−θ) and an alternative probabilistic version of RESCAL trained using a Kullback-P Leibler loss L=Xlog, with S= −θ. As an additional reference, we applied the translating embedding model TransE [20], where embeddings are found such that S= keee+ rrr− eeek ≈ 0 for valid triples, with relation embeddings rrr∈ R. For this benchmark task, we are not looking for a binary anomaly classiﬁer, but rather a severity estimator, i.e., a model that can sort events according to different levels of suspicion. Typically, such alerts would be sent to a security analyst for further investigation, and thus having meaningful scores is beneﬁcial as it allows prioritization of alerts. Therefore, we evaluate models by investigating their capability of sorting system events according to their ground truth severity. 3) Results: After training on the baseline, our proposed model is capable of generating well-calibrated estimates in all three test scenarios, with triple likelihoods approximately matching their assigned degree of severity (Fig. 3A, top left). This trend appears consistently over a wide range of hyperparameters (not shown here) and is even more pronounced when looking at individual system components (Fig. 3B): in this case, we only look at triples that include a certain entity of the automation system as subject or object. We then rank the triples in a list according to the suspiciousness our model assigns. As expected, our model is capable of sorting the severity of events correctly, e.g., Highly Suspicious events are on the top, while Expected and Observed ones are on the bottom of the ranked list (Fig. 3B). ReSE and TransE perform rather poorly on this task, producing scores for the different suspiciousness levels that are almost binary, i.e., events are either scored high or low (Fig. 3A, right). For instance, contrary to EnMd, ReSE and TransE are not capable of separating Expected, Suspicious and Highly Suspicious events consistently in all scenarios. This is most likely due to the stochastic nature of our data set, which we test by modeling the data with an alternative probabilistic version of RESCAL using a Kullback-Leibler loss (ReKL), resulting in much better performance (Fig. 3A, bottom left), but still slightly worse than with the energy-based approach. The presented results depend on the chosen graph representation of the data, and we are conﬁdent that performances can be further improved by choosing representations that are more attuned to the individual benchmark cases. Simulations were done using Python 3.7.7 and PyTorch 1.6.0. Embeddings were initialized from a normal distribution Nµ, σ. The simulation parameters are given in Table I. Data and code are available on github.com/dodo47/cyberML. For the suggested application, our algorithm is required to be trained and deployed on the edge to ensure continuous monitoring of industrial automation systems. An emerging technology for edge computing is neuromorphic hardware [21]–[23], i.e., chips that borrow the architecture of the brain to achieve high energy efﬁciency and low latency. However, for an algorithm to be applicable on neuromorphic hardware, it has to take a neuronal form that obeys physical restrictions like locality of information. In the following, we show that our model can be written in such a form, opening the dynamic world of system monitoring as a potential application area for neuromorphic edge devices. The neuronal representation of our model is shown in Fig. 4A. Entity embeddings eeeare encoded by populations of neurons, i.e., with one neuron per dimension of eee. These project statically to output neurons, one for each relation type. Every output neuron integrates input using a tree-like structure, where each branch encodes a component of the relation embedding RRR. At each of these branches, tripleproducts of the form eReare evaluated and subsequently integrated with contributions from other branches through the tree-like structure (Fig. 4B). The integrated input is then fed into an activation function σ(u) = min1, with η ∈ [−1, 0, 1]. Through η, the output neurons can return both the probability σ(·) of a triple statement to be true (η = −1 or 1) and the transition probability T (·) required for sampling (η = 0). If both probabilities are evaluated using sampling, the output can be interpreted as stochastic spiking neurons that spike given an input (“triple accepted”) or remain silent (“triple rejected”). η is further used to gate between three different phases for learning: a data and model-driven phase (η = ±1) and a freerunning phase (η = 0) – which is introduced in the learning rules by adding η as a multiplicative factor. In biologically inspired network models, such a global “third factor” [24], [25] is often used to model the effect of neuromodulators, which are known to modulate both the intrinsic ﬁring properties of biological neurons as well as plasticity [26], [27]. During the free-running phase, the model generates triples which are then replayed to the network in the model-driven phase. Both during the data and model-driven phase, for each triple (s, p, o) parameter updates are calculated, i.e., ∆RRR∝ η · s· eeeeee, the spike response of output neuron p, acting as a teaching signal. For instance during training, if a triple (s, p, o) is shown to the network, we excite output neuron p (s= 1) while inhibiting all other output neurons (s= 0). To update the node embeddings, feedback signals are sent from the output neurons to the entity neurons through a prewired feedback structure. Relation embeddings are updated using local information available in the output neurons. Input is presented to the network by selecting the corresponding embedding populations, which can, e.g., be achieved through gating, resembling a “memory recall” of learned concepts that are imprinted into the network through repeated co-activation of embedding populations and output neurons. A critical property of our model enabling this neuromorphic representation is that predictions are probabilities that can be locally calculated by output neurons. This is, e.g., not the case for ReSE, where the score is unbounded, and ReKL, where the score of all possible triples is required to calculate probabilities via the softmax operation. As a downside, embedding neurons are not guaranteed to take strictly positive values and hence are not compatible with a spike-based or even ratebased interpretation – as required for neuromorphic devices. Nevertheless, a non-negative variant of RESCAL that might be incorporated in our model has been described in [28]. Energy-based models have a long history in neuroscience and artiﬁcial intelligence [29], [30], taking inspiration from physics to describe neuronal dynamics and computations in a structured and compact way. We introduce an energybased model for probabilistic modeling of multi-relational graphs. Different from classical graph embedding algorithms, our model is capable of dealing with the characteristics and stochastic variations of dynamically generated data, like multiple observations of the same triple statement over time. Furthermore, our model can be trained on true triples only and hence, no assumptions have to be made about how negative samples are generated from true triples in the training data. In principle, other scoring functions besides the one from RESCAL can be used in the energy function as well. By rephrasing anomaly evaluation as a link prediction task, we use our proposed model to estimate the severity of anomalous events in an industrial automation demonstrator. Our model is capable of producing well-calibrated estimates for the severity of anomalous events, and outperforms standard graph algorithms like RESCAL and TransE on this application. In fact, our model learns to approximate the data distribution, i.e., the typical behavior of the demonstrator, and can consequently be used to detect shifts in the data generation process. Since triples are human-readable and many system events (like network connections) are represented by multiple triples, we are conﬁdent that such estimates can provide security analysts with additional context information and detailed clues to quickly identify and assess system anomalies. A hallmark of this use case is the dynamic nature and privacy requirements of the recorded graph data that, in practice, require processing on the edge. Therefore, since our model can be mapped to features of neuromorphic devices, the presented results hint at a novel application area of neuromorphic hardware as edge devices for online graph analytics. This work was partially funded by the Federal Ministry for Economic Affairs and Energy of Germany (BMWi) within the IIP-Ecosphere Project. We thank Marcel Hildebrandt, Serghei Mogoreanu and Martin Ringsquandl for helpful discussions, Johannes Frank for setting up the demonstrator and our colleagues at Siemens SMR and the AI Lab for their support.