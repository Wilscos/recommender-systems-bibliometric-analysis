The industry for children’s apps is thriving at the cost of children’s privacy: these apps routinely disclose children’s data to mu ltiple data trackers and ad networks. As children spend increasing time online, such exposure accumulates to long-term privacy risks. In this paper, we used a mixed-methods approach to investigate why this is happening and how developers might change their pr actices. We base ou r analysis against 5 leading data protection frameworks that set out requirements and recommendations for data collection in children’s ap ps. To understand developers’ perspect ives and co nstraints, we conducted 134 surveys and 20 semi-structured interviews with popular Android children’s app developers. Our analysis revealed that developers largely respect children’s best interests; however, they have to make compromises due to limited monetisation options, perceived harmlessness of certain thirdparty libraries, and lack of availability of design guidelines. We identiﬁed concrete approaches and directions for future research to help overcome these barriers. • Huma n-centered computing → Empirical studies in HCI. children’s privacy, age-appropriate design, developer practices, developer values, children’s apps ACM Reference Format: Anirudh Ekambaranathan, Jun Zhao, and Max Van Kleek. 2021. “Money makes the world go around”: Identifying Barriers to Better Privacy in Children’s Apps From Developers’ Perspectives. In CHI Conference on Human Factors in Computing Systems (CHI ’21), May 8–13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3411764.3445599 Children are now spending an unprecedented amount of time online [52]. In the UK, for instance, over 83% of children between the ages of 12 and 15 own a smartphone, and spend over 20 hou rs a week using online apps and services [27]. It is not only teens and older children who are establishing an increasing presence online; even children under the age of 5 are now spending time daily on tablets and smartphones, leading to an increase in the number of apps designed for children [27]. Children are a vulnerable user group to a wide range of online risks, such as exposure to inappropriate content [29, 36, 78] and potential health risks caused by prolonged screen time [72]. However, one particularly important, yet often overlooked, risk for children today is the omnipresence o f data trackers in thirdparty libr aries in apps they use every day [23, 67]. These libraries often collect sensitive data about children [24, 49, 68] including location information, which can then be sent to data brokers for data proﬁling. Such tr ackers are widely identiﬁed in apps often used by children [23], because developers predominantly rely on targeted advertisements from third-party libraries as their key revenue [14, 28, 46] . M oreover, previous research has shown that o nly a fraction of the apps categorised to be intended for children are associated with a privacy policy [47]. In response to concerns about the fraught state of children’s privacy, governments and public sector or ganisations have assembled working groups and consultations to understand the privacy landscape for children in the digital space and proposed regulatory interventions. The privacy landscape has seen signiﬁcantly changed in t he past few years. For example, in 2018 Europe saw the introduction of the General Data Protection Regulat io n (GDPR), which aims to recognise personal d ata as a fundamental right, w hich was followed by a speciﬁc section about GDPR for children (GDPRK). Another important initiative is the statutory Age Appropriate Design Code issued by the Information Commissioner’s Oﬃce (ICO) in the UK [11], as a clariﬁcation of GDPR-K in the UK, aiming to make data protection a key element when designing services for children from 2021. Despite these initiatives, research has shown that children’s data protection still has a long way to go. For example, recent research shows t hat developers still rely on privacy invasive advertising networks in their development pract ices [55].Thus in this work we aim to examine the following research questions: • RQ1: What did app developers perceive as their responsibilities when designing for children? • RQ2: What are the main practices adopted by developers for achieving data protection and how do they align with the leading data protection frameworks from diﬀerent sectors? • RQ3: What are key barriers for the adoption of the leading data protectio ns for the app developers and how can these barriers be overcome? We conducted 20 interviews and 134 surveys with family app developers, and accompanied by an analysis of 5 leading data protection frameworks to understand best practices expected from developers and how they support developers in implementing these practices. Our ﬁndings show that developers feel responsible for designing apps with the best interests of children in mind, including that children’s privacy should be respected and thus dat a col lection should be minimised. However, we also identiﬁed several barriers which were in the way of realising best p ractices set ou t in the data protection frameworks. First, developers ﬁnd it diﬃcult to navigate the complex and opaque landscape of third-party libraries. T here is a lack of awareness of age-appropriate libraries, forcing developers to choose major data controllers as their thirdparty service providers. Second, developers struggled to monetise their apps in ways that did not rely on advertising. The mar ket pressure and co mpetition made alternative and potentially pr ivacyfriendly business models, such as oﬀering premium apps, unsustainable. Third, t here was a general lack of awareness of guidelines for designing for children, resulting in developers relying on terms and requirements set out by market leaders, such as Google, which are not always aligned with the best interests of children. We found that data protection frameworks did not suﬃciently support developers in addressing these barriers. Based on our ﬁndings, we make recommendations to help improve the state of app development for children. We propose to include concrete recommendations in guidelines regarding selecting data controllers, advertising networks, and third-party libraries. Furthermore, we also advocate for heightened support for both end users and developers through tools which can shed light on the otherwise opaque data economy. Lastly, our research also highlights the impo rtance of needing industry support to make major changes to the app development landscape. Children spend more time on digital devices now than ever before [65] and this is raising concerns in public and policy circles about children’s privacy and the commercial use of their data [54]. As it stands, children’s privacy ﬁnds itself in a vulnerable and endangered position [52]. In the UK alone, 82% of children between 5 and 7 spend almost 10 hours per week online [65]. This is particularly signiﬁcant today, as the digital p latforms c hildren use and its resulting data traces are owned by the private sector [57]. In fact, commercial o rganisations are gathering more data from children than governments are capable of [63]. They use a range of methods, often invasive, to track children’s activities [58], such as cookie-placements, web-beacons, and advertising IDs. In addition, children are also often nudged into disclosing more personal information than is necessary [17, 70] or as a tr ade-oﬀ to access a service [45, 56]. However, despite their omnipresence online, children do not understand digital risks as well as most adults [52], and they have a poor understanding of privacy related contexts [43, 81]. For example, children fail to understand why their data is valuable to third parties [45], how their data is collected [13, 34], how it is stored and analysed [25], and how it may be used in the future [25, 60, 66]. They also ﬁnd it c hallenging to understand privacy terms and conditions, because of their length and legalese [22], and feel forced to accept the terms laid out to them [4 5]. Children feel that targeted advertising and p roﬁling is a p ar t of digital life [45] and that there is very little they can change abou t their behaviour t o prevent this [45, 66]. Loss of privacy and data sharing to third parties is known to lead to concrete harms, such as identity theft and fraud [33], and the normalisation of a data surveillance culture [42]. However, the most crit ical reason why a proactive stance against data collection is needed, is due to harms arising from long t erm risks to children’s reputation and opportunities as they grow older [53]. These harms and risks are particularly important to consider, as their exact nature is still unknown and may evolve over time. It is for these reasons that children form a particularly vulnerable user group and that concerns have been expressed about the the ‘dataﬁcation’ of children [54]. The need for children’s support in navigating privacy choices, is not entirely ungrounded. In the next section we discuss the pervasiveness of data harvesting features in apps for children. Mobile apps have been shown to be particular ly threatening to children’s privacy [2 3]. This is primarily attributed to the use of thirdparty libraries, which are increasingly prevalent in t oday’s apps [20]. These libraries have permissions to collect sensitive data [24, 49] and frequently access location permissions. They are known to track call logs, browser history, and contact information for the purpose of targeted advertisements, even if that was not the intended functionality [37]. This is not diﬀerent for children’s data, as apps in the “Family" category of the Google Play Store have shown to have the second highest number of data trackers associated with them [23]. The prevalence of third-party libraries can be explained by the fact that developers rely on targeted advertising for generating revenue [14, 28, 46], which in turn uses third-party libraries to collect targeted data. Additionally, they also simplify development, provide increased functionality, and may be more secure than proprietary software modules [35]. Addressing these trackers is not trivial on mobile applications, as Android and major other smartphone operating systems do not provide end users the freedom to control third-party tracking through apps. Users are therefore dependent on privacy regulations and app review processes of marketplaces [15] to protec t them. However, review processes are not always transparent, and Google and Apple have thus far been poorly incentivised to exert control over the data tracking ecosystem, as they hold a vested interest in the advertising industry [10]. Instead, we have seen an increase in tool s to better inform users on the dissemination of their data to third-party trackers [19, 30, 74, 75]. However, more often than not, such tools are aimed at adults, and both children and adul ts may have a diﬃcult time contextualising implications of tracking in t he privacy and security landscape. The problem in tackling these issues that children’s privacy rights have thus far not been suﬃciently supported [51] and there have been calls for regulatory interventions to add ress this issue [41]. In recent years, the development landscape has seen changes. Europe introduced the General Data Protection Regulation (GDPR) in 2018, which recognises the protection of personal data as a fundamental right. The UK Information Commissioner’s Oﬃce put into eﬀect a statutory code for developers requiring them to make data protection a central tenet in their design [11]. It is not only p ublic bodies which have pushed for changes, in 2 019 Apple changed their policies to prohibit third-party advertising and analytics [8]. The privacy changes seen in recent years is placing more responsibility on developers to create appropriate apps for children. However, research on developers’ privacy perceptions and practices is limited. In a survey with 228 app developers exploring security and privacy decisions, it was shown that developers often are not familiar with the practices of third-party APIs due to diﬃculties in reading their privacy policies [2 0]. Anot her study investigated the app developers’ preference for using advertising as a revenue model and how they selected advertising networks [55]. The study found that developers perceived advertising often as the only way to proﬁt oﬀ apps and often chose advertising networks based on their popularity. In this section we present our review of ﬁve leading children’s data protection frameworks from three diﬀerent sectors (regulatory, private, and human rights organisations), with the aim of understanding requirements and expectations put on developers. Given that the main objective of our research is to investigate app developers’ data protection practices, our review focuses on parts of the frameworks related to data collect ing and handling. To compare across t hese frameworks, we use the statutory Age Appropriate Design Code (AADC), developed by the Information Commissioner’s Oﬃce (ICO) in the UK as the benchmark framework, which is seen as one of the most comprehensive regulatory frameworks to date and which ﬁts with our analysis apps from the UK app market. We aligned the data protection principles from the AADC against those from the following four frameworks: (1) (Regulatory) COPPA - The US Children’s Online Privacy Protection Act; (2) (Human r ights) COPFE - UNICEF’s Children’s Online Privacy & Freedom of Expression ; (3) (Private sector) Google’s ‘Designing Apps for Children and Families’; (4) (Private sector) Apple’s App Store Review Guidelines. We have not included the GDPR(-K) in this, because the AADC is designed to be a clariﬁcation of the enforcement of GDPR-K. The UK Information Commissioner’s Oﬃce (ICO) is an independent authority aimed at protecting and upholding information rights in the p ublic interest and promoting data privacy for individuals. In an eﬀort to address privacy concerns of children in the digital world, the ICO introduced the statu tory Age Appropriate Design Code (AADC) [11]. The code aims to ensure online services safeguard children’s personal data and comply with the GDPR. If services are not compliant by September 2021, the ICO can issue ﬁrms enforcement notices and ﬁnes up to 4% of their global turnover if they breach these data protection guidelines. The AADC consists of 15 codes which take into account principles set out in the United Nations Convention on the Rights of the Child (UNCRC). The code touches on many diﬀerent aspects related to design for children, including user-facing design practices (such as ‘transparency’, or ‘high privacy by default’), alignment with fundamental principles (such as supporting children’s best interest), support for data protection in new emerging technologies (e.g. connected toys), and regulations regarding data collection and minimisation. The focus of our study is to understand children’s app developers’ choices around data handling practices, and therefore we focus on those AADC codes specially related to data collection and minimisation. We excluded codes not directly related to development practices or data handling, such as ‘Data protection impact assessments’ and ‘Transparency’. We identiﬁed ﬁve codes from the AADC that are directly related to regulations of processing children’s data, which are summarised and explained in Table 1. The United Nations Children’s Fund (UNICEF), is an agency which is part of the United Nations and globally provides humanitarian aid to children. To support developers in realising the fundamental privacy r ights, in 2018 UNICEF introduced the ‘Children’s Online Privacy & Freedom of Expression’ (COPFE) industry toolkit, which describes ﬁve overarching principles [7, 63] to protect children’s right to privacy, personal data freedom of expression, protec tion of reputation, and access to remedy. This framework is largely aligned with the AADC, with a great emphasis on children’s fundamental rights; however, the framework also discusses many additional elements on the rights for children, such as the rights to be educated, have access to resources fo r risk coping, and the r ight for parents to have access to resources to help their children. Five of the sub-codes are identiﬁed to be closely related to the set o f AADC benchmark codes, with some slight diﬀerent emphases. Data proﬁling is discussed in COPFE, at a more general level: “Children enjoy protection from online proﬁling”. It d oes not specifically give examples of proﬁling or explain what constituted proﬁling. In the checklist, p roﬁling is placed in the context of behavioural Ta ble 2: This table lists 4 leading data protection frameworks and their alignment with the data protection principles s et out by the AADC. The cells without content indicate that the framework does not mention the associated AADC code. 5: Detrimental useChildren have the right not to of databe subje cted to attacks on their 8: Data minimisa-Children’s data are kept toShould not condition a child’s tionwhat is minimally necessaryparticipation on the collection advertising and suggests applying “speciﬁc protection”, as it “involves collecting and agg regating personal data”. As for data minimisation, COPFE states that collected data should be “ﬁt for purpose” and limited to “what is minimally necessary”. T he stakeholders involved in da ta sharing are explicitly named to include both “parents or guardians”, “media outlets and other third parties”, who should “refrain from sharing information that could undermine children’s current or future reputation”. Lastly, COPFE also argues against detrimental use of data, stat ing that “children have the right not to be subjected to attacks on their reputation” and “can seek the removal of content they believe is damaging to their reputation”. It does not explicitly talk about geolocation or location settings. The US Children’s Online Privacy Prote c tion Act COPPA R ule was established primarily to cur b direct marketing aimed at children under 13. It ﬁrst went into eﬀect in 2000 to implement the US Children’s Online Privacy Protection Act (COPPA) 1998. COPPA is aimed at operators who provide services targeted to children under the age of 13. Broadly speaking, COPPA focuses on the enforcement of a clear online privacy policy for services targeted at children, and support ing parents t o provide consent on their children’s behalf and protect their children’s personal data online. COPPA has one speciﬁc rule about data handling, which describes that children’s data should be retained “for only as long as is necessary to fulﬁll the purpose for which it was collected and delete the information using reasonable measures to protect against its unauthorized access or use” [12]. COPPA has an inherent data minimisation principle: service providers “Should not condition a child’s participation on the collection of m ore than the personal information that is required to drive that activity”. It has no spe ciﬁc conditions on data sharing, apart from that it requires parental consent assurance that reasonable security measures are in place by the party the data is being shared with. Geolocation data is treated as personally identifying information if it can be narrowed down to street or city name, in which case parental consent is requried. Regarding data proﬁling, a service is COPPA-compliant so long as no “speciﬁc individual can be identiﬁed proﬁling including through their persistent identiﬁers”. Under COPPA, the use of any persistent identiﬁers for the identiﬁcation of a speciﬁc individual is prohibited. When publishing apps on Google’s Playstore or Apple’s A ppstore, there are sp eciﬁc terms that the app has to uphold. Both Google and Apple have sections reserved for apps targeted at children in these t erms. Google has these outlined in their ‘Designing Apps for Children and Families’ [4] guidelines. Apps targeted at children must comply with the ‘Designed for Families’ programme, which lists 12 requirements apps must fulﬁl. The requirements cover diﬀerent aspects of app design, including the content of apps, policies regarding ads, and special restrictions about the use of Augmented Reality. Advertising is only allowed through certiﬁed ad SDKs [5] and personalised advertising, where users’ behaviour and interest data is used to customise advertising content, is not allowed. Collecting personal data of users is allowed as long as it is disclosed to them. Google fur ther requires apps to comply with GDPR, COPPA, and other applicable regulations. Similarly, before publishing on Apple’s App Store, apps have to comply with the ‘App Store Review Guidelines’ [40]. Apple has speciﬁc sections in t heir terms aimed at protecting children’s privacy. Third-party analytics and third-party advertising for children is permitted in limited cases, given that services do not transmit personally identiﬁable information and have their practices and policies publicly documented. Neither guidelines state anything speciﬁcally regarding detrimental use of d ata in the context of data processing, while both have clear requirements ab out data minimisation. Apple requires apps to only request access to data relevant to the core functionality of the app [40], and Google requires apps to disclose the col lection of any personal and sensitive information [4]. This is also the same for proﬁling, where neither allow personalised advertising and marketing. Apple also does not allow third-party targeted analytics and third-party advertising. In fact, Ap ple does not allow any data sharing with third p ar ties, while Google does not have a clear policy on this. Geolocation is also discussed in both guidelines. In this regard, Google is stricter than both the AADC and Apple, requiring t hat “Apps designed speciﬁcally for children cannot request location permissions” [4]. Apple’s stance is more aligned with the AADC and requires that location services should only be used “when it is d irectly relevant” [40]. Table 2 summarises how the terms speciﬁc to data handling in these guidelines are aligned with the AADC codes previously presented in Table 1. For those principles which do agree with AADC, we include d the deﬁnition of the principle to highlight the nuanced diﬀerences between all the frameworks. It shows that regulation/guidance around data-based proﬁling is required by all frameworks; however, there are some nuanced emphases amongst them. The AADC takes the position that children merit speciﬁc protections with regard to the use of their personal data, and therefore requires proﬁling to be transparent and to be turned oﬀ b y default. COPPA is stricter in this regard, prohibiting any proﬁling that may identify any speciﬁc individual. Apple and Google are aligned with this, and prohibit personalised advertising and marketing, with Apple strict er than Google and the AADC, as they prohibit third-party analytics and advertising. Data minimisation is also widely required by all frameworks, but with diﬀerent emphases. Apple’s is most aligned with the AADC, although it does not p rovide an option for children to ‘make separate choices over which elements’ to be activated yet. Interestingly, because of its sensitive nature, geolocation is specially discussed in all the frameworks except COPFE. While Apple allows the use of geolocation with reason, Google does not allow location permissions to be requested in any app directed at children. Finally, detrimental use of data is only discussed by COPFE and AADC, as probably both have a primary focus on children’s wellbeing and b est interest. It can be interesting to see with the eﬀect of AADC in September 20 20, how this may eﬀect the current guidelines. The analysis of the ﬁve leading children’s data protection frameworks gives us a concrete understanding of the current landscape for protecting the collection, processing and handling of children’s data. We will revisit this framework with the ﬁndings from our studies with children’s app developers for Google’s Playstore, and identify the gaps and barriers for the implementations of these guidelines. In this work, we seek to understand how developers perceive their responsibilities in the creation of apps for children, and how such perceptions impact app development practices. To this end, we conducted IRB-approved semi-structured interviews and surveys with them. The interview and survey included similar open-ended questions to allow developers to process the ideas before elaborating on them during the interview. They were designed to capt ure developer perceptions of risks online for c hildren, their views on data collection practices by third-party libraries and trackers, and their development practices. Below are sample questions which the survey and interview had in common: • What do you think p rivacy and security risks are for children online and which role do parents play in this? • What are your views o n current data collection practices used by apps and third-party companies right now? • What are your practices when it comes to using third-party libraries and how do you ensure its safety? • What development process do you follow while developing the app? Do you fo llow any known design or development process? The full survey is available in the supplementary materials. 4.1.1 Participants. Participants were recru ited through direct email communications using the address they made publicly available on the Google Playstore. We contacted developers whose apps were declared from the ‘family’ and ‘education’ genres (not excluding apps with particular age ratings), available in Europe. We then extended this to developers of parental control apps, as they are increasingly used by families and children. They were retrieved using keyword search [79], including terms such as ‘parental control’, ‘online safety’ and ‘online privacy’. We sent out a total of 11,000 emails and received 134 survey responses (S1-S134). 4.1.2 Procedure. The online sur vey was distributed in l ate July 2019 to participants who had agreed to join in the study. The survey was designed to take approximately 15 minutes. Before beginning the survey, participants had to give consent, indicate that they were over t he age of 18, and that they had read the accompanying information sheet. The survey included closed, open-ended, shor t answer, and likert-scale questions. Participants were not compensated for completing the survey. The study was approved by the research ethics committee of t he university. 4.2.1 Participants. We recruited interview participants u sing the same method as describ ed above and conducted interviews with 20 family app developers. The interviews were held remotely between July and November of 2019. Participants were not compensated for participating in the interview. 4.2.2 Procedures. A ll interviews were conducted remotely over Skype and participants consented to being audio recorded. Interviews lasted between 24 and 4 3 minutes. Participants were initially asked to provide background information about their d evelopment experiences, the motivation to develop their ap p, and its functionality. Then, we asked questions about 1) their perceptions of risk for children online; 2) their views on data collectio n practices in the mobile ecosystem; 3) t heir personal data collection practices in their app development; and ﬁnally 3) current practices of making use of third-party libraries, including any information factors, quality assessment or decision making procedure involved in choosing and enclosing these libraries in t heir app. The audio recordings were transcribed with all personally identiﬁable information anonymised. The transcripts and the open-ended survey questions were analysed in two iterations. First, using (coding reliability) Thematic Analysis [26], the ﬁrst two authors coded six diﬀerent transcripts indep endently and then convened to consolidate themes to derive a c ommon codebook. Then, using this codebook, the next 8 transcripts were independently coded until an inter-rater reliability score, using Cohen’s kappa [32], of 0.85 was achieved. The ﬁrst aut hor then re-coded the ﬁrst six transcripts, code d the remaining six transcripts, and coded the open-ended survey questions. 4.4.1 Surveys. 88% of the survey participants identiﬁed as male and 27% of the participants was between 30 and 3 9 years old, representing the largest age demographic. 25% of the participants was between 40 and 4 9, 22% was between 23 and 29, 15% percent was between 18 and 22, and 11% of t he par ticipants was over the age of 50. English was the native language to 26% of the participants. Most of survey respondents originated from Eu rope (40.5%) and Asia (34.4%), with others from North America (10.7%), South America (9.3%) and Africa (4%). 4.4.2 Interviews. The total duratio n o f the interview recordings is 698 minutes. The average interview length is 34.5 minutes (Sd = 6.5). The shortest interview was 24 minutes and the l ongest 43 minutes. The app characteristics and d eveloper demographics are summarised in Appendix 1. In this section, we report our qualitative and quantitative results that provide us an insight about children’s app developers’ data protection practices, situated in the context of their app development motivations and awareness of current guidelines and regulations. Our thematic analysis of the data shows that there have mainly been four major themes: (i) developers’ perceived respo nsibilities and motivations in designing for children’s best interests; (ii) perceptions of data collection practices; (iii) reliance on third-party libraries; and (iv) the ne ed to earn money. A summary of results can be found in Tabl e 3. We make u se of the descriptors ‘S’ and ‘I’ to refer to the surveys and interviews respectively. The majority of developers reported that designing apps for children c omes with moral responsibilities (15/2 0) and they believed that apps for children should be designed ‘diﬀerently’ than apps for adults. More speciﬁcally, developers emphasised the responsibility to make apps age-appropriate and take children’s developmental needs into consideration (10/20): “So we interact with the child, as the ap p interacts with the kid during a very sensitive period, when all the behavioural pattern s are formed for the future. So compared to general app, or an app that an adult can use, we t hink we bear a much more responsibility for t he user.” – I10 However, when developers sought to design age-appropriate apps for children, they faced a range of challenges. More than half indicated that they struggled to ﬁnd suitable and speciﬁc design guidelines (11/20). Guidelines provided by the app marketplaces provide limited discussions o n how to design for children’s developmental needs: “There are some general guidelines, like the posts which say, above a certain age you are allowed to show violence, but nothing speciﬁc to the education process it self: not pedagogical re commendation, no tutorial design recommendations, nothing of the sort. We wouldn’t mind if such things existed and were paid atten t ion to.” — I1 1 Developers also indicated that following guidelines has not always been possible, as they constantly changed, and provided insuﬃcient support for smaller companies or individual developers (7/20). “We are aware of codes, but they are constantly - th e guidelin es - they are constantly changing, and our problem is, since we are in another country, here the laws are diﬀerent. We usually get and try to read every guideline and everything since we are only two people and we don’t have any lawyers or any law advice from lawyers, we sometimes can miss few guidelines.” – I9 As a result, they defaulted to following Google’s Guidelines on “Designing Apps for Children and Families” (11/20), as it was the most “accessible” or “required” (10/20). However, the principles from these guidelines referred to are primarily centred around app publication requirements, such as the use of speciﬁc ad SDKs: “Google Play wants to be sure about if you wanna advertise a parental control or child orie nted application, it is much easier to obey these rules. You have to accept these restrictions to publish your app lications” – I1. In the end, only a small number of d evelopers used spe c iﬁc methods or techniques for designing for children. A few survey respondents (3%) and interviewees (3/20) consulted professionals to help them with the app design: “We have a very good n e twork of scientists who take a look at what we are doing and steer us in the right direction. For example, we are working together with a professor who knows about a lot of gamiﬁcation.” – I7 Survey respondents generally believe that the privacy of children should be respe cted, in terms of data collection, data sharing, and third-party analytics. The majority of them reported to not collect personal data of the users (90%) and almost all developers indicated to not share data with third parties (95%). Similarly, they strongly felt that it is unethical, even for the purpose of sustaining the business, to sell children’s data (90%) or make use of tar geted advertisements containing third-party trackers (85%): “I was approached by about 10 companies to add tracking li braries to my app in exchange for a payment of about 100$ to 400$ per month from each company. Since I ﬁnd this unethical I refuse t o do so by so far. ” – S11 Some interviewees were opposed to any data collection in apps due to the asso c iated risks and potential privacy problems (8/20 ). “I strongly opp ose to data collection, especially if its an app directed at children, I don’t think any data should be collected. I think it should be turned oﬀ. And there should be very strict policies on that.” – I17 However, a few interviewees stated that limiting col lection to non-personal data is not necessarily harmful (4/20) and a small number of them even said that data collection does not aﬀect individuals at all (2/20). “Its not always bad. For example, we have to collect some data for our own analytical pu rposes to design the app better, and these are oftentimes anonymised data; these are not per sonal data to children.” – I12 “On an individual basis, I can see this (data collection) being very damaging. [...] But on a mass scale, I just don’t see the risks and the likeliho od of the risk being that great.” – I5 Apart from their beliefs about d ata collection, they were also faced with technical constraints (7/20), such as increased costs of hosting a storage server, and liability issues (4/20) in collecting data: “Initially, [we] collected a lot of additional data, like IP addresses of users when they upl oaded things, published geographic location, and so on. [..] With the European, you know, all the law changes and so on, we decided okay, le ts not do that. We’ll only collect the absolute minimum that we can.” – I6 Developers also mentioned several important goals which could only be realised through third-party services (13/20). For example, some of them expressed the need to further understand user behaviour and interests to improve the app performance and usability (9/20): “So when a chi l d [uses our app], it gets a dedicated individual training. And in order to deliver something like this we need to collect tr aining data from the child. But only in order to i mprove t he training for the children.” – I7 Two developers al so indicated that children’s data is not very “reliable” (I1), as children’s b ehaviour is erratic, because “they don’t even know what they are doing on the phone” (I1). Another developer explained that children’s data does not serve a useful function: “I don’t se e the value in it [collecting data] as much as adults, because adults have a lot more capital to spend. So... to be honest with you. So I see the privacy issues on one side, on the other side I don’t see why you’d even want to capture information of peop le who don’t even have a lot of money.” – I5 At the same time, developers realise that data-based service optimisation is a sensitive topic, especially where it concerns children and happens through third-party services. Others stated they needed third-party services to simply maintain eﬃcient core services for their apps (5/20), e.g. “We use like Firebase for p u sh notiﬁcations” (I4). This was also reﬂected in the survey data, as the majority of the developers reported to make use of third-party libraries (63%). The primary reasons for this were ‘Convenience’ (72%), ‘To reuse existing software modules’ (52 %), for ‘Targeted ads’ (21%), and to “Gain insight into user behaviour” (S53) (6%). In the trade-oﬀ between collecting data to provide core services and avoiding data collection, developers used various strategies to support children’s p rivacy needs and minimise data collection. Survey respondents indicated to avoid collecting unnecessary data (68%), put privacy settings high by default (37%), and did not nudge users into making privacy reducing choices (31%). Similarly, developers reported to avoid collecting personal data (6/20) and limit themselves to collecting anonymous data without aﬀecting ‘individuals’ (9/20): “I don’t collect any children informati on; we use Firebase Analytics for app usage statistics, which is required for product improvement.” – I2 Beyond t he conﬂicting views developers had about the need and appropriateness of data collection for children, they also faced challenges in selecting data controllers and third-party service providers. In particular,the opaqueness of the data e conomy made it extremely challenging to navigate the APIs and SDKs that are available on the market (13/20). For example, it was unclear to developers what libraries are doing in the background (10/13) and what organisation are secretly doing with the collected data (3/13): “We were in this situation where we were so convinced that we don’t show [ads] and don’t do anything wrong. I mean, we didn’t knowingly gather any data, we didn’t run any campaigns, we didn’t use any marketi ng techniques or anyt hing. So there was nothing of that [data colletion] in our ads. [...] For example, Unity, which we use to make our apps, they have this solution to deliver in-app purchases. S o we use it. Now we found out that whenever there is a purchase, Unity takes some personal data, to make this transaction possible. So, and its not cool with us, but there is no other way to do this.” – I8 Developers expressed that they wished data controllers would be more transparent and accountable, but doubt ed that current governmental regimes would be l ikely to help achieve this (6/20): “Well, I’d love if companie s took a little bit more responsibility. But they are not going to; I mean they don’t think twice of increasing user rate. [..] It doesn’t seem that politicians nowadays have a vague understanding of how the internet works, or how digital works. And it seems for me to be impossible for them to impose the correct laws.” – I17 Survey respondents reported to have no internal policies regarding the choice of libraries or third-parties. The majority of them chose libraries themselves (39.3%) or discussed this amongst colleagues (26%). A small number of participants needed app roval from their manager or supervisor (16%). In making this choice, developers were left with relatively little information to base their decision. Thus, they resorted to relying their perceptions of the trustworthiness and quality of the software typically produced by third-parties as a basis for selecting t hem (11/20): “I trust Google. I used a lot of their tools, and I think they are well developed and are maintaine d constantly. I use Facebook SDKs and its also very good and well developed library.” – I14 Such providers were also the most p opular and prominent, which further made it likely they would be selected by developers. Thus, unsurprisingly, the majority of both interviewees (13/20) and survey respondents (64%) selected l ibraries from Go ogle. Survey respondents further indicated that “Google librarie s should be safer than any others” (S29) and that they “only select reputable libraries from large companies” (S113). As a result, 65% of the survey respondents and 9/20 interviewees reported to use Google’s libraries by default, such as Google Analytics (59%S) and Firebase (22%S). “We are typically using libraries from Apple of Google. Rarely do we use like something we found on a developer network forum, where you could potentially get in trouble. So yeah, the libraries we use are primarily from major parties that h ave created their own stuﬀ.” – I3 Only a few survey respondents indicated t o look at the security aspects, for example by performing ‘Pentesting’ (4%), ‘Internal Testing’ (1%), reading the Terms of Service and Privacy Policies (6%), or considering any reported security issues (4%): “We read the terms of use and i f it a code library, we do a quick code review to identify any security vulnerabil ities” (S47). Understandably, sustaining a viable business was one of the primary objectives for developers to support sustained development (13/20), because by t he “end of the day you’ve g otta m ake money. Money makes the world go around” (I3). This sentiment was reﬂected by the majority of survey respondents as well, who reported that commercial success of the app was an important fact or in development (60%). In fact, 63% said they relied on app development as a major source of income, either through the revenue the software generated (37%), third-party investments (16%), or through contract work (10%). As a result, the majority (79%) of them monetised their app, either through ads (32.6%), a pay-to-download model (25%), in-app purchases (21.2%), or subscriptions (18.9%). Approximately a ﬁfth of the developers, however, did not make use of any monetisation features (21%) and were running the app for ‘non-proﬁt’, stating to “develop for fun and learning” (S46). Almost all interviewees who did monetise their apps wanted to do this in an age-appropriate way. They felt a moral obligation to not make use of harmful techniques (17/20), such as the use of “manipulative ﬂashing ligh t s, but t on that move, sparkling colours”, because then “of course [a] child would click on in app purchases: they would get pleasure from it” (I9). In monetising their apps, developers fou nd it important t o take children’s developmental needs into consideration: “We want to make sure that we also have a sensitive side where we take the users into account, and we don’t want th e m to be like, ‘oh I’ve gotta check my phone again, who knows if I have another like’.” – I18 In attempts to monetise apps in an age-appropriate way, they faced several major challenges. First, apps not based on ads, i.e., paid apps, did not earn enough for d evelopers to sustain their business or earn a suﬃcient income (15/20): “Well the ﬁrst app, I made a free version, which only has about half of the elements to choose from. And then I made a paying version. Turns out, th at [I ] do not earn any money at all. I earned like 40 euros last year.” – I17 Second, several interviewees and survey respondents d iscussed that u sers often revolted against paid apps, using rating systems to pressure developers to make them available fo r free (3/20): “You know, 100,000 people in Russia give you 1 star, just because they are used to seeing ads instead of payin g money and you don’t oﬀer this opportunity to them. This impacts your global rating; and this impacts your global sales in territories where you don’t show ads and are dependent on sales. [...] In order for us to make revenue, your game design has to be focused around showing ads.”’ – I8 “The primary reason to adopt child-safe advertising was an outrage on the app store comm e nt/ratings that resulted in low ratings of our paid (premium) games just because it’s paid. We were forced to react and oﬀer some alternatives to appease those users, including the display of child-safe ads.” – S54 Lastly, one discussed the cutthroat and sinister nature of marketplaces: even if you tried t o use age-appropriate methods and became successful, it was likely t hat some clone-maker would duplicate your app and use privacy-invasive (but higher-yield) monetisation metho ds in it instead. That was particularl y the case for paid apps, which were often cloned, turned into ad -based monetisation vehicles, and released for free (3/20): I can ﬁnd and show you dozens of ap ps that are clones of our games, so someone somewhere took our app, reskinned it a little bit, and then left it as it is, with all the game design and whatever, and oﬀered it on Google Play, and show all kinds of ads: safe, unsafe, whatever. [.. . ] So if this continu e s, essentially it means that users who are looking for free stuﬀ and so on, they will still be able to ﬁnd whatever they want. [. . .] It’s not even David vs Goliath, it’s a Flea vs Goliath. ” – I8 As a consequence, many accepted that, while using ads did not align with their goal to limit data collection, they conceded there were few options, as these were a “necessary evil to keep the business aﬂoat” (I3) (14/20). As a result, interviewees felt that commercial success of the app often came at a cost of the b est interests of children (10/20), because “the two things don’t align very well, I suppose, and its kind of hard t o do them both ways: to have an ethical thing and have the bigg est payout in your app” (I18). Several others expressed t hat whilst t hey tried methods other than advertising, sooner or later they had to resort to it in the end: “We were completely driven by the good for the end user. And then we ran out of money each time. And then we realised there has to b e a balance of some degree of commercial success that will allow the app to sustain itself, and earn money to continue to operating and be as responsible for the end user.” – I10 “We didn’t want ads. If the market would allow us, if there was a possibility to still exist and do what you love, by oﬀering paid apps, for 2.99, we would gl adly remain i n that ﬁeld. Its just that the business there became impossible and unsustainable.” – I8 From our ﬁndings we identiﬁed three key challenges in designing privacy-friendly and age-appropriate apps for children: (i) navigating the complex and opaque landscape of third-party ser vices and their underlying data eco nomy, (ii) the lack of viable monetisation options not relying co nsumer data, and (iii) the lack of awareness and applicability of regulatory guidelines. An overview of this is also presented in Table 3. Below we discuss why these challenges interfere with best practices set out in data protection frameworks and how they fail to address these issues. Our ﬁndings show that developers extensively use and rely on third-party libraries to p rovide core services and gain u ser insight. More speciﬁcally, our part icipants often relied on libraries from major data controllers, like Google Analytics (58.5%), Google Firebase (22%), and Facebook APIs (29.3%). These ﬁndings are consistent with former research. Large scale analyses of Android apps has shown up to 8 2% of these apps and 60% of all code in t hese apps is from third-party libraries [50, 77]. Use of third-party libraries has been shown to be critical in saving development time and eﬀorts [18, 48, 59 ]. While developers demonstrated to care about children’s privacy, it also well known that third-party libraries can be a major gateway to global data tracking and proﬁling networks [23, 67]. Products like Google Analytics and Firebase, which were popular amongst our participants, are free so that it can “provide conﬁdence and prove the valu e of online advertising to potential new advertisers" [31], meaning they rely on the p roﬁling of large volumes of user data. These issues are also addressed by the dat a protection frameworks we analysed in section 3. The use of analytics and storage libraries relates to requirements about data sharing and proﬁling (see Table 1). Both the AADC and COPPA require that data should not be shared with third parties unless there is a compelling reason to do so and the AADC only allows proﬁling under the assurance that reasonable security measures are in place. This is not to say that app developers should never make use of such third-party libraries, but it d oes have imp ortant implications for eﬀorts addressing ‘age-appropriate’ design. Firstly, our ﬁndings show that developers have conﬂicting perceptions of major data controllers. They realise that, on the one hand, data controllers are nontransparent and potentially harmful in their data handling practices (13/20), whilst simultaneously relying on them Ta ble 3: Barriers and practices related to each of the three key objectives of our app developers. There where survey data is available, it is presented in terms of percentages and suﬃxed with the letter ‘S’. Design for children:Challenges with existing design guide• Design for children is ‘diﬀerent’ than des ignlines: for adults (15/20).• Struggle to ﬁnd children-speciﬁc design • Children’s developmental needs are impor-guidelines (11/20). tant to consider (10/20).• Challenging to keep up with the constant • Important to enforce age appropriate designchange of guidelines (7/20). features (11/20). Create good apps:Perception of children’s data: • Data is needed to understand user behaviour• Collecting any data from children is wrong and improve app (9/20; 6%).(8/20). • Data collection is needed to main eﬃcient• Data colle ction is necessary and not always core service (5/20).harmful (6/20). • Need for third-party libraries to develop• Children’s data not useful (2/20). apps (13/20; 63%S).• Collecting data poses legal risks (4/20). Earn an income to sustain the business:Developer p erceptions: • Need to earn e nough income to continue de-• Apps themselves do not make enough velopment(13/20; 63%S).money (15/20). • Monetise in an age-appropriate way ( 17/20;• Pressure from the end users (3/20). 85%S).• Free clones due to competition (3/20). to p roduce high-quality and unmalicious software (13/20). This forced t rade-oﬀ highlights the complexity and lack of guidance in navigating the often treacherous landscape of third-party services. Data protection guidelines, such as the AADC and COPPA, do not provide suﬃcient support to developers in this regard. For example, they do not concretely state which libraries are simultaneously reliable and ‘age-appropriate’. Second, developers generally perceived the use of libraries such as Google Analytics and Firebase as harmless and non-detrimental to children’s privacy (8/20). This demonstrates the lack of a clear deﬁnition of what ‘privacy-friendly’, ‘age-appropriate’, or ‘safe’ thirdservices are supposed to be. While these boundaries are not clearly deﬁned in data protection frameworks, both developers and regulators cannot make sound judgements in selecting or auditing thirdparty libr aries. Lastly, developers also indicated that third-party libraries often behave in unpredictable and unknown ways (13/20). This demonstrates that even if developers have the best of intentions, they cannot always enforce their well intended principles of designing for children. The AADC and COPPA require developers to ensure that third-party services have reasonable protections in place, for example by examining their privacy policies. However, what they fail to take into account is the widely accepted fact that privacy policies are rarely read [64] and that developers have diﬃculties understanding the legal language presented in them [16, 20]. Our participants expressed their aim to monetise their apps in a privacy friendly way without using advertisements. This sentiment is consistent with the key message in the data protectio n frameworks we examined. The AADC mentions personalised marketing and advertising in their proﬁling section, stating that parental consent is required for behavioural advertising, as “‘legitimate interests’ is unlikely to provide a valid lawful basis for processing for this purpose" [11]. Similarly, Google and Apple do not allow third-party personalised advertising, with or without p arental consent. Apple actually provides additional information for developers on business models and explains how best to implement these models [3]. However, they are geared towards max imising revenue by emphasising the importance of “optimizing with analytics" and “extensive user acquisition marketing campaigns" [9], which is slightly misaligned the principles advocated by the AADC, COPPA, and COPFE. A few developers tried to adhere to the pr inciples set out above, by making use of non-advertisement based monetisation. However, they faced several barriers: First, earning a sustainable income through subscriptions or payto-download business models, requires a large number of downloads, which is diﬃcult to achieve for the average developer in a marketplace with over 3 millio n apps. Approximately 40% of the apps never gain more than a handful of users [71] and adding paywalls or making the app pay-to-use increases the threshold for a user to download it. It is also for this reason that a large number of our interviewees indicated that apps do not earn enough (10/20). Second, merely having in-app purchases or subscription options is not enough. Participants expressed that earning money through in-app purchases or sub scr iptions also requires persuading users into making these purchases. At the same time, they felt mo rally conﬂicted in doing t his. Third, business models based on premium apps are not always accepted by the larger end user community. Several of our participants faced p ressure to make their paid apps free, as their users could not aﬀord to pay for it. Users either leave negative reviews or simply switch to a diﬀerent app which can also fulﬁl their req uirements. The competitive nature of the Google Playstore has put developers in a position to oblige with user’s requests, and users generally want free apps. Developers are then forced to implement ads as the only alternative. As a result, many developers in the interview study reported that they had to rely on in-app ads or sharing of data wit h thirdparties to retain a business viability (14/20). This was furt her support ed by our survey data, which shows that more than 79% monetised their app. This phenomenon demonstrates the problem that guidelines do not address developers’ need to monetise apps, nor do they provide enough guidance on this matter. First, while the need for advertising is evident, they do not specify which and when advertising networks are appropriate to use. This lack of regulation in the mobile advertising industry makes it challenging for developers to judge the boundaries of ‘safe’ and ‘privacy-friendly’ monetisation. Second, there is conﬂicting information between commercial guidelines, which focus on ‘u ser acquisition’ and data processing, and principles set ou t by the AADC and COPFE, which is not discussed by any of the frameworks. Finally, we identiﬁed the lack of awareness and accessibility to guidelines as a key challenge for developers (11/20). As a result, our participants reported to default to making use of terms and conditions set out by Google as a framework for child appropriate development (10/20). However, there are a few problems with relying on recommendations made by Google. First, Google’s “Designing Apps for Children and Families” guidelines are not directly aligned with the AADC. Importantly, they do not adequately address data minimisation principles which are core t o the AADC. For example, the “Ads and Monetization” po licy for children, by Google, is primarily focused on content based restrictions, such as ‘inappropriate ad content’, ‘multiple ad placements’, and ‘use of shocking or emotionally manipulative tactics’ [1], instead of addressing any privacy implications of ad-based monetisation. Second, Google’s guidelines and tutorials place their own products in the limelight, such as Google AdMob and Google Ad Manager [6], thereby increasing the adoption rate of third-party libraries in children’s apps by major d ata controllers. Our study has given us a detailed understanding of challenges developers face in designing privacy friendly and age-appropriate apps for children. Based on our ﬁndings and analyses, in this section we make concrete recommendations to address some of the problems highlighted above. 6.4.1 Include developers as stakeholders. Our work highlights the importance of including app developers as stakeholders in creating design guidelines [16]. It is widely recognised that design guidelines should reﬂect st akeholders’ values [62, 79] and be co-developed with key stakeholders [39, 44], such as children and families. 6.4.2 The need for supporting documents. In order to help d evelopers navigate the compl ex app development ecosystem, or ganisations like the ICO need to provide clarity and speciﬁc requirements for what constitutes ‘age-appropriate’ or ‘privacy-friendly libraries’. To expose developers to appropriate libraries other than those from Google, guidelines need concrete recommendations in select such services, for example through a knowledge base o f resources and services approved by regulatory bodies. 6.4.3 Additional tools for developers. Many developers mentioned that they do not fully understand how data is handled by data controllers and market leaders, or the libraries they use behave in unpredictable ways. This highlights the need for additional support through practical tools and resources. There is an increasing amount of research support ing usable security tools for developers [16, 73, 80], however what is needed are resources to help developers navigate the vast choices of third-party libraries. 6.4.4 Need for industry support. History has shown us that it takes a market leader to move the market. For example, in 2019 Apple restricted all third-party advertising and analytics. Similarly, their new iOS 14 operating system will require apps to explicitly ask its users permission to allow tracking services [40]. This approach to technology design has impacted companies whose business model centres around data collection and sharing, such as Facebook [8]. For data tracking to come to an end on the Google Playstore, Google will have to push for similar policies. While the need for industry support is evident, it is unrealistic for Google to adopt such principles. Apple can aﬀord to do t his, because their advertising revenue is believed to only make up a small part of their total earnings [61]. Google, on the other hand, produced 83.3% of the 2019 revenue through the advertising industry [10], which is strongly grounded in the d ata tracking and proﬁling. While industr y support is lacking, the HCI community plays in an important role in facilitating change in the ecosystem. For example, by raising awareness of privacy risks, making these risks more comprehensible, and facilitating the ado ption of p rivacy preserving tools amongst (young) end users. 6.4.5 Alternate monetisation methods. The need for privacy friendly monetisation methods is already recognised by the wider community. For example, Apple has an initiative which facilitates privacyfriendly monetisation, call ed Apple Arcade [2]. End users have access to a set of apps through a monthly subscription, without inapp purchases or advertisements. While this is a good initiative, it is not a solution to the problem of data-driven monetisation. Apple Arcade only has about 100 games and studios were paid a fee for having their game included, which does not scale well to the large number of apps t argeted at children. Similarly, business models which do not make use of ad s, such as the ‘pay-to-download’ business model, are not automatically privacy friendly. Paid services come with their own set of problems and ethical dilemmas. Firstly, it leads to the problem that privacy beco mes a rather expensive commodity, creating digital exclusion and a lack of access to those who cannot aﬀord it. It is widely accepted that marginalised groups [76] have diminished privacy, and commoditising privacy in a densely populated marketplace would only exacerbate the problem. Secondly, numerous studies have shown that paying for apps does not signiﬁcantly reduce the amount of dat a tracking and collection [21, 38, 69]. Paid apps still contain t hird-party tracking libraries and require dangerous permissions. The fact that there is no trivial so lution to this, highlights the need for additio nal research in this area. 6.4.6 Support for end users. Lastly, it is also important to consider support for end-users. Research around end users has been focused on raising privacy awareness. However, research in raising users’ awareness of age-appropriate design implications is limited, which can be a key barrier to privacy-friendly technology innovation. By not depending on higher level policy changes by developers and market leaders, end users can exercise a ﬁner level of control over their privacy choices. For example, there are technologies currently in existence which inform users of trackers in app s and also give them the ability to control how these apps behave. We are aware of several limitations and methodological decisions which constrain our ﬁndings. Below we provide a rationale for this and suggest opportunities for future research which can build on our ﬁndings. 6.5.1 Recruitment. In our recruitment we limited ourselves to the Android marketplace. The primary reason for this is that Android is the dominant operating system with a global market share of 75%. Also, our research is motivated b y the knowledge that Android applications are known to distribute data to third-parties on a large scale. While the presence of trackers and third-party libraries in Android applications is well researched [23, 77], the privacy landscape of iOS apps is less documented. In our future work we aim to extend this study to include iOS apps and developers as well. Additionally, we also limited ourselves to the western app development market, focusing on US and European regulations. This choice is motivated by regulatory interventions taking place in Europe, primarily ICO’s enforcement of the statutory AADC, which will come into eﬀect in 2021. These changes will have an imminent impact on developers who wish to publish apps in the UK Google Playstore. 6.5.2 Study participants. Since participation in our study was purely voluntary, the participants who responded may have represented those with a bett er awareness of potential issues or took children’s best interests more seriously than the average developer. Although this may have skewed our ﬁndings, our focus on the gaps in current practices and knowledge still provided valuabl e insights into a research direction of which little is known. Similarly, our study captured only a small fragment of all Android developers. While the principles developers aimed to realise aligned with best practices, we do not claim that this is the c ase for all developers. It is likely that have malicious intents or are indiﬀerent about children’s privacy. However, we wanted to show that those developers who prioritise children’s b eneﬁcence, wellbeing, and safety, are faced with real challenges and conﬂicts in supporting these priorities. Lastly, we have not yet veriﬁed the features claimed to have been implemented by the part icipants by conducting a technical analysis of their apps (for example by analysing t racker domains the app contacts). This would not only help us to follow up conversations with our study participants to gain a deeper understanding of their awareness and knowledge, but also to validate the claims given b y the part icipants. Through 20 in-depth interviews and 134 surveys with children’s app developers, our study provided a much-needed developers’ perspect ive regarding challenges and practices related to developing Android apps for children. We analysed the results of our dat a through the lens of 5 leading data protection frameworks. Our ﬁndings show that most developers were morally and legally motivated to try supporting children’s best interests. They indicated to make use of techniques to avoid engaging in unnecessary data collection and data sharing practices. However, at the same t ime, we also identiﬁed key barriers and practices which stood in the way of designing for children’s best interests. Firstly, developers relied on third-party libraries from major dat a controllers for analytics and functionality. They often were not aware of alternatives or assumed them to be harmless to end users. Secondly, developers struggled to monetise their apps in privacy-friendly ways without relying on advertising networks. Market pressure and a lack of alternatives often forced t hem to adopt advertising as a p rimary source of revenue, thus making a trade-oﬀ between commercial success and children’s privacy needs. Lastly, we identiﬁed a lack of awareness and accessibility to concrete and neutral design guidelines. Developers often relied on Google’s app pu blication requirements, which are not always directly aligned with principles set out by the AADC. We call for (1) supporting documents to aid developers to navigate the o paque and complex development ecosystem, (2) research into alternative and privacy-friendly monetisation methods, (3) tools to support developers in auditing their ap ps, and (4) tools to empower end u sers in case developers do not keep up with age-appropriate practices.