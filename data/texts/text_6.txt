1.1 Background ommendation (CDR) [1] and Cross-System Recommendation (CSR) [2], [3] have been proposed to leverage the richer information from a richer dataset (domain/system) to help improve the recommendation accuracy in a sparser one, resulting in single-target CDR (Conventional Scenario 1) and single-target CSR (Conventional Scenario 2). For example, in Douban system, the recommender system can recommend books to a target user (e.g., Alice in Fig. 1(a)) according to her movie knowledge, i.e., this is single-target CDR. In contrast, the recommender system can recommend movies to a target user in MovieLensaccording to the knowledge of these movies (e.g., Titanic in Fig. 1(b)) learned from Netﬂix, i.e., this is single-target CSR. In addition to the above-mentioned rating systems, CDR and CSR have been applied to other application scenarios as well, including academic searching (e.g., Arnetminer[4]), e-commerce (e.g., Amazon[5]), and social networking (e.g., Facebook [6] and Tencent Weibo[7], [8]). CDR and CSR have different kinds of overlapping entities that serve as the ‘bridge’ to link the two data sources. These overlapping (common) entities are relations between two domains/systems, and thus the two domains/systems are termed as related domains/systems. In CDR, there are two related domains (e.g., movie domain and book domain) in the same system (e.g., Douban) and thus CDR techniques can utilize common users to transfer/share their knowledge across domains. Likewise, in CSR, the two related systems (e.g., Netﬂix and MovieLens) have the same domain (e.g., movie domain) and thus contain common items (e.g., movies). Technically, in solutions, we only need to replace the ‘bridge’ from common users in a CDR model to common items so as to support CSR, and vice versa. This means that CDR and CSR techniques can be applied to each other’s scenarios. Thus, in this paper, our proposed approaches can be applied for all related domains/systems. In this paper, without a special explanation, we basically focus on CDR when discussing solutions, except the scenario of CDR+CSR. 1.2 Limitations of Conventional Single-Target CDR Existing single-target CDR approaches can be generally classiﬁed into two groups: content-based transfer approaches and feature-based transfer approaches. Content-based transfer tends to link different domains by identifying similar content information — such as user proﬁles, item details [1], user-generated reviews [9], and social tags [10]. Featurebased transfer [3], [11], [12], [13], [14], [15], [16], [17], [18], [19] ﬁrst trains different Collaborative Filtering (CF) based models — such as Bayesian Personalised Ranking (BPR) [20], Neural Matrix Factorization (NeuMF) [21], and Deep Matrix Factorization (DMF) [22], to obtain user/item embeddings or patterns, and then transfers these embeddings through common or similar users across domains. In contrast to the content-based transfer approaches, feature-based transfer approaches typically employ machine learning techniques — such as transfer learning [23] and neural networks [24], to transfer knowledge across domains. Motivating Example 1. Fig. 1(c) depicts a special case in the conventional single-target CDR system (i.e., Douban) that contains two domains — DoubanMovie (the richer domain) and DoubanBook (the sparser domain) — including users, items (movies or books), and interactions (e.g., ratings and reviews). In contrast to Alice in Fig. 1(a), who is one of majority users in the dataset, Bob in Fig. 1(c), who is one of minority users in the dataset, reviewed few movies and many books, and thus Bob’s knowledge (e.g., user embedding) in the book domain would be more accurate than his knowledge in the movie domain. However, the knowledge in the book domain cannot be used to improve the knowledge in the movie domain since the conventional single-target CDR system can only leverage the information from the richer domain to improve the recommendation accuracy in the sparser domain. However, all these existing single-target CDR approaches only focus on how to leverage the source domain to help improve the recommendation accuracy in the target domain, but not vice versa. This is also explained in Motivating Example 1. In fact, each of the two domains may be relatively richer in certain types of information (e.g., ratings, reviews, user proﬁles, item details, and tags); if such information can be leveraged well, it is likely to improve the recommendation performance in both domains simultaneously, rather than in a single target domain only. Therefore, the novel dual-target CDRs [25], [26], [27], [28] have been recently proposed to improve the recommendation accuracy in both richer and sparser domains simultaneously by making good use of the information or knowledge from both domains. 1.3 Our Target Scenarios Dual-target CDR is our ﬁrst target scenario. Intuitively, based on the existing single-target CDR approaches, it seems to be a solution for dual-target CDR (Target Scenario 1, see Fig. 1(d)) by simply changing their transfer direction from “Richer→Sparser” to “Sparser→Richer”. However, as referred to as Negative Transfer [29], this idea does not work, because, in principle, the knowledge learned from the sparser domain is less accurate than that learned from the richer domain, and thus, the recommendation accuracy in the richer domain is more likely to decline by simply and directly changing the transfer direction. Therefore, dualtarget CDR/CSR demands novel and effective solutions. Additionally, inspired by dual-target CDR, multi-target CDR (Target Scenario 2, see Fig. 1(e)), namely, improving the recommendation accuracy in multiple domains simultaneously, is also an interesting and challenging research problem for CDR. However, unlike dual-target CDR, in Target Scenario 2, more non-IID (independent and identically distributed) data from multiple domains may negatively affect the recommendation performance, which is likely to cause negative transfer. This is the new challenge. Though there are no solutions reported in the literature yet, multitarget CDR is similar to Multi-Domain Recommendation (MDR) to some extent. Nevertheless, MDR [23], [30], [31], [32] tends to improve the recommendation accuracy in a single target domain or the recommendation accuracy of a mixed user set from multiple domains by leveraging the auxiliary information from multiple domains. Therefore, a feasible multi-target CDR/CSR solution is in demand. Moreover, it would be promising to devise a hybrid approach that can leverage the auxiliary information from both multiple domains and multiple systems to further improve the accuracy in these domains and systems simultaneously, i.e., CDR+CSR (Target Scenario 3, see Fig. 1(f)). This means that CDR+CSR should utilize the information of both common users and common items in the same approach. This is also an interesting and challenging research problem. 1.4 Challenges Targeting Scenario 1, there are two challenges (CH1 and CH2) as follows. CH1: how to leverage the data richness and diversity to generate more representative single-domain user and item embeddings for improving recommendation accuracy in each of the domains? Both traditional Collaborative Filtering (CF) models, e.g., BPR [20], and novel neural CF models, e.g., NeuMF [21] and DMF [22], are based on the user-item relationship to learn user and item embeddings. However, most of them ignore the user-user and item-item relationships, and thus can hardly enhance the quality of embeddings. CH2: how to effectively optimize the user or item embeddings in each target domain for improving recommendation accuracy? The state-of-the-art dual-target CDR approaches either adopt ﬁxed combination strategies, e.g., average-pooling, max-pooling, and concatenation [25], [33], or simply adapt the existing single-target transfer learning to dual transfer learning [26]. However, none of them can effectively combine the embeddings of common entities, and thus it is hard to achieve an effective embedding optimization in each target domain. Targeting Scenario 2, there is a new challenge (CH3). CH3: how to avoid negative transfer when combining the embeddings of common users from multiple domains? Compared with dual-target CDR (Target Scenario 1), the core goal of multi-target CDR (Target Scenario 2) is to leverage more auxiliary information from more domains to improve the recommendation performance. However, it is worth noting that more non-IID data from more domains may negatively affect the recommendation performance. This is because such incomplete non-IID data, especially in sparser domains, can only reﬂect biased features of common users. Therefore, in Target Scenario 2, the recommendation performance in some domains may decline as more sparser domains join in, i.e., the negative transfer can thus happen. Targeting Scenario 3, there is a new challenge (CH4). CH4: how to effectively leverage the auxiliary information of both common users and common items simultaneously? In a dual-target or multi-target CDR scenario, we only need to optimize the embeddings of common users from dual or multiple domains. Then, based on CF models in each domain, the embeddings of distinct users and items can be optimized gradually. However, in a CDR+CSR scenario, it should effectively leverage the embeddings of common users and common items simultaneously, which may improve the recommendation performance in each dataset (domain/system) more quickly. To address the above four challenges, in this paper, we propose a uniﬁed framework for all dual-target CDR, multitarget CDR, and CDR+CSR scenarios. The characteristics and contributions of our work are summarized as follows: called GA, for Dual-Target CDR (GA-DTCDR) scenario, which can leverage the data richness and diversity (e.g., ratings, reviews, and tags) of different datasets, share the knowledge of common entities across domains; considering not only user-item relationships (based on ratings), but also user-user and item-item relationships (based on content similarities). Then, with this heterogeneous graph, we apply a graph embedding technique, i.e., Node2vec, to generate more representative single-domain user and item embeddings for accurately capturing user and item features; mechanism to effectively combine the embeddings of common entities learned from dual domains, which can signiﬁcantly enhance the quality of user/item embeddings and thus improve the recommendation accuracy in each of both domains simultaneously. It is worth mentioning that this work is an extension of our preliminary work [27]. In this paper, we further deliver the following contributions: ports dual-target CDR scenario, we extend the above proposed GA framework and adopt a Personalized training strategy to support all Dual-Target CDR (GA-DTCDR-P), Multi-Target CDR (GA-MTCDR-P), and CDR+CSR (GACDR+CSR-P) scenarios; egy, deriving GA-DTCDR-P and GA-MTCDR-P, to train the recommendation models in different domains, which can ﬁrst give personalized weights to the pair-wise embedding differences of common users between every two domains and then minimize these pair-wise embedding differences. The embeddings of common users in different domains tend to be similar but remain personalized, and thus the personalized strategy can avoid negative transfer to some extent; structure of GA-DTCDR to support CDR+CSR scenario and thus GA-CDR+CSR-P can enhance the qualities of the embeddings of common users and items simultaneously. We conduct extensive experiments on four real-world datasets, which demonstrate that our GA-DTCDR-P signiﬁcantly outperforms the best-performing baselines by an average of 9.04% in terms of recommendation accuracy. Additionally, we conduct more multi-target CDR and CDR+CSR experiments (see Tasks 4 and 5 in Experiments and Analysis) to demonstrate that our GA-MTCDR-P and GA-CDR+CSR-P can further improve the best-performing baselines by an average of 9.21%. 2.1 Single-Target CDR Most of the existing single-target CDR approaches tend to leverage auxiliary information from the source domain to improve the recommendation accuracy in the target domain. According to their transfer strategies, these single-target CDR approaches are classiﬁed into two categories: contentbased transfer and feature-based transfer. richer and sparser domains by content information, e.g., user/item attributes [1], tags [34], [35], social relations [7], [8], [36], semantic properties [37], thumbs-up [6], text information [9], metadata [38], browsing or watching history [39]. Then they transfer/share user preferences or item details across domains. some classical machine learning techniques — such as multi-task learning [40], transfer learning [13], [14], [15], [16], [17], [18], [19], [41], [42], clustering [43], reinforcement learning [44], deep neural networks [3], [5], [24], [45], relational learning [46] and semi-supervised learning [47], to map or share features, e.g., user/item lembeddings and rating patterns [42], [48], learned by CF-based models (e.g., classical factorization models and novel neural CF models), across domains. Additionally, some studies [23], [30], [32], [49] focus on a derivational problem, i.e., multi-domain recommendation, which is to improve the recommendation accuracy on the target domain by leveraging the auxiliary information from multiple domains. However, all of them are single-target models, which means they cannot improve the recommendation accuracy in the richer domain even if the sparser domain may contain certain types of auxiliary information to support the richer domain. 2.2 Dual-Target CDR Dual-target CDR is still a novel concept for improving the recommendation accuracy in both domains simultaneously. Therefore, existing solutions are limited. The existing dualtarget CDR approaches mainly focus on applying ﬁxed combination strategies [25], [33], or they focus on simply changing the existing single-target transfer learning to become dual-transfer learning [26], [28]. However, none of them can effectively combine the embeddings of common users. In [25], Zhu et al. proposed the DTCDR, which is the ﬁrst dual-target CDR framework in the literature that uses multi-source information to generate more representative embeddings of users and items. Based on multi-task learning, the DTCDR framework uses three different combination strategies, e.g., average-pooling, max-pooling, and concatenation, to combine and share the embedding of common users across domains. Later on, similarly, in [33], Liu et al. also use a ﬁxed combination strategy, i.e., hyper-parameters and data sparsity degrees of common users. In addition, in [26], Li et al. proposed the DDTCDR, a deep dual-transfer framework for dual-target CDR. The DDTCDR framework considers the bidirectional latent relations between users and items and applies a latent orthogonal mapping to extract user preferences. Based on the orthogonal mapping, DDTCDR can transfer users’ embeddings in a bidirectional way (i.e., Richer → Sparser and Sparser → Richer). Recently, Li et al. proposed an improved version of DDTCDR in [28], i.e., a dual metric learning (DML) model for dual-target CDR. 2.3 Graph Embedding Graph Embedding is to learn a mapping function that maps the nodes in a graph to low-dimensional latent representations [50]. These latent representations can be used as the features of nodes for different tasks, such as classiﬁcation and link prediction. According to embedding techniques, this section classiﬁes the existing graph-embedding approaches into two categories: dimensionality reduction and neural networks. Dimensionality reduction-based approaches — such as multidimensional scaling [51], principal component analysis [52] and their extensions [53] — involve optimising a linear or non-linear function that reduces the dimension of a graph’s representative data matrix and then produces low-dimensional embeddings. Neural networkbased approaches — such as DeepWalk [54], LINE [55] and Node2vec [56] — involve treating nodes as words and the generated random walks on graphs as sentences, and then learning node embeddings based on these words and sentences [50]. Also, recently, there are some graph embedding approaches that can leverage both explicit preferences and heterogeneous relationships by graph convolutional networks [57], [58]. 2.4 Attention Mechanism Attention is ﬁrstly introduced in [59], which provides more accurate alignment for each position in a machine translation task. Apart from machine translation, recently, attention mechanism also has been widely used in recommendation [60]. The general idea of the attention mechanism is to focus on selective parts of the whole information, which can capture the outstanding features of objects. For recommendation, the existing attention approaches [61], [62], [63] tend to select more informative parts of explicit or implicit data to improve the representations for users and items. In this section, we ﬁrst formalize the dual-target CDR, multitarget CDR, and CDR+CSR problems. Then, we preliminarily propose a Graphical and Attentional framework, called GA, for DTCDR (GA-DTCDR) scenario. Next, we extend the above GA framework and adopt a Personalized training strategy to support all dual-target CDR (GA-DTCDRP), multi-target CDR (GA-MTCDR-P), and CDR+CSR (GA-CDR+CSR-P) scenarios. Finally, we present the detailed components of GA-DTCDR (or GA-DTCDR-P), GAMTCDR-P, and GA-CDR+CSR-P. 3.1 Problem Statement First, for the sake of better readability, we list the important notations of this paper in Table 1. Then, we deﬁne the DualTarget CDR, Multi-Target CDR, and CDR+CSR as follows. Deﬁnition 1. Dual-Target Cross-Domain Recommendation (DTCDR): Given two related domains 1 and 2, with explicit feedback (e.g., ratings and comments), implicit feedback (e.g., purchase and browsing histories), and side information (e.g., user proﬁles and item details), DTCDR is to improve the recommendation accuracy in both domains simultaneously by leveraging their observed information. Deﬁnition 2. Multi-Target Cross-Domain Recommendation (MTCDR): Given multiple related domains 1 to a, with explicit feedback, implicit feedback, and side information, MTCDR is to improve the recommendation accuracy in all domains simultaneously by leveraging their observed information. Deﬁnition 3. Cross-Domain and Cross-System Recommendation (CDR+CSR): Given multiple related domains/sytems 1 to a, with explicit feedback, implicit feedback, and side information, CDR+CSR is to improve the recommendation accuracy in all domains and systems simultaneously by leveraging their observed information. Note that a certain degree of overlap between the users of different domains, i.e., common users, and overlap between the items of different systems, i.e., common items, play a key role in bridging the different datasets (domains/systems) and exchanging knowledge across them. This is a common idea of the existing CDR and CSR approaches [3], [24], [64]. 3.2 Overview of GA Framework In this section, we ﬁrst take GA-DTCDR-P (or GA-DTCDR) as an example to introduce the general structure of GA. As shown in Fig. 2, GA-DTCDR-P framework is divided into ﬁve main components, i.e., Input Layer, Graph Embedding Layer, Feature Combination Layer, Neural Network Layers, and Output Layer. The main differences between GA-DTCDRP and other two sub-frameworks, i.e., GA-MTCDR-P and GA-CDR+CSR-P, are the network structures of elementwise attention (see the Graph Embedding Layers and Feature Combination Layers of Figs. 2, 3, and 4). For clarity, we ignore the same components of GA-MTCDR-P and GA-CDR+CSRP with GA-DTCDR, i.e., (1) Input Layer, (4) Neural Network Layers, and (5) Output Layer. We will present the details of each component in the following sections. Like the single-target or dual-target CDR approaches in [3], [25], [64], our GA-DTCDR-P and GA-MTCDR-P can be applied to dual-target CSR and multi-target CSR as well, where the two/multiple systems have the same domain but different users, and thus contain common items only — such as DoubanMovie and MovieLens (see Task 3 in Experiments and Analysis). Accordingly, in Figs. 2 and 3, we only need to replace common users with common items for supporting dual-target CSR and multi-target CSR. In fact, GA-MTCDR-P is an extension of GA-DTCDR-P from dual domains to multiple domains. GA-CDR+CSR-P is the full version of GA to handle almost all CDR and/or CSR scenarios. If there are only common users among all datasets in GA-CDR+CSR-P, GA-CDR+CSR-P will be degraded to GA-DTCDR-P or GA-MTCDR-P. Similarly, if there are only common items among all datasets in GA-CDR+CSR-P, then GA-CDR+CSR-P will be degraded for dual-target CSR or multi-target CSR. The time complexities of GA-DTCDR-P and GA-PP MTCDR-P are both O(i∗(m+k)), where iis the number of interactions in domain D, mis the number of users in domain D(note that for DTCSR or MTCSR, the number of users mis replaced by the number of the number of items nin the time complexity expression), k is the number of nodes in each MLP layer (the node number is relative to the embedding dimension k), and l is depth of MLP layers. Similarly, the time complexity of GA-PP CDR+CSR-P is O(i∗ ((m+ n) + k)). Compared with GA-DTCDR-P and GA-MTCDR-P, GACDR+CSR-P can share the embeddings of both common users and common items across domains/systems, and thus there is the sum of the number of users and the number of items, i.e., (m+ n), in the time complexity expression. Although k and l are constants in our experiments, kis still very large. However, a deep MLP structure can represent a complex and well-trained non-linear relation between users and items, and thus can enhance the recommendation accuracy. This is a trade-off between running time and recommendation accuracy. We now brieﬂy present each component of GA as follows. GA-MTCDR-P, and GA-CDR+CSR-P, we consider both explicit feedback (ratings and comments) and side information (user proﬁles and item details). These input data can be generally classiﬁed into two categories, i.e., rating information and content information. content information of each domain to construct a heterogeneous graph, representing user-item interaction relationships, user-user similarity relationships, and itemitem similarity relationships. Based on the graph, we apply the Graph Embedding model, i.e., Node2vec [56], to generate user and item embedding matrices. element-wise attention mechanism to combine the common users’ embeddings from dual (GA-DTCDR-P) or multiple (GA-MTCDR-P) domains. This layer intelligently gives a set of weights to the embeddings of a common user learned from dual/multiple domains and generates a combined embedding for the common user, which remains his/her features learned from different domains with different proportions. Additionally, for GACDR+CSR-P, the element-wise attention mechanism can be applied to combine both the common users’ embeddings and the common items’ embeddings. a fully-connected neural network, i.e., Multi-Layer Perceptrons (MLP), to represent a non-linear relationship between users and items in each domain. teraction predictions. The training of our model is mainly based on the loss between predicted user-item interactions and observed user-item interactions. Next, we will introduce the details of Graph Embedding Layer, Feature Combination Layer, Neural Network Layers, and Output Layer in the following sections. 3.3 Graph Embedding Layer The existing embedding strategies for recommender systems mainly focus on representing the user-item interaction relationship. Apart from the user-item interaction relationship, we use a graph to represent user-user and itemitem relationships as well. Therefore, based on the rating and content information observed from dual or multiple domains, we construct a heterogeneous graph, including nodes (users and items) and weighted edges (ratings and content similarities), for each domain. Then, we can generate more representative user and item embedding matrices. The Graph Embedding contains three main sub-components, i.e., Document Embedding, Graph Construction, and Output. 3.3.1 Document Embedding To construct the heterogeneous graph, we need to compute the content similarities between two users or two items. To this end, we consider multi-source content information, e.g., reviews, tags, user proﬁles, item details, observed from dual/multiple domains, to generate user and item content embedding matrices. In this paper, we adopt the most widely used model, i.e., Doc2vec [65], as the document embedding technique. The detailed document embedding process works as follows: (1) First, in the training set, for a user u, we collect the comments (reviews and tags) Cand the user proﬁle upof uinto the same content document cd, while for an item v, we collect the comments (reviews and tags) Con the item and its item detail idinto the same content document cd; (2) Next, we segment the words in the documents CD = {cd, cd, ..., cd} by using the most widely used natural language tool, i.e., StanfordCoreNLP [66]; (3) Finally, we apply Doc2vec model to map the documents CD into the text vectors UC and V C for users and items, respectively. 3.3.2 Graph Construction First, we link the users and items via their interaction relationships. The weights of these interaction edges are normalized ratings, i.e., R/max(R). To consider the user-user and item-item relationships in the heterogeneous graph, we generate the synthetic edges between two users or two items according to their normalized content similarities (edge weights). The generation probability P (i, l) of the edge between users uand uis as follows: where α is a hyper-parameter which controls the sampling probability and sim(UC, UC) is the normalized cosine similarity between UCand UC. Similarly, we can obtain the generation probability between two items. Based on the user-item interaction relationships, user-user similarity relationships, and item-item similarity relationships, we can construct the heterogeneous graphs Gfor domain x, where x ∈ {1, 2, ..., a}. Similar to the approaches proposed in [7], [8], we also construct a heterogeneous graph to represent the relations among users and items. But we construct a heterogeneous graph in each domain rather than a common graph as in [7], [8]. 3.3.3 Output Based on the heterogeneous graph G, we employ the graph embedding model, i.e., Node2vec [56], to generate user embedding matrix U and item embedding matrix V for domain x. 3.4 Feature Combination Layer Feature Combination Layer is to combine the embeddings of common entities learned from dual/multiple datasets. By doing so, the combined embeddings of common entity for each dataset can remain all features learned from the two/multiple datasets in different proportions. To this end, we propose an element-wise attention mechanism. The traditional attention mechanism tends to select a certain part of representative features and give these features higher weights when generating the combined features [59]. Similarly, for a common entity, our element-wise attention mechanism tends to pay more attention to the more informative elements from each set of embedding elements (the embeddings of this common entity learned from different datasets). Compared with DTCDR and MTCDR scenarios (only common users), in CDR+CSR scenario, our elementwise attention mechanism needs to combine the embeddings of common users and items simultaneously. Therefore, we will separately introduce the feature combination layers of GA-DTCDR-P and GA-MTCDR-P and the feature combination layer of GA-CDR+CSR-P. 3.4.1 For GA-DTCDR-P and GA-MTCDR-P In GA-DTCDR-P and GA-MTCDR-P (see Figs. 2 and 3), the feature combination layers are to combine the embeddings of common users learned from dual/multiple domains by our element-wise attention mechanism. For a common user u, our element-wise attention mechanism tends to pay more attention to the more informative elements from each set of elements in {U, U, ..., U}, where a is the total number of domains (for DTCDR, a = 2 and for MTCDR, a > 2). Thus our element-wise attention mechanism can generate more representative embeddings {˜U,˜U, ...,˜U} of the common user ufor domains 1, 2, ..., a, respectively. The structures of element-wise attention are shown in Feature Combination Layer of Figs. 2 and 3, respectively. The combined embedding˜Uof a common user ufor domain x can be represented as: where  is the element-wise multiplication and Wis the weight vector of the embedding of common users from domain y for domain x. Note that for the distinct users and all the items in each domain, we just reserve their embeddings without using the attention mechanism because they do not have dual/multiple embeddings. 3.4.2 For GA-CDR+CSR-P In GA-CDR+CSR-P (see Fig. 4), the element-wise attention mechanism is used to combine both the embeddings of common users for related domains and the embeddings of common items for related systems. Unlike GA-DTCDRP and GA-MTCDR-P, in GA-CDR+CSR-P, if two or multiple datasets have common users, they should make crossdomain recommendations, thus these datasets are related domains to each other. While if two or multiple datasets have common items, they should make cross-system recommendations, thus these datasets are related systems to each other. For example, in Fig. 4, domain/system 2 has the common users with domain 1, thus it is a related domain for domain 1. Meanwhile, domain/system 2 has the common items with system 3, thus it is also a related system for system 3. Domain/system 2 plays two roles, i.e., a related domain and a related system, in GA-CDR+CSR-P. In GA-CDR+CSR-P, there are a related datesets DS (domains and systems). Similar to Eq. (2), for a common user ufrom dual/multiple domains D (D ∈ DS), his/her combined embedding˜Ufor a domain D(D∈ D) can be represented as: Similarly, for a common item vfrom dual/multiple systems S (S ∈ DS), its combined embedding˜Vfor a system S(S∈ S) can be represented as: where Wis the weight vector of the embedding of common items from system y for system x. In this section, we introduce two training strategies, i.e., preliminary training and personalized training, for the neural network layers and output layer of our GA models. The preliminary training strategy is adopted by our preliminary work [27] and the personalized training strategy is adopted in this work (marked with ‘-P’, e.g., GA-DTCDR-P). 3.5.1 Preliminary Training In our preliminary work [27], we train our models with the following objective function in domain x: min`(y, ˆy) + λ(kPk+ kQk),(5) where `(y, ˆy) is a loss function between an observed interaction y and its corresponding predicted interaction ˆy (see Eq. (7)), Yand Ydenote all the observed and the unobserved user-item interactions in domain x respectively, kPk+ kQkis the regularizer (see Eq. (8)), λ is a hyperparameter which controls the importance of the regularizer, and Θis the parameter set. To avoid our model over-ﬁtted to Y(positive instances), we randomly select a certain number of unobserved user-item interactions as negative instances, denoted by Y, to replace Y. This training strategy has been widely used in the existing approaches [21]. Unlike the uniﬁed loss functions in [33], [36], [67], we train our recommendation model in each domain respectively and parallelly, which focuses on speciﬁcally improving the recommendation accuracy in each of the domains. Based on rating information, the user-item interaction y between a user uand an item vcan be represented as: We choose a normalized cross-entropy loss which can be represented as: `(y, ˆy) =ymax(R)log ˆy + (1 −ymax(R)) log(1 − ˆy), (7) where max(R) is the maximum rating in a domain. As shown in Neural Network Layers of Fig. 2, our GA subframeworks employ a neural network, i.e., MLP, to represent a non-linear relationship between users and items. The input embedding matrices of users and items in domain x for the MLP are P= [˜U; U] and Q= Vrespectively, where˜Uis the combined embedding matrix of common users for domain x, and Uis the embedding matrix of distinct users in domain x. Therefore the embedding of user uand item embedding of item vin the output layer of the MLP can be represented as: P= P= f(...f(f(P· W) · W)), Q= Q= f(...f(f(Q· W) · W)),(8) where the activation function f (∗) is ReLU, W, W... and W, W... are the weights of multi-layer networks in different layers in domain x for Pand Q, respectively. Finally, in Output Layer of Fig. 2, the predicted interaction ˆybetween uand vin domain x is as follows: Compared with the conventional inner product, the biggest advantage of cosine distance for interaction prediction is that it does not need to normalize separately. Similarly, we can train our models in each system. 3.5.2 Personalized Training Although we have adopted the element-wise attention to combine the embeddings of common entities (users/items) from different datasets (domains/systems), our preliminary training strategy still suffers from the negative transfer problem. Especially in multi-target CDR and CDR+CSR scenarios, the recommendation performance may decline as more sparser datasets join in. Inspired by the optimization problem in [68], we propose a personalized objective function for our GA framework. This personalized training strategy ﬁrst gives trainable weights on the pair-wise embedding differences of common entities between every two datasets, and then minimizes both local loss (see Eq. (7)) and these pair-wise embedding differences. The embeddings of common entities in different datasets tend to be similar but remain good personalization. Therefore, this personalized strategy can avoid negative transfer to some extent. The objective function in dataset x is represented as follows: min`(y, ˆy) +λA(kW− Wk),(10) where `(y, ˆy) is the normalized corss-entropy loss (see Eq. (7)), A is the attention-inducing function, which measures the embedding difference in a non-linear manner, W(P or Q) is the embeddings of common entities in dataset i. We adopt the negative exponential funciton, i.e., 1 − ewith a hyper-parameter θ, which has been widely-used in many existing personalized approaches [69]. Additionally, the existing personalized approaches tend to choose a ﬁxed hyper-parameter λ to control the weight on embedding difference. But in our objective function, we useP a set of trainable variables (λ= 1, e.g., λis the weight for the embedding difference between datasets i and j), to train suitable weights on the embedding differences. These trainable weights can effectively control the importance of pair-wise embedding difference of common entities and thus the trained recommendation model in each dataset can achieve good personalization. Therefore, our GA-DTCDR-P, GA-MTCDR-P, and GA-CDR+CSR-P can alleviate negative transfer by using this personalized training strategy. We conduct extensive experiments on four real-world datasets to answer the following key questions: P, and GA-CDR+CSR-P) perform when compared with the state-of-the-art models (see Result 1)? personalized training strategy contribute to performance improvement (see Result 2)? performance of our models (see Result 3)? lists (see Result 4)? affect the performance of our models (see Result 5)? 4.1 Experimental Settings 4.1.1 Experimental Datasets and Tasks To validate the recommendation performance of our GA approaches and baseline approaches, we choose four realworld datasets, i.e., three Douban subsets (DoubanBook, DoubanMusic, and DoubanMovie) [25], and MovieLens 20M [70]. For the three Douban subsets, we retain the users and items with at least 5 interactions each user, while for MovieLens 20M, we extract a MovieLens subset containing 10, 000 users with at least 5 interactions each user as well. This ﬁltering strategy has been widely used in the existing approaches [25], [48]. The three Douban subsets contain ratings, reviews, tags, user proﬁles, and item details while MovieLens contains ratings, tags, and item details. Based on these four datasets, we design two CDR tasks (Task 1 & 2 in Table 2) and one CSR task (Task 3) to validate the recommendation performance in dual-target CDR and CSR scenarios, respectively. In addition, we design one MTCDR task (Task 4) and one CDR+CSR task (Task 5) to validate the recommendation performance in multi-target CDR and CDR+CSR scenarios, respectively. We list the dataset statistics and designed tasks in Table 2. 4.1.2 Parameter Setting For a fair comparison, we optimize the parameters of our GA-DTCDR-P, GA-MTCDR-P, GA-CDR+CSR-P, and those of the baselines according to the parameter settings in their original papers. For Graph Embedding Layer of GA framework, we set the hyper-parameters of Doc2vec and Node2vec models as suggested in [56], [65], and the sampling probability α as 0.05. In Neural Network Layers of GA framework, the structure of the layers is ‘k → 2k → 4k → 8k → 4k → 2k → k’, the parameters of the neural network are initialized as the Gaussian distribution X ∼ N (0, 0.01). For training our GA-DTCDR-P, GA-MTCDR-P, and GACDR+CSR-P, we randomly select 7 negative instances for each observed positive instance into Y, adopt Adam [71] to train the neural network, and set the maximum number of training epochs to 50. The learning rate is 0.001, the regularization coefﬁcient λ is 0.001, and the batch size is 1, 024. To answer Q3, the dimension k of the embedding varies in {8, 16, 32, 64, 128}. 4.1.3 Evaluation Metrics To evaluate the recommendation performance of our GADTCDR-P, GA-MTCDR-P, GA-CDR+CSR-P models, and baseline models, we adopt the ranking-based evaluation strategy, i.e., leave-one-out evaluation, which has been widely used in the literature [22], [63]. For each test user, we choose the latest interaction with a test item as the test interaction and randomly sample 99 unobserved interactions for the test user, and then rank the test item among the 100 items. Leave-one-out evaluation includes two main metrics, i.e., Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) [63]. HR@N is the recall rate while NDCG@N measures the speciﬁc ranking quality that assigns high scores to hits at top position ranks. Note that we only report HR@10 and NDCG@10 results in Results 1-3, and HR@N and NDCG@N results in Result 4. 4.1.4 Comparison Methods As shown in Table 3, we compare our GA models with seven baseline models in three groups, i.e., (1) Single-Domain Recommendation (SDR), (2) Single-Target Cross-Domain Recommendation (CDR), and (3) Dual-Target CDR. All seven baselines are representative and/or state-of-the-art approaches for each group. Also, for the ablation study, in addition to GA-DTCDR and GA-DTCDR-P, we implement a simpliﬁed version of GA-DTCDR, i.e., GA-DTCDR Average (replacing element-wise attention with a ﬁxed combination strategy, i.e., average-pooling). For a clear comparison, in Table 3, we list the detailed training data types, encoding strategies, embedding strategies, and transfer strategies of all the models implemented in the experiments. 4.2 Performance Comparison and Analysis 4.2.1 Result 1: Performance Comparison (for Q1) To answer Q1, we compare the performance of our GADTCDR-P with those of the seven baseline models. Note that for the SDR baselines, we train them in each domain and then report their performance in each domain; for the single-target CDR baselines, we train them in both domains and then only report their performance on the sparser domain; and for the dual-target CDR models, we train them in both domains and then report their performance in each domain. Table 4 shows the experimental results in terms of HR@10 and NDCG@10 with different k embedding dimensions for Tasks 1, 2, and 3, respectively. As indicated in Table 4, our GA-DTCDR-P outperforms all the SDR, singletarget CDR, and dual-target CDR baselines by an average improvement of 9.04%. In particular, our GA-DTCDR-P improves the best-performing baselines (with results marked by * in Table 4) by an average of 10.85% for Task 1, an average of 11.21% for Task 2, and an average of 5.06% for Task 3. This is because our GA-DTCDR-P effectively leverages the richness and diversity of the information in both domains, and intelligently and effectively combines the embeddings of common users. Tables 5 and 6 show the experimental results of our GAMCDR-P and GA-CDR+CSR-P with different embedding dimensions k for Tasks 4 and 5, respectively. Compared with the results of the seven baselines in Table 4, our GA-MTCDR-P and GA-CDR+CSR-P can improve the bestperforming baselines by an average of 9.21% (the general improvement of our GA-DTCDR-P is 9.04%). This means that, in general, the recommendation performance in all datasets improves as more datasets join in, and hence avoiding negative transfer to some extent. 4.2.2 Result 2: Ablation Study (for Q2) To answer Q2, we implement a variant of our preliminary GA-DTCDR, i.e., GA-DTCDR Average, by replacing the element-wise attention with average-pooling, which can demonstrate the detailed contribution of the elementwise attention in our GA models. Average-pooling is the combination strategy used by the existing dual-target CDR approaches [25], which gives the weight equally, i.e., 0.5, to the embeddings of common users learned from dual domains. Additionally, to demonstrate the detailed contribution of our proposed personalized training strategy, we also compare the performance of GA-DTCDR with that of GA-DTCDR-P in this section. On the one hand, as we can see from Table 4, with the element-wise attention, our preliminary GA-DTCDR improves GA-DTCDR Average by an average of 6.76%. This means that element-wise attention plays a very important role in our GA-DTCDR and the existing ﬁxed combination strategies can hardly achieve an effective embedding optimization in each target dataset. On the other hand, compared with our preliminary GA-DTCDR, our GA-DTCDR-P achieves an average improvement of 0.54% (according to the results in Table 4). This result indicates that our personalized training strategy can further improve the recommendation accuracy of the baselines and our DTCDR models and can alleviate negative transfer. 4.2.3 Result 3: Impact of Embedding Dimension k (for Q3) To answer Q3, we analyze the effect of k on the performance of our preliminary GA-DTCDR, GA-DTCDR-P, GAMTCDR-P, and GA-CDR+CSR-P, as depicted in Tables 4, 5, and 6. In general, in terms of HR@10 and NDCG@10, the recommendation accuracy of our GA models increases with HR@NDCG@ k because a larger embedding can represent a user/item more accurately. However, considering the structure of the neural network layers in Parameter Setting, the training time of our GA models also increases with k. This is a tradeoff. Therefore, considering both aspects, k = 64 is ideal in our experiments. 4.2.4 Result 4: Top-N Recommendation (for Q4) To answer Q4, we compare the performance of top-N recommendation in terms of HR@N and NDCG@N where N ranges from 1 to 10. In fact, the performance trends of all top-N experiments (for all the tasks with different k) are similar. Thus, due to space limitation, we only report the Top-N recommendation results of all the seven baseline models, GA-DTCDR Average, and GA-DTCDR for Task 1 (k = 8). In Fig. 5, in both DoubanBook (sparser) and DoubanMovie (richer), the performance of our preliminary GA-DTCDR is consistently better than those of all the seven baselines. On DoubanBook, considering all the Top-N recommendations, our preliminary GA-DTCDR improves the best-performing baselines in different experimental cases by an average of 1.74% for HR@N, and by an average of 5.83% for NDCG@N, while on DoubanMovie, our preliminary GA-DTCDR improves the best-performing baselines in different experimental cases by an average of 8.13% for HR@N, and by an average of 7.55% for NDCG@N . 4.2.5 Result 5: Impact of Sparsity and Overlap Scale (for Q5) To answer Q5, we extract different sparsity degrees of subdatasets to analyze the effect of sparsity and overlap scale on the performance of our models. Due to space limitations, we only report the experimental results of GA-MTCDRP (k = 64) for Task 4 on different sub-datasets in Table 7. From it, we ﬁnd that, in general, the recommendation performance increases with densities of sub-datasets. However, the recommendation results in Dataset Version v3 are better than those in Dataset Version v4. This is because the number of common users, i.e., overlap scale, signiﬁcantly decreases with the increase of density. Therefore, according to the experimental results, in general, the recommendation performance of our GA-MTCDR-P increases with density (i.e., decreases with sparsity) and overlap scale. In this paper, we have proposed a uniﬁed framework, called GA (based on Graph embedding and Attention techniques), for all dual-target CDR (GA-DTCDR-P), multi-target CDR (GA-MTCDR-P), and CDR+CSR (GA-CDR+CSR-P) scenarios. In our GA framework, the element-wise attention mechanism and the personalized training strategy effectively improve the recommendation accuracy in all datasets and avoid negative transfer to some extent. Also, we have conducted extensive experiments to demonstrate the superior performance of our proposed GA models. In the future, we plan to take more training strategies and further alleviate negative transfer.