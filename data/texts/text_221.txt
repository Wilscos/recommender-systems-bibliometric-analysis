1 INTRODUCTION Oﬀ-policy evalu ati on (OPE) is the met hod that attempts to estimate the performance of decision making policies using historical data generated by diﬀerent policies without conducting costly online A/B tests [1, 2]. Accurate OPE is essential in domains such as healthcare, marketing or recommender systems to avoid deploying poor performing policies, as such policies may hart human lives or destroy the user experience. Thus, many OPE methods with t heoretical backgrounds have been proposed, including Direct Method (DM), Inverse Probability Weighting (IPW), and Doubly Robust (DR). One emerging challenge with this trend is that a suitable estimator can be diﬀerent for each application setting. For example, DM has low variance but has a large bias, and thus, performs better in small sample settings. On the other hand, IPW has a low bias but has a large variance, and thus reveals better performance in large sample sett ings. It is often unknown for practitioners which estimator to use for their speciﬁc applications and purposes. To ﬁnd out a suitable estimator among many candidates, we use a data-driven estimato r selection procedure for oﬀ-policy policy performance estimators as a p ractical solution. As proof of concept, we use our procedure to select the best estimator to evaluate coupon treatment policies on a real-world online content delivery ser vice. In the experiment, we ﬁrst observe that a suitable estimator might change with diﬀerent deﬁnitions o f the outcome variable, and thus the accurate estimator selection is critical in real-world applications of OPE. T hen, we demonstrate that, by utilizing our estimator selection procedu re, we can easily ﬁnd ou t suitab le estimators for each purpo se. We believe that our estimator selection procedure and case study help practitio ners identify the best OPE method for their environments. 2 SET UP AND METHOD We denote 𝑋 ∈ X as a context vector and 𝑇 ∈ T = {0, 1} as a binary treatment assignment indicator individual user 𝑖 receives the treatment, 𝑇 denoted as (𝑌 (0), 𝑌 (1)) for each individual. 𝑌 (0) is a potential outcome associated with 𝑇 = 0, and 𝑌 (1) is associated with 𝑇 = 1. No te that each individual receives only one treatment, and only a potential outcome for the received treatment is observed. We c an represent the observed outco me as: 𝑌 = 𝑇𝑌 (1) + (1 − 𝑇 )𝑌 (0). Table 1. Relative RMSE of oﬀ-policy estimators when 𝑌 is con-Table 2. Relative RMSE of oﬀ-policy estimators when 𝑌 is revtent consumption indicatorenue from users Note: D→ 𝜋is a case where we attempt to estimate the performance of 𝜋using log data generated by 𝜋. In contrast, D→ 𝜋is a case where we attempt to estimate the performance of 𝜋using lo g data ge nerated by 𝜋. The bold fonts represent the best oﬀ-policy estimator among DM, IP W, and DR for each setting (lower value is better). A policy automatically assigns t reatments to users aiming to maximize the out come. We denote a policy as a function that maps a context vector to one of the possible treatments, i.e., 𝜋 : X → T . Then, the performance of a policy is deﬁned as 𝑉 (𝜋) = E[𝑌 (𝜋 (𝑋 ))]. The goal of OPE is to estimate 𝑉 (𝜋 ) for a given new policy 𝜋 using log data D = {(𝑋,𝑇, 𝑌)}collected by an old policy diﬀerent from 𝜋. Our strategy to select the suitable estimator is to use two sou rces of logged bandit feed back collected by two diﬀerent behavior policies. We denote log data generated by 𝜋and 𝜋as D= {(𝑋,𝑇, 𝑌)}and D= {(𝑋,𝑇, 𝑌)}, respectively. To evaluate the performance of an estimato rˆ𝑉 , we ﬁrst estimate policy performances of 𝜋and 𝜋by ˆ𝑉 (𝜋; D) andˆ𝑉 (𝜋; D). Then, we use on-policy estimates as the ground-truth policy performances, i.e., 𝑉 (𝜋) ≈ ˆ𝑉 (𝜋; D) = 𝑛Í𝑌and 𝑉 (𝜋) ≈ˆ𝑉 (𝜋; D) = 𝑛Í𝑌. Finally, we compare the oﬀ-policy estimates ˆ𝑉 (𝜋; D) andˆ𝑉 (𝜋; D) with their ground-truths (on-policy estimates) 𝑉 (𝜋) and 𝑉 (𝜋) to evaluate the estimation accuracy of an estimatorˆ𝑉 . We evaluate the estimation accuracy of an estimatorˆ𝑉 by the relative root mean-squared-r error deﬁned as𝐾Í()(same for 𝜋) where 𝑘 denotes a diﬀerent subsample of logged bandit feedback made by the sample splitting or bootstrap sampling. By applying the above procedure to several candidate estimators, we can select the estimator having the best estimation accuracy among candidates in a data-driven manner. 3 A CASE STUDY To show the usefulness of our procedure, we constructed Dand Dby randomly assigning two diﬀerent policies (𝜋and 𝜋) to users on our content delivery platform. Here, 𝑋 is a user’s context vector, 𝑇 is a coupon assignment indicator, and 𝑌 is either a user’s content consumption indicator (binary) or the revenue from each user (continuous). We report the estimator selection results for each deﬁnition of 𝑌 in Table 1 and 2. We u sed DM, IPW, and DR as candidate oﬀ-policy estimators. The tables show that diﬀerent estimators should be used for each setting and purpose. This is because the prediction accuracy of the outcome regressor used in DM and DR can be diﬀerent for each deﬁnition of 𝑌 . We conclude from the results that we should use DM when we want to maximize the users’ content consumption probability. In contrast, we use IPW or DR when we consider the revenue from users as the outcome. After the successful empirical veriﬁcation, our data-driven estimator selection method has been used t o decide which estimators to use to create coupon allocation policies on our platform.