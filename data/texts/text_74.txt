NetEase Cloud Music, NetEase Inc.NetEase Cloud Music, NetEase Inc. User behavior has been validated to be eective in revealing personalized preferences for commercial recommendations. However, few user-item interactions can be collected for new users, which results in a nullspace for their interests, i.e., the cold-start dilemma. In this paper, a two-tower framework, namely, the model-agnostic interest learning (MAIL) framework, is proposed to address the cold-start recommendation (CSR) problem for recommender systems. In MAIL, one unique tower is constructed to tackle the CSR from a zero-shot view, and the other tower focuses on the general ranking task. Specically, the zero-shot tower rst performs cross-modal reconstruction with dual autoencoders to obtain virtual behavior data from highly aligned hidden features for new users; and the ranking tower can then output recommendations for users based on the completed data by the zero-shot tower. Practically, the ranking tower in MAIL is model-agnostic and can be implemented with any embedding-based deep models. Based on the cotraining of the two towers, the MAIL presents an end-to-end method for recommender systems that shows an incremental performance improvement. The proposed method has been successfully deployed on the live recommendation system of NetEase Cloud Music to achieve a click-through rate improvement of 13%∼15% for millions of users. Oine experiments on real-world datasets also show its superior performance in CSR. Our code is available. • Information systems → Recommender systems;• Computing methodologies → Articial intelligence. NetEase Cloud Music, NetEase Inc. Recommender Systems, Two-Tower Structure, Cross-Modal Reconstruction, Commercial Application ACM Reference Format: Philip J. Feng, Pingjun Pan, Tingting Zhou, Hongxiang Chen, and Chuanjiang Luo. 2021. Zero Shot on the Cold-Start Problem: Model-Agnostic Interest Learning for Recommender Systems. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482312 Recommender systems are playing important roles in various web and mobile applications, e.g., music platforms, which pick a fraction of items that a user might enjoy based on the user’s past behavior and current context [7,25]. Despite the success of recent deep learning [30] and matrix factorization-based methods [17], a common challenge for recommender systems is the cold-start recommendation (CSR); that is, the recommendation model shows degraded performance in selecting satisfactory items for new users because of the lack of user-item interaction records [9, 15]. Dierent approaches have been proposed to handle the cold-start problem. A basic CSR method is to leverage personal information to generate recommendations for new users. This is reasonable because some basic attributes of users, e.g., age and occupation, can be collected when they register accounts, and people with similar attributes usually have similar interests and behaviors [2]. Some other auxiliary information is also used in CSR. For example, crossdomain knowledge is used by Hsu et al. [27] and Philip et al. [5] to enrich the feature representation of new users. Complementary heterogeneous information is leveraged to enrich user-item interactions by Lu et al. [13] using heterogeneous information networks. A stacked denosing autoencoder is used in a hybrid model to extract embedding knowledge for CSR by Wei et al. [22]. In [10,20,26], the popular meta-learning framework is applied to transfer knowledge from support sets and treat CSR as a few-shot learning task. However, the major limitation of these approaches is that new users’ behaviors and interests are still missing. The auxiliary information, such as semantic knowledge and pretrained embedding, cannot change the dilemma that the recommender system knows less about new users than old users. Figure 1: Paradigms of ZSL and CSR. New users’ b ehavior in CSR is missing like features of unseen class in ZSL. Similar paradigms allow addressing CSR from a zero-shot view. Recently, zero-shot learning (ZSL) [6,23], which has the same intention as CSR, has attracted ever-increasing attention in the computer vision eld. Both ZSL and CSR involve the missing data problem of some objects in a specic scene. To provide a better understanding, we compare paradigms of ZSL and CSR in Figure 1. In the image recognition task, the training features of the unseen class are missing, and ZSL uses textual descriptions of objects to perform knowledge transfer from the seen class to the unseen class. In the cold-start recommendation task, behavior data of new users are missing, and CSR can utilize user attributes to generalize interests from old users to new users. Based on the zero-shot view, Li et al. [4] proposed the low-rank linear autoencoder (LLAE) for CSR. The LLAE addresses CSR with a linear autoencoder, where a new user’s behavior is learned from attributes with a low-rank constraint. Kumar et al. [24] also applied some classic zero-shot models, e.g., ESZSL [21] and ConSE [14], for unseen hashtag recommendation. However, the behavior data in CSR are dierent from the image features in ZSL. A fact that should be emphasized is that a user can interact with any items in the recommender system. The huge number of items in real-world applications make the modal-shift problem between user behavior and attributes more challenging than that in ZSL. The linear models discussed above may not well cover the complex mapping, and a more eective alignment technique is desired to overcome the shift problem between user behavior and attributes. Moreover, we argue that a practical cold-start module is supposed to be a supplement to the existing ranking model instead of a substitute. Some methods address CSR by replacing the existing ranking model with a new method. For example, Zhao et al. [19] designed a probabilistic matrix factorization method based on the similarity propagation algorithm, and Wang et al. [11] used improved collaborative ltering to obtain rating information for item recommendation. The limitation of these methods is that they pay too much attention to the cold-start problem but ignore the state-of-the-art ranking performance. Although popular ranking models, e.g., ESMM [16] and DMR [18], suer from the cold-start problem on new users, they usually perform well on old users. Incremental performance improvement can be expected by integrating the cold-start module with the ranking model together. Based on the above discussion, we propose a model-agnostic interest learning (MAIL) framework for recommender systems to address the cold-start problem in this paper. In MAIL, a new zeroshot tower and a general ranking tower are constructed together. From a cotraining view, the zero-shot tower formulates CSR as a ZSL task to provide virtual behavior data for new users, and the ranking tower can then capture new user’s interests based on the generated virtual data. Specically, in the zero-shot tower, two spaces, i.e., user behavior space and user attribute space, are explicitly constructed and learned by dual autoencoders. In the common hidden space of autoencoders, cross-modal reconstruction is performed to overcome the modal-shift problem between user behavior and user attributes for old users. Based on the homogeneity of user’s attributes, new user’s behavior can be readily reconstructed from well-aligned hidden features. For the ranking tower, it is model-agnostic and can be implemented with any embedding-based ranking models. The ranking tower is assigned to share its embedding layer with the zeroshot tower to tackle the data sparsity problem for ecient model training. We show that integrating zero-shot learning together with popular ranking models makes recommender systems free from CSR so that performance improvement on state-of-the-art results is achieved. The contributions of this paper are fourfold: 1) A new framework, namely, the model-agnostic interest leaning (MAIL) framework, is designed for recommender systems, which constructs a unique zero-shot tower to provide virtual behavior data for new users, enabling the ranking tower to capture their interests for cold-start recommendations. 2) Based on cross-modal reconstruction with dual autoencoders, the zero-shot tower in MAIL makes the hidden features of user attributes and user behavior highly aligned to generalize behavior data from old users to new users. 3) The ranking tower in MAIL can be implemented with any embedding-based deep models for practical application, and it also shares its embedding layer with the zero-shot tower to tackle the data sparsity problem for eective model training. 4) In additon to oine experiments, we deploy the proposed MAIL model on an online large-scale live recommendation system to verify the model eectiveness with millions of users in a realworld setting. In the computer vision eld, intelligent models optimize millions of parameters using sucient training samples to obtain recognition ability for target objects [1]. However, due to the long-tailed distribution of object categories, the collection of numerous samples for some rarely seen objects is generally challenging. To recognize unseen objects, zero-shot learning is explored by Lampert et al. [23] and Akata et al. [12] in recent years. From a macro perspective, existing ZSL methods can be grouped into three categories, including classic probability-based methods, compatibility-based methods, and generative models. Probability-based methods are represented by direct attribute-based prediction [8], as these methods learn a number of probabilistic classiers for attributes and combine the scores to make predictions. Incremental classiers are also used by Feng et al. [33] to balance the learning between seen and unseen classes. Compatibility-based methods learn a compatibility function between the semantic embedding space and the vision feature space. For example, the ESZSL model proposed by Romera-Paredes et al. [21] applies square loss and implicit regularization between embedding spaces to rank the class possibilities in an embarrassingly simple way. A hashing model is trained by Ji et al. [29] to transfer knowledge and implement cross-model learning based on the semantic embedding space. Generative models train a conditional generator for visual features based on semantic features. Popular generative models include autoencoders and generative adversarial networks [3]. In comparison with the other two paradigms, generative models tackle the essential insucient data problem for ZSL to achieve state-of-the-art results. The proposed zero-shot tower in MAIL is also constructed as a generative model to tackle the missing behavior problem for CSR. While deep ranking models have achieved considerable success in recommender systems, diculty usually arises in contending with new users with a nullspace for their interests, known as cold-start recommendation. Some solutions in solving this problem have been proposed in current publications, including the input of semantic knowledge, second-domain knowledge transfer, active learning and meta-learning. For example, Chou et al. [35] proposed a tensor factorization-based algorithm that exploits content features extracted from music audio to address CSR for the emerging application next-song recommendation. A unique characteristic of the algorithm is that it learns and updates the mapping between the audio feature space and the item latent space each time during the iterations of the factorization process, which makes the content features more eective. Li et al. [2] proposed an innovative cross-domain recommendation model based on partial least squares regression. The cross-domain model is able to purely use source-domain ratings to predict the ratings for cold-start users who never rated items in the target domains. Apart from utilizing auxiliary information from other scenes and datasets, some researchers have applied active learning approaches to deal with CSR. Zhu et al. [32] designed useful user selection criteria based on items’ attributes and users’ rating history, and combined the criteria in an optimization framework for selecting users. By exploiting the feedback ratings, accurate rating predictions for the other unselected users can then be generated. The popular meta-learning [28] approach is also applied to address CSR, which regards modeling each user’s preference as a learning task and minimizes the sum of the recommendation loss on training users to globally update the parameters of the meta-learner. By providing the initialization of parameters, the meta-learner guides the learning process for new users. The major dierence between the proposed MAIL and the above works is that the MAIL addresses the essential missing behavior problem for new users by transferring knowledge from old users with a zero-shot tower. In addition to the basic user attributes, neither auxiliary information nor an additional dataset is added in the modeling for CSR, which is convenient when feature engineering and data collection are expensive. 3.1.1 Features in Recommender Systems. There are usually four categories of features in recommender systems: user attributes 𝒂 ∈ A, user behavior𝒗 ∈ V, context𝒄 ∈ C, and target item𝒕 ∈ T. User attributes inAcontain the user ID, occupation, and so on; user behavior inVis a sequential list of user interacted items with corresponding IDs. Context inCcontains the date, recall method, location, etc. Features of target item inTare the item ID, category ID, and so on. The ranking model in a recommender system learns a mapping function𝑓 (𝒙)to output the score^𝑦for an item, where 𝒙 = {𝒂, 𝒗, 𝒄, 𝒕}is the input. Generally, items with higher scores will be recommended to users with priority. Note that most of the mentioned features are categorical, which are represented by one-hot vectors with high dimension. In deep learning models, one-hot features are usually transformed into low-dimensional dense features by an embedding layer. Hence, the notations, e.g.,𝒂and𝒗, actually denote the embedding vectors. For example, user attributes are𝒂 = [𝒂, ..., 𝒂] ∈ R, where𝑛is the number of attributes and𝑑is the embedding dimension. For the user behavior sequence𝒗 = [𝒗, ..., 𝒗] ∈ R, each item𝒗is obtained by performing element-wise multiplication between the embedding of the item ID and the embedding of the user’s action. Moreover, some dense features can be directly used without the embedding transformation. 3.1.2 Zero-Shot Learning for CSR. In cold-start recommendation, we have two scopes: the old user scopeSand the new user scope O. For old users, four categories of features are ready for ranking models, which are denoted as𝒙= {𝒂, 𝒗, 𝒄, 𝒕}. For new users, the behavior𝒗is missing, and features of new users are denoted as𝒙= {𝒂, 𝒄, 𝒕}. In this paper, we train a zero-shot tower^𝒗= 𝑔(𝒂, 𝒗)based on old users, which can be applied to new users to obtain virtual behavior data^𝒗= 𝑔(𝒂)to tackle the cold-start problem. The notations^𝒗= 𝑔(𝒂, 𝒗)and^𝒗= 𝑔(𝒂)denote that only old users’ behavior data𝒗are available for the zero-shot tower while new users’ behavior data are not available. To summarize, the whole model in this paper can be formulated as^𝑦= 𝐹 (𝒙) = 𝑓 (𝒂, 𝒄, 𝒕, 𝒗)for old users and^𝑦= 𝐹 (𝒙) = 𝑓 (𝒂, 𝒄, 𝒕,^𝒗)for new users, where^𝒗= 𝑔(𝒂). 3.2.1 Overall Idea. The overall idea of the proposed MAIL is illustrated in Figure 2. In addition to a general ranking tower, a new zero-shot tower can be observed. The ranking tower outputs recommendations based on the completed data by the zero-shot tower, and the zero-shot tower enjoys the dense embedding by the ranking tower. The cotraining of the two towers make the recommender systems free from CSR so that an incremental performance improvement is achieved. 3.2.2 Zero-Shot Tower for Cold-Start Problem. Here, we show how to construct the zero-shot tower for CSR in detail. First, it is claried that the zero-shot tower is trained on old users and makes inference on new users to transfer behavior data from old users to new users based on their attributes. Hence, two kinds Figure 2: Illustration of the proposed MAIL, where a two-tower structure can be observed. The zero-shot tower addresses the CSR by providing virtual new user’s behavior data^𝒗for the ranking tower. The ranking tower outputs recommendations based on the completed data and shares its embedding lookup table with the zero-shot tower. of embedding features, i.e., user attributes𝒂and user behavior𝒗, are used for model training. The embedding features are trained by the ranking tower. Through learning from the dense embedding vectors, the zero-shot tower obtains more eective training and extraction processes. To weight dierent features, the self-attention technique is then applied on𝒂and𝒗to construct attribute features𝒉and behavior features𝒉, respectively. Taking𝒉as an example, the attention mechanism is performed as follows: where𝑛is the number of user attributes and𝒂∈ Ris the embedding vector of the𝑖-th user attribute in𝒂. The attention weight 𝛼is obtained by where𝒛 ∈ R,𝒘 ∈ R, and𝒃 ∈ Rare trainable parameters, and 𝑑is the given attention dimension. Based on𝒉and𝒉, cross-modal reconstruction is performed to align user attributes and behavior for a more eective attributebased behavior generation. In the attribute space, the cross-modal reconstruction loss is: and in the behavior space, the cross-modal reconstruction loss is: where𝒑= 𝐸1(𝒉) ∈ R,𝒒= 𝐸2(𝒉) ∈ R, and𝑑is the given hidden dimension.𝐸1and𝐸2are encoders for user attributes and user behavior, and𝐷1and𝐷2are corresponding decoders, which are shown in Figure 2. The cross-modal reconstruction loss assigns the hidden features 𝒑and𝒒to reconstruct𝒂and𝒗, respectively, with the common decoders, which actually makes𝒑and𝒒have the same prediction ability for user attributes and user behavior. Take the exposition a step further, we denote𝑷∈ Ras a batch of𝒑,𝑸∈ Ras a batch of𝒒, and𝑛as the batch size. The maximum mean discrepancy (MMD) constraint is then applied to𝑷and𝑸to minimize their distribution distance and guide the convergence of cross-modal reconstruction, which can be formulated as: whereHis the reproducing kernel Hilbert space (RKHS),𝜙 : 𝒑, 𝒒→ H, 𝑘 is a Gaussian kernel function: and 𝜎 = 1 is the bandwidth in this paper. To summarize, the training loss of the zero-shot tower is: The rst two items inLrequire both𝒑and𝒒to have the reconstruction ability for𝒂and𝒗, while the third item inL makes𝒑and𝒒enjoy the same hidden space for a better convergence of cross-modal reconstruction. With the triple-aligned hidden features, the virtual behavior data can be readily generated for new users from their attributes as follows: where𝒉is the attention weighted features of𝒂and can be obtained by Eqs. (1) and (2). 3.2.3 Ranking Tower for Recommendation. Here, we show how to construct the ranking tower for recommendation in detail. Generally, the ranking tower is model-agnostic and can be implemented by any embedding-based models for estimating the clickthrough rate (CTR), post-click conversion rate (CVR), or post-view click through&conversion rate (CTCVR) [16]. Here, the basic deep model shown in Figure 2 is introduced to provide a better understanding. The ranking tower treats new users and old users in dierent ways, which means there should be a ag to indicate whether a user is a new user, which can be obtained by where ag = 𝑡𝑟𝑢𝑒 for new users and ag = 𝑓 𝑎𝑙𝑠𝑒 for old users. Based on the embedding lookup table and ag, the input𝒙of the ranking tower can be denoted as: where^𝒗obtained from Eq. (8) is the virtual behavior data for new users. A basic sum pooling layer is then added to obtain the concatenated hidden features𝒛 = [𝒛, 𝒛, 𝒛, 𝒛] ∈ Rfrom𝒙, where𝑑 is the embedding dimension. Taking𝒛as an example, the sum pooling is performed as follows: where 𝒂is the 𝑖-th attribute embedding vector in 𝒂. The nal output^𝑦is obtained by a multilayer perceptron, which is formulated as: 𝒉= 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐵𝑁 (𝐷𝑒𝑛𝑠𝑒(𝐷𝑟𝑜𝑝𝑜𝑢𝑡 (𝒛, 0.5)))) 𝒉= 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐵𝑁 (𝐷𝑒𝑛𝑠𝑒(𝐷𝑟𝑜𝑝𝑜𝑢𝑡 (𝒉, 0.5))))(12) and the training loss of the ranking tower is: where 𝑛 is the batch size. In summary, the overall training loss of MAIL is: where two towers are cotrained to tackle the CSR for recommender systems. It is worth mentioning that, in practical applications, not only will there be new users in the test data but there will also be new users in the training data. It is dicult for a ranking and CSR two-stage model to address the missing data problem in the ranking model’s training phase because the CSR model based on embedding features has to be trained after the ranking model. In contrast, the proposed MAIL is designed as an end-to-end model to obtain virtual behavior data from the zero-shot tower for the ranking tower whenever the need arises and is hence more practical for applications. Figure 3: Illustration of the autoencoder used in MAIL, where the encoder and decoder are designed with residual structures for ecient feature extraction. 3.2.4 Implementation Tricks and the Algorithm of MAIL. Here, implementation tricks and the training algorithm of MAIL are summarized. 1) Cotraining with two optimizers: The two towers in MAIL are trained by two optimizers, respectively. One optimizer𝐺is used forL, and the other𝐺is forL. It is noted that the embedding lookup table is completely trained by𝐺together with the ranking tower and is not aected by𝐺. Because the aim of the zero-shot tower is to reconstruct embedding vectors instead of changing them, applying the gradients of the zero-shot tower on the embedding lookup table would degrade model performance. 2) Autoencoders with residual structure: Two autoencoders are used in the zero-shot tower to perform the cross-modal reconstruction. Considering the sparse information in recommender systems, a residual structure is designed for the autoencoder to perform the reconstruction task better. Taking the left autoencoder in Figure 2 as an example, the residual structure is shown in Figure 3. The forward-propagation of the residual encoder can be formulated as follows: 𝒛= 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐷𝑒𝑛𝑠𝑒 (𝐷𝑟𝑜𝑝𝑜𝑢𝑡 (𝒉, 0.5))) 𝒛=𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐷𝑒𝑛𝑠𝑒 (𝐷𝑟𝑜𝑝𝑜𝑢𝑡 (𝐶𝑜𝑛𝑐𝑎𝑡 (𝒛, 𝒉), 0.5)))(15) 𝒑= 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐷𝑒𝑛𝑠𝑒 (𝐶𝑜𝑛𝑐𝑎𝑡 (𝒛, 𝒉)))), and the forward-propagation of the residual decoder can be formulated as: 𝒛= 𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 (𝐷𝑒𝑛𝑠𝑒 (𝐷𝑟𝑜𝑝𝑜𝑢𝑡 (𝐶𝑜𝑛𝑐𝑎𝑡 (𝒛, 𝒑), 0.5)))(16) The principle of the residual structure is similar to that of the wide&deep model [34], where both the wide module’s memory ability and the deep module’s generalization ability are retained by feature concatenation. 3) Training algorithm of MAIL: The algorithm of MAIL is summarized in Algorithm 1. In MAIL, the zero-shot tower generalizes behavior data from old users to new users based on their attributes. A basic assumption behind the knowledge transfer is that people with similar attributes have similar behaviors. However, there seems to be a contradiction in that the user attribute space is much smaller than the user behavior space in recommender systems. For better clarication, we denote the number of attributes as𝑛and assume that each attribute has𝑘dierent values. Similarly, the length of a user’s behavior sequence is denoted as𝑛and the number of items that can be interacted with users is denoted as𝑘. The condition𝑛𝑘>= 𝑛𝑘 seems to be required for eective learning from attributes to behavior, while both𝑛and𝑘may be larger than𝑛and𝑘, respectively, in a real recommender system. Here, we highlight that the core of user behavior is user interest instead of items. Compared with learning what items users have interacted with, it is better to learn what style of items users like for recommender systems. This is one of the important reasons why the dense embedding vector𝒗is implemented as the learning target of the zero-shot tower. Based on the dense vector, a user’s interest 𝜙 (𝒓 ) can be readily represented by a set of items as where𝜖is an error bound representing the interest range. As long as the generated^𝒂 ∈ 𝜙 (𝒓), it is feasible for the ranking tower to Algorithm 1 Learning Procedure for MAIL Zero-Shot Tower, Ranking Tower, Training Phase: 1Construct the computing graph as Figure 2 and initializethe model parameters using the Xavier method. 2Two Adam optimizers are used for model optimization.Optimizer 𝐺→ L; Optimizer 𝐺→ L. For 𝑗 = 1 : 𝐽 (rounds in an epoch) do 3Make ag for a batch of data by Eq. (9). 4Obtain Lby Eq. (7) for users with ag = 𝑓 𝑎𝑙𝑠𝑒. 5Perform back-propagation and apply 𝐺for L. 6Obtain^𝒗by Eq. (8) for users with ag = 𝑡𝑟𝑢𝑒. 7If ag, 𝒙 = {𝒂, 𝒄, 𝒕,^𝒗}; else 𝒙 = {𝒂, 𝒄, 𝒕, 𝒗} 8Obtain Lby Eq. (13) for all users. 9Perform back-propagation and apply 𝐺for L. Testing Phase: 10Make ag for all users by Eq. (9). Zero-Shot Tower: 11Obtain^𝒗for users with ag = 𝑡𝑟𝑢𝑒 by Eq. (8). Ranking Tower: 12Obtain^𝑦 for all users by Eq. (12). capture user interest. Hence, the object of the user attribute space is actually the user interest space instead of the user behavior space. Meanwhile, the kinds of user interests𝑛are much smaller than 𝑛𝑘, which makes the attribute-based knowledge transfer available for CSR. Here, the MAIL is validated on both public and industrial datasets. First, we introduce experimental settings and compared methods in detail. Next, the recommendation performance and an ablation study are given to show the state-of-the-art results. Visualized features are also presented to provide an intuitive understanding. BaseDNN 0.5862 0.00% 0.5658 0.00% 0.6271 0.00% 0.5897 0.00% 0.6771 0.00% 0.6380 0.00% 0.7782 0.00% 0.6459 0.00% MetaEmb 0.5911 0.84% 0.5682 0.42% 0.6273 0.03% 0.5900 0.05% 0.6834 0.93% 0.6404 0.37% 0.7788 0.07% 0.6466 0.10% MAIL-Base 0.5934 1.22% 0.5710 0.91% 0.6271 0.00% 0.5898 0.02% 0.6849 1.15% 0.6413 0.51% 0.7788 0.07% 0.6460 0.01% MAIL-DMR 0.5958 1.34% 0.5733 1.14% 0.6302 0.00% 0.5915 0.06% 0.6895 1.05% 0.6465 0.70% 0.7838 0.06% 0.6558 0.10% Red font and blue font are the highest and the second highest results in each column. EmbLR is the baseline of LLAE; BaseDNN is the baseline of MetaEmb and MAIL-Base; DMR is the baseline of MAIL-DMR. 4.1.1 Datasets. Two large-scale real-world dataset are used here. 1)Public DatasetThe Alimama Datasetcontains ad displays and click logs randomly sampled from Taobao over 8 days. It includes 1.14 million users and 0.84 million items with 6 million user-item interaction logs. The logs from the rst seven days are used as the training set, and logs from the last day are used as the testing set. Note that the dataset is provided without new users. Hence, we randomly select forty percent of users in the training and testing set and treat them as new users by removing their behaviors. 2)Industrial DatasetThe industrial dataset from NetEase Cloud Music contains live recommendation and user behavior logs randomly sampled from a company (abiding by the double-blind principle) over 8 days. It includes 613 thousand users and 52 thousand items with 2 million user-item interaction logs. In the industrial dataset, approximately half of the users are new users. The statistics of the two datasets are summarized in Table I. 4.1.2 Compared Methods. Seven models are compared. 1)EmbLRLogistic regression is a classic linear model that can be regarded as a shallow neural network. Here, we implement logistic regression with an embedding layer, i.e., EmbLR, to content with sparse IDs. EmbLR is the baseline of LLAE. 2)LLAE (2019)LLAE [4] is a zero-shot learning-based cold-start method, that generates behavior data for logistic regression with a linear low-rank autoencoder. As discussed in [4], the weight of low-rank loss is decided by cross-validation and set to 30. 3)BaseDNNBaseDNN is the ranking tower shown in Figure 2, which is used here as a baseline of MetaEmb and MAIL-Base. 4)MetaEmb (2019)MetaEmb [28] is a meta-learning-based coldstart method, that learns to generate desirable initial embeddings for new IDs. The base model is implemented as the BaseDNN. 5)MAIL-Base (Ours)MAIL-Base is the proposed two-tower model shown in Figure 2, which uses the zero-shot tower to tackle the cold-start problem for BaseDNN. 6)DMR (2020)DMR [18] is a state-of-the-art model for recommender systems, that consists of an item-to-item network and a user-to-item network to combine the thought of collaborative ltering for the ranking task. DMR is the baseline of MAIL-DMR. 7)MAIL-DMR (Ours)MAIL is the proposed two-tower model, that uses DMR as the ranking tower to obtain an incremental performance improvement based on the designed zero-shot tower. In regard to the above seven models, EmbLR and LLAE are linear models, BaseDNN, MetaEmb, and MAIL-Base are basic deep models, and DMR and MAIL-DMR are improved deep models. 4.1.3 Metrics. The area under the ROC curve (AUC) and the group area under the ROC curve (GAUC) are used as metrics. The model performance on both new users and old users are reported. In addition, for each result, the relative improvement (RI) between the baseline and the improved method is provided for an intuitive comparison. RI is obtained as follows: where𝑎is the result of the baseline and𝑏is the result of the improved method. In the experiments, we set the learning rate to 0.001, embedding size to 32, batch size to 1024, and dropout rate to 0.5. The maximum length of the behavior sequence is set to 100 for all users. If the length of a user’s behavior sequence is less than 100, we concatenate the behavior sequence with zero padding. In MAIL, dual autoencoders are used. The dimension of hidden layers in encoders is 1024-512-512, and the dimension of hidden layers in the decoders is 512-1024-𝑑, where𝑑denotes the dimension of target vectors. The dimension of the hidden layers in the multilayer perceptron of the ranking tower is 512-256. The Adam optimizer [31] is used with 𝛽= 0.9 and 𝛽= 0.999 in this paper. The results are summarized in Table II. LLAE performs better than the basic EmbLR method. An AUC improvement of 0.6%∼0.9% is obtained on new users, which is signicant for large-scale realworld datasets. However, LLAE is limited to linear transformations and based on linear logistic regression. In comparison with the other ve deep models, the weak representation ability of LLAE can be observed, which demonstrates the eectiveness of nonlinear transformation and a deep structure. For MetaEmb and MAIL-Base, the BaseDNN model is used as a baseline and an AUC improvement of 0.8%∼1.3% is obtained on new users. The proposed MAIL-Base generates behavior data based on user attributes to address CSR, while the MetaEmb gives a better initial embedding for the ranking model. In comparison, the generative strategy of MAIL is more eective than the initialization strategy of MetaEmb. Moreover, by comparing MAIL-Base and MAIL-DMR, it can be observed that MAIL can use better models in the ranking tower to achieve an incremental performance improvement. For the public dataset, MAIL improves the AUC on new users from 0.5934 to 0.5958 by replacing BaseDNN with DMR. For the industrial dataset, the AUC on new users is improved from 0.6849 to 0.6895. The model-agnostic property is very attractive for practical applications since there are various ranking models for dierent scenarios in the real world. Furthermore, although cold-start methods, i.e., LLAE, MetaEmb, and MAIL, focus on the new users, we nd that they show a slight AUC improvement on old users. For the public dataset, an AUC improvement of 0%∼0.06% on old users can be observed. For the industrial dataset, an AUC improvement of 0%∼0.1% on old users is obtained. This may be explained by noting that cold-start methods generate virtual behavior data or make a better initial embedding, which alleviates the sparse data problem for ranking models that enables performance improvement. The results of the GAUC metric are similar to those of AUC. Table 3: Results of Ablation Study on Public and Industrial Datasets There are three items in the training lossLof the zero-shot tower, including two cross-modal reconstruction items and an MMD item. Here, an ablation study is performed to explore the eectiveness of the three items. Five models are designed for comparison as follows: 1)BaseDNNBaseDNN is the ranking tower shown in Figure 2. 2)MAIL-NoneMAIL-None has no zero-shot training loss in the zero-shot tower, but only has a linear mapping from user attributes to user behaviors. The ranking tower of MAIL-None is BaseDNN. 3)MAIL-SingleThe zero-shot tower of MAIL-Single is trained by Eq. (3). The ranking tower of MAIL-Single is BaseDNN. 4)MAIL-DualThe zero-shot tower of MAIL-Dual is trained by Eqs. (3) and (4). The ranking tower of MAIL-Dual is BaseDNN. 5)MAIL-BaseThe zero-shot tower of MAIL-Base is trained by Eq. (7). The ranking tower of MAIL-Dual is BaseDNN. Table 2 gives the results of MAIL with dierent zero-shot towers on both public and industrial datasets. As the table indicates, MAIL-None and MAIL-Single show worse results than BaseDNN. For the public dataset, the AUC on new users of BaseDNN is 0.5862, while that of MAIL-None and MAIL-Single is 0.5834 and 0.5844, respectively. For the industrial dataset, the AUC on new users of BaseDNN is 0.6771, while that of MAIL-None and MAIL-Single is 0.6755 and 0.6764, respectively. The two models generate virtual data depending on user attributes and do not regularize the autoencoders by setting user behavior as input, which results in overtting and performance degradation. In comparison, the results of MAIL-Dual and MAIL-Base are much better, the stronger regularization by dual cross-modal reconstruction and maximum mean discrepancy reduces the modal-shift problem between user behavior and user attributes to achieve satisfactory virtual data for the ranking tower. Also, it is noticed that the unreliable data generated by MAILNone and MAIL-Single slightly aect the model performance on old users. For the public dataset, an AUC decrease of 0.06%∼0.12% is obtained. For the industrial dataset, an AUC decrease of 0.05%∼ 0.07% is obtained. In comparison, MAIL-Dual and MAIL-Base are not observed to aect the ranking performance on old users. Figure 4: The training loss of MAIL on the public and industrial datasets. In addition, we show the training loss of MAIL-Base in Figure 4, including the ranking tower lossL, the behavior-based crossmodal reconstruction lossL, the attribute-based cross-modal reconstruction lossL, and the MMD lossL. Generally, the training loss decreases rapidly in one hundred batches for both of the public and industrial datasets, which veries the learnability from user attributes to user behaviors. In this subsection, we perform a visualization study for various hidden features in the zero-shot tower using the t-SNE [36]. 4.4.1 Visualization for the Public Dataset. For the public dataset, we provide a visualization comparison between the generated features ^𝒗and the real features𝒗to validate the eectiveness of the zeroshot tower. Note that the new users in the public dataset are made by removing their behavior data from the dataset; hence, we can perform the comparison on both the old users (^𝒗and𝒗) and new users (^𝒗and𝒗). The visualization is given in Figure 5. As shown, most of the yellow points are overlapped with purple points and the generated distribution is covered by the real distribution, which demonstrates the eectiveness of the zero-shot learning loss for the modal-shift problem. 4.4.2 Visualization for the Industrial Dataset. For the industrial dataset, we provide a visualization comparison between the new user’s features and old user’s features to validate the assumption that users enjoy a common attribute space to allow the behavior transfer from old users to new users. Specically,𝒉is compared with𝒉, and^𝒗is compared with and^𝒗. The visualization is given in Figure 6. It is observed that the new users’ attribute distribution 𝒉is almost the same as the old user’s attribute distribution𝒉, which validates the assumption that all users enjoy a common attribute space. Moreover, based on the common feature space, the generated^𝒗and^𝒗are also similar to each other, which demonstrates the feasibility of attribute-based behavior transfer from old users to new users. We conducted online A/B testing to validate the model performance for real applications in our recommender system. The MAIL-DMR and DMR were separately deployed for comparison, where three million users were assigned to each model. In the half-month of A/B testing, the proposed MAIL-DMR improves CTR by 13%∼15% and CTCVR by 3%∼4% relatively compared with DMR, which is the last version of the recommender model in our system. Note that users would perform some conversion actions, e.g., purchase goods and collect videos, only when they are genuinely interested in the recommended items. Hence, we highlight the contribution of MAIL-DMR to the improvement of CTCVR in real applications, which is more dicult than the improvement of CTR. In this paper, the MAIL model is proposed to address the coldstart problem for recommender systems in a two-tower framework. Inspired by the thought of zero-shot learning, a new zero-shot tower is designed and cotrained with a model-agnostic ranking tower in MAIL. By performing the cross-modal reconstruction and the maximum mean discrepancy-based modal alignment, the zero-shot tower generalizes behavior data from old users to new users based on their attributes. Using the virtual behavior data, the ranking tower can then capture new user’s interests better for recommendation. An attractive property of the proposed method for practical application is that the ranking tower in MAIL can be implemented with any embedding-based ranking models, which allows MAIL to achieve an incremental performance improvement based on previous works. To verify the eectiveness of MAIL, ranking experiments, an ablation study, and a visualization study are performed on two large-scale real-world datasets. Furthermore, we have deployed the proposed MAIL on an online large-scale live recommendation system of a commercial application to show the signicant improvements of CTR and CTCVR.