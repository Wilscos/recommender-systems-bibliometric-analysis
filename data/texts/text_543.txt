Digital advertising is a critical part of many e-commerce platforms such as Taobao and Amazon. While in recent years a lot of attention has been drawn to the consumer side including canonical problems like ctr/cvr prediction, the advertiser side, which directly serves advertisers by providing them with marketing tools, is now playing a more and more important role. When speaking of sponsored search, bid keyword recommendation is the fundamental service. This paper addresses the problem of keyword matching, the primary step of keyword recommendation. Existing methods for keyword matching merely consider modeling relevance based on a single type of relation among ads and keywords, such as query clicks or text similarity, which neglects rich heterogeneous interactions hidden behind them. To ll this gap, the keyword matching problem faces several challenges including: 1) how to learn enriched and robust embeddings from complex interactions among various types of objects; 2) how to conduct high-quality matching for new ads that usually lack sucient data. To address these challenges, we develop a heterogeneous-graphneural-network-based model for keyword matching named HetMatch, which has been deployed both online and oine at the core sponsored search platform of Alibaba Group. To extract enriched and robust embeddings among rich relations, we design a hierarchical structure to fuse and enhance the relevant neighborhood patterns both on the micro and the macro level. Moreover, by proposing a multi-view framework, the model is able to involve more positive samples for cold-start ads. Experimental results on a large-scale industrial dataset as well as online AB tests exhibit the eectiveness of HetMatch. • Information systems → Retrieval models and ranking;Computational advertising;• Computing methodologies →Neural networks. online advertisement, keyword recommendation, graph neural networks ACM Reference Format: Zongtao Liu, Bin Ma, Quan Liu, Jian Xu, Bo Zheng. 2021. Heterogeneous Graph Neural Networks for Large-Scale Bid Keyword Matching. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM ’21), November 1–5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3481926 Figure 1: A real-world integrated keyword recommender system. Sponsored search is one of the main fashions of online advertising where advertisers acquire their desired ad impressions or clicks via bidding proper keywords. At the sponsored search platform of Alibaba, millions of advertisers in total manually add tens of millions of keywords every day, which reects advertisers’ strong bidding willingness to make their products obtain more potential consumers. Compared with such strong willingness, many advertisers lack expert knowledge to choose proper keywords thus fail to get expected ad impressions or clicks for their products. As illustrated in the previous empirical study [30], many advertisers tend to bid on a small number of popular keywords in the advertising platform, which makes those advertisers with low bid prices harder to obtain impressions. This challenge is also common at the sponsored search platforms in Alibaba, where only less than 10% of handcrafted selected keyword can obtain ad impressions on the next day. To improve marketing eectiveness and lighten the burden of advertisers, many sponsored search platforms are now oering dierent products that perform keyword recommendation, either in a white-box or a black-box fashion. White-box products oer many keyword candidates where advertisers can manually choose from, while black-box tools provide automatic keyword hosting services. Like many other industrial recommender systems (RSs), an advertising keyword recommender system can be achieved via two phases: 1) keyword matching/retrieval which collects a subset of tokens among sea of words for a given ad; 2) ecient ranking which puts an order on individual subset based on relevance and estimated eect (C.f. Figure 1). In this work, we address the problem of keyword matching, which is a fundamental step of keyword Figure 2: HIN schema for e-commerce bid keyword matching problem. An ad refers to an ad-group created in the advertising platform, a key word can either denote a bidding keyword or an ordinary query searched by users, and an item denotes an ordinary goods in the e-commerce platform. Besides, each ad is corresponding to a unique item. recommendation and also plays a determining role in the quality of the ranking stage. Regarding the keyword matching or retrieval problem, various approaches have been explored over years, including text similarity, collaborative ltering, and topic-based ones [2,13]. However, there are several limitations of existing methods: 1) these approaches neglect rich heterogeneous information hidden behind the simple <ad, keyword>pair (Figure 2 illustrates a toy example of HIN schema in which ads, keywords, and items are denoted by dierent types of nodes), which helps understanding the relevance between them ; 2) This also causes the cold-start problem to be worse for new ad units, especially those of new advertisers. To utilize the rich information hidden behind ads and keywords, we resort to heterogeneous graph neural networks (HGNNs), which have various successful applications in modeling complex interactions between dierent types of objects [4,6], and then integrate HGNNs in an embedding-based framework which is proven eective in multiple information retrieval tasks [19]. Embedding-based methods have been extensively studied in recommendation scenarios to improve the quality of matching. These methods aim to represent each node with dierent types of features in heterogeneous information network as a low-dimensional embedding vector, expecting that similar source objects (ads) and target objects (keywords) have similar embeddings. Among them, GNN-based approaches benet from their strong ability to fuse relevant information from neighbors at dierent distances in the network, thus achieving state-of-the-art performance in matching task [26]. More recent studies have extended GNN to heterogeneous information networks to capture the complex interactions among various types of nodes[19]. Most of these studies achieve this goals by aggregating neighbors’ information by sampling subgraphs via dierent metapaths [16,32], where a metapath denotes a sequence of meta relations (C.f. Section 3). For heterogeneous recommendation such as keyword recommendation, the selected metapaths vary a lot between dierent sides of a two-tower structure [16,32], where a tower refer to a subnetwork to compute a node’s nal embedding, resulting that the sampled attributed subgraph in each tower is very dierent. More specically, the types of nodes, edges and their corresponding features in the same position of each subgraph vary a lot. In this way, it might be hard to guarantee that the nal embeddings of two towers fall into adjacent feature space. Besides, it is also a challenging problem to lter irrelevant information when involving heterogeneous relations and attributes. This problem is especially severe in industrial web-scale scenes such as in Alibaba’s e-commerce sponsored search platform. How to learn comprehensive and robust embeddings in modeling complex interaction among nodes is an intractable problem. Another factor aecting the quality of keyword matching is the cold start of new ads. In Taobao, a leading e-commerce platform in China, millions of new ads are continuously created by advertisers every day. However, there are no user behaviors for these new ads throughout the platform. How to process these “cold start" ads is another challenging problem. Motivated by the concerns mentioned above, this paper proposes aHeterogeneous graph neural network for keywordMatching (HetMatch) for e-commerce sponsored search. To enhance the robustness of the extracted embeddings with various semenatics, on the macro level, we develop a Siamese network architecture by aggregating each tower’s nal embeddings and the averaged neighborhood embedding to conduct matching. In this way, the original heterogeneous matching problem between ads and keywords is transformed into a homogeneous matching between two <ad, keyword>pairs. On the micro level, we apply autoencoder in graph convolution as a fundamental component of our model to reduce the inuence of noisy signals and the computational cost. Moreover, to improve the capability to match cold-start ads, we leverage a multi-view framework to use multiple types of relations besides click data of ads as our objectives. Accordingly, our contributions are as follows: •We propose a novel HGNN-based keyword matching framework that leverages the complex relational data behind ads and keywords. To extract enriched and robust embeddings among dierent relations, on the micro level, we apply an autoencoder in graph convolution to mitigate the irrelevant patterns in neighborhood; while on the macro level, we develop a Siamese neighbor matching layer on the top of HGNNs to involve more relevant neighborhood information. •To introduce more supervised signals for alleviating the coldstart problem, we use a multi-view framework to learn the posterior probability of various objectives. •Experiment results exhibit our model’s advantages over other methods both on oine and online evaluation. •To the best of our knowledge, it is the rst work to apply a HGNN-based method to keyword recommendation, which consolidates the foundation of sponsored search. Keyword Recommendation and Suggestion.Early methods of bid keyword recommendation in sponsored search mainly focus on retrieving keywords from the perspective of relevance. These methods can be broadly categorized into the following types: collaborative methods [2], proximity-based methods [1,13], and topicbased methods [3]. Some collaborative methods like [2] aim to mine phrases that co-occur with the seed queries, which is applied in Google’s Adword Tools. Proximity-based approaches mainly focus on designing dierent distance metrics to evaluate the similarity between queries, which can be further divided into network-based methods [13] and kernel-based methods [1]. Topic-based methods use a hierarchical or unsupervised approach to cluster queries to topic [3]. These methods mainly address the relevance of retrieved corpus and ignore modeling the eect of selected keywords; thus may not perform well in practice. Another line of keyword recommendation focuses on retrieving the keywords from click-logs and estimate the user-click or impression brought by selected keywords [7, 15, 24]. For instance, Fuxman et al. [7]propose algorithm within Markov Random Field model to estimate user clicks. However, these methods are computationally expensive and lack the generalization ability for new ads; thus are usually applied to ranking retrieved keywords rather than performing matching tasks. Representation Learning on Heterogeneous Networks.Network representation learning (NRL) aims to automatically encode node information and network structure to xed-sized latent representation, which can be used in downstream graph mining tasks. Over the years, quite a few NRL methods have been proposed. These methods can be divided into random-walk-based methods [4], factorization-based methods [17] and GNN-based methods [18,25,29]. Earlier works mostly focused on studying the network representation learning method on homogeneous graphs. Recently, inspired by the past homogeneous NRL, studies have attempted to extend homogeneous NRL methods on heterogeneous networks. And the core problem is to fuse diverse node attributes and different types of relations into a latent vector; thus, the extracted embeddings can preserve both semantic and structural properties among nodes. Based on previous random-walk-based homogeneous NRL, Dong et al. [4]and Fu et al. [6]introduce metapath-based random walk strategies, in which the walker is conned to transit based on given metapaths. On the other line of works, applying GNNs on heterogeneous networks has also been proven a promising direction [18,25,29]. For instance, Wang et al. [25]maintains dierent weights for dierent metapath-dened edges when applying aggregation in the graph attention networks. However, current HGNN-based methods do not explicitly address the importance of ltering irrelevant patterns propagated through complex interactions, which is achieved by our extended version of graph convolution. Heterogeneous-Information-Network based Recommendation.More recently, leveraging heterogeneous information networks (HIN) becomes an emerging direction in recommender systems due to its capability of characterizing complex objects and rich relations [19]. For instance, Feng and Wang[5]proposed to alleviate the cold start issue with heterogeneous information network contained in the social tagged system. Metapath-based methods were introduced into hybrid recommender system in [28]. Recently, Hu et al. [9]leveraged metapath-based context in top-N recommendation. However, none of them addresses the problem of the heterogeneity of metapaths between two towers, which might lead to diculties for learning a robust embedding for matching with very dierent computational graphs. Recently, several researches have explored how to utilize the rich relations and complex objects of heterogeneous information network (HIN) in recommendation systems, which is an auspicious direction. Keyword recommendation also deals with various types of objects and rich relations (C.f. Figure 2), thus it would be promising to leverage the HIN-based recommendation paradigm. Here we rst give the necessary concepts about HIN. Denition 3.1.Heterogeneous Information Network[20]. A heterogeneous information network (HIN) can be denoted asG = (V, E, 𝜙,𝜓 ), consisting of an object setVand a relation setE. Each node𝑣owns a type𝜙 (𝑣), and each edge𝑒has a type𝜓 (𝑒).TandR denote the sets of predened object types and relation types, where |T | + |R| >2.Vcan be written asV = V∪ ... ∪ V∪ ... ∪ V, where𝑡∈ TandVrepresents the node set with type𝑡. Besides, each node𝑣∈ Vis augmented with node attributex∈ R, where𝑑is the attribute dimension of the nodes typed𝑡. Similarly, Ecan be written asE∪ ... ∪ E∪ ... ∪ E, where𝑟∈ Rand Erepresents the edge set with type 𝑟. More specically, in the scenario of keyword recommendation, we denote the whole node setV = V∪ V∪ V, whereV, VandVrepresents the ad set, keyword set, and item set, respectively. More specically, an ad refers to an ad-group created in the advertising platform, a keyword can either denote a bidding keyword or an ordinary query searched by users, and an item denotes an ordinary goods in the e-commerce platform. Among these nodes, there exists various types of relations which capture dierent semantic connections, such as the bid or click relations between ads and keywords. In order to capture the semantic and structural relation between dierent objects, the metapath is proposed by [21] as a relation sequence connecting two objects, and is widely applied in HIN model research. Denition 3.2.Metapath and Metapath-guided Neighbors [21]. For a relation𝑒 = (𝑠, 𝑡)linking from node𝑠to𝑡, its meta relation is dened as< 𝜙 (𝑠),𝜓 (𝑒), 𝜙 (𝑡) >. A meta-path𝑝is then dened as a sequence of meta relations𝑡−−→ 𝑡−−→ · · ·−→ 𝑡, which describes a composite relation𝑅 = 𝑟◦ 𝑟· · · ◦ · · · 𝑟between nodes𝑎and𝑎, where◦denotes the composition operator on relations. Furthermore, the metapath-guided neighborhood is dened as the set of all visited nodes when a given node𝑣walks along the given metapath 𝑝. Finally we present the problem denition of HIN-based keyword matching. Denition 3.3.HIN-based Ad-Keyword Matching Problem. Given a HING = (V, E), letA = {𝑎, 𝑎, ..., 𝑎} ⊂ Vbe a set of selected ads to match keywords, andE, E⊂ A × Vbe the candidate relation set and target relation set respective, where ‘×’ represents the Cartesian product operator between two set. More specically, each ad𝑎corresponds to a target keyword set Q⊂ Vand a candidate keyword set Q⊂ V. The ad-keyword matching problem can be formulated as a recall optimization task [11]. Given a set of adsA = {𝑎}with theirÐ corresponding candidate relation setsE=(𝑎× Q)and theÐ target relation setsE=(𝑎× Q), the problem is to pick aÐ retrieved relation setE=(𝑎× O), whereOrepresents the retrieved keywords for ad𝑎sized no more than𝐾, the object is to maximize the total recall ratio: Usually, the candidate set of a matching problem is set as a group of target nodes which is related to the source nodes under a certain criterion. In our task, we dene the candidate set𝑄of an ad𝑎 as the keywords that share the same category with𝑎, which is adjusted by a trained category classier. The target relation set Econtains the node pair of<ad, keyword>which would bring user clicks in the future. In other words, we aim to optimize our matching model in that the matched keywords can cover as many eective bidding keywords as possible. In recent years, attention in the eld of RS is increasingly shifting towards HIN-based recommendation. We extend this paradigm of RS for bid keyword matching problem and design a hierarchical network architecture to model the enriched interactions behind ads and keywords in a robust way. On the micro level, we leverage metapath-based graph convolution to aggregate neighborhood context from dierent perspectives and utilize an autoencoder module to lter irrelevant neighborhood patterns. On the macro level, we design a Siamese network structure to enhance the complementary patterns in the neighborhood. To further alleviate the cold start problem of newly created ads, we extend our framework as a multi-view matching problem among dierent relation targets between ads and keywords. In this way, more supervising information are introduced and the associated tasks can improve each other’s performance by sharing information. In this section, we describe the details of our proposed model,Heterogeneous Graph Neural Network for keywordMatching (HetMatch). Figure 3 shows the architecture of our proposed model. Before integrating data from metapath neighbors for each node, we rst extract heterogeneous feature vectorx∈ RfromXfor each node𝑣∈ Vand transform it as a xed-length embedding. We list the details of features in Table 1. These features can be categorized into two types, i.e., id features and numeric features. We transform each numeric feature as a discrete value, which represents the quantile of its feature distribution. After discretization, we can acquire the embedding of each feature via its corresponding lookup table. To lighten the computation burden, the same features from dierent node types will share one look-up table. For instance, the feature terms of title appears both in ad’s and item’s feature list, and thus the embedding of this feature shall be acquired from the same look-up table. We then concatenate these embeddings and feed the concatenation into a type-specic neural network𝑓to get the node-level embedding h. Metapath.After fusing the node-level information, the next step is to involve the rich context information around each node. Under the paradigm of metapath-based heterogeneous GNNs, multiple metapaths are sampled which play dierent roles in capturing the structural and semantic context. We thus introduce the metapaths used in our model, which can be categorized into the following two groups: (1) Bid-based group: The subgraphs based on bid relations around a given ad or keyword can directly characterize the bidding environment around it, which involves the competitive ads that bid on the shared keywords and a surge of keywords that the competitive ads are interested in. We use the following four metapaths constructed by user clicks and advertiser bids to capture such environment: • 𝑞−−−−→ 𝑎−−−−→q, 𝑞−−−−→ 𝑎−−−→q • 𝑎−−−−→ 𝑞−−−−→a, 𝑎−−−→ 𝑞−−−−→a where the user-click relations can stand for the bids that can directly bring clicks, and the ordinary bid relations can make appropriate supplements to the cold-start ads. (2)Item-based group: Sometimes a user might pay attention to an ad and an ordinary item in the same page view. These items can build bridges between ads and related keywords that are not directed connected and provide extra useful semantics that helps capture richer context information. Besides, those co-clicked items can also oer similar textual and behavioral patterns that can characterize how the connected nodes are like. Graph Convolution Layer with AutoEncoder.After introducing the metapaths used in our model, we then present how we aggregate the node-level embeddings robustly and eciently given a certain metapath. The core idea of graph neural networks is to iteratively aggregate feature information in the neighborhood by a local lter. However, the neighborhood information aggregated by dierent heterogeneous relations might contain irrelevant information. Without loss of generality, we apply the autoencoder, which is successfully used in previous tasks for learning the compressed representation for denoising [23], to lter irrelevant neighborhood patterns and meanwhile improve the eciency of aggregating. The Node Type Features adad id, terms of title/description, category path, brand, propitemitem id, terms of title/description, category path, brand, keywordkeyword id, terms of keyword, predicted category, aver- Figure 3: Architecture of our proposed model. The left part is the two-tower structure of our proposed method in which we use a Siamese neighborhood matching layer to involve more homogeneous node patterns and utilize a view-transformation module to involve more supervised signals. The right part is the metapath-based heterogeneous GNN in which we use an autoencoder to reduce the inuence of noisy signals and the computational cost. process of graph convolution can be formulated as below: ℎ= AGGREGATE(ℎ, ∀𝑢 ∈ N whereℎdenotes the hidden state of node𝑣after the𝑘-th convolution layer, andAGGREGATEis a pooling operator (mean, sum, and etc.) andℎis the neighborhood vector of𝑣that incorporates the surrounding information of𝑣given a meta relation𝑟. Without loss of generality, we instantiate the pooling function as a sum operator. In(3),𝑓 (·)and𝑔(·)are two distinctive data-driven functions that mapsℎandℎto a new feature space respectively. HereN(𝑣)contains the neighbors of𝑣with the top-m largest edge weights. The motivation we select two distinctive functions is due to the heterogeneity of these two hidden representations which correspond to dierent node types. For the hidden features of the node itself in(4), we directly use a linear projection with a weight matrix W∈ R. For the neighborhood vector in(5), we rst compress it to a low-dimension vector sized𝑙by a linear transformation with V∈ R, where𝑙 < 𝑑, followed by a activation function𝜎. For term convenience, we name𝑙as latent feature size. We then apply another linear combination withU∈ Rto map the intermediate hidden features to the original dimension sized𝑑. We select such autoencoder-based method for the neighborhood vector because it can be viewed as a noise lter from the neighborhood context which usually contains some irrelevant information. Besides, it can also reduce the computational complexity compared with the direct assignment of a weight matrix sized𝑑 × 𝑑on the hidden features fromO(𝑑)toO(𝑑 ·𝑙). Although the graph attention network (GAT) [22,27] also addresses the problem to extract the most inuential information from neighbors by the attention mechanism, it has been experimentally veried that GAT performed the same as or worse than GCN in noisy graphs [33]. This is because GAT introduces too many parameters when learning attention coecients, which leads to overtting to noise. Besides, compared with our design, GAT is more computation expensive. After feeding the node information to𝐾such layers, which are dened by metapath𝑝, we can eventually obtain the node semantic embeddingℎ. To reduce the computational cost, the network parameters of the graph convolution are shared across dierent nodes at the same layer. Semantic Attention Layer.We next introduce the semantic attention layer, which aims to fuse multiple embeddings extracted based on various metapaths. The general idea is as follows: dierent metapaths reveal dierent aspects of node context; thus, there is a necessity to aggregate the semantics revealed from dierent metapaths. To address this issue, we follow the idea proposed in [?], which uses a self-attention mechanism to capture the diverse semantics revealed from dierent metapaths. The mathematical expressions of the semantic fusion are as follows: where𝑤is the learned importance weight of metapath𝑝, which is computed by a scaled self-attention mechanism from(6), and 𝑊∈ Ris the weight matrix for mapping the original hidden representationℎto a scalar which will be scaled to the importance weight of the corresponding metapath 𝑝. In the past two sections, we have presented the methodology to fuse node-level and subgraph-level information. In an ordinary matching model, the next phase is usually to feed the source and target embeddings into a contrastive loss function, like in [12]. Before calculating the loss function, in this section, we design a Siamese neighbor matching layer to transform the original matching problem between heterogeneous nodes to a matching problem between two pairs of<ad, keyword>, which is illustrated in the left part of Figure 3. Each pair contains the source or target node and its most inuential neighbors. More specically, the<ad, keyword> pair in the tower of ads contains the embedding of source ad𝑣 and the averaged embedding of𝑣’s𝜅most inuential neighbors with type of keyword, while the<ad, keyword>pair in the tower of keywords contains the embedding of target keyword𝑣and the averaged embedding of its𝜅most inuential neighbors with type of ad. We nally combine the averaged semantic embedding of a node’s most inuential neighbors with the node itself. The motivation here is that although in a heterogeneous matching problem, the embeddings of both the source node (ad) and the target node (keyword) are expected to be close in feature space by optimization, there is no explicit guarantee for such expectations due to their dierences in the network structures, parameters, and metapaths to compute the nal embeddings. Besides, by introducing so much side-information brought by the heterogeneous graph neural networks, it would become more challenging to satisfy such assumption. In contrast, compared with only utilizing the source semantic embedding itself, introducing the averaged semantic embedding of its inuential neighbors (named neighborhood embedding) would help match with target embedding. This is because they share the same raw feature types, network structures, metapaths, and parameters. Furthermore, the inuential neighbors can also directly characterize what the source nodes are like in an ecommerce scenario. The mathematical expressions are formulated as below: whereN(𝑣)denotes𝜅most inuential keywords bid by node 𝑣; similarly,N(𝑣)denotes𝜅most inuential ads bidding on keyword𝑣. In this way, the whole structures to compute𝑧and 𝑧are symmetric, which is also the reason we name this network Siamese neighbor matching layer. It’s important to note that Siamese neighborhood matching mechanism diers from the dual matching scheme used in [14,16,31]. These studies design symmetric network structures for heterogeneous objects in two-tower; however, their neighborhood embedding is computed based on dierent metapaths and network parameters thus tend to be hard for heterogeneous matching. For improving the eectiveness of matching with cold-start ads, we introduce dierent types of relations between ads and keywords as our objectives. More specically, we select click relations between ads and keywords, bid relations between ads and keywords, and click relations between keywords and items (each ad corresponds to an item (Figure 2)) as our objectives. As dierent type of objectives might have dierent feature space to match, we use a view transformation function𝑓(·)to map the original embeddings𝑧to a view-specic embedding𝑧, where𝑓(·)is a view-specic multi-layer perceptron and 𝑟 represent the view type. We train our HetMatch in a supervised manner using a contrastive loss used in [12]. The basic idea is that we aim to maximize the inner product of positive pairs, i.e., the transformed embedding of the source ad and the corresponding related keyword. Meanwhile, we also want the inner product of negative examples, the source ad and its unrelated keyword, to be smaller than that of the positive sample. To achieve this, we compute the posterior probability of a keyword given an ad from the semantic relevance score between them through a softmax function: where𝛾is a smoothing factor in the softmax function.Qdenotes a subset of candidate keywords to match. For each positive pair, denoted by (𝑣,𝑣) where𝑣is the relevant keyword,Qincludes 𝑣and ve randomly selected keywords in the candidate setQ. The candidate setQcontains the keywords that are discriminated by a category classier as the same category as ad 𝑣. In the training phase, we optimize the whole networks’ parameter set by maximizing the likelihood of the posterior probability of all keywords across the training set. Equivalently, we acquire to optimize the following loss function: whereΘrepresents the parameter set of the whole networks which is updated iteratively by an Adam optimizer. Pipeline of Keyword Recommendation.Keyword recommendation is a multi-stage system, consisting of matching, ltering and ranking phases. In the keyword matching phase, we rst handle network construction from user behavioral records and advertiser bidding records oine by MapReduce downstream in the Alibaba Cloud platform. During the training and inference, we leverage the distributed deep learning framework XDLand graph search engine, Euler, to conduct run-time subgraph extraction and parameter update in a distributed fashion. After inference, ad embeddings and keyword embeddings are stored in a database for online services, and we use an approximate nearest neighbor (ANN) search engine to retrieve the most relevant keywords for each ad. In the ltering phase, we use a term-match based relevance model to lter the unrelated keywords for each ad. Lastly, in the ranking phase, we use an MLP-based model with enriched features to estimate the potential number of clicks brought by each remained keyword and rank those keywords based on the estimated values. In this way, we can recommend advertisers with the keywords that can bring as many user clicks as possible. Acceleration.HetMatch can be implemented by sampling subgraphs separately for each node and computing each node’s corresponding nal embedding via neural networks, which is computation consuming. As the extracted subgraphs based on the top-k sampling strategy of a particular node and its inuential neighbors usually share plenty of relation-paths, we lighten the computation burden by only calculating the intermediate embedding once for each distinctive relation-path. Negative Sampling.As illustrated in Section 4.5, for each positive pairs, we need to sample multiple negative keyword nodes for training. The whole procedure for negative sampling can be divided into two phases: 1) we calculate the weights (square root of the searched count) of keywords in each leaf category oine. 2) At run time of training, for each positive<ad, keyword>, we pick ve negative keywords in the same leaf category by weighted random selection with the help of the graph index engine Euler. Dataset Description.We evaluate our proposed method based on a real-world dataset collected from the search logs in Taobao platform and the advertiser behavioral records in Alibaba’s e-commerce sponsored search platform. The dataset covers seven consecutive days of these records, spanning from August 19th, 2020 to August 26th, 2020. We construct our heterogeneous network from these logs based on the network schema illustrated in Figure 2, where the edge weight (the importance of an edge) is dened as the appearing times of a particular relation in the records. For the training set, we sample 10M ad-keyword pairs in each view. The target relation set contains 50M click relations between ads and keywords in the constructed network. To prevent information leakage, we drop the relations in the target relation set from the original network. The more specic data statistical information is exhibited in Table 2. Baselines.To demonstrate our proposed method’s eectiveness, we compare our method with multiple baseline methods and their variants. To ensure the fairness of the comparison, the results we reported are all under the multi-view learning framework mentioned in Section 4.4 unless otherwise stated. • Term-Match: This is a keyword retrieval method that extracts related keywords by calculating the term similarity between ads’ titles and keywords, which is one of the keyword matching models currently deployed in Alibaba’s ecommerce sponsored search platform. • DSSM: This is a two-tower matching model that projects the features of objects to a low dimensional space via multi-layer perceptrons (MLPs) [12]. • HAN: We replace the MLPs in DSSM by heterogeneous attention networks (HANs) proposed in [25]. The metapaths to aggregate neighbors are mentioned in Section 4.2. • IntentGC: This is a dual-HGCN-based recommendation model that captures heterogeneous relations between nodes with the same node types [31]. • HetMatch: This is our proposed model. We set the embedding length𝑑as 64 and the latent size𝑙as 16. The model is optimized using an Adam optimizer with a learning rate of 0.03, and the batch size is 512. • DSSM(s), HAN(s), IntentGC(s): Based on DSSM, HAN or IntentGC, we add the Siamese matching layer between the output of multi-layer perceptrons and the multi-view transformation layer’s input. • HetMatch: This model drops the Siamese neighbor matching layer in HetMatch. The remaining settings for this variant are the same as for our proposed methods. • HetMatch: This model only utilizes click data between ads and keywords as the labeled positive samples. • HetMatch: This variant replaces the graph convolution layer with autoencoder of HetMatch by GraphSage [8]. • HetMatch, HetMatch: Each of these variants only use the bid/item relations to aggregate neighbors’ information. HetMatch18.82% 29.60% 48.34% 63.31% HetMatch19.32% 30.28% 49.77% 65.63% HetMatch17.99% 23.91% 26.66% 47.43% HetMatch15.97% 26.74% 46.33% 64.81% HetMatch15.78% 26.62% 47.48% 66.11% As our task focuses on retrieving hundreds of keyword candidates, we use the recall rate as our evaluation metrics as illustrated in Section 3, which is also adopted in [11]. As our model is under a multi-view framework (the number of view is 3), we use Recall@3K instead of Recall@K as our metrics. For the approaches under the multi-view framework, we retrieve each ad’s the top-K most related keywords in each view, and aggregate these relations as the nal retrieved setOto compute Recall@3K. To ensure fair comparison, for those methods that are not under a multi-view framework (TermMatch and HetMatch), we directly retrieve the top-3K most related keywords for each ad to compute Recall@3K. Table 3 reports the performance of our proposed method compared to other competing methods or some variants from them. It can be seen from the table that our model can consistently outperform all comparative approaches or variants by achieving the highest recall rate in most cases. Firstly, we compare the relevancebased model deployed in our platform with other embedding-based retrieval methods. We observe that our relevance-based model (Term-Match) cannot achieve a good performance, especially when K is large. This nding demonstrates that the retrieval model that only relies on text relevance might not have good practical performance. Then we compare HetMatch with DSSM that does not utilize neighbor information. We note that our method achieves much better performance, as HetMatch can capture auxiliary information from heterogeneous relationships. Besides, we compare HetMatch with GNN-based methods (HAN, IntentGC); here, HAN and IntentGC also performs the aggregation operations on neighbors, but it does not explicitly consider the misleading inuence brought by noisy links and features, and the attention mechanism used in HAN’s aggregator might lead to overtting. To further demonstrate how our Siamese neighbor matching layer helps matching task, we conduct several ablation studies on DSSM(s), HAN(s), IntentGC(s) and HetMatch𝑠. The comparison between DSSM with DSSM(s), HAN with HAN(s), IntentGC with IntentGC(s) and HetMatch with HetMatchpresents the eectiveness of introducing Siamese neighbor matching in a DNNbased matching task. Interestingly, we observed that adding our Siamese neighbor matching layer to a simple DNN-based matching approach (DSSM) can signicantly improve performance, achieving very close performance to other GNN-based methods. This again shows the importance of conducting Siamese neighbor matching. We can also note that HetMatch surpasses the performance of HetMatch. One reason for this is that the autoencoder based graph convolution layer prevents misleading information from propagating from neighbors and thus enhances the quality of neighbor embeddings. Besides, by eliminating the multi-view framework, we observe a drastic drop in performance, which demonstrates the need to involve dierent types of objectives in learning. We also explore how our proposed model performs with dierent groups of metapaths. Compared with sampling neighbors based on both bid-based and item-based metapaths, sampling neighbors with only one group of metapath harms the performance. This is because the bid-based and item-based relations can provide useful information from dierent aspects, which can complement each other. It would also be interesting to note that the performance of HetMatch is better than the performance of HetMatchwhen K is no more than 200, and vice versa when K is more than 200. This is because the item-based metapaths brings more diverse neighbors compared with those sampled based on the bid-based metapaths (since this group is not directly related to the object task), which might hamper the quality of matched elements whose matching score is relatively high, but improve the total recall rate when K is relatively large. This also shows the importance of selecting proper types of metapaths in the task of keyword matching. This section aims to compare the performance of dierent approaches for each view objectives (Table 4). Our model achieves state-of-the-art performance in each view, and its improvement on the performance over other baseline methods is signicant. This nding demonstrates our model can achieve a good performance not only in total but also in each channel. Another interesting nding is that we note that the performance in the channel of bidding relations is worse than that in the channels of item-clicking and ad-clicking in most cases. This is because the labels of bidding relations are extracted by advertisers’ manual selection of keywords, which might introduce some non-professional bidding behaviors and achieve performance. Besides, we also note that HetMatch achieves better performance over HetMatchin the view of adclick, which demonstrates that learning with auxiliary labels in other views can improve the performance in the each view. Conducting cold-start recommendation is a critical problem for a recommender system, which is not an exception for keyword recommendation. In this section, we present the experimental results for the cold-start ads. More specically, Table 5 reports the performance of dierent methods on the ads which are newly created on August 26, 2020. Unlike the target set we used in Section 5.2, we select the candidate set of a new ad as the keywords that bring user clicks to the ad in the next 14 days. The performance of these methods is consistent with that in Section 5.2. HetMatch performs the best across all the competitive approaches, which demonstrates the HetMatch works well on the whole dataset and has an excellent capability to retrieve useful keywords for new ads. More specically, we nd that learning with various groups of metapaths and using autoencoder module in graph convolution and Siamese neighbor matching layer can improve the performance on cold-start ads. Table 5: Performance comparison of cold-start ads. HetMatch17.05% 28.13% 48.93% 67.01% HetMatch19.32% 30.28% 49.77% 65.63% HetMatch12.95% 22.91% 31.78% 41.50% HetMatch16.35% 27.58% 49.35% 68.77% HetMatch16.22% 27.33% 48.99% 68.31% To evaluate the eectiveness of our proposed method on the realworld products, we deployed our HetMatch both on the keyword suggestion tool and the automatic keyword hosting tool of Alibaba’s e-commerce sponsored search platform. More details about these tools can be referred to in the part of Appendix. Table 6 presents the results of online AB tests on the keyword suggestion tool and the automatic keyword hosting tool in Alibaba’s e-commerce sponsored search platform. For the keyword suggestion tool, our proposed method achieves an improvement of +4.19% in terms of the adopting rate and +5.35% in terms of the number of clicks over the online deployed term-match based model. For the automatic keyword hosting tool, our proposed method obtains an improvement of +10.89% in terms of the number of clicks over the online deployed GraphSage-based matching model. Table 6: Online performance of compared methods in dierent scenes. Keyword suggestiona term-match-basedadopting rate + 4.19%, toolmodel#click +5.35% Keyword hosting toola two-layer Graph-#click +10.89% In this paper, we proposed HetMatch for the keyword matching problem based on a metapath-based heterogeneous GNN, which consists of a hierarchical structure to capture complex structures and rich semantics behind heterogeneous information networks in a robust way. The proposed model leverages node-level fusion, subgraph-level fusion, and Siamese neighbor matching to adaptively aggregate relevant neighborhood patterns. By introducing the autoencoder-based graph convolution layer and the Siamese matching layer, HetMatch can mitigate the negative eect from noisy patterns and enhance relevant information. In addition, we use a multi-view framework to learn the posterior probability of various objectives, which helps introduce more supervised signals. Experimental results show that our model consistently outperforms competitive approaches. Extra online AB tests also demonstrate the superiority over existing methods deployed online. For future work, we’d like to continue exploring into other sophisticated relations hidden in the heterogeneous graph, such as text similarity, user proles, etc. Furthermore, HetMatch still relies on human-crafted metapaths while recently there are a few transformer-based models that could potentially learn all aggregation strategies by themselves [10], which is an auspicious direction. Last, considering Bert-like pre-trained models have pushed state of the art in many areas of NLP, combining Bert with GNN to enhance the quality of keyword retrieval may also be promising. That being said, both transformer-based HGNN and Bert-like models are usually of very large model size, which would be a great challenge for training and deployment in industrial systems. Further eorts like model compression might also be required. For the implementations of all comparable methods, we set the same hyperparameters. Keyword Suggestion Tool.This is a suggestion tool for advertisers when they manually add keywords. More than 30% of new keywords with impressions are brought by the suggestion tools each day. For each ad, the suggestion tool will present hundreds of keyword candidates; the advertisers can adopt the candidates from the suggestion list. The control group contains a relevance-based keyword matching model. In the treatment group, we combine keywords retrieved by HetMatch and the relevance-based model. The adopting rate of the suggested keywords and the total clicks brought by the suggested keywords are the main metrics that we are concerned about. Keyword Hosting Tool.In Alibaba’s e-commerce sponsored search platform, advertisers are provided with many black-box marketing tools such as the automatic keyword hosting tool. By merely setting budgets and bid prices, advertisers can handle online marketing for sponsored search without explicitly choosing any keywords. The automatic hosting procedure hidden behind the product can be divided into two parts. The rst part is the addition of keywords based on the retrieved keywords with estimated ad clicks, while another part is to delete existing keywords that cannot bring impressions or have very low ctr/cvr. In the experiment, the control group contains a two-layer GraphSage based matching model, which is proven eective in the previous AB test. In the treatment group, we use the keywords retrieved by HetMatch. We use the averaged ad clicks brought by daily keyword addition to evaluate dierent methods’ performance.