Abstract—Computing latent representations for graphstructured data is an ubiquitous learning task in many industrial and academic applications ranging from molecule synthetization to social network analysis and recommender systems. Knowledge graphs are among the most popular and widely used data representations related to the Semantic Web. Next to structuring factual knowledge in a machine-readable format, knowledge graphs serve as the backbone of many artiﬁcial intelligence applications and allow the ingestion of context information into various learning algorithms. Graph neural networks attempt to encode graph structures in low-dimensional vector spaces via a message passing heuristic between neighboring nodes. Over the recent years, a multitude of different graph neural network architectures demonstrated ground-breaking performances in many learning tasks. In this work, we propose a strategy to map deep graph learning architectures for knowledge graph reasoning to neuromorphic architectures. Based on the insight that randomly initialized and untrained (i.e., frozen) graph neural networks are able to preserve local graph structures, we compose a frozen neural network with shallow knowledge graph embedding models. We experimentally show that already on conventional computing hardware, this leads to a signiﬁcant speedup and memory reduction while maintaining a competitive performance level. Moreover, we extend the frozen architecture to spiking neural networks, introducing a novel, event-based and highly sparse knowledge graph embedding algorithm that is suitable for implementation in neuromorphic hardware. Index Terms—knowledge graph, graph embedding, graph convolution, efﬁcient machine learning, spiking neural network A quintessential aspect of neural networks is the adjustment of their synaptic weights during training to optimize a given cost function [1], [2]. Since the cost function is, in principle, arbitrary, neural networks have emerged as ﬂexible models for a multitude of applications [3]–[5]. However, even with static weights, neural circuits have been shown to possess intriguing properties, e.g., for information processing [6]–[8], to support learning [9]–[11], for transfer learning [12] or to enable efﬁcient hardware realizations of neural networks with reduced silicon area and power consumption [13]. We propose a frozen architecture inspired by relational graph convolutional networks (R-GCN) [14] that is suitable for efﬁcient hardware realizations like neuromorphic systems [15] while offering advanced functionality for reasoning on symbolic data like knowledge graphs (KGs) – despite having random and static weights. KGs structure information in a uniﬁed, machine-readable format to represent relational knowledge. Thereby, nodes correspond to entities of the real-world and typed edges between pairs of nodes indicate their relationships and encode factual statements (Fig. 1A). While some modern KGs are massive in size, most KGs exhibit incompleteness meaning that not all true facts are contained in the knowledge base. Thus, a popular learning task on KGs is concerned with deriving new facts based on observed connectivity patterns (knowledge base completion, KBC). While classical KG reasoning methods employ logical reasoning techniques, scalability issues and breakthroughs of data-driven machine learning methods on other data modalities gave rise to KG reasoning that follow the representation learning paradigm. The basic idea is to embed both entities and relations into low-dimensional vector spaces and model the truthness of facts via functionals on the embedding spaces (see [16]–[19]). From an encoder-decoder perspective, earliest KG reasoning models employed shallow embedding lookup as the encoder. More recently, KG reasoning methods that use graph neural networks (GNNs) as an encoder achieved state-of-theart performance [14], [20]. The underlying rationale consists in producing more expressive entity embeddings via pooling information from neighboring entities (Fig. 1B). In most state-of-the-art models, for this pooling operation, to acquire a new embedding for a node, the embeddings of its neighboring nodes are aggregated, linearly transformed using a convolutional ﬁlter mask and subsequently averaged (Fig. 2A). Compared to simple lookup encoders, this aggregation-based encoder allows embedding previously unseen nodes [21] as well as masking of a node’s neighborhood to produce subgraphs that act as explanations for the output of the decoder [22], [23]. However, the pooling operator used in GNNs introduces weight sharing during training, i.e., updates to the weights are non-local, which is in stark contrast to the distributed and local design philosophy of neuromorphic hardware. Additionally, in multi-relational settings, GNNs struggle with overﬁtting due to the large amount of hyperparameters introduced by the convolutional weights [14]. To solve these challenges, we introduce a frozen R-GCN architecture where the convolutional weights are randomly initialized and kept constant (i.e., frozen) at all times. During training, we only tune the parameters of a task-speciﬁc decoder and – by letting the gradients ﬂow through the static RGCN – the initial node embeddings (Fig. 2B). This way, the model is optimized towards the static ﬁlter masks and learns to utilize the aggregation structure to produce richer node embeddings. Using such a frozen architecture allows us to harness the functional beneﬁts of the aggregation operation while eliminating weight sharing and reducing the amount of trainable parameters in the model. Bringing such models to neuromorphic hardware, which promise energy efﬁciency and low times to solution [24]–[30], has the potential of opening a plethora of novel applications and use-cases for these devices – especially since often recorded data, e.g. in industrial projects [31]–[35], has no natural representation as spikes, but can be modelled as heterogeneous graphs. We are conﬁdent that the frozen R-GCN can be mapped to neuromorphic devices as a static structure that allows sensible, local accumulation of information from graph data. Furthermore, to move closer to the architecture of actual neuromorphic hardware, we combine the frozen R-GCN with a recently proposed spike-based algorithm for shallow graph embeddings [36], extending it to inductive settings like the handling of dynamic graphs. An intriguing hallmark of this spike-based relational graph convolutional network (SR-GCN) is the simultaneous and highly sparse calculation of initial and ﬁnal embeddings as time unfolds – different from non-spiking R-GCNs where embeddings have to be calculated layer by layer. Moreover, already on conventional computing hardware, for instance CPUs, the suggested frozen architecture shows several beneﬁts like reduced memory footprints and considerable speedups of training without signiﬁcant performance losses. The contributions of this work are summarized as follows: for efﬁcient calculation of expressive graph embeddings. reduces memory and compute time without signiﬁcant performance losses. with arbitrary shallow decoder models, which we demonstrate by constructing the ﬁrst spike-based R-GCN. and calculating graph embeddings in a purely temporal and event-based way. In the remaining sections, we ﬁrst introduce the mathematical notation used throughout the paper as well as the background required to follow this work in Section II. Subsequently, in Section III, we propose the frozen R-GCN and evaluate the KBC performance on the benchmark data sets FB15k-237, UMLS, and Countries S1. Section IV outlines ﬁrst proof of concepts of spike-based R-GCNs for KG reasoning before we summarize our results and conclude in Sections V and VI. Before proceeding, we ﬁrst deﬁne the mathematical notation that we use throughout this work and provide the necessary background on KGs. Scalars are indicated by lower case letters (x ∈ R), column vectors by bold lower case letters (xxx ∈ R), and matrices by upper case letters (X ∈ R). Moreover, three-way tensors are denoted by calligraphic letters (X ∈ R). Sets are either indicated by their canonical symbols (e.g., N denotes the set of natural numbers) or by calligraphic letters. ˙x is the time derivativeof x. KGs are collections of factual statements that specify the relations between entities of the real world. We denote with E the set of relevant entities. R denotes the set of binary relations. In this work, a KG is deﬁned as a collection of triples KG ⊂ E × R × E. Each triple in KG corresponds to a factual statement (s, p, o) – where s indicates the subject, p the predicate, and o the object. While every triple in KG is interpreted as a true fact, there exist different interpretations of absent triples. Since large scale KGs are typically incomplete, it is common to make the open world assumption (OWA). The OWA states that one cannot conclude that absent triples are false – their truth value is rather unknown. In this setting, KBC is a typical learning task related to KGs. Data-driven KBC techniques are studied under the umbrella term statistical relational learning (SRL) [37]. Among these methods KG embeddings have become the dominant approach. Thereby, both entities and relations are projected into lowdimensional vector spaces encoding connectivity patterns between entities. In these embedding spaces, the interactions between the embeddings of entities and relations can be efﬁciently modelled via functionals to produce scores that indicate the likelihood of triples. KG embedding methods can be categorized according to fundamental interaction mechanisms of the functionals. For example, translational models such as TransE [16] embed both entities and relations into the same vector space and model the action of different relations as vector space translations. Concretely, TransE imposes where the bold letters correspond to d−dimensional vector space embeddings of the corresponding entities and relations. During training, these embeddings are tuned such that the discrepancy between eee+rrrand eee(measured by some metric on R) serve as a proxy for the plausibility of triples. Multiplicative models implicitly correspond to tensor decomposition models where the score of each fact is given by a bilinear form. Note that KGs have a natural representation in terms of adjacency tensors X ∈ {0, 1}. Similar to an adjacency matrix of a homogeneous graph, an entry of X indicates the absence (0) or presence (1) of a triple. Various multiplicative KG reasoning models correspond to different formulations of the bilinear forms. The tensor factorization model RESCAL [17] considers bilinear forms induced via relation-speciﬁc, quadratic matrices. However, this leads to one of the main disadvantages of RESCAL: the number of parameters grows quadratically in the embedding space dimension. As a remedy to this problem, the quadratic matrices of RESCAL are constrained to be diagonal, which is generally known as DistMult [18]. Concretely, DistMult scores triples via where eee, rrr, eee∈ Rand diag(rrr) is a diagonal matrix with diagonal entries given by rrr. GNNs are neural networks that aim to produce expressive representations of the nodes in homogeneous graphs via a message passing heuristic between neighboring nodes (see [38]). One of the most inﬂuential methods that led to a widespread popularization of GNNs is the graph convolutional network (GCN) introduced by [39]. Similar to convolutional neural networks (CNNs) on regular grids (e.g., images or time series), GCNs aim to extract localized features by aggregating information from different neighborhoods in the graph via the same ﬁltering operations. This imposes not only location invariant feature mappings but also leads to parameter sharing and an efﬁcient regularization effect. Recently, GNNs have not only been applied to classical graph learning tasks but have also achieved state-of-the-art performance on various data modalities (e.g., [40], [41]). Moreover, and most relevant for this work, there have been attempts to generalize GNNs to KGs. The GNN model that is most relevant to this work is the R-GCN [14]. The underlying idea of R-GCNs is to process the embeddings of neighboring entities via relationspeciﬁc linear mappings, pool this information, and combine it with the center node embedding to update the center node embedding. Concretely, for a center node i ∈ E, we have that the embedding after layer l of the R-GCN is given by where φ is a non-linearity, Ndenotes the graph neighborhood of node i with respect to relation p (i.e., N= {j ∈ E|(i, p, j) ∈ KG}) and |N| is the number of elements in N. Moreover, eee∈ Rwith l ≥ 1 denotes the embedding of entity i produced by the l-th layer of the R-GCN and eeethe initial embedding. W, W∈ Rcorrespond to trainable weight matrices that act on the embeddings of neighboring nodes and the center node, respectively. Note that while thePP ﬁrst summandWeeepools the information from neighboring nodes, the second summand Weee is a transform of the center node’s own representation in the previous layer (Fig. 2A). Thus, the second term corresponds to a self-loop and, depending on the intended usage, can be omitted, e.g., to enable the R-GCN to operate in an inductive setting. Multiple layers corresponding to (3) can be stacked on top of each other to increase the receptive ﬁeld. In particular, in a R-GCN with L layers each center node receives information from the entities L hops away (Fig. 1B). In order to address various tasks, R-GCNs can be composed with task-speciﬁc decoders. For example, for the KBC task, [14] compose a R-GCN encoder with the scoring function of DistMult, i.e., triples are scored according to (2). Based on the observation of [42] that GCNs with randomly sampled weights are able to produce meaningful node embeddings that preserve local neighborhood structures of homogeneous graphs, we propose to compose an untrained RGCN with an arbitrary link prediction model. Concretely, we call a R-GCN model frozen if the weight matrices, i.e., W and Win (3), are initialized according to some (possibly random) law but are subsequently not tuned any more to ﬁt the training data. The frozen R-GCN constitutes an entity encoder that takes as input the initial entity representations and computes a neighborhood-aware embedding. To ease the notation, we denote with fff: E → Rthe mapping induced (3). Note that we consider formulations of the R-GCN layer both with (W6= 0) and without (W= 0) self-loops. Thereby, the initial features of an entity are given by where E ∈ Ris a matrix that contains trainable entity embeddings and∈ Ris a vector of zeros except for a one at the position corresponding to the index of entity s. Subsequently, in order to compute scores for the plausibility of a triple, we feed the entity representation resulting from fffinto the scoring function of TransE. This leads to where k·k denotes the L1 norm. We also experimented with other scoring functions like RESCAL, DistMult, or ComplEx [19], but found that TransE yields the best performance. rrris a trainable representation of relation type p produced via an embedding lookup where R ∈ Ris an embedding matrix for all relations. Then, up to additive regularization terms, the training objective is given by the hinge loss where [x] = max(x, 0) and γ ≥ 0 is a hyperparameter that determines the margin of the hinge loss. Furthermore, T⊂ KG is the set of observed training triples and T⊂ E × R × E is a set of negative triples where each element is obtained by substituting either the subject or the object entity from an observed triple in T. θ = {E, R} denotes the set of trainable parameters and it is crucial for our method that the weight matrices of the R-GCN are not contained here. However, during the backward pass, the gradient of the supervision signal produced by (7) is propagated through fffto tune the entity embeddings E (see Fig. 2), allowing them to learn how to utilize the frozen R-GCN structure. In what follows we detail an empirical study to (i) evaluate the performance of the proposed method on the KBC task and (ii) compare the running time and memory consumption of the frozen R-GCN architecture with a model that optimizes all weight parameters. a) Data sets: We evaluate our method on three different data sets: Countries S1, UMLS, and FB15k-237. Table II contains the most important summary statistics on the sizes of the three data sets. Countries is a carefully designed data set to examine the reasoning abilities of KBC models. Thereby, the entities correspond to either countries, regions, or subregions and the task is to infer geographic relations. UMLS is a biomedical KG holding facts about diseases, chemical compounds, and their relations. FB15k-237 is a general-purpose KG extracted from the bigger data set FB15k to prevent leakage between the training and the test set, making the data sets more challenging. b) Experimental protocol and metrics: We compare the performance of the frozen R-GCN on the KBC task with the shallow KG embedding methods TransE and DistMult as well as the original R-GCN model that uses DistMult as a decoder. Moreover, we also evaluate the performance of a R-GCN model composed with TransE, where all parameters – including the weight matrices in (3) – are trained in an end-to-end fashion (with and without self-loops). The hyperparameters of all considered methods are speciﬁed in the supplementary material of this work. We adopt the standard ranking-based procedure proposed by [43]. Concretely, for each test triple (s, p, o) we remove either the subject (object) entity to create a query (s, p, ?) ((?, p, o)). Subsequently, all entities e ∈ E that do not correspond to observed subject/object entities (i.e., (s, p, e) ∈ KG and (e, p, o) ∈ KG) substitute the placeholder in the query and the resulting candidate triples are scored via a KBC model. These scores are used to rank all entities and the different models are evaluated by their ability to rank the original triples as high as possible, i.e., in the best case we rank the original triple (s, p, o) at the ﬁrst position. To compare the performance of different methods, we use the standard performance measures mean reciprocal rank (MRR; the average of the inverse ranks), the hits@1, and hits@3, i.e., the proportion of test triples that is ranked as the top triple or among the top three triples, respectively. c) Results: Table II summarizes the ﬁndings of the experimental study with the frozen R-GCN. On the largest data set FB15k-237, we have that the frozen R-GCN with self-loops achieves the best performance among all considered methods with respect to all metrics. In particular, the frozen R-GCN with self-loops outperforms both the shallow TransE model and a R-GCN with tuned weight matrices. Based on the rather low performance of the frozen R-GCN without self-loops, we can see that including information of the center node from the previous layer is an essential feature that leads to a signiﬁcant performance boost. The results on the biomedical KG UMLS follow the same pattern previously described on FB15-237. Again, the frozen R-GCN shows the best performance with respect to all metrics (the shallow TransE model achieves the same value for hits@3). On the smallest data set that we considered in this study, Countries S1, the results are more ambiguous in the sense that the fully trained R-GCN with selfloops outperforms the frozen R-GNC with respect to MRR and hits@1. Moreover, the shallow TransE model outperforms all other considered methods with respect to hits@3 by achieving a perfect score of 1. Overall, across all data sets, we ﬁnd that the frozen R-GCN model with self-loops can keep up with the performance of all considered baseline methods. In particular, on the larger data sets UMLS and FB15k-237, the frozen R-GCN outperforms both the fully trained R-GCN and the shallow TransE model. We further measured the running time during training and the memory consumption of both the frozen R-GCN and the fully trained model. The most relevant results are shown in Figure 3. For example, we ﬁnd that when averaging over the running times of 100 backward passes, the frozen architecture leads to speedups of more than 90% compared to the fully tuned model. Thereby, the relative speedup is more pronounced the larger the training set and the embedding size. Similarly, the memory reduction of the frozen model reaches more than 90% on FB15k-237 as the embedding size increases. The speedup and memory reduction are mostly due to less demanding operations in the backward pass, since the gradients with respect to the GNN weights are not calculated and stored when freezing the weights. In Fig. 3B, we list the most time-consuming operations of the backward pass. For small embedding dimensions, the differences are negligible, but for higher dimensions, operations like batch matrix multiplication (bmm) take considerably longer if weights are not frozen. Next to experiments in the canonical, transductive KBC setting, we also perform a qualitative evaluation of the inductive reasoning capabilities of the proposed frozen architecture. Concretely, we want to analyze whether our model can compute meaningful predictions when the subject entity is not encountered during training. This setting is challenging, because the embedding of the subject is not tuned during training and all relevant information needs to be obtained via pooling from the neighboring nodes. Note that without the self-loop in (3), the frozen R-GCN does not rely on the embedding of the center node but only on the embeddings of its neighbors. In this experiment, we assume that all entities in the receptive ﬁeld are known and only the center node is novel. Due to its intuitive nature, we consider a modiﬁed version of the Countries data set. Concretely, we add a new entity (i.e., a new country that does not exist) to the graph with the only provided neighbors being Italy and Greece. Fig. 4A shows a visualisation of the node embeddings using the ﬁrst two principal components. It is apparent that the new entity is placed close to other countries situated in Europe (orange cross). A closer look at the neighborhood of the novel node can be obtained by ranking all other countries using our model and, depending on the scores, creating a list of plausible neighbors. In Fig. 4B, we show the highest ranking countries in blue. As expected, most predicted neighbors are located between Italy and Greece. Furthermore, by changing the new country’s neighborhood, e.g., by replacing Italy with Egypt, the embedding starts moving in the embedding space, placing the new node into a region that is more consistent with the adjusted neighborhood (Fig. 4A, black cross). To summarize, we observe that even untrained, i.e., randomly initialized and frozen, GNN architectures are sufﬁciently structured (aggregate, ﬁlter and average) to be used as sensible feature selectors. Consequently, during training, the initial entity embeddings align themselves accordingly with these static feature masks to achieve high performance on KBC tasks – as seen in the previously presented experiments. The modular structure of R-GCNs enables us to combine the proposed frozen structure with arbitrary shallow embedding methods. Hence, to move the proposed architecture closer to current iterations of neuromorphic hardware – which mostly implement spiking neurons – we investigate a spike-based version of R-GCNs based on the shallow graph embedding model SpikE introduced in [36]. A natural way of mapping the symbolic structure of graphs to spiking neural networks is by representing nodes as spike times of neuron populations and relations as spike time differences between populations. Similar to [36], we represent a spike embeddings node s in the graph by the ﬁrst spike times ttt∈ Rof a population of N ∈ N integrate-and-ﬁre neurons (nLIF) with ex- ponential synaptic kernel κ(x, y) = Θ (x − y) exp−, where uis the membrane potential of the ith neuron of population s, τthe synaptic time constant and Θ (·) the Heaviside function. A spike is emitted when the membrane potential crosses a threshold value u. ware synaptic weights from a pre-synaptic neuron population, with every neuron j emitting a single spike at ﬁxed time t(Fig. 5A). Similarly, relations are encoded by a N-dimensional vector of spike time differences ∆∆∆∈ R. Whether a triple (s, p, o) is plausible or not is evaluated by looking at the discrepancy between the spike time differences of the node embeddings, ttt−ttt, and the relation’s embedding ∆∆∆(Fig. 5B): where k·k is the L1 norm. If a triple is valid, then the patterns of node embeddings and relation match, leading to d(s, p, o) ≈ 0, i.e., ttt≈ ttt+ ∆∆∆(Fig. 5B,C). If the triple is not valid, we have d(s, p, o) > 0, with higher discrepancies representing less plausibility. Given a KG KG ⊂ E × R × E, suitable spike embeddings are found by minimizing the margin-based ranking loss (7), with θ now being the weights w, ∀s ∈ E, of the nLIF embeddings and the relation embeddings ∆∆∆, ∀p ∈ R. As a ﬁrst step, we use the spike-based model to obtain initial embeddings ttt, which are subsequently fed into the frozen RGCN architecture with one layer to produce more expressive spike embeddings The updated spike embedding fff(i) of node i is given by the weighted average of its neighboring nodes’ and its own spike times. Since this can be interpreted as feeding spike times into a layer of artiﬁcial neurons, we call this version the “hybrid” model in this document. During training, the initial spike times adapt to the ﬁxed weights used for averaging to produce neighborhood-aware embeddings. This is demonstrated in Table III, where the hybrid model achieves similar or better results than the vanilla spike-based embedding model on the UMLS and Countries S1 data sets. A downside of the hybrid model is the locked calculation of the spike time averages, which contradicts the event-based and asynchronous computing paradigm of spiking neural networks. Hence, in the following, we introduce a fully spiking R-GCN which we call SR-GCN. The default R-GCN structure (3), (10) is basically a layer of artiﬁcial neurons with special routing of the input. Thus, a fully spiking model can be obtained by replacing the artiﬁcial neurons in the R-GCN layer by spiking neurons. To guarantee consistency with the ﬁrst layer, i.e., the initial embeddings (8), we again use nLIF neurons, resulting in the following interaction for the SR-GCN where κκκ is applied component-wise, i.e., κκκ(x, yyy)= κ(x, y). Updated spike embeddings are then obtained by applying the spike condition, e.g., for the i’th neuron of population s, the time to ﬁrst spike is calculated via u(t)= u. The fully spiking model consists of three nLIF layers (Fig. 6A): ttt(Fig. 6A, bottom). node in the graph. The populations get tttas input through trainable weights. The initial embedding of node s is given by the vector of spike times tttof population s. node in the graph (Fig. 6A, top). Each population obtains inputs from the initial embedding layer through the frozen R-GCN structure. The embedding of node s is given by the vector of spike times tttof population s. To train the model, we use the same decoder and loss function as for SpikE (9), (7) and optimize both the weights for the initial embeddings and the relation embeddings while keeping the R-GCN weights frozen. In contrast to classical GNN models, the spike-based version computes embeddings in all layers simultaneously (Fig. 6B). Furthermore, when accumulating the embeddings of neighboring nodes, only causal input spikes that precede the output spike are taken into account to update the embeddings (Fig. 6C), different from classical GNNs that average and ﬁlter the whole embedding vector (Fig. 6D). This constitutes a novel way of processing graphs, where important information is encoded in early spike times and message passing between nodes in the graph is done in a purely event-based way. Simulating SR-GCNs is computationally quite demanding, and hence we restrict ourselves here to a proof of concept on smaller data sets that greatly reduce simulation times. We designed two smaller data sets for our experiments: (i) one based on the famous video game StarCraft: Brood War, consisting of 32 entities, 5 relation types, 65 training triples and 11 evaluation and test triples, and (ii) one modelling the geographic relationships between the federal states in Germany, consisting of 27 entities, 2 relation types, 95 training triples and 10 evaluation and test triples. The data sets are available on github [44]. Since the data sets are quite small, we report the performance on training, evaluation and test split here to guarantee a complete picture of the training process. On both data sets, our model is capable of learning meaningful embeddings for nodes and relations in the graph, reaching similar performances as, e.g., TransE (Table IV). Due to the event-based message passing, for the Brood War data set, only (28.44 ± 9.72)% of aggregated spikes are used to produce the ﬁnal spike embedding, and on the Federal States data set (18.91 ± 7.73)% – therefore being much sparser operations than in traditional, non-spiking GNNs that need all vector components of the aggregated embeddings for updates. We are conﬁdent that the recent attention in optimizing simulating and training spiking neural networks [45]–[47] will allow us to implement a considerably faster version of our proposed model in the near future that both speeds up the hyperparameter search to improve the presented results and allows us to apply the SR-GCN to larger KGs. We propose a strategy to map R-GCNs for KG reasoning to an architecture that is closer to potential implementations on neuromorphic hardware, creating a ﬁrst link between the ﬁelds of deep learning on graph-structured data and neuromorphic computing. Our results address two challenges for mapping RGCN-based models to neuromorphic architectures: (i) weight sharing introduced by the convolution operator, which requires non-local weight updates during learning and (ii) mapping the encoder-decoder structure of graph embedding models to spiking neural networks. Concretely, we developed a model that composes a randomly initialized and frozen R-GCN encoder with a shallow decoder, which we subsequently mapped to spiking neurons. In this context, we deploy a training strategy that does not train the weights of the R-GCN, but allows the gradient to ﬂow through the network to tune the initial entity embeddings. Since the aggregation of local neighborhood information is still intact with frozen weights – basically acting as a form of graph-controlled routing – the initial embeddings learn to utilize the frozen R-GCN to generate richer node embeddings. By freezing the R-GCN weights, gradients and updates for much less parameters have to be calculated which greatly reduces the computational cost of our model compared to standard R-GCNs. We show this experimentally on an offthe-shelf processor using a standard PyTorch implementation, resulting in a signiﬁcant speedup and a reduction of memory requirements during the backward pass – between 20-90% depending on the embedding dimension – while keeping up or even outperforming other end-to-end, fully trained R-GCNs on the KBC task. Even higher gains could be achieved by adding sparsity constraints to the ﬁlter weights [48]. We further map the frozen R-GCN architecture to spiking neural networks, proposing a fully spike-based R-GCN model which extends previous work on spike-based graph embedding [36] to inductive settings [21] with dynamic graphs that can grow over time. Moreover, although not demonstrated in this work, the frozen R-GCN structure is sufﬁcient to allow the application of state-of-the-art explainable techniques on graph data, which are most often based on masking the KG; changing the ﬁnal embeddings to identify which parts of the graph are responsible for a certain link prediction or node classiﬁcation outcome [22], [23]. Apart from functional beneﬁts, an intriguing property of SR-GCNs is the event-based calculation of embeddings in a ﬁrst come ﬁrst served fashion. Conceptually, this purely temporal neighborhood aggregation strongly differs from how GNNs usually pool and combine embeddings, and leads to much sparser and potentially faster computation of neighborhood-aware embeddings. For instance, in this work, we observe that only 20-30% of the embeddings’ vector components are used to update spike-based embeddings. Although our current results are limited by the increased overhead of simulating and training spiking neurons (especially in the RGCN layer), we are conﬁdent that more rigorous simulation code [47] or emulations on accelerated neuromorphic devices [30] will allow us to scale these models up to larger KGs and reach competitive performances. Nowadays, KGs act as the backbone for various artiﬁcial intelligence tasks in numerous ﬁelds such as named entity disambiguation in NLP [49], visual relation detection [50] and visual question answering [51] in computer vision. Thereby, the underlying principle consists of condensing structural information in shallow KG embeddings which can subsequently be processed by other machine learning modules to perform various downstream tasks. Real-world, industrial applications that make use of this strategy are, for example, drug repurposing [52], context-aware recommender systems [34], [53] and context-aware security monitoring [35]. We are convinced that our work constitutes an important step towards enabling a similar modular synthesis of neuromorphic machine learning methods and graph embedding algorithms, unlocking the potential of joining symbolic and numeric data to build powerful artiﬁcial intelligence applications and reasoning systems. In particular, the proposed model can be used to learn spikebased representations of data structures that have no obvious or natural representation as spikes, e.g., social networks and tabular data, but can be modelled as a KG. To the best of our knowledge, this constitutes the ﬁrst deep learning architecture suitable for neuromorphic realizations that can reason on KGs, offering many attractive properties like sparse and resource efﬁcient message passing between nodes in a KG. Moreover, our results contribute to previous evidence [6]–[8], [11]–[13] that static connectivity motifs harbor potential functional beneﬁts for neural networks without adding extensive computational costs – especially when the surrounding neural structure is allowed to adapt to the static and frozen parts of the whole network – ultimately reducing the amount of resources and complexity required for realization in application-speciﬁc integrated circuits. This work was partially funded by the Federal Ministry for Economic Affairs and Energy of Germany (BMWi) within the IIP-Ecosphere Project and by the German Federal Ministry for Education and Research (BMBF), funding project “MLWin” (grant 01IS18050). We thank Serghei Mogoreanu and Josep Soler Garrido for helpful discussions and inspiration. We further thank our colleagues at the Semantics and Reasoning Research Group and the Siemens AI Lab for their support.