Motivated by recent progress in quantum technologies a nd in particular quantum software, r esearch and industrial communities have be e n trying to discover new applications of quantum algorithms such as quantum optimization and machine learning. Regardless of which hardware platform these novel algorithms operate on, whether it is adiabatic or gate based, from theoretical point of view, they are performing dr astically better than their class ical counterparts. systems of equations, introduced by Harow et. al. [15]. At its core, this algorithm uses the “quantum phase estimation” procedure, which is used in many 3.2 Quantum inspired Singular Value Transformation (SVT) . . . . . 11 3.3 Methods from Randomized Numerical Algebra . . . . . . . . . . 11 4.1 Parametrized Complexity . . . . . . . . . . . . . . . . . . . . . . 14 One of the remarkable examples is the quantum algorithm for solving linear important quantum algorithms. For the input matrix A ∈ C dimension, and Ax = b, where x, b ∈ C that is at lea st s e ntry at each row of the matrix is zero, the HHL algorithm terminates with time complexity O(log(N )s number and approximation constant, respectively. The best classical algorithm for this problem has the complexity of O(Nsκlog(1/ǫ)). As we will detail brieﬂy in the next section, the HHL algorithm has the following characteristics: performing matrix operations using quantum states and operation, in order to speed up data analysis. The main intuition behind Linear Algebra based quantum machine learning [4], is that data analytic deals with high dimensional vector spaces, where quantum computing c an eﬃciently work with them. This is in contrast with many classical algorithms of matrix computation, where eﬃciency can be conditionally met, e.g. assuming low rank and sparsity of input matrices. as one example of linear algebraic quantum machine learning and optimization algorithm. We review new remarkable re sult [21] that how a classical algorithm can replace the quantum algorithm of recommendation system, only with poly logarithmic overhead. The main c oncept behind this algorithm is to drop the assumption of lo ading data into quantum states (as explained above for HHL). To this end we use a tailored data structure (QRAM), that can deal with both quantum and classical input, as well as a process called ”dequantization”, which produces a novel classical algorithms for problems such as recommendation systems and principal component analysis [17]. This process replaces q uantum state preparatio n with “sampling and query” assumptions. The c laim in [16] is that for the current applications, the latter assumptions are easier to s atisfy than quantum state prepara tion. Formally, for a quantum protocol P (|φ place quantum state preparation with a classical sampling and query algorithm: SQ(φ mation (SVT) is descr ibed. This is based on a recent algo rithm for quantum SVT [14]. Combining concepts of [21] and quantum SVT, we end up with a classical algorithm that under suitable sampling assumptions, computes SVT, independent of the dimension of the input matr ic es [6]. This work generalizes the current trends of quantum algorithms of quantum linea r algebra in terms of quantum SVT. Here the intuition is also built upon the fact that practical QML and optimization algorithms assume constraints on the input (e.g. sparsity) and • It assumes that data, such as vector b, is already loaded to a qua ntum state, such that quantum operations bec ome possible on it. • It as sumes there is an eﬃcient ora cle that simulate Hamiltonians. • As for solutio n, it produces approximation of a function of solution. Many quantum optimization and machine learning algorithms are ba sed on In the Section 2, we review quantum recommendation system [16] algorithm , . . . , φ) = SQ(ψ). In Section 3.2, another line of research in terms of singular value transfo rgenerally loading input data into an input quantum state and extracting a mplitude data from an output quantum state are hard. ality of input matrices to speed up linear algebra computations. Therefore, using matrix sketching techniq ue s is natural he re. The work [5] uses tools from randomized numerical linear Algebra to obtain signiﬁcant improvements in the sublinear terms fo r the dynamic (fast upda tes to the input ma trix) and static settings. Its a nalysis r e lies on simulating leverage score sampling and ridge leverage score sampling, using a weight-balanced data structure that samples matrices rows proportional to squared Euclidean norm (l will be reviewed in Section 3.3. spired algorithms are exponentially faster than their classical versions, given that the input satisfy some constraints . However, compared to their quantum counterparts, they suﬀer computational (polynomial) overhead. This raises the question, how these quantum inspired algorithms perform in pr actice? The work of Arrazola et. al. [2], tries to answer this question and co nc ludes tha t the so called algorithms are in fact very promising in practical setting and benchmarks, by ensuring some str ingent conditions are satisﬁed. the actual bottlenecks of the recent quantum inspired algo rithms are? The complexity dependence to the input structure points us that per haps the input size is not an appropriate measure of complexity, and hence we resort to Parametrized Complexity [8]. To this end we rev ie w some works in the area of parametrized co mplexity that we might ﬁnd possible links to the above quantum inspired algo rithms. In [9], parametrized complexity of classical PCA is studied. Furthermo re, we review the work about rigidity pr oblem [10]. Moreover the works [3, 10] describe some recent work in the area o f parametrized algorithm, relevant to machine learning and optimization. Finally, we review the parametrized complexity of matrix factorization [24], a concept that is used in recommender system extensively. In this section, ﬁrst we review the HHL algorithm for solving linear systems of equation. Then we review the algorithm of Lloyd e t. al. [17], for the problem of principal component analysis (PCA), which is widely used in the area of machine learning. Finally, we review the Quantum Sing ular Value Transformation algorithm [14], which is a uniﬁcatio n methods for quantum algorithms based on quantum phase estimation, in a single algorithm. A linear s ystem of equation can be described by a n equation of the form Ax = b, where A is a ma trix and b and x are vectors of ﬁnite dimension, e.g. N . In our A fundamenta l goal of quantum-inspired algorithms is to reduce dimension- From computational complexity point of view, the described quantum in- Studying the research mentioned above, motivates us to understand where case, A ∈ C i.e. x. The algorithm [15], uses a sub algorithm, like many other quantum algorithm, for phase e stimation. For a description of this useful and impo rtant algorithm see [18]. Her e we only describe what the phase estimation does. tors with associated eig envalue e approximates θ for given unitary matrix. Mor e formally: Where binary approximation of 2 the case of HHL algorithm, the unitary U has the form of e matrix coming from linear system of equations. Furthermore, we can rewrite U as fo llowing: Here the in six main steps. Schematically, these are shown in Figure 1 , in the language of quantum circuits [18]. Here we have thre e quantum registers. One is use d for storage of binary representation (indexed by n register to load vector b, indexed by n is N = 2 Suppose U ∈ Cbe a unitary matrix and |ψi∈ Cbe its eigenvec- ˆλis n-bit binary approximation to 2. Now we c an describe HHL algorithm for solving linear system of equation . Note that we have ancillary qubits in a third register, indexed by 5. Mea sure auxiliary qubit in computational basis. If the outcome is 1, then 6. Apply an o bservable M to calculate F (x) = hx|M |xi. In recommendation system, the goal is to use information on purchase/ra ting of n products/services by a group of m clients, to provide personalize recommendation to individual clients. This information is modeled by a m ×n preferenc e matrix, where the entries indicates how user i evaluates j. The challenge is the preference matrix is not known a prio ri, such that by looking into the high evaluation make good recommendation to the users. The information from clients are received online. So the goal is to infer a good approximation o f preference matrix that reﬂects high value ra ting. are that they are low rank. This is because users interests fall in certain types, not all of the products. the a low matrix appr oximation by using samples (available informatio n) of preference matrix, up to a measure of matrices, e.g. l ommender systems is an online co mputation that requires O(poly(k)polylog(mn)), where k is the rank of the input matrix. data structure to load classical matr ix into quantum states. Theor em 2 shows transformation: |0i7→ |bi. the state: here the n-bit binary representation of λis encoded by |λi. in Figure 1). the register would be in post-measurement state: which corresponds to the solution up to the normalization factor. One important characteristics of preference matrix in recommender systems The ﬁrst algorithm by Kerenidis et. al. [16], uses a technique to reconstruct The result of sampling technique and ma trix reconstruction of quantum rec- One important as sumption of [16] is that there is an eﬃcient algorithm and with the assumption that the trans formation A 7→ |Ai can be done e ﬃcie ntly, we can store the input of quantum recommendation system optimally. However, this assumption is dropped in [21]. Mo re details on the algorithm and in particular sampling technique can be found in [16]. We will detail some aspects of sa mpling in the next section. Theorem 1. [16] Let A ∈ R system with the format (i, j, A of input that already been taken. Then, there exists a data structure to store the input of the recommendation system with the following features: One of the widely used dimension reduction techniques for large data sets is principal component analysis (PCA). In PCA we use spectral decompos ition of matrices in terms of eigenvectors and corresponding e igenvalues, and discards eigenvalues below certain threshold, to o btain a good low rank appr oximation of the input matrix. matrix C illustrate the correlation be twe e n diﬀerent components of the data and is deﬁned a s C = Now only a few eigenvalues e could be discarded. The remaining called principal components and any given data vector v, can be succinctly described by these k components: v = This operation requires O(d domly chosen classical data to a quantum state v with a dedicated data structure,called QRAM. Becaus e these data are picked in random, the resulting state has the density matrix 1/N N is the size o f the data set. This is in fact equals to covariance matrix up to a constant factor. Now by incorpora ting a sampling technique [17] and phase estimation (as in section 2.1) we can ﬁnd estima tion of eige nvalues and corresponding eigenvectors : ˜A ∈ Rdeﬁned by˜A= ||A||. A quantum algorithm with access to this data st r ucture can perform the following mappings in timeE polylog(mn):˜U : |ii|0i 7→ |ii|Ai for i ∈ [m], and˜V : |0i|ji 7→˜A|ji for j ∈ [m] (projecting A rows and columns, respectively). Let data are represented by d-dimensional vectors v. Then the covariance The quantum algorithm for PCA [17], in the ﬁrst step needs to map a ran- Now the pr operties of principal comp onents of C can be inferred from measurements o n the quantum representation of its eigenvectors. The time and query complexity of this algorithm is O((logN ) The area of quantum a lgorithms involves with two types of algorithms: QPE based (s uch as the algorithms described in the prev ious s ections) and Quantum Fourier Transfo rmation algorithms [18] (such as Shor factorization). The aim of Quantum singular value transformation (QSVT) [14], is to ﬁnd a general framework for QP E based algorithms. sition and trans formations, and therefore use QP E, which we reviewed some of them in the previous sections. Nevertheless, these algorithms work with quantum states and need to perform encoding and then re ading measurement outcomes to learn certain properties up to a speciﬁed precision ǫ. Therefore the improvements in terms of computational complexity of these algorithms is merely polynomially. Of course for large scale optimization applications, such as semi-deﬁnite quantum program solvers this polynomial improvement matters signiﬁcantly [23]. are orthonormal projectors . Let U b e any unitary. For an ope rator A, we say U, Π a nd decomposition (SVD). Now the sing ular value transformation is deﬁned by: is a unitary U only U and U In this section we revie w the new trend in designing quantum inspired algorithms, appeared in a series of publications [21, 13, 20]. We also r eview the singular value transformation based technique [6]. In both frameworks, we deal with dequantization concept, as described in introduction section. Furthermore, we explore new techniques from [5], where more advanced sampling methods for Many important applications of quantum computing use spectral decompo- The central concept of QSVT is projected unitary encoding. Let Π and˜Π ˜Π form a projected unitary encoding, if we have: A =˜ΠUΠ. Let P ∈ C[x] be an odd polynomial and A = W ΣVbe a singular value The main re sult of [14] is stated as following: For any odd polynomial of degree d, i.e. P ∈ C[x], where |P (x)| ≤ 1, there See [14] for full details as well as examples. matrices such as leverage score, is used to achieve improvement. Also new classical data structure to support dequantization is introduced in this work. O ur review captures the main ideas and drop most of technical details for brevity. In this section we explain o n of the central notions introduced in [21] and the subsequent works. The goal is to cons truct a classical input model which is close quantum input model (quantum state), while we maintain complexity of sublinear time. To do so we deﬁne sampling and query access as following: Deﬁnition 1. We deﬁne query access model for v ∈ C sample and query model, denoted by SQ(v): 1. for all i ∈ [n], we have oracle access and can qu ery for v(i). Similarly, 2. We have s ampling and query access, SQ(v) if following conditions are (dynamic) data structure. In a typical Random Access Memory (RAM) model, query access has linear co mputational complexity. However, we are interested in a (classical) data structure that support sublinear acc e ss complexity. In [21] uses a data structure tha t [16] uses for preparation of quantum states, which satisﬁes all condition we need. rithms is a binary search tree, that leaf nodes store v sgn(v updating and entry, all nodes above the corresponding leaf has to be updated. For sampling, star t from the root of tree and randomly choose a child, where the probability is pr oportional to its weight. Note tha t because we mostly deal with sparse input (such as recommendation systems), we only include non zero nodes in the tree. Figure 2, shows and example of such tree for a vector v ∈ R Lemma 1. To store a vector v ∈ R structure, supporting the following operations: 1. Qu erying and updating an entry of v in O(logn) t ime. 2. Qu erying kvk for a matrix A ∈ C, we have Q(A) if for all (i, j) ∈ [m] ×[n], we can query for A(i, j). met: (a) For any v, we can query Q(v). (b) If we can get independent samples, i, over [n] with the distribution D∈ R, deﬁned as D:= |v(i)|/kvk. (c) We can query for kvk. Note that in order to implement the above input model, we need a dedicated One of the dynamic data structures which is used by mentioned QML alg o- ). The interior nodes weight is the sum of their children weights. For Now we have the following lemma : Figure 2: Dynamic binary search tree for sto ring inputs of quantum inspired algorithms Lemma 2. There exists a data structure storing a matrix A ∈ R nonzero entries in O(wlogmn) space, supporting the following operations: of A and properties, and in fact, is the data structure Kerenidis and Prakash [16] use to prepare arbitrary quantum states, which enables our algorithm to o perate on the same input. Figure 3 s hows an example of such data structure for a matrix A ∈ R Theorem 2. [[21]] Let A be an input matrix that s upports query and sampling as speciﬁed in Lemma 2, suppose i ∈ [m] be a row and σ, η be singular value thresholds for low rank approximation of A (denoted by A there is a classical algorithm that has an output distribution ǫ-close to D where M ∈ R plexity Note that the actual format of the input in our problems is matrix so for k (lnorm of the irow of A). Then we have the following lemma [21] • Reading and updating an entry of A in O(logmn) time; • Finding˜Ain O(logm) time; • Finding (kAk)(Frobenius norm) in O(1) time; • Sampling from Dand Din O(logmn) time. By having a copy of the data str uc ture speciﬁed in Le mma 1 for each row ˜A we can form the above data structure. This has all of the desired put dimension (m, n). To implement the needed sampling operations we need the dynamic data structure of Lemma 2. Note that using that data structure will incur additiona l O(log(mn)) overhead. Comparing this algorithm and the quantum algorithm of [16], shows that in the quantum version of producing low rank approximation, the dependence on ǫ is logarithmic, compare to the classical quantum inspired version. gebraic operations should not add the cost of reading a full row or column of the input, and by using a number of sampling routines (see [21] for full details) and applying it to the input of re commendation system, i.e. preference matrix, we have the following theorem: Theorem 3. [[21]] By applying Theorem 2 to the input model of recommendation system, which uses the dynamic data structure in [16], we obtain the same complexity bounds up to constant factors for small ǫ. original algo rithm of Lloyd et. al. [17] best perfor ms in low rank setting therefore we have the following problem: Problem (PCA for low-rank matrices). For a Given a matrix A ∈ C where we have access to SQ(A) such that AA and eigenvectors {v access to {SQ( ˆv sampling techniques and low rank a pproximation. For details of its algo rithm see [20], pa ge 3. The Theorem 2 states its algorithm has a runtime indpe ndent of the in- Now by using the above theorem and impose the constraint that linear al- For the PCA problem we use similar techniques of sampling and query. The The quantum inspired algorithm in [20] exactly solve this pr oblem by using As we described in Sec tion 2.4, the quantum algorithms based on phase estimation can be g eneralized by transformation on singular values. methods. Fo r instance, QML algorithms such as quantum support vector machines [19], principal component analysis [17], training Boltzmann machines for state tomography [22], all are based on singular value transformation. unitary operations in describing evolution o f quantum systems with exponential size. This leads to the idea of block encoding of large matrices which is a special case of projected encoding described in Section 2.4. for the se minal work of quantum singular transformation of [14]. to SQ(A), we can obtain a succinct characterization of singular transformation of A, eﬃciently. Deﬁnition 2 (Even singular value). For a given A ∈ C [0, ∞) 7→ C, and SVD of A = of A, deﬁned as f ( Theorem 4. Assume for a the input A ∈ C f : [0, ∞) 7→ C such that f is L-Lipschitz and for small enough ǫ, δ ≥ 0 , we can ﬁnd a subset of A’rows, denoted by R ∈ C and a subset of R’s column, denoted by C ∈ C If sampling and query takes T time, then ﬁnding R, C and computing has the complexity O(r theorem says that RUR decomp osition expresses a good approximation of input matrix as a linear combination of r thorough details. In the previous sections we drew parallels to quantum algorithms by implementing the so called dequantization technique. One can observe the heavy relia nc e of this technique on randomized numerical linear algebra, where low matrix rank computation and matrix sketching has been used at large. In a recent work [5] raises the following questions: Is the running time of quantum inspired algorithms, that are based on sample and query techniques, and have sublinear terms is improvable? Here the goal is to maintain the eﬃcient creation of the dynamic data structure in O(nnz(A)) of a given input matrix A, and updating In fact transforming singular values is the crux of many machine learning Note that the speed up in quantum algorithms stems from the ability of However, the a im of this section is to extract a sampling based methods [6] The main r e sult of [6] states that the for a matrix A ∈ C, with access The R¯f(CC)R in [6] is called RUR decomposition. In that case the above it in O(log n). Another q ue stion is can we use a vast number of techniques in randomized numerical linear algebra in this context? technic ality of the methods in the context of numerical linear algebra we refer the reader to the main text a nd we only give statement of the improvement result for the quantum inspired algorithm for recommender systems here. computation. The goa l is to ﬁnd a new matrix with its rows (or columns) are subset of the target matrix but still hold its main structures. Lemma 9 of [5], summarizes the main sketching techniques, relevant to this work. σ(A), is denoted by ˆσ Now Theorem 5, pre sents a sampling technique that compared to [21], has a lower overhead and smaller relative error. For further de tails on how a dynamic data s tructure can be constructed from sketching matrix, see [5]. Theorem 5. [5 ] Let A ∈ R k and ǫ, be target rank and error parameter, respectively. Suppose A best k rank approximation of A. Then we can ﬁnd sampling m atr ices S, R and W (a rank k m atr ix ) such that: with the running time: In this section we complement theor etical analysis o f q uantum inspired algorithms that are based on sampling and query and matrix sketching techniques, by reviewing implementation and numerical benchmarking, ﬁrst repo rted in [2]. The aim of such experiment is to acquire an understanding of the practical complexity of quantum inspired algorithms in terms of erroneous and runtime. The main ﬁnding is that although we have seen in the previous sections a hefty polynomial overhead compare to quantum algorithms, however in practice and in the case we only have indirect access to massive data sets via query, quantum inspired algorithms performs very well. This is a reminding to the fact that computational complexity of these problems does not depend on the dimension of input, but other quality of the input and the manner input is accessible. In the following we brieﬂy report on the ﬁndings of [2]. dimension of less than 10 The main contribution of [5] is answering to the above questions. Given the Sketching is a widely used solution for reducing dimensionality of matrix For a given n ×d matrix A of rank k, the lower bound for the singular value The ﬁrst example concerns with sampling according to the lnorm. For the data structure of [16] and [21]. The Figure 4 illustrate the runtime of producing 1000 sample of l explained previously and direct sampling method. Figure 4: Runtimes for generating one thousand samples from a length-square distribution in [2]) is movie recommendation s ystems ba sed on MovieLens 1 00K data set. It consists of a pre ference matrix with 100,000 ratings from 611 use rs across 9,724 movies. where ra tings are scaled within the range [0.5, 5]. Here majority of users watch only a small fraction of available movies, and therefore the matrix is sparse. The preference matrix has full rank of k = 611, its largest and smallest singular values are respectively σ number (for l ratings and subsequently recommend movies that have a high predicted rating, which we explained in Section 3. gorithm and the direct calculation method. Figure 5: Running times (in seconds) fo r the quantum-inspired alg orithm for recommendation s ystems. The pa rameter t length-squar e d (LS) probability distributions over rows and columns time. The runtime of the quantum-inspired algorithm is domina ted by the The second example which we mention here (see more case studies and details Figure 5 [2 ] compares the running times between the quantum-inspired al- For sma ll size matrices direct computation of SVD can be done in a shor ter coeﬃcient estimation step, even when the resulting errors in estimating these coeﬃcient is relatively high. As a conclusion, the numerical examples we mentioned here suggest that when applied to mode rately-sized data sets, because they rely on a more intricate procedure, the quantum-inspired algorithms take more time than ex act diagonalization, and because they rely on sampling for coeﬃcient estimation, lead to higher inaccuracies. These results show that in order to provide a speedup over preexisting classical algorithms, the quantuminspired algorithms must b e applied to extremely large data sets where exact diagonalization is impossible and where even the linear scaling of SVD approximation algorithms (for example those based on monte carlo methods such as FKV [11]) prevents its direct application. where we have perturbation in the input. T his has be do ne in the context of smooth analysis in a recent work of Dunjko et. al. [25]. They proved, using smooth complexity that if the quantum (inspired) algorithm is robust against small entry-wise input per turbation, state prepara tion can always be achieved with c onstant queries. f(x): random ac cess memory, provides the mapping i 7→ f (x samples from entry-wise oracle and unstructured search has lower bound Ω(D) queries for x ∈ R turbation occurs. Subsequently, smooth complexity is deﬁned. Deﬁnition 3 (Smooth Complexity [25]). Given an algorithm A with an input domain Ω is deﬁned as: runtime of A. Having deﬁned s mooth complexity, the main result of [25] is the following theorem: Theorem 6. Given oracle access, O encoding of x into |xi (state preparation) has smoothed complexity O(1/σ). In this section we review some works in the context of cla ssical algor ithms that may have relevance to parametrized complexity analysis. The goal of our It is also relevant to examine the complexity o f q uantum inspired algorithm The entry-wise access is done by an oracle O, with an arbitrary function For s implicity we assume f(x) = x. Note that in the classical setting, To model noisy data input, it is assumed that Gaussia n element wise per- Here g is a Gaussian random vector with varia nce σ, and Tdenotes the project is to extend parametriz e d complexity concepts to the quantum inspired algorithms w hich we explained in the previous sections. ity (FPT). For FPT algorithm, the performance is heavily depends on some ﬁxed parameters rather than the input size. So we can view these pr oblems in sliced way where each slice co rresponds to a tractable problem. Another impor tant concept is ”kernelization” [8], where we reduce the para metrized input to a new parametrization where the algorithm perfo rms better. The formal deﬁnition of FPT is as following: Deﬁnition 4. [Fixed parameter tractable] We say an algorithm A is FPT if for all parametrized input hx, ki, constant c, and a commutable function f, A has a running time f(k)|x| complexity bound is in terms of commutable function f and g : f (k)|x| plained its quantum version in Section 2.2). Here ([24]) the problem formulated as “weighted low rank approximation” and “matrix completion problems”. The formal deﬁnition of these problems a re as following: Deﬁnition 5. [Weighted low rank approximation] Where OP T := Min and the task is to complete the matrix in a way that (1) minimizes the number of distinct rows and columns, (2) minimizes the number of distinct column, or (1) minimizes the rank. It is s hown in [12] that this problem is FPT for bounded domain. Deﬁnition 6 (matrix completion problem). three pa rametrizations. A central concept in parametrized complexity [8] is Fixed Parameter Tractabil- Parametrized algor ithms with higher complexity belong to XP , where the The ﬁr st example concerns with recommendation system (which we ex- In the matrix completion problem, we are given a matrix with missing entries Now the [24], considers the weighted low rank approximation with these tion 5, with probability 9 /10 can be done in O(nnz(A) + nnz(W ) + n)) + n2, for arbitrary small γ, and hence is in FPT. 3. with rank r, computing and [7], Fomin et. al. showed that for the robust PCA and PCA with outliers problems there are exact and ra ndomized FPT algorithms. Their results are based on the relation of these pro ble ms with the matrix rigidity problem [10]. A normal PCA formulation (as in Section 2.3) is not ro bus t agains t corrupted observations. For a data M ∈ R sparse matrix S. Thus we have a supe rposition of a low-rank component L and a sparse component S : M = L + S. Now the robust PCA is as following: a matrix M ∈ R by update is ca lle d matrix rigidity problem. Theorem 8 ([10]). The problem of max rigidity (robust PCA) is solvable in poly(M )2 called PCA with outliers. The parametrized complexity of this problem in both classical and approximation setting is studied in [7, 9]. Here the problem is as following: identify the outliers so that the remaining set of points ﬁts best into an unknown r-dimensional subspace” outliers, represented by the non-zero rows of N , whose removal from A leaves the remaining n − k inliers as close as possible to an r-dimensional subspace. The matrix L contains then the orthogonal projections of the inliers onto this subspace” the following theorem: Theorem 9. Solving PCA with out liers is reducible to solving |A| of PCA. since only it erative algorithms for PCA and SVD exist, t his algorithm does not solves PCA with outliers in a ﬁxed number of operations. However, up to some constant precision, PCA is solvable in polynomial number of operations ability 9/10 can be done in O(nnz(A) + nnz(W ) + n)) + n2, for arbitrary small γ, and hence is in FPT. can be done in O(n, and hence is in FPT. Our second example c oncerns with PCA a s mentioned in Section 2.3. In [9] On the other hand robust PCA is equivalent to matrix rig idity problem: for (M), then the following problem: Note that this problem is NP-complete for r ≥ 0 . Another formulation is “Given a set of n points in Rwith (unknown) k outliers, the problem is to One can interpret this problem in the following geometrical setting: “Given n points in R, represented by the rows of A, we seek for a set of k Now the formal deﬁnition of the PCA with outliers is shown in Figure 6. The algorithms of [9] reduces the problem to iteration of normal PCA as in nential Time Hypothesis (ETH) [1], they improve d Theorem 9 to achieve ﬁxed parameter tractability. with n variables and m clauses cannot be solved in time O(2 This means that k-SATISFIABILITY cannot be solved in subexponential in n time. Now the following theorem is shown in [9]: Theorem 1 0. For any ǫ ≥ 1, there is no ǫ-approximation algorithm for PCA with outliers with running time f(d) N ETH fails, where N is the bitsize of the input matrix. This is investigated in [7], resulting in several approximation alg orithm for PCA with outlier. Here we conclude this section with the following theorem, and refer the reader to [7] for further details. Theorem 11. For every ǫ > 0, an 1 + ǫ-approximate solution to PCA with outliers can be found in time n Conjecture 1. Quantum inspired algorithm for recommendation system and PCA, that are based on low rank approximation and also dependent to sample and query technique, are FPT, using appropriate parametrization. Moreove r by using computational complexity as sertion that is called Expo- Roughly ETH says that for k ≥ 3, a nd δ> 0, k-SATISFIABILITY problem One way to circumvent ETH assumption is to consider ﬁxed rank setting.