In recommender systems (RecSys) and real-time bidding (RTB) for online advertisements, we often try to optimize se quential decision making using bandit and reinforcement learning (RL) techniques. In these applications, oine reinforcement learning (oine RL) and o-policy evaluation (OPE) are benecial because they enable safe policy optimization using only logged data without any risky online interaction. In this position paper, we explore the potential of using simulation to accelerate practical research of oine RL and OPE, particularly in RecSys and RTB. Specically, we discuss how simulation can help us conduct empirical research of oine RL and OPE. We take a position to argue that we should eectively use simulations in the empirical research of oine RL and OPE. To refute the counterclaim that experiments using only real-world data are preferable, we rst point out the underlying risks and reproducibility issue in real-world experiments. Then, we describe how these issues can be addressed by using simulations. Moreover, we show how to incorporate the benets of both real-world and simulation-based experiments to defend our position. Finally, we also present an open challenge to further facilitate practical research of oine RL and OPE in RecSys and RTB, with respect to public simulation platforms. As a possible solution for the issue, we show our ongoing open source project and its potential use case. We believe that building and utilizing simulation-based evaluation platforms for oine RL and OPE will be of great interest and relevance for the RecSys and RTB community. ACM Reference Format: Haruka Kiyohara, Kosuke Kawakami, and Yuta Saito. 2021. Accelerating Oine Reinforcement Learning Application in Real-Time Bidding and Recommendation: Potential Use of Simulation . In RecSys 2021 Workshop on Simulation Methods for Recommender Systems, October 2, 2021, Amsterdam. ACM, New York, NY, USA, 8 pages. https://doi.org/xxx In recommender systems (RecSys) and real-time bidding (RTB) for online advertisements, we often use sequential decision making algorithms to increase sales or to enhance user satisfaction. For this purpose, interactive bandit and reinforcement learning (RL) are considered powerful tools. The RecSys/RTB research communities have studied many applications of bandit and RL and demonstrated their eectiveness in a wide variety of settings [ 55]. However, deploying RL policies in real-world systems is often dicult due to the need for risky online interactions. Specically, when we use an adaptive policy and learn it in the real environment, numerous numbers of exploration is needed before acquiring near-optimal decision makings [ damage sales or user satisfaction [ in the real environment. However, it involves high stakes because the unseen new policy may perform poorly on the system [12]. Therefore, online deployment of RL policies is often limited due to risk concerns, despite their potential benets after the successful deployment. Emerging paradigms such as oine reinforcement learning (oine RL) and o-policy evaluation (OPE) try to tackle these issues in a data-driven manner [24]. In oine RL and OPE, we aim to learn and evaluate a new policy using only previously logged data, without any risky online interaction. The major benet of oine RL and OPE is that we can obtain a new policy that is likely to perform well in a completely safe manner, by 1) learning a new policy using only the logged data (oine RL), and 2) estimating the policy performance using the logged data to guarantee the safety in deployment (OPE). The potential to reduce the risks in deploying RL policies is gaining researchers’ interest. There are their applicability in RecSys practice [5, 12, 13, 25, 27, 32, 34, 35, 43]. Discussion topic.In this paper, we discuss how simulation studies can help accelerate oine RL/OPE research, especially in RecSys/RTB. In particular, we focus on the roles of simulations in the evaluation of oine RL/OPE because empirical research is essential for researchers to compare oine RL/OPE methods and analyze their failure cases, leading to a new challenging research direction [9,33,39]. Moreover, validating the performance of the oine RL policies and the accuracy of OPE estimators is crucial to ensure their applicability in real-life situations [9]. Our position.We take a position thatwe should eectively use simulations for the evaluation of oline RL and OPE.Against the position to argue that only the real-world data should be used in the experiments, we rst show the diculties of comprehensive and reproducible experiments incurred in real-world experiments. Then, we demonstrate the advantages of simulation-based experiments and how both real-world and simulation-based experiments are important from dierent perspectives. Finally, by presenting our ongoing open source project and its expected use case, we show how a simulation platform can assist future oine RL/OPE research in RecSys/RTB. 2 PRELIMINARIES In (general) RL, we have total𝑇timesteps to optimize our decision making (the special𝑇 =1 case is called the contextual bandit problem). At every timestep𝑡, the decision maker rst observes state𝑠∈ Sand decide which action𝑎∈ Ato take according to the policy𝜋 (𝑎| 𝑠). Then, the decision maker receives a reward𝑟∼ 𝑃(𝑟| 𝑠, 𝑎)and observes the state transition𝑠∼ T (𝑠| 𝑠, 𝑎), where𝑃𝑟 (·)andT (·)are the unknown probability distributions. For example, in a RecSys setting,(𝑠, 𝑎, 𝑟)can be user features, an item that the system recommends to the user, and the user’s click indicator, respectively. Here, the objective of RL is to obtain a policy that maximizes the following policyÍ performance (i.e., expected total rewards)𝑉 (𝜋):= E𝛾𝑟, where𝛾 ∈ (0,1]is a discount factor andE[·]isÎ the expectation over the trajectory distribution 𝑝(𝜏) ∼ 𝑝(𝑠)𝜋 (𝑎| 𝑠)𝑃𝑟 (𝑟| 𝑠, 𝑎)T (𝑠| 𝑠, 𝑎). Let us suppose there is a logged dataset Dcollected by a behavior policy 𝜋as follows. where the dataset consists of𝑛trajectories. In oine RL, we aim to learn a new policy𝜋that maximizes the policy performance𝑉 (𝜋)using onlyD. In OPE, the goal is to evaluate, or estimate, the policy performance of a new (evaluation) policy𝜋using an OPE estimatorˆ𝑉andDasˆ𝑉 (𝜋;D) ≈ 𝑉 (𝜋). To succeed in oine RL/OPE, it is essential to address the distribution shift between the new policy𝜋and the behavior policy𝜋. Therefore, various algorithms and estimators have been proposed for that purpose [ To evaluate and compare these methods in empirical research, we need to access both the logged dataset ground-truth policy performance of evaluation policy Dand 𝑉 (𝜋 3 IS THE USE OF REAL-WORLD DATA SUFFICIENT TO FACILITATE OFFLINE RL? In this section, we discuss the advantages and drawbacks of the counterclaim: only the real-world data should be used in experiments of oine RL and OPE. We can implement a real-world oine RL/OPE experiment by running (at least) two dierent policies in the real-world environment. First, behavior policy RL/OPE, we need to approximateÍÍ real-world experiments compared to simulation is that it is informative in the sense that the experimental results are expected to generalize in real-world applications [33]. However, there are two critical drawbacks in empirical studies using only real-world data. The rst issue is the risky data collection process and resulting limited experimental settings in the comprehensive experiments. The real-world experiments always necessitate the high-cost data collection process because the online interactions can be harmful until the performance of data collection policies ( variety of policies due to this risk concern, and the available empirical ndings in real-world experiments are often limited. For example, when evaluating oine RL algorithms, we often want to know how well the algorithms learn from dierent logged data, such as the one collected by a sub-optimal policy [ behavior policy is often demanding because it may damage sales or user satisfaction [ of OPE estimators, researchers are often curious about how the divergence between behavior and evaluation policies aects the accuracy of the performance estimation [ policies is challenging, as there is huge uncertainty in their performance [24]. The second issue is the lack of reproducibility. Due to condentiality and data collection costs in RecSys/RTB practice, there is only one public real-world dataset for OPE research (Open Bandit Dataset [ a real-world dataset for oine RL because the evaluation of a new policy requires access to the environment [ Therefore, conducting a reliable and comprehensive experiment is extremely dicult using only real-world data, which we argue is a bottleneck of the current oine RL/OPE research in RecSys/RTB practice. 4 HOW CAN SIMULATIONS ACCELERATE OFFLINE RL RESEARCH? In this section, we describe how simulations can help evaluate oine RL/OPE methods together with real-world data. An alternative way to conduct experiments is to build a simulation platform and use it as a substitute for the real environment. Specically, we can rst deploy behavior policy synthetic dataset estimation the whole experimental procedure does not require any risky online interaction in the real environment. Since the policy deployment in the simulation platform is always safe, we can gain abundant ndings from simulation research [ which is often dicult in real-world experiments [ ) to conduct experiments of oine RL/OPE using both real-world and simulation-based synthetic data. 𝛾𝑟(≈ 𝑉 (𝜋)), by deploying evaluation policy𝜋to an online environment. The advantage of D. Then, we can calculate the ground-truth policy performance𝑉(𝜋)or approximate it by on-policy 𝑉(𝜋)when the ground-truth calculation is computationally intensive. The important point here is that 9,10,39,42]. For example, in the evaluation of oine RL, we can easily deploy a sub-optimal behavior policy, RL by deploying a new policy several times in dierent training checkpoints, which is challenging due to risks and deployment costs in the real-world experiments [26]. In addition, we can test how well an OPE estimator identies evaluation policies that perform poorly, which is crucial to avoid failures in practical scenarios [28]. Furthermore, we can also tackle the reproducibility issue in real-world experiments by publicizing simulation platforms. Using an open-access simulation platform, researchers can easily reproduce the experimental results, which leads to a reliable comparison of the existing works [8,15]. Therefore, simulation-based experiments are benecial in enabling reproducible comprehensive studies of oine RL/OPE. Although the simulation-based empirical research overcomes the drawbacks of real-world experiments, it should also be noted that simulation-based experiments have a simulation gap issue [49]. Specically, to model the real environment, we need function approximations for the probability distributions (i.e.,𝑃𝑟 (𝑟| 𝑠, 𝑎)and𝑇 (𝑠| 𝑠, 𝑎)). Unfortunately, there must be an inevitable modeling bias which may lead to less informative results. However, since both real-world and simulation-based experiments have dierent advantages, we can leverage both for dierent purposes, as shown in Table 1. Specically, we can rst conduct simulation-based comprehensive experiments to see how the conguration changes aect the performance of oine RL/OPE methods to discuss both the advantages and limitations of the methods in a reproducible manner. We can also verify if oine RL policies and OPE estimators work in a real-life scenario using real-world experiments with limited online interactions. Here, by performing preliminary experiments on a simulation platform and removing policies that are likely to perform poorly in advance, we can implement real-world experiments in a less risky manner. Thus, we argue that we should eectively use simulations in the empirical research of oine RL and OPE. 5 TOWARDS PRACTICAL RESEARCH OF OFFLINE RL IN RECSYS AND RTB In this section, we discuss how we can further accelerate oine RL/OPE research in RecSys/RTB practice. The benets of the simulation-based experiments have indeed pushed forward the oine RL/OPE research. Specifically, many research papers [2,6,9,14,20,22,29,44–46] have been published using a variety of simulated control tasks and their standardized synthetic datasets collected by diverse policies [8,15]. Moreover, the simulation-based benchmark experiments play important roles for researchers to discuss both advantages and limitations of the existing oine RL and OPE methods [9, 10, 39, 42]. Practical applications, however, are still limited, especially for oine RL (such as [5,25,27,31,43,47]). We attribute this to the lack of application-specic simulation environments that provide useful insights for specic research questions. For example, RecSys/RTB are unique regarding their huge action space and highly stochastic and delayed rewards [4,25,55]. Therefore, we need to build a simulation platform imitating such specic characteristics to better understand the empirical performance of oine RL/OPE methods in these particular situations. In the RecSys setting, there are two dominant simulation platforms well-designed for oine RL/OPE research, OpenBanditPipeline (OBP) [33] and RecoGym [32]. They are both benecial in enabling simulation-based experiments in a fully oine manner. Moreover, OBP is helpful in practice because it provides streamlined implementations of the experimental procedure and the modules to preprocess real-world data. However, their limitation is that they are unable to handle RL policies. OPE methods of RL policies relevant to the real-world sequential decision makings [ simulation platforms in RTB. There is a need to build a simulation-based evaluation platform for oine RL and OPE in RecSys/RTB settings. Motivated by the above necessity, we are developing an open-source simulation platform in the RTB setting. Our design principle is to provide an easy-to-use platform to the users. Below, we present an expected use case and describe how to utilize our platform in oine RL/OPE empirical research. We aim to conduct both simulation-based and real-world experiments, as described in Section 4. The solid arrows in Figure 1 show the workow of simulation-based comprehensive experiments based on our platform. The key feature of the platform is that there are design choices for researchers, such as what behavior and evaluation policies to use and what oine RL and OPE methods to test. Moreover, researchers can also customize the environmental congurations in the simulation platform, such as action space performance of oine RL/OPE. After the detailed investigation in simulation-based experiments, we can also verify if the oine RL/OPE methods work in real-life scenarios with a limited number of online interactions. Our platform also provides streamlined implementation and data processing modules for assisting real-world experiments, as shown with the dotted arrows in Figure 1. The platform also allows researchers to identify a safe policy in advance using our semi-synthetic simulation, which replicates the real environment based on the original real-world dataset. The results of such a semi-synthetic simulation may help reduce the risks in real-world experiments. Finally, since we plan to publicize the platform, the research community can engage in our project to make the simulation platform to be a more diverse benchmark and more practically relevant. Moreover, we plan to extend our platform to the RecSys setting. These additional eorts will allow researchers to easily involve in the empirical research of oine RL/OPE in RecSys/RTB.