Rishabh Bhardwaj, Tushar Vaidya & Soujanya Poria Information Systems Technology and Design Singapore University of Technology and Design Singapore {rishabh bhardwaj@mymail, tushar vaidya@, sporia@}sutd.edu.sg Due to recent technological advancements, more than two-thirds of the world’s population has access to mobile phones amount of data obtained from user-device interactions, sensors, etc. Learning algorithms can use this data to provide enhances user-experience. However, directly accessing this data comes at the cost of risking user privacy (Jeong et al., 2018). To mitigate the issue, federated learning (FL) (shown in ﬁg. 1) is a mechanism that retrieves the parameters of the (local) user-speciﬁc model and performs federation of knowledge either by distillation or merging the models (Kone learn a domain-generalized central (global) model. The classic FL algorithms such as federated averaging and its adaptations are based on averaging of local model parameters, and thus only applied when the client models posses the similar network architectures (Mohri et al., 2019; Li et al., 2019a) However, FL paradigm has critical limitations of being costly in terms of communication load with the increase in local model sizes and demands all the participating models to have same architecture (homogenity) (Jeong et al., 2018; Lin et al., 2020). Federated distillation (FD) proposes to exchange only the outputs of the local model, i.e, logits or probability measures whose dimensions are usually much smaller than the size of models themselves (Jeong et al., 2018). Therefore, FD allows to learn from an ensemble of different local models https://datareportal.com/global-digital-overview Enhancing the user experience is an essential task for application service providers. For instance, two users living wide apart may have different tastes of food. A food recommender mobile application installed on an edge device might want to learn from user feedback (reviews) to satisfy the client’s needs pertaining to distinct domains. Retrieving user data comes at the cost of privacy while asking for model parameters trained on a user device becomes space inefﬁcient at a large scale. In this work, we propose an approach to learn a central (global) model from the federation of (local) models which are trained on user-devices, without disclosing the local data or model parameters to the server. We propose a federation mechanism for the problems with natural similarity metric between the labels which commonly appear in natural language understanding (NLU) tasks. To learn the global model, the objective is to minimize the optimal transport cost of the global model’s predictions from the conﬁdent sum of soft-targets assigned by local models. The conﬁdence (a model weighting scheme) score of a model is deﬁned as the L2 distance of a model’s prediction from its probability bias. The method improves the global model’s performance over the baseline designed on three NLU tasks with intrinsic label space semantics, i.e., ﬁne-grained sentiment analysis, emotion recognition in conversation, and natural language inference. We make our codes public at https://github.com/declare-lab/sinkhorn-loss. of dissimilar conﬁgurations at reduced user-privacy risk, low communication overhead, and less memory space utilisation. However, entropy-based losses do not allow one to deﬁne metric structure in the label space. This can be critical when the task possesses a natural similarity relationship between the output labels. Generally, FD algorithms adopts local model’s ensemble knowledge distillation to global model using Kullback–Leibler (KL) divergence (Gou et al., 2021; Lin et al., 2020) or cross-entropy based losses (Jeong et al., 2018) as they are easy to compute and facilitate smooth backpropagation (Murphy, 2012). However, one critical limitation of such entropy-based losses is that they do not allow one to deﬁne metric structure in the label space. During global model ensemble training, leveraging such semantic structure can be useful when the task possesses a natural similarity relationship between the output labels. For instance, in the task of ﬁne-grained sentiment classiﬁcation of a text, strongly positive sentiment is closer to positive while far from strongly negative. Contrary to information-theoretic losses, Optimal Transport (OT) admits prior relationship in the label space (Frogner et al., 2015). The major contributions of this work are: Contribution:1 For the tasks with intrinsic label-space semantics, we propose a better federated (ensemble) distillation of local models to learn global model by encoding inter-label relationships using optimal transport. The user-speciﬁc local data is potentially non-independent and non-identically distributed (non-IID) (Tong et al., 2020). Thus, the local models are prone to acquire biases such as population bias: the local user may not represent the target overall population; and sampling bias: knowledge transfer from a local model may not be useful to general over larger group (Mehrabi et al., 2019). Contribution:2 To minimize the effect of intrinsic probability bias arising from user-centric (nonIID) training of the local models, we introduce an L2 distance-based weighted distillation. As shown in ﬁg. 1, the global model M with weights w Contribution:2 presents a better weighting mechanism. To further support our contributions, we derive the Lipschitz constant and Rademacher complexitybased generalisation bounds of the unregularized 1-Wasserstein based conﬁdent ensemble distillation. In the end, we empirically demonstrate the strong performance, generally improving upon the baselines, on the three natural language understanding (NLU) tasks, i.e., ﬁne-grained sentiment analysis, emotion recognition in conversation, and natural language inference. Figure 1: Local models acting as multiple teachers trained locally on user data while the global model acts as a student. The global model can only observe predictions of local models. There have been many approaches to FL such as local model parameter averaging (McMahan et al., 2017; Lin et al., 2018) based on local SGD updates. It requires global and local models should have the same model architecture. Another line of work is multiple-source adaptation formulations , w, w, respectively. Contribution:1 provides a better distance calculation while where a learner has access to source domain-speciﬁc predictor and no access to the labeling data from these domains. The expected loss is the mixture of source domains (Hoffman et al., 2018). Even though the formulation is close, our solutions are different as we do not have access to the local or global data domain distribution. In natural language processing, (Hilmkil et al., 2021; Lin et al., 2020; Liu & Miller, 2020) ﬁne-tune Transformer-based architecture in the federated setting on small scale datasets. However, they do not leverage label space semantics and the analysis is restricted to small-scale datasets. Closest to our work aims to improve local client training based on local data heterogeneity (Li et al., 2018; Nedic, 2020; Wang et al., 2019; Khaled et al., 2019; Li et al., 2019b). Knowledge distillation aims to transfer knowledge from a (large) teacher model to a (smaller) student model (Hinton et al., 2015; Bucilu student can imitate the teacher’s behavior (Romero et al., 2014; Tian et al., 2019; Tung & Mori, 2019; Koratana et al., 2019; Ahn et al., 2019). A few works are dedicated to the distillation of the ensemble of teacher models to the student model. This includes logit averaging of teacher models (You et al., 2017; Furlanello et al., 2018; Anil et al., 2018) or feature level knowledge extraction (Park & Kwak, 2019; Liu et al., 2019; Wu et al., 2019). For Contribution:1, we use standard and widely used Entropy-based loss (KL-divergence) as our baseline. We construct two baselines for conﬁdence score calculation (Contribution:2) from the prior works, i.e., logit averaging and weighting scheme based on local model dataset size (McMahan et al., 2017). In this work, we compare the proposed approach with baselines on the three NLU tasks. Traditional divergences, such as KL, ignore metric structure in the label space Y. Optimal Transport (OT) metric can be extremely useful in deﬁning inter-label semantic relationships. OT offers an additional advantage when measures have non-overlapping support (Peyr to our (classiﬁcation) problems, we will focus on discrete measures. Assume Y possess a metric d(·, ·) referred to as ground truth metric. It establishes the semantic similarity between labels. The original OT problem is a linear program attributed to Kantorovich. Let µ masses respectively applied to i ∈ Y that costs C The primal goal is to ﬁnd the plan π ∈ Π(µ, ν) that minimizes the transport cost where n metric d Although research in Wasserstein has been active, there have been challenges in its computation and implementation. The OT problem (equation 1) can be solved with combinatorial algorithms such as simplex-based methods (Courty et al., 2016). The computational complexity is, however, shown to be O((n of dataset increases. The interest in the machine learning community took off after Cuturi’s seminal paper (Cuturi, 2013). Rather than work with pure OT (Wasserstein) distances, we will restrict our attention to plain regularized OT, i.e, vanilla Sinkhorn distances. The distance can be efﬁciently solved by iterative Bregman Projections (Benamou et al., 2015). ˇa et al., 2006). Given the output logit/softmaxed valued of the teacher model, the , i.e., an element of the cost matrix C. We denote Frobenius inner product by h·, ·i. Π(µ, ν) = {π ∈ (R)π= µandπ= ν} = |Y| and n= |Y|. In our problem, Y= Y= Y. When the cost is given in terms of (·, ·) also known as Wasserstein distance (Bogachev & Kolesnikov, 2012). +n)nnlog(n+n)) at best (Ahuja et al., 1988), and thus, the utility lessens as the size Deﬁnition 3.1. Vanilla Sinkhorn Distance where, D loss function and thus is a computational advantage in computing gradients (Luise et al., 2018; Peyr et al., 2019; Feydy et al., 2019). As ε → 0 The main participants in this framework are: 1) global model M The central server can retrieve back a local model’s prediction for a particular input. Thus, global model training can beneﬁt from the hypotheses of local models, denoted by h eters set M preoccupied with the knowledge applicable across the domains. To enhance the user service, the server distills the knowledge from h fer happens with the assistance of a transfer set. Transfer set is the set of unlabeled i.i.d. samples used to learn global model parameters θ. It creates a crucial medium to transfer the knowledge from the local models to the global model. To facilitate the knowledge transfer, we consider the (noisy) soft-labels are obtained from h Since only the h global models can have heterogeneous architectures. This is useful when client devices at certain locations do not have enough resources to run and ﬁt over large models. For demonstration, we consider a user application that performs sentiment classiﬁcation task on user-generated text x y∈ Y, where X denotes the input space of all possible text strings and label space Y = {1 (strong negative), 2 (weak negative), 3 (neutral), 4 (weak positive), 5 (strong positive)}. In this work, all the hypotheses are of the form h : X 7→ ∆ tion on the set of labels Y. We propose a learning algorithm that runs on the central server to ﬁt M’s parameters θ by receiving predictions such as (softmaxed) logits from h of generality, the goal is to search for a hypothesis h the loss is deﬁned as where T the sample-speciﬁc weight assigned to the k the functions T Π(µ, ν) = {π ∈ (R)π= µandπ= ν} (π, µ⊗ν) =Pπlog− π+ µν. Entropic regularisation convexiﬁes the , K = {1, . . . , K}. We denote the set {M}by M. the central application server. or the private user-generated data. The global model on the server Mis generally is shared with Mto ﬁnd a better hbased on user feedback, the local models and (·, ·) is the discrepency between the two probability measures as its arguments; W(.) is (·, ·) and W(.) which are crucial for our proposed FD algorithm. For entropy-based loss, we adopt KL divergence. As discussed in section 3.1, we employ Sinkhorn distance to implement OT-based loss. For a text input x between the probability output of global model h the sample-wise distance is computed between h h. A simple approach to ﬁt the global hypothesis h user-speciﬁc hypotheses, i.e., W The user-generated local datasets are potentially non-IID with respect to the global distribution and possess a high degree of class imbalance (Weiss & Provost, 2001). As each local model M trained on samples from potentially non-IID and imbalance domains, they are prone to show skewed predictions. The unweighted distillation tends to transfer such biases. One might wonder for a given transfer set sample, which local model’s prediction is reliable?. Although an open problem, we try to answer this question by proposing a local model (teacher) weighting scheme. It calculates the conﬁdence score of a model’s prediction and performs weighted distillation—weights being in positive correlation with the local model’s conﬁdence score. Next, we deﬁne the conﬁdence score. Conﬁdence score (L2) For a given sample x from the transfer set, the skew in a local model’s prediction h(x) can help us determine the conﬁdence score (W (·) in eq. (4)) with which it can transfer its knowledge to the global model. However, a model can show skew due to training on an imbalance dataset or chosen capacity of the hypothesis space which can potentially cause a model to overﬁt/underﬁt (Caruana et al., 2001). For instance, a model has learned to misclassify negative sentiment as strongly negative samples owing to a high confusion rate. Such models are prone to show inference time classiﬁcation errors with highly skewed probabilities. Thus, conﬁdence scoring based on the probability skew may not be admissible. Hence, we incorporate L2 conﬁdence. For a given sample, we deﬁne the model’s L2 conﬁdence score W output probability distribution from the probability bias B. We deﬁne probability bias B of a local model as the expected value of prediction when a model h receives noise (random text) at the input. Let h(x) ∈ Y denotes the predicted distribution of a model for an input text x: As shown in ﬁg. 2 for a three-class classiﬁcation, the equidistant distributions lie on an arc with center at B. Points with high conﬁdence score lie on distant arcs. As radius of the arc increases, majority of its portion lies towards the high value of p is biased against since b (pin the ﬁgure). Moreover, the maximum conﬁdence score is achieved at the vertex p Proposition 5.1. From a given point B in a ksimplex, point with the highest conﬁdence lies on one of its vertices. Proof. First, we analyse the case of a 2-simplex deﬁned in a three-dimensional Euclidean space. Table 1: Data statistics and performance of local models. For SA and ERC tasks, score denotes Macro F-1 performance metric, while it denotes Accuracy metric for NLI. Let f= gram can be formulated as max{f on the axes forms a closed and bounded feasible region. Thus, from extreme value theorem, there exist absolute maximum and minimum. f attains its minimum at p = b, which is also the critical point of f 1-simplices (line segments) {p simplex p b+ (1 − b other line segments, the complete set of boundary values of f where k = b maximum of f a probability simplex in higher dimensions. As shown above, each iteration of a lower dimensional simplex will return vertices as the point of maxima in the end. L2 conﬁdence is a proper distance metric and computationally stable. The distance metric is invariant to translation in the Euclidean space. Thus, W prediction from its intrinsic probability bias B weights for both distillation methods, i.e., Sinkhorn and Entropy. Let the samples S = {(x transfer set and h s ◦ H a scalar (logit) value for each label. Assuming vanilla Sinkhorn with ε → 0 property for 1-Wasserstein. Theorem 5.2. If the global loss function (as in eq. (4)) uses unregularized 1-Wasserstein metric between predicted and target measure, then for any δ > 0, with probability at least 1-δ where R the hypothesis space H case of SA, C Ignoring the critical point which gives the minima and perpendicular drawn from b to the line segment. P(p− b), the quadratic pro- . Now, we need to ﬁnd its value on the boundary points contained in the set of + p= 1, p= 0, the values of fat its end points that are (1 − b)+ b+ band )+ b, one of which is maxima of f attained over the 1-simplex. Similarly for the + b+ b+ 1, occurring at P = (1, 0, 0), (0, 1, 0) and (0, 0, 1), respectively. Thus, the will lie on i-axis such that b= min (b, b, b). This proof can be generalized for , i.e., composition of softmax and a hypothesis H: X 7→ R, that maps input text to (x), h(x))≤ infˆEL(h(x), h(x))+ 32 × |Y| × R(H) + 2C|Y||Y|log1/δ2N(5) (H), decays with N , denotes Rademacher complexity Bartlett & Mendelson (2002) of = 4, |Y| = 5. The expected loss of the empirical risk minimizer happroaches the best achievable loss for H relegated to the Appendix. To comprehensively analyse the importance of OT over entropy for the task with intrinsic label similarity, we introduce a new performance metric that evaluates a model based on the semantic correctness of its predictions. During the evaluation, most common metrics (such as accuracy and F1) observe the label with the highest logit (or probability) against ground truth, hence, ignore the overall distribution. However, for tasks with label space semantics, it can be of great importance. Thus, we deﬁne a new performance metric—Semantic Distance (SD)—that measures the semantic closeness of the output distribution against the ground truth. Given a label coordinate space, SD is deﬁned as the mean Euclidean distance of expected output from the ground truth label. For instance, given the sentiment classes {1,2,3,4,5}, the probability scores of two models m1 and m2 assigns to a strongly negative text input be {0.2, 0.7, 0.033, 0.033, 0.033} and {0.4, 0.1, 0.1, 0.1, 0.3}, respectively. The argmax output of m2 is correct. However, even when the argmax output of m1 is incorrect, the expected output of m1, i.e., 1.97 is (Euclidean) closer to the ground truth label 1 than m2, i.e., 2.80, and thus more semantically accurate. A low score denotes more semantically accurate prediction. The lowest possible value of SD is 0 while the highest possible value depends on the number of labels and their map in the semantic space. For datasets with class imbalance, such as SA task in this work Table 1, we ﬁrst calculate label-wise SD values and compute their mean. Analysis—To demonstrate the usefulness of the SD metric, for the SA, we draw box plots of pretrained global models via Entropy (KLdivergence) and Sinkhorn-based losses. The pretraining methodology is described in section 6. As shown in ﬁg. 4, we observe the median SD of Sinkhorn (green box, red line) is closer to the ground truth sentiment classes— 1,2,3, and 5 as compared to median SD of Entropy (blue box-red line). Similarly, the means (black diamond), the ﬁrst quartile (25% of the samples), and the third quartile (75% of the samples) for the Sinkhorn-based model are relatively closer to the ground truth. Baselines —We setup the following baselines for a thorough comparison between Sinkhon and Entropy-based losses. Let [Method] be the placeholder for Sinkhorn and Entropy. [Method]-A denotes unweighted distillation of local models (section 5.1), i.e., W In [Method]-D, W speciﬁc conﬁdence (weights) as the distance of output from the uniform distribution over labels. For each sample, [Method]-E computes weight of k probability bias B Tasks We set up the three natural language understanding tasks with intrinsic similarity in the label space: 1) ﬁne-grained Sentiment Analysis (SA); 2) Emotion Recognition in Conversation (ERC); and 3) Natural Language Inference (NLI). NLI is the task of determining the inference relation between two texts. The relation can be entailment, contradiction, or neutral (MacCartney & Manning, 2008). For a given transcript of conversation, the ERC task aims to identify the emotion of each utterance from the set of pre-deﬁned emotions (Poria et al., 2019). For our experiments, we choose the ﬁve most common emotions that are sadness, anger, surprise, and happiness, and no emotion. Datasets For the SA task, we use four large-scale datasets: 1) Toys: toys and games; 2) Cloth: clothing shoes and jewelry; 3) Cell: cell phones and accessories; 4) Food: Amazon’s ﬁne food reviews, speciﬁcally curated for the ﬁve-class sentiment classiﬁcation. For the transfer set, we use grocery and gourmet food (104,817 samples) and discard the labels (He & McAuley, 2016). Each dataset consists of reviews rated on a scale of 1 (strong negative) to 5 (strong positive). Similarly, for ERC, we collect three widely used datasets: DyDa: DailyDialog (Li et al., 2017), IEMOCAP: interactive emotional dyadic motion capture database (Busso et al., 2008), and MELD: Multimodal EmotionLines Dataset (Poria et al., 2018). To demonstrate our methodology, we partition the DyDa dataset into four equal chunks. DyDa Dropping the labels from DyDa et al., 2015) as global dataset and MNLI (Williams et al., 2017) as local dataset. We split the latter across its ﬁve genres, which are, ﬁction (Fic), government (Gov), telephone (Tele), travel (Trv), and Slate. This split assists in simulating distinct user (non-IID samples) setup. We use ANLI dataset (Nie et al., 2020) as transfer set. Architecture We set up a compact transformer-based model used by both global and local models (ﬁg. 1), although, the federation does not restrict both the local and model conﬁgurations to be the same. The input is fed to the pretrained BERT-based classiﬁer (Turc et al., 2019; Devlin et al., 2018). Thus, we obtain probabilities with support in the space of output labels, i.e., Y. We keep all the parameters trainable, hence, BERT will learn its embeddings speciﬁc to the classiﬁcation task. For NLI task, we append premise and hypothesis at input separated by special token [SEP] token, followed by a standard classiﬁcation setup. Training local models To compare Sinkhorn-based distillation with baselines, ﬁrst, we pretrain local models. Since cross-entropy (CE) loss is less computationally expensive as compared to OT, we use CE for local model training. For all model training, we tuned hyperparameters for all the models separately and chose the model that performs best on the validation dataset. The data statistics and performances of local models on individual tasks are shown in Table 1. Training a global model We make use of transfer set (unlabeled) samples and obtain noisy labels. For a text sample in transfer set, eq. (4) aims to ﬁt a global model to the weighted sum of noisy predictions of the local models. To retain the previous knowledge of a global model, we adapt learning without forgetting paradigm. To incorporate this, we store predictions of the pretrained global model on the transfer set and perform its distillation along with local models. Label-space We deﬁne label semantic spaces for the three tasks. As shown in ﬁg. 3, we assign sentiment labels a one-dimensional space. For the ERC task, we map each label to a two-dimensional valence-arousal space. Valence represents a person’s positive or negative feelings, whereas arousal denotes the energy of an individual’s affective state. As mentioned in (Ahn et al., 2010), anger (-0.4, 0.8), happiness (0.9, 0.2), no emotion (0, 0), sadness (-0.9, -0.4), and surprise (0.4, 0.9). The cost (loss) incurred to transport a mass from a point p to point q is C deﬁne a three dimensional coordinate with entailment (1, 0, 0), contradiction (0,0,1) and neutral (0.5, 1, 0.5). The cost C higher than it is to neutral. It is noteworthy that for this task, we perform a manual search to identify label coordinate space and transportation cost. Table 2: Fine-grained SA task: Macro F1 and Semantic Distance. Table 2, Table 3, and Table 4 show performance, i.e., Macro-F1 (or Accuracy) score and Semantic Distance of global models predictions from ground truth. Evaluations are done for ﬁne-tuned (after distillation) global model with respect to the test sets of both local and global datasets. The testing over local datasets will help us analyse how well the domain generic global model performs over the individual local datasets and the testing over the global dataset is to make sure there is no catastrophic forgetting of the previous knowledge. For the SA task, we observe the global models trained from Sinkhorn distillation of local models is amongst the best F1 scores on all the local domains. For the most simplistic baseline, i.e., unweighted distillation, Sinkhorn-A consistently gives a higher F1 score as compared to Entropy-A. Even though the performance of Sinkhorn-A is close to Sinkhorn-E, the latter is more semantically accurate as depicted by corresponding SA scores. This shows the efﬁcacy of weighting local models’ prediction with L2 conﬁdence. Sinkhorn-D based global model training gives the best F1 and SA scores on global datasets. We postulate this is due to a large number of global samples as compared to local dataset sizes that bias the distillation weights W (h better on the global dataset. It can be observed that this comes at the cost of poor performance on the local datasets. For the ERC and NLI tasks, although it is hard to ﬁnd a model that shows consistently better F1 or accuracy scores (one cause could be the small small size of local and global datasets), we observe the SD score of Sinkhorn-E is, in general, better amongst the baselines. As observed for the SA task in Entropy-D and Sinkhorn-D settings, since the SNLI dataset is bigger, the distillation forces global models to perform better on the global dataset. It is seen to come at the cost of degraded performance on the other (local) datasets. Comparing Table 2, Table 3, and Table 4 all together, for the three tasks with intrinsic similarity in the label space, we observe Sinkhorn-based loss are in general better than KL-divergence, i.e., an entropy-based loss. Moreover, we observe L2 distance gives better scores amongst the individual loss-speciﬁc groups. We also notice that in certain tasks, where it is difﬁcult to identify the best model, one can refer to the semantic distance. Besides this, as compared to other baselines, empirical observations suggest that Sinkhorn-E (our contribution) works well for the large-scale SA datasets, hence potentially scalable. In this work, we introduced an algorithm for efﬁcient federated distillation of natural language understanding from client devices to the central (global) model. We deﬁned a new Euclidean distancebased metric to compute a local model’s intrinsic probability bias. We analysed theoretical generalization bounds of empirical risk of the proposed loss function. In the end, we demonstrated the efﬁcacy of the novel approach on the three NLU tasks of ﬁne-grained sentiment analysis, emotion recognition in conversation, and natural language inference.