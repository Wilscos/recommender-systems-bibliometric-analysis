{bencheng.ybc,pengjie.wpj,victorlanger.zk,kuang-chih.lee,xiyu.xj,bozheng}@alibaba-inc.com,lwsaviola@163.com Embedding learning for categorical features is crucial for the deep learning-based recommendation models (DLRMs). Each feature value is mapped to an embedding vector via an embedding learning process. Conventional methods congure a xed and uniform embedding size to all feature values from the same feature eld. However, such a conguration is not only sub-optimal for embedding learning but also memory costly. Existing methods that attempt to resolve these problems, either rule-based or neural architecture search (NAS)-based, need extensive eorts on the human design or network training. They are also not exible in embedding size selection or in warm-start-based applications. In this paper, we propose a novel and eective embedding size selection scheme. Specically, we design an Adaptively-Masked Twins-based Layer (AMTL) behind the standard embedding layer. AMTL generates a mask vector to mask the undesired dimensions for each embedding vector. The mask vector brings exibility in selecting the dimensions and the proposed layer can be easily added to either untrained or trained DLRMs. Extensive experimental evaluations show that the proposed scheme outperforms competitive baselines on all the benchmark tasks, and is also memory-ecient, saving 60% memory usage without compromising any performance metrics. • Information systems → Recommender systems. Recently, deep learning-based recommendation models (DLRMs) have been widely adopted in many web-scale applications such as recommender systems [2,5,9–11,16]. One of the main parts of DLRMs is the embedding layer, which exploits the categorical features. A standard embedding layer maps the categorical feature Figure 1: Comparison among existing methods and ours. to an embedding space [2,5,16]. Specically, given a feature eld𝐹 and let its vocabulary size be|𝐹 |, each feature value𝑓∈ 𝐹is mapped to an embedding vector by an embedding matrix𝑊 ∈ R, where 𝐷 is a predened embedding dimension. However, the above standard method can lead to two problems. First, in real applications, dierent feature values in the same feature eld can have signicantly dierent frequencies. For highfrequency feature values, it is necessary to use a suciently large embedding dimension to express rich information. Meanwhile, assigning too large embedding dimensions to low-frequency feature values is prone to over-tting issues. Therefore, a xed and uniform embedding dimension for all the feature values in a feature eld can undermine eective embedding learning for dierent feature values. Second, storing the embedding matrix with a xed and uniform dimension may result in a huge memory cost [14,17,18]. A exible dimension assignment is needed to reduce the memory cost. There are some existing works trying to learn unxed and nonuniform embedding dimensions for dierent feature values. They can be primarily divided into two categories. (1) Rule-based methods adopt human-dened rules, typically according to the feature frequencies, to give dierent embedding dimensions to dierent feature values [4] (see Fig. 1 (b) for an example). The problem with this category of methods is that they heavily rely on human knowledge and human labor. The resulting rough dimension selection for groups of feature values can often lead to poor performance (see Section 3.2). (2) Neural architecture search (NAS)-based methods use NAS techniques to search from several candidate embedding dimensions to nd a suitable one for each feature value [8,12,18,19] (see Fig 1 (c) for an example). These methods require careful design of the search space and training-searching strategies. The search space is usually limited to a restricted set of discrete dimensions. Besides, both categories of methods mentioned above require training (i.e., embedding learning) from scratch. However, in real applications, there may exist some embedding matrices already trained with a huge amount of data. Such embedding matrices can be utilized for warm starting (see Section 3.4). Unfortunately, existing methods are not friendly to accommodate such a warm start mechanism. In this paper, we propose a novel and eective method to select proper embedding dimensions for dierent feature values. The basic idea is to add an Adaptively-Masked Twins-based Layer (AMTL) on top of the embedding layer. Such a layer can adaptively learn a mask vector to mask the undesired dimension of the embedding vector for each feature value. The masked embedding vectors can be taken as the vectors with adaptive dimensions and are fed into the subsequent processes in DLRMs. This method exhibits some nice properties. First, it is eective for embedding learning because the embedding dimension of dierent feature values can be learned and adjusted in continuous integer space with sucient exibility without human interaction or specic NAS design (see Section 3.2). Second, it is ecient since a memory-ecient model can be built by adjusting the embedding dimension (see Section 3.3). Third, the parameters of the embedding matrix can be eciently trained with the warm start mechanism (see Section 3.4). We summarize our contributions as follows: (1) We propose a novel embedding dimension selection method that completely removes the necessity of human rules or NAS architectures to facilitate adaptive dimension learning. (2) The proposed method (AMTL) can be easily applied in trained DLRMs to facilitate a warm start. The twins-based architecture successfully tackles the sample unbalance problem. (3) Extensive experimental results demonstrate that the proposed method outperforms strong baseline methods. The nice properties of AMTL helped us reduce memory cost by up-to 60% without compromising any performance metrics, and can further improve the performance by the warm start mechanism. We rst recall a standard embedding layer which can be expressed as𝑒= 𝑊𝑣where𝑣is a one-hot vector for the feature value 𝑓,𝑊 ∈ Rrefers to the embedding table and𝑒∈ Ris the embedding vector of𝑓. Then we dene a mask vector𝑚∈ {0, 1} for 𝑓. This mask vector should satisfy where𝑘∈ [0, 𝐷 − 1]is a learnable integer parameter which is inuenced by the frequency of𝑓. Then, to allow dierent𝑓can adjust its embedding dimension, the basic idea is that we can use the mask vector 𝑚to mask the embedding vector 𝑒, i.e.,^𝑒= 𝑚⊙ 𝑒 where⊙represents the element-wise multiply. Since the value whose index is larger than𝑘in^𝑒is zero, the masked embedding vector^𝑒can be taken as an embedding vector where the embedding dimension is adaptively adjusted by the mask vector, and the rst 𝑘+1dimensions (i.e., from 0-th to𝑘-th dimension) of𝑒is selected. Memory Saving.When storing^𝑒, we can simply drop the zero values in^𝑒to save memory and when fetching the stored vector, we can simply re-pad zero values to recover^𝑒. Embedding Usage.When embedding vectors are assigned with dierent dimensions, all of the existing methods [4,8,12,18,19] have to design an additional layer to unify these vectors to a same length to t the following uniform MLP layers in DLRMs. Unlike these methods, our method does not need any additional layers since the masked embedding vectors^𝑒have the same length by zeros paddings, and can be directly fed into the following layers. Why select rst 𝑘+ 1 dimensions?In this paper, We take the strategy about selecting rst𝑘+ 1dimensions (i.e., from 0-th to 𝑘-th dimension) of𝑒as an example and others (e.g., selecting last 𝑘+ 1dimensions) are also allowed. One should keep in mind is that the select strategy should follow some rules. In other words, randomly selecting𝑘+ 1dimensions of𝑒is not a good strategy because we can hardly directly drop and recover these zeros values in^𝑒to save memory due to the random distribution of zeros values in^𝑒, and it also prevents the model to characterize each feature value since the same feature value may be mapped to dierent embedding vectors due to the random selection. Adaptively-Masked Twins-based Layer (AMTL) is designed to generate a mask vector𝑚in Eq 1 for each feature value𝑓. The framework of AMTL is shown in Fig 2. 2.2.1 Input and Output.Input:Since𝑚is required to be adjusted by the feature frequency, to allow AMTL to have such frequency knowledge, we take the frequency attribute (e.g., the appear times in history, the frequency rank in this feature eld and so no) of𝑓 as the input of AMTL. The input sample is denoted as𝑠∈ 𝑅and𝑧 is the input dimension.Output:The output of AMTL is a one-hot vector (called selection vector) to represent 𝑘in Eq 1. 2.2.2 Architecture. We propose a twins-based (i.e., two branches) architecture and each branch is an Adaptively-Masked Layer (AML). Note parameters of the two branches are not sharing. Both of AML is a multilayer perceptron:ℎ= 𝜎 (𝑊ℎ+ 𝑏)whereℎis the frequency vector,𝑊and𝑏are the parameters of the𝑙-th layer,𝜎 is an activation function andℎ∈ Ris the output of the last layer. The motivation of such twins design is that if we only take a single branch (i.e., AML), the parameters update of AML will be dominated by the high-frequency feature values due to the unbalanced problem. Specically, since high-frequency feature values appear more times in samples, the major part of the input sample of AML represents high-frequency vectors. Then, the parameters of AML may be heavily inuenced by the high-frequency vectors and AML may blindly select large embedding dimensions. Hence we design twins-based architecture to address this problem where the two branches (i.e., h-AML and l-AML) are used for high- and low- frequency samples respectively. In this way, the parameters of the l-AML will not be dominated by high-frequency samples and can give an unbiased decision. Weighted Sum.However, one challenge is that we can hardly give a threshold to dierentiate the high- and low- frequency samples to feed dierent samples to dierent branches. Hence, we propose a soft decision strategy. Specically, we dene the frequency value of𝑓as𝑞which refers to the present times of𝑓in history, and feed the input sample𝑠into h-AML and l-AML respectively. A weighted sum is applied on the𝐿-th outputs (i.e.,ℎ∈ R and ℎ∈ R) of h-AML and l-AML, i.e., where𝛼∈ [0, 1]is the weight score which is inuenced by𝑞. 𝑛𝑜𝑟𝑚operation normalizes𝑞to a standard normal distribution which allows𝛼is distributed smoothly around1/2. Otherwise, all 𝛼may be close to one without the𝑛𝑜𝑟𝑚operation. In this way, for the samples with high-frequency, the correspondingℎ is dominated byℎdue to a large𝛼. Then the parameters of h-AML are mainly updated during back-propagation and vice versa. Hence, AMTL can adjust the gradients of h-AML and l-AML to address the unbalanced problem. Note here we only give an example to calculate the weight value𝛼, other ways are also allowed as long as the produced 𝛼has similar properties. Then we apply softmax function on ℎ, i.e., where𝑜∈ Rrefers the probability to select dierent embedding dimension of𝑓, and𝑜is the𝑝-th element of𝑜. The selection vector can be obtained by Then the corresponding mask vector can be generated by where𝑀 ∈ Ris a pre-dened mask matrix and𝑀= 1when 𝑗 ≤ 𝑖otherwise𝑀= 0. Then the masked embedding^𝑒can be obtained by𝑚. Note in practice, we usually apply dierent AMTLs on dierent important feature elds (e.g., User ID and Item ID) and the parameters of these layers are not sharing for the purpose of eld awareness. 2.2.3 Relaxation. However, the problem is that the learning process of AMTL is non-dierentiable due to the discrete process in Eq 5. It means the parameters of AMTL cannot be directly optimized by a stochastic gradient descent (SGD). To address this problem, we relax𝑡to a continuous space by temperated softmax [6,7,13]. Concretely, the 𝑝-th element of 𝑡can be approximated as 𝑡≈^𝑡= 𝑒𝑥𝑝 (ℎ/𝑇 )/Í(𝑒𝑥𝑝 (ℎ where𝑇is the temperature hyper-parameter. When𝑇 → 0, this approximation becomes exact. Since^𝑡is a continuous vector with dierentiable process, SGD can be naturally applied. Hence, instead of learning the discrete vector 𝑡, we learn^𝑡to approximate 𝑡. However, there exists an information gap between training and inference phases when using temperature softmax. Specically, we use the vector^𝑡for training. While in inference, we only use the discrete vector𝑡. To close this gap, inspired by the idea StraightThrough Estimator (STE) [1], we rewrite 𝑡as where stop_gradient is used to prevent the gradient from backpropagation through it. Since the forward pass is not aected by stop_gradient,e𝑡= 𝑡during this phase. For the back-propagation, it avoids the non-dierentiable process by stop_gradient. Data Sets.(1) MovieLensis a user review data about movies and is collected from MovieLens website. There are a total of 1,000,209 records. (2) IJCAI-AACis collected from a sponsored search in Ecommerce. There are a total of 478,138 records. (3) Taobao Dataset is an industrial dataset which is constructed from Taobao. There are a total of 50 billion around records. Baselines.We consider dierent kinds of state-of-the-art embedding methods as baselines (1) Standard: traditional Fixed-based Embedding (FBE). (2) Rule-Based: MDE [4] divides dierent feature values into several blocks by their frequency, and assigns dierent embedding dimensions for dierent blocks by rules. (3) NAS-Based: AutoEmb [19] adopts NAS to select embedding dimensions among some candidate embedding dimensions for dierent feature values. Setting.The maximal embedding dimension and the DLRM boneskeleton of all methods are set the same. The dimension selection strategy is applied to the feature elds which are related to the user and item property (e.g., User ID and Item ID). For AutoEmb, the candidate dimension list is set smoothly by following the original paper [19]. The temperature 𝑇 in Eq 7 is set by grid search. Here, we compare our method AMTL with baselines on clickthrough rate (CTR) prediction tasks and take the AUC [3] score as the metric. Note a slightly higher AUC at0.1%-levelis regarded as signicant for the CTR task [15,20]. As shown in Table 1, we can conclude that: (1) Compared with FBE, AMTL can archive better performance in all datasets. It shows that adopting an unxed embedding dimension can improve the model performance. (2) Compared with the rule-based method (i.e., MDE), AMTL outperforms MDE. Besides, MDE only obtains similar performance with FBE. It indicates a rough human rule on dimension selection cannot always guarantee an improvement. (3) For the NAS-based method (AutoEmb), AMTL also archives better performance. It demonstrates that AMTL adopts a more suitable scheme i.e., selecting a dimension from a continuous integer space. Here, we compare the memory cost of dierent methods. Since the memory size of the embedding matrix is in direct proportion to the dimension [14,17], for simplicity, the averaged dimension (i.e., Avg(Dim)) are reported. We take the feature eld "User ID" in Taobao as an example (others can have similar conclusions), and its’ maximal dimension is set as 300. Table 2 shows the results. We also show the Avg(Dim) ratio compared with FBE. We can nd that (1) Compared with FBE, all the dimension selection methods can save memory size by reducing the embedding to a suitable dimension. (2) Since AMTL allows a more exible dimension selection in a continuous integer space, it reduces memory cost more signicantly by around 60 %. Here, we conduct experiments to evaluate the warn start on the dataset Taobao which is close to the industrial and real system. There are two kinds of parameters in DLRM, i.e., the parameters of embedding matrix and hidden layers. In the warm start setting, for existing dimension selection methods, we initialize the parameters of hidden layers by loading the parameters from the online model in Taobao, and the parameters of embedding matrix are randomly initialized due to the inability on the warm start of embedding matrix. While, since AMTL and FBE can warm start the parameters both of the embedding matrix and the hidden layers, we load both of them from the online model for these two methods. The results are shown in Table 3. The results of random start are also provided. We can nd that compared with MDE and AutoEmb, AMTL can perform better in the warm start setting. Specically, compared with the best baseline AutoEmb, the gain of AMTL is 1.6% in the warm start manner, which is 5×times larger than the gain in the random start manner. Furthermore, due to the inability for the warm start of the embedding matrix, MDE and AutoEmb even perform worse than the standard full embedding. It demonstrates that the dimension selection scheme designed in AMTL is a more wise and exible way in real application systems. Here we analyze whether AMTL can give suitable dimensions for dierent feature values. Specically, we divide the feature value into 7 groups (i.e.,𝐺, 𝑖 ∈ {0, 1, 2, ..., 6}) by frequency and the average frequency of dierent groups is increased from𝐺to𝐺. Note there are too many feature elds in dierent datasets, we take the feature eld "User ID" in Taobao as an example, and others have similar results. The averaged embedding dimension in dierent groups is reported in Fig 3 (a). It shows that when the frequency increases (i.e., from𝐺to𝐺), the selected average dimension is increased. Figure 3: Dimension section of AMTL and AML. It indicates AMTL can assign suitable embedding dimensions to dierent frequency feature values adaptively. Here, we conduct an ablation study on the twins-based architecture and STE. Due to the limited space, the results on IJCAI-AAC are reported and similar conclusions can be found from other datasets. Evolution on twins-based architecture.We compared the AUC score between AMTL and AML (only a single branch) on CTR task. From Table 4, although both AML and AMTL perform better than FBE, AMTL archives a higher performance. It indicates twins-based architecture plays an important role in feature learning. Besides, similar to Section 3.5, we also visualize the dimension selection of AML in Fig 3 (b). We can nd that AML only successfully gives suitable dimensions in high-frequency groups (i.e.,𝐺to𝐺). In low-frequency groups, due to the unbalanced problem, it blindly gives high dimensions for low-frequency values. And the lower the frequency, the worse it is. Evolution on STE.Here, we analyze the eectiveness of STE, and implement a variant of AMTL without STE, denoted as AMTL-nSTE. From Table 4, compared with AMTL-nSTE, AMTL can archive better performance. It demonstrates the usefulness to bridge the information gap between training and inference phases by STE. In this section, we conduct experiments to report the time cost per epoch of dierent methods on IJCAI-AAC (similar conclusions can be found in other datasets). As shown in Table 5, we nd that compared with FBE, the dimension selection methods need more time to train the model per epoch due to the dimension selection processes. During inference, we can directly look up the learned embedding table which has adaptive dimensions without the process of dimension selection to save time. Traditional embedding learning methods usually adopt a xed dimension for all features which may cause problems in space complexity and performance. To address this problem, we propose a novel dimension selection method called AMTL which produces a mask vector to mask the undesired dimensions for dierent feature values. Experimental results show that the proposed method can archive the best performance on all tasks especially in the case of embedding warm start, give a suitable dimension for dierent features and save memories at the same time.