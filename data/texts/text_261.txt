Algorithmic recommendations and decisions have become ubiquitous in today’s society. Many of these and other data-driven policies, especially in the realm of public policy, are based on known, deterministic rules to ensure their transparency and interpretability. For example, algorithmic pre-trial risk assessments, which serve as our motivating application, provide relatively simple, deterministic classiﬁcation scores and recommendations to help judges make release decisions. How can we use the data based on existing deterministic policies and learn new and better policies? Unfortunately, prior methods for policy learning are not applicable because they require existing policies to be stochastic rather than deterministic. We develop a robust optimization approach that partially identiﬁes the expected utility of a policy, and then ﬁnds an optimal policy by minimizing the worst-case regret. The resulting policy is conservative but has a statistical safety guarantee, allowing the policy-maker to limit the probability of producing a worse outcome than the existing policy. We extend this approach to common and important settings where humans make decisions with the aid of algorithmic recommendations. Lastly, we apply the proposed methodology to a unique ﬁeld experiment on pre-trial risk assessment instruments. We derive new classiﬁcation and recommendation rules that retain the transparency and interpretability of the existing instrument while potentially leading to better overall outcomes at a lower cost. Keywords: algorithm-assisted decision-making; observational studies; optimal policy learning; randomized experiments; robust optimization Algorithmic recommendations and decisions are ubiquitous in our daily lives, ranging from online shopping to job interview screening. Many of these algorithm-assisted, or simply data-driven, policies are also used for highly consequential decisions including those in the criminal justice system, social policy, and medical care. One important feature of such policies is that they are often based on known, deterministic rules. This is because transparency and interpretability are required to ensure accountability especially when used for public policy-making. Examples include eligibility requirements for government programs (e.g., Canadian permanent residency program; Supplemental Nutrition Assistance Program or SNAP, Center on Budget and Policy Priorities, 2017) and recommendations for medical treatments (e.g., MELD score for liver transplantation, Kamath et al., 2001). opportunity to learn new policies that improve on the status quo. Unfortunately, prior approaches for policy learning are not applicable because they require existing policies to be stochastic, typically relying on inverse probability-of-treatment weighting. To address this challenge, we propose a robust optimization approach that ﬁnds an improved policy without inadvertently leading to worse outcomes. To do this, we partially identify the expected utility of a policy by calculating all potential values consistent with the observed data, and ﬁnd the policy that maximizes the expected utility in the worst case. The resulting policy is conservative but has a statistical safety guarantee, allowing the policy-maker to limit the probability for yielding a worse outcome than the existing policy. function of restrictions imposed on the class of outcome models as well as on the class of policies. After developing the theoretical properties of the safe policy in the population, we show how to empirically construct the safe policy from the data at hand and analyze its statistical properties. We then provide details about the implementation in several representative cases. We also consider two extensions that directly address the common settings, including our application, where a deterministic policy is experimentally evaluated against a “null policy,” and humans ultimately make decisions with the aid of algorithmic recommendation. The availability of experimental data weakens the required assumptions while human decisions add extra uncertainty. American criminal justice system. The goal of a pre-trial risk instrument is to aid judges in deciding which arrestees should be released pending disposition of any criminal charges. Algorithmic recommendations have long been used in many jurisdictions to help judges make release and sentencing decisions. A well-known example is the COMPAS score, which has ignited controversy (e.g., Angwin et al., 2016; Dieterich et al., 2016; Rudin et al., 2020). We analyze a particular pre-trial risk assessment instrument used in Dane county, Wisconsin, that is diﬀerent from the COMPAS score. This risk assessment instrument assigns integer classiﬁcation scores to arrestees according The large amounts of data collected after implementing such deterministic policies provide an We formally characterize the gap between this safe policy and the infeasible oracle policy as a Our motivating empirical application is the use of pre-trial risk assessment instruments in the to the risk that they will engage in risky behavior. It then aggregates these scores according to a deterministic function and provides an overall release recommendation to the judge. Our goal is to learn new algorithmic scoring and recommendation rules that can lead to better overall outcomes while retaining the transparency of the existing instrument. Importantly, we focus on changing the algorithmic policies, which we can intervene on, rather than judge’s decisions, which we cannot. assessment (Greiner et al., 2020; Imai et al., 2020). Our analysis focuses on two key components of the instrument: (i) classifying the risk of a new violent criminal activity (NVCA) and (ii) recommending cash bail or a signature bond for release. We show how diﬀerent restrictions on the outcome model, while maintaining the same policy class as the existing one, change the ability to learn new safe policies. We ﬁnd that if the cost of an NVCA is suﬃciently low, we can safely improve upon the existing risk assessment scoring rule by classifying arrestees as lower risk. However, when the cost of an NVCA is high, the resulting safe policy falls back on the existing scoring rule. For the overall recommendation, we ﬁnd that noise is too large to improve upon the existing recommendation rules with a reasonable level of certainty, so the safe policy retains the status quo. Related work. Recently, there has been much interest in ﬁnding population optimal policies from randomized trials and observational studies. These methods typically use either inverse probability weighting (IPW) (e.g. Beygelzimer and Langford, 2009; Qian and Murphy, 2011; Zhao et al., 2012; Zhang et al., 2012; Swaminathan and Joachims, 2015; Kitagawa and Tetenov, 2018; Kallus, 2018) or augmented IPW (e.g. Dudik and Langford, 2011; Luedtke and Van Der Laan, 2016; Athey and Wager, 2021) to estimate and optimize the expected utility of a policy — or a convex relaxation of it — over a set of potential policies. that generated the data is randomized — or is stochastic in the case of observational studies — with non-zero probability of assigning any action to any individual. Kitagawa and Tetenov (2018) show that with known assignment probabilities the regret of the estimated policy relative to the oracle policy will decrease with the sample size at the optimal n probabilities without unmeasured confounding, Athey and Wager (2021) show that the augmented IPW approach will achieve this optimal rate instead. Cui and Tchetgen Tchetgen (2021) also use a similar approach to learn optimal policies in instrumental variable settings. between the treated and untreated groups. In this setting, we cannot use (augmented) IPW-based approaches because the probability of observing an action is either zero or one. We could take a direct imputation approach that estimates a model for the expected potential outcomes under diﬀerent actions and uses this model to extrapolate. However, there are many diﬀerent models that ﬁt the observable data equally well and so the expected potential outcome function is not uniquely point identiﬁed. Our proposal is a robust version of the direct imputation approach: we We apply the proposed methodology to the data from a unique ﬁeld experiment on pre-trial risk All of these procedures rely on some form of overlap assumption, where the underlying policy In contrast, our robust approach deals with deterministic policies where there is no overlap ﬁrst partially identify the conditional expectation, and then use robust optimization to ﬁnd the best policy under the worst-case model. 2005), which bounds the value of unidentiﬁable quantities using identiﬁable ones. We also rely on the robust optimization framework (see Bertsimas et al., 2011, for a review), which embeds the objective or constraints of an optimization problem into an uncertainty set, and then optimizes for the worst-case objective or constraints in that set. We use partial identiﬁcation to create an uncertainty set for the objective. (2021) consider the IPW approach in the possible presence of unmeasured confounding. They use robust optimization to ﬁnd the optimal policy across a partially identiﬁed set of assignment probabilities under the standard sensitivity analysis framework (Rosenbaum, 2002). In a diﬀerent vein, Pu and Zhang (2021) study policy learning with instrumental variables. Using the partial identiﬁcation bounds of Balke and Pearl (1994), they apply robust optimization to ﬁnd an optimal policy. We use robust optimization in a similar way, but to account for the partial identiﬁcation brought on by the lack of overlap. In a diﬀerent setting, Gupta et al. (2020) use robust optimization to ﬁnd optimal policies when extrapolating to populations diﬀerent from a study population, without access to individual-level information. Finally, Cui (2021) discusses various potential objective functions when there is partial identiﬁcation, derived from classical ideas in decision theory. Paper outline. The paper proceeds as follows. Section 2 describes the pre-trial risk assessment instrument and the ﬁeld experiment that motivate our methodology. Section 3 deﬁnes the population safe policy optimization problem and compares the resulting policy to the baseline and oracle polices. Section 4 shows how to compute an empirical safe policy from the observed data and analyzes its statistical properties. Section 5 presents some examples of the model and policy classes that can be used under our proposed framework. Section 6 extends the methodology to incorporate experimental data and human decisions. Section 7 applies the methodology to the pre-trial risk assessment problem. Section 8 concludes. In this section, we brieﬂy describe a particular pre-trial risk assessment instrument, called the Public Safety Assessment (PSA), used in Dane county, Wisconsin, that motivates our methodology. The PSA is an algorithmic recommendation that is designed to help judges make their pre-trial release decisions. After explaining how the PSA is constructed, we describe an original randomized experiment we conducted to evaluate the impact of the PSA on judges’ pre-trial decisions. In Section 7, we apply the proposed methodology to the data from this experiment in order to learn Our approach builds on the literature about partial identiﬁcation of treatment eﬀects (Manski, There are several recent applications of robust optimization to policy learning. Kallus and Zhou a new, robust algorithmic recommendation to improve judicial decisions. Interested readers should consult Greiner et al. (2020) and Imai et al. (2020) for further details of the PSA and experiment. potentially lead to a higher overall expected utility than the status quo rules we discuss, while retaining the high level of transparency, interpretability, and robustness. In particular, we would like to develop robust algorithmic rules that are guaranteed to outperform the current rules with high probability. Crucially, we are concerned with the consequences of implementing these algorithmic policies on overall outcomes (see also Imai et al., 2020). Although evaluating the classiﬁcation accuracy of these algorithms also requires counterfactual analysis (see, e.g., Kleinberg et al., 2018; Coston et al., 2020), this is not our goal. Similarly, while there are many factors besides the risk assessment instruments that aﬀect the judge’s decision and the arrestee’s behavior (e.g., the relationship between socioeconomic status and the ability to post bail), we focus on changing the existing algorithms rather than intervening on the other factors. The goal of the PSA is to help judges decide, at ﬁrst appearance hearings, which arrestees should be released pending disposition of any criminal charges. Because arrestees are presumed to be innocent, it is important to avoid unnecessary incarceration. The PSA consists of classiﬁcation scores based on the risk that each arrestee will engage in three types of risky behavior: (i) failing to appear in court (FTA), (ii) committing a new criminal activity (NCA), and (iii) committing a new violent criminal activity (NVCA). By law, judges are required to balance between these risks and the cost of incarceration when making their pre-trial release decisions. terministic functions of 9 risk factors. Importantly, the only demographic factor used is the age of an arrestee, and other characteristics such as gender and race are not used. The other risk factors include the current oﬀense and pending charges as well as criminal history, which is based on prior convictions and prior FTA. Each of these scores is constructed by taking a linear combination of underlying risk factors and thresholding the integer-weighted sum. Indeed, for the sake of transparency, policy makers have made these weights and thresholds publicly available (see https://advancingpretrial.org/psa/factors). six levels and is based on four risk factors. The values range from 0 to 7, and the ﬁnal FTA score is thresholded into values between 1 and 6 by assigning {0 → 1, 1 → 2, 2 → 3, (3, 4) → 4, (5, 6) → 5, 7 → 6}. The NCA score also has six levels, but is based on six risk factors and has a maximum value of 13 before being collapsed into six levels by assigning {0 → 1, (1, 2) → 2, (3, 4) → 3, (5, 6) → 4, (7, 8) → 5), (9, 10, 11, 12, 13) → 6}. Our primary goal is to construct new algorithmic scoring and recommendation rules that can The PSA consists of separate scores for FTA, NCA, and NVCA risks. These scores are de- Table 1 shows the integer weights on these risk factors for the three scores. The FTA score has Finally, the NVCA score, which will be the focus of our empirical analysis, is binary and is Table 1: Weights placed on risk factors to construct the failure to appear (FTA), new criminal activity (NCA), and new violent criminal activity (NVCA) scores. The sum of the weights are then thresholded into six levels for the FTA and NCA scores and a binary “Yes”/“No” for the NVCA score. based on the weighted average of ﬁve diﬀerent risk factors — whether the current oﬀense is violent, the arrestee is 20 years old or younger, there is a pending charge at the time of arrest, and the number of prior violent and non-violent convictions. If the sum of the weights is greater than or equal to 4, the PSA returns an NVCA score of 1, ﬂagging the arrestee as being at elevated risk of an NVCA. Otherwise the NVCA score is 0, and the arrestee is not ﬂagged as being at elevated risk. require a signature bond for release or to require some level of cash bail, and what, if any, monitoring conditions to place on release. In this paper, we analyze the dichotomized release recommendation, i.e., signature bond versus cash bail, and ignore recommendations about monitoring conditions. Both of these recommendations are constructed via the so-called “Decision Making Framework” (DMF), which is a deterministic function of the PSA scores. For our analysis, we exclude the cases where the current charge is one of several serious violent oﬀenses, the defendant was extradited, or the NVCA score is 1, because the DMF automatically recommends cash bail for these cases. We do not consider altering this aspect of the DMF. Figure 1 shows a simpliﬁed version of the DMF matrix highlighting where the recommendation is to require a signature bond (beige) versus cash bail (orange) for release. If the FTA score and the NCA score are both less than 5, then the recommendation is to only require a signature bond. Otherwise the recommendation is to require cash bail. These three PSA scores are then combined into two recommendations for the judge: whether to For the remaining cases, the FTA and NCA risk scores are combined via a decision matrix. Figure 1: Decision Making Framework (DMF) matrix for cases where the current charge is not a serious violent oﬀense, the NVCA ﬂag is not triggered, and the defendant was not extradited. If the FTA score and the NCA score are both less than 5, then the recommendation is to only require a signature bond. Otherwise the recommendation is to require cash bail. Unshaded areas indicate impossible combinations of FTA and NCA scores. To develop new algorithmic scoring and recommendation rules, we will use data from a ﬁeld randomized controlled trial conducted in Dane county, Wisconsin. We will brieﬂy describe the experiment here while deferring the details to Greiner et al. (2020) and Imai et al. (2020). In this experiment, the PSA was computed for each ﬁrst appearance hearing during the study period and was randomly either made available in its entirety to the judge or it was not made available at all. If a case is assigned to the treatment group, the judge received the information including the three PSA scores, the PSA-DMF recommendations, as well as all the risk factors that were used to construct them. For the control group, the judge did not receive the PSA scores and PSA-DMF recommendations but sometimes received some of the information constituting the risk factors. Thus, the treatment in this experiment was the provision of the PSA scores and PSA-DMF recommendations. mendation (signature bond or cash bail), the underlying risk factors used to construct the scores, the binary decision by the judge (signature bond or cash bail), and three binary outcomes (FTA, NCA, and NVCA). We focus on ﬁrst arrest cases in order to avoid spillover eﬀects between cases. All told, there are 1891 cases, in 948 of which judges were given access to the PSA. Table 2 shows the case counts disaggregated by bail type and NVCA, the outcome we consider in Section 7. In 1410 of these cases, the judge assigned a signature bond with 109 leading to an NVCA. A slightly For each case, we observe the three scores (FTA, NCA, NVCA) and the binary DMF recom- Table 2: Number of cases where the judge assigned an arrestee a signature bond or cash bail, that eventually did or not result in an NVCA. lower fraction of cases where the judge assigned cash bail resulted in an NVCA than cases where the judge assigned a signature bond (χ part of the study. Since our goal is to learn a new, better recommendation system, the problem is the lack of overlap: the probability that any case would have had a diﬀerent recommendation than it received is zero. Therefore, existing approaches to policy learning, which rely principally on the inverse of this probability, are not applicable for our setting. Instead, we must learn a robust policy through extrapolation. In the remainder of this paper, we will develop a methodological framework to learn new recommendation rules in the absence of this overlap in a robust way, ensuring that the new rules are no worse than the original recommendation, and potentially much better. In order to separate out the key ideas, we will develop our optimal safe policy approach in two parts. In this section, after introducing the notation and describing our setup, we show how to construct a safe policy in the population, i.e., with an inﬁnite number of samples. We will ﬁrst describe the population optimization problem that constructs a safe policy. Then, we will give concrete examples to build intuition before describing our methodology in greater generality. Finally, we develop several theoretical properties of our approach. In Section 4, we will move from the population problem to the ﬁnite sample problem, and discuss constructing policies empirically. Suppose that we have a representative sample of n units independently drawn from a population P. For each individual unit i = 1, . . . , n, we observe a set of covariates X outcome Y be taken for each unit. For each unit, action A on its pre-treatment covariates X treatment (Rubin, 1980). Then, we can write the potential outcome under each action A Y(a) where a ∈ A (Neyman, 1923; Holland, 1986). Crucially, each component of the PSA is deterministic and no aspect of it was randomized as generated the observed action A write Y of covariates where the baseline action is a as X when convenient, we will also refer to the baseline policy as ˜π(x | a) ≡ {˜π(x) = a}, the indicator of whether the baseline policy is equal to a. Because our setting implies that ({Y is independently and identically distributed, we will sometimes drop the i subscript to reduce notational clutter. Our primary goal is to ﬁnd a new policy π : X → A, that has a high expected utility. again use the notation π(a | X) ≡ {π(X) = a} for the policy being equal to action a given the covariates X. Letting u(y, a) denote the utility for outcome y under action a, the utility for action a with potential outcome Y (a) is given by, Note that this utility only takes into account the policy action and the outcome. In Section 6.2, we will show how to include the costs of human decisions into the utility function as well. a, u(a) ≡ u(1, a) − u(0, a), and the utility for an outcome of zero with an action a, c(a) ≡ u(0, a); we will refer to this latter term as the “cost” because it denotes the utility under action a when the outcome event does not happen. The value of policy π, or “welfare,” is the expected utility under policy π across the population, Using the law of iterated expectation, we can write the value in the following form, V (π, m) = E where m(a, x) ≡ E[Y (a) | X] represents the conditional expected potential outcome function. We include the dependence on the conditional expected potential outcome function m(a, x) to explicitly denote the value under diﬀerent potential models in our development below. For two policies, we We consider the setting where we know the baseline deterministic policy ˜π : X → A that = Y(˜π(X)). This baseline policy partitions the covariate space, and we denote the set The two key components of the utility are the utility change between the two outcomes for action will deﬁne the regret of π can write a population optimal policy as the one that maximizes the value, π or, equivalently, minimizes the regret relative to ˜π, π this optimal policy may not be unique. The policy class Π is an important object both in the theoretical analysis and in applications. We discuss the theoretical role of the policy class further in Sections 3.4 and 4.2, the important special case of policy classes with ﬁnite VC dimension in Section 5.2, and the substantive choices when applied to pre-trial risk assessments in Section 7. all candidate policies π ∈ Π. Equation (2) shows us that in order to point identify the value we will need to point identify the conditional expectation m(a, x) for all actions a ∈ A and covariate values x ∈ X. If the baseline policy ˜π were stochastic, we could identify the conditional expectation via IPW (see, e.g. Zhao et al., 2012; Kitagawa and Tetenov, 2018). Alternatively, we could use direct model-based imputation by using the conditional expectation of the observed outcomes E[Y | X = x, A = a]. However, in our setting where the baseline policy ˜π is a deterministic function of covariates, we cannot point identify the conditional expectation m(a, x). Therefore, we cannot point identify the value V (π) for all policies π ∈ Π. In order to understand how lack of point identiﬁcation aﬀects our ability to ﬁnd a new policy, we will separate the value of a policy into two components: one that is point identiﬁable and one that is not. We will then attempt to partially identify the latter term, and optimize for the worst-case value. To do this, we will use the fact that we can identify the conditional expectation of the potential outcome under the baseline policy as the conditional expectation of the observed outcome, We can then write the value V (π) in terms of the identiﬁable partial model m(˜π(x), x) by using the observed outcome Y when our policy π agrees with the baseline policy ˜π, and the unidentiﬁable full model m(a, x) when π disagrees with ˜π, when a is diﬀerent from the baseline policy and so we cannot identify V (π, m) for an arbitrary policy π. However, we can identify the value of the baseline policy ˜π as simply the utility using the Ideally, we would like to ﬁnd a policy π that has the highest value across a policy class Π. We In order to ﬁnd the optimal policy, we need to be able to point identify the value V (π) for V (π, m) = Eπ(a | X) {u(a) [˜π(a | X)Y + {1 − ˜π(a | X)}m(a, X)] + c(a)} Without further assumptions, we cannot point identify the value of the conditional expectation observed policy values and outcomes, given policy π. Speciﬁcally, we encode the conditional expectation as a function m : A×X → [0, 1], and restrict it to be in a particular model class F. We then combine this with the fact that we have identiﬁed some function values, i.e., m(˜π(x), x) = ˜m(x) = E[Y | X = x], to form a restricted model class: We discuss particular choices of model class F and how to construct the associated restricted model class M in Section 5.1 below. This restricted model class combines the structural information from the underlying class F with the observable implications from the data to limit the possible values of the conditional expectation function m(a, x). across the set of potential models M for m(a, x). An equivalent approach is to minimize the worstcase regret relative to the baseline policy ˜π, because the value for ˜π is point identiﬁed. Therefore, the robust policy is a solution to, best policy in this worst-case setting. Since we are only optimizing over the unknown components, the worst-case value and the true value coincide for the baseline policy, i.e., V Therefore, so long as the baseline policy ˜π is in the policy class Π, the safe optimal policy π be at least as good as the baseline. Furthermore, the baseline policy acts as a fallback option. If deviating from the baseline policy can lead to a worse outcome, the safe optimal policy will stick to the baseline. In this way, this robust solution only changes the baseline where there is suﬃcient evidence for an improved value. Finally, note that this is a conservative decision criterion. Other, less conservative approaches include minimizing the regret relative to the best possible policy, or maximizing the maximum possible value; see Manski (2005) for a general discussion and Cui (2021) for other possible choices with partial identiﬁcation. Now, if we place restrictions on m(a, x), we can partially identify a range of potential values for a With this, we take a maximin approach, ﬁnding a policy that maximizes the worst-case value We characterize the resulting optimal policy as “safe” because it ﬁrst ﬁnds the worst-case value, (π) ≡ minV (π, m), by minimizing over the set of allowable models M, and then ﬁnds the To give intuition on the proposed procedure, we will consider two special cases: (i) a single discrete covariate, and (ii) two binary covariates. Consider the case where we have a single discrete covariate with J levels x ∈ {0, . . . , J −1}, which we will assume is drawn uniformly with probability 1/J for notational simplicity. Suppose we have a binary action, i.e., A = {0, 1}, and a binary outcome. Then, we can use the following vector representation; the conditional expectation function of the potential outcome given an action a ∈ A as m policy as ˜π ≡ (˜π the observed outcome as a vector ˜m ≡ ( ˜m lie in a subset F ⊂ [0, 1] components m where λ the observable outcomes: m class bounds each of the components of the model vectors, where the lower and upper bounds are given by, with K c(a) = 0, for all actions a. Then, the robust maximin problem given in Equation (5) becomes: ≡ (m, . . . , m) ∈ [0, 1], a policy as π ≡ (π, . . . , π) ∈ {0, 1}, and the baseline Our ﬁrst step is to constrain the model class, in this case restricting the vectors mand mto is a constant. We can now combine this Lipschitz property with the constraint based on = {k | ˜π= a} being the set of indices where the baseline action is equal to a. For simplicity, assume that the utility change is constant, u(a) = 1, and the cost is zero, To further illustrate this case, consider the following numerical example where the action has a Figure 2: The robust policy with a single discrete variable, binary action set A = {0, 1}, constant utility change u(0) = u(1) = 1 and zero cost c(0) = c(1) = 0. The black dashed line indicates the decision boundary for the baseline policy ˜π; the red dashed line is the boundary for the robust policy π purple; action 0 is green), while the hollow points represent the unidentiﬁable values. Each line shows a partial identiﬁcation region, the range between the lower and upper bounds in Equation (6) above with the true Lipschitz constants λ constant eﬀect on the logit scale: m intercept for each j. Suppose that the baseline action is given by ˜π setting, the solid points in Figure 2 indicate the identiﬁable values of m (action 1 is green; action 0 is purple), while the hollow points represent the unidentiﬁable values. Each line shows a partial identiﬁcation region, the range between the lower and upper bounds in Equation (6) above with the true Lipschitz constants λ π= argmax via Equation (7), we assign action 1 wherever we can guarantee that action 1 has a higher expected outcome than action 0, or vice versa for action 0, no matter the true underlying model. These are the values where the solid points are entirely above (between red and black dashed lines) or below (right of the black dashed line) the partial identiﬁcation lines. Wherever there is no such guarantee—where the lines contain the solid points (left of the red dashed line)—the maximin policy falls back to the baseline. The result is the robust policy that assigns action 1 for j ≥ 4 and action 0 otherwise. This safe policy improves welfare by 2.5% relative to the baseline, compared to the optimal rule which improves welfare by 4.2%. . Solid points indicate the identiﬁable values of m, colored by the action (action 1 is Since the treatment eﬀect is always positive, the oracle policy that optimizes the true value, Next, we consider a case with two binary covariates x = (x for simplicity — where the utility changes are constant, i.e., u(0) = u(1) = 1, the cost is zero, i.e., c(0) = c(1) = 0, and a baseline policy assigns action 1 when both covariate values are 1, ˜π(x) = {x model with an interaction term, m(a, x) = β expectation of the observed outcome as ˜m satisfy the following four linear constraints: learn any policy other than the baseline policy without restrictions on the unidentiﬁable coeﬃcients. It turns out, however, that if we are willing to assume that the conditional expectation is additive, i.e., β represent these models as 3 dimensional vectors, Then, the restricted set M consists of vectors (β linear constraints in Equation (8). values ˜m action 0 are uniquely identiﬁed: β the coeﬃcients for action 1 are only restricted to sum to ˜m single constraint, the restricted model set can be written as, Without any further assumptions, we can only identify β, β, and β. Therefore, we cannot = 0 for both actions, we can make progress. Under this additional assumption, we can Using linear algebra tools, we can write the restricted set in terms of the observable model Optimizing over the two unknown parameters, (b, b), we ﬁnd the following worst-case value: V(π) ={π(x, x) = 0}{˜m+ ( ˜m− ˜m)x+ ( ˜m− ˜m)x} where I(x ∈ S) is equal to ∞ if x ∈ S and is equal to 0 otherwise. When ﬁnding the safe policy, this constrains the policy so that π(x) 6= 1 for all x optimization problem (5) is given by, max policy class, the robust policy is constrained to agree with the baseline policy ˜π for all x but can still disagree for x gives an action of zero, π(1, 1) = 0, the worst-case value V model, extrapolating to the unobserved case as m(0, (1, 1)) = ˜m learn a safe policy π We now derive the theoretical properties of the proposed population safe policy π the statements of the results, we will assume that the utility gain across diﬀerent actions is constant and, without loss of generality, is positive, u(a) ≡ u(a, 1) − u(a, 0) = u > 0 for all actions a ∈ A. First, the proposed policy is shown to be “safe” in the sense that it never performs worse than the baseline policy ˜π. This conservative principle is the key beneﬁt of the robust optimization approach. The following proposition shows that as long as the baseline policy is in our policy class Π, and the underlying model lies in the restricted model class M, the value of the population safe policy is never less than that of the baseline policy. Proposition 1 (Population safety). Let π then R(π perform much worse than the infeasible, oracle optimal population policy, π Although we never know the oracle policy, we can characterize the optimality gap, V (π which is the regret or the diﬀerence in values between the proposed robust policy and the oracle. width of some function class F in the direction of function g as: subject to π(0, 0) = π(1, 0) = π(0, 1) = 0. Note that the only free parameter in the robust optimization problem is the policy action at = 1; the three other policy values are constrained to be zero. Therefore, with a fully ﬂexible < ˜m+ ˜m− ˜m. , ˜π, m) ≤ 0. However, this guarantee of safety comes at a cost. In particular, the population safe policy may To do this, we consider the “size” of the restricted model class M. Speciﬁcally, we deﬁne the This represents the diﬀerence between the maximum and minimum cross-moment of a function g and all possible functions f ∈ F. We then deﬁne the overall size of the model class, W maximal width over all possible policies: where G = {g size of the restricted model class W If we can point identify the conditional expectation function, then the size will be zero; larger partial identiﬁcation sets will have a larger size. by the size of the model class. In other words, the cost of robustness is directly controlled by the amount of uncertainty in the restricted model class M. Theorem 1 (Population optimality gap). Let π the regret of π X = x], M contains only one element. Then, the size W zero. This means that the solution to the robust optimization problem in Equation (5) will have the same value as the oracle, reducing to the standard case where we can point identify the conditional expectation. Conversely, if we can only point identify the conditional expectation function m(a, x) for few action-covariate pairs, then the size of the restricted model class M will be large, there will be a greater potential for sub-optimality due to lack of identiﬁcation, and the regret of the safe policy π of the restricted model class W depends on the policy class as well. In practice, we do not have access to an inﬁnite amount of data, and so we cannot compute the population safe policy. Here, we show how to learn an empirical safe policy from observed data of ﬁnite sample size. The following theorem shows that the optimality gap, scaled by the utility gain u, is bounded In the limiting case where we can fully identify the conditional expectation m(a, x) ≡ E[Y (a) | relative to the infeasible optimal policy πcould be large. Finally, note that the size Suppose we have n independently and identically distributed data points {X From this sample we wish to ﬁnd a robust policy empirically. To do so, we begin with a sample analog to the value function in Equation (3) above, With this, we could ﬁnd the worst-case sample value across all models in the restricted model class M from Equation (4). However, we do not have access to the true conditional expectation ˜m(x) = E[Y (˜π(X))] and so cannot compute the true restricted model class. One possible way to address this is to obtain an estimator of the conditional expectation function, the estimate in place of the true values. However, this does not take into account the estimation uncertainty, and could lead to a policy that improperly deviates from the baseline due to noise. This approach will have no guarantee that the new policy is at least as good as the baseline without access to many samples: it would rely on convergence of the model ˆm(x), which may be slow. contains the true restricted model class with probability at least 1 − α, Then we construct our empirical policies by ﬁnd the worst-case in-sample value then maximizing this objective across policies π We discuss concrete approaches to constructing the empirical model class and solving this optimization problem in Section 5.1. In general, the empirical model class will be larger than the true model class and so a policy derived from it will be more conservative. What are the statistical properties of our empirical safe policy ˆπ in ﬁnite samples? We will ﬁrst establish that the proposed policy has an approximate safety guarantee: with probability approximately 1 − α we can guarantee that it is at least as good as the baseline, up to sampling error and the complexity of the policy class. We then characterize the empirical optimality gap and show that it can be bounded using the complexity of the policy class as well as the size of the empirical restricted model class. For simplicity, we will consider the special case of a binary action set A = {0, 1}. We use the population Rademacher complexity to measure the complexity of the ˆV (π, m) =1XXπ(a | X) {u(a) [˜π(a | X)Y+ {1 − ˜π(a | X)}m(a, X Instead, we construct a larger, empirical model classcM(α), based on the observed data, that policy class: where ε expectation is taken over both the Rademacher variables ε complexity is the average maximum correlation between the policy values and random noise, and so measures the ability of the policy class Π to overﬁt. Theorem 2 (Statistical safety). Let ˆπ be a solution to Equation (12). Given the baseline policy ˜π ∈ Π and the true conditional expectation m(a, x) ∈ M, for any 0 < δ ≤ e relative to the baseline ˜π is, with probability at least 1 − α − δ, where C = max controlled by the Rademacher complexity of the policy class Π, and an error term due to sampling variability that decreases at a rate of n quality of the safety guarantee for any level α. If the policy class is simple, then the bound will quickly go towards zero for any level α; if it is complex, then we will require larger samples to ensure that the safety guarantee is meaningful, regardless of the level α. mation error for the conditional expectation However, if we cannot estimate ˜m(x) well, the empirical restricted model class which will aﬀect how well the empirical safe policy compares to the oracle policy. To quantify this, we will again rely on a notion of the size of the empirical restricted model class setting, however, we will use an empirical width, Similarly to above, we deﬁne the empirical size of F, potential models, Theorem 3 (Empirical optimality gap). Let ˆπ be a solution to Equation (12) and assume that the utility gains are equal to each other, u(1) = u(0) = u. If the true conditional expectation m ∈ M, ’s are i.i.d. Rademacher random variables, i.e., Pr(ε= 1) = Pr(ε= −1) = 1/2, and the First, we establish a statistical safety guarantee analogous to Proposition 1. Theorem 2 shows that the regret for the empirical safe policy versus the baseline policy is Importantly, by taking a conservative approach using the larger model classcM(α), the estithen for any 0 < δ ≤ e with probability at least 1 − α − δ, where C = max role in bounding the gap between the empirical safe policy and the optimal policy. In addition, the Rademacher complexity again appears: policy classes that are more liable to overﬁt can have a larger optimality gap. For many standard policy classes, we can expect the Rademacher complexity to decrease to zero as the sample size increases, while the empirical size of the restricted model class may not. Furthermore, there is a tradeoﬀ between ﬁnding a safe policy with a higher probability — setting the level 1 −α to be high — and ﬁnding a policy that is closer to optimal. By setting 1 −α to be high, the width of tradeoﬀ is similar to the tradeoﬀ between having a low type I error rate (α low) and high power function well, then the size The two important components when constructing safe policies are the assumptions we place on the outcome model — the model class F — and the class of candidate policies that we consider Π. We will now consider several representative cases of model classes, show how to construct the restricted model classes, and apply the theoretical results above. Then, we will further discuss the role of the policy class, considering the special cases of the results for policy classes with ﬁnite VC dimension. We now give several examples of model classes F and the restricted model classes induced by the data M. For all of the model classes we consider, the restricted model class will be a set of functions that are upper and lower bounded point-wise by two bounding functions, We will also create an empirical restricted model class tee in Equation (11). This similarly results in the form of a point-wise lower and upper bound on Comparing to Theorem 1, we see that the size — now the empirical version — plays an important low) in hypothesis testing. Similarly, if we cannot estimate the conditional expectation the conditional expectation function, with lower and upper bounds tively. These point-wise bounds yield a closed form bound on the size of the restricted model class M and the empirical size of the empirical restricted model class diﬀerence between the bounds: In Appendix A.1 we use these bounds to specialize Theorems 1 and 3 to this case. 2021). Finding the empirical safe policy by solving Equation (12) is equivalent to solving an empirical welfare maximization problem using a quasi-outcome that is equal to the observed outcome when the action agrees with the baseline policy, and is equal to either the upper or lower bound when it disagrees, With this, Equation (12) specializes to the outcomes directly. In the counterfactual case where the baseline action is diﬀerent from a, the value will use either the upper or lower bound of the outcome model, depending on the sign of the utility gain. Using bounds in place of outcomes in this way is similar to the approach of Pu and Zhang (2021) in instrumental variable settings. Since the optimization problem (16) is not convex, it is not straightforward to solve exactly. As many have noted (e.g. Zhao et al., 2012; Zhang et al., 2012; Kitagawa and Tetenov, 2018), this optimization problem can be written as a weighted classiﬁcation problem and approximately solved via a convex relaxation with surrogate losses. An alternative approach is to solve the problem in Equation (16) directly. In our empirical studies, we consider thresholded linear policy classes that mirror the NVCA and DMF rules we discuss in Section 2; these turn Equation (16) into a mixed integer program that we can solve eﬃciently with commercial solvers. Alternatively, we can use the approach for ﬁnite-depth decisions tree policies implemented in Sverdrup et al. (2020), and for continuous, non-deterministic policies (or approximations to deterministic ones) we could use stochastic gradient descent methods designed to escape from local minima. The point-wise bound also allows us to solve for the worst-case population and empirical values (π) andˆV(π) by ﬁnding the minimal value for each action-covariate pair (see Pu and Zhang, bΥ(a) = ˜π(a | X)Y+ {1 − ˜π(a | X)}{u(a) ≥ 0}bB(a, X) + {u(a) ≤ 0}bB(a, X). In eﬀect, for an action a where the baseline action ˜π(x) is equal to a, the minimal value uses We now give several examples of model classes that lead to point-wise bounded restricted model classes, deferring derivations and additional examples to Appendix A.2. Example 1 (No restrictions). Suppose that the conditional expectation has no restrictions, other than that it lies between zero and one, i.e., F = {f | 0 ≤ f(a, x) ≤ 1 ∀a ∈ A, x ∈ X}. Then the restricted model class M = {f ∈ F | f(a, x) = ˜m(x) for a with ˜π(x) = a} provides no additional information when the policy π disagrees with the baseline policy ˜π and the upper and lower bounds in Equation (14) are B respectively. In the absence of any additional information, the worst case conditional expectation is 0 or 1 (depending on the sign of the utility gain) whenever it is not point identiﬁed. The size of this model class is then W model class expectation function ˜m(x), with lower and upper bounds See Srinivas et al. (2010); Chowdhury and Gopalan (2017); Fiedler et al. (2021) for examples on constructing such simultaneous bounds via kernel methods in statistical control settings. With this conﬁdence band, we can use the upper and lower bounds of the conﬁdence band in place of the true conditional expectation ˜m(x), i.e. bB(a, x) = ˜π(a | x) Example 2 (Lipschitz Functions). Suppose that the covariate space X has a norm k· k, and that m(a, ·) is a λ Taking the greatest lower bound and least upper bound implied by this model class leads to lower and upper bounds, B where recall that X tion a. The further we extrapolate from the area where the baseline action ˜π(x) = a, the larger the value of kx−x size of M will depend on the expected distance to the boundary between baseline actions and the value of the Lipschitz constant. If most individuals are close the boundary, or the Lipschitz constant is small, M will be small and the safe policy will be close to optimal. Conversely, a large number of individuals far away from the boundary or a large Lipschitz constant will increase the potential for suboptimality. To construct the empirical version, we again use a simultaneous conﬁdence band bC(x) satisfying Equation (17). Then the lower and upper bounds use the lower and upper conﬁdence limits in place of the function values, (a, X) = infbC(x) − λkX − xk. Example 3 (Generalized linear models). Consider a model class that is a generalized linear model in a set of basis functions φ : A × X → R {f(a, x) = h h( ˜m(x)) = b · φ(a, x) for all x and a such that ˜π(x) = a. Let β solution and let D ∈ R 0 ∀ ˜π(x) = a}. Then we can re-write the restricted model class as The free parameters in this model class are represented as the vector b worst-case value will involve a non-linear optimization over b failure. Rather than taking this approach, we will consider a larger class f(a, x) ≤ B class will be conservative. Since m(a, x) is between 0 or 1, we can use this bound when φ(a, x) is in the null space N to get upper and lower bounds The worst-case value uses β one of the bounds for units assigned to action a when ˜π(x) 6= a and φ(a, x the null space. The size of the model class is the percentage of units that are in the null space, and the closer the safe policy is to optimal. this time for the minimum norm prediction β Hotelling-Scheﬀ´e procedure (Wynn and Bloomﬁeld, 1971; Ruczinski, 2002), where variance from the MSE, Φ = [φ(˜π(x denotes the pseudo-inverse of a matrix A. This gives lower and upper bounds, = 1 − PrDφ(a, X) = 0 ∀ a ∈ A. The fewer units in the null space, the smaller the size To construct the empirical model class we again begin with a simultaneous conﬁdence band, ˆβis the least squares estimate of the minimum norm solution, ˆσis the estimate of the is the 1 − α quantile of an F distribution with r and n − r degrees of freedom, and A The choice of model class F corresponds to the substantive assumptions we place on the outcomes in order to extrapolate and ﬁnd new policies. The choice of policy class Π is equally important: it determines the type of policies we consider. An extremely ﬂexible policy class with no restrictions will result in the highest possible welfare, but such a policy is undesirable for two reasons. First, they are all but inscrutable by both those designing the algorithms and those subject to the algorithm’s actions (see Murdoch et al., 2019, for discussion on interpretability issues). Second, policies that are too ﬂexible will have a high complexity, and so the bounds on the regret of the empirical safe policy—versus either the baseline policy or the infeasible optimal policy—will be too large. integer m for which there exists some points x policy values π(x and uniform laws, see Wainwright, 2019, §4). Examples of policy classes with ﬁnite VC dimension include linear policies, Π and depth L decision trees with VC dimension on the order of 2 G with ﬁnite VC dimension ν < ∞, the Rademacher complexity is bounded by, R for some universal constant c (Wainwright, 2019, §5.3). In Appendix A.3, we use this bound to specialize the results in Section 4.2, ﬁnding that the higher the VC dimension, the more liable a policy is to overﬁt to the noisy data, and the more samples we will need to ensure that the regret bound is low. For a policy class with ﬁnite VC dimension, the rate of convergence will still be the rate of growth must be less than equal to zero. See Athey and Wager (2021) for further discussion. Motivated by our application introduced in Section 2, we consider two important extensions of the methodology proposed above. First, we consider the scenario under which the data come from a randomized experiment, where a deterministic policy of interest is compared to a status quo without such a policy. Second, we consider a human-in-the-loop setting, in which an algorithmic policy recommendation is deterministic, but the ﬁnal decision is made by a human decision-maker in an unknown way. In this case, we must adapt the procedure to account for the fact that the policy may aﬀect the ﬁnal decisions, but does not determine them, implying that actions only incur costs through the ﬁnal decisions. For notational simplicity, we will again assume, throughout this section, that the utility gain is constant across all actions and is denoted by u. One way to characterize the complexity of the policy class Π is via its VC-dimension: the largest The VC dimension gives an upper bound on the Rademacher complexity: for a function classp n. However, for a policy class with VC dimension growing with the sample size, ν  n, In many cases, a single deterministic policy is compared to the status quo of no such policy via a randomized trial for program evaluation. In our empirical study, the existing policy was compared to a “null” policy where no algorithmic recommendations were provided. The goal of such a trial is typically to evaluate whether one should adopt the algorithmic policy. We now show that one can use the proposed methodology to safely learn a new, and possibly better, policy even in this setting. In particular, we can weaken the restrictions of the underlying model class by placing assumptions on treatment eﬀects rather than the expected potential outcomes. We focus here on comparing a baseline policy ˜π to a null policy that assigns no action, which we denote as Ø(x) = Ø, and has potential outcome Y (Ø). enacted (i.e., null policy), and Z 1 | X = x) be the probability of assigning the treatment condition for an individual with covariates x. Since it is an experiment, this probability is known. Rather than minimize the regret relative to the baseline policy ˜π as in Equation (5), we will minimize regret relative to the null policy Ø. Deﬁning the conditional average treatment eﬀect (CATE) of action a relative to no action Ø, τ(a, x) = m(a, x) − m(Ø, x), we can now write the regret of a new policy π relative to the null policy Ø as, R(π, Ø) = −E baseline policy ˜π(x) using the transformed outcome Γ(Z, X, Y ) = Y {Z −e(X)}/{e(X)(1 −e(X))}, which equals the CATE in expectation, i.e., τ(˜π(x), x) = E[Γ(Z, X, Y ) | X = x]. With this, we follow the development in Section 3.2, with the transformed outcome Γ replacing the outcome Y and a restricted model class for the treatment eﬀects T = {f ∈ F | f(˜π(x), x) = τ (˜π(x), x)} replacing the model class for the outcomes M. Speciﬁcally, we decompose the regret into an identiﬁable component and an unidentiﬁable component, and consider the worst-case regret across all treatment eﬀects in T , giving the population robust optimization problem, We can similarly construct the empirical analog by creating a larger empirical model class as in Section 4. In this setup, let Z∈ {0, 1} be a treatment assignment indicator where Z= 0 if no policy is Now, following Kitagawa and Tetenov (2018), we can identify the the CATE function for the ∈ argminmax−Eπ(a, X) (c(a) + u(a) [˜π(a | X)Γ(Z, X, Y ) + (1 − ˜π(a | X)) f (a, X)]). From the perspective of ﬁnding a new, empirical safe policy π, the key beneﬁt of experimentally comparing two deterministic policies in this way is that the primary unidentiﬁed object is the CATE, τ(a, x) = E[Y (a) −Y (Ø) | X = x], rather than the conditional expected potential outcome, m(a, x) = E[Y (a) | X = x]. Only placing structural assumptions on the CATE amounts to imposing weaker assumptions to guarantee robustness than making assumptions about each expected potential outcome. In addition, treatment eﬀect heterogeneity is often considered to be signiﬁcantly simpler than heterogeneity in outcomes (see, e.g., K¨unzel et al., 2019; Hahn et al., 2020; Nie and Wager, 2021). Therefore, we may consider a smaller model class for the treatment eﬀects T than the class for the conditional expected outcomes M, leading to better guarantees on the optimality gap between the robust and optimal policies in Theorems 1 and 3. In many cases, an algorithmic policy is not the ﬁnal arbiter of decisions. Instead, there is often a “human-in-the-loop” framework, where an algorithmic policy provides recommendations to a human that makes an ultimate decision. Our pre-trial risk assessment application is an example of such algorithm-assisted human decision-making (Imai et al., 2020). under action (or an algorithmic recommendation in our application) a ∈ A, and Y be the potential (binary) outcome for individual i under decision d ∈ {0, 1} and action a ∈ A. We denote the expected potential decision conditional on covariates x as d(a, x) = E[D(a) | X = x]. Further, we denote the potential outcome under action a as Y the conditional expectation by m(a, x) = E[Y given by D Finally, we write the utility for outcome y under decision d as u(y, d). We make the simplifying assumption that the utility gain is constant across decisions, u(1, d) − u(0, d) = u for d ∈ {0, 1}, index the utility for y = 0 and d = 0 as u(0, 0) = 0, and denote the added cost of taking decision 1 as c = u(0, 1) −u(0, 0). Now we can write the value by marginalizing over the potential decisions, yielding, that the key diﬀerence is the inclusion of the potential decision D(a) in determining the cost of an To formalize this setting, let D(a) ∈ {0, 1} be the potential (binary) decision for individual i With this setup, the value for a policy π is: V (π) = Eπ(a | x)[u(1, d)Y (d, a) + u(0, d)(1 −Y (d, a))] {D(a) = d}. Comparing Equation (19) to the value in Equation (1) when actions are taken directly, we see action. Rather than directly assigning a cost to an action a, there is an indirect cost associated with the eventual decision D(a) that action a induces in the decision maker. Therefore, lack of identiﬁcation of the expected potential decision under an action given the covariates, d(a, x), also must enter the robust procedure. Denoting the conditional expected observed decision as d(˜π(x), x) = E[D | X = x], we can posit a model class for the decisions F d(˜π(x), x)}. We can now create a population safe policy by maximizing the worst case value across the model classes for both the outcomes M and the decisions D, By allowing for actions to aﬀect decisions through the decision maker rather than directly, the costs of actions are not fully identiﬁed. Therefore, we now ﬁnd the worst-case expected outcome and decision when determining the worst case value in Equation (20). In essence, we solve the inner optimization twice: once over outcomes for the restricted outcome model class M and once over decisions for the restricted decision model class D. stricted model classes for the outcome and decision functions, Bonferonni correction so that P (M ∈ empirical analog to Equation (20). Finally, we can incorporate experimental evidence as in Section 6.1. In this case, the conditional expected potential decision d(a, x) and outcome m(a, x) — and their model classes — are replaced with the conditional average treatment eﬀect on the decision E[D(a) − D(Ø) | X = x] and on the outcome τ (a, x). We apply the proposed methodology to the PSA-DMF system and the randomized controlled trial described in Section 2. We will focus on learning robust rules for two aspects of the PSA-DMF system: the way in which the binary new violent criminal activity (NVCA) ﬂag is constructed and the overall DMF matrix recommending a signature bond or cash bail. For both settings, we use the incidence of an NVCA as the outcome. We can treat lack of identiﬁcation of the potential decisions in a manner parallel to the outcomes. maxEπ(a | x)˜π(a | x)uY+ minEπ(a | x){1 − ˜π(a | x)}uf(a, X) From here, we can follow the development in the previous sections. We create empirical re- Figure 3: Learning a new NVCA ﬂag threshold. The x-axis shows the total number of NVCA points, x thin lines around them are point estimates an a simultaneous 80% conﬁdence interval for the partial CATE function τ (˜π(x is triggered (˜π(x for the unobservable components of the CATE, τ(1, x The purple dashed line represents the baseline policy of triggering the ﬂag when x pink dashed line is the empirical safe policy that only triggers the ﬂag when x We begin by learning a new threshold for the NVCA ﬂag. Let x NVCA points for an arrestee, computed using the point system in Table 1. Recall that the baseline NVCA algorithm is to trigger the ﬂag if the number of points is greater than or equal to 4, i.e. ˜π(x the set of threshold policies, Here, we will keep the baseline weighting on arrestee risk factors and only change the threshold α; in Section 7.2 below we will turn to changing the underlying weighting scheme. we impose a Lipschitz constraint on the CATE, following Example 2. For this model class, we need to specify the Lipschitz constants for the CATE when the ﬂag is and is not triggered (λ for τ (1, x by taking the diﬀerence in NVCA rates with and without provision of the PSA at each level of between CATE estimates, yielding λ To construct the empirical restricted model class, we set the level to 1 − α = 0.8 and construct a ) = {x≥ 4}. Our goal in this subsection is to ﬁnd the optimal worst-case policy across Having chosen the policy class Π, we need to restrict the CATE function τ(a, x). Here, . Then, we choose the Lipschitz constants to be three times the largest consecutive diﬀerence simultaneous 80% conﬁdence interval for the CATE via the Working-Hotelling-Scheﬀ´e procedure, as in Example 3, and use the upper and lower conﬁdence limits. Recall that in our parameterization we must deﬁne the diﬀerence in utilities when there is and is not a new violent criminal activity, u(a) = u(1, a) − u(0, a), for both actions a ∈ {0, 1}. Similarly, we need to deﬁne the baseline “cost” of action a, c(a) = u(0, a). While the marginal monetary cost of triggering the NVCA ﬂag versus not can be considered approximately zero given the initial ﬁxed cost of collecting the data for the PSA, there are other costs to consider. For instance, to the extent that triggering the NVCA ﬂag increases the likelihood of pre-trial detention, it will lead to an increase in ﬁscal costs — e.g. housing, security, and transportation — directly incident on the jurisdiction. Furthermore, there are potential socioeconomic costs to the defendant and their community. To represent these costs, we will place zero cost on not triggering the NVCA ﬂag, c(0) = 0, and a cost of 1 on triggering the ﬂag, c(1) = −1. We then assign an equal utility loss (i.e., a cost) from an NVCA, u(1) = u(0) = −u. This yields a utility function of the form u(y, a) = −u × y −a. We will consider how increasing the cost of an NVCA relative to the cost of triggering the ﬂag changes the policies we learn. estimates and simultaneous 80% conﬁdence intervals for the observable component of the CATE function τ(˜π(x that there is substantially more information when extrapolating the CATE for the case that the NVCA ﬂag is not triggered; this is because the point estimates do not vary much with the total number of NVCA points, and so we use a small Lipschitz constant. On the other hand, there is essentially no information when extrapolating in the other direction. Because there is a large jump in the point estimates between x means that the empirical restricted model class puts essentially no restrictions on τ(1, x assign a cost to triggering the NVCA ﬂag. To compute the empirical robust policy ˆπ we can solve Equation (16) via an exhaustive search, since the policy class Π Solving this for diﬀerent costs of an NVCA, we ﬁnd that when the cost is between 1 and 9 times the cost of triggering the ﬂag — that is, 1 < u < 9 — the new robust policy is to set the threshold to η = 6, only triggering the ﬂag for arrestees with the observed maximum of 6 total NVCA points. This is a much more lenient policy, reducing the number of arrestees that are ﬂagged as at risk of an NVCA by 95%. Conversely, when the cost of an NVCA is 9 times the cost of triggering the ﬂag or more, the ambiguity about treatment eﬀects leads the empirical safe policy ˆπ to revert to the status quo, keeping the threshold at η = 4. The next important consideration in constructing a new policy is the form of the utility function. Figure 3 shows the results. It represents the empirical restricted model class by showing point < 4: the treatment eﬀects can be anywhere between −1 and 1. While the treatment eﬀects are ambiguous, we can still learn a new threshold because we We now turn to constructing a new, robust NVCA ﬂag rule by changing the weights applied to the risk factors in Table 1. We use the same set of covariates as the original NVCA rule, represented as 7 binary covariates X ∈ {0, 1} oﬀense and 20 years old or younger, pending charge at time of arrest, prior conviction (felony or misdemeanor), 1 prior violent conviction, 2 prior violent convictions, and 3 or more prior violent convictions. The status quo system uses a vector of weights NVCA ﬂag if the sum of the weights is greater than or equal to four, i.e., ˜π(x) = the new rule has the same structure as the status quo rule, making it easier to adapt the existing system and use institutional knowledge in the jurisdiction. Speciﬁcally, we use the following policy class that thresholds an integer-weighted average of the 7 binary covariates, This policy class therefore includes the original NVCA ﬂag rule as a special case (see Table 1). In addition, to understand any diﬀerences between policies in this class, we can simply compare the vector of weights (θ We begin by deﬁning several possible models for the outcomes. We consider two models of the conditional expected potential incidence of an NVCA: an additive outcome model class M an outcome model with separate additive terms and common two-way interactions M We additionally restrict the outcome models to be bounded between zero and one. For these two model classes, we only use those cases where the NVCA ﬂag was shown to the judge. to the PSA-DMF system, we can alternatively follow the development in Section 6.1 and use the structure of the experiment to place restrictions on the eﬀect of assigning an NVCA ﬂag of 1, τ(a, x) = m(a, x) − m(Ø, x). An advantage is that we can use both the cases that did and did not have access to the PSA. We consider two diﬀerent treatment eﬀect models: an additive eﬀect Given this, a key consideration is the form of the policy class Π that we will use. We ensure that Because cases were randomly assigned to the control group for which the judge has no access Figure 4: The empirical size (as a percentage of its maximum value) of four diﬀerent model classes versus the conﬁdence level 1 −α. The green and orange lines separate the size into the component stemming from the region where the NVCA ﬂag is zero and one (a = 0 and a = 1 separately). The purple line shows the overall empirical size: the expected maximum across both levels. The empirical size when the conﬁdence level is zero serves as a proxy of the population size. For the additive models, the overall size and the size when the NVCA ﬂag is triggered are the same and fully overlap. model T Here, we also restrict the treatment eﬀects to be bounded between −1 and 1, since the outcome is binary. Note that this is not the tightest possible bound, since the restriction is that 0 ≤ m(Ø, x) + τ(a, x) ≤ 1. However, to incorporate this information in ﬁnite samples we would also have to consider the uncertainty in estimating m(Ø, x), which we would like to avoid. we can learn from this experiment. This is partly because even with inﬁnite data the models may not be identiﬁable. But, it is also because with ﬁnite data there is a diﬀerent amount of uncertainty on each model class. Figure 4 depicts this information by showing how the empirical size of the model class (vertical axis), deﬁned in Equation (13), changes with the the desired conﬁdence level 1 − α (horizontal axis). Recall that the size when the conﬁdence level is zero serves as a proxy for the population size of the model class deﬁned in Equation (9). These four model classes each lead to diﬀerent restrictions, and ultimately aﬀect what policies For the additive outcome and eﬀect models, the size is zero when the conﬁdence level is zero, implying that these models are identiﬁable. This is due to the structure of the NVCA ﬂag rule: for given values of the covariates, it is possible to observe cases with the ﬂag set to zero or one. When taking into account the statistical uncertainty, the widths increase. This is primarily due to greater uncertainty for cases with a ﬂag of 1, which account for only 16% of the cases. The size for ﬂag 1 (orange) determines the overall size (purple) and so the lines overlap. uncertainty in the treatment eﬀect and so the size of the additive eﬀect model is larger at every value of the conﬁdence level than the additive outcome model. However, the additive treatment eﬀect assumption is signiﬁcantly weaker than the additive outcome assumption. Relative to the two additive models, the second order models have signiﬁcantly more uncertainty reﬂected in larger empirical sizes. This is primarily due to the lack of identiﬁcation; even without accounting for statistical uncertainty, the widths are already over 75% of their maximum values. Indeed, there are many combinations of the binary covariates where we cannot observe cases that have an NVCA ﬂag of both zero and one. For example, there are only 28 cases (1.5% of the total) where we can identify the model for both values. Moreover, all of these cases have the same characteristics: the arrestee has committed a violent oﬀense, is over 21 years old, has a pending charge, and a prior felony or misdemeanor conviction. the empirical safe policy we learn and the true optimal policy. Unfortunately, this statistic does not describe how ﬂexible the class is and whether we should expect it to contain the true relationship between the potential outcomes and the covariates, since it only describes how much of the model is left unidentiﬁed. These considerations are crucial to guarantee robustness. The additive outcome model M by the second order interaction outcome model M rate in testing: the larger model M a greater degree of robustness, but choosing the smaller model M does indeed contain the truth. In contrast, the additive treatment eﬀect class T than the second order outcome model M make weaker assumptions without limiting our ability to ﬁnd a good policy as much. This is the key beneﬁt of incorporating the control information in this study. too large to provide any guarantees on the regret relative to the optimal policy. Therefore, for the reminder of this section, we focus on the additive treatment eﬀect model, which allows us to include the control group information and make weaker assumptions than the additive outcome model. The two model classes also diﬀer in how they vary with the conﬁdence level; there is more The empirical size of the model class gives an indication of the potential optimality gap between Choosing between these two models is akin to choosing to control the risk of a type I or II error Note that the second order treatment eﬀect model, which makes the weakest assumptions, is Figure 5: The diﬀerence between the robust policy and the original NVCA ﬂag rule as the cost of an NVCA increases from 100% to 1,000% of the cost of triggering the NVCA ﬂag, and the conﬁdence level varies between 0% and 100%. The shading in the left panel shows the percentage of recommendations that diﬀers between the two policies; in the right panel it shows how much the robust policy improves the worst case value relative to the original rule. In all cases, the robust policy changes the ﬂag from a “Yes” to a “No.” Figure 5 shows how the robust policy, which solves the optimization problem given in Equation (16) with the additive treatment eﬀect class T NVCA −u (horizontal axis) and the conﬁdence level 1−α (vertical axis). With the integer-weighted policy class Π solver (Gurobi Optimization, LLC, 2021). The left panel shows the percent of recommendations changed from the original ones, while the right panel displays the improvement in the worst-case value over the original NVCA ﬂag rule. Across every conﬁdence level, the robust policy diﬀers less and less from the original rule as the cost of an NVCA relative to the cost of triggering the ﬂag increases. For a given cost of an NVCA, policies at lower conﬁdence levels are more aggressive in deviating from the original rule, prioritizing a potentially lower regret relative to the optimal policy at the expense of guaranteeing that the new policy is no worse than the original rule. level, as the cost of an NVCA increases. In the limiting setting where an NVCA is given the same cost as triggering the NVCA ﬂag, the robust policy diﬀers substantially from the original rule, placing no weight on prior violent convictions. Once the cost is at least ﬁve times the cost of triggering the ﬂag, the robust policy reduces to the original rule. For intermediate values, the robust policy places less — but not zero — weight on the number of prior violent convictions than the original rule. (a) Change from the original rule(b) Improvement over the original rule Figure 6 inspects the integer-weights on the risk factors for the robust policy at the 1−α = 80% Figure 6: Change in the robust NVCA ﬂag weights θ in Equation (22) as the cost of an NVCA increases from 100% to 1,000% of the cost of triggering the NVCA ﬂag, at a conﬁdence level of 1 − α = 80%. uncertainty in the eﬀect of triggering the NVCA ﬂag relative to not. When the cost of an NVCA is low, the robust policy will not trigger the NVCA ﬂag for cases that triggered the original ﬂag; even with the increased uncertainty, it is preferable in these cases to not trigger the ﬂag. Conversely, when the cost of an NVCA is high the increased uncertainty in the eﬀect of triggering the ﬂag makes the robust policy default to the original rule. In this case, the high costs of an NVCA make any change in the policy too risky to act upon. So far, we have only considered the outcomes of triggering the NVCA ﬂag and have assigned costs directly to the ﬂag. However, the PSA serves as a recommendation to the presiding judge who is the ultimate decision maker. Following the discussion in Section 6.2 we can incorporate this into the construction of the robust policy. We use the judge’s binary decision of whether to assign a signature bond or cash bail, and place a cost of −1 to assigning cash bail. Unlike the cost on the NVCA ﬂag above, this allows us to address the costs of detention directly. As discussed above, the cost on the judge’s decision to assign cash bail may include the ﬁscal and socioeconomic costs. account for increasing costs of an NVCA relative to assigning cash bail, at various conﬁdence levels. However, for the additive and second order eﬀect models we ﬁnd policies that diﬀer from the original rule only when we do not take the statistical uncertainty into account — with conﬁdence level 1 −α = 0 — and have no ﬁnite sample guarantee that the new policy is not worse than the In light of the empirical sizes displayed in Figure 4, this behavior is primarily due to increased Following the same analysis as above, we can ﬁnd robust policies that take the decisions into Figure 7: Upper bound on the treatment eﬀects under the additive model τ NCA scores. Values below and to the right of the dashed white line are areas where cash bail is recommended, and the bounds are on the eﬀect of recommending a signature bond. Values above and to the left are areas where a signature bond is recommended, and the bounds are on the eﬀect of recommending cash bail. existing rule. In this case, the policy is extremely aggressive, responding to noise in the treatment eﬀects. Otherwise, we cannot ﬁnd a new policy that safely improves on the original rule. This is primarily because the overall eﬀects of the PSA on both the judge’s decisions and defendants behavior are small (Imai et al., 2020); therefore there is too much uncertainty to ensure that a new policy would reliably improve upon the existing rule. Another key component of the the PSA-DMF framework is the overall recommendation given by the DMF matrix (see Figure 1). This aggregates the FTA and NCA scores into a single recommendation on assigning a signature bond versus cash bail. We now consider constructing a new DMF matrix based on the FTA and NCA scores, which we combine into a vector (x restrict our analysis to the 1,544 cases that used the DMF matrix rather than those that cash bail was automatically assigned. nents of the DMF decision matrix. Because x parameterize the additive terms as six dimensional vectors. Importantly, this rules out interactions between the FTA and NCA scores in the eﬀect. If this assumption is not credible, we could use a Lipschitz restriction as in Example 2. This alternative assumption may be signiﬁcantly weaker, though it would require choosing the Lipschitz constant. inspect the upper bounds on the treatment eﬀects as the conﬁdence level changes. Figure 7 shows these bounds for the diﬀerent values of the FTA and NCA scores. As in Section 7.2 above, the Here, we again focus on the class of additive treatment eﬀect models τ(a, x) = τ(a, x)+ (a, x), where we only condition on the FTA and NCA scores since they are the two compo- To understand how this additive treatment eﬀect model facilitates robust policy learning, we bounds with zero conﬁdence level correspond to bounds induced by the model class in the population. Because we can never observe a case where the DMF recommends a signature bond with either an FTA score or NCA score above 4, we cannot identify the additive model components for either variable above 4. Because of this, the upper bound on the eﬀect of recommending a signature bond for these cases is 1, the maximum value. FTA score below 2 or an NCA score below 3. This precludes assigning cash bail to these cases. In the middle is an intermediate area with FTA scores between 2 and 4 and NCA scores between 3 and 4 where we can fully identify the eﬀect of assigning cash bail under the additive model. However, for values with an FTA score of 2 or an NCA score of 3, there is a signiﬁcant amount of uncertainty due to small sample sizes. Indeed, there are only 3 cases where cash bail is recommended that have an NCA score of 3 and 2 cases that have an FTA score of 2. covariates. This monotonic policy class contains the DMF matrix rule as a special case and incorporates the notion that no case should move from a cash bail to a signature bond recommendation if the risk of an FTA or NCA increases. Formally, this monotonic policy class is given by, As in Section 7.2, we consider parameterizing the utility in terms of a ﬁxed cost of 1 for recommending cash bail — reﬂecting the ﬁscal and socioeconomic costs of detention — and varying the cost of an NVCA. conﬁdence levels. In the limiting case where the cost of an NVCA is equal to recommending cash bail, the safe policy is to assign a signature bond for all but the most extreme cases. This is because even if assigning a signature bond is guaranteed to lead to an NVCA, the utility is equal to assigning cash bail and not leading to an NVCA. sample statistical guarantees and set the conﬁdence level to 0. That is, we ignore any statistical uncertainty in estimating the conditional expectation function, and instead use the point estimate directly. When doing this, increasing the cost of an NVCA relative to recommending cash bail leads to more of the intermediate area with FTA scores between 2 and 4 and NCA scores between 3 and 4 being assigned cash bail, until the cost is high enough that the entire identiﬁed area is assigned cash bail. However, this does not hold up to even the slightest of statistical guarantees due to the uncertainty in the treatment eﬀects. Because the eﬀects of assigning cash bail are both small and uncertain, the robust policy reduces to the existing DMF matrix. Similarly, we can never observe a case where the DMF recommends cash bail with either an To search for new policies, we consider a policy class that is monotonically increasing in both = {π(x) ∈ [0, 1] | π(x, x) ≤ π(x+ 1, x) and π(x, x) ≤ π(x, x+ 1)}. Figure 8 shows the robust policies learned for the varying cost of an NVCA and diﬀerent Figure 8: Robust monotone policy recommendations under an additive model for the treatment eﬀects, as the cost of an NVCA and the conﬁdence level vary. The dashed black line indicates the original decision boundary between a signature bond (above and to the left) and cash bail (below and to the right). In recent years, algorithmic and data-driven policies and recommendations have become an integral part of our society. Being motivated in part by this transformative change, the academic literature on optimal policy learning has ﬂourished. The increasing availability of granular data about individuals at scale means that the opportunities to put these new methodologies in practice will only grow more in the future. to ensure that it does not perform worse than the existing policy. This safety feature is critical, especially if relevant decisions are consequential. In this paper, we develop a robust optimization approach to deriving an optimal policy that has a statistical safety guarantee. This allows policy makers to limit the probability that a new policy achieves a worse outcome than the existing policy. randomized experiment for ethical and logistical reasons. Observational studies bring additional uncertainty due to the lack of identiﬁcation. Moreover, for transparency and interpretability, most policies are based on known, deterministic rules, making it diﬃcult to learn a new policy using standard methods such as inverse probability-of-treatment weighting. We develop a methodology that addresses these challenges and apply it to a risk assessment instrument in the criminal justice system. Our analysis suggests an opportunity for improving the existing scoring rules. One important challenge when learning and implementing a new policy in the real world is The development of a safe policy is essential particularly when it is impossible to conduct a The structure of the baseline will determine what is identiﬁable and what is not. For example, in the PSA-DMF system we explore here, we were able to fully identify additive models because the NVCA scoring rule incorporates several risk factors and no single risk factor guarantees that the ﬂag will ﬁre. On the other hand, we could not fully identify an additive model for the DMF matrix because if either the NCA or FTA scores are large enough, the recommendation is always cash bail. This logic extends to higher dimensions. For example, we could not identify many terms in the interactive eﬀect model because most combinations of two risk factors result in an NVCA ﬂag. Therefore, this framework is likely to be most successful for policies based on several covariates that are aggregated to a single score before thresholding. tation choices under the proposed approach. While we consider several representative cases, there are many other structural assumptions that would lead to diﬀerent forms of extrapolation. For instance, we could consider a global structure in the form of Reproducing Kernel Hilbert Spaces, or incorporate substantive restrictions such as monotonicity. In addition, while our study on pre-trial risk scores focused on discrete covariates, deterministic policies with continuous covariates opens the opportunity to directly identify treatment eﬀects on the decision boundary, leading to a different form of restriction on the model class. We can also generalize this approach to consider cases where policies consist of both stochastic and deterministic components. This would nest the current deterministic case including the experimental setting discussed in Section 6.1. considering long term societal outcomes rather than short term ones. For example, pre-trial detention brought on by a recommendation may in turn alter the long term behavior and welfare of an arrestee. Understanding how to design algorithms when they aﬀect decisions that mediate future outcomes is key to ensuring that recommendations do not take a myopic view. One potential way to incorporate long term outcomes may be with the use of surrogate measures. More work needs to be done on the question of how to incorporate surrogate measures into our policy learning framework while providing a safety guarantee. expanded. In this paper, we consider policies to be safe if they do not lead to worse outcomes on average; however, this does not guarantee that outcomes are not worse for subgroups. A more equitable notion of safety would be to ensure safety across subgroups, though doing so may reduce the ability to improve overall welfare. Similarly, the robust optimization framework can be made to incorporate statistical fairness criteria — a diﬀerent form of safety. Such constraints may be themselves uncertain or only partially identiﬁed, and so a robust approach would account for this as well (Imai and Jiang, 2020). An important aspect of this methodology is that it depends on the design of the baseline policy. There are several avenues for future research. The ﬁrst set of questions relates to the implemen- Second, there are many ways in which optimal algorithmic recommendations may diﬀer when Finally, within the robust optimization framework, the notion of “safety” can be considerably