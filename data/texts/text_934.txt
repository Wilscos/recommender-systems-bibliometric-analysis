Noname manuscript No. (will be inserted by the editor) Abstract Sources of complementary information are connected when we link user accounts belonging to the same user across diﬀerent platforms or devices. The expanded information promotes the development of a wide range of applications, such as cross-platform prediction, cross-platform recommendation, and advertisement. Due to the signiﬁcance of user account linkage and the widespread popularization of GPS-enabled mobile devices, there are increasing research studies on linking user account with spatio-temporal data across location-aware social networks. Being diﬀerent from most existing studies in this domain that only focus on the eﬀectiveness, we propose a novel framework entitled HFUL (A Hybrid Framework for User Account Linkage across Location-Aware Social Networks), where eﬃciency, eﬀectiveness, scalability, robustness, and application of user account linkage are considered. Speciﬁcally, to improve the eﬃciency, we develop a comprehensive index structure from the spatio-temporal perspective, and design novel pruning strategies to reduce the search space. To improve the eﬀectiveness, a kernel density estimation-based method has been proposed to alleviate the data sparsity problem in measuring users’ similarities. Additionally, we investigate the application of HFUL in terms of user prediction, time prediction, and location prediction. The extensive experiments conducted on three real-world datasets demonstrate the superiority of HFUL in terms of eﬀectiveness, eﬃciency, scalability, robustness, and application compared with the state-of-the-art methods. Keywords User Account Linkage · Social Networks · Location Data The proliferation of GPS-enabled devices, such as vehicles, mobile phones, and smart bracelets, leads to the increasing availability of location data from two perspectives: 1) the volume of location data increases unprecedentedly; 2) the resources of location data tend to be more diverse. Recently, much more location data has been generated by newly-emerging location-aware social networks (LBSNs), such as Foursquare, Twitter, and Instagram. Many users have registered accounts on these platforms, and posted their statuses associated with location information, referred as “check-ins”. Compared with other online activities, such as commenting, tagging, and following, “check-ins” bridge the gap between the real world and the virtual world with the geographical data [1]. The study of check-in data provides an unprecedented opportunity to analyze users’ real world behaviors and potentially improve a variety of location-aware services [2] [3]. For example, in [4], check-in records are used to link user accounts across diﬀerent platforms. Obviously, compared with the information collected from one speciﬁc platform, we can obtain more comprehensive user information after identifying and linking user accounts across platforms, since the sources of complementary information are integrated. From a commercial perspective, this expanded information will beneﬁt many location-aware applications, such as maps and cross-platform recommendation. Consequently, linking user accounts across location based social networks has attracted increasing attention. However, despite of the signiﬁcance of the study, following inevitable problems bring great challenges for this work. Data Sparsity. The density of the check-in data for each user is of critical importance to user account linkage across LBSNs. This is because we can model a user’s real behaviors more precisely with a denser dataset, which enables us to link user accounts across diﬀerent platforms more accurately. Unfortunately, user-generated check-in datasets are extremely sparse. Compared with the traditional GPS datasets, where users’ geographical locations are automatically recorded by the GPS devices and the time periods between two consecutive points are usually short, the check-in process is userdriven on location-aware social networks, i.e., users decide whether to check in at a speciﬁc place or not. Such user-driven mechanism leads to the data sparsity problem from the following aspects. First, the number of check-in records generated by each user is rather limited, as many users are reluctant to post their statuses [5]. Second, the spatial span of check-in data is extremely large [1]. For example, a user may usually check in at Boston, but with the latest check-ins at California. Third, the time spans between consecutive check-ins are usually wide, where some users even have more than one-year gaps between consecutive checkins [1] [6]. All these behaviors lead to the signiﬁcant sparsity of geographical data on location-aware social networks, and bring enormous diﬃculty for precise linkage. To illustrate the data sparsity problem of user checkins more clearly, we conduct an analysis on two real cross-platform datasets, i.e., the dataset FoursquareTwitter (FS-TW) provided by [7] [4] and the dataset Instagram-Twitter (IT-TW) provided by [4]. The analysis results are presented in Figure 1. The distribution of the average number of check-in records is presented in Fig. 1(a). Obviously, most users of the given datasets have a small volume of check-in records (i.e., usually less than 50 check-in records). Fig. 1(b) reveals the data sparsity problem from a diﬀerent perspective, where the density is deﬁned as the number of check-in records per km. For most users, they have less than 0.2 check-ins in one km. Data Imbalance. Data imbalance in user account linkage refers to the phenomenon that, the number of check-in records of the same user is diﬀerent across different platforms. This is because a user is not likely to post the check-in about the same activity many times across diﬀerent platforms, i.e., some of a user’s checkin records are missing on certain platforms. For example, many users have registered accounts on Facebook, Twitter, and Instagram simultaneously, yet they may just select one platform to post a check-in after taking an activity in a venue or location. Moreover, users’ consistent preferences make the situation worse. Users’ consistent preferences refer to that users may prefer one platform to post their check-ins consistently, which can be caused by various factors (e.g., the inﬂuence of social friends). For example, if most of a user’s friends use Facebook, he/she may always prefer posting check-ins on Facebook, which means that most of his/her activities are missing on other platforms. Negative Coincidence. Negative coincidence occurs when two diﬀerent user accounts happen to have check-in records at same places many times [2]. Such phenomenon tends to happen at popular and crowded places (e.g., supermarkets, cafeteria, and schools), where many users tend to visit repeatedly [8] [9] and share their statuses associated with location data. Although many records are generated at these places, we cannot distinguish users from each other, due to the low discrimination of these check-ins. Low Veracity The instability of GPS-enabled devices leads to the low quality of spatio-temporal data, where there usually exist some outliers. Obviously, data outlier is a big negative factor for precise user linkage. Unfortunately, such a phenomenon is ubiquitous across diﬀerent location-aware social networks. A straightforward approach to discover two actually linked users is to measure their similarity by comparing the common locations that they both have visited. However, users normally share few co-visited locations across diﬀerent platforms due to the data sparsity and data imbalance problems. To overcome the challenge, we propose a kernel density estimation (KDE) based method to accurately characterize the spatial and temporal patterns of an individual’s check-in activities and then perform user account linkage based on these patterns, inspired by [3]. Although KDE is able to alleviate the data sparsity, this approach is inherently time consuming [10] [11] [12], and the complexity is O(mn) as presented in Theorem 1. To improve the computational eﬃciency, we propose an index-based KDE. Speciﬁcally, we divide the space into d ×d grid cells and M time periods, and then each user is represented by a sequence of grid cells and time periods with corresponding probability. Compared with representing a user with a sequence of check-in records, the index based method is more eﬃcient as the total number of grid cells and time periods is much smaller than that of check-ins. To further improve the eﬃciency, we propose a novel pruning strategy to reduce the number of account pairs to be measured, where only topk nearest neighbors of a user account are considered. Another beneﬁt brought by index-based KDE is to relieve the data sparsity and imbalance problem. This is because although a user often posts diﬀerent check-in activities on diﬀerent social network platforms, the spatial distribution (e.g., the cell distribution) of his/her check-in records generated on each platform tends to be similar to each other [13]. To improve the eﬀectiveness, we ﬁrstly develop a density based method to delete outliers, where abnormal grid cells and time periods are pruned before the calculation of user account similarity. Additionally, we design an entropy-based weight scheme for locations and grid cells to reduce the impact of negative coincidence. As we introduced before, people tend to visit popular locations, which leads to the location coincidence. Obviously, these locations usually have large entropy in terms of the visited users. However, they are less useful in distinguishing users from each other. In contrast, private locations (e.g., homes, oﬃces, etc.) are more discriminative in identifying users and usually have smaller entropy in terms of the visited users. Therefore, the key idea of our entropy-based weight calculation is to penalize locations and grid cells with high entropy by assigning low weights. Apart from the evaluation of eﬀectiveness of the proposed framework on several real-world datasets, some carefully generated synthetic/noisy datasets are also used to evaluate the scalability and robustness of HFUL. Additionally, we study the application of HFUL by user prediction, location prediction, and time prediction. Diﬀerent from most existing studies that mainly consider the eﬀectiveness of user account linkage, our proposed framework HFUL has taken more factors into account. 1) Eﬃciency. The time cost of user account linkage is a signiﬁcant factor for HFUL, since it means whether the framework can be widely used in many real applications. 2) Eﬀectiveness. The eﬀectiveness of user account linkage refers to the precision, recall, and F1. To achieve high eﬀectiveness, we propose a novel kernel density estimation method for user account similarity calculation, and design a Renyi entropy-based weighting scheme to account for the importance of each extracted feature. 3) Scalability. The scalability, which is also an important factor that aﬀects the availability of the proposed framework in real applications, refers to the stable performance of HFUL when there are a large-scale of user accounts to be measured and linked. 4) Robustness. The robustness refers to the reliable performance of HFUL while linking user accounts from datasets with many noisy records. 5) Application. Successfully linking users across diﬀerent social platforms is able to beneﬁt many real-world applications. In this paper, we show how to extend HFUL to several real applications including user prediction, time prediction, and location prediction. In this study, several approaches have been proposed to tackle the challenges that we are facing in user account linkage across location-aware social networks. To sum up, we make the following contributions. – We are the ﬁrst to propose a hybrid framework to perform user account linkage with spatio-temporal data by considering eﬀectiveness, eﬃciency, scalability, robustness, and application simultaneously. – To tackle the data sparsity problem, we design a novel algorithm based on kernel density estimation. To tackle the data imbalance problem, we construct new index structures in both spatial and temporal domain. Furthermore, we design an entropy-based weight scheme with the goal of alleviating the challenges caused by negative coincidence. – We reduce the computational complexity of the proposed framework by designing a novel pruning strat- egy to retrieve top-k candidates for each user account. – We have conducted extensive experiments, where the real-world datasets based results demonstrate the high performance of our proposed framework HFUL in terms of eﬀectiveness, eﬃciency, and application; the synthetic datasets based results demonstrate the high robustness and scalability of HFUL. Compared to the conference version of this study [14], we make the following improvements: – Apart from spatial features, temporal features are also taken into account for better measuring the similarity between diﬀerent user accounts. We characterize users from a comprehensive perspective by constructing a spatio-temporal index in Section 7, instead of characterizing them only based on spatial information. – An outlier detection method is proposed to improve the eﬀectiveness of user account linkage, where abnormal records are detected and removed before measuring user account similarity. A novel pruning strategy is developed to reduce the number of user accounts pairs to be measured from mto mk, where m and k denote the number of user accounts on each platform and neighbors to be considered respectively. . – To study the applications of our work, we investigate the user prediction, location prediction, and time prediction on the cross-platform datasets by fusing check-ins belonging to the same user after linkage. – Eﬃciency, eﬀectiveness, scalability, robustness, and application of our proposed framework are investigated in experiments. The rest of the paper is organized as follows. We present the related work in Section 2, and formulate the problem in Section 3. The overview of this work is presented in Section 4, and the KDE-based user similarity is introduced in Section 5. We construct the spatial and temporal index structures in Section 6, and optimize the proposed framework in Section 7, and investigate the application of HFUL in Section 8. The experiments are conducted in Section 9, and the paper is concluded in Section 10. The related studies, which contain the cross-platform user account linkage and kernel density estimation in spatio-temporal database, are discussed in this section. The increasing popularity of social networks has enabled more and more people to participate in multiple online services [15] [16]. Linking the same users across diﬀerent platforms brings a great opportunity to fully understand users’ behaviors and provide better recommendations [17] [18] [19]. The study is ﬁrstly proposed in [20], where cross-community identities are connected with corresponding websites by measuring the identity similarity with usernames. Vosecky et al. [21] proposed a method to identify users based on web proﬁle matching and further extended its eﬀectiveness by incorporating the user’s friend network. To investigate whether users can be identiﬁed across systems based on their tag-based proﬁles, an aggregate proﬁle was constructed by combining usernames and user tags [22]. Following these studies, more abundant information was considered to link user accounts [23] [24] [25] [26] [27]. To build a comprehensive user proﬁle for improving online services, various sources of complementary information were integrated [24], where the username features, prior-username features, and the relation between the candidate usernames and prior usernames were taken into account. To match user accounts from diﬀerent online social networks, Peled et al. [25] used supervised learning techniques to construct diﬀerent classiﬁers, where three main types of features were utilized, i.e., name based features, social network topological based features, and general user info based features. To address the multi-platform user identity linkage problem, Mu et al. [28] proposed two eﬀective algorithms, a batch model ULink and an online model ULink-On, based on latent user space modeling. Inspired by the fast development of the embedded technology, some people turn to investigate the cross-platform linkage with embedding [29] [30] [31] [32] [33] [34]. A novel framework called “factoid embedding” is proposed by [30], the core idea of the work is that each piece of information about a user identity describes the real identity owner, and thus distinguishes the owner from others. An attention-based network embedding model was proposed in [32], and the study contains two main components: a masked graph attention mechanism and an embedding algorithm which tries to learn a common vector space. Fu et al. [34] proposed a deep multi-granularity graph embedding model DeepMGGE, which utilizes the random walk to capture the higher-order structural proximities. In recent advances [35] [4] [36], researchers focused on using location data to achieve user account linkage. By utilizing the user-generated location data in social media platforms, a co-clustering-based framework was proposed [35], where account clusterings in spatial and temporal and dimensions were carried out synchronously. To address the challenges in general crossdomain case, where users have diﬀerent proﬁles independently generated from a common but unknown pattern, a generic and self-tunable algorithm that leverages any pair of sporadic location-aware datasets was proposed to determine the most likely matching between users [4]. To answer the question: is movement history suﬃciently representative and distinctive to identify an individual? Jin et al. [37] formalized the problem of moving object linking as a k-nearest neighbor search on the collection of signatures, and aimed to improve eﬃciency considering the high dimensionality of signatures and the large cardinality of the candidate object set. To estimate two users whether have a social link, Zhang et al. [38] devised a novel multiview matching network MVMN, which contains three components, i.e., location matching module, time-series matching module, and relation matching module. Kernel density estimation (KDE) is a statistical technique for estimating a probability density function from a random sample set [39] [40]. As a common tool, KDE has been explored in various areas for diﬀerent purposes [11], especially in spatio-temporal database [41] [3] [42] [43] [44]. To study the personalized geographical inﬂuence of locations on a user’s behaviors, the kernel density estimation was used to model the personalized distribution of the distance between any pair of locations [41]. To understand urban human activity and mobility patterns, Hasan et al. [45] applied a two dimensional Gaussian kernel to estimate the checkin density of each grid cell. To investigate the spatiotemporal clustering of trajectories, a Gaussian kernel function was used to calculate the spatio-temporal kernel density of each trajectory unit [46]. To determine the geographical point of a text document, Hulden et al. [42] investigated an enhancement of common methods by kernel density estimation. To address the limitations of existing methods on understanding traﬃc accidents occur, a new method called Spatial-Temporal Network Kernel Density Estimation (STNKDE) is proposed in [47]. To examine whether a kernel density map could be reverse-transformed to its original map with discrete crime locations, Wang et al. [48] used the Epanecknikov, a default setting in ArcGIS for density mapping, to examine its impact on the deconvolution process. A new AE location method using tri-variate kernel density estimator was developed in [49], and the experimental results veriﬁed that the proposed method was more accurate and eﬀective than traditional methods in the location performance. Coleman et al. [50] proposed RACE, an eﬃcient sketching algorithm for kernel density estimation on high-dimensional streaming data. The algorithm compresses a set of N high dimensional vectors into a small array of integer counters, and the array is suﬃcient to estimate the kernel density for a large class of kernels. Connecting user accounts across diﬀerent social platforms has been well studied by previous work from diﬀerent perspectives, yet there is no study considers the eﬀectiveness, eﬃciency, scalability, robustness, and application of user account linkage synchronously in spatio-temporal domain. Consequently, we propose the framework HFUL in this work to address the issue. In this section, we ﬁrst present the notations that used throughout the paper in Table 1 and then formulate the problem. On social networks, many users share ideas and check in after taking activities at a place. Then, the following information will be recorded and sent to the server: a unique user account id that distinguishes it from others; location information that consists of latitude and longitude; and time-stamp of the check-in record [2]. Deﬁnition 1 Check-in Record. A check-in record of a user is deﬁned as r= (l, t), where u is a user account, l is deﬁned as (lat, lng) with lat represents latitude and lng represents longitude, and t is the timestamp. Note that, the time-stamps can be used to distinguish records from each other. For instance, given two records r= (l, t) and r= (l, t), they are deﬁned as diﬀerent records if t6= teven though u= uand l= l. This deﬁnition is appropriate, as a user may frequently check in at the same place where he/she usually visits. The semantic information behind the records with the same location may be diverse. For example, a user may check in at a cafeteria on Monday, but check in at the same place on Sunday. Problem Formulation. Given two user account sets U= {u, u, ··· , u} and U= {u, u, ··· , u} on two diﬀerent location-aware social networks, where each user account is associated with a set of check-in records, our goal is to identify all account pairs (u, u) of the same user from O = {(u, u)|u∈ U, u∈ U}, on condition that S(u, u) ≥ S, where Sis a given similarity threshold. To link user accounts across diﬀerent social platforms with location data, we propose the framework HFUL, which is composed of the following four main components as illustrated in Fig. 2. KDE-based Similarity. To tackle the problem “data sparsity” introduced in Section 1, we propose a kernel density estimation-based algorithm, where the similarity between two user accounts is directly measured based on the naive KDE introduced in Section 5. Index Construction. To reduce the high computational complexity brought by kernel density estimation, we design two indexes, i.e., grid map and time period structure, to organize the input data with grid and time representation. Optimization. The proposed framework is optimized with the following steps: 1) instead of computing spatial and temporal similarity independently, we measure the user account similarity by considering these two parts of information simultaneously; 2) a density based clustering is developed to detect outliers; 3) an entropy based method is proposed to compute the weight for each grid cell and time period; 4) a novel algorithm is developed to retrieve candidates and an upper bound is proposed to further reduce the number of candidates to be measured. Application of HFUL. We obtain abundant data for each user following linkage, based on the cross-platform dataset we investigate the application of HFUL, i.e., user prediction, location prediction, and time prediction. In this section, we propose a kernel density estimationbased solution to perform user account linkage across location-aware social networks. The intuition behind our solution is that given two user accounts uand uof the same user on two diﬀerent LBSNs, the distributions of her/his generated check-in records on the two LBSNs are similar to each other, even if the user posts diﬀerent check-ins on these two platforms. For each user account pair (u, u) in the Cartesian product O = U×U= {(u, u)|u∈ U, u∈ U}, we ﬁrst compute their KDE-based similarity S(u, u). Then, based on the inferred similarity and a user-deﬁned similarity threshold S, we decide whether these two accounts belong to the same user. A straightforward way to measure the similarity between two user accounts with discrete check-in records is to directly compare the records happened at same locations. Unfortunately, as discussed in Section 1, user generated check-in records on location-aware social networks are extremely sparse. Moreover, the issue of data missing worsens the situation. In light of these two challenges, we propose a kernel density estimation (KDE) based solution, inspired by its success in modeling individuallevel location data [3]. Kernel density estimation is a non-parametric method for estimating the probability density function of a sample set with unknown distribution. Given a set of locations L = (l, l, ··· , l) and a location l, where each location is a two-dimensional tuple in the form of (lat, lng), the density of lover L is estimated as follows: where K(·) is the Gaussian kernel function, h is a bandwidth parameter, and (l−l) is deﬁned as the Euclidean distance between land l. We use the probability density function f(l|L, h) to denote the similarity between L and l. The similarity value f(l|L, h) will be large if lis close to the points in L. In contrast, the value of f(l|L, h) is small if points in L are far away from l. Given two user accounts uand uwith check-in record sets R= (r, r, ··· , r) and R= (r, r, ··· , r), the spatial similarity between uand uis deﬁned as: where r.ldenotes the location of the record r. Similarly, we deﬁne the temporal similarity S(u, u) as follows by replacing location in Eq.(3) with time-stamp. Then, the similarity between uand uis deﬁned as: Kernel density estimation is an important statistical technique in data analysis. According to Eq. (5), it requires 2nn(|R| = nand |R| = n) kernel evaluations to measure the similarity S(u, u), and the complexity of this method is O(mn) as presented in Theorem 1. Obviously, the naive evaluation of KDE is very time consuming, especially for large-scale datasets with millions of check-ins. To speed up the evaluation of KDE, we propose novel index structures to organize the spatial and temporal data. As shown in the example of Fig. 3(a), we divide the space into 10×10 square cells, and the grid id and record information are presented in Fig. 3(b). By assigning each cell a unique numerical id from bottom to top and from left to right, uand uare represented by a set of discrete cells, i.e., {2, 73, 88, 38} and {24, 73, 78, 38}. Similarly, in temporal domain, we divide time into different periods. In Fig. 4, the time is divided into M periods {T, T, ··· , T}. Then, each user can be represented by a set of time periods when he/she has visited. Due to the personal interests and geographical inﬂuence [9], the probabilities that a user visits diﬀerent places are diﬀerent in real life. Even though the same location, the user may visit at diﬀerent time periods. As a result, we propose to compute the grid and time period probability for each user account based on Eq. (6). Deﬁnition 2 Grid and Time Period Probability. Given a user account u with a set of check-in records R= (r, r, ··· , r), the probability of each grid cell and time period visited by u is deﬁned as: where Θ(g) denotes the number of records falling into the grid cell g, and Θ(T) represents the number of records falling into the time period T. Based on the grid id and the calculated grid probability, the grid representation of each user account is deﬁned as G(u) = {(g, $(g)), (g, $(g)), ··· , (g, $(g))}. Continue the example in Fig. 3, we have G(u) = {(2, 0.2), (73, 0.2), (88, 0.4), (38, 0.2)} and G(u) = {(24, 0.2), (73, 0.2), (78, 0.4), (38, 0.2)}. Based on the grid representation, we redeﬁne the computation of KDE and similarity S(u, u) as follows: where the grid representations of uand uare G(u) = {(g, $(g)), (g, $(g)), ··· , (g, $(g))} and G(u) = {(g, $(g)), (g, $(g)), ··· , (g, $(g))}, respectively. (g−g) denotes the Euclidean distance between the center coordinates of cells gand g. Compared with the naive evaluation of KDE, the grid representation is a coarse-grained method, where the grid cell is the basic unit that may contain many points. Note that, implementing KDE with grid representation is able to: (1) reduce the number of kernel function evaluation, since we have |G(u)| ≤ |R| for each user account; (2) alleviate the data imbalance problem, since the probability that a user visits a speciﬁc geographical region tends to be similar across two LBSNs [13]. Grid-based kernel density estimation is more eﬃcient than the method in Section 5, especially given a large dataset. Similarly, in temporal domain, we can redeﬁned the computation of Gaussian kernel function K(·) and similarity S(u, u) by representing each user account with a sequence of time periods with corresponding probability as follows: where the time period representations of uand uare assumed as Γ (u) = {(T, $(T)), (T, $(T)), ··· , (T, $(T))} and Γ (u) = {(T, $(T)), (T, $(T )), ··· , (T, $(T))}, respectively. Mand Mdenote the number of time periods containing at least one check-in of uand urespectively, and (T−T) denotes the Euclidean distance between the centers of periods Tand T. By constructing the time period structure, we can also improve the eﬃciency and alleviate the data imbalance problem in temporal domain. Following the redeﬁnition of S(u, u) and S(u, u), the similarity S(u, u) is redeﬁned as: By constructing the index structure, we can improve the eﬃciency of user account linkage. This is because the number of grids and time periods is less than that of check-in records. Next, we use the following outlier detection and pruning strategy to optimize the eﬀectiveness of HFUL. 7.1 Spatio-temporal information based comprehensive user account similarity To ﬁnd the actually linked user accounts across diﬀerent social platforms, the aforementioned methods calculate the spatial similarity S(u, u) and temporal similarity S(u, u) based on location and time information respectively. To improve the eﬃciency, the grid and time period structures are developed to organize the input data with corresponding representations. However, in real life, a check-in record is by default a spatiotemporal event. The location (lat, lng) and time t of a check-in record are usually sent to the server synchronously if a user decides to share his/her status associated with an address in real life. Consequently, to exploit users’ behaviors more precisely and achieve higher performance (i.e., precision, recall, and F1), we consider the spatial and temporal information simultaneously during the measurement of user account similarity. Continue the example in Fig. 3, instead of mapping the check-in records of a user into grid cells and time periods independently, each user is characterized from a joint spatio-temporal perspective. As shown in Fig. 5, we extract the time distribution of a user in each grid cell. Given a set of check-in records R= (r, r, ··· , r) of u, the spatio-temporal representation GT (u) of a user account is deﬁned as: Γ (g) = {(T, $(T)), ··· , (T, $(T))} GT (u) = {(g, $(g), Γ (g)), ··· , (g, $(g), Γ (g))} where |R| is the number of check-ins of u in g, Θ(T) and Mdenotes the number of records falling into the period Tand periods containing at least one record in grespectively. Then, the Gaussian kernel function is redeﬁned as: where the parameter α(0 ≤ α ≤ 1) is a trade-oﬀ between the spatial and temporal information, which is obtained by maximizing the F1 score in experiments. $(T) denotes the probability of time period Tin grid g. Next, we have the new similarity S(u, u) by replacing the Gaussian kernel function in Eq. (7). Compared with the method in Section 6, where the minimum granularities are (g, $(g)) and (T, $(T)) in spatial and temporal domain respectively, the comprehensive granularity (g, $(g), Γ (g)) based approach is more likely to link user accounts precisely. Due to the instability of GPS-enabled devices, there usually exist some outliers in spatio-temporal data. As these outliers are negative factors for precise linkage, we develop the following approach to detect and remove anomalous grids before computing similarity. We design a novel method based on an advanced density-based clustering (DP) [51], and the reason behind choosing to augment DP for detecting outliers is twofold. 1) There usually exist some outliers on real spatio-temporal datasets due to the instability of GPSenabled devices [52] [53], and DP has been proved to be able to ignore anomalous points [51]. 2) Many clustering algorithms require the user to set many parameters, while only two parameters are necessary for the DP algorithm [54]. The idea of DP is: cluster centers are surrounded by neighbors with lower local density and that they are at a relative large distance from any points with a higher local density. Given a user account u with spatio-temporal representation GT(u) = {(g, $(g), Γ (g)), ··· , (g, $(g), Γ (g))}, we ﬁrstly compute the local density pand the distance from grids with higher local density δfor each grid cell with following equations, p=χ(d− d),χ(x) = 1, if x < 0χ(x) = 0, otherwise min(d), if p> p δ=max(d), otherwise(11) where dis the cutoﬀ distance given by users. pis the number of grids that are closer to the candidate grid g than d. δdenotes the minimum distance between g and any other grid with higher density, especially, δis measured as the maximum distance dif ghas the highest density. Following the calculation of pand δ, the top-k centers with the highest value ξ= p×δ are returned as cluster centers, and we assign diﬀerent labels to these centers. Then, we assign each remaining grid to the same cluster as its nearest neighbor of higher density. If there exists a grid gthat do not belong to any cluster, then gis deﬁned as an outlier. However, in real life, there may exist some grid cells that contain many check-ins but without any neighbor, such as g in Fig. 6. To avoid misjudgment, we need to set a probability threshold $, i.e., if $(g) ≥ $, then the grid cell g is not an outlier even though it has no neighbor. The main information to be used during the detection of outliers are locations of users, this is because the density of them is usually higher than that of time on location-based social networks. For example, in real life many users may share statues at same places in diﬀerent time periods. In this case, the time information is useless to characterize a user, especially when the distribution of the user’s records is very sparse. This signiﬁcant phenomenon, i.e., the representativeness of spatial is usually much higher than that of time on locationbased social networks, has been fully investigated and demonstrated in our experiments. Consider the example in Fig. 6, by computing the local density pand distance δfor each grid and setting ξ= 5 ×6, $= 0.1, then we can delete anomalous grids g, g, and g. Even though these grids have large distance δ, the local density of them is 0, as there is no record falling into their neighbor grids. Additionally, the probability of each of them is 0.025 based on Eq. (6), which is less than $. Note that, the grid gwith probability 0.125 cannot be deleted since $(g) > $. Observed from this example, the novel method is able to detect outliers, and we can achieve higher precision through outlier detection, since the remaining grids are more likely to reﬂect the real behaviors of a user in real life. Next, we redeﬁne the similarity S(u, u) as follows: where X and Y denote the number of grids of uand urespectively after outlier detection. Intuitively, the popular places, such as shopping mall, cafeteria, and cinema, are more attractive and more likely to be visited by many people than personal private places, such as home and oﬃce. This phenomenon leads to the low peculiarity of popular places, i.e., these places are useless for distinguishing users from each other. In contrast, the personal private places visited by less people are more discriminative [13]. In other words, the importance of diﬀerent places are diﬀerent. To achieve user account linkage with higher accuracy, we highlight discriminative grids and time periods with large weight, and lighten popular ones with small weight. Inspired by [2], we propose to use Entropy from information theory to compute the weight of each grid cell and time period in this section. Renyi entropy is a generalized version of Shannon entropy and it’s deﬁned as follows in our application: H(T ) =11 − qlogΘ(T )|R|(14) where N and M denote the number of user accounts having check-ins in grid cell g and time period T respectively. Compared with Shannon entropy, the adjustable q makes Renyi entropy much more expressive and ﬂexible. Following the study in [2], the parameter q indicates entropy’s sensitivity to the number Θ(g) and Θ(T ). Speciﬁcally, it has following properties: Θ(g) and Θ(T ). – If q < 1, the entropy penalizes the higher value of Θ(g) and Θ(T ). – If q = 1, it is the meeting point of Shannon entropy and Renyi entropy. According to the Renyi entropy, we give the deﬁnitions of the grid weight and time period weight. ω(g) = exp− H(g)=Θ(g)|R|q − 1(15) ω(T ) = exp− H(T )=Θ(T )|R|q − 1(16) To normalize the weight of grid and time period, we use ω(g) =ω(g)max ω(g)and ω(T) =ω(T)max ω(T ), where max ω(g) and max ω(T ) denote the maximum weight of all grids and time periods respectively. Next, we can update the Gaussian kernel function K(·) as follows: Then, we can obtain the ﬁnal user account similarity S(u, u) based on Eq. (18). To further improve the eﬃciency of HFUL, we propose the following strategy to reduce the complexity of user account pair similarity calculation. As presented in Algorithm 1, we use the spatial information to retrieve user account pair candidates. The core idea of the pruning strategy is that: two user accounts have more common grid cells, then they are more likely to be an actually linked pair. During the process, an empty list Lis created to store the nearest neighbors of user account u, during each process. Then, for each grid cell gin G(u), we add the user account u into Lif ualso has check-in records falling into g (i.e., $(g) > 0). Next, we sort users accounts in L to select the top-k nearest neighbors of ubased on |GT (u) ∩ GT (u)|, which is deﬁned as: =min($(g), $(g)) Finally, we add the candidate (u, u), which is likely to be an actually linked pair, into O by selecting the top-k accounts in L(i.e., L[1 : k]). For each candidate (u, u) in the set O, we can obtain the ﬁnal similarity S(u, u) based on Eq. (18) and return the pair on condition that S(u, u) ≥ S. The computational complexity is a signiﬁcant factor affecting the eﬃciency and scalability of HFUL, thus we reduce the complexity of it by developing the abovementioned pruning strategy. Theorem 1 The complexity of naive KDE-based method is O(mn). Proof: The similarity between two user accounts calculated based on the naive KDE-based method is presented in Eq. (5), and the complexity is O(n) if both uand uhave n check-in records. Then, the ﬁnal complexity of this method is O(mn) if there exist m user accounts on two given platforms respectively. Theorem 2 The complexity of measuring the similarity between two user accounts based on Eq. (18) is O(n). Proof: In Eq. (18), the upper bound of XY MM is non condition that each time period in grid cells visited by uand uat most contains one check-in, i.e., we need to compute the similarity between any two check-ins in this extreme case. Consequently, the complexity is O(n). Theorem 3 The complexity of measuring user account similarity based on the candidate set O obtained in Algorithm 1 is O(mkn). Proof: In Algorithm 1, we only consider top-k neighbors for each user account in U, thus the complexity of this method is O(mkn). The eﬃciency of HFUL can be signiﬁcantly improved with the pruning strategy, and the reason is twofold. 1) The upper bound of user account pairs to be considered has been reduced from mto mk based on the Algorithm 1, where the parameter k is a constant with k << m. 2) Although the upper bound of XY MM is n, we have XY MM< nfor most user account pairs during the calculation of similarity. This is because there are many records falling into the same time period in a grid cell, and the extreme case mentioned in Theorem 2 is uncommon in real life. Additionally, we need some time to ﬁnd top-k neighbors from L in Algorithm 1, but this time is low-impact on the eﬃciency of our proposed method, since it is much smaller than the time saved from pruning search space. The experimental results in Section 9 also demonstrate this improvement. Intuitively, the goal of cross-platform user account linkage is to integrate the sources of complementary information from diﬀerent platforms for each user. Obviously, each user can obtain more history data following the linkage. Based on these data, we can exploit users’ behaviors more precisely. This is because compared with behavior analysis focusing on a speciﬁc platform, we have more insight into user behaviors with cross-platform datasets. The reason also answers the question “Why to link accounts belonging to the same user across diﬀerent platforms?”. Next, we investigate the application of HFUL on user prediction, location prediction, and time prediction. An individual’s mobility usually centers at diﬀerent personal geographical regions, historical check-in records of a user are usually generated at some speciﬁc regions, such as home region and work region [9]. Based on the density-based clustering method (DP) mentioned in Section 7.2, we can extract the multinomial distribution of region p(l|u) for each user. Obviously, grid cells falling into these regions usually have higher density compared with grid cells outside these regions. Next, we extract the multinomial distribution of time p(t|u, l) in each region l for all users, based on the temporal index introduced in Fig. 4. Following the analysis of users’ historical behaviors, we investigate the following predictions. – User Prediction. Based on the region distribution p(l|u) and time distribution p(t|u, l), we can predict the likelihood of a user visiting a target region at a speciﬁc time p(u|l, t). This could be very useful for merchants for planning purpose, or for them to target on speciﬁc costumers [9]. Given location (lat, lng) and time t, we ﬁrstly locate the region l that the point (lat, lng) falls into, then rank candidate users by p(u|l, t), which is computed as follows: p(u|l, t) =p(u, l, t)Pp(u, l, t)(20) where p(u, l, t) = p(u)p(l|u)p(t|u, l), and p(u) denotes the percentage of records of u in all records,P i.e., p(u) = |R|/|R|. – Location Prediction for User. This task is to predict the region where a user stays at a given time. This would be useful for location-aware advertisement recommendation, and for people to arrange a meeting with a speciﬁc user or a group of users. Given a user u and time t, we aim to rank all candidate region based on p(l|u, t) with following method: p(l|u, t) =p(u, l, t)Pp(u, l, t)(21) – Time Prediction for User. This task is to predict the time when a user may visit a speciﬁc region. This would be useful for time-based advertisement delivery and real-time recommendation. Formally, given a user u and a location (lat, lng), we can directly return the time interval t with the maximum p(t|u, l), since we have extracted the time distribution in the region l that contains the location (lat, lng). During the user prediction and time prediction, there may exist some locations that cannot be contained by a historical region, we will select a nearest region to the given location for prediction in this case. Extensive experiments are conducted in this section. First, we describe the experiment setup, which contains dataset introduction, baseline algorithms presentation, and evaluation metric discussion. Then, the eﬀectiveness, eﬃciency, scalability, robustness, and application of the proposed framework HFUL are reported. Foursquare-Twitter (FTW). Foursquare and Twitter are two widely used social networks, where users can post statuses associated with location information. To investigate the performance of the proposed approach in linking cross-platform user accounts, we use the dataset provided by [7] [4], where they select users with records presented in both platforms. The dataset contains 862 users with 13177 Foursquare records and 174618 Twitter records. Instagram-Twitter (ITW). Instagram is another popular photo-sharing application, where users can share pictures and videos with location information with mobile, desktop, laptop, and tablet. To link the user accounts across Instagram and Twitter with location data, we use the dataset processed by [4]. Similarly, each user of the dataset has check-in records generated in both platforms. The dataset contains 1717 users with 337934 Instagram records and 447366 Twitter records. GOW. Gowalla is a location-based social network, where users can share their locations by checking-in. The dataset collected from Gowalla contains 1855 user accounts with 2097885 check-in records. To simulate user account linkage across two platforms, we randomly divide the dataset into two components GOWA and GOWB, i.e., the records of each user is randomly divided into two subsets with roughly equal size. 9.2 Compared Methods. We compare the performance of our method with several state-of-the-art location based user account linkage approaches. Although existing methods [24] [26] [28] also work on user account linkage, their results are not comparable here, as they use diﬀerent input data, such as text messages, user proﬁle, and language style. Algorithms proposed by [37] [38] only make sense in trajectory data and cannot be extended to discrete check-in records. GRID: The ﬁrst method is based on of the work proposed by [8], where the top-p% = 15% grid cells with the maximum density are returned to denote a user. Based on these grid cells, we use following method to measure the similarity between a user account pair. S(u, u) =|r∩ r||r∪ r|·min(f (r), f(r)) where f(r) denotes the density of the j-th grid of u. BIN: The second method is proposed by [4], where each record in region l during time interval t is associated with bin (l, t). The similarity between uand u is deﬁned as: S(u, u) =S(u, u, l, t) and the similarity S(u, u, l, t) in each common bin (l, t) is: P [A(u, l, t) = a∧ A(u, l, t) = a|σ(u) = u] P [A(u, l, t) = a] · P [A(u, l, t) = a] where A(u, l, t) is the number of actions in the given bin (l, t) of u, P [·] is the likelihood, and σ(u) = u means uand uare the same user. DG: The third method is proposed by [13], where a density-based clustering method is used to extract the stay regions of a user, and a Gaussian Mixture Model (GMM) based approach is proposed to model users’ temporal behaviors. Then, the similarity between two user accounts is measured based on these features. GS: The fourth method is a variant of the approach proposed by [55]. Based on the idea of [55], {(g, o), ··· , (g, o)} is used to denote the observed co-occurrences of two users, where o(1 ≤ i ≤ m) denotes the corresponding frequency, and the weight of gis deﬁned as: where η and γ are set to 16 and 0.2, respectively [55]. Then, we can give the similarity S(u, u) as follows: EEUL: This method is our previous work [14], where each user account is only represented by a sequences of grid cells, and a k × k square region is constructed to improve the eﬃciency. HFUL: Our proposed framework HFUL diﬀers from EEUL with following peculiarities: 1) user account similarity is measured from the spatio-temporal perspective instead of spatial perspective; 2) eﬃciency, eﬀectiveness, scalability, robustness, and application are investigated; 3) an outlier detection method is designed to improve the eﬀectiveness; 4) a novel pruning strategy is developed to reduce the number of user account pairs to be measured. To evaluate the eﬀectiveness of above algorithms, we use precision, recall, and F1. Given two sets of user accounts U= {u, u, ··· , u} and U= {u, u, ··· , u}, we return the user account pair (u, u) with S(u, u) ≥ S. The precision is deﬁned as the fraction of user account pairs contained by the returned result that are correctly linked, and the recall is deﬁned as the fraction of the actual linked user account pairs contained by the returned result [13], where N is the number of actually linked user account pairs in the ground truth, K is the number of returned user account pairs, and M is the number of actually linked user account pairs in the returned result. To evaluate the eﬃciency of the proposed algorithms, we compare the time cost of them. Note that we report the best performance of baseline methods GRID, BIN, DG, GS, and EEUL on all datasets. The performances of diﬀerent methods are reported in Fig. 7, where the precision, recall, and F1 are presented. As expected, all methods perform better than GRID, since only the common grid cells with high density are considered while measuring the similarity between a user account pair in GIRD. Both methods BIN and DG do not perform well as they did in [4] [13]. This is because we have proposed a novel evaluation metric, where all user account pairs with similarity larger than Sare returned. Such metric makes our approach become more general and applicable to many applications, especially when two datasets have diﬀerent numbers of user accounts and there exist many-to-many mappings. Compared with other baseline methods, our previous work EEUL performs much better, due to the following reasons. On the one hand, the kernel density estimation based similarity measurement is able to tackle the challenges data sparsity and imbalance introduced in Section 1, since we use a set of grid cells with corresponding probability to denote a user. On the other hand, we calculate the grid cell weight based on Renyi entropy, where the important and discriminative grid cells visited by few visitors are highlighted. In contrast, the popular grid cells with large entropy are assigned with small weights, due to the low discrimination of them. Without surprise, the framework HFUL performs better then EEUL, this is because: 1) we have ﬁltered the noisy records before similarity calculation based on the outlier detection method; 2) the user account similarity is measured from the spatio-temporal perspective instead of calculating the similarity only with spatial information; 3) we reduce the computational complexity of HFUL, where the number of candidates is reduced from mto mk based on Algorithm 1. Additionally, we ﬁnd that all methods have better performance on the dataset GOW. This is because, user accounts on GOW contain more check-in records, which means the extracted features are more likely to reﬂect the real behaviors of a user, and we can ﬁnd the actually linked pairs more precisely. The running time is another important factor needed to be considered. As seen from Table 2, we report the average running time of all methods on diﬀerent datasets. Obviously, GRID is the most time consuming method, as it needs to take all historical records into account while computing the density of a grid cell. Calculating the density for all grid cells before returning the top-p% ones leads to the large time cost of the method. The second method BIN is also time consuming, since we need to measure user similarity in each bin (l, t). For DG, we need to spend much time to extract stay regions and time clusters, especially the weight calculation of these features. Although GS uses a set of grid cells to represent a user, the number of user accounts to be measured is m, thus the time cost of this method is larger than that of EEUL and HFUL. EEUL performs better than other baseline approaches, since a k × k square region has been constructed to prune search space. Our framework HFUL needs the least time to ﬁnd an actually linked user account pair, and the main reason behind this is the reduction of computational complexity, as discussed in Section 7.5. This result also demonstrates that the time cost to retrieve candidates in Algorithm 1 has low eﬀect on the eﬃciency of our propose method. That is to say, the running time saved from reducing the number of candidates to be considered is larger than that spent ﬁnding neighbors for each user account. To investigate the scalability of the proposed framework, we design a Gaussian distribution based data generator to synthesize several new datasets based on the idea of [56]. For the dataset FTW, we ﬁrst randomly select a user account pair (u, u) from the ground truth. Then, we randomly choose several records (in the range [2,10]) from Ras centers to generate new records. Note that, the reason behind this selection is that [9] has claimed that an individual’s mobility usually centers at some personal geographical regions. The Gaussian probability density function of generated records around each center is12πδδexp−12((x − mean)δ+ (y − mean)), where meanand meandenote the latitude and longitude of a selected record respectively, both δand δare set to 0.01, and based on this value the six sigma of Gaussian distribution contains 5×5 grid cells centered on (mean, mean). To generate the time-stamps, we use the Gaussian function1√exp− (x − mean), where meanis the time-stamp of the selected record, δ is set to 30, and the six sigma of the Gaussian distribution contains 5 time periods centered on meant. Additionally, the number of generated records of a new user account equals to |R|. On platform Twitter, we generate a new user count based on u with similar method. Repeating the action 1724 times, we obtain the new cross-platform dataset FTW2, which contains 1724 user accounts with 190780 records. The statistics of all synthesized datasets are presented in Table 3. In Table. 4, the total time cost of HFUL on each dataset contains the time spent in preprocessing (index construction, outlier detection, feature weight calculation, and candidate retrieval) and calculating user account similarity. Without surprise, we spend more time with the increase of the size of input dataset, since more user accounts should be considered. But an interesting observation is that the time increases slowly. This is because: 1) most of preprocessing time is spent constructing d ×d grid maps and M time periods, and this action is same for all datasets; 2) the ﬁltration of user accounts before computing similarity leads to the slow increase of running time. These factors lead to the decrease of the average time cost of HFUL. Additionally, we observe that the eﬃciency of HFUL is mainly determined by: 1) preprocessing, if the given dataset is small such as FTW-FTW6; 2) preprocessing and calculation, if the given dataset is medium such as ITW-ITW6 and GOW-GOW2; 3) calculation, if the given data is very large, such as GOW3-GOW6. To study the robustness of HFUL, we randomly select 5%, 10%, 15%, 20%, 25%, and 30% records for each user account on FTW, ITW, and GOW to add noise. Specifically, for each selected record, we use the same Gaussian function in Section 9.6 to generate a new record containing latitude, longitude, and time-stamp. By replacing each selected record with a new one, we can obtain some datasets with noise. The experimental results based on these datasets are presented in Fig. 8, observed from which the precision, recall, and F1 of HFUL will decrease while varying percentage from 5% to 30%. Fortunately, these metrics have no signiﬁcant change. This is because most of abnormal records are ﬁltered before computing user account similarity based on the outlier detection method proposed in Section 7.2. In a word, the results in Fig. 8 demonstrate the high robustness of our proposed framework HFUL. Following the account linkage, each user can obtain more data, based on which we study user, location, and time prediction, by choosing a 80-20 split on these data for features extraction and prediction. The results are presented in Fig. 9, where we only present the performance of HFUL on dataset GOW since the density of which is the highest, i.e., users’ features extracted from which are most likely to reﬂect the real behaviors of them in real life. Observed from Fig. 9: 1) all algorithms have better performance on GOW, since GOWA and GOWB are only part of GOW and the features extracted from them are less likely to reﬂect the real behaviors of a user; 2) our proposed framework HFUL performs better than all baseline methods, this is because the linking precision and recall of HFUL are much higher than that of others. To explore the beneﬁts brought by outlier detection, feature weight calculation, and pruning strategy, we design following compared method: HFUL-S1, HFUL-S2, and HFUL-S3, and the properties of them are presented in Table 5. The impacts of diﬀerent factors on eﬀectiveness and eﬃciency are presented in Table 6, where we observe that HFUL outperforms the three baselines, indicating that HFUL beneﬁts from synchronously considering three factors in a joint way. Compared with HFUL-S1, HFUL-S2 has lower eﬃciency and higher eﬀectiveness, since it needs to spend some time to prune abnormal grids and time intervals before measuring the similarity between two user accounts. The method HFUL-S3, which beneﬁts from feature weight calculation, is more eﬀective than HFUL-S1 and HFUL-S2. However, the time cost of HFUL-S3 is larger than that of others, as it needs to calculate the weight for each grid cell and time interval. The highest eﬀectiveness and eﬃciency of HFUL demonstrate that ﬁltering user pairs that cannot be results with the pruning strategy not only reduces the time cost but also makes the returned results more accurate. To obtain the best performance of HFUL, tuning parameters, such as q, bandwidth h, grid granularity, number of time periods and neighbors, and the similarity threshold S, is of critical importance. We therefore study the impact of diﬀerent parameters in this section. Varying q. As discussed in Section 7.3, the elegance of using the Renyi entropy lies inside the parameter q. According to Eq. (15) and (16), we can obtain a larger and larger grid cell weight ω(g) and time period weight ω(T ) with the increase of q. Then, it leads to the increase of user account similarity when other parameters are ﬁxed. Just like the results of varying bandwidth h, the precision and recall have opposite change in Fig. 10, and the reasons behind the phenomenon are similar. To balance precision and recall, we set q = 0.1 for FTW and ITW, and q = 0.4 for GOW. Varying grid granularity. The grid granularity is another important parameter of HFUL, where the selection of such parameter has two extremes: 1) extreme coarse granularity, the whole space is regarded as one grid cell that contains all check-in records; 2) extreme ﬁne granularity, where each record is a grid cell and the method degrades into the naive kernel density estimation. Obviously, a too large or too small grid granularity is not appropriate for balancing eﬀectiveness and eﬃciency, as presented in Fig. 11 and Table 7. The time cost of HFUL is very sensitive to the grid granularity, since the increase of which means the records of a user may fall into more grid cells and the cardinality of grid representation of the user becomes larger. As a result, we have observed the increase of the running time of HFUL while varying the grid granularity from small to large. Taking various factors into account, we divide the space into 9000 × 9000 for FTW and ITW, and 15000 × 15000 for GOW. Varying bandwidth h. From the results in Fig. 12(a) and (b), we observe that the eﬀectiveness of HFUL is sensitive to the value of bandwidth h, producing a larger and larger user account similarity S(u, u) with the increase of h. Thus, it leads to the increase of recall as many user account pairs are returned, yet it also leads to the decrease of precision as many user account pairs contained by the returned result are not actually linked. As a consequence, we set h = 30m, h = 10m, and h = 60m for datasets FTW, ITW, and GOW respectively, with the goal of balancing precision and recall. Varying k. The number of neighbors to be considered in candidate retrieval also aﬀects the performance of HFUL. As shown in Fig. 13, a too large or too small number is not appropriate for balancing precision and recall. This is because: considering too many neighbors leads to the decrease of precision, as many returned combinations are not actually linked; only considering a small number of neighbors leads to the ﬁltration of many matched combinations. To achieve the best performance of HFUL, we set k to 12, 10, and 1 for FTW, ITW, and GOW respectively. Furthermore, HFUL needs more running time with the increase of the number of neighbors since more candidates are considered, and the results are presented in Table 8. Varying α. To study which one of spatial and temporal information is more important in linking user accounts, we vary the parameter α in diﬀerent datasets. Observed from Fig. 14, HFUL achieves the best performance when we set α to 1 on datasets FTW and ITW, which means the spatial information are far more important than the temporal information in reﬂecting users’ real behaviors on these datasets. Furthermore, HFUL has the best performance when α is set to 0.5 on GOW, since the quality of which is high in both spatial and temporal domain, where the user accounts belonging to the same individual have many common grids and time intervals. Varying $. During the detection of outliers, the probability threshold $is set to prune a grid cell g on condition that it has no neighbor and $(g) < $. Observed from Fig. 15, the precision of HFUL shows a increasing tendency on all datasets, while the recall will decrease with the increase of $. This is because: given a larger $, more outliers are pruned, then the similarity between two speciﬁc user accounts will decrease as less grids are taken into account. Although the returned results are less likely to contain wrong pairs with a large $, some actually linked pairs are pruned by mistake, since many grids that are not outliers will be deleted in this case. As a result, we set $= 0.00005, $= 0.0001, and $= 0.00005 for FTW, ITW, and GOW respectively, to balance the precision and recall. Varying S. In real scenarios, the datasets across diﬀerent platforms may have diﬀerent numbers of user accounts and there may exist many-to-many mappings, thus we propose a general method where the user account pairs {(u, u)|u∈ U, u∈ U} with S(u, u) ≥ Sare returned. Observed from Fig. 16, the eﬀectiveness of our method is very sensitive to the selection of S. On one hand, many actual linked user account pairs are ﬁltered with a too large S. On the other hand, the returned results may contain too many unmatched user account pairs if given a small S. To balance the precision and recall, and consider the characteristics of diﬀerent datasets, we set S= 0.004, S= 0.006, and S= 0.00002 for FTW, ITW, and GOW respectively. Varying percent of biased check-ins. The checkins collected from social networks may deviate from the real locations of a user due to the instability of the GPS devices. To study the performance of HFUL in dealing with biased data, we randomly select 10%, 20%,30%, and 40% records for each user account and replace these records with that generated by the Gaussian function in Section 9.6. The results are presented in Fig. 17, observed from which, with more records are replaced by biased check-ins, the precision, recall, and F1 of HFUL presents a downward trend. The expected decreasing trend is caused by the noise information brought by the Gaussian function. Fortunately, the performance change is not large, and this demonstrates that our proposed framework HFUL is able to handle biased checkin records. Varying percent of original check-ins. To investigate the performance of HFUL in dealing with datasets with diﬀerent sizes, we select 40%, 60%, 80%, and 100% original data for each user account, and the corresponding results are presented in Fig. 18. Without surprise, with more check-ins are selected, the precision, recall, and F1 of HFUL presents a upward trend. This is because the similarity between two account can be measured more precisely with more abundant data. Varying period number. As shown in Fig. 19, the temporal information is a negative factor for user account linkage on datasets FTW, and ITW, since this part of information is very sparse and many accounts belonging to same users have totally diﬀerent checkin timestamps, even though their records have similar distribution in spatial domain. Thus, we only report the performance of HFUL on GOW while varying the number of time intervals from 1920 to 3840 in Fig. 19. Obviously, a too small or large number is not the optimal choice. Additionally, HFUL needs to spend more time to link user accounts with the increase the number of intervals. Taking various factors into consideration, we divide temporal space into 2880 intervals. Linking user accounts across diﬀerent platforms with location data has received great attention, due to the increasing availability of spatio-temporal data with checkin information, and the wide applications of the study, such as cross-platform recommendation and advertisement. To achieve user account linkage with high eﬀectiveness, eﬃciency, scalability, and robustness, we have proposed several novel methods. Firstly, to tackle the data sparsity, we develop a kernel density estimation based approach to directly measure the similarity between two user accounts. Secondly, we construct the spatial and temporal indexes to improve the eﬃciency of HFUL and tackle the data missing problem. Thirdly, to further improve the eﬀectiveness of HFUL, novel methods are proposed to ﬁlter outliers and calculate the weight for each grid cell and time period, where the individual ones are highlighted with large weight, yet the popular ones visited by many users are lightened due to the low discrimination of them. The experiments conducted on three real datasets demonstrate the superiority of our propose method. In the future work, we can extend the user account linkage with location data from a certain city to a global scale, by deeply exploring the cross-city, cross-country, and cross-continental check-in behaviors. Additionally, the multimodal data such as texts, photos, videos, and social graph between users can be further utilized for more precise account linkage, by developing higher performance multimodal representation learning models. Acknowledgments. This work is supported by Australian Research Council Future Fellowship (Grant No. FT210100624) and Discovery Project (Grant No. DP190101985). It is partially supported by the National Natural Science Foundation of China under Grant No. 61902270 and No. 62072125, and the Major Program of the Natural Science Foundation of Jiangsu Higher Education Institutions of China under Grant No. 19KJA610002.