For cyber-physical systems, nding a set of test cases with the least cost by exploring multiple goals is a complex task. For example, Arrieta et al. reported that state-of-the-art optimizers struggle to nd minimal test suites for this task. To better manage this task, we proposeDoLesS(Domination withLeastSquares Approximation) which uses a domination predicate to sort the space of possible goals to a small number of representative examples. Multi-objective domination then divides these examples into a “best” set and the remaining “rest” set. After that,DoLesSapplies an inverted least squares approximation approach to learn a minimal set of tests that can distinguish best from rest in the reduced example space. DoLesShas been tested on four cyber-physical models: a tank ow model; a model of electric car windows; a safety feature of an AC engine; and a continuous PID controller combined with a discrete state machine. Comparing to the recent state-of-the-art paper attempted the same task,DoLesSperforms as well or even better as state-of-the-art, while running 80-360 times faster on average (seconds instead of hours). Hence, we recommendDoLesS as a fast method to nd minimal test suites for multi-goal cyberphysical systems. For replication purposes, all our code is on-line: https://github.com/hellonull123/Test_Selection_2021. Search-based software engineering, Machine learning with and for SE, Software testing, Modeling and Model-Driven Engineering, Validation and Verication, Embedded and cyber-physical systems Simulation models play an important role in many domains. Engineers build such models to simulate complex systems [29]. In the case of cyber-physical systems, these models are sometimes shipped along with the actual device, which means that analysts can now access high-delity simulations of their systems. Hence, much of the work on cyber-physical testing focuses on taking full advantage of high-delity simulators, prior to live testing [3]. For example, analysts can use the simulators for test suite minimization; i.e. they can explore many tests in the simulator in order to remove tests that do not need to be explored in the real world. However, using these models for test case minimization can be a very dicult process [3]. These simulation models are built to simulate complex systems such as electronic and physical models. Hence executing these simulation models can be very time consuming [5]. This problem gets even worse for multi-goal problems (e.g. minimizing runtime and maximizing the number of bugs found) since it is necessary to run the models multiple times for dierent subsets of the goals [5]. For example, Arrieta et al. reported that testing a high delity simulation model can take hours to days [3,19,37]. Further, they warned that state-of-the-art multi-goal optimizers (e.g. NSGA-III and MOEA/D) struggle to nd minimal test suites for this task. Recently, Chen [10] and Agrawal et al. [1] reported successes with a variant of optimization called “DUO” (data mining using/usedby optimizers). In this approach, a data mining method rstly divides the problem space, and then an optimizer executes in each small division. Inspired by that DUO approach, in this work, we apply a sorting method on the objective space to divide the multiobjective test suite minimization problem into several smaller partitions. OurDoLesSalgorithm (Domination withLeastSquares Approximation) applies a domination predicate to sort the example space to a small number of representative data points. Multiobjective domination divides these data points into a “best” set and the remaining “rest” set. After all that,DoLesSapplies an inverted least squares approach to learn a minimal set of tests that can distinguish the best from the rest in the reduced example space. To evaluate our proposed test case selection approach for simulation models, we compareDoLesSwith the most recent state-ofthe-art approach produced by Arrieta et al. [3]. In that companion, we ask the following research questions. RQ1: Can we verify that test case selection for multi-goal cyber-physical systems is a hard problem?Arrieta et al. [3] reported that standard multi-goal optimizers such as NSGA-III and MOEA/D failed in this task. This is an important observation since, if otherwise, there will be no clear motivation for this paper. Accordingly, as a rst step, we replicate their results in our experiment. RQ2: Can DoLesS nd better ways to select test cases which result in test suites with higher eectiveness measure (objective) scores?Section 3.1 of this paper reviews ve eectiveness measurement metrics which are used to select test cases for cyberphysical systems in the previous study [3]. Our results show that on those ve metrics, general performances ofDoLesSis better than previous state-of-the-art method. RQ3: Do selected test cases by DoLesS beat the prior stateof-the-art?Apart from the ve eectiveness measurement metrics used by Arrieta et al. [3], two other evaluation scores of interest are (a) reduction in the number of test case and (b) faults detection performance with the reduced test cases. As shown in our result section §5,DoLesSusually performs as well, if not better, than the prior state-of-the-art. RQ4: Is DoLesS far more ecient than prior state-of-theart in terms of running time?For all the reasons stated above, we need methods that oer faster feedback from models of cyberphysical systems. In this regard, it is signicant to note thatDoLesS runs 80-360 times faster than the prior state-of-the-art. Based on the above, we say our novel contributions are: (1) We propose a novel test generation method (DoLesS). (2)We verify thatDoLesSsolves a hard problem (test case selection for multi-goal cyber-physical systems). This is a problem that defeats state-of-the-art optimizers (NSGA-III and MOEA/D). (3)We clearly document the value of doingDoLesS. When testing on four cyber-physical models,DoLesSnds test suites as good, as even better, than those found by Arrieta et al.’s approach [3]. Further,DoLesSdoes so while running 80-360 times faster (seconds instead of hours, mean time). Hence, we recommend DoLesSas a fast method to nd minimal test cases for multigoal cyber-physical systems. The rest of this paper is structured as follow. Section 2 introduces the background and related work in test case selection for simulation-based testing. Section 3 introduces the problem of studying eectiveness measurement metrics in cyber-physical systems and illustrates how they are calculated by mathematical formula. Moreover, multi-objective optimizers and our proposed approach are introduced in this section as well. Section 4 introduces the case studies, performance evaluation metrics, and statistical analysis method used in this study. Section 5 shows our experimental results. Section 6 explores threats to validity and Section 7 makes the summary of our study and states the possible future work. Based on the above, we can conclude thatDoLesSis faster, yet more eective, than prior results since: • DoLesScan handle multiple goals (in our experiment, 5 goals) simultaneously. Hence it does not need to loop the algorithm 𝑛times (where𝑛is the number of subsets for the goals) like the prior state-of-the-art method. • DoLesS’s sorting procedure uses continuous domination to very quickly divide candidates into a very small “best” set (that we can focus on) and a much larger “rest” (that we can mostly ignore). Like much research before us, we argue that continuous domination is more informative than binary domination [38, 42, 52]. •Chen et al. [10] argues that some SE optimization problems can be solved better by over-sampling than via evolutionary methods. For example, the evolutionary NSGA-II method mutates 100 individuals for 250 generations (these parameters were selected to ensure comparability to the prior study). On the other hand, ourDoLesSover-sampling method explores 10,000 individuals for one generation. This result suggests that cyber-physical system testing might be another class of problem that better to be solved via the over-sampling methods which stated by Chen et al [10]. A repeated result is that test suites can be minimized (i.e. we can run fewer tests) while still being as eective (or better) than running the larger test suite [2,15,44,45,49]. Note that “eective” can mean dierent things in dierent domains, depending on the goals of the testing. For example, at FSE’14, Elbaum et al. [16] reported that Google could nd similar number of bugs, but after far fewer tests execution. This was an important result since, at that time, the initial Google test suites were taking weeks to execute. Such long test suite runtimes is detrimental to many agile software practices. Research has found many test case selection techniques such as DejaVu based, rewall based, dependency based, and specication based techniques [17]. We note that dierent test suite minimization methods need dierent kinds of data. For example, in 1995, Binkley et al. proposed a semantic-based method which takes the use of dierences and similarities of two consecutive versions to select test cases [8]. Rothermel et al. developed a test case selection technique for C++ software on 2020 [36]. In 2001, Chen et al. developed test case selection strategies based on the boolean specications [11]. In 2005, a fuzzy expert system was developed in test case selection by Xu et al. [47]. In 2006, Grindal et al. presented an empirical study on evaluating ve combination strategies for test case selection [20]. In 2011, Cartaxo et al. implemented a similarity function for test case selection in model-based testing [9]. Pradhan et al. [35] proposed a multi-objective optimization test case selection approach which can be used with limited time constraints. Arrieta et al. also used test case execution history to select test cases [5]. Also in 2017, Lachmann et al. [23] did an empirical study on several black-box metrics and made comparisons on their performance in selecting test cases in system testing. Due to the data requirements, many of the above methods are unsuitable for cyber-physical systems, for two reasons. Firstly, cyber-physical systems are embodied in their environment. Hence, it is not enough to explore static features of (e.g.) the code base. Rather, it is required to test how that code base reacts to its surrounding environments. Hence, using just static information such as (e.g.) code coverage metrics is not recommended for testing cyber-physical systems. Secondly, at least for the systems studied here, cyber-physical systems make extensive use of process variables with the desired set- Hence, for test suite minimization of process control applications, the requirement is data collected from the feedback loops inside the cyber-physical systems. Accordingly, here we use input and output signals in the simulation models instead of execution history or coverage information. In one of the IST’19 journal paper, Arrieta et al. [3] explored issues associated with test suite minimization by using the data extracted from feedback loops. They noted that feedback loops Figure 2: Examples of anti-patterns seen for systems under feedback. The name of anti-patterns from left to right are instability, discontinuity, and growth to negative innity correspondingly. The alarming patterns are shown in red marks. have anti-patterns; i.e. undesirable features that appear in a time series trace of the output of the system. Figure??shows three such features correspondingly from left to right in the red dash rectangle: instability, discontinuity, and growth to innity. Later in this paper we will mathematically dene these anti-patterns. In all, Arrieta et al. [3] explored seven goals for cyber-physical model testing: maximizing the three anti-patterns that shown in Figure??, maximizing three other measures of eectiveness, as well as minimizing total execution time. Arrieta et al. used mutation testing to check the validity of their minimized test suite. Mutation based testing is a fault-based testing technique which implements “mutation adequacy score” to assess test suite adequacy by creating mutants [22], and then pruning test cases which cannot distinguish the original model from the mutant. In our study, we use the mutants generated in the experiments from Arrieta et al. [3]. Those mutants were generated with Hanh et al.’s technique [7] and some of the mutants are removed if (a) they are not detected by any test case, (b) they are killed by all test cases, and (c) they are equivalent mutants [34]. Like Arrieta et al., we say a test suite is minimal when it retires as many mutants as a larger suite. Mutation testing is the inner loop of Arrieta et al.’s process and, in their experiments, they found mutation testing to be an eective technique. The problem area in their work was the outer loop that optimized for seven goals. They found that standard optimizers such as NSGA-II [14] can be ineective for more than three goals (a result that is echoed by prior work [38]). More recent optimizers like NSGA-III [13] and MOEA/D [50] also failed for this multi-goal task. Later in this paper, we replicate their experiment and strengthen that nding (see RQ1). To address this optimization failure, they resorted to “pairwise” approach based on NSGA-II. That is, they ran NSGA-II with all 21 subsets of “choose two or three from seven” goals then returned the test suite associated with the run which has the best scores (where the “best” here is measured just on a subset of goals). While denitely an extension to the state-of-the-art, Arrieta et al.’s [3] study had two drawbacks. Firstly, the test cases selected in this way was only the best which measured on a subset of the optimization goals. Secondly, the “pairwise” approach increased optimization time by an order of magnitude, which is a major issue for large simulators, especially when we are running these algorithms 20 times (to check the generalizability of this stochastic process). Hence in this work, we seek to improve the mutation based test suite minimization method from Arrieta et al.’s study [3]. Like them, we will optimize for the anti-patterns and eectiveness measures seen in process control systems. But unlike that prior work, we will oer methods that simultaneously succeed across many goals (without needing anything like the pairwise heuristic used in Arrieta et al.). Further, we show that all this can be achieved without additional runtime cost. Cyber-physical system developers often use simulation tool (e.g. Simulink) to build cyber-physical models [12]. For an example Simulink models, see Figure 3. This is a model with two hierarchical levels [3]. A complex model will have far more blocks and operators. In Simulink models, the inputs and outputs are all signals (here signal means a time series function). This means at each simulation time stepΔ𝑇, there will be a value in each input and in each output regarding to that time step. For example, if we simulate a model for 5 seconds in real time and the time stepΔ𝑇is 0.05, then there will be5/0.05 + 1 = 101simulation steps, which means each input or output should be a vector of length 101. Assuming an initial set of𝑛test cases{𝑡, ·· · , 𝑡}for a simulation, each test simulates the model from a set of unique𝑘input signals{𝑖𝑠, ·· · , 𝑖𝑠}to a set of𝑙output signals{𝑜𝑠, ·· · , 𝑜𝑠}[3,29]. Our goal for this study is to select representative test cases from the initial test suite to minimize the test execution time, but not inuence the testing performance. Here we can dene the test selection problem as follow: Given an initial set of𝑛test cases𝑇 = {𝑡, 𝑡, 𝑡, ·· · , 𝑡}, we want to nd a subset of that set of test cases𝑇𝑆 = {𝑡𝑠, 𝑡𝑠, ·· · , 𝑡𝑠}which can test the model as initial test suite does 𝑃𝑒𝑟 𝑓 𝑜𝑟𝑚(𝑇𝑆) = 𝑃𝑒𝑟 𝑓 𝑜𝑟𝑚(𝑇 ) with 1 ≤ |𝑇𝑆| ≤ |𝑇 |. If we search in the space which contains all the subsets of the initial test suite, then the search space will be very large. For example, with only 100 test cases, there will be2−1possible subsets. Thus, cost-eectively selecting test cases is a signicant problem. In this study, we implement ve out of seven eectiveness measurement metrics which Arrieta et al. [3] used in their study. The rst three metrics are also widely used in previous studies [27,28,43], and the forth metric is proposed by Arrieta et al. [3]. Aside: We exclude two of the metrics explored by Arrieta et al. (Input & Output-based test similarity metrics) since these two metrics will always result similar normalized values (0.95-0.99) with dierent test case selections. Such similar normalized values can aect the performance of multi-objective optimization algorithms. We will introduce the remaining ve metrics in the rest of this section. 3.1.1 Test Execution Time. Total test execution time is the rst metric we implement in our study. Wang et al. [43] stated that the number of selected test cases can be treated as the measurement for selecting representative test cases from the initial test suite. However, Arrieta et al. pointed out the problem that each test case has dierent execution time [3]. In our study, we use the similar Figure 3: Simple example of a Simulink model - Cruise Controller of a car [3]. calculation that Arrieta et al. [3] did in their study to deal with the test execution time. The test execution time is calculated as follow: Dene a set of test cases𝑇𝐶= {𝑇, ·· · , 𝑇}that are selected from the initial test suite𝑇𝐶= {𝑇, ·· · , 𝑇}. Let 𝑇 𝐸𝑇denotes the execution time of the test case𝑇in𝑇𝐶, and𝑇 𝐸𝑇denotes the execution time of the test case𝑇in 𝑇𝐶. The total test execution time of a set of selected test cases is [3] In our study, we want tominimizethis metric because the goal of test case selection is to decrease the test execution time. 3.1.2 Discontinuity in Output Signal. Discontinuity is the second metric we implement in our study. As Matinnejad et al. [27] stated, the discontinuity of the output signal is a short duration pulse in the output signal, which means the output signal increases or decreases to a value in a very short time, and recovers back to normal. If executing a test case causes discontinuity in the output signal, then that test case detects the faulty behavior in the model. Assume we have 𝑁output signals{𝑂, 𝑂, ·· · , 𝑂}. For discontinuity score of each output signal𝑑𝑖𝑠𝑐𝑜𝑛𝑡𝑖𝑛𝑢𝑖𝑡𝑦(𝑂)where1 ≤ 𝑗 ≤ 𝑁, Matinnejad et al. calculated it with [27, 28] 𝑑𝑖𝑠𝑐𝑜𝑛𝑡𝑖𝑛𝑢𝑖𝑡𝑦(𝑂) =max(max(𝑚𝑖𝑛(𝑙𝑐, 𝑟𝑐 where𝑙𝑐= |𝑠𝑖𝑔(𝑖 ·Δ𝑡)−𝑠𝑖𝑔((𝑖 −𝑑𝑡 )·Δ𝑡)|/Δ𝑡is the left change rate of step𝑖and𝑟𝑐= |𝑠𝑖𝑔((𝑖 +𝑑𝑡) · Δ𝑡) − 𝑠𝑖𝑔(𝑖 · Δ𝑡)|/Δ𝑡is the right change rate of step 𝑖. The discontinuity rate of a set of selected test cases is calculated as follow: With same denition of test case above, let𝐷𝐶denotes the discontinuity score of the test case𝑇in𝑇𝐶, and𝐷𝐶 denotes the discontinuity score of the test case𝑇in𝑇𝐶. The total discontinuity score of a set of selected test cases is [3] 𝐷𝐶=is the normalized discontinuity score of the test case𝑘among𝑁output signals {𝑂, 𝑂, ·· · , 𝑂}. In our study, we want tomaximizethis metric because the goal is to detect more discontinuity in the output signal. 3.1.3 Instability in Output Signal. Instability is the third metric we implement in our study. As Matinnejad et al. [27] stated, the instability of the output signal is a duration of quick and frequent oscillations in the output signal, which means the output signal increase and decrease repeatedly in a duration of time. If executing a test case causes instability in the output signal, then that test case detects the undesirable impact on physical process [27]. Assume we have𝑁output signals{𝑂, 𝑂, ·· · , 𝑂}. For instability score of each output signal𝑖𝑛𝑠𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦 (𝑂)where1 ≤ 𝑗 ≤ 𝑁, Matinnejad et al. calculated it with [27] 𝑖𝑛𝑠𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦 (𝑂) = where𝑘is the total number of simulation steps andΔ𝑡is the time stamp in the simulation model for each step. The instability score of a set of selected test cases is calculated as follow: With same denition of test cases above, let𝐼𝑆denotes the instability score of the test case𝑇in𝑇𝐶, and𝐼𝑆denotes the instability score of the test case𝑇in𝑇𝐶. The total instability score of a set of selected test cases is [3] 𝐼𝑆=is the normalized instability score of the test case𝑘among𝑁output signals {𝑂, 𝑂, ·· · , 𝑂}. In our study, we want tomaximizethis metric because the goal is to detect more instability in the output signal. 3.1.4 Growth to Infinity in Output Signal. This is the forth metric we implemented in our study. As Matinnejad et al. [28] pointed out, the growth to innity of the output signal is the phenomenon that the output signal increases or decreases to innity value. If executing a test case causes growth to innity in the output signal, then that test case detects the faulty behavior in the model. Assume we have𝑁output signals{𝑂, 𝑂, ·· · , 𝑂}. For growth to innity score of each output signal𝑖𝑛𝑓 𝑖𝑛𝑖𝑡𝑦(𝑂)where1 ≤ 𝑗 ≤ 𝑁, Matinnejad et al. calculated it with [28] where𝑘is the total number of simulation steps andΔ𝑡is the time stamp in the simulation model for each step. The growth to innity score of a set of selected test cases is calculated as follow: With same denition above, let𝐼𝐹denotes the innity score of the test case𝑇in𝑇𝐶, and𝐼𝐹denotes the innity score of the test case𝑇in𝑇𝐶. The total growth to innity score of a set of selected test cases is [3] 𝐼𝐹=is the normalized innity score of the test case 𝑘 among 𝑁 output signals {𝑂, 𝑂, ·· · , 𝑂}. In our study, we want tomaximizethis metric because the goal is to detect more growth to innity situation in the output signal. 3.1.5 Output Minimum and Maximum Dierence in Output Signal. This is the last metric we implemented in our study. Arrieta et al. proposed this metric in their work because the dierence between maximum output signal and minimum output signal can indicate the level of how a model is being tested. If executing a test case results in large minimum and maximum dierence in the output signal, then that test case can detect more parts in the simulation model. Assume we have𝑁output signals{𝑂, 𝑂, ·· · , 𝑂}. For output minimum and maximum dierence score of each output signal𝑚𝑖𝑛𝑚𝑎𝑥 (𝑂)where1 ≤ 𝑗 ≤ 𝑁, Arrieta et al. calculated it with [3] 𝑚𝑖𝑛𝑚𝑎𝑥 (𝑂) = |max(𝑠𝑖𝑔(𝑖 · Δ𝑡)) −min where𝑘is the total number of simulation steps andΔ𝑡is the time stamp in the simulation model for each step. The dierence of output minimum and maximum of a set of selected test cases is calculated as follow: With same denition of test case above, let𝑀𝑀𝐷denotes the output minimum and maximum dierence of the test case 𝑇in𝑇𝐶, and𝑀𝑀𝐷denotes the output minimum and maximum dierence of the test case𝑇in𝑇𝐶. The total output minimum and maximum dierence score of a set of selected test cases is [3] 𝑀𝑀𝐷=is the normalized output minimum and maximum dierence score of the test case𝑘among 𝑁 output signals {𝑂, 𝑂, ·· · , 𝑂}. In our study, we want tomaximizethis metric because the goal is to coverage more parts that can be tested. 3.2.1 Binary vs Continuous Domination. In the following, all the algorithms use binary domination except forDoLesSthat uses continuous domination. Binary domination decides one individual is better than another if it is better on at least one goal and worse on none. Numerous studies [38, 42, 52] warn that binary domination is hard to distinguish candidates once the number of goals grows to three or more. For many-goal problems, Zilter’s continuous domination predicate [52] is useful [38,42,52]. Continuous domination judges the domination status of pair of individuals by running a “what-if” query which checks the situation when we jump from one individual to another, and back again. Specically: • For the forward jump, we compute 𝑠= −Í𝑒.Í • For the reverse jump we compute 𝑠= −𝑒. where𝑎and𝑏are the values on the same index from two individuals,𝑛is the number of goals (in our case𝑛 = 5), and𝑤is the weight {-1,1} if we are minimization or maximizing the goal𝑖 correspondingly. According to Zitler [52], one example is preferred to another if we lost the least jumping to it; i.e. 𝑠< 𝑠. Specically, in this work, we use this predicate to select better goal sets that (a)minimizetest execution time, (b)maximize discontinuity score, (c)maximizeinstability score, (d)maximize growth to innity score, and (e)maximizeoutput minimum & maximum dierence. 3.2.2 NSGA-II. NSGA-II is a common evolutionary genetic algorithm [14]. Firstly, it generates an initial set of population as the starter of the entire algorithm. Secondly, these candidates will evolve to osprings in a series of generations by implementing the crossover and mutation operators with their individual probability. In our reproduction experiment, we use single point crossover with 0.8 probability and bit-ip mutation with1/𝑁probability (𝑁is the number of test cases). Thirdly, parents for next generation will be selected by selection operator, which utilizes a non-dominated sorting algorithm to select top non-dominated solutions [32]. In the situation where a front needs to be divided because it exceeds the total number of population, NSGA-II uses the crowding distance to split candidates in that group. 3.2.3 NSGA-III. NSGA-III is an improved NSGA-II algorithm [13]. In NSGA-III, all procedures such as initial population generation, crossover, and mutation are similar to NSGA-II, except selection procedure. In NSGA-III, the selection procedure is applied based on a set of reference points. The reference points are uniformly distributed on the normalized hyper-plane with some division number𝑝[13]. After that, each objective point is normalized adaptively and associated with a reference point by calculating its distance to the corresponding reference line. Niche-Preservation operation is then applied to select candidates which will be used in the next generation [13]. 3.2.4 MOEA/D. MOEA/D is the rst multi-objective optimization algorithm which utilizes decomposition technique [50]. More specifically, MOEA/D explicitly decomposes the problem into multiple sub-problems with less objectives in each subgroup and solves these sub-problems simultaneously [50]. To so, prior to inference, all examples get random weights assigned to their goals. Examples are then clustered by those weights such that all examples know the space of other examples that weighted in a similar direction. Next, during the execution, if one example𝑋nds a way to improve Figure 4: DoLesS Framework - Continuous domination selects representative goals from the large initial random population; Data processing reads the data set in and forms the linear equation system; Linear least square solver solves the least square approximation; and Evaluation evaluates the selected test cases. itself, its local neighborhood will move in the same direction as𝑋 does. 3.2.5DoLesS. Figure 4 shows the entire framework of our approach. Unlike above evolutionary algorithms, our proposed approach DoLesS (Domination with Least Squares Approximation): •Uses continuous domination (dened below, see the rst block in Figure 4) to reduce the size of initial large random sets of goals and nd a “best” group of representative samples. •For each data entry in the “best” group,DoLesSthen uses a least square approximation technique (the third block in Figure 4) to inversely predict the test selection outcomes which can t the representative sets of goals best. This least square approximation technique is discussed below. After sorting on the domination score, DoLesS divides data into:√ •The𝑛“best” items. In our case study, we randomly generate 10000 initial candidates, hence the rst 100 candidates with highest domination score are grouped into the “best” group. • And the remaining “rest” items. Here we select the number of nal population as 100 with two reasons: (a) 10000 random initial population is large enough to cover a wide range of possible outcomes and (b) to make comparison fair, we select same number of nal candidates as previous work [3]. In the data processing stage of Figure 4, we take data from each model (which Arrieta et al. also used in their study [3]), and then processes it into the form of least square approximation structure by combining with the representative goals generated from continuous domination. Table 1(i) shows a simple example of eectiveness measurement data collected from the models. We can nd that each test case will have a single score for all eectiveness measurement data (𝑎means the score of test case𝑖in eectiveness measure𝑗). The corresponding matrix equation system for the above example is shown in Table 1(ii). This equation shows the linear relationship of test selection outcomes and the nal eectiveness measure scores (e.g. the nal score of eectiveness measure 1 can be obtained by 𝑒𝑚= 𝑎·𝑡+𝑎·𝑡+𝑎·𝑡+𝑎·𝑡where𝑡is the outcome of test selection). In this example, our goal is to nd the best outcomes Table 1: An example of (i) collected eectiveness measurement data (EM means eectiveness measure) and (ii) its corresponding matrix equation form of𝑡to𝑡which can result𝑒𝑚to𝑒𝑚. To summarize the above example, in our approach, we collect eectiveness measurement data for𝑛test cases (like Table 1(i)) and want to nd the best set of outcomes for𝑡to𝑡which can get the closest scores to representative goals which are selected by continuous domination. Linear Least Squares Approximation is a method which predicts the best value of a set of unknown variables that ts the relationship between expected and observed sets of data. In general, solving a system of linear equations (𝐴𝑥 = 𝑏) will result no solution or innite solutions. This always happens when (a) the number of constraints (equations) greater than the number of variables (overdetermined) or (b) the number of variables greater than the number of constraints (underdetermined). The way to nd the best approximate solution is called the linear least square approximation. As mentioned in Section §3.2.5, in our study, we have 5 goals and𝑛number of test cases (where100 ≤ 𝑛 ≤ 150). Thus, our equation system contains 5 equations (5 goals) and𝑛variables (where𝑛must greater than 5). In this case, nding possible selections of test cases becomes a underdetermined least square approximation. In the formulation𝐴𝑥 = 𝑏, where𝑥is an outcome vector of test cases, we want to predict the value (0/1) for each entry of𝑥. The nal outcome of𝑥is a vector of oat numbers (ranged from 0 to 1 by controller) which indicates lower eect to the nal score with Table 2: Summary of number of I/O signals, number of test cases, and number of mutants in four case studies coecient→higher eect to the nal score with coecient from 0→1. Since test selection outcomes can only have 0 (discard that test) and 1 (select that test), we use the threshold of 0.5 to indicate higher probability or lower probability. A value < 0.5 means higher chance to be 0 and a value > 0.5 means higher chance to be 1. For each representative candidate found by continuous domination, DoLesSnds the test selection which can get the closest score to that candidate. Although there exists delta between original ideal scores and truth scores generated by predicted results because of the approximation procedure, our results show that least square approximation can nd adequate test cases which perform as well or better as the previous state-of-the-art. Our implementation of above step usesscipy.optimize, a python library, and uses the function calledlsq_linear, which solves the above problem by using either dense QR decomposition technique or Singular Value Decomposition technique. Finally, in the Evaluation stage of Figure 4,DoLesSselects the Pareto front setfrom the nal population as Arrieta et al. did in their study [3]. All evaluations are made through 20 repeats for each of algorithm. We use four Cyber-physical system (CPS) models to evaluate our proposed approach. These four models come from the previous state-of-the-art study [3]. We implement the test cases and mutants that Arrieta et al. generated from these four models. The summary of number of initial test cases and number of mutants are shown in Table 2. In that table: •Two Tanks project is a model that simulate the incoming and outgoing ows of the tanks [30]; •CW project is a model that simulate the electrics and mechanics of four car windows [3]; •AC Engine project is a model that simulate some safety functionalities in the AC engine [4, 6]; •and EMB project simulates the software model controller which includes a continuous PID controller and a discrete state machine [27]. At rst glance, the case studies in Table 2 may appear to contain very small test cases. But appearances can be deceiving; e.g. the number of input signals is a poor measure of the internal complexity of a cyber-physical system. As shown in ourRQ1results, the systems of Table 2 are so complex that, for the purposes of test suite minimization, they defeated state-of-the-art optimizers (NSGA-III and MOEA/D). To evaluate the selected test cases, we use two evaluation metrics from prior work [3]. These two evaluation metrics are (a) normalized test execution time and (b) mutant detection score. Previous study used these two metrics to calculate the hypervolumne indicator and average weighted sum of mutation score and normalized test execution time [3], while in our study, we directly compare the performance of algorithms in these two metrics. Normalized test execution time(TET-): Our goal for selecting test cases from the initial test suite is to speed up the testing process. Thus, test execution time is a very important indicator which can indicate whether selected test cases can signicantly reduce the cost of testing. In this study we want tominimizethis value since, as discussed in our introduction, the whole point of this paper is to reduce the time required for testing cyber-physical systems. Mutant detection score(MS+): If a set of selected test cases can signicantly reduce the test execution time, but cannot detect most of the mutants, then such a selection is a bad choice. Our goal for selecting test cases is detecting as much as mutants when minimizing the test execution time. Therefore, mutant detection score becomes another important evaluation metric. In this study we want tomaximizethis value since higher value means the test suite is better since it can detect more mutants. That is, a good test case selection approach can both (a) minimize the test execution time and (b) maximize the mutant detection score. In our study, we record the value of above two evaluation metrics in 20 repeats. To compare the total performance of dierent algorithms, we implement A Scott-Knott analysis [31]. The Scott-Knott analysis can sort the candidates by their values, and assign candidates to dierent ranks if the values of candidate at position𝑖is signicantly Table 3: RQ1 results: Reproduction results of Arrieta et al.’s study [3]. The metric with "-" means less is better while "+" means more is better. The light gray cell in each project means that approach wins others signicantly (as computed by the statistical metho d in §4.3). Table 4: RQ2 results: Scores of ve eectiveness measurement metrics calculated by sets of selected test cases. All entries report the median score of 20 repeats. In the title row, the metric with “-” means we want to minimize that metric and the metric with “+” means we want to maximize that metric. The light gray cell in each project means that approach wins over another approach signicantly (as computed by the statistical method of §4.3) in that metric. Last column counts the number of wins for each approach. ACEngine dierent (by more than a small eect size) to the values of candidate at position 𝑖 − 1 [24]. More precisely, Scott-Knott sorts the candidates by their median scores (and in our study, the candidates are the test case selection approaches). Scott-Knott method will split the sorted candidates into two sub-lists which maximize the expected value of dierences in the observed performances before and after division [41]. After that, Scott-Knott will declare the one of the split as the best split. The best split should maximize the dierence𝐸(Δ)in the expected mean value before and after the split [40, 46]: mean value of list 𝑙, 𝑙, and 𝑙. After the best split, Scott-Knott then implements some statistical hypothesis tests to check the division. If two items𝑑and𝑑after division dier signicantly by applying hypothesis test𝐻, then such division is dened as a "useful" division. Scott-Knott will run recursively on each half of the best division until no division can be made. In our study, we use cli’s delta non-parametric eect size measure as the hypothesis test. Cli’s delta quanties the number of dierence between two lists of observations beyond p-values interpolation [46]. The division passes the hypothesis test if it is not a "small" eect (𝐷𝑒𝑙𝑡𝑎 ≥0.147). The cli’s delta non-parametric eect size test explores two list 𝐴 and 𝐵 with size |𝐴| and |𝐵|: Cli’s delta estimates the probability that a value in the list𝐴is greater than a value in the list𝐵, minus the reverse probability [25] in the above formula. This hypothesis test and its eect size is supported by Hess and Kromery [21]. Returning now to the research questions oered in the introduction, we oer the following results. RQ1: Can We Verify that Test Case Selection for Multigoal Cyb er-physical Systems is a Hard Problem?To answer RQ1, we replicate Arrieta et al.’s experiment [3]. Table 3 shows our replication results with 20 repeats (with dierent random number seeds). Two algorithms dier signicantly if they separate in dierent ranks in the Scott-Knott analysis. As seen in Table 3, we can found the approach with NSGA-II that Arrieta et al. proposed [3] performs better than other multigoal optimizers (MOEA/D and NSGA-III). Specically, NSGA-II has higher performance in both test execution time and mutation score in four case studies. However, even though NSGA-II beats the other methods, we cannot sanction its use. NSGA-II has all the problems discussed in §2. Specically, NSGA-II can only handle pairs of goals. Hence, it has to be re-run multiple times to explore all pairs of ve goals. As shown below, we can achieve better results, orders of magnitude faster. Therefore, we can answer RQ1 as follow: Test case selection for multi-goal cyber-physical models is a hard problem that cannot be solved by merely applying, o-the-shelf, the latest optimizer technology. TheseRQ1results motivate the rest of this paper where we develop a fast approach, which can handle multiple goals simultaneously for the cyber-physical systems. RQ2: Can DoLesS Find Better Ways to Select Test Cases which Result in Test Suites with Higher Eectiveness Measure (Objective) Scores?To answer RQ2, we calculate the scores of eectiveness measures (test execution time, discontinuity, growth to innity, instability, minimum and maximum dierence) after we generate the subsets of selected test cases. Table 4 shows our simulation results. For each project, we use Scott-Knott statistical analysis to compare the performance across 20 repeats. Table 4 reports the median scores for each eectiveness measurement metric. To visualize the nal results, we mark the winning approach in each metric bylight gray, and count the number of wins in the last column. As seen in Table 4,DoLesSwins in three out of four projects (in Twotanks, CW, and EMB). Moreover, in Twotanks and EMB, our proposed approach results higher scores on most of the eectiveness measurement metrics while previous approach only wins in one (DoLesSwins in all eectiveness measurement metrics in Twotanks and wins in 4 out of 5 eectiveness measurement metrics in EMB). These outcomes can indicate that in most of the cases, our proposed approach gets signicant improvement than previous approach in these ve metrics. Moreover, in half projects, the state-of-the-art approach concentrates on optimizing the Time metric and Instability metric (because of the algorithm design). However, by usingDoLesS, we can nd that most of the “goals” are equally optimized. Hence, DoLesScan handle multiple goals all the time while state-of-the-art method can only handle one or two goals in some cases. By summarizing above ndings, we answer RQ2 as follow: DoLesScan nd better test selections than state-of-the-art approach in terms of ve eectiveness measurement metrics. RQ3: Do selected test cases by DoLesS beat the prior stateof-the-art?To answer RQ3, we compare scores of TET- (normalized test execution time) and MS+ (mutant detection score) between the prior state-of-the-art andDoLesS. It is important to have higher performance on these two evaluation metrics since they directly indicate whether a test case selection is good or not. Table 5 shows our simulation results (note that TET - test execution time & MS Mutation Score). For each method in each project, we repeat experiment 20 times and calculate the value of two evaluation metrics for each repeat. To obtain the nal conclusion, we implement ScottKnott statistical method to check if our approach signicantly dier to state-of-the-art approach in each metric. Thelight graycells mark the winning method (in the rst rank) resulted from the ScottKnott test. Moreover, we count the number of higher performance in these two evaluation metrics for each algorithm and record the number of wins in the last column. As seen in Table 5,DoLesSgets better performance in both two evaluation metrics in two out of four projects (ACEngine & EMB). Moreover, in Twotanks,DoLesSand state-of-the-art method achieve the same performance (both two evaluation metrics are tied in the rst rank). In the CW project,DoLesShas higher performance in minimizing test execution time when state-of-the-art method gets a little bit higher mutation score thanDoLesS. Taking above comparisons together, we can conclude that test cases selected by DoLesScan achieve similar or better performance in minimizing test execution time and detecting more mutants in all projects. By summarizing above ndings, we answer RQ3 as follow: Table 5: RQ3 results: Scores of two evaluation metrics calculated by sets of selected test cases. All entries are reported the median score of 20 repeats. In the title row, the metric with “-” means less is better while “+” means more is better. The light gray cells mark the winning approach (as computed by the statistical method in §4.3) in that metric. Last column counts the number of wins for each approach. Table 6: RQ4 results: Runtime comparison for our proposed DoLesS and the state-of-the-art approach. The light gray cell marked the fastest approach in each project. The last column marks how many times DoLesS faster than state-ofthe-art. Twotanks83 ACEngine179 EMB319 Inallprojects, comparing to the state-of-the-art,DoLesScan get similar or better performance on minimizing the execution time of selected test cases while keeping to detect most of the mutants. RQ4: Is DoLesS far more ecient than prior state-of-theart in terms of running time?To answer RQ4, we count the execution time for both algorithms during the experiment. To make comparison fair enough, we run both two algorithms on the same 64-bit Windows 10 machine with a 4.2 GHz 8-core Intel Core i7 processor and 16 GB of RAM. Moreover, when running experiments, we make sure no huge process is starting or ending in our machine. Table 6 shows the recorded runtime for each project. For each method, we repeat experiments 20 times and record the total runtime. The light gray cells mark the fastest approach. As seen in Table 6, in both four projects,DoLesSruns signicantly faster (80-360 times faster) than the previous method. By analyzing our proposed algorithm and state-of-the-art approach, we nd previous approach implemented NSGA-II as their multiobjective optimization, which designed for 2 or 3 objectives [32]. To handle this issue, Arrieta et al. group objectives into 21 dierent combinations with two or three objectives in each combination, and select one of the best combinations by repeating their approach in those 21 groups [3]. However, in our approach, we just need continuous domination to nd “ideal goals” and approximate corresponding test cases inversely. By summarizing above ndings, we answer RQ4 as follow: Inboth fourprojects,DoLesSrun signicantly faster (80-360 times) than the previous method. In other words,DoLesSis far more ecient than state-of-the-art approach. Even though our current empirical results can only boost a speed of (up to) 300 times faster, we can make a theoretical case that, if the number of goals increases, our technique would be even more comparatively faster (please note that in the following counts, test execution time is the metric that must be in every combination): • 5goals will result10dierent combinations of 2 or 3 objectives. • 7goals will result21dierent combinations of 2 or 3 objectives. • 9goals will result36dierent combinations of 2 or 3 objectives. •The above pattern shows that with more goals being utilized, the number of repeats for state-of-the-art NSGA-II approach increases exponentially. However, our approach can handle multiple goals simultaneously in one time. This can show the eciency of our approach. This section discusses issues raised by Feldt et al. [18]. Construct validity:The construct validity threat mainly exists in the parameter settings of algorithms. For example, in our replication experiment, we use one point crossover with 0.8 crossover probability and bitip mutation with1/(number of variables) mutate probability as prior studies did in order to keep consistent. For another example, in least square approximation, we use 0.5 threshold to indicate whether a test case has large probability to be chose or not. Moreover, we use the default setting of Python least square solver in our algorithm. Changing these parameters can result dierences in selecting test cases. Therefore, our observation may dier when dierent parameters are used. We would consider hyper-parameter tuning [39,40] in future work to mitigate this threat. Conclusion validity:The conclusion validity threat in this study is related to the random variations of our algorithm. To reduce the eect caused by this threat, we repeat all experiments 20 times in the same machine. Moreover, we apply Scott-Knott statistical test to compare if the outcomes of our proposed approach and the previous methods dier signicantly. Internal validity:Internal validity focuses on the correctness of the treatment caused the outcome. In this study, we constraint our simulations to the same data set. Moreover, we evaluate our approach and the previous approach in the same workow. Another internal validity threat can refer to the mutants generated from the projects. To mitigate this threat, we use the same mutants that Arrieta et al. [3] used in their study, which they have removed duplicated mutants. External validity:External validity concerns the application of our algorithm in other problems. In this study, we generate our conclusion from four real-world Simulink cyber-physical systems. When applying our method into other case studies, these concerns may raised: (a)DoLesSmay not be applicable in projects which cannot obtain the eectiveness measurement data; (b)DoLesSmay needs modication for those projects which eectiveness measurement data and test cases are not in the linear relationship. For those projects, we would consider non-linear least square approximation as future work to mitigate this threat; (c) Other algorithms may perform better if other information is utilized. Finding representative test cases from the initial test suite is an important task in simulation based model. Better test case selection methods can not only reduce the test execution time in the future testing, but also maintain the same testing performance as usual. In other word, a good test case selection approach can (a) minimize the test execution time and (b) maximize the mutation score. Previous literature by Arrieta et al. [3] has shown the great success on using NSGA-II as the multi-objective optimization method to select representative test cases. However, their design has a deciency where they need to evaluate 21 combinations rst to select the best subset. Moreover, since NSGA-II is a randomized algorithm, repeats are necessary during the experiment. Therefore, their approach will execute NSGA-II 420 times with 20 repeats. In this study, we address this deciency by selecting test cases from all eectiveness measurement metrics. To do that, we use a very fast approach - continuous domination to select representative goals. Moreover, we make a better use of the linear relationship between test cases and goals to nd the best test selections correspond to the representative goals (by linear least square approximation). Our experimental results show that our proposed approachDoLesS can (a) achieve better scores in terms of ve eectiveness measurement metrics, (b) achieve better scores in terms of two evaluation metrics, and (c) solve test selection problem in a very fast speed (80360 times faster than the state-of-the-art method) for cyber-physical models. We conjecture that our method would be a better candidate for scaling to large systems than the method proposed by Arrieta et al. [3]. To see that, consider the following scenario: To successfully perform test case selection on selected cyber-physical case studies, Arrieta et al.’s approach required 3-5 hours execution time. Now imagine in some higher complexity simulation models (e.g. drone simulation models) with dozens more of test cases in the initial test suite, and these models have more signal processing criteria in I/O signals, both evaluation time and objectives are increased. In such scenario, the execution time of running NSGA-II in all subsets of objectives will exponentially increase as we mentioned at the end ofRQ4. Moreover, such models (e.g. drone simulation models) requires far more fast feedback than usual cyber-physical models. Due to the above reasons, the ideal test case selection approach for complex simulation models needs to handle multiple goals (more than 4 goals) in the same time and perform selection in a very short execution time for the fast feedback. For that task, we recommend DoLesS. As to further work, apart from extending this exploration of feedback loop anti-patterns, we conjecture that our methods could be useful for other multi-objective reasoning tasks. Standard practice in this area is to mutate large populations across a Pareto frontier. This has certainly been a fruitful research agenda [26,33,48,51]. But perhaps the testing community could reason about more goals, faster, if used our domination methods and least squares methods to “reason backwards” from goal space to decision space. Hence, future works can be conducted with •Finding more simulation projects which can strength our approach. •Developing more eectiveness measurement metrics which can better indicate representative test cases. •Adjusting our approach based on the testing scenarios of dierent projects. Moreover, in some cases, a new test case selection approach is needed for those projects. This research was partially funded by blinded for review.