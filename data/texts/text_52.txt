Rapidly growing online podcast archives contain diverse content on a wide range of topics. These archives form an impor tant resource for entertainment and professional use, but their value can only be realized if users can rapidly and reliably locate content of interest. Search for relevant content can be based on metadata provided by content creators, but also on transcripts of the spoken content itself. Excavating relevant content from deep within these audio streams for diverse types of information needs requires varying the approach to systems prototyping. We describe a set of diverse podcast information needs and diﬀerent approaches to assessing retrieved content for relevance. We use these information needs in an investigation of the utility and eﬀectiveness of these information sources. Based on our analysis, we recommend approaches for indexing and retrieving podcast content for ad hoc search. • Information systems → Test collections; Relevance assessment; Presentation of retrieval results. datasets, search, information retrieval, spoken content retrieval, podcasts, broadcast media ACM Reference Format: Ben Carterette, Rosie Jones, Gareth F. Jones, Maria Eskevich, Sravana Reddy, Ann Clifton, Yongze Yu, Jussi Karlgren, Ian Soboroﬀ. 2021. Podcast Metadata and Content: Episode Relevance and Attractiveness in Ad Hoc Search . In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3404835.3463101 Podcasts are a rapidly expanding as a pop ular medium for delivery of spoken audio content. As of 2021, more than 38 million podcast episodes are available online [22]. Given the amo unt of po dcast Netherlands,United States material available, we believe it is increasingly important that it be fully searchable if it is to be fully exploited by users. At present, most podcast search is done via catalog match, using show t itles, episode titles, and sometimes metadata provided by p odcast creators. This metadata is of highly varied quality and hence usefulness in supporting search operations. Reco mmendation from friends and family [16] remains in the top-three ways people ﬁnd podcasts, while non-podcast-listeners in the same study say that they do not know how to ﬁnd a podcast, or that they do not really know where to start. In this paper we repor t experiments using the test collection from the TREC 2020 Podcasts Track which show that the aut omatically transcrib ed content of podcast episodes is more reliably useful for search t han metadata provided by podcast creators. Podcasts are distributed as audio streams or ﬁles, through RSS feeds containing multiple metadata ﬁelds [2]. A podcast show has a title, description, language, consumption order (episodic or sequential), and a list of categories (e.g., News, Sports, Comedy) selected by the creator from a predeﬁned taxonomy. A show typically has multiple episodes, which are the distinct audio ﬁles. Each episode has its own title, description, and other information. All this metadata may be noisy or inadequate [18]. Podcasting is typically a spoken-word medium. However, the relative ease and low cost of recording and publishing means there is great variability in the speciﬁcs. Podcast episodes have a wide range of lengths, from just a few minutes to hour s, although they tend to be between half an hour and an hour long. Podcast content can vary in form, e.g. being scripted or informal dialogues. These can pose problems for search systems buil t for text archives, and also for the transcription of the content. At ﬁrst glance, podcasts are spoken docu ments, which have been well-studied in the TREC Spoken Document Retrieval Track which ran from 1997-2000 [9]. However, this work was based o n news corpora, which are relatively homogenous in genre, style, and speaking professionalism, while podcasts come in many disparate forms, increasing the diﬃculty of the task. Retrieval from an archive of oral histo ry has also been well-studied [15], but lacks the multispeaker, multi-genre aspect of podcasts. The NTCIR Spoken Query and Document tasks [1] used a document collection of spontaneous speech, but the 600 hours of speech is tiny c ompared to the 50,000 hours of speech in the Spotify podcasts corpus [4]. The identiﬁcation of “jump-in” points in multimedia content based on the spoken soundtrack at Mediaeval from 2011-2015 [6, 7, 14] motivates an approach to podcast search in which we identify the best place to start listening. We refer to Jones [10] for a more complete overview of research in spoken co ntent retrieval from its beginnings in the early 19 90s to today. Because of the availability of metadata ﬁelds, podcasts can be represented as semi-structured documents, and models like BM25F [17], Field Relevance Models [1 2], and NRMF [23], can be adopted for podcast search tasks. Evaluation campaigns such as the INEX XML retrieval initiative [8, 13] have studied such models. As argued by Besser et al. [3], the goals of podcast search may be similar to tho se for blog search, if podcasts are viewed as audio blogs. In addition, the publication format of podcasts, as series o f episodes typically consumed in sequence, and the prominence of hosts and certain popular guests, act as a ﬁlter on top of the to pical search. Tsagkias et al. [20] argued that the quality and credibility of podcasts, which are sometimes considered during the relevance assessment process, can be characterized using four types of indicators pertaining to the podcast content, the podcast creator, the podcast context, or the technical execution of the podcast. These facts distinguish podcast search from most well-established search tasks including adhoc, web, personal, and enterprise search. The Podcasts Track r an for the ﬁrst time at TREC 2020 [11]. It featured two tasks: a search task and a summarizatio n task, the former is the focus of this paper. Podcast search focused on a segment retrieval task deﬁned as the problem of ﬁnding relevant segments of podcast episodes given a query representing a speciﬁc information ne ed. The corpus for the task comprised 100,000 podcast episodes released in 2019, including metadata, audio, and full transcripts pro duced by automatic speech recognition (ASR) [4]. Participants were asked to retrieve unique episode identiﬁers (episode URIs) along with a time oﬀset to the start of a two-minute segment starting on the minute within that episode. The segment corp us comprises 3.4M segments with an average word cou nt of 340 ± 70 per segment. Seven participating groups submitted a total of 24 runs to the search task. Each run ranked up to 1,000 segments fo r each of the 50 queries. Ru ns used retrieval techniques such as relevance feedback, query expansion, word2vec, BERT reranking, and fusion. We now take a step back to ask: why are we interested in this kind of podcast search? What are the user needs that can be addressed by retrieving segments of episodes? What o ther information could be retrieved that might improve the user experience? User needs pertaining to podcasts include education, entertainment, and information. The format and variability o f the podcast medium – series of episodes, the importance of t he speciﬁc host or guest in the episode, range of presentation styles from monologues to interviews and banter, production quality – are additional aspects that may determine whether a user is interested in listening or not, entirely separate from topical relevance. These factors make podcast search diﬀerent from traditional search tasks. Since there is currently no large-scale content-based podcast search engine, there is no simple source of sample topics. Thus we developed topics by introspection, considering what we might use a podcast search engine for, what we could imagine ot hers using it for, what types of information needs make it more attractive than web search, etc. Topic development used several sources: lists of events in 2019, topic creators interests, and browsing metadata for potentially interesting content. To determine whether the topic would be interesting, we roughly compared metadata matches, web search results, and the results from our in-house t ranscript search engine. Topics were ﬁnally selected based on whether they retrieved interesting content from a simple search index of the p odcast transcripts. We also experimented with “known item” and “reﬁnding” information ne eds. In to tal, eight development and 50 test topics were develope d, with 13 topics within the t est set b eing labeled as reﬁnding or known-item. 5.3.1 TREC. Participant submissions were pooled to depth 20 (minimum 128 segments per topic, maximum 306) and reviewed by NIST assessors. The assessors primarily reviewed the ASR t ext of the segment, but could listen to au dio if the need arose. The assessor’s view showed each retrieved segment within the context of the transcript of the ful l episode, so that they coul d explore the context of the segment to better understand it. All segments from the same episode were judged in se quence together. The relevance judgments were on a four-point scale of bad (0), fair (1), good (2), or excellent (3). For known-item and reﬁnding topics, an additional level of “perfect” (4) was ad ded and meant t hat the segment was precisely what the user was looking for. Excellent segments were comp letely o n-topic, provided highly relevant information, and represented an ideal entry point into the episode. 5.3.2 Additional Assessments. The authors of this work independently assessed p odcast metadata and full transcripts. From the metadata, we extracted episode titles and descriptions. Episode titles and descriptions are frequently too shor t or unspeciﬁed t o accurately reﬂect episode relevance, so instead we assess how attractive they might be to a user with the stated information need. We produced the following ad ditional assessments: • Title attractiveness. Would a user with the given information need ﬁnd the episode a good candidate to stream based solely on the title? Table 1: New assessments colle cted for this work. Assessments were collected from documents retrieved for a subsample of topics from TREC runs, and from all documents retrieved from indexes described in Section 6. • Title and description attractiveness. Would the user ﬁnd the episode a good candidate to stream based on the title and description together? • Full transcript relevance. Is the episode relevant to the information need (based on the full transcript)? We also used TREC segment-level judgments to obtain transcriptlevel judgments by taking the maximum relevance of any segment of an episode as the relevance of the transcript. We refer to this as the transcript-segment judgment. We acknowledge that these may be unreliable; it is possible that there is a segment of the episode more relevant than any seen by a TREC assessor and therefore our transcript-level judgment is low. We selected documents to judge as fo llows: First, we built ﬁve new indexes using diﬀerent combinations of metadata and transcripts; see Section 6 for details. We retrieved the top 10 results for al l TREC topics from each of these ﬁve indexes, pool ed them, and judged all titles and descriptions for attractiveness, plu s a select subset of tr anscripts for relevance. This provided about 1,000 episode title and d escript io n attractiveness judgments, p lus about 250 transcripts that had not p reviously been assessed for TREC. We also pooled the top 10 results from all 24 T REC submitted runs. We selecte d a random sample of 16 topics for judging title and description attractiveness in this pool. We judged another 1,400 episode titles and descriptions in this set, as well as another 100 transcripts. Episodes were ordered by a function of rank position in runs, which may produce ordering eﬀects [5]. We did not investigate this. Table 1 summarizes the new assessments. There is very little overlap between episodes from our new indexes and episodes from TREC submitted runs. Across all 50 topics, there are only 79 episodes that occur in the top 10 of both our new runs and the 24 TREC submissions—only 7.7% of all episodes retrieved by our ﬁve new runs. By focusing on metadata we retrieve many more potentially relevant episodes. An RSS feed with podcast episod e titles, d escriptions, and other metadata, in addition to the audio ﬁle, includes a lot of information that could be indexed for retrieval. In order to understand the relative utility of various free-text ﬁeld s, we constructe d Lucene indexes of episode titles, episode descriptions, titles and descriptions concatenated, full ASR transcripts, and transcripts concatenated Table 2: NDCG@10 results for ﬁve diﬀerent indexes evaluated with four assessment types. run hltcoe1 hltcoe3 hltcoe2 BERT-DESC-TD BM25 LRGREtvrs-r_2 hltcoe4 run_dcu1 UTDThesis_Run1 run_dcu5 UMD_IR_run2 run_dcu4 UMD_IR_run1 UMD_IR_run3 UMD_IR_run5 Table 3: Evaluation of TREC submitted runs and baselines by NDCG@10 with four new types of assessments. with titles and descriptions. We retrieved rankings of episode URIs for all 50 TREC topics from each of these ﬁve indexes. We also ob tained the TREC submit ted runs. In o rder to make the segment rankings comparable to URI rankings from our indexes, we compressed all retrieved segments from one URI to a single result for that URI, r anked at the position of the top-ranked segment, and then removed all sub sequent mentions of that URI. Table 2 demonstrates how the cho ice of assessment and index impact retrieval results. Rows correspond to retrieval from the indexes described above. Columns correspond t o t he four diﬀerent judgment types summarized in Table 1. Conclusions from this table: (1) A r anking of episode tit les from a title-only index is more attractive than a ranking of episode titles from a descriptiononly index, w hile a ranking of descriptions from a descriptiononly index is more attractive than a r anking of titles from a description-only index, suggesting that titles and descriptions are not always signaling the same topical relatedness. (2) The most attractive results are achieved by retrieving against an index of episode titles and descriptions. (3) A ranking based on retrieval of transcripts returns many more relevant episodes, though titles and d escriptio ns are much less attractive. (4) Indexing transcripts and metadata together provides the strongest relevance by either transcript judgment type with a relatively small decrease in attractiveness. (5) Result attractiveness remains surprisingly low in all cases— many results will appear not relevant. From this we further conclude that there is a great deal of relevant material t o be excavated from the transcripts that cannot be accessed via the metadata. But enabling users to ﬁnd that information via full-document search means that result presentation will be negatively impacted. In other words, metadata search is inadequate for ﬁnding relevant information, but full-text search results in unappealing r ankings. Thus some for m of query-biased summarization [19] or passage retrieval is necessary for presenting results to users in an appealing way. Table 3 shows the TREC runs evaluated by the four new assessment types. We observe the following: • There is stronger co rrelation between diﬀerent assessment types among TREC runs than among those in Table 2. • By the transcript-segment judgments, our indexes are not competitive with the best TREC submitted runs. • By the new transcript-level relevance assessments, all but one of our runs are bett er than the best TREC ru n. • Our runs are substantially better than TR EC runs on title and description attractiveness. Based on these ﬁndings, it could be argued that indexing metadata and transcripts together, then searching full transcripts rather than segments, is likely to provide the best overall user experience. Transcript relevance. Transcript-segment and transcript-full assessments agree that an episode is relevant in 71% of cases. 43% of episodes have agreement on the exact grade of relevance. This is a high level of agreement compared to other retrieval tasks [21]. Attractiveness and relevance. When either the title is judged attractive or the episode is jud ged relevant, in about 50% of cases both are true. This is a decent level of agreement for a retrieval task, but it means that episode title attractiveness is often not a very good predict or o f episode relevance. The results are very similar for description attractiveness. Interannotator a g reement. URIs that overlapped between the ﬁve metadata runs and the TR EC runs were independently judged by two diﬀerent assessors. Among episode titles that at least one assessor marked attractive, assessors agreed 88% of the time. Similarly with d escript io ns, in 82% of cases bot h assessors found the description attractive. Agreement on exact grade of attractiveness was high as well: 72% for titles and 68% for descriptions. Table 4 shows the coverage of content words for select categories of podcasts. Some genres have episodes with relatively high top ical focus, which is reﬂected in the topical coverage of the descriptions. Each transcript and description is ﬁltered to only contain nouns, verbs, and adjectives; the table reports cou nts of these tokens. The average ratio of vocabulary size to length, a measure of topical variation within a text, varies across the categories. While all these scores are normal, the topical variation of "Fiction" podcasts is considerably wider than that of "Business and Technology" podcasts. The last column of the table demonstrates a diﬀerence in terminological coverage. This can be understood as a measure of the topical representativity of the descriptions with respect to the episode. A higher score will mean that the description represents more of the topical content. The score variation indicates a potential for determining the topicality of the episode or show, and thus the ut ility of using search technology optimised for topical retrieval and content analysis (as opposed to usage-based similarity measures). It is not diﬃcult to ﬁnd information needs for podcast search such that highly relevant content is burie d in episodes, its presence not indicated by either episode title or descript io n. Episode titles and descriptions that appear attractive may lead to irrelevant content and frustrated users. Podcast search engines should index both metadata and episode content, and episode segments or query-biased summaries may be necessary to help users understand why retrieved episodes are relevant to their need. Acknowledgments Thanks to the reviewers for their constructive suggestions, and to TR EC participants and organizers for helping make the track a success. Gareth Jones is partially supported by Science Foundation Ireland as part of the ADAPT Centre (Grant 13/RC/2106) at Dublin City University.