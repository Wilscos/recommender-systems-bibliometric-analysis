In every day of our life, our choices rely on recommendations made by others based on their knowledge and experience. The prosperity of online platforms and artiﬁcial intelligence have enabled us to develop data-based recommendations, and many systems have been implemented in practice. Successful examples include e-commerce (Amazon), movies (Netﬂix), music (Spotify), restaurants (Yelp), sightseeing spots (TripAdvisor), hotels (Booking.com), classes (RateMyProfessors), hospitals (RateMD), and route directions by car navigation apps (Google Maps). These “recommender systems” First, the system can leverage experiences of the most knowledgeable experts. Once the system learns experts’ behavior using data, the system can report what a user would do if he had experts’ knowledge. Accordingly, with the help of the recommender system, all users can optimize their payoﬀs even when they have no experience with the problem they are facing. Second, the system can utilize information that an individual cannot access easily or quickly. For example, restaurant-reservation systems present the list of all available reservation slots at that moment, and online travel agencies provide the prices and available rooms of hotels. These conditions change over time; thus, it would be very diﬃcult for an individual user to keep up to the minute with the latest conditions on their own. Accordingly, even experts beneﬁt from information provided by recommender systems. payoﬀs associated with speciﬁc alternatives. Real-world recommenders always face the problem of insuﬃcient initial experimentation (known as the “cold start” problem). Utilization of feedback provided by users is necessary, but such data are often incomplete and insuﬃcient. In particular, the system can rarely observe information about users’ payoﬀs, which is crucial in many learning methods (e.g., reinforcement learning and the multi-armed bandit problem). As a proxy for payoﬀs, many recommender systems already implemented have adopted rating-based learning, which substitutes the ratings submitted by the users for the true payoﬀs of users. Nevertheless, a number of previous studies have reported that usergenerated ratings often involve various types of biases and are not very informative signals of users’ true payoﬀs (e.g., Salganik et al., 2006; Muchnik et al., 2013; Luca and Zervas, 2016). The advantages of the data-based recommender systems can be classiﬁed into two groups. One of the largest challenges in developing a recommender system is to predict users’ In this paper, we propose deviation-based learning, a new approach for training recommender systems. In our model, a recommender (she) faces many rational users (he) sequentially. Neither users’ payoﬀs nor ratings are available. Instead, we train a recommender system using data about past recommendations and users’ ﬁnal decisions. If the recommender has not yet been well-trained, expert users often deviate from her recommendations. On the ﬂip side of the coin, upon observing expert users’ deviations, the recommender system can recognize the fact that it had misestimated the underlying state. Conversely, if a user follows the recommendation even though the recommender is not completely conﬁdent in her prediction, then she can improve her conﬁdence in the accuracy of her recommendations. Our deviation-based learning approach accumulates these observations to make better predictions of users’ payoﬀs. In recent years, such navigation apps have become extremely popular. have an immense information advantage over individual drivers; using the app-generated data, an app can dynamically detect traﬃc jams and then recommend less congested routes. Accordingly, the app is useful even for local drivers who have memorized the (static) local road map and can ﬁgure out the shortest route without the recommender’s help. acteristics. For example, the app may miss information about hazard conditions associated with speciﬁc roads (e.g., high-crime-rate areas, rock-fall hazard zones, accident blind spots). Such hazardous roads are often vacant because local drivers avoid them; thus, a naive recommender might consider such a route desirable and recommend it. familiar with this hazard information might then follow the recommendation and be exposed to danger. problem because (i) detailed ratings and reviews are often unavailable, and (ii) the app should not wait until it observes low payoﬀs because it means incidents or accidents indeed occur, and some users suﬀer from them. Moreover, this problem cannot be solved completely by inputting the hazard information manually because it is diﬃcult to list all the relevant hazard conditions in advance. knowledge of local drivers (the experts). When a hazardous route is recommended, a local driver ignores the recommendation and chooses a diﬀerent route. Given that the app has An illustrative example is app-based car navigation systems (e.g., Google Maps, or Waze). In the beginning, a navigation app does not have complete information about road char- The rating-based approach is not suitable for detecting hazards in the car navigation Our deviation-based learning approach solves this dilemma by taking advantage of the an information advantage about road congestion, such events would not occur unless the app misunderstands something about the static map with which the local driver is very familiar. Thus, upon observing a deviation, the app can update its knowledge about the static map. Conversely, if the app recommends a route that involves a potentially hazardous road but observes that the local driver followed the suggested route, then the app can conclude that the road is not that dangerous. In this manner, the app can obtain a better understanding of the static map and improve its recommendations. Note that the deviationbased learning approach can detect hazardous roads before any additional incident occurs because the recommender can observe that local drivers avoid such hazardous roads from the outset. seminal papers on information design theory (e.g., Kremer et al. 2014 and Che and H¨orner 2017). A user’s payoﬀ from an arm is normalized to zero, and his payoﬀ from another arm is given by xθ + z. The context x speciﬁes the user’s problem (in the navigation problem, a context includes such elements as the origin, destination, and means of transportation). We assume each user is an expert; he knows the parameter θ and can correctly interpret his context x to predict the ﬁrst term of his payoﬀ, xθ (i.e., he knows the static map and ﬁnd the shortest safe route). The recommender has additional information about the value of z (e.g., congestion), which is not observed by the user. We assume that local drivers are more knowledgeable than the recommender about the static map; the recommender does not at ﬁrst know the parameter θ and only learns it over time. For each user, the recommender sends a recommendation (message) based on a precommitted information structure. Upon observing the recommendation, the user forms a belief about the unobservable payoﬀ component z and selects either one of the two actions. ization of the tradeoﬀ between the message space and the learning rate is of practical interest because the recommender often wants to minimize the message space to reduce users’ cognitive costs. We show that by making the message space slightly larger than the action space, we obtain a large welfare gain. A large message space enables the recommender to send a signal that indicates the recommender is “on the fence” — i.e., the payoﬀs associated with two distinct actions are predicted to be similar. The availability of such messaging improves the learning rate exponentially without sacriﬁcing the utilization of current knowledge. tion space have the same size), then learning is very slow. In this case, the recommender recommends the better arm based on her latest information. Since the recommender has an information advantage, users often want to follow the recommendation blindly even if Formally, we analyze a stylized model in which each user has two arms (actions), as in We demonstrate that the size of the message space is crucial for eﬃciency. The character- We ﬁrst prove that when the message space is binary (i.e., the message space and acit is imperfect. Here, the recommender knows that no deviation will occur, and therefore, she learns nothing from the users’ behaviors. Formally, we prove that the expected number of users required to improve the recommendations increases exponentially to the quality of recommendations. This eﬀect slows down learning severely and causes a large welfare cost: while the per-round welfare loss in this situation is relatively small (since most users want to obey the recommendation blindly), the loss adds up to a large amount in the long run. focus on a simple policy that recommends a particular arm only if the recommender is conﬁdent in her prediction. Otherwise, the recommender honestly discloses the fact that the two actions are predicted to produce similar payoﬀs according to the recommender’s current information. When the recommender is conﬁdent about her prediction (which is almost always the case after the quality of her recommendation has become high), the user also conﬁdently follows the recommendation, which maximizes the true payoﬀ with high probability. Furthermore, when the recommender admits that she is on the fence, the user’s choice is very useful in updating the recommender’s belief. With the ternary message space, the total welfare loss is bounded by a constant (independent of the number of users). Information Design This paper elucidates how the recommender learns about experts’ knowledge through users’ actions. This contrasts with previous studies on information design (e.g., Kamenica and Gentzkow, 2011; Bergemann and Morris, 2016a,b) and strategic experimentation (e.g., Kremer et al., 2014; Che and H¨orner, 2017) that have explored how to incentivize agents to obey recommendations. Indeed, when either (i) the recommender (sender) has complete information about the underlying parameter (as in information design models) or (ii) payoﬀs (or signals about them) are observable (as in strategic experimentation models), a version of the “revelation principle” (originally introduced by Myerson, 1982) holds. In these cases, without loss of generality, we can focus on policies that always recommend actions from which no user has an incentive to deviate. In contrast, we demonstrate that when the recommender learns about underlying parameters by observing what users will do after knowing her recommendation, recommending just one arm is often ineﬃcient. Recommender System While the recommender system has mostly focused on predicting ratings, the vulnerability of rating-based learning has been widely recognized. Salganik et al. (2006) and Muchnik et al. (2013) show that prior ratings bias the evaluations of subsequent reviewers. Marlin and Zemel (2009) show that the rating often involves nonrandom missing The learning rate is improved drastically when a ternary message space is allowed. We data because users choose which item to rate. Mayzlin et al. (2014) and Luca and Zervas (2016) report that ﬁrms attempt to manipulate their online reputations strategically. While the literature has proposed several methods to address these issues (for example, Sinha et al. (2016) propose a way to correct bias by formulating recommendations as a control problem), the solutions proposed thus far are somewhat heuristic in the sense that their authors have not identiﬁed the fundamental source of the biases in rating systems using a model with rational agents. because our approach does not rely on ratings. Learning from Observed Behaviors In the literature of economic theory, inferring a rational agent’s preferences given their observed choices is rather a classic question (revealed preference theory, pioneered by Samuelson, 1938). Furthermore, recent studies on machine learning and operations research, such as inverse reinforcement learning (Ng and Russell, 2000) and contextual inverse optimization (Ahuja and Orlin, 2001; Besbes et al., 2021) have also proposed learning methods to recover a decision-maker’s objective function from his behavior. prediction about users’ payoﬀs. the eﬀect of the recommender’s information advantage. In many real-world problems (e.g., navigation), the recommender is not informationally dominated by expert users; thus, decisions made by experts who are not informed of the recommender’s information are typically suboptimal. This paper proposes a method to eﬃciently extract experts’ knowledge and combine it with the recommender’s own information. Second, we articulate the role of users’ beliefs about the accuracy of the recommender’s predictions. When the recommendation is accurate, users tend to follow recommendations blindly, and therefore, learning stalls under a naive policy (the binary recommendation policy). The extant studies have overlooked this eﬀect because their learning models do not take into account interactions between the learner (the recommender) and the decision-maker (the users). Third, we demonstrate that the recommender can improve her learning rate signiﬁcantly by intervening in the data generation process through information design. In our environment, learning under the ternary recommendation policy is exponentially faster than learning under the binary recommendation policy. The diﬀerence in social welfare achieved is also large. Our contribution to this literature can be summarized as follows. First, we elucidate In the marketing science literature, adaptive conjoint analysis has been proposed as a method to pose questions to estimate users’ preference parameters in an adaptive manner. Several studies, such as Toubia et al. (2007) and Saur´e and Vielma (2019), have considered adaptive choice-based conjoint analysis, which regards choice sets as questions and actual choices as answers for them. This strand of the literature has also developed eﬃcient methods for intervening in the data generation process to extract users’ knowledge. However, in the recommender problem, the recommender is not allowed to select users’ choice sets to elicit their preferences. Instead, the recommender needs to design information. We consider a sequential game that involves a long-lived recommender and T short-lived users. At the beginning of the game, the state of the world θ ∼ Unif[−1, 1] is drawn. We assume that all the users are experts and more knowledgeable than the recommender is about the state θ initially. θ, the recommender knows only the distribution of θ. Accordingly, the recommender learns about θ through the data obtained in the game. with the shared context x unit variance) normal distribution. The context x both user t and the recommender. The recommender also observes her private information z∼ N, whose realization is not disclosed to user t. Each user has binary actions: arm −1 and arm 1. to zero: r We refer to x the navigation problem presented as an illustrative example, in which users are assumed to be familiar with the static road map but do not observe dynamic congestion information until they actually select the route. message space. For example, if the recommender just wants to recommends an arm, then Users arrive sequentially. At the beginning of round t ∈ [T ]:= {1, . . . , T }, user t arrives In round t, the recommender ﬁrst selects a recommendation a∈ A, where A is the the message space is equal to the action space: A = {−1, 1}. Observing the recommendation a, user t chooses an action b the market. The recommender cannot observe users’ payoﬀs. to an information structure that mechanically decides which message to submit. When the recommender makes her round-t recommendation, she can observe the sequences of all past recommendations, (a information, the recommender computes her belief about parameter θ using Bayes’ rule. The recommender’s belief at the beginning of round 1 is the same as the prior belief: Unif[−1, 1]. Due to the property of uniform distributions, the posterior distribution of θ always belongs to the class of uniform distributions. The posterior distribution at the beginning of round t is speciﬁed by Unif[l of the conﬁdence region in the beginning of round t. Note that the conﬁdence region [l shrinks over time: and thus the width of the conﬁdence region w assume users are informed of the recommender’s current conﬁdence region [l perspective of the recommender in round t, the predicted payoﬀ from arm 1 is where m z. User t computes the conditional expected payoﬀ from choosing arm 1, E[r Then, user t selects an arm b b= 1 if E[r take in a perfect Bayesian equilibrium of this sequential game. about θ, characterized by (l of (l as the sum of all users’ payoﬀs: ﬁrst-best scenario using regret. Regret is deﬁned as follows: As in the literature of information design, we assume that the recommender commits −1 =:l≤ l≤ ··· ≤ l≤ l≤ θ ≤ u≤ u≤ ··· ≤ u≤ u:= 1, Upon observing recommendation a, user t updates his belief about the dynamic payoﬀ After observing user t’s choice b, the recommender updates the posterior distribution , u) will be explained in detail in Section 4. The (utilitarian) social welfare is deﬁned where b reg(t) indicates the loss of welfare caused by the suboptimal decision-making in round t; thus, the maximization of the social welfare is equivalent to the minimization of the regret. If the recommender already knows (or has learned accurately) the state θ, then the recommender would always inform user t of the superior arm, and the user would always obey the recommendation. Therefore, b the recommender’s belief about the state θ is not accurate, then users cannot always select the superior arm. Therefore, the regret also measures the progress of the recommender’s learning of θ. and the order of regret Reg(T ). It is easy to observe that the regret achievable is closely related to the size of the message space. When the message space is a singleton (i.e., |A| = 1), the recommender can convey no information about the dynamic payoﬀ component z a result, users suﬀer from constant welfare loss for each round; therefore, the regret grows linearly in T : Reg(T ) = Θ(T ). In contrast, if the message space is a continuum (i.e., A = R), the recommender can inform each user t of the “raw data” about the dynamic payoﬀ z she can send a the superior arms for every round. There is no need for the recommender to learn, and the regret of exactly zero is achieved: Reg(T ) = 0 for all T . Nevertheless, an inﬁnite message space incurs a large cognitive cost, and is therefore inconvenient. Indeed, it is infeasible for real-world recommender systems to disclose all the information about current congestion. In the following, we evaluate the regret incurred with small ﬁnite message spaces, namely, the case of binary and ternary message spaces (|A| = 2, 3). First, we consider the case of the binary message space, i.e., A = {−1, 1} = B. We consider a natural recommendation policy that simply discloses an arm predicted to be superior; it := maxr(b) is the superior arm with respect to true payoﬀs. The value In this paper, we characterize the relationship between the size of the message space |A| Figure 1: Region of possible z quantity Z recommends arm 1 if and only if ˆr User t’s conditional expected payoﬀ from choosing arm 1 is given by where Z is the standard normal distribution, N. In addition, a a= −1 implies z truncated standard normal distribution. Let N distribution with support (α, β). Then, the posterior distribution of z a= −1 are N illustrated as Figure 1. To summarize, we have The arm that user t will choose is as follows: Since user t observes the quality of the recommender [l, u], he also knows the value of := (l+ u)/2. In addition, he observes the recommendation abefore choosing an arm. We evaluate Zto identify the user’s equilibrium behavior. The prior distribution of z x> 0, this is equivalent to θ > −Z Using this information, the recommender may be able to shrink the support of the posterior distribution about θ. We can analyze the case of b update rule is as follows: where sgn is the following signum function: First, we present a fundamental theorem that constitutes the basis for the concept of deviation-based learning. Conversely, if a recommendation (i.e., when a be at least halved. Since the recommender has an information advantage about z tion occurs only if the recommender signiﬁcantly misestimates the static payoﬀ component. Accordingly, upon observing a deviation, the recommender can update her belief about θ by a large amount. In contrast, when a user obeys the recommendation (i.e., when a decrease of w Upon observing the user’s decision b, the recommender updates her conﬁdence region, ]. When the user chooses b= 1, the recommender can recognize that xθ + Z> 0. If All the formal proofs are relegated to the appendix. Theorem 1 elucidates the informativeness of deviations. When a user deviates from the no update occurs and so w is small, and therefore, the user follows the recommendation blindly, given any θ ∈ [l We present our ﬁrst main theorem, which evaluates the order of total regret under the binary recommendation policy. Theorem 2 (Regret Bound of Binary Recommendation). There exists a rithmic) function large per-round regret even in the long run. zonly via recommendation. To help the user make the best decision, the recommender must identify which arm is better as a whole. The recommender must therefore learn about the state θ in order to ﬁgure out the value of r b. The more the recommender learns about θ, the less informative is the users’ feedback: rational users hardly deviate from (moderately) accurate recommendations because the recommender’s information advantage (in terms of information about the dynamic payoﬀ term) tends to dominate the estimation error. Consequently, when recommendations are accurate, deviations are rarely observed, and the recommender has few opportunities to improve her estimator how we derive Theorem 2 from these lemmas. Lemma 3 (Lower Bound on Regret per Round). The following inequality holds: where C recommendation. The probability that the recommender fails to recommend the superior Theorem 2 shows that the total regret is˜Ω(T ), which implies that users suﬀer from a While each user precisely knows his static payoﬀ xθ, he has access to the dynamic payoﬀ In the following, we provide two lemmas that characterize the problem and then discuss Since the recommender does not know θ, she substitutes mfor θ to determine her arm is proportional to |θ − m |θ − m Lemma 4 (Upper Bound on Probability of Update). There exists a universal constant error of the static payoﬀ term |x about the dynamic payoﬀ term z the recommendation, and the user’s decision does not provide additional information. Since w> |θ−m must be Ω(1/w a normal distribution, the probability that such a context arrives decreases exponentially in 1/w particular, if w is 1/T 1 −1/T . Lemma 4 implies the update of θ is likely to stall when it reaches w Θ(1/(log T )). Given |θ − m regret is Θ(1/(log T ) implying that users suﬀer from large per-round regrets even in the long run. This is how we obtained the regret bound presented as Theorem 2. Section 4 considered a binary message space, A = {−1, 1} = B. We have shown that the recommender fails to learn θ with the binary message space. In this section, we consider an alternative recommender system with a ternary message space, A = {−1, 0, 1} = B ∪ {0}. |. Accordingly, the per-round expected regret is at the rate of O(|θ − m|). > 0 such that, for all w≤ C, User t compares two factors for making his decision: (i) the recommender’s estimation Lemma 4 states that the recommender’s learning stalls when wis moderately small. In . This implies that no update occurs in the next T rounds with a probability at least We use these lemmas to obtain the total regret bound presented in Theorem 2. First, This ternary message space allows the recommender to inform users that she is “on the fence.” When the recommender is conﬁdent in her recommendation, she sends either a a= 1. If the recommender predicts that the user should be approximately indiﬀerent between two arms, then she sends a of parameters ( is conﬁdent in her prediction. If ˆr superiority of arm 1, and therefore recommends arm 1: a recommends arm −1: a honestly states that she is on the fence; she sends the message a predicts that the payoﬀs associated with arms 1 and −1 will be similar. This recommendation policy is summarized as follows: (ii) z given a distribution is as follows. which is illustrated in Figure 2. Given the new speciﬁcations of a rule for choosing b (given in Eq. (2) and (3)) are unchanged. The following theorem characterizes the total regret achieved by the ternary recommendation policy. Theorem 5 (Regret Bound of Ternary Recommendation). Let  regret is bounded as: Speciﬁcally, we focus on the following recommendation policy. We introduce a sequence , then the recommender is conﬁdent about the superiority of arm −1, and therefore User t’s posterior belief about zis given by (i) z∼ N(−∞, −xm−) given a= −1; ∼ N(−xm− , −xm+ ) given a= 0; and (iii) z∼ N(−xm+ , ∞) = 1. Accordingly, the conditional expectation of zwith respect to the posterior Figure 2: Region of possible z where C policy suﬀers from space drastically improves the eﬃciency of the recommendation policy. or not x the value of |x the recommendation. If the user were to know that |x would have a stronger motivation to defy the recommendation because the estimation error |x(θ − m recommender could “encourage” users to deviate from her recommendation so as to exploit the users’ knowledge more eﬃciently. When |x this fact by submitting message a making a better estimate of θ. The more accurate estimate on θ the recommender makes, the smaller the value of  the end, she is able to recommend a truly superior arm for every user. ﬁrst lemma, Lemma 6, computes the probability that a > 0. The quantity Zis the expected value of the gray region. This result contrasts with Theorem 2, which shows that the binary recommendation When the message space is binary, the recommender can inform user t only of whether m+ z> 0; thus, user t cannot ﬁgure out how large |xm+ z| is. However, )| matters. Accordingly, by informing users of the magnitude of |xm+ z|, the Our ternary recommendation policy eﬀectively achieves the scenario described above. m+ z| is smaller than the threshold value , the recommender informs the agent The following lemmas characterize the key properties that we use in Theorem 5. The result immediately follows from the fact that (i) z and (ii) a function of the policy parameter,  Lemma 7 (Upper Bound on Regret per Round). The following inequality holds: where C binary recommendations to derive the per-round expected regret of O(w a= 0 is sent with probability Θ( utility is (approximately) indiﬀerent between two arms, the per-round regret is bounded by then for every time that a Lemma 8 (Geometric Update). Let  where user, upon observing a the realization of the dynamic payoﬀ term: z if x recommender can identify whether or not θ > m region [l the recommender chooses a suﬃciently small  because in that case the probability of sending a must be chosen to balance this trade-oﬀ. Lemma 8 shows that  choice in the sense that it achieves a constant per-round probability of geometric updates. Lemma 6 states that the probability that a= 0 is recommended is linear in . This The second lemma, Lemma 7, bounds the per-round regret, reg(t), using a quadratic When an arm is recommended (i.e., when a6= 0), then we can apply the analysis for in this case. Hence, the per-round expected regret for this case is O(max(, w)). Finally, the third lemma, Lemma 8, guarantees that when is selected appropriately, The intuition for the geometric gain after a= 0 is as follows. When ≈ 0, then, the θ + z≈ x(θ − m) > 0 and b= −1 otherwise; in other words, by observing b, the , u], this observation halves the width of the conﬁdence region. Accordingly, when While a smaller results in a larger update after a= 0 is sent, we cannot set = 0 Θ( the width of conﬁdence region w leads an exponential convergence of has sent a is O(w geometric update occurs with probability Θ(w in one epoch is Θ(1/w incurred in one epoch is Θ(w with each epoch is bounded by a geometric sequence whose common ratio is C The total regret is the sum of the regret from all the epochs. Consequently, the total regret is bounded by the sum of a geometric series, which converges to a constant. This section provides the simulation results. For each path, we draw θ from Unif[−1, 1], and regret Reg(t) and the width of the conﬁdence region w and ternary recommendation policies. areas cover between 25 and 75 percentiles over runs. The whiskers drawn at the ﬁnal round (T = 10, 000) are the two-sigma conﬁdence intervals of the average values. We plot the cumulative regret, Reg(t), in Figures 3 and 4. As proved in Theorem 2, the simulation result implies almost-linear growth of the cumulative regret under the binary recommendation: Reg(T ) mulative regret is bounded by a constant. This observation is also consistent with Theorem 5, which shows that Reg(T ) = O(1) under the ternary recommendation. The simulation result additionally demonstrates that there is a large quantitative diﬀerence between the magnitude of total regret incurred by these two policies. For T = 10, 000, while the binary recommendation policy produces Reg(T ) ≈ 10 on average, that under the ternary recommendation policy produces Reg(T ) ≈ 0.2. Moreover, we can infer from Figures 3 and 4 that the diﬀerence The proof outline of Theorem 5 is as follows. By Lemma 6, the probability of a= 0 is ) = Θ(w). Together with Lemma 8, it follows that, in round t, with probability Θ(w), ). Let us refer to an interval between two geometric intervals as an epoch. Since a from the standard normal distribution i.i.d. for T = 10, 000 rounds. We analyze how For all graphs in this section, the lines are averages over 500 sample paths, and the shaded Figure 3: The evolution of cumulative regret Reg(t) under binary and ternary recommendation. would grow almost linearly as we further increase the number of rounds T . We now investigate how the width of the conﬁdence region, w on when and how frequently updates occur. We count the occurrence of belief updates, i.e., the number of rounds such that w (i) the set of rounds in which the user followed the recommendation is denoted by T (obedience), (ii) the set of rounds in which the user deviated from the recommendation is denoted by T recommend a particular action is denoted by T Note that |T tal, |T The updates by obedience occurs more often than the updates by deviation. ommends an arm only if she is conﬁdent about it, users follow the recommendation blindly whenever an arm is recommended; therefore, an update occurs only if the recommender con- Figures 5 and 6 plot the number of updates, |T(t)|, |T(t)|, |T(t)|, and their to(t)|+|T(t)|+|T(t)|. Figure 5 exhibits the case of binary recommendations. Figure 6 represents the case of ternary recommendations. Since the recommender rec- Figure 5: Number of updates under binary recommendations. fesses that she is on the fence. The opportunities for her learning are mostly concentrated at the beginning of the game, but updates occur occasionally even in the later stages of the game. On average, updates occur more frequently than in the case of binary recommendations. Note that it is always the case that ACC(t) = ACC binary recommendations and ternary recommendations. As illustrated in Figure 5 under the binary recommendation policy, learning from obedience occurs more frequently than learning Next, we evaluate the total amount of information acquired from each recommendation. − l= 1 − (−1) = 2, and therefore, ACC(0) is normalized to zero. We deﬁne the accuracy gain from each recommendation as follows: Figures 7 and 8 depict the accuracy gain from each recommendation under the cases of Figure 7: The breakdown of accuracy gains under binary recommendations. from deviations. Nevertheless, Figure 7 shows that the recommender acquires information more from deviations than obedience. This is because once a deviation occurs, it is much more informative than obedience (as implied by Theorem 1). are obtained when the recommender signals being on the fence. For any stage of the game, the learning rate is higher than that under binary recommendations, and the diﬀerence is quantitatively large. In round 10, 000, the average accuracy under the ternary policy becomes larger than that under the binary policy by (roughly) six points, which implies that w the ternary policy is e In this paper, we propose deviation-based learning, a novel approach for training recommender systems. Our approach is built upon a simple idea. When a user deviates from a recommendation, he is aware of the merit of the recommended option, but has concluded that another option provides him with a better payoﬀ. This event indicates that the recommender has misestimated the user’s preference, and so the recommender can update her estimate based on this information. Conversely, if a user follows a recommendation even though the recommender is not perfectly conﬁdent, then the recommender can increase her conﬁdence. The deviation-based learning is eﬀective when (i) payoﬀs are unobservable, (ii) there are many knowledgeable experts, and (iii) the recommender can easily identify the set of experts. Figure 8 reveals that, under the ternary recommendation, almost all the accuracy gains learning. In a stylized model with two arms, we demonstrated that a binary message space results in a large welfare loss. After the recommender is trained to some extent, users start to follow her recommendations blindly, and users’ decisions are uninformative for advancing the recommender’s learning. This eﬀect signiﬁcantly slows down her learning, and the total regret grows almost linearly in the number of users. In contrast, when the message space is ternary, the recommender can sometimes disclose the fact that she predicts that two arms will produce similar payoﬀs. With the ternary message space, the total regret is bounded by a constant (which does not depend on the number of users). caveat: the recommender should not consider the rate at which users follow recommendations to be a key performance indicator. When the recommender has an information advantage, the user may follow a recommendation even when it does not fully respect his own information and preference. Accordingly, if such a performance indicator is used, then the recommendation system may incur a large welfare loss, and the recommender may not be able to realize this fact. In practice, observable contexts (x are rarely linear in the parameter (θ), and their functional form may be unknown ex ante; thus, the recommender may have to adopt a nonparametric approach. While we believe that the insight obtained from our stylized model will be informative in general environments, more comprehensive and exhaustive analyses are necessary for practical applications. Our analysis reveals that the size of the message space is crucial for the eﬃciency of Our analysis of the binary recommendation policy also provides a simple but useful Future studies could investigate deviation-based learning in more complex environments.