The two-tower architecture has been widely applied for learning item and user representations, which is important for large-scale recommender systems. Many two-tower models are trained using various in-batch negative sampling strategies, where the eects of such strategiesinherently rely on the size of mini-batches. However, training two-tower models with a large batch size is inecient, as it demands a large volume of memory for item and user contents and consumes a lot of time for feature encoding. Interestingly, we nd that neural encoders can output relatively stable features for the1 same input after warming up in the training process. Based on such0 facts, we propose a simple yet eective sampling strategy called Cross-BatchNegativeSampling (CBNS), which takes advantage of the encoded item embeddings fromrecent mini-batches to boost the model training. Both theoretical analysis and empirical evaluations demonstrate the eectiveness and the eciency of CBNS. • Information systems → Recommender systems. Recommender systems; information retrieval; neural networks ACM Reference Format: Jinpeng Wang, Jieming Zhu, and Xiuqiang He. 2021. Cross-Batch Negative Sampling for Training Two-Tower Recommenders. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21), July 11–15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3404835.3463032 The recommender systems play a key role in recommending items of personal interest to users from a vast candidate pool, where learning high-quality representations for users and items is the core challenge. With the rapid development of deep learning, many approaches [6,12,27] are proposed to incorporate side information (e.g. the content features) into modeling user-item interactions, which eectively alleviate the cold-start problem [25] and improve the generalization ability to new items. Recently, neural recommenders with two-tower architecture, i.e., dual neural encoder for separating item and user representations, have been shown (a) In-Batch Negatives.(b) Cross-Batch Negatives. Figure 1: Sampling Strategies for Two-Tower Models to outperform traditional matrix factorization (MF)-based methods [8,17,24] and become popular in large-scale systems [4,9,20]. Training a recommender over the large-scale item corpus is typically translated into an extreme multi-class classication task using sampled softmax loss [2,9,23], in which the negative sampling counts for much. In the batch training for two-tower models, using in-batch negatives [13,36], i.e., taking positive items of other users in the same mini-batch as negative items, has become a general recipe to save the computational cost of user and item encoders and improve training eciency. There have been some recent eorts to bridge the sampling gaps between local (i.e., in-batch) and global (i.e., the whole data space) distributions [33,35], and it is not new to adopt some o-the-shelf hard-mining techniques [5,18,28] to make better use of informative negatives in a batch. Unfortunately, these in-batch strategies are inherently limited by the size of mini-batches. It is reasonable that increasing the size of mini-batches benets negative sampling schemes and can usually boost the performance immediately, while simply enlarging a minibatch of heavy sample contents subjects to the limited GPU memory. A naive solution is that, at each training iteration, we traverse the entire candidate set and collect the encoded dense representations of all candidates ina memorybank, after which we conduct negative sampling and optimize the softmax loss. Obviously, such a solution is too time-consuming for training two-tower recommenders, but it inspires us to develop a better scheme. In this paper, we nd an interesting characteristic of the neural encoders termed embedding stability. Concretely, a neural encoder tends to output stable features for the same input after the warmup stage of the training process. It suggests the worth of reusing the item embeddings from recent mini-batches as the negative embeddings for current mini-batch training because the stability guarantees that the reused embeddings approximate the output of current encoder for those reused items if we encode them again. Motivated by such phenomenon, we propose a novel sampling approach calledCross-BatchNegativeSampling (CBNS). Specically, we use a rst-in-rst-out (FIFO) memory bank for caching item embeddings across mini-batches in temporal order. The negatives of our CBNS come from two streams, i.e., the in-batch negatives and the cached negatives from recent mini-batches in the memory bank. In terms of computation of encoding features, CBNS only takes the basic cost to encode the users and items in the current training mini-batch, which is as ecient as the naïve in-batch sampling. It is worth noting that CBNS eectively decouples the linear relation between the number of negatives and the size of mini-batch, thus we can leverage much more informative negatives to boost model training. We also provide a theoretical analysis of the eectiveness of CBNS for training two-tower recommenders. Our contributions can be summarized as follows. ♣We show the embedding stability of neural encoders, which sheds light on the motivation of reusing the encoded item embeddings from recent mini-batches. ♣We propose a simple yet eective sampling approach termed Cross-BatchNegativeSampling (CBNS), which helps to leverage more informative negatives in training two-tower recommenders and thus boosts performance. ♣We provide both theoretical analysis and empirical evaluations to demonstrate the eectiveness and the eciency of CBNS. Two-Tower Models.The two-tower architecture is a general framework of a query encoder along with a candidate encoder, which has been widely studied in language retrieval tasks such as question answering [18], entity retrieval [13] and multilingual retrieval [34]. Recently, the two-tower architecture has been adopted into large-scale recommendation [4,9,16,20] and is becoming a fashion in content-aware scenarios [12,32]. Specially, the twotower models in recommendation are usually applied on a much larger corpus than in language retrieval tasks, which leads to the challenge of ecient training. Negative Sampling for Two-Tower Models.Traininga retrieval model is usually reformulated as an extreme classication task, where lots of sampling-based [1–3,9,23] methods have been proposed to improve the training eciency. In general, these methods globally (i.e., among the whole training set) sample negatives from some pre-dened distributions (e.g. uniform, log-uniform or unigram distributions). However, global sampling strategies are burdensome in training the two-tower models with content features, because:i) we have to make space for negatives apart from the positive pairs of users and items;ii) the intensive encoding becomes the bottleneck to training. As a result, when incorporating input features into two-tower models, sharing or reusing items as negatives in a mini-batch is a typical pattern for ecient training [7]. In this direction, there are recent in-batch mechanisms for selecting hard negatives [13,18,29,30], correcting sampling bias under an online training setting [35] or making a mixture with global uniform sampling [33]. Note that in-batch strategies are still not satisfactory: due to the pair-wise relation of sampled users and items, the maximum number of informative negatives is essentially bounded by the batch size while enlarging the batch suers from training ineciency just as in global sampling. In contrast, we nd it feasible to reuse the embedded items from the previous mini-batches and propose a simple, ecient and eective cross-batch negative sampling scheme, which improves the aforementioned drawbacks in a low-cost way. We note a parallel and complementary study [37] with CBNS that focuses on reducing sampling bias in large-scale recommendation from a view of contrastive learning [14]. It also veries the eectiveness of memory queue in training recommenders, while we further analyse the embedding stability to justify the CBNS. Our work is orthogonal to existing studies of sophisticated hard negative mining [10,31] or debiasing [35], which can also benet from our scheme with more negatives. We leave the explorations for the future work. We consider the common setup for large-scale and content-aware recommendation tasks. We have two sets of user informationU = {𝑼}and item informationI =𝑰respectively, where 𝑼∈ Uamd𝑰∈ Iare sets of pre-processed vectors of features (e.g. IDs, logs and types). In a user-centric scenario, given a user with features, the goal is to retrieve a subset of items of interests. Typically, we implement it by setting up two encoders (i.e., “tower”) 𝑓:𝑼 ↦→ R, 𝑔:𝑰 ↦→ Rfor users and items respectively, after which we estimate the relevance of user-item pairs by a scoring function, namely,𝑠 (𝑼 , 𝑰 ) = 𝑓(𝑼 )𝑔(𝑰 ) ≜ 𝒖𝒗, where𝒖and𝒗 denote the encoded embeddings for user and item from 𝑓and 𝑔. Typically, the large-scale retrieval is treated as an extreme classication problem with a (uniformly) sampled softmax, i.e., where𝚯denotes the parameters of themodel,Nis the sampled negative set and the superscript “” denotes the negatives. The models are trained with cross-entropy loss (equivalent to log-likelihood): To improve the training eciency of two-tower models, a commonly used sampling strategy is the in-batch negative sampling, as shown in Figure 1(a). Concretely, it treats other items in the same batch as negatives, and the negative distribution𝑞follows the unigram distribution based on item frequency. According to sampled softmax mechanism [1, 2], we modify equation (1) as 𝑠(𝑼 , 𝑰 ;𝑞) = 𝑠 (𝑼 , 𝑰 ) − log𝑞(𝑰 ) = 𝒖 wherelog𝑞(𝑰 )is a correction to the sampling bias. In-batch negative sampling avoids extra additional negative samples to the item tower and thus saves computation cost. Unfortunately, the number of in-batch items is linearly bounded by the batch size, thus the restricted batch size on GPU limits the performance of models. 3.3.1 Embedding Stability of Neural Model. As the encoder keeps updating in the training, the item embeddings from past minibatches are usually considered out-of-date and discarded. Nevertheless, we identify that such information can be reused as valid negatives in the currentmini-batch, because of theembedding stability of neural model. We investigate this phenomenon by estimating the feature drift [26] of the item encoder 𝑔, namely, where𝜽isthe parameters of𝑔,𝑡andΔ𝑡denotethe number and the interval of training iteration (i.e., mini-batch). We train a Youtube DNN [9] from scratch with an in-batch negative softmax loss and compute the feature drift with dierent intervals in{1, 5, 10}. As shown in Figure 2, the features violently change at the early stage. As the learning rate decreases, the features become relatively stable at about 4×10iterations, making it reasonable to reuse them as valid negatives. We termsuch phenomenon as “embedding stability”. We further showin Lemma3.1 that the embedding stability provides an upper bound for the error of gradients of the scoring function, so the stable embeddings can provide valid information for training. Lemma 3.1. Suppose thatˆ𝒗−𝒗< 𝜖, the output logit of scoring function isˆ𝑜≜ 𝒖ˆ𝒗and the user encoder𝑓satised Lipschitz continuous condition, then the deviation of gradient w.r.t user 𝒖is: where 𝐶 is the Lipschitz constant. Proof. The approximated gradient error can be compueted as: Empirically,𝐶 ≤1 holds with the models used in our experiments, thus the gradient errorcan becontrolled by embedding stability.□ 3.3.2 FIFO Memory Bank for Cross Batch Features. Since the embeddings change relatively violently at the early stage, we warm up the item encoder with naïve in-batch negative samplings for 4×10iterations, which helps the model approximate a local optimal and produce stable embeddings. Then we begin to train recommenders with an FIFO memory bankM ={(𝒗, 𝑞(𝑰))}, where 𝑞(𝑰)denotes the sampling probability of item𝑰under the unigram distribution𝑞and𝑀is the memory size. The cross-batch negative sampling (CBNS) with the FIFO memory bank is illustrated in Figure 2: Feature drift of YoutubeDNN [9] w.r.t. Δ𝑡s on Amazon-Books dataset [15, 22]. Figure 1(b), and the output of softmax of CBNS is formulated as At the end of each iteration, we enqueue the embeddings and the respective sampling probabilities in the current mini-batch and dequeue the earliest ones. Note that our memory bank is updated with embeddings without any additional computation. Besides, the size of the memory bank can be relatively large, because it does not require much memory cost for those embeddings. In this session, we conduct experiments with 3 typical two-tower models and compare CBNS with 3 representative competitors. Besides, we explore the choices of dierent memory bank sizes. Dataset.We conduct experiments on a challenging public dataset, Amazon, which consists of product reviews and metadata from Amazon [15,22]. In our experiment, we use the Books category of the Amazon dataset. The Amazon-book dataset is a collection of 8,898,041 user-item interactions between 459,133 users and 313,966 items. To standardize the evaluations, we follow [4,21] to split all users into training/validation/test sets by 8:1:1. Models.Togeneralize the conclusions, we conductexperiments on 3representativetwo-towermodels,i.e.,YoutubeDNN[9],GRU4REC[16] and MIND [20], with item and user features. Negative Sampling Strategies.We compare our CBNS withthree representative sampling strategies:i) Uniform Sampling;ii) InBatch Negative Sampling;iii) Mixed Negative Sampling [33] is a recent work to improve the in-batch sampling by additionally sampling some negatives from the global uniform distribution. Metrics.We report results of convergence for all experiments and use the following metrics in our evaluations in terms of performance and training eciency.i) Recall.ii) Normalized Discounted Cumulative Gain (NDCG).iii) Convergence Time. We report the convergence time in minutes for dierent sampling strategies with dierent models.iv) Average Training Time per 1000 Mini-Batches. To compare the batch-wise training eciency, we also report the average time in seconds for training every 1000 Mini-Batches. Table 1: Model performance with dierent sampling strategies on the Amazon-Books dataset. Bolde d numbers are the best performance of each group. “Conv. Time” and “Avg. Time” denote the training time till the early stop (in minutes) and the average time per10mini-batches (in seconds). Parameter Congurations.The embedding dimension𝑑is set to 64. The size of mini-batch is set to 128. The coecient forℓ regularization is selected from0, 10, 10, 10, 10via crossvalidation. We use the Adam optimizer [19] with the base𝑙𝑟 =0.001. The patience counter of the early stopping is set to 20. The numbers of globally sampled negatives for uniform sampling and MNS are set to 1280 and 1152 (there are 128 in-batch negatives in MNS). The default memory bank size 𝑀 for CBNS is 2432. Eectiveness of CBNS.We rst show the results of the combinations of 3 models and 4 sampling strategies in Table 1. Our CBNS consistently keeps the superior on Recalls and NDCGs over other sampling strategies with all tested models. For CBNS, it takes an acceptably longer average time per 1000 mini-batches than the in-batch sampling to compute similarities of cached embeddings, while it gets3.23%,12.19%,11.51%improvements to in-batch sampling with all tested models on NDCG@50, because of involving more informative negatives in batch training. We also discover several interesting insights from Table 1:1) Sharing encoded item embeddings in batch or reusing item embeddings across batches can improve the average training time, while it doesn’t mean accelerating convergence. The MNS converges much slower than other competitors since it takes more time for models to adapt to the mixed distribution.2) Though in-batch sampling is the most ecient strategy when we keep the same batch size for all strategies, we nd it inferior because of the lack of informative negatives. 3) Generally, the in-batch sampling and CBNS are unigram distributed, which is more similar to the true negative distribution and makes it compatible for models to serve online. The validation recall and NDCG curves w.r.t. the training iteration and the wall time respectively are shown in Figure 3, which demonstrates the fast convergence and satisfactory performance of CBNS. Memory Bank Size 𝑀 for CBNS.We further explore CBNS by investigating how the model performance changes under dierent memory bank size𝑀with a xed mini-batch size, as shown in Table 2. Dierent models have dierent best𝑀s. The Youtube DNN performs almost best when𝑀is 512 or 1152, while GRU4REC and Figure 3: Validation recall curves (a-c) w.r.t. the training iteration (i.e., mini-batch) and validation NDCG curves (d-f) w.r.t. the training wall time, all under the same batch size. Table 2: Metrics@50 (%) of 3 models trained with CBNS of dierent size of memory bank. Bolded numbers are thebest performance of each column. All the batch sizes are 128. MIND perform best under larger𝑀s as 4992 and 2432, respectively. Note that embedding stability only holds within a limited period, thus an overlarge𝑀brings less benet or adversely hurts models. In this paper, we propose a novel Cross-Batch Negative Sampling (CBNS) strategy for eciently training two-tower recommenders in a content-aware scenario, which breaks the dilemma of deciding between eectiveness and eciency in existing sampling schemes. We nd that neural encoders have embedding stability in the training, which enables reusing cross-batch negative item embeddings to boost training. Base on such facts, we set up a memory bank for caching the item embeddings from previous iterations and conduct cross-batch negative sampling for training. Compared with commonly used sampling strategies across three popular two-tower models, CBNS can eciently involve more informative negatives, thus boosting performance with a low cost.Future work includes exploring the combinations of CBNS with negative mining techniques to further improve the training of the two-tower recommenders.