Abstract—Graph Convolution Network (GCN) has been widely applied in recommender systems for its representation learning capability on user and item embeddings. However, GCN is vulnerable to noisy and incomplete graphs, which are common in real world, due to its recursive message propagation mechanism. In the literature, some work propose to remove the feature transformation during message propagation, but making it unable to effectively capture the graph structural features. Moreover, they model users and items in the Euclidean space, which has been demonstrated to have high distortion when modeling complex graphs, further degrading the capability to capture the graph structural features and leading to sub-optimal performance. To this end, in this paper, we propose a simple yet effective Quaternion-based Graph Convolution Network (QGCN) recommendation model. In the proposed model, we utilize the hypercomplex Quaternion space to learn user and item representations and feature transformation to improve both performance and robustness. Speciﬁcally, we ﬁrst embed all users and items into the Quaternion space. Then, we introduce the quaternion embedding propagation layers with quaternion feature transformation to perform message propagation. Finally, we combine the embeddings generated at each layer with the mean pooling strategy to obtain the ﬁnal embeddings for recommendation. Extensive experiments on three public benchmark datasets demonstrate that our proposed QGCN model outperforms baseline methods by a large margin. Index Terms—Recommender Systems, Collaborative Filtering, Graph Neural Network, Quaternion Embedding Recommender systems have been widely used for alleviating information overload in real-world applications, such as social media [1], news [2], videos [3], and E-commerce [4]. It aims to estimate whether a user will show a preference for an item, based on the user’s historical interactions. Among existing recommendation methods, Collaborative Filtering (CF) based models [5]–[9] have shown great performance in user and item representation learning. For example, Matrix factorization [10] and Neural collaborative ﬁltering model [6] are widely used CF models, which embed users and items into the latent space and model the user-item interactions with inner product. Recently, GCN-based recommendation models have surged to learn better user and item representations in the useritem bipartite graph. The typical ﬂow can be summarized Rutgers University, New Jersey, USA victor.sheng@ttu.eduzxf@cse.ust.hk as follows: 1) Initialize user and item representations by embedding them into the latent space; 2) Use an aggregation function over neighbors of each node to update its representation iteratively; 3) Readout the ﬁnal representation of each node by combining or concatenating. The paradigm of GCN iterative aggregating feature information from local graph neighbors has been proved to be an efﬁcient way to distill additional information from graph structure and thus improve user and item representation learning. For example, PinSage [11] combines random walk and graph convolutions to learn the embeddings of nodes. GC-MC explores the ﬁrst-order connectivity between users and items by utilizing only one convolution layer over the user-item bipartite graph. NGCF [12] leverages the message-passing mechanism to obtain highorder connectivity and collaborative signal in the user-item integration graph. LightGCN [13] removes two components, feature transformation and non-linear activation in NGCF [12], leading to improvement in training efﬁciency and generation ability. Despite effectiveness, GCN is still vulnerable to noisy and incomplete graphs, which are common in real-world scenarios, due to its recursive message propagation mechanism [14]–[16]. However, some latest GCN-based recommendation models (e.g. LightGCN [13]) propose to remove the feature transformation during message propagation, but making it unable to effectively capture the graph structural features and become more sensitive to noisy or missing information. Moreover, they model users and items in the Euclidean space, which has been demonstrated to have high distortion when modeling complex graphs [17], [18], further degrading the capability to capture the graph structural features and leading to sub-optimal performance. Can we move beyond the Euclidean space to learn better user and item representations and feature transformation, capture the graph structural features more effectively, and thus improve both recommendation performance and model robustness? Quaternion space - a hyper-complex vector space, where each quaternion is a hyper-complex number consisting of one real and three imaginary components, has shown great performance in representation learning [19]–[21]. Hamilton product, which is the multiplication of quaternions, enhances the inter-latent interactions between real and imaginary components of two quaternions, and any slight change in the input quaternion results in an entirely different output, leading to highly expressive computations, and thus the intricate relations are captured more powerfully [22]. As shown in Fig. 1, real-value transformation consists of 16 different components {Q|i ∈ [1, 4], j ∈ [1, 4]}, while Quaternion transformation consists of 4 weighting components {Q, Q, Q, Q} due to the wight sharing nature of Hamilton product (c.f. Equation 14), leading to up to four times reduction of parameters. There has been signiﬁcant success of quaternion-based methods in various ﬁelds. For example, [23] applies a quaternionic Fourier transform and a quaternionic Gabor ﬁlter and exploits the symmetries inherent in the quaternion to ﬁnd differences between subtly varying images. [24] explores the beneﬁts of generalizing one step further into the Quaternion space and provides the architecture components needed to build deep quaternion networks. [25] re-designs the basic modules like convolution layers and fully-connected layers in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks. [26] applies the Quaternion space into recurrent neural network (RNN) and long-short term memory neural network (LSTM) and achieves better performance than the basic model in a realistic application of automatic speech recognition. [27] integrates multiple feature views in quaternion-valued convolutional neural network (QCNN) to be used for sequence-to-sequence mapping with the CTC model. [28] investigates modern quaternion-valued models such as convolutional and recurrent quaternion neural networks in the context of speech recognition. Recently, there has been some work introducing the Quaternion space into graph representation learning to obtain more expressive graph-level representations [19]–[21]. For example, [19] generalizes graph neural networks within the Quaternion space for graph classiﬁcation, node classiﬁcation, and text classiﬁcation. [20], [21] introduce more expressive quaternion representations to model entities and relations for knowledge graph embeddings for knowledge graph completion. However, there is almost no exploration of the Quaternion space in GCNbased recommendation scenarios. Some challenges during this process remain to be explored. The most crucial one is that: The model should not be designed to be very complex or redundant to better validate the effectiveness of the Quaternion space and for more intuitive comparison. In other words, how to introduce the Quaternion space while keeping the model as simple as possible remains to be considered. To this end, in this paper, we propose a simple yet effective Quaternion-based Graph Convolution Network (QGCN) recommendation model, which improves both performance and robustness. Speciﬁcally, we ﬁrst embed all users and items into the Quaternion space with quaternion embeddings. Then, we introduce the quaternion embedding propagation layers with quaternion feature transformation to perform message propagation for aggregating more useful information. Finally, we combine the embeddings generated at each layer with the mean pooling strategy to obtain the ﬁnal embeddings for recommendation. The quaternion feature transformation enhances the inter-latent interactions between real and imaginary components, enabling it to capture the graph structural features more effectively, distinguish the contribution of different nodes during message propagation, and thus improve both performance and robustness. Extensive experiments are conducted on three public benchmark datasets to validate the effectiveness of our proposed QGCN model. Results show that QGCN outperforms the state-of-the-art methods by a large margin, which indicates that it can better learn user and item representations. Besides, with further robustness analysis, we ﬁnd that the performance of our QGCN model remains steady in various noisy or incomplete graphs, while that of compared state-of-the-art methods declines dramatically. This indicates that our model is more robust and can effectively capture the graph structural features. We summarize the contributions of this work as follows: • To the best of our knowledge, we are the ﬁrst to introduce the Quaternion space into GCN-based recommendation models. • A QGCN model is proposed to model users and items in the Quaternion space and propagate them with quaternion feature transformation, which signiﬁcantly enhances both recommendation performance and model robustness. • We conduct extensive experiments on three public benchmark datasets to evaluate the effectiveness of our proposed model. Experimental results demonstrate that our QGCN model outperforms baseline methods by a large margin, conﬁrming the effectiveness of the quaternion embeddings and quaternion feature transformation. Results of robustness analysis show that our QGCN model is more robust to noisy and incomplete graphs, verifying the effectiveness of the quaternion feature transformation capturing the graph structural features. In this section, we ﬁrst introduce the notations used in this paper and give a formal problem deﬁnition of graph-based collaborative ﬁltering for recommendation. Table I summarizes the notations and the corresponding description. We denote the set of users and items as U and I, and the number of users and items are respectively M and N. We construct the user-item interaction matrix R ∈ R where R= 1 represents user u has interacted with item i. Nand Nrespectively denote the user u’s interacted items and the item i’s interacted users, respectively. The adjacency matrix A ∈ Ris constructed based on the user-item interaction matrix. Then, we deﬁne the graph-based collaborative ﬁltering for recommendation as follows. Given the user-item interaction matrix R, our goal is to estimate whether a user u ∈ U will show a preference for an item i ∈ I based on the user and item embedding generated after L layers’ graph convolution. In this section, we ﬁrst recap the start-of-the-art framework of GCN-based recommendation models and then cover some necessary background on quaternion before delving into the architecture of our proposed model. Let edenote the ID embedding of user u and edenote the ID embedding of item i. 1) NGCF: NGCF [12] leverages the message-passing mechanism to obtain high-order connectivity and collaborative signal in the user-item integration graph. The message passing strategy and node aggregation is deﬁned as follows: where mand mrespectively denote the message propagated from user u’s and item i’s neighbors; pis set top the graph Laplacian norm 1/|N||N|, where |N| and |N| respectively denote user u’s interacted items and item i’s interacted users; Wand Ware the trainable transformation matrices. Then the user and item embedding are updated by the sum of the node embedding itself and its neighbors with an activation function LeakyReLU: NGCF adopts the concatenation strategy that the representations generated at each layer are concatenated as the ﬁnal node representation: 2) LightGCN: LightGCN [13] removes two components, feature transformation and non-linear activation in NGCF. It not only simpliﬁes the model itself but also leads to improvement in training efﬁciency and generation ability. The embedding propagation is deﬁned as follows: Different from NGCF, LightGCN adopts weighted sum strategy to aggregate the representations at each layer: where λ≥ 0 denotes the importance of the k-th layer embedding for the ﬁnal node embedding. After obtaining the ﬁnal representations of nodes, the inner product is conducted to estimate the user u’s preference towards the target item i: The Bayesian Personalized Ranking (BPR) loss [29] is employed in both NGCF and LightGCN to optimize the model parameters, i.e. minimizing the following loss function: Loss =−ln σ(ˆy− ˆy) + λkΘk where Ndenotes user u’s interacted items; σ is the sigmoid function; λ represents the regularization weight and Θ denotes model parameters. B. Quaternion 1) Quaternion: A quaternion Q ∈ H is a hyper-complex number consisting of one real part and three imaginary parts deﬁned as: where Q, Q, Q, Q∈ R, and i, j, k are imaginary units, satisfying the following rule: Corresponding to the deﬁnition of quaternion, the ndimensional vector form of quaternion Q ∈ His deﬁned as: where Q, Q, Q, Q∈ R. 2) Quaternion Addition: The addition of two quaternions Q and P is deﬁned as: Q + P = (Q+ P) + (Q+ P)i + (Q+ P)j + (Q+ P)k, 3) Quaternion Inner Product: The inner product of two quaternions Q and P is deﬁned as: 4) Hamilton Product: The quaternion product of two quaternions Q and P is deﬁned as: We further simplify the result of Hamilton product above into matrix form as follows: In this section, we present our proposed QGCN model. As illustrated in Fig. 2, the model contains three main components: Quaternion Embedding Layer, Quaternion Embedding Propagation Layers, and Prediction Layer. Firstly, we embed all the users and items into the Quaternion space. For each user u ∈ U, we represent it with a quaternion ID embedding e∈ H, where d represents the quaternion dimension. And the same for item quaternion ID embeddings, each item i ∈ I is initialized with a quaternion ID embedding e∈ H. The initial quaternion ID embedding for users and items can be deﬁned as follows: where e, e, e, e∈ R, ∀m ∈ {1, 2, . . . , M } and e, e, e, e∈ R, ∀n ∈ {1, 2, . . . , N}. M, N respectively denote the number of users and items. B. Quaternion Embedding Propagation Layers 1) Quaternion Embedding Propagation: Next, we perform message propagation within the Quaternion Embedding Propagation Layers with quaternion feature transformation. As mentioned above, we argue that removing the feature transformation during message propagation makes it unable to effectively capture the graph structural features and become more sensitive to noisy or missing information, further degrading the model performance. So in this part, we introduce the feature transformation in the Quaternion space at each layer for message propagation to aggregate more useful information. In order to prove our quaternion feature transformation to be valid more intuitively, we adopt the simple message propagation procedure like the vanilla GCN [30] without the nonlinear activation function, only involving the user and item embeddings and the quaternion transformation matrices. We generate the quaternion transformation matrix at layer l as follows: where W, W, W, W∈ R. Thus, our quaternion embedding propagation rule in QGCN is deﬁned as: where eand erespectively represent user u’s quaternion embedding and item i’s quaternion embedding after l layersp propagation; 1/|N||N| is the symmetric normalization term following the vanilla GCN [30], designed to avoid the scale of embeddings increasing with graph convolution operations, where Nand Nrespectively denote the user u’s interacted items and the item i’s interacted users; W∈ His the quaternion feature transformation matrix at layer l; ⊗ denotes Hamilton product. To facilitate the implementation of the quaternion embedding propagation, we derive the Hamilton product ⊗ between Wand ein Equation 17 as follows (c.f. Equation 14): Similarly, the result of Hamilton product ⊗ between W and ecan be derived as follows: 2) Dropout and L2Norm: Dropout drops the units of the neural networks with a certain probability during the training process, which proves to be an effective way to prevent neural networks from overﬁtting [31], [32]. Motivated by the previous work of introducing dropout into graph convolutional network [33] and GCN-based recommendation models [12], we apply dropout to the user and item embeddings at each layer l with a certain dropout rate p, which is one of the critical hyperparameters to be tuned. Then, we perform L2 Normalization function on them for training speed and stability. We summarize the dropout and L2 normalization as follows: 3) Quaternion Propagation Rule in Matrix Form.: To better facilitate the implementation of our QGCN model, we provide the quaternion embedding propagation rule in matrix form. As deﬁned in Table I, we denote the the user-item interaction matrix as R ∈ R, where M and N denote the number of users and items respectively and each element in R denotes the interaction, that if user u has interacted with item i, then Ris set to 1, otherwise 0. Then, the adjacency matrix of the user-item graph A ∈ Rcan be generated as: Next, we can obtain the diagonal matrix D ∈ R correspondingly, where each diagonal element Ddenotes the number of nonzero nodes in the i-th row vector of the adjacency matrix A. Then, we generate the Laplacian matrix L = DAD. As mentioned above, we derive the Hamilton product to facilitate the implementation of the quaternion embedding propagation in Equation 18 and Equation 19. Thus, we obtain the quaternion propagation in matrix form as: where E∈ Hdenote the embedding look-up table at layer l, E= (e, . . . , e, e, . . . , e); W∈ Hdenotes the quaternion feature transformation matrix at layer l. After the quaternion embedding propagation, we apply dropout and L2 normalization to them: After the above L layers’ quaternion embedding propagation, dropout and L2 normalization, we obtain L + 1 representations for each user u and item i, including the user embedding initialized at quaternion embedding layer, eand user representations generated at each layer during propagation, {e, e, . . . , e}. And the same for item i, we obtain L + 1 item representations which consist of {e, {e, e, . . . , e}}. Since the output of different layers expresses different connections, utilizing the representations of all layers seems like an effective method for GCN-based models. Readout function is the method to obtain the ﬁnal node representation, e.g. Max, Sum, Concat, Mean pooling, which are the most primitive and simple pooling methods. Speciﬁcally, Max, Sum, Mean pooling respectively take the maximum, sum, mean value of the corresponding position of representations generated at each layer, and Concat concatenates representations generated at each layer. We summarize these readout functions as follows: Max pooling takes the maximum value of the corresponding position of representations at each layer: Sum pooling sums over value of the corresponding position of representations at each layer: Concat concatenates representations at each layer: Mean pooling takes the mean value of the corresponding position of representations at each layer: Since we generate user and item representations in the form of quaternion hyper-complex vector, we ﬁrst concatenate the real and imaginary components of the node embeddings and then apply the original pooling methods as follows: where Readout is the readout function (i.e. Max, Sum, Concat, Mean pooling) applied on the node embeddings generated at each layer. We further conduct experiments and investigate the inﬂuence of the readout function applied to our model in the ablation study part. After generating the ﬁnal user and item embeddings, we predict by the inner product of user u and item i: We adopt Bayesian Personalized Ranking (BPR) loss [29], which encourages the observed interactions to achieve higher scores than the unobserved ones. The objective function for our QGCN model is as follows: Loss =−ln σ(ˆy− ˆy) + λkΘk, (32) where Ndenotes user u’s interacted items; σ is the sigmoid function; λ represents the regularization weight, which is Lregularization to prevent overﬁtting; Θ =no {e}, {e}, {W}denotes all trainable parameters of QGCN. The mini-batch Adam [34] is adopted to optimize the prediction model and update the model parameters. In particular, for a batch of randomly sampled triples {(u, i, j)|i ∈ N, j /∈ N}, their representations can be obtained by the propagation rules and then the model parameters are updated by using the gradients of the loss function. Input: User-item interaction matrix R, the number of graph convolution layers L, the initialized user embedding eand item embeddings e, the quaternion transformation matrix = Concat{e, ee, e} = M ean{e} Output: Final user embedding eand item embedding e for recommendation. As deﬁned in Table I, R, |V | and |E|, L and d respectively represent the user-item interaction matrix, the number of nodes and edges, the number of graph convolution layers, and the quaternion dimension. 1) Time Complexity: The time complexity of our model is mainly in the following three parts, adjacency matrix, graph convolution, and BPR loss. For the adjacency matrix, the time complexity is O(|E|) that we set each element R= 1 in user-item interaction matrix R if user u has interacted with item i. For the graph convolution, the quaternion embedding propagation has computation complexity O(L|E|d). For the BPR loss, the time complexity is O(|E|d). Therefore, the overall time complexity of our model is O(|E|+ L|E|d+ |E|d). 2) Space Complexity: The space complexity of our model is mainly in the user and item embeddings and the quaternion transformation matrix at each layer. Therefore, the overall space complexity of our model is O(|V |d + Ld). In this section, we ﬁrst brieﬂy describe the datasets and our experimental settings, including evaluation metrics, baselines, and parameter settings. Then, we conduct a detailed comparison with LightGCN [13] and some state-of-the-art baseline methods, followed by the experimental results and our detailed analysis. Moreover, we perform a robustness analysis to explore the robustness of our QGCN model to noisy and incomplete graphs. Besides, ablation studies are performed to investigate the inﬂuence of readout function and different components of our QGCN model on the model performance. Finally, we discuss the impact of the critical hyper-parameters on the ﬁnal results. Speciﬁcally, we conduct experiments to try to answer the following research questions: • RQ1 How does our proposed QGCN model perform compared with the state-of-the-art baselines? • RQ2 How can QGCN alleviate the problem of noisy or incomplete graphs? • RQ3 What is the inﬂuence of readout function, quaternion embedding and quaternion weight matrices on the model performance? • RQ4 How do the key hyper-parameters, such as dropout rate and regularization affect the effectiveness of QGCN? To evaluate the effectiveness of QGCN, we conduct experiments on three benchmark datasets: Yelp2018 [13], AmazonBook [13], and Amazon-Kindle-Store [35], which are publicly available. The ﬁrst dataset is the 2018 edition Yelpreleased by the Yelp challenge. The last two datasets are two widely used datasets for product recommendation from Amazon review. Following the general dataset settings in previous recommendation methods, we ﬁlter users and items with few interactions to ensure the quality of the datasets [12], [13], [35]. Speciﬁcally, for all the datasets, we use the 10-core settings, which ensure that each user and item have at least 10 interactions. The detailed statistics of the three datasets are shown in Table II. We randomly split each dataset into training, validation, and testing set with a ratio of 80:10:10 for each user. For each observed user-item interaction, we treat it as a positive instance. Then, we randomly sample one negative item that the user did not consume before as a negative instance to pair the positive instance. B. Experimental Settings 1) Evaluation Metrics: To evaluate the effectiveness of our model on top-K recommendation, we take two evaluation metrics widely used in previous work: Recall@K and NDCG@K. Here, we set K = 20 by default, and the average results for all users in the testing set are reported. The speciﬁc deﬁnition is as follows: • Recall@K describes the percentage of user-item rating records included in the ﬁnal recommendation list. We denote the recommendation list for a user as R, and the corresponding testing set as T . Then, the speciﬁc deﬁnition of Recall@K is as follows: • NDCG@K i.e. Normalized Discounted Cumulative Gain measures the quality of ranking, which emphasizes more on the relevance of the items on the top of the recommendation list. We denote the relevance of the i-th item in the recommendation list as r, and the set of relevant items as R. Then, the speciﬁc deﬁnition of NDCG@K is:DCG@K where DCG@K and IDCG@K are deﬁned as follows: 2) Baselines: To demonstrate the effectiveness of our proposed QGCN model, we compare QGCN with the following competitive baseline methods: • NeuMF [6]: NeuMF, a state-of-the-art neural collaborative ﬁltering model, captures the non-linear interactions between user and item embeddings with multiple hidden layers. • HOP-Rec [36]: HOP-Rec, a state-of-the-art graph-based model, exploits the high-order connectivity between users and items by performing random walks to augment a user’s interactions. • GC-MC [37]: GC-MC explores the ﬁrst-order connectivity between users and items by utilizing only one convolution layer over the user-item bipartite graph. • NGCF [12]: NGCF leverages the message-passing mechanism to obtain high-order connectivity and collaborative signal in the user-item integration graph. • LightGCN [13]: LightGCN removes two components, feature transformation and non-linear activation in NGCF, leading to improvement on training efﬁciency and generation ability. 3) Parameter Settings: We implement our QGCN model in PyTorch. The embedding size is ﬁxed to 64 for all models. We optimize QGCN with Adam [38] with the default learning rate of 0.0001 and set batch size as 2048 for speed. We apply a grid search for the only two hyper-parameters: the dropout rate is tuned among {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8} and the coefﬁcient of Lnormalization in Equation 32 is searched in {1e, 1e, . . . , 1e}.The embedding parameters are initialized with the Xavier method [39]. C. Performance Comparison (RQ1) 1) Comparison with LightGCN: We conduct experiments under different graph convolution layer numbers for detailed comparison with LightGCN, and results are shown in Table III. The percentage of relative improvement at each layer on Recall@20 and NDCG@20 is calculated as well. Speciﬁcally, the results of LightGCN on Yelp2018 and Amazon-Book are copied from its original paper [13], and we tune the hyperparameters of LightGCN (i.e. the Lregularization coefﬁcient λ) on Kindle-Store and report with the optimal settings. We further plot the training curves of training loss and testing recall per 10 epochs on Kindle-Store and Amazon-Book with optimal settings on both LightGCN and our QGCN model in Fig. 3, where results on Yelp2018 show the same trend and are omitted for space. We summarize the main observations as follows: • In most cases, QGCN outperforms LightGCN by a large margin under different layer numbers ranging from 1 to 4 layers. The average improvement over all the datasets under different layer numbers is 19.11%, 14.70%, 10.88% and 9.08% w.r.t. Recall@20 and 19.43%, 15.25%, 10.74% and 9.16% w.r.t. NDCG@20. The signiﬁcant improvement under each layer number on each dataset indicates that the quaternion embedding and quaternion feature transformation enhance the representation learning a lot. • The improvements on Yelp2018 are relatively less signiﬁcant, and the best results are gained at four layers. On Amazon-Book and Kindle-Store, our QGCN model gains huge and up to 30%, relative performance improvement, and it can achieve the best results with only one quaternion embedding propagation layer. We ascribe this to the characteristics of the datasets, sparsity, and the excellent representation learning capability of the quaternion embedding and quaternion feature transformation. As we mentioned above, the quaternion feature transformation enhances the inter-latent interactions between real and imaginary components, enabling it to capture the graph structural features more effectively, distinguish the contribution of different nodes during message propagation, and thus improve both performance and robustness. Therefore, as the sparsity of the dataset decreases, the quaternion feature transformation could highlight its contribution to distilling sufﬁcient information from the sparse user-item interaction graphs and further lead to more signiﬁcant performance improvement. Moreover, the observations mentioned above that our QGCN model gaining huge relative performance improvement on sparse graphs is of great signiﬁcance to the practical applications and real recommendation scenarios since real-world graphs are often extremely sparse. • Our QGCN model obtains relatively lower training loss during the whole training process than that in LightGCN, which indicates our QGCN model can better ﬁt the training data and further obtains better testing results. It demonstrates that with the quaternion embedding and quaternion feature transformation, our model obtains stronger generalization capability. 2) Comparison with SOTA Methods: Table IV shows the performance with competing methods. The best results are highlighted in bold. From Table IV, we have the following observations: • NeuMF, a state-of-the-art neural collaborative ﬁltering model, performs relatively poorly since it captures the connectivity between user and item embeddings in the embedding learning process rather than leveraging the high-order user-item interactions. • Compared with NeuMF, GC-MC utilizes one convolution layer to explore the ﬁrst-order connectivity between users and items and improve the performance, demonstrating the inﬂuence of ﬁrst-order neighbors for representation learning. • HOP-Rec exploits the high-order connectivity between users and items by performing random walks to augment a user’s interactions, resulting in better performance than GC-MC. NGCF performs much better over the above baselines. It leverages the message-passing mechanism to obtain high-order connectivity and collaborative signal in the user-item integration graph. LightGCN removes two components, feature transformation and non-linear activation in NGCF, leading to improvement in training efﬁciency and generation ability. • QGCN outperforms all the baselines by a large margin over all the datasets. In particular, compared with the strongest baseline, i.e LightGCN, QGCN gains on average 14.03% improvement w.r.t. Recall@20 and 15.30% improvement w.r.t. NDCG@20 over all the datasets. The signiﬁcant improvements reveal that QGCN can better capture high-order user-item connectivity and learn better user and item embeddings. D. Robustness Analysis (RQ2) 1) Random Edges Injection: To investigate the robustness of our QGCN model to noisy graphs, we conduct simulated experiments to explore the inﬂuence of random injection of edges. Speciﬁcally, we randomly connect the unobserved edges in the user-item interaction graph R as noisy edges to construct a noisy graph for the training process. The noise ratio is set in {5%, 10%, 15%, 20%, 25%}. By the way, the compared LightGCN model and our QGCN model are trained with the same constructed noisy graph for a fair comparison. And we evaluate with the original graph (i.e. 0% edges injection). We further plot Recall@20 and relative drop compared with their original performance of both LightGCN and our QGCN model on Kindle-Store and Yelp2018 in Fig. 4. We observe that QGCN consistently outperforms LightGCN by a large margin under different ratios of random edges injection on both Kindle-Store and Yelp2018. Along with the increase of the noise ratio, the performance of LightGCN decreases accordingly, while that of our QGCN model remains almost unchanged. For example, Recall@20 of LightGCN in the noisy graph with 25% noise ratio of noise injection on Yelp2018 is 0.0568, dropping 12.48% (i.e. -12.48%) compared to the original performance, 0.0649. In contrast to the large drop percent of LightGCN, the performance of our QGCN model under 25% noise ratio even rises by 0.75% (i.e. +0.75%) compared to that under 0% noise ratio. The sharp decline of the relative drop of Recall@20 of LightGCN along with the increase of noise ratio reveals that LightGCN is extremely sensitive to noise, which is consistent with our argument mentioned before. Compared with the steep decline curve of Recall@20 of LightGCN, the relative performance change curve of our QGCN model is more steady, which demonstrates the robustness of our QGCN model to noisy graphs. 2) Random Edges Discard: In addition to the characteristic of real-world user-item graphs containing a lot of noise, they are often incomplete as well. Thus, besides the simulated experiments on exploring the inﬂuence of random injection of edges, we also conduct experiments to explore the inﬂuence of the random discard of edges. Similarly, we construct a corrupted graph by randomly disconnect the existing edges in the user-item interaction graph R with a drop ratio ranging in {5%, 10%, 15%, 20%, 25%}. We then train the compared LightGCN model and our QGCN model with the corrupted graph and evaluate with the original graph (i.e. 0% edges discard). The details of Recall@20 and relative drop are shown in Fig. 5. We have similar observations from Fig. 5. Speciﬁcally, QGCN consistently outperforms LightGCN by a large margin w.r.t different ratios of random edges discard on both KindleStore and Yelp2018. The steep performance decline curve of LightGCN is in sharp contrast to the steady curve of QGCN, demonstrating the robustness of our QGCN model to corrupted graphs. The simulated experiments on exploring the inﬂuence of random injection and discard of edges both demonstrate the robustness of our QGCN model. We ascribe this to the expressive quaternion feature transformation, distinguishing the contribution of different nodes and effectively capturing the graph structural features during message propagation. Thus, it can aggregate more useful information and further lead to better model performance and robustness. E. Ablation Study (RQ3) 1) Inﬂuence of Components: We perform ablation studies to explore the contribution of different components to the model performance by comparing QGCN with the following two variants: • QGCN-Q: In this variant, we embed all users and items into the real-value space instead of the Quaternion space and maintain the component of feature transformation. • QGCN-W: This variant removes the quaternion transformation matrices during message propagation. Table V shows the results of the two variants of QGCN, and the best results are highlighted in bold. QGCN performs much better than QGCN-Q, which shows the signiﬁcant inﬂuence of modeling in the Quaternion space. And QGCN outperforms QGCN-W in most cases, indicating the effectiveness of quaternion transformation matrices. The comparison between QGCN and its two variants demonstrates that the design of our proposed QGCN model is reasonable and effective. 2) Inﬂuence of Readout Function: Since different pooling methods generate different ﬁnal user and item embeddings, we conduct experiments and investigate the inﬂuence of the readout function applied to our model. Table VI shows the results under different readout functions, and the best results are highlighted in bold. We can observe that Mean pooling performs relatively better than the other three readout functions, Max, Sum, Concat pooling. We think Mean pooling method could not only maintain the information of nodes but also uniform the user and items representations generated at each layer, leading to more powerful generalization capability. F. Hyper-parameter Study (RQ4) 1) Effect of Dropout Rate: Dropout drops the units of the neural networks with a certain probability during the training process, which proves to be an effective way to prevent neural networks from overﬁtting [31], [32]. Motivated by the previous work of introducing dropout into graph convolutional network [33] and GCN-based recommendation models [12], we investigate the inﬂuence of the dropout rate p ranging from 0.0 to 0.8 on our proposed QGCN model. Fig. 6 displays the experimental results, including Recall@20 and NDCG@20, under different dropout rates over all the datasets. For Yelp2018 and Amazon-Book, the dropout rate set as 0.1 leads to the best performance, while that set as 0.0 leads to the best performance on Kindle-Store. Besides, the performance degrades generally after the peak in that too many neurons lost leads to underﬁtting and limits the expression of our model. These observations are consistent with the ﬁndings of prior effort [12] and demonstrate the effectiveness of proper dropout rate settings in our model. 2) Effect of Regularization: Regularization is an effective strategy to prevent overﬁtting, so that we tune the coefﬁcient of Lnormalization λ among {1e, 1e, . . . , 1e} to investigate the inﬂuence of the regularization on our proposed model. Fig. 7 shows the performance of our QGCN model under different regularization coefﬁcients λ on Yelp2018 and Amazon-Book, and the effect of regularization over KindleStore are omitted for exactly the same trend. As shown in Fig. 7, too small or too large regularization coefﬁcient result in relatively poor performance. Results are relatively steady when the regularization coefﬁcient λ is set between 1eand 1e, while the performance signiﬁcantly decrease when λ is set larger than 1eor smaller than 1e. This indicates that a medium regularization coefﬁcient is more suitable for our model. Speciﬁcally, the optimal regularization coefﬁcient for Yelp2018, Amazon-Book, and Kindle-Store is 1e, 1eand 1erespectively. Quaternion space is a hyper-complex vector space, where each quaternion is a hyper-complex number consisting of one real and three imaginary components. Owing to Hamilton product, which is the multiplication of quaternions, the interactions between real and imaginary components of two quaternions are enhanced, leading to highly expressive computations and up to four times reduction of parameters. In addition, if any slight change happens in the input quaternion, Hamilton product will generate an entirely different output [22] and further inﬂuence the ﬁnal performance. The Quaternion space has been successfully employed in various ﬁelds. For example, [23] applies a quaternionic Fourier transform and a quaternionic Gabor ﬁlter and exploits the symmetries inherent in the quaternion to ﬁnd differences between subtly varying images. [24] explores the beneﬁts of generalizing one step further into the Quaternion space and provides the architecture components needed to build deep quaternion networks. [25] re-designs the basic modules like convolution layer and fullyconnected layer in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks, and results show that they outperform the real-valued CNNs with the same structures. [26] applies the Quaternion space into recurrent neural network (RNN) and long-short term memory neural network (LSTM) and achieves better performance than the basic model in a realistic application of automatic speech recognition. [27] integrates multiple feature views in quaternion-valued convolutional neural network (QCNN), to be used for sequence-to-sequence mapping with the CTC model. [28] investigates modern quaternion-valued models such as convolutional and recurrent quaternion neural networks in the context of speech recognition. Recently, there has been some work introducing the Quaternion space into graph representation learning to obtain more expressive graph-level representation [19]–[21]. For example, [19] generalizes graph neural networks within the Quaternion space for graph classiﬁcation, node classiﬁcation, and text classiﬁcation. [20], [21] introduce more expressive quaternion representations to model entities and relations for knowledge graph embeddings for knowledge graph completion. Collaborative Filtering (CF) based models [5]–[9] have shown great performance in learning user and item representations. Matrix factorization [10] and Neural collaborative ﬁltering model [6] are widely used CF models, which embed users and items into the latent space. Some methods consider a user’s historical interactions as his or her feature, such as FISM [40] and SVD++ [41] which lead to a better user representation. In addition, side information is leveraged to further improve the recommendation quality like image [42], review [43] and knowledge graph [44]–[47]. Recently, attention mechanisms have been widely introduced to recommendation models, such as ACF [48] and NAIS [49], in order to capture the different contributions of a user’s historical interactions, improving their performance a lot. Another research line exploits the user-item interaction graph for recommendation. Prior efforts like ItemRank [50], adopt label propagation on the graph and encourage connected nodes to have similar labels. HOP-Rec [36] ﬁrstly performs random walks to augment a user’s interactions. The powerful performance of HOP-Rec over MF suggests that exploiting the connectivity information leads to better user and item representations. However, HOP-Rec relies on random walks and is unable to explore the high-order connectivity between users and items, leading to careful tuning efforts indispensable. Recently, GCN-based recommendation models have surged to learn better user and item representations in user-item bipartite graphs. For example, PinSage [11] combines random walk and graph convolutions to learn the embeddings of nodes. GC-MC explores the ﬁrst-order connectivity between users and items by utilizing only one convolution layer over the user-item bipartite graph. NGCF [12] leverages the messagepassing mechanism to obtain high-order connectivity and collaborative signal in the user-item integration graph. LightGCN [13] removes two components, feature transformation and nonlinear activation in NGCF [12], leading to improvement in training efﬁciency and generation ability. We move a step further on this research line. Despite the great success of existing GCN-based recommendation models, GCN is still vulnerable to noisy and incomplete graphs, which are common in real-world scenarios, due to its recursive message propagation mechanism [14]–[16]. However, some latest GCN-based recommendation models (e.g. LightGCN [13]) remove the feature transformation during message propagation, making it unable to effectively capture the graph structural features and become more sensitive to noisy or missing information. Moreover, they model users and items in the Euclidean space, which has been demonstrated to have high distortion when modeling complex graphs [17], [18], further degrading the capability to capture the graph structural features and leading to sub-optimal performance. Therefore, we move beyond the Euclidean space and fully utilize the Quaternion space, a hyper-complex space, to learn better user and item representations and feature transformation and thus improve both performance and robustness. In this work, we argued the limitation of the unreasonable operation of removing the feature transformation and modeling users and items in the Euclidean space and performed empirical studies to justify this argument. We moved beyond the Euclidean space, fully utilized the Quaternion space, a hyper-complex space, and proposed a simple yet effective Quaternion-based Graph Convolution Network model formed by a Quaternion Embedding Layer, Quaternion Embedding Propagation Layers, and a Prediction Layer. Speciﬁcally, we ﬁrst embedded all users and items into the Quaternion space with quaternion embeddings. Then, we introduced the quaternion embedding propagation layers with quaternion feature transformation to perform message propagation for aggregating more useful information. Finally, we combined the embeddings generated at each layer with the mean pooling strategy to obtain the ﬁnal embeddings for recommendation. Extensive experiments on three public benchmark datasets were conducted to evaluate the effectiveness of our proposed model. Results showed that our model outperforms the stateof-the-art methods by a large margin. This indicates that it can better learn user and item representations. Besides, further robustness analysis demonstrated that our QGCN model is more robust to noisy and incomplete graphs and can effectively capture the graph structural features. Moreover, speciﬁc performance comparison showed that our QGCN model gains huge performance improvement on sparse graphs, which is of great signiﬁcance to the practical applications and real recommendation scenarios. This work represents an attempt to explore the Quaternion space to model users and items and the effectiveness of quaternion transformation in the Quaternion-based GCN collaborative ﬁltering methods. We believe the insights in this study are enlightening for introducing the Quaternion space into other recommendation scenarios and digging into the nature and effectiveness of quaternion transformation. This research was partially supported by NSFC (No. 61876117, 61876217, 61872258, 61728205), Exploratory Self-selected Project of the State Key Laboratory of Software Development Environment, and Priority Academic Program Development of Jiangsu Higher Education Institutions.