From the start of the commercialization of the internet in the late 198 0s, the question was raised of how this technology could be leveraged in employee recruitment to enhance job see ker - vacancy matching. Even befo re the start of the world wide web, Vega [116] alre ady propos e d a system to match job seekers and jo bs , which could “be consulted by Minitel, using telephone number 3615 and selecting the LM/EMPLOI service”. I.e., the service allowed job seekers to send text messages in the form of search queries or their digital resume, over the telephone line, using a computer terminal called Minitel query/resume to a knowledge base, which used a ﬁxed job taxonomy to return a set of potentially interesting vacancies for the job seeker. Although more than 3 0 years have passed since this early contribution, the usage of a ﬁxed job taxonomy to extract information from a resume including “branch of industr y” (industry) and “qualiﬁcation” (skill) using “(a) dictionary specialized in the universe of employment”, seems vaguely similar to LinkedIn’s query processing method [75], which can be queried using your mobile phone This paper provides a review of the job recommender system (JRS) literature pub lished in the past decade (2011-2021). Compared to previous literature reviews, we put more emphasis on contributions that incorporate the temporal and reciprocal nature of job recommendations. Previous studies on JRS suggest that taking such views into account in the design of the JRS can lead to improved model performance. Also, it may lead to a more uniform distribution of candidates over a set of similar jobs. We also consider the literature from the perspective of algorithm fairness. Here we ﬁnd that this is rarely discussed in the literature, and if it is discussed, many authors wrongly assume that removing the discriminatory feature would be suﬃcient. With respect to the type of models used in JRS, authors frequently label their method as ‘hybrid’. Unfortunately, they thereby obscure what these methods entail. Using existing recommender taxonomies, we split this large class of hybrids into subcategories th at are easier to analyse. We further ﬁnd that data availability, and in particular the availability of click data, has a large impact on the choice of method and validation. Last, although th e generalizability of JRS across diﬀerent datasets is infrequently considered, results suggest that error scores may vary across these datasets. view on reality: Vega’s 200 simultaneous Minitel connections could not have served the curr e ntly c lose to 750 million LinkedIn users worldwide [78]. Nonetheless, the problem of recommending the right job to job seekers remains as pressing as it was more than 30 years ago. In this paper , we will provide an overview of the literature on job reco mmender systems (JRS) from the past decade (20 11-2021). We will consider the diﬀerent methods used in these systems, and consider these from a reciproc al, temporal, and ethical perspective. Furthermore, we will consider the inﬂuence o f data availability on the choice of method and validation, and put extra emphasis on branching the large class of hybr id recommender systems used in this application domain. Our results sugg e st that JRS could beneﬁt from a more application-oriented view: the reciprocal and temporal nature of JRS are infrequently discussed in the literature, while contributions that do consider these show considera ble beneﬁts. Furthermore, fairness is rarely considered in job recommender systems, and if it is considered, authors too often conclude that removing discriminatory features from the data is suﬃcient. However, recent scientiﬁc attention on fairness, in particular in candidate search engines, has introduced various metrics and algorithms, which we believe have the opportunity to be applied in the job recommender domain as well. In accorda nce with recommender system literature, deep language models have also bee n more frequently applied in the domain of job recommender systems. What remains somewhat unknown, is how well results on JRS ge neralize across datasets: the only study that considers this question shows that error metrics my vary g reatly over diﬀerent datasets. This paper has the following structure. Section 2 discuss e s some earlier literature surveys, thereby providing an ar gument fo r our inter e st in writing this review. Also, it discusses the method us e d for obtaining and selecting literature, and provides a brief discussion on some datas e ts and terminology we will frequently mention in this paper. Section 3 discusses our ﬁndings, in which Section 3.1 classiﬁes the job literature into the familiar recommender system taxonomy, while at the same time branching the large class of hybrid contributions. The remainder of Section 3 considers auxiliary topics s uch as the inﬂuence of data science competitions (Section 3.2), validation of JRSs (Section 3 .3), JRSs taking a temporal and/or reciprocal view (Section 3.4), ethical considera tions in JRS (Section 3.5), and considers contributions discussing job sear ch and recommendation at LinkedIn (Section 3.6). Last, Section 4 draws a conclus ion and discusses directions for further research. Before we discus s the literature, it is useful to observe that in recent surveys on applications of recommender systems, job recommender systems and (more general) recommender systems in e-recruitment, are frequently not included. I.e., in the well-cited review on applications of r ecommender systems, Lu et al. [81] do not mention the application area of e-recruitment, the same holds fo r the earlier review by Felfernig et al. [41]. Also, although most papers on neural networks in job recommender systems were published after 2018, the survey on (deep) neural networks in recommender systems (including a section on application areas) also neglects this application [11]. From the HR perspe c tive, job search and recommendation are also not always mentioned as an application area, as opposed to candidate selection, while in the end these systems do determine who will be in the applicant pool in the ﬁrst place [111]. One pos sible explanation could be that, from a technical p e rspective, the pro blem of job search and job recommendation is little diﬀerent from a general information retrieval/recommendation task. Job se e kers frequently use ‘genera l-purp ose’ search engines and online social networks to search for jobs (e.g., [56, 32, 66]). Furthermore, many job recommender systems we will discuss in this paper could very well b e used in other application areas (and vice vers a). Nonetheless, we will arg ue that factors such as the large amount of textual data , the reciprocal and tempor al nature of vacancies, and the fact that these systems deal with personal data does require a tailored approach, and the sheer volume of contributions make it clear that this application area should not be neglected. Previous surveys on job recommender systems, which consider JRS contributions before 2012, include AlOtaibi and Ykhlef [5] and Siting e t al. [110], though especially the latter survey is very limited in scope. More recent is the survey on recommender systems in e- recruitment by Freire and de Castro [43]. Although our work has some overlap, we especially wish to address some of the limitations of the work by Freire and de Castro in this paper. Even though the work by Fr e ire succe e ds in collecting a substantial number of contributions in the JRS application domain, they seem to fail to properly classify these contributions, making it diﬃcult to see patterns in this literature. A clear example of this is that approximately 20% of the contributions discusse d in their paper is labeled as hybrid, whereas another 33% is being labeled as “other”. Although the rea der would later ﬁnd that the “other" category includes for 25 % contributions using (deep) neural networks, this still leaves a large number of contributions with an unsatisfying label. Furthermore, as shown by Batmaz et al. [11], there is a considerable develo pment within the class of (deep) neural networks applied to recommender systems, which we also ﬁnd in job recommender systems. This aspe ct is neglected by Freire and de Castro. The classiﬁcation given by Freire and de Castro is understandable, given that so many contributions use mixtures of c ollaborative ﬁltering and content-based techniques, and given that these are presented by the contributions themselves as hybrids. However, these labels do not provide much insight into what these contributions a c tua lly entail. Furthermore, Freir e and de Castro [43] fo c us solely on methods and validation, whereas we, among other subjects, will als o take into consideration ethical c onsiderations. We will also put sp ecial emphasis on job recommender systems which, often successfully, take into account the reciprocal and temporal nature of jo b recommendations. As we will discuss in Section 3.2, many contributions use one of the data sources made available thr ough data science competitions for training and validating job recommender systems. Most noticeably, these include the RecSys 2016 and RecSys 2017 competitions ([1], and [2] respectively), using a dataset from the job board Xing[121], and the CareerBuilder 2012 Job Recommendation Challenge [60], which was hosted by CareerBuilder on Kaggle[21, 61]. All three datasets contain data with respect to candidate proﬁles, vacancies and online interaction between the two. Another resource often used is the Occupation Information Network (O*NET)[93], an English-based job ontology that is frequently used in knowledge-based job recommender systems (see Section 3 .1.4). We will use the terms vacancy, job posting, and job somewhat interchangeably throug hout this paper to represent the item in the classical recommender system setting, whereas job seekers are considered as users. Although, as in the early paper by Vega[116], job seekers are still often described by their resumes, some current e-recruitment systems allow for descriptions that move beyond self-descriptions of one’s professional self. Here one should think of the social connections one can observe on (professional) social networks. When we speak of resumes, CVs, user proﬁles, or job seeker proﬁles, we assume these are synonyms and may contain additional information (such as social relations) beyond the s e lf-description. L ast, we will sometimes speak of “textbook" or “oﬀ-the-shelf", by which we mean methods one can ﬁnd in popular machine learning/pattern recognition textbooks such as by Bishop [16] and Flach [42], or Aggarwal [3] in the case of commo nly used recommender systems. To obtain the set of contributions we will discuss in this paper, the following method was employed. English contributions published between January 1st 2011 and January 1st 2021, published as a n article in a scientiﬁc journal or in conference proceedings were se arched using Google Scholar , using key phrases job recommender systems, job recommendation, and job matching, where additiona lly we replaced “job" by “occupation" in these phrases. Furthermore, a for ward and backward search was applied, which after reviewing the titles and abstracts resulted in 192 contributions . Next, contributions on career trajectory recommendation were omitted, that is, recommender systems that only recommend a job type (such as “data scientist"), but not a speciﬁc occupation at a given orga nization at a speciﬁc time. Furthermore, also papers on employee selection, recommender systems sp e c iﬁcally for freelancers, and candidate recommender systems (i.e., recommending candidates for open vacancies) were ﬁltered out. We further re moved contributions without (some form of) e mpirical validation of their recommender system. The resulting set contained 87 contributions, which we will discuss in this paper. As discussed in Section 2.1, although the work by Freire and de Castro [43] is considerable, our critique lies in how they classify the diﬀerent job recommender systems. As is common in the literature (e.g., [3]), these are split into Content-Based Recommender Systems (CBR), Collaborative Filtering (CF), Knowledgebased Recommender Systems (KB), and Hybrid Recommender Systems (HRS). However , given that so many contributions are hybrid, we will further make a distinction between monolithic and en s emble hybrid recommender systems [3, p. 200] by [20]. The interpretation o f monolithic and ensemble hybrids, and the diﬀerent ensemble hybrids, will be discussed in Section 3.1.3. Content-based recommender systems (CBRs) in the context of JRS are models which, to construct a recommendation, only use a semantic similarity measure between the user pro ﬁle and the set of available vacancies. I.e., the semantic similarity is used as a proxy for estimating the relevance of each vacancy to the job seeker. In CBRs, one cre ates vector representations of the vacancy and user proﬁle in an unsup e rvised way, i.e., the dimensions of these representations may not have an intuitive inter pretation. Many authors use Bag of Words (BoW) with T F-IDF weighting [90, 63, 28, 37, 2 7], though also Latent Dirichlet Alloca tion is used [9], and the more recent word2vec [45, 115, 57]. Interestingly, CBR contributions have been relatively stable in the past 10 years, but also, they were not part of the top contributions during the 201 6 and 2017 RecSys competitions (see Table 1). Perhaps the main challenge in content-based JRS is that, in t he words of Schmitt et al. [106], “Job seekers and recruiters do not [always] speak the same language”. I.e., job seekers and recruiters may use a diﬀerent terminology to describe jobs, knowledge, o r skills. As a result, two entities having the same meaning may end up in diﬀerent vector representations when described by the recr uiter or by the jo b seeker. Unfor tunately, this discrepancy is often neglected in content-base d JRS. In collaborative ﬁltering (CF), recommendations are ba sed solely on behavioral da ta, typically stored in a user × items rating matrix. In the e-recruitment setting, like in the e-commerce setting, this matrix is usually ﬁlled with click behavior (e.g., the rating matrix equals 1 if the job seeker (row) clicked on a vacancy (column) to see the vacancy details, 0 otherwise). Though, in case such interaction data is missing, also the sequence of previous job occupations can be used to ﬁll the r ating matrix (e.g., [72]). The latter case does require jobs to be categorize d, such that they can serve as discrete items in the rating matrix. For simplicity, we will refer to the entries in the rating matrix as ratings, irrespective of the ex act type of behavioral feedback. The literatur e commonly distinguis hes between two types of CFs: memory-based CF and model-base d CF [3]. In memory-based CF, recommendations are created using a K-nearest neighbor (KNN) type of approach. That is, for some user u, one tries to ﬁnd either K users similar to u (user-based CF), or K items similar to the items u has already ra nked (item-based CF). Here, the similarity is always based on the entries of the rating matrix. Contributions using textbook CF methods include Lee et al. [72], Reusens et al. [102], Ahmed et al. [4], Liu et al. [79], where the latter two applied it to the RecSys 2016 dataset. Lacic et al. [68] co mpare several auto-encoders to encode user interactions, based on which similar use rs ar e determined. Model-based CF attempts to ﬁll the missing values in the rating matrix using regression-based models, based solely on the rating matrix. However, likely due to the large amount of textual data, which c an ideally function as features in such regression models, we are not aware of any studies using model-based CF in job recommender systems. On the other hand, although we classiﬁed regression models using textual data as hybr id, it should be noted that these models have a strong CF ﬂavor. Even though textual features are used, their weights in the regression model are determined by be havioral feedback. Hence, even though not only behavioral data is used, still the resulting recommendation is largely determined by what vacancies similar job seekers have intera cted with. O nly the deﬁnition of ‘similar’ has changed, as it now partly includes features based on co ntent. As discussed earlier , hybrid recommender systems combine several models into one recommender system. Our aim here is to split this group into smaller, more homo geneous methods. To do so, we follow Aggarwal [3, pp. 199-20 4], who split the hybr id recommender systems into those having a monolithic, or ensemble design. A monolithic design refers to hybrid recommender systems in which it is not possible to extract one component of the hybrid, and build reco mmendations ba sed o n this component independent of the other components in the hybr id, without altering the algorithm. On the contrary, e nsemble metho ds do allow splitting the hybrid recommender system into at least two c omponents that c an (independently) be used for recommendation. I.e., the ensemble may consist of some models which by itself may be monolithic. Model-based methods on shallow embeddi ngs In the cla ss of monolithic des igns, we again consider two classes . We will refer to the ﬁrst class as model-based methods on shallow embeddings (MM-SE). These approaches commonly use oﬀ-the-shelf machine learning methods such as support vector machines [98], naive Bayes [9 5], or gradient boosting [126, 122], though also co nvolutional neural networks (CNNs) [85] are used. What uniﬁes these contributions, is that the textual data is mapped to a vector space using linear or small order transformations. Although we w ill leave the deﬁnition of shallow or small order somewhat vague on purp ose, these embeddings do have in common that this order is explicit in the method, whereas in deep embeddings this could be arbitrarily large. These shallow embeddings include TF-IDF weighted document representations [122], r e presentations based on a predeﬁned job classiﬁcation [126, 95], word2vec [85], and a probabilistic stacked denoising auto- e ncoder [25]. Also gr aphical models are included in the MM-SE class of monolithic hybrids. Dave et al. [29], map jobs and skills to two matrices sharing the same latent space. Although the mapping is trained independently from O*NET, the re sulting vector representations showed considerable resemblance to the O*NET ontology. Jiang et al. [59], extra c t a large number of features from their dataset of the vacancies, job seeker proﬁles, and their interactions, to train a graphical model in which clicks are predicted based on estimates of whether a (job seeker, vacancy) pair is of some latent type z. They also make use of O*NET and use bin-counting (i.e., [131, Ch. 5]) to obtain a numeric representation of some categorical features. Although the model provides an explicit click probability, as opposed to only predicting the appropriate ranking of items, the paper does mainly c ompare its approach w ith learning to r ank models such as AdaRank and RankB oost, where the model is found to be favorable. Three contributions model the job recommendatio n problem with a somewhat diﬀere nt objective function, though, we still label these as MM-SE. Dong et al. [38] and subsequent work [26] propose an MM-SE monolithic hybrid. Contrary to the approaches discusse d so far, they consider the problem as a reinforcement learning problem. Given a successful or failed recommendation, a reward function is updated, taking as input the representation of a job seeker and a vacancy. To speed up computations, vacancies are represented in a binary tree. Wang et al. [118] not only consider what jo b should be recommended, but also when this recommendation should be made, based on estimations of when a job seeker will switch jobs. Deep neural networks Apart from shallow embeddings, also deep representations have become common strategies, accounting for approximately 50% of all contributions in 2019 and 2020 (see Table 1). Since this class consists of only deep neural networks (DNNs), we will also refer to this class by DNN. Many studies follow a similar approach as language models such as BERT [33] or ELMo [9 7], and extend on these embeddings by adding additional hierarchical a ttention networks [91, 100]. Also embeddings based on CNNs are common [7 0], or mixtures of the above appr oaches [84, 13, 14, 58, 125]. Siamese neural networks have also been used for this purpose [107]. The increasing usage of DNNs in recommender systems is not limited to recommending jobs, but holds for recommender systems in general [11]. Although it is too soon to draw ﬁrm conclusions, the absence of MM-SE contributions in the last two years is somewhat striking , giving that these were quite common in 2017 and 2018. Furthermore, some contributions did benchmark their proposed DNN with models that we may label MM-SE, e.g., using word2vec or BoW to obtain vector representations, and found the DNN to be favorable [100, 91, 132]. Also, in some DNN contributions, the neural network on top of deep job/user/session embeddings is compared to other commonly used machine learning algorithms, such as gradient boosted decision trees (GBDT), with the latter using the same job/user/session embedding as the proposed DNN. In such comparisons, DNNs are also found to outperform other machine learning models on the same embeddings [84, 5 8]. To what extend this argument is completely satisfying remains somewhat unknown. If we view the architecture on top of initial job/user/sess ion embeddings as automated feature engineering, then comparing these models with models without additional feature engineering is perhaps not completely fair. Moving away from the application of job recommender s ystems, DNNs also have not always been outperforming gradient boosting or ens emble techniques in recent data science competitions, in which participants put more eﬀort on feature engineering for these boosting/ensemble models [55]. Ensemble hybrids Following Burke [20], we split ensemble hybrids into four classes. Cascade hybrids reﬁne the recommendation given by another recommender system. These commonly include (gradient) boosting models [89, 76, 94, 119, 120, 117, 105], where in particula r XGBoo st[24] is popular. It should be noted that all these contributions were competitors in either the 20 16 or 2017 RecSys competitions, with [120] and [117] being the winning contributions to the 2016 and 2017 competitions respectively. The problem has also been a ddressed from a learning to rank perspective, using LambdaMART [54]. Besides boosting, also reﬁnements using common CBR or CF methods have been propo sed [30, 124, 82] Gui et al. [46] propose the so-called beneﬁt-cost model, a re-ranking procedure for recommender systems that takes into account downside in recommender s ystems, and which is validated on a job recommender problem. To our knowledge, this contribution is the only contribution that ra ises the q uestion of whether in some scenarios a recommendation should be given at all. Ill ﬁtted recommendations could hurt the trus t job se e kers have in the rec ommender system, making them decide not to use it [69]. Borisyuk et al. [17] reﬁne an earlier job recommendation by predicting the numbe r of applications to each vacancy, and pushing less popular vacancies to the top of the recommendation list. The idea b e hind this strategy is that, contrary to the e-commerce recommender setting, it can be hurtful to have too many applicants on a single vacancy. Recruiters will have to spend at least some time o n each application to evaluate the ﬁt and communicate the result of this evaluatio n to the job seeker (e.g., c ommunicate a rejection or invite the job seeker for an interview). Furthermore, Borisyuk et al. [17] also ﬁnd that pushing less po pular vacancies does incre ase the number of applications to those vacancies, hence causing a better spread of the applicants over the vacancy po rtfolio, though this is conditioned on whether the less popular vacancies are suﬃciently relevant. Similar to cascade hybrids is the feature augmentation class, in which case the result of the previous recommender system in the sequence is not so much reﬁned, but simply used as an input for the next model. The somewhat similar approaches by Diaby et al. [35, 36], Diaby and Viennet [34], and Malherbe et al. [86], Faliagka et al. [40], Gupta and Garg [49], Guo et al. [47] use such an approach, in all case s applying oﬀ-the-shelf machine learning methods on top of an initial CBR. In weighted hybrids, the output of the separate models is combined using some (pos sibly non-linear) combination of the predicted scores. Commonly, CBR and model-based CF are in this way combined [99, 73, 74, 114, 52, 22, 15]. Others combine a large number of textual and behavioral feature s [133, 130, 19], which we consider an ensemble hybrid as some of these features may be used for job recommendation by themselves. The output of several oﬀ-the-shelf machine learning methods has also been used for this purpose [77]. A common problem in CF is the cold-start problem: new users/items have not given/received any behavioral feedback yet, making CF-based recommenda tions diﬃcult. Although multiple hybrid approaches can be used to resolve this problem, perhaps the most dire ct approach is to use switching hybrids. In JRS, this mos t often implies that the recommender system uses CF by default. However, if an item or a user has insuﬃcient data, the recommender s ystem switches to CBR. Contributions using such approach with oﬀ-the-shelf CF and CBR methods include [129], and [106]. Although Aggarwal [3] deﬁne knowledge-based recommender systems as recommender systems having the conceptual goal to “Give me recommendations based on my explicit speciﬁcations of the kind of content (attributes) I want”, we will rather use the deﬁnition given by Freire and de Castro [43], who deﬁne it as “[Recommender systems] which rely on deep knowledge about the product domain (job) to ﬁgure out the best items (job vacancies) to recommend to a user (candidate)". In job recommender systems, this often implies that both job and candidate proﬁles are mapped to some predeﬁned job ontology (i.e., the knowledge base), after which the two are matched. A common strategy to generate job recommendations is then to compute the similarity between the c andidate proﬁle and vacancy in the ontology space [104, 64, 88, 65, 127, 128, 48, 109, 6, 83], where the overlap can be computed by for example the Jaccard index ([48]). Although one can imag ine that the construction of such ontologies can take considerable eﬀort, they have been used successfully in practice by, for example, LinkedIn [75] or Textkernel [113]. Nudging users to complete their pro ﬁle by recommending skills from the ontology has also been shown to be a successful strategy to improve such recommendations [10]. Mapping jobs and candidate pro ﬁles to a shared ontology also provides a solution to the discrepancy between how job seekers and recruiters deﬁne jobs without requiring behavioral feedba ck. An advantage of using job ontologies is that this also simpliﬁes the implementation of keyword-based search engines and simpliﬁes ﬁltering. Gutiérrez et al. [50] consider such an approach, in which an interactive recommender system was built on top of the knowledge-based recommender ELISE [39], where users are able to ﬁlter jo bs based on travel distance or the type of contract. The recommender sys tem also explains why the recommendation is (not) given, e.g., by indicating that a required skill is not mastered by the job seeker. Participants in the study indicated that the tool allowed (citing the authors) “greater autonomy, a better understanding of needed competencies and potential location- based job mobility”. Reusens et al. [103] compare several CF approaches with keyword search in terms of recall and reciprocity, both for job recommendation and job seeker recommendatio n. With respect to job recommendations, although traditional CF a pproaches performed acceptably in terms of recall, they were inferior to keyword search in terms of reciprocity. The authors did ﬁnd that using a ‘reversed’ rating matrix for job recommendation (that is, building a rating matrix on whether recruiters saved job seeker proﬁles for speciﬁc vacancies) improved reciprocity even beyond keyword search, though with a trade-oﬀ in terms of recall. Competitions have played an important ro le in the job recommendation literature. These competitions are settings in which organizations share a dataset, an objective, and an error measure with respect to this objective, typically via a competition platform such as Kaggle[61]. Teams can enroll in these compe titions, and in the case of job recommender competitions, the goal for each team is to construct a job recommendation for a set of hold-out users . As already hinted by Table 1, to our knowledge three comp etitions have had a considerable impact on the job recommender literature: 1) The 2012 Careerbuilder Kaggle competition [60], 2) the RecSys 2016 competition [1], and 3) the RecSys 2017 competition [2]. The latter two both used a dataset from the German job board Xing [121]. Besides being used for contributions to the competitions themselves, the datasets are also commonly used after completion of the competition, e.g., to tr ain and validate job recommender systems w hen no da taset is available. Given that approximately 32% of all job recommender sy stem contributions use a dataset originating fro m one of these contributions, these datasets have had a considerable inﬂuence on the job recommender literature. Although many models proposed for the RecSys 20 16 and 2017 competitions were cascade or weighted hybrids (and in particular gradient boosted trees were quite successful [120, 117]), there is no free lunch: all contributions show that a considerable time and eﬀort was spent on constr ucting useful features to represent job seekers, jobs, and their interaction. We will emphasize two mor e observations with respect to competitions. First, the 2012 Career Builder dataset contains user interactions, hence could be used to evaluate a collaborative o r hybrid recommender system. However, it is frequently used to evaluate content-based recommender systems (see Table 1). Second, to our knowledge, only Lacic et al. [68] compare their proposed recommender system over multiple datasets: the Careerbuilder dataset, the 2017 RecSys dataset, and a private dataset originating from the student job portal Studo Jobs[112]. What is striking, is that the performance of the diﬀerent models diﬀers considerably across datasets, with no unanimous agreement acro ss data sets and across error measures which model should be preferred. I.e., the results are rather dataset dependent. However, the authors do ﬁnd that from the set of models they consider, using variational auto- e ncoders to embed user sessions g ave overall the best performance. Validation when lacking job seeker - vacancy interactio n Many authors state in their introduction that there is a vast amount of online data available, commonly followed by a reference to the current LinkedIn user count [78]. Although this may be true in terms of vacancies and job seeker proﬁles, researchers do not always have access to inter actions between the two. Naturally, this also impacts the type of methods and validation that is used. In case inter action data is lacking, authors propose se veral str ategies to still be able to evaluate their recommendations. One o f these is to use one of the competition datasets fo r validation and training. As already discussed in Section 3.2, approximately 32% of all contributions use one of the competition datasets, but from Table 1, we also ﬁnd that these were used after the competitions had ﬁnished. Expert validation is also used frequently for validation. During s uch expert validation, the quality of recommendations is inquired by a group of ‘experts’, which may be the researchers themselves, HR/recruitment experts, or sometimes students (e.g., [85]). Although the choice for expert validation is rarely discussed, we do ﬁnd that for CBR and KB job recommender systems, approximately half of the contributions use expert validation [63, 6 4, 27, 65, 9, 127, 128, 57, 115, 88, 10 9, 45]. Another way to obtain behavioral feedback when interaction data is lacking, is by using previous N jobs in the job detail section of a resume to predict the N + 1-th job. This does come with the challenge of rightfully deﬁning a job. As mentioned by [31], job seekers may have diﬀerent interpretations of a “job". That is, if the deﬁnition is too sp e c iﬁc, each job would have too few observations for inference. Wher e as if the deﬁnition is too broad, the recommendation may be accurate, but not precise, and therefore not useful. Furthermor e , many details about the job may be missing from the job history, and job seekers tend to indica te their jobs at diﬀerent levels of granularity [31] (e.g., are multiple related p ositions at the same ﬁrm one or multiple “jobs"?). A somewhat interesting observation with respect to the best performing contributions during the RecSys 2016 and 2017 competitions is the fact that none of the best performing contributions were CBRs. This may be because interaction data was given, hence, why not use it? However, also in other hybrid recommender systems, incorporating behavioral feedback is found to improve the recommender system, even if one has access to a well-deﬁned and validated job ontology [75]. Choices in negative sampling In case interaction data is used for validation, one mostly only observes those jobs that the job seeker has positively interacted with. I.e., the jo b recommender system problem is most often modeled as an unary classiﬁcation problem, and therefore training requires deﬁning a set of negative samples. We consider two types of behavioral feedback, which commonly lead to somewhat diﬀerent approaches in negative sampling. We consider using previously held jobs in the resume as positives as Type 1 behavioral feedback, w hereas Type 2 behavioral feedback is obtained from observed online job seeker vacancy interaction. For Type 2 behavioral feedback, common stra tegies for deﬁning negatives include using shown but skipp e d items [54, 59, 70, 79, 89, 91, 94, 99, 101, 120, 126, 130, 133, 47], picking negative samples at random (not per se uniform) [13, 124, 125, 73, 29], replacing the job (but not the candidate and further context) at random [132], using vacancies of which the vacancy details were shown, but did not lead to an application [101, 100], or if the method allows for sparse matrices (such as in some matrix factorization methods): using all possible vacancy-user interactions [74, 98, 71, 68, 19, 102, 4, 72, 80, 22, 15, 83, 105]. Others incorporate negative sampling into the estimation method itself [76, 14]. Some datasets include even more user actions besides those of Type 2, such as deletion of stored vacancies from a proﬁle [119, 117], or whether the candidate was hired/rejected [5 8], which can be use d to deﬁne positive/negative instances. In case the behavioral data is of Type 1, also all possible pa i rwise job seeker-vacancy interactions are used. As discussed in Section 3.3, this is somewhat less memory demanding as jobs, in this case, are deﬁned at a lower level of granularity [95, 77, 52]. In Borisyuk et al. [17], the authors note on interaction with vacancies posted at LinkedIn that “Since users like to apply for new jobs, our [previous] job recommendation system tends to always show new jobs more often than old jobs”, a r e lationship that is also found by [67]. Furthermore, wha t is interesting about the winning RecSys 201 6 contribution is that the model explicitly takes into account the recency of items, something which contributions with similar methods (i.e., using XGBoost) did not. Whether the claim by Borisyuk et al. (users like to apply to new jobs ) holds the question. The relationship could also be reversed: recent vacancies are shown more on top of the recommendation list, and therefore more likely to be clicked. Furthermore, given that vacancies with high demand will ﬁnd a candidate faster, there is also poss ibly a survival bias: vacancies with longer le ad times are likely to be vacancies with low demand, as those w ith high demand were already removed from the platform (i.e., as they found a suitable candidate). Nonetheless, irrespectively of the causal chain, we would argue that the vacancy lead time should be taken more into account in job recommender systems, whereas we ﬁnd that in most literature this aspect is currently neglected. In one early contribution of job recommender systems [8 7], the authors represent the job recommendation problem as a reciprocal recommendation problem. They state that “Theory shows that a good match between persons and jobs needs to consider both, the preferences of the recruiter and the preferences of the candidate”. Although some contributions do take this reciprocal nature into account, and usually with success (e.g., [17, 103, 70]), most contributions consider the one-dimensional job seeker perspective, neglecting possible harm to the employer and/or job portal. For example, the long tail pr oblem in recommender systems [3, p. 33] also plays a role in job recommender systems and job search engines (e.g., [32]), which in extreme cases may lead to receiving over 1500 applications per vacancy, as was the case for oﬃce jobs a t the United Nations oﬃce in New York [18, p. 62]. However, the probability of hiring a c andidate seems to be concave increasing in the size of the applicant pool [17]. I.e., growing the size of the applicant pool is likely to lead to diminishing returns. In case there is a large applicant pool for a single vacancy, this not o nly implies that the recruiter will have to sift through more resumes, but also has to reject more (possibly also well-suited) candidates, which may damage the employer brand. Furthermore, as also shown by Borisyuk et al. [17], job seekers do apply to less popular but still relevant vacancies, if these are pushed to the top of the recommendation list. In their literature review on applied machine learning in human resource management, Strohmeier and Piazza [111] state that “Ethical and legal aspects are not broadly considered in current research, only equality of treatment and protection of privacy issues are discussed". Furthermor e : “A few of the research contributions address equality of treatment [...], mostly by ex clu ding potential discriminatory features from mining". Unfortunately, in job recommender literature, equality of tr e atment is also rarely considered, a nd if it is considered, the authors indeed come to the conclusion that simply excluding the dis c riminatory feature would be suﬃcient (e.g., [100]). Excluding discriminatory features may s e em logical from the perspective of preserving pr ivacy, or when trying to eliminate human bias, but at the same time this may a c tua lly hurt equality of treatment. To check whether the algorithm is discriminating against a certain group, one would require these discriminatory features. Following the argument made by Perez [ 96, Ch. 3-6] perhaps caus e d by the fact tha t the impact of HR/recruitment policies, and as we argue here, the choice for implementing a certain job recommender system, are no t tested against speciﬁc groups. As a result, the chosen policy is often (in Perez’ argument male-) biased. One can easily ﬁnd examples of how sensible features extract ed from a r e sume or vacancy may lead to inequality of treatment, which is also referred to in the literature as fairness. Trivial ones include work experience (age-biased), or ﬁrst name (gender-biased), but many are also less trivial. These may be the type of words used in a vacancy text [12, Ch. 4], or examples from O’Neil’s well-known weapons of math destruction[92, Ch. 6], of how online (professional) social activity may be gender-biased, or how commuting time may be related to welfare (especially if the oﬃce is located in, say, the city center o f London). Hence, these should make us aware tha t it is not an option to as sume that if a (discriminatory) feature is not observed, it does not exist. The recent increased scientiﬁc attention on algorithm fa irness [8] will hopefully turn this tide, as online job portals are more likely to come under scrutiny by algorithm audits. Chen et al. [23] perfor m such an audit on the candida te search engines of Monster, Indeed, and CareerBuilder, testing these search engines on gender bias. Although all three search engines are found to not allow for direct discrimination on inappropriate demographics (e.g., by allowing to ﬁlter on gender), the researchers do ﬁnd indirect discrimination both in terms of individua l and group fairness. Geyik et al. [44], compare several algorithms for ensuring group fairness in candidate r e commender systems/s e arch engines, with a focus on mea sures for equal opportunity and demographic parity, one of which has been implemented in LinkedIn. Another concern is the usage of so-called “People aggregator s", in which researchers or people from industry use a web crawler to obtain a substantial number of professional or personal proﬁles. Although this is sometimes with the user’s consent (e.g., [35, 34, 40]), also in many cases this remains unclear. We did not ﬁnd any work in which user pro ﬁles are explicitly anonymized before processing. Although many job recommender systems have been proposed in the literature, and the internet holds a considerable number of websites wher e job seekers can search for jo bs, we do ﬁnd it relevant to put some emphasis on contributions published by LinkedIn employees for a number of reasons. Even though many JRS have been proposed in the literature, few of these dominate the (global) recruitment market [51]. Hence, we consider it likely that job seekers’ perception will be bias e d towards the JRS these larger players a re using. Furthermore, although many job seekers also use other well-known general-purpose sear ch engines/social networks, for which the large-scale argument also holds, LinkedIn includes algorithms sp eciﬁcally designed for establishing professional matches. Also, LinkedIn has been considerably transpare nt about the algorithms they use, and their respective performance, and is capable of testing these alg orithms online via their A/Btesting platform XLNT [123]. LinkedIn’s jo b recommender system/ search engine is largely describe d in [6 2, 75], respectively. To facilitate diﬀerent information needs, LinkedIn encompasses multiple sear ch indices, which ar e combined into one to facilitate feder ated search [7]. The job search engine itself is compos e d of a linear combination of several features, which can be grouped into four classes. First, a query-document se mantic matcher. Second, an estimator for the searcher’s skill with respect to the set of potential jobs. Third, an estimator for the quality of the document, independent of the query and/or searcher. And fourth, a collection of miscellaneous features such as textual, geographical and social features. LinkedIn’s job recommender system consists of a GLMix model, including a large number of proﬁle features from both the candidate, the job, their interac tions, and the cosine similarity between user and job features in the same job domain [130]. Additionally, the job rank ing is adjusted base d o n the expected number of applications for each job in such a manner that the job seekers are better spread over the set of potential jobs, without reducing the relevance of the high ranked jobs in the recommendation too much [17]. From considering the diﬀerent algorithms and their validation, a few observations can be made. Both the JRS and JSE quite heavily rely on the set of skills users indica te o n their proﬁle. The algorithms thereby beneﬁt from LinkedIn’s p olicy to nudge user s to complete their lis t of skills, or to provide endorsements to other users, partly by recommending skills to add to their proﬁle [10]. Also, o f all individual classes of features, the estimate of one’s expertise seemed to lead to the largest improvement in model performance. Given that endorsements from other users have a considerable inﬂuence on this estimate, the JRS/JSE s e ems to beneﬁt from LinkedIn being a social network. Although LinkedIn strives towards per sonalized results, the algorithms are required to be scalable to meet latency requirements [62], which leads to some algorithms being competitive to, but not outperform, other models [13 0]. In this paper , we have considered the job r ecommender system (JRS) literature fro m several perspectives. These include the inﬂuence of data science competitions, the eﬀect of data availability on the choice of method and validation, and ethical considerations in job r ecommender systems. Furthermore, we branched the large class of hybrid recommender systems to obtain a better view o n how these hybrid recommender systems diﬀer. Both this multi-perspective view, a nd the new taxonomy of hybr id job recommender systems has not been dis c ussed by previous reviews on job recommender systems. Application-oriented challenges in JRS were already highlighted in early JRS contributions, though, still most literature does no t take these into account. Contributions that do take diﬀerent views on the JRS problem, however, do show that such views can have considerable beneﬁts. These beneﬁts may include improved model performance (temporal perspective), improved distribution of candidates over a set of homogeneous vacancies (reciprocal perspective), or ensuring algorithm fa irness (ethical perspective). Curr ently, most attention goes out to how to represent the substantial amount of textual data from both candidate proﬁles and vacancie s to create job rec ommendations, for which recently especially deep representations have shown promising results. However, this focus may also create the illusion that this is the only perspective that is relevant. Especially in terms of fairness, such a single perspective can be considerably harmful. Although we are not aware of algorithm audits on job re c ommender systems, an audit on the candidate search engines of Indeed, Careerbuilder, and Mons ter, did show signiﬁcant results for both individual and group unfairness in terms of gender. The increased scientiﬁc attention towards algorithm fairness, however, does provide algorithms and metrics that can be a pplied to measure and ensure algorithm fairness. Hence, there is a research opportunity to study how these can be transfer red to the job re c ommender system domain. Many authors state in the introduction of their contribution that there is a vast amount of data available in the form of vacancies and job seeker proﬁles. However, there is a clear split in the literature with regards to contributions having also acces s to interaction data between these two, in particular in the form of clicks/skips on the recommendatio n list. Interaction data can resolve the language inconsistency between job se e kers and recruiters, which is especia lly troublesome in content-based and some knowledge-based JRS. In case interaction data is missing, one common resort is to use one of the available datase ts originating from JRS co mpetitions, in particular the CareerBuilder 2012, RecSys 2016, and RecSys 2017 competitions, which therefore have had a considerable inﬂuence on the JRS literature. An interesting aspect with respect to the usa ge of these competition datasets, beyond the contributions to the competitions themselves, is that these datasets are mostly used for training, but rarely for validation. This is unfortunate, a s the (to our knowledge) only contribution that compares JRS on diﬀerent competition datasets shows that error metrics may diﬀer substantially acro ss diﬀerent datasets. I.e., this raises q uestions with respect to the generalizability of JRS trained on one dataset. Another interesting question why (online) interaction data is sometimes not taken into account, or along the same line, why researchers often resort to the competition datasets, beyond the motives of contributing the the competition or for validation. Although there may be many valid reas ons, we would like to hypothesize from anecdotal experience that it can be diﬃcult to obtain such interaction da tasets, as recruitment organizations are not always part o f research communities, or given that these recruitment organizations have no t always considered the implications of sharing data for res e arch, either from a technical or legal point of view, making it diﬃcult to use such datasets on a short term. Although we believe to have given a broad overview of contributions on job recommender systems, we do like to address some limitations of this review. First, although we managed to further split JRS hybrids into smaller catego ries, still so me classes comprise similar methods. One particular example is that some methods, currently classiﬁed a s MM-SE, are quite similar to cascade hybrids. I.e., if a cascade hybrid would have used the s e mantic representation of jobs and job seekers as features in a boosting model, instead of using the similarity between the two as feature, it would have been classiﬁed as MM-SE. A similar argument holds for the feature augmentation and cascade hybrid classes. Second, to limit the scope of this literature review, we only considered literature on job recommender systems. However, such systems do not exist in isola tion: they are commonly part of an e-recruitment platform also comprising candidate recommendations, or even employee selectio n, which are spar sely included in this review.