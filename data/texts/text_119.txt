Understanding the world and the complex processes that take place in it has always been a challenge for human beings. Centuries and centuries of scientiﬁc research and experiments have increased our knowledge, allowing us to model some of these Screening traditionally refers to the problem of detecting active inputs in the computer model. In this paper, we develop methodology that applies to screening, but the main focus is on detecting active inputs not in the computer model itself but rather on the discrepancy function that is introduced to account for model inadequacy when linking the computer model with ﬁeld observations. We contend this is an important problem as it informs the modeler which are the inputs that are potentially being mishandled in the model, but also along which directions it may be less recommendable to use the model for prediction. The methodology is Bayesian and is inspired by the continuous spike and slab prior popularized by the literature on Bayesian variable selection. In our approach, and in contrast with previous proposals, a single MCMC sample from the full model allows us to compute the posterior probabilities of all the competing models, resulting in a methodology that is computationally very fast. The approach hinges on the ability to obtain posterior inclusion probabilities of the inputs, which are very intuitive and easy to interpret quantities, as the basis for selecting active inputs. For that reason, we name the methodology PIPS — posterior inclusion probability screening. Keywords: Screening, variable selection, computer model, Gaussian process, processes. Here, by modeling we mean that researchers have been able to put together mathematical and/or statistical formulas combining parameters and variables in order to explain the mechanics of a phenomenon of interest. as computer models, that is, scientiﬁc models that are used to understand and explore real phenomena. Often, because of their complexity, these models are implemented in computer programs. Only deterministic models are within the scope of the methodology that we propose. nomenon. Hence, an important issue lies in the validation of the model, a process that relies on the confrontation of the model with ﬁeld data representing possibly noisy measurements of the real phenomenon (Bayarri et al., 2007a). In this scenario, the sources of uncertainty are multiple (Kennedy and O’Hagan, 2001). To name but a few: noise on the ﬁeld measurements, unknown settings for some parameters included in the computer model or the inadequacy between the computer model and the reality. usually incorporated within the statistical framework that links the ﬁeld data with the computer model. Moreover, it is commonly modeled as a Gaussian Stochastic Process (GaSP) (Kennedy and O’Hagan, 2001; Higdon et al., 2004; Bayarri et al., 2007b), leading to additional diﬃculties and potential to confounding issues (Tuo and Wu, 2015) with the estimation of the unknown model parameters, known as the calibration task. Some proposals have been made to use prior information on the possible structure of the discrepancy (Brynjarsd´ottir and O’Hagan, 2014). Other approaches, such as Plumlee (2017) or Gu and Wang (2018) aim at separating the discrepancy from the computer model. se: the goal of including an element in the modeling of the data intended to account for the possible gap between the real phenomenon and the computer model is to obtain sensible calibration values and to improve prediction. Damblin et al. (2016) and Kamary et al. (2019) focus on determining whether a discrepancy function is needed or not in the statistical model. function which helps the modeler in understanding the ﬂaws of the computer model and in pinning down in which aspects improvement is needed. In particular, we adopt the classical framework mentioned above, modeling the discrepancy as a GaSP depending on, at least, the same input variables considered as part of the computer model. known as screening, to determine which are the inﬂuential input variables in this discrepancy function. At the end of the day, we would like to have labels for each of the input variables determining which are active and which are inert. Then, the In this work, we focus on the improvement of mathematical models, also known A computer model only reﬂects our (always limited) knowledge of the real phe- The last one, also known as the discrepancy or the bias of the computer model, is In most of the papers cited above, the main interest is not the discrepancy per Contrary to these contributions, our focus is explicitly on adding a discrepancy In this scenario, we aim to perform a Bayesian variable selection procedure, also group of inert inputs may be the ones correctly taken into account in the computer model, while the group of active inputs are the ones to be examined if one aims to improve the computer model. input variables and, in particular, to the idea of spike and slab prior distributions (Mitchell and Beauchamp, 1988) which are commonly used for selecting variables in linear regression models (Bai et al., 2020). In the speciﬁc case of the discrepancy function modeled as a GaSP, we use these prior distributions to model the range parameters responsible for the inﬂuence of input variables on the output of the computer model. We take advantage of the reparametrization of the GaSP correlation function proposed in Linkletter et al. (2006), which puts the range parameters in the compact interval [0, 1] with the value 1 corresponding to an inert input. the best of our knowledge, two main approaches have been used for performing variable selection within the GaSP: the spike and slab prior distribution, which only needs an MCMC sample from a single model, namely, the model that includes all the input variables. Then, a postprocessing of the posterior distribution allows us to obtain the posterior probability of each input variable being active. Outline of the paper In Section 2, we provide the background and establish the statistical model as well as the notation considered in the rest of the paper. Then, in Section 3, we describe the proposed methodology. Section 4 is devoted to simulated and synthetic examples used to assessed the eﬃcacy of the PIPS method in comparison with the RDVS approach and as a means to illustrate the potential With this idea in mind, we resort to Bayesian methods for selecting the active After setting prior distributions, a method for variable selection is needed. To • the reference distribution variable selection (RDVS) method, where a ﬁctitious variable is artiﬁcially added to the GaSP, an MCMC algorithm is performed to sample from the posterior of the range parameters, and the posterior distributions of the parameters associated with the input variables are compared with those associated with the ﬁctitious variable in order to determine which are active and which are not (Linkletter et al., 2006). • high dimensional MCMC algorithms (Savitsky et al., 2011; Chen and Wang, 2010), where the exploration step goes through the diﬀerent models, each with potentially diﬀerent numbers of active inputs. This type of methods provides a posterior probability of each input variable being active, calculated as the proportion of times a model which includes this input was visited in the MCMC. This approach has the advantage of being formally easier to justify. However, it is computationally demanding and the MCMC schemes that visit models with varying dimensions are often hard to tune. Instead, in this work we propose an approach relying on a smooth version of beneﬁts brought by PIPS in the realm of uncertainty quantiﬁcation. Section 5 show a real example where the discrepancy function of a computer model that models the electrical production of a photovoltaic plant is screened. Finally, Section 6 concludes the paper with some general comments and directions for future research. Computer models typically have two kinds of inputs, namely, input variables and model parameters. Input variables are often controllable inputs which can be set at chosen values in ﬁeld experiments designed to observe the real process which the computer model aims at reproducing. They may also be environmental variables which are observed during the ﬁeld experiments. On the other hand, model parameters are usually unknown in the context of physical experiments; they are typically estimated using the information contained in the physical experimental data, a task that is usually called calibration. We denote the generic vector of input variables by x = (x are present in the process of linking experimental observations and computer models, often for the purpose of calibration. One of these sources of uncertainty is referred to by these authors as model inadequacy and results from the fact that no model is a perfect representation of the real process it aims at reproducing. They propose that this uncertainty should be captured by the so-called discrepancy or bias function; Craig et al. (2001) advocate a similar approach. Since then, this has become standard practice, although various cautionary remarks have been made over the years as to how diﬃcult it is to account for this extra source of uncertainty, cf. e.g. Brynjarsd´ottir and O’Hagan (2014). Speciﬁcally, denote by x rations at which the ﬁeld experiments are conducted; that is, x denotes the values of the input variables that have been set for the ith experiment (or that will be observed as part of that experiment, if corresponding to environmental variables). Following Kennedy and O’Hagan (2001), we model the ﬁeld data where, for i = 1, . . . , n,  measurement error, f(·, ·) denotes the computer model and θ represents the true but unknown value of the vector of model parameters. The discrepancy function, δ(·), is modeled a priori as a GaSP: . . . , x)and the generic vector of model parameters by θ = (θ, . . . , θ). Kennedy and O’Hagan (2001) described the various sources of uncertainty that where c(·, · | ψ) is chosen as a separable correlation kernel which, for any two conﬁgurations x and ψ c is the power exponential kernel with 0 < a ≤ 2 ﬁxed. Notice that we impose a zero mean for this Gaussian process. This is justiﬁed by the belief that the computer model accounts for the main trend in the ﬁeld data. such that, with f(θ) = (f(x where R is a n × n matrix with entries R = [c(x the order-n identity matrix. paired with prior distributions on the unknown parameters θ, σ us to obtain the posterior distribution on the vector of model parameters θ. Additionally, it also allows us to predict the real process at particular conﬁgurations of the input variables. Such predictions incorporate model inadequacy and are, for that reason, called bias-corrected predictions in Bayarri et al. (2007a). situation they are typically replaced by surrogate models, i.e. fast approximations to its output. A very important class of surrogate models, often called emulators, are constructed using Gaussian process response surface methods, as originally used in Kennedy and O’Hagan (2001) and detailed for example in Bayarri et al. (2007a). We will be back to these emulators in Section 3.3 to discuss how our methodology also applies to them. to ascertain which of the input variables x function — eﬀectively, what we are proposing is screening the discrepancy function for active inputs. This is a relevant question because its answer tells us which are the input variables that the computer model is not handling correctly, information that may be a useful insight for the modeler but also an indication that along such input variables it may not be a good idea to extrapolate. tion based on the existing literature. Although this literature was concerned with ∈ (0, +∞) denotes a range parameter. One of the most common choices for In this setting, the sampling distribution of the ﬁeld data y= (y, . . . , y) is In the situation where f(·, ·) is computationally fast, the statistical model (5), Of course, computer models are often computationally demanding, and in that The focus of this paper is on how to use this same Bayesian statistical model In the next subsection, we present methods for screening the discrepancy funcscreening a GaSP emulator (Linkletter et al., 2006) or with variable selection for Gaussian process regression (Savitsky et al., 2011), screening the discrepancy function when the prior (2) is used is actually similar to these problems. In fact, the statistical model on which screening a computer model is based is very similar to (5) but with f (θ) ≡ 0 and δ being the prior on the computer model. In that case, y is the so-called model data, obtained by running the computer model at a set of carefully designed conﬁgurations for its inputs. Additionally, σ variance of the GaSP prior assumed for the computer model and σ of the nugget — see Gramacy and Lee (2012) for reasons why a nugget may be used when emulating deterministic models. In the context of model (5), and given the separable structure that we have assumed for the correlation function, as expressed in (3), the statement that one of the p input variables, say x a single parameter, namely, ψ c(x This can be veriﬁed in the particular case of the power exponential correlation function in (4), which is very popular in the area of computer models and that we assume throughout this paper with a convenient reparametrization. That is, instead of the range parameter, we use the parametrization introduced by Linkletter et al. (2006) speciﬁcally to address the problem of screening of computer models: ρ= exp(−(1/2) with a ﬁxed at some value in the range of (0, 2]. if ρ thinking about priors for this parameter, particularly when additionally the inputs are also transformed to vary in [0, 1] indexing the models that result from selecting all possible subsets of active input variables using the vector γ = (γ now be cast as assessing the evidence in favor of each of the competing models. Under model M for the fact that R is replaced by R set of ρ This language is very similar to the one used in Bayesian variable selection in the context of the linear regression model (cf, e.g., Clyde and George, 2004). , x| ψ) → 1 ∀i, j = 1, . . . , n and i 6= j, so xdoes not contribute to R. There are two advantages to this parametrization: ﬁrst, the `-th input is inert = 1; second, the parameter space for ρis the unit interval, which simpliﬁes It is natural to pose screening as a variable selection problem and proceed by such that γ= 1, ` = 1, . . . , p. Equivalently, under M, ρ= 1 if γ= 0. A natural way to quantify model uncertainty in this context is through the posterior distribution on the model space, where π(γ) = P(M posterior probabilities of model M distribution of the observable y under that model, that is, m(y | γ) = Besides the integration present in the formula above, another diﬃculty in this approach is the speciﬁcation of the prior on the model speciﬁc parameters π(σ γ). The prior on the calibration parameters θ, however, is typically speciﬁed using expert information. screening of computer models to more general data structures and models. They propose to use π(σ with Dir spike and slab prior of Bayesian variable selection (Mitchell and Beauchamp, 1988): if a variable is present in the model, its prior is the ‘slab’, a U(0, 1) here; otherwise it’s a ‘spike’, a point mass at 1. The prior (9) is paired with where τ additionally propose fairly sophisticated MCMC schemes to sample from the posterior distribution of (ρ, σ the posterior on (ρ, γ). can be obtained from the set up described above by setting τ and then integrating out γ from π(ρ, γ) = π(ρ | γ) π(γ), resulting in Since the model indicator γ is integrated out, variable selection can only be based on the posterior distribution of ρ, and one needs to ﬁnd a criterion to determine Savitsky et al. (2011) extends the work initiated by Linkletter et al. (2006) for (·) representing the Dirac delta at 1. This is inspired by the (discrete) is a ﬁxed number representing the prior probability that xis active. They The approach in Linkletter et al. (2006) is quite diﬀerent. The prior on ρ they use whether x large number of times (say, T = 100), add a ﬁctitious input x kernel (along with ρ (ρ, ρ if the posterior median of ρ distribution of the posterior median of ρ the acronym of their approach: RDVS, or reference distribution variable selection. Note that the price to pay for avoiding a sophisticated MCMC algorithm to sample from ρ, γ, σ T times. are computationally demanding since they resort, respectively, either to a highdimensional MCMC algorithm or to many repetitions of an MCMC algorithm. Along with the idea of screening not the computer model but rather the discrepancy function for active inputs, another major contribution of this paper is to propose an alternative prior distribution π(σ deem the input variables as active or inert. In the previous section, we have laid out the problem of screening the discrepancy function and have cast the screening problem as a model selection exercise. Here, we detail the methodology that we propose to solve this problem. We ﬁrst present the class of competing models and then we expose our methodology to compute their posterior probabilities. Finally, we describe how to extend the methodology in order to address the problem when the computer model is slow. Let η = (σ the variance of the measurement error, and the vector of calibration parameters. The vector γ, as deﬁned in the previous section, indexes all the possible selections of active input variables. The issues that we have to address are the speciﬁcation of the prior on the vectors η and ρ under each model M problem of obtaining the marginals in (8). selection in the linear regression context; see Mitchell and Beauchamp (1988). The continuous spike and slab, originally proposed by George and McCulloch (1993), diﬀers from its discrete version in that the ‘spike’ is no longer a Dirac delta but rather a continuous distribution highly concentrated around the value of the parameter that corresponds to removing the variable from the model, zero in the case of linear regression. ), record the posterior median of ρ. In the end, deem an input xas inert The methods proposed in Savitsky et al. (2011) and Linkletter et al. (2006) The prior in (9) is reminiscent of the discrete spike and slab prior of variable the case that each of the competing models has a diﬀerent number of parameters; rather, there is only one statistical model and what changes is the prior. We can still use the vector γ to index the diﬀerent selections of active inputs, but now γ no longer means ρ is interpreted as the impact of the input being of no practical signiﬁcance. The quantities in (7) are still well deﬁned, and form the basis for posterior inference. For that reason, we still identify diﬀerent values of γ with diﬀerent competing models. to compute all 2 posterior of unknowns under the full model, the one corresponding to γ = 1, plus some minimal additional post processing. where B(· | α, β) represents the Beta density with positive shape parameters α and β. In (11), α is active then its corresponding ρ follows a priori a uniform distribution; otherwise, instead of being set at 1 like in the discrete spike and slab, it follows a distribution highly concentrated around 1. This is depicted in Figure 1. In §4.1, in the context of a simulation exercise, we address the issue of how α recommending taking α approaches the one obtained via the discrete spike and slab prior, so that this construction can be viewed as a relaxation technique to facilitate the sampling that we are proposing. Theorem. For any `, we denote by M equivalent to removing the input variable x minor conditions on the prior distributions, it holds that multiplicity (Scott and Berger, 2010), an issue that, to the best of our knowledge, has not been addressed in the context of screening. Once the marginals have been computed, it is straightforward to include any prior on γ via (7). In our numerical examples, the constant prior will be the implemented choice. is usually utilized to specify π(θ). Inverse gamma distributions are chosen for the We modify (9) in the spirit of the continuous spike and slab. Hence, it is no longer We introduce this modiﬁcation for computational reasons, as it will allow us Speciﬁcally, we propose to use π(ρ, η, γ) = π(η) π(ρ | γ) π(γ) where We show in the following theorem that, as α→ +∞, the resulting inference The proof and the minor conditions are provided in Appendix A. The prior on γ is not the subject of this paper. It is typically used to control for The prior on η is broken down as π(η) = π(θ) π(σ) π(σ). Expert knowledge Figure 1: Two components of the spike and slab prior distribution for the ρ parameters (the slab component is depicted in blue, and the spike one in red). priors on the variances; see sections 4 and 5 for examples on how to specify the parameters of these priors. Then, we need an eﬃcient way to compute the marginal m(y | γ) for all possible choices of γ. The ability to compute the marginal distribution of the observable under each of the competing models is one of the main computational obstacles to implementing model selection strategies. Han and Carlin (2001) oﬀers a comparative review of most of the available tools to accomplish this task. Many are based on some post-processing of a sample from the posterior distribution of the model-speciﬁc parameters, and this means that one would need an MCMC sample from each of the competing models, which is not feasible even for relatively small p. Our problem has a very speciﬁc structure that can be exploited. written as a function of the Bayes factors of each competing model to M which is a ratio of normalizing constants. Since the support of the posterior is the same under model M importance sampling (c.f., e.g., Chen and Shao, 1997) as an expectation with respect First, notice that the posterior distribution over the model space in (7) can be to the posterior under M where f(y | ρ, η, γ) is given by (5). This allows us to estimate all the ratios using an MCMC sample from the full model only: if {ρ from the posterior distribution of the unknowns for γ = 1, then which is a consequence of (5) being the same regardless of γ, of the prior independence between ρ and η given γ, and of the fact that under the full model the prior on ρ is a product of uniform distributions on the unit interval. text, because one would have to compute the ratio of normal densities centered at zero but with diﬀerent variances, the one in the numerator having a much larger variance than the one on the denominator, which is not likely to be numerically unstable even for values far away from the origin. This will be explored elsewhere. with a prior on γ, e.g. the constant prior, and to obtain the posterior probabilities of each of the competing models. It then provides us with the so-called posterior (marginal) inclusion probabilities of each input which are an easy to interpret measure of the importance of x response. Our approach to screening then reduces to the computation of such quantities and this is the reason for its acronym PIPS: posterior inclusion probability screening. clusion probability of a pair of variables: When the computer model is slow, MCMC algorithms are typically unfeasible since they require many evaluations of the computer model. A classical technique is to Note that this strategy would also potentially work in the linear regression con- Once all 2− 1 Bayes factors are computed, it is straightforward to pair them We can additionally obtain other interesting summaries, e.g. the posterior inbypass the computational eﬀort by using a surrogate which approximates the computer model but is fast to run. Thus the surrogate can be plugged in the likelihood derived from the model given in (5) instead of the computer model. However, the main limitation of this approach lies in the fact that the bias not only accounts for the modeling error of the computer model but also for the discrepancy between the surrogate and the computer model. lator (Sacks et al., 1989; Currin et al., 1991). In addition to the fast approximation, they provide the practitioner with a measure of the additional source of uncertainty coming from the approximation. It is based on a limited number of runs of the computer model. We denote by f(D) the corresponding outputs of the computer model with respect to the design of numerical experiments D = {(x which consists of combinations of conﬁgurations of input variables and model parameters. Note that the chosen values for the input variables x are not necessary constrained to be chosen among the ones corresponding to the ﬁeld data x eled as a GaSP. Usually, the principle of modularization (Liu et al., 2009) is used. The parameters of this GaSP are estimated using only the data f (D) (and not the ﬁeld data, y). The emulator then consists of the process conditioned on f(D) with plugged-in parameters. It is still a Gaussian process, the conditional mean of which, denoted by e(·), gives an approximation of the computer model and the conditional covariance of which, denoted by k(·, ·), provides a measure on the uncertainty coming from this approximation. This source of uncertainty can be incorporated within the likelihood/statistical model: where R and I of the conditional mean of the emulator and K is the n × n correlation matrix corresponding to the emulator, with entries K = [k((x now we have an additional correlation matrix K that needs to be recomputed for each proposal of θ. However, there is no additional technical or conceptual diﬃculty to apply the PIPS method in the context of a slow computer model. The aim of this section is to assess the performance of the PIPS methodology in simulated scenarios. In §4.1, we compare the PIPS results with the results obtained using the RDVS method when screening the discrepancy function for active inputs. In §4.2, we introduce a simulation study where we try to mimic situations one might encounter when confronting ﬁeld observations with the output of a computer model. Some surrogates are emulators, perhaps the most utilized being the GaSP emu- This will lead to a added computational burden in the MCMC algorithms, since All the computer codes utilized in the analyses of this section are available as an R package in https://github.com/Demiperimetre/PIPScreening. We consider a scenario where we have p = 8 input variables. The n = 50 experimental units will be chosen as a Latin-Hypercube-Design (LHD) and denoted as where the statistical analyses that will follow, these parameters will be considered either known and ﬁxed at their true value, or as unknown, and in this case they will be calibrated. which crucially depends only on 4 of the 8 original input variables, namely on x xand x the discrepancy function (x and x ﬁnally, variables which do not appear neither in the model not in the discrepancy function (x PIPS approach as well as the RDVS method to identify the variables which are active in the discrepancy function. As mentioned, we consider θ to be ﬁxed at its true value or unknown and hence calibrated. Notice that the discrepancy function is treated as unknown (i.e., the formula (19) used to generate the data is not used in the analysis) and is modeled as a GaSP as in (2). The measurement error variance σ is also treated as unknown. Metropolis within Gibbs (MwG) algorithm followed by a Metropolis-Hastings (MH) algorithm. First, 5, 000 iterations of an MwG algorithm are run. Then, the covariance matrix of the posterior distribution of the parameters is estimated from this sample and is used in order to set the covariance of the proposal distribution of , x. . . , x)∈ [0, 1], for i = 1, . . . , n. The ﬁeld observations will be generated from the model The four parameters (θ, j = 1, . . . , k = 4)are ﬁxed at (0.3, 0.4, 0.5, 0.6). In . Finally, the measurement error was simulated as ε∼ N (0, σ= 0.05). Notice that in this scenario we have variables which are both in the model and in ), variables which are only part of the discrepancy function (xand x) and, We used the model above to generate 100 datasets. We then applied the proposed For the two methods, the MCMC algorithm consists in a combination of a an MH algorithm. This MH algorithm is run for 10, 000 iterations. The proposal distributions are Gaussian random walks in 1 dimension for the MwG and in all dimensions for the MH algorithm. The parameters are transformed to lie in a nonconstrained real interval. The variance parameters are then taken in the logarithm scale and the logit function is used to map the the model parameters to calibrate and the ρ parameters (all lying in the unit interval) to the real interval. 100 times for the RDVS method, in order to learn the distribution of the posterior median for an inert input variable. both methods require a prior for the variances, which we specify as inverse gamma: variance of the discrepancy term to be larger than the measurement error variance. When the parameter θ is treated as unknown, a uniform distribution in the interval [0, 1] is set independently for each of its dimensions. The parameters in ρ have all the slab part of the prior distribution since the MCMC algorithm is run only under the model M PIPS, we need to specify the spike part of the prior on ρ are scaled to the [0, 1] interval, we set the shape parameters to be the same regardless of `: α and have concluded that the computed posterior inclusion probabilities were not sensitive to the choice of α in this particular range. Accordingly, hereafter we ﬁx this hyperparameter at α = 100. ing to each input variable is compared to a quantile of the posterior median of the range parameter of the ﬁctitious variable. In particular, the three percentiles used are 5%, 10% and 15%. Notice that Linkletter et al. (2006) argue for using a relatively large percentile in order to avoid missing an active variable; accordingly, they recommend the 10th percentile. threshold. The natural threshold is 0.5 but, if one wants to be more conservative, one can choose a smaller one. In particular, we consider 0.1, 0.5 and 0.9 and found the results to be not particularly sensitive to this choice in this example. identiﬁed as active according to the two methods in competition under diﬀerent settings. Table 1 corresponds to the case where the parameter θ in the computer model is ﬁxed at its true value, and Table 2 corresponds to the case where this parameter is simultaneously calibrated. performed or not. The RDVS method may suﬀer from some false detection of active variables which increases as the quantile increases, as it may be expected. Hence, For the PIPS method, this algorithm is run only once, while it is repeated T = We ﬁx the parameter a in (6) at a = 1.9. Regarding the prior distributions, ∼ IG(3, 1) and σ∼ IG(4, 0.02). This implies that, a priori, we expect the Regarding the post-processing of the posterior distribution that takes place in = α. We have experimented with values for α ranging between 50 and 200, For the RDVS method, the posterior median of the range parameter correspond- For the PIPS method, the posterior probability of activeness is compared to a Tables 1 and 2 display the proportions of simulations where the variables are Both methods show convincing results regardless of whether the calibration was Table 1: Proportion of detection for a variable to be active when using RDVS and PIPS methods when the parameters θ are ﬁxed to their true values. For RDVS, the posterior median is compared with quantiles q = 5%, 10%, 15% of the posterior median of the ﬁctitious variable. For PIPS, the tested thresholds are ﬁxed to th = 0.1, 0.5, 0.9. Shadowed columns represent the truly active variables in the discrepancy function. Table 2: Proportion of detection for a variable to be active when using RDVS and PIPS methods when the parameters θ are calibrated. For RDVS, the posterior median is compared with quantiles q = 5%, 10%, 15% of the posterior median of the ﬁctitious variable. For PIPS, the tested thresholds are ﬁxed to th = 0.1, 0.5, 0.9. Shadowed columns represent the truly active variables in the discrepancy function RDVSq10% 1.00 1.00 0.15 0.05 1.00 1.00 0.07 0.00 RDVSq10% 1.00 1.00 0.07 0.05 1.00 1.00 0.03 0.00 PIPS the PIPS method leads to results at least as good as the RDVS method but only requires one MCMC sample instead of a hundred. Furthermore, the PIPS method is stable and does not appear to be too sensitive to the choice of the threshold. In the previous subsection, the goal was to establish the ability of PIPS to screen the discrepancy function for active inputs when compared with an alternative approach. For that reason, the setup of the experiment was arguably far too removed from a realistic situation, in that there was only uncertainty in the discrepancy function: the statistical analysis incorporated the knowledge of the analytic expression of the computer model that was actually used to generate the ﬁeld data. In this subsection, we make an eﬀort to construct more realistic scenarios. We want to mimic plausible situations that one might encounter when analyzing a computer model and ﬂesh out the advantages brought by PIPS in the realm of uncertainty quantiﬁcation. These simulations can be replicated from the vignette available at https: //demiperimetre.github.io/PIPScreening/articles/ExampleScenario.html. variables x distributions except for x with ζ(x ζ(x actual computer model is where θ may either be known or calibrated. We also consider a discrepancy function δ(x reality and the computer model. Notice that the computer model only involves the ﬁrst 3 input variables. With this purpose in mind, we used the values of 100 observations on 5 input , θ) diﬀering in each scenario and ε∼ N (0, σ= 0.05). Conceptually, , θ) is the reality which the computer model is built to replicate. The statistical analysis of the data is constructed under the assumption that the ), which is a function of all 5 variables, meant to capture the diﬀerences between In particular, the considered four diﬀerent situations: is incorrectly modeled by the computer model: 4. x of Case 1 (x model) with the three other cases. The vector of model parameters is chosen as tion, including setting a = 1.9 in (6) and α variables in the discrepancy function under the diﬀerent scenarios. For the two ﬁrst cases, the results are clear and the variables which diﬀer from the computer model to reality appear as active variables in the discrepancy function. For the last case, which is more diﬃcult because an active variable is not in the computer model but another one correlated with it is, the PIPS method manages to detect that something is happening with these input variables, although the probabilities of activeness stay mostly under the 0.5 threshold. In this section, we introduce a real application which was studied by Carmassi et al. (2019). The idea is to use a computer model to predict the power produced by a photovoltaic plant (PVP) given some meteorological conditions such as the temperature and the sun irradiation. The computer model also depends on some parameters which can be ﬁxed at so-called nominal values speciﬁed using expert knowledge or can be calibrated from experimental data. misspeciﬁcation by screening the discrepancy function using the PIPS methodology. 12 panels connected together. This computer model is quick to run and can be represented by a function f : R has no impact in reality: was forgotten in the computer model: ζ(x, θ) =|4x− 2| + θ1 + θ+|4x− 2| + θ1 + θ+|4x− 2| + θ1 + θ+|4x− 2| + θ1 + θ, instead of xis in the computer model From these four situations, we build three scenarios, which are combinations , θ, θ, θ)= (0.4, 0.5, 0.6, 0.7, 0.8). MCMC and prior speciﬁcations were exactly as laid out in the previous subsec- Figure 2 provides the boxplots of the probabilities of activeness of the input Here, we will use the experimental data in order to learn about possible model The computer model aims at mimicking the behavior of a PVP consisting of Figure 2: Boxplots of the probabilities of activeness over the 100 replications. On the left are the cases with parameters ﬁxed to their true values and, on the right, the case where the parameters are considered unknown and calibrated. From top to bottom, mixed of cases 1 and 2, mixed of cases 1 and 3, and mixed of cases 1 and 4. x = (t, I the global irradiation of the sun, I ambient temperature. The parameter θ consists actually of 6 parameters but only one parameter is calibrated, the module photo-conversion eﬃciency. A sensitivity analysis has proven the other parameters to be of negligible importance. The output y is the resulting instantaneous power delivered by the PVP. lected over two months every 10 seconds, which makes for a huge amount of data. Only the data for which the production is positive are kept, corresponding to daytime measurements. The data contain the power production, which is to be compared with the output of the computer model; the four input variables of the computer model and an additional temperature (temperature on the panel, T also recorded. Although this last variable is not an input variable of the computer model, it is tested as a potential active variable in the discrepancy. In order to detect the input variables which may cause the discrepancy between the computer model output and the reality, we applied the PIPS methodology to the data obtained on a single day. The considered days are in August and September 2014. September 7 was removed since the recorded production was null which is a consequence of a sensor malfunction. ﬁve minutes, which gives between 99 and 178 data points per day. Figure 3 illustrates four diﬀerent days of power recording. Although these four days exhibit somewhat diﬀerent patterns of production, the common feature is the global increase in production beginning at sun rise followed by a decay which reaches a null production when the sun sets. Higher frequency peaks may be a consequence of clouds limiting the sun irradiation. We emphasize that this case study is challenging because of the structure of the correlation between the input variables in the computer model. value and when the module photo-conversion eﬃciency is calibrated simultaneously with the detection of active variables. All the input variables and the model parameter were scaled to the [0, 1] interval. The output data were normalized by subtracting their mean and divided by their standard deviation. The prior distributions for the variances of the measurement error and the discrepancy were chosen as inverse gamma distribution with parameters favoring a larger variance for the discrepancy and in line with the known precision of sensors, namely, IG(4, 1/400) for the measurement error and IG(2, 1/200) for the variance corresponding to the GaSP which models the discrepancy. A uniform prior within an interval provided by experts was used for the only parameter to calibrate. We also choose a = 1.9 in the correlation kernel deﬁned in (6), and set α , Id, T)consist of t, the UTC time since the beginning of the year, I, The experimental data correspond to the test stand of 12 panels and were col- To limit the computational burden for a given day, we took a measurement every We consider the situation where all the parameters were ﬁxed at their nominal Figure 3: Power produced by the PV power plant on days: August 1st, August 15, September 1st, September 15. A measure every ﬁve minutes is kept for a positive production. tion 4. Using the resulting sample, we obtain the posterior probabilities of activeness for each day as well as the probability that at least one of the two temperatures (T and T that is, the boxplots describe the set of 60 activeness probabilities corresponding to each of the days. For the two ﬁrst variables (time t and global irradiation I results are clearly in favor of including them in the discrepancy. For the diﬀuse irradiation I show that at least one of the two temperatures should be included in the discrepancy and it seems that the ambient temperature T the computer model is calibrated simultaneously, the calibrated value is shifted toward its upper bound (more than 75% of the days). Related analysis of these data In Carmassi et al. (2019), the computer model was calibrated by assuming diﬀerent statistical models with or without the discrepancy function. When the discrepancy function was part of the model, an isotropic Gaussian kernel was chosen. The temperature on the panel was not considered in the discrepancy. Only the four input variables of the computer model were taken into account in the discrepancy, which makes for a 4-dimensional input space. In addition, to limit the size of the data, they made the choice to compute hourly averages of the input variables and of the power produced. The discrepancy was found to play an important role. We tried the same strategy by only keeping the hours after 8 a.m. and before 7 p.m. where the production is likely to be higher. It resulted in 572 hourly measurements. When screening the discrepancy, all the variables were found active. This is quite expected since the computer model is not directly run on the observed input data but on averages of these input data. In this paper, we focused on screening the discrepancy function which is used to model the gap between a real phenomenon and its “in silico” simulation. The outcome of the screening procedure should help provide the practitioner with a better understanding of the ﬂaws of the computer model by determining which input variables are incorrectly taken into account, i.e., the ones that are active in the discrepancy function. We cast this screening procedure into the more general problem of variable selection for GaSP regression. We then proposed an original method, named PIPS, which aims to select the active variables on the basis of a unique MCMC sample. The eﬃciency of the PIPS method was demonstrated on synthetic examples and was also illustrated on a real world application. Although the computer model in the applications were fast enough so that we did not need to The MCMC set up is the same as the one used in the examples presented in Sec- ) is active in the discrepancy. Figure 4 provides the boxplots of these probabilities computed for 60 days of data, Figure 4: Boxplots of probabilities of activeness of the input variables in the discrepancy computed for the 60 days of data. The column (T fact that at least one of two temperatures is active. The top boxplots correspond to the case when the parameter θ is ﬁxed to its nominal value. The bottom boxplots correspond to the case when θ is calibrated. resort to an emulation step, the PIPS method could be combined with an emulator. However, one has to be careful that, in this case, the discrepancy may account for both the ﬂaws of the computer model and of its emulator. unfeasible if p is too large (say p > 20). In this case, the strategies described in Garcia-Donato and Martinez-Beneito (2013) should obtain reliable estimates of the posterior inclusion probabilities for moderately large values of p. Adaptive MCMC schemes could be combined with some intermediate results from the PIPS method in order to reinforce the sampling in the dimensions where the input variables are the most likely to be active. A complementary issue is the determination of the validity area of the computer model. The PIPS method could then be used to feed adaptive design strategies to accurately determine this region. This work was partially developed while the authors were visiting the Statistical and Applied Mathematical Sciences Institute, and were hence partially supported by the National Science Foundation under Grant DMS-1638521. This paper has been partially supported by research grant PID2019-104790GB-I00 from the Spanish Ministry of Science and Innovation. Pierre Barbillon has received support from the Marie-Curie FP7 COFUND People Programme of the European Union, through the award of an AgreenSkills/AgreenSkills+ fellowship (under grant agreement n◦609398). Rui Paulo was partially supported by the Project CEMAPRE/REM— UIDB/05069/2020—ﬁnanced by FCT/MCTES through national funds, and by the sabbatical fellowship SFRH/BSAB/142992/2018 attributed by FCT. The PIPS method relies on the enumeration of 2models, which might may be A particular concern is the performance of the MCMC sampling, which is crucial.