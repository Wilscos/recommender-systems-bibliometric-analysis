<title>You Get What You Chat: Using Conversations to Personalize Search-based Recommendations</title> <title>1 Introduction</title> Motivation: Recommender systems are at the heart of personalized shopping and online services for music and video streaming, hotels and restaurants, or food recipes [32,6,18]. Search-based recommendation is a setting where the user starts with a query and the recommendation model determines the result ranking based on the user’s interests and preferences. This paper considers medium-grained queries about product entities (books, food recipes, and travel destinations) such as paranormal romance or wine lover destinations – in contrast to coarse-grained queries such as love novels or Europe and ﬁne-grained queries such as similar to Stephenie Meyer’s Twilight or vine- <title>arXiv:2109.04716v1  [cs.IR]  10 Sep 2021</title> yards of the Bourgogne. Results are assumed to come from a search engine (restricted to suitable domains for the respective vertical). Therefore, the personalization amounts to re-ranking the top results with regard to a model of the user’s individual tastes. For this setting, the user model or proﬁle can be represented explicitly in a personal knowledge base [5] or implicitly in a latent model [22,51]. These models can be constructed from various kinds of observations on user behavior: A: Explicit signals like clicks, likes, ratings and purchases. B: User proﬁles such as adssettings.google.com where users can see and check or un-check topics (even if the proﬁle itself is learned from other signals). C: Implicit signals from other online behavior, like social media posts or conversations with other users. Option A is most widely used in practice (e.g., [52,25,19]) and includes standard recommenders based on collaborative ﬁltering [35]. However, this rich kind of data is available only to major service providers, such as music streaming where playlists and other I-like-the-song signals are abundant. Option B operates on concise digests of user interests and item properties, for example, a list of topics and tags (e.g., [45]). This is less informative than A, but has the advantage that the user can easily interpret her proﬁle and adjust it at her discretion (e.g., dropping a topic that is unwanted). Option C has been studied for recommending news and discussions, but the best signals are still the user histories of clicks, dwell times and likes (e.g., [28,44]). For search-based recommendation of product entities, C has not been explored at all, except for the speciﬁc case of leveraging product reviews (e.g., [14,7,34]). This paper focuses on option C. It investigates how online chats between users can be leveraged for personalization in the outlined setting. To the best of our knowledge, it is the ﬁrst work that studies chats as a source for search-based recommendation. Research Questions: We investigate the following research questions: • RQ0: How can we leverage signals from user-user chats to personalize search-based recommendations across a variety of domains: books, food recipes, and travel destinations? • RQ1: How do methods that tap into individual conversations compare to methods that merely access concise user proﬁles? • RQ2: How important is it to customize the per-user models to the speciﬁc domain at hand, for example, books vs. travel? • RQ3: How much added value can we get from entity awareness: detecting named entities in user chats, mapping them to a background knowledge base, and using that information for expansion of user models and re-ranking techniques? Contributions: We devise techniques for constructing language models and using them for re-ranking, with various components derived from chats: i) computing domainspeciﬁc vocabularies and ii) entity detection and entity-based expansions. The chats are recorded real-time conversations, gathered in a substantial user study with 14 students and 83 pair-wise chats (with 9,797 utterances and 59k tokens in total and a total duration of 93 hours). We contrast chat-based personalization against techniques that merely build on concise user proﬁles derived from short questionnaires [43]. The paper makes the following contributions: • It is the ﬁrst approach to consider user chats as a source for search-based recommendation across a variety of vertical domains. Chats are a rich source of information about individual interests and tastes. In contrast to latent models learned from clicks, likes, ratings, etc., a user can more easily interpret and edit/censor this information to selectively restrict its usage for privacy reasons. • We systematically compare chat-based personalization against a more restrictive approach that merely uses concise user proﬁles based on short questionnaires. In our experiments, both show advantages in certain domains, and perform on par overall. • We devise techniques for per-domain customization by controlling the vocabulary and appropriate weighting of terms, and report on their experimental effectiveness. • We devise techniques to harness entities and background knowledge in the construction of user models, and report on their experimental effectiveness. • We release a dataset consisting of ﬁlled questionnaires, pair-wise user chats, document URLs, and search result assessments by users for three domains (books, travel, food). The data is available at http://personalization.mpi-inf.mpg.de/ <title>2 Computational Model and Re-ranking</title> We approach the personalization of entity-search answers by re-ranking a pool of initial non-personalized results using three different methods for scoring and ranking: the BM25 family, statistical language models, and neural ranking. Beginning with these ranking methods, we incorporate a user model to personalize results and domain-speciﬁc term weights to identify terms that are informative with a domain. We additionally apply expansion techniques to expand entities found in the user model. Rerankers thus consider a user model in addition to queries and documents. In our setting, • Queries are short, medium-grained bags of words (or phrases), such as “scandinavian suspense” (for the books domain) or “wine lover destinations” (travel). • Documents are entity-level answers obtained from speciﬁc websites that provide comprehensive contents about three domains: goodreads.com for books, wikivothat can be easily identiﬁed (e.g., from the URL string or page title) and comprises an informative description of the entity. Two of the sites include also extensive reviews and discussion by their communities. • User models represent a user’s interests and tastes as a bag of words (or n-grams) taken from either a short questionnaire/proﬁle ﬁlled in by the user or a collection of online chats with other users. Both of these options are further reﬁned by instructing users to focus on speciﬁc scopes: general, books, travel, and food. This yields 8 basic options for the user model, which we further augment with techniques for domain-speciﬁc vocabularies and entities. For illustration, Figure 1 shows excerpts from the questionnaire and the chat collection for an example user. For the query “temples and culture”, this user-speciﬁc information led to high ranks of travel destinations like Borobudur, Delphi and Ellora – all conﬁrmed as very good recommendations by that user. Given a query q, a user model u, and a document d from a pool of non-personalized results, we personalize the results by re-ranking them according to the user model. We explore three re-ranking methods for doing this. Language Models: The ﬁrst variant for re-ranking is based on language models (LMs) [50], which provide a natural way to incorporate the user model. We compute the Kullback-Leibler divergence between a query model and a document model with Dirichlet smoothing over unigrams or n-grams. In pilot experiments, unigrams outperformed bigrams and trigrams; hence we focus on the unigram case. To personalize for a speciﬁc user, we compute the Kullback-Leibler divergence i) between the query q and the document d and ii) between the user model u and the document d. These two components are combined into a linear mixture with hyper-parameter λ. Additionally, we incorporate a background model C for smoothing, based on ClueWeb’09. That is, score(q, d, u) ∝ −(λdiv(θ kθ ) + (1 − λ)div(θ kθ )) ∝ terms (above a threshold): sim(w, t) · p(t|θ ). BM25: The second variant for re-ranking is the Okapi BM25 model [33]. We incorporate the user model by query expansion. In principle, all terms from the entire chat collection of a user are added to the query. We will discuss ways of reducing noise and focusing the query in Sections 3 and 4. That is, tf(w, d) · (k + 1) score(q ∪ u, d) ∝ spy(w) · idf(w) · (2) tf(w, d) + k 1 − b + b · Neural Ranking with KNRM: The third variant for re-ranking is the KNRM neural method [46] which takes a bag-of-words query as input. KNRM produces a querydocument relevance score by comparing embedding similarities between query and document terms. During training, KNRM learns how to weigh different embedding similarity levels. As with BM25, we incorporate the user model by query expansion. <title>3 Domain-speciﬁc Vocabulary Weighting</title> As described in the previous section, the ranking models are further augmented by awareness of domain-speciﬁc vocabularies, customizing the user models and document models to books, travel or food, respectively. The intuition is that terms in a user chat are informative if they refer to a certain meaning within a particular domain. For example, terms like “history” or “museum” are good cues about a user’s travel interests, whereas terms like “price” or “bargain” are uninformative – although all these terms have comparable idf values in large corpora. We incorporate this idea of domain speciﬁcity by computing per-domain weights for terms, and weighing term contributions by the various ranking models accordingly (or even eliminating low-weight terms). To this end, we estimate the conditional probability of a term occurring in a domain-speciﬁc context (document or chat) given that it occurs in a general corpus: tf(w ∈ Dom)/|Dom| spy(w) = P (w ∈ Dom|w ∈ All) ∝ (3) tf(w ∈ All)/|All| As underlying text collections for this estimator, we use the pool of all retrieved documents per domain (e.g., all answers for book search, including book descriptions and user reviews) against the pool of documents for all three domains together. We also experimented with term weighting for user-speciﬁc vocabularies, contrasting all chats by the same user against a universal corpus. This did not lead to signiﬁcant changes in the empirical results, though, and is disregarded in the following. <title>4 Entity Expansion</title> Named Entity Recognition and Disambiguation (NER/NED): Among all terms and phrases in the user’s chats and questionnaires, entities and concepts deserve speciﬁc treatment. We ran standard NER (stanfordnlp.github.io) and NED (github.com/ambiversewhich in turns links most of these to Wikipedia. However, the NER stage produced both many false positives and false negatives. This is largely caused by the very colloquial nature of user chats, with short-hands, misspellings, ungrammatical utterances and ad-hoc choice between upper-case and lower-case. To mitigate this effect, we hired crowdsourcing workers to mark up text spans for entities and also general concepts that exist in YAGO and Wikipedia (e.g., “history“ or “Buddhist art”). This way we eliminated nearly all NER errors. As a result, the NED stage performed well, with precision reaching approximately 0.83 (estimated by sampling). We obtained this perfect mark-up only for NER as this is much easier for crowd workers than NED. User Model Expansion: Rather than adding the names of these detected and linked entities to the user model directly, which is likely to overﬁt given that we deal with many long-tail entities (e.g., lesser-known books or special travel destinations), we experimented with expanding entities using embeddings and Wikipedia descriptions. We ﬁrst conducted pilot experiments with entity embeddings using Wikipedia2vec [48,47] to achieve proper generalization, but this did not perform well: many terms that are highly related by Wikipedia2vec are quite uninformative if not misleading (e.g., history being most related to literature; modern, natural, and wine being most related to coffee, beer, food). Ultimately, to avoid this noise and topical drift, we expanded the entities using their descriptions from (the ﬁrst paragraph of) their Wikipedia articles. This captures, for example, content sketches of books, highlights of travel destinations, etc. The resulting terms were added to general as well as domain-speciﬁc user models. For the latter, we computed the domain speciﬁcity of an entity and its descriptive terms, using the weighting model of Section 3. Selective Expansion by Domain-Speciﬁcity: Some of the extracted entities may be poor cues for a certain target domain (e.g. a user chatting about “Italian cuisine” is not helpful for books and could even be misleading for travel). To counter this potential dilution, we use the domain-speciﬁcity of entities to ﬁlter the candidate entities before expanding the user model. To this end, we construct a domain model for each of the three domains using Wikipedia2vec embeddings which capture both entity-level linkage and textual descriptions [48,47]. Candidate entities are mapped into the same latent space, and the cosine similarity between entity and domain is used to select entities above a threshold. Specifically, the domain vectors are computed by a weighted average of the m = 50 words and entities that are most related to the Wikipedia articles on “book”, “travel” and “food”, respectively, with weights proportional to cosine between vectors. For selective entity expansion of per-domain user models, we pick entities whose similarity to the respective domain model is above a speciﬁed threshold. This approach introduces several thresholds and hyper-parameters: per-domain numbers of related terms for the domain model and similarity thresholds for pruning entities. We tuned these via grid search with the objective of maximizing the area under the precision-recall curves for entity detection and disambiguation. We used the manu- ally annotated entities in the domain-speciﬁc questionnaires as ground-truth for domain relatedness. <title>5 Data Collection</title> We gathered personal data in a 4-week user study with 14 students who were paid ca. 10 Euros per hour. We randomly paired two users for 3 chats per week. For the ﬁrst week, users were instructed to chat generally, like mutual introductions. During the remaining weeks users were asked to chat about speciﬁc topical domains: users’ interests and tastes in books and their experience and interests in traveling and food. On average, each user had 2.8 sessions for each domain, totaling to ca. 11 sessions overall, with an average of 653 utterances and 3934 tokens per user. In addition, each user ﬁlled in several questionnaires upfront: a general one with 18 questions about demographics, general interests and personality, and one for each of the themes books, travel and food with 2, 5 and 10 questions, respectively (see left side of Figure 1 for an example excerpt). The general questionnaire included personality-oriented questions such as “What are your hobbies?”, “What makes you happy?”, and “Your golden rule?”. <title>6 Experimental Studies</title> The 14 users from whom we collected questionnaire and chat data also participated in an assessment study of personalized search results. To this end, we compiled 75 mediumgrained keyword queries (25 per domain). Example queries are shown in Table 1. All queries were issued to a commercial search engine with site restrictions as described in document models (section 2). The top-100 answers were retrieved, keeping only those that were about speciﬁc entities and discarding general list pages – this left us with 90 or more answers for each query. The users were asked to identify around 5 queries for each domain on topics that looked potentially appealing to them. This way we avoided personalized judgements on topics that the user does not care about. For each query, a user assessed 20 results that were sampled uniformly at random (to avoid ranking bias) and, additionally, the top-10 results from the original ranking (with the risk of bias). We asked for subjective, graded assessments with labels: 2 = strongly interested, 1 = mildly interested, 0 = uninterested, and discarded all “I don’t know” assessments. We required the users to enter justiﬁcation sentences along with their judgements. In total, we obtained 2673 individual assessments for 113 user-query pairs with 73 distinct queries. Evaluation Metrics: The primary metric is NDCG@20, which we use to refer to methods’ effectiveness when re-ranking the 20 randomly sampled query results. In addition, we report on precision@1 where we compare the highest-ranked results from the 20 random samples against a user judgement of 1 or 2 (= strongly or mildly interested). For completeness, we also consider NDCG@top10 for the top-10 results of the original, potentially biased, rankings from a commercial search engine. Methods under Comparison: We cover the following methods and conﬁgurations. • LM denotes the language model approach. To isolate the effect of the user model in the re-ranking, and as our initial pool of entities are to some extent relevant to the query, we either set the λ to 0 or 1. When λ = 1 the input to the re-ranker is the query model and when λ = 0 only the user model is given as input. • LM-embed is the language-model method with word embeddings using word2vec. The term-term similarity threshold is set to 0.5. • BM25 is the BM25 method with parameters set to the following values widely used in the literature: b = 0.75, k = 1.5. • KNRM is the neural ranker, with the maximum query and document lengths set to 50 and 5000, respectively. The terms for the query/user model are obtained by tf order, selecting the top 50 distinct terms. Document terms are the top-5000 terms. Models are trained on data per domain with 504, 772 and 806 assessments for book, food and travel, respectively. As this training is fairly low-end, we also study a variant KNRM-all where we combine all domains into a single training set with 2082 labeled samples. We report on ten-fold cross-validation with 8, 1 and 1 folds for training, validation and test, respectively. • SE is the initial ranking from a commercial search engine. Table 2 shows the NDCG@20 results for the inﬂuence of different user models. The top part of Table 2 gives the overall results across all domains (averaged over the 113 user-query pairs). The other parts show per-domain results. The user models under comparison here are query-only vs. questionnaires-based vs. chats-based. For the latter two, we varied the speciﬁc setting by deriving models from all available inputs regardless of the domains (All), using only general questionnaires or chats (Gen, see Section 5), using only domain-speciﬁc inputs (Dom), or using both general and per-domain inputs (Dom + Gen). In this comparison, all methods were conﬁgured without entity expansion and without domain-speciﬁc vocabularies (which will be discussed in the next subsections). Overall results (top part of Table 2): The overriding observation is that almost all rankers with different degrees of personalization improve over the SE baseline and that both questionnaire-based and chat-based user models achieve notable gains over the query-only rankers: in the order of 2 to 4 percentage points in NDCG@20. While the effect size of personalization is only moderate, the relative gains are statistically significant and come at little cost for the ranker efﬁciency. For signiﬁcance, two-tailed paired t-tests in comparison to the Query-Only baselines mostly had p-values < 0.05. These results are marked with an asterisk in Table 2. Interestingly, LM-embed did not improve over LM. The term-term relatedness by word2vec seems to be too crude for our task and dilutes the query focus. KNRM and KNRM-all were inferior to the Query-Only case. The combination of small training data and limited input size is the likely cause for this disappointing result. When comparing questionnaire-based vs. chat-based personalization, the former performs slightly better than the latter, but the differences are minor. For both, the best variants were the ones with user models Dom or Dom + Gen, indicating awareness of the domain is beneﬁcial. Dom is almost always preferable to Dom + Gen in the case of chats, but there is no clear trend when using questionnaires. This is likely due to the fact that the general questionnaires were designed to reveal user personalities, whereas the general chats were mostly introductory and less informative. These gains are not always statistically signiﬁcant, but the best cases are: for example, the improvement for LM from 0.811 with chats-All to 0.822 with chats-Dom had a p-value of 0.0018. Per-domain results: The results vary among the different domains in an interesting way. We base the discussion on LM and BM25 as they achieved the best results. For books and travel, the gains from user models are most pronounced. For books, the chat-based models achieved a small but notable and signiﬁcant improvement over the questionnaire-based ones. We observe that for questionnaire-based models Dom+Gen outperforms Dom. This is due to the low coverage of the book domain with only two questions on the user’s favorite books and genres, whereas the general questionnaire includes demographics and personal traits. On the other hand, for the travel domain with 5 speciﬁc questions, Dom performs better than Dom+Gen in both questionnairebased and chat-based models, with the former giving the best results. For food, personalization led to gains, but the absolute NDCG scores were substantially lower than for the other two domains. Here, the SE performed better than the re-rankers with the query-only model. However, using Dom+Gen questionnaire-based proﬁles, we achieved up to 2% improvement over the SE results. It seems that the food domain is inherently difﬁcult to understand, as its vocabulary mixes speciﬁc and very common words with a strong inﬂuence of the latter on tastes and sentiments (e.g., “hot”, “terriﬁc” etc.). As for precision@1, the overall gains by personalization were nearly 10 percent: considering the best-performing rankers on overall results, the LM improved from 70% with query-only models to 81% with questionnaire-based models, and BM25 went up from 66% to 83%. Again, the gains were most substantial for books and travel, but here food as well showed notably improved precision@1. We further evaluated NDCG@top10: not surprisingly, the SE baseline was stronger for this metric, but was still outperformed by re-ranking with personalization. The best values for our method were comparable to those for NDCG@20, around 83% across all domains and up to 87% for travel. Recall from Section 3 that we optionally incorporate domain-speciﬁc term weighting to reduce the inﬂuence of irrelevant wording from the user chats. Table 3 shows NDCG@20 results with this awareness of domain vocabularies, for the four chat-based conﬁgurations All, Gen, Dom and Dom + Gen. We show only overall results across all domains, but for each domain, all user-model terms were weighted by the respective spy(w) domain model. For brevity, we restrict ourselves to the LM-based ranker; the ﬁndings were similar for the other two rankers. Table 3 indicates that there are small gains from this domain-speciﬁc weighting, but the effect size is marginal and not statistically signiﬁcant (p-value > 0.1). It seems that chats are not sufﬁciently focused on domain-speciﬁc topics. Humans do jump between topics, so chats naturally have a high level of thematic diversity. To study the inﬂuence of entity expansion for the user models, we compared different settings against the previously reported conﬁgurations without entity awareness: all expands all entities including concepts (in Wikipedia, such as “history” or “Buddhist art”); domain restricts the entities to those that are related to the respective domain (see Section 4); NE-all uses only named entities (i.e., discarding general concepts); NE-dom uses only named entities with domain relatedness above a threshold. Table 4 shows the overall NDCG@20 for these settings with the different conﬁgurations for the user-model construction. We observe that almost none of the expansion methods signiﬁcantly improve the models derived from questionnaires. The reason is that these models are already very concise given their high-quality inputs. For chatbased user models, on the other hand, entity expansion led to small, but notable and statistically signiﬁcant (p-values < 0.05 ), improvements of ca. 1%. <title>7 Related Work</title> user activities reﬂected in queries, clicks and emails, all the way to news and other contents read by a user. For personalized ranking, language models were enhanced with user-speciﬁc priors [39]. The interplay of long-term behavior and short-term sessions of a user was studied by [8,10]. Other work [42,9] investigated the issue of when to personalize and when to disregard user proﬁles. None of these prior works is speciﬁcally geared for entity search, and none considers user models derived from chats. Entity search (e.g., [4,16]) has been studied for personalization only in limited settings. The CLEF competition on book recommendations [21] relied on extensive data (posts, tags, reviews, ratings) by large user communities at LibraryThing and Amazon. Most related to our work is [2] on personalized product search, based on embeddings for users and products in a joint latent space. That method exploited user reviews on product pages. In contrast, our approach is based on user-user chats, an unintrusively observable asset disregarded in prior works. Query expansion is a well established methodology in IR (see, e.g., [13] for a survey). Personalization has been studied in this context along various routes. Notable examples are [11,53] based on user-provided tags, and [23] based on email histories and utilizing word embeddings learned from email contents. Recently, [49] has pursued the theme of personalized word embeddings further, based on query histories. <title>8 Conclusion</title> To the best of our knowledge, this is the ﬁrst work that explores leveraging user-to-user conversations as a source for personalization of search-based recommendations. We compared chat-based user models against models derived from concise questionnaires. Both achieved substantial improvements over both the original search-engine ranking and non-personalized query-only re-rankings. Between chat-based and questionnaire-based re-rankings, there is no clear winner. The two paradigms of user models each have speciﬁc beneﬁts: • Questionnaries are transparent and scrutable for users. However, they require an explicit effort. Most users seem ﬁne with a one-time questionnaire, but few seem ready for periodic updating as their interests and tastes evolve. • Chats, on the other hand, require no effort at all from the user side, and could be easily updated without user intervention. However, the derived models are less transparent to humans and not easily adjustable by users themselves. Also, chat data comes with higher privacy risks. The additional enhancements devised in this paper – domain awareness and entity expansion – further improved the NDCG scores, but only to a small extent. On the other hand, focusing on entities in conversations and casting them into an explicit user model is a step towards making chat-based proﬁles more transparent and scrutable for users. Acknowledgements. This research was supported by the ERC Synergy Grant 610150 (imPACT). <title>References</title>