Recommender systems predict an afﬁnity score between users and items. Current recommender systems are based on content-based ﬁltering (CB), collaborative ﬁltering techniques (CF), or a combination of both. CF recommender systems rely on (USER, ITEM, INTERACTION) triplets. CB relies on (ITEM, FEATURES) pairs. Both system types require a costly structured data collection step. Meanwhile, web users express themselves about various items in an unstructured way. They share lists of their favorite items and ask for recommendations on web forums, as in (1) between the enumerated movies. (1) Films like Beyond the Black Rainbow, Lost River, Suspiria, and The Neon Demon. The web also contains a lot of information about the items themselves, like synopsis or reviews for movies. Language models such as GPT-2 [14] are trained on large web corpora to generate plausible text. We hypothesize that they can make use of this unstructured knowledge to make recommendations by estimating the plausibility of items being grouped together in a prompt. LM can estimate the probability of a word sequence, P (w, ...w train a neural network, its parameters Θ are optimized for next word prediction likelihood maximization over k-length sequences sampled from a corpus. The loss writes as follows: https://colab.research.google.com/drive/...?usp=sharing https://www.reddit.com/r/MovieSuggestions/...lost river/ Abstract. Recommendation is the task of ranking items (e.g. movies or products) according to individual user needs. Current systems rely on collaborative ﬁltering and content-based techniques, which both require structured training data. We propose a framework for recommendation with off-the-shelf pretrained language models (LM) that only used unstructured text corpora as training data. If a user u liked Matrix and Inception, we construct a textual prompt, e.g. ”Movies like Matrix, Inception, <m>” to estimate the afﬁnity between u and m with LM likelihood. We motivate our idea with a corpus analysis, evaluate several prompt structures, and we compare LM-based recommendation with standard matrix factorization trained on different data regimes. The code for our experiments is publicly available. ). Neural language models are trained over a large corpus of documents: to We rely on existing pretrained language models. To make a relevance prediction , we build a prompt for each user: where <m ordered movies liked by u. We then directly use to sort items for user u. Our contributions are as follow (i) we propose a model for recommendation with standard LM; (ii) we derive prompt structures from a corpus analysis and compare their impact on recommendation accuracy; (iii) we compare LM-based recommendation with next sentence prediction (NSP) [12] and a standard supervised matrix factorization method [9,15]. Language models and recommendation Previous work leveraged language modeling techniques to perform recommendations. However, they do not rely on natural language: they use sequences of user/item interactions, and treat these sequences as sentences to leverage the architectures inspired by NLP, such as Word2Vec [7,1,4,11] or BERT [19]. Zero-shot prediction with language models Neural language models have been used for zero-shot inference on many NLP tasks [14,2]. For example, they manually construct a prompt structure to translate text, e.g. Translate english to french : ”cheese” =>, and use the language model completions to ﬁnd the best translations. Petroni et al. [13] show that masked language models can act as a knowledge base when we use part of a triplet as input, e.g. Paris in <mask>. Here, we apply LM-based prompts to recommendation. Hybrid and zero-shot recommendation The cold start problem [17], i.e. dealing with new users or items is a long-standing problem in recommender systems, usually addressed with hybridization of CF-based and CB-based systems. Previous work [20,10,5,6] introduced models for zero-shot recommendation, but they use zero-shot prediction with a different sense than ours. They train on a set of (USER, ITEM, INTERACTION) triplets, and perform zero-shot predictions on new users or items with known attributes. These methods still require (USER, ITEM, INTERACTION) or (ITEM, FEATURES) tuples for training. To our knowledge, the only attempt to perform recommendations without such data at all is from Penha et al. [12] who showed that BERT [3] next sentence prediction (NSP) can be used to predict the most plausible movie after a prompt. NSP is not available in all language models and requires a speciﬁc pretraining. Their work is designed as a probing of BERT knowledge about common items, and lacks comparison with a standard recommendation model, which we here address. > is the name of the movie mand <m...m> are those of randomly Dataset We use the standard the MovieLens 1M dataset [8] with 1M ratings from 0.5 to 5, 6040 users, and 3090 movies in our experiments. We address the relevance prediction task and we discard the other ratings. We select users with at least 21 positive ratings and 4 negative ratings and thus obtain 2716 users. We randomly select 20% of them as test users. 1 positive and 4 negative ratings are reserved for evaluation for each user, and the goal is to give the highest relevance score to the positively rated item. We use 5 positive ratings per user unless mentioned otherwise. We remove the years from the movie titles and reorder the articles (a, the) in the movie titles provided in the dataset (e.g. Matrix, The (1999) → The Matrix). Evaluation metric We use the mean average precision at rank 1 (MAP@1) [18] which is the rate of correct ﬁrst ranked prediction averaged over test users, because of its interpretability. Pretrained language models In our experiments we use the GPT-2 [14] language models, which are publicly available in several sizes. GPT-2 is trained with LM pretraining (equation 1) on the WebText corpus [14], which contains 8 million pages covering various domains. Unless mentioned otherwise, we use the GPT-base model, with 117M parameters. <m>, <m>, <m>, <m> 85 Table 1: Occurrence counts of 3-6 grams that contain movie names in the Reddit corpus. <m> denotes a movie name. We analyze the Reddit comments from May 2015 lists of movies in web text. This analysis will provide prompt candidates for LM-based Item relevance could be mapped to ratings but we do not address rating prediction here. Training users are only used for the matrix factorization baseline. https://www.kaggle.com/reddit/reddit-comments-may-2015 , so we consider a rating r as positive if r ≥ 4.0, as negative if ≤ 2.5 recommendations. We select comments where a movie name of the MovieLens dataset is present and replace movies with a <m> tag. This ﬁltered dataset of comments has a size of > 900k words. We then select the most frequent pattern with at least three words, as shown in table 1. Movie names are frequently used in enumerations. The patterns Movies like <m> and Movies similar to conﬁrm that users focus on the similarity of movies. Figure 1 shows that prompt design is important but not critical for high accuracy. Our corpus-derived prompts signiﬁcantly outperform if you like <m like <m due to its superior results and its simplicity. We investigate the effect of the number of mentioned movies in prompts. We expect the accuracy of the models in making recommendations to increase when they get more info about movies a user likes. We compare the recommendation accuracy on the same users 0,1,2,3,5,10,15 or 20 movies per prompt. Fig. 2: MAP@1 of LM models with a varying number of movies per user sampled in the input prompt. Figure 2 shows that increasing the number of ratings per user has diminishing returns and lead to increasing instability, so specifying n ≈ 5 seems to lead to the best results with the least user input. After 5 items, adding more items might make the prompt > used in [12]. We will use <m...m>, <m> in the remaining of the paper less natural, even though the LM seems to adapt when the number of items keeps increasing. It is also interesting to note that when we use an empty prompt, accuracy is above chance level because the LM captures some information about movie popularity. We now use a matrix factorization as a baseline, with the Bayesian Personalised Ranking algorithm (BPR) [15]. Users and items are mapped to d randomly initialized latent factors, and their dot product is used as a relevance score trained with ranking loss. We use [16] implementation with default hyperparameters 0.001. We also compare GPT-2 LM to BERT next sentence prediction [12] which models afﬁnity scores with movies liked by u. BERT was pretrained with contiguous sentence prediction task [3] and Penha et al. [12] proposed to use it as a way to probe BERT for recommendation capabilities. Fig. 3: MAP@1 for BPR models with increasing numbers of users compared the zeroshot language models (with 0 training user). BERT-base and BERT-large respectively have 110M and 340M parameters. GPT-2-base and GPT-2-medium have 117M and 345M parameters. https://cornac.readthedocs.io/en/latest/models.html# bayesian-personalized-ranking-bpr, we experimented with other hyperparameter conﬁgurations but did not observe signiﬁcant changes. Figure 3 shows that the proposed LM-based recommendation signiﬁcantly outperforms BERT tually model text likelihood, while next sentence prediction is discriminative and can be based on simple discursive coherence features. It is also interesting to note that LMbased models outperform matrix factorization when there are few users, i.e < 50 and < 100 for BASE and MEDIUM GPT-2, which demonstrates that LM-based recommendation is viable for cold start regimes. Using models larger than the BASE versions lead to better results, however when we evaluated with larger versions (we did not perform the full experiments due to memory limitations), we did not see additional improvement, which could be explained by overﬁtting. Up until there, we have used LM to score the likelihood of sequences. LM can also be used directly for text generation, unlike BERT. We here show LM-generated prompt completions randomly sampled in our dataset, using greedy decoding. Prompt (P1): Forrest Gump, Blade Runner, Modern Times, Amelie, Lord of the Rings The Return of the King, Shaun of the Dead, Alexander, Pan’s Labyrinth, Cashback, Avatar: Completion (C1): 3, The Hunger Games: Mockingjay Part 2, King Arthur, A Feast for Crows, The Hunger Games: Catching Fire, Jackass, Jackass 2, King Arthur Prompt (P2): Independence Day, Winnie the Pooh and the Blustery Day, Raiders of the Lost Ark, Star Wars Episode VI - Return of the Jedi, Quiet Man, Game, Labyrinth, Return to Oz, Song of the South, Matrix: Completion (C2): and many more. The list can be read by clicking on the relevant section at the left of the image. To access the list of releases Some prompts, i.e. (P1) generate valid movie names, but others, like (P2), do not. LM-based recommender do need a post-processing to match movie names in the possible sampled generations. We showed that standard language models can be used to perform item recommendations without any adaptation and that they are competitive with supervised matrix factorization when the number of users is very low (less than 100 users). LM can therefore be used to kickstart recommender systems if items are frequently discussed in the training corpora. Further research could explore ways to adjust LM for recommendation purposes or to combine LM with matrix factorization into hybrid systems. Another way to use of our ﬁndings would be to generate movie recommendation datasets by mining web data which could feed standard supervised recommendation techniques. . We explain the difference by the fact that LM are generative and ac- This work is part of the CALCULUS project, which is funded by the ERC Advanced Grant H2020-ERC-2017 ADG 788506