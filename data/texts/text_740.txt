 commercial product technical designs. However, challenge called the cold-start problem. The problem is so notorious that almost every industrial practitioner needs to resolve this issue when building recommender systems. Most cold-start problem solvers need some kind of data input as the starter of the system. On the other hand, many real-world applications place popular items or random items as recommendation results. In this paper, we propose a new technique called ZeroMat that requries no input data at all and predicts the user item rating data that is competitive in Mean Absolute Error and fairness metric compared with the classic matrix factorization with affluent data, and much better performance than random placement. Keywords—cold start problem; Zipf’s Law; matrix factorization; recommender system; probabilistic matrix factorization acquisition and monetization of web products. More than 30% of products sold on Amazon.com are due to recommendation. It would cost much more money by scale to entice customers for the same amount of purchases via other venues such as conventional ads or Google Adwords. Therefore, most E-commerce companies have spent a tremendous amount of investment in recommender system research and development. Every recommender system receives new users and new items. However, these new data records have no history in the past that facilitate the recommendation. This problem is known as the cold start problem, and is faced by every recommender system builder in the industry. Some products select random items or popular items and recommend them to new users, or use content-based recommendation in certain contexts such as news recommendation to resolve the cold-start problem for new items. Other more rigorous methods utilize algorithms to alleviate the cold-start problem, but they more or less rely on some data inputs as the starter to solve this issue. In this paper, we propose a new method called ZeroMat to resolve this problem. Our method is inspired by Probabilistic Matrix Factorization. By modeling the user item rating distribution using Zipf’s Law, we acquire a new approach that resolves the cold start problem with no need of input data. In the Experiment Section, we illustrate that our method is mostly competitive with the classic matrix factorization with affluent input data. It must be surprising that a recommender system without input data can yield good predictions for userrating-matrix, but the experimental result serves as very strong evidence to support the validity of our method. This might lead to the conclusion that although recommender system has developed for a couple of decades, we are really still not doing very well in accuracy and fairness metrics. Recommender system has a decade-long history of enhancing commercial values for business firms. Famous recommender system techniques such as collaborative filtering [1], matrix factorization [2][3][4], recommendation [7][8] have all been applied on large scales in companies throughout the world [9]. Recommender system faces notorious challenges such as Matthew Effect problem and Cold-start Problem. As a special case of the fairness problem, researchers such as Wang [10][11][12] in recent years, and fairness problem [13][14][15] in general is a hot research topic in many research conferences. Unlike the emerging interest on the Matthew Effect problem, the cold-start problem is usually considered as being more challenging and unsolvable. Very few researchers have shed some insightful light on the topic [16][17][18], and most of them require some sort of data input as the starter to solve the issue. Probabilistic Matrix Factorization problem is a framework equivalent to the classic matrix factorization. The classic matrix factorization is formulated as follows: The Probabilistic Matrix Factorization [19] formulates the problem as a maximum likelihood problem. , and we are looking for the solution that maximizes the following maximum posterior: By taking the natural log of the maximum posterior and simple mathmatical calculation, it can be shown the equivalent to the classic matrix factorization approach with regularization. We formulate ZeroMat by Zipf’s Law as follows: We omit maximum value of the dot product for now and represent the ratio of rating scores as R and formulate the maximum posterior of R below: We further dictate that U and V follows the zero-mean normal distribution as specified in the Probabilistic Matrix Factorization framework. We acquire the following posterior formula: We omit the joint distribution of hyper-parameters and acquire the following formula: Once again, we assume the probability distribution of U and V are zero-mean normal distributions. We plug in the probability distributions into the posterior formula of U and V: After taking the natural log of the posterior of U and V, we obtain the loss function L for optimization: we can solve for the optimal values of U and V that maximize L, i.e.: . Please notice the computation of U and V require no knowledge of the user rating values R. We reconstruct the values of the user ratings using U and V by the following formula: The estimated rating values can be used for cold start problem. The entire algorithmic procedure requires no knowledge of user rating matrix. The only thing we need to know is the maximum value of the ratings. This is usually a product design knowledge that we know beforehand. For example, MovieLens dataset has a rating scale ranging from 1 to 5, then we know is 5. In the next section, we compare our algorithm in technical accuracy metric and fairness metric against the random value placement heuristics and the classic matrix factorization solved by Stochastic Gradient Descent. The metrics that we use are Mean Absolute Error (MAE) for accuracy performance and the Degree of Matthew Effect (Wang [12]) for fairness evaluation. We use the small dataset of MovieLens that include 610 users and 9724 items to test ZeroMat against random placement and the classic matrix factorization solved by Stochastic Gradient Descent. We do a grid search on the gradient learning step to check the overall performance and the robustness of the method (Fig. 1 and Fig. 2) : The small scale dataset demonstrates that our algorithm is competitive with the classic matrix factorization approach. ZeroMat even outperforms the classic matrix factorization in MAE for a large number of parameters, and its worst performance is less than 30% worse than the classic matrix factorization. ZeroMat outperforms random placement by a large margin in MAE. As for fairness metric, the 3 methods are comparable with each other. For large scale dataset (Fig. 3 and Fig. 4), we test our approach on MovieLens 1 million dataset that comprises 6040 users and 3706 items. The result is consistent with MovieLens small dataset (Fig. 1 and Fig. 2). However, due to randomness, in some cases, with competitve MAE score, ZeroMat outperforms the classic matrix factorization in fairness metric by large margin (Fig.5). In this paper, we propose a new method called ZeroMat that requires no data input as the starter to solve the cold start problem. Our approach outperforms the random recommendation by a large margin. Even compared with the classic matrix factorization, our method is competitive in both MAE and fairness metric. The implication is two folds : 1. Our method is an ideal tool for the cold-start problem. 2. Matrix factorization techniques are far from being good enough. In future work, we would like to explore in depth the reason why ZeroMat can outperform the classic matrix factorization technique in a large range of gradient step learning parameters.