Extreme multi-label learning is an active research problem in big data applications with se veral applications in tagging, recommendation, and ranking. Consider a features matrix of n examples X ∈ R of multilabel classiﬁcation problem is to assign some relevant labe ls out of a tota l of ℓ-labe ls to new data sample. The extreme multilabel cla ssiﬁcation problem refers to the setting when n, d, and ℓ quickly scale to large numbers often upto several millions. Moreover, number of data samples also are in several millions . This is one of the classic challenge problems of big data analytics. is found that the average number of labels per data points is usually very small, moreover, it is know that label frequencies follow the so-c alled Zipf’s law, which means that there are only sma ll number of labels which are found in large number of samples, such labels are called head labels. On the other hand, there are large number of lables that occur less freq ue ntly, and such labels are called ta il labels. Such a distribution creates a bias in the classiﬁer, because since head labels occur more frequently, the classiﬁer may learn more robustly a bout predicting head labels compared to tail labels, ther e by, leading to a classiﬁer that is biased towards predicting hea d labels better. Despite all these challenges, one of the majo r concerns is designing scala ble a lgorithms for modern day hardwares. and implementation of extreme classiﬁcation algorithms is essential that can exploit shared as well as distributed memory architectures [10,12,3,13,11]. In this paper , we show a parallel design and implementa tion for a hybrid distributed memory and shared memory implementation using MPI and OpenMP. To the bes t of our knowledge, this is the ﬁrst time a hybrid parallel implementation has been shown for extreme classiﬁcation. We derive the communication bounds fo r distributed memory, and parallelism bound for shared memory implementation. Our pr eliminary numerical experiments suggest that we have fastest training and test times when compared to some of the existing parallel implementations in C/C++. Paper accepted in BDA 2021. Abstract. As a big data application, ex treme multilabel classiﬁcation has emerged as an imp ortant research topic with applications in ranking and recommendation of products and items. A scalable hybrid distributed and shared memory implementation of extreme classiﬁcation for large scale ranking and recommendation is proposed. In particular, the implementation is a mix of message passing using MPI across no des and using multithreading on the nodes using OpenMP. The expression for communication latency and communication volume is derived. Parallelism using work-span model is derived for sh ared memory architecture. This throws light on the expected scalability of similar extreme classiﬁcation metho ds. Experiments show that the implementation is relatively faster to train and test on some large datasets. In some cases, model size is relatively small. Code: https://github.com/misterpawan/DXML Keywords: extreme multilabel classiﬁcation · distributed memory · multithreading. with d-dimensional features with their corresponding la bels matrix Y ∈ R, the aim There are several challenges in designing algorithms for extreme multilabel cla ssiﬁcation. It Looking at the increasing trend of number of labels going upto millions, a hybrid parallel design In section 3, we discuss distributed memory implementation. We derive expressions for communication volume and latency. Finally, in section 4, we discuss numerical exp eriments on multilabel datasets. Some papers for multilabel classiﬁcation were proposed during 2006-2014 [17 ,24]. Some of these papers explored k nearest neighbours [25]. The idea of using random forest were also proposed [9]. With the re cent demand for scaling the algorithms to millions of labels, new class of methods have been proposed, where scalability is achieved by either pa rallelism [21,22], dimension reduction [4,16,19,23], or by hierarchical embedding [6,7,14,20,8]. We s how a hybrid parallel (MPI+OpenMP) implementa tion of CRAFTML [15], and call it DXML. During training, DXML computes a forest F of m by r ecursive partitioning. The training algorithm is shown in Algorithm 1. In line 1, the input is a feature matrix X and a label matrix Y. We wish to build a lab el-tree with nodes denoted by v. We then apply the termination condition. The termination condition of the recursive partitioning are the following 1. cardinality of the node’s instance subset is less than a given threshold n 2. all the given instances have the same features 3. all the given instances have the same labels classiﬁer is built using Algorithm 2. utive steps: – a random projection into lower dimensiona l spaces of the features and label vec tors corre- – from projected labels, partitioning of of the corresponding instances into k temporary s ubsets – A multiclass class iﬁe r is trained to assign each instance to the rele vant temp orary subset common lab els in a same subset, but the computation is diﬀerent. Finally, once a tree has been trained, its leaves store the average label vecto r of their associated instances. The partition strategy is driven by two constraints: partition computation must be based on randomly projected instances to ensure diversity, and must perform low complexity operations for scalability. The training algorithm is given below. Step 1: Random projections of the instances of v: T he feature and the label vectors x and y of each instance in v are projected into a space with a lower dimensionality: x The rest of the paper is organized as folows. In section 2, we discus s previo us related work. If the stop condition is false , and the current node v is not a leaf as in line 4, then a multi-class The sequential node training stage in DXML will be decomposed into following three consecsponding to the node’s instances. That is, in Algorithm 2, in line 2, we ﬁrst sample Xand Yfrom Xand Ywith a sample s iz e n. Then in lines 3 and 4, we do the r andom projection using projection feature matrix P, and a random label projection matrix P. using k−means. (that is, the cluster index found at step 2 above) from the co rresponding feature vector. The classiﬁer then partitiones the instances into k ﬁnal subsets or child nodes. This is achieved in by ﬁrst ca lling a splitting (line 6, Algorithm 1) of the instances into child nodes using output of k-means of Algorithm 2. Then trainTree function is called recursively on the child nodes. Similar to FastXML, the nodes partitioning objective of DXML is to regroup instances with Algorithm 1 trainTree 1: Input: Training set with a feature matrix X and a label matrix Y. 2: Initialize node v. 3: v.isLeaf ← testStopCondition(X, Y ) 4: if v.isLeaf = false then Algorithm 2 trainNodeClassiﬁer 1: Input: feature matrix (X 2: X 3: X 4: Y 5: c ← k-means(Y 9: end for 10: Output: Classiﬁer classif(∈ R where P dare the dimensions of the reduced feature and label spac es resp. The projection matrices are kept diﬀerent from one tree to another. The random projection considered is a sparse orthogonal projection matrix [18] with values of −1 or +1 on each row. The sparsity of the projections lead to faster computations, hence, faster projections. To retain the spa rsity we use the hashing; we describe it next. dimension. Projection is done row after row in this algorithm. In a row of original matrix each element index is considered as a key a nd each element index of corresponding row in the projected matrix is considered as the bucket. Each index o f original row (key) is mapped to index of lower dimension projected row (bucket) using the has h function. Similarly each key is also mapped to one of the two signs (+ or −) using another hash function. Multiple ke ys may be mapped to the same bucket, in that case elements in the same bucket are multiplied by their respective signs (obtained from second hash function) and added. Below is the algorithm of Has hing Trick. , Y← sampleRows(X, Y, nis the sample size ← XP⊲ random feature projection ← YP⊲ random label projection and Pare random projection matrices of Rand Rrespectively, and dand In this so-called hashing trick algorithm, high dimensional dataset is projected into a lower Algorithm 3 Hashing Trick 1: Input: X 2: X 3: p ← projectedSpaceDimension 4: nR ← X 5: nC ← X 6: for i ← 0 to nR : 11: Output: X hash1, hash2 functions respectively), and returns projected samples X the number of rows in X with i iterating over the row indices of X (with index i). In line 8 , the Column Index (key) is hashed using hash1 function and if its more than the projectedSpaceDimension, then remainder when divided by p is considered to map it in the range(0, p). In line 9, 2 × hash2(key)%2 − 1 ma ps key to +1 or −1, and ﬁnally in line 10, the element X X[i, Index]. may have tried diﬀerent projections per node, but we don’t consider that in this pape r. matrix of a sample drawn without replacement. Let the sample size be at most n partitioned with a spherical k-means applied on Y by the facts that it is well-adapted to sparse data, moreover, the cosine metric is fast to compute. stability, and to improve the algorithm per formance against a random initialization. 1. Among the label centers, choose one center uniformly at random. 1. Choos e one center uniformly at random from among the label vectors. 2. For each lab e l vector y, compute the dista nc e D(y), deﬁned to be the distance between y and 3. Choos e one new data point at random as a new center, using a weighted probability distribution 4. Repeat steps 2 and 3 a bove until all centers have been chosen. 5. After all the centers have b een chosen, proceed with the spherical k-means clustering. centroid of the projected feature vectors is computed. During the prediction phase, if the centroid of the subset is closest to the projected fea ture vector, then the classiﬁer assigns this subset. For computing the closeness, the cosine measure is used. which is deter mined by the successive decisions of the clas siﬁer. The prediction is the average label vector store d in the leaf reached. The fo rest then aggregates the tree predictions with the average operator. Algorithm Analysis Let s (label) vectors of the instances . Due to the hashing trick, the projected featur e and label vectors have less than s the number of instances of the subset associated to v. Let i be the number of iterations of the spherical k-means algorithm. [i,Index] ← X[i,Index] + (Sign* X[i,key]) This Algorithm 3 ta kes X( or Y) and the projectedSpaceDimension, and S, SS(seeds for We co nsider the case where feature and label projections are the s ame in each node of T. We Step 2: Partitioning of Instance into k Temporary Subsets : Let Ybe the label The cluster centroids are initialized using the k-means+ + strategy to amelior ate the cluster The kmeans++ initialization strategy is a s follows: the closest center that has already been chosen ab ove. where a point y is chosen with probability propo rtional to D(y). Step 3: Assigning a subset from the projected features: In each temporary subse t the Prediction: In the prediction phase, for each tree, the input sample goes from root to leaf, Lemma 1. For a node v of a tree T, the t ime complexity C is the complexity per instance. Proposition 1. If the tree T is balanced, its training time complexity C Proof. See [15]. and d complexity are further reduced. Proposition 2. The memory complexity of a tree T is bounded by Proof. See [15]. We use message passing interfa ce MP I [1] to train for each learner . Each lea rner or process r eads the data, and calls trainTree in Algorithm 1. The classiﬁcation for each node (child) at a given level is processed by multiple threads. Each learner stores their own mea n labe l vector computed in line 11 of Algorithm 1. The model parameters for each tree is sent to master node for faster prediction. L e t n volume from the worker node s to the mas ter node is O(m of messages pas sed. In this case, each processor communicates once to master node. Hence the latency cost is O(m processo rs as number of trees in the forest, P = m We also exploit share d memory parallelism using Op enMP [2] when training each tree. This follows a spawn-sync mode l. At root, a master thread launches k child processes, and each of the k child process calls trainNodeClassiﬁer. We use work-span model [5] to do parallel complexity analy sis of trainTree. The span denoted by T from root no de to a leaf node. Now we calculate the work deno ted by T done to train the classiﬁer for all the nodes. We deﬁne the parallelism to be T following proposition. Let T be a stric tly k-ary tree and ℓbe its number of leaves. Let m=ℓ− 1k − 1be its number It can be o bserved that the time complexities are independent of the projection dimensions d . The c lustering is done after sampling from the ins tances, thus the training and clustering Proposition 4. Let T be a strictly k−ary tree, ℓ deﬁned as above, then the parallelism for a tree is bounded by Proof. From 1, the total work done which is denoted by C is given by the following bound We did our MPI+OpenMP experiments on Intel Xeon architecture with 10 nodes with 120GB RAM. We used 10 MPI processes and 5 threads per processes. We choose m The feature and label projection dimensions are d We show the precision scores in Table 2. The best precision scores among the tree-based classiﬁers are indicated in bold. Fo r example, the DXML has highest P@1 precision score s for Mediamill, EURLex-4K, Delicious-200K among the tree based classiﬁers. In other cases, it is clos e to the precisions of other classiﬁers. In Table 1, we show the train time, test time, and mo de l size. The train times for DXML was best among all methods for all the datasets. The model s iz e for DXML is same as the one for CRAFTML. The model size for DXML/CRAFTML was be st for Amaz on670 and Delicious-200K. The model size for PPDSp is very large for Amaz on-670 . For DXML, the learned model parameters r e main distributed, hence, there is an additional cost of reduction operation. For lar ge scale r e commendation problem using extreme multilabel classiﬁcation, a scalable recommender model is essential. We proposed a hybrid parallel implementation of extreme classiﬁcation using MPI and OpenMP that can scale to arbitrary number of processors. The best part is that this doe s not involve any loss function or iterative gradient methods. Our preliminary re sults show that our training time is fastest for some datasets. We derived the communication complexity analysis bounds for both the shared and distributed memory implementations. With more cores, our parallel analysis suggests that the proposed implementation has the potential to scale fur ther. Table 1. Train Time, Test Time, an d Model Size. Here NA means not available. This work was done at I IIT, Hyderabad us ing IIIT seed grant. The author acknowledges all the support by institute.