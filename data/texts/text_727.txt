sively deployed to alleviate information overload in various web services, such as social media, E-commerce websites and news portals. To predict the likelihood of a user adopting an item, collaborative ﬁltering (CF) is the most widely adopted principle. The most common paradigm for CF, such as matrix factorization [36] and neural collaborative ﬁltering [27], is to learn the user/item embeddings based on the user-item interactions. However, these models fail to learn high-quality embeddings for the cold-start users/items with sparse interactions. To address the cold-start problem, traditional recommender systems incorporate the side information such as content features of users and items [25], [26] or external knowledge graphs (KGs) [4], [5] to compensate the lowquality embeddings caused by sparse interactions. However, the content features are not always available, and it is not easy to link the items to the entities in KGs due to the incompleteness and ambiguities of the entities. Another research line is to adopt the Graph Neural Networks (GNNs) to solve the problem, examples include PinSage [15], NGCF [16] and LightGCN [17]. The basic idea is to incorporate the high-order neighbors to reﬁne the representations of the users/items. However, the GNN models for recommendation can not thoroughly solve the cold-start problem, as the embeddings of the cold-start users/items aren’t explicitly optimized, and the cold-start neighbors have not been considered in these GNNs. In our previous work, we propose PT-GNN [30], which pre-trains the GNN model to reconstruct the user/item embeddings under the meta-learning setting [8]. To further reduce the impact from the cold-start neighbors, PT-GNN incorporates a self-attention based meta aggregator to enhance the aggregation ability of each graph convolution step, and an adaptive neighbor sampler to select proper high-order neighbors according to the feedbacks from the GNN model. However, PT-GNN still suffers from the following challenges: 1) In terms of the model architecture, PT-GNN suffers from lacking the ability to capture long-range dependencies. Existing researches such as Chen et al. [46] pointed out that GNN can only capture up to 3-hop neighbors due to the overﬁtting and over-smoothing problems [17]. However, in the cold-start scenario, both low-order neighbors (L ≤ 2, where L is the convolution layer) and high-order neighbors (L > 2) are important to the target cold-start user/item. On one hand, the low-order neighbors are crucial for representing the target cold-start users/items, as the ﬁrst-order neighbors directly reﬂect the user’s preference or the item’s target audience, and the second-order neighbors directly reﬂect the signals of the collaborative users/items. On the other hand, due to the extremely sparse interactions of the cold-start users/items, the ﬁrst- and second-order neighbors are insufﬁcient to represent a user or an item. Thus, for the target users/items, it is crucial to capture not only the short-range dependencies from their low-order neighbors, but also the long-range dependencies from their Fig. 1: The improvement of MPT on the original PT-GNN. X denotes the capacity of PT-GNN and %denotes the high-order neighbors. Nevertheless, due to the limitation of the network structure, most of the existing GNN models can only capture the short-range dependencies, which inspires the ﬁrst research question: how to capture both short- and long-range dependencies of users/items? 2) In terms of the pretext task, PT-GNN only considers the intra-correlations within the subgraph of the target user or item, but ignores the inter-correlations between the subgraphs of different users or items. More concretely, given a target cold-start user or item, PT-GNN samples its high-order neighbors to form a subgraph, and leverages the subgraph itself to reconstruct the cold-start embedding. Essentially, the embedding reconstruction task is a generative task, which focuses on exploring intra-correlations between nodes in a subgraph. On the contrary, some recent pre-training GNN models (e.g., GCC [20], GraphCL [54]) depending on the contrastive learning mechanism, can capture the similarities of nodes between different subgraphs, i.e., the intercorrelations. Thus, this inspires the second research question: how to capture the inter-correlations of the target cold-start users/items in addition to the intra-correlations? Present work. We propose a Multi-strategy based PreTraining method for cold-start recommendation (MPT), which extends PT-GNN from the perspective of model architecture and pretext tasks to improve the cold-start recommendation performance. The improvements over PTGNN are shown in Fig. 1. First, in terms of the model architecture, in addition to the original GNN encoder which captures the short-range dependencies from the user-item edges in the user-item graph, we introduce a Transformer encoder to capture the long-range dependencies from the user-item paths, which can be extracted by performing random walk [41] starting from the target user or item in the user-item graph. The multi-head self-attention mechanism in the Transformer attends nodes in different positions in a path [37], which can explicitly capture the long-range dependencies between users and items. Besides, we change the RL-based neighbor sampling strategy in the original GNN encoder into a simple yet effective dynamic sampling strategy, which can reduce the time complexity of the neighbor sampling process. Second, in terms of the pretext task, in addition to the original embedding reconstruction task which considers the intra-correlations within the subgraph of the target user or item, we add embedding contrastive learning [50] task to capture the inter-correlations across the subgraphs or paths of different users or items. Speciﬁcally, we ﬁrst augment a concerned subgraph or path by deleting or replacing the nodes in it. Then we treat the augmented subgraphs or paths of the concerned user or item as its positive counterparts, and those of other users or items as the negative counterparts. By contrastive learning upon the set of the positive and negative instances, we can pull the similar user or item embeddings together while pulling away dissimilar embeddings. We train the GNN and Transformer encoders by the reconstruction and contrastive learning pretext tasks under the meta-learning setting [8] to simulate the cold-start scenario. Speciﬁcally, following PT-GNN, we ﬁrst pick the users/items with sufﬁcient interactions as the target users/items, and learn their ground-truth embeddings on the observed abundant interactions. Then for each target user/item, in order to simulate the cold-start scenario, we mask other neighbors and only maintain K ﬁrst-order neighbors, based on which we form the user-item subgraph and use random walk [41] to generate the paths of the target user or item. Next we perform graph convolution multiple steps upon the user-item subgraph or self-attention mechanism upon the path to obtain the target user/item embeddings. Finally, we optimize the model parameters with the reconstruction and contrastive losses. We adopt the pre-training & ﬁne-tuning paradigm [20] to train MPT. During the pre-training stage, in order to capture the correlations of users and items from different views (i.e., the short- and long-range dependencies, the intra- and inter-correlations), we assign each pretext task an independent set of initialized embeddings, and train these tasks independently. During the ﬁne-tuning state, we fuse the pre-trained embeddings from each pretext task and ﬁnetune the encoders by the downstream recommendation task. The contributions can be summarized as: chitecture. In addition to the short-range dependencies of users and items captured by the GNN encoder, we add a Transformer encoder to capture long-range dependencies. In addition to considering the intra-correlations of users and items by the embedding reconstruction task, we add embedding contrastive learning task to capture intercorrelations of users and items. extrinsic downstream tasks demonstrate the superiority of our proposed MPT model against the state-of-the-art GNN model and the original proposed PT-GNN. In this section, we ﬁrst deﬁne the problem and then introduce the original proposed PT-GNN . Bipartite Graph. We formalize the user-item interaction data for recommendation as a bipartite graph and denoted it as G = (U, I, E), where U = {u, · · · , u} is the set of users and I = {i, · · · , i} is the set of items. U and I comprise two types of the nodes in G. E ⊆ U ×I denotes the set of edges that connect the users and items. We use N(u) to represent the l-order neighbors of user u. When ignoring the superscript, N (u) indicates the ﬁrst-order neighbors of u. N(i) and N (i) are deﬁned similarly for items. User-Item Path. The user-item path is generated by the random walk strategy from the user-item interaction data, and has the same node distribution with the graph G, as Perozzi et al. [41] proposed. Formally, there exists two types of paths: P = UIUI or P = IUIU, where U denotes the node type is user and I denotes the node type is item. Problem Deﬁnition. Let f : U ∪ I → Rbe the encoding function that maps the users or items to d-dimension realvalued vectors. We denote a user u and an item i with initial embeddings hand h, respectively. Given the bipartite graph G or the path P, we aim to pre-train the encoding function f that is able to be applied to the downstream recommendation task to improve its performance. Note that for simplicity, in the following sections, we mainly take user embedding as an example to explain the proposed model, as item embedding can be explained in the same way. 2.2 A Brief Review of PT-GNN The basic idea of PT-GNN is to leverage the vanilla GNN model as the encoder f to reconstruct the target cold-start embeddings to explicitly improve their embedding quality. To further reduce the impact from the cold-start neighbors, PT-GNN incorporates a self-attention based meta aggregator to enhance the aggregation ability of each graph convolution step, and an adaptive neighbor sampler to select proper high-order neighbors according to the feedbacks from the GNN model. 2.2.1 Basic Pre-training GNN Model The basic pre-training GNN model reconstructs the coldstart embeddings under the meta-learning setting [8] to simulate the cold-start scenario. Speciﬁcally, for each target user u, we mask some neighbors and only maintains at most Kneighbors (K is empirically selected as a small number, e.g., K=3) at the l-th layer. Based on which we perform graph convolution multiple steps to predict the target user embedding. Take GraphSAGE [29] as the backbone GNN model as an example, the graph convolution process at the l-th layer is: where his the reﬁned user embedding at the l-th convolution step, his the previous user embedding (his the initialized embedding). his the averaged embedding of the neighbors, in which the neighbors are sampled by the random sampling strategy. σ is the sigmoid function, Wis the parameter matrix, and || is the concatenate operation. We perform graph convolution L-1 steps, aggregate the reﬁned embeddings of the ﬁrst-order neighbors {h, · · · , h} to obtain the smoothed embedding h, and transform it into the target embedding h, i.e., h= AGGREGATE({h, ∀i ∈ N (u)}), h= σ(W· h). Then we calculate the cosine similarity between the predicted embedding hand the ground-truth embedding hto optimize the parameters of the GNN model: where the ground-truth embedding his learned by any recommender algorithms (e.g., NCF [27], LightGCN [17]), Θ is the model parameters. Similarly, we can obtain the item embedding hand optimize model parameters by (Eq. (2)). 2.2.2 Enhanced Pre-training Model: PT-GNN The basic pre-training strategy has two problems. On one hand, it can not explicitly deal with the high-order cold-start neighbors during the graph convolution process. On the other hand, the GNN sampling strategies such as random sampling [29] or importance sampling [55] strategies may fail to sample high-order relevant cold-start neighbors due to their sparse interactions. To solve the ﬁrst challenge, we incorporate a meta aggregator to enhance the aggregation ability of each graph convolution step. Speciﬁcally, the meta aggregator uses selfattention mechanism [37] to encode the initial embeddings {h, · · · , h} of the K ﬁrst-order neighbors for u as input, and outputs the meta embedding˜hfor u. The process is: {h, · · · , h} ← SELF ATTENTION({h, · · · , h}), We use the same cosine similarity described in Eq. (2) to measure the similarity between the predicted meta embedding˜hand the ground truth embedding h. To solve the second challenge, we propose an adaptive neighbor sampler to select proper high-order neighbors according to the feedbacks from the GNN model. The adaptive neighbor sampler is formalized as a hierarchical Markov Sequential Decision Process, which sequentially samples from the low-order neighbors to the high-order neighbors and results in at most Kneighbors in the l-th layer for each target user u. After sampling the neighbors of the former L layers, the GNN model accepts the sampled neighbors to produce the reward, which denotes whether the sampled neighbors are reasonable or not. Through maximizing the expected reward from the GNN model, the adaptive neighbor sampler can sample proper high-order neighbors. Once the meta aggregator and the adaptive neighbor sampler are trained, the meta embedding˜hand the averaged sampled neighbor embedding˜hat the l-th layer are added into each graph convolution step in Eq. (1): For the target user u, Eq. (4) is repeated L−1 steps to obtain the embeddings {h, · · · , h} for its K ﬁrst-order neighbors, then the predicted embedding his obtained by averaging the ﬁrst-order embeddings. Finally, Eq. (2) is used to optimize the parameters of the pre-training GNN model. The pre-training parameter set Θ = {Θ, Θ, Θ}, where Θis the parameters of the vanilla GNN, Θis the parameters of the meta aggregator and Θis the parameters of the neighbor sampler. The item embedding hcan be obtained in a similar way. We ﬁne-tune PT-GNN in the downstream recommendation task. Speciﬁcally, for each target user u and his neighbors {N(u), · · · , N(u)} of different order, we ﬁrst use the pre-trained adaptive neighbor sampler to sample proper high-order neighbors {ˆN(u),ˆN(u) · · · ,ˆN(u)}, and then use the pre-trained meta aggregator to produce the user embedding h. The item embedding hcan be obtained in the same way. Then we transform the embeddings to calculate the relevance score between a user and an item, i.e., y(u, i) = σ(W · h)σ(W · h) with parameters Θ= {W}. Finally, we adopt the BPR loss [17] to optimize Θand ﬁne-tune Θ: However, as shown in Fig. 1, due to the limitation of the GNN model, PT-GNN can only capture the short-range dependencies of users and items; besides, the embedding reconstruction task only focuses on exploring the intracorrelations within the subgraph of the target user or item. Therefore, it is necessary to fully capture both short- and long-range dependencies of users and items, and both intraand inter-correlations of users or items. We propose a novel Multi-strategy based Pre-Training method (MPT), which extends PT-GNN from the perspective of model architecture and pretext tasks. Speciﬁcally, in terms of the model architecture, in addition to using the GNN encoder to capture the short-range dependencies of users and items, we introduce a Transformer encoder to capture long-range dependencies. In terms of the pretext task, in addition to considering the intra-correlations of users and items by the embedding reconstruction task, we add embedding contrastive learning task to capture intercorrelations of users and items. Hence, by combining each architecture and each pretext task, there are four different implementations of pretext tasks, as shown in Fig. 2. We detail each pretext task implementation in Section 3.1 - Section 3.4, and then present the overall model training process in Section 3.5. Finally, we analyze the time complexity of the pretext task implementation in Section 3.6. 3.1 Reconstruction with GNN Encoder In this pretext task, we primarily follow the original PTGNN model to reconstruct the user embedding, as proposed in Section 2.2. We modify PT-GNN in terms of its neighbor sampling strategy to reduce the time complexity. The adaptive neighbor sampling strategy in PT-GNN suffers from the slow convergence issue, as it is essentially a Monto-Carlo based policy gradient strategy (REINFORCE) [32], [33], and has the complex action-state trajectories for the entire neighbor sampling process. To solve this problem, inspired by the dynamic sampling theory that uses the current model itself to sample instances [38], [39], we propose the dynamic sampling strategy, which samples from the low-order neighbors to the high-order neighbors according to the enhanced embeddings of the target user and each neighbor. The enhanced embedding for the target user (or each neighbor) is obtained by concatenating the meta embedding produced by the meta aggregator in PTGNN (Section 2.2.2) and the current trained embedding. The meta embedding enables considering the cold-start characteristics of the neighbors, and the current trained embedding enables dynamically selecting proper neighbors. Formally, at the l-th layer, the sampling process is: {h, · · · , h} ← S TOP(a, · · · , a, · · · , a), where || is the concatenate operation, ais the cosine similarity between the enhanced target user embedding˜h|| h and the enhanced j-th neighbor embedding˜h|| h,˜hand ˜hare the meta embeddings produced by the meta aggregator, hand hare the current trained user embedding, {h, · · · , h} is the top Krelevant neighbors. S TOP is the operation that selects top neighbor embeddings with top cosine similarity. Compared with the adaptive sampling strategy in PT-GNN, the proposed dynamic sampling strategy not only speeds up the model convergence, but also has competitive performance. As it does not need multiple sampling process, and considers the cold-start characteristics of the neighbors during the sampling process. Once the neighbors are sampled, at the l-th layer, we use the meta embedding˜h, the previous user embedding hand the averaged dynamically sampled neighbor embedding˜hto perform graph convolution, as shown in Eq. (4). Then we perform graph convolution L steps to obtain the predicted user embedding h, and use Eq. (2) to optimize the model parameters. Fig. 2(a) shows this task, and the objective function is as follows: 3.2 Contrastive Learning with GNN Encoder In this pretext task, we propose to contrast the user embedding produced by the GNN encoder across subgraphs, as shown in Fig. 2(b). similar as PT-GNN, we also train this task under the meta-learning setting to simulate the coldstart scenario. Speciﬁcally, we also select users with abundant interactions, randomly select at most K(1 ≤ l ≤ L) neighbors at layer l to form the subgraph G. Then we perform contrastive learning (CL) upon Gto learn the representations of users. In the following part, we ﬁrst present the four components in the CL framework and then detail the key component, i.e., the data augmentation operation. subgraph Gby deleting or replacing the users or items in it, which results in two random augmentations˜G = AUG(G, seed) and˜G= AUG(G, seed), where seedand seedare two random seeds. Following SimCLR [35], we evaluate individual or compositional data augmentation operations. steps using Eq. (4) to generate the representation of the target user for each augmented subgraph. i.e., h= tations hand hinto two vectors z= g(h), z= g(h). This operation is the same with SimCLR [35], as its observation that adding a nonlinear projection head can signiﬁcantly improve the representation quality. ment between positive augmentation pair ( ˜s, ˜s) in the set {˜s}. We construct the set {˜s} by randomly augmenting twice for all users in a mini-batch s (assuming s is with size N), which gets a set ˜s with size 2N. The two variants from the same original user form the positive pair, while all the other instances from the same minibatch are regarded as negative samples for them. Then the contrastive loss for a positive pair is deﬁned as: l(m, n) = − logexp(cos(z, z)/τ)P whereis the indicator function to judge whether k 6= m, τ is a temperature parameter, and cos(z, z) = zz/(||z||||z||) denotes the cosine similarity of two vector zand z. The overall contrastive loss L deﬁned in a mini-batch is: whereis a indicator function returns 1 when m and n is a positive pair, returns 0 otherwise, Θis the parameters of the CL framework. The key method in CL is the data augmentation strategy. In recommender system, since each neighbor may play an essential role in expressing the user proﬁle, it remains unknown whether data augmentation would beneﬁt the representation learning and what kind of data augmentation could be useful. To answer these questions, we explore and test two basic augmentations, deletion and substitution. We believe there exist more potential augmentations, which we will leave for future exploration. items and delete them. If a parent user or item is deleted, its child users or items are all deleted. users or items. For each user u or item i in the selected list, we randomly replace u or i with one of its parent’s interacted ﬁrst-order neighbors. Note that we keep a and b as the same ratio and leave the tuning of different ratios in the future work. Once the CL framework is trained, we leave out other parts and only maintain the parameters of the trained GNN encoder. When a new cold-start user u comes in, same as Section 3.1, the GNN encoder performs graph convolution L steps to obtain the enhanced embedding h. 3.3 Reconstruction with Transformer Encoder In this pretext task, we propose using Transformer encoder to reconstruct the embeddings of target users in the useritem path, as shown in Fig. 2(c). This pretext task is also trained under the meta-learning setting. Speciﬁcally, similar to PT-GNN, we ﬁrst choose abundant users and use any recommender algorithms to learn their ground-truth embeddings. Then for each user, we sample K(1 ≤ l ≤ L) neighbors, based on which, we use random walk [41] to obtain the user-item path P = IUIU (or P = UIUI) with path length T . Finally, we mask the target user in the input path with special tokens “[mask]”, and use Transformer encoder to predict the target user embedding. Formally, given an original path P=[x, · · · , x], where each node xin P represents the initial user embedding hor the initial item embedding h. We ﬁrst construct a corrupted pathˆP by replacing the target user token in P to a special symbol ”[mask]” (suppose the target user is at the t-th position inˆP). Then we use Transformer encoder to map the input pathˆP into a sequence of hidden vectors Tr(ˆP) = [Tr(ˆP), · · · , Tr(ˆP)]. Finally, we fetch the t-th embedding in Tr(ˆP) to predict the ground-truth embedding, and use cosine similarity to optimize the parameters Θof the Transformer encoder. For simplicity, we use notation hto represent the t-th predicted embedding, i.e., Tr(ˆP)= h. The objective function is: Once the Transformer encoder is trained, we can use it to predict the cold-start embedding. When a new cold-start user u with his interacted neighbors comes in, we ﬁrst use random walk to generate the path set {P, · · · , P, · · · , P}, where in the t-th path P, u is in the t-th position. Then we replace u with the ”[mask]” signal to generate the corrupted pathˆP. Next we feed all the corrupted paths {ˆP, · · · ,ˆP} into the Transformer encoder, obtain the predicted user embeddings {h, · · · , h}. Finally, we average these predicted embeddings to obtain the ﬁnal user embedding h. 3.4 Contrastive Learning with Transformer Encoder In this task, we propose to contrast the user embedding produced by the Transformer encoder across different paths, as shown in Fig. 2(d). We train this task under the metalearning setting. Same as Section 3.3, we choose abundant users, sample K(1 ≤ l ≤ L) order neighbors, and use random walk to obtain the path P. Then we perform the CL framework to learn the cold-start user embedding: P=[x, · · · , x] by randomly deleting or replacing the users or items in it, i.e.,˜P= AUG(P, seed) and˜P= AUG(T, seed). Similar to Section 3.2, we evaluate individual or compositional data augmentation operations. encodes the target user from two augmented paths into latent vectors, i.e., h= Tr(˜P) and h= Tr(˜P). mentations hand hinto two vectors z= g(h), ment between positive augmentation pair ( ˜s, ˜s) in the set {˜s}. Same as Section 3.2, we also construct the set {˜s} by randomly augmenting twice for all users in a mini-batch s to get a set ˜s with size 2N , use the two variants from the same original user as positive pair, use all the other instances from the same mini-batch as negative samples, and use Eq. (8) as the contrastive loss l(m, n) for a positive pair. Similar to Eq. (9), the overall contrastive loss Ldeﬁned in a mini-batch is: where Θis the parameters of the CL framework. The data augmentation strategies are as follows: and items and delete them. users and items. For each user u or item i in the selected list, we randomly replace u or i with one of its parent’s interacted ﬁrst-order neighbors. Once the CL framework is trained, we leave out other parts and only maintain the parameters of the Transformer encoder. When a new cold-start user u with his interacted neighbors comes in, same as Section 3.3, we generate the path set {P, · · · , P}, use Transformer encoder to obtain the encoded embeddings {h, · · · , h}, and average these embeddings to obtain the ﬁnal embedding h. We adopt the pre-training and ﬁne-tuning paradigm [20] to train the GNN and Transformer encoders. During the pre-training stage, we independently train each pretext task using the objective functions Eq. (7), Eq. (9), Eq. (10) and Eq. (11) to optimize the parameters {Θ, Θ, Θ, Θ}. We assign each pretext task an independent set of initialized user and item embeddings, and do not share embeddings for these pretext tasks. Therefore, we can train these pretext tasks in a fully parallel way. During the ﬁne-tuning process, we initialized the GNN and Transformer encoders with the trained parameters, and ﬁne-tune them via the downstream recommendation task. Speciﬁcally, for each target cold-start user and his interacted neighbors {N(u), · · · , N(u)} of each order, we ﬁrst use the trained GNN and Transformer encoders corresponding to each pretext task to generate the user embedding h, h, hand h. Then we concatenate the generated embeddings and transform them into the ﬁnal user embedding: where Θ= {W} is the parameter matrix. We generate the ﬁnal item embedding hin a similar way. Next we calculate the relevance score as the inner product of user and item ﬁnal embeddings, i.e., y(u, i) = hh. Finally, we use BPR loss deﬁned in Eq. (5) to optimize Θand ﬁne-tune 3.6 Discussions As we can see, the GNN and Transformer encoders are the main components of the pretext tasks. For the GNN encoder, the time complexity is O(T+ T), where O(T) represents the time complexity of the layer-wise propagation of the GNN model, and O(T) represents the time complexity of the sampling strategy. Since different GNN model has different time complexity O(T), we select classic GNN models and show their O(T). LightGCN [17]: O(|R| + T), NGCF [16]: O(|R| d+ T), GCMC [49]:O(|R| d+ T), GraphSAGE [29]: O(|N| + T), where |R| denotes the number of nonzero entries in the Laplacian matrix, |N| is the number of totally sampled instances and d is the embedding size. We present the time complexity of dynamic sampling strategy O(T) = O(E ∗ d ∗ |N|) and the adaptive sampling strategy O(T) = O(E ∗ M ∗ d∗|N|), where E is the number of convergent epochs, M is the sampling times. Compared with adaptive sampling strategy, the dynamic strategy does not need multiple sampling time M, and has fewer convergent epochs E, thus has smaller time complexity. For the Transformer encoder, the time complexity is O(T∗ d), where T is the length of the user-item path. Following PT-GNN [30], we conduct intrinsic evaluation task to evaluate the quality of user/item embeddings, and extrinsic task to evaluate the cold-start recommendation performance. We answer the following research questions: cold-start recommendation compared with the state-ofthe-art GNN and pre-training GNN models? both intrinsic and extrinsic evaluation? of the proposed MPT model? 4.1 Experimental Settings 4.1.1 Datasets We evaluate on three public datasets MovieLens-1M (Ml1M)[42], MOOCs[43] and Gowalla[45]. Table 1 illustrates the statistics of these datasets. 4.1.2 Comparison Methods We select three types of baselines, including the state-of-theart neural matrix factorization model, the GNN models and the self-supervised graph learning models. combines Multi-layer Perceptron and matrix factorization to learn the embeddings of users and items. samples neighbors randomly and aggregates them by the AVERAGE function. learn the embeddings of users and items. GNN model [56], but additionally adds second-order interactions during the message passing process. nonlinear activation functions in NGCF. graphs from multiple views, where node dropout, edge dropout and random walk are adopted to generate these views. We ﬁnd edge dropout has the best performance. pretext task to explicitly improve the embedding quality. For each vanilla GNN model (e.g., GCMC, NGCF), notation GNNmeans we apply GNN in PT-GNN, and notation GNNmeans we apply GNN in MPT. Since SGL adopts multi-task learning paradigm for recommendation, for fair comparison, we compare it in the extrinsic recommendation task and use notation GNN-SGL to denote it. 4.1.3 Intrinsic and Extrinsic Settings. We divide each dataset into the meta-training set Dand meta-test set D. We train and evaluate the proposed MPT model in the intrinsic user/item embedding inference task on D. Once the model is trained, we ﬁne-tune it in the extrinsic recommendation task and evaluate it on D. We select the users/items from each dataset with sufﬁcient interactions as the target users/items in D, as the intrinsic evaluation needs the true embeddings of users/items inferred from the sufﬁcient interactions. In the cold-start user scenario, we divide the users with the number of the direct interacted items more than ninto Dand leave the rest users into D. We set nas 25, 10 and 25 for the dataset Ml-1M, MOOCs and Gowalla, respectively. Similarly, in the cold-start item scenario, we divide the items with the number of the direct interacted users more than ninto D and leave the rest items into D, where we set nas 15, 10 and 15 for Ml-1M, MOOCs and Gowalla, respectively. We set K as 3 and 8 in the intrinsic task, and 8 in the extrinsic task. By default, we set d as 32, the learning rate as 0.003, Kas 3, L as 4, T as 6, a as 0.2, b as 0.2 and τ as 0.2. 4.1.4 Intrinsic Evaluations: Embedding Inference We conduct the intrinsic evaluation task, which aims to infer the embeddings of cold-start users and items by the proposed MPT model. Both the evaluations on user embedding inference and item embedding inference are performed. Training and Test Settings. We perform intrinsic evaluation on the meta-training set D. Speciﬁcally, same as PT-GNN, we train NCF [27] to get the ground-truth embeddings for the target users/items in D. We also explore whether MPT is sensitive to the ground-truth embedding generated by other models such as LightGCN [17]. We randomly split Dinto the training set T rainand the test set T estwith a ratio of 7:3. To mimic the cold-start users/items on T est, we randomly keep K neighbors for each user/item, which results in at most Kneighbors (1 ≤ l ≤ L) for each target user/item. Thus T estis changed into T est. We train NCF transductively on the merged dataset T rainand T est. We train the vanilla GNN models by BPR loss [17] on T rain. We train PT-GNN on T rain, where we ﬁrst perform Eq. (4) L-1 steps to obtain the reﬁned ﬁrst-order neighbors, and then average them to reconstruct the target embedding; ﬁnally we use Eq. (2) to measure the quality of the predicted embedding. We train MPT on T rain, where we ﬁrst perform four pretext tasks by Eq. (7), Eq. (9), Eq. (10) and Eq. (11), and then use Eq. (12) to fuse the generated embeddings; ﬁnally we use Eq. (2) to measure the quality of the fused embedding. Note that the embeddings in all the models are randomly initialized. Following [44], [57], we use Spearman correlation to measure the quality of learned embeddings, as Lazaridou et al. [57] pointed out Spearman correlation can measure the agreement between the ground-truth human annotations and the machine-generated ones. We apply the pre-training GNN model into the downstream recommendation task and evaluate its performance. Training and Testing Settings. We consider the scenario of the cold-start users and use the meta-test set Dto perform recommendation. For each user in D, we select top c% (c=0.2 by default) of his interacted items in chronological order into the training set T rain, and leave the rest items into the test set T est. We pre-train our model on Dand ﬁne-tune it on T rainaccording to Section 3.5. The vanilla GNN and the NCF models are trained by the BPR loss on Dand T rain. For each user in T est, we calculate his relevance score to each of the rest 1-c% items. We adopt Recall@K and NDCG@K as the metrics to evaluate the items ranked by the relevance scores. By default, we set K as 20 for Ml-1M, MOOCs and Gowalla. 5.1 Performance Comparison (RQ1) We report the overall performance of intrinsic embedding inference and extrinsic recommendation tasks in Table 2 and Table 3. We ﬁnd that: vanilla GNN model, which indicates the effectiveness of the pre-training strategy. Compared with the pre-training model PT-GNN (denoted as GNN) and SGL (denoted as GNN-SGL), MPT also performs better than them. This indicates the superiority of simultaneously considering the intra- and inter- correlations of nodes as well as the short- and long-range dependencies of nodes. of all the baseline methods drops by a large scale, while MPT drops a little. This indicates MPT is able to learn high-quality embeddings for users or items that have extremely sparse interactions. Fig. 3: Intrinsic and extrinsic task evaluation under individual or composition of pretext tasks. K=3, L=4, T =6, c=20%. We evaluate the variant models on MOOCs (results on Ml-1M and Gowalla show the same trend which are omitted for space). Fig. 4: Cold-start recommendation under different nand c. 5.1.2 Interacted Number and Sparse Rate Analysis It is still unclear how does MPT handle the cold-start users with different interacted items nand sparse rate c. To this end, we change the interaction number nin the range of {5, 10, 15} and the sparse rate c in the range of {20%, 40%, 60%}, select LightGCN as the backbone GNN model and report the cold-start recommendation performance on MOOCs dataset in Fig. 4. The smaller nand c are, the cold-start users in Dhave fewer interactions. We ﬁnd that: to all the other baselines, which justiﬁes the superiority of MPT in handling cold-start recommendation with different nand c. provement compared with other baselines, which veriﬁes its capability to solve the cold-start users with extremely sparse interactions. provement compared with other baselines, which again veriﬁes the superiority of MPT in handling cold-start users with different sparse rate. 5.2 Ablation Study (RQ2) 5.2.1 Impact of Pretext Tasks It is still not clear which part of the pretext tasks is responsible for the good performance in MPT. To answer this question, we apply LightGCN in MPT, perform individual or compositional pretext tasks, report the performance of intrinsic and extrinsic task in Fig. 3, where notation R, C, Rand Cdenote the pretext task of reconstruction with GNN, contrastive learning with GNN, reconstruction with Fig. 5: Comparison among different sampling strategies. Transformer and contrastive learning with Transformer, respectively; notation LightGCN-Rmeans we only discard reconstruction task and use the other three pretext tasks. Other variant models are named in a similar way. Aligning Table 2 with Fig. 3, we ﬁnd that: (1) Combining all the pretext tasks can beneﬁt both embedding quality and recommendation performance. This indicates simultaneously consider intra- and inter-correlations of users and items can beneﬁt cold-start representation learning and recommendation. (2) Performing contrastive learning with only GNN, Transformer encoder or their combinations performs not good in the intrinsic task, but has satisfactory performance in the recommendation task. The reason is that contrastive learning does not focus on predicting the target embedding, but can capture the inter-correlations of users or items, and thus can beneﬁt the recommendation task. 5.2.2 Impact of the Sampling Strategy We study the effect of the proposed dynamic sampling strategy. For fair comparison, we compare four variants of the LightGCN model: LightGCNthat adaptively samples neighbors [30], LightGCNthat samples neighbors according to the importance sampling strategy [55], LightGCN that randomly samples neighbors [29] and LightGCNthat dynamically samples neighbors. We report the recommen- Fig. 6: Sensitive analysis of ground-truth embeddings. dation performance on MOOCs and Ml-1M in Fig. 5. We ﬁnd that: (1) Although the adaptive sampling strategy has competitive performance than the random and important sampling strategies, it has slow convergence speed due to the complex RL-based sampling process. (2) Compared with the other sampling strategies, the proposed dynamic sampling strategy not only has the best performance, but also has quick convergence speed. 5.3 Study of MPT (RQ3) 5.3.1 Effect of Ground-truth Embedding As mentioned in Section 2, when performing embedding reconstruction with GNN and Transformer encoders, we choose NCF to learn the ground-truth embeddings. However, one may consider whether MPT’s performance is sensitive to the ground-truth embedding. To this end, we use the baseline GNN models to learn the ground-truth embeddings, and only perform embedding reconstruction task with LightGCN or Transformer. For NCF, we concatenate embeddings produced by both the MLP and GMF modules as ground-truth embeddings. While for PinSage, GCMC, NGCF and LightGCN, we combine the embeddings obtained at each layer to form the ground-truth matrix, i.e., E = E+ · · · + E, where E∈ Ris the concatenated user-item embedding matrix at l-th convolution step. Fig. 6(a) and Fig. 6(b) shows the recommendation performance using LightGCN and Transformer encoder, respectively. Sufﬁx +NCF, +L, +G, +P and +N denote that the ground-truth embeddings are obtained by NCF, LightGCN, GCMC, PinSage and NGCF, respectively. We ﬁnd that: (1) All the models that equipped with different ground-truth embeddings achieve almost the same performance. This indicates our model is not sensitive to the ground-truth embeddings, as the NCF and vanilla GNN models are good enough to learn high-quality user or item embeddings from the abundant interactions. (2) Besides, when LightGCN is used to obtain the ground-truth embeddings (denoted as LightGCN+L and Trans+L), we can obtain marginal performance gain compared with other variant models. 5.3.2 Effect of Data Augmentation To understand the effects of individual or compositional data augmentations in the contrastive learning (CL) task Fig. 7: Recommendation performance under individual or compositional data augmentations. c= 20%, L=4. Fig. 8: Embedding inference and recommendation using GNNs or Transformer encoder, we investigate the performance of CL pretext tasks in MPT when applying augmentations individually or in pairs. We report the performance in Fig. 7. Notation LightGCND, LightGCNS and LightGCNSD mean we apply LightGCN into the CL task and use deletion, substitution and their combinations, respectively; Notation TransD, TransS and TransSD denote we use Transformer encoder into the CL task and use deletion, substitution and their combinations, respectively. We ﬁnd that: (1) Composing augmentations can lead to better recommendation performance than performing individual data augmentation. (2) Aligning Fig. 7 and Table 3, we ﬁnd combining substitution and deletion using GNNs has competitive performance than SGL. The reason is that, same as SGL, our proposed contrastive data augmentation can also change the graph structure, and thus inter-correlations of nodes can be captured. 5.3.3 Effect of Hyperparameters We move on to study different designs of the layer depth L and user-item path length T in MPT. Due to the space Fig. 9: Embedding inference and recommendation limitation, we omit the results on MOOCs and Gowalla which have a similar trend to Ml-1M. in MPT, and report the results in Fig. 8. We ﬁnd that the performance ﬁrst increases and then drops when increasing L from 1 to 4. The peak point is 3 at most cases. This indicates GNN can only capture short-range dependencies, which is consistent with LightGCN’s [17] ﬁnding. former encoder in MPT, and report the results in Fig. 9. We ﬁnd that the performance ﬁrst improves when the path length T increases from 3 to 6, and then drops from 6 to 8. This indicates Transformer encoder is not good at capturing short-range dependencies of nodes, but can capture long-range dependencies. Pre-training GNNs. Recent advances on pre-training GNNs aim to empower GNNs to capture the correlations of nodes in an input graph, so that it can easily generalize to any downstream tasks with a few ﬁne-tuning steps on the graphs. Examples include GPT-GNN [19], GCC [20], DGI [21], InfoGraph [22] and GraphCL [54]. More recently, some researchers explore pre-training GNNs on user-item graphs for recommendation. For example, PT-GNN [30] reconstructs embeddings under the meta-learning setting. SGL [51] contrasts node representation by node dropout, edge dropout and random walk data augmentation operations. PMGT [53] reconstructs graph structure and node feature using side information. GCN-P/COM-P [52] learns the representations of entities constructed from the side information. However, these methods suffer from ineffective long-range dependency problem, as the GNN model can only capture low-order correlations of nodes. Besides, the pretext tasks of these methods consider either intracorrelations [30], [52], [53] or inter-correlations [51] of nodes, rather than both. Further, the side information is not always available, making it difﬁcult to construct the pretext tasks. To solve the above problems, we propose MPT, which considers both short- and long-range dependencies of nodes, and both intra- and inter-correlations of nodes. Cold-start Recommendation. Cold-start issue is a fundamental challenge in recommender systems. On one hand, existing recommender systems incorporate the side information such as spatial information [25], [26], social trust path [1], [2], [3], [18] and knowledge graphs [4], [5] to enhance the representations of the cold-start users/items. However, the side information is not always available, making it intractable to improve the cold-start embedding’s quality. On the other hand, researchers solve the cold-start issue by only mining the underlying patterns behind the user-item interactions. One kind of the method is metalearning [6], [7], [8], [9], which consists of metric-based recommendation [10] and model-based recommendation [11], [12], [13], [14]. However, few of them capture the high-order interactions. Another kind of method is GNN-based recommendation, which incorporates signals of high-order neighbors to reﬁne the user/item embedding. The representative models include PinSage [15], NGCF [16], LightGCN [17] and CAGR [18]. However, these GNN-based methods address the cold-start embeddings through optimizing the likelihood of a user adopting an item, which isn’t a direct improvement of the embedding quality. We propose a multi-strategy based pre-training method, MPT, which extends PT-GNN from the perspective of model architecture and pretext tasks to improve the coldstart recommendation performance. Speciﬁcally, in addition to the short-range dependencies of nodes captured by the GNN encoder, we add a transformer encoder to capture long-range dependencies. In addition to considering the intra-correlations of nodes by the embedding reconstruction task, we add embedding contrastive learning task to capture inter-correlations of nodes. Experiments on three datasets demonstrate the effectiveness of our proposed MPT model against the vanilla GNN and pre-training GNN models.