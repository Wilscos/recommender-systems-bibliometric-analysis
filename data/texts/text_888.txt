Online reinforcement learning (RL) algorithms are often difﬁcult to deploy in complex human-facing applications as they may learn slowly and have poor early performance. To address this, we introduce a practical algorithm for incorporating human insight to speed learning. Our algorithm, Constraint Sampling Reinforcement Learning (CSRL), incorporates prior domain knowledge as constraints/restrictions on the RL policy. It takes in multiple potential policy constraints to maintain robustness to misspeciﬁcation of individual constraints while leveraging helpful ones to learn quickly. Given a base RL learning algorithm (ex. UCRL, DQN, Rainbow) we propose an upper conﬁdence with elimination scheme that leverages the relationship between the constraints, and their observed performance, to adaptively switch among them. We instantiate our algorithm with DQN-type algorithms and UCRL as base algorithms, and evaluate our algorithm in four environments, including three simulators based on real data: recommendations, educational activity sequencing, and HIV treatment sequencing. In all cases, CSRL learns a good policy faster than baselines. Online reinforcement Learning (RL) algorithms have the large potential for improving real world systems with sequential decisions such as recommendation systems(Theocharous et al. 2020) or intelligent tutoring systems (Bassen et al. 2020). Such domains often have large or inﬁnite state spaces, and existing RL methods that can scale to these settings frequently require a prohibitively large amount of interaction data to learn a good policy. Incorporating human expert knowledge can accelerate learning, such as through expert demonstrations (Wu et al. 2019; Arora and Doshi 2021; Hussein et al. 2017; Taylor, Suay, and Chernova 2011) or having them provide properties the optimal policy is guaranteed to satisfy, such as a speciﬁc function class constraint (Ijspeert, Nakanishi, and Schaal 2002; Tamosiunaite et al. 2011; Buchli et al. 2011; Kober, Bagnell, and Peters 2013). However, such approaches also run the risk of reducing the system performance when the provided information is misleading or suboptimal. For example, in recommendation systems, prior Human Computer Interaction (HCI) literature has found that diversity of recommendations across Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. time is important (Nilashi et al. 2016; Bradley and Smyth 2001; Komiak and Benbasat 2006; Chen et al. 2019). But conﬁdently concluding this ﬁnding will generalize to a new system, and translating this high level knowledge into a concrete constraint for the policy class is subtle (does ’diversity’ mean 3 or 4 different item categories? Within the last 5 or the last 10 recommendations?). Choosing the wrong formulation can signiﬁcantly impact the resulting reward. An alternate approach is to allow a human domain expert to provide many different formulations, and use model or algorithm selection to automatically learn to choose the most effective (e.g. (Lee et al. 2021; Laroche and Feraud 2018). However, much of the work on selection focuses on theoretical results and cannot be used with popular deep RL algorithms that may be of more practical use (Lee et al. 2021). Perhaps most closely related to our work is the work of Laroche and Feraud (2018), which uses an upper conﬁdence bound bandit approach to learn to select among a set of reinforcement learners. This work was constructed for optimizing across learning hyperparameters (such as learning rate or model size) and, like many upper conﬁdence bound approaches, relies on tuning the optimism parameter used, which is often hard to do in advance of deployment. In this work we instead focus on leveraging human insight over the domain to speed RL through weak labels on the policy. We propose an algorithm, Constraint Sampling Reinforcement Learning (CSRL)which performs adaptive selection and elimination over a set of different policy constraints. Our selection algorithm optionally use these constraints to learn quickly, and to distinguish this from the safety constraints used in safe RL, we will also refer to them as policy restrictions. A policy constraint or restriction limits the available actions in a state and can speed learning by potentially reducing the exploration of suboptimal actions. Our method performs algorithm selection over a set of RL learners, each using a different policy restriction. For example, in a recommendation system, one restriction might specify that 3 items should be from unique categories in the past 5 items shown, and another could require at least 2 unique items in the past 10, but only for users who have used the system for more than 6 months. A third RL learner could use the unrestricted policy class, which does not limit the available ac- Code: https://github.com/StanfordAI4HI/CSRL tions in any state. Note that other areas of machine learning have signiﬁcant reductions in cost by allowing people to provide weak supervision through labeling functions that may be imperfect(e.g. (Ratner et al. 2017)). At a high level, we apply a similar idea here for reinforcement learning, allowing people to provide weak, potentially imperfect policy restrictions to be used by different RL learners. Our algorithm then performs adaptive selection over the set using optimism under uncertainty over the potential performance of the RL learners, each of which is operating with a different set of restrictions. A second technical innovation comes from noting that tuning the optimism parameter of the selection mechanism in advance can be infeasible, and a parameter either too high or too low can slow learning. Instead, we introduce a simple heuristic which uses the amount of model change to estimate the convergence of each RL learner and use this to eliminate RL learners with low performance. This allows us to achieve good performance much faster than through optimistic adaptive selection alone. These simple ideas lead to substantial empirical improvements in the diverse range of settings we consider, which include simulators created with real data in movie recommendations, tutoring system problem selection, and HIV treatment, as well as the Atari lunar lander simulator. We conduct a careful experimental analysis to illustrate the beneﬁts of our additional change-based learner elimination and the robustness to the inclusion of poor constraints. Our approach is simple and can be used with a wide variety of base reinforcement learners, and may make RL more feasible for a wider set of important domains. A Markov Decision Process (MDP) (Bellman 1957) is described as a tuple (S, A, P, R) where S is the set of states and A is the action set. The transition dynamics, P (s|s, a) deﬁnes the probability distribution of next states safter taking action a in state s, and R(r|s, a) deﬁnes the distribution of rewards r. We assume the action space is discrete (however the state space can be either discrete or continuous) and rewards are bounded |r| < R. We consider the episodic, ﬁnite horizon setting where the length of each episode is less than or equal to the maximum horizon length H. A policy, denoted as π is a potentially stochastic mapping from states to actions, where π(a|s) deﬁnes the probability of taking action a in state s. A trajectory, τ:= (s, a, r, s, a, r, ...), is deﬁned as the sequence of states, actions, and rewards in an episode. The state-action value of a policy, Q(s, a), is the expected discounted sum of rewards of starting in a state s, taking action a and then following the policy:P Q(s, a):= E[γr|s= s, a= a]), where γ ∈ [0, 1] is the discount factor. The value function is the expected discounted sum of rewards obtained starting in state sP and following policy π: V(s):= E[γr|s= s] The optimal policy, denoted π, is the policy that maximizes V : π= arg maxV. We ﬁrst brieﬂy present the aim and overview of our algorithm, Constraint Sampling Reinforcement Learning (CSRL), before going into detail. The goal of our work is to provide a simple method for leveraging (potentially weak) human domain knowledge for faster learning without sacriﬁcing ﬁnal performance. CSRL takes as input a set of different candidate policy constraints (we also refer to as restrictions to distinguish our goal from that of the Safe RL work which uses constraints), each of which is used to deﬁne a RL learner that must follow the restriction while learning. Some or all of the provided restrictions may disallow the (unknown) optimal policy and the unrestricted option may be included as a candidate. CSRL then uses an upper conﬁdence bandit (UCB) (Auer 2002) method that considers the prior performance of the learners to adaptively select the next learner to use to make decisions for the episode. In practice such optimism based approaches often require careful tuning to achieve good performance which can be infeasible in new domains lacking prior data. To increase robustness to this hyperparameter and speed learning, our CSRL method introduces a simple heuristic that tracks the model parameter changes in the RL learner for hypothesizing when a learner has converged, and eliminates those that may be misspeciﬁed or have low performance. We now introduce some notation. In this paper we use the following deﬁnition of constraint/restriction: Deﬁnition 3.1 (Constraint/Restriction) A constraint (or restriction) C is a function that maps each state to a set of allowable actions, C(s) = {a, a, . . .}. Given a set of K restrictions C, let Cdenote the krestriction. We say a policy π satisﬁes a restriction Cif for every state, π only takes actions allowed by C: ∀(s, a) π(a|s) > 0 only if a ∈ C(s). Deﬁnition 3.2 (Restricted Policy Set) We denote the policy set of restriction Cas Πand deﬁne it as the set of all policies π that satisfy C: Π= {π : ∀(s, a)π(a|s) > 0 → a ∈ C(s)} Deﬁnition 3.3 (Restricted RL Learner) Given a restriction C, and a base RL learning algorithm that can learn under constraints (such as DQN) we instantiate a restricted reinforcement learner, denoted l. lis restricted to executing and optimizing over policies in Π We assume each restriction in the set is unique and we deﬁne the subset property between restrictions: Deﬁnition 3.4 (Subset/More Restricted) Restriction Cis a subset of restriction Cif every action allowed in Cis also allowed in C: ∀s C(s) ∈ C(s). In this case, we will also refer to Cas more restricted than Cand deﬁne the < operator: C< C. We also apply this notation to describe the corresponding policy sets and RL learners. Note this in is contrast to the Constrained MDP (CMDP) framework (Altman 1999), which has a different focus on costs and budgets. Note on specifying restrictions: Note that while policy restrictions are deﬁned by the state-action pairs allowed, they often do not need to be speciﬁed by humans at that level of granularity. For example, a human expert might specify that students should only be given practice problems involving at most one new skill. As long as the state and action space has features representing the skills involved, it is easy to programmatically translate this high level speciﬁcation to the desired constraint without requiring the human expert to enumerate all state-action pairs. Our algorithm, CSRL (Algorithm 1), takes as input a base RL algorithm, Alg, a set of potential restrictions C and a conﬁdence bound function B(h, n). CSRL starts by initializing |C| RL learners, each which use the input base algorithm Alg, along with one of the restrictions C∈ C. Learner lwill only chose actions and learn over policies that satisfy C. Let L denote the set of active learners and initially set L = {1, . . . , |C|}. Each episode proceeds in 3 steps. First CSRL chooses a RL learner lin the active set L to select actions in this episode. Second, CSRL gathers a trajectory of states, actions, and rewards using l. This data is used both to update the learner las well as all the relevant other learners: sharing data speeds learning across all the RL learners. Third, a heuristic is used to potentially eliminate lfrom L. The next episode then starts. We now describe step 1, RL learner selection. We use UCB to select the RL learner with the maximum estimated upper bound on the potential return in the active set. The UCB mechanism uses the average prior returns observed during past executions of RL learner l, denoted ˆµ, and the input conﬁdence bound function. There are two important things to note about this learner selection strategy. First, CSRL does not use any RL learners’ internal estimates of their own performance or potential value function. This allows us to leverage very general base algorithms Alg without requiring that they accurately estimate their own value functions. Instead CSRL relies only on the observed returns from executing each learner, treating them as one arm in a multi-armed bandit. Second, note that the estimated upper bound for a given learner l in Equation 1 will generally not be a true upper conﬁdence bound on the performance of the RL algorithm. This is because the UCB multi-armed bandit algorithm assumes that the stochastic returns of individual arms are unknown, but stationary. In contrast, in our setting, arms are RL learners whose policies are actively changing. Fortunately, prior related work has successfully used UCB to select across arms with nonstationary returns in an effective way: this is the basis of the impactful upper conﬁdence trees, a Monte Carlo Tree Search method that prioritizes action expansions in the tree according to upper bounds on the non-stationary returns of downstream action decisions (Shah, Xie, and Xu 2020). It has also been used in related work on online RL (Laroche and Feraud 2018), and we will demonstrate it can both be empirically effective in our setting, and, under mild assumptions, still guarantee good asymptotic performance. After gathering data using the selected RL learner, this data is provided to all RL learners to optionally update their algorithm. Then the third step is to eliminate the potential learner associated with the chosen constraint. The elimination heuristic checks if a RL learner l’s value or stateaction value has stabilized, and if its average empirical performance is lower than another RL learner l, we will eliminate lif a less constrained learner lis in the active set. We now state this more formally. When RL learner lis used for the ntime, and generates trajectory τ, let δrepresent the change in the value function. For example, in tabular domains, we can measure the average absolute difference in the state-action values where Qrepresents the state-action value function after updating using the data just gathered. In value-based deep RL we can use the loss function over the observed trajectory as an approximate estimate of the amount change δ=(rQ(s, a)) − Q(s, a). (3) For a RL learner to be potentially eliminated, the change, δ, must be below a threshold Tfor at least Tsuccessive steps, suggesting that the underlying learning process is likely to have stabilized. If this condition is satisﬁed for a RL learner lthe meta-leaner ﬁrst checks there exists a less constrained learner lin L. If such a lexists and at least one other learner lin L has higher average performance: ˆµ< ˆµ, then lis removed. See Algorithms 1 and 2 for pseudocode. We give examples of instantiations of CSRL with base learners of UCRL and DDQN in section 3 below and we will release all our code on Github. Elimination Mechanism Intuition: Recall that CSRL should try to select the most restricted learner that is compatible with the optimal policy, since learning with a reduced action space will often speed learning and performance. To increase robustness, the RL learner selection UCB strategy relies on minimal information about the internal algorithms. However, UCB often can be conservative in how quickly it primarily selects high performing arms (in our case, RL learners) rather than lower reward arms (in our case, learners with restrictions incompatible with the optimal policy). Consider if one can identify when learner lhas converged and can correctly evaluate its performance through the running average µ. If there exists lwith higher average returns µ, restriction Cis likely to not include the optimal policy and lcan be removed from the set of potential RL learners. Our method uses a proxy heuristic for convergence and A reader may wonder if providing such off policy data is always useful or how best to incorporate it. In the particular base RL algorithms we use, it is straightforward to incorporate the observed trajectories into experience replay or in an estimate of dynamics and reward models, but more sophisticated importance sampling methods could be used for policy gradient based methods. Note one could use other measures of the change in the RL learner, including differences in the neural network parameters, or changes in predictions of the value of states. Algorithm 1: CSRL Inputs: Alg, C, Z, B(h, n), T, T Initialize: L ← Create Restricted Learners from C D= [ ] ∀C for Episode h = 1, 2, ... do l← select RL learner [Eqn. 1] τ← GenerateTrajectory (l)P UpdateLearners (L, τ) δ← Calculate Change of l[ eq. 2 or 3] if Eliminate (l, D) then Algorithm 2: Eliminate (Eliminate Learner) Inputs: l, D if ∃n ∈ {n− T: n} such that D(n) > Tthen Return False (Don’t Eliminate) if Exists l, l∈ L : C> Cand ˆµ> ˆµthen Return True (Eliminate) Return False sample estimates for returns which can be noisy. Therefore it may incorrectly conclude a learner lhas converged and/or has suboptimal returns. To ensure that we preserve a learner that admits the optimal policy, we only eliminate a learner l, and its constraint C, if it is a subset of at least one other active constraint C. Therefore the the set of policies that satisfy Cwill continue to exist in the active set, even when Cis eliminated. We now provide some basic soundness to this proposed approach, before describing instantiations of CSRL with particular base learners, and demonstrating its performance empirically. Brief Theoretical Discussion We brieﬂy provide a guarantee that at least one learner whose policy set contains the optimal policy will be taken more than all other learners under mild assumptions. Assumption 1 (Model Parameter Convergence) Let Mrepresent the model parameters of the learner lafter the nupdate. In every run of CSRL, the model parameters of every learner converge: limM→ Mfor all C∈ C. Let πdenote the policy corresponding to the converged model parameters Mof RL learner l. Let E[V(s)] = µ, µ= maxµand πdenote the policy that achieves µ. Note that πis deﬁned as the policy with the highest return across all learners in the set. Without loss of generality assume πis unique. We refer to the set of constraints compatible with πas the set of optimal constraints and denote this set as Cwith a corresponding indices set K. Then require: Assumption 2 (Convergence to Optimal) Given a speciﬁc run of CSRL, let every learner in Kconverge to the µof the run: µ= µfor all k ∈ K We now show that at least one RL learner in Cwill be chosen more than all suboptimal learners asymptotically. Theorem 1 Assume Assumptions 1 and 2 hold. Also assume as input a conﬁdence bound B(h, n) of the formwith 0 < η <and z(h) satisfying the following two conditions: (i) z(h) is non-decreasing and (ii) O(z(h)) < O(h). Let T(h) be the number of times RL learner lhas been selected at episode h. Then for at least one k∈ K, T(h) > T(h) for all k /∈ Kas h → ∞ The proof of theorem 1 is provided in the appendix. When the base algorithm has convergence guarantees, such as UCRL, we can additionally provide guarantees on the rate of convergence. We provide these rates and a discussion of the UCRL case in the appendix: our analysis drawns upon the analysis of convergence rates for Monte Carlo Tree Search from Shah et al. (Shah, Xie, and Xu 2020). Algorithm Instantiations We discuss speciﬁc instantiations of CSRL with various base RL algorithms. CSRL-UCRL. UCRL (Auer, Jaksch, and Ortner 2009) is an inﬂuential strategic RL algorithm with good regret bounds this is based on optimism under uncertainty for tabular settings. It is simple to incorporate action constraints during the value iteration steps: V(s) ← max(˜R(s, a) +˜P (s|s, a)γV(s)) All observed (s, a, r) tuples are used to update an estimated model of transitions and rewards that is shared across all RL learners. Equation 1 is used to track convergence in the estimated value function. CSRL-DQN, CSRL-DDQN and CSRL-Rainbow Deep reinforcement learning has shown powerful results across a wide variety of complex domains. Our CSRL-DQN, CSRLDDQN, and CSRL-Rainbow implementation uses a separate DQN (Mnih et al. 2015), DDQN (van Hasselt, Guez, and Silver 2016), or Rainbow (Hessel et al. 2018) learner for each restriction. We used epsilon greedy exploration with epsilon decay. Experience is shared across learners in the form of a shared replay buffer. The UpdateLearners function places the tuples from the most recent trajectory in the shared replay buffer. Each learner lis then updated, using only samples from the buffer that satisfy the associated restriction C. The learner lis updated using a constrained Q loss (Kalweit et al. 2020) (see Equation 3). We brieﬂy introduce the evaluation environments and the constraints used and then discuss our results. Due to space constraints, we defer detailed descriptions of the environments and constraint constructions to the appendix. We explored sharing some model weights but found that resulted in worse performance Figure 1: (a) An image of the recommendation system environment (RS env.) (b) A visualization of the policy space for the RS env. For example, every policy allowed under the exactly 2 variability constraint (’e2’) is also allowed under the at least 2 variability (’l2’) constraint. (c) An example curriculum graph in the education domain. Each directed edge indicates the source node is a prerequisite of the sink node. Environments and Constraints Recommendation System Environment The movie recommendations (See Fig 1a for illustration) environment is a slighlty modiﬁed environment from prior work (Warlop, Lazaric, and Mary 2018) ﬁt with the Movielens 100K dataset (Harper and Konstan 2015) which contains 100K user-movie ratings across 1000 users with |A| = 5 and |S| = 625. Each action is a movie genre and the state space encodes recently seen movies. The rewards correspond to the ratings of the recommended movies. The episode length is random and each state-action pair has some probability of leading to the terminal state. Following prior work that suggests diversity is correlated with system usage (Nilashi et al. 2016), we design a set of 12 constraints using a variability factor, which we deﬁne as the number of unique actions in recent history. Our constraints require the policy to maintain a certain level of variability. A partial visualization of the structure between some constraints in the set is given in Figure 1b. Because our state space contains the history of past actions, these high level constraint speciﬁcation are easily translated programmatically. Education: Deep Knowledge Tracing Student. Our new educational activities sequencing environment uses a popular student learning model, the Deep Knowledge Tracing (DKT) model (Piech et al. 2015), to simulate students. The model is trained with the ASSISTment 2009-2010 (Feng, Heffernan, and Koedinger 2009) dataset of algebra learning, containing data from 3274 students over 407K studentproblem interactions. Each action (|A| = 51) corresponds to presenting a different problem to the student. The horizon is length H = 100. The state space is a continuous Rand encodes the current proﬁciency (the predicted probability of mastery by the DKT model) on each problem and the binary encoded timestep. The reward corresponds to a signal of mastery and is 1 when the proﬁciency of a problem ﬁrst exceeds a threshold m= 0.85 and 0 otherwise. In education, curriculum or prerequisite graphs are common; however, setting the correct granularity to model knowledge can be difﬁcult. We create a constraint set consisting of different graphs: Figure 1c shows an example using automatic curriculum generation methods that requires hand specifying a hyperparameter to adjust the number of edges (Piech et al. 2015). We construct a constraint set containing 13 different graphs. Given a graph, we only allow unmastered problems that have all prerequisites mastered (this information is encoded in the state space) to be selected. HIV treatment Simulator The HIV treatment simulator (Adams et al. 2004; Ernst et al. 2006) simulates a patient’s response to different types of treatments. The action space is discrete with size 4 and represents various treatment actions. The state space is continuous R, where each dimension represents a marker of the patient’s health. Each episode is 200 timesteps and the reward encourages patients to transition to and maintain a healthy state while penalizing drug related actions. We created a simulator for multiple heterogeneous patient groups by perturbing the internal hidden parameters of the system following Killian, Konidaris, and Doshi-Velez (2017). We learn an optimal decision policy for 3 different groups, and then use the known optimal policies as constraints to learn in a new group which may or may not be similar to a group with a known policy. We create a constraint set of 7 constraints. Lunar Lander The Lunar Lander environment from Open AI Gym (Brockman et al. 2016) simulates landing an aircraft. The action space is discrete with 4 actions which correspond to ﬁring different actuators. The state space is continuous Rand gives position and velocity information. Positive reward is given for safely landing and negative reward is given for ﬁring the actuators and crashing. We generate different policies with differing performance levels to mimic access to policy information from multiple human pilots. We create a constraint set of 10 constraints each of which limits the available action to that of the policy of one of the past policies or a mixture of them. Results and Discussion We compare CSRL against 4 baselines: unrestricted reinforcement learning, reinforcement learning with the oracleconstraint which is the best constraint (but unknown in real situations), reinforcement learning under a non-oracle constraint, and SSBAS (Laroche and Feraud 2018), a prior algorithm for selecting across learners with different hyperparameters or models that uses a UCB selection approach but does not eliminate learners. We used UCRL as a base learner for our Recommendation System experiments, DDQN for Education and HIV Treatment, and Rainbow for Lunar Lander. In each experiment, the same base learning model parameters (model architecture, learning rates, exploration rate scheduling, etc) were used for all algorithms: see appendix for details. For CSRL and SSBAS, total episode return was scaled to the range [0, 1] for the adaptive learner selection and we used the con-√ ﬁdence bound B(h, s) = c. This bound satisﬁes the conditions of B(h, s) required for Theorem 1 to hold. We did not tune c and used c = 1 for both CSRL and SSBAS Figure 2: We plot all values with 95% conﬁdence intervals. Top Row We plot episode returns during training from the Education, Lunar Lander and HIV domains. We plot our algorithm CSRL, as well as the SSBAS (Laroche and Feraud 2018) and unconstrained (labelled No-Const) baselines. In the educational domain, a misspeciﬁed constraint, labelled Over-Const, is also shown. The Oracle-Const plots the performance of following the oracle best constraint in the set which is not known beforehand. In the bottom row, we provide further experiments and visualizations used to illustrate various properties of our algorithm for our discussion ( Section 4). Additional plots including parameter sweeps in other environments and plots over longer episodes are provided in the Appendix. Table 1: The sample complexity speedup of CSRL over baselines in terms of the factor of episodes more required by baselines to achieve returns 90% and 97% of the maximum observed value compared to CSRL. for all experiments. We later present a discussion of sensitivity to different values of c. For CSRL, Twas set to 20 and Twas set to 0.05 for all experiments. We did not tune either parameter and we later present a discussion of robustness to different values of T. For each experiment, the results were averaged over multiple random seeds, with 200, 20, 20, and 40 seeds for the recommendations, education, HIV treatment, and Lunar Lander experiments respectively. Results Across all domains CSRL learned a good policy faster than non-oracle baselines, and often signiﬁcantly faster. In Table 1 we list the speedup of CSRL over the SSBAS and Unrestrained (Unconst) baselines in terms of the factor of episodes more compared to CSRL needed by the baselines to achieve mean returns 90% and 97% of the observed maximum return. In most cases, we observe CSRL can achieve a high level of performance signiﬁcantly faster than baselines, often at least 1.5 times as fast and occasionally much more. In the appendix we give a table of the raw number of episodes needed to reach these performances for each environment. In Figure 2a we plot the returns through training along with 95% conﬁdence intervals for the Education, HIV treatment and Lunar Lander domains. These plots for the other domains are presented in the appendix. We observe CSRL is able to utilize the constraints to avoid the extended initial poor performance compared to the unconstrained condition. Additionally the elimination mechanism allows the algorithm to eliminate suboptimal learners to quickly achieve near-oracle performance. We investigate adaptive selection and elimination in depth in the discussion: Importance of Adaptive Selection It is expected that following a single good constraint can lead to quick learning by reducing the policy space that needs to be explored. We see this is indeed true as the oracle-constraint performs best for most cases. However the best constraint is not known beforehand, and following a single constraint that is misspeciﬁed can lead to suboptimal performance. We demonstrate this in the education domain, shown in Figure 2a where we additionally plot the tightest restriction in the set, labeled Over- Const (which stands for Over-Constrained). Over-Const also quickly converges but to a return much lower than optimal. We see the adaptive selection mechanism of CSRL and SSBAS over the set can leverage the constraints to learn faster than the unrestricted baseline while avoiding this potential misspeciﬁcation. Additionally the beneﬁt of adaptive selection has over standard unrestricted RL increases with action space size as larger action spaces are harder to explore. This is illustrated by comparing the speed of learning unrestrained in our Education (|A| = 51) and Lunar Lander (|A| = 4) environments (Fig 2a). Importance of Elimination In our setting where we expect some constraints to be misspeciﬁed, we found eliminating suboptimal learners to be very important for robustness against performance decreases due to missspeciﬁcation. This is illustrated in the HIV experiments shown in Figure 2a. In this case, all constraints are suboptimal and the unconstrained option is optimal. We notice that CSRL is able to quickly use elimination to approach the unconstrained performance compared to SSBAS. In Figure 2b we plot the rate of selecting the optimal constraint in the lunar lander experiment for CSRL and SSBAS through learning. We see that elimination allows the algorithm to achieve high rates of selecting the optimal policy much faster. When is elimination not important? We expect elimination to not be important when the performance gap between the optimal and suboptimal constraints is large. Intuitively a large difference is easier for the UCB mechanism to distinguish the best item so CSRL and the SSBAS baseline learn quickly to choose the best item and achieve nearly identical performance. We demonstrate a case of this in the appendix. The conﬁdence bound parameter In Figure 2c, we plot performance for various values of c, the multiplier on the conﬁdence bound of the UCB constraint selection mechanism in the lunar lander environment. For both algorithms, we see that a low value, c = 0.05, results in higher initial performance but a much slower rate of learning which is a poor outcome for both algorithms. On the other hand, a higher value, c = 3 leads to signiﬁcantly worse performance for SSBAS while CSRL’s elimination mechanism protects the performance from suffering. The uncertainty over the value of the multiplier naturally comes in when the maximum reward is unknown. It is undesirable to underestimate the maximum reward as it may lead to a slow rate of learning. In these cases CSRL along with a high estimate of maximum reward can lead to good performance. The effect of the loss threshold T: In Figure 2d we plot the performance for various values of T, the model change threshold for the elimination mechanism, in the education environment. We demonstrate that even for Tset to large values (T= 0.25, 5 times greater than initial used value), the performance does not decrease (in fact it increases due to faster elimination). This shows our robust elimination procedure is able to maintain performance when the algorithm incorrectly hypothesizes a constraint as suboptimal. When Tis set to low values (T= 0.00125, 40 times less than initial), the elimination of constraints slow and we approach the performance of the SSBAS baseline. Summary Overall when there is a good constraint in the set, we demonstrate our algorithm is (1) able to achieve a good policy quickly, often signiﬁcantly faster than baselines (2) this performance improvement is due to CSRL’s robustness against both misspeciﬁed constraints and hyperparameters such as the conﬁdence bound parameter. We discuss some additional areas of related work not previously mentioned. Constrained RL Our work is related to work in constrained RL. Most prior work considers learning under a single constraint that is always enforced. Constrained RL has been studied under various combinations of what is known and unknown about the components of the model (the constraints, rewards, and transition dynamics). It has been studied in cases where all components are known (Altman 1999), as well as cases where one or more of them need to be learned online (Efroni, Mannor, and Pirotta 2020; Zheng and Ratliff 2020; Achiam et al. 2017; Wachi and Sui 2020; Bhatnagar and Lakshmanan 2012). This work spans a variety of different algorithms including tabular (Efroni, Mannor, and Pirotta 2020), policy search (Achiam et al. 2017), and actor-critic (Bhatnagar and Lakshmanan 2012) RL algorithms. Contrary to this work that focuses on learning to satisfy a single constraint, we consider a set of weak constraints, which may or may not be compatible with the action selections of the unknown optimal policy. We additionally note our method is separate from the constraint sampling algorithms for solving unconstrained dynamic programming problems approximately using linear programming (De Farias and Van Roy 2004). Inferring Constraints/Rules There has been prior work on inferring constraints or rules from demonstrations (Taylor, Suay, and Chernova 2011). In two papers (Noothigattu et al. 2019; Balakrishnan et al. 2019) a single constraint is inferred from demonstrations, and a 2-armed bandit learns if the inferred constraint should be followed. Our work differs in that we consider utilizing domain knowledge instead of demonstrations and we consider multiple potential constraints as opposed to a single constraint. We additionally differ in considering a method for RL learner elimination. There often exists domain expertise for real world systems that can be leveraged in RL algorithms to speed learning. In this work we propose a method, CSRL, to incorporate this knowledge in the form of constraints. As it is often difﬁcult to create a single constraint the algorithm designer is conﬁdent is correct, our work takes as input a set of potential constraints the algorithm designer hypothesizes, but does not have to be certain will speed learning. We provide a brief theoretical discussion on our upper conﬁdence with elimination selection algorithm and focus on showing strong empirical results. We show this simple approach is compatible with deep RL methods and that CSRL can learn a good policy substantially faster than state-of-the-art baselines, suggesting its potential for increasing the range of applications with RL is feasible by leveraging imperfect human guidance. This material is based upon work supported by the Stanford Human Centered AI Hoffman Yee grant and the Graduate Fellowships for STEM Diversity.