Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classication and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which unies two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to eectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the ne-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task specic modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classication with multi-modalities. We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and ne-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multitask classication. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the eectiveness of both cross-modality attention modules and our method’s robustness against noise in image and text inputs, which is a common challenge in practice. Inspired by the recent rise of the pre-trained NLP models such as BERT [11], learning to classify image-text pairs for vision-language (𝑉 𝐿) tasks using Transformer [37] based encoders has received much attention as both modalities can be informative and benecial to each other. Current methods can be classied into two main categories: one-stream methods and two-stream methods. One-stream methods capture the cross-modality attention across brywan@amazon.com yisun@amazon.com Seattle, WA, USA image and text by concatenating them at early stage and input the concatenated feature into one unied transformer encoder. Twostream methods rst extract the image and text features using two separate encoders and then learn their cross-modal relationship through various methods such as contrastive learning [3], etc. Among those one-stream and two-stream methods, the Contrastive Language-Image Pre-Training (CLIP) [30] has achieved great success recently. CLIP is trained on the WebImageText (WIT) Dataset which consists of 400 million image-text pairs collected from a variety of publicly available sources on the Web and achieves many state-of-the-art results in zero-shot learning tasks, pre-training tasks, and supervised classication tasks when a linear probe is added on top of it. Despite of CLIP’s [30] strength, it is mainly designed for zeroshot image classication, resulting in the limitation of its ability to leverage both image and text input when available. Since the training of CLIP only involves global image and text features, thus the ne-grained relationship between image patches and text tokens are not modeled. Such relationship is useful in ne-trained classication tasks, especially in the situations where only a small proportion of image patches or text tokens are related to the classication tasks. Moreover, since it chooses the user-dened textual description called "prompts" with class value that matches the most with the image as the classication result, signicant eorts to engineer the prompts for optimizing downstream tasks are required. Last but not least, in practice, it is quite common for the image-text pairs to contain noise. For instance, on E-commerce websites, some images or text could be irrelevant to the product due to catalog errors. In social media apps, users might enter irrelevant textual comments or upload unrelated images. Treating input from both modalities equally in such situation may lead to poor classication performances as one of the modalities could be pure noise. To address the aforementioned issues, in this paper we propose the Cross-Modality Attention CLIP (CMA-CLIP). Our contributions include: •We combine CLIP with a sequence-wise attention module, which renes the CLIP-generated image and text embeddings v-neck sleeveless Text 𝑇 cocktail EncoderT dressT by modeling the relationship among the embedding of the sequence of image patches and text tokens. This transformerbased module makes the embedding more context-aware. 𝐸.𝑔., the embedding of the black image patches are more correlated with the ‘𝑏𝑙𝑎𝑐𝑘’ token in text. We experimentally prove that such renement can improve the performance of classication tasks. •We adopt a modality-wise attention module to assign learnable weights to each modality that measures its relevance to the classication task. The impact of the irrelevant modality will be dampened, and therefore our network is robust against noisy image or text inputs, which is a common challenge in practice. •We add task specic modality-wise attentions and MLP heads on top of the sequence-wise attention module, so that the same network can be leveraged for multi-task classication. Moreover, compared with CLIP, this architecture enables the network to leverage both image and text inputs in both training and inference stage. •On the MRWPA, CMA-CLIP outperforms the raw CLIP and the ne-tuned CLIP (ne-tuned using image-text pairs from a major retail website) on the classication of three product attributes by 10.9% and 12.9% in recall at the same level of precision. It also improves the state-of-the-art performance [44] on Fashion-Gen Dataset from 88.1% to 93.6% in accuracy and achieves competitive performance on Food101 Dataset against [21]. Current multi-modality learning methods are mainly one-stream and two-steam where one-stream methods use a single Transformer 𝐼[Task2][Task2] encoder to process the concatenated image and text embedding, while two-stream methods use both image encoder and text encoder to extract image and text embeddings at early stage and then learn their cross-modal relationship. The one-stream methods, such as ViLBERT [28], VisualBERT [26], VL-BERT [35], Unicoder-VL [25], ImageBERT [29] and Unied VLP [43], concatenate the image’s Region-Of-Interest (ROI) patches and text tokens as the input tokens for BERT [11]. These models are typically pre-trained using tasks including Masked Language Modeling (MLM), Masked Region Modeling (MRM), Multi-Model Alignment Prediction (MMAP). The UNITER [8] and OSCAR [27] incorporate additional pre-training tasks. The UNITER uses the Optimal Transport (OT) [38] to model the relationship between the image patches and the text tokens. The OSCAR uses the object categories detected by the Faster-RCNN [31] and encodes the category text as additional input tokens to BERT. Instead of using the Faster-RCNN to detect ROIs, methods such as ICMLM [33], Pixel-BERT [18], and SOHO [17] use a CNN to extract the feature maps of an image and use the depth vectors in feature maps as image tokens. Such conguration is able to capture the semantic connection between image pixels and text tokens, which is overlooked by region based image features extracted by Faster-RCNN. Similar to ICMLM, the VirTex [10] also uses the depth vectors in feature maps as image tokens and input the image and text tokens into a forward transformer decoder and a backward transformer decoder. Instead of feeding the whole image or ROI into a CNN, methods like FashionBERT [13], KaleidoBERT [44], and ViLT [22] cut an image into patches and treat each patch as an "image token". For FashionBERT, it uses a pre-trained image model such as InceptionV3 [36] or ResNeXt-101 [40] to extract image features. Dierent from FashionBERT, KaleidoBERT adopts the SAT [41] network to generate description of salient image patches aiming to nd an approximate correlation between image patches and text tokens to serve their pre-training tasks,𝑖.𝑒., Aligned Masked Language Modeling (AMLM), Image and Text Matching (ITM), and Aligned Kaleido Patch Modeling (AKPM). ViLT diers from all above-mentioned methods by simply applying linear projection on attened image patches which greatly reduce the model size, thus leading to signicant runtime and parameter eciency. The two-stream methods are mainly motivated by self-supervised learning methods [2–7,14,15]. In self-supervised learning, two views (𝑒.𝑔., two augmentations), of a single image are forwarded into one network respectively. Their outputs are compared using the contrastive loss [3], so that the two views of the same image are much similar than the two views from two dierent images. The ConVIRT [42] adopts this idea on the self-supervised learning of image-text pairs. Two networks are used to extract the image and text features respectively. The image and text features from the paired image-text input is trained to be much similar than the unpaired ones. CLIP [30] is a simplied version of ConVIRT, where the text in each image-text pair is a single sentence instead of a pool of sentences as in ConVIRT. Similar as CLIP, BriVL [19] uses MoCo [15] which is a more advanced cross-modal contrastive learning algorithm to help train the network with limited GPU memory by leveraging more negative samples. The ALIGN [20] collects 1.8 billion image-text pairs, and adopts a similar network architecture as CLIP. The performance of ALIGN is comparable to CLIP on the ImageNet dataset for the classication task, Flickr30K and MSCOCO datasets for image-text retrieval task. In order to perform image-text classication tasks, a classication layer needs to be added on top of the image-text embeddings of the pre-trained models such as VL-BERT [35], UNITER [8], et al. The MMBT [21] is specically designed for image-text classication tasks. Unlike VL-BERT or UNITER, MMBT directly loads weights from BERT which does not require pre-training. In MMBT, ResNet [16] is used to extract image features. The image features are projected into the same space as the text tokens, used as image tokens and feed into a BERT together with text tokens. A linear layer is added on the classication embedding to perform supervised tasks. However, both one-stream and two-stream methods have their own inadequacies. One-stream methods heavily rely on pre-trained Faster-RCNN [31] or ResNet [16] to extract image feature which does not support end-to-end training of the whole network, and therefore, the extracted image and text features are not optimized to model the image-text relationship. Two-stream methods only focus on learning global image and text features, and cannot capture the ne-grained relationship between image patches and text tokens. In order to overcome the aforementioned disadvantages, our proposed method fuses both one-stream and two-stream architectures which complements each other’s inadequacies. It leverages the pre-trained CLIP, a two-stream architecture, to capture the overall alignment between image and text. Subsequently, we add a sequence-wise attention module, which is a transformer based cross-modality attention module used in most one-stream architectures, to capture the ne-grained relationship between image patches and text tokens. SemVLP [24] applies similar fusion logic. The dierence between our method and SemVLP is that, SemVLP leverages the same cross-modality attention module to capture both high-level and ne-grained relationship between image and text. It was pre-trained on tasks such as MLM and MRM, whereas CLIP was directly pre-trained to maximize the overall image-text alignment through contrastive learning. More importantly, SemVLP does not consider the situation where one of the modalities is irrelevant to the downstream classication tasks due to input noises, which is a common challenge in practice. To handle such situation, we add a modality-wise attention module, which learns the importance of both modalities so that the irrelevant modality can be dampened for the classication tasks. At last, by adding task specic modalitywise attentions and MLPs, our model is able to perform multi-task classications. The rest of the paper is arranged as follows: In Section 3, we rst give a brief review of CLIP, and then we introduce our proposed CMACLIP with detailed explanation of each component. In Section 4, we introduce the datasets that we use, the corresponding experimental results, the visualization of the sequence-wise attention module, and the ablation study to prove the eectiveness of modality- and sequence-wise attention modules. In Section 5, we conclude this paper and elaborate our future work. The Contrastive Language-Image Pre-Training (CLIP) consists of an image encoder and a text encoder. For each image-text pair, the image and text encoders project the pair into an image and text embedding in the same multi-modal space. Given𝑁imagetext pairs, the training objective of CLIP is to maximize the cosine similarity of the paired image and text embedding while minimize the cosine similarity of the unpaired ones. During inference, for a classication task with𝐾classes, it rst uses the𝐾class values to construct𝐾prompts such as ‘A photo of{class value}’. These𝐾prompts are then projected to𝐾text embeddings by the text encoder. For any given image, it is projected to an image embedding by the image encoder, then CLIP computes the cosine similarities between the image embedding and those 𝐾 text embeddings. The class value with the largest similarity is then considered as the class prediction. CLIP is trained using WIT Dataset which contains 400 million image-text pairs collected from the Web. According to the results reported in [30], its zero-shot classication performance surpasses the supervised linear classier tted on ResNet50 [16] features on datasets such as StanfordCars [23], Country211 [30], Food101 [1], and UCF101 [34] etc. CLIP focuses on the learning of the global image and text features. In CMA-CLIP, we build a sequence-wise attention module to capture the ne-grained relationship between the image patches and the text tokens such as the black image patches and the ‘black’ tokens in text. This module leverages the transformer architecture [37]. It takes the sequence of embeddings corresponding to all the image patches and text tokens generated by CLIP as input. The Fashion-Gen DatasetJeans module outputs two embeddings incorporating the aggregated image and text information. Instead of directly leveraging these two embeddings for classication, we add a modality-wise attention module to handle the situation where a certain modality (image or text) is irrelevant to the classication task. This is because, in practice, it is common for the image-text pairs to contain noise. 𝐸.𝑔., a retailer might upload wrong product images to E-commerce website, or a user might enter random textual comments on social media apps. To handle such situations, we leverage the similar architecture as in [39] to learn the importance of each modality to the classication tasks. The sum of the two embeddings weighted by their importances is followed by a MLP head for the classication. To leverage the network for multiple classication tasks, we congure task specic modality-wise attention modules and MLP heads. The complete architecture of CMA-CLIP is shown in Fig. 1. 3.2.1 Sequence-wise Aention. In our implementation, the sequencewise attention module is a transformer encoder [37]. Let𝑋 ∈ R be the matrix of the sequence of embedding of all the image patches and text tokens generated by CLIP, where𝑠is the length of the sequence and𝑑is the dimension of the embedding. Let𝑊∈ R, 𝑊∈ Rand𝑊∈ Rbe the projection matrices which project each embedding in𝑋to key space, query space and value space respectively: The self-attention block learns a similarity matrix𝑄𝐾between each pair of embeddings in𝑋. Each embedding in the sequence is then updated as the average of the projected embedding across all the embeddings in the value space weighted by their similarities. This sequence-wise attention module captures the ne-grained relationship between each image patch and text token. 3.2.2 Modality-wise Aention. After the image feature and the text feature are generated, they need to be aggregated to form the nal feature for the classication tasks. In order to dampen the irrelevant modality, we leverage the keyless attention module proposed in [39]. Given an image-text pair, the sequence-wise attention module will output𝐼and𝑇as the global image and text embedding, respectively. The aggregated embedding is the weighted average of The weight 𝜆 is computed by: where𝑤is a learnable parameter vector that is of the same dimension as 𝐼and 𝑇. Algorithm 1 Training Process of CMA-CLIP. Line 4-16: Warm-up Stage, Line 18-21: End-to-End Training Stage, Line 23-26: Tuning Stage Input: Image-text pairs and their labels of K tasks (X, Y, L, L, ..., L denotes the modality-wise attention model. 𝑀𝐿𝑃 denotes the multi-layer perception head for classication. Freeze the image encoder, the text encoder, the sequence-wise attention transformer. Fine-tune on the best check-points from the previous stage. Output: 𝐶𝐿𝐼𝑃 model, 𝐶𝑀𝐴model, 𝐶𝑀𝐴model, and 𝑀𝐿𝑃 model. Table 2: Recall (%) at 90% precision on the MRWPA dataset. Table 3: Accuracies (%) on the Food101 dataset. 3.2.3 MLP Heads for Classification. For any classication task, an Multi-Layer Perception (MLP) head is added on top of the nal feature outputted by the modality-wise attention. The cross entropy Table 4: Accuracies (%) on the Fashion-Gen dataset. is used to compute the loss for this classication task. For multitask classication, we add task-specic modality-wise attention and MLP for each task separately. This is because the relevance of modality is dependent on the task, hence we need task-specic modality-wise attentions and multiple MLPs as the classication heads. Table 5: Examples where CMA-CLIP is able to give the correct attribute classication while CMA-CLIP w/o the modality-wise attention cannot. Noting that, text tokens which are shown in red are the attribute label keywords that do not exist in the original titles. We add them in and re-do the prediction with b oth methods to further validate that, without the modality-wise attention, the model is not able to manage noise properly. Table 6: Ablation study of CMA-CLIP. Recall (%) at 90% precision on the MRWPA with Color, Pattern and Style attributes. 𝑀𝐴 denotes modality-wise attention and 𝑆𝐴 denotes to sequence-wise attention. We perform experiments on three datasets, the MRWPA Dataset, the Food101 Dataset [1] and the Fashion-Gen [32] Dataset. All three datasets consist of image-text pairs. Data samples from the three datasets are shown in Table 1. 4.1.1 The MRWPA Dataset. This dataset includes the product image and title pairs of dress products from a major retail website. The goal is to classify three dress related product attributes, color, pattern, and style. Color attribute has 17 classes such as black and white, pattern has 12 classes such as graphic and plain, and style has 21 classes such as pencil and a-line. The training data consists of 5.8 Million product image-title pairs. We also prepare 310 and 132 image-title pairs as the validation and test set, which are used for hyper-parameter tuning and performance evaluation respectively. Women’s Sexy V Neck Crisscross Backless Cocktail Party Bodycon Peplum [Plain] Dress, White, L 4.1.2 The Food101 Dataset. This dataset contains 101 food categories. The goal is to classify each image-text pair to a food category. We download the preprocessed images and texts from the Kaggle competition. In the processed data, 67971 images are in the training set, and 22715 images are in the testing set. During training, we randomly split 80% of the data in the training set for training and the rest 20% data for validation. 4.1.3 The Fashion-Gen Dataset. This dataset contains 293,008 fashion images. Each image is paired with a text describing the image. This dataset contains 48 main categories, such as "DRESSES", "JEANS", "SKIRTS", "SHIRTS", etc., and 121 sub-categories, such as "SHORT DRESSES", "LEATHER JACKETS", "MID LENGTH SKIRTS", "T-SHIRTS" and so on. In our experiments, we perform 121 subcategory classication. We use the same data as used in [44] for training and testing. The number of training data is 260480, and the number of testing data is 32528. 4.2.1 Experiment Seings. Same as CLIP, the image encoder of CMA-CLIP is a 12-layer 768-width ViT-B/32 [12] with 12 attention heads, and the text encoder of CMA-CLIP is a 12-layer 512-width Transformer with 8 heads used in [37]. The sequence-wise attention transformer is also a 12-layer 512-width model with 8 attention heads. In all the experiments, the batch size is set to 1024, weight decay of Adam is set to1𝑒 − 4, and the learning rate is set to1𝑒 − 5. Figure 2: Visualization examples. Each image is highlighted using the attention map between the image embedding and the embedding of the most relevant text token. 4.2.2 Training Strategy. We use the pre-trained weights of CLIP as the initial weights of the image encoder and text encoder in CMACLIP. We randomly initialize the weights in the sequence-wise attention module, modality-wise attention module and MLP. As CMA-CLIP contains a mixture of pre-trained weights and randomly initialized weights, instead of training the model end-to-end which may cause under- or over-tting of certain modules, we adopt a multi-stage training strategy to train CMA-CLIP. The training stages are listed below: • Warm-up stage.In this stage, the weights of the image encoder and the text encoder are frozen. We train the sequencewise attention, the modality-wise attention and the MLP modules. • End-to-end training stage.In this stage, we unfreeze the weights of the image encoder and the text encoder, and train all the components together. • Tuning stage.This stage is for multi-task training. The weights of the image encoder, the text encoder and the sequence-wise attention are frozen. We train the modalitywise attentions and MLPs for all the tasks. 4.2.3 Implementation. Detailed training process of CMA-CLIP is summarized in Algorithm 1. For the MRWPA Dataset, all three stages are trained for 20 epochs. For Food101 and Fashion-Gen Datasets, Warm-up stage is trained for 100 epochs and End-to-end training stage is trained for 300 epochs. Since for the two public datasets, they are both single-task classication so the Tuning stage is not needed. During the Warm-up stage, due to the freeze of the CLIP module, only the check-points of the sequence-wise attention, the modality-wise attention and the MLP modules are updated. During the End-to-end training stage, check-points of all three modules are updated. And during the Tuning stage, only the checkpoints of the modality-wise attention the MLP modules are updated. At the end of each training stage, the best check-points with the lowest validation accuracy are used for either next stage’s netuning or inference. 4.3.1 The MRWPA Dataset. We compare CMA-CLIP with the zeroshot performance of raw CLIP and ne-tuned CLIP (ne-tuned using image-title pairs in MRWPA dataset) in terms of the recall at 90% precision for the color, pattern, and style attributes. The results are included in Table 2. We observe that CMA-CLIP consistently outperforms both raw CLIP and ne-tuned CLIP by a large margin across all three attributes. 4.3.2 The Food101 Dataset. On the Food101 dateset, we compare CMA-CLIP with two single-modality baseline methods including BERT [11] and ViT [12], and two multi-modality baseline methods including raw CLIP [30] with same ViT-B/32 and MMBT [21]. Results are included in Table 3. CMA-CLIP achieves the best accuracy of 93.1%, which improves 1% over the a current strong baseline method MMBT. Using only image features achieves 81.8% by ViT, and using only text features achieves 87.2% by BERT. 4.3.3 The Fashion-Gen Dataset. On the Fashion-Gen dataset, we compare CMA-CLIP with multiple SOTA methods including FashionBERT [13], ImageBERT [29], OSCAR [27] and KaleidoBERT [44]. CMA-CLIP achieves the highest accuracy of 93.6%, which improves over KaleidoBERT [44], the previous SOTA method, by 5.5%. We conduct systematic ablation study to validate the eectiveness of modality-wise attention module and sequence-wise attention module by removing them sequentially and comparing the performance with CMA-CLIP. Detailed results are shown in Table On MRWPA, the average recall at 90% precision across the 3 attributes drops from 53.4% to 47.5% after removing the modalitywise attention module. This is because the proportions of titles that contain tokens related to color, pattern, and style are 67%, 25% and 15% respectively. When a title does not contain any tokens related to an attribute, it becomes irrelevant for the classication of that attribute. The performance drop indicates that the modality-wise attention module signicantly improves CMA-CLIP’s robustness against noisy inputs. To illustrate our model’s robustness to input noise, in Table 5 we randomly pick some product image-title examples that CMA-CLIP is able to give correct classication whereas CMA-CLIP without modality-wise attention module cannot. We can clearly observe that in those examples, the product titles do not contain any tokens related to the attribute labels. Furthermore, for these examples, we complete the titles by adding the label related keywords and re-test them. This time, both methods can provide correct classication results which further proves that the modality-wise attention has the ability of ltering out irrelevant information (text without label information is considered as noise). We are not able to select similar examples in the Food101 and Fashion-Gen datasets, because in these two public datasets, there are no images or text that are irrelevant to the classication task. The average recall drops from 47.5% to 45.8% after further removing the sequence-wise attention module. The sequence-wise attention module enhances the context-awareness of the image and text embedding by capturing the ne-grained correlation among image patches and text tokens, and the resulting embedding is expected to yield better results for classications. The performance drop supports this conclusion. We also visualize the result of sequence-wise attention for MRWPA, Food101 and Fashion-Gen datasets in Figure 2. For each text input, we locate the token that is related to the classication task, and visualize the image patches that are most correlated to it by checking the inner product between the query embedding of the text token and the key embeddings of the image patches. In Figure 2, red regions are where the correlation is high. We observe that the sequence-wise attention is able to identify the highly correlated image patches and text tokens across all three datasets. In this paper, we propose the CMA-CLIP, which unies two types of cross-modality attentions: sequence-wise attention, a transformer based attention module that captures the ne-grained relationship between image patches and text tokens, and modality-wise attention, which learns the importance of image and text modalities in order to lter out the irrelevant modality for the classication task. We also design task specic modality-wise attentions and MLPs so that we can leverage a unied network for multi-task classications. We evaluate our method on the MRWPA Dataset, the Food101 dataset and the Fashion-Gen dataset. CMA-CLIP outperforms the pre-trained and ne-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA Dataset for the classications for color, pattern, and style attributes. It also surpasses the state-of-the-art method on the Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on the Food101 Dataset. For the future work, we are interested in training CMACLIP with other datasets to enable the contrastive loss, improving CMA-CLIP’s robustness against noisy labels, and also, exploring semi-supervised learning methods so that unlabeled image-text pairs can be leveraged in the training process to improve model generalizability.