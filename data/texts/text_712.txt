In the last two years, the U.S. government has emphasized the importance of accelerating artiﬁcial intelligence (AI) and machine learning (ML) within the government and across the nation. In particular, the National Artiﬁcial Intelligence Initiative A ct of 2020, which became law on January 1, 2021, provides for a coordinated program across the entire federal government to accelerate AI research and application. The U.S. government can beneﬁt from public artiﬁcial intelligence and machine learning challenges through the development of novel algorithms and participation in experiential training. Although the public, private, and non-proﬁt sectors have a history of leveraging crowdsourcing initiatives to generate novel solutions to difﬁcult problems and engage stakeholders, interest in public competitions has waned in recent years as a result of at least three major factors: (1) a lack of high-quality, high-impact data; (2) a narrow engagement focus on specialized groups; and ( 3) insufﬁcient operationalization of challenge results. Herein we identify common i ssues and recommend approaches to increase the effectiveness of challenges. To address these barriers, enabling the use of public competitions for accelerating AI and ML practice, the U.S. government must leverage methods that protect sensitive data while enabling modelling, enable easier participation, empower deployment of validated models, and incentivize engagement from broad sections of the population. The White House, Congress, and Federal agencies recognize the beneﬁts of artiﬁcial intelligence (AI) and machine learning (ML) and are accelerating AI and ML adoption. In addition to serving citizens more effectively, acceleration of AI and ML research and application is critical for the economic prosperity and national security of the U.S. (Schmidt et al. 2021). In particular, federal age ncies are (1) modernizing infrastructure to support AI and ML developmen t and operations; (2) adopting AI and ML solutions for improving and automating business processes; and (3) offering training to increase staff AI and ML awareness. For example, the U.S. Food and Drug Administration (FDA), through its Technology Modernization Action Plan (T MAP) and Data Modernization Action Plan (DMA P), is upgrading FDA’s technical infrastructure; building processes for innovative product development (such as AI and ML models); developing consistent, repeatable, and modern data ma nagement practices; and developing data science talent within the workforce via recruitment and retention activities, training, and k nowledge sharing (FDA 2019) (FDA 2021). While these actions are advancing the use of AI and ML within the federal government, these steps alone are n ot sufﬁcient to ensu re that the most beneﬁcial applications of AI and ML are prioritized and staff are AI-ready. Federal agenc ie s often offer training on informatics, data science, and data mana gement via virtual self-paced ondemand courses, webin ars, tutorials and documentation, and classroom-based learning (National Library of Medicine 2021). While th is variety of a pproaches provides ﬂexibility to learners, the lack of time constraints and instructor feedback may hinder course co mpletion and reduce the ability of learners to convert gained knowledge to action. Augmenting curre nt training options with experiential learning techniques will boost engagement and emp ower staff to seek and utilize AI and ML solutions. Experiential learnin g is em ployed in a variety of learning settin gs (e.g., med ic a l and business school) to better engage students, enable co llaboration and creativity, and achieve a better real-world understanding of a topic. AI-focused public challenges can provide similar experiential learning beneﬁts, bridging the gap between self-paced virtual tra ining and utilizing AI and ML. The public, private, and nonproﬁt sectors have all utilized crowdsourcing to increase engagement, spur innovation, and solve real-world problems. In fact, the U.S. g overnment has made a signiﬁcant commitment to engag ing citizens in voluntary crowdsourcing activities through its Challenge.gov platform which serves as a central source of governmentwide challenges and prize competitions. Challenge.gov links to competitions on a variety of other federal government platforms including the National Aeronautics an d Space Administration’s (NASA) Tournament Lab (Gustetic et al. 2015) and the FDA’s precisionFDA platform (Altman et al. 2016). Currently, Challenge.gov is hosting 29 active federal government challenges. In addition, 849 challenges were completed on Challenge.gov betwee n 2010-2020. The Department of Health and Human Services (HHS), NASA, and Departmen t of Defense have been crowdsourcing leaders, each running more than 60 challenges since 2010. Moreover, more than 25 Challenge.gov hosted challenges have focused on AI and ML model development sinc e 2018. In the private and non-pro ﬁt sectors, notable crowdsourcing challenge platforms include Kaggle, InnoCentive, TopCoder, and the DREAM Challenges. Technically complex competitions, including AI, ML, and bioinformatics, ar e targeted toward industry, research communities, and educationa l institutions where they strengthen collaboration, engage new organizations and individuals, encourage innovation, supply opportunities for hands-on training, increase openness and availability of high-q uality data sets and tools, and provide indepen dent evaluations of too ls and techniques. Despite this commitm ent from public, private, and nonproﬁt sectors, and the sign iﬁcant gains in educational attainment an d internet access, the interest and perceived effectiveness of crowdsourcing com petitions is decreasing. Notably, the global gross enrollment ratio in tertiary education increased to 38% in 2017 (UNESCO 2019), and there has been a 10% annual increase in worldwide Internet users, topping out at mo re than 4.1 billio n in 2019 (International Telecommunication Union 2019). Within the United States, 83.7 million adults, aged 25 and over, have achieved a bachelor’s degree or higher as of 2020 ( United States Census Bureau 2021). I n addition, Google Trends shows that worldwide web search inter est in the term ”mac hine learning” has been a t or near all-time highs since early 2019. Yet interest in “crowdsourcing” has decreased. Google Trends shows a decrease by more than 50% for the term “crowdsourcing” since peaking in late 2013. Moreover, Citizen Data Science is rate d as entering the “Trough of Disillusionment” in the 2021 Gartner Hype Cycle for Machine Learning and Data Science. This practice paper reports on a novel initial explorator y analysis of U. S. government hosted public cha llenges, and describes opportu nities to reinvigorate competitions by leveraging under utilized an d unused approaches in the crowdsour cing community. Since 2010, the U.S. government has invested signiﬁcantly in crowdsourcing efforts. In ad dition to the innumerable person-hours spent organizing and running challeng e s, the government has allocated more than $204 million dollars in prizes fo r the 878 completed a nd active challenges h osted on Challenge.gov. To increase the eff ectiveness of crowdsourcing com petitions f or advancing AI and ML literacy and applications, U.S. government organized challenges must better align with the expectations placed on them (Simula 2013). There are three major barriers that decrease the effectiveness of U.S. government AI and ML crowdsourcing challenges: (1) a lack of h igh-quality, h igh-impact data, (2) a narrow engagement focus on specialized groups, and (3) insufﬁcient operationaliza tion of challenge results. Insufﬁcient high-quality publicly available high-impact data. In order to protect personally identiﬁable information, deidentiﬁed or synthetic data often is used in place of sensitive data. By using synthetic data instead of, for example, elec tronic health records or human genomes, the resulting models ma y be less applicable to real-world problems, and as such, may dissuade public engagement and discourage operationalization of developed AI and ML models. Narrow engagement focus on specialized groups. Ideally, crowdsourcing competitions would leverage the ”wisdom of the crowd”. However, public challenges ofte n are organized for, and advertised to, relatively small, specialized groups, such as academic data scientists and bioinform a ticians. While specialized knowledge is important, the diverse thinking that results from engagement of a wider audience can lead to new innovations and improved understanding of AI and ML th rough experiential learning. Insufﬁcient o perationalization of challenge results. The post-challenge phase is crucial for extracting knowledge from the challenge results and validating, imp roving, and operationalizing models. However, challenge sponsors, organizers, and participants often spend the majority of their focus on the modeling phase. Without additional fo cus on the post-challenge collaborative phase, challenges will not provide the beneﬁts or have the impac t that they are ca pable of. There are several approaches to evolve and increase the effectiveness of crowdsourcing challen ges for advancing AI and ML use in the U.S. Government. These approaches include: model-to-data, autoML, design-a-thon s, MLOps, and the introduction of novel ince ntives. Model-to-data. Popularized by the D REAM Challenges, model-to-data app roaches can address da ta privacy concerns by evaluating models in a secure private computational environm ent that holds the underlying sensitive data. In this approa c h, participants develop and train their model on nonsensitive data, then containerize and submit their model for evaluation in the private computational environment (Ellrott et al. 2019). A distributed model-to-d ata framework is being used in the current COVID-19 EHR DREAM Challen ge to enable development and evaluation of models that use electronic health records (EHRs) to predict patient speciﬁc risk for COVID-19 associated health outcomes. The U.S. government AI and ML challenge community should continue to adopt model-to-data approaches, enabling challenges that use high-quality, real, high-impact data and pr oduce generalizable models. AutoML. Automated machine le a rning (AutoML) tools automate many of the steps in the machine learning pipeline, including feature engineerin g, model selection, mode l training, and model validation (Waring, Lindvall, and Umeton 2020). There are a number of vendors (e. g., Amazon Web Services (AWS), Google Cloud, Microsoft Azure, DataRobot) and open source tools (e.g., H2O, R, Python) that provide autoML tools. The U.S. g overnment AI an d ML challenge community should leverage autoML to expand access to challenges and increase e fﬁciency. For example, beginner tracks of AI and ML challenges can be hosted on user-friendly point-an d-click interfaces that simplify modeling-based decision making. Design-a-thons. Similar to hack-a-thons, design-a-thons engage a broad array of stakeholders to ideate possible solutions to real-world problems. Importantly, design-a-thons are welcoming to a b roader audience by not requiring specialized subject matter or programming knowledge for participation. For example, the precisionFDA platform recently hosted the FDA New Era of Smarter Food Safety Low- or No-Cost Tech-Enabled Traceability Challenge to promote ideation and innovation of hardware, software, and advanced analytics solutions for enabling digital traceability along the entire food system. By requiring PowerPoint and video presentations, rather than an implemented solution, this ch allenge enabled broader particip ation, leading to more than 90 subm issions. The U.S. government AI and ML challenge community shou ld leverage design-a-thons to engage employees and the public in the prioritization of AI and ML use cases. MLOps. Machine learning operations (MLOps) is a set of ma c hine learning and DevOps practices for developing, deploying, and maintaining machine learning solutions, which includes model and data versioning, pipeline automation, testing, continuous integration and continuous delivery, and monitoring. The U.S. government AI and ML challenge commu nity should adopt MLOps in the post-challenge phase to ensure that community developed models can be operationalized to beneﬁt the government and public by being testable, transparent, scalable, secure, and reproducible. For example, the three top performing teams from the precisionFDA NCI- CPTAC Multi-omics Enabled Sample Mislabeling Correction Challenge participated in a collaborative po st-c hallenge phase w ith the challenge organizers to (1) validate their computational methods for id e ntifying and correcting samp le mislabeling on independent datasets and (2) generate a single-b e st consensus pipeline. This consensus appr oach, named COrrection of Sample Mislabelin g by Omics (COSMO), was d eveloped and validated following MLOps considerations includ ing scalability, reproducibility, and deployability via the u se of Docker containerization and Nextﬂow (Yoo et al. 2021). Novel incentives. Novel incentives for top perf ormance, such as fast-tracking pilots, par tnerships, and contracts, will boost engagement while ensuring that clear steps are in place to reward winning m odels. For example, Artiﬁcial Intelligence Tech Sprin ts, o rganized by the National Artiﬁcial Intelligence Institute (NAII), award both monetary prizes and opportunities for partnersh ip and piloting of selected prototypes (National Artiﬁcial Intelligence Institute 2020). M oreover, while only 3.3% of the 432 cha llenges completed on the Kaggle platform from 2010-2020 utilized jobs as an incentive, there is an 80% increase in the median number of participating teams as compared to the 70% of Kaggle challenges that utilized monetary incentives. The U.S. government is leading a national initiative to accelerate AI/ML research and development, and upsk ill the workforce to enable AI /ML integrattion. Public crowdsourcing challenges have long been used as a tool for innovation and stakeholder engagement. For example, from 20062009 Netﬂix ran the Netﬂix Prize public competition, which offered a grand prize of $1,000,000 to the top per forming team tha t could improve the prediction of user ratings of ﬁlms by 10%. Through this p ublic competition, Netﬂix was able to d irectly engage with over 40,000 registered teams that participated in the challenge, enc ourage advancements in the ﬁeld of collaborative ﬁltering (Koren and Bell 20 15), and increase the public’s awareness of Netﬂix and recommendation systems. Improved utilization of pu blic crowdsourcing competitio ns can provide the U.S. government with similar beneﬁts, including AI/ML innovation and improved workforce AI-readiness. To achieve these b eneﬁts, U.S. government sponsored public challenges must democra tize challenge participation (e.g., through autoML, design-a-thons, and novel incentives), enable validation on real-world data (e.g., v ia model-to-data), and focus on operationalizing high-perf orming validated models (e. g., u sing MLOps). In addition to the crowdsourcing barriers and improvements discu ssed in this paper, more study is needed to identify and quantify the factors that inﬂuence the success of public competitions. More analysis of pu blic challenges, such as those hosted by Challen ge.gov and Kaggle, is needed to understan d all predictive factors. To empower this analysis, the crowdsourcing community, including the Federal Community of Practice on Crowdsourcinig and Citizen Scienc (FedCCS), must improve the measurement and documentatio n of challen ge outcomes. Measurement and documentation of outcome s, such as deployed models, scientiﬁc p ublications, and educa tional attainment will strengthen subsequen t recommendations and ultim a te ly increase the impact of federal AI/ML public challenges.