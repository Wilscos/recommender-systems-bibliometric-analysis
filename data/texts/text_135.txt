We study Comparative Preference Classiﬁcation (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which quality CPC models can signiﬁcantly beneﬁt applications such as comparative question answering and review-based recommendation. Among the existing approaches, nondeep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset present a signiﬁcant improvement on the F1 scores over the best existing CPC approaches. Comparative Preference Classiﬁcation (CPC) is a natural language processing (NLP) task that predicts whether a preference comparison exists between two entities in a sentence and, if so, which entity wins the game. For example, given the sentence: Python is better suited for data analysis than MATLAB due to the many available deep learning libraries, a decisive comparison exists between Python and MATLAB and comparatively Python is preferred over MATLAB in the context. The CPC task can profoundly impact various real-world application scenarios. Search engine users may query not only factual questions but also comparative ones to meet their speciﬁc information needs (Gupta et al., 2017). Recommendation providers can analyze product reviews with comparative statements to understand the advantages and disadvantages of the product comparing with similar ones. Several models have been proposed to solve this problem. Panchenko et al. (2019) ﬁrst formalize the CPC problem, build and publish the CompSent-19 dataset, and experiment with numerous general machine learning models such as Support Vector Machine (SVM), representation-based classiﬁcation, and XGBoost. However, these attempts consider CPC as a sentence classiﬁcation while ignoring the semantics and the contexts of the entities (Ma et al., 2020). ED-GAT (Ma et al., 2020) marks the ﬁrst entityaware CPC approach that captures long-distance syntactic relations between the entities of interest by applying graph attention networks (GAT) to dependency parsing graphs. However, we argue that the disadvantages of such an approach are clear. Firstly, ED-GAT replaces the entity names with “entityA” and “entityB” for simplicity and hence deprives their semantics. Secondly, ED-GAT has a deep architecture with ten stacking GAT layers to tackle the long-distance issue between compared entities. However, more GAT layers result in a heavier computational workload and reduced training stability. Thirdly, although the competing entities are typically connected via multiple hops of dependency relations, the unordered tokens along the connection path cannot capture either global or local high-quality semantic context features. In this work, we propose a Sentiment Analysis Enhanced COmparative classiﬁcation Network (SAECON), a CPC approach that considers not only syntactic but also semantic features of the entities. The semantic features here refer to the context of the entities from which a sentiment analysis model can infer the sentiments toward the entities. Speciﬁcally, the encoded sentence and entities are fed into a dual-channel context feature extractor to learn the global and local context. In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneﬁcial to the comparison classiﬁcation. ABSA aims to detect the speciﬁc emotional inclination toward an aspect within a sentence (Ma et al., 2018; Hu et al., 2019; Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020). For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food. These aspect entities, such as service, staff, and food, are studied individually. The well-studied ABSA approaches can be beneﬁcial to CPC when the compared entities in a CPC sentence are considered as the aspects in ABSA. Incorporating the individual sentiments learned by ABSA methods into CPC has several advantages. Firstly, for a comparison to hold, the preferred entity usually receives a positive sentiment while its rival gets a relatively negative one. These sentiments can be easily extracted by the strong ABSA models. The contrast between the sentiments assigned to the compared entities provides a vital clue for an accurate CPC. Secondly, the ABSA models are designed to target the sentiments toward phrases, which bypasses the complicated and noisy syntactic relation path. Thirdly, considering the scarcity of the data resource of CPC, the abundant annotated data of ABSA can provide sufﬁcient supervision signal to improve the accuracy of CPC. There is one challenge that blocks the knowledge transfer of sentiment analysis from the ABSA data to the CPC task: domain shift. Existing ABSA datasets are centered around speciﬁc topics such as restaurants and laptops, while the CPC data has mixed topics (Panchenko et al., 2019) that are all distant from restaurants. In other words, sentences of ABSA and CPC datasets are drawn from different distributions, also known as domains. The difference in the distributions is referred to as a “domain shift” (Ganin and Lempitsky, 2015; He et al., 2018) and it is harmful to an accurate knowledge transfer. To mitigate the domain shift, we design a domain adaptive layer to remove the domainspeciﬁc feature such as topics and preserve the domain-invariant feature such as sentiments of the text so that the sentiment analyzer can smoothly transfer knowledge from sentiment analysis to comparative classiﬁcation. CPC originates from the task of Comparative Sentence Identiﬁcation (CSI) (Jindal and Liu, 2006). CSI aims to identify the comparative sentences. Jindal and Liu (2006) approach this problem by Class Sequential Mining (CSR) and a Naive Bayesian classiﬁer. Building upon CSI, Panchenko et al. (2019) propose the task of CPC, release CompSent19 dataset, and conduct experimental studies using traditional machine learning approaches such as SVM, representation-based classiﬁcation, and XGBoost. However, they neglect the entities in the comparative context (Panchenko et al., 2019). ED-GAT (Ma et al., 2020), a more recent work, uses the dependency graph to better recognize longdistance comparisons and avoid falsely identifying unrelated comparison predicates. However, it fails to capture semantic information of the entities as they are replaced by “entityA” and “entityB”. Furthermore, having multiple GAT layers severely increases training difﬁculty. ABSA derives from sentiment analysis (SA) which infers the sentiment associated with a speciﬁc entity in a sentence. Traditional approaches of ABSA utilize SVM for classiﬁcation (Kiritchenko et al., 2014; Wagner et al., 2014; Zhang et al., 2014) while neural network-based approaches employ variants of RNN (Nguyen and Shirai, 2015; Aydin and Güngör, 2020), LSTM (Tang et al., 2016; Wang et al., 2016; Bao et al., 2019), GAT (Wang et al., 2020), and GCN (Pouran Ben Veyseh et al., 2020; Xu et al., 2020). More recent workswidely use complex contextualized NLP models such as BERT (Devlin et al., 2019). Sun et al. (2019) transform ABSA into a Question Answering task by constructing auxiliary sentences. Phan and Ogunbona (2020) build a pipeline of Aspect Extraction and ABSA and used wide and concentrated features for sentiment classiﬁcation. ABSA is related to CPC by nature. In general, entities with positive sentiments are preferred over the ones with neutral or negative sentiments. Therefore, the performance of a CPC model can be enhanced by the ABSA techniques. Figure 1: Pipeline of SAECON. Sentences from two domains shown in the gray box are fed into text encoder and dependency parser. The resultant representations of the entities from three substructures are discriminated by different colors (see the legend in the corner). “Cls.” is short for classiﬁer. In this section, we ﬁrst formalize the problem and then explain SAECON in detail. The pipeline of SAECON is depicted in Figure 1 with essential notations. 3.1 Problem Statement CPCGiven a sentencesfrom the CPC corpus Dwithntokens and two entitieseande, a CPC model predicts whether there exists a preference comparison betweeneandeinsand if so, which entity is preferred over the other. Potential results can beBetter(ewins),Worse(ewins), or None (no comparison exists). ABSAGiven a sentencesfrom the ABSA corpusDwithmtokens and one entitye, ABSA identiﬁes the sentiment (positive, negative, or neutral) associated with e. We denote the source domains of the CPC and ABSA datasets byDandD.DandDcontain samples that are drawn fromDandD, respectively.DandDare similar but different in topics which produces a domain shift. We usesto denote sentences inD∪ DandEto denote the entity sets for simplicity in later discussion.|E| = 2if s ∈ Dand |E| = 1 otherwise. 3.2 Text Feature Representations A sentence is encoded by its word representations via a text encoder and parsed into a dependency graph via a dependency parser (Chen and Manning, 2014). Text encoder, such as GloVe (Pennington et al., 2014) and BERT (Devlin et al., 2019), maps a wordwinto a low dimensional embedding w ∈ R. GloVe assigns a ﬁxed vector while BERT computes a tokenrepresentation by its textual context. The encoding output ofsis denoted by S= {w, . . . , e, . . . , e, . . . , w}whereedenotes the embedding of entityi,wdenotes the embedding of a non-entity word, and w, e∈ R. The dependency graph ofs, denoted byG, is obtained by applying a dependency parser tossuch as Stanford Parser (Chen and Manning, 2014) or spaCy.Gis a syntactic view ofs(Marcheggiani and Titov, 2017; Li et al., 2016) that is composed of vertices of words and directed edges of dependency relations. Advantageously, complex syntactic relations between distant words in the sentence can be easily detected with a small number of hops over dependency edges (Ma et al., 2020). Global Semantic ContextTo model more extended context of the entities, we use a bidirectional LSTM (BiLSTM) to encode the entire sentence in both directions. Bi-directional recurrent neural network is widely used in extracting semantics (Li et al., 2019). Given the indices ofe andeins, the global context representationsh andhare computed by averaging the hidden outputs from both directions. −−→h,←−−h= BiLSTM(S)[e Local Syntactic ContextIn SAECON, we use a dependency graph to capture the syntactically neighboring context of entities that contains words or phrases modifying the entities and indicates comparative preferences. We apply a Syntactic Graph Convolutional Network (SGCN) (Bastings et al., 2017; Marcheggiani and Titov, 2017) toGto compute the local context featurehandhfore ande, respectively. SGCN operates on directed dependency graphs with three major adjustments compared with GCN (Kipf and Welling, 2017): considering the directionality of edges, separating parameters for different dependency labels, and applying edge-wise gating to message passing. GCN is a multilayer message propagation-based graph neural network. Given a vertexvinGand its neighborsN(v), the vertex representation ofv on the (j + 1)th layer is given as whereρ(·)denotes an aggregation function such as mean and sum,W∈ Randb∈ Rare trainable parameters, anddand ddenote latent feature dimensions of the(j + 1)th and the jth layers, respectively. SGCN improves GCN by considering different edge directions and diverse edge types, and assigns different parameters to different directions or labels. However, there is one caveat: the directionalitybased method cannot accommodate the rich edge type information; the label-based method causes combinatorial over-parameterization, increased risk of overﬁtting, and reduced efﬁciency. Therefore, we naturally arrive at a trade-off of using direction-speciﬁc weights and label-speciﬁc biases. The edge-wise gating can select impactful neighbors by controlling the gates for message propagation through edges. The gate on thejth layer of an edge between vertices u and v is deﬁned as wheredandldenote the direction and label of edge(u, v),βandγare trainable parameters, and σ(·) denotes the sigmoid function. Summing up the aforementioned adjustments on GCN, the ﬁnal vertex representation learning is Vectors ofSserve as the input representations hto the ﬁrst SGCN layer. The representations corresponding toeandeare the output {h, h} with dimension d. We have discussed in Section 1 that ABSA inherently correlates with the CPC task. Therefore, it is natural to incorporate a sentiment analyzer into SAECON as an auxiliary task to take advantage of the abundant training resources of ABSA to boost the performance on CPC. There are two paradigms for auxiliary tasks: (1) incorporating ﬁxed parameters that are pretrained solely with the auxiliary dataset; (2) incorporating the architecture only with untrained parameters and jointly optimizing them from scratch with the main task simultaneously (Li et al., 2018; He et al., 2018; Wang and Pan, 2018). Option (1) ignores the domain shift betweenD andD, which degrades the quality of the learned sentiment features since the domain identity information is noisy and unrelated to the CPC task. SAECON uses option (2). For a smooth and efﬁcient knowledge transfer fromDtoDunder the setting of option (2), the ideal sentiment analyzer only extracts the textual feature that is contingent on sentimental information but orthogonal to the identity of the source domain. In other words, the learned sentiment features are expected to be discriminative on sentiment analysis but invariant with respect to the domain shift. Therefore, the sentiment features are more aligned with the CPC domain Dwith reduced noise from domain shift. In SAECON, we use a gradient reversal layer (GRL) and a domain classiﬁer (DC) (Ganin and Lempitsky, 2015) for the domain adaptive sentiment feature learning that maintains the discriminativeness and the domain-invariance. GRL+DC is a straightforward, generic, and effective modiﬁcation to neural networks for domain adaptation (Kamath et al., 2019; Gu et al., 2019; Belinkov et al., 2019; Li et al., 2018). It can effectively close the shift between complex distributions (Ganin and Lempitsky, 2015) such as Dand D. LetAdenote the sentiment analyzer which alternatively learns sentiment information fromDand provides sentimental clues to the compared entities inD. Speciﬁcally, each CPC instance is split into two ABSA samples with the same text before being fed intoA(see the “Split to 2” in Figure 1). One takes eas the queried aspect the other takes e. A(S, G, E) = h,h, andh∈ R. These outputs are later sent through a GRL to not only the CPC and ABSA predictors shown in Figure 1 but also the DC to predict the source domainyofswhere y= 1ifs ∈ Dotherwise0. GRL, trainable by backpropagation, is transparent in the forward pass (GRL(x) = x). It reverses the gradients in the backward pass as Herexis the input to GRL,αis a hyperparameter, andIis an identity matrix. During training, the reversed gradients maximize the domain loss, forcingAto forget the domain identity via the backpropagation and mitigating the domain shift. Therefore, the outputs ofAstay invariant to the domain shift. But as the outputs ofAare also optimized for ABSA predictions, the distinctiveness with respect to sentiment classiﬁcation is retained. Finally, the selection ofAis ﬂexible as it is architecture-agnostic. In this paper, we use the LCF-ASC aspect-based sentiment analyzer proposed by Phan and Ogunbona (2020) in which two scales of representations are concatenated to learn the sentiments to the entities of interest. SAECON optimizes three classiﬁcation errors overall for CPC, ABSA, and domain classiﬁcation. For CPC task, features for local context, global context, and sentiment are concatenated:h= [h; h; h],i ∈ {1, 2}, andh∈ R. GivenF,F,F, andFbelow denoting fullyconnected neural networks with non-linear activation layers, CPC, ABSA, domain predictions are obtained by ˆy= δ(F([F(h); F(h ˆy= δ(F(h ˆy= δ(F(GRL(A(S, G whereδdenotes the softmax function. With the predictions, SAECON computes the cross entropy losses for the three tasks asL,L, andL, respectively. The label ofLisy. The computations of the losses are omitted due to the space limit. In summary, the objective function of the proposed model SAECON is given as follows, L = L+ λL+ λL+ λreg(L2), whereλandλare two weights of the losses, and λis the weight of anL2regularization. We denote λ = {λ, λ, λ}. In the actual training, we separate the iterations of CPC data and ABSA data and input batches from the two domains alternatively. Alternative inputs ensure that the DC receives batches with different labels evenly and avoid overﬁtting to either domain label. A stochastic gradient descent based optimizer, Adam (Kingma and Ba, 2015), is leveraged to optimize the parameters of SAECON. Algorithm 1 in Section A.1 explains the alternative training paradigm in detail. 4.1 Experimental Settings DatasetCompSent-19 is the ﬁrst public dataset for the CPC task released by Panchenko et al. (2019). It contains sentences with entity annotations. The ground truth is obtained by comparing the entity that appears earlier (e) in the sentence with the one that appears later (e). The dataset is split by convention (Panchenko et al., 2019; Ma et al., 2020): 80% for training and 20% for testing. During training, 20% of the training data of each label composes the development set for model selection. The detailed statistics are given in Table 1. Three datasets of restaurants released in SemEval 2014, 2015, and 2016 (Pontiki et al., 2014, 2015; Xenos et al., 2016) are utilized for the ABSA task. We join their training sets and randomly sample instances into batches to optimize the auxiliary objectiveL. The proportions ofPOS,NEU, and NEG instances are 65.8%, 11.0%, and 23.2%. Note.The rigorous deﬁnition ofNonein CompSent-19 is that the sentence does not contain a comparison between the entities rather than that Table 1: Statistics of CompSent-19. The rows of Flipping labels and Upsampling show the numbers of the augmented datasets to mitigate label imbalance. entities are both preferred or disliked. Although the two deﬁnitions are not mutually exclusive, we would like to provide a clearer background of the CPC problem. Imbalanced DataCompSent-19 is badly imbalanced (see Table 1).Noneinstances dominate in the dataset. The other two labels combined only account for 27%. This critical issue can impair the model performance. Three methods to alleviate the imbalance are tested. Flipping labels: Consider the order of the entities, an originalBetterinstance will become aWorseone and vice versa if querying(e, e)instead. We interchange theeande of allBetterandWorsesamples so that they have the same amount. Upsampling: We upsampleBetterandWorseinstances with duplication to the same amount ofNone. Weighted loss: We upweight the underpopulated labelsBetter andWorsewhen computing the classiﬁcation loss. Their effects are discussed in Section 4.2. Evaluation MetricThe F1 score of each label and the micro-averaging F1 score are reported for comparison. We use F1(B), F1(W), F1(N), and micro-F1 to denote them. The micro-F1 scores on the development set are used as the criteria to pick the best model over training epochs and the corresponding test performances are reported. ReproducibilityThe implementation of SAECON is publicly available on GitHub. Details for reproduction are given in Section A.2. Baseline ModelsSeven models experimented in Panchenko et al. (2019) and the state-of-the-art ED-GAT (Ma et al., 2020) are considered for performance comparison and described in Section A.3. FixedBERT embeddings are used in our experiments same as ED-GAT for comparison fairness. Comparing with BaselinesWe report the best performances of baselines and SAECON in Table 2. SAECON with BERT embeddings achieves the highest F1 scores comparing with all baselines, which demonstrates the superior ability of SAECON to accurately classify entity comparisons. The F1 scores forNone, i.e., F1(N), are consistently the highest in all rows due to the data imbalance whereNoneaccounts for the largest percentage.Worsedata is the smallest and thus is the hardest to predict precisely. This also explains why models with higher micro-F1 discussed later usually achieve larger F1(W) given that their accuracy values on the majority class (None) are almost identical. BERT-based models outperform GloVe-based ones, indicating the advantage of contextualized embeddings. In later discussion, the reported performances of SAECON and its variants are based on the BERT version. The performances of the GloVe-based SAECON demonstrate similar trends. SVM-Tree 68.12 53.35 13.90 78.13 BERT-CLS 83.12 69.62 50.37 89.84 AvgWE-G 76.32 48.28 20.12 86.34 AvgWE-B 77.64 53.94 26.88 87.47 ED-GAT-G 82.73 70.23 43.30 89.84 ED-GAT-B 85.42 71.65 47.29 92.34 SAECON-G 83.78 71.06 45.90 91.05 SAECON-B 86.74 77.10 54.08 92.64 Table 2: Performance comparisons between the proposed model and baselines on F1 scores (%). “-B” and “-G” denote different versions of the model using BERT (Devlin et al., 2019) and GloVe (Pennington et al., 2014) as the input embeddings, respectively. All reported improvements over the best baselines are statistically signiﬁcant with p-value < 0.01. Ablation StudiesAblation studies demonstrate the unique contribution of each part of the proposed model. Here we verify the contributions of the following modules: (1) The bi-directional global context extractor (BiLSTM); (2) The syntactic local context extractor (SGCN); (3) The domain adaptation modules ofA(GRL); (4) The entire auxiliary sentiment analyzer, including its dependent GRL+DC (A+GRL for short). The results are presented in Table 3. Four points worth noting. Firstly, the SAECON with all modules achieves the best performance on three out of four metrics, demonstrating the effectiveness of all modules (SAECON vs. the rest); Secondly, the synergy ofAand GRL improves the performance (−(A+GRL)vs. SAECON) whereas theAwithout domain adaptation hurts the classiﬁcation accuracy instead (−GRL vs. SAECON), which indicates that the auxiliary sentiment analyzer is beneﬁcial to CPC accuracy only with the assistance of GRL+DC modules; Thirdly, removing the global context causes the largest performance deterioration (−BiLSTM vs. SAECON), showing the signiﬁcance of long-term information. This observation is consistent with the ﬁndings of Ma et al. (2020) in which eight to ten stacking GAT layers are used for global feature learning; Finally, the performances also drop after removing the SGCN (−SGCN vs. SAECON) but the drop is less than removing the BiLSTM. Therefore, local context plays a less important role than the global context (−SGCN vs. −BiLSTM). −(A+GRL) 85.97 74.82 52.44 92.45 Table 3: Ablation studies on F1 scores (%) between SAECON and its variants with modules disabled. “−X” denotes a variant without module X. Removing A will also remove GRL+DC. Hyperparameter SearchingWe demonstrate the inﬂuences of several key hyperparameters. Such hyperparameters include the initial learning rate (LR,η), feature dimensions (d = {d, d, d}), regularization weightλ, and the conﬁgurations of SGCN such as directionality, gating, and layer numbers. For LR,d, andλin Figures 2a, 2b, and 2c, we can observe a single peak for F1(W) (green curves) and ﬂuctuating F1 scores for other labels and the micro-F1 (blue curves). In addition, the peaks of micro-F1 occur at the same positions of F1(W). This indicates that the performance onWorseis themostinﬂuential factor to the micro-F1. These observations help us locate the optimal settings and Figure 2: Searching and sensitivity for four key hyperparameters of SAECON in F1 scores. also show the strong learning stability of SAECON. Figure 2d focuses on the effect of SGCN layer numbers. We observe clear oscillations on F1(W) and ﬁnd the best scores at two layers. More layers of GCN result in oversmoothing (Kipf and Welling, 2017) and hugely downgrade the accuracy, which is eased but not entirely ﬁxed by the gating mechanism. Therefore, the performances slightly drop on larger layer numbers. Table 4 shows the impact of directionality and gating. Turning off either the directionality or the gating mechanism (“73” or “37”) leads to degraded F1 scores. SGCN without modiﬁcations (“77”) drops to the poorest micro-F1 and F1(W). Although its F1(N) is the highest, we hardly consider it a good sign. Overall, the beneﬁts of the directionality and gating are veriﬁed. Directed Gating Micro. F1(B) F1(W) F1(N) Table 4: Searching and sensitivity for the directionality and gating of SGCN by F1 scores (%). Alleviating Data ImbalanceThe label imbalance severely impairs the model performance, especially on the most underpopulated labelWorse. The aforementioned imbalance alleviation methods are tested in Table 5. The Original (OR) row is a control experiment using the raw CompSent-19 without any weighting or augmentation. The optimal solution is the weighted loss (WL vs. the rest). One interesting observation is that data augmentation such as ﬂipping labels and upsampling cannot provide a performance gain (OR vs. FL and OR vs. UP). Weighted loss performs a bit worse on F1(N) but consistently better on the other metrics, especially onWorse, indicating that it effectively alleviates the imbalance issue. In practice, static weights found via grid search are assigned to different labels when computing the cross entropy loss. We leave the exploration of dynamic weighting methods such as the Focal Loss (Lin et al., 2017) for future work. Table 5: Performance analysis with F1 scores (%) for different methods to mitigate data imbalance. Alternative TrainingOne novelty of SAECON is the alternative training that allows the sentiment analyzer to learn both tasks across domains. Here we analyze the impacts of different batch ratios (BR) and different domain shift handling methods during the training. BR controls the number of ratio of batches of the two alternative tasks in each training cycle. For example, a BR of2 : 3sends 2 CPC batches followed by 3 ABSA batches in each iteration. Figure 3a presents the entire training time for ten epochs with different BR. A larger BR takes shorter time. For example, a BR of 1:1 (the leftmost bar) takes a shorter time than 1:5 (the yellow bar). Figure 3b presents the micro-F1 scores for different BR. We observe two points: (1) The reported performances differ slightly; (2) Generally, the performance is better when the CPC batches are less than ABSA ones. Overall, the hyperparameter selection tries to ﬁnd a “sweet spot” for effectiveness and efﬁciency, which points to the BR of 1:1. Figure 4 depicts the performance comparisons of SAECON (green bars), SAECON−GRL (the “−GRL” in Figure 3, orange bars), and SAECON Figure 3: Analyses for batch ration (BR). The values of micro-F1 are actual numbers minus 0.85 for the convenience of visualization. with pretrained and ﬁxed parameters ofA(the option (1) mentioned in Section 3.4, blue bars). They represent different levels of domain shift mitigation: The pretrained and ﬁxedAdoes NOT handle the domain shift at all; The variant−GRL only attempts to implicitly handle the shift by alternative training with different tasks to converge in the middle although the domain difference can be harmful to both objectives; SAECON, instead, explicitly uses GRL+DC to mitigate the domain shift between Dand Dduring training. As a result, SAECON achieves the best performance especially on F1(W),−GRL gets the second, and the “option (1)” gets the worst. These demonstrate that (1) the alternative training (blue vs. green) for an effective domain adaptation is necessary and (2) there exists a positive correlation between the level of domain shift mitigation and the model performance, especially on F1(W) and F1(B). A better domain adaptation produces higher F1 scores in the scenarios where datasets in the domain of interest, i.e., CPC, is unavailable. Figure 4: Visualization of the domain shift mitigation. In this section, we qualitatively exemplify the contribution of the sentiment analyzerA. Table 6 reports four example sentences from the test set of CompSent-19. The entitieseandeare highlighted together with the corresponding sentiment predicted byA. The column “Label” shows the ground truth of CPC. The “∆” column computes the sentiment distances between the entities. We assign+1,0, and−1to sentiment polarities of POS,NEU, andNEG, respectively.∆is computed by the sentiment polarity ofeminus that ofe. Therefore, a positive distance suggests thatereceives a more positive sentiment fromAthane and vice versa. InS1, sentiments to Ethernet and USB are predicted positive and negative, respectively, which can correctly imply the comparative label asBetter.S2is aWorsesentence with Bash predicted negative, Python predicted neutral, and a resultant negative sentiment distance−1. For S3andS4, the entities are assigned the same polarities. Therefore, the sentiment distances are both zeros. We can easily tell that preference comparisons do not exist, which is consistent with the ground truth labels. Due to the limited space, more interesting case studies are presented in Section A.4. This paper proposes SAECON, a CPC model that incorporates a sentiment analyzer to transfer knowledge from ABSA corpora. Speciﬁcally, SAECON utilizes a BiLSTM to learn global comparative features, a syntactic GCN to learn local syntactic information, and a domain adaptive auxiliary sentiment analyzer that jointly learns from ABSA corpora and CPC corpora for a smooth knowledge transfer. An alternative joint training scheme enables the efﬁcient and effective information transfer. Qualitative and quantitative experiments veriﬁed the superior performance of SAECON. For future work, we will focus on a deeper understanding of CPC data augmentation and an exploration of weighting loss methods for data imbalance. We would like to thank the reviewers for their helpful comments. The work was partially supported by NSF DGE-1829071 and NSF IIS-2106859. This section states the broader impact of the proposed CPC model. Our model is designed speciﬁcally for the comparative classiﬁcation scenarios in NLP. Users can use our model to detect whether a comparison exists between two entities of interest within the sentences of a particular sentential corpus. For example, a recommender system equipped with our model can tell whether a product is compared with a competitor and, further, which is preferred. In addition, Review-based platforms can utilize our model to decide which items are widely welcomed and which are not. Our model, SAECON, is tested with the CompSent-19 dataset which has been anonymized during the process of collection and annotation. For the auxiliary ABSA task, we also use an anonymized public dataset from SemEval 2014 to 2016. Therefore, our model will not cause potential leakages of user identity and privacy. We would like to call for attention to CPC as it is a relatively new task in NLP research.