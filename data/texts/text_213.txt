<title>Multi-Level Visual Similarity Based Personalized Tourist Attraction  Recommendation Using Geo-Tagged Photos  Ling Chen* </title> <title>Dandan Lyu </title> <title>Shanshan Yu </title> <title>Gencai Chen </title> With the advent of the smart era, people can easily share their travel experiences on social platforms, e.g.,  uploading some wonderful photos during a trip, forming abundant geo-tagged photos [1-3]. Users can manually  search through the miscellaneous online information to find a few tourist attractions that meet their travel  preferences. This usually costs much time and energy. Tourist attraction recommendation systems [4] can  provide  users  with  great  convenience,  as  it  can  infer  their  travel  preferences  from  travel  history  and  automatically plan their trips.  In the  past decade, geo-tagged photo based tourist  attraction recommendation has become one of the  research  hotspots.  In  the  early  years,  researchers  mainly  consider  users’  travel  preferences  to  make  personalized recommendation [5, 6]. In recent years, various types of side information have been introduced to  get more appropriate recommendation results [7-9]. Owing to the efficiency and effectiveness of deep neural  networks (DNNs) in image processing, the visual contents of photos have gradually received attention. Existing  visual content based methods usually first extract features from the visual contents of photos, and then use  these features as prior knowledge to constrain the training of the recommendation model constructed based on  users’  travel  history  [10-12].  These  methods  cannot  extract  visual  features  adaptive  to  tourist  attraction  recommendation, as the extraction of visual features is mostly guided by computer vision tasks that have no  relationship with the recommendation scenario.  To deal with the aforementioned problem, VPOI was proposed [13], which jointly extracts features from the  visual contents of photos, classifies the photos according to who they are taken by and where they are taken,  and factorizes the user-tourist attraction interaction matrix for personalized recommendation. Given a photo,  this method independently  exploits the user  and tourist attraction information to partition other photos into  visually similar/non-similar groups, assuming that the similarities of the photos taken by the same user or taken  at the same tourist attraction are higher than those of the other photos. However, this method cannot capture  multi-level visual similarity as shown in Figure 1, i.e., given a photo, its visual similarity with another photo taken  by the same user at the same tourist attraction ranks first, as they usually capture same objects from different  directions; its visual similarity with another photo taken by a different user at the same tourist attraction ranks  second, as they usually capture same objects from different directions and under different lighting conditions;  its visual similarity with another photo taken by the same user at a different tourist attraction ranks third, as they  usually capture similar sceneries that the user prefers; its visual similarity with another photo taken by a different  user at a different tourist attraction ranks last, as the travel preferences of different users and the sceneries of  different tourist attractions usually differ greatly. In addition, this method treats different photos taken by a user  or taken at a tourist attraction equally, without differentiating their significances.  To deal with the aforementioned problems, we propose multi-level visual similarity based personalized tourist  attraction  recommendation  using  geo-tagged  photos  (MEAL).  By  crossing  the  user  and  tourist  attraction  information of photos, we define multi-level similarity for visual content embedding.  The crucial contributions of this paper are summarized as below:   1) Propose MEAL, combining the visual representations obtained by fusing the visual features of photos  through  self-attention  mechanism  and  the  latent  factors  obtained  by  factorizing  the  user-tourist  attraction  interaction  matrix  to  obtain  the  final  embeddings  of  users  and  tourist  attractions,  which  can  capture  the  significances of different photos for representing users and tourist attractions.  2) Propose multi-level similarity aware visual content embedding for geo-tagged photos, trying to ensure that,  considering the visual similarities with a given photo, other photos are ranked as follows: photos taken by the  same user at the same tourist attraction > photos taken by different users at the same tourist attraction > photos  taken by  the same user at different  tourist attractions >  photos taken by  different users at different  tourist  attractions, which can fully exploit the user and tourist attraction information of photos.  3) Evaluate the proposed method on a real-world dataset crawled from Flickr and make comparison with the  state-of-the-art methods. The experimental results show the advantage of this method.  The rest of this paper  is organized as follows. Section 2 reviews the  related work. Section 3 gives  the  preliminaries  of  this  paper  and defines  the  research  problem.  Section  4  introduces  the proposed  method  MRATE in detail. Section 5 presents the experimental settings and results. Finally, Section 6 concludes the  paper and gives a brief discussion of the future work.  In this part, some recent works closely related to our work are introduced, consisting of geo-tagged photo based  tourist attraction recommendation and deep metric learning based visual content embedding.  Geo-tagged  photos  imply  the  travel  history  of  users,  which  provide  rich  data  for  tourist  attraction  recommendation.  In  the  early  years,  researchers  mainly  consider  users’  travel  preferences  to  make  personalized recommendation [5, 6]. Clements et al. [5] firstly computed the similarities between users based  on the Gaussian kernel convolution values of their geotag distributions in a common visited city, and then  recommended tourist attractions in a previously unvisited city according to the rankings of users with similar  travel preferences. Popescu and Grefenstette [6] also followed the idea of collaborative filtering, but they applied  different similarity measures compared to [5].  To get more appropriate recommendation results, various types of side information have been introduced  [7-9]. Majid et al. [8, 9] proposed to recommend tourist attractions and tourist routes by considering users’ travel  preferences under different contexts (e.g., season and weather). Bhargava et al. [7] jointly factorized usertourist attraction-activity-time tensor, tourist attraction-activity matrix, tourist attraction-tourist attraction similarity  matrix, and activity-activity correlation matrix to provide multi-dimensional recommendation.  Owing to the efficiency and effectiveness of DNNs in image processing, the visual contents of photos have  gradually received attention. Existing visual content based methods usually first extract features from the visual  contents  of  photos,  and  then  use  these  features  as  prior  knowledge  to  constrain  the  training  of  the  recommendation model constructed based on users’ travel history [10-12]. DTMMF [10] firstly extracted the  gender and age information of people appearing in photos to represent users and tourist attractions, based on  which user-user and tourist attraction-tourist attraction similarities were calculated to constrain the factorization  of the user-tourist attraction interaction matrix. WIND-MF [11] followed a similar idea; one of the main differences  is that it extracted the visual feature of each photo via a variational auto-encoder, and then averaged the visual  features of corresponding photos to get the visual representations of users and tourist attractions. VPMF [12]  extracted  more  visual  features  compared  to WIND-MF  [11],  including  the color histogram  features,  scaleinvariant feature transform (SIFT) features, and VGG16 features extracted via a pre-trained network, and then  obtained the visual representations of users and tourist attractions via max pooling. These methods cannot  extract visual features adaptive to tourist attraction recommendation, as the extraction of visual features is  mostly guided by computer vision tasks that have no relationship with the recommendation scenario.  To deal with the aforementioned problem, VPOI [13] was proposed, which jointly extracts features from the  visual contents of photos via a VGG16 model, classifies the photos according to who they are taken by and  where  they  are  taken,  and  factorizes  the  user-tourist  attraction  interaction  matrix  for  personalized  recommendation. Specifically, the visual feature of a photo and the factorized latent vector of a user are fed into  a softmax function to identify the probability that the photo is taken by the user. Similarly, the visual feature of  a photo and the factorized latent vector of a tourist attraction are fed into a softmax function to identify the  probability that the photo is taken at the tourist attraction. Given a photo, this method independently exploits the  user and tourist attraction information to partition other photos into visually similar/non-similar groups, assuming  that the similarities of the photos taken by the same user or taken at the same tourist attraction are higher than  those of the other photos. However, the photos taken by the same user at different tourist attractions may vary  significantly on visual features, as the sceneries of different tourist attractions are usually different, and the  photos taken at the same tourist attraction by different users may also vary significantly on visual features, as  the preferences of different users are usually different, which cannot be captured by this method. In addition, it  treats  different  photos  taken  by  a  user  or  taken  at  a  tourist  attraction  equally,  without  differentiating  their  significances.  Deep metric learning has been widely applied in computer vision and pattern recognition area, and it nonlinearly  embeds data using DNNs with contrastive loss [14, 15] or triplet loss [16, 17]. Contrastive loss based methods  mostly use the Siamese architecture [15], which cannot directly consider relative distances between classes,  and may present poor performance if there are large intra-and inter-class variations. Triplet loss based methods  adopt  a loss function  based on  large margin nearest  neighbor  [20], and  force  the difference  between the  distances of anchor-positive and anchor-negative pairs to be larger than a fixed margin. For these methods,  triplet sampling is crucial for fast and stable convergence. In this paper, we employ semi-hard sampling [17],  which converges more quickly while being less aggressive.  Modelling similarity at different levels can yield better classification results [19-21]. Yang et al. [19] proposed  sentiment constraints for understanding affective images via deep metric learning, which considers emotion  labels with the same or different polarities by generalizing the triplet loss. Zhang et al.  [20] embedded label  structures (e.g., hierarchy or shared attributes) by generalizing the triplet loss to obtain fine-grained feature  representations. Inspired by these studies where multi-level similarity is defined by a tree-like hierarchy [20] or  cluster distribution [21], we define four similarity levels by crossing the user and tourist attraction information of  photos. A corresponding quintuplet loss is then introduced to ensure the proper order of these similarities when  embedding the visual contents of photos.  In this part, we firstly formally define some basic concepts used throughout the paper, and then clarify the  research problem of this paper.    Notation: Capital letters denote sets, and   denotes the cardinality of a set. Bold upper-case letters denote    matrices, and bold lower-case letters denote vectors.   denotes the transpose operation.   denotes the  Euclidean norm of a vector.  denotes the vector concatenation operator.  Definition 1: (Geo-tagged photo) A geo-tagged photo is usually taken by a user at an interesting place  during a trip, and contains time and geographical coordinate (usually referred as geotag) information indicating  when and where it was taken. The geo-tagged photo set can be denoted by      . The users  taking these geo-tagged photos can be denoted by      .  Definition 2: (Tourist attraction) A tourist attraction is a specific geographic area in a city, e.g., a park, a  museum, and a lake, which is usually visited and photographed frequently by tourists, and can be denoted by    , where  is the city it lies in and  is its geographical coordinate. The tourist attraction set can be  denoted  by        .  The  cities  containing  these  tourist  attractions  can  be  denoted  by       .  Definition 3: (Visit) A visit indicating that at time , tourist attraction  is visited by user  can be denoted by      Definition 4: (User-tourist attraction interaction matrix) A user-tourist attraction interaction matrix indicating  the visit frequencies of users to tourist attractions can be denoted by    Definition 5: (Quintuplet) A quintuplet can be denoted by        , where    is a geotagged photo taken by a specific user at a specific tourist attraction,    denotes another photo taken by  the same user and at the same tourist attraction as  ,    denotes a photo taken by a different user from   but at the same tourist attraction as  ,    denotes a photo taken by the same user as   but at a  different tourist attraction from  ,    denotes a photo taken by a different user and at a different tourist  attraction from  . The quintuplet set can be denoted by      .  The research problem of this paper is: Given the geo-tagged photos  taken by users  in cities , for a user     and a city    where the user has never visited, i.e., the query is   ), we want to recommend a  list of tourist attractions in city  that user  would be interested in.  4  METHODOLOGY  Figure 2 shows the framework of MEAL. Firstly, we extract user-tourist attraction interaction matrix from geotagged photos. Then, we extract the visual features of photos via the VGG16 model, based on which we utilize  self-attention mechanism to obtain the visual representations of users and tourist attractions. We also factorize  the  user-tourist  attraction  interaction  matrix  to  obtain  the  latent  factors  of  users  and  tourist  attractions.  Afterwards, we concatenate the visual representations and latent factors to obtain the final embeddings of users  and tourist attractions, based on which we can predict the visit probabilities.  Classical clustering algorithms, e.g., mean-shift and DBSCAN, have been exploited to extract tourist attractions  from geo-tagged photos [22, 23]. P-DBSCAN [24] is a density-based clustering algorithm specialized for place  analysis using large collections of geo-tagged photos, which defines neighborhood density as the number of  users who have taken photos in the area, and proposes adaptive density to optimize search for dense areas.  Specifically, by  inputting the geographical  coordinate and user information of geo-tagged  photos to the  PDBSCAN algorithm, we can obtain the tourist attraction set .  The preference score of a user to a tourist attraction is proportional to the corresponding visit frequency. Like  Xu et al. [25], the geo-tagged photos taken by user   at tourist attraction   are firstly ordered according to their  taken time. Secondly, visits are identified by considering the taken time difference between successive photos.  Specifically, several successive photos are assumed to be taken within a same visit if the taken time difference  between the beginning photo and the ending photo is smaller than visit duration threshold  , as a user may  have taken multiple geo-tagged photos within one visit. Then the time of this visit is calculated by averaging the  taken time of these photos. Thirdly, we count the number of visits to obtain the visit frequency of user   to  tourist attraction  , i.e.,  . After processing the geo-tagged photos of all the possible user-tourist attraction  pairs, we can obtain the user-tourist attraction interaction matrix .  VGG16 [26] is a deep learning model designed for image classification task and has shown its efficiency in  various tasks, e.g., video captioning [27, 28], multimedia retrieval [29, 30], and recommendation [12, 13], which  can obtain representative visual features of input images. Specifically, given a geo-tagged photo  , its pixel  values are firstly resized into a tensor of shape 224×224×3, which is then input to a VGG16 model to extract its  visual feature, denoted by    The architecture of the VGG16 model for visual feature extraction is illustrated in Figure 3, which is composed  of five convolution blocks and five corresponding pooling layers, as well as two fully connected layers. The first  two convolution blocks contain two convolutional layers, while the last three ones contain three convolutional  layers. Specifically, the output dimension of the VGG16 model is decided by the number of neurons of the last  fully connected layer, i.e.,   .  Matrix factorization has been widely applied in recommender systems to model user-item interactions [38-41],  which can project users and items into a same low dimensional latent space. Weighted matrix factorization [42]  is exploited here to model visit frequency, which is a kind of implicit feedback, and the objective function is given  by (5).  where   and   are the latent factors of user   and tourist attraction  , respectively.   is a hyperparameter  used to control the weights of regularization terms.   denotes the confidence weight of  , and is formalized  by (6).   , if   ; otherwise   .  where  is a hyperparameter used to control the confidence weight of  In order to combine both the visual contents of photos and interaction behavior data to represent users and  tourist attractions, we obtain the final embedding of user  , i.e.,  , and the final embedding of tourist attraction  , i.e.,  , by concatenating the respective visual representation and latent factor, which can be formulated by  (7) and (8), respectively.  Note that, there are multiple alternative approaches can be utilized to fuse visual representation and latent  factor into a single vector, we choose concatenation here, as it can reduce information loss and the number of  model parameters.  4.6.1  Predicting Visit Probabilities.  Finally, the probability that user   will visit tourist attraction   can be calculated by (9).  where the sigmoid function ensures the prediction score to be in the range of [0, 1].  is a learnable weight  matrix, and  is a learnable bias vector.  The prediction loss is defined as the binary cross-entropy loss between the predicted visit probabilities and  the ground truth, which can be formulated by (10).  4.6.2  Preserving Multi-Level Visual Similarity.  Given a quintuplet        , (11)-(19) need to be satisfied for preserving multi-level visual  similarity.  where  ,  ,  ,  , and   are the visual features of  ,  ,  ,  , and  .  ,  ,  ,  ,  and   are hyperparameters used to control the margins between photo pairs    and    and     and     and     and   , as well as    and   , respectively.  The corresponding triplet losses of (11)-(16) are given by (20)-(25), respectively.      where the value of   is the same as the value in   if it is positive, otherwise it is 0.  The final quintuplet loss can be calculated by (26).  where  includes the training quintuplets that are selected by using the hard mining technique proposed by  Schroff et al. [19]. Specifically, for any photo   , all the other photos taken by the same user and at the  same tourist attraction as   should be selected as  . After selecting  , all the photos taken by a different  user from   but at the same tourist attraction as  , photos taken by the same user as   but at a different  tourist attraction from  , as well as photos taken by a different user and at a different tourist attraction from  that satisfy (27)-(32) should be selected as  ,  , and  , respectively.  4.6.3  Joint Learning.  The final objective function of MEAL is given by (33).  where  denotes the learnable parameters of the model.   is a hyperparameter used to control the weight of    parameter regularization term.  denotes the Frobenius norm of a matrix.  After training the model, given a query   ), we first obtain the embedding of user  and the embeddings  of all the tourist attractions in city , based on which (9) is exploited to obtain the visit probabilities, and finally  recommend top  tourist attractions in city  to user .  In this part, the  experimental dataset, settings, and results are presented to evaluate the recommendation  performance of MEAL. Specifically, MEAL is compared to its simplified variants and the state-of-the-art methods  to prove its superiority. In addition, an example of tourist attraction recommendation is provided to illustrate the  effectiveness of MEAL.  The dataset used in this paper is crawled from Flickr by using the public API . It consists of 699,896 geo-tagged  photos that were taken in Beijing, Chengdu, Guangzhou, Hangzhou, Hong Kong, and Shanghai in China. The  time span of the dataset is from January 1st 2001 to July 1st 2011. Table 1 shows the statistics of the dataset  after tourist attraction extraction introduced in Section 4.1.  Following the settings of Majid et al. [8], we set the parameters for P-DBSCAN, and set visit duration threshold    hours.  Models are trained and evaluated on a server with one GPU (Nvidia GTX 1080 Ti). The code is released on  GitHub . Adam is used as the optimizer to train the model. Through parameter tuning,  ,  ,  ,  ,  , and   in the quintuplet loss are set as 0.1, 0.2, 0.3, 0.1, 0.2, and 0.1, respectively, the weight vector length  of  the self-attention mechanism is set as 10,  is set as 15, and the regularization coefficients   and   are set as  0.001 and 0.0003, respectively, which outperform the other settings on the validation data.  To evaluate the performance of MEAL, for each individual user who has visited at least three cities, we select  two of his/her visited cities for validation and test, while the rest cities are used for training. Specifically, if a user  has visited  cities,    segments would be obtained.  Mean average precision (MAP) is employed to evaluate the recommendation performance, which is a widely  used evaluation metric for recommender systems [43] and can be calculated by (34) and (35).  where  denotes the number of users who have visited at least three cities.   equals 1 if the user has really  visited the -th tourist attraction in the recommendation list, otherwise   equals 0.  Paired t-tests are used to determine whether the recommendation performance of MEAL and each compared  method is significantly different when the significance level is 5%.  We have fully tuned the parameters of all the comparison methods to ensure fairness.  photos taken by the same user, as the scenery of a tourist attraction is rather stable, while the preference of a  user may be diverse. MEAL-U/L outperforms MEAL-U and MEAL-L, as it considers both the visual similarity  levels regarding users defined in MEAL-U and the visual similarity levels regarding tourist attractions defined in  MEAL-L. MEAL-U&L outperforms MEAL-U/L, as it crosses the user and tourist attraction information to provide  fine-grained visual similarity levels regarding both users and tourist attractions. MEAL outperforms MEAL-U&L,  as it considers different ways of crossing the user and tourist attraction information to provide multi-level visual  similarity.  2) MEAL outperforms MEAL-max and MEAL-average, which verifies the effectiveness of exploiting selfattention  mechanism  to  capture  the  significances  of  different  photos  for  representing  users  and  tourist  attractions. In addition, MEAL-average slightly outperforms MEAL-max, which might be that MEAL-max is more  vulnerable to outliers.  To show the superiority of MEAL, the state-of-the-art visual content based tourist attraction recommendation  methods  exploiting  geo-tagged  photos,  i.e.,  DTMMF  [10],  WIND-MF  [11],  VPMF  [12],  and  VPOI  [13]  are  compared.  Table 3 shows the experimental results, from which the following observations can be concluded:  1) DTMMF performs the worst among all the methods. The reason might be that DTMMF only extracts the  gender and age information in photos that contain people faces, ignoring other photos and a lot of other visual  information.  2) VPMF performs better than WIND-MF. The reason might be that WIND-MF only extracts visual features  via DNNs, while VPMF also extracts the traditional color histogram features and SIFT features.  3) VPOI performs better than VPMF. The reason might be that VPOI jointly extracts visual features and  factorizes  the  user-tourist  attraction  interaction  matrix  for  recommendation,  while  the  extraction  of  visual  features in VPMF has no relationship with the recommendation scenario.  4) MEAL performs better than VPOI. The reason might be that MEAL crosses the user and tourist attraction  information  to  partition  other  photos  into  different  groups  for  multi-level  similarity  aware  visual  content  embedding. In addition, VPOI treats different photos taken by a user or taken at a tourist attraction equally,  while MEAL exploits self-attention mechanism to infer the weights of different photos for representing users and  tourist attractions.  the effectiveness of multi-level similarity aware visual content embedding and self-attention mechanism based  visual representation learning.  In  this paper, multi-level visual  similarity  based personalized  tourist attraction  recommendation  using geotagged photos (MEAL) is proposed. MEAL embeds the visual contents of photos by exploiting a quintuplet loss  to ensure the proper order of the visual similarities defined by crossing the user and tourist attraction information,  represents users and tourist attractions by exploiting self-attention mechanism to infer the weights of different  photos as well as by factorizing the user-tourist attraction interaction matrix, and predicts the visit probabilities  for recommendation. We conducted experiments on a dataset crawled from Flickr, and the experimental results  proved the advantage of this method.  There is still room for further expansion of the proposed method. We consider exploiting the photo sequences  of users and tourist attractions to better represent them. In addition, we will introduce the categories of tourist  attractions to provide more visual similarity levels.  This  work  is  supported  by  the  National  Key  Research  and  Development  Program  of  China  (No.  2018YFB0505000) and the Fundamental Research Funds for the Central Universities (No. 2020QNA5017). 