<title>Is AÓÄùention always needed? A Case Study on Language Identification from Speech</title> <title>arXiv:2110.03427v1  [cs.LG]  5 Oct 2021</title> models are capable of capturing the content-based global interactions. ALBERT [Lan et al 2020]. In Machine Translation domain researcher [Takase and Kiyono 2021] state-of-the-art (sota) algorithms. In other domain like Language Modelling, Text ClassiÓÄõcation, Topic Modelling, Emotion ClassiÓÄõcation, Sentiment Analysis, etc Transformer has widely used. languages. In this work we focused on using diÓÄùerent approaches for Spoken Language IdentiÓÄõcation. language setting accordingly. is world second populated and seventh largest country in landmass and also have dynamic culture. its own language, but none of the language is recognised as the national language of the country. Chapter 1 Article 343 . Currently, only 22 languages have been accepted as regional languages. Andaman and Nicobar Islands, Bihar, Chhattisgarh, Dadra and Nagar Haveli and Daman and Diu, Delhi, Haryana, Himachal Pradesh, 6 Hindi Indo-Aryan Jammu and Kashmir, Jharkhand, Ladakh, Madhya Pradesh, Mizoram, Rajasthan, Uttar Pradesh, from Indo-Aryan and Dravidian language family. It can be seen from the Table 1 that diÓÄùerent languages are spoken in diÓÄùerent states, however, languages do not obey the geographical boundaries. Therefore, many of these languages, particu- languages. are under resourced. variations. The likely diÓÄùerences among various spoken languages are in their phoneme inventories, predict the correct spoken language regardless of the above-mentioned constraints. agnostic. The main contributions of this work can be summarized as follows: achieving state-of-the-art accuracy. language family as well as in noisy scenarios. by ÓÄõnding the optimal learning rate using fastai [J. 2018]. converts speech into a sequence of vectors and was used to capture temporal features in speech. classiÓÄõcation task. Recent years have found the use of feature extraction with neural networks, particularly with Long Short Term Memory (LSTM) [Zazo et al 2016] [Gelly et al 2016] [Lozano- joined together and used to classify the language of the input samples. [Lozano-Diez et al 2015] the time domain for the x-axis and frequency bins for the y-axis. better performance when combining both the CNN features and identity vectors. [R. and T. 2019] linearly. Our research diÓÄùers from the previous works on LID in the following aspects: ‚Ä¢ Comparison of performance of CNN, CRNN, as well as CRNN with Attention. Extensive experiments with our proposed model shows its applicability both for close lan- guage as well as noisy speech scenarios. Our proposed architecture consists of three models. (3, 512), (3, 512), (3, 256), and (3, 128), respectively. LSTM by going forward and backward. where, ùêø is the number of audio specimens. and 1, and also maps zeros to near-zero. during the training process. languages. Figure 1 provides a schematic overview of the network architecture. For serving input to the neural network, MFCCs of each audio was extracted using the librosa. width of 10 msec and sliding window of 25 msec. for evaluating new LID models and methods are NIST LRE evaluation dataset [Sadjadi et al 2018]. distributed by the Linguistic Data Consortium (LDC) and cost thousands of dollars. For example, academic ÓÄõeld of LID. Furthermore, the NIST LRE evaluations focus mostly on telephone speech. data for 4 major European languages ‚Äì English (en), French (fr), German (de) and Spanish (es ). Statistics of the dataset are given in Table 2. the statistics of this dataset which we used for our experiments. language dataset into training, validation, and testing set, containing 80%, 10%, and 10% of the data, respectively, for each language and gender. to all the trainable weights in the network. We train the model with Adam [Kingma and Ba 2014] optimizer with 9, 98, and ùúñ = 10 and learning rate schedule [Vaswani et al 2017], "Sparse Categorical Crossentropy" as the loss function were used. language. our evaluation results are rounded up to 3 digit after decimal point. results. We evaluated system performance using the following evaluation metrics ‚Äì Recall (TPR), Precision (PPV), f1-score, and Accuracy. weight balancing as a dynamic method using scikit-learn [Pedregosa et al. 2011]. PPV, TPR, f1-score and Accuracy scores are reported in Table 5 for the three architectures - CNN, and CRNN with Attention provide competitive results of 0.987 accuracy. with Assamese, Meitei, Tamil and Telugu; and Hindi gets confused with Malayalam. Assamese and Bengali have originated from the same language family and they share approx- also similar phoneme set as both the languages borrowed most of the vocabularies from Sanskrit. common classiÓÄõcation errors encountered during evaluation. their phonetic similarity. Cluster internal languages are phonetically similar, close, and geographi- cally contiguous, hence diÓÄúcult to be diÓÄùerentiated. internal languages for Cluster 1, 2 and 3, and the experimental results are presented in Table 10. provide competitive results for every language cluster. For cluster-1 CRNN architecture and CRNN with Attention provides accuracy of 0.98/0.974, for cluster-2 0.999/0.999, and for cluster-3 0.999/1, respectively. CNN architecture also provides comparable results to the other two architectures. 3, respectively. and CRNN with Attention perform well to discriminate between Bengali and Odia. confusion is minimised. these languages. in cluster 1 than the other two clusters. our experiments till 571 samples. A comparison of the results in Table 14, 15, and 16 reveals the following observations. ‚Ä¢ All the models perform consistently better with more training data. ‚Ä¢ CRNN and CRNN with attention perform consistently better than CNN. scenario. Appendix. and noisy scenarios. We carried out extensive experiments and our architecture produced state- noise and its extensibility to new languages. The model exhibits the overall best accuracy of 98.7% with diverse language families and smaller audio samples.