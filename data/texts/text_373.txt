<title>Frequency-aware SGD for Efﬁcient Embedding Learning with Provable Beneﬁts</title> <title>1 Introduction</title> Embedding learning describes a problem of learning dense real-valued vector representation for categorical data, often referred to as token (Pennington et al., 2014; Mikolov et al., 2013a,b). Good quality embeddings can capture rich semantic information of tokens, and thus serve as the cornerstone for downstream applications (Santos et al., 2020). Due to their signiﬁcant impact on model performance and large memory footprint (21.8% of total parameters for BERT (Devlin et al., 2018), <title>arXiv:2110.04844v3  [cs.LG]  23 Nov 2021</title> Can we design a frequency-dependent adaptive learning rate schedule? Can we show provable beneﬁts over SGD? Our contributions. We answer the previous question by showing that token frequency information can be leveraged to design provably efﬁcient algorithms for embedding learning. Speciﬁcally, • We propose Frequency-aware Stochastic Gradient Descent (FA-SGD), a simple modiﬁcation to standard SGD, which applies a token-dependent learning rate that inversely proportional to the frequency of the token. We also propose a variant, named Counter-based Frequency-aware Stochastic Gradient Descent (CF-SGD), which is able to estimate frequency in an online fashion, much similar to Adagrad (Duchi et al., 2011) and Adam (Kingma and Ba, 2014). • Theoretically, we show that both FA-SGD and CF-SGD outperform standard SGD for embedding learning problems. Speciﬁcally, they are able to signiﬁcantly improve convergence for learning infrequent tokens, while maintaining convergence speed for frequent tokens. To the best of our knowledge, our proposed algorithms are the ﬁrst to show provable speed-up over standard SGD for non-convex embedding learning problems. This is in sharp contrast with other popular adaptive learning rate algorithms, whose empirical performance can not be explained by existing theories. • Empirically, we conduct extensive experiments on benchmark datasets and a large-scale in- dustrial recommendation system. We show that FA/CF-SGD is able to signiﬁcantly improve over SGD, and improves/matches popular adaptive learning rate algorithms. We also observe the second-order moment maintained by Adagrad and Adam highly correlates with the frequency information, demonstrating intimate connections between adaptive algorithms and the proposed FA/CF-SGD. Adaptive algorithms for non-convex problems. There has been a fruitful line of research on analyzing the convergence of adaptive learning rate algorithms in non-convex setting. These results aim to match the convergence rate of standard SGD given by O(1/ T) (Ghadimi and Lan, 2013), however often with additional factor of log T (Ward et al., 2018; D efossez et al., 2020; Chen et al., 2018; Reddi et al., 2018), or with worse dimension dependence (Zhou et al., 2018a) for smooth problem (assumed by almost all prior works). Moreover, all existing works aim to analyze the convergence for general non-convex problems, ignoring unique data features in embedding learning problems, where adaptive algorithms are most successful. We explicitly take account into the sparsity of stochastic gradient, and token distribution imbalancedness into the design and analysis of our proposed algorithms, which are the keys to better convergence properties. Adaptive algorithms and SGD. To the best of our knowledge, the study on understanding why adaptive learning rate algorithms outperform SGD is very limited. Zhang et al. (2019) argue that BERT pretraining (Devlin et al., 2018) has heavy-tailed noise, implying unbounded variance and possible non-convergence of SGD. Normalized gradient clipping method is proposed therein and converges for a family of heavy-tailed noise distributions. Our results focus on a different direction by showing that imbalanced token distribution is an important factor that can be leveraged to design more efﬁcient algorithms for embedding learning problems. Our result also does not rely on the noise to be heavy-tailed for the convergence beneﬁts of the proposed FA/CF-SGD to take effect. Notations: For a vector/matrix, we use to denotes its ` -norm/Frobenius norm. We use to denote the spectral norm of a matrix. <title>2 Problem Setup</title> We consider an embedding learning problem which aims to learn user and item embeddings through their interactions. We denote U as the set of users, and V as the set of items, and let X = U ∪ V denote the union, referred to as tokens throughout the rest of the paper. We assume |X| = N, i.e., the total number of user and item is N. For the ease of presentation, we always use letter i to index user set U, letter j to index item set V, and letter k to index the union set X. The embedding learning problem can be abstracted into the following stochastic optimization problem: denote the marginal distribution over U and V. Remark 2.1. Our analysis also allows treatment of additional network structure (with parameters denoted by W) that takes nonlinear transformation of embedding vectors, e.g., f (Θ, W) = ` (θ , θ , W; y ). We omit their explicit treatment for presentation simplicity. In addition, although we mainly discuss in the context of recommendation, our analysis and results only relies on sparsity of stochastic gradient and the imbalancedness of token distributions, which allow one to extend our results to other embedding learning problems (e.g., language model pretraining). |j = j = 0 for all i ∈ U, j ∈ V. We pose the following assumptions on the its variance. Assumption 1 (Bounded conditional variance). We assume that the variance of δ is bounded. That is, Assumption 1 allows us to provide a ﬁner characterization on the variance of stochastic gradient compared to typical variance assumption in literature. To illustrate, recall that standard assump- tion in the stochastic optimization literature assumes Var(g ) = E − g ≤ σ for some universal constant σ > 0. Consider an extreme setting, where we have exact gradient for the sampled user-item pair, i.e., g ∇f and g ∇f , then we have σ = 0 for all k ∈ X. In contrast, the variance of g is still non-zero. In general setting, we can bound the variance as shown in the following proposition. Note that the variance lower bound arises naturally form the extreme sparsity of the stochastic gradient. Assumption 2 (Smoothness of prediction loss). We assume `(u, v; y) is symmetric w.r.t. u and v for any y ∈ {−1, +1}, and there exists L > 0 such that ` (·, ·; ·) ≤ L, ` (·, ·; ·) ≤ L. The assumption on the symmetry of ` is readily satisﬁed by almost all neural network architecture. In essence, this assumption only requires that the parameterization of embedding vector is token agnostic. On the other hand, the spectral upper bound on the Hessian matrix is a standard assumption in optimization literature. <title>3 Theoretical Results</title> We ﬁrst present the convergence results of FA-SGD and standard SGD for embedding learning problem formulated in (1), and discuss the advantage that FA-SGD offers when the token distribution is highly imbalanced. We further propose a variant, named CF-SGD, which can estimate frequency information in an online fashion and still provably enjoys the beneﬁts of FA-SGD. Remark 3.1 (Connection with Stochastic Block Coordinate Descent). Our FA-SGD shares some similarities with Stochastic Block Coordinate Descent (SBCD) (Nesterov, 2012; Dang and Lan, 2015; Richt arik and Tak c, 2014) applied to problem (1), in the sense that each iteration we sample certain blocks of variables (θ , θ in our case), and only update the sampled blocks by following its stochastic gradient. Different from SBCD, the stochastic gradient of the block variable g in the FA-SGD is biased, as shown in (4). Note that with unbiased stochastic gradient, SBCD method typically converges slower than standard SGD by a factor that can be as large as number of blocks. As a concrete example, when the token distribution is uniform, SBCD converges slower than standard SGD by a factor of |X|, hence slower than FA-SGD by a factor of |X| from Corollary 3.1 developed later. Recall that from Proposition 2.1, the variance of stochastic gradient is heavily inﬂuenced by the population gradient ∇ f (Θ), and can be huge whenever the population gradient is, presumably in the early phase of training. This relationship is also supported by empirical ﬁndings in Zhang et al. (2019) (Figure 2a), where the authors show that for BERT pretraining, the noise distribution in stochastic gradient g is highly non-stationary, which has large variance in the beginning of the training and smaller variance at the end of training. Since existing analysis of SGD in literature assumes a constant variance bound for the stochastic gradient, our observation in Proposition 2.1 requires an alternative analysis of SGD for problem (1). To obtain the convergence rate of standard SGD in the presence of iterate-dependent variance (6), our key insight is to tailor the convergence analysis to the sparsity of the stochastic gradient for problem (1). We show the convergence of standard SGD as the following. Note that both FA-SGD and standard SGD attain a rate of O(1/ T). Compared to existing rates of standard SGD (Ghadimi and Lan, 2013), we do not require constant variance bound on stochastic gradient, as we have discussed above. Compared to existing rates of adaptive learning rate algorithms (Zhou et al., 2018a; Chen et al., 2018), both rates obtained here exhibits dimension-free property. We emphasize here that due to the dimension-free nature of the bounds for both SGD and FA-SGD, we do not claim the proposed FA-SGD has better dependence on dimension, which is the main motivation of adaptive algorithms (Duchi et al., 2011; Kingma and Ba, 2014; Reddi et al., 2019). Instead, the major difference on the convergence of FA-SGD (8) and that of standard SGD (9) is that the former one is token-dependent. Speciﬁcally, for FA-SGD, each token k ∈ X has its own convergence characterization, while all the tokens have the same convergence characterization in the standard SGD. We ﬁrst make a simple observation stating the equivalence of FA-SGD and standard SGD, when the token distribution is uniform. Corollary 3.1 (Uniform Distribution). Suppose the user distribution {p and item distribution {p is the uniform distribution. Then FA-SGD and standard SGD is equivalent to each other, in terms of both algorithmic execution and convergence rate. 3.2 When does FA-SGD outperform standard SGD? We show FA-SGD shines when the token distribution {p , deﬁned in (2), is highly imbalanced. Before we present detailed discussions, we make an important remark that highly imbalanced token distributions are ubiquitous in social systems, presented in the form power-law. Examples of such distributions include the degree of individuals in the social network (Muchnik et al., 2013); the frequency of words in natural language (Zipf, 2016); citations for academic papers (Brzezinski, 2015); number of links on the internet (Albert et al., 1999). For more discussions on power-law distributions in social and natural systems, we refer readers to Kumamoto and Kamihigashi (2018). In Figure 1c, 1d we plot the user and item counting distribution of Movielens-1M dataset. One could clearly see that the user and item distributions are highly imbalanced, with a small percentages of users/items taking up the majority of rating records. We defer details on the skewness of token distributions for Criteo dataset to Appendix C. Figure 1: Token distribution with an exponential and polynomial tail, and the user/item counting distributions for Movielens-1M dataset. We remark that |U|, |V| ≥ is a very mild condition, as it only requires that the most infrequent user/item should have its frequency smaller than the most frequent user/item by at least a factor of e. i.e., the non-top user/item set U \ U , V \ V is nonempty, This is readily satisﬁed by the token distributions in recommendation systems and natural language modeling (Celma, 2010; Zipf, 2016), where the lowest frequency is at least orders of magnitude smaller than the highest frequency. The factor of e in deﬁning U , V can also be readily replaced by any constant larger than 1. From Corollary 3.2, we can see that FA-SGD improves signiﬁcantly over standard SGD for user/item distribution with exponential tail. Speciﬁcally, FA-SGD achieves the same convergence rate of top users/items compared to SGD, meanwhile it signiﬁcantly improves the convergence of the nontop users/items. Moreover, the strength of such an improvement increases exponentially as we move towards the tail users/items. We remark that polynomial tail (37) is also the prototypical example of the power law distribution class for modeling social behaviors (Kumamoto and Kamihigashi, 2018). The constant 2 in the condition ν ≥ 2 can be replaced by any constant strictly larger than 1, with slight changes to the constant factor in the statements of the corollary. From Corollary 3.3, we can see that FA-SGD improves signiﬁcantly over standard SGD for user/item distribution with polynomial tail. Speciﬁcally, FA-SGD achieves the same convergence rate of top users/items compared to SGD, meanwhile it signiﬁcantly improves the convergence of the nontop users/items. Moreover, the strength of such an improvement increases in polynomial order as we move towards the tail users/items. In certain application scenarios, the token distribution {p can be unknown in advance of learning. To apply FA-SGD, one needs to employ a preprocessing step in order to estimate the token distribution to a high accuracy, and then run the algorithm with estimated token distribution. Such a preprocessing step often requires additional human efforts and data. To remove such an undesirable preprocessing step, below we present an online variant of FA-SGD, which uses the counter of tokens collected during training to estimate the token distribution dynamically. We show that the proposed Counter-based Frequency-aware Stochastic Gradient Descent (CF-SGD) is able to retain the beneﬁts of FA-SGD despite unknown token distribution. We believe the assumption on gradient bound ∇f (·) ≤ G is not strictly necessary and can be removed with more reﬁned analysis. Nevertheless, the requirement on T only logarithmically depends on the gradient bound G. In addition, we highlight that the convergence characterization in Theorem 3.3 is still token-dependent. Speciﬁcally, we can show that despite not knowing token distribution beforehand, CF-SGD can gain the same advantages that FA-SGD enjoys over SGD. Corollary 3.5 (Polynomial Tail). Suppose we have the same set of conditions given in Corollary 3.3, and σ/ f (Θ ) − f ≤ 1. Deﬁne U as the set of users whose frequencies are within 2-factor from the highest frequency: U = {i : n ≥ 1/16}, and V similarly as V = {j : m ≥ 1/16}. We refer to as the top users, and V as the top items. Then given |U|, |V| ≥ 16 , the FA-SGD, compared to standard SGD: (1) Obtains the same rate of convergence, for the top users U and top items V n (2) E ∇f can converge faster by a factor of Ω for each i ∈ U \U n (3) E ∇f can converge faster by a factor of Ω for each j ∈ V \V Figure 2: Movielens-1M dataset with FM and DeepFM model. CF-SGD signiﬁcantly outperforms standard SGD, and is highly competitive against Adam, Adagrad. <title>4 Experiments</title> Figure 3: (a-b) Second-order gradient moment correlates linearly with frequency maintained by CF-SGD; (c-d) Comparisons on Criteo dataset with FM model. CF-SGD algorithm. We further make an empirical observation that draws a close connection between adaptive algorithms and CF-SGD. We plot the second-order gradient moment maintained by Adagrad and Adam against the estimated frequency maintained by CF-SGD. Surprisingly, the second-order gradient moment quickly develops a close-to linear relationship with the frequency information accumulated by CF-SGD (Figure 3a,3b) . This observation suggests that Adagrad and Adam are exploiting frequency information implicitly to a large extent. Criteo: We observe qualitative behavior of CF-SGD similar to Movielens-1M dataset, as can be seen in Figure 3c,3d, 4a,4b. Figure 4: (a-b) Comparisons on Criteo dataset with DeepFM model; (c-d) Comparisons on a industrial-scale recommendation dataset with an ultra-large recommender model. Industrial Recommendation System: We train an ultra-large Alg NE Diff % industrial recommendation model with the proposed CFAdagrad 0.78643 0.0 SGD. The training data contains 10 days of user-item interCF-SGD 0.78628 -0.02 action records, with ∼2.5 billion examples per day. We use Table 1: Eval NE Diff % around 800 features, with ∼100 million average number of tokens per feature. We compare CF-SGD with Adagrad, which has been carefully tuned in production usage. For both algorithms, we use a batch size of 64k and do one-pass training. Different from benchmark academic datasets, we use Normalized Entropy (NE) as the evaluating metric (He et al., 2014) (smaller is better), which is the cross-entropy loss normalized by the entropy of background click through rate. Note that due to numerous iterations of the production model, any relative improvement ∼ 0.02% is considered to be signiﬁcant. In Figure 4c, 4d we compare the training NE curve CF-SGD and Adagrad, we can see that CF-SGD shows faster convergence than Adagrad during training (see NE difference % in Figure 4d). Moreover, from Table 1 we can observe that CF-SGD also improves over Adagrad during the serving phase. – faster than standard SGD, and comparable (if not better) to adaptive algorithms, we further highlight that CF-SGD learns cheap. Speciﬁcally, adaptive algorithms require additional memory to store history information for each parameter. For an embedding table of size N ×d (N tokens, d being embedding dimension), the memory needed is at least 3N ×d for Adam (ﬁrst/second-order gradient moment), and 2N × d for Adagrad (second-order gradient moment). In sharp contrast, CF-SGD only requires N additional memory, for storing the estimated frequency. Since the choice of typical embedding dimension d exceeds 64 (Yin and Shen, 2018), adaptive algorithms require memory at least twice the size of the embedding table, while CF-SGD requires negligible memory overhead. Note the industrial recommendation model in our experiments has a size over multiple terabytes, with above 95% of consumed by embedding tables. Doubling the memory footprint by using standard Adam/Adagrad is infeasible in terms of both engineering and environmental concern. <title>5 Conclusion</title> We propose (Counter-based) Frequency-aware SGD for embedding learning problems, which adopts frequency-dependent learning rate schedule for each token. We demonstrate provable beneﬁts that FA/CF-SGD enjoy over standard SGD for imbalanced token distributions, with extensive experiments supporting our theoretical ﬁndings. Our empirical ﬁndings also suggest that adaptive algorithms can implicitly exploit frequency information and hence share close connections with the proposed algorithms, this connection might be helpful in the direct analysis of adaptive algorithms for embedding learning problems, which we leave as a future direction. Moreover, we will further investigate whether the convergence upper bounds for SGD and FA/CF-SGD are minimax optimal for the embedding learning problem. <title>Acknowledgements</title> We deeply appreciate Aaron Defazio and Michael Rabbat for their valuable feedbacks and insightful discussions. We are also grateful to Yuxi Hu for his help on production model experiments. <title>References</title> ALBERT, R., JEONG, H. and BARAB 130–131. ARJEVANI, Y., CARMON, Y., DUCHI, J. C., FOSTER, D. J., SREBRO, N. and WOODWORTH, B. (2019). Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365 . BRZEZINSKI, M. (2015). Power laws in citation distributions: evidence from scopus. Scientometrics 103 213–228. CELMA, O. (2010). The long tail in recommender systems. In Music Recommendation and Discovery. Springer, 87–107. CHEN, X., LIU, S., SUN, R. and HONG, M. (2018). On the convergence of a class of adam-type algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941 . CLAUSET, A., SHALIZI, C. R. and NEWMAN, M. E. (2009). Power-law distributions in empirical data. SIAM review 51 661–703. DACREMA, M. F., BOGLIO, S., CREMONESI, P. and JANNACH, D. (2021). A troubling analysis of reproducibility and progress in recommender systems research. ACM Transactions on Information Systems (TOIS) 39 1–49. DANG, C. D. and LAN, G. (2015). Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM Journal on Optimization 25 856–881. adam and adagrad. arXiv preprint arXiv:2003.02395 . DEVLIN, J., CHANG, M.-W., LEE, K. and TOUTANOVA, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . DRORI, Y. and SHAMIR, O. (2020). The complexity of ﬁnding stationary points with stochastic gradient descent. In International Conference on Machine Learning. PMLR. DUCHI, J., HAZAN, E. and SINGER, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research 12. GHADIMI, S. and LAN, G. (2013). Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization 23 2341–2368. GOYAL, P., DOLL hour. arXiv preprint arXiv:1706.02677 . GUO, H., TANG, R., YE, Y., LI, Z. and HE, X. (2017). Deepfm: a factorization-machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247 . HE, K., ZHANG, X., REN, S. and SUN, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. HE, X., PAN, J., JIN, O., XU, T., LIU, B., XU, T., SHI, Y., ATALLAH, A., HERBRICH, R., BOWERS, S. ET AL. (2014). Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising. KINGMA, D. P. and BA, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 . KUMAMOTO, S.-I. and KAMIHIGASHI, T. (2018). Power laws in stochastic processes for social phenomena: An introductory review. Frontiers in Physics 6 20. LIU, L., LIU, X., GAO, J., CHEN, W. and HAN, J. (2020). Understanding the difﬁculty of training transformers. arXiv preprint arXiv:2004.08249 . LIU, Y., OTT, M., GOYAL, N., DU, J., JOSHI, M., CHEN, D., LEVY, O., LEWIS, M., ZETTLEMOYER, L. and STOYANOV, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 . MERITY, S., XIONG, C., BRADBURY, J. and SOCHER, R. (2016). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843 . MIKOLOV, T., CHEN, K., CORRADO, G. and DEAN, J. (2013a). Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 . MIKOLOV, T., SUTSKEVER, I., CHEN, K., CORRADO, G. S. and DEAN, J. (2013b). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. MUCHNIK, L., PEI, S., PARRA, L. C., REIS, S. D., ANDRADE JR, J. S., HAVLIN, S. and MAKSE, H. A. (2013). Origins of power-law degree distribution in the heterogeneity of human activity in social networks. Scientiﬁc reports 3 1–8. NESTEROV, Y. (2012). Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization 22 341–362. PENNINGTON, J., SOCHER, R. and MANNING, C. (2014). GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar. PIANTADOSI, S. T. (2014). Zipf?s word frequency law in natural language: A critical review and future directions. Psychonomic bulletin & review 21 1112–1130. REDDI, S., ZAHEER, M., SACHAN, D., KALE, S. and KUMAR, S. (2018). Adaptive methods for nonconvex optimization. In Proceeding of 32nd Conference on Neural Information Processing Systems (NIPS 2018). REDDI, S. J., KALE, S. and KUMAR, S. (2019). On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237 . RENDLE, S. (2010). Factorization machines. In 2010 IEEE International conference on data mining. IEEE. RICHT scent methods for minimizing a composite function. Mathematical Programming 144 1–38. SANTOS, J., CONSOLI, B. and VIEIRA, R. (2020). Word embedding evaluation in downstream tasks and semantic analogies. In Proceedings of the 12th Language Resources and Evaluation Conference. European Language Resources Association, Marseille, France. TAK proaches for large recommender systems. The Journal of Machine Learning Research 10 623–656. WARD, R., WU, X. and BOTTOU, L. (2018). Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization. arXiv preprint arXiv:1806.01811 2. YIN, Z. and SHEN, Y. (2018). On the dimensionality of word embedding. arXiv preprint arXiv:1812.04224 . ZHANG, J., KARIMIREDDY, S. P., VEIT, A., KIM, S., REDDI, S. J., KUMAR, S. and SRA, S. (2019). Why are adaptive methods good for attention models? arXiv preprint arXiv:1912.03194 . ZHOU, D., CHEN, J., CAO, Y., TANG, Y., YANG, Z. and GU, Q. (2018a). On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671 . ZHOU, G., ZHU, X., SONG, C., FAN, Y., ZHU, H., MA, X., YAN, Y., JIN, J., LI, H. and GAI, K. (2018b). Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ZIPF, G. K. (2016). Human behavior and the principle of least effort: An introduction to human ecology. Ravenio Books. <title>A Experiment Details</title> For both Movielens-1M and Criteo dataset, we random split into training set, validation set and test set, taking up 80%, 10%, and 10% of the total samples respectively. Implementation: We build upon torchfm , which contains implementation of various popular recommendation models. For all FM models, we use 64 as the embedding size. For all DeepFM models, we use 16 as the embedding size, and use (16, 16) as the widths of the hidden layers. For Movielens-1M dataset, the learning rate of different algorithms are list in Table 2. Table 2: Learning rates for Movielens-1M dataset. For Criteo dataset, the learning rate of different algorithms are list in Table 3. Table 3: Learning rates for Criteo dataset. All the algorithms use 1024 as the batch size during training. <title>B Additional Experiments on Word2Vec Embedding Learning</title> We demonstrate the effectiveness of the proposed FA/CF-SGD for embedding learning problems in natural language modeling. Speciﬁcally, we conduct experiments for learning Word2Vec embeddings proposed in Mikolov et al. (2013a). Two learning models are considered: (1) Continuous Bag-of-Words (CBOW): CBOW aims to predict each word (which we refer to as the center word), given its neighboring words. The training task is deﬁned by taking each word in the corpus as the center word, and minimize the total prediction loss. (2) Skip-Gram: Skip-Gram aims to predict each context word, given a center word. The training task is deﬁned by taking each word in the corpus as the center word, and minimize the total prediction loss. Dataset and Preprocessing. We use WikiText-2 dataset (Merity et al., 2016), which contains 36k text lines and 2M tokens in the training dataset. We remove extremely rare tokens with less than 50 occurrences in the training dataset. Note that removing extremely rare tokens was also proposed in the original Word2Vec paper Mikolov et al. (2013a), where only the top 1 million most frequent tokens are selected. Experiment details and results. We choose the embedding dimension to be 300 as suggested value in Mikolov et al. (2013a). Note that for each word w, Word2Vec represents it by a pair of embedding vectors (u , v ), which we refer to as the center embedding and context embedding, respectively. Speciﬁcally, u is used when w serves as the center word, and v is used when w serves as the context word. This makes the proposed FA/CF-SGD perfectly applicable for learning Word2Vec embeddings, by simply setting U as the set of center embedding vectors, and V as the set of context embedding vectors. We compare CF-SGD with standard SGD, and Adam. Following the suggestion from Mikolov et al. (2013a), we decrease the learning rate linearly as epoch increases. We use an initial stepsize of 1.0 for both CF-SGD and SGD, and the stepsize of 0.025 for Adam. We iterate over the training dataset for 20 epochs, with a batch size of 96. Note the original Word2Vec was trained with only 3 epochs, albeit on a much larger corpus. The results are reported in Figure 5. Figure 5: Comparison between CF-SGD, SGD, and Adam for learning Word2Vec embeddings on WikiText-2 dataset. One can clearly see that for both CBOW and Skip-Gram models, CF-SGD is able to signiﬁcantly improve over standard SGD. For Skip-Gram model, we observe that CF-SGD even yields comparable performance to Adam. For CBOW model, CF-SGD is able ﬁll in the huge performance gap between Adam and SGD, and yields similar testing performance compared to Adam. Note that we do not extensively tune the initial stepsize of CF-SGD, and the linearly-decaying stepsize annealing rule was proposed in Mikolov et al. (2013a) for speeding-up SGD, which we believe might not the optimal choice for CF-SGD. We believe further improvements can be made by searching for the best initial learning rate and proper stepsize annealing rule for CF-SGD. <title>C Real World Token Distributions</title> undisclosed due to privacy concern. <title>D Analysis</title> Throughout our analysis, we use ξ = (i , j ) to denote the random user/item ids sampled from the unknown distribution D. We use ξ = {ξ to denote the random samples collected up to the beginning of the t-th iteration, and use ξ and ξ interchangeably when the context is clear. Finally, we use F = σ(ξ ) to denote the σ-algebra generated by the random variables ξ Proof of Proposition 2.1. Let ∇f ∈ R denote the submatrix that contains gradient of users embeddings, and ∇f ∈ R for the gradient of item embeddings. Similarly, let g ∈ R , g denote the stochastic gradient of user and item embeddings, respectively. Note that −∇f −∇f −∇f , from which we conclude the proof. where L = 2Lp for all k ∈ X. H = ∇ f ( Θ) = D(i, j) ∇ ` ( ; y ), E = ∇ f ( Θ) = D(i, j)∇ ` ( ; y ), F = ∇ f ( Θ) = D(i, j) ∇ ` ( ; y ) = D(i, j) ∇ ` ( ; y ), where in the ﬁrst inequality we use δ Eδ ≤ L ), and in the last equality we use the deﬁnition that L = 2Lp for all k ∈ X. Before we specify the concrete learning rate, we have the following generic convergence characterization. Proposition D.2. Given learning rate {η , we have the following holds for Algorithm 1. Proof of Theorem 3.1. Given Proposition D.2, suppose we use constant stepsize, i.e., η = η for all t ∈ [T], and sample τ ∼ Unif([T]), then for any k ∈ X, which implies that for any k ∈ X, ( f (Θ ) − f )L f (Θ ) − f ∇f = O , ∀k ∈ X. Proof of Theorem 3.2. Given Proposition D.2, suppose we use token-agnostic constant stepsize, i.e., = η for all t ∈ [T], k ∈ X, and sample τ ∼ Unif([T]), then for any k ∈ X, which implies that for any k ∈ X, ( η f (Θ ) − f ∇f + 2 η − η − ( f (Θ ) − f )L f (Θ ) − f ∇f = O , ∀k ∈ X. ∇f ≤ f (Θ ) − f + 2 cL | {z } (1 + α ( η + α exp(−c)L . (29) | {z } | {z } ∇f = f (Θ ) − f + 2 | {z } exp(−c) + (1 + α {z } | {z } ∇f ≤ f (Θ ) − f + 2 | {z } + (1 + α exp(−c)L , (30) | {z } | {z } or equivalently, Let denote a positive integer to be determined later, recall that from (27), for any t ≥ T , with probability at least 1 − (here = exp(−tp )), we have |η | ≤ α hold. Denote B = {w : |η | ≤ α }, then we have that for any k ∈ X, f (Θ ) − f LM = O , ∀k ∈ X, Hence both Corollary 3.2 and 3.3 hold for Counter-based Frequency-aware SGD after properly adjusting the constant factor in the statement. 1 + exp(−τ|U|) 1 −exp(−τ) 1 + exp(−τ|V|) 1 −exp(−τ) ) ( ) ( 1 −exp(−τ|U|) 1 + exp(−τ) 1 −exp(−τ|V|) 1 + exp(−τ) ) ( ) ( Thus we obtain, for some ν ≥ 2. Deﬁne U as the set of users whose frequencies are within 2-factor from the highest frequency: U = {i : n ≥ 1/2}, and V similarly as V = {j : m ≥ 1/2}. We refer to U as the top users, and V as the top items. Then there exists an absolute constant C > 0, such that ≤ Cn ≤ Cm , ∀i ∈ U, ∀j ∈ V. Take C = 16, we obtain the desired result.