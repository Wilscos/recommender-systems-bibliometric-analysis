The identiﬁcation of the most relevant results is a crucial task in many Information Retrieval applications including conversational IR [19], advertising, machine translation, question answering, recommender systems [14], etc. An example are recent conversational assistants such as Siri, Google Assistant, and Alexa, which are becoming very popular. These devices exploit a new way of interaction where the user conversate with the device by asking a question and the device interactively provides her the most relevant answer. The identiﬁcation of the most relevant result undergo a ranking task. State-of-the-art solutions for ranking leverage ad-hoc machine learning techniques, known as Learning-to-Rank [18], to score the candidate results and select the top-1 achieving the highest relevance score. The retrieval of the most relevant result can be addressed in two different ways: i) by exploiting state-ofthe-art Learning-to-Rank techniques such as λMART [27], which are based on univariate scoring functions that individually score one candidate result at a time, to select the candidate achieving the highest score; ii) by employing pairwise Learning-to-Rank techniques such as DUOBERT [22], which are based on bivariate scoring functions that score a pair of candidate results at a time, e.g., a binary judgment stating which of the results is more relevant, to select the candidate achieving the highest sum of pairwise scores of an all-vs-all tournament. While the former approach exploits only the information of a single result at a time for computing the ranking score, the latter approach is potentially more powerful because it exploits the information of two candidates at a time for computing the outcome of the tournament. However, the latter approach is more expensive than the former one as it performs a quadratic number of comparisons to score all pairs of candidate results, thus making pairwise approaches unappealing in scenarios with tight time constraints. In this article, we investigate efﬁcient algorithms to reduce the number of time-consuming comparisons in tournaments. In detail, we map the task of identifying the most relevant result to the task of efﬁciently ﬁnding a winner of the round-robin tournament. A round-robin tournament among n results, where each result is compared to all others, can be modeled by a tournament graph of n nodes. A tournament graph is an oriented complete graph T = (V, E) [24]. The orientation of an arc in E tells the winner of the comparison, namely, we have the arc (u, v) ∈ E iff u beats v in their match. We address the problem of ﬁnding a champion of the tournament, also known as Copeland winner [9], which is a vertex in V with the maximum out-degree, i.e., a result that wins the highest number of matches. Our goal is to ﬁnd a champion by minimizing the number of arc lookups, namely, the number of comparisons to perform. The main contribution of this paper is an optimal deterministic algorithm that ﬁnds all Copeland winners employing O(`n) comparisons, where ` is the minimum number of matches lost by any player. Moreover, we prove that Ω(`n) comparisons are necessary, even for randomized algorithms, to obtain a correct answer with any constant probability. It is worth to notice that we match a randomized lower bound with a deterministic algorithm, showing that randomization does not give any advantage on this problem. Then, we extend our result to three strictly related problems. First, we show how to retrieve all top-k players in time O(`n), where `is the number of matches lost by the k-th best player. Second, we consider a model of computation in which we are allowed to play a batch of B matches in parallel, and we design an algorithm that achieves optimal speedup with respect to the sequential version. Third, we suppose that playing a match does not return a binary result, but a distribution over the outcomes, i.e., a pair of complementary probabilities. In this setting we deﬁne the champion as the player that minimizes the expected number of matches lost and we show an algorithm to ﬁnd all champions in time O(`n), where ` is the expected number of matches lost by the champion. Finally, we report a comprehensive experimental assessment of the proposed algorithms: we evaluate their performance in terms of running time and number of comparisons, which are then compared with respect to a baseline that perform the evaluation of all the possible pairwise comparisons. We focus our attention on the execution of the three-stage ranking architecture introduced by Nogueira et al. [23]. Results show that our proposed algorithms achieve a remarkable speedup of up to 13× with respect to the resolution of the full tournament. The rest of the article is structured as follows: Section 2 discusses the related work, while Section 3 deﬁnes the problem. Section 4 provides a detailed analysis of the problem complexity and Section 5 presents an efﬁcient algorithm to solve it. Moreover, Section 6 discusses two variants of that algorithm that solve two extensions of the original problem. Finally, Section 7 presents a comprehensive analysis of proposed algorithms in a ranking scenario, and Section 8 concludes the work. Tournament graphs are a well-known model that has been applied to several different areas such as sociology, psychology, statistics, and computer science. Examples of applications are round-robin tournaments, paired-comparison experiments, majority voting, communication networks, etc. [6], [15], [17], [20], [24]. In this area, we identify two different research lines. The ﬁrst one aims at ﬁnding the tournament winner, while the second one aims at ranking the list of candidates using pairwise approaches. Given a ranking of candidates, we can easily deﬁne the champion as the top-1 element of a the global ranking, therefore the two tasks are related with each other. In this section, we describe the most important results concerning these two problems. According to previous works [6], [17], [20], there is no unique deﬁnition of the notion of a tournament winner. Nevertheless, all of them agree on deﬁning the winner whenever there is a candidate, called Condorcet winner, which beats all the others. Different deﬁnitions of winner require different complexities of the algorithms used to identify it. The easiest case to consider appears when T is a transitive tournament graph, i.e., a directed acyclic graph, since it is trivial to ﬁnd the Condorcet winner in linear time by performing a knock-out tournament where the loser of any match is immediately eliminated. Instead, for a general tournament T , the complexity of ﬁnding a winner is much higher and strictly depends on the deﬁnition of winner. A winner as deﬁned by Banks [4] is the Condorcet winner of a maximal transitive sub-tournament of T . As there may be several of these sub-tournaments, the Banks solution is the set of all these winners. The problem of ﬁnding just one winner can be computed in Θ(n) arc lookups, while ﬁnding all of them is a NP-hard problem [15]. Slater [26] deﬁned the winner starting from a ranking of candidates. He deﬁned a Slater solution to be a total order ≺ on vertices that minimizes the number of mis-ordered pairs of vertices, where a pair (u, v) is mis-ordered if u beats v and u ≺ v. The champion is then deﬁned as the maximum element with respect to ≺. However, the computation of the Slater solution is NP-hard as it reduces from the Feedback Arc Set Problem [8]. Ailon et al. [1], [2] provide a bound to the error achieved by the Quicksort algorithm when used to approximate a Slater solution. The error is deﬁned as the number of misordered pairs of vertices. Ailon et al. show that the expected error is at most two times the best possible error. It is apparent that the proposed algorithm requires Ω(n log n) arc lookups with high probability. Even though the overall approximation is good, this algorithm fails in ﬁnding a champion w every time one of the Quicksort pivots beats w, hence it is not suitable for our purposes. The results by Shen et al. [25] and Ajtai et al. [3] provide a ranking based on the deﬁnition of king. The vertex u is a king if for every vertex v there is a directed path from u to v of length at most 2 in T . The ranking algorithm by Jian et al. [25] ﬁnds a sorted sequences of vertices u, u, . . . , u such that for every i 1) ubeast u, and 2) uis a king in the sub-tournament induced by the items u, u, . . . , u. The authors provide a O(n) deterministic algorithm to compute this sequence. In addition, quicksort produces such a sequence in O(n log n) comparisons w.h.p. and quickselect retrieves a king in expected linear time. Unfortunately, the deﬁnition of king is weaker than the one of Copeland winner. Indeed, the latter implies the former [24], and it is possible to construct tournaments in which every vertex is a king. Thus the deﬁnition of king does not help us in the identiﬁcation of the best candidate. A proliﬁc research line studies the ranking problem under persistent comparison errors [7], [10], [11], [16]. This task deals with queries affected by random noise in a scenario where comparison errors are persistent. In this setting, we consider the set of vertices as equipped with a transitive order ≺, and every arc of the tournament as the result of a noisy comparison between two items. The answer associated to the comparison (u, v) is modeled as {u ≺ v} · η, where {η}are independent and identically distributed Bernoulli random variables of probability p ≈ 1. By deﬁning the dislocation of u as the difference between its real rank and the rank assigned by an algorithm, Geissmann et al. [10] proved that every algorithm produces a ranking with maximum dislocation Ω(log n) and total dislocation Ω(n). A recent work by Geissmann et al. [11] settles the problem, matching both lower bounds in O(n log n) time. Unfortunately, this model does not produce a strong enough guarantee on the quality of the champion, that is only known to be within the top O(log n) candidates of the original ranking. There are several other notions of winner, and most of them can be computed in polynomial time. We refer to Hudry [15] for a complete survey on this topic. The deﬁnition used in this paper is the one given by Copeland [9], called Copeland solution, where we rank vertices according to the number of matches they win, and a champion is the candidate winning the most matches. As we already mentioned, the Copeland solution requires Ω(n) arc lookups and there is a trivial algorithm to match it [13]. However, Geissmann et al. [12] considered a model, similar to the aforementioned persistent comparison errors model, in which errors are no longer stochastic but their total number is bounded. They ﬁx an upper-bound e to be the total number of errors and they propose an algorithm to ﬁnd the Copeland winner of the resulting tournament in√ O(ne) comparisons and time. In this article, we advance the state of the art by reporting an improvement of the result provided by Geissmann et al.. In particular, we propose an algorithm that ﬁnds the Copeland winner in O(`n) time and comparisons, where ` is the minimum number of matches lost by any player,√ hence ` ≤e. Our novel algorithm presented in Section 5 is oblivious with respect to `, while the algorithm by Geissmann et al. [12] assume to know e in advance. Nevertheless, by combining the result by Geissmann et al. [12] with the aforementioned 2-approximation of Min-FAS by Ailon et√ al. [1], [2], we obtain a O(ne + n log n) time algorithm oblivious with respect to e. We map the problem of efﬁciently retrieving the top-1 result from a list of item to the task of ﬁnding a champion of the round-robin tournament. A tournament T = (V, E) can be modeled as a directed acyclic graph. Given u, v ∈ V , we say that u wins against v if (u, v) ∈ E. In several real-world applications, the direction of the arc is inferred by employing machine learning classiﬁers that estimate whether u wins against v or viceversa. In the following, we call “arc lookup” the above estimation done on a pair of vertices, i.e., (u, v). We deﬁne the champion as the player, i.e., one vertex of the tournament graph T , winning the highest number of matches in the inferred tournament graph, i.e., the Copeland winner, and our goal is to ﬁnd it by employing the minimum number of comparison queries, i.e., arc lookups. If the tournament is transitive, i.e., if u wins against v and v wins against w, then u wins against w, we can trivially identify the unique champion with Θ(n) arc lookups. Indeed, the champion is the only vertex that wins all its matches and, thus, we can perform a knock-out tournament where the loser of any match is immediately eliminated. However, ﬁnding the champion of general tournament graphs requires Ω(n) arc lookups [13]. As a consequence, there is nothing better to do than playing all the matches in order to ﬁnd the champion. This means that the structure of the underlying tournament graph heavily impacts the complexity of the problem. We consider therefore a novel parameterization and we deﬁne ` as the number of matches lost by the champion. Then, we design a deterministic algorithm that solves the problem in O(`n) comparison queries and time. This parameterization is motivated by the fact that it is possible to design very accurate machine learning classiﬁers for several tasks. For this reason, we expect a low number of matches ` lost by the most relevant item, hence a quasi-linear number of arc lookups is required by our algorithm to ﬁnd it. This compares with the quadratic number of lookups of the previously known algorithms [13]. We ﬁrst prove that Ω(`n) arc lookups are required to solve the problem with any deterministic algorithm. We also prove that the same bound holds if we allow the algorithm to be incorrect with any ﬁxed probability. Then, we present the optimal deterministic algorithm to solve the problem whose time complexity matches the lower bound without any prior knowledge about the parameter `. We ﬁnally detail how to extend our algorithm to solve interesting generalizations of the problem. The ﬁrst one asks for reporting the top-k players with the highest number of wins. Our novel algorithm solves this problem with Θ(n`) arc lookups, where `is the number of matches lost by the k-th player. We also present a variant of our algorithm to take full advantage of parallel inference mechanisms to perform arc lookups with a machine learning classiﬁer, i.e., where a batch B of pairwise comparisons are performed in parallel. This is useful in practice because pairwise comparisons are batched when the inference is done on novel computing platforms like, for example, GPGPUs. We show that our algorithm ﬁnds the winner by performing O(+ ` log B) arc lookups. Finally, we generalize the tournament problem in a probabilistic framework, where each arc (u, v) ∈ E is labeled with the likelihood that u wins against v. These probabilities can be interpreted as the conﬁdence of the classiﬁer about the outcome of the comparison. In this framework, the goal is to ﬁnd the player with the largest sum of probabilities. To this end, we present a deterministic algorithm to ﬁnd the champion of a probabilistic tournament by performing the minimum number of arc lookups needed to solve this problem. In this section, we prove the lower bound of the Copeland winner problem. An adversarial argument is used by Gutin et al. [13] to prove that ﬁnding a champion requires Ωn arc lookups. Therefore, the trivial algorithm that ﬁnds a champion by performing all the possible matches is optimal in general. The problem is indeed much more interesting if we parameterize it with `, the number of matches lost by the champion. Note that ` is unknown to the algorithm. The goal of this section is to prove that Ω(`n) arc lookups are necessary to ﬁnd a champion. We ﬁrst show that this bound applies to deterministic algorithms. Then we generalize it to the class of “Monte Carlo” randomized algorithms that are allowed to return an incorrect answer with a ﬁxed positive probability. The latter result clearly implies the former. However, in the following we report them in increasing order of difﬁculty. 4.1 Deterministic lower bound The following theorem shows that any deterministic algorithm employs Ω(`n) arc lookups to ﬁnd a champion. Theorem 4.1. Any deterministic algorithm that ﬁnds a champion in a tournament graph T with n vertices and with ` matches lost by the champion requires Ω(`n) arc lookups. Proof. The lower bound is proved by using an adversarial argument. Assume that an algorithm claims that a vertex u, losing ` matches, is a champion by performing`(n−1) arc lookups. There must exist a node v such that the algorithm has unfolded less than ` arcs incident to v. We thus can make v to win more matches than u by setting v’s unfolded matches, then let the algorithm be incorrect. In other words any correct algorithm, claiming that a vertex u is a champion with ` matches lost, must be able to certiﬁcate its answer by showing: 1) a list of n −1 −` matches won by u and 2) a list of ` matches lost by any other vertex v. We just proved that no deterministic algorithm can unfold extend such a non-existence result to any randomized algorithm, which is allowed to be wrong with a ﬁxed probability. This section is devoted to prove the following theorem stating that it does not exist a Monte Carlo algorithm that ﬁnds the Copeland winner by unfolding O (`n) arcs. Theorem 4.2. Given a tournament T with n vertices and with ` matches lost by the champion, it does not exist a randomized algorithm that unfolds O (`n) arcs and outputs the Copeland winner of T with ﬁxed positive probability. To prove the theorem above, we need to deﬁne the auxiliary problem below and operate a reduction. Deﬁnition 4.3 (Anomalous Row Problem). Given a matrix M ∈ Fsuch that every row but one presents k + 1 zeroes and the remaining one presents k zeroes, ﬁnd the k-zeroes row. We now state the following theorem referring to the anomalous row problem, which is strictly related to Theorem 4.2. Theorem 4.4. It does not exist a randomized algorithm that solves the anomalous row problem (Deﬁnition 4.3) by probing answer with ﬁxed positive probability. We now show that if there exists an algorithm violating Theorem 4.2 then we can design an algorithm that violates Theorem 4.4. Thus, proving the latter is sufﬁcient to prove the former. Given an instance of the anomalous row problem, M ∈ F, we can assume that k and m are odd and m > 3k. Indeed, if this is not the case, it is sufﬁcient to add a dummy row containing k + 1 zeroes and several dummy columns containing only ones. It is apparent that this modiﬁcation preserves both the k-zeroes row and the asymptotic complexity. Then, we construct a tournament having n = k + m players and adjacency matrix where B ∈ Fand C ∈ Fare the adjacency matrices of regular tournamentsandfM is the complementary matrix of M, meaning thatfM= 1 − M. We can easily prove that the champion is among the ﬁrst k players and loses exactly ` = (3k − 1)/2 matches. In fact, due to regularity, every row in B contains exactly (k−1)/2 zeroes and M satisﬁes the hypotheses of Deﬁnition 4.3. Thus, every player among the ﬁrst k ones loses either ` or ` + 1 matches. On the other hand, any player in the last m rows, loses at least (m − 1)/2 matches, and m > 3k guarantees that (m − 1)/2 > `. Therefore, if we ﬁnd a champion of the constructed tournament then we automatically solve the anomalous row problem. We now need to show that the anomalous row problem cannot be solved with O (km) = o (`n) probes, entailing the lower bound for the Copeland winner problem. First we enunciate a game-theoretic lemma by Yao [28] declined within the terms of our problem. Lemma 4.5 (Yao’s Lemma). Let A be the family of deterministic algorithms that output a, possibly wrong, solution to the anomalous row problem and probe O (km) cells. Consider A equipped with a probability distribution. Then consider the function C(A, x) that returns 1 if the algorithm A is correct on input x and 0 otherwise. Finally, consider a probability distribution D over F. We have minE[C(A, x)] ≤ E[C(A, x)] ≤ maxE[C(A, x)] . We know that a Monte Carlo algorithm that proves bution A, in fact it just tosses some coins at run-time and it decides which algorithm to branch into. Therefore minE[C(A, x)] is the probability of the Monte Carlo algorithm deﬁned by A of being right in the worst case, and maxE[C(A, x)] is the average case of the best deterministic algorithm against a random input with distribution D. Finally, we can now prove Theorem 4.4. Proof. It is sufﬁcient to show an input distribution D such that any deterministic algorithm with running time O (km) succeeds with arbitrarily small probability, for k, m → ∞. We choose the permutation φ of {1 . . . k} and k permutations σ. . . σof {1 . . . m} uniformly at random. Consider the random matrix X ∈ Fsuch that where M is a deterministic input matrix as in Deﬁnition 4.3. Let D be the distribution of X, and A ∈ A be the algorithm such that E[C(A, x)] is maximum. It is sufﬁcient to show that E[C(A, x)] → 0 to prove that no Monte Carlo algorithm can perform less than Ω (km) cell probes. Consider the maximum number P of cells probed by A and We now color Γcells in the input matrix. We ﬁrst color a 1-valued cell in the k-zeroes row, then we choose Γ−1 rows containing k + 1 zeroes and color a 0-valued cell drawn from each of those. To this end, we assume to perform such coloring before randomizing the input. We want to estimate the probability that the algorithm probes any colorful cell. Deﬁne the event E“the algorithm picks a colorfull cell during the i-th probe”. The probability of Eis Γ/km since the chosen cell’s row contains a colorful cell with probability Γ/k and, given that , the probability of picking the colorful cell is 1/m. Therefore, Finally, we notice that, in case none of the colorful cells is probed, the algorithm “sees” a perfectly symmetric distribution over the Γrows containing a colorful cell. Therefore, the best it can do is to produce a random output, which is right with probability 1/Γ, at most. To conclude, considerS E =E, we have E[C(A, x)] ≤ P(E) + P(C(A, x) = 1 |E) where the last limit holds for k and n that goes to inﬁnity simultaneously. In this section, we present a simple, deterministic, and optimal algorithm that ﬁnds every champion in O(`n) arc lookups and time. We ﬁrst introduce the algorithm. Then, we prove its correctness and we bound the number of arc it unfolds. Finally, we discuss some implementation details to show that the number of operations performed by the algorithm is O(`n) and the space required is O(n). 5.1 Algorithm Description We detail our optimal deterministic algorithm in Algorithm 1. The number ` of matches lost by the champion is unknown to the algorithm. Thus, it performs an exponential search to ﬁnd the suitable value of α such that α/2 ≤ ` < α (line 2) so to solve the problem by assuming that the champion loses less than α matches. At each iteration, the algorithm maintains a set A of “alive” vertices that is initially equal to V . Then, it performs an elimination tournament among the vertices in A by eliminating a player each time it loses α matches (line 12) until only 2α vertices remain alive (line 6). This stop condition guarantees the convergence of the algorithm. The matches are selected arbitrarily to avoid to play the same match multiple times (line 7). When the elimination tournament ends, a candidate champion is found via the FINDCHAMvertex c of A with the maximum out-degree in T . Whenever the candidate c loses at least α matches (line 16), the value of α is not the correct one and the champion may have been erroneously eliminated before. Thus, c could not be a Algorithm 1 champion and the algorithm continues with the next value of α (line 2). In the reminder of this section, we prove the following theorem stating that Algorithm 1 matches the number of arc lookups indicated by the lower bound (Theorem 4.1) and requires linear space. Theorem 5.1. Given a tournament graph T with n vertices and with ` matches lost by the champion, Algorithm 1 ﬁnds every champion with O(`n) arc lookups and time. It also requires linear space. 5.2 Correctness Let us ﬁrst assume that the value of α is such that α/2 ≤ ` < α. We now prove that, under this assumption, the algorithm correctly identiﬁes a champion. First, we observe that the algorithm cannot eliminate the champions as each of them loses less than α matches. Thus, if we prove that the algorithm terminates, the set A contains all the champions and the FINDCHAMPIONBRUTEFORCE procedure will identify any (potentially all) of them. Note that a champion of T may not be a champion of the sub-tournament restricted to only the vertices in A. This is why FINDCHAMPIONBRUTEFORCE procedure computes the out-degrees of all vertices in A by looking at the edges of the original tournament T . We use the following lemma to prove that, eventually, the condition |A| = 2α is met and the algorithm terminates. Lemma 5.2. In any tournament T of n vertices there is at least one vertex having in-degree (n −1)/2. Proof. The sum of the in-degrees of all vertices of T is exactly =. Since there are n vertices, there must be at least one vertex with in-degree. Thus, each tournament of 2α + 1 vertices, or more, has at least one vertex losing at least α matches. This means that the algorithm has always the opportunity to eliminate a vertex from A until there are 2α vertices left. Notice that the above discussion is valid for any value of α smaller than the target one. Thus, any iterations of the exponential search will terminate and it eventually ﬁnds a suitable value of α, i.e., α/2 ≤ ` < α, where a champion will be identiﬁed. 5.3 Complexity We now present an analysis of the complexity of the algorithm. Let us ﬁrst consider the cost of an iteration of the exponential search. We observe that each arc lookup increases one entry of lost by one and that none of these entries is ever greater than α. Thus, the elimination tournament takes no more than nα arc lookups. Moreover, the FINDCHAMPIsince it just unfolds every arc of the remaining 2α alive nodes. Thus, an iteration of the exponential search takes less than 3nα arc lookups. We get the complexity of the overall algorithm by summing up over all the possible values of α, which are all the powers of 2 from 1 up to 2`. Thus, we have at mostP 3n2= O(`n) arc lookups. 5.4 Implementation details We now prove that Algorithm 1 can be implemented in O(`n) time and linear space. We do this by exploiting the fact that Algorithm 1 allows us to choose any arc as soon as its vertices are alive and it has never looked up before. An efﬁcient implementation is achieved by maintaining two arrays of n elements each: an array A storing the alive vertices and an array lost storing the number of matches lost by each vertex. A counter numAlive stores the number of alive vertices. Our implementation maintains the invariant that the preﬁx A[1, numAlive] contains only alive vertices. We use two cursors pand pto iterate over the elements in A. At the beginning p= 1, p= 2 and numAlive = n. Our implementation performs a series of matches involving vertex A[p] and all other vertices in A[p+ 1, numAlive], thus, advancing the cursor p. Then, it moves pto the next position. After every match between A[p] and A[p], we increment lost of the loser, say vertex v. Whenever lost[v] equals α, we eliminate v according to the following two cases, then we decrease numAlive by one. The ﬁrst case occurs when v is A[p]. We swap A[p] and A[numAlive], we end the current series of matches, and we start a new one. The second case occurs when v is A[p]. Here, we swap A[p] and A[numAlive], and we continue the current series of matches. In both cases, we decrease numAlive by 1 so that we preserve the invariant. A similar, slightly less efﬁcient, implementation employs a linked list A to store the alive vertices. In this implementation, the removal of an element from the list is trivial, and pand pare pointers that always advance in the list. When preaches the end of the list, we advance pby one position in the list and we set pto point to the element just after p. This implementation allows us to process the vertices according to the input order (as we never swap elements), which may be desirable in practice if we can somehow predict the strongest of the vertices and sort them according. As each step of the exponential search ignores the arc lookups of the previous steps, certain arcs may be unfolded more than once. Therefore, to reduce the number of arc lookups preserving the time complexity at the cost of using O(`n) space instead of O(n), an hash table can be employed to store the results of all arc lookups across the exponential search steps so to avoid unnecessary unfolds. In detail, each time Algorithm 1 wants to unfold an arc, it checks the hash table ﬁrst and, only if this is a new arc, the algorithm unfolds the arc and stores the result in the hash table for the next exponential search steps. We now discuss some generalizations of the Copeland winner problem and we modify Algorithm 1 to solve these problems efﬁciently. First, we show how to retrieve the top k items, i.e., not only the top-1, by maintaining the complexity proportional to the number of matches lost by the k-th player. Then, we consider the case of a binary machine learned classiﬁer returning a pair of probabilities instead of a binary outcome and redeﬁne the problem in a probabilistic fashion. Finally, we consider the case in which we are able to process batches of arc lookups in parallel, so to exploit parallel processing units, e.g., GPUs. 6.1 Top-k retrieval version A simple and useful generalization of the Copeland winner problem is to ﬁnd the top-k results, i.e., the k vertices with the highest out-degrees. In this setting, the exponential search of Algorithm 1 can be modiﬁed to ﬁnd the minimum value of α such that the number `of matches lost by the kth result is between α/2 and α. To this end, the exponential search must end whenever it ﬁnds k vertices with less than α comparisons lost. To accomplish this task, the FINDCHAMPIONBRUTEFORCE(A, E) procedure must be modiﬁed to return the indices of the top-k results of A along with number of matches lost by them. Since `≤ `≤ . . . ≤ `, the higher the value of k, the higher the time complexity O(n`) of the algorithm. 6.2 Probabilistic version Typically, the outcome of a pairwise classiﬁers is not a binary response, instead it is a pair of complementary probabilities that can be interpreted as the algorithm’s conﬁdence about the comparison’s outcome. Thus, a natural generalization of the Copeland winner problem emerges if we associate to each arc (u, v) the probability pof u beating v. Since the probabilities are complementary, we also know that p= 1 − p. We refer to this graph as probabilistic tournament graph. In this setting, the arcs are Bernoulli random variables, and we deﬁne the champion as the player u minimizing the expected number of matches lost, i.e.,P pby linearity of the expectation. Since we want our complexity to be parameterized with the expected number of matches lost by the champion, we coherently call this quantity `. In this section, we show that Algorithm 1 needs only little adaptation to work in this setting. Consider the pseudocode of Algorithm 1, we treat lost counters as real-valued and substitute line 10 with two commands incrementing lost[u] by pand lost[v] by p. Once operated these slight modiﬁcations we are ready to prove the following theorem (analogous of Theorem 5.1). Theorem 6.1. Let T be a probabilistic tournament graph with n vertices and with ` the expected number of matches lost by the champion. The modiﬁed version of Algorithm 1 described above ﬁnds every champion by requiring O(`n) arc lookups and time. The algorithm requires linear space. Correctness The correctness proof is almost identical to the one we have detailed in Section 5. We are not repeating the whole proof, in fact it is sufﬁcient to substitute occurrences of “losses” with “expected losses” and reformulate the Lemma 5.2 as follows to obtain the desired proof. Lemma 6.2. In any probabilistic tournament T of n verticesP there is at least one vertex u such thatp≥ (n − 1)/2. In other words, there exists a player whose expected number of matches lost is at least (n − 1)/2. Proof. The sum of the “expected losses” of all vertices of T is exactly=. Since there are n vertices, there must be at least one vertex losingmatches, on average. Complexity The complexity analysis is again akin to the one of Section 5, but we dig in a deeper details here. Each unfolded arcP increaseslost[u] by one; since lost[u] of any u ∈ V is incremented until it surpasses α of at most one unit at a time, then lost[u] cannot be greater than α + 1 andP lost[u] < (α + 1)n. Therefore no more than (α + 1)n arcs are unfolded during the elimination step of a single iteration of the exponential search. Moreover, as in Section 5, FINDCHAMPIONBRUTEFORCE procedure takes less than 2nα arc lookups. Thus an iteration of the exponential search takes less than 3n(α + 1) arc lookups, and summing up all these arc lookups we get the desired O(`n) complexity. 6.3 Parallel (Batched) version In modern architectures, e.g., GPUs, it is possible to perform multiple arc lookup operations in parallel. A natural question is whether we are able to take full advantage of this parallelism to cut down the complexity of Algorithm 1. In this subsection, we propose Algorithm 2 under the assumption to be able to unfold a batch of B arcs in parallel. In particular, Algorithm 2 processes O+ ` log B batches, so the overhead is asymptotically negligible if B = O(n/ log n), which is a condition that often holds in practice. Algorithm 2 is a slight modiﬁcation of Algorithm 1. As the previous algorithm, it performs an exponential search of ` repeatedly doubling the parameter α. For each α it assumes that the champion belongs to the set of alive vertices A and performs an elimination tournament among the vertices of A eliminating any player that loses α matches. The elimination step is now performed in batches (line 12) and terminates when the alive players are few enough (line 7). The method FINDCHAMPIONBRUTEFORCE(line 18) can be parallelized with no efforts by unfolding all O(6αn) arcs in batches of B arcs at a time, hence we focus on the elimination step. The main difference with respect to Algorithm 1 resides in the procedure BUILDBATCH, which decides what are the B arcs to lookup in parallel. It creates local copies Aand lostof the set A and the vector lost, then the procedure selects matches in A× Athat have not been played yet and, for each of them, assigns a loss to both opponents. Now suppose that the batched games were played sequentially (namely, played at line 31) and lost and A were updated accordingly: we would have that lostprovides an upper estimate of lost and A⊆ A. Therefore, it is guaranteed that every insertion in a batch will produce a match loss for a player that would be still alive in case we unfolded the batch sequentially. This is a point worth stressing since it guarantees that lost[u] ≤ α for each u ∈ V. Finally, even though it is not guaranteed that BUILDBATCH produces a B-sized batch, for that to happen it is sufﬁcient that A has at least 2B + 2α elements. This can be enforced halving the batch size every time this condition does not hold (line 8) and we will see that this will not spoil the complexity of Algorithm 2. Intuitively, the elimination step consists of two different epochs: the ﬁrst one unfolds arcs in B-sized batches (where B is the original batch size) until |A| ≥ 2B + 2α; the second one processes smaller and smaller batches until |A| is small enough (line 7). Correctness The correctness can be proven exactly in the same way as the sequential case, the only detail to take care about is that the function BUILDBATCH terminates by producing a B-sized batch. It is sufﬁcient to notice that as long as |A| > 2α there is an arc to unfold in A\S (using the same argument of the sequential case), and that since we call INCREASELOSS at most 2Btimes at each iteration, then |A| ≥ 2B+2α (line 8) is sufﬁcient to ensure the termination. Complexity Theorem 6.3. Given a tournament graph T with n vertices and with ` matches lost by the champion, Algorithm 2 ﬁnds every champion by requiring O+ ` log Bcalls to UNFOLDINPARALLEL and O(`n) time and space. Proof. Consider the i-th iteration of the cycle at line 7 and denote with Athe number of alive vertices |A| and with Bthe value of B, evaluated immediately before calling the BUILDBATCH function at line 11. In particular we have A= |V | and B= B. First, we prove the following lemmas. Lemma 6.4. For each i ≥ 0, A− B≤ A≤ Aholds. Proof. The ﬁrst inequality holds since no more than B games are played during the i-th iteration and thus no more than Bplayers are removed from the alive set. The second inequality holds since the set A of alive vertices decreases over time. Lemma 6.5. Let j be the ﬁrst iteration in which the conditional statement at line 8 is true, that is j = min {i |A< 2B + 2α}. For each i ≥ j, 2B+ 2α ≤ A≤ 4B+ 2α holds. Proof. We prove it by induction. Base Case, i = 1: it is sufﬁcient to notice that B= 2B and A< 2B+ 2α ≤ Ahold thanks to the deﬁnition of j, and combine those equations with Lemma 6.5. Algorithm 2 Inductive Case, i > 1: during the i-th iteration we have two cases depending on whether we update the value of B or not. If we do not update B, that is A≥ 2B+ 2α, then we have B= Band A≤ A≤ 4B+ 2α by inductive hypotheses. Otherwise, if we update B, we have A< 2B+ 2α and B= 2B, then A< 4B+ 2α and A≥ A− B≥ B+ 2α = 2B+ 2α. We now ﬁx α and upper-bound the number of arcs unfolded for each batch size B. First we deal with the case B= B in which we employ the original batch size; in that case, we can safely upper-bound the number of arcs with αn since every lost counter is never greater than α and every arc unfolded increases a lost counter by one. Then, consider the case B= B/2, for a speciﬁc value of k; we have that |A| ≤ 4B/2+ 2α and, by applying the same argument as above on the elements of A, that at most α4B/2+ 2α arcs are unfolded using a batch of size B/2. Thanks to the clauses at lines 7 and 8, we have 6α ≤ A≤ 4B+ 2α, which implies B≥ α. To compute the total number of calls to UNFOLDINPARALLEL, it is sufﬁcient to divide the maximum number of arc lookups (α|A|) by the appropriate batch size (B) and sum them up where the ﬁrst addendum refers to the batches processed unfolding B arcs at a time, while the other addenda refers to the case of smaller batch sizes. Finally, to get the number of parallel unfoldings during the entire execution, it sufﬁces to sum the quantity above for α = 1, 2, . . . , 2and we get the desired O+ ` log B. Now it remains to prove that Algorithm 2 uses O(`n) operations and space. The proof is the same as for Algorithm 1, we mainly need to pay attention to lines 26 and 27 since creating local copies would increase the complexity. Fortunately, it is sufﬁcient to use the global versions of A and lost, store in a list the changes performed on them, then restore their state before terminating BUILDBATCH. In fact, we adopted local copies only to make the pseudocode clearer. Moreover, since the BUILDBATCH can temporarily skip some vertices (according to the local copy of lost) that may be re-included later after the parallel unfold, we cannot employ the linear-space selection described in Section 5.4. In this case, we further need to associate to each node u the set (hash table) of all arcs (u, ·) ∈ E unfolded by the algorithm, so to skip the ones already unfolded, in constant time. The solution proposed in Section 5.4, which employs the cursors pand pto decide the arcs to unfold, properly extended with this check, guarantees O(`n) time and space. Implementation Details Algorithm 2 could not use all the comparisons that are available in a single batch, because of the batch size halving (row 8) or because the brute force call (row 18) involves a number of arcs that is not divisible for the batch size. For this reason, we employ a simple heuristic to exploit each batch the most, which applies when employing the hash table to store the results of the arc lookups (Section 5.4). In detail, we add new arcs to the batch, deterministically, each time Algorithm 2 asks to unfold a partially ﬁlled batch. We take the node with the smallest number of comparisons lost that still has unfolded arcs, e.g., using an heap data structure, and we add the remaining unfolded arcs, in the order they appear, to the batch until it becomes full. If all node’s arcs are added and the batch is still non-full, then the previous operation is repeated until either the batch becomes full or all arcs have been unfolded. In this section, we present a comprehensive experimental assessment of the proposed algorithms focusing on a ranking task. First, we describe the pairwise ranking model employed for the experiments along with the dataset used for the assessment. Then, we evaluate the performance achieved by the proposed algorithms in terms of running time and number of comparisons. Pairwise model Nogueira et al. recently tackled the task of ranking documents in a Web scenario by using a three-stage ranking architecture [23]. The duoBERT models recently scored among the top-10 solutions of the MS MARCO Passage Ranking Leaderboard. Moreover, it is the ﬁrst solution of the MS MARCO leaderboard whose public code is publicly available. The ﬁrst stage selects the top-1000 results according to BM25. The second stage re-ranks these results using a monoBERT neural model [22], which ingests the text of a document at a time to classify it as relevant or not. Lastly, the third stage re-ranks the top-30 results of the previous stage by using a duoBERT pairwise model [23], which classiﬁes all pairs of document’s texts to induce a tournament among the results. In particular, the two most promising conﬁgurations presented in Nogueira et al. [22] have been tested: duoBERTand duoBERT. The former works by assigning to each document the sum of the probabilities of all comparisons, while the latter rounds these probabilities in {0, 1} before summing them. In our experiments, we replicate the entire multi-stage pipeline proposed by Nogueira et al. and we apply our algorithms in the last stage of the pipeline, i.e., duoBERT and duoBERT, to reduce the number of pairwise inferences to perform through these models. Dataset We apply the duoBERT model to the Microsoft MAchine Reading COmprehension dataset (MS MARCO) [21], as done by the authors of the model. The MS MARCO dataset is created from approximately half a million anonymized questions sampled from the search query logs of Bing. The objective of the Passage Ranking task is to retrieve passages that answer the questions. The development set used for the assessment contains 6,980 queries. On average, one relevant passage document is available per query. For our purposes, we focus on the performance of the third stage of the pipeline described above, which works by re-ranking 30 documents per query. Metrics We assess the performance of Algorithms 1 and 2 in terms of number of comparisons and time required by the model to compute the solution. For fairness, even if our contribution does not regard the effectiveness of the model, we also report the Recall@k metric assessing the fraction of relevant documents captured within the top-k results. Testing details The tests were performed on a machine with sixteen Intel Xeon E5-2630 cores clocked at 2.40GHz, 192GiB RAM, equipped with a NVIDIA TITAN Xp GPU. The GPU has been used to run the monoBERT and duoBERT models. 7.1 Experimental results We now present the results of our experimental evaluation. To ease the discussion, we start by discussing the evaluation in the binary setting for the retrieval of the top-1 result (Algorithm 1 and its possible implementations). We then present the results of the proposed algorithms on the problem generalizations, i.e., top-k retrieval, probabilistic setting, and parallel setting (Algorithm 2). 7.1.1 Optimal deterministic algorithm As discussed in Section 5.4, Algorithm 1 can be implemented in different ways to take into account two orthogonal aspects: pre-ordering of the input lists and arc lookups memory. While the ﬁrst aspect can be exploited using a linked list in place of an array to exploit the order of the input list when deciding the order of the arc lookups, the second aspect can be faced using an hash table to store all comparisons and avoid multiple unfold of a same arc (refer to Section 5.4 for the implementation details). In Table 1, we report the average number of inferences of the different implementations of Algorithm 1 when applied to duoBERT to retrieve the top-1 result on the MS MARCO dataset. Since our input lists (vertices) are already sorted according to the monoBERT model, which may be an important indicator of relevance, we expect to see a smaller number of comparisons when exploiting the input order. Indeed, as expected, both aspects contribute to reduce the average number of inferences. In particular, we notice that the implementation employing the linked list is more efﬁcient when used together with the hash table, and that their combination nearly halves the number of inferences. In Table 2, we present the performance of this implementation within the ranking pipeline proposed by Nogueira et al.. We report Recall@1, number of inferences and inference time of all ranking stages. The ﬁrst row shows the performance of the ﬁrst two stages of the ranking pipeline, i.e., BM25 + monoBERT, used here to retrieve the top-30 results to re-rank. It retrieves the correct answer for about 25% of the queries but it requires, on average, about 66 seconds when applied to the top-1,000 results returned by BM25. The second row shows the performance of duoBERTwhen employed as third stage of the ranking pipeline. As this model does not guarantee symmetric predictions, each comparison needs two inferences, i.e., a versus b and b versus a; it thus requires 30 ×29 = 870 inferences. duoBERTimproves the quality of the returned list with respect to the previous stage as it retrieves the correct answer for about 27% of the queries. However, we want to highlight that this third stage almost doubles the running time, as it require about 57 seconds, while the combination of ﬁrst plus second stage, i.e., BM25 + monoBERT, require about 66 seconds. The third row of Table 2 shows the performance of duoBERTused as third stage when employing Algorithm 1 to perform the tournament among the top-30 results of each query as provided by the second stage. The recall metric is the same as duoBERT. This result is expected as we proved the correctness of the algorithm. On average, this conﬁguration requires about 4 seconds per query and it speeds up the ranking process of the third stage of about 13× with respect to the previous conﬁguration. Moreover, the time cost of the third stage is now negligible with respect to the one of the second stage. The average number of inferences required by our approach is about 65, which is very close to the minimum number of inferences required to solve this problem when the Champion wins all comparisons, i.e., 29 × 2 = 58 inferences. In particular, 95% of the queries are solved with only 50 comparisons or less, i.e., solved with less than 100 model inferences. In addition, we want to point out that if the underlying model is symmetric, the number of inferences required is competitive with respect to other pointwise approaches, as they cannot perform less than 30 inferences to score all items. 7.1.2 Top-k retrieval and Probabilistic versions In Table 3, we now report the performance of the top-k retrieval performed by Algorithm 1 in the probabilistic settings. As before, we report Recall@k, for k in {1, 2, 3, 4, 5}, number of inferences, and inference time of all ranking stages. The ﬁrst row shows the performance of the ﬁrst two stages of the ranking pipeline introduced by Nogueira et al., i.e., BM25 + monoBERT. The second and fourth rows show the performance of duoBERTand duoBERTwhen employed as third stage of the ranking pipeline. The two conﬁgurations require the same number of inferences, i.e., 30 × 29 = 870, and the same inference time, as the underlying model is the same. The binary conﬁguration shows a slightly higher recall than the probabilistic one, and both of them improve the recall of the Tournament Typek previous ranking stage, thus conﬁrming that tournaments allow for a good modeling of this problem. The third and ﬁfth rows show the performance of these models when employing Algorithm 1 to perform the tournament among the top-30 results of each query. In both cases the recall is preserved, as the algorithm is correct, while the ranking process is speeded up from 13× to 2× for k ranging from 1 to 5. Taking into account that `, i.e., the number of matches lost by the k-th result, drives the time complexity of our algorithm, we report in Table 4 the different values of ` when varying k and the tournament type, i.e., binary or probabilistic. In terms of comparisons, we see in Table 3 that the number of inferences performed by our algorithm increases as k grows. Moreover, we also see that it performs a slightly higher number of inferences in the probabilistic setting than in the binary setting. The ﬁrst consideration follows from the fact that the higher k, the higher `, while the second consideration follows from the fact that, on this dataset, `is lower in the binary setting than in the probabilistic setting. In Table 5, we now show the performance of Algorithm 2 in the parallel setting where the algorithm can unfold a batch of multiple arcs in parallel. Table 5 reports the number of inferences and the inference time of all ranking stages, for values of batch size between 2 and 256 when retrieving the top-1 result on the MS MARCO dataset. The Recall@1 metric is not reported as the correctness of the algorithm guarantees that the effectiveness does not change with the batch size. The ﬁrst row shows the performance of the ﬁrst two stages of the ranking pipeline, i.e., BM25 + monoBERT, while the second row shows the performance of the third stage, i.e., duoBERT. The number of batch inferences decreases when increasing the batch size for both conﬁgurations, as expected. The third row shows the performance of duoBERTused as third stage when employing Algorithm 2 to perform the (batched) tournament among the top-30 results of each query. Our algorithm speeds up the ranking up to 13×, depending on the batch size, and this gap decreases as the batch size increases. This is due to the fact that the number of results involved in the tournament is ﬁxed to 30 while the batch size increases up to 256, thus making the choices of the algorithm less and less impacting. Nevertheless, Algorithm 2 remarkably speeds up the ranking of duoBERTfor all reasonable (w.r.t. the number of results to re-rank) values of batch size. We addressed the problem of how to efﬁciently solve the retrieval of the top-1 result when employing pairwise machine learning classiﬁers. We mapped it to the problem of ﬁnding champions in tournament graphs by minimizing the number of arc lookups, i.e., the number of comparison done through the classiﬁer. We showed that, given the number ` of matches lost by the champion, Ω(`n) arc lookups are required to ﬁnd a champion, and generalized this statement for randomized algorithms that are only correct with some constant probability. Then, we presented an optimal deterministic algorithm that solves the problem and matches the lower bound without knowing `. We also turned our attention to three natural variants of the original problem, and showed algorithms that solve them. First, we solved the problem of ﬁnding all the top-k players simultaneously. Second, we considered a probabilistic tournament in which any cell of the adjacency matrix contains a probability, and achieved the same performance in that more general case. Third, we supposed we were able to probe B adjacency matrix cells in parallel and achieved a linear (and thus optimal) speedup. Finally, we experimentally evaluated the proposed algorithms to speed-up a state-of-the-art solution for ranking on public data. Results show that our proposals achieve important speed-ups ranging from 2× up to 13× in this scenario.