{d.provodin,p.gajane,m.pechenizkiy}@tue.nl, M.C.Kaptein@tilburguniversity.edu Background and motivation. of modern literature on sequential decision making, which aims to determine policies that maximize the expected outcome. These policies are often learned either online (sequentially) (see, e.g., [Li et al., 2010, Jun et al., 2017, Dimakopoulou et al., 2018]) or ofﬂine (statically) (see, e.g., [Swaminathan and Joachims, 2015, Zhou et al., 2018, Joachims et al., 2018, Athey and Wager, 2020]. In online problems, the agent learns through sequential interaction with the environment adjusting the behavior for every single response. In ofﬂine learning on the other hand, the agent learns from a ﬁxed historical data without the possibility to interact with the environment and, therefore, the goal of the agent is to maximally exploit the static data to determine the best policy. However, neither setting provides a close approximation of the underlying reality in many cases. While ofﬂine setting is simply not conducive to sequential learning, applicability of online learning is often curtailed by limitations of practical applications. For example, in recommender systems and ad placement engines, treating users one at a time can become a formidable computational burden; in online marketing and clinical trials, environments designs (campaigns/trials) and the presence of delayed feedback result in treating patients/customers organized into groups. In all of these applied cases, it is infeasible to learn one-by-one due to computational complexity or the impact of delay. Because of the practical restrictions described above, we consider sequential batch learning in bandit problems – sequential interaction with the environment when responses are grouped in batches and observed by the agent only at the end of each batch. Broadly speaking, sequential batch learning is a more generalized way of learning which covers both ofﬂine and online settings as special cases bringing together their advantages. Workshop on the Ecological Theory of Reinforcement Learning at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. Danil Provodin, Pratik Gajane, Mykola Pechenizkiy, Maurits Kaptein Jheronimus Academy of Data Science, ‘s-Hertogenbosch, The Netherlands We consider a special case of bandit problems, namely batched bandits. Motivated by natural restrictions of recommender systems and e-commerce platforms, we assume that a learning agent observes responses batched in groups over a certain time period. Unlike previous work, we consider a more practically relevant batchcentric scenario of batch learning. We provide a policy-agnostic regret analysis and demonstrate upper and lower bounds for the regret of a candidate policy. Our main theoretical results show that the impact of batch learning can be measured in terms of online behavior. Finally, we demonstrate the consistency of theoretical results by conducting empirical experiments and reﬂect on the optimal batch size choice. Unlike ofﬂine learning, sequential batch learning retains the sequential nature of the problem. Unlike online learning, it is often appealing to implement batch learning in large scale bandit problems as In many application domains, batched feedback is an intrinsic characteristic of the problem [Bertsimas and Mersereau, 2007, Chapelle and Li, 2011, Schwartz et al., 2013, Hill et al., 2017]. Although the batch setting is not ubiquitous in the traditional stochastic MAB formulation, there have been several attempts to extend that framework. A more restrictive version of the batch problem, where the agent can choose each arm at most once in a single batch, was studied by Anantharam et al. [1987]. Jun et al. [2016] generalized the batch problem by introducing budget (how many times each arm can be chosen). However, unlike our case, the authors considered the problem of identifying toparms with high probability. The origin of our problem formulation can be traced back to Perchet et al. [2016], who proposed an explicit batched algorithm based on explore-then-commit policy for a two-armed batch bandit problem with a relatively small number of batches and explore its upper and lower regret bounds, giving rise to a rich line of work Gao et al. [2019], Esfandiari et al. [2020], Han et al. [2020]. The problem of batched bandits is also related to learning from delayed or/and aggregated feedback (see, e.g., [Joulani et al., 2013, Vernade et al., 2017, Pike-Burke et al., 2018]), since the decision maker is not able to observe rewards in the interim of a batch. Although these are two similar problems, learning from delayed feedback deals with exogenous delays, whereas feedback delays in sequential batch learning are endogenous and arise as a consequence of the batch constraints [Han et al., 2020]. Unfortunately, a comprehensive understanding of the effects of the batch setting is still missing. Withdrawing the assumptions of online learning that has dominated much of the MAB literature raises fundamental questions as how to benchmark performance of candidate policies, and how one should choose the batch size for a given policy in order to achieve the rate-optimal regret bounds. As a consequence, it is now frequently a case in practice when the batch size is chosen for the purpose of computational accessibility rather than statistical evidence [Chapelle and Li, 2011, Hill et al., 2017]. Moreover, while the asymptotics of batched policies are known (see, e.g., Auer and Ortner [2010], Cesa-Bianchi et al. [2013]), the relatively small horizon performance of batch policies is much less understood while simultaneously being much more practically relevant. Thus, in this work, we make a signiﬁcant step in these directions by providing a systematic study of the sequential batch learning problem. Main contribution. mentioned above. First, we formulate a more practically relevant batch-centric problem. The second dimension of contribution lies in the analysis domain. We provide a reﬁned theoretical analysis and establish upper and lower bounds on the performance for an arbitrary candidate policy. On the modeling side, we demonstrate the validity of the theoretical results by conducting empirical experiments and reﬂect on the choice of the optimal batch size. Lastly, we present directions for future work. We consider a variant of stochastic bandits, which we call sequential batch learning. In this problem, the decision-maker (agent) has to make a sequence of decisions (actions), and for each decision it receives a stochastic reward. environment n, at each time step The goal of the agent is to maximize the total reward the expected reward of action a classical performance measure of the agent – regret, which is the difference between the players’ rewards after n rounds and the best reward possible given the strategy of playing a single arm: We use the same problem formulation for online setting as described in [Lattimore and Szepesvári, 2020]. –it does not require much engineering efforts and resources, as experimental control over the system is not necessary; and –it does not require resources to make the feedback loop shorter in time-delayed bandit problems. ν = (P: a ∈ A)(Pis the distribution of rewards for actiona), and a time horizon . Note that we only require the existence of expectation from distributions Pfor a ∈ A. Throughout the work we assume that drop the dependence on ν in various quantities. A policy is a rule that describes how the actions should be taken in light of the past. Here, the past at time step t > 0 is deﬁned as which is the sequence of action-reward pairs leading up to the state of the process at the previous time step setX. As such, a policy is a ﬁnite sequence actions, formally, t > 0, the distribution of the action will write π In contrast to conventional approaches that require the reward to be observable after each choice of the arm, our setting assumes only that rewards are released after byT = t 1 = t< ... < t thatn = bM the rewards after a batch ends, meaning that the agent operates with the same amount of information within a single batch. For simplicity, we assume that as long as the history remains the same the decision rule does not change as well. Note that this assumption does not impose any signiﬁcant restrictions. Indeed, instead of applying a policy once, one can always do it b times until the history updates. Thus, a batch policy is also a ﬁnite sequence of distributions over actions: for the agent in timestep a property of the environment, we consider this limitation from a policy perspective. With this, we assume that it is not the online agent who works with the batch environment, but the batch policy interacts with the online environment. To distinguish between online and batch policies we will denote the last as π Before proceeding, we will deﬁne a binary relation on a set of policies. We say that the decision expected reward under policy π For building up to the proof of the main result, we also need to require some properties of policy that would distinguish "good" policies from the rest. First, we require that if we have two vectors of conditional variances associated with two different histories, then the decision rule based on history with lower variances (over the best arms) is better than the decision rule based on history with higher variances. Assumption 2.1. H, respectively. If comparison over the elements a t − 1. Note thatH= ∅. LetM(X)be a set of all probability distributions over a ﬁnite isπ(H)(a). Since writingπ(H)(a)is quite cumbersome, we abuse notation and (a|H). Thus, when following a policy π, in time step t we get that , ..., ta grid, which is a division of the time horizonntoMbatches of equal sizeb, , otherwise we can taken :=b. Recall that in the batch setting the agent receives (·|H)is not worse than the decision ruleπ= π(·|H)(and writeπ≥ π) if the π> π). Denote byσ= (σ)the vector of conditional variances of estimated )given the history H: σ= V ar(ˆµ|H), where ˆµ= E(µ|H). Next, we assume that policy regret decreases. Assumption 2.2. Intuitively, if an online policy an online "short" policy could perform better as it omits these suboptimal choices. Indeed, using Assumption 2.2 for horizons andMrespectively and using get bR The following lemma provides properties of a policy that improves over time. Lemma 2.3. Let π = (π Proof.1. First, we need to show that ¯π(a) ≥ 0 for all a ∈ A. Indeed, Finally, The result is completed by rearranging the sums and using the deﬁnition of ¯π (π) > R(π). > ¯π, where ¯π=is an average decision rule; > ¯π∀t such that 1 ≤ t ≤ n, + πis an elementwise addition of two probability vectors for some t, s. (a) ≥ 0 for all a ∈ A and for all 1 ≤ s ≤ t, ¯π(a) ≥ 0 for all a ∈ A. ] = EX= EXI{A= a}= EE[XI{A= a}|A] : n< n. We have>. Expressing the regret by its deﬁnition, one can get >, and hence−> 0. We next provide lower and upper bounds on the best achievable performance. Intuitively, if policy πis "good enough" then it should be better than policy decisions") having the maximum available information about the environment. In some sense, we can consider a batch policy speciﬁcation of "good" policy theorem formalizes the described above intuition. Theorem 3.1. assumptions 2.1 and 2.2 hold. Then, for b > 1, We can consider the term environment by following policy the performance of a batch policy. Indeed, imagine that this is one agent that deliberately repeats each step an online manner. Then, after ﬁrst reward from the previous batch. So, while this policy could perform as an online policy within these repetitions, it pretends that rewards are not observable and acts using the outdated history (just like a batch policy does). For notational simplicity, we denote the online "short" policy as the theorem allows to benchmark any batch policy in terms of its online analogs. Note that there is no any behavioral difference between the original policy π and the "short" policy π First, we analyze the performance of the three policies at the beginning of a batch the online policy not worse than the online "short" policy regrets being related reversely. Then, by Assumption 2.1 we establish that the transition from the end of one batch to the beginning of the next batch retains the relation between policies. Within batch. in timestep batch policy, which is not worse than the decision rule of the online "short" policy, Since the online policy πand online "short" policy by applying Lemma 2.3 we have beginning of a batch with the batch. Between batches. timestep Transiting from timestep updated in this step. To show that the relation between batch policy and its bounds holds, we need to analyze how much information was gained by all three policies, namely, we need to show that We now exploit the fact that policies whatever exploration-exploitation scheme is implemented in policy same scheme but in slower manner. In other words, ones policy policiesπ a slower version of policy Letπbe a batch speciﬁcation of a given policyπandM =. Suppose that btimes instead of immediately updating its beliefs and proceeding to the next round as in Lets consider the timesteptwhich is the beginning of thej-th batch. Assume that tthe decision rule of the online policy is not worse that than the decision rule of the t, correspondingly. Letσ,(σ),(σ)be the corresponding conditional variances. )≤(σ). Then, we can just apply Assumption 2.1. andπreach the same conﬁgurations(σ),(σ)much later in time. Similarly, policyπis 4.2 Proof of Theorem 2 The sketch of the proof works for an arbitrary number of actions K but, to bring it to life, one needs to show that online policy cannot pull as many "bad" arms as batch and "short" online policies do. Below we provide the formal proof for the case Step 1 (Within batch). timesteps By the end of batch j have: for any timestep deﬁnition of batch policy. Thus, starting with ¯π> ¯π Step 2 (Between batches). 1 ≤ l < j t, correspondingly. Let number of times we have received a reward from armP ¯π(a) > ¯π 1 ≤ l < j has been received. By applying Assumption 2.1 we have that π Step 3 (Regret throughout the horizon). policy, batch policy and "short" online policy being equal to each other: π Step 1, by the end of the ﬁrst batch we have Step 2, the transition to the second batch retains the relation between policies: so on. Finally, summing over M = This concludes the proof. Usually, it is deﬁned as a number of times action than receives rewards, we deﬁne it that way. tandtas¯π=; and an average decision rule in batchjas¯π=. t< t < t, where(a)follows from Lemma 2.3 (1); and(b)and(c)hold by the ≥ ¯π. Moreover, by Lemma 2.3 (2), we have π> π≥ π. . LetH,H,Hbe histories collected by policiesπ,π,πby the timestep I{A= a}. Note thatE[T(π, j)] = b · ¯π(a). SinceK = 2,¯π> ¯π≥ ¯πimplies (a) ≥ ¯π(a)for1 ≤ l < j. Hence,E[T(π, l)] > E[T(π, l)] ≥ E[T(π, l)]forPPP and, therefore,E[T(π, l)] >E[T(π, l)] ≥E[T(π, l)]. In such case, we ≤(σ)≤(σ), since the variance is inversely proportional to the number of rewards We perform experiments on two different applications: on simulated stochastic bandit environments; and on a contextual bandit environment, learned from the logged data in an online marketing campaign. We examine the effect of batch learning on Thompson Sampling (TS) and Upper Conﬁdence Bound (UCB) policies for the stochastic problems, and linear Thompson Sampling (LinTS) [Chapelle and Li, 2011] and linear Upper Conﬁdence Bound (LinUCB) [Li et al., 2010] for the contextual problem. Simulated environments. create 3 environments with different mean rewards for each information about vectors of mean rewards used in the environments. Real data. logged dataset. KPN has recently used 3 different campaigns that all aimed to sell an extra broadband subscription to their customers. In the current dataset, a randomly selected set of customers received one of the three campaigns randomly. The data contains a sample of a campaign selection from October 2019 until June 2020 combined with customer information. We adopt an unbiased ofﬂine evaluation method of [Li et al., 2011] to compare various bandit algorithms and batch size values. We use conversion rate (CR) as the metric of interest, which is deﬁned as the ratio between the number of successful interactions and the total number of interactions. To protect business-sensitive information, we only report relative conversion rate, therefore Figure 2 demonstrates the CR returned by the off-policy evaluation algorithm with hidden y-axis values. Results. expected, the regret has an upward trend as batch size increases. It can be seen that, for both algorithms, the impact of batch learning depends on environment parameters: the bigger a suboptimality gap, the stronger the impact of batching. While the exact behavior of the batch learning is still not understood (and it pretty much depends on an algorithm itself rather than the experiment design), we can see a clear difference between a randomised policy and a deterministic one. Indeed, TS turns out to be more robust to the impact of batching, whereas UCB algorithm ﬂuctuates considerably as batch size increases. The results for the real data conﬁrms this fact as well: from Figure 2 we observe that the impact of batching is milder for randomised policy (LinTS) than for deterministic policy (LinUCB) in contextual problem. It is important to note that both experiments demonstrate results consistent with the theoretical analysis conducted in Section 4. As the upper bound in Theorem 3.1 suggests, the performance metric reacts evenly to the increasing/decreasing of the batch size. Thus, the conducted analysis guarantees that the choice of the batch size should be rather based on computational capabilities or other features of the problem. Finally, although it was not an initial goal of the study, we recommend to resort to randomised policies when it becomes necessary to learn in batches. Batch learning is an integral part of any learning system, and the understanding of its impact is of a ﬁrst magnitude. In this study, we have presented a systematic approach for batch learning in stochastic bandits. Formulating the problem from a more practically relevant perspective, we have a dutch telecommunications provider The source code of the experiments can be found in We also consider batch learning in a personalized marketing campaign on the KPN Figure 1 shows the regret of two algorithms in six environments across batch size. As Figure 1: Empirical regret performance by batches. The plots are averaged over 500 repetitions. Figure 2: Empirical conversion rate by batches. The plots are averaged over 20 repetitions. shown the true effect of batch learning by conducting a comprehensive theoretical analysis, which is conﬁrmed by our strong empirical results. Practically speaking, we have investigated one component of the performance-computational cost trade-off and demonstrated that it deteriorates gradually depending on the batch size. Thus, in order to ﬁnd a suitable batch size, practitioners should take the necessary steps to estimate the second component (computational costs) based on computational capabilities. Concerning future work, the ﬁrst obvious step is to extend proof of the Theorem 3.1 to It is also crucial to identify a policy class extend the provided theoretical guarantees to a wider class of MAB problems (e.g., contextual and non-stationary cases). While our formulation focuses on stochastic bandits, our established bounds should extend naturally for contextual problems as well. In fact, the main technical contribution does not rely on any properties of stochastic bandits. Finally, as we mentioned earlier, randomised policies seem to be more robust to the batch learning than deterministic policies. An interesting future direction, therefore, is to provide theoretical comparison of batch learning impact on randomised and deterministic policies separately. We would like to thank the anonymous reviewers for their thoughtful and helpful suggestions and comments. This project is partially ﬁnanced by the Dutch Research Council (NWO) and the ICAI initiative in collaboration with KPN, the Netherlands.