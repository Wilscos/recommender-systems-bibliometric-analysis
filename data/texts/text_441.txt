Measuring Cognitive Status from Speech in a Smart Home Environment Kathleen C. Fraser and Majid Komeili The population is aging, and becoming more tech-savvy. The United Nations predicts that by 2050, one in six people in the world will be over age 65 (up from one in 11 in 2019), and this increases to one in four in Europe and Northern America. Meanwhile, the proportion of American adults over 65 who own a smart phone has risen 24 percentage points from 2013-2017, and the majority have Internet in their homes [1]. Smart devices and smart home technology have profound potential to transform how people age, their ability to live independently in later years, and their interactions with their circle of care. Cognitive health is a key component to independence and well-being in old age, and smart homes present many opportunities to measure cognitive status in a continuous, unobtrusive manner. In this article, we focus on speech as a measurement instrument for cognitive health. Existing methods of cognitive assessment suffer from a number of limitations that could be addressed through smart home speech sensing technologies. We begin with a brief tutorial on measuring cognitive status from speech, including some pointers to useful open-source software toolboxes for the interested reader. We then present an overview of the preliminary results from pilot studies on active and passive smart home speech sensing for the measurement of cognitive health, and conclude with some recommendations and challenge statements for the next wave of work in this area, to help overcome both technical and ethical barriers to success. Declines in cognitive ability can occur as a result of a number of underlying physical factors: traumatic brain injury, stroke, and neurodegenerative diseases such as Alzheimer’s disease (AD), among others. The set of cognitive symptoms associated with neurodegenerative diseases is known as dementia. Since dementia develops slowly over time, there is increasing interest in methods to detect it at the earliest possible stages, when interventions aimed to slow down the decline process are most effective. It is estimated that approximately half of dementia cases go undiagnosed, which can lead to poorer outcomes for both the individual and their caregivers [2]. Part of the challenge lies in the fact that ‘cognition’ is not a physical property to be measured, and all measures of cognition are by necessity indirect. A physician may look at an MRI image and see the patterns of atrophy in the brain, but in many cases such information is only roughly correlated with the actual behaviour and function of the patient, particularly at the early stages. Typically, cognition is measured through standardized ‘cognitive tests’, which target different areas of cognition and are validated against existing population norms. However, as a measurement instrument, these tests suffer from both practical and theoretical limitations: they are administered in a clinic by a trained professional, which is expensive and time-consuming, but also affects the measurement itself, as social anxiety and pressure to do well can potentially lead to poorer performance. Furthermore, the measurements are not generally repeatable, due to a learning effect, making measurement reliability difﬁcult to estimate. At the same time, repeated measurements are highly desirable, as it is known that people with dementia have “good days and bad days,” and cognitive ability can vary from day-to-day. Alternative measurement instruments, particularly those that can be applied outside of the clinic, are needed. Speech and language offer a natural and incredibly rich view into cognitive function. In dementia, various cognitive areas such as memory, executive function, attention, processing speed, and language itself may be affected, and these deﬁcits can all show up in what a person says and how they say it. Therefore, there has been growing interest in automatically detecting the changes in speech and language that indicate underlying cognitive decline. Preliminary lab-based studies have supported the hypothesis that speech carries information about cognitive status. For example, a machine learning classiﬁer trained on linguistic and acoustic features extracted from short speech samples was able to distinguish between participants with and without AD with 81% accuracy [3]. Other work used automatic conversation analysis to differentiate between participants with progressive neurodegenerative dementia and those with functional memory disorders with 90% accuracy [4]. Perhaps even more promising is the use of automated speech analysis to detect mild cognitive impairment (MCI), a period of subtle decline which can occur prior to dementia. Roark et al. distinguished between patients with MCI and healthy older controls with an area under the receiver operating characteristic curve (AUC) of 0.86 based on measures relating to speech rate and pause rate, as well as syntactic measures [5]; similar results have subsequently been obtained by other research groups [6, 7]. We now turn to the practical details involved in speech analysis. Note that the term speech analysis actually encompasses a wide range of lower-level features when we delve into the details of the processing. Figure 1 offers a non-exhaustive summary of the features that can be extracted from speech and linked to cognition. Such features can include low-level acoustic metrics of voice quality and regularity, prosodic features relating to pausing and speech rate, as well as summary features relating to how much time is spent speaking. For example, OpenSMILE tool that can be used to extract 1582 acoustic features from the Interspeech 2010 Paralinguistic Challenge feature set. Audio ﬁles can then be automatically transcribed using automatic speech recognition (ASR) software, e.g. Kaldi, script, a wealth of additional information becomes available: features relating to the complexity and grammaticality of utterances (syntactic features), word repetitions, the variety of different words used and their linguistic properties (semantic features), and even the speaker’s ability to carry on a conversation and handle misunderstandings (pragmatic features) are all indicative of cognitive health. The interested reader is referred to [8] for a more complete description of the commonly used features listed in Figure 1. COVFEFE is an open-source package for extracting approximately two thousand acoustic, prosodic, utterance as well as syntactic, semantic and pragmatic features [8]. This package can be applied arbitrarily to studies that include linguistic data. While individual features could be linked directly to cognitive status, in most current research the features are combined in a machine learning framework. The machine learning models used in the literature vary from simple logistic regression and decision tree classiﬁers, to state-of-theart deep learning models. Such models vary in terms of complexity and interpretability, and the choice depends on a number of factors including how much training data is available, the data representation or number of features, and whether the output should be categorical (classiﬁcation) or continuous (regression). The Python package scikit-learn is one widely-used option for Figure 1: Example features extracted from speech data for the measurement of cognitive status. While a complete description of the features is outside the scope of this article, detailed feature descriptions are available in the literature, e.g., [3, 4, 5, 6, 8, 10]. machine learning. Thus, a typical analysis pipeline would involve recording the speech, generating text from the speech, extracting features from the audio and/or text streams, and then feeding the features to a machine learning model to output an estimate of cognitive status. Evidently, there are numerous sources of error and uncertainty along that pipeline that may propagate through the system. In their discussion on uncertainty in medical measurements, Parvis and Vallan enumerate three distinct categories of medical measurement instruments: (1) those that directly measure some physical quantity of clinical interest; (2) those which convert the extracted quantities into more interpretable features; and (3) those which combine several quantities into a single indicator [9]. Clearly, the measurement of cognition from speech falls into the last and most complex category. Accordingly, there are a number of concerns that arise when we consider the current state-ofthe-art in using speech as a measurement instrument for cognition: • Reference standard: In many cases, researchers consider a brief cognitive screen, such as the Mini-Mental Status Exam (MMSE), to be the reference standard. In addition to the • Repeatability: Ideally, if we use the same instrument to measure the same quantity under • Resolution: As discussed, many studies characterize cognitive status as a binary classiﬁcageneral problems of cognitive testing instruments mentioned above, the MMSE has speciﬁc known issues, including non-linearity as well as ﬂoor and ceiling effects – a highly-educated individual, for example, may report a decline in their own cognition while still scoring within the ‘normal’ range on the MMSE. Therefore, engineering a speech-based instrument which faithfully reproduces the MMSE would represent progress, but could not claim to measure the ‘true’ value of cognition, which we must take to be “not only unknown but also unknowable” [11]. the same conditions, we will receive the same measurement. However, the vast majority of research on speech-based dementia detection considers only a single speech sample per person, making it impossible to assess the stability of the measurement. When uncertainty is discussed, it is typically only in the context of the machine learning algorithm: if some parameter is changed (e.g., the training data in the case of cross-validation), then what effect does it have on the accuracy of the prediction with respect to the reference? This source of variance in the outcome is sometimes reported in terms of conﬁdence intervals or statistical signiﬁcance. However, to our knowledge the following basic experiment has not been conducted: If we took a speech sample from a person today and another sample tomorrow, extracted the same features from each and fed them to the same machine learning model, how closely would the outputs match? This concept of repeatability (or in the medical community, test-retest reliability) is not assessed in single-sample, cross-sectional study designs. tion: participants are labelled as either having dementia, or not. To monitor change over time, it will be necessary to recognize cognition as a continuous measure. Some work has started to move in this direction by re-formulating the classiﬁcation task as a regression task, with the aim of predicting an MMSE score rather than a diagnostic category. While this would certainly represent an improvement in resolution, even MMSE is a relatively coarse scale from the metrology perspective, with only 30 possible values to cover the entire range of human cognitive ability. • Sensitivity: Most studies on speech-based measures of cognition involve participants who Continuous speech sensing in a smart home environment can mitigate some of these issues. While reference to an established clinical instrument such as the MMSE is still necessary, continuous longitudinal sensing makes it possible to use an individual’s own normal functioning as a reference, and detect decline with respect to that baseline – that is, to calibrate the measurement relative to each individual. Furthermore, continuous (or high-frequency) speech recording should make it possible to measure the variance in the signal, develop conﬁdence intervals around the measurement, and detect deviations from that normal range. Finally, if sensors are installed in the homes of cognitively healthy seniors and allowed to operate unobtrusively over time, then in cases where the user does develop dementia, we will have a better understanding of the sensitivity of speech as a measurement of cognitive decline. There have been a number of proposals for how smart home speech sensing might be implemented (see Figure 2). These proposals fall broadly into two categories: active sensing, in which users must perform some action to initiate the recording, and passive sensing, in which the recording occurs unobtrusively, without the user’s intervention. In either case, the encrypted audio data is then sent to an external server for processing and analysis. If a concerning trend is detected, the user’s healthcare provider is notiﬁed for further assessment and possible intervention. have already been diagnosed with dementia, due to the difﬁculties in identifying and recruiting study participants who will develop cognitive impairment over the course of a timelimited study period. However, this limits our understanding of how early in the disease trajectory we can actually pick up on changes. Computational studies of speech from public ﬁgures, such as politicians and professional athletes [12], suggest that the changes may be detectable from speech prior to formal diagnosis, but this work has yet to be extended to a large sample of ordinary citizens. Figure 2: A number of audio sensing options for smart homes have been proposed. Here, arrows show the ﬂow of information (raw data, results of automatic processing, physician-veriﬁed health information) through the system. There has been growing interest in providing mobile cognitive assessments in the form of apps that can be deployed on a tablet or smart phone. Many of these assessments are simply digitized versions of traditional pen-and-paper tests, and are designed to be administered by a health professional, rather than independently in the home. Here we focus instead on a few examples of remote, repeated, home-based assessments where speech was a primary outcome measure. Jeffrey Kaye at the Oregon Health and Science University proposed that cognitive tests displayed on a home computer screen, recorded over the microphone, and scored using ASR would greatly improve outcome assessment in clinical trials for AD medications, by allowing assessments to occur frequently, over a longer period of time, in a manner convenient to participants [13]. A test system was deployed, along with wireless passive infrared motion sensors, in the homes of 265 older participants for an average of 33 months [14]. As part of the protocol, participants took part in daily conversations with research assistants over the computer webcam. Subsequent analyses of the conversations were able to detect MCI based on vocabulary richness measures with AUC = 0.71 [15], and with an accuracy of 84% based on the psycholinguistic properties of the words used [16]. In a similar vein, the CAL: Cognitive Assisted Living smart home architecture [17], and the Lilly Exploratory Digital Assessment Study of smart device sensors [18] both incorporated regularly-scheduled, audio-recorded interviews for the purposes of detecting cognitive status through speech. A potential drawback to these types of active assessments is that they place a burden on the participant to complete, which over time may lead to boredom, apathy, or non-compliance. Depending on the nature of the tasks, they may also suffer from a learning effect, much like traditional cognitive testing. They also assume some level of technological ﬂuency, for the user to log on to the device and complete the procedure [19]. Passive sensing frameworks have the beneﬁt of collecting speech data unobtrusively over long periods of time. However, they also generate more technical challenges, as well as ethical issues surrounding privacy and consent, which will be discussed in more detail below. First, we examine some of the proposed passive sensing technologies. Smartphone conversation recording: As an increasing proportion of the population owns a smart phone, it is a natural choice for recording health data, including variables such as movement, location, heart rate, respiration, and of course voice. In [20], conversations were recorded between participants with early dementia and a family caregiver, and acoustic and linguistic processing was used to identify the speakers with dementia with 92% accuracy. While the conversations were actually recorded live, the author proposed that a similar methodology could be used to analyze phone conversations with family members [20]. A group from IBM Research recorded regular phone conversations between elderly individuals and a monitoring service who called to check in on their clients at home [21]. They found that the participants with dementia repeated themselves more from day-to-day than those without dementia. One might suspect that users would object to the idea of their phone calls being recorded and analyzed; however, it appears that even just analyzing the frequency of conversation, without recording the audio itself, is correlated with some measures of mental well-being [22]. Wearable devices: Wearable recording devices, as a category distinct from smart phones, have been considered in a few research studies. A group at Cornell used a mobile sensing device to capture both physical activity and audio data from older adults in a continuing care retirement community. Acoustic features were extracted from the speech signal without actually recording the raw data, to protect privacy. The proportion of time spent in conversation correlated with mental health scales for social functioning, as well as correlating negatively with a depression scale [23]. In another study, researchers used a wearable electronically-activated recorder to intermittently sample audio snippets throughout the day, and found that markers of complex, analytic, and speciﬁc language correlated with measures of executive function and working memory [24]. Smart home virtual assistants: One exciting possibility for speech sensing in the home is the use of virtual assistants. Virtual assistants (such as Google Home and Amazon Alexa) provide a natural interface to smart home features and assistive technologies, with older adults ﬁnding voice interfaces more user-friendly than touch-based tablet interfaces [25]. Technological advances in natural language processing and machine learning are making virtual assistants more capable. This enables a richer conversation between user and the virtual assistant which in turn could provide a rich source of information for long-term study of cognitive status in a smart home environment. Since these spoken interactions would be analyzed in any case for the purposes of question-answering and information retrieval, it would require minimal additional processing to extract features relevant to cognitive status. With work in this area ongoing, preliminary results have shown promise: researchers found statistically signiﬁcant correlations between MMSE scores and features measuring pauses, hesitations, and error-handling when older adults interacted with virtual assistants [25]; achieved 79% accuracy in predicting future car accidents (within 1.5 years) based on pauses, ﬁlled pauses, and pronoun use in conversations with virtual assistants [26]; and found that people with MCI pause more often and for longer, and produce fewer words and shorter speech chunks when interacting with virtual assistants than do cognitively healthy users [27]. In additional to the existing commercial offerings, another type of digital assistant may soon be available: personal care robots. Often proposed as an assistive technology to allow older adults to live in the community longer, voice-controlled robots could also record and analyze the user’s voice for signs of cognitive decline. In one study, a conversational robot was found to be generally well-liked and engaging by participants with AD, and a number of the features extracted from the dialogues distinguished between persons with mild, moderate, and severe dementia [28]. Ambient embedded sensors: Perhaps the most comprehensive speech sensing methodology is to embed microphones throughout the smart home and record everything that occurs. Embedded microphone arrays have been previously proposed to help detect falls and to recognize cries for help. The SWEET-HOME project in France focused intensively on bringing audio technology to smart homes for older people, and built a home lab with 150 sensors and actuators, including multiple omnidirectional microphones. They found that audio recording was much more acceptable to older users than video recording [29]. The focus of that and other audio-based smart home projects has typically been on providing assistive technologies, not assessing cognitive state, though there is clearly potential for further research. For example, it has been proposed that ambient assistive living technologies for persons with dementia could additionally report some behavioural metrics to the care team, including instances of repetitive speech [30]. However, privacy preservation is paramount in the extensive recording scenarios discussed in this section. Thus far, we have focused on the beneﬁts of smart home speech recording for cognitive monitoring: it can be continuous, unobtrusive, real-time, longitudinal, inclusive to those with mobility issues or who live in remote areas, can avoid unreliable self-reported data, and is potentially more sensitive to decline than traditional cognitive tests [2, 13, 22, 31]. However, there are a number of serious challenges which also must be addressed, both on the technical side and from the ethical perspective. These challenges are summarized in Figure 3 and further detailed in the following Figure 3: Some of the advantages and challenges associated with smart home speech-based measurement of cognitive status. sections. There are technical challenges associated with continuously recording, transmitting, and analyzing audio data that may not arise or be addressed in lab-based feasibility studies. The recording device itself obviously a critical decision. As Shirmohammadi et al. [32] point out in their review on biomedical instrumentation and measurement, the primary question in homebased biomarker measurement is: “How can we safeguard that the clinical quality of measurements is achieved using the low cost measurement equipment in a home environment?” For microphones, the sensitivity, frequency response, and sampling rate will all affect the quality of the audio data. For recording the data, sensor location relative to the speaker is also important. If using a mobile or wearable device, is the microphone obscured by clothing? If using a virtual assistant or embedded microphones, the distance between speaker and sensor will be constantly changing, and in the case of a robotic assistant, the acoustic properties of the environment will also vary, leading to complications when processing the data. Once the data is recorded, it must be transmitted, typically to an encrypted server for processing. Some on-device processing may be possible, but in most cases processing and storage will occur on an external server. One advantage to home-based systems is that they can be connected to wired internet and a continuous power source, avoiding issues due to lost wiﬁ connection or drained batteries. However, any time data is transmitted there is some vulnerability to security threats. Issues of data security, integrity, conﬁdentiality, and authentication must be considered. Secure storage infrastructure is also an issue, as well as the question of how long the audio data must be conserved, and at what resolution. A strategy to protect data against corruption or loss will be crucial for longitudinal analyses spanning many years. In terms of processing, the details depend on the exact audio features under consideration, but some common challenges remain. • Background subtraction: removing irrelevant ambient noise from the audio stream. Back- • Sound classiﬁcation: speech segments must be distinguished from non-speech segments, • Speaker identiﬁcation: the target speaker’s voice must be segmented from those of visitors • Automatic speech recognition (ASR): open-vocabulary ASR of elderly voices in real-world • Feature extraction: as seen in Figure 1, a large number of features may be extracted from • Output: machine learning and/or time series analysis is used to combine speech features and ground speech data, such as from a television or radio, can pose a particular challenge [23]. for example using pitch detection. or other household members. environments has proven to be a challenge. In research studies, the data is often manually transcribed, with the expectation that advances in ASR will be forthcoming [15, 16, 21]. Other approaches avoid ASR altogether and focus only on acoustic and prosodic cues [23]. the data. The exact feature set may depend on factors such as efﬁciency of processing, nonredundancy of information, and whether the results need to be computed in real-time. output a measure of cognitive status, as well as monitor changes over time. To date, binary classiﬁers have often been employed, but more sophisticated approaches are needed. The length of the speech samples may be highly variable and they might have been obtained at Finally, any concerning events that are detected (e.g., a sudden or gradual decline in cognitive ability) must be appropriately communicated to either the user or, preferably, their healthcare provider. Sabbagh et al. [19] refer to this process as “integration into the clinical care pathway.” It is of little use, and in fact may be psychologically harmful, if a decline is detected and communicated to the user, but not followed up on by a trained professional. Numerous ethical concerns may be raised by smart home speech recording; the most commonlyraised concern is privacy [33]. People have a right to personal privacy, particularly in their own homes, and systems such as those described above can lead to psychological discomfort, intrusiveness, and the feeling of “always being watched” [34]. There are also questions of data privacy, such as who can access the data [33] and who sees the results of the analysis [19]. Some of these concerns may be alleviated by technological solutions, such as those described in relation to a privacy-by-design personal assistant for seniors [35]. Typically, the current approach has been to transmit raw speech samples and process them on a remote server. While this helps with safeguarding the stored data and simplifying the client side of the system, privacy concerns may be allayed if only the extracted features are transmitted to the remote server for generating the desired output. The raw speech data will be discarded after features are extracted locally. Another concern is that of informed consent. Obviously, it would be inappropriate to install an audio surveillance system in someone’s house without their knowledge, but the notion of informed consent becomes trickier when working with users who may have, or develop, dementia. Particularly with the most unobtrusive recording architectures, an individual may forget, over time, to what they have consented [34]. There is also the issue of visitors to the home: users have indicated that they prefer such a system to be hidden from visitors [33], but at the same time there is an obligation to disclose to visitors that they may be recorded [34]. One possible solution to this is using voice detection to not record, or immediately delete, recordings from non-consented speakers [31]. irregular time intervals. Appropriate machine learning methods are needed to handle such irregular longitudinal data. Concerns have also been raised that using the voice as a digital biomarker may have negative social justice implications. It is well-known that many AI technologies, including speech recognition, perform signiﬁcantly worse for speakers of languages other than English, people who speak English with an accent, or speakers of African American English (AAE) [36]. Furthermore, installing and maintaining smart home technology can be expensive and therefore not accessible to all. It is important that such technology not perpetuate existing racial and socioeconomic inequities in healthcare access [36]. There is great potential for speech-based measurement of cognitive status. Many researchers have reported high accuracy in distinguishing between speakers with and without dementia, in crosssectional studies performed under lab conditions. Some, but fewer, studies have reported promising results when recording speech in-situ, through video chat, wearable devices, and virtual assistants. However, many gaps remain before we can consider this technology fully validated. Many of the studies described above employed at least some manual processing, either via Wizard-of-Oz dialogues with participants, manual audio segmentation, or manual transcription. A fully automated, end-to-end architecture must be developed, deployed, and evaluated. As previously mentioned, a thorough accounting of the measurement uncertainty at each stage in the pipeline must be performed, as well as an analysis of how those uncertainties propagate through the pipeline [9]. None of the articles surveyed tracked participants long enough to observe individual trajectories of cognitive change. The ability to do is one of the purported beneﬁts of a home-based system, but it has not yet been achieved in reality, perhaps due to unaddressed privacy concerns. Of course, it makes sense to ﬁrst design and validate a prototype system over short time spans, but it is our hope to see more long-term, longitudinal investigations of privacy-preserving speech technology in the home. Such a study design would also answer the question of repeatability, or test-retest reliability. Related to this issue, many of the one-shot, cross-sectional research studies on speech analysis actually use elicited speech protocols (e.g., describe a picture, or recall a story). These protocols induce a highly-speciﬁc structure and content in the narrative speech. In contrast, most smart home studies measure conversational speech, which is more natural and presumably more repeatable. However, it is not clear whether the same amount or kind of information is available in conversation, as compared to the more targeted elicitation tasks. Most studies consider speech as an independent data stream [22]. Again, this is reasonable ﬁrst step to assess the advantages and disadvantages of speech as a measurement instrument for cognition, but the true value in a smart home environment will likely lie in multimodal data streams coming together to form a complete assessment of health and well-being. For example, combining speech analysis with other smart home measures of cognition [37] and physiological health [38], in addition to measures of functional ability such as activities of daily living [39] and gait analysis [40], will almost certainly give a more accurate and robust measurement of physical, mental, and cognitive health. Related to this, most studies based on speech have looked into a single health condition; a holistic approach that simultaneously assesses multiple health conditions may help justify privacy concerns and facilitates integration of such a system within a smart home environment. Finally, there is a pressing need for validation with more diverse user populations. In additional to clinical diversity (i.e., different types of dementia and related pathologies, brain injuries, comorbid mental health issues, etc.), it is important that these technologies are validated with users representing different cultural and educational backgrounds, varieties of English, and varied living situations. This will ensure the validity and reproducibility of the instrument on a heterogeneous user base. Smart home technologies offer the potential to change the way we live throughout our lives. Speech, as a natural and ubiquitous human behavior, will undoubtedly be incorporated into such environments – and indeed, already is, through our use of voice commands and virtual assistants. As a measurement instrument of cognitive status, speech presents many opportunities, but also many open questions to be answered in future research. It will be crucial to bring together multi-disciplinary teams to fully understand and serve the needs of older adults as they face the challenges of cognitive decline. Nelson et al. [31] recommend recruiting expertise from disciplines as varied as “behavioral science, affective computing, mobile sensing, mental health, signal processing, artiﬁcial intelligence, biomedical engineering, data mining, computer networks, machine learning, bioethics, and technology development, along with industry partners.” In this article we have emphasized the need for the thoughtful application of the principles of measurement to the assessment of such automated systems, to protect and promote the health and well-being of our aging population.