Point-of-interest (POI) type prediction is the task of inferring the type of a place from where a social media post was shared. Inferring a POI’s type is useful for studies in computational social science including sociolinguistics, geosemiotics, and cultural geography, and has applications in geosocial networking technologies such as recommendation and visualization systems. Prior efforts in POI type prediction focus solely on text, without taking visual information into account. However in reality, the variety of modalities, as well as their semiotic relationships with one another, shape communication and interactions in social media. This paper presents a study on POI type prediction using multimodal information from text and images available at posting time. For that purpose, we enrich a currently available data set for POI type prediction with the images that accompany the text messages. Our proposed method extracts relevant information from each modality to effectively capture interactions between text and image achieving a macro F1 of 47.21 across eight categories signiﬁcantly outperforming the state-of-the-art method for POI type prediction based on textonly methods. Finally, we provide a detailed analysis to shed light on cross-modal interactions and the limitations of our best performing model. A place is typically described as a physical space infused with human meaning and experiences that facilitate communication (Tuan, 1977). The multimodal content of social media posts (e.g. text, images, emojis) generated by users from speciﬁc places such as restaurants, shops, and parks, contribute to shaping a place’s identity, by offering information about feelings elicited by participating Figure 1: Example of text and image content of sample tweets. Users share content that is relevant to their experiences and feelings in the location. in an activity or living an experience in that place (Tanasescu et al., 2013). Fig. 1 shows examples of Twitter posts consisting of image-text pairs, shared from two different places or Point-of-Interests (POIs). Users share content that is relevant to their experience in the location. For example, the text imagine all the people sharing all the world which is accompanied by a photograph of the Imagine Mosaic in Central Park; and the text Next stop: NYC along with a picture of descriptive items that people carry at an airport such as luggage, a camera and a takeaway coffee cup. Developing computational methods to infer the type of a POI from social media posts (Liu et al., 2012; Sánchez Villegas et al., 2020) is useful for complementing studies in computational social science including sociolinguistics, geosemiotics, and cultural geography (Kress et al., 1996; Scollon and Scollon, 2003; Al Zydjaly, 2014), and has applications in geosocial networking technologies such as recommendation and visualization systems (Alazzawi et al., 2012; Zhang and Cheng, 2018; van Weerdenburg et al., 2019; Liu et al., 2020b). Previous work in natural language processing (NLP) has investigated the language that people use in social media from different locations, by inferring the type of a POI of a given social media post using only text and posting time, ignoring the visual context (Sánchez Villegas et al., 2020). However, communication and interactions in social media are naturally shaped by the variety of available modalities and their semiotic relationships (i.e. how meaning is created and communicated) with one another (Georgakopoulou and Spilioti, 2015; Kruk et al., 2019; Vempala and Preo¸tiuc-Pietro, 2019). In this paper, we propose POI type prediction using multimodal content available at posting time by taking into account textual and visual information. Our contributions are as follows: •We enrich a publicly available data set of social media posts and POI types with images; •We propose a multimodal model that combines text and images in two levels using: (i) a modality gate to control the amount of information needed from the text and image; (ii) a cross-attention mechanism to learn crossmodal interactions. Our model signiﬁcantly outperforms the best state-of-the-art method proposed by Sánchez Villegas et al. (2020); •We provide an in-depth analysis to uncover the limitations of our model and uncover cross-modal characteristics of POI types. POIs have been studied to classify functional regions (e.g. residential, business, and transportation areas) and to analyze activity patterns using social media check-in data and geo-referenced images (Zhi et al., 2016; Liu et al., 2020a; Zhou et al., 2020a; Zhang et al., 2020). Zhou et al. (2020a) presents a model for classifying POI function types (e.g. bank, entertainment, culture) using POI names and a list of results produced by searching for the POI name in a web search engine. Zhang et al. (2020) makes use of social media check-ins and street-level images to compare the different activity patterns of visitors and locals, and uncover inconspicuous but interesting places for them in a city. A framework for extracting emotions (e.g. joy, happiness) from photos taken at various locations in social media is described in Kang et al. (2019). POI type prediction is related to geolocation prediction of social media posts that has been widely studied in NLP (Eisenstein et al., 2010; Roller et al., 2012; Dredze et al., 2016). However, while geolocation prediction aims to infer the exact geographical location of a post using language variation and geographical cues, POI type prediction is focused on identifying the characteristics associated with each type of place, regardless of its geographic location. Previous work on POI type prediction from social media content has used Twitter posts (text and posting time), to identify the POI type from where a post was sent from (Liu et al., 2012; Sánchez Villegas et al., 2020). Liu et al. (2012) incorporate text, temporal features (posting hour) and user history information into probabilistic text classiﬁcation models. Rather than a user-based study, our research aims to uncover the characteristics associated with various types of POIs. Sánchez Villegas et al. (2020) analyze semantic place information of different types of POIs by using text and temporal information (hour, and day of the week) of a Twitter’s post. To the best of our knowledge, this is the ﬁrst study to combine textual and visual features to classify POI types (e.g. arts & entertainment, nightlife spot) from social media messages, regardless of its geographic location. The combination of text and images of social media posts has been largely used for different applications such as sentiment analysis, (Nguyen and Shirai, 2015; Chambers et al., 2015), sarcasm detection (Cai et al., 2019) and text-image relation classiﬁcation (Vempala and Preo¸tiuc-Pietro, 2019; Kruk et al., 2019). Moon et al. (2018b) propose a model for recognizing named entities from short social media texts using image and text. Cai et al. (2019) use a hierarchical fusion model to integrate image and text context with an attention-based fusion. Chinnappa et al. (2019) examine the possession relationships from text-image pairs in social media posts. Wang et al. (2020) use texts and images for predicting the keyphrases (i.e. representative terms) for a post by aligning and capturing the cross-modal interactions via cross-attention. Previous text-image classiﬁcation in social media requires that the data is fully paired, i.e. every post Table 1: POI categories and data set statistics showing the number of tweets for each category, and number (%) of tweets having an accompanying image contains an image and a text. However, this requirement may not be satisﬁed since not all posts contain both modalities. This work considers both cases, (1) all modalities (text-image pairs) are available, and content in only one modality (text or image) is available. Social media analysis research has also looked at the semiotic properties of text-image pairs in posts (Alikhani et al., 2019; Vempala and Preo¸tiucPietro, 2019; Kruk et al., 2019). Vempala and Preo¸tiuc-Pietro (2019) investigate the relationship between text and image content by identifying overlapping meaning in both modalities, those where one modality contributes with additional details, and cases where each modality contributes with different information. Kruk et al. (2019) analyze the relationship between the text-image pairs and ﬁnd that when the image and caption diverge semiotically, the beneﬁt from multimodal modeling is greater. Sánchez Villegas et al. (2020) deﬁne POI type prediction as a multi-class classiﬁcation task where given the text content of a post, the goal is to classify it in one of theMPOI categories. In this work, we extend this task deﬁnition to include images in order to capture the semiotic relationships between the two modalities. For that purpose, we consider a social media postP(e.g. tweet) to comprise of a text and image pair(x, x), wherex∈ R andx∈ Rare the textual and visual vector representations respectively. 3.1 POI Data We use the data set introduced by Sánchez Villegas et al. (2020) which contains196, 235tweets written in English, labeled with one out of the eight POI broad type categories shown in Table 1, which correspond to the 8 primary top-level POI categories in ‘Places by Foursquare’, a database of over 105 million POIs worldwide managed by Foursquare. To generalize to locations not present in the training set, we use the same location-level data splits (train, dev, test) as in Sánchez Villegas et al. (2020), where each split contains tweets from different locations. 3.2 Image Collection We use the Twitter API to collect the images that accompany each textual post in the data set. For the tweets that have more than one image, we select the ﬁrst available only. This results in91, 224 tweets with at least one image. During the image processing (see Section 5.3) we removed 129 images because we found they were either damaged, absent, or no objects were detected, resulting in 91, 095text-image pairs (see Table 1 for data statistics). In order to deal with the rest of the tweets with no associated image, we pair them with a single ‘average’ image computed over all images in the train set:x= avg(x). The intuition behind this approach is to generate a ‘noisy’ image that is not related and does not add to the meaning (Vempala and Preo¸tiuc-Pietro, 2019). 3.3 Exploratory Analysis of Image Data To shed light on the characteristics of the collected images, we apply object detection on the images Table 2: Most common objects for each POI category. collected using Faster-RCNN (Ren et al., 2016) pretrained on Visual Genome (Krishna et al., 2017; Anderson et al., 2018). Table 2 shows the most common objects for each speciﬁc category. We observe that most objects are related to items one would ﬁnd in each place category (e.g. ‘spoon’, ‘meat’, ‘knife’ in Food). Clothing items are common across category types (e.g. ‘shirt’, ‘jacket’, ‘pants’) suggesting the presence of people in the images. A common object tag of the Shop & Service category is ‘letters’, which concerns images that contain embedded text. Finally, the category Great Outdoors includes object tags such as ‘cloud’, ‘hill’, and ‘grass’, words that describe the landscape of this type of place. 4.1 Text and Image Representation Given a text-image postP = (x, x),x∈ R, x∈ R, we ﬁrst compute text and image encoding vectors f, frespectively. TextWe use Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) to obtain the text feature representationsf by extracting the ‘classiﬁcation’ [CLS] token. ImageFor encoding the images, we use Xception (Chollet, 2017) pre-trained on ImageNet (Deng et al., 2009).We extract convolutional feature maps for each image and we apply average pooling to obtain the image representation f. 4.2 MM-Gate Given the complex semiotic relationship between text and image, we need a weighting strategy that assigns more importance to the most relevant modality while suppressing irrelevant information. Thus, a ﬁrst approach is to use gated multimodal fusion (MM-Gate), similar to the approach proposed by Arevalo et al. (2020) to control the contribution of text and image to the POI type prediction. Given f,fthe text and visual vectors, we obtain the multimodal representationhof a postPas follows: are learnable parameters, tanh is the activation function andh, h∈ Rare projections offandf. [; ]denotes concatenation andσis the sigmoid activation function.his a weighted combination of the textual and visual informationhandhrespectively. We ﬁne-tune the entire model by adding a classiﬁcation layer with a softmax activation function for POI type prediction 4.3 MM-XAtt The MM-Gate model does not capture interactions between text and image that might be beneﬁcial for learning semiotic relationships. To model crossmodal interactions, we adapt the cross-attention mechanism (Tsai et al., 2019; Tan and Bansal, 2019) to combine text and image information for multimodal POI type prediction (MM-XAtt). Cross-attention consists of two attention layers, one from textualfto visual featuresfand one from visual to textual features. We ﬁrst linearly project the text and visual representations to obtain the same dimensionality (d). Then, we compute the scaled dot attention (a = sof tmax√V) with the projected textual vector as query (Q), and the projected image vector as the key (K) and values (V), and vice versa. The multimodal representationhis the sum of the resulting attention layers. The entire model is ﬁne-tuned by adding a classiﬁcation layer with a softmax activation function. 4.4 MM-Gated-XAtt Vempala and Preo¸tiuc-Pietro (2019) have demonstrated that the relationship between the text and Figure 2: Overview of our MM-Gated-XAtt model which combines features from text and image modalities for POI type prediction. image in a social media post is complex. Images may or may not add meaning to the post and the text content (or meaning) may or may not correspond to the image. We hypothesize that this might actually happen in posts made from particular locations, i.e. language and visual information may or may not be related. To address this, we propose (1) using gated multimodal fusion to manage the ﬂow of information from each modality, and (2) also learn cross-modal interactions by using cross-attention on top of the gated multimodal mechanism. Fig. 2 shows an overview of our model architecture (MM-Gated-XAtt). Given the text and image representationsf,frespectively, we computeh,h, andzas in Equation 1, 2 and 3. Next, we apply cross-attention using two attention layers where the query and context vectors are the weighted representations of the text and visual modalities,z ∗ h and(1 − z) ∗ h, and vice versa. The multimodal context vectorhis the sum of the resulting attention layers. Finally, we ﬁne-tune the model by passinghthrough a classiﬁcation layer for POI type prediction with a softmax activation function. We compare our models against (1) text-only; (2) image-only; and (3) other state-of-the-art multimodal approaches. Text-onlyWe ﬁne-tune BERT for POI type classiﬁcation by adding a classiﬁcation layer with softmax activation function on top of the [CLS] token which is the best performing model in Sánchez Villegas et al. (2020). Image-onlyWe ﬁne-tune three pre-trained models that are popular in various computer vision classiﬁcation tasks: (1) ResNet101 (He et al., 2016); (2) EfﬁcientNet (Tan and Le, 2019); and (3) Xception (Chollet, 2017). Each model is ﬁne-tuned on POI type classiﬁcation by adding an output softmax layer. Text and ImageFor combining text and image information, we experiment with different standard fusion strategies: (1) we project the image representationf, to the same dimensionality as f∈ Rusing a linear layer and then we concatenate the vectors (Concat); (2) we project the textual and visual features to the same space and then we apply self-attention to learn weights for each modality (Attention); (3) we also adapt the guided attention introduced by Anderson et al. (2018) for learning attention weights at the objectlevel (and other salient regions) rather than equally sized grid-regions (Guided Attention); (4) we compare againstLXMERT, a transformer-based model that has been pre-trained on text and image pairs for learning cross-modality interactions (Tan and Bansal, 2019). All models are ﬁne-tuned by adding a classiﬁcation layer with a softmax activation function for POI type prediction. Finally, we evaluate a simple ensemble strategy by using LXMERT for classifying tweets that are originally accompanied by an image and BERT for classifying text-only tweets (Ensemble). We use the same tokenization settings as in Sánchez Villegas et al. (2020). For each tweet, we lowercase text and replace URLs and @-mentions of users with placeholder tokens. Each image is resized to (224 × 224) pixels representing a value for the red, green and blue color in the range of[0, 255]. The pixel values of all images are normalized. For LXMERT and Guided Attention fusion, we extract object-level features using Faster-RCNN (Ren et al., 2016) pretrained Table 3: Macro F1-Score, precision (P) and recall (R) for POI type prediction (± std. dev.) Best results are in bold. † indicates statistically signiﬁcant improvement (t-test, p < 0.05) over BERT (Sánchez Villegas et al., 2020). on Visual Genome (Krishna et al., 2017) following Anderson et al. (2018). We keep 36 objects for each image as in Tan and Bansal (2019). We select the hyperparameters for all models using early stopping by monitoring the validation loss using the Adam optimizer (Kingma and Ba, 2014). Because the data is imbalanced, we estimate the class weights using the ‘balanced’ heuristic (King and Zeng, 2001). All experiments are performed using a Nvidia V100 GPU. Text-onlyWe ﬁne-tune BERT for 20 epochs and choose the epoch with the lowest validation loss. We use the pre-trained base-uncased model for BERT (Vaswani et al., 2017; Devlin et al., 2019) from HuggingFace library (12-layer, 768dimensional) with a maximal sequence length of 50 tokens. We ﬁne-tune BERT for 2 epochs and learning rate η = 2ewith η ∈ {2e, 3e, 5e}. Image-onlyFor ResNet101, we ﬁne-tune for 5 epochs with learning rateη = 1eand dropout δ = 0.2(δin[0, 0.5]using random search) before passing the image representation through the classiﬁcation layer. EfﬁcientNet is ﬁne-tuned for 7 epochs withη = 1eandδ = 0.5. Xception is ﬁne-tuned for 6 epochs withη = 1eand δ = 0.5. Concat-BERT+ResNet and Guided Attention- BERT+Xception are ﬁne-tuned for 2 epochs withη = 1eandδ = 0.25; ConcatBERT+EfﬁcientNet for 4 epochs withη = 1e andδ = 0.25; Attention-BERT+Xception for 3 epochs withη = 1eandδ = 0.25; MM-XAtt for 3 epochs withη = 1eandδ = 0.15; MM-Gate and MM-Gated-XAtt for 2 epochs with η = 1eandδ = 0.05;η ∈ {2e, 3e, 5e}, δfrom[0, 0.5](random search) before passing through the classiﬁcation layer. The dimensionality of the multimodal representationh(Eq. 4) is set to 200. We ﬁne-tune LXMERT for 4 epochs with η = 1ewhereη ∈ {1e, 1e, 1e}and dropoutδ = 0.25(δin[0, 0.5], random search) before passing through the classiﬁcation layer. We evaluate the performance of all models using macro F1, precision, and recall. Results are obtained over three runs using different random seeds reporting the average and the standard deviation. The results of POI type prediction are presented in Table 3. We ﬁrst examine the impact of each modality by analyzing the performance of the unimodal models, then we investigate the effect of multimodal methods for POI type prediction, and ﬁnally we examine the performance of our proposed model MM-Gated-XAtt by analyzing each component independently. Table 4: Macro F1-Score for POI type prediction on tweets that are originally accompanied by an image. Best results are in bold. We observe that the text-only model (BERT) achieves 43.67 F1 which is substantially higher than the performance of image-only models (e.g. the best performing EfﬁcientNet model obtains 24.72 F1). This suggests that text encapsulates more relevant information for this task than images on their own, similar to other studies in multimodal computational social science (Wang et al., 2020; Ma et al., 2021). Models that simply concatenate text and image vectors have close performance to BERT (44.0 for Concat-BERT+Xception) or lower (41.56for Concat-BERT+EfﬁcientNet). This suggests that assigning equal importance to text and image information can deteriorate performance. It also shows that modeling cross-modal interactions is necessary to boost performance of POI type classiﬁcation models. Surprisingly, we observe that the pre-trained multimodal LXMERT fails to improve over BERT (40.17 F1) while its performance is lower than simpler concatenative fusion models. We speculate that this is because LXMERT is pretrained on data where both, text and image modalities share common semantic relationships which is the case in standard vision-language tasks including image captioning and visual question answering (Zhou et al., 2020b; Lu et al., 2019). On the other hand, text-image relationships in social media data for inferring the type of location from which a message was sent are more diverse, highlighting the particular challenges for modeling text and images together (Hessel and Lee, 2020). Our proposed MM-Gated-XAtt model achieves 47.21F1 which signiﬁcantly (t-test,p < 0.05) improves over BERT, the best performing model in Sánchez Villegas et al. (2020) and consistently outperforms all other image-only and multimodal approaches. This conﬁrms our main hypothesis that modeling text with image jointly to learn the interactions between modalities beneﬁt performance in POI type prediction. We also observe that using only the gating mechanism (MM-Gate) outperforms (44.64 F1) all other models except for MMGated-XAtt. This highlights the importance of controlling the information ﬂow for the two modalities. Using cross-attention on its own (MM-XAtt), on the other hand, fails to improve over other multimodal approaches, implying that learning crossmodal interactions is not sufﬁcient on its own. This supports our hypothesis that language and visual information in posts sent from speciﬁc locations may be or may not be related, and that managing the ﬂow of information from each modality improves the classiﬁer’s performance. Finally, we investigate using less noisy textimage pairs in alignment with related computational social science studies involving text and images (Moon et al., 2018b; Cai et al., 2019; Chinnappa et al., 2019). We train and test LXMERT, MM-Gate, MM-XAtt, and MM-Gated-XAtt on tweets that are originally accompanied by an image (see Section 3), excluding all text-only tweets. The results are shown in Table 4. In general, performance is higher for all models using less noisy data. Our proposed model MM-Gated-XAtt consistently achieves the best performance (57.64F1). In addition, we observe that LXMERT and MMXAtt produce similar results (47.72and48.93F1 respectively) suggesting that cross-attention can be applied directly to text-image pairs in low-noise settings without hurting the model performance. The beneﬁt of controlling the ﬂow of information through a gating mechanism, on the other hand, strongly improves model robustness. To compare the effect of the ‘average’ image (see Section 3) on the performance of the models, we train MM-Gate, MM-XAtt, and MM-Gated-XAtt on tweets that are originally accompanied by an image excluding all text-only tweets; and we test on all tweets as in our original setting (text-only tweets are paired with the ‘average’ image). The results are shown in Table 5. MM-Gated-XAtt is consistently the best performing model, followed by MM-Gate. However, their performance is inferior than when models are trained on all tweets using the ‘average’ image as in the original setting. This suggests that the gate operation not only regulates the ﬂow of information for each modality but also learns how to use the noisy modality to Table 5: Macro F1-Score for POI type prediction. Models are trained on tweets that are originally accompanied by an image. Results are on all tweets. Best results are in bold. Figure 3: Average percentage of MM-Gated-XAtt activations for the textual and visual modalities for each POI category on the test set. improve classiﬁcation prediction. This result is similar to ﬁndings by (Arevalo et al., 2020). To determine the inﬂuence of each modality in MM-Gated-XAtt when assigning a particular label to a tweet, we compute the average percentage of activations for the textual and visual modalities for each POI category on the test set. The outcome of this analysis is depicted in Fig. 3. As anticipated, the textual modality has a greater inﬂuence on the model prediction, which is consistent with our ﬁndings in Section 6. The category where the visual modality has greater impact on the predicted label is Professional & Other Places (43.20%) followed by Shop & Service (43.11%). To examine how the visual information impacts the POI type prediction task, Fig. 4 shows examples of posts where the contribution of the image is large while the text-only model (BERT) misclassiﬁed the POI category. We observe that the text content of Post (a) misled BERT towards Food, Figure 4: POI type predictions of MM-Gated-XAtt (Ours) and BERT Sánchez Villegas et al. (2020) showing the contribution of each modality (%) and the XAtt visualization. Correct predictions are in bold. probably due to the term ‘powder’. On the other hand, MM-Gated-XAtt can ﬁlter irrelevant information from the text, and prioritize relevant content from the image in order to assign the correct POI category for Post (a) (Great Outdoors). Likewise, Post (b) was correctly classiﬁed by MM-GatedXAtt as Shop & Service and misclassiﬁed by BERT as Arts & Entertainment. For this post 40% of the contribution corresponds to the image and 60% to text. This shows how image information can help to address the ambiguity in short texts (Moon et al., 2018a), improving POI type prediction. 7.2 Cross-attention (XAtt) Fig. 4 shows examples of the XAtt visualization. We note that the model focuses on relevant nouns and pronouns (e.g. ‘track’, ‘it’), which are common informative words in vision-and-language tasks Tan et al. (2019). Moreover, our model focuses on relevant words such as ‘track’ for classifying Post (a) as Great Outdoors. Lastly, we observe that the XAtt often captures a general image information, with emphasis on speciﬁc sections for the predicted POI category such as the pine trees for Great Outdoors and the display racks for Shop & Service. To shed light on the limitations of our multimodal MM-Gated-XAtt model for predicting POI types, we performed an analysis of misclassiﬁcations. In general, we observe that the model struggles with identifying POI categories where people might perform similar activities in each of them such as Food, Nightlife Spot, and Shop & Service similar to ﬁndings by Ye et al. (2011). Fig. 5 (a) and (b) show examples of tweets misclassiﬁed as Food by the MM-Gated-XAtt model. Post (a) belongs to the category Nightlife Spot and Post (b) belongs to the Shop & Service category. In both cases, the text and image content is related to the Food category, misleading the classiﬁer towards this POI type. Posting about food is a common practice in hospitality establishments such as restaurants and bars (Zhu et al., 2019), where customers are more likely to share content such as photos of dishes and beverages, intentionally designed to show that are associated with the particular context and lifestyle that a speciﬁc place represents (Homburg et al., 2015; Brunner et al., 2016; Apaolaza et al., 2021). Similarly, Post (b) shows an example of a tweet that promotes a POI by communicating speciﬁc characteristics of the place (Kruk et al., 2019; Aydin, 2020). To correctly classify the category of POIs, the model might need access to deeper contextual information about the locations (e.g. ﬁner subcategories of a type of place and how POI types are related to one another). This paper presents the ﬁrst study on multimodal POI type classiﬁcation using text and images from social media posts motivated by studies in geosemiotics, visual semiotics and cultural geography. We enrich a publicly available data set with images and we propose a multimodal model that uses: (1) a gate mechanism to control the information ﬂow from each modality; (2) a cross-attention mechanism to align and capture the interactions between modalities. Our model achieves state-of-the-art performance for POI type prediction signiﬁcantly outperforming the previous text-only model and competitive pretrained multimodal models. In future work, we plan to perform more granular prediction of POI types and user information to provide additional context to the models. Our models could also be used for modeling other tasks where text and images naturally occur in social Figure 5: Example of misclassiﬁcations made by our MM-Gated-XAtt model. media such as analyzing political ads (Sánchez Villegas et al., 2021), parody (Maronikolakis et al., 2020) and complaints (Preo¸tiuc-Pietro et al., 2019; Jin and Aletras, 2020, 2021). Our work complies with Twitter data policy for research,and has received approval from the Ethics Committee of our institution (Ref. No 039665). We would like to thank Mali Jin, Panayiotis Karachristou and all reviewers for their valuable feedback. DSV is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by the UK Research and Innovation grant EP/S023062/1. NA is supported by a Leverhulme Trust Research Project Grant.