Graph is a exible and eective tool to represent complex structures in practice and graph neural networks (GNNs) have been shown to be eective on various graph tasks with randomly separated training and testing data. In real applications, however, the distribution of training graph might be dierent from that of the test one (e.g., users’ interactions on the user-item training graph and their actual preference on items, i.e., testing environment, are known to have inconsistencies in recommender systems). Moreover, the distribution of test data is always agnostic when GNNs are trained. Hence, we are facing the agnostic distribution shift between training and testing on graph learning, which would lead to unstable inference of traditional GNNs across dierent test environments. To address this problem, we propose a novel stable prediction framework for GNNs, which permits both locally and globally stable learning and prediction on graphs. In particular, since each node is partially represented by its neighbors in GNNs, we propose to capture the stable properties for each node (locally stable) by re-weighting the information propagation/aggregation processes. For global stability, we propose a stable regularizer that reduces the training losses on heterogeneous environments and thus warping the GNNs to generalize well. We conduct extensive experiments on several graph benchmarks and a noisy industrial recommendation dataset that is collected from 5 consecutive days during a product promotion festival. The results demonstrate that our method outperforms various SOTA GNNs for stable prediction on graphs with agnostic distribution shift, including shift caused by node labels and attributes. Stability; Stable Prediction; Graph Neural Network; Distribution Shift ACM Reference Format: Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu. 2021. Stable Prediction on Graphs with Agnostic Distribution Shift. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/1122445.1122456 Graphs are commonly used to represent the complex structures of interacting entities in a exible and eective manner, with applications and domains varying from social networks, knowledge graphs, and recommender systems. The challenging and open-ended nature of graph representation learning lends itself to a variety of diverse models [13]. Recent advances in the literature have convincingly demonstrated the high capability of Graph Neural Networks (GNNs) [18,31,35] on graph representation learning and inference. GNNs typically follow a neighborhood aggregation scheme, i.e., a node is represented by recursively aggregating other nodes’ information within the neighborhood [11,43]. Since messages pass along the edges, GNNs can eectively capture the high-order structural correlations between nodes. Most of existing GNNs are proposed and evaluated with an underlying assumption, i.e., the distribution of collected training data is consistent with the distribution of testing data. Under this assumption, many studies follow the convention of performing random splits on graph benchmarks [18,42] to train and test GNNs. Unfortunately, this assumption can hardly be satised in real-world application-specic systems [15,24] due to the ubiquitous selection bias, leading to distribution shift between training and test data. Moreover, the test distribution is always agnostic during optimizing GNNs on training graph data. The agnostic distribution shift includes shift caused by both node labels and node attributes. For example, in recommender systems, a notorious problem is that the logged interactions collected from current undergoing systems may warp (graph) recommenders biased towards popular items, i.e., label-related distribution shift. Such a shift may degrade the recommendation eectiveness on deployed environment, e.g., causing unfairness towards less exposed items. In addition, male users often have less logged interactions on e-commerce platforms since they often online-shop with specic purposes. Therefore, the (graph) recommenders may be trained with bias towards female users and thus deteriorates the experience of male users, i.e., node attribute related distribution shift. Generally, the agnostic distribution shift between training and testing would render traditional GNNs over-optimized on the labeled training samples and render their predictions errorprone on test samples, resulting in unstable predictions. Therefore, learning GNNs that are resilient to distribution shifts and able to make stable predictions on graphs will be important for real-world applications. Many techniques [4,9,16,23,33] designed for general machine learning problems are proposed in the literature of stable learning. These methods typically propose to re-weight training samples with a density ratio, thus driving the training distribution closer to the testing distribution. However, how to learn structural node representations based on which we can make stable prediction on graphs across agnostic environments remains largely unexplored in the literature of graph neural networks. More recently, [14] proposes to learn a stable graph that captures general relational patterns, and directly optimizes the adjacency matrix with a predened set generation task. However, they mainly aim to obtain an optimized and static adjacency matrix for a given graph and cannot dynamically process unseen neighborhood structures. Their framework also has the disadvantage of being tightly coupled with the set generation problem and cannot be easily adapted to other graph tasks. In this paper, we aim to learn task-agnostic GNNs that can make stable predictions on unseen neighborhood structures. Stable prediction poses a quantitative goal that, given one observational graph environment, a stably trained GNN should achieve a high average score and a low variance of scores onmultipleagnostic testing environments. We propose a novel stable prediction framework for GNNs, which permits both locally stable learning at the nodelevel, and globally stable learning at the environment-level. The proposed framework rst performs biased selection on the observational environment and constructs multiple training environments. For locally stable learning, we start from the perspective that each node is partially represented by other nodes in the neighborhood in GNNs, and we propose to capture stable properties by re-weighting the neighborhood aggregation process. Under the invariant prediction assumption for causal inference [30], we can improve the stability of predictions when the model relies more on stable properties. For globally stable learning, we inspect the training of GNNs at the environment-level and empirically nd that the losses for dierent environments progressively diverge in biased training, which eventually leads to unstable performance across environments. Therefore, we propose to reduce the training gap between environments to explicitly warp the GNNs to generalize well across environments. We conduct experiments on various public graph benchmarks and a real-world recommendation dataset that is collected from a world-leading e-commerce platform during an annual productpromotion festival where selection biases naturally exist. We evaluate a bunch of generic SOTA GNNs and methods that are specically designed for mitigating selection biases. We concern both traditional task-specic evaluation metrics and protocols that are especially designed for stable learning [20,21]. Extensive results demonstrate the capability of our framework on learning GNNs that make stable predictions on graphs. In summary, the contribution of this paper are: •We propose to achieve stable prediction on graphs for explicitly reducing the performance variances across environments with distribution shifts. This is less explored in the literature. •We devise a novel stable prediction framework for GNNs, which captures stable properties for each node, based on which we learn node representations and make predictions (locally stable), and regularizes the training of GNNs on heterogeneous environments (globally stable). •We conduct comprehensive experiments on dierent public graph benchmarks and a noisy industrial recommendation dataset, which jointly demonstrate the eectiveness of the proposed framework. The recent advances in graph neural networks [7,18,19,27,35, 38,39,47] have convincingly demonstrated high capability in capturing structural and relational patterns within graphs. Typically, GNNs follow a message passing schema for representation learning by transforming and aggregating the information from other nodes within the neighborhood. Dierent neighborhood aggregahave been proposed to reach state-of-the-art performances in various tasks. Typically, GAT [35] introduces the attention mechanism into the information aggregation process. SGC [39] simplies the original Graph Convolutional Network [18] by linearly propagating information and collapsing weights among graph layers. APPNP [19] extends the utilized neighborhood for node representation and achieves an adjustable neighborhood for classication. Many existing GNNs are evaluated on graph benchmarks that are randomly split [18,42], i.e., the training and testing data share similar data distribution. However, in real-world applications, e.g., recommender systems, training samples are observed and collected with selection bias, leading to inconsistencies between the training and testing distribution. Few works in the literature investigate such a real-world problem. Recently, GNM [49] confronts a related problem named non-ignorable non-response, which indicates that the unlabeled nodes are missing not at random (MNAR). However, they only consider distribution caused by node labels and neglect distribution shift related to node attributes, and they solely discuss binary-class datasets in the experiments, which can be less plausible for real-world scenarios. In this paper, we propose a framework that can alleviate the negative eects from shift related to both labels and attributes, and obtains stable predictions on various graph benchmarks and real-world recommendation datasets. [14] proposes to learn a static adjacency matrix for a given graph and expects that the learned adjacency matrix captures general relational patterns that are free from selection biases. We largely dier from this work as illustrated in the Introduction. Recently, many methods [4,9,16,23,33] are proposed to address selection bias for general machine learning problems. They typically align the training distribution with the testing distribution by re-weighting the training samples. Following these works, [20,21] rst propose the stable learning framework as well as the evaluation protocols. They mainly decorrelate dierent dimensions of the hidden variable, and they prove that the prediction based on decorrelated variables should be stable. More recently, GAT-DVD [3] directly borrows such a decorrelation idea to the literature of GNNs. In this paper, instead of borrowing generic o-the-shelf stable learning tools, we propose to inspect the neighborhood aggregating process and design graph-specic architectures. Besides, we show a large improvement over GAT-DVD in the experiments. LetXdenote the space of observed features,Adenote the space of adjacency matrix, andYdenote the outcome space. Following [30], we dene a graphenvironmentas the joint distribution𝑃on X ×A ×Yand useEto denote the set of all environments. For each environment, we have a graph dataset𝐺= (X, A, 𝑌), where X∈ Xare node features,A∈ Ais the adjacency matrix, and 𝑌∈ Yis the response variable (e.g., node labels in the node classication problem). The joint distribution of features and outcomes on(X, A, 𝑌 )can vary across environments, i.e.,𝑃≠ 𝑃 for𝑒, 𝑒∈ E, and𝑒 ≠ 𝑒. In this paper, we aim to learn node representations based on which we can make stable predictions across environments with various degrees of selection biases. Before giving the specic denition of stable prediction, we rst dene 𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒and𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝑆𝑐𝑜𝑟𝑒similar to [20] of a GNN model 𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒 =1|E|S𝐺 𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝐸𝑟𝑟𝑜𝑟 =1|E| − 1(S(𝐺)− 𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒). (2) where|E|denotes the number of environments evaluated, and S(𝐺)refers to the predictive score computed with a specic numerical assessment on dataset𝐺. Therefore,𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒indicates the overall performance of the learned GNN on heterogeneous testing environments, and𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝐸𝑟𝑟𝑜𝑟indicates how dierently the learned GNN performs on these environments. Based on these two metrics, we dene the problem of stable prediction on graphs: Problem 1 (Stable Prediction on Graphs).Givenone training environment𝑒 ∈ Ewith dataset𝐺= (X, A, 𝑌), the task is to learnnode representations based on which the predictions yield high 𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒 but small 𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝐸𝑟𝑟𝑜𝑟 across environments E. Here we briey summarize typical GNNs and then give an illustration on why they suer from the distribution shift. Modern GNNs follow a neighborhood aggregation schema by iteratively aggregating representations of its neighbors to update the representation of the target node. One iteration can be generally formulated as: whereNdenotes the indices of nodes in the neighborhood of node 𝑖,xdenotes the updated representation of node𝑖. The modeling ofAGGREGATEandCOMBINEcan be various and essential for dierent GNN architectures. Most GNNs are optimized to obtain an overall best performance on the observational environment, i.e., 𝐺= (X, A, 𝑌). In this way, we may expect an overly-optimized solution after training and some distribution-specic patterns may warp the GNN biased towards a globally sub-optimal solution since the distribution of real-world testing data can be agnostic and mostly yields shift from the training graph distribution in realworld applications (e.g., recommender systems). We denote this phenomenon as global instability. In the local view, i.e., neighborhood aggregation process, each neighbor node contributing to the nal aggregated representation can be viewed as a property of the root node. Most GNNs are proposed disregarding a fundamental problem, i.e., "what are the stableproperties of the root node?" The connection between capturing stable properties and stable prediction can be interpreted as a plausible hypothesis: Hypothesis 1. If we consider all stable properties ("direct causes") of the target node of interest, then the conditional distribution of the outcome given the stable properties will not change when interventions (e.g., biased selection) are performed without aecting the target and stable properties themselves. This stability idea is closely linked to causality and has been discussed or empirically demonstrated to be eective in the literature [2,12,28]. In other words, we can achieve stable prediction on graphs across environments with various selection biases by capturing the stable properties for each node. Indeed, some existing GNNs (e.g., GAT [35]) propose to specify dierent weights to dierent nodes in a neighborhood and somewhat "stabilize" the weights learning process by leveraging multiple heads. However, we argue that the weights they learned explore the subtle statistical relationship in training data and can be far from being stable due to potential confounders [29]. A confounder denotes the common cause of the predictor and the outcome, which may lead to spurious correlations between the root node (outcome) and some "important" neighbors (predictor). We denote this phenomenon as local instability. The essence of our methodology for stable learning on graphs is depicted in Figure 1. Given the environments priorly generated by various selection biases, we propose the locally stable learning strategy, which explicitly captures the stable properties in learning the representation of each node by re-weighting the information propagation/aggregation processes, and the globally stable learning, which regularizes the errors across environments to be closer and thus improving the stability. We borrow the idea from the literature of causality [30] and propose to rst perform several interventions on the observational data based on selection biases, and create some environmentsEaccordingly. We note that we keep an observational environment in E. 4.2.1 Locally Stable Learning. In the neighborhood aggregation process, we regard nodes that are consistently important across environments as stable properties, as depicted in Figure 1. Therefore, the nal representation of the target node of interest is primarily based on such stable properties. Given one environment𝑒, we obey a weighted schema of the commonly used neighborhood aggregation in most GNNs: Figure 1: The overall schema of the proposed framework, which consists of two essential components, i.e., the locally stable learning that captures properties that are stable across environments in the representation learning of each target node, and the globally stable learning that explicitly balances the training of dierent environments. wherexdenotes the updated representation of the target node 𝑥,Wdenotes the weight matrix of the linear transformation,𝜎 denotes the nonlinear activation function,Ndenotes the indices set of node𝑥’s neighbors, and𝛼is the weight that indicates the importance of one property𝑥to the target node𝑥learned in environment𝑒. The weight𝛼can be learned in various ways and we adopt the formulation of graph attention as the weight predictor 𝜑for its simplicity: where LeakyReLU denotes the nonlinearity (with negative input slope 0.2), andadenotes the parameter vector to determine the weight𝛼for environment𝑒. The weights in graph attention indicate relative importance in the neighborhood, and it is also acceptable to use the following formulation, which has a sense of absolute importance: We note that each environment𝑒has its parameter vectora, which helps nd important properties especially for this environment (or data distribution). To identify and leverage properties that are consistently important across all environmentsE, a straightforward solution is to treat properties with weights higher than a certain threshold as stable properties and manually encourage the model to rely on them for prediction while suppressing the others. However, this solution has at least two drawbacks: 1) manually modifying the weights of properties will lead to inconsistencies between the current weight prediction module and other modules in GNN since they will be trained based on the modied weights rather than the predicted weights; 2) some properties that are not important may happen to obtain high weights during training and model regarding them as stable properties may draw false conclusions. In this regard, we propose to identify and leverage stable properties in a soft way: 1) we propose to pull the weight predicted across environments by using a distance loss; 2) we will regard properties with weights that are both high in value and can be easily pulled together as stable properties and suppress the others. In this way, we can confront the randomness of gradient descent based training (i.e., the second problem) by pulling sensitive weights towards the corresponding insensitive ones, and confront the inconsistent training problem (i.e., the rst problem) since other parts in GNNs will always reply on the currently predicted weights. To further simplify the training process, we note that the rst pulling process can also help to suppress properties that yield inconsistent weights since they will be driven towards the average. Therefore, the locally stable regularizer can be written as the following: whereVdenotes the indices set of all nodes, andDistdenotes a distance function which is set as the L2 distance in our experiment for its empirical eectiveness. When there are multiple GNN layers equipped with the re-weighting module, the regularizer can be written as: where𝑁denotes the number of GNN layers equipped with the re-weighting module. 4.2.2 Globally Stable Learning. Besides investigating stable prediction from a local view, i.e., learning node representations that capture stable properties, we further investigate stable prediction from a global and environment-level view. Recall that one of the fundamental goals of stable prediction is to reduce𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝐸𝑟𝑟𝑜𝑟, i.e., the standard deviation of scores across environments. However, although we can individually compute scores for all environments, it is not directly applicable to optimize objectives related to nondierentiable metrics. We start with a perspective that model performances correlate well with training losses and empirically nd that, in the observational environment, the sub-losses that belong to dierent environments progressively diverge during training with selection bias. Such divergence in training losses will eventually lead to gaps in testing performance. In this regard, we propose to explicitly reduce the gap between losses across environments to rectify the unstable training. Mathematically, the proposed stable regularizer for globally stable learning can be formularized as the following: whereLdenotes the task-specic loss computed in the observational environment, i.e., environment 0, andLdenotes the sub-loss that belongs to environment 𝑒, i.e., whereYdenotes the indices set of nodes that have labels for environment e. We note that minimizing the pair-wise distance of losses as dened in Equation 10 is equivalent to minimizing the variance of losses. Since the negative of loss function can be interpreted as a certain scoring function, it is equivalent to minimizing the stability error dened in Equation 2. 4.2.3 Training. With the proposed two building blocks, i.e., locally and globally stable learning, we give a detailed illustration of how we train the entire framework. The training procedure is summarized in Algorithm 1. Given one observational environment 0 for training with dataset𝐺= (X, A, 𝑌), we perform biased selection with one or more factors (e.g., node label or semantic node attributes) on𝐺at the beginning of training or each training epoch. After selection, we obtain several environmentsEwith the corresponding graph datasets{𝐺= (X, A, 𝑌)}. Each environment keeps a weight predictor that is individually trained, and all the environments share the same GNN backbone, which Input: Observational graph datasets 𝐺= (X, A, 𝑌) Output:Parameters of stably learned GNN backbone𝜃and Perform biased selection on 𝐺and obtain {𝐺= (X, A , 𝑌)}. Initialize 𝜃 and {𝜑} while not converged do for e = 1 to|E|do end Compute Land Las in Eq. 10-11, and compute Lwith cached {𝛼}as in Eq. 8. Optimize 𝜃, 𝜑to minimize L as in Eq. 12. end is solely trained on environment 0, i.e., the observational environment. We train the entire framework following an environment-byenvironment procedure and update the corresponding parameters with the task-specic objectiveL(e.g., node classication, recommendation). For the observational environment, we train the weight predictor for this environment and the GNN backbone with combined loss function: We do not train other environments with the correspondingL since we mainly aim to rectify the importance weights in the observational environment. We note that the GNN backbone and the weight predictor in the observational environment will be used for testing/inference. We note that instability in prediction arises due to the inconsistencies of the joint distribution of features, adjacency matrix, and outcomes(X, A, 𝑌 )across environments (e.g., training/testing environments) according to Section 3. To test the stability of all methods, for each dataset, we construct several training and testing environments by varying the joint distribution of(X, 𝑌 ). We consider both shift related to node labels by directly varying𝑌and shift related to node attributes by varying additional factors that well correlate with X. We disregard the shift related to A in the experiment. Following previous GNNs [18,35] , we evaluate the proposed stable learning framework on public graph benchmarks, including the recently proposed Open Graph Benchmark [15], in Section 5.1. Since these datasets seldom accompany meaningful attributes, we are mainly concerned with label-related shift. We conduct hyperparameter analysis to obtain a better understanding of the designs. We further demonstrate its ecacy on real-world recommendation scenarios that are known to be full of sample selection biases [5] in Section 5.2. We mainly report results with shift caused by node attributes as a complement to label shift on graph benchmarks. We (a) 𝜏= 0.9, results by varying 𝜏(b) 𝜏= 0.8, results by varying 𝜏 (g) 𝜏= 0.9, original val/test results(h) 𝜏= 0.8, original val/test results Figure 2: Testing results on OGB-Arxiv dataset by varying the training bias ratio 𝜏 show the metrics by varying the testing bias ratio. Subgures 2d - 2f explictly show the stability of prediction by plotting the 𝐴𝑣𝑒𝑟𝑎𝑔𝑒_𝑆𝑐𝑜𝑟𝑒 and 𝑆𝑡𝑎𝑏𝑖𝑙𝑖𝑡𝑦_𝐸𝑟𝑟𝑜𝑟. Subgures 2g - 2i show the results on the original validation and testing datasets. also consider the settings with agnostic real-world selection biases in Section 5.2.3. 5.1.1 Experimental Setup. We conduct experiments on the following datasets, of which the statistics are listed in Table 1: OGB Arxiv.The OGB datasets are proposed with real-world (nonrandom) training/testing split, which is suitable for stability evaluation. We perform biased selection on the half nodes from the original training set based on their corresponding labels (i.e., label shift) to construct the observational environment. We construct multiple testing environments from the remaining half nodes. For each environment, the probability of node𝑖with label𝑦to be selected can be𝑃(𝑠= 1)= 𝜏if𝑦 ≥24 and 1− 𝜏otherwise. We note that other selection choices are acceptable and we here mainly aim to get originally equal-size environments by splitting in the middle, i.e., 24.𝜏denotes the bias ratio indicating the severity of sample selection bias. We use𝜏, 𝜏to denote the training and testing bias ratio, respectively. In the experiments, we vary the training (b) Detailed statistics of the recommendation dataset, which contains 5 consecutive days collected during a product promotion festival. (IteracN stands for the number of Interactions at Day N.) Users Items Iterac1 Iterac2 Iterac3 Iterac4 Iterac5 bias ratio𝜏among{0.9,0.8,0.7}indicating heavy, medium, and light selection biases, following [20]. We consider several SOTA GNNs as comparison methods, including GCN [18], GAT [35], SGC [39], and APPNP [? ]. Citeseer.We also report the results on a traditional benchmark, Citeseer [32]. The probability of node𝑖with label𝑦to be selected can be𝑃(𝑠= 1)= 𝜏if𝑦 ≥3 and 1− 𝜏otherwise. We further add two comparison methods that are designed for alleviating selection biases, i.e., GNM [49], and GAT-DVD [3]. We do not test them on OGB-Arxiv since they are not easily adaptable for large-scale datasets. We consider training bias ratio𝜏=0.8, which is the same as the medium-level selection bias in the Arxiv dataset. We follow [18,35] to construct two-layer GCN and GAT. All other methods (including ours) also contain two graph layers for a fair comparison. All methods are with hidden size 250 for OGBArxiv and 64 for Citeseer, and learning rate 0.002. 5.1.2 Stability comparison with SOTA GNNs. The testing results on the OGB-Arxiv dataset and Citeseer dataset are shown in Figure 2 and Figure 3. Specically, each gure in Figure 2a - 2c plot the evaluation results across dierent testing environments with bias ratio𝜏varying among{0.0,0.1,0.2, . . . ,1.0}. Figure 2a - 2c dier from each by the training bias ratio𝜏. Since stable prediction expects both low performance variances across multiple testing environments and an overall high performance, we explicitly plot the corresponding Stability_Error and Average_Score in Figure 2d 2f. In a nutshell, our framework achieves more stable results than SOTA GNNs, including both generic ones (GAT, GCN, SGC, and APPNP) and those designed for reducing selection biases (GATDVD and GNM). In dierent testing environments constructed by varying the testing bias ratio, we observe that most GNNs suer from the distributional shifts and yield poorer performances when the testing distribution is more dierent from the training distribution (e.g., the right of Figure 2a). Although our framework sacrices some performance in testing environments with distribution closer to the training (e.g., the left of Figure 2a), our framework obtains signicantly higher Average_Score and lower Stability_Error, which are important metrics (as illustrated in Section 3) for stable prediction [20], across heterogeneous environments as indicated in Figure 2d - 2f, 3b. By varying the training bias ratio from light to heavy (from Figure 2f to 2d), we can observe that the stability errors are increasing signicantly for most conventional GNNs. This means that they are sensitive to the selection biases and the resulted distribution shifts. When the distributional shifts are heavier, the performance across multiple testing environments yields larger dierences, i.e., unstable predictions. Our framework yields the least stability error that can be almost negligible, which demonstrates the superiority. On the other hand, when the distribution shifts are increasing from light to heavy, the least Average_Score drop of our framework demonstrates that we are not achieving some trivial solutions. For example, one trivial solution is to deteriorate the performance on data distributions similar to the training environments, which can somehow reduce the Stability_Error but have the cost of deteriorating the overall eectiveness. In a nutshell, our framework achieves stable prediction on graphs without sacricing eectiveness. Although APPNP yields high Average_Score compared to other SOTA GNNs, their performance scores across dierent testing environments mostly vary signicantly (by taking an in-depth analysis on Figure 2c or 2a), i.e., unstable predictions. Surprisingly, with the same hyper-parameters, GAT-DVD and GNM achieve more unstable results than other generic GNNs. As shown in Figure 3b, GAT-DVD and GNM yield similar or better Average_Score but signicantly larger Stability_Error compared to other SOTA GNNs. We attribute these results to that GNM is designed for binary-class datasets and may perform poorly or need further improvements when extending it to multi-classes datasets, and that the work of GAT-DVD [3] is still under development. 5.1.3 Performance on the original validation/testing datasets. Since the Open Graph Benchmark splits the datasets under real-world settings, we also report the performance on the original OGB validation and test datasets, which can be viewed as real-world environments. As shown in Figure 2g-2i, our framework consistently outperforms the other methods. The improvement can be larger when the bias is more severe. These results again veries the importance of alleviating selection biases and the eectiveness of our framework. 5.1.4 Analysis on Hyper-parameters. To investigate how the proposed two regularizers aect the stability of prediction, we vary the corresponding hyper-parameters, i.e.,𝜆and𝜆, on the OGB-Arxiv dataset with training bias ratio𝜏=0.8. Figure 4 summarizes the results. When increasing the𝜆and𝜆from a lower rate, we observe a signicant stability improvement, i.e., improved Average_Score and reduced Stability_Error. This demonstrates the eectiveness of our framework in the sense of ablation study. In other words, progressively reducing the impact of the two regularizers to a lower Figure 4: Hyper-parameter analysis on the OGB-Arxiv dataset with training bias ratio 𝜏= 0.8. rate will lead to a performance drop. We also observe that when the impact of the two regularizers becomes increasingly larger, e.g., 𝜆=10 or 100, 𝜆=2 or 4, there is a performance drop in the model’s stability. We attribute this phenomenon to that a large𝜆 of one regularizer (global or local) may deteriorate the capability of the other regularizer (local or global) as well as the task-specic capacity, thus leading to unstable predictions. When comparing the global and local regularizers, the eectiveness of local stable learning is less sensitive to the corresponding hyper-parameter𝜆. In other words, we can easily improve the model stability using the local regularizer without cautiously tuning𝜆. As for the global regularizer, relatively small or large𝜆will harm the stability. This suggests that universally pulling the losses of each node across heterogeneous environments at the𝑠𝑎𝑚𝑒rate might not be an optimal solution. Therefore, we leave nding adaptive pulling strategies for dierent nodes and environments as a promising future work. 5.2.1 Exp erimental Setup. To demonstrate the ecacy of the proposed framework on real-world applications, we conduct experiments on the user-item bipartite graph of recommender systems, where sample selection bias are arguably ubiquitous [5]. Collecting a Real-world Noisy Dataset.Specically, we collect an industrial dataset from one of the world-leading e-commerce platforms from June 11th, 2020, to June 15th, 2020, when an annual product promotion festival is being celebrated. In such a period, the promotion strategies from online shop owners can be various and time-evolving. Therefore, inconsistencies between the users’ clicks and their satisfactions naturally exist, leading to a distribution shift from the collected data and the real-world testing environment. We select users and items that have interactions in all ve days, which means we do not consider the cold-start setting. We mainly consider click interactions, which is a common setting for the deep candidate generation phase in recommendation. We further select users that have 200-300 interactions to ensure the quality of data. We split the interactions into several environments according to the day they happen, and the statistics are listed in Table 1. We use data samples from the rst day for training and use the remaining for evaluation. We keep the gender and age attributes for users. For users that do not provide these attributes, we use the value predicted by the e-commerce platform. There are 8 age sections provided by the platform, including 1-18, 19-25, 26-30, 31-35, 36-40, 41-50, 51-60, and >=61. We group 1-18 and 19-25 into a holistic section and the remaining into another section. This split results in approximately equally-sized sections. EvaluationWe mainly focus on the deep candidate generation stage of recommendation. The user-item bipartite graph consists of user nodes and item nodes. A user is connected to an item when the user interacts with the item. Solely id feature is considered and transformed to dense vectors using a learnable embedding matrix, which is a common practice for deep candidate generation models [1,37]. The embedding matrix is learned along with the graph recommenders. The collected dataset contains rich semantic attributes, and we discuss user gender/age in experiments, respectively. The probability of a user to be selected can be𝑃(𝑠= 1)= 𝜏 if𝑎𝑔𝑒 ≤25𝑜𝑟 𝑔𝑒𝑛𝑑𝑒𝑟 = 𝑀and 1− 𝜏if𝑎𝑔𝑒 >25𝑜𝑟 𝑔𝑒𝑛𝑑𝑒𝑟 = 𝐹. 𝑀denotes male and𝐹denotes female. We also discuss agnostic selection bias by viewing each of the following days as an individual environment. We incorporate a widely used metric NDCG [1,37]. Compared to other widely used metrics such as Recall and Hit Ratio, NDCG considers the positions of recommended items. NDCG can be formally written as: whereUis the set of users, N is the number of recommended items, andˆ𝑖indicates the𝑘th item recommended for user𝑢. denotes the indicator function.IDCG@Ndenotes the ideal discounted cumulative gain and is the maximum possible value ofDCG@N. consider the top 100 generated candidates of each model for evaluation, i.e., NDCG@100. We report the percentage score in the results. Baselines We consider the following models as baselines: •NGCG leverages GCN to represent users and items based on the user-item bipartite graph. •LightGCN linearly propagates user/item embeddings on the useritem graph and thus simplifying the NGCF. •Stable Graph Recommender. We build the proposed stable graph recommender based on NGCF. NGCF largely follows the standard GCN model. The proposed stable graph recommender propagates Figure 5: Results on real-world recommendation dataset with distribution shift caused by node attributes. embeddings on the user-item bipartite graph as the following: whereeandedenote the updated user and item embedding after𝑘layers propagation.𝜎denotes a certain nonlinear activation function and isLeakyReLUas NGCF.Ndenotes the set of interacted items for user𝑢andNdenotes the set of interacted users for item 𝑖. Wand Ware learnable transformation matrices.𝛼denotes the importance of interaction< 𝑢, 𝑖 >for representing user𝑢and item𝑖. We note that interactions that are consistently important across environmentsEare stable properties, as illustrated in Section 4.2.1. We keep a global𝛼for layers due to its eciency and do not compute weights per layer. We note that deep candidate generation models, which recall Top K items from a billion-scale item gallery are largely sensitive to model eciency. For example, one of the contribution of LightGCN is to remove the nonlinearity𝜎in NGCF and thus improving eciency. We train the stable graph recommender as illustrated in Section 4.2.3. 5.2.2 Analysis on Stable Prediction. The testing results with distribution shift caused by node attributes are shown in Figure 5. Specically, we set training bias rate𝜏=0.6 and test the model on testing environments with bias rate varying among{0.0,0.1,0.2, . . . ,1.0}. Figure 6: Results on recommendation dataset with realworld environments (each day as an individual environment). Overall, we observe that our framework achieves signicantly more stable results than the other two SOTA graph recommenders. Specically, the Average_Score is signicantly improved with Stability_Error largely reduced. Noteworthy, similar to the ndings to those on the graph benchmarks, the stable graph recommender improves the stability mostly by boosting the performance on environments where NGCG/LightGCN performs poorly. For example, as shown in Figure 6b, NGCF/LightGCN achieves about 0.6/0.5 NDCG score when𝜏=0.6 and𝜏=0.0 while the proposed stable graph recommender signicantly boost the NDCG to nearly 0.77 with the same distribution shift. These results further demonstrate the eectiveness of the proposed method on broader real-world applications. 5.2.3 Evaluation with Real-world Environments. To evaluate our framework on real-world environments with agnostic selection bias, we construct testing environments from days that follow the day where graph recommenders are trained. To be specic, the model is trained on the recommendation data logged on June 11th, 2020, and we test the trained model on the data logged on June 12-15th, 2020, individually. Therefore, there are four testing environments in total, and we report the results in Figure 6. We observe that there is a consistent performance improvement across heterogeneous real-world environments. These results suggest that our framework is capable of capturing stable properties that should persist across time in node representation learning. For example, a user might click some items due to external distractions, which can hardly represent the user’s inherent interest. This phenomenon is known as the natural noise [26], and such clicks are unstable properties when representing the user node on the user-item bipartite graph. The proposed stable graph recommender captures non-noisy interactions for users on the user-item bipartite graph and obtains stable/inherent interest representations that are eective across time. In this paper, we argue that real-world applications (e.g., recommender systems) are full of selection bias, leading to a distribution shift from the collected graph training data to testing environments. We propose a novel stable prediction framework that permits locally stable learning by capturing properties that are stable across environments in node representation learning, and globally stable learning to balance the training of GNNs on dierent environments. We conduct extensive experiments on newly proposed and traditional graph benchmarks as well as datasets collected on real-world recommender systems concerning shift related to both node labels and attributes. Results demonstrate the capability of the proposed framework on stable prediction on graphs. We believe that the insights of the proposed framework are inspirational to future developments of stable learning techniques on graphs. Mitigating selection biases can be essential for deploying GNNs on real-world application systems (e.g., confronting the inconsistencies between the training and testing distributions, and achieving fairness for dierent groups of users), while few works in the literature are proposed for learning stable GNNs. We plan to further investigate such a problem from the perspective of causal discovery/inference, e.g., disentangling the stable/unstable properties for a given neighborhood structure or a hidden variable. Another future direction is to design techniques for application-specic selection biases, e.g., exposure bias in recommender systems. We plan to make further investigations.