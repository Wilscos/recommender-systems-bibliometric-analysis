The utilisation of large and diverse datasets for machine learning (ML) at scale is required to promote scientiﬁc insight into many meaningful problems. However, due to data governance regulations such as GDPR as well as ethical concerns, the aggregation of personal and sensitive data is problematic, which prompted the development of alternative strategies such as distributed ML (DML). Techniques such as Federated Learning (FL ) allow the data owner to maintain data governance and perform model training locally wit hout having to share their data. FL and related techniques are often described as privacy-preserving. We explain why this term is not appropriate and outline the risks associated with overreliance on protocols that were not designed with formal definitions of privacy in mind. We further provide recommendations and examples on how such algorithms can be augmented to provide guarantees of governance, security, privacy and veriﬁability for a general ML audience without prior exposure to formal privacy techniques. Machine learnin g (ML) has shown promise in solving a number of important problems such as disease survival prediction (Vann eschi et al. 2011; Yan e t a l. 2020) or earlystage cancer discovery (Kourou et al. 2015). However, in order to train ML models capable of solving these tasks effectively, as well as fulﬁlling requirem e nts such as fairness and generalisation, large, high-quality and unbiased datasets are required. Procuring these datasets has been problematic in contexts that rely on scarc e, sensitive data especia lly in the context of healthcare. So far, this issue has been alleviated through data centralisation, where these datasets are aggregated at a single lo cation, and model training is perfo rmed. However, the introduction of stricter data protection and governance regula tions (e.g. the general data protection regulation (Radley-Gardner, Beale, and Zimmermann 2016)), and an increased societal awareness of privacy issues resulting in public resistance against aggressive data collection, centralisation of data is increasingly becoming an unsustainable solution . As a resu lt, the ML community has witnessed a surge in interest for scalable, privacy-preserving technologies allow- Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. ing large- scale training of ML models on distributed data across geog raphically distant institutions. The com mon factor of this family of protocols is their reliance on sha ring the algorithms or the model updates instead of the datasets, thus remaining in line with existing data governance and data pr otection regulations. This allows the researchers to conduct collaborative mac hine learning at scale while minimising the da ta transmitted. One such mechanism, namely federated learning (FL) (Koneˇcn´y et al. 2016), has received particular appraisal from the research community and has seen widespread adoption in a large number of studies (Bonawitz et al. 2019; Nasirigerdeh et al. 2020; Brisimi et al. 2018; Roy et a l. 2019; Nasirigerdeh et al. 2021; Zhao et al. 2018; Brisimi et al. 2018). However, while FL –by de sig n– only avoids transmitting data and thus preserves data governance, it is often wro ngly referred to as privacy-preserving. In fact, collaboratively trained models ca n unintentionally leak in formation about the sensitive properties of the training population. Attacks on the models trained and transmitted between institutions have, on numerous o c casions, been shown to be able to reveal sensitive informa tion (Usynin et al. 2021; He, Zhang, and Lee 201 9; Geiping et al. 2020; Sho kri et al. 2017; Zh ang et al. 2020; Ziller et al. 2021). Thus, FL and other governance mechanisms alone are insufﬁcient m eans of privacy protection and require augmenta tions in the forms of formal privacy enhancing technologies (PETs). Beyond privacy, a number of additio nal considerations arise when data is processed, independent of whether such processing occurs centrally or –as in FL– in a decentralised manner. The requir ements towards such systems are outlined in a the oretical framework named structured transpare ncy (ST) (Trask et al. 2020; Kaissis et al. 2020), whose terminology we will employ in the current work. These include: 1. The ab ility to train a model collaboratively without revealing the inp ut data to othe r contributors (input privacy); 2. The protection of private information which can be learned from the results (i.e. the outp ut) of the computation (output privacy); 3. The ability to verify the origin of the computation’s result i.e. that the model update is not submitted by an unauthorised p arty (input veriﬁcation); 4. The capability to guarantee the correctness of the output and that the processing of inputs is honest (output veriﬁcation) and 5. The ability to control data ownership and exercise governance over it (ﬂow governance). Adhering to these principles (along with others, such as exp la inability, that lie outside of scope of this work) is fundamental to fr ameworks that claim to be trustworthy, giving sound guarantees of privacy to the individuals whose sensitive data is utilised, but also of reliability of the trainin g protocol in general. Beyond FL, (an d in part due to its vulnerabilities), a numb e r of oth er DML frameworks have been intr oduced, for instance Swarm Learning (SL) (Warnat-Herresthal et al. 2021), Split Learning (Vepakomma et al. 2018) or gossip learning (Heged˝us, Danner, and Jelasity 2019)). These attempt to address some of the aforemention ed requirements, but often fail to offer full ST guarantees either. In this article, motivated by the increasin g popularity of DML, we aim to equip ML practitioners interested in applying DML techniques or reading related literature with an overview of relevant techniques, so that they can avoid misunderstandings and common pitfalls. Our contributions can be summarised as f ollows: • We provide deﬁnitions for the commonly used (and misunderstood) terms governance, privacy, secrecy, security, accountability and veriﬁcation. • We contextualise these techniques within the stru c tured transparency framework. • Finally, we discuss mechanisms by which the governance-preserving attributes of DML frameworks can b e supplemented to render them fully privacy-preserving, veriﬁab le , and secure. In its core, data governance is a framework that deﬁnes the way data should be handled from the perspectives of access, ownership and auditability (Eryurek et al. 2021). For the purposes of distributed a lgorithmic processing, we consider aud itability (that is, the ability of being inspected for e.g. quality) as a process that is either p erformed loca lly by the data owner on their own data or, alternatively, by the central aggregator on the up dates subm itted by the members of the consortium. We discuss auditability in the Accountability Section. Central to exercising data governance is the ability to control data locality an d d ata accessibility. In fra meworks such as FL, control over data locality is maintained through local model training and subsequent u pdate sharing, meaning that the data does not leave its owner owner and only the results of the algorithmic p rocessing of this data are shared with the rest of the federatio n. Nevertheless, DML allows (indirectly) accessing the data (or, more prec isely, computational results derived from it), to the members of th e federation. To prevent unwarranted or unauthorised access, security me a sures must b e put in place, which are discussed below. In this regard, governance over data is maintained in DML paradigms such as FL, provided a suitable framework for auditing the input data is in plac e. However, even though appropriately audited data remains local and is only accessible through the collabor ative learning protocol, the protection of privacy and secrecy are contingent on additional techniques being utilised. In this section we consid er the fundamental differences between the concepts of secrecy and privac y , which are o ften conﬂated. Privacy is the ability to control how much can be learned from the data about an individual. In the context of ST, there exist two separate privacy-related concepts: input and output privacy. Input privacy controls the extent to which the input data is visible and acc essible to other actors, whereas output privacy is concerned with how mu ch can be learnt from the data itself. The former can be achieved through strategies that maintain th e secrecy of the computation, whereby secrecy implies that the sensitive data itself cannot be seen by a nyone other than the data owner, concealing it from other p a rties. While decentralised protocols such as FL or SL allow data owners to enforce governance over their data and prevent unwarranted access to the training data directly, they do n ot provide the contributors with any mechanisms to contro l what can be inferred about that data when it is used for model tra ining (output privacy). Input and output privacy pursue two comple mentary goals: secrecy prevents data from being usable when accessed inappropriately while privacy reduces th e amount of information that can be extracted from the data about the individual while no t w holly preventing drawing insigh ts from the data oth erwise. The secrecy of da ta sets or of the computational proce ss itself is often maintaine d through the application of formal cryp tographic protocols. For example, individual contributions can be encrypted to co nceal them from other participants dur ing training. One such solution, namely homomor phic encryption (HE) (Gilad-Bachrach et al. 2016; Hesamifard, Takabi, and G hasemi 2017) allows the federation to run the entire training procedure on encrypted data and only decryptin g the end result of the computation. Another solution, namely secure multi-party computation (SMPC) (Rouhani, Riazi, and Koushanfar 2018; Mohassel and Zhang 2017) offers the fe deration an option of masking their contributions through splitting them into a number of encrypted shares, which, unless aggregated with the agreement of a quorum of particip a nts (sometimes all participants), do not allow another client to learn anything about the model u pdate. This fact also renders SMPC an attractive option for distributed data governance, whe re only a given number of parties are able to reveal the data that is shared between them. Most DML algorith ms c an be augmented with either of these mechanisms to provide the data owners with guarantees of secrecy, and, as a con sequence, of input privacy. However, it is important to outlin e that typically such mechanisms usu a lly come with a p e rformance overhead that can affect the training tim e and/or the utility of the ﬁnal model (Kaissis et al. 2021; Mohassel and Zhang 2017). In contrast, enforcing output p rivacy requires limiting the informa tion that can be derived about an individual from the data used for model training. While DML ca n guarantee data governance, it does not in itself offer any meaningful privacy guaran tees to the data owners. Therefore, the over-reliance on DML protocols alone, and their further adoption without the inclusion of formal privacy-preserving mechanisms must be v iewed critically. Differential privacy (DP) (Dwork et al. 2006), which allows to objectively quantify and bound the amount of information that can be inferred from the training data, has established itself a s the gold standard of formal privacy protection. DP allows data owners to perform model training while offering the ind ividuals whose data is used to train the model, a quantiﬁable privacy guarantee in the form of a privacy budget. This privacy budget c a n be th ought of as fungible, because it can be expended through model training. After an individual’s privacy budget is exhausted, no further interaction with their data is permitted. The comb ination of DML with DP and cryptographic tools can equip the federation w ith the means of ascertaining governance as well as input and output priva cy. In addition to the c oncepts of privacy and security outlined above, we additionally need to consider security. In general, security is a property of a protocol or system where the protocol or system as-a-whole c annot be threatened by an adversarial actor. Security is thus not just complementary to privacy and secre cy, but a prerequisite for the design of any private and secure system. For instance, the physical security of the buildings in wh ic h computational equipment is housed is paramo unt to ensuring that these systems cannot be ta mpered w ith or destroyed. Moreover, a series of software and hardware measures are employed to safeguard the security of the learning protocol. For example, technologies such as transport layer security (TLS) are deployed to encrypt the data in transit and protec t it against adversaries who could intercept data packets and obtain potentially sensitive information (so-called man-in-the-middle attacks). In general, the secur ity of the system concerns all factors that can be exploited by adversaries that a re covered in the ST framework and beyond. I ssues such as phy sical unwarranted access to the d a ta owner cannot be mitigated regardless of the privacy measur e s deployed, as these are c overed under the n otion of physical security. Similarly, issues such as attacks that are not exploiting/targeting the DML models directly (e.g. malicious hackers that attempt to steal the d a ta set through exploitation of the site itself rather than the trained model) do not lie within the scope of the ST framework. In or der to verif y that the model is trained well on the underlying learning task, the federation needs to e stablish a method to ascertain that the data of the federation was used faithfully and the training protocol was not subverted by any party. Cur rently, no published DML implementation support this notion of output veriﬁcation, as it co mes in direct conﬂict with the notions of privacy described above, as such veriﬁcation could entail inspection of the training protocol an d –as a result– of sensitive data supplied by individual participants. One solution that c a n be employed for this task can build upon techniques from the veriﬁable computing (VC) doma in to enable the actors of the federation to certify the resu lts they compute. VC encompasses methods that can be used to attest that a given computatio n’s result was produced by a speciﬁc instance of an algorithm, matching its dependencies to the very bit, as pre-agreed upon between the prover, who ru ns the computation, and the veriﬁer, who seeks to obtain guara ntees of integrity of the computa tion, i.e. that the result of the computation was no malformed and represents an ho nest processing of inpu ts by the algorithm. There exist two main strategies of implementations of VC in the context of ML: one involves spe cial hardware embodied as trusted execution environment (TEE) (Chen et al. 2020; Lee et al. 2020a) and relies on the hardware properties in conjunction with a mechanism termed remote attestation (RA) (Ali, Nauman, and Jan 201 8) to fulﬁl the veriﬁable trait of the execution. We note that TEEs (also term ed secure enclaves) can also be used fo r end-to-end encrypted computations, thus fulﬁlling the r ole of an input pr ivacy mechanism. The second form of VC tha t has gained traction over the past few year s is entirely software-ba sed and relies on cryptographic proofs. A popular form of cryptographic proof, namely zero knowledge succinct non-interactive arguments of knowledge (zk-SNARKs) (Bitansky et al. 2014), gained signiﬁcant interest from the research community and has been previously leveraged in the context of neural network veriﬁcation (Lee et al. 2020b; Weng et al. 2021). Augmented with these mechanisms, DML is able to guaran te e that the com putation was performed faithfully. We note that –althoug h veriﬁcation of correctness is possible with these techniques– the veriﬁcation of data quality is much more complex. For instance, a number of measures may b e employed to gauge data quality, such as the reduction in model uncertainty obtained through each individual data sample. Moreover, members of the federation may agree upon metrics other tha n data quality (e.g. the sp e ed with which a computational result is returned), to determine participant reimbursement. In a setting where several parties collaborate to solve a learning task, one cannot easily anticipate the intentions of each individual participant, as some actors might be actively attempting to subvert the training protocol. As a result, attacks on the utility of the resulting model are possible in decen tralised learning, such as model poisoning (Fang et al. 2020; Yang et al. 2017) or backdoor attacks (Bagdasaryan et al. 2020; Bagdasaryan and Shmatikov 2020). Thu s, paradigms such as FL require substantial augmentations to mitigate potential adversarial inﬂuence from being incorporated into the jointly trained model. This inﬂuenc e can take multiple forms ranging from colluding with other participants to submitting malformed updates or not submitting model updates at all, causing the training procedure to halt indeﬁnitely. The ability to track the source of malicious interference is ther efore a required component of trustworthy DML protocols. Moreover, as described above, it may also be required to track properties of the data p ertaining to its quality, or about the computation, such as its speed or the result of a veriﬁcation workﬂow. (Warnat-Herresthal et al. 2021), propose the utilisation of a permissioned blockchain to track the contributions of each individual data owner, discouraging them from submitting intentionally malformed model updates. Concretely, model aggr egatio n and the selection of the aggregation server for the round (so-called leader) occur thro ugh the execution of blockchain-backed smart contracts. In general, the reliance on blockchains in DML can thus allow the federation to obtain a decentralised, immu ta ble transcript o f ind ividual contributions to the tra ining protocol. This c an include the information about how eac h individual’s contribution inﬂuences the resulting model, the time it took them to produce each contribution e tc . Such information is essential in the identiﬁcation of malicious actors whose contributions (or deliberate lack thereof, which can result in protocol halting), affect the utility of the jointly trained model. The immutability of blockchains additionally prevents suc h actors from concealing their contributions. Moreover, DML protocols combining the notions of data governance and a ccountability within the same distributed learning system have been developed (Passerat-Palmbach et al. 2020). While acc ountability approaches undoubtedly increase the overall trust level, their contribution m ust be weighed against the cost of developing and deploying such c omplex solutions. A potential solution for future generations of truly trustless (i.e. those that can function when 50% of all par ties are assumed to be malicious) D ML systems would be to beneﬁt from leveraging a pub lic blockchain network such as the Ethereum main network . On such truly decentralised public infrastructure, no participant in the federation can tamper with the data stored nor with the execution of the smart contracts. In this work, we study the vulnerabilities associated with the na¨ıve utilisation of DML. We deduce that most DML protocols cannot be r elied upon unless a c companied by additional mechanisms enhancing trust between participants. Unfortunately, much published literature implies that FL and other DML protocols provide privacy protections. It is paramount to note that a ll that is offered by DML itself is –at best– a semblance of priva cy. As a result, promoting the utilisation of DML without any formal measures of input or ou tput privacy protection in place can lead to the disclosure of sensitive information, potentially causing irreparable damage. As evident from the comparison between the compone nts of the ST framework and the current abilities of published DML systems, most frameworks can currently only satisfy a subset of ST requirements. Moreover, we contend tha t the terms privacy, secre cy and secur ity should not be used interchangeably, and tha t all are required for ensuring the trustworthiness of DML systems. As noted by (Carlini et al. 2021) “There is no room for error in privacy”, showing that the misinterpretation or entanglement of concepts can result in the violation of trust b etween p arties of DML protocols. We summarise these (along with other) deﬁnitions in Table Finally, we prop ose the following recommendations that arise from our work: 1. Future research in the ﬁeld of trustworthy AI (particu larly in the ﬁeld of private ML ) should agree upon and adhere to terminological guidelines. For instance, neither systems without formal privacy guarantees (e.g. FL) nor systems only offering input privacy (e .g. encryption) should be haphazardly termed privacy-preserving (or similar). 2. Only system s adhering to all aforementioned principles should be term e d trustworthy, to avoid negative consequences associate d with over-reliance on p rotocols tha t were not design e d with fundamental privacy requirements in mind. Such systems should undergo external auditing in order to verif y their correctness e.g. through the me ans of for mal network certiﬁcation (Lecuyer et al. 2019), 3. Most existing DML solu tions already offer promising foundations for emergence of private reliable ML systems, but they must be augmented with additional mechanisms in order to be able to guarantee bo th the privacy of the participants and the robustness of the jointly tr ained models. We emphasise the importance of education pertaining to these systems, both for experts and laypeople, in this regard.