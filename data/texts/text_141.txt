This paper provides the rst formalisation and empirical demonstration of a particular safety concern in reinforcement learning (RL)-based news and social media recommendation algorithms. This safety concern is what we call “user tampering” – a phenomenon whereby an RL-based recommender system may manipulate a media user’s opinions, preferences and beliefs via its recommendations as part of a policy to increase long-term user engagement. We provide a simulation study of a media recommendation problem constrained to the recommendation of political content, and demonstrate that a Q-learning algorithm consistently learns to exploit its opportunities to ‘polarise’ simulated ‘users’ with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. Finally, we argue that given our ndings, designing an RL-based recommender system which cannot learn to exploit user tampering requires making the metric for the recommender’s success independent of observable signals of user engagement, and thus that a media recommendation system built solely with RL is necessarily either unsafe, or almost certainly commercially unviable. Broadly speaking, recommender systems are algorithms which oer the ability to lter through large pools of some kind of entity to identify and recommend particularly relevant entities to an individual or group [6]. Among other domains, recommender systems have been widely deployed to recommend movies and videos, music, and goods on e-commerce platforms. One of their most signicant areas of application is within news and social media platforms, where they are used to provide users with some content of interest. We refer to recommender systems in this specic area as ‘media recommender systems.’ One emergent approach to implementing recommender systems involves treating the recommendation problem as a Markov Decision Process (MDP) and applying reinforcement learning (RL) to the recommendation task. While this approach was suggested some time ago [24,26,27], lately it has garnered new interest, due to the emergence of ‘Deep RL’ and its ability to handle larger, more complex problems [15,31,32]. Research has begun to explore the applicability of Deep RL-based recommendation in the news and social media space [23,33]; this work has demonstrated signicantly increased user engagement and activeness relative to other leading approaches to the recommendation problem, which are predominantly: (i) ‘static’ machine learning approaches [2,6,9,16,18]; and (ii) contextual Multi-Armed Bandit approaches [14, 28–30]. As RL research continues to advance and RL techniques become more eectively applicable at scale, RL-based recommender systems’ impact in the domain of media recommendation will likely continue to grow, and may even eclipse that of the current dominant techniques. Indeed, it is already the case that leading social media platforms like Facebook are undertaking this research & development [10, 17]. The social and ethical implications of media recommender systems have also recently received signicant research attention [1,13,19,20,25]. A recent survey on the subject enumerated the main areas of concern as ‘Biased/unfair recommendations,’ ‘Encroachment on individual autonomy and identity,’ ‘Opacity,’ ‘Questionable content,’ ‘Privacy’ and ‘Social manipulability and Polarisation’ [19]. This paper particularly focuses on the last of these concerns. We argue that the risks posed by an RL-based approach in the space of manipulation and polarisation require serious attention. It has been theorised – but neither demonstrated nor formalised in previous work, to the best of the authors’ knowledge – that a particularly problematic variant of this concern has the potential to emerge when the recommendation problem is framed as a MDP and RL is employed to solve it [21,22]. The basic idea is that the recommendation algorithm could learn to make recommendations which inuence users into becoming easier, more predictable targets for recommendation, as this would heighten the algorithm’s success in the long term. This paper provides the rst concrete formalisation – and experimental verication of the potentiality – of the just-described issue, which we call “user tampering.” This paper makes two core contributions to the literature on the ethics and safety of media recommender systems. Firstly, we formalise the notion of user tampering as a potential safety issue specic to RL-based media recommenders; we do this by using the Causal Inuence Diagram techniques proposed by Everitt et al. [7] to extract the specic causal phenomenon enabling RL-based recommenders to learn to manipulate users’ preferences, opinions and interests. Secondly, we simulate a simple media recommendation problem. We show that a standard Q-learning algorithm can learn to exploit user tampering, by developing a policy of making recommendations that aect our simulated users’ content ‘preferences’, before capitalising on those eects in later recommendations. While our simulation occurs on a signicantly smaller scale than a real recommendation problem scenario, it nonetheless arms the user tampering theory and so has signicant implications for actual RL-based media recommender systems. In this section, we rst generically frame the media recommendation problem as a Markov Decision Process (MDP), and then use Causal Inuence Diagrams (CIDs) to extract the relevant causal dependencies that particular variables exhibit under this model. For some background on CIDs, see Appendix A. We have endeavoured to keep the MDP as general as possible, while also incorporating design insights from recent work in implementing RL-based media recommender systems [23, 33]. We now build up a model of the media recommendation problem as a Markov Decision Process ⟨𝑆, 𝐴,𝑇, 𝑅,𝛾⟩; this is the problem interpretation upon which RL algorithms are based. Here, we assume that articles/posts are represented in a parameterised form, i.e. as an𝑛-dimensional vector, where we have identied𝑛numeric characteristics of the article by which it can be identied (such as topic, stance, author, etc.). This is in opposition to representing articles atomically, which is undesirable; it rules out the use of Deep RL, without the use of which the media recommendation problem is untenably large for RL at industrial scales. Say that we begin by dening the MDP’s individual elements as follows: • 𝑆is a set of states. The specic denition of a state could take any number of denitions in a particular implementation, but we generally dene it here as a collection of data points which can be divided into ‘necessary’ and ‘optional’ elements: –Necessary: The state must contain a representation of how the recommender’s recent recommendations have performed. For example, this could be a collection of|𝑛 × 𝑚| datapoints, representing users’ aggregate clicks on recommended items across𝑛categories over𝑚dierent interpretations of ‘recent history’ e.g. the last 1 hour, 6 hours, 1 day, etc. (similar to the approach taken by Zheng et al. [33]). This inclusion is ‘necessary’ as without it, the theoretical advantage associated with using RL in the rst place would be lost. Learning policies which capitalise on future as well as current opportunities for reward relies upon including this information in the state representation. –Optional: Other observable features which inform about this user’s preferences, including the activity of their ‘friends’, contextual features such as time of day, and more. • 𝐴is a set of actions. An action is an𝑛-dimensional vector, representing the characteristics of one article that could be recommended to the user. This could be extended to dening an action as recommending a xed-size set of articles to the user, as the characteristics of all the articles could simply be aggregated into the vector. • 𝑅is the reward function, mapping the agent’s activity to numeric rewards to give it feedback about the ‘goodness’ of that activity. Commonly, recommmender systems could base reward on observable indicators of engagement such as a click, Figure 1: A naive CID of the media recommendation problem. ‘like’, or so forth. Depending on the specic implementation and the denition of the state space, this function could have multiple dierent signatures, including𝑅:𝑆 → R, 𝑅 : 𝑆 × 𝐴 → R and others. • 𝑇:𝑆 × 𝐴 × 𝑆 → [0,1]is the transition function. It returns the probability of an agent reaching a particular ‘successor state’ 𝑠, given it has taken a certain action 𝑎 from a state 𝑠. • 𝛾 ∈ R is the discount factor for future rewards. In terms of these variables, the recommendation problem can simply be described as an agent taking an action𝑎at time𝑡, which will transition the system from a current state𝑠to a successor state 𝑠with probability𝑇 (𝑠, 𝑎, 𝑠). The agent would thereafter be rewarded with the value𝑅(𝑠), and then another action would be chosen at time 𝑡 + 1, and so forth. If we were to naively model this MDP’s causal structure as a CID without further consideration, we would reach a representation similar to that shown in Figure 1.At a time-step𝑥, the distribution over possible current states is represented by𝑆. Once the actual value of the state as of time𝑥is observed, this will constitute the only information available to the agent in its selection of an action at𝐴. The distribution over possible states in𝑆is then dened by𝑇, given𝑆and𝐴. Finally,𝑅represents the distribution over the reward value achieved from the action taken at𝐴. We have assumed an interpretation of the reward function as𝑅:𝑆 × 𝑆 → Rhere, where reward is determined by a comparison of two consecutive states, as this will provide sucient information to deduce the success of the most recent action (recommendation). However, a simple thought experiment can demonstrate that this CID underspecies the causal relationships in the actual problem, by leaving key variables external to the MDP unacknowledged. Consider the following: Alice and Bob are two university students who have just created accounts on some media platform, who have so far both been recommended the same three articles about the student politics at their university, and who have both clicked on all three articles. Within our general denitions, it is quite plausible that the states of the system have been identical thus far from the agent’s perspective. However, what if Bob is uninterested in politics and is just clicking on the articles because his friends feature prominently in the cover photos of all three, whereas Alice is clicking out of a genuinely strong interest in politics, including student politics? If the recommendation to both Alice and Bob at the next time-step – say,𝐴– is an article about federal politics, it is intuitively untrue that the distribution over possible states at𝑆is the same; Alice is surely more likely to observably engage with this content. Evidently, a random variable exogenous to the MDP must be introduced to properly model the causal properties of the true system. Informally, we argue that this variable can be characterised as the preferences/opinions/interests of the specic user to which the agent is recommending media. Given that it is exogenous to the MDP, it is unnecessary to assign this variable a specic form, but we assign it the symbol𝜃for the purposes of our causal modelling. That is to say, the exogenous variable that is the user’s preferences etc., at time𝑥, is represented as𝜃. We do not enforce any Markov assumptions on𝜃; that is, it may be dependent on the variable’s value at multiple, or all, previous time-steps’ values. A vital observation to make at this stage is the causal relationship between𝜃and𝑆. As the example above demonstrated, without acknowledging the eect of𝜃, the real distribution over states 𝑆cannot be explained. So, there exists a causal link between the former and latter variables. Moreover, the specic elements of𝑆 whose distribution would otherwise be unexplainable are precisely those which we explained are necessary inclusions in the denition of𝑆in order to produce the desirable properties of an RL approach. So, this link cannot be removed by any practical redesign of the state space. Finally, it is crucial to recognise that an inuence link will also exist between𝐴and𝜃; intuitively, this just reects the reality that a user’s consumption of information will update and aect their preferences, interests, etc. going forward. Given that𝜃is exogenous and given no specic formal interpretation, it is not our claim that there is a precise model available for how 𝐴 aects the distribution over possible values of𝜃; rather, we are just acknowledging the existence of the dependency. It is evident that the CID of Figure 1 is in need of revision. If we introduce the exogenous variable to the system, without changing any other denitions, we arrive at the CID shown in Figure 2. This CID, we argue, more completely captures the actual causal dynamics of the Media Recommendation MDP. We note that previous literature has acknowledged a similar causal structure to the recommendation process [12]; however, this was not formulated in the CID framework that we have used, which permits sophisticated graphical analysis of the kind developed in the next section. Depending on the exact design of the MDP, there are variations on this CID that could exist; see Appendix B for an example of the causal structure that is implied if the designer wishes to expand the reward function to account for observations that are not captured in the state representation. However, such variations have no eect on the role and inuence of𝜃; its links from the preceding action and to the succeeding state necessarily remain part of the model’s causal structure. Given that these causal relationships are exactly where we intend to focus our analysis in the next section, the CID in Figure 2 is a suciently general representation for our needs going forward. Figure 2: A CID of the media recommendation problem, extended to include the exogenous variable aecting state transitions. In this section, we use the CID formulated in the previous section (Figure 2) to analyse the safety of the RL-based approach to media recommendation, specically with respect to the high-level concerns of user manipulation and polarisation. After introducing the phenomena of ‘instrumental control incentives’ and ‘instrumental goals’ from the RL incentive analysis literature, we show that in the CID, an instrumental goal exists for the agent to manipulate the expected value of the exogenous variable𝜃. This lends a concrete, formal interpretation to the (formerly only hypothesised) safety issue that we have called ‘user tampering’. An ‘instrumental control incentive’ (ICI) is a graphical property of CIDs introduced by Everitt et al. [7]. An ICI exists on a Structural Node𝑋if it lies on a path in the CID that begins at a Decision Node and ends at a Utility Node, and basically implies that the action chosen at the former aects the expected utility at the latter through aecting the distribution over values at 𝑋 . The importance of ICIs is that they provide a simple graphical criterion that can establish either the potential presence or the categorical absence of a so-called ‘instrumental goal’ on certain events in a RL problem [8]. An RL agent is said to have an instrumental goal to inuence the distribution at a Structural Node𝑋in a certain way if it has an ICI on𝑋and that particular inuence increases the expected reward accumulated by the agent – in short, if it has both the ability and a reason to aect the distribution at 𝑋 . In the CID we have presented, intuitively there are a subset of Structural Nodes upon which instrumental goals are desirable – those representing the set of random state variables{𝑆|𝑡 ∈ N}. This reects the very premise of RL – we want the agent to be able to manipulate the state of the system in pursuit of ‘good’ states (in our context, these are states where many of its recent recommendations have been well-received by the user). As such, any path through the CID from a Decision Node to a Utility Node that only passes through Figure 3: An annotated version of the media recommendation CID for state-based rewards. An example of an undesirable causal path introducing an instrumental control incentive on 𝜃is shown in bolded red. random state variables (e.g. [𝐴→ 𝑆→ 𝑅], or [𝐴→ 𝑆→ 𝑆→ 𝑅]) only involves intended and safe instrumental goals. However, other paths from Decision to Utility Nodes exist in the CID. Specically, there are paths which visit the exogenous random variables – for example, [𝐴→ 𝜃→ 𝑆→ 𝑅]. Figure 3 traces this path in the CID. Clearly, there is an ICI on𝜃, or on any other variable in {𝜃|𝑡 ∈ N} that appears in similar paths. Given this, if the agent stands to generate higher amounts of reward by recommending to a user with particular preferences, opinions and interests (represented by𝜃), then the agent may have an instrumental goal to inuence𝜃in that direction, as this may lead to higher expected reward in the long term. In essence, the CID’s admission of an ICI on at least one node in{𝜃|𝑡 ∈ N}is precisely the necessary graphical condition that allows the manipulation of users to emerge as an instrumental goal for an RL agent. If such an instrumental goal does exist (i.e. if by aecting users’ preferences/opinions/interests, the agent can increase its expected reward), then we can expect that an arbitrarily capable RL agent would learn to act on that instrumental goal – we say that this makes user tampering a ‘learnable’ phenomenon. Denition 1. User tampering is a ‘learnable’ phenomenon for an RL-based media recommendation algorithm i it has an instrumental goal to aect at least one of the variables in {𝜃|𝑡 ∈ N}. Importantly, however, an instrumental goal on aecting some variable in{𝜃|𝑡 ∈ N}does not imply that an arbitrary RL agent will learn to aect the user in the way necessary to increase its expected reward; it only implies that it could learn this behaviour. So, user tampering’s learnability in some model is a necessary but insucient condition for user tampering actually manifesting in an RL agent’s learned policy. It is therefore useful to introduce a second denition relating to user tampering, such that we can separate our discussions of its theoretical learnability from our discussions of it actually manifesting in a given recommender’s policy. We introduce a second denition to address this: Denition 2. An RL-base d media recommendation algorithm ‘exploits’ user tampering i there exists a state𝑠such that𝜋 (𝑠) = 𝑎 and𝜋(𝑠) ≠ 𝑎, for the algorithm’s actual learned policy𝜋, and the hypothetical policy𝜋that the same learning process would have produced in a world where 𝐴⊥⊥ 𝜃. In Appendix C, we relate these formalisms to a dierent form of ‘tampering’ in RL: ‘Reward Function (RF)-tampering.’ We explain that although the two phenomena seem to describe similar highlevel issues, the issues are quite separate on a causal level; and that these dierences rule out the transferral of promising solutions for the RF-tampering issue to the user tampering context. In this section, we empirically analyse the user tampering phenomenon formalised in the previous section. Firstly, we introduce a simple abstraction of the media recommendation problem, which involves simulated users and a user tampering incentive inspired by recent empirical results about polarisation on social media. Then, we present a Q-learning agent intended to mimic the Deep Qlearning algorithms used in recent media recommendation research, and train it in this environment [23,33]; we show that its learned policy clearly exploits user tampering in pursuit of greater rewards. We begin by introducing our example problem. In this problem, we will have a recommender agent makeℎsequential recommendations of ‘political posts/articles’ to a user. At each timestep𝑡, 0≤ 𝑡 ≤ ℎ, the agent chooses one of three ‘sources’ from which to recommend; the rst source being consistently left-wing in its perspective, the second being consistently centrist, and the last being consistently right-wing. For the purposes of our example, we assume a denition of the exogenous parameter𝜃introduced in Section 2 – recall that the agent does not explicitly model this variable, but we need to here for the purposes of constructing our simulation. We dene𝜃as a tuple of three probabilities as of time𝑡, i.e.Θ= {(𝜃, 𝜃, 𝜃) ∈ R| ∀𝑥 ∈ {𝐿, 𝑅, 𝐶}.𝜃∈ [0,1]}. For some arbitrary user, their probability𝜃represents their probability of clicking an article from the left-wing source if recommended it; the same can be said of 𝜃for the centrist source, and𝜃for the right-wing source. We say that a user is initially “right-wing” i𝜃> 𝜃∧ 𝜃> 𝜃, and “left-wing” i 𝜃> 𝜃∧ 𝜃> 𝜃. Finally, we include a simple environmental dynamic whereby users who are recommended content from a source that is politically opposed to their own wing gradually become more polarised in favour of their own wing. This is inspired by recent research into user polarisation on social media, which has demonstrated that showing people who identify with one wing of the political spectrum volumes of content from the opposing wing can often increase user polarisation [4, 5]. We do not at all claim that this completely simulates the polarisation phenomenon described in the works cited just above; it is obviously a signicant simplication. However, our intention here is not to simulate this eect accurately, but rather to create an environment which allows the hypothesised eect of user tampering to be tested by introducing a causal eect that could be used by the agent as part of an instrumental goal (while still having its simplied dynamics grounded in actual sociological results). The full denition⟨𝑆, 𝐴,𝑇, 𝑅, 𝛾⟩of the media recommendation MDP, as well as the precise implementation of the ‘polarisation’ eect we have just described, is provided in Appendix D. Next, we train a Q-learning agent in this environment and show that it learns to perform user tampering on our simulated users. Some extra specications are needed to operationalise this environment.ℎwas set to 30, and the probabilities dening the exogenous variable𝜃were limited to maximum values of 0.75.𝑝, the ‘polarisation factor’ by which a user’s subsequent probability of clicking on content from their aligned source would increase after being recommended a post from the opposing source, was sampled from the uniform distribution U(1.01, 1.10), making E[𝑝] = 1.055. We also dened, for the purposes of our experiment, a population of ve ‘users’ with varying preference proles. This contained: • A ‘strong left’ user with 𝜃= (0.4, 0.1, 0.1) • A ‘moderate left’ user with 𝜃= (0.3, 0.25, 0.1) • A ‘centrist’ user with 𝜃= (0.2, 0.4, 0.2) • A ‘moderate right’ user with 𝜃= (0.1, 0.25, 0.3) • A ‘strong right’ user with 𝜃= (0.1, 0.1, 0.4) We trained a Q-learning agent in this environment, with a user randomly selected from the population to provide the initial value of𝜃for each episode.Non-deep Q-learning was used for training, in spite of deep Q-learning being the more viable approach at industrial scales; this was a deliberate choice, because unlike deep Q-learning, non-deep Q-learning provably converges towards the optimal policy for the problem.Nonetheless, for consistency with a real Deep RL application, we have still modelled the state space in a parameterised fashion that is amenable to those algorithms. 4.2.1 Results. For each of the ve users in our population, we provide two plots based on 10000 evaluation episodes with the user (using the policy learned from the training process described above). Respectively, these two plots estimate: •The probability with which the learned policy chooses each action, at each time-step of the problem, by taking the perepisode average of each choice’s total frequency. •The expected reward accumulated up to and including each time-step𝑡, 0≤ 𝑡 ≤ ℎ. For context, we plot this against the expected reward accumulated by: Figure 4: Evaluation of the policy learned with Q-learning for each member of our sample user population. –A recommender that makes uniformly random recommendations at each time-step. –A recommender that follows a simple multi-armed banditesque policy, which provides a ‘baseline’ of a good policy. This policy makes random recommendations for the rst third of the episode, but then operates like a multi-armed bandit, by always recommending from the source which has the highest mean reward in the episode so far. Figure 4 displays these plots for each simulated ‘user’ we dened above. These results possess several interesting properties: • For all users except the Centrist user, the exploitation of user tampering in the learned policy is clear. Focusing on the strategy plots for the two ‘left-wing’ users, we can see a clearly dominant strategy has emerged, where: –The recommender attempts to prole the user and their preferences, by testing their reaction to centrist and leftwing content (roughly the rst quarter of the episode). –The recommender predominantly recommends right-wing content in spite of its low expected reward, which will tamper with the user’s preferences and increase the expected reward from subsequent left-wing recommendations (roughly the second quarter of the episode). –The recommender predominantly recommends left-wing content to the (now more) left-wing user, maximising the high expected rewards that action will now oer (roughly the second half of the episode). Given user tampering’s learnability in this problem and the expected rewards from right-wing content here (low), the recommender’s propensity to nevertheless heavily recommend right-wing content before switching to left-wing recommendations for the remainder of the episode is a clear exploitation of user tampering. Moreover, the inverse behaviour has been learned for right-wing users – the model is not blindly trying to polarise all users to the left, but has developed a sophisticated policy for identifying and exploiting the causal relationship between its actions and the user’s exogenous variable. Further evidence to this eect is given by the policy for the ‘centrist’ user – here, the plot shows clearly that the recommender has recognised that its actions have no discernable causal impacts by which the user could be tampered with, and so makes recommendations which are proportionate to the user’s initial preferences. • The agent heavily exploits user tampering even though we were able to generate similar cumulative rewards with our crude ‘baseline’ policy.This adds weight to the safety concerns with respect to user tampering. It indicates that there exist other policies which do not exploit user tampering (although they may make a handful of ‘polarising’ recommendations by chance) and which oer similar rewards to the one that the recommender learned; nonetheless, over several iterations of retraining, the policy consistently converged to the policy we have presented here (with small natural variations). This implies that in this environment, the unsafe policy is not only learned occasionally, but presents a likely direction of convergence for the learning algorithm. It is also worth establishing that the exploitation of user tampering in the learned policy was robust to simulated users not encountered during training. We generated the same policy plots for the recommender over 10000 evaluation episodes spent recommending to each user in a new, ‘unseen’ population: an ‘extremely left’ user with𝜃= (0.5, 0.05, 0.05), an ‘extremely right’ user with 𝜃= (0.05, 0.05, 0.5), a ‘left anti-centrist’ user with𝜃= (0.35, 0.05, 0.2), and a ‘right anti-centrist’ user with 𝜃= (0.2, 0.05, 0.35). Figure 5: Action probabilities at each time-step for each user in the ‘unseen’ population. These results are shown in Figure 5. Although these specic users were never encountered during training, the same unsafe strategies appear here; the three phases of user proling, then polarisation, and nally preference satisfaction are clearly visible. These results, in combination with the previous sections’ formalisations, justify the claims that user tampering is both almost unavoidably learnable for commercially viable media recommender systems built entirely with RL, and potentially highly unsafe in its eects. Specically, the primacy of user engagement in content recommendation makes achieving complete safety from user tampering inconvenient at best, and impossible at worst. Appendix E unpacks this claim further for the interested reader. This paper has substantiated concerns about the risks of emergent RL-based recommender systems with respect to user manipulation and polarisation. We have formalised these concerns as a causal property – “user tampering” – that can be isolated and identied within a recommendation algorithm, and shown that by designing an RL-based recommender which can account for the temporal nature of the recommendation problem, user tampering also necessarily becomes learnable. Moreover, we have shown that in a simple simulation environment inspired by recent polarisation research, a Q-Learning-based recommendation algorithm consistently learned a policy of exploiting user tampering – which, in this context, took the form of the algorithm explicitly polarising our simulated ‘users.’ This is obviously highly unethical, and the possibility of a similar policy emerging in real-world applications is a troubling takeaway from our ndings. Due to a combination of technical and pragmatic limitations on what could be done dierently in RL-based recommender design, it is unlikely that commercially viable and safe recommenders based entirely on RL can be achieved, and this should be borne in mind when selecting future directions for advancement in media recommendation research & development.