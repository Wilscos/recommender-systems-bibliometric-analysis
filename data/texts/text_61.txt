Noname manuscript No. (will be inserted by the editor) The Interplay Between Automatic Generation and Human Exploration Abstract Automatically generating test cases for software has been an active research topic for many years. While current tools can generate powerful regression or crash-reproducing test cases, these are often kept separately from the maintained test suite. In this paper, we leverage the developer’s familiarity with test cases ampliﬁed from existing, manually written developer tests. Starting from issues reported by developers in previous studies, we investigate what aspects are important to design a developer-centric test ampliﬁcation approach, that provides test cases that are taken over by developers into their test suite. We conduct 16 semi-structured interviews with software developers supported by our prototypical designs of a developer-centric test ampliﬁcation approach and a corresponding test exploration tool. We extend the test ampliﬁcation tool DSpot, generating test cases that are easier to understand. cases from their familiar environment. From our interviews, we gather 52 observations that we summarize into 23 result categories and give two key recommendations on how future tool designers can make their tools better suited for developer-centric test ampliﬁcation. Keywords Software Testing · Test Ampliﬁcation · Test Exploration · Test Generation · Developer-Centric Design Testing is an important (Whittaker et al., 2012), but time-consuming activity in software projects (Beller et al., 2015a,b, 2019). Automatic test generation aims to alleviate this eﬀort by reducing the time developers spend on writing test cases. The software engineering community has created a plethora of powerful tools, that can automatically generate JUnit test cases for software projects written in Java. For example, a widely known tool is EvoSuite (Fraser and Arcuri, 2011), which generates test cases from scratch using search-based algorithms. It starts from a group of randomly generated test cases and optimizes them by mutating their code and combining them with each other. This paper focuses on test ampliﬁcation, a technique that automatically generates new test cases by adapting existing, manually written test cases (Danglot et al., 2019a). The state-of-the-art test ampliﬁcation tool DSpot (Danglot et al., 2019b) mutates the setup phase of manually written test cases and generates new assertions to test previously untested scenarios. For both EvoSuite and DSpot, studies have shown that the tools are eﬀective in generating or extending test suites to reach a high structural coverage and mutation score (Danglot et al., 2019b; Fraser and Arcuri, 2011; Rojas et al., 2015; Serra et al., 2019). son et al., 2011), reproduce crashes (Derakhshanfar et al., 2020b,a), uncover undertested scenarios (STAMP, 2019b) and generate test data (Haq et al., 2021). For these use cases, it is often suﬃcient to keep the generated test cases separate from the manually written and maintained test suite (STAMP, 2019b; Nassif et al., 2021). This separation is reinforced by several hard-tosolve challenges that limit the understandability of the automatically generated tests, such as their readability (Daka et al., 2015; Grano et al., 2018) or generating meaningful names (Zhang et al., 2016; Daka et al., 2017), or documentation (Roy et al., 2020; Panichella et al., 2016; Bihel and Baudry, 2018). written ones. This opens up the chance to generate test cases that are easier to understand by developers, as they are likely familiar with the original test case, which the ampliﬁed test case is based on. In this paper, we want to leverage this aspect and take a look at generating ampliﬁed test cases that developers can take over into the manually maintained test suite as if they would have written them themselves. To describe this kind of test generation we use the term developer-centric, as the developer accepting the test case is central for this kind of test generation. maintained test suite also fulﬁll several further typical uses for developer tests. For example, as a form of executable documentation (Hoﬀman and Strooper, 2003; Beck, 2003; Kochhar et al., 2019), or to locate the fault that causes a failing test by understanding the test in question (Panichella et al., 2016). Automatic test generation is, for example, used to detect regressions (Robin- The ampliﬁed test cases created by DSpot are closely based on manually Generated test cases that are accepted by developers and are part of the suite, the interaction of the developer with the test ampliﬁcation tool in which they review the proposed ampliﬁed test cases is critical. In past projects, users of DSpot reported that the tool was complex to conﬁgure and they had to wait long for the tool to ﬁnish and for them to see results (STAMP, 2019b). There is little support that guides developers through the list of generated test cases so they can eﬀectively judge whether to keep or discard a newly ampliﬁed test case. To address these issues and realize developer-centric test ampliﬁcation, we embed the test ampliﬁcation tool in a so-called test exploration tool: cation approach with a test exploration tool, we introduce an exemplary use case, which is also illustrated in Figure 1: ampliﬁcation approach to be successful with developers. As automatic test ampliﬁcation is not widely used and to prevent re-studying the already known issues in current tools, we develop prototypes of a developer-centric test ampliﬁcation approach and a corresponding test exploration tool. To motivate the design choices we take for our prototypes, we derive four design intentions from the test generation literature and the use case we propose for developer-centric test ampliﬁcation. Based on these intentions we revise the test ampliﬁcation process of DSpot to generate shorter, easier to understand test cases. Our cortest cases right from their integrated development environment (IDE). To provide ampliﬁed test cases that developers take over into their test A test exploration tool forms the interaction layer between the developer and the test ampliﬁcation tool. It lets the developer start the test ampliﬁcation tool, and later explore and inspect the diﬀerent ampliﬁed test cases. To illustrate how a developer would use a developer-centric test ampliﬁ- Hannah, a software developer, wants to expand the test suite of the industrial project she is working on to cover more functionality and give her conﬁdence that they are not breaking important behavior when changing something. As her management is constantly asking for new features, she is pressed on time and decides to use an automatic test ampliﬁcation tool to improve her project’s test suite. From her integrated development environment (IDE), she starts the test ampliﬁcation. The tool generates several new test cases for her and notiﬁes her that it is ﬁnished. Hannah inspects the test cases one by one directly from the test exploration tool integrated into her IDE. The exploration tool shows her the code of each new test case and where in the production code new instructions are covered. Hannah browses through the new test cases and if she is happy with any test case, she adds it to the test suite with one click of a button. After exploring all proposed test cases, she commits her changes and can lean back with the conﬁdence of a better tested system. In this paper, we investigate how we should design a developer-centric test Fig. 1: Overview of test ampliﬁcation with the help of a test exploration tool. are successful in supporting developer-centric test ampliﬁcation and uncover further aspects that should be addressed to realize it. readability of generated unit tests, the generated input data, and the generated assertions” (Almasi et al., 2017), while DSpot users found it diﬃcult to understand the generated test cases (STAMP, 2019b). Stemming from these observations, we investigate in our ﬁrst research question what developers ﬁnd important in the code and behavior of an ampliﬁed test case. The answers to this question give guidance on what factors in ampliﬁed test cases are relevant for developers to include the test cases in their maintained test suite. What are the key factors to make ampliﬁed test cases suited for developercentric test ampliﬁcation? should be designed to support developer-centric test ampliﬁcation. What are the key factors to make test exploration tools suited for developer-centric test ampliﬁcation? We conduct a qualitative study to explore which aspects of our prototypes In previous studies, developers using EvoSuite were “concerned about the Research Question 1 In our second research question, we explore how test exploration tools Research Question 2 oper with information beyond the test code itself. We already know from test review that developers are interested in understanding the code under test, as well as knowing the code coverage of test cases (Spadini et al., 2018). To deepen our insights into what information test exploration tools should make accessible to developers, we pose our third research question. What information do developers seek while exploring ampliﬁed test cases? using it. We also need to convey the value our tool brings to them. Therefore our fourth research question asks what value developers can gain from using developer-centric test ampliﬁcation. With the answers to this question, future tool creators know which beneﬁts they can focus on when they seek to convince users to start or keep using their tool. What value does developer-centric test ampliﬁcation bring to developers? with 16 software developers from varied backgrounds. The participants tried out our prototypes and provided us with rich insights on their impressions of our prototypes and how we could improve them to better ﬁt their needs. We group 52 recurring observations from the interviews in 23 result categories for our four research questions. During the discussion of these results, we identify two key recommendations on how we should design future developer-centric test ampliﬁcation tools. pliﬁcation. In short, we contribute: – two recommendations on how to design developer-centric test ampliﬁcation – a structured overview of the key factors to make ampliﬁed tests as well as – a reﬁned, developer-centric test ampliﬁcation approach, based on the DSpot – a developer-oriented test exploration plugin for the IntelliJ IDE In this paper, we aim to investigate the key aspects that make ampliﬁed test cases and test exploration tools suited for developer-centric test ampliﬁcation by conducting semi-structured interviews with software developers. To illustrate the concept of test ampliﬁcation to our participants and receive rich and concrete input, we want to let them try out a test ampliﬁcation tool A powerful capability of such test exploration tools is to provide the devel- Research Question 3 Creating a great tool alone is not enough for developers to appreciate Research Question 4 To answer these research questions, we conduct semi-structured interviews With this paper, we are taking a step towards developer-centric test amtools test exploration tools suited for developer-centric test ampliﬁcation test ampliﬁcation during the interview. A state-of-the-art test ampliﬁcation tool is DSpot (Danglot et al., 2019b), which was developed and evaluated during the European H2020 STAMP project. We adapt DSpot’s ampliﬁcation process based on the feedback from the reports of the European project (STAMP, 2019b) and the requirements posed by our use case of developer-centric test ampliﬁcation. To facilitate the interaction of the developer with the test ampliﬁcation we also design a prototype of a test exploration tool. prototypes, which is described in Section 3. The goal of this section is to clarify our reasoning behind the design choices we took and connect them to the existing literature and user reports about DSpot. We formulate four design intentions and present their connection to our design choices in Table 1. derstand the test cases and judge whether they check intended behavior. According to Meszaros, obscure tests that are diﬃcult to understand at a glance are an anti-pattern, as it makes tests harder to maintain and potential bugs in the test code more diﬃcult to detect (Meszaros, 2007). As automatically generating human-readable code is a hard problem to solve, readability and understandability of generated test cases are recurring topics in developer’s feedback: developers from the STAMP project stated about DSpot that it was hard to interpret the tool’s output and the “resulting tests were diﬃcult to understand for a human developer” (STAMP, 2019b). In some cases, the developers found the generated tests useful, but so hard to read that they wrote a corresponding test case themselves. Nevertheless, they were glad that DSpot pointed to real bugs and supported them in testing exceptional behavior in systems where only the optimal behavior was tested before. Several previous works in test generation were concerned with making the generated tests more understandable for developers (Panichella et al., 2016; Daka et al., 2017; Roy et al., 2020; Palomba et al., 2016; Rojas et al., 2015). This clearly shows that we should also take understandability into account while designing our prototypes. Therefore, our ﬁrst design intention is to generate test cases that are understandable for developers. Generate test cases that are understandable for developers of current test ampliﬁcation tools face obstacles that lead them to abandon automatic test ampliﬁcation. During the STAMP project (STAMP, 2019b), various industrial partners noted that DSpot takes very long to generate test cases. The conﬁguration is overly “complex because of the multitude of possible parameter values” which require experience to tweak correctly. The high eﬀort required by users was reported for other test generation tools, too. Previous studies of EvoSuite pointed out the high load on developers to inspect generated test suites and to decide if assertions in test cases are correct (Fraser et al., 2015). They also spend a lot of time analyzing generated test cases to In this section we discuss the inspirations leading to our design of both A central part of the load on developers comes from them having to un- Intention 1 (I 1) From literature and our own experiences, we understand that the users decide whether to improve or discard them (Fraser et al., 2015) as generated test cases tend to be less readable than manually written ones (Grano et al., 2018). That is why another intention leading the design of our prototypes is to decrease the load on the developers while they use test ampliﬁcation. Easy interaction to decrease the load on the developer should be fast enough so that developers can start it and receive new test cases in the same session. This means, for example, that they do not have to wait for an external build process to ﬁnish. We conjecture that this makes it easier for them to understand the results and the value of the test ampliﬁcation as they can include it directly while they work on improving their test suite. Fast enough for direct interaction ampliﬁed test case. They should see the test case as a useful addition when taken over into their test suite. Impact in this case could refer, for example, to the coverage, code quality, test code size or test suite runtime. We assume that understanding the impact is a pre-requisite to deciding whether the test is useful or not. The test exploration tool should make the impact and the quality of the ampliﬁed test cases clear so that the developers see the value that the automatic test ampliﬁcation brings them. Impact is clear to developers and they ﬁnd the tests useful cation of DSpot itself. Rather this shows the need for a layer in between the test ampliﬁcation tool and the developer that facilitates their interaction. This role is taken by the test exploration tool. 3 Bringing Test Ampliﬁcation to the Developer (IDE) Based on the intentions we deﬁned in Section 2, we develop prototypes for both a developer-centric test ampliﬁcation tool as well as a test exploration tool. We will use these prototypes during our interviews to illustrate a possible version of developer-centric test ampliﬁcation. In the following, we present our design and explain how our choices are motivated by the intentions we set. Table 1 gives an overview of these choices, which we mark throughout the text with (C n). Intention 2 (I 2) Another intention leading our design, is that the ampliﬁcation process Intention 3 (I 3) Lastly, it is our intention that the developers can grasp the impact an Intention 4 (I 4) Both (I 2) and (I 4) can not be addressed by modifying the test ampliﬁ- Table 1: The relation between our design intentions (I 1-4) and the design choices we take for our developer-centric test ampliﬁcation and exploration prototypes (C 1-15). clariﬁes why we choose to base our developer-centric test generation on this technique. We describe how we adapt DSpot to generate test cases that are better suited to be read by developers. Further, we present our test exploration tool, the IntelliJ Plugin TestCube , which enables developers to easily use our developer-centric test ampliﬁcation with minimal conﬁguration, right from their familiar development environment. Introduction to Test Ampliﬁcation Test ampliﬁcation is a term for test generation techniques that take manually written test cases as their primary input. Danglot et al. conducted a literature study to map this emerging ﬁeld and deﬁned test ampliﬁcation as follows: This section starts with a more detailed deﬁnition of test ampliﬁcation and Test ampliﬁcation consists of exploiting the knowledge of a large number of test cases, in which developers embed meaningful input data and For our prototype design we choose test ampliﬁcation to generate the test cases. We exploit the existing test cases, as well as the code under test to create additional test cases that improve the instruction coverage of a test suite. that for a developer already familiar with the test suite it will be easier to understand a variation of an existing test case than a completely new one. In addition, most software projects that are looking to improve their testing already have at least a rudimentary test suite. During our interviews, we want to showcase a possible version of developercentric ampliﬁed test cases to software developers. We adapt Danglot et al.’s tool DSpot (Danglot et al., 2019b), addressing the issues which were already reported by developers. Figure 2 gives an overview of our revised test ampliﬁcation approach. Starting with the original test case from the existing test suite, we remove all existing assertions, modify the objects and values in the setup phase of the test case, add new assertions based on the changed behavior, and select test cases that cover additional instructions in the code under test. glot et al., 2019b) and DSpot version 3.1.0 itory request ampliﬁcation as well as the changes we made to generate more understandable test cases (I 1) and convey their value to developers more easily (I 4). We illustrate our explanations with a running example in Figures 3, 4, 5 and 6. At the start of the ampliﬁcation process, DSpot removes all assertions in the original test case, as they will likely no longer match the new ampliﬁed test case. All method calls within the assertions are preserved because they might have side eﬀects that inﬂuence the rest of the method calls in the test case. of the assertion. As the behavior of the test cases is changed through the expected properties in the form of oracles, in order to enhance these manually written tests with respect to an engineering goal (e.g., improve coverage of changes or increase the accuracy of fault localization). (Danglot et al., 2019a) We base our approach on test ampliﬁcation (C 1), because we expect Our design and implementation is strongly based on Danglot et al. (Danand contributed our changes back to DSpot through an accepted pull . In the following, we describe for each step the behavior of the original However, these method calls tend to be confusing outside of the context Fig. 2: Overview of our automatic, developer-centric ampliﬁcation process within DSpot. Fig. 3: Example ampliﬁcation: Remove all existing assertions from the original test case. ampliﬁcation anyways, we decide to also remove method calls within assertions (C 2). Figure 3 shows the ﬁrst ampliﬁcation step for our example, where the two assertions of a test case are removed completely. DSpot uses a variety of mutations to explore the input space of a test case. Literals like integers, booleans, and strings are slightly modiﬁed or replaced by completely random values. On existing objects, the input ampliﬁcation removes, duplicates, or adds new method calls. It can also create new objects or literals that are then used as parameters for mutated method calls. of DSpot. However, from Grano et al. we know that the more complex a test case is, the harder it is to understand for a developer (Grano et al., 2020). To make the generated test case easier to understand for the developer, we focus on one input modiﬁcation at a time (C 3) and add an explanatory comment to every mutation (C 4). We make use of all available mutation operations in DSpot 3.1.0. In Figure 4 one of the string parameters in the constructor for the object attr is replaced with a new string that contains the special nonbreaking space character. We also add a comment that details which value was changed to which new value to help the developer spot the change easily. Generating new assertions is one of the central features of DSpot. The tool instruments the test case to observe the state of the objects under test after Our developer-centric ampliﬁcation leverages the powerful input mutations Fig. 4: Example ampliﬁcation: Mutate string parameter in the constructor of the object under test. Fig. 5: Example ampliﬁcation: Generate an assertion which checks a behavior changed by mutating the input. the setup phase. Then it generates assertions comparing the return value of every method call on the objects under test with the observed value. While adding all generated assertions leads to a more powerful test case with respect to structural coverage, it also makes the test case hard to understand and unclear which of the added assertions improve these metrics. To minimize the generated test cases, DSpot provides a prettifier stage. It removes the assertions one by one, reruns the metric calculation, and adds the assertion back if the score decreased. Unfortunately, the stage multiplies the already long runtime of DSpot. add one assertion to each test case (C 5). While this at ﬁrst generates more test cases, the ones with assertions that do not improve the ﬁnal selection metric are excluded in the following step. To produce test cases that developers ﬁnd useful (I 4), the generated assertion should assert a behavior that changed through the previously mutated input. To achieve this, we compare all assertion candidates before and after the mutation and only include an assertion if the value it asserts changed through the mutation (C 6). turn value of attr.toString(), which shows the changed input "Hello\\nthere NBSP". 3.1.4 Select Test Cases and generating assertions, DSpot selects which test cases to keep. Depend- To generate shorter, more understandable test cases (I 1), we opt to only As shown in Figure 5, the assertion generated for our example checks the re- After generating a broad range of test cases through mutating input values Fig. 6: Example ampliﬁcation: Information about the coverage improvement of the ampliﬁed test case. ing on the conﬁguration, DSpot selects test cases that improve instruction coverage, improve mutation score, or cover the changes in a speciﬁc commit. rently not a feasible option if the test generation should run on the developer’s local computer and we want to enable direct interaction with as little wait time as possible (I 3). Therefore, we select test cases based on instruction coverage (C 7). more lines than the original test case they are based on. However, for a developer, it is not important that the coverage of one test case is high. Rather, a new test case should contribute additional coverage to the test suite. To determine this, we measure the instruction coverage of the original test suite on a ﬁne-grained, line-by-line basis. For each generated test case, we check whether it covers additional instructions on any line (C 8). If that is the case, we keep the test case, if not, we discard it. The combination of this ﬁne-grained coverage comparison together with the small number of additions we make to the original test case (C 3) (C 5) enables us to generate smaller test cases (I 2) compared to DSpot. Furthermore, these test cases have a local and therefore easier to understand impact on the coverage of the test suite (I 4). our developer-centric ampliﬁcation reports for each test case in which lines in the production code additional instructions are covered (C 9). Figure 6 shows a pretty-printed version of the additional information we provide. The ampliﬁed test case in our example covers 8 more instructions over two lines in the escape method of the Entities class. For successful developer-centric test ampliﬁcation, we conjecture that the second important step to support developers with ampliﬁcation test cases is exploration. In this section, we describe the design of our prototype of a developera plugin to the IntelliJ IDE (C 10). A lot of previous research points out the importance of integrating tools into existing development environments. It reduces time and focus lost by switching tools (Liu and Holmes, 2020) As determining the mutation score is computationally expensive, it is cur- DSpot originally keeps all generated test cases that by themselves cover To communicate to the developer which additional instructions are covered, and enables developers to inspect test cases and related code (Spadini et al., Brains Marketplace editor, inspect the generated test cases and easily integrate them into their code base. After installing TestCube , the developer can start the ampliﬁcation in the same way as she would execute a JUnit test (C 11). She picks an original test case to be the input for the ampliﬁcation process ( she can click on the green arrow next to the test, and select the new option ‘Amplify’ ( amplifying the test in a background task (C 12). parameters to the speciﬁc project increases performance, using the default settings already produces good results (Arcuri and Fraser, 2013). To take the conﬁguration burden oﬀ the developer as much as possible (I 2) during our interviews and to evaluate how well our new approach performs with its default conﬁguration, we choose to set default values for the vast number of DSpot parameters (C 13). 3.2.2 Result Inspection up from the built-in notiﬁcation system ( Fig. 7: Overview of the interaction with the TestCube plugin. For other test generation tools, research has shown that even though tuning then choose to inspect the test cases or, in case of reported errors, the terminal output of DSpot. tool window separated from the code. The tool window is located on the right, next to the editor with the original test case selected by the developer. It has various components: – At the top of the tool window we present information about the currently – Next we present navigation buttons to the developer ( – Below the navigation buttons we present the ampliﬁed test cases in a fully – The developer can click on the additionally covered lines in the test case The goal of this paper is to explore which aspects are important to create a successful developer-centric test ampliﬁcation approach. To this end, we invited project. We observed their interaction with the tool and interviewed them on their experience and opinions on how an ideal test generation tool should behave. We report our observations split along four sub-questions: With the interviews we investigate what makes ampliﬁed tests (RQ1) and exploration tools (RQ2) suited for developer-centric test ampliﬁcation, what information the developers are seeking while investigating the test cases (RQ3) and what value developers see in test ampliﬁcation (RQ4). In the following we describe the design of our interview study: How we recruit participants and ask for their consent, our technical setup, the ﬂow of one interview as well as our data collection and analysis process. We present the results of the test ampliﬁcation within IntelliJ, but in a selected ampliﬁed test case (○in Figure 7): Which modiﬁcations were applied to it and which additional instructions are covered (C 14). these buttons, the developer cycles through the proposed test cases, can add the current test to the test suite, ignore the current test or close the ampliﬁcation results all together. functioning editor, as shown in (○in Figure 7). The developer can edit the test case in their familiar environment and use code navigation to inspect called methods. information to open up the coverage inspection editor on the bottom of the tool window. The editor shows the class where coverage was improved and highlights all additionally covered lines in green (C 15). Showing the added coverage in the context of the code under test should help the developer judge the value of the generated test case (I 4). We used convenience sampling to recruit participants for our interviews. We posted about it on Twitter tion, we contacted participants of a previous survey about motivation to write test cases who indicated to be open for a follow-up interview. guidelines of the TU Delft’s Human Research Ethics Council our study design to them for review. Before each interview, we explained to the interviewees how we will process their data and asked for consent on participating in the interview, recording the session for later analysis and publishing the anonymized results in an online research repository. One participant wished to not be quoted and the corresponding results not to be published in a research repository, therefore our online appendix (Brandt and Zaidman, 2021) excludes the data collected from that interview. HTML parser jsoup which is built with Maven, tested with JUnit 5 and was part of Danglot et al.’s evaluation of DSpot (Danglot et al., 2019b). We chose it because we expected HTML to be a relatively simple and widely understood application domain, which would require less time to explain to our participants during the interviews. Jsoup has a test suite with a relatively good instruction coverage of 86%. Our interviews focused on the classes Attribute and AttributeTest, as we expected the concept of an HTML attribute to be known by our participants. However, its custom implementation of hashCode, clone and several branches in equals were not covered. RQ1: What are the key factors to make ampliﬁed test cases suited for developer-centric test ampliﬁcation? RQ2: What are the key factors to make test exploration tools suited for developer-centric test ampliﬁcation? RQ3: What information do developers seek while exploring ampliﬁed test cases? RQ4: What value does developer-centric test ampliﬁcation bring to developers? As we are conducting a study with human participants, we followed the IntelliJ with our plugin on a server and let the interviewees interact with it through the browser. This was possible through the JetBrains Projector tool To get a rough context of the participant’s opinion on and knowledge about software testing, we asked an open question about the participant’s prior experiences with testing software. Then we brieﬂy introduced test ampliﬁcation: automatically modifying existing test cases to generate new ones that improve the coverage and can be taken over into the test suite. We explained that the goal of the interview is to see their interaction with our tool and gather feedback on what aspects they like, what they would change, and how they would use such a test ampliﬁcation tool in their work. We sent the developers a link to our online setup of IntelliJ which they accessed through their browser. We introduced the example project and explained how to start TestCube . From this point on we invited the interviewee to explore on their own, thinking aloud about all their thoughts and impressions. We did not deﬁne an explicit task to solve, rather our introduction of test ampliﬁcation and the user interface of judge whether to include them, and in some cases include them in the test suite of the example project. We kept any more explanations to a minimum to observe a situation as close as possible to the developer interacting with the tool alone. We let each participant amplify and browse through several test cases for about twenty minutes. During this time we ask them to think aloud about their impressions. We nudge them by asking questions about their to ﬁll out the System Usability Score questionnaire, a metric frequently used in the ﬁeld of Human-Computer Interaction to assess how useable a product is (Bangor et al., 2008). While ﬁlling out the questionnaire, we ask them to reﬂect on the usability of the plugin and how it could be adapted to better ﬁt their needs. We recorded all interviews, including the screen of the developer while they were interacting with TestCube . In addition, the interviewer took extensive notes. We performed open coding (Corbin and Strauss, 1990) to analyze the interview notes, checking back with the recording when anything was unclear or missing from the notes. Following that, we applied axial coding (Corbin and Strauss, 1990) to structure the emerged codes. We report our ﬁndings along these axial codes, which we assigned to each of the research questions. Table 2 presents the axial codes arising from our analysis of the interviews. To take the setup burden oﬀ of our participants, we set up an instance of To increase the validity of our analysis, the second author watched two of the performed interviews, took notes and coded them separately. Then we compared the codes both authors created for the validation interview and reﬁned our coding schema and our interpretations of the interviews. We saw that both focused on diﬀerent aspects of the interviews, one assigning about 20 codes and the other one about 10 codes per interview. In total, we agreed on 90% of the assigned codes in the ﬁrst validation step. As a second validation step, we performed an inter-rater reliability analysis. We selected re-occurring topics from our codes that appear in 4 or more interviews. The second author assigned them to 3 further interviews. To compare the assignments of both authors, we calculate the percentage agreement (70%) and Cohen’s Kappa (60%, moderate agreement). The value of Cohen’s Kappa is relatively low, because some of the codes we validate have skewed values. If a code appears in nearly all the cross-validated interviews, its chance of appearance is close to one, leading to a small Cohen’s Kappa because arithmetically the agreement could be a coincidence. We provide our code book together with the code’s frequencies in the interviews, as well as our cross-validation ratings in our online appendix (Brandt and Zaidman, 2021). In this section, we present the results we elicited from our study. Firstly, we give an overview of key demographic factors characterizing our study participants. Secondly, we detail what factors are important to the developers when it comes to the generated test cases themselves (RQ1) and which aspects make a test exploration tool developer-friendly (RQ2). Next, we describe the various kinds of information the developers sought while exploring the generated test cases (RQ3) and what value our interviewees saw in automatic test generation (RQ4). Every observation that comes from the interviews will be labeled with (O n) and if it is directly tied to one of the codes we assigned, we also report its support, i.e., in how many interviews we observed it. The observations without explicitly mentioned support summarize multiple codes, describe anecdotal evidence or report general impressions we obtained overarching the single interviews. Even though we cannot link them to a speciﬁc code from our interview notes, we still consider them valuable to report for our qualitative study. we cannot infer from a small support number that an aspect is less relevant. As we wanted to explore as many aspects of developer-centric test ampliﬁcation as possible, we mainly let the comments of our participants guide the direction of the interviews, similar to an unstructured interview (Zhang and Wildemuth, 2009). This lead to many observations only appearing in a small number of interviews, possibly because the topic they concerned was only reached in a small number of interviews. All interviews and the initial coding were performed by the ﬁrst author. While high support signals that a topic is very relevant for our participants, Table 2: Structured overview of the answers to our research questions. Shown are the axial codes we obtained during our data analysis described in Section 4.3. of our results presented in Table 2. We recruited 16 participants for our study, whose demographics are summarized in Figure 8. As shown in Figure 8a, their previous experience with software development was distributed in a range from two to 23 years. Most of them work in teams of two to nine and consider Java to be among their primary programming languages. Our participants work in a wide variety of industries, which are presented in Figure 8d. 5.2 RQ1: What Are the Key Factors to Make Ampliﬁed Test Cases Suited for Developer-Centric Test Ampliﬁcation? During the interviews, it quickly became clear to us how important the test cases themselves are for the developers. Many of our participants were directly Throughout this section, we structure our explanation along the axial codes focusing on the test cases and spend much of their time praising or critiquing them (O 1). This is reﬂected in the large number of observations with high support we present in this section. What uniﬁed our participants is that they tried to understand the generated test cases (O 2). Rojas et al. also noted that how easy it is to obtain the intent and behavior of a test case is a strong indicator for its quality (Rojas et al., 2015). Code: Identiﬁers Looking at the code of the test cases, the most prevalent comment was the need for less cryptic identiﬁers (O 3, Support: 14). The developers wished that the identiﬁers, such as the test name or variable names, convey the intent of the new test case (O 4, Support: 2). They gave examples such as which code is additionally covered or simply which methods are newly called. More expressive identiﬁers would help them understand the intent of the test case faster. While integrating the test cases, many participants renamed the test cases (Support: 7) and the used variable identiﬁers (Support: 3). For identiﬁer names that should be renamed by the developer during the inspection, two participants promoted a clearer naming such as “placeholderN”. Code: Concise We observed that the code should be as concise as possible. Developers were confused by unnecessary statements (O 5, Support: 7), which were left over from the ampliﬁcation process. Underneath them were object initializations or method calls no longer relevant for the intent of the new test case. The developers needed additional time to detect these statements as unimportant and to delete them (O 6, Support: 3). Fig. 8: Summarized demographics of our interview participants. troduced by DSpot for type safety. In many cases, the casts could be identiﬁed as superﬂuous, but the developers were unhappy that extra work is necessary to remove obviously superﬂuous code. actively splits the function call and the assertion through a variable declaration. While one participant preferred this step for clarity and would ﬁnd it easier to understand with an expressive variable name, three other participants were annoyed by the additional bloating of the code (O 8, Support: 3). While inlining the variable declaration a participant even pointed out that splitting the function call from the assert statement lead to less powerful assertions being used (O 9). Instead of assertNotEquals the test used assertion fails. assertNotEquals throws a org.junit.ComparisonFailure and prints the expected and actual values, while assertFalse simply throws an To generate parameters while creating new objects for the setup phase of a test case, DSpot can generate unnecessarily long random strings of characters (O 10, Support: 2). To understand the test’s behavior, developers now would need to know which of these characters are important for the intent of the test case, which takes a lot of time and eﬀort. In one case we observed, DSpot created a new object and checked that it is not equal to the existing one. The developer spent a lot of time going through the various special characters in the constructor parameters for the new object, trying to determine which one is triggering the behavior of the test case. In the end, none of the special characters were necessary, the strings simply had to be diﬀerent from the strings initializing the existing object. This anecdote points to the need for minimizing input values in generated test cases, which was also pointed out by Fraser et al. (Fraser and Arcuri, 2013) Code: Consistent Apart from the need for the test code to have a high quality Athanasiou et al. (2014) itself, it should also be consistent with the rest of the test suite. Three participants pointed out that the assertion methods were fully qualiﬁed instead of statically imported, like in the original test case (O 11, Support: 3). A similar comment was made for the identiﬁers, which one participant wanted to be in the same naming schema as existing test cases. Behavior: Relevant Moving to the behavior of the test cases, we saw that it is important to test methods relevant to the developers. As our example project already had a relatively good test suite covering most core functions of the class, many of the proposed test cases covered extra branches in equals, not test this method” (O 12, Support: 11), leading some to discard the test directly. Others investigated further and uncovered that hashCode was overwritten with a custom implementation, which for one participant meant it was We saw a similar eﬀect with unnecessary casts (O 7, Support: 8) in- For generated assertions that check the return value of a function, DSpot Another issue brought up by the developers was concise input strings. relevant after all to test the method. We believe that it is not only important to focus on testing methods important to the developer, such as core functions deﬁned in a class, but also to make it clear why a newly covered function is considered important, e.g., because it overwrites the defaults with a custom implementation. How many interesting test cases are proposed was an important point for the developers we interviewed, this would majorly inﬂuence Behavior: Invariant A further comment on the behavior of the generated test cases was that developers would like them to test invariants of methods instead of absolute values (O 14, Support: 8). The most prominent example being hashCode. As other test generation tools, DSpot uses the current behavior of a system as an oracle. To test hashCode, it calls the method on an object and creates an assertion comparing the resulting value to the return value of changes are made to the class’s attributes. Our participants advocated testing the invariant of hashCode instead of an absolute value. Interestingly though, they proposed a variety of ways how to test this invariant: Cloning an object and checking the hashCode is still the same (as well as that they are equals), creating the same object twice and compare the hashCode, check that if equals is true the hashCode is also the same, or even creating random objects and verifying that only a few of them lead to hashCode collisions. While proposing ampliﬁed test cases to open source projects, Danglot et al. also saw diverging reactions to test cases testing hashCode (Danglot et al., 2019b). Behavior: Diverging One of our observations that is special to test ampliﬁcation is how far the behavior of the generated test case should diverge from the original test case (O 15, Support: 6). Some of our participants were enthusiastic that the generated test cases explored so many new paths and scenarios, even naming this as one of the key strengths of TestCube . Others were confused by this divergence as they mainly focused on comparing the behavior of both test cases. The most severe cases of such a divergence approach when the original test case involves objects from another class than the class under test. Some of the ampliﬁed test cases then test functionality in this other class and completely disregard the original class under test (O 16, Support: 3). While these tests can be valid and helpful additions to the test suite, our participants mostly disliked them, because they were focussing on testing the original class under test. In a future version of TestCube , tests for another focal class should be marked as such and proposed to be added to the ﬁtting test class. 5.3 RQ2: What Are the Key Factors to Make Test Exploration Tools Suited for Developer-Centric Test Ampliﬁcation? To enable developers to interact with and judge the ampliﬁed test cases, we created a test exploration tool. In the following, we will explain our observations from the interviews on what factors are important for such a tool to be suited for developer-centric test ampliﬁcation. Ease of Use: Minimal Conﬁguration First and foremost, a test exploration tool should be easy to use and especially easy to start. Our approach of using a default conﬁguration was successful, two of our participants pointed out how little eﬀort was needed from their side to get started (O 17, Support: 2). Some still noted concerns about how easy the tool would be to set up locally for their projects (O 18, Support: 2), so clear supporting documentation is important if one wants to let the developer try out and discover a tool all by themselves. Ease of Use: Integration A factor that helped developers start up and Support: 3). Participants noted that it was easy to start from the “run test” location, two made use of the built-in code navigation to explore the code under test (O 20, Support: 2) and one liked that they could perform all actions without having to switch tools (O 21, Support: 1). Ease of Use: Usability In addition to minimal conﬁguration and being integrated, a developer-centric exploration tool should also adhere to the longestablished criteria for usability from Human-Computer-Interaction research (Bevan, 2001). We have seen that it is important to give the developer control Summarizing the results and observations described in this section, the key factors to make ampliﬁed test cases suited for developer-centric test ampliﬁcation are concerned with the code and the behavior of the test cases. When it comes to code, the variable identiﬁers and test names should be meaningful, the code should be short and concise, and consistent in terms of quality and style with the rest of the test suite. With respect to its behavior, an ampliﬁed test case should execute scenarios that are relevant for the developer, should test invariants in place of absolute values where possible, and should match the developer’s expectation in terms of divergence from the original test case. over the layout of TestCube : various participants had diﬀerent wishes for which information they want to see and how much space the tool should take up on their screen (O 22, Support: 1 each from 5 codes). Some were looking for buttons to close, e.g., the coverage editor they no longer needed (Support: 3) or got confused after they could not undo an unintentional action (Support: 3). We believe it is crucial for a successful tool to give the developer options to conﬁgure the layout of the tool to ﬁt their needs and let them recover from errors. ability of TestCube . 44% of our participants rated the usability as “Excellent”, 38% as “Good” and 19% as “Poor”. This shows that even with the above mentioned issues, we overall succeeded in creating a tool that is easy to use. Information Management We observed big diﬃculties with managing which instructions are additionally covered was overlooked by many study participants, some later said they thought it is “unimportant debug output” (O 23, Support: 3). Providing the information sought out by developers in a way that does not overwhelm them and is accessible to them where they expect it is one of the big challenges looking at future versions of TestCube . Also the number of generated test cases should not be too large (O 24, Support: 4). For some methods, over ﬁfty new test cases were proposed that one by one tested a previously uncovered class. One participant said they lost interest after looking over several of these test cases and seeing how many were left (O 25, Support: 1). An eﬀective test exploration tool should focus on a few impacting test cases to not overwhelm the developer and keep each interaction session compact. Additionally it would help to rank the generated test cases and show the most impactful ones ﬁrst. Focus Related to information management is also the issue of focus. Through nearly each one of our interviews, we saw how important it is to only show information to the developer which they are supposed to focus on in that moment. In the current design of TestCube , the ampliﬁed test cases are all part of the same text ﬁle presented at once in an editor. This means that multiple e.g., the coverage information and the automatic adding to the test suite, was focused only on the ﬁrst test case at the start, many of our participants started scrolling through the list of test cases immediately (O 26, Support: 5). Later some of them were confused (O 27, Support: 3), as they tried to add the test case they were currently focussing on to the test suite, while TestCube copied over the ﬁrst one in the list. It is therefore extremely important for a future test exploration tool to make sure the focus of the developer aligns with the focus of the tool. For example, by only showing the code of one test at a time and therefore forcing the user to click on the next and previous buttons to explore the generated test cases. With the help of the System Usability Score, we evaluated the overall us- Manage Expectations: Runtime A test exploration tool should manage the expectations of its users. We observed this with the runtime of the ampliﬁcation process. Even though we took care in our conﬁguration to keep the runtime of DSpot as low as possible, in some cases the generation still took several minutes to complete, which four participants considered as too long (O 28, Support: 4). While we included a business indicator that signaled to the developers that the ampliﬁcation is running a background task, many were wondering how long it will take before they get results. Our participants wished for an expressive progress bar that either gives an estimation of the remaining time or at least shows an approximated form of progress (O 29, Support: 2). Some were wondering whether, or even expecting that, it is possible to switch to another task while they were waiting. Manage Expectations: Capabilities The expectations with respect to the capabilities of a tool should also be correctly set. As we gave the developers only a minimal introduction to the tool, it was not clear for some whether the generated test cases are meant to replace the existing test case or are meant to be an addition to the test suite. Toward the end of their interviews, participants pointed out that they slowly understand the power of the tool better and see clearer how they would employ it (O 30). One pointed out that the generated test cases were much more appreciated by him now that he understood the editing eﬀort which was necessary before including them. ploration tool developer-centric. It should be easy to start and use, the way of displaying information needs to be carefully chosen, it has to keep track of the focus of the developer and manage the user’s expectations towards its capabilities and runtime. Overall, we observed a plethora of important aspects to make a test ex- In summary, a key factor to make test exploration tools suited for developer-centric test ampliﬁcation is making the tool easy to use: through minimal conﬁguration, through a tight integration into the developer’s existing environment and through adhering to established usability principles. Such tools should manage the information they present to the developer and help the developer focus on the information they need for their current task. Further, a test exploration tool should manage the expectations their users have towards the runtime and the capabilities of the tool to ensure that these expectations can be fulﬁlled. 5.4 RQ3: What Information Do Developers Seek While Exploring Ampliﬁed Test Cases? While exploring the generated test cases, our participants did not only scrutinize the test code itself but were also looking for and asking about a lot of additional information. We saw that it is crucial to provide quick and familiar ways for developers to provide this information so they can eﬃciently decide on whether to keep or how to adapt an ampliﬁed test case. Test Case: Behavior / Intent As mentioned in Section 5.2, the test cases themselves and their behavior or rather their intent were a main focus of the developers. After making edits to a test case, one participant wondered whether the original intent of the generated case was still preserved (O 31, Support: 1). This is in line with Grano et al.’s results: developers are concerned with determining whether a unit test “actually exercises the corresponding unit” and how many relevant scenarios are covered (Grano et al., 2020). Prado and Vincenzi showed that the code of a test case is one of the main sources of information about a test case for the developer (Prado and Vincenzi, 2018), an observation corroborated by Aniche et al. (Aniche et al., 2021). As far as we observed, the current editor displaying the code of the generated test case is enough to satisfy this information need for developers. Test Case: Outcome Furthermore, the developers were interested in the outcome generated test cases (O 32, Support: 3), i.e., whether they are passing or failing. As all test cases generated by DSpot pass, this could be addressed by a better explanation of the tool. Alternatively, tool developers could provide the existing IDE utility to run a test case in the editor proposing the new test case or provide functionality such as Inﬁnitest, a tool that runs JUnit tests continuously in the background (Inﬁnitest, 2021). This would allow the developer to easily check that a test case is still passing after editing it and before integrating it to the test suite. Test Case: Runtime The runtime of a test case was also pointed out by one of our participants (O increasing the runtime of the continuous integration build was frowned upon. Test exploration tools should include a note about the measured execution time with each test case. Code Under Test While inspecting the new test cases, most of our participants quickly jumped to also inspecting the code under test. Two were trying to understand its behavior (O 34, Support: 2) to see the intent of the test case and to judge whether the additional coverage was relevant. Also, they checked if the tested method overrides standard behavior and if exceptions were thrown and tested. As Spadini et al. already pointed out, it is crucial for test review tools to provide easy navigation between test code and the code under test (Spadini et al., 2018). Prado and Vincenzi point out that developers should receive tool support to build the context between test code and code under test (Prado and Vincenzi, 2018). Through reusing the standard editor navigation tools, such as command-click to go to the deﬁnition of a method. Coverage The third large area developers wondered about was coverage. Under this falls the original coverage of the test suite and which additional coverage each generated test case and all the generated test cases together yield. A recurring question was whether a functionality covered by the generated test cases was already covered by another test case (O 35, Support: 2). Developers scanned the test suite to ﬁnd other tests calling the same method. are additionally covered by the new test case, not all participants understood that this implies the instructions were not covered by any existing test case. The developer that found our visualization of the added coverage, found it helpful to see the covered lines not only as numbers but also in the code context (O 36, Support: 1). They wished for a separate report of the original instruction coverage and the improvement of instruction coverage after including the ampliﬁed test cases. We also observed two times that the developers used the coverage of a test case to infer its intent (O 37, Support: 2), an observation also made by Grano et al. (Grano et al., 2020). Sometimes it was unclear to our participants why the new code covers these additional instructions (O 38, Support: 3). This points towards a need for exploration tools to visualize clearer how test code and code under test connect. ipants. Some of them wished for more information about how many branches are covered or hoped the tool would help them cover all branches as they would aim for while writing unit tests themselves. Even though some of our participants were aware of the concept of mutation score, none of them asked for information about improved mutation score or for test cases that kill additional mutants. Rojas et al. saw that in an industrial context many developers used coverage to evaluate a generated test case (Rojas et al., 2015). Original Test Case Special for the case of test ampliﬁcation, was the interest of the participants to inspect the original test case that was the basis for the ampliﬁcation. They tried to understand the intent of the original test case (O 39, Support: 5) and used this information, together with the knowledge of which instructions were changed to determine the behavior of the ampliﬁed test cases (O 40, Support: 2). Highlighting the changes from the original to the generated test case through comments (C 4) was not successful in our study. The developers ignored them and questioned their usefulness (O 41). We hypothesize that the generated test cases were short enough to spot the changes without the comments. Instruction coverage seemed to be a satisfying metric for most of our partic- 5.5 RQ4: What Value Does Developer-Centric Test Ampliﬁcation Bring to Developers? One way to make developer-centric test ampliﬁcation successful, is to bring across the value they can expect from the ampliﬁed test cases and from using the test ampliﬁcation tool. To give us an indication, which values we should focus on, we collected comments from our participants about the beneﬁts they believe they would achieve from using a tool similar to TestCube . Improve Test Suite: Ease Test Engineering First and foremost, automatic test ampliﬁcation would help them improve their test suite. By proposing complete, ready-to-run test cases that cover more code or interesting behavior (O 42, Support: 4), automatic test ampliﬁcation eases test engineering for developers. The test ampliﬁcation alleviates the developer from having to write test cases from scratch, reducing the eﬀort necessary to develop a test suite. Reducing eﬀort is a concern for developers: one participant stated, that they would “either look for less work or for tests with a better quality” (O 46, Support: 1). Improve Test Suite: Inspiration The generated test cases also provided inspiration. Several users created new test cases to cover the behavior of the ampliﬁed test cases (O 47, Support: 4). They were glad to be pointed to untested code paths (O 43, Support: 4) and to unexpected scenarios that could happen in the system (O 44, Support: 1). A recurring comment was that a test covers methods the participant always forgets to test (O 45, Support: 3). By proposing new scenarios with the generated test cases, test ampliﬁcation tools can take the burden of designing test scenarios of the developers. Learning Packaging test case generation in an easily accessible plugin can be a valuable step to enable more developers to learn about test ampliﬁcation itself. Many of our participants did not know about the technique of test a way to bring this idea into industry (O 48). The participants got more In our interviews we observed that developers are interested in a wide array of information while exploring and inspecting ampliﬁed test cases. For a test case itself, developers try to understand its behavior and intent, ask whether it is passing and how long it takes to execute. Beyond the test case, they are concerned with the code under test, the original and added coverage as well as the original test case the ampliﬁcation was based on. Table 3: Connection of our recommendations to our interview observations. for them and how they could apply it eﬀectively (O 30). In general, we saw that ampliﬁcation was easy to grasp for the developers (O 49). A participant pointed out they would like to use such a tool while they are working on improving the test suite (O 50, Support: 1) and another was eager to try it on their own projects (O 51, Support: 1). increase their conﬁdence in their test suite (O 52, Support: 1). On the one hand simply through the higher coverage after adding the generated test cases, and on the other hand because they see more important scenarios being covered. In the following, we consolidate our results into two actionable recommendations on how to make ampliﬁed test cases and test exploration tools suited for developer-centric test ampliﬁcation. Table 3 shows from which of our interview observations we infer the recommendations. Our participants named a number of beneﬁts they would gain from using an automatic test ampliﬁcation tool regularly. It would make it easier for them to develop test cases, by alleviating them from the eﬀort to write the test cases and by providing inspiration of scenarios they tend to forget to test. A developer-centric test ampliﬁcation approach would support them learning about automatic test ampliﬁcation and using it would increase their conﬁdence in their test suite. oper, a test exploration tool, to address the issues users previously reported with DSpot. Our prototypes could already surface diﬀerent kinds of information the developers were seeking, such as the behavior of the test case (O 2), its coverage (O 35), or which code is tested (O 34). Further, it became clear how tightly the characteristics of the test exploration tool are bound to the kind of test cases it presents to the developer. In our design, the technique of test ampliﬁcation (C 1) and the information from the ampliﬁcation process reports (C 9), is tightly bound to what our exploration tool TestCube presents to its users about the test cases (C 14). Our participants were questioning how the test cases are generated, and also sought information especially related to test ampliﬁcation, like the original test case (O 39) (O 40). We saw the importance of expectation management (O 30), e.g., on how much they should edit the proposed test cases, and conveying the value the test ampliﬁcation can bring to the developer, such as pointing to untested code paths (O 43). The tight integration into the developer’s IDE was helpful to get them started quickly (O 19). Overall, we saw a positive eﬀect of using a test exploration tool to facilitate the developer-centric test ampliﬁcation. We conjecture that this support of an integrated test exploration tool is also beneﬁcial for other test generation approaches that aim to be developer-centric. We recommend to future authors of developer-centric test generation approaches to provide a test exploration tool that is targeted towards the test generation method they employ and accessible to the developer from their familiar environment. Consider the interaction of the developer with the test cases: provide a test exploration tool that is targeted towards the test generation method and integrated into the developer’s environment. alization of the connection from the ampliﬁed test case to the additionally covered instructions in the code under test (O 23) (O 34) (O 38) and describing the behavior of the ampliﬁed test case and how it diverges from the original test case (O 31). Further we can help developer focus by proposing one test case at a time (O 24) (O 26) (O 27) and address waiting time (O 28) by generating test cases before they are requested. velopers were mainly concerned with understanding the test cases TestCube presented to them (O 2). The observations which occurred in most interviews are about the identiﬁers (O 3), the conciseness of the code (O 5) (O 7), and trying to understand the behavior and intent of the test case (O 31), be it through the test code itself or the various other kinds of information sought. During our interviews, the understanding was always the ﬁrst step—only after the participants understood a test case they started to judge the impact or relevance (O 12) (O 35). When judging the test cases, we observed that not We chose an additional layer between the test ampliﬁcation and the devel- Recommendation 1 Looking at the results of our ﬁrst research question, we can see that the deall tests which increase instruction coverage are relevant to developers, e.g., because they test a to them less important method (O 12) or a too narrow behavior (O 14). From these results, we infer that for a developer-centric approach, where the central aim is for a developer to take over the generated test case into their maintained test suite, the understandability of the generated test case and the relevance to the developer is of a bigger concern than how high its numeric impact is on the coverage of the test suite. An understandable test case with a weaker coverage contribution is more likely to be accepted by developers, compared to a test case that increases coverage greatly but they discard because they can not understand what it does. We recommend to future authors of developer-centric test generation approaches to prioritize the understandability of the generated test cases and their relevance to the developer higher than their impact on the coverage of the test suite. When the main goal is for developers to accept a test case into their maintained test suite, it is more important that the test case is understandable and relevant to the developer, than how much it impacts the coverage of the test suite. generating useful identiﬁers (O 3), possibly informing about the unique coverage provided by the test case (O 35). Further unnecessary statements (O 5) and casts (O 7) should be removed, the style of the test cases (O 11) can be adapted to ﬁt the existing test suite, randomly generated strings shortened and focused to the part triggering the tested behavior (O 10), and assertions adapted to use the most speciﬁc assertion giving an informative error message (O 9). right from their IDE (O 17) (O 19) (O 20), many found test cases that they liked and added them into the test suite of our example project. We conjecture that combining the already powerful state-of-the-art test ampliﬁcation approaches with well-designed, developer-centric test exploration tools will let us reach more developers to amplify their software testing practice. There are several threats to the validity of our results which we discuss in this section. Conﬁrmability To ensure that our results are formed by the interviewees and not by the authors, we base our results as closely as possible on the interviews. While coding and analyzing the interviews we performed extra steps to validate the codes elicited from the interviews and evaluated the inter-rater reliability, as described in Section 4.3. Nevertheless, other researchers might structure the resulting codes diﬀerently or draw varying conclusions from them. We publish Recommendation 2 Concretely, the ampliﬁed test cases generated by DSpot can be improved by Our participants pointed out how easy it was to interact with TestCube the full codes together with their frequency in our interviews (Brandt and Zaidman, 2021) for others to further explore the research area and add to our study. Reactivity and Respondent Bias As the ﬁrst author created the prototypes and conducted all interviews, the statements of the participants might be inﬂuenced by the participants wanting to please the creator of the tool they are evaluating. To mitigate this threat, we repeatedly invited the participants to be critical and refrained from defending the current state of the tool. Based on the wide variety of critical and positive points we could collect, we conjecture to have mitigated this threat. Construct Validity A threat to the construct validity of our study is that our participants interacted with an early prototype showing one possible design of a test ampliﬁcation tool. Bugs in the prototype or design decisions we took could inﬂuence the developer’s experience and the generalizability of the results to general test ampliﬁcation approaches. We identify several of our results as being related to our choice of test ampliﬁcation to generate the test cases (C 1), which we indicated while reporting them in Section 5. Similarly, our observations can be inﬂuenced by our default conﬁguration of DSpot. Optimizing the conﬁguration of DSpot to ﬁt the target project would likely lead to more relevant test methods being generated. Furthermore, our participants were not developers of the example project we used in the study. We expect that developers familiar with a project would spend less time on understanding the original and ampliﬁed test cases and could judge more easily if a production method is relevant to be tested. Dependability Whether our results are consistent and can be repeated in a replication is the concern of dependability. With 16 participants we were able to interview a relatively large number of software developers. Our presented results mainly focus on observations that we made in multiple interviews (that have high support). Nevertheless, there were many insightful comments that only emerged from one or a few interviews. Through the openness of our setup and questions, the interviews went in many diﬀerent directions and the observations we could make are dependent on the taken direction. We expect that repeating this study would yield diﬀerent support for the rarely-mentioned aspects, however the overall conclusions will likely stay the same. External validity There are several threats to the generalizability of our results. As well as other state-of-the-art test generation tools, our prototypes address Java and its speciﬁc properties. We expect our results to generalize to other object-oriented, statically typed languages and are curious to see the diﬀerent information needs developers of other programming languages have. Jsoup can also impact our results. Because equals, hashCode and clone were not covered by the existing test suite, DSpot generated tests mainly for these The choice of presenting our prototypes together with the example project functions that were named “irrelevant” by several of our participants. In other projects whose test suite has a lower or diﬀerently distributed coverage, the aspect of testing relevant methods might be less apparent. be inﬂuenced by our professional networks, as well as a self-selection bias of developers that are especially interested in high-quality test suites. From the demographic information we collected, we conclude that we sampled from a broad variety of experiences, industry domains and team sizes. Various past works have focused on the two main parts of or approach, mainly improving the understandability of test cases and integrating test generation tools into development environments. The issues of cryptic identiﬁers and lack of documentation in generated test cases are addressed by Roy et al. (Roy et al., 2020) in their tool DeepTCEnhancer. With a combination of templates and deep learning, they generate comments that explain the behavior of a test case and meaningful identiﬁers. Their work is an extension of TestDescriber by Panichella et al. (Panichella et al., 2016) and was evaluated by 36 developers. The developers were most enthusiastic about the meaningful identiﬁers, while some said the explanatory comments are not concise enough. In our interviews, we also observed the importance of expressive test names and variable identiﬁers. While our participants were trying to understand the behavior of the ampliﬁed test cases, they could interpret the raw code of the test cases well. Therefore, we do not believe any additional summarization of the test itself is necessary. Easier access to the code under test and information about previous and added coverage are more relevant concerns going forward. In similar vein, Li et al. describe UnitTestScribe (Li et al., 2016). tant for the understandability of automatically generated SQL schema tests. They saw that human-readable string values are better to understand than randomly generated ones and the repetition between generated test cases made it easier to focus on the relevant diﬀerences of the test cases towards each other. Their results align with ours: Randomly generated strings were mentioned as confusing and our interview participants repeatedly used the similarity between the original and the ampliﬁed test case to understand the behavior and impact of the newly generated test case. ability based on various syntactic properties of test cases. They integrate the model into the ﬁtness function of EvoSuite (Fraser and Arcuri, 2011) to generate more readable test cases. In their model and post-experiment survey As we performed convenience sampling, the results of our study might Alsharif et al. (Alsharif et al., 2019) investigated which factors are impor- Daka et al. (Daka et al., 2015) deﬁne a regression model for test case readthey identiﬁed several important factors overlapping with our ﬁndings: Identiﬁers are important for the understandability of a test case, as well as no unnecessarily deﬁned variables and short string literals. works focus on generating meaningful names for test cases. NameAssist by Zhang et al. (Zhang et al., 2016) infers test names from the class under test, the expected outcome stated in the assertion and the overall test scenario deﬁned in the body of the test. Daka et al. (Daka et al., 2017) derive test names from additionally covered exceptions, methods, outputs and inputs of the component under test. They showed that the generated names are equally excepted compared to names given by developers and made it easier for developers to match a test to the code under test. Including an advanced name generation approach such as the one by Daka et al. (Daka et al., 2017) would ing tests ampliﬁed by DSpot more accessible for developers. They generate a natural language description of the changes made during the ampliﬁcation, of the value observations which lead to new assertions, and of the mutants which will be killed by the newly added test cases. These descriptions are designed to accommodate a pull request proposing to add an ampliﬁed test developer, embedding test ampliﬁcation into their IDE. In our scenarios, not only tool performance, but also the amount of presented information is a distinguishing challenge. Compared to Bihel and Baudry’s approach (Bihel and to the developer. We also evaluate our approach in a study with developers. opted for integration into the continuous integration process (Arcuri et al., 2016; Danglot et al., 2020). This, however, leads to a long time distance between triggering the test generation and receiving results Beller et al. (2017), as well as the developers having to inspect the tools outside of their familiar development environment. To provide more immediate value and direct many more constraints regarding the available execution power and therefore possible complexity of the applied algorithms. Several other test generation tools have been integrated into IDEs up until now. Following an industrial study of EvoSuite, Rojas et al. (Rojas et al., 2015) pointed out the importance to integrate test generation tools into development environments. Since then, EvoSuite has been lightly integrated into IntelliJ IDEA as a plugin (Arcuri et al., 2016) which provides options to conﬁgure the test generation within an existing build process. In contrast to this, TestCube Next to DeepTC-Enhancer by Roy et al. (Roy et al., 2020), several further Bihel and Baudry (Bihel and Baudry, 2018) focused speciﬁcally on mak- Because of the high computational cost of test generation, many tools have runs independently from a project’s build process and can be installed and applied with nearly no conﬁguration other tools from the STAMP project (STAMP, 2019a). The plugin oﬀers a graphical interface to set the various conﬁguration parameters of DSpot and start the ampliﬁcation process. Compared to TestCube , the additional information showing the impact of a generated test case is just presented as a JSON text and the developer is still confronted with many conﬁguration parameters. Pex tool which generates inputs for parameterized tests based on program analysis. They integrated their tool into Visual Studio, enabling the developer to generate and execute the unit tests by right-clicking on the parameterized unit test. The tool presents the generated inputs and corresponding test results in a new window as a simple table. The idea of connecting the developer closer with the test generation is also realized in Interactive Search-Based Software Testing (ISBST). In the concept of Marculescu et al. (Marculescu et al., 2012), domain experts decide the importance of diﬀerent components in the ﬁtness function leading the automatic optimization of the test cases. The interaction happens during the search process, where in deﬁned moments the expert evaluates the current candidate test cases and adapts the ﬁtness function for the next round of test generation. When compared to manual testing, ISBST could ﬁnd diﬀerent test cases and execute behavior previously not considered by developers, similar to also investigated the mental workload of developers using ISBST compared to manually writing test cases. They did see a higher load and explain it through the distance from the developer’s interaction with the ﬁtness function to the outcome of the search process. Similarly, we saw during our interviews that the developers tried to retrace the generation of the test cases, strengthening the choice to perform only small edits during the ampliﬁcation to make the process easier to retrace. After transferring their approach to industry (Marculescu et al., 2018), Marculescu et al. point out the need for ISBST and other automated test systems to eﬀectively communicate their results to their users. Our work addresses this by prototyping a developer-centric test exploration tool and eliciting the key factors to make such tools suited to be used for test ampliﬁcation. DSpot has been integrated into the Eclipse IDE as a plugin together with Tillmann and de Halleux (Tillmann and de Halleux, 2008) developed the With this paper, we are setting a step towards test ampliﬁcation that is centered around the developer and their needs. Based on reported issues with current state-of-the-art tools, we devised design intentions for a developercentric test ampliﬁcation approach that aims to generate test cases that will be taken over into the manually maintained test suite. We used these intentions to adapt DSpot‘s test ampliﬁcation and create TestCube , a powerful test exploration plugin for IntelliJ. With the help of these tools, we interviewed 16 software developers from a variety of backgrounds and collected detailed insights on how the ampliﬁed test cases and the exploration tool should be adapted to best ﬁt their needs. Through evaluating the information sought during the test exploration, as well as the value test ampliﬁcation brings to developers, we guide future tool developers on what they should bring forward in their upcoming, developer-centric test generation tools. We summarized our observations and results into two recommendations: Tool makers should consider the interaction of the developers with the ampliﬁed test cases and provide a targeted and integrated test exploration tool. If taking over the test cases into the maintained test suite is the declared goal, the understandability of the ampliﬁed test cases should be prioritized over optimizing the coverage of the test suite. – two recommendations on how to design developer-centric test ampliﬁcation – a structured overview of the key factors to make ampliﬁed tests as well as – a reﬁned, developer-centric test ampliﬁcation approach, based on the DSpot – a developer-oriented test exploration plugin for the IntelliJ IDE centric test ampliﬁcation in more depth. We want to look into generating meaningful identiﬁers fast enough, ranking test cases according to their relevance to the developer, and providing them information such as runtime or coverage when they look for it, but without overwhelming them. Our tools will dive deeper into their day-to-day development, for example by helping them incrementally generate test cases for new or untested classes. We want to give them more power to direct the ampliﬁcation and receive test cases that cover code or scenarios they are interested in, while also providing them with subtle, helpful recommendations before they realize they need another test case. Our vision is to build tools and methods that empower developers to create better test suites with less eﬀort, while they are at the steering wheel deciding over, leading, and beneﬁting from our automatic test ampliﬁcation. In short, we contribute: tools test exploration tools suited for developer-centric test ampliﬁcation test ampliﬁcation Going forward we want to understand the diﬀerent aspects of developer-