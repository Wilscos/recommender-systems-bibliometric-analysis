We investigate whether model extraction can be used to ‘steal’ the weights of sequential recommender systems, and the potential threats posed to victims of such attacks. This type of risk has attracted attention in image and text classication, but to our knowledge not in recommender systems. We argue that sequential recommender systems are subject to unique vulnerabilities due to the specic autoregressive regimes used to train them. Unlike many existing recommender attackers, which assume the dataset used to train the victim model is exposed to attackers, we consider a data-free setting, where training data are not accessible. Under this setting, we propose an API-based model extraction method via limited-budget synthetic data generation and knowledge distillation. We investigate state-of-the-art models for sequential recommendation and show their vulnerability under model extraction and downstream attacks. We perform attacks in two stages. (1) Model extraction: given dierent types of synthetic data and their labels retrieved from a black-box recommender, we extract the black-box model to a white-box model via distillation. (2) Downstream attacks: we attack the black-box model with adversarial samples generated by the white-box recommender. Experiments show the eectiveness of our data-free model extraction and downstream attacks on sequential recommenders in both prole pollution and data poisoning settings. ACM Reference Format: Zhenrui Yue, Zhankui He, Huimin Zeng, and Julian McAuley. 2021. Black-Box Attacks on Sequential Recommenders via Data-Free Model Extraction. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 16 pages. https://doi.org/10.1145/3460231.3474275 Model extraction attacks [ Model extraction exposes issues such as sensitive training information leakage [ Recently, this topic has attracted attention in image classication [ work, we show that model extraction attacks also pose a threat to sequential recommender systems. Sequential models are a popular framework for personalized recommendation by capturing users’ evolving interests and item-to-item transition patterns. In recent years, various neural-network-based models, such as RNN and CNN frameworks (e.g. GRU4Rec[ are widely used and consistently outperform non-sequential [ However, only a few works have studied attacks on recommenders, and have certain limitations: (1) Few attack methods are tailored to sequential models. Attacks via adversarial machine learning have achieved the state-of-the-art in general recommendation settings [4,6,39], but experiments are conducted on matrix-factorization models and are hard to directly apply to sequential recommendation; though some model-agnostic attacks [2,22] can be used in sequential settings, they heavily depend on heuristics and their eectiveness is often limited; (2) Many attack methods assume that full training data for the victim model is exposed to attackers [4,6,24,39,44]. Such data can be used to train surrogate local models by attackers. However this setting is quite restrictive (or unrealistic), especially in implicit feedback settings (e.g. clicks, views), where data would be very dicult to obtain by an attacker. We consider a data-free setting, where no original training data is available to train a surrogate model. That is, we build a surrogate model without real training data but limited API queries. We rst construct downstream attacks against our surrogate (white-box) sequential recommender, then transfer the attacks to the victim (black-box) recommender. Model extraction on sequential recommenders poses several challenges: (1) no access to the orignial training dataset; (2) unlike image or text tasks, we cannot directly use surrogate datasets with semantic similarities; (3) APIs generally only provide rankings (rather than e.g. probabilities) and the query budget can be limited. Considering these challenges, sequential recommenders may seem relatively safe. However, noticing sequential recommenders are often trained in an autoregressive way (i.e., predicting the next event in a sequence based on previous ones), our method shows the recommender itself can be used to generate sequential data which is similar to training data from the ‘real’ data distribution. With this property and a sampling strategy: (1) ‘fake’ training data can be constructed that renders sequential recommenders vulnerable to model extraction; (2) the ‘fake’ data from a limited number of API queries can resemble normal user behavior, which is dicult to detect. Downstream attacks are performed given the extracted surrogate model (see Figure 1). But attack methods tailored to sequential recommenders are scarce [44]. In this work, we propose two attack methods with adversarial example techniques to current sequential recommenders, including prole pollution attacks (that operate by ‘appending’ items to users’ logs) and data poisoning attacks (where ‘fake’ users are generated to bias the retrained model). We extensively evaluate the eectiveness of our strategies in the setting that a black-box sequential model returns top-k ranked lists. 2 RELATED WORK 2.1 Model Extraction in Image and Text Tasks Model extraction attacks are proposed in [26,40] by ‘stealing’ model weights to make a local model copy [18,20,30– 32,46]. Prior works are often concerned with image classication. To extract the target model weights, JBDA [30] and KnockoNets [32] assume the attackers have access to partial training data or a surrogate dataset with semantic similarities. Recently, methods have been proposed in data-free settings. DaST [ Adversarial Networks [ generated inputs that maximize the disagreement between the attacker and the target model. MAZE used zeroth-order gradient estimation to optimize the generator module for accurate attacks. Because of the discrete nature of the input space, the methods above cannot transfer to sequential data directly. For Natural Language Processing (NLP) systems, THIEVES [ exist for NLP, where authors nd random word sequences and a surrogate dataset (e.g. WikiText-103 [ create eective queries and retrieve labels to approximate the target model, we found that (1) in recommendation, it is hard to use surrogate datasets with semantic similarities (as is common in NLP); we also adopt random item sequences as a baseline but their model extraction performance is limited. Therefore we generate data following the autoregressive property of sequential recommenders; (2) Compared with NLP, it is harder to distill ‘ranking’ (instead of classication) in recommendation. We design a pair-wise ranking loss to tackle the challenge; (3) downstream attacks after model extraction are under-explored, especially in recommendation, so our work also contributes in this regard. 2.2 Aacks on Recommender Systems Existing works [ attacks, aecting a recommender system in test and training phases respectively. Prole pollution attacks aim to pollute a target user’s prole (such as their view history) to manipulate the specic user’s recommendation results. For example, [ user views into target user logs in real-world websites including YouTube, Amazon and Google Search. However the strategy used in [ victim recommenders. In our work, given that we can extract victim recommender weights, more eective attacks can be investigated, such as evasion attacks in general machine learning [ can append items to target user logs with injection attacks via implanted malware [ could be added on users’ behalf, therefore, we focus on injecting algorithm design and attack transferabilities (exact approaches for malware development and activity injection are cybersecurity tasks and beyond our research scope). Data poisoning attacks (a.k.a. Shilling attacks [ data. Some poisoning methods are recommender-agnostic [ model architecture) but they heavily depend on heuristics and often limit their eectiveness. Meanwhile, poisoning attacks have been proposed for specic recommender architectures. For example, [ for matrix-factorization recommenders and [ has been widely applied to recommendation, [ framework (NCF) [ which adopt adversarial machine learning techniques to generate ‘fake’ user proles for matrix-factorization models. One (fairly restrictive) limitation of LOKI is the assumption that the attacker can access complete interaction data used for training. Another limitation is that LOKI is infeasible against deep networks (e.g. RNN / transformer), due to an unaordable (even with tricks) Hessian computation. In our work, black-box recommender weights are extracted for attacks without any real training data; we further design approximated and eective attacks to these recommenders. 20] studied model extraction attacks on BERT-based APIs [5]. Even though attacks against BERT-based models 15,42] categorize attacks on recommender systems as prole pollution attacks and data poisoning 41] to decide which ‘fake’ item should be injected is a simple and heuristic without the knowledge of 12]. The closest works to ours are perhaps LOKI [44], because of the sequential setting and [4,6,39] 3 FRAMEWORK Our framework has two stages: (1) Model extraction: we generate informative synthetic data to train our white-box recommender that can rapidly close the gap between the victim recommender and ours via knowledge distillation [14]; (2) Downstream attacks: we propose gradient-based adversarial sample generation algorithms, which allows us to nd eective adversarial sequences in the discrete item space from the white-box recommender and achieve successful prole pollution or data poisoning attacks against the victim recommender. 3.1 Seing To focus on model extraction and attacks against black-box sequential recommender systems, we rst introduce some details regarding our setting. We formalize the problem with the following settings to dene the research scope: • Unknown Weights: Weights or metrics of the victim recommender are not provided. • Data-Free: Original training data is not available, and item statistics (e.g. popularity) are not accessible. •Limited API Queries: Given some input data, the victim model API provides a ranked list of items (e.g. top 100 recommended items). To avoid large numbers of API requests, we dene budgets for the total number of victim model queries. Here, we treat each input sequence as one budget unit. •(Partially) Known Architecture: Although weights are condential, model architectures are known (e.g. we know the victim recommender is a transformer-based model). We also relax this assumption to cases where the white-box recommender uses a dierent sequential model architecture from the victim recommender. 3.2 Threat Model 3.2.1 Black-box Victim Recommender. Formally, we rst denoteIas the discrete item space with|I |elements. Given a sequence of length𝑇ordered by timestamp, i.e.,𝒙 = [𝑥, 𝑥, . . . , 𝑥]where𝑥∈ I, a victim sequential recommender𝒇 is a black-box, i.e., the weights are unknown.𝒇should return a truncated ranked list over the next possible item in the item spaceI, i.e.,^𝑰= 𝒇(𝒙), where^𝑰is the truncated ranked list for the top-k recommended items. Many platforms surface ranked lists in such a form. 3.2.2 White-box Surrogate Recommender. We construct the white-box model𝒇with two components: an embedding layer𝒇and a sequential model𝒇such that𝒇(𝒙) = 𝒇(𝒇(𝒙)). Although the black-box model only returns a list of recommended items, the output scores^𝑺of white-box model𝒇over the input spaceIcan now be accessed, 3.3 Aack Goal 3.3.1 Model Extraction. As motivated previously, the rst step is to extract awhite-boxmodel𝒇from a trained, black-boxvictim model𝒇. Without accessing the weights of𝒇, we obtain information from the black-box model by making limited queries and saving the predicted ranked list^𝑰from each query. In other words, we want to minimize the distance between the black-box𝒇and white-box model𝒇on query results. We are able to achieve the goal of learning a white-box model via knowledge distillation [14]. Mathematically, the model extraction process can be formulated as an optimization problem: whereXrepresents a set of sequences and ranked lists, where Instead, they are generated with specic strategies, whose details will be included in Section 4. measuring the distance between two model outputs, such as a ranking loss. 3.3.2 Downstream Aacks. We use the extracted white-box model construct attacks. In this work, we investigate exposure to users as much as possible, which is a common attack scenario [ also be constructed with similar techniques. Formally, the objective of targeted promotion attacks is: •Prole Pollution Attack: We dene prole pollution attacks formally as the problem of nding the optimum injection items exposure 𝑬 where attacks setting, no retraining needed and this injected (e.g. using malwares [23, 33]; see Section 2.2). •Data Poisoning Attack: Similarly, poisoning attacks can be viewed as nding biased injection proles that after retraining, the recommender propagates the bias and is more likely to recommend the target. Z ∪ X refers to the injection of fake proles recommender training loss function L 4 METHODOLOGY 4.1 Data-Free Model Extraction To extract a black-box recommender in a data-free setting, we complete the process in two steps: (1) data generation, which generates input sequences dierence between the black- and white-box recommender. 4.1.1 Data Generation. Considering that we don’t have access to the original training data and item statistics, a trivial solution is to make use of random data and acquire the model recommendations for later stages. •Random: Items are uniformly sampled from the input space to form sequences iments) are acquired from the victim recommender in each step to form the output result set, i.e., {[𝒇 (corresponding to a recommendation list after each click). a pre-dened distribution or simply set as a xed value. Following this strategy, we generate inputs and labels ^𝑰∈^Iis the black-box model output for𝒙. Note that data(X,^I)are not real training data. , which can be characterized with common ranking measures like Recall or NDCG [17, 25, 37]: [𝒙; 𝒛]refers to the concatenation of the sequence𝒙and attacking items𝒛. Note that in prole pollution is a generated sequence with identier𝑖and𝐵is the budget size. Top-k items (𝑘 = 100in our exper- (𝒙), . . . , 𝒇(𝒙)]}, where the[: 𝑡]operation truncates the rst𝑡items in the sequence (a) Autoregressive Data Generation(b) Model Extraction via Distillation However, random data cannot simulate real user behavior sequences𝒙where sequential dependecies exist among dierent steps. To tackle this problem, we propose an autoregressive generation strategy. Inspired by autoregressive language models where generated sentences are similar to a ‘real’ data distribution, and the nding that sequential recommenders are often trained in an autoregressive way [13, 17, 25], we generate fake sequences autoregressively. •Autoregressive: To generate one sequence𝒙, a random item is sampled as the start item𝒙(the[𝑡] operation selects the𝑡-th item in sequence𝒙) and fed to a sequential recommender to get a recommendation list𝒇(𝒙). We repeat this step autoregressively i.e.,𝒙= sampler(𝒇(𝒙))to generate sequences up to the maximum length𝑇. Heresampleris a method to sample one item from the given top-k list. In our experiment, sampling from the top-k items with monotonically decreasing probability performs similarly to uniform sampling, so we favor sampling when selecting the next item from the top-k list. Accordingly, we generate𝐵sequencesX= {𝒙}and record top-k lists, forming a dataset(X,^I)for model distillation. For autoregressive sequence generation, Figure 2a visually represents the process of accumulating data by repeatedly feeding current data to the recommender and appending current sequences with the sampled items from the model output. Autoregressive method is benecial as: (1) Generated data is more diverse and more representative of real user behavior. In particular, sampling instead of choosing the rst recommended item will help to build more diversied and ‘user-like’ distillation data; (2) Since API queries are limited, autoregressive method generates data by resembling real data distribution can obtain higher-quality data to train a surrogate model eectively; (3) It resembles the behavior of real users so is hard to detect. Nevertheless, autoregressive method does not exploit output rankings and item properties like similarity due to restricted data access, which could limit the full utilization of a sequential recommender. 4.1.2 Model Distillation. We use model distillation [14] to minimize the dierence between𝒇and𝒇by training with generated data(X,^I)(see Figure 2b). To get the most out of a single sequence during distillation, we generate sub-sequences and labels to enrich training data; for an input sequence𝒙 = [𝑥, 𝑥, . . . , 𝑥]fromX, we split it into 𝑇 − 1 entries of 𝒙, 𝒙, . . . , 𝒙following training strategies in [13, 25]. Compared to traditional model distillation [14,20], where a model is distilled from predicted label probabilities, in our setting we only have top-k ranking list instead of probability distributions. So we propose a method to distill the model with a ranking loss. We can access the white-box model𝒇output scores to items from the black-box top-k list^𝑰 = 𝒇(𝒙), which is dened as^𝑺= [𝒇(𝒙)]. For example, for^𝑰= [25, 3, . . . , 99], ^𝑺= [𝒇(𝒙), 𝒇(𝒙), . . . , 𝒇(𝒙)]). We also sample𝑘negative items uniformly and retrieve their scores as The loss function consists of two terms. The rst term emphasizes ranking by computing a marginal ranking loss between all neighboring item pairs in as the black-box recommender system. The second term punishes negative samples when they have higher scores than the top-k items, such that the distilled model learns to ‘recall’ similar top-k groups for next-item recommendation. 𝜆 and 𝜆are two margin values to be empirically set as hyperparameters. 4.2 Downstream Aacks To investigate whether attacks can be transfered from the trained white-box model we introduce two model attacks against sequential recommender systems: prole pollution attacks and data poisoning attacks, see Figure 1 for illustration of the two attack scenarios. 4.2.1 Profile Pollution Aack. As described in Figure 3a, we perform prole pollution attacks to promote item exposure and use Algorithm 1 to construct the manipulated sequence in the input space. Because we can access the gradients in white box models, we are able to append ‘adversarial’ items by extending adversarial example techniques (e.g. Targeted Fast Gradient Sign Method (T-FGSM) [ (e.g. item IDs), with an assumption that the optimal item is ‘close’ to target item in embedding space. Therefore we can achieve user-specic item promotion without the black-box recommender being retrained as below. Step 1: Compute Gradients at the Embedding Level. 𝒛by appending adversarial items after 𝒇from the previous step, we feed the embedded input input embeddings using a cross-entropy loss, where the target item 𝑡 is used as a label: ∇ = ∇ Step 2: Search for Adversarial Candidates. compute the perturbed embeddings computed with all item embeddings across items are tested with Algorithm 1: Adversarial Item Search for Prole Pollution sequence𝒙, target𝑡, expected length of polluted sequence𝑇, white-box model𝒇(i.e.𝒇,𝒇),𝜖and𝑛; Algorithm 2: Adversarial Prole Generation for Data Poisoning item. It can be repeated for multiple injection items for better attack performance, and to avoid disproportionate target items in injection, we require target items not appear continuously. 4.2.2 Data Poisoning. Data poisoning attacks operate via fake prole injection, to promote target item exposure as much as possible (after retraining with the fake and normal proles). We propose a simple adversarial strategy (visualized in Figure 3b) to generate poisoning data with white-box model𝒇. The intuition behind our poisoning data generation is that the next item should be the target even given sequences of seemingly irrelevant items. In this case, we follow co-visitation approach [36,42] and apply adversarial example techniques [8] to generate poisoning data. (1) We consider one poisoning sequence using alternating items pairs (e.g.𝒛 =[target,𝑧, target,𝑧, target, ...]); (2) We try to nd irrelevant/unlikely items to ll𝑧in𝒛. In detail, we use a similar approach to Algorithm 1, where the generation process rst computes backward gradients similarly to the cross entropy loss and T-FGSM. However, in narrowing candidate items we choose fromIwith the lowest similarity scores to˜𝒛 − 𝜖sign(∇)(instead of the highest); (3) We repeat (1) and (2) to generate the poisoning data; details can be found in Algorithm 2. Note that though the alternating pattern of co-visitation seems detectable, we can control the proportion of target items (apply noise or add ‘user-like’ generated data) to avoid this rigid pattern and make it less noticeable. Gradient information from the white-box model also empowers more tailored sequential-recommendation poisoning methods. 5 EXPERIMENTS 5.1 Setup 5.1.1 Dataset. We use three popular recommendation datasets (see Table 1) to evaluate our methods: Movielens-1M (ML-1M) [ data into implicit feedback. We follow SASRec [ for validation and testing, and the rest for training. We set black-box API returns to be top-100 recommended items. 5.1.2 Model. To evaluate the performance of our attack, we implement a model extraction attack on three representative sequential recommenders, including our PyTorch implementations of NARM [ with dierent basic blocks and training schemata, shown in Table 2. •NARM is an attentive recommender, containing an embedding layer, gated recurrent unit (GRU) [ and local encoder, an attention module to compute session features and a similarity layer, which outputs the most similar items to the session features as recommendations [25]. •SASRec consists of an embedding layer that includes both the item embedding and the positional embedding of an input sequence as well as a stack of one-directional transformer (TRM) layers, where each transformer layer contains a multi-head self-attention module and a position-wise feed-forward network [17]. •BERT4Rec has an architecture similar to SASRec, but using a bidirectional transformer and an auto-encoding (masked language modeling) task for training [37]. 5.1.3 Implementation Details. Given a user sequence data and use the last two items for validation and testing respectively. We use hyper-parameters from grid-search and suggestions from original papers [ in Table 3. Additionally, all models are trained using Adam [ batch size 128 and 100 linear warmup steps. We follow [ Beauty as 1% of a real prole’s size as a poisoning prole size. Code and data are released Table 3. Configurations. N:NARM, S:SASRec, B:Bert4Rec, ly:layer, h:aention head, dr:dropout rate, mp:masking probability. 5.1.4 Evaluation Protocol. We follow SASRec [ for each user. Then we rank them with the positive item and report the average performance on these 101 testing items. Our Evaluation focuses on two aspects: Table 1. Data StatisticsTable 2. Sequential Model Architecture 10], Steam [27] and Amazon Beauty [29]. We follow the preprocessing in BERT4Rec [37] to process the rating {200, 50, 50}respectively, which are also applied as our generated sequence lengths. We follow [39] to use Table 4. Extraction performance under identical model architecture and 5k budget, with Black-box original performance. (a) Data Distribution for origi-(b) Cross model extraction results. Horizontal / vertical axes represent white-box / black-box nal and generated data.model architectures. Darker colors represents larger values of Agr@10. • Ranking Performance: We to use truncated Recall@K that is equivalent to Hit Rate (HR@K) in our evaluation, and Normalized Discounted Cumulative Gain (NDCG@K) to measure the ranking quality following SASRec [17] and BERT4Rec [37], higher is better. • Agreement Measure: We dene Agreement@K (Agr@K) to evaluate the output similarity between the black-box model and our extracted white-box model: whereBis the top-K predicted list from the black-box model andWis from our white-box model. We report average Agr@K with 𝐾 = 1, 10 to measure the output similarity. 5.2 RQ1: Can We Extract Model Weights without Real Data? 5.2.1 Standard Model Extraction. We evaluate two ways (random and autoregressive as mentioned in Section 4.1.1) of generating synthetic data with labels for model extraction. In standard model extraction, we assume the model architecture is known, so that the white-box model uses the same architecture as the black-box model (e.g. SASRec→SASRec) without real training data. We report results with a xed budgets 𝐵 = 5000 in Table 4. Observations.From Table 4 we have a few observations: (1) Interestingly, without a training set, random and autoregressive can achieve similar ranking performance (N@10 and R@10) as the black-box. For example, compared with black-box NARM on ML-1M, R@10 for random drops 1.34% and autoregressive drops only 0.98%. On average, extracted recommenders’ R@10 is about 94.54% of the original. Particularly, random is trained on random data, but labels are Table 5. Influence of data sparsity. Model extraction on 𝑘-core Beauty retrieved from black-box model, reecting correct last-click relations. Last clicks help random rank well, but agr@K is much poorer than autoregressive (see Table 4). (2) Autoregressive has signicant advantages in narrowing the distance between the two recommenders in all datasets, with an average Agr@10 of resembles the true training data distribution much better than random, because autoregressive generates data following interactions with recommended items. Although sampling from the popularity distribution can also resemble the original data distribution, it breaks the assumption that we have no knowledge about the training set, and cannot capture similarities from sequential dependencies. (4) We also note that the datasets have large dierences in the distillation process. For example, relatively dense datasets with many user interactions like ML-1M and Steam increase the probability of correct recommendations. Extracted recommenders based on such data distributions sustain a high level of similarity with respect to the black-box output distribution, while in sparser data, it could lead to problems like higher divergence and worse recommendation agreement, which we will examine in the next subsection. (5) Moreover, Table 4 indicates that NARM has the best overall capability of extracting black-box models, as NARM is able to recover most of the black-box models and both its similarity and recommendation metrics are among the highest. As for SASRec and BERT4Rec, both architectures show satisfactory extraction results on the ML-1M and Steam datasets, with SASRec showing slight improvements compared to BERT4Rec in most cases. 5.2.2 Cross Model Extraction. Based on the analysis of dierent architectures, a natural question follows: which model would perform the best on a dierent black-box architecture? In this setting, we adopt the same budget and conduct cross-extraction experiments to nd out how a white-box recommender would dier in terms of similarity when distilling a dierent black-box architecture. Cross-architecture model extraction is evaluated on these three datasets. Observations. box / black-box architectures. The NARM model performs the best overall as a white-box architecture, successfully reproducing most target recommender systems with an average of compared to 0.548 of SASRec and 0.471 of BERT4Rec. Results are visualized with heatmaps in Figure 4b, where the horizontal / vertical axes represent white- 5.3 RQ2: How do Dataset Sparsity and Budget Influence Model Extraction? Data Sparsity.In the previous experiments, we notice that the sparsity of the original dataset on which the black-box is trained might inuence the quality of our distilled model, suggested by the results on the Beauty dataset from Table 4. For the sparsity problem, we base a series of experiments on the most sparse dataset (Beauty); all three models are used to study whether model extraction performance deterioration is related to the dataset. We choose slightly dierent preprocessing techniques to build a𝑘-core Beauty dataset (i.e. densify interactions until item / user frequencies are both≥ 𝑘). The processed𝑘-core datasets are denser with increasing𝑘. Our item candidate size (100 negatives) in evaluation does not change. Black-box models are trained on such processed data followed by autoregressive extraction experiments with a5000sequence budget in Table 5. As sparsity drops, compared to the 5-core Beauty data, both black-box and extracted models perform better, where the increasing Agr@10 indicates that the extracted models become more ‘similar’ to the black-box model. Our results indicate that data sparisty is an important factor for model extraction performance, where training on denser datasets usually leads to stronger extracted models. Budget.We also want to nd out how important the query budget is for distillation performance. In our framework we assume that the attacker can only query the black-box model a limited number of times (corresponding to a limited budget). Intuitively, a larger budget would induce a larger generated dataset for distillation and lead to a better distilled model with higher similarity and stronger recommendation scores. However, it is preferable to nd an optimal budget such that the white-box system is ‘close enough’ to generate adversarial examples and form threats. Our experiments in Table 6 suggest that an increasing budget would lead to rapid initial improvements, resulting in a highly similar white-box model. Beyond a certain point, the gains are marginal. 5.4 RQ3: Can We Perform Profile Pollution Aacks using the Extracted Model? Setup.In prole pollution, we inject adversarial items after original user history and test the corrupted data on the black-box recommender. The average lengths of ML-1M, Steam and Beauty are 166, 13 and 9 respectively; based on this we generate10adversarial items for ML-1M and2items for Steam and Beauty. We perform prole pollution attacks on all users and present the average metrics in Figure 5. We avoid repeating targets in injection items to rule out trivial solutions like sequence of solely target items. Based on this setting, we introduce the following baseline methods: (1) RandAlter: alternating interactions with random items and target item(s) [36]; (2) Deep Q Learning (DQN): naive Q learning model with RNN-based architecture, the rank and number of target item(s) in top-k recommendations are used as training rewards [44,45]; (3) WhiteBox SimAlter: alternating interactions with similar items and target item(s), where similar items could be computed based on similarity of the white-box item embeddings. (4) In addition, we experiment with the black-box recommender as a surrogate model and perform our attacks (BlackBox-Ours.). Popularity Attack NARM SASRec BERT4Rec NARM SASRec BERT4Rec NARM SASRec BERT4Rec head middle tail Table 7. Profile pollution aacks to dierent sequential models for items with dierent popularity, reported in N@10. General Attack Performance Comparisons. with baselines on three dierent datasets. (1) Comparing our results with BlackBox-Ours. black-box models show the vulnerabilities to pollution attacks from extracted white-box model generally. We notice that the attacking performance of distilled models is comparable in ML-1M, but leads to worsening metrics as datasets become sparser and recommenders become harder to extract, for eaxmple, the average metrics of NARM on ML-1M reach 94.8% attack performance of the black-box model, compared to 66.2% on Steam and 55.6% on Beauty. (2) On all datasets, our method achieves the best targeted item promotion results. For example, on Steam, N@10 scores of the targeted items are signicantly improved from 0.070 to 0.381. This shows the benet of exploiting the surrogate model from model extraction, and our attacking method is designed as an adversarial example method [ the victim models. (3) Empricially, we nd that the robustness varies for dierent victim recommenders. For example, results on Steam and Beauty datasets show SASRec is the most vulnerable model under attacks in our experiments. For instance, N@10 of SASRec increases from 0.071 to 0.571 on average on Steam and Beauty datasets, but N@10 of NARM increases from 0.0680 to 0.286. Items w/ Dierent Popularities. group target items using the following rules [ rest according to the item appearance frequency (popularity). From Table 7, our attack method is eective for items with dierent popularities. But in all scenarios, ranking results after attacking decrease as the popularity of the target item declines; popular items are generally more vulnerable under targeted attacks and could easily be manipulated for gaining attention. Unpopular items, however, are often harder to attack. 5.5 RQ4: Can We Perform Data Poisoning Aacks using the Extracted Model? Setup.Dierent from prole pollution, we do not select a single item as a target and use target groups instead as the attack objective to avoid a large number of similar injection proles and accelerate retraining. Target selection Popularity Attack NARM SASRec BERT4Rec NARM SASRec BERT4Rec NARM SASRec BERT4Rec head medium tail Table 8. Data poisoning aacks to dierent sequential models for items with dierent popularity, reported as N@10. is identical to prole pollution and we randomly select an attack target from the 25 items in each step during the generation of adversarial proles. Then, the black-box recommender is retrained once and tested on each of the target items, we present the average results in Figure 6. We follow [39] to generate proles equivalent to1%of the number of users from the original dataset and adopt the same baseline methods in prole pollution. General Attack Performance Comparisons.(1) Our methods surpass other baselines but overall promotion is not as eective as prole pollution. It is because we adopt multiple targets for simultaneous poisoning, and prole pollution attacks specic user with proles information, where attacking examples can be stronger. (2) Compared to RandAlter, the proposed adversarial prole generation further enhances this advantage by connecting the unlikely popular items with targets and magnifying the bias for more exposure of our target items. For example in Beauty, the average N@10 is 0.066 before attack against 0.240 by our method across three models. Moreover, we notice that DQN performs worse than in prole pollution and could occasionally lead to performance deterioration. Potential reasons for the performance drop could be: Less frequent appearance of target items against the co-visitation approach; Updated model parameters are independent from the generated fake proles, as in [39]. (3) Comparable results between BlackBox-Ours. and WhiteBox-Ours. suggest the bottleneck for data poisoning can be generation algorithm instead of white-box similarity. Items w/ Dierent Popularities.Table 8 reveals the poisoning eectiveness as a function of target popularity. In contrast to the numbers in Table 7, the relative improvements are more signicant for middle and tail items. For example, the relative improvement for head is 30.8%, compared to over 300% for middle items and over 1000% for tail items in N@10. The results suggest that data poisoning is more helpful in elevating exposure for less popular items, while the promotion attacks of popular items via prole injection can be harder in this case. 6 CONCLUSION AND FUTURE WORK In this work, we systematically explore the feasibility and ecacy of stealing and attacking sequential recommender systems with dierent state-of-the-art architectures. First, our experimental results show that black-box models can be threatened by model extraction attacks. That is, we are able to learn a white-box model which behaves similarly (for instance, with 0.747 top-10agreement on the ML-1M dataset) to the black-box model without access to training data. This suggests that attacks on the white-box could be transferred to the black-box model. To verify this intuition, we conduct further experiments to study the vulnerability of black-box models using prole pollution and data poisoning attacks. Our experiments show that the performance of a well-trained black-box model can be drastically biased and corrupted under both attacks. For future work, rst, we can extend our framework to more universal settings. In particular, how can we perform model extraction attacks for matrix factorization models or graph-based models? Second, it would be interesting to develop defense algorithms or other novel robust training pipelines, so that sequential recommender systems could be more robust against adversarial and data poisoning attacks. Third, active learning can be applied to nd more eective sampling strategies for model extraction with fewer queries.