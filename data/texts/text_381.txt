Over the past decade, tremendous progress has been made in inventing new RecSys methods. However, one of the fundamental problems of the RecSys research community remains the lack of applied datasets and benchmarks with well-dened evaluation rules and metrics to test these novel approaches. In this article, we present the TTRS - Tinko Transactions Recommender System benchmark. This nancial transaction benchmark contains over 2 million interactions between almost 10,000 users and more than 1,000 merchant brands over 14 months. To the best of our knowledge, this is the rst publicly available nancial transactions dataset. To make it more suitable for possible applications, we provide a complete description of the data collection pipeline, its preprocessing, and the resulting dataset statistics. We also present a comprehensive comparison of the current popular RecSys methods on the next-period recommendation task and conduct a detailed analysis of their performance against various metrics and recommendation goals. Last but not least, we also introducePersonalizedItem-FrequenciesbasedModel (Re)Ranker – PIFMR, a simple yet powerful approach that has proven to be the most eective for the benchmarked tasks. • Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability. datasets, neural networks, recommender systems ACM Reference Format: Sergey Kolesnikov, Oleg Lashinin, Michail Pechatov, Alexander Kosov . 2021. TTRS: Tinko Transactions Recommender System benchmark. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn In the current era of big data and global personalization, Recommender Systems (RecSys) are playing a pivotal role in improving user experience in a large variety of domains: movies [43], music [35], news [12] and many more [5,34,55]. There are many dierent types of recommendations, such as rating prediction [23], as well as top-n [9], sequential [54], session-based [53], and next-basket [16,37,40] recommendations. Thanks to close collaboration between academia and industry professionals [3,7] on joint research topics, many new articles and methods are released every year [58]. Various new techniques aim to improve RecSys approaches with deep learning-based methods [28,37,56], memory-based methods [42], latent factor-based methods [9,24,38], or reinforcement learning [1,29]. With such a rapid growth of new approaches, it is almost impossible to evaluate all of them correctly. Moreover, each application requires its own logic for data preprocessing and evaluation setup [32]. Last but not least, current machine learning methods require careful selection of hyperparameters, which only complicates evaluation correctness [4, 50]. To combat the challenges above, many RecSys benchmark frameworks have been created: RecBole [60], Elliot [2], ReChorus [51], DaisyRec [47]. All these benchmarks cover such popular tasks as top-n, next-item, or next-basket recommendations, which are essential for real-world applications. However, other equally valuable scenarios have received little attention in the research community. In our work, we focus on one of these scenarios. Specically, we consider forming personalized recommendations for a certain period – the next-period recommendation task [59]. This task can have various business applications, such as monthly customers’ cashback recommendations or products-of-interest list creation during users’ online sessions on websites [48]. To this end, we would like to propose a large-scale nancial transactions dataset and provide a comprehensive comparison of the current popular RecSys methods on the next-period recommendation task. The contributions of this paper are summarized as follows: • Our rst contribution is a large-scale nancial transactions dataset – TTRS (Section 3). It contains over 2 million interactions between almost 10,000 users and more than 1,000 merchants for a total of 14 months. To the best of our knowledge, it is the rst publicly available dataset of nancial transactions. •Our second contribution is benchmarking a various number of existing RecSys methods on the next-period recommendation task (Section 4). To ensure the reproducibility criteria, we carefully describe the evaluation techniques and the metrics required for correct benchmarking. In addition, we conduct a comprehensive quantitative study and comparative analysis of the dierent methods used in our benchmarking. •The nal point of our study is the proposal of a new approach: PersonalizedItem-Frequencies-basedModel (Re)Ranker, or PIFMR. It summarises our experiments’ ndings and achieves the best results on the benchmarked tasks. Dataset# users # items # interactions # users # items # interactions We hope that our open-sourced TTRS dataset with a described benchmark on the next-period recommendation task and the proposed PIFMR approach will serve as a foundation for other industrial research labs to develop real-world applied standards for the progress of the RecSys eld. Large-scale datasets and benchmarks have successively proven their fundamental importance in many research elds. RecSys was no exception, and many recent breakthroughs came with the emergence of such benchmarks [11,30]. In this section, we observe popular datasets and methods as well as associated benchmarks and evaluation strategies. In general, the majority of recommender systems aim to learn users’ interests. There are several specic ways we can formulate such recommendation tasks. In the classical formulation, the researchers hide some of the user’s interactions for a test set. Next, the model ranks all items for each user, trying to predict the hidden interactions. This task is called the top-n recommendation task [10]. In another scenario, the model knows the user’s previous ordered history of interactions and predicts the next item the user could interact with. This is the next-item prediction task [36]. If the user can consume sets of items simultaneously and we want to predict the whole set of interactions, this is called the next-basket prediction task [36]. Finally, if we are curious about user interests over time, we could predict their interactions for a predened period, which is the next-period recommendation task [59]. In this work, we are focused on the former task. Other similar tasks, for example, click-through rate prediction [20], rating prediction [23], are not the focus of our work. To compare with the transactional nature of TTRS, we reviewed the available RecSys datasets for additional properties: interaction timestamp, transaction amount, and meta-information. These properties are necessary for future studies of the proposed standard. With such requirements, we found two publicly available transaction-based datasets: •Ta-Feng- a Chinese grocery store dataset that has basketbased transaction data from November 2000 to February 2001. Each transaction has a timestamp. Items with identical timestamps are considered as one basket. This dataset is widely used for next-basket prediction research [14,16,25]. •Dunnhumby- a dataset provided by Dunnhumby that contains customers transaction data over a period of 117 weeks from April 2006 to July 2008. For benchmarking purposes, we select the "Let’s Get Sort-of-Real sample 50K customers" version of the dataset, which is well-known among the research community [13, 15, 16]. Statistical information on the raw datasets is summarized in Table 1. Statistical Recommenders. Statistical Recommenders base their predictions on learned regular patterns. The simplest methods are TopPopular and TopPersonal, which we will describe in detail in the Experiments section. General Recommenders. General Recommenders explore user feedback, which can be represented as a user-item interaction matrix. Historical data for each user is treated as an unordered collection of items, and the main goal is to predict relevance scores in missing values for future recommendations. The most known method for this task is matrix factorization [9,17,26,39], which learns the hidden representations of users and items. Another possible approach is item similarity models [21,33,45,52], which learn the item-item similarity matrix. Besides machine learning-based techniques, there is also a broad range of deep learning methods. For example, there are plenty of VAE-based methods [27,28,31,44], which generalize linear latent-factor models with neural networks. Sequential Recommenders. Another subeld of the RecSys methods domain is sequential recommenders. In the next-item prediction task, historical data for each user is stored as a time-ordered sequence of items users interacted with, and the goal is to predict the next relevant item. While prior methods, like First-order Markov Chains [40], assume that users’ later actions depend only on the last item they interacted with, current sequential recommenders nd dependencies using a complete history of user’s interactions. To nd such sequential patterns, CNNs [57], RNNs [49], and the selfattention mechanism [22,46] are widely used. In the next-basket prediction task, the data is represented as a sequence of sets of purchased items, and the main goal is to predict a whole set of required items that would follow already purchased ones, rather than only one item. A wide range of basket-based methods [14,16,37] can be used for this task. Table 2: TTRS dataset description. We use 𝑝𝑎𝑟𝑡𝑦_𝑟𝑘 as user identier and 𝑚𝑒𝑟𝑐ℎ𝑎𝑛𝑡_𝑔𝑟𝑜𝑢𝑝_𝑟𝑘 as item identier. Evaluating Recommender Systems can be challenging due to various possible data splitting strategies and data preparation approaches. In early works [6,8], the researchers highlighted the importance of time-based algorithm validation. In a prior study [47], the authors sampled 85 papers published in 2017-2019 from top conferences and concluded that random-split-by-ratio and leaveone-out splitting strategies are used in 82% cases. At the same time, recent studies [32] pointed out that the most strict and realistic setting for data splitting is a global temporal split, where a xed time-point separates interactions for training and testing. The authors found that only 2 of 17 recommender algorithms (published from 2009 to 2020) were evaluated using this scenario. In another work [18], the authors compare the impact of data leaks on dierent RecSys methods. They found that "future data" can improve or deteriorate recommendation accuracy, making the impact of data leakage unpredictable. In this paper, to avoid all the issues above, we use a global temporal K-fold validation scheme (Section 4). In this section, we describe the pipeline for collecting and processing the TTRS dataset. We rst present the raw data collection pipeline and then provide details of the data preprocessing logic and extra ltering eorts to make the data consistent for research needs. Data Description. The crucial part of the TTRS dataset is the diversity of the transaction sources. While other datasets handle only merchant user activity, TTRS contains the whole user nancial activity – supermarkets, clothing stores, online delivery shops, cinemas, gas stations, cafes and restaurants, museums, etc. Thus, TTRS contains anonymized information about the daily interests of users based on their transactional activity. To the best of our knowledge, this is the rst publicly available dataset that makes it possible to build nancial activity recommendations. Data Collection. Our dataset contains transaction information of a randomly selected 50 thousand Tinko users from January 2019 to March 2020. Each transaction contains an anonymized user id, transaction type, information about a purchased merchant, transaction timestamp, and transaction amount. Full description of the dataset can be found in Table 2. Data Preprocessing Pipeline. To prepare the dataset for benchmarking, we apply a few preprocessing steps. First, we truncate the dates to have full months for simplied evaluation on time periods of weeks and months. Secondly, we remove users and items with less than ten interactions in the rst six months and lter users with less than one transaction per month to reduce possible anomalies. As the number of interactions between the remaining users and items could change after ltering, we repeat the second step several times until the data converges. Statistic information about clean datasets after preprocessing is summarized in Table 1. For our benchmark, we chose the next-period recommendation task with a period of one month. The main goal of our benchmark was to predict users’ interactions in the next month, using their interaction history over the past few months. In this section, we will go through the experiment setup, benchmarked methods and introduce our main ndings and improvements. Metrics. We compare models with each other using standard metrics widely utilized by researchers: Recall@K, NDCG@K, and MAP@K. Each metric can be calculated for a recommendation list of length K, where K ranges from 1 to the number of items. K is usually called the cuto, which stands for the length to which the recommendations are cut. We use @10, @20, @50 cutos during benchmarking. Validation Scheme. To get the most accurate results, we use several ideas from prior articles [6,11,32]. Firstly, we use a global temporal split to separate our training data from test one and prevent possible data leaks associated with seasonal user preferences. Secondly, we use temporal cross-validation with several folds for a more precise model evaluation. Finally, for each such fold with N periods, we use the optimal hyperparameter search through extra data partitioning into ”𝑡𝑟𝑎𝑖𝑛” (N-2 periods), ”𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛” (1 period), and ”𝑡𝑒𝑠𝑡” (1 period) splits. The best hyperparameters found on the(”𝑡𝑟𝑎𝑖𝑛”,”𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛”)split were used to initialize and train the model for nal testing on the(”𝑡𝑟𝑎𝑖𝑛 +𝑣𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛”,”𝑡𝑒𝑠𝑡”)split. The entire validation process is shown in Figure 1 Hyperparameter Search. Similar to previous studies [11], we search for the optimal parameters through Bayesian search using the implementation of Scikit-Optimize. For each pair (algorithm, test Figure 1: Evaluation procedure. On step (I), we train model on the ( for the rst fold. On step (II), we use these hyperparameters to train the model on the ( evaluate the model on the rst fold. On step (III), we repeat the (I) and (II) steps for the next fold. fold), we iterate over 25 hypotheses, where the rst 5 are random. We use𝑀𝐴𝑃@10 metric for model selection. We share all the code as well as details of the respective hyperparameter ranges and the nal algorithms’ hyperparameters online. Computational Resources. The experiments were run on a single machine with NVIDIA Tesla V100, 200GB RAM, and Intel(R) Xeon(R) CPU @ 3.00GHz (16 cores) for 5 days. In this section, we would like to briey describe approaches used for benchmarking next-period recommendations: • TopPopular[19],TopPersonal[16] are simple RecSys baselines that work based on item popularity. The TopPopular algorithm recommends the most popular items, sorting them in descending order of global popularity. The TopPersonal method focuses on items with which the user has already interacted. The recommendation list is created by sorting them in descending order of interaction frequency. If no personal recommendations are found, TopPersonal uses the TopPopular approach for prediction. • NMF[26],PureSVD[9],IALS[17] are matrix factorizationbased (MF-based) models. These models are designed to approximate any value in the interaction matrix by multiplying the user and item vectors in the hidden space. The interaction matrix could be represented as a matrix with interaction frequencies or in binary form. We use𝑏𝑖𝑛𝑎𝑟𝑦_𝑚𝑎𝑡𝑟𝑖𝑥hyperparameter to handle such data preprocessing. In the rst case, we apply the𝑙𝑜𝑔(1+𝑝)transformation. In another case, all frequencies above 0 were replaced by 1. • SLIM[33],EASE[45] are linear models that learn an itemitem weight matrix. Similar to MF-based models, we add a hyperparameter𝑏𝑖𝑛𝑎𝑟𝑦_𝑚𝑎𝑡𝑟𝑖𝑥to approximate frequencies or a binary mask of interactions. ”𝑇𝑟𝑎𝑖𝑛1”,”𝑉 𝑎𝑙𝑖𝑑𝑎𝑡𝑖𝑜𝑛1”) split to nd optimal hyperparameters • Multi-VAE[28],Macrid-VAE[31],RecVAE[44] are variational autoencoder approaches (VAE-based) for a top-n recommendation task. They utilize the idea of using multinomial likelihood to recover inputs from hidden representations and use them for recommendations. • GRU4Rec[49],SASRec[22],BERT4Rec[46] are sequentialbased models. Unlike previously mentioned methods, these models know the sequence of users’ interactions and learn sequence representation with RNN- or self-attention-based neural networks. This representation is then used for nextitem recommendations. • RepeatNet[37] is an RNN-based model that uses a repeatexplore mechanism for session-based recommendations. The model has two dierent recommendation modes. In the rst, "repeat" mode, the model recommends something from users’ consumption history. In the second, "explore" mode, the model recommends something new that hasn’t been listed in the input sequence. • ItemKNN[41] is an item-based k-nearest neighbors method, which utilizes similarities between previously purchased items. Similar to [11], we used dierent similarity measures during our experiments: Jaccard coecient, Cosine, Asymmetric Cosine, and Tversky similarity. • TIFUKNN[16] is a current state-of-the-art method for the next-basket recommendation task. It uses the idea of learning Temporal Item Frequencies with the k-nearest neighbors approach. To summarize, we consider 16 models, 7 of which are neural networks, and 5 are sequence-aware. For benchmarking purposes, we include approaches of dierent types such as matrix factorization, linear models, variational autoencoders, recurrent neural networks, and self-attention-based methods. We adapted all the above models for the next-period recommendation task. To predict the next period (one month in our case), we use all available history for Statistical or General Recommenders Table 3: Next-month recommendation benchmark. Ground truth test interactions are not necessarily new to users, and recommendation lists are not ltered in any way. All metrics are averaged over 2 (TaFeng) or 6 (TTRS and Dunnhumby) test folds. Benchmark time is estimated as the overall time required for a hyperparameter search (25 hypotheses) and all included metrics calculations. = (𝑆 − 𝑆+ 𝑐)/(𝑆− 𝑆+ 2 ∗ 𝑐) and 100 last interactions for Sequential Recommenders. All models produced predictions in the form of sorted lists of items, and were evaluated on the next-month recommendation task in the same manner. The results of the model’s comparison can be found in Table 3. Several observations can be made based on these results. First, the simple baselines based on personalized items’ popularity are compatible with other methods across all datasets. This indicates the importance of the user’s repeat consumption pattern for recommendations. Second, many current RecSys methods of dierent types (MF, linear, VAEs, sequential) could hardly beat the TopPersonal baseline across all metrics and datasets. We believe the reason for that is that for these approaches, the repetitive nature of the datasets is too dicult to generalize. Third, recently proposed deep learning methods, such as RepeatNet, achieve performance comparable with the TopPersonal baseline. We believe the reason is that the "repeat" mode of the proposal network helps generalize the datasets’ repetitive nature. Analyzing benchmark results, it is easy to notice the importance of repetitive user interactions with items. For example, the TopPersonal baseline often gives better results than recent machine learning approaches, where only one method out of 14 could achieve a better MAP@10 score. However, note that TopPersonal also has several disadvantages: (1) it cannot correctly rank items with the same consumption frequency, (2) it is unable to utilize users’ interaction history to identify novel items for recommendation. To overcome these issues, we propose a simple yet powerful improvement by introducingPersonalizedItem-Frequencies-based Model (Re)Ranker - PIFMR. Suppose we have a history of user interaction with items, and we can aggregate these interactions into a vector𝐹 = (𝑓, ..., 𝑓)where𝑓is the number of times that itemiwas purchased by a useruandmis the number of items. Recommending new items by these frequencies will bring us to a TopPersonal-like approach. On the other hand, using the same interaction history, we could train a simple model such as EASE, SLIM, MF, or VAE and form their prediction vector as𝑆 = (𝑠, ..., 𝑠)where𝑠is a relevance score for a user-item pair (u, i). The main idea of PIFMR is to use Personalized Item Frequencies (vector𝐹) to re-rank model predictions (vector𝑆). We also perform a monotonic transformation on the model’s scores so that they lie in the(0,1)interval. As a result of this, for any pairs of items𝑘and𝑟 PIFMR gives such predictions𝑝, where𝑝< 𝑝if𝑓< 𝑓. The nal algorithm is presented in Algorithm 1. Retrained PIFMR-based models could be found in Table 3, "PIFMR" type. Combining predictions with a PIFMR-based model, we solved several challenges at once: (1) the model is capable of learning how to rank items with the same consumption frequency, (2) thanks to base model usage, we could nd patterns in the users’ behavior and recommend new personalized items, (3) the model easily identies high-frequency repetitive patterns in consumption history, (4) as well as robust low-frequency purchase "anomalies", thanks to the 𝑀𝑖𝑛𝐹𝑟𝑒𝑞 threshold. When it comes to the importance of repeated patterns, it is also interesting to investigate the benchmarked methods’ ability to nd new relevant items for users that are not yet present in their history. To do so, we test the same trained models on a slightly dierent task. Rather than analyze the method’s performance across all possible items, both new and repeated for that user, we measure its eciency on new items only. The results of the models’ comparison for this recommendation goal can be found in Table 4. A few interesting observations can be made based on our results. First, while TIFUKNN is showing average results across datasets, it does the best on the TaFeng one. A possible reason for this could be that the amount of data in the TaFeng dataset is rather small compared to the other ones. Second, TTRS is the only dataset where the TopPopular approach still achieves competitive results that could show the similarity of user interests in this dataset. Third, MF-based methods show the worst performance on this task across all datasets. Fourth, RepeatNet performs worse than TopPopular, which may indicate that it overts to the "repeat" mode rather than "explore". Lastly, while the proposed PIFMR approach usually slightly worsens the performance of the MultiVAE, EASE, and SLIM methods on this task, it actually improves the performance of the ItemKNN approach (and SLIM on the TaFeng dataset). In addition to the Table 3 results, the above ndings may serve as a good reason for further research on the methodology of PIF-based recommendations. In this paper, we proposed a large-scale nancial transactions benchmark named TTRS that is based on user-merchant interactions. We evaluated various RecSys methods on several transaction-based datasets to compare the eects of dierent factors on the nextperiod recommendation task. As shown by the benchmark, the user consumption repeatability factor is ubiquitous in many real-world applications and challenging for current RecSys methods. With this new benchmark, we also presented a simple yet powerful approach: PersonalizedItem-Frequencies-basedModel (Re)Ranker, or PIFMR, which helped in improving the performance of RecSys methods on benchmarked tasks.