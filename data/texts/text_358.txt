Session-based recommendation plays a central role in a wide spectrum of online applications, ranging from e-commerce to online advertising services. However, the majority of existing session-based recommendation techniques (e.g., attentionbased recurrent network or graph neural network) are not well-designed for capturing the complex transition dynamics exhibited with temporally-ordered and multi-level interdependent relation structures. These methods largely overlook the relation hierarchy of item transitional patterns. In this paper, we propose a multi-task learning framework with Multi-level Transition Dynamics (MTD), which enables the jointly learning of intra- and inter-session item transition dynamics in automatic and hierarchical manner. Towards this end, we ﬁrst develop a position-aware attention mechanism to learn item transitional regularities within individual session. Then, a graph-structured hierarchical relation encoder is proposed to explicitly capture the cross-session item transitions in the form of high-order connectivities by performing embedding propagation with the global graph context. The learning process of intra- and inter-session transition dynamics are integrated, to preserve the underlying low- and highlevel item relationships in a common latent space. Extensive experiments on three real-world datasets demonstrate the superiority of MTD as compared to state-of-the-art baselines. Personalized recommendation has attracted a lot of attention in real-life applications, to alleviate information overload on the web (Xia et al. 2020). In various recommendation scenarios, session-based recommendation has become an important component in many online services (e.g., retailing and advertising platforms) (Huang et al. 2004), to address the unavailability issue of user information in realistic scenarios (such as non-logged in customers or users without historical interactions) (Quadrana et al. 2017; Ren et al. 2019; Yuan et al. 2020). At its core is to predict the next Corresponding author: Yong Xu Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. York University, Canada interactive item based on a group of anonymous temporallyordered behavior sequences of users (e.g., clicked, browsed or purchased item sequences) (Liu et al. 2018; Wang et al. 2020, 2019a). To facilitate the study of session-based recommendation, many efforts have been devoted to developing various deep neural network models, by exploring correlations between the future interested item and past interacted ones, which contributes to smarter recommendations. Existing session-based recommendation methods for understanding the item transitional regularities can be grouped into several key paradigms. For example, one key research line aims to capture transitional patterns of interacted item sequence with recurrent neural network (Hidasi et al. 2015; Hidasi and Karatzoglou 2018). Along this line, to aggregate sequential embeddings into a more summarized sessionlevel representation, researchers recently propose to augment recurrent session-based recommendation frameworks with attention mechanism (Li et al. 2017), or rely on the memory network (Liu et al. 2018; Wang et al. 2019a). Furthermore, another recommendation paradigm utilizes graph neural network as the item transitional relation encoder, to model long-term item dependencies within the session based on the structured relation graph (Wu et al. 2019). Despite their effectiveness, we argue that these methods are not sufﬁcient to yield satisfactory recommendation results, due to their failure in encoding complex item transition dynamics which are exhibited with multi-levels in nature (Song et al. 2019). Particularly, in the practical session-based recommendation scenarios, there exist session-speciﬁc short-term and long-term item transitions, as well as the long-range cross-session item dependencies in global context (Al-Ghossein, Abdessalem, and Barr´e 2018). These different inter-correlations among items constitute the underlying multi-level item transition dynamics. As illustrated in Figure 1, while item tand tare not directly connected within the same session, there exist implicit interdependency among them, due to the item transitional relationship of t→ tand t→ tin session B and A, respectively. In such cases, items from different sessions are no longer independent. The dependent signals between interactive items may come from not only the intra-session transition regularities, but also inter-session item relations. However, to simplify the model design, most of current sessionbased recommender systems only explore local contextual features, while the global item transitional patterns across exogenous sessions are neglected. This restricts the capabilities of current models in capturing the hierarchical transition signals for making recommendations. While intuitively useful to perform the joint learning of item relation structures with multi-level transition dynamics, it is non-trivial to do it well. In particular, the item dependencies across different sessions can be complex. It is not necessary that a future interactive item is more relevant to items from a recent session than one that is further away (Kang and McAuley 2018). Hence, when tackling the cross-session item dependencies at various neighbor distances, the high-order relation structures exhibited with item transition patterns from a global perspective over all sessions, is necessary to be investigated in the relation embedding function. Additionally, intra-session item transition patterns vary by sessions. When modeling the time-evolving item correlation within a session, both the user’s sequential behavior (short-term) and the overall cross-session dependencies (long-term) should be taken into account (Liu et al. 2018; Li, Wang, and McAuley 2020). Therefore, it is a signiﬁcant challenge to jointly integrate the intra-session item correlations and inter-session item transition patterns, into the recommendation framework in a fully adaptive manner. Present Work. Motivated by the aforementioned challenges, we propose a new multi-task learning model with Multi-level Transition Dynamics (MTD) for session-based recommendation. In our MTD framework, we ﬁrst devise a position-aware attention mechanism to jointly capture the intra-session sequential item transitions, and sessionspeciﬁc main purchase with the incorporation of position information. Speciﬁcally, we integrate a self-attention model with an attentive aggregation layer to capture the sequential transitional patterns of items within each individual session, without the rigid order assumption of user behavior (i.e., latent states are propagated through temporally-ordered sequences in recurrent framework). To argument the representation learning ability over individual session, an attentive summarization layer is introduced to adaptively perform pattern aggregation. In the hierarchical attentive component, we also seek to explore the item positional information under a sequential encoding module to learn the inﬂuence of time factors. Additionally, inspired by the effectiveness of mutual information maximization in prioritizing global or local structural information in feature learning (Hjelm, Fedorov et al. 2019), we model the cross-session item dependencies in a hierarchical manner, i.e., from item-level embedding learning to global graph-level representation. The developed hierarchically structured encoder via graphical mutual information maximization, endows the MTD with the capability to incorporate inter-session transitional signals from lowlevel to high-level across different sessions. Source code is released at the link https://github.com/sessionRec/MTD. We highlight key contributions of this paper as follows: • We exploit multi-level item transition dynamics in studying the session-based recommendation task. Towards this end, we propose a new recommendation framework which captures the item transition patterns, in the form of of the intra-session item dependencies, as well as the crosssession item relation structures. • We ﬁrst develop a position-aware attentive mechanism to learn the evolving intra-session behavioral sequential signals and the summarized session-speciﬁc knowledge. Furthermore, a global context enhanced inter-session relation encoder is built upon the graph neural network paradigm, to endow MTD for capturing the inter-session item-wise dependencies. • Our extensive experiments on three real-world datasets demonstrate that MTD outperforms different types of baselines in yielding better recommendation results. Also, we show the efﬁciency of our developed model as compared to representative competitors and perform case studies with qualitative examples to investigate the interpretation capability of our MTD model. In this section, we present the technical details of our proposed recommendation framework MTD. We ﬁrst formulate our studied session-based recommendation scenario as follows: Session-based recommendation aims to predict the next action of users based on their anonymous historical activity sequences (e.g., clicks or purchases). Let S = {v, ..., v, ..., v} denote the item candidate set, where M is the number of items. An anonymous session s is a item sequence s = [v, ..., v, ..., v] in a chronological order, where v∈ S denotes the i-th item interested by the user in the session s, and I denotes the length of session s. The recommendation model outputs a list Y = [y, y, ..., y] for each session s, where ydenotes the probability that the next interacted item is v. We ﬁnally make recommendations based on the top-K ranked items in terms of their estimated probability values. To capture item transitional relationships within a session, we integrate two modules for learning the session-speciﬁc item transition patterns: (i) position-aware self-attention network for sequential transition modeling; (ii) attentive aggregation for session-speciﬁc knowledge representation. Self-Attentive Item Embedding Layer. In MTD framework, we leverage the self-attention mechanism to learn the relevance scores over historical interested items within the session and draw the sequential contextual signals. Motivated by the attentive neural network in relation learning (Huang et al. 2019b), self-attention mechanism has been proposed to tackle various sequence modeling tasks such as machine translation (Yang et al. 2019) and user behavior modeling (Kang and McAuley 2018)). Different from the standard attention module, self-attention could bring the beneﬁts of capturing the relevance of past instances (e.g., words or behaviors), and reﬁne the representation process on the single sequence at various distance (Vaswani et al. 2017). Following the transformer network, we build the intra-session transition modeling layer upon the dot-product attention which consists of query, key and value dimensions. The weight matrices W, W, W∈ Rrespectively corresponds to the query, key, value vectors, to map initial item embeddings E∈ Rof session s into latent representations. The operations of self-attention network are deﬁned as follows: K= EW; Att(Q, K, V) = δ(QK√ where we deﬁne X∈ R= Att(Q, K, V) to represent the learned item embeddings with the modeling of pairwise relations between items [v, ..., v, ..., v] in session s.√ δ(·) denotes the softmax function andd is the scaling factor during the inner product operation. We further enhance the self-attentive transition learning module with the modeling of non-linearities with the feedforward network as shown below: eX= FFN(X) = ϕ(X· W+ b) · W+ b(2) we utilize ϕ(·)=ReLU as the activation function. W, W∈ Rand b, b∈ Rare trainable weight matrices and bias terms. After integrating the self-attention layer with the feed-forward network, we generate the embeddingseX∈ Rfor all items [v, ..., v] in each session. Position-aware Item-wise Aggregation Module. We further design a position-aware attentive aggregation component to fuse the encoded item-wise relations for capturing the user main purpose within individual session s. We assign larger importance to the item states in which they have more contextual relations with the future interested item. In particular, for the set of items in session s, we learn a set of weights {α,...,α,...,α} corresponding to the set of learned item embeddingseX= {x, ..., x, ..., x}. Formally, αis calculated as follows: where g ∈ Ris a linear projection vector for generating the weight scalar α. W, W∈ R. σ(·) and δ(·) denotes the sigmoid and softmax function, respectively. The aggregatedP session representation as x, i.e., x=α· x. We further augment the intra-session item-wise fusion module with the injection of positional information, to capture the session-speciﬁc temporally-order signals of items. The dimensionality of positional representation is also set as d. This endows the modeling of relative positions with the incorporation of decay factor into linear transformations: where pdenotes the fused representation with the preservation of relative positional information across different items. We construct a concatenated embedding for individual session of s as q= W[x, x, p], where W∈ R performs the transformation operation. After that, following the implicit feedback-based recommendation paradigm in (He et al. 2020; Wang et al. 2019b), we utilize the inner product between qand embedding of item candidate vas z= qvand deﬁne our loss function of intra-session item relation learning with the cross-entropy as follows: L= −ylog(˜y) + (1 − y)log(1 −˜y) (5) where ydenotes the ground truth label of n-th instance and ˜yis the corresponding estimated result (i.e.,˜y= δ(z)). To comprehensively capture the global cross-session transition dynamics among items, we develop a graph neural network architecture (as illustrated in Figure 2) to inject highorder dependent signals across different sessions into session representations. In particular, we ﬁrst formulate a crosssession item graph G = (V, E) in which nodes V and E are generated from historical sessions. Each session s can be regarded as a path which starts from vand ends at vin graph G. The adjacent matrix A is constructed where each entry a= 1 if there exists a transition relation from item vto vand a= 0 otherwise. We ﬁrst propose a graph-structured message passing architecture to model the local context of transitional signals between different items. We formally deﬁne the corresponding encoding function as follows: where H∈ Rdenotes the learned representations over items under the l-th propagation layer. With the aim of incorporating the self-propagated signals, we update the adjacent matrix with the summation of identify matrix I and the original adjacent matrix A asˆA = A + I. Then, we further apply the symmetric normalization strategy to conduct the information aggregation as:ˆDˆAˆD, whereˆD is the diagonal node degree matrix of A. Global Dependency Representation. After obtaining H = {h, ..., h, ...h}, we propose to capture the highorder global dependencies across correlated items from different sessions. Speciﬁcally, we ﬁrst generate a fused graphlevel emebdding with the aggregation function as: z = τ (H) (R→ R), where τ (·) denotes the mean pooling operation. Motivated by the paradigm of global feature representation with mutual information (Veliˇckovi´c et al. 2019; Velickovic et al. 2019), we enhance our cross-session item relation encoder with the global context of the mutual information between local-level embedding (H) and graph-level representation z. We develop a classiﬁer to perform the global dependency representation under the mutual information learning paradigm. It aims to differentiate positive (h, z) and negative instances (eh, z) in graph G by preserving the underlying cross-session item transition dynamics, the negative sample pair (eh, z) are generated by associating sampled item nodes with the fake embeddings based on the node shufﬂing strategy (Velickovic et al. 2019). Then, both the positive and negative instances are fed into the classiﬁer for classiﬁcation task with the encoding function ξ(·): ξ(h, z) = σ(h· W· z); R× R where W∈ Ris the projection matrix. The classiﬁer function outputs a probability score of the target node belongs to G given the corresponding embedding pair (h, z). The loss function of our graph-level global dependency representation component is deﬁned as follows: L= −1N+ Nρ(h, z) · logξ(h, z) +ρ(eh, z) · log[1 − ξ(eh, z)](8) where ρ(·) is an indicator function where λ(h, z) = 1 and ρ(eh, z) = 1 corresponds to positive and negative instance pairs, respectively. We deﬁne the number of positive and negative samples as Nand N. By minimizing L(maximizing the mutual information between locallevel and graph-level representations), we could generate the enhanced user representations H∈ Rby encoding cross-session item transitional patterns from low-level (locally) to high-level (globally). Model Inference Based on the multi-task learning framework of MTD, we deﬁne our loss function with the integration of both intraand inter-session transition dynamics as follows: where Θ are learnable parameters. λand λbalance the losses from two module and prevent over-ﬁtting, respectively. Since the input of cross-session relation encoder and attention network are different, we employ mini-batch Adam to optimize Land Lalternatively. We further deﬁne additional parameter f to denote the training frequency of L optimization for loss balance. In each epoch, we ﬁrst optimize the graph-structured relation encoder and initialize the item representations with the current embeddings. Note that the local representation H, which are generated by the graph neural network, implies the global transition of items. To capture the global signal in recommendation module, we update the embedding table of items with H after the optimization step of L. Complexity Analysis of MTD Framework. The intrasession item relation learning requires O(I×d+I×d) calculations to compute the Q, K, V and attentive embeddings Xin the self-attention layer. After that, the rest of the intrasession learning spends most complexity on transformations in the d-dimensional hidden space (e.g., the two-layer feedforward network), which costs O(I × d) complexity, and results in O(L×I ×d+ I×d) overall complexity. Here, Ldenotes the number of d × d transformations. Furthermore, the graph-based inter-session item transition modeling component requires O(|A| × d + M × d) complexity for message passing and embedding transformation, where |A| denotes the number of neighboring item pairs. In this section, we perform extensive experiments on three publicly available real-life recommendation datasets and compare MTD with various state-of-the-art techniques. Particularly, we aim to answer the following research questions: • RQ1: Does MTD consistently outperform other baselines by yeilding better recommendation results? • RQ2: How do different sub-modules in our MTD framework affect the recommendation performance? MTD for the model performance? • RQ4: How is the model interpretation capability of MTD? • RQ5: How is the computational cost of MTD method ? Table 1: Statistics of the experimented datasets. Experimental Settings Data Description. The data statistics with training/test detailed split settings are shown in Table 1. We present the details of experimented datasets as below: Yoochoose Data. This data comes from an online retailing site to log half year of user clicks (released by Recsys’15 Challenge). Following the pre-processing strategies in (Li et al. 2017; Liu et al. 2018), the sessions with the length of ≥ 2 and items with the appearing frequency of ≥ 5 are kept in the training and test set. Diginetica Data. This data is collected from the CIKM Cup platform 2016 which records the user clicks from the time period of six months. To be consistent with the settings in (Wu et al. 2019; Liu et al. 2018), we do not include the sessions that contains single clicked item. Sessions in the test set are generated from the last week. Retailrocket Data. It contains the user browse data from another e-commerce company. Following the same settings in (Xu et al. 2019), we ﬁlter out the items with the browsed frequency less than 5 and sessions with the length of less than 2. We set the data from the last week for test and the remaining part for training. Evaluation Metrics. We leverage two metrics which are widely adopted in the session-based recommendation applications: Precision@K (Pre@K) and Mean Reciprocal Rank@K (MRR@K). Following the same rubric in (Wu et al. 2019; Li et al. 2017), MRR@K=0 if the ﬁrst correctly recommended items is not among the top-K ranked items. Note that larger Pre@K and MRR@K scores indicate better recommendation performance. Compared Methods. In our experiments, we consider the following baselines for performance comparison. • POP: it explores users’ past interested items and makes recommendations with the identiﬁed most frequent items. • S-POP: it recommends the most popular items to users by considering their activities from the current session. • ItemKNN (Davidson et al. 2010): it considers the item correlations using k-nearest neighbors algorithm based on items’ cosine similarity. • GRURec (Hidasi et al. 2015): it is a representative session-based recommendation approach using the gated recurrent unit to encode the transitional regularities. • NARM (Li et al. 2017): it is a neural attention model to argument recurrent network for session representations, by attending deferentially to sequential items. http://cikm2016.cs.iupui.edu/cikm-cup http://2015.recsyschallenge.com/challenge.html https://www.kaggle.com/retailrocket/ecommerce-dataset • STAMP (Liu et al. 2018): this approach is an attention model to capture user’s temporal interests from historical clicks in a session. • SASRec (Kang and McAuley 2018): this method is built upon the self-attention architecture to model the long-term item transition dynamics. • SR-GNN (Wu et al. 2019): it proposes a graph neural network model to encode item transitions within a session to generate item embedding. • CSRM (Wang et al. 2019a): it integrates the inner memory encoder through an outer memory network by considering correlations between neighborhood sessions. • CoSAN (Luo et al. 2020): it designs self-attention networks to model the collaborative feature information of items from neighborhood sessions. Parameter Settings Our implement is based on Tensorﬂow. The embedding dimensionality d is set as 100. We assign the regularization penalty λ= 10. All models are optimized using the Adam optimizer with the batch size and learning rate as 512 and 1e, respectively. The training frequency f in each epoch is set as 1, 4, 6 corresponding to the Yoochoose, Diginetica, Retailrocket, respectively. Furthermore, the dropout technique is applied in the training phase to alleviate the overﬁtting issue, with the ratio of 0.2. Experiments of most baselines are conducted with their release source code. Performance Validation (RQ1) We present evaluation results of all methods on different datasets in Table 2, and show the performance of several recent baselines when varying the value of top-K in Table 3. We can observe that MTD consistently outperforms other baselines in most cases on different datasets, which justiﬁes the effectiveness of our model in comprehensively capturing multi-level transition dynamics from intra-session and intersession relations in a hierarchical manner. The naive frequency (POP and S-POP) and similarity (ItemKNN) based recommendation approaches perform much worse than other baselines due to their limitations in capturing the dynamic sequential patterns of item transitions. Additionally, the attention-based recommendation techniques (NARM and STAMP) outperform the mere RNN approach (GRU4REC)–considering singular level of item sequential relations. However, the signiﬁcant improvement between MTD and attentive recommendation model suggests that only considering the intra-session item transitions is insufﬁcient to fully capture the complex item transition dynamics from both local and global perspectives. While SR-GNN tries to encode the long-term item dependencies using the graph neural network, it yields suboptimal results because its failure in learning cross-session dependency. Model Ablation and Effect Analyses (RQ2) We consider several model variants to investigate the efﬁcacy of key modules in our learning framework of MTD. Table 3: Evaluation results with different top-K values. Effect of Hierarchical Attention Network. We design two contrast models: i) MTD-va generates the session-level embeddings with the vanilla attention layer; ii) MTD-at further incorporates the temporal factor into the MTD-va method. Effect of Cross-Session Dependency Encoder. i) MTD-lo only encodes the local-level item transition patterns without the cross-session dependency encoder; ii) MTD-ga replaces our graph-structured hierarchical relation encoder with the graph attention network operated on all relevant sessions. We report the results in Figure 3 and observe that MTD outperforms all other variants on all datasets in terms of P re@K and M RR@K under K = 20, which justiﬁes the effectiveness of the design of individual component in our MTD framework. In particular: (1) The performance gap among MTD-va, MTD-at, and MTD-lo shows the effectiveness of our position-aware hierarchical attention network in modeling the local item transitions. (2) Without the consideration of cross-session item dependencies, MTD-lo performs worse than MTD. It suggests the necessity of modeling the inter-session item correlations based on our developed graph-structured framework; (3) While the graph attention network (MTD-ga) could learn global-level item relations, it still falls behind MTD since it does not capture the hierarchical informativeness across relevant sessions. Hyperparameter Study of MTD (RQ3) We further investigate the hyperparameter sensitivity of our MTD (as shown in Figure 4) and summarize the following observations. To save space and integrate results on different datasets with different performance scales into the one Figure 4: Hyper-parameter study of MTD. ﬁgure, we set y-axis as the performance degradation ratio compared to the best performance. (1) Effect of Hidden Dimensionality d. The performance saturates as the hidden dimensionality d reaches around 100. This is because a larger dimensionality d brings a stronger representation ability at the early stage, but might lead to overﬁtting as the continuously increasing of d. (2) Impact of Training Frequency f. We perform the training frequency study by varying f from 1 to 8, and could notice that a large value of f (≥ 5) will degrade the performance by misleading the objective function optimization. (3) Inﬂuence of Depth in Graph Neural Architecture. Stacking more graph convolution layers with the adjacent matrix-based aggregation will involve more redundant information of high-order connectivity, which hinders the learning process of global item relational structures in MTD. This observation also suggests the rationality of our designed graph neural component in simplifying and powering the cross-session item dependency learning, via the exploration of mutual relations between low-level item embeddings and high-level graph representation. Case Studies: Model Interpretation (RQ4) Hierarchical Relation Interpretation across Items. We visualize the hierarchical item relations with quantitative weights learned from our intra-session attention network on Diginetica. Figure 5 (a) and Figure 5 (b) show the encoded pairwise item correlations in modeling the intra-session sequential patterns of two sampled sessions across different time steps. From Figure 5 (c), we can observe that different items contribute differently to summarize the sessionspeciﬁc main purchase with hidden representations. Visualizations of Learned Session Embeddings. We further visualize the projected session representations by our MTD and two state-of-the-arts: SR-GNN and STAMP (as shown in Figure 5 (d)). We randomly sample 180 session instances and label each one with its corresponding next clicked item (ground truth). It is easy to see that embeddings of sessions with the same label (6 classes and each one is represented with the same color) cluster closely and can be better distinguished by MTD as compared to other two methods. This demontrates the effectiveness of our learned item transitional patterns with session embeddings. Model Scalability Study (RQ5) Since efﬁciency is a key factor in many real-life recommendation applications, we ﬁnally investigate the computational cost (measured by running time of individual epoch) of our MTD and other state-of-the-art recommendation models. Our experiments are conducted on different datasets are summarized in Table 4. From the evaluation results, we can observe that MTD outperforms most competitive baselines with different deep neural network architectures (e.g., attention mechanisms and graph-based message passing frameworks). Particularly, SR-GNN involves much computation cost in the gating mechanisms from neural network over each constructed session graph. Additionally, it is timeconsuming to discover collaborative neighborhood sessions for each batch during the training phase of CSRM method. In the occasional cases that MTD miss the best performance (as compared to a streaming algorithm STAMP–only using attention mechanism for transition aggregation), MTD still achieves competitive model efﬁciency. Overall, the proposed MTD is efﬁcient and scalable for large-scale session-based recommendation applications. Table 4: Computational time cost (seconds) investigation. Session-based Recommender Systems. To model sequential patterns of user behaviors, many recommender systems have been proposed to predict future interactions based on users’ historical observations (Huang et al. 2019a). In recent years, many session-based recommendation techniques have been developed based on various neural network architectures (Qiu et al. 2020a). Particularly, one intuitive approach is to apply the recurrent neural network (e.g., GRU) for modeling the item sequential correlations (Hidasi et al. 2015). Furthermore, attention mechanisms have been adopted for pattern aggregation through relation weight learning, such as NARM (Li et al. 2017) and STAMP (Liu et al. 2018). Different from the method (Xu et al. 2020) which replies on the random walk-based skipgram model for capturing the dependency, we leverage graph neural networks to consider the global item dependency across different sessions. Another paradigm of session-based recommendation models lie in utilizing graph neural networks to capture the graph-structured item dependencies, such as attributed graph neural network for streaming recommendation (Qiu et al. 2020b) and graph-based message passing architectures (Wu et al. 2019). Different from the above work, our MTD framework aims to jointly captures the local and global item transitional signals in a hierarchical manner. Graph Neural Networks for Recommendation. Recently emerged graph neural networks shine a light on performing information propagation over user-item graph for recommendation. Inspired by the graph convolution, several efforts have been devoted to capturing collaborative signals from the graph-based interacted neighbors, such as and LightGCN (He et al. 2020) and PinSage (Ying et al. 2018). Additionally, graph neural networks have also been integrated for recommendation to aggregate external knowledge from user side (Huang et al. 2021) or item side (Wang et al. 2019c). In this work, we propose to capture cross-session item dependencies in a hierarchical manner upon a global context enhanced graph network. This work develops a new graph learning framework–MTD, which aims to inject multi-level transition dynamics into the session-based recommendation. By integrating a positionaware dual-stage attention network and graph hierarchical relation encoder, MTD not only models the intra-session sequential transitions, but also derives the high-order item relationships across sessions. Experimental results on different real-world datasets show that MTD is superior to many state-of-the-art baselines. In the future, we plan to incorporate item content information (e.g., item text description or reviews) into MTD to deal with external attributes in learning semantic-aware item transitions. We thank the anonymous reviewers for their constructive feedback and comments. This work is supported by National Nature Science Foundation of China (62072188, 61672241), Natural Science Foundation of Guangdong Province (2016A030308013), Science and Technology Program of Guangdong Province (2019A050510010). This work is also partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and the York Research Chairs (YRC) program.