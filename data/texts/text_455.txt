ECOMMENDATION systems have been widely used to help users make decisions by suggesting them a list of items they may have interests. Various types of recommendation methods have been developed based on collaborative ﬁltering [1] and deep learning techniques [2]. Although these methods can usually achieve satisfactory performances, it is still very hard to explain their recommendation mechanisms. Thus, many recent research efforts [3], [4], [5], [6], [7] have been devoted to building explainable recommendation models that explain why an item is recommended by generating high-quality explanations, which can help improve the transparency and persuasiveness of recommendation systems. In practice, different strategies may be adopted to explain the recommendation results, e.g., images, the behaviors of relevant users, and textual descriptions of relevant items [8]. In this work, we focus on generating high quality review text to explain the recommendation results presented to the user. The review generation methods for explainable recommendation can be roughly classiﬁed into two groups: template-based approaches and natural language generation approaches [8]. The template-based methods generate explanations by ﬁlling the generated words in a sentence template. For example, in the template “You might be interested in [aspect], on which this product performs well”, we can replace [aspect] by a generated aspect to produce text explanation for item recommendation [9]. However, template-based explanations are uninformative and not persuasive. Moreover, designing high-quality templates usually requires domain knowledge. Natural language generation approaches can generate more natural and ﬂexible sentences. Such approaches have recently attracted increasing research attention. However, the pioneer works [10], [11] focus on generating short reviews or tips only based on given attributes (e.g., user ID, item ID, and rating value). Thus, it is difﬁcult for them to generate reliable explanations without considering other generative signals [9], [11]. Aspects, an important type of generative signal, which usually represent item features (e.g., ”price” and ”romance”), have recently been exploited to build aspectaware explanation generation models [12], [13], [14]. In these methods, the aspects are extracted from the user-generated reviews and used to train the explanation generation model. These methods assume the user’s interested aspects are available for explanation generation. In practice, this assumption usually does not hold, because we need to predict the user’s preferences for different aspects of a target item in many application scenarios. The hierarchical generation framework [15] provides a potential solution to address this problem. However, this method only considers the user ID, item ID, and the rating value as inputs. Thus, it cannot be effective in capturing the aspect-relevant details of the user and item for generating long and informative explanations. To address this problem, we build a review-based syntax graph to provide a uniﬁed view of the user/item details based on the review data. Firstly, a syntax dependency tree is built from each review. The relations in the dependency tree provide important clues to mine aspects, details, and opinions. From Figure 1 (a), we observe two sub-trees: story −−−−−−→ twists−−−→ interesting, story−−−−−−→ characters Fig. 1: An example of the review-based syntax graph. (a) shows the dependency tree structures of two reviews. (b) shows the review-based syntax graph built based on the nodes and relations extracted from the dependency trees of reviews. −−−→ memorable. They are both built up with the structure of aspect −→ details −→ opinion. To aggregate the details of the same aspect in different reviews, we construct the reviewbased syntax graph by connecting details in different dependency trees. As shown in Figure 1 (b), details about love story, such as characters and twists, can be directly connected to love story in the review-based syntax graph. Moreover, we propose a novel explainable recommendation framework, i.e., Hierarchical Aspect-guided explanation Generation (HAG). Speciﬁcally, HAG performs hierarchical aspect-guided graph pooling on the user/item review-based syntax graph to extract the aspect-relevant information for building the user/item representation. The user’s interests are matched with the item properties at the aspect level to predict her preferences for different aspects of the item. Then, a hierarchical explanation decoder is developed to generate aspects and aspect-relevant explanations based on attention mechanisms. To demonstrate the effectiveness of the proposed HAG model, we perform extensive experiments on three real-world datasets. The experimental results indicate that HAG outperforms stateof-the-art explanation generation methods, and achieves comparable or even better preference prediction accuracy than baseline methods. There are two main groups of explanation generation methods for explainable recommendation, namely templatebased and generation-based methods. Template-based methods generate recommendation explanations by ﬁlling pre-deﬁned templates with different words for different users. For example, the explicit factor model [9] generates the explanations by ﬁlling in the aspect in the pre-deﬁned template. Moreover, [16] introduces a template-based explainable recommendation model with aspects and opinions. However, these methods cannot provide more details about user preferences, and manually designing the template is also time-consuming. Generation-based methods focus on developing natural language generation methods for explainable recommendation. For example, [10], [11], [17] employ Recurrent Neural Networks (RNNs) based methods to encode the attribute information, e.g., user ID, item ID, and rating value, to generate explanations for recommendation results. These methods cannot generate reliable and precise explanations due to the lack of guided information. To address this problem, [12] exploits auxiliary information to generate explanations from given aspects. Moreover, [14] proposes the neural template-based explanation generation framework, which integrates the advantages of both the template-based and language generation methods. This method ﬁrst uses the given aspect as a template and then generates templatecontrolled explanations. The aforementioned methods usually focus on short explanation generation with only one aspect. To support long and informative explanation generation, [15] proposes a coarse-to-ﬁne generation framework that ﬁrst generates a sentence skeleton and then generates the aspect-aware explanation. This method only considers the user ID, item ID, and rating value as input attributes. As these input attributes usually contain less information, [13] introduces the reference-based seq2seq model that treats historical justiﬁcations as references. In addition, [18] presents a knowledge enhanced review generation model to combine review and knowledge graph information. In this section, we introduce some background about the construction of review-based syntax graph and the research problem studied in this work. 3.1 Review-based Syntax Graph Construction In this work, we denote the historical reviews of a user u by D= {d, d, · · · , d} and the historical reviews of an item i by D= {d, d, · · · , d}, where nand ndenote the number of reviews associated with u and i, respectively. To better understand the review data, we build a user review-based syntax graph Gand an item review-based syntax graph Gfrom the review sets D and D, respectively. For each user u, we ﬁrst apply text pre-processing techniques (e.g., tokenization and spelling correct) on D. Then, dependency parsing is used to automatically generate constituent-based representation (i.e., dependency tree) for each review sentence based on syntax [19]. Moreover, we also perform pruning to remove the words with little information. For example, as ”det” Max Pooling Fig. 2: Overall structure of the proposed HAG model. In this ﬁgure, we use the aspect and explanation generation processes of a single word ”characters” as an example. in Figure 1 (a) in the main content is the relation between ”story” and its determiner ”a” which has little semantic information, thus we remove the relation ”det” and only keep the head and tail nodes. Similarly, we also remove the relations such as ”nmod:poss” and ”punct”. Then, we connect all the dependency trees of the review sentences in Dby replacing the same words in different dependency trees with the same node. After that, we can obtain the user review-based syntax graph G= {X, E}, where X denotes the set of nodes (i.e., words), and Edenotes the set of edges E= {(x, r, x)|x, x∈ X, r ∈ R}. Here, r denotes the relation connecting the two nodes, and R denotes the set of all possible relations. Similar operations are performed on the item reviews Dto obtain the item review-based syntax graph G= {X, E}. 3.2 Problem Formulation Following [14], [20], we ﬁrst extract aspects from the observed review data using the tool developed in [9], and use Land Lto denote the lists of aspects derived from the user review set Dand item review set D, respectively. In L, we sort aspects according to their occurrence frequency in descending order, and choose the top-n ranked aspects to describe the user properties, which are denoted by Q= {q, q, · · · , q}. Similarly, we choose the top-n ranked aspects from Land denote them by Q= {q, q, · · · , q}. In this work, we study the following explainable recommendation problem: given a user u and an item i, their historical aspect sets Qand Q, and reviewbased syntax graphs Gand G, we aim to predict the user’s preference ron the item, mine the user’s interested aspects A= {a, a, · · · , a} of the item, and generate aspect-aware explanations P= {p, p, · · · , p}, in the form of a set of sentences. Figure 2 shows the overall framework of the proposed explainable recommendation framework HAG. As shown in Figure 2, HAG contains the following main components: 1) review-based syntax graph representation learning (RGRL), 2) aspect matching, 3) preference prediction, and 4) hierarchical explanation decoder. Next, we introduce the details of each component. The objective of the RGRL module is to extract representations for each review-based syntax graph. Firstly, we apply a BiLSTM f to encode the word embedding of an aspect q, and obtain output backward hidden state as the representation f(q). Then, hierarchical aspect-guided graph pooling is used to extract the review-based syntax graph representation at the aspect level. We propose an aspectguided graph pooling (AGP) operator to effectively extract the aspect-speciﬁc knowledge from the review-based syntax graph. AGP employs the user/item historical aspects to guide review-based syntax graph representation learning. 4.1.1 Aspect-guided Graph Pooling Operator Figure 3 (a) shows the workﬂow of the AGP operator. The inputs of an AGP operator include an input graph G = {X , E, X, A} and an aspect q with its representation f(q). Here, X and E denote the set of nodes and edges in G, respectively. X is the node feature matrix and A is the adjacency matrix. Firstly, a graph attention network (GAT) [21] is used to encode graph G. For each node x∈ X , we denote its ﬁrst-hop neighbors in the graph by N. The feature of xis updated by aggregating the input features of neighborhood nodes and adding its input feature xby self-loop as, where W∈ Ris the weight matrix, and α(x, r, x) denotes the attention score between two nodes xand x. Following [21], we deﬁne the attention score α(x, r, x) as, Fig. 3: (a) shows the ﬂow of the AGP operator where q and G denote the input aspect and graph. (b) shows the ﬂow of review-based syntax graph representation learning. Here, π(x, r, x) is implemented by the following attentional mechanism, where r is the embedding of the relation r, σ(·) is LeakyReLU activation function, W, W, W∈ Rare weight matrixes. We use a matrix X to denote the updated features of all nodes. Inspired by previous graph pooling methods [22], [23], [24], we deﬁne the following aspect-aware importance score to describe the relevance of each node in G to the given aspect q, where abs(·) denotes the absolute value function. The nodes in G can be ranked according to δ(x) in descending order. Then, we denote the set of top-K ranked nodes byeX and their indices by idx(s). In this work, we empirically set K = dρ|X |e, where ρ is the pooling ratio, | · | denotes the cardinality of a set, d·e is the ceiling function. The new features of nodes ineX and adjacency matrixeA of the corresponding graph are deﬁned as, where W∈ Rand b∈ Rare the weight matrix and bias vector respectively. X[idx(s), :] is row-wise indexed feature matrix. A[idx(s), idx(s)] aims to get the row-wise and column-wise indexed adjacency matrix from A. Then,eX andeA are the new feature matrix and the corresponding adjacency matrix after pooling. Moreover, we useeE to denote the set of edges that describe the connecting relationships between the nodes ineX . Then, the output of the AGP operator is denoted byeG = {eX ,eE,eX,eA}. We perform hierarchical graph pooling on the item/user review-based syntax graph, by staking AGP operators. Here, we only introduce the hierarchical graph pooling on the user review-based syntax graph G. The same process is also followed on the item review-based syntax graph G. As shown in Figure 3 (b), we conduct L layers graph pooling on G, guided by the user aspect set Q= {q, q, · · · , q}. At the `-th layer, there are n AGP operators. The input graph of the k-th AGP operator is G= {X, E, X, A}. The representation f(q) of aspect qis used to guide the pooling in the k-th AGP operator. The output graph of this AGP operator is G, which will be used as the input of the k-th AGP operator at the (` + 1)-th layer. Note that, at the ﬁrst layer, the inputs G= G, for k = 1, 2, · · · , n. We perform max pooling on Xto obtain the aspect-aware graph representation gat the `-th layer. After performing the above pooling operation L times, we can obtain multiple representations of Gthat are relevant to the aspect q, i.e., g, g, · · · , g. To fuse the graph representations from ﬁne-grained to coarse, we concatenate these representations to form the representation gof the review-based syntax graph Gas g= g⊕ g⊕ · · · g. To better describe the user’s preferences for different aspects, we compute aspect level user representation Sand item representation Sby concatenating the representations of historical aspects with those of the review-based syntax graphs as follows, Following [25], [26], we use Sand Sas features to deﬁne the following aspect level importance weight matrix, where W∈ Ris a learnable weight matrix, and d= d× L + d. In M∈ R, each element M[x, y] describes the importance of the y-th item aspect to the x-th user aspect. Then, we fuse the aspect level information of the user and item with Mas follows, where W, W∈ Rare trainable parameters, and φ(·) denotes the mean pooling operation. Here, vaims to incorporate user interested aspects and the user preferences for the item aspect. Similarly, vaims to combine item typical aspect and the representation of aspects that are highly relevant to the user. 4.3 Preference Prediction For each user u, we feed the user ID embedding einto a Multi-Layer Perceptron (MLP) and concatenate its output with her aspect level preference vector vto form the ﬁnal representation as follows, where W∈ Ris a learnable parameter. Similarly, we can obtain the ﬁnal representation xof the item i. Then, we concatenate xand xto form x. A Factorization Machine (FM) layer [27] is applied to predict the user u’s preference on the item i as follows, where b, band bare global bias, user bias, and item bias. vand vare the i-th and j-th variants. w ∈ Ris coefﬁcient vector, and h·, ·i is the dot product of two vectors. 4.4 Hierarchical Explanation Generation In HAG, we adopt two attention-based Long Short-Term Memory (LSTM) models as the aspect decoder and explanation decoder, respectively. This section introduces the details of these two decoders. 4.4.1 Aspect Decoder As shown in Figure 3, for each aspect q, we can obtain the node feature matrix Xof the graph Gafter graph pooling on the user review-based syntax graph. Similarly, we can also obtain the node feature matrix Xafter graph pooling on the item review-based syntax graph. Then, we stack all aspect-relevant node feature matrices to form the following matrices, To initial the hidden state, we ﬁrst map the predicted rating ˆrinto a sentiment representation vto guide aspect and explanation generation as, where W∈ Rand b∈ Rare a trainable weight matrix and a bias vector. Then, we feed v, v, and vinto an MLP layer as, To fully exploit the review-based syntax graph information, at each decoding time-step j − 1, we incorporate hidden state hand node feature x∈ Xto calculate attention vector cas, Similarly, we can obtain the attention vector cfrom X. The next time-step hidden state his, where the E(w) is the embedding of the previous aspect w. his fed into an MLP layer to obtain the probability of target aspect was, where W∈ Ris a trainable parameter, dis the size of the aspect vocabulary. 4.4.2 Explanation Decoder After the user interested aspect set whas been generated, we can further generate aspect-relevant explanations. For the j-th explanation p∈ P , we utilize the j-th hidden state hof aspect encoder as the initial hidden state h. Similar to Eq. (16), we further use review-based syntax graph to obtain attention vector cand cat each decoding time-step t−1. We also concatenate h, c, cand previous hidden state to obtainˆh. Thenˆhand the embedding of previous predicted word E(w) are fed into the decoder as, The probability of target word wis calculated as, where W∈ Ris a weight parameter and dis the size of the vocabulary. 4.5 Multi-task Learning Objective Function For the explanation generation task, we deﬁne the following cross-entropy losses for aspect and explanation generation respectively, where |A| and |P| denote the length of golden aspect and explanation sets for a given user-item pair (u, i). |p| denotes the length of the ground-truth explanation for the j-th aspect. P(w) and P(w) denote the probability of aspect wand word w. In addition, we also choose preference prediction as an auxiliary task to learn the HAG model and deﬁne the loss function as follows, where ˆrand rdenote the predicted and ground-truth rating values respectively. The ﬁnal loss function of the proposed HAG model is deﬁned as follows, TABLE 1: Statistics of the experimental datasets. where O denotes the set of observed user-item pairs in the training data, and |O| is the cardinality of set O. The entire framework can be effectively trained by minimizing Eq. (23) using end-to-end back propagation. In this work, we perform experiments to evaluate both the explanation generation performance and preference prediction performance of the proposed HAG model. 5.1 Experimental Settings 5.1.1 Experimental Datasets The experiments are conducted on the Amazon Review dataset [28] and Yelp Challenge 2019 dataset, which have been widely used for explanation generation. For the Amazon review dataset, we choose the following 5-core subsets for evaluation: ”Kindle Store” and ”Electronics” (respectively denoted by Kindle and Elec.). For the Yelp dataset and Amazon review dataset, we keep users and items that have more than 20 and 5 reviews for experiments respectively, due to the limitation of computation resources. In each dataset, a record consists of user ID, item ID, overall rating, and textual review. Following previous studies [20], [29], we ﬁrst extract aspects from the review data by the tool developed in [9]. Then, we only keep records that contain more than one aspect and extract aspect-relevant sentences from reviews as target explanations. Table 1 summarizes the statistics of experimental datasets. For each dataset, we randomly split the data into training, validation, and test data by the ratio 8:1:1. 5.1.2 Evaluation Metrics For the explanation generation task, we use BLEU [30], ROUGE [31], and METEOR [32] as the evaluation metrics, which evaluate the text similarity between the generated and gold explanations. BLEU evaluates the n-gram overlap between gold and generated explanations. ROUGE evaluates the recall, precision, and accuracy of the n-gram overlap. METEOR calculates the harmonic mean of each word precision and recall based on the whole corpus. However, these traditional language generation metrics can not measure whether the predicted explanations could express the gold aspects. To better evaluate the generated explanation, we also employ the Feature Matching Ratio (FMR) [14] to measure whether generated explanation can include the target aspects. To evaluate preference prediction performance of different methods, we use Mean Absolute Error (MAE) as the evaluation metric. The deﬁnition of MAE is as follows, where T denotes the set of test data, ˆrdenotes the predicted rating value, rdenotes the rating value in test data, | · | denotes the size of a set. Note that larger BLEU, ROUGE, METEOR, and FMR values indicate better results for explanation generation task, and lower MAE values indicate better performance for preference prediction task. 5.1.3 Baseline Methods We compare HAG with the following state-of-the-art explanation generation methods, and attention mechanism to learn the user’s preference from the user attributes and generate review explanations. to expand a short phrase to a long review by combining the user and item information with other auxiliary side information. Seq2Seq and learns the representation from the user and item reviews to generate explanations. then generates a template-controlled sentence with the predicted aspect. and applies a coarse-to-ﬁne decoding model to generate long reviews. Moreover, we also compare HAG with the following rating prediction methods to evaluate its ability in predicting users’ preferences, method developed for rating prediction. erences on items and the inﬂuences between items for recommendation. aspects from the review data; incorporate the review contents and the users’ rating behaviors for the recommendation. ID to an MLP to predict rating scores. 5.1.4 Implementation Details In this work, all the evaluated methods are implemented by PyTorch [38]. For explanation generation methods, the learning rate is chosen from {0.0005, 0.0007, 0.002}, and the batch size is set to 16. We empirically set the max vocabulary size dto 30,000. All the remaining words are replaced by the special token 〈UNK〉. For single-aspect explanation generation, we set the max sentence length to 15. For multiaspect explanation generation, we set the max feature length to 4 and set the max sentence length to 25. During the inference process, we use the greedy search algorithm to generate explanations. In HAG, we set the hidden size dand word embedding size to 128. The pre-trained Google News vectors TABLE 2: Explanation generation performance achieved by different methods in terms of FMR, BLEU (%), ROUGE (%), METEOR (%). B, R and M refer to BLEU, ROUGE and METEOR. Note that NETE-PMI generates explanations with a single aspect, thus we only report its single-aspect explanation generation performance. are used to initialize the word embeddings. The graph node embedding size dis chosen from {8, 16, 32, 64, 128}, and the dimension of user and item id embeddings dis chosen from {8, 16, 32, 64, 128}. Adam is used to optimize the model with a cosine annealing learning rate decay [39]. The number of graph pooling layers L is set to 2. Moreover, we set the number of top-ranked aspects n extracted from the user/item reviews to 4. In Att2Seq, we set the dimension of attribute embeddings to 64. For the decoding process, the dimension of word embeddings and hidden vectors are set to 512. The dropout rate is set to 0.2. For the ExpNet model, we set the dimension of attribute embeddings and word embedding to 64 and 512 respectively. The dimension of the aspect embedding is 15. For the decoder part, we set the hidden size of GRU to 512, and the dropout rate is set to 0.1. For Ref2Seq, the dimension of word embeddings and hidden vector are 256. For encoder and decoder, the dropout rates are set to 0.5 and 0.2. For NETE-PMI, we set the dimensions of word embeddings and attribute vectors to 200. The size of RNN hidden states is set to 256. The dropout rate is 0.2, and the regularization parameter is set to 0.0001. In ACF, aspects are extracted by TwitterLDA, and the number of aspects is set to 10. For each aspect, the number of keywords is set to 50. The dimension of word embeddings is set to 512. The hidden size of the 2-layer GRU is set to 512. The hyper-parameters of preference prediction methods are set as follows. For PMF and SVD++, the learning rate is chosen from {0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05}, the dimensionality of embeddings is chosen in {8, 16, 32, 48, 64, 128}, and the batch size is set to 512. For RMG, the learning rate is selected from {0.01, 0.005, 0.001, 0.0005}, the number of CNNs ﬁlters is set to 150 and the kernel size is 3. For CARL, the learning rate is chosen from the {0.05, 0.01, 0.005, 0.001}. The dimension of ID embedding and hidden state are 50 and 200. The number of CNN ﬁlters is 40. 5.2 Performance Comparison Table 2 summarizes the performance of the single-aspect and multi-aspect explanation generation tasks achieved by TABLE 3: The preference prediction performance achieved by different methods in terms of MAE. The best results are in bold faces and the second-best results are underlined. different methods. As shown in Table 2, ExpNet usually achieves better performance than Att2Seq by using aspects to guide the generation process. Ref2Seq obtains better results than ExpNet and Att2Seq. One potential reason is that Ref2Seq uses the user’s historical reviews as inputs for explanation generation. In addition, ExpNet outperforms NETE-PMI on the Kindle dataset, in terms of all metrics. It may because that ExpNet uses an attention fusion layer to control the generation outputs. Compared with Ref2Seq, ACF does not extract side information from the review data as input, and it also generates sentences by ﬁlling the generated templates based on predicted aspects. Its explanation generation performance is highly dependent on the quality of input aspects. In ACF, the aspects are extracted by TwitterLDA [40], thus the aspect quality could be inﬂuenced by the review data quality and the given number of latent topics. The preference prediction performance achieved by different methods is shown in Table 3. We can note that the review-based methods usually achieve better performance than traditional matrix factorization method (i.e., PMF and SVD++). Moreover, the proposed HAG method achieves the best preference prediction performance on Kindle and Yelp datasets, and achieves the second best prediction performance on Electronics dataset, in terms of MAE. These observations indicate that the user preference predicted by HAG can support the high-quality explanation generation. For both tasks, HAG usually achieves the best performance on all datasets, by enhancing explanation genera- Fig. 4: Performance of HAG with respect to different settings of historical aspect number n on Kindle dataset. TABLE 4: The performance achieved by different variants of HAG on Kindle dataset. tion with user and item review-based syntax graphs. The review-based syntax graph aggregates information of all the reviews associated with the user/item to provide a uniﬁed view about the aspect-relevant details about the user/item. Thus, the review-based syntax graph can also help weaken the impacts caused by noise reviews. In this section, we perform ablation study to analyze the effectiveness of different components of HAG. To study the contributions of the AGP operator, we study the performance of the following three variants of HAG, moved from the model. For the aspect and explanation attention, we use the node feature of the original review-based syntax graph to replace the node feature of the L-th graph. ing module of AGP is removed. We only use GAT to learn the representation of the review-based syntax graph. with LSTM to update the representations of nodes in the graph. For the aspect and explanation attention calculation, it is same as HAG. As shown in Table 4, HAG outperforms HAG, HAG, and HAG, in terms of BLEU-4, ROUGE-L, and MAE. This demonstrates that the AGP operator can beneﬁt both the prediction of user preferences and the explanation generation. ROUGE-L(%) TABLE 5: Performance of HAG with respect to different graph pooling ratio ρ on Kindle dataset. TABLE 6: Performance of HAG with respect to different settings of the number of pooling layers L on Kindle dataset. 5.3.2 Impacts of Historical Aspects To study the impacts of historical aspects, we consider the following two variants of HAG for evaluation, to extract features from the historical aspects. the historical aspect with a randomly initialized vector, which can be learned in model training. As shown in Table 4, HAG achieves better performance than HAGand HAG. This observation indicates that the historical aspect set is helpful to mine the user preferences for aspects, and the BiLSTM structure can help learn better representations for historical aspects. 5.3.3 Impacts of Grammatical Relations To study the impacts of grammatical relations in the reviewbased syntax graph, we remove the relation embedding in Eq. (3) and denote this variant of HAG by HAG. As shown in Table 4, HAG achieves better FMR, BLUE-4 and MAE values than HAG. This indicates that the grammatical relations can help improve the explanation generation performance. 5.4 Parameter Sensitivity Study In this work, we employ the AGP operator to mine the aspect relevant information from the review-based syntax graph. We perform experiments to evaluate the impacts of the number of historical aspects n on the performance of HAG. Figure 4 shows performance trends of HAG with TABLE 7: Explanations generated by different methods. The target aspects are in bold faces. Note that NETE-PMI cannot generate multi-aspect relevant explanations. Compared with baseline methods, the proposed HAG model can generate more informative explanations, which cover more target aspects. respect to different settings of n. As shown in Figure 4, we can note that HAG achieves the best FMR value when n is set to 2. This indicates that the generated sentences have the highest probability to cover the user’s interested aspects. The best BLEU-4 and MAE values are achieved when n is set to 4. Moreover, the best ROUGE-L value is achieved when n is set to 8. In the experiments, we empirically set the number of historical aspects n to 4, due to its best performance on BLEU-4. Moreover, we also study the performance of HAG with respect to different settings of the graph pooling ratio ρ, which represents the proportion of nodes retained. When ρ is set to 0.1, it means that the model only retains 10% of nodes in each graph pooling operation. As we set the graph layer to 2, only 1% of nodes in the review-based syntax graph are remained after two AGP operations. Table 5 summarizes the performance of HAG with respect to different settings of ρ. We can note that the best FMR values are achieved by setting ρ to 0.1 and 0.7. For text generation metrics, the best BLEU-4 and ROUGE-L values are achieved when ρ is set to 0.5 and 0.9, respectively. The best preference prediction performance in terms of MAE is achieved when ρ is set to 0.3 and 0.5. Furthermore, we also conduct experiments to study the performance of HAG with respect to different number of graph layers L on the Kindle dataset. As shown in Table 6, HAG achieves better FMR values when L is set to 1 and 4. However, better BLEU-4, ROUGE-L, and MAE values are achieved when L is set to 2 and 3. When L = 2, HAG can achieve the best performance in terms of BLEU-4 and MAE, thus it is appropriate to set L to 2 in the experiments. 5.5 Case Study Table 7 shows examples of the single-aspect and multiaspect explanations generated by different methods. For the single-aspect generation task, we can note that the ”price” is the target aspect. As shown in Table 7, the explanations generated by HAG can better express the target aspect. The explanations generated by Att2Seq and ExpNet include irrelevant properties of the product. The sentences generated by ACF and Ref2Seq are not natural enough as the outputs have repetitive words or sentences. Moreover, we can note that ACF cannot generate a meaningful explanation. The main reason is that the quality of generated explanations highly depends on the accuracy of the predicted aspects, and ACF cannot predict the user’s interested aspects accurately. Although NETE-PMI can cover the target aspect, the TABLE 8: Performance of HAG and Ref2Seq with/without pre-trained word embeddings on Kindle dataset, where “P” means with pre-trained embedding and “w/o P” means without pre-trained embedding. Note that Ref2Seq does not include the preference prediction module. subject of the sentence is not correct. For the multi-aspect generation task, we can note that most outputs of baseline models cannot cover the target aspects such as “romance” and “characters”. Thus, they cannot generate high-quality explanations based on the user preferences on aspects. The explanation sentences generated by HAG not only cover speciﬁc aspects but also are more natural. Compared to baseline methods, the proposed HAG model can generate more expressive and reliable explanations. 5.6 Impacts of Pre-trained Embeddings Pre-trained embeddings are obtained from models trained on large datasets, thus they usually contain rich semantic information. To evaluate the performance of HAG without pre-trained embeddings and the effectiveness of pre-trained embeddings in our tasks, we summarize the performance of HAG with/without pre-trained embeddings on the Kindle dataset in Table 8. Here, we only compare HAG with Ref2Seq, which is the best baseline method in explanation generation, and it also uses reviews as inputs. As shown in Table 8, the pre-trained embeddings do not help improve the performance of the proposed HAG model. The performance of HAG without pre-trained embeddings are even better than the performance of HAG with pre-trained embeddings. For the Ref2Seq model, the performance decreases without using the pre-trained embeddings. In conclusion, the proposed HAG model achieves excellent performance even without pre-trained embeddings, and the effectiveness of pre-trained embeddings in the studied explanation generation task is limited. This paper proposes a novel explainable recommendation model, namely Hierarchical Aspect-guided explanation Generation (HAG). Speciﬁcally, HAG employs a reviewbased syntax graph that captures the user/item details from the review data to enhance explanation generation. An aspect-guided graph pooling (AGP) operator is proposed to distill the aspect-relevant information from the reviewbased syntax graph. Moreover, an aspect matching mechanism is developed to match the user preferences and item properties at the aspect level. Furthermore, a hierarchical decoder is also developed to ﬁrst predict the user interested aspects and then generate the aspect-relevant explanations. The experimental results on real datasets demonstrate that HGA outperforms state-of-the-art explanation generation methods. This research is supported by the National Research Foundation, Prime Minister’s Ofﬁce, Singapore under its AI Singapore Programme (AISG Award No: AISG-GC-2019003) and under its NRF Investigatorship Programme (NRFI Award No. NRF-NRFI05-2019-0002). Any opinions, ﬁndings and conclusions or recommendations expressed in this material are those of the authors and do not reﬂect the views of National Research Foundation, Singapore.