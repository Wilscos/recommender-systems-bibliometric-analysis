Reinforcement Learning (RL) has been successfully applied to solve sequential-decision problems in many real-world scenarios, such as medical domains [ 19], and personalized recommendation [ often desirable to restrict the agent from adjusting its policy too often due to the high costs and risks of deployment. For example, updating a treatment strategy in medical domains requires a thorough approval process by human experts [ the policy online based on instantaneous data and a more common practice is to aggregate data in a period before deploying a new policy [ placements [ launch a large-scale evaluation procedure on hardware devices like FPGA; for learning to control a real robot, it may be risky or inconvenient to switch the deployed policy when an operation is being performed. In these settings, it is a requirement that the deployed policy, i.e., the policy used to interact with the environment, could keep unchanged as much as possible. Formally, we would like our RL algorithm to both produce a policy with the highest reward and at the same time reduce the number of deployed policy switches (i.e., a low switching cost) throughout the training process. Shusheng Xu, Yancheng Liang, Yunfei Li, Simon Shaolei Du, Yi Wu {xuss20, liangyc19, liyf20}@mails.tsinghua.edu.cn, A ubiquitous requirement in many practical reinforcement learning (RL) applications, including medical treatment, recommendation system, education and robotics, is that the deployed policy that actually interacts with the environment cannot change frequently. Such an RL setting is called low-switching-cost RL, i.e., achieving the highest reward while reducing the number of policy switches during training. Despite the recent trend of theoretical studies aiming to design provably efﬁcient RL algorithms with low switching costs, none of the existing approaches have been thoroughly evaluated in popular RL testbeds. In this paper, we systematically studied a wide collection of policy-switching approaches, including theoretically guided criteria, policy-difference-based methods, and non-adaptive baselines. Through extensive experiments on a medical treatment environment, the Atari games, and robotic control tasks, we present the ﬁrst empirical benchmark for low-switchingcost RL and report novel ﬁndings on how to decrease the switching cost while maintain a similar sample efﬁciency to the case without the low-switching-cost constraint. We hope this benchmark could serve as a starting point for developing more practically effective low-switching-cost RL algorithms. We release our code and complete results in https://sites.google.com/view/ low-switching-cost-rl. 20], it is desirable to limit the frequency of changes to the policy since it is costly to Ofﬂine reinforcement learning [ that also has a capability of avoiding frequent policy deployment. Ofﬂine RL assumes a given transition dataset and performs RL training completely in an ofﬂine fashion without interacting with the environment at all. [ training procedure, i.e., re-collecting transition data using the current policy and re-applying ofﬂine RL training on the collected data, for about 10 times. However, similar to the standard ofﬂine RL methods, due to such an extreme low-deployment-constraint, the proposed solution suffers from a particularly low sample efﬁciency and even produces signiﬁcantly lower rewards than online SAC method in many cases [ minimal switching cost, low-switching-cost RL aims to reduce the switching cost while maintain a similar sample efﬁciency and ﬁnal reward compared to its unconstrained RL counterpart. In low-switching-cost RL, the central question is how to design a criterion to decide when to update the deployed policy based on the current training process. Ideally, we would like this criterion to satisfy the following four properties: 1. Low switching cost: 2. High reward: 3. Sample efﬁciency: 4. Generality: From the theoretical side, low-switching-cost RL and its simpliﬁed bandit setting have been extensively studied [ gain. Speciﬁcally, they update the deployed policy only if the measurement of information gain is doubled, which also leads to optimality bounds for the ﬁnal policy rewards. We suggest the readers refer to the original papers for details of the theoretical results. We will also present algorithmic details later in Section 4.4. However, to our knowledge, there has been no empirical study on whether these theoretically-guided criteria are in fact effective in popular RL testbeds. In this paper, we aim to provide systematic benchmark studies on different policy switching criteria from an empirical point of view. We remark that our experiment scenarios are based on popular deep RL environments that are much more complex than the bandit or tabular cases studied in the existing theoretical works. We hope this benchmark project could serve as a starting point towards practical solutions that eventually solve those real-world low-switching-cost applications. Our contributions are summarized below. There are a total of 57 Atari games. However, only 56 of them (excluding the “surround” game) are supported by the atari-py package, which we adopt as our RL training interface. policy switching criterion should have a reduced frequency to update the deployed policy. data can be highly off-policy. We need this criterion to deploy policies at the right timing so that the collected samples can be still sufﬁcient for ﬁnally achieving the optimal reward. with such a criterion to produce a similar sample efﬁciency to the unconstrained RL setting without the low-switching-cost requirement. rather than a speciﬁc task. 3,5,4,22,6,26,27]. The core notion in these theoretical methods is information We conduct the ﬁrst empirical study for low-switching-cost RL on environments that require modern RL algorithms, i.e., Rainbow [9] and SAC [8], including a medical environment, 56 Atari gamesand 6 MuJoCo control tasks. We test theoretically guided policy switching criteria based on the information gain as well as other adaptive and non-adaptive criteria. We ﬁnd that a feature-based criterion produces the best overall quantitative performance. Surprisingly, the non-adaptive switching criterion serves as a particularly strong baseline in all the scenarios and largely outperforms the theoretically guided ones. Through extensive experiments, we summarize practical suggestions for RL algorithms with with low switching cost, which will be beneﬁcial for practitioners and future research. Low switching cost algorithms were ﬁrst studied in the bandit setting [ with low switching cost is mostly theoretical. To our knowledge, [ problem for the episodic ﬁnite-horizon tabular RL setting. [ OHSA log (K) of actions, bound was improved in [27, 26]. Ofﬂine RL (also called Batch RL) can be viewed as a close but parallel variant of low-switching-cost RL, where the policy does not interact with the environment at all and therefore does not incur any switching cost. Ofﬂine RL methods typically learn from a given dataset [ applied to many domains including education [ Some methods interpolate ofﬂine and online methods, i.e., semi-batch RL algorithms [ update the policy many times on a large batch of transitions. However, reducing the switching cost during training is not their focus. [ reduce the switching cost without the need of a given ofﬂine dataset. Given a ﬁxed number of policy deployments (i.e., 10), the proposed algorithm collects transition data using a ﬁxed deployed policy, trains an ensemble of transition models and updated a new deployed policy via model-based RL for the next deployment iteration. However, even though the proposed model-based RL method in [ outperforms a collection of ofﬂine RL baselines, the ﬁnal rewards are still substantially lower than standard online SAC even after consuming an order of magnitude more training samples. In our work, we focus on the effectiveness of the policy switching criterion such that the overall sample efﬁciency and ﬁnal performances can be both preserved. In addition to ofﬂine RL, there are also some other works that aim to reduce the interaction frequency with the environment rather than the switching cost [7, 10], which is parallel to our focus. Markov Decision Process: Sis the state space, reward function, from state an action, which can be either deterministic or stochastic. An episode starts with an initial state the current state We assume an episode will always terminate, so each episode always have a ﬁnite horizon lengthH reward,π experienced across all the episodes during training. Ideally, we also want the agent to consume as few training samples as possible, i.e., a minimal long-term value for the action Deep Off-policy Reinforcement Learning: popular off-policy RL algorithm leveraging a deep neural network to approximate current state and maintain all the transition data in the replay buffer.For each training step, the temporal difference error is minimized over a batch of transitions sampled from this buffer by Rainbow [ and achieves strong and stable performances on most Atari games. In this paper, we adopt a His the planning horizon andKis the number of episodes the agent plays. The upper pis the initial state distribution, andP (x|x, a)denotes the transition probability xto statexafter taking actiona. A policyπ : S → Ais a mapping from a state to . At each stephin this episode, the agent chooses an actionafromπ(x)based on x, receives a rewardr(x, a)and moves to the next statex∼ P (·|x, a). ). The goal of the agent is to ﬁnd a policyπwhich maximizes the discounted expectedhPi = arg maxEγr(x, a). LetKdenote the total transitions that the agent Q(x, a) := r(x, a) + Eγr (x, π (x))x= x, a= a x, the agent selects an actionagreedily based on parameterized Q-functionQ(x, a) represents the parameters of the target Q-network, which is periodically updated fromθ. 9] is perhaps the most famous DQN variant, which combines six algorithmic enhancements 9: end for deterministic version also adopt count-based exploration [24] as a deterministic exploration bonus. For continuous action domains, soft actor-critic (SAC) [ rithm. SAC uses neural networks parameterized by policyπ ing In standard RL, a transition the policy is updated, a switching cost is incurred. In low-switching-cost RL, we have two separate policies, a deployed policy trained by the underlying RL algorithm. These policies are parameterized by Suppose that we totally collect k, the agent is interacting with the environment using a deployed policy collected, the agent can decide whether to update the deployed replacing deﬁned by the number of different deployed policies throughout the training process, namely: The goal of low-switching-cost RL is to design an effective algorithm that learns while produces the smallest switching cost of the switching criterion the switching cost. The overall workﬂow of low-switching-cost RL is shown in Algorithm 1. In the following content, we present a collection of policy switching criteria and techniques, including those inspired by the information gain principle (Sec. 4.4) as well as other non-adaptive (Sec. 4.1) and adaptive criteria (Sec. 4.2,4.3). All the discussed criteria are summarized in Algorithm 2. This simplest strategy switches the deployed policy every in our experiments. Empirically, we notice that “FIX_1000” is a surprisingly effective criteria which remains effective in most of the scenarios without ﬁne tuning. So we primarily focus on “FIX_1000” as our non-adaptive baseline. In addition, We speciﬁcally use “None” to indicate the experiments without the low-switching-cost constraint where the deployed policy keeps synced with the online policy all the time. Note that this “None” setting is equivalent to “FIX_1”. Standard Rainbow adds random noise to network parameters for exploration, which can be viewed as constantly switching policies over a random network ensemble. This contradicts the low-switching-cost constraint. by π(x), execute action aand observe reward r, state x , a, r, x) in D (a|s).Q-network is trained to approximate entropy-regularized expected return by minimiz- (r+ γ(Q(x, a) − α log π(a|x)) − Q(x, a))|a∼ π(·|x), (3) is the entropy coefﬁcient. We omit the parameterization ofπsinceπis not updated w.r.tL. θwithθ, according to some switching criterionJ. Accordingly, the switching cost is Another straightforward criterion is to switch the deployed policy when the action distribution produced by the online policy signiﬁcantly deviates from the deployed policy. Speciﬁcally, we sample a batch of training states and count the number of states where actions by the two policies differ in the discrete action domains. We switch the policy when the ratio of mismatched actions exceeds a threshold σ Beyond directly consider the difference of action distributions, another possible solution for measuring the divergence between two policies is through the feature representation extracted by the neural networks. Hence, we consider a feature-based switching criterion that decides to switch policies according to the feature similarity between different Q-networks. Similar to the policy-based criterion, when deciding whether to switch policy or not, we ﬁrst sample a batch of states replay buffer, and then extract the features of all states with both the deployed deep Q-network and the online deep Q-network. Particularly, we take the ﬁnal hidden layer of the Q-network as the feature representation. For a state The similarity score between f We then compute the averaged similarity score on the batch of states B Reset-Checking as a Practical Enhancement: tation enhancement, which produces lower switch costs and is more robust across different environments: we only check the feature similarity when an episode resets (i.e., a new episode starts) and additionally force deployment to handle extremely long episodes (e.g., in the “Pong” game, an episode may be trapped in loopy states and lead to an episode length of over 10000 steps). Hyper-parameter Selection: a batch of size 512 from recent 10,000 transitions and compare the action differences or feature similarities between the deployed policy and the online policy on these sampled transitions. We also tried other sample size and sampling method, and there is no signiﬁcant difference. For the switching threshold (i.e., the mismatch ratio criterion), we perform a rough grid search and choose the highest possible threshold that still produces a comparable ﬁnal policy reward. Existing theoretical studies propose to switch the policy whenever the agent has gathered sufﬁcient new information. Intuitively, if there is not much new information, then it is not necessary to switch the policy. To measure the sufﬁciency, they rely on the visitation count of each state-action pair or the determinant of the covariance matrix. We implement these two criteria as follows. Visitation-based Switching: state-action pair reaches an exponent (speciﬁcally scheme is theoretically justiﬁed with bounded switching cost in tabular cases. However, it is not feasible to maintain precise visitations for high-dimensional states, so we adopt a random projection function to map the states to discrete vectors by to the hashed states as an approximation. distribution N(0, 1) and g is a ﬂatten function which converts x to a 1-dimensional vector. We list the hyper-parameter search space in Appendix B. . For continuous actions, we use KL-divergence to measure the policy differences. Algorithm 2 Switching Criteria (J in Algorithm 1)  Non-adaptive Switching input environment step counter k, ﬁxed switching interval n output bool(k mod n == 0)  Policy-based Switching input deployed and online policy π Compute the ratio of action difference or KL divergence for π output bool(δ ≥ σ  Feature-based switching input Encoder of deployed and online policy f Compute sim(B) via Eq.(6) output bool(sim(B) ≤ σ  Visitation-based Switching input the current visited times of state-action pair n(φ(x output bool(n(φ(x  Information-matrix-based Switching input episode timestep h, current covariance matrix Λ Information-matrix-based Switching: icy switches is based on the property of the feature covariance matrix [P within this episode, and each episode timestep empirically observe that the approximate determinant computation can be particularly inaccurate for complex RL problems. Instead, we adopt an effective alternative, i.e., switch the policy when the least absolute eigenvalue doubles. In practice, we again adopt a random projection function to map the state to low-dimensional vectors, get ψ(x, a) = [φ(x), a]. In this section, we conduct experiments to evaluate different policy switching criteria on Rainbow DQN and SAC. For discrete action spaces, we study the Atari games and the GYMIC testbed for simulating sepsis treatment for ICU patients which requires low switching cost. For continuous control, we conduct experiments on the MuJoCo [25] locomotion tasks. GYMIC to an infection, where sepsis is caused by the body’s response to an infection and could be lifethreatening. GYMIC built an environment to simulate the MIMIC sepsis cohort, where MIMIC is an open patient EHR dataset from ICU patients. This environment generates a sparse reward, the reward is set to +15 if the patient recovers and -15 if the patient dies. This environment has 46 clinical features and a 5 × 5 action space. Atari 2600 agents [9]. We evaluate the efﬁciency of different switching criteria on a total of 56 Atari games. ψ(x, a)ψ(x, a)+ λI, whereedenotes a training episode,hmeans theh-th timestep GYMIC is an OpenAI gym environment for simulating sepsis treatment for ICU patients Atari 2600 games are widely employed to evaluate the performance of DQN-based Figure 1: Results on GYMIC. Top: the learning curve of reward vs. steps. Bottom: switching cost. Note that the switching cost of “Visitation” almost overlaps with “None”. MuJoCo control tasks benchmarks in the MuJoCo physics simulator, including Swimmer, HalfCheetah, Ant, Walker2d, Hopper and Humanoid. For GYMIC and Atari games whose action space is discrete, we adopt Rainbow DQN to train the policy; for MuJoCo tasks with continuous action spaces, we employ SAC since it is more suitable for continuous action space. We evaluate the efﬁciency among different switching criteria in these environments. All of the experiments are repeated over 3 seeds. Implementation details and hyperparameters are listed in Appendix B. All the code and the complete experiment results can be found at https://sites.google.com/view/ low-switching-cost-rl. We evaluate different policy switching criteria based on the off-policy RL backbone and measure the reward function as well as the switching cost in both GYMIC and MuJoCo control tasks. For Atari games, we plot the average human normalized rewards. Since there are 56 Atari games evaluated, we only report the average results across all the Atari games as well as 8 representative games in the main paper. Detailed results for every single Atari game can be found at our project website. To better quantitatively measure the effectiveness of a policy switching criterion, we propose a new evaluation metric, performance and the switching cost improvement into account. Speciﬁcally, suppose the standard online RL algorithm (i.e., the “None” setting) can achieve an average reward of costˆC. Now, an low-switching-cost RL criterion cost of C where[·] allowed performance drop with the low-switching-cost constraint applied. In our experiments, we choose a ﬁxed threshold parameter moderate (i.e., within the threshold improvements; while when the performance decreases signiﬁcantly, the RSI score will be simply 0. We use We also tried a variant of RSI that remove the log function. The results are shown in the appendix C. using the same amount of training samples. Then, we deﬁne RSI of criterion J as is the indicator function andσis a reward-tolerance threshold indicating the maximum ˆC instead of K here since some RL algorithm may not update the policy every timestep. Figure 3: The average results on Atari games. We compare different switching criteria across 56 Atari games with 3 million training steps. We visualize the human normalized reward on the left. The ﬁgure on the right shows the average switching cost, which is normalized by the switching cost of “None” and shown in a log scale. We compare the performances of all the criteria presented in Sec. 4, including unconstrained RL (“None”), non-adaptive switching (“Fix_1000”), policy-based switching (“Policy”), feature-based switching (“Feature”) and two information-gain variants, namely visitation-based (“Visitation”) and information-matrix-based (“Info”) criteria. GYMIC: learning curves as unconstrained RL as shown in Fig. 1. However, the switching cost of visitationbased criterion is signiﬁcantly higher – it almost overlaps with the cost of “None”. While the other information-gain variant, i.e., information-matrix-based criterion, performs much better in this scenario. Overall, feature-based criterion produces the most satisfactory switching cost without hurt to sample efﬁciency. Note that in GYMIC, different methods all have very similar performances and induce a particularly high variance across seeds. This is largely due to the fact the intrinsic transition dynamics in GYMIC is highly stochastic. GYMIC simulates the process of treating ICU patients. Each episode starts from a random state of a random patient, which varies a lot during the whole training process. In addition, GYMIC only provides a binary reward (+15 or -15) at the end of each episode for whether the patient is cured or not, which ampliﬁes the variance. Furthermore, we notice that even a random policy can achieve an average reward of 13, which is very close to the maximum possible reward 15. Therefore, possible improvement by RL policies can be small in this case. However, we want to emphasize that despite similar reward performances, different switching criteria lead to signiﬁcantly different switching costs. Atari Games: complex Atari games. The state spaces in Atari games are images, which are more complicated than the low-dimensional states in GYMIC. Fig. 3 shows the average reward and switching of different switching criteria across all the 56 games, where the feature-based solution leads to the best empirical performance. We also remark that the non-adaptive baseline is particularly competitive in Atari games and outperforms all other adaptive solutions except the feature-based one. We also show the results in 8 representative games in Fig. 4, including the reward curves (odd rows) and switching cost curves (even rows). We can observe that information-gain variants produce substantially more policy updates while the feature-based and non-adaptive solutions are more stable. In addition, we also noticed that the policy-based solution is particularly sensitive to its hyperparameter in order to produce desirable policy reward, which suggests that the neural network features may change much more smoothly than the output action distribution. To validate this hypothesis, we visualize the action difference and feature difference of the unconstrained Rainbow DQN on the Atari game “Pong” throughout the training process in Fig. 2. Note that in this case, the deployed policy is synced with the online policy in every training iteration, so the difference is merely due to a single training update. However, even in a unconstrained setting, the difference of action distribution ﬂuctuates signiﬁcantly. By contrast, the feature change is much more stable. We also provide some theoretical discussions on feature-based criterion in Appendix D. MuJoCo Control: 6 MuJoCo continuous control tasks. The results are shown in Fig. 5. In general, we can still observe This medical environment is relatively simple, and all the criteria achieve similar Figure 4: The results on several representative Atari games. In each environment, we visualize the training reward over the steps on the top and the switching cost in a log scale at the bottom. that the feature-based solution achieves the lowest switching cost among all the baseline methods while the policy-based solution produces the most unstable training. Interestingly, although the nonadaptive baseline has a relatively high switching cost than the feature-based one, the training curve has the less training ﬂuctuations, which also suggests a future research direction on incorporating training stability into the switching criterion design. Average RSI Scores: different domains. For each domain, we compute the average value of RSI scores over each individual task in this domain. The results are reported in Table 1, where we can observe that the feature-based method consistently produces the best quantitative performance across all the 3 domains. We list the results of details. Table 1: RSI (Eq. 7, “None”) performance as the RSI reference, so the RSI value for “None” is always zero. In addition, we apply a is carried out for each pair of methods over all the experiment trials across all the environments and seeds. For Atari and MuJoCo, the results show that there are signiﬁcant differences in switching cost between any two criteria (with with no signiﬁcant difference. It is also worth mentioning that RSI for “Visitation” for GYMIC is 0, which also shows the switching costs of “Visitation” and “None” are nearly the same. In this paper, we focus on low-switching-cost reinforcement learning problems and take the ﬁrst empirical step towards designing an effective solution for reducing the switching cost while maintaining good performance. By systematic empirical studies on practical benchmark environments with modern RL algorithms, we ﬁnd the existence of a theory-practice gap in policy switching criteria and suggest a feature-based solution can be preferred in practical scenarios. We remark that most theoretical works focus on simpliﬁed bandits or tabular MDPs when analysing mathematical properties of information-gain-based methods, but these theoretical insights are carried out on much simpliﬁed scenarios compared with popular deep RL testbeds that we consider here. In addition, these theoretical analyses often ignore the constant factors, which can be crucial for the practical use. By contrast, feature-based methods are much harder to analyze theoretically particularly in the use of non-linear approximators, which, however, produces much better practical performances. This also raises a new research direction for theoretical studies. We emphasize that our paper not only provides a benchmark but also raises many interesting open questions for algorithm development. For example, although feature-based solution achieves the best overall performance, it does not substantially outperform the the naive non-adaptive baseline. It still has a great research room towards designing a more principled switching criteria. Another direction is to give provable guarantees for these policy switching criteria that work for methods dealing with large state space in contrast to existing analyses about tabular RL [ important problem, which could serve as a foundation towards great future research advances. Thanks to the strong research nature of this work, we believe our paper does not produce any negative societal impact. Avg. RSI Feature Policy Info Visitation FIX_1000