Abstract—With the global metamorphosis of the beauty industry and the rising demand for beauty products worldwide, the need for an efﬁcacious makeup recommendation system has never been more. Despite the signiﬁcant advancements made towards personalised makeup recommendation, the current research still falls short of incorporating the context of occasion in makeup recommendation and integrating feedback for users. In this work, we propose BeautifAI, a novel makeup recommendation system, delivering personalised occasion-oriented makeup recommendations to users while providing real-time previews and continuous feedback. The proposed work’s novel contributions, including the incorporation of occasion context, region-wise makeup recommendation, real-time makeup previews and continuous makeup feedback, set our system apart from the current work in makeup recommendation. We also demonstrate our proposed system’s efﬁcacy in providing personalised makeup recommendation by conducting a user study. The beauty industry has seen massive growth over the past few years, given the advent of social media and targeted advertising. For several decades, humans have used makeup and beauty products to enhance their appearance and to make themselves more attractive and presentable to the world. As we move through the world, we apply, add, and refresh our makeup throughout the day in anticipation of the different social contexts in which we will interact with others. Scant social scientiﬁc attention has been paid to the creation and marketing of tastes and desires by the beauty industry in India and women’s engagement with those desires [1]. The socioeconomic conditions in India and other South-Asian countries are unique. They indicate that while a large section of the population has access to smartphones, very few have the resources to regularly utilise beauty consultants’ services to obtain personalised makeup beauty regimens. To ensure makeup recommendation services to a broader audience, we offer a personalised makeup recommendation system that is mobile deployable and easily accessible. Various research and commercial tools have attempted to tackle various makeup and beauty-related issues like makeup recommendation [2]–[5], makeup detection [6]–[8], makeup classiﬁcation, makeup product retrieval [9] and makeup style transfer [10]– [14]. While these works excellently tackle speciﬁc sub-domains of makeup and beauty-related AI, they still fall short of providing a truly personalised experience in terms of makeup recommendation. These works lack the element of interactivity and do not capture the context of occasion for makeup recommendation/retrieval. The recommendation systems [2]–[5] generally recommend a holistic makeup recommendation (providing one single makeup recommendation for the entire face) which only allows limited diversity. Tackling these issues with makeup recommendation is the main motivation behind the proposed work. We deﬁne makeup knowledge as consisting of both makeup styles and the context for appropriate occasions. Despite the presence of many before and after makeup datasets [6], [15], [16] and a few labelled makeup datasets [2], we ﬁnd that all current makeup datasets fail to incorporate the context for occasion. To address this, we curate our makeup dataset for incorporating the context of occasion for makeup, which consists of images extracted from social media platforms, representing a diverse range of occasionoriented makeup knowledge. The proposed work is based on the principle that an optimal makeup recommendation for a particular occasion scenario would essentially be suitable for that occasion while keeping the makeup styles suited to a user’s unique facial structure and liking. Any attempt at tackling the makeup recommendation problem must address the questions, (1) What kind of makeup should I wear to a particular occasion/event? (2) How will the chosen makeup look on my face? (3) Have I applied the makeup accurately with respect to my choice? To address these questions, we propose BeautifAI - an end-to-end makeup recommendation system that allows users to choose from a plethora of makeup styles most suitable to their faces for a particular occasion, customise them to their preference, visualise how the chosen makeup style will look on their face as well as provide continuous feedback on how their applied makeup looks in comparison with the recommended makeup style. The proposed system requires users to provide two inputs a picture of their face and the occasion for which they require makeup recommendations. The system then recommends users a combination of makeup options for the skin, eyes and lips most suited to their face and the occasion. The previews of these recommendations are then displayed on the picture of the users’ face, allowing them to visualise how the makeup would look on their faces. While the users apply this recommended makeup, the system continuously provides them feedback on how accurately they have applied the recommended makeup combination. To integrate all these functionalities into the system, we design four modules, three of which are novel contributions of this work (Module 2,3,4). Firstly, we propose a facial feature extraction module (Module 1) by ﬁne-tuning different state of the art algorithms for facial feature extraction. We extract these facial features to ensure that we recommend users makeup styles tailored to their face’s unique features. This is a signiﬁcant aspect of personalisation. Secondly, we contribute a novel occasion-oriented region-wise makeup recommendation module (Module 2) that recommends users a large number of makeup styles for different regions of the face (skin, eyes, lips) by comparing neural embeddings, based on the user’s choice of occasion. This region-wise recommendation module provides greater customizability, better engagement and greater diversity in recommendations than the holistic recommendation as proposed by current work. Thirdly, we propose a styletransfer module (Module 3) to provide users with previews of how the recommended makeup styles would look on their face. We propose a region-wise method for makeup style transfer based on Generative Adversarial Networks (GANs) to provide visuals of our region-wise makeup recommendation styles. Fourthly, we design an innovative continuous feedback module (Module 4) using colour correlogram features that assists users while applying makeup by dispensing detailed real-time feedback on how to achieve their desired look. In summation, the main contributions of the proposed work are as follows: that contextualises the occasion aspect in makeup recommendation. that improves upon the limited diversity in recommendation styles offered by the present work and affords greater personalisation. based on GAN technology for visualising makeup recommendation results. evaluation that assists users in achieving their desired makeup look in real-time. We further evaluate different aspects of our system’s performance by conducting a comprehensive user study and ﬁnd that most participants rate our system’s region-wise makeup recommendations highly in terms of both relevance and diversity of the recommended makeup styles. Users also rate highly the results of our style-transfer technique in comparison with others. A majority of participants also rate the continuous feedback for makeup application highly/moderately effective while applying the recommended makeup styles. The prime objective of our proposed work is to design and deploy an end-to-end makeup recommendation system for users while integrating facial feature extraction and style-based makeup transfer. Hence, we primarily focus on recent developments in makeup recommendation and makeup style transfer. In recent works, various attempts have been made towards personalised makeup recommendation by using technologies such as those proposed by [2]–[5]. Alashkar et al. proposes a unique methodology where an inference engine is trained on a set of predeﬁned rules followed by a deep neural network on a labelled dataset to recommend makeup styles. Nguyen et al. employs a latent SVM model to draw relationships between makeup invariant features and makeup attributes for efﬁcient makeup recommendation. [4] propose a multiple tree-structured super-graphs model to express the complex relationships among beauty and beauty-related attributes from extracted facial and clothing features. Despite these methods achieving excellent results with their unique approaches for makeup recommendation, they are limited in their scope as they lack context for occasion and provide a holistic recommendation as opposed to the occasion-oriented region-wise makeup recommendation proposed by this work. Style Transfer is deﬁned as transferring the makeup style (fully or partially, as needed) of a person from one image to another image. There has been a lot of research as well as online tools available for makeup style transfers/makeovers. Recently, GANs have been widely used for transferring style, patterns, texture from one image to another and are capable of generating high resolution realistic result images [17], [18]. The research - GANs Disentangled Makeup Transfer with Generative Adversarial Network [10], PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup [11], BeautyGAN: Instance-level Facial Makeup Transfer with Deep Generative Adversarial Network [12] utilise GANs for makeup transfer and provide accurate and natural makeup style transfer results with very little distortion. After evaluating many recently developed makeup style transfer techniques, we found [12] the best on the basis of efﬁciency and compatibility of results. The user study [12] found that BeautyGAN fares best when compared with other makeup style transfer techniques, [13] and [14], on the basis of quality, realism and makeup style similarity. These works transfer the entire makeup from one face to another which is not compatible with the results provided by our region-wise makeup recommendation module. Hence, we propose our novel style-transfer method in module 3. Presently, various makeup datasets have been collected for different research perspectives - YMU (YouTube Makeup) [15], VMU (Virtual Makeup) [15], MIW (Makeup in the ”wild”) [6], MIFS (Makeup Induced Face Spooﬁng) [16], MT [12] and SMU [19]. However, none of these datasets has adequate labelling for makeup styles as well as occasion. YMU, VMU, MIFS, MIW and MT only consist of images before and after makeup application with no annotation for makeup attributes or occasions whereas [2] contributes a Before-After Makeup dataset with makeup attribute labelling but no occasion labelling. To perform accurate occasionoriented makeup recommendation, we contribute our high-quality dataset to integrate the aspect of occasion with makeup styles and attributes. Social media platforms provide much user-centric data with images displaying various real-world makeup styles with natural backgrounds from users across the globe. Our dataset consists of images scraped from Instagram and Pinterest. Our dataset comprises 804 annotated images. The detailed comparison of the datasets is shown in Table 1. Collecting the images from social media ensures that they are up-to-date with the latest makeup trends. We extract images from these platforms using numerous currently trending hashtags and keywords most relevant to that category. On interviewing makeup experts, we ﬁnd that the three occasions - Casual, Professional/Ofﬁce and Party most accurately represent the images in our dataset. We ensure that the images COMPARISON OF RELEASED DATASETS WITH OUR DATASET extracted are diverse in terms of ethnicity and balanced in terms of occasion and makeup styles. We also perform manual ﬁltering to ensure all images adequately represent makeup knowledge and are face-frontal images. We deﬁne a set of attributes and classes, similar to that of [2], to best represent the fashion knowledge in terms of the makeup knowledge in each image, as shown in Figure 1. We believe it is essential to label the attributes of the makeup styles because this would allow users to acquire the products from the recommended makeup styles with ease. We estimate that most images depict females in the age group of 18-45. Three fashion/makeup experts annotate each image for ten different makeup attributes and three different occasions, and the inter-annotator agreement is calculated using kappa statistics. Each image is annotated by two fashion experts, and in case of a dispute, the third expert breaks the tie. As a general guideline, a kappa value above 0.6 is considered adequate, and above 0.8 is considered almost perfect [20]. We ﬁnd the inter-annotator agreement reasonably high with a kappa value of 0.76 for makeup attribute annotation and 0.88 for occasion annotation. Therefore, we achieve our makeup dataset, which describes each image in terms of the person, makeup and occasion. We will be making our curated dataset publicly available on publication. Figure 1. The proposed annotation scheme for our dataset, including 10 attributes and 3 different occasions. The proposed system consists of four modules to provide users with the best experience while recommending a beauty regimen of makeup styles. The system requires two inputs from the user - an image of their face and the occasion for which the user wants makeup recommendations. The proposed pipeline for our system consisting of different modules is shown in Figure 2. These four modules are described in detail below. Modules 2, 3 and 4 are unique contributions of our research while Module 1 is implemented by ﬁne-tuning open-source algorithms. During the past decade, there have been great advances in the ﬁelds of computer vision and image processing for detection and extraction of facial features for a variety of applications namely surveillance, human-computer interaction, etc. We extract facial features to provide high quality makeup recommendations that are personalised for every user. Each makeup recommendation is unique to each user as it is suited to their peculiar facial structure and features. We propose a system where for a given image of the user as input, this module identiﬁes three regions of interest i.e. the skin, the eyes and the lips. We ﬁnd that various algorithms work with different efﬁciency on extracting facial features. For identifying the facial/skin region - the entire face except the eyes and lips (depicted by region of interest A in Figure 3), we utilise the ensemble method proposed in DeepFace [21], a deep neural network with remarkably high performance for identifying and segmenting entire faces as bounding boxes. This model is state of the art for facial detection and veriﬁcation. The algorithm employs explicit 3D face modeling in order to apply a piecewise afﬁne transformation, and derive a face representation from a ninelayer deep neural network. This method provides an accuracy of 97.35%. After extracting the skin region, we extract the eyes and lips region of interest. For identifying the skin and lips region of the face (regions of interest B and C in Figure 3), we use a Faster RCNN model (with a ResNet-50 backbone) [22], which is a state of the art convolutional neural network for object detection, pretrained on the MS Coco Dataset [23]. We then ﬁne-tune this model by freezing the ﬁrst 50% layers and performing transfer learning on the MUCT Face Database [24]. We ﬁne-tune this model on the MUCT dataset as it contains facial images with 76 landmarks for 3755 images that allows for greater precision in identifying the relevant regions of interest. For training, we generated the bounding boxes for each of the eyes and the lips region on the MUCT Database on the basis of 76 facial landmarks and resize the images to 128x128, allowing the model to detect the regions of interest in real time. B. Module 2: Occasion-oriented Region-wise Makeup Recommendation This module’s central premise is to contextualise the occasion concept for makeup and ensure greater customisability over existing works in terms of makeup recommendation. Post identiﬁcation and segmentation of the regions of interest via module 1, this module recommends users suitable makeup styles personalised to their faces while incorporating the occasion context. We assume that two individuals with similar facial features and similar facial structure would be suited to similar makeup styles [5]. Therefore, to personalise makeup recommendation, users are recommended styles from the dataset with whom their facial features match with the highest probability. The user picks the occasion (Casual, Professional/Ofﬁce, Party) and provides a picture of their face I as an input image to the system. We then perform occasion-based ﬁltering to ﬁlter out the makeup styles from our dataset based on the occasion and extract facial features from image I using module 1. The three extracted regions of interest for image I are I, Iand I. After extracting these facial features I, Iand I, we compare them with the occasion-ﬁltered makeup images from our curated dataset in terms of extracted facial features. Let us assume that the occasion-ﬁltered images in our dataset are S, where i=1, 2, 3......n and n is the number of occasion-ﬁltered images in our dataset. We extract three regions of interest for all images Si.e. S, S and Sas bounding boxes using module 1. We effectively compare regions of interest of I with S(skin, eyes and lips) by building a CNN model and leveraging the concept of neural embeddings (mapping discrete variables to continous vector matrices). For the backbone of our CNN model, we use a SE-ResNeXt-50 [25] pre-trained on the VGGFace2 dataset [26]. We ﬁne-tune this CNN model by performing transfer learning on the extracted regions of interest (resizing to 128x128) for our entire dataset D, Dand D, after freezing 70% of the hidden layers. After training this model, we then generate neural embeddings for each of those bounding boxes in our dataset and for the regions of interest for the input image I. The neural embeddings for the occasion-ﬁltered dataset are S, Sand Sand for the input image are I, Iand I. The Squeeze-andExcitation (SE) blocks added to the ResNeXt-50 model, improve its representational ability by enabling it to perform dynamic channelwise feature recalibration and ensure that the neural embeddings are robust. We then calculate the similarity between the embeddings I and S. We calculate the similarity by comparing these embeddings in the euclidean space [27]. We use neural embeddings instead of classifying the regions of interest into discrete categories as it allows us to detect ﬁner margins and compare them with greater effectiveness. This module recommends makeup for the regions - skin, eyes and lips in three phases. First, the module recommends skin makeup by retrieving the ten most similar images from the dataset by comparing the neural embeddings I(the entire face except the eye and lip region) with the embeddings S. Once the user picks a skin makeup style, the module recommends the user ten eye makeup styles by similarly comparing the embeddings I with S. After the user chooses the skin and eye makeup styles, the module recommends makeup for the lips. The module then compares the embeddings Iwith Sand recommends the ten most ideal lip makeup styles. The alogrithm for this module is as follows: The incorporation of region-wise recommendation instead of the holistic makeup recommendation (providing a single makeup recommendation for the entire face), as present in the current state of the art, ensures users a greater variety of makeup styles to choose from and greater personalisation. Figure 4 visualises the different steps for the proposed makeup recommendation module. We believe that greater customisability is essential for personalisation as it allows users to pick only the components of a makeup style that they like and choose makeup styles on the basis of the makeup accessories they already own or can easily purchase. Users can choose from nearly 500 million possible makeup styles (804*804*804) instead of the 804 possibilities had we opted for the holistic recommendation. These makeup recommendation results are passed to the next module to provide users with previews of the makeup outlook. To show users previews of how the chosen makeup combination would look on their face, we contribute a style-transfer module in our system. This module provides the user with a preview of how their face would look after every makeup stage. Module 2 recommends makeup styles from different regions of different facial images in our dataset. The proposed system in BeautyGAN transfers ﬁner makeup details by incorporating both global domain-level loss and local instance-level loss in a dual input/output Generative Adversarial Network. This method only conceptualises transferring the entire makeup from one face to another. However, to provide previews for the results of module 2, we require an advanced solution for a region-wise makeup transfer. Figure 3. Regions of Interest after performing facial feature extraction. To generate real-time previews for our region-wise makeup recommendation, we implement a region-wise style transfer module based on GAN technology, given it’s recent success in makeup transfer based works. We initialise our model as a general pretrained GAN-based network [18]. We extract the facial regions and facial features on the entire MT dataset using module 1. We then ﬁne-tune this pre-trained GAN network on the MT Dataset using a Dilated ResNet (DRN) architecture after freezing the ﬁrst 25% layers [11]. We separately train the model for each eyes, lips and skin to ensure our model is able to perform style-transfer for each region of interest independently and robustly in accordance with our region-wise makeup recommendation. The results of our model are considerably better than those of other methods because our model while utilising the DRN architecture, is trained on the much larger MT dataset independently for all three facial regions. We resize the images to 128x128 to ensure real time-previews as taking higher resolution images would take larger computation time and is not mobile deployable. Figure 5 shows several example results of our style transfer method. This module provides users with continuous feedback on how their applied makeup looks compared to the makeup recommended by the system in real-time and is a novel contribution of this Figure 5. Example previews of our Style Transfer Method work. Applying the makeup with similar precision as a systemrecommended makeup image is nearly impossible for the average user. This module’s central proposition is to make the system more interactive and provide the user with a better experience by continuously providing the users with interpretive feedback to replicate their desired makeup look with the greatest precision. To provide continuous feedback, the module requires the user to provide a picture after applying all the makeup (skin, eyes and lips). Let the image uploaded by the user with the makeup be U . The module then compares this image U with the desired makeup image X to tell the user if the makeup has been applied accurately (shown in the Continuous Evaluation step in Figure 2) with respect to each skin, eyes and lips makeup. We compare these regions by their makeup colour intensities by calculating their Color Correlogram [28] vectors. The primary beneﬁt of using this technique is that it incorporates color adjacency and intensity and robustly tolerates signiﬁcant changes in viewing positions, camera zooms, etc. The two correlogram vectors are compared by measuring their similarity (Refer Eq. 2) and then telling the user whether the makeup (lips, eyes or skin) is either accurate or more/less based on empirically obtained thresholds. The premise behind the colour correlogram is that we pick any pixel p of colour Cin image I and then ﬁnd the probability that another pixel pat distance d away from pixel pis also of colour C. The colour correlogram of image I for colour Cat a distance d is given by Eq. 1. In order to measure similarity for colour correlograms of two images say I and I, we calculate aggregate similarity at all distances (Refer Eq. 2), where m is the set of all colours and k is the set of all distance measures used to calculate the colour correlogram. To perform continuous evaluation, we again segment the image U into three regions U, Uand Uby utilising module 1 and compare the segmented regions of interests with the corresponding region of interest from the ﬁnal preview image X, Xand X(visuals generated by style transfer). Let the colour correlogram vector of the user-applied makeup regions be U, Uand Uand for the image generated by module 3 be X, Xand X. We then calculate aggregate similarity for each of these regions (as per Eq. 2) and provide user feedback on whether the makeup for each region is more, less or accurate as shown below. We also compare the performance of different image processing techniques with that of the colour correlogram, namely, Colour Histogram [29] and SURF [30] (Refer Table 5). To comprehensively evaluate our system quantitatively and qualitatively, we conduct a user study. The users comprise 45 Indian females belonging to the age group 18-45. Given that our system’s central aim is to provide quality recommendations, improve personalisation and the user experience, we believe that a user study is the most appropriate method to evaluate our system thoroughly, as done by other state of the art works. We allow users to evaluate our approach in terms of all its different functionalities. We evaluate our region-wise makeup recommendation module to demonstrate its efﬁcacy over the traditional holistic makeup recommendation. We ask users to upload a selﬁe/full frontal facial image and their preferred occasion for makeup. Then we provide the users RESULTS OF THE USER STUDY FOR MAKEUP STYLE TRANSFER with makeup styles using holistic makeup recommendations, followed by makeup styles by utilising our novel region-wise makeup recommendation. We ask users to rate each recommendation on a scale of 1-10 based on (1) how relevant the recommendations are in terms of occasion and makeup styles? (2) Is there diversity in the recommended makeup styles for a chosen occasion? The user study results (Refer Table 2) show that users rate the recommendations of our proposed system highly in terms of diversity, afﬁrming the success of the region-wised makeup recommendation module over the holistic makeup recommendation approach. We also ﬁnd that, on average, users ﬁnd recommendations with our novel module more relevant in terms of occasion and the makeup styles themselves than the holistic makeup recommendation, as utilised in the current work. This conﬁrms that we have been able to optimise personalised makeup recommendation in comparison with previous works. To evaluate the quality of previews by our style transfer module, we ask users to compare the results of the previews provided by our module versus that provided by [12] and [11]. For every recommendation, we provide users with three different previews and ask the reviewer to rate which visual they ﬁnd most appropriate in terms of eyes, lips and skin. Table 3 depicts the percentage of users that pick each algorithm. We ﬁnd that a majority of users rate the results of our system the best in terms of eyes and lips whereas a majority rate BeautyGAN highly for skin. We then evaluate how effective the addition of our novel continuous evaluation module is to makeup recommendation. We ask users to evaluate its efﬁcacy in providing feedback and helping users achieve their look better in terms of each skin, eyes and lips makeup. Table 4 depicts the percentage of users that ﬁnd the continuous evaluation module valuable in assisting them while applying their makeup. We ask the users to rate the continuous evaluation as ’Highly Effective’, ’Moderately Effective’ or ’Not Effective’. The results of this evaluation (Refer Table 4) afﬁrm that a majority of users ﬁnd the continuous evaluation module highly/moderately effective for all three regions of makeup. We also ﬁnd that most users ﬁnd the continuous evaluation for skin makeup the most helpful while applying makeup. We also evaluate the effectiveness of the different image processing algorithms for continuous makeup evaluation by asking users to choose between the makeup feedback provided by the colour histogram, colour correlogram and SURF algorithms. We ﬁnd that users ﬁnd the feedback provided by the colour correlogram algorithm the most accurate for skin, eyes and lip makeup (Refer Table 5), afﬁrming our choice. In this paper, we designed a system ensuring high-quality personalised makeup recommendations and greater interactivity. We made contributions to the domain of makeup recommendation by incorporating occasion-oriented region-wise makeup recommendation, region-wise makeup transfer and continuous makeup evaluation. We successfully incorporated the context of occasion and improved over existing works by proposing a region-wise makeup recommendation module. Our user study conﬁrmed the efﬁcacy of these novel additions in terms of personalisation. We conclude that users found the recommendations provided by this system highly relevant and diverse. Users also rated the results of our style transfer method highly and found the addition of the continuous makeup evaluation feature highly useful. Our dataset primarily consists of female images in the 18-45 group, and we conducted our user study on a similar age-wise demographic. In the future, to further assess the performance of our system, we plan on expanding our dataset to cover a larger demographic and more occasions. We also believe that the system can be further improved by instilling higher-level makeup concepts like ethnicity, cultural dependence of makeup styles, time (day/night), body features, and dress, making recommendations more unique and personal. We plan on further exploring these avenues in the future.