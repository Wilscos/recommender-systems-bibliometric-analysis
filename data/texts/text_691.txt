Traditional statistical estimation, or statistical inference in general, is static, in the sense that the estimate of the quantity of interest does not aﬀect the future evolution of the quantity. In some sequential estimation problems however, we encounter the situation where the future values of the quantity to be estimated depend on the estimate of its current value. Examples include: 1) stock price prediction by big investors, where the prediction of today’s price of a stock aﬀects today’s investment decision, which further changes the stock’s supply-demand status and hence its price tomorrow; 2) interactive product recommendation, where the estimate of a customer’s preference based on their activity leads to certain product recommendations, which would in turn shape the customer’s future activity and preference; 3) behavior prediction in multi-agent systems, e.g. predicting the intentions of vehicles on the road adjacent to the ego vehicle, where the prediction of an adjacent vehicle’s intention based on its current driving situation leads to a certain action of the ego vehicle, which can change the future driving situation and intention of that adjacent vehicle. agent that interacts with a system of interest, through a measurement-inference-action loop. During the interaction, the inference, either estimation or prediction, of a property of the system based on the measurements of its current and past states aﬀects the action to be taken by the autonomous agent, which further inﬂuences the future states and properties of the system of interest. We may call such problems as dynamic inference. probabilistic framework. It is shown in Section 3 that this problem can be converted to a Markov decision-making process (MDP), and the optimal estimation strategy that minimizes the overall Traditional statistical estimation, or statistical inference in general, is static, in the sense that the estimate of the quantity of interest does not change the future evolution of the quantity. In some sequential estimation problems however, we encounter the situation where the future values of the quantity to be estimated depend on the estimate of its current value. Examples include stock price prediction by big investors, interactive product recommendation, and behavior prediction in multi-agent systems. We may call such problems as dynamic inference. In this work, a formulation of this problem under a Bayesian probabilistic framework is given, and the optimal estimation strategy is derived as the solution to minimize the overall inference loss. How the optimal estimation strategy works is illustrated through two examples, stock trend prediction and vehicle behavior prediction. When the underlying models for dynamic inference are unknown, we can consider the problem of learning for dynamic inference. This learning problem can potentially unify several familiar machine learning problems, including supervised learning, imitation learning, and reinforcement learning. Broadly speaking, this type of interactive sequential estimation problems arises in any autonomous In Section 2, a mathematical formulation of dynamic inference is given under a Bayesian inference loss can be derived as the optimal policy of this MDP through dynamic programming. Two examples, stock trend prediction and vehicle behavior prediction, are given in Section 4 to illustrate how the optimal estimation strategy for dynamic inference works, and how it diﬀers from the solution to the traditional statistical inference. the situation where the underlying probabilistic models for dynamic inference become unknown. Learning for dynamic inference can potentially serve as a unifying meta problem of machine learning, such that supervised learning, imitation learning, and reinforcement learning can be cast as its special instances. Having a good understanding of dynamic inference and its learning extension will thus be helpful in gaining better understandings of a broad spectrum of machine learning problems. The formulation of dynamic inference appears to be new, but it can be related to a variety of existing interactive decision-making problems and prediction problems that take the consequence of the prediction into account. Moreover, any MDP may be thought of as a dynamic inference problem. These related problems are discussed in Section 6. In traditional statistical inference, the goal is to estimate a quantity of interest an observation (X, Y ) ∈ X × Y is modeled as a jointly distributed random vector with distribution P loss function that achieves the minimum expected loss: A basic result from estimation theory is that for any is a minimizer of the expected posterior loss, i.e. statistical inference problem is static, in the sense that only one round of estimation is considered. When there is a need to estimate a sequence of quantities to minimize the accumulated expected loss can be optimally solved by repeatedly using the same single-round optimal estimator ψ The problem of based on observations observation previous round; the estimate so far, namely ( rounds. Here it is assumed that after the can also happen that based on estimate Y Section 5 brieﬂy discusses the problem of learning for dynamic inference, which is to address ,(X, . . . , X), if the pairs (X, Y) are i.i.d. fori= 1, . . . , n, the sequential estimation problem X. Nevertheless, it will be shown in Section 3 that an optimal estimation strategy can probability transition kernel estimate in the previous round, ith quantity of interest given the probability transition kernels the observation-transition model and the quantity-generation model, respectively. The estimates Deﬁnition 1. which maps the history of observations and revealed quantities of interest ( all the random variables ( variables in dynamic inference with a Markov estimation strategy, meaning that each estimate has the form ψ Figure 1: Bayesian network of the random variables under consideration with assume the estimates are made with Markov estimators, such that dynamic inference. This loss function is a generalization of the one used in statistical inference, in the sense that the estimate in each round is evaluated in the context of the observation in that round. Given an estimation strategy loss over the strategy to minimize the inference loss: where the two distinctive features of dynamic inference: •The joint distribution of the pair ( depends on (X In this section we show that the dynamic inference problem can be converted to a Markov decision process, and the optimal estimation strategy can be found via dynamic programming. Formally, we assume the knowledge of the distributionPof the initial observation, the = (ψ, . . . , ψ), whereψ: X×Y→bYis the estimator used in theith round,i= 1, . . . , n, , such thatbY= ψ(X, Y). Any speciﬁcation ofP, (Pb), (P)andψdeﬁnes a joint distribution of We use a loss function`: X×Y×bY → Rto evaluate the estimate made in each round in bY=ψ(X, Y). Comparing with the statistical inference problem in(1), we summarize For a given loss function corresponding observation-estimate loss function for ( previous section, we know that in dynamic inference X, therefore for any realization ( We see that estimator inference loss can be expressed in terms of the observation-estimate loss: Lemma 1. For any estimation strategy, the inference loss in (2) can be rewritten as Proof. For each i = 1, . . . , n, where conditionally independent of ( then follows from the fact that E where J(ψ Next, we show that the search space of the optimization problem in estimators of irrelevant information [1], and provide a proof for completeness. Lemma 2. For any ﬁxed function f : U × V → R and for any jointly distributed pair (U, Z), x, ˆy)∈X×bY. From the speciﬁcation of the joint distribution of the random variables in the , ˆy) can be computed as ψ. This fact is crucial for the optimality proof later. With the above deﬁnition, the (7)is a consequence of the fact thatbYis determined by (X, Y) and the fact thatYis With Lemma 1, the optimization problem in (2) becomes equivalent to ¯ψ: X→bY, such thatbY=¯ψ(X). We start with a lemma known as Blackwell’s principle Proof. to the loss function risk of estimating written as from a data processing inequality of Bayes risk [2, Lemma 1] that asU − U − strategy can be replaced by a Markov one, which preserves the optimality. Lemma 3 The existence of such an estimator is guaranteed by Lemma 2. estimator can also be replaced by a Markov one which preserves the optimality. Proof. According to Lemma 1, the inference loss of the given (ψ Since the ﬁrst expectation in Markov estimator where used in the ( expectation on the right side of (15) as The left side of(10)is the Bayes risk of estimatingUbased on (U, Z), deﬁned with respect R(U|U). It is clear from their deﬁnitions thatR(U|U, Z)≤ R(U|U). It also follows (U, Z) form a Markov chain. HenceR(U|U, Z) =R(U|U), which proves the claim. The ﬁrst application of Lemma 2 is to prove that the last estimator of an optimal estimation (Last-round lemma).Given any estimation strategyψ, there exists a Markov estimator bY, such that J(ψ, . . . , ψ,¯ψ) ≤ J (ψ). Lemma 2 can be further used to prove that whenever the last estimator is Markov, its preceding ((i −1)th-round lemma).For anyi ≥2, given any estimation strategy (ψ, . . . , ψ,¯ψ) i-round dynamic inference, if the last estimator is a Markov one¯ψ: X→bY, then there exists , . . . , ψ,¯ψ). ¯Xon the left side is the observation in theith round when the Markov estimator¯ψis and notice that the inner conditional expectation as a function of ( probability transition kernel P where the function estimator which proves (15) and the claim. Proof. one that preserves the optimality of the strategy, as guaranteed by Lemma 3. Then, for we repeatedly replace the ( previous strategy, as guaranteed by Lemma 4 and the additive structure of the inference loss as same inference loss as the originally picked strategy. Theorem 1 with Lemma 1 imply that the original dynamic inference problem in With this reformulation, we see that the unknown quantities in(22) become the states in this MDP, the estimates kernel policy of this MDP to minimize the expected accumulated loss with respect to solution to the MDP will be an optimal estimation strategy for dynamic inference. . This is because the conditional distribution ofXgiven (X,bY) is speciﬁed by the With Lemma 3 and Lemma 4, we can ﬁnally prove the optimality of Markov estimators. Picking an optimal estimation strategyψ, we ﬁrst replace its last estimator by a Markov (ψ). Finally we obtain an estimation strategy consisting of Markov estimators achieving the any more, and the optimization problem becomes a standard MDP: the observationsX Pbnow deﬁnes the controlled state transition, and any Markov estimation strategy becomes a policy of this MDP. The goal of dynamic inference then becomes ﬁnding the optimal From the theory of MDP [3] it is known that the optimal policy for the MDP in estimation strategy for dynamic inference, can be found via dynamic programming. To derive the optimal estimators, deﬁne the functions as Q and For any estimation strategy that the observation in the ith round is x: The following theorem states that the estimation strategy ( programming not only achieves the minimum inference loss, but also achieves the minimum loss-togo in each round with any observation in that round. Theorem 2. for dynamic inference, which achieves the minimum in strategy ψ for all x ∈ X and i = 1, . . . , n, with equality if ψ (x, ˆy) ,¯`(x, ˆy), xas the conditional expected loss accumulated from theith round to the ﬁnal round given The ﬁrst claim stating that the estimation strategy (ψ, . . . , ψ) achieves the minimum in follows from the equivalence between the original problem in(2)and the MDP reformulation in as discussed in Section 3.1.3, and from the well-known optimality of the solution via dynamic The second claim can be proved via backward induction. Consider an arbitrary Markov estimation • In the ﬁnal round, for all x ∈ X, where computed as in holds if ψ ﬁrst show a self-recursive expression of V where the ﬁrst term of follows from the fact that is a consequence of the assumption that the estimators under consideration are Markov and the speciﬁcation of the joint distribution of (X where given and the ﬁnal equality condition follows from the deﬁnitions of V This proves the second claim. in terms of optimal estimation strategy for any ( results are stated in the following corollary. (28)follows from the deﬁnition ofVin(26);(29)follows from the way how¯`can be i=n −1, . . . ,1, as the inductive assumption, suppose(27)holds in the (i+ 1)th round. We (36)follows from the inductive assumption;(37)follows from the fact thatbYis determined X=xthrough the Markov estimatorψ;(38)follows from the deﬁnition ofQin(24); A consequence of Theorem 2 is that the minimum loss-to-go at theith round can be expressed V. Moreover, once the values ofV(x) for allx ∈X andi= 1, . . . , nare computed, the i, . . . , ncan be determined by these values and the observation-estimate loss function¯`. These Corollary 1. For any i = 1, . . . , n and any initial distribution P and the minimum is achieved by the estimators (ψ We now work out an example to illustrate how an optimal estimation strategy for dynamic inference works. Consider a situation where the observations, the quantities of interest, and the estimates all take binary values, i.e. X = Y = to be stationary and deterministic, such that and takes the form `(x, y, ˆy) = 1{y 6= ˆy}. estimates during the dynamic programming procedure presented in Section 3.2. With the Fig. 3. The optimal estimate at each observation is also labeled: a solid branch indicates an optimal estimate, while a dashed branch indicates a non-optimal one. We see that at each observation, the optimal estimate for dynamic inference can be diﬀerent from that for the single-round estimation. For example, at Figure 3: Unrolled observation-transition diagram for the dynamic inference example given in Section 3.3, with A solid branch indicates an optimal estimate, while a dashed branch indicates a non-optimal one. The three blue branches indicate the optimal estimates for dynamic inference that are diﬀerent from the optimal single-round estimates. = 1, as depicted in Fig. 2. The quantity-generation model is also stationary and is described (Y= 1|X= 0) = 0.1 andP(Y= 1|X= 1) = 0.6. The loss function neglects the observation Figure 2: An example of stationary and deterministic observation-transition model. With this setup, the goal of dynamic inference is to minimize the expected number of wrong Vfunction are labeled at each observation in the unrolled observation-transition diagram in whereas the optimal estimate for the single-round estimation at these observations would be to minimize E[1{Y 6= ψ(X)}]. inference: in dynamic inference, the optimal estimate in each round strives to balance the lossto-incur in that round and the loss-to-go from that round. Consequently, an optimal estimation strategy may need to, at least occasionally, make non-optimal single-round estimates, in order to steer the future observations toward those with which the associated quantities of interest are easier to estimate or less costly if inaccurately estimated. Having formulated the dynamic inference problem and derived its solution, in this section we study its applications to real challenges. The two examples given below are simplistic, but they capture the essence of how dynamic inference can be used to model and solve various sequential and interactive estimation or prediction problems. The ﬁrst application is the prediction by big investors of the trend of stock, which could be the trend of the price of an individual stock or the index of a bundle of stocks. The trend, either rising or falling, statistically depends on some observable market signal, e.g. the supply-demand proﬁle of the stocks under consideration. The prediction is sequentially made for several rounds, e.g. one round each day for that day’s trend, each based on the past observed market signals. Once a prediction is made, it inﬂuences that day’s investment decision and hence the supply-demand proﬁle of the stocks under consideration, which will be reﬂected by the market signal in the next day and will further inﬂuence the next day’s trend. be predicted as observation model, we consider the situation where model of the next round’s market signal given the signal and prediction in the current round can then be described by dependence of the trend on the market signal can be described by and function can be simply `(x, y, ˆy) = 1{y 6= ˆy}. Figure 4: A stationary observation-transition model for stock trend prediction. The solid arrows represent the deterministic transition X This example reveals a key diﬀerence between dynamic inference and the traditional statistical Formally, fornrounds of prediction, letY∈ {0,1}be the trend in theith round, which is to P(Y= 1|X= 1) = 0.7 where the trend positively correlates with the market signal. The loss Figure 5: Unrolled observation-transition diagram for the stock trend prediction example, with n= 6. The value of indicates an optimal prediction, while a dashed branch indicates a non-optimal one. The three blue branches indicate the optimal predictions for dynamic inference that are diﬀerent from the optimal single-round predictions. similar to the example presented in Section 3.3, only with a more general observation-transition model. Figure 5 shows the optimal estimation strategy of this example when the observationtransition model is deterministic, such that we see from Fig. 5 that at is diﬀerent from the optimal single-round prediction. These predictions are made to steer the market signal to 1, with which it is more certain that the trend will be rising according to smaller prediction error probability to occur. Only when with the optimal single-round predictions, as the future prediction errors to accumulate weigh less toward the end of dynamic inference. Another challenging problem that could be cast as dynamic inference is behavior prediction of vehicles on the road. For example, a desired feature of a self-driving system is to predict whether the following vehicle in the neighbor lane would yield if the ego vehicle initiates a cut-in to that lane, whenever there is a need for lane change. This task may be termed as yield prediction, which can be sequential and interactive especially when the traﬃc is dense: the predicted intention determines the action to be taken by the ego vehicle, e.g. to turn on the blinker and initiate the cut-in when the following vehicle is predicted to yield, or not to cut-in and shoot for another gap when it is predicted not to yield; in response to the ego vehicle’s behavior and according to the driving situation, the following vehicle would either slow down or accelerate, which can change the driving situation of the two vehicles and aﬀect their subsequent behaviors; the interaction continues until the cut-in is completed due to a yield by some vehicle, or given up due to an opposite. prediction. Suppose the prediction can be deconstructed into Xdenote the driving situation, which could be the positions and velocities of the vehicles under consideration, and let simplest setting, vehicles, and the probabilistic model relating With all these elements speciﬁed, the problem is recognized as a dynamic inference problem As in the stock trend prediction, we can formally deﬁne a dynamic inference problem for yield can be empirically determined. The observation-transition model in yield prediction is non-stationary and depends on the design of the planner in the self-driving system. For example, when and increase the gap and still aims to cut-in, or smaller if the planner decides to slow down to shoot for another gap behind the following vehicle. Another feature of this problem is that the loss function should be contextual and carefully designed. For example, when loss can be small and proportional to other hand, when to heavily penalize a wrong prediction that can lead to a dangerous situation. With all the elements speciﬁed, the problem can in principle be solved through dynamic programming as in Section 3.2, to minimize the overall prediction cost. are depicted in Fig. 6. In the ﬁrst case, initiates the cut-in by turning on the blinker, which results in a slow-down of the following vehicle, allowing the cut-in to be completed. There is only one round of prediction in this case. In the second case, on the blinker, which results in an acceleration of the following vehicle, not allowing the cut-in to be completed, and leads to a dangerous driving situation. The ego vehicle then starts a second round of prediction under this situation. In the third case, ego vehicle predicts that the following vehicle will not yield if the blinker is on, it does not initiate a cut-in, and slows down to shoot for a gap behind the following vehicle. It then starts a second round of prediction, which can potentially be easier and less costly compared with the ﬁrst round of prediction. By properly specifying the models and the loss function, a yield predictor designed under the framework of dynamic inference should enable the ego vehicle to drive in the ﬁrst and the third case most of the time according to diﬀerent driving situations, and avoid the behavior as in the second situation, unless it is deliberately designed to support aggressive cut-in. bY=not yield, depending on the planner,Xcould be either larger if the planner decides to To better illustrate the idea, three typical cases that can be encountered by the yield prediction Figure 6: Three typical cases of interactive vehicle behaviors in yield prediction. Solving the dynamic inference problem requires the knowledge of two important elements: the observation-transition model and the quantity-generation model. However, in many practically interesting situations, we may not have such knowledge. Instead, we either have a training dataset from which we can learn these models oﬄine before doing inference, or we can learn them on-the-ﬂy during inference if learning for dynamic inference, either oﬄine or online. These problems can also be studied under a Bayesian framework, where the unknown models are assumed to be members of parametrized model families with certain priors, and the optimal learning rule that minimizes the expected inference loss can be mathematically derived [4]. as a meta problem for machine learning, such that almost all familiar learning problems can be cast as its special cases, examples including supervised learning, imitation learning, and reinforcement learning. For instance, the oﬄine learning for dynamic inference can be viewed as an extension of the behavior cloning method in imitation learning [5 action-generation model and state-transition model, but simultaneously learns a policy based on the learned models to minimize the overall contextual-aware imitation error. As another instance, any loss function of the form contextual loss function quantity this view, any reinforcement learning problem [9] can be solved as an instance of online learning for dynamic inference, where the quantities to be estimated are the latent variables of the loss function, and the quantity-generation model to be learned is the conditional distribution of the latent variable. More detailed discussions on the connection between learning for dynamic inference and other learning problems are made in [4]. The study of dynamic inference and its learning extension thus help us gain deeper and unifying understandings of a broad spectrum of machine learning problems. The formulation of dynamic inference appears to be new, but it can be viewed from diﬀerent angles, and is related to a variety of existing problems. Sequential interactive prediction as dynamic inference naturally formulated as dynamic inference are estimation or prediction problems in sequential and interactive settings. Traditionally some of those problems are formulated and studied using game theory [10]. This type of problems become more common in recent years with widespread adoption of AI systems, e.g., they arise in behavior prediction of vehicles [11 with user feedback [14], and prediction in ﬁnance [15]. Dynamic inference provides a rigorous mathematical formulation of such problems, and provides an optimal solution to it. Dynamic inference as performative prediction with a recent trend of research called performativity [16 the tendency that the decision to make in optimization or prediction problems can change the underlying distribution the decision is made for. A method called repeated risk minimization is proposed to solve the performative prediction, either with [18] or without [17] state transitions. The goal there is to minimize the loss in each single round of prediction, based on the distribution from Perhaps more importantly, the problem of learning for dynamic inference can potentially serve Ythat depends onXcan then be viewed as a latent variable of the loss function. With the last round, and the hope is that such a method can reach a minimax equilibrium under certain conditions. On the contrary, dynamic inference aims to minimize the overall inference loss, and it explicitly considers multiple rounds of estimation and state transitions in these problems. Dynamic inference as imitation game viewed as a game of imitation, where the learner drives a system with state, observes an action from a demonstrator at each encountered state, and tries to imitate the demonstrator’s actions by minimizing the accumulated state-aware imitation error. When the underlying models are unknown, such a view can provide a rigorous formulation of imitation learning, both online and oﬄine, and the optimal learning strategy is derived in [4]. In practice, this formulation has already been implicitly adopted by practitioners in imitation learning [19]. MDP as dynamic inference reformulating it to an MDP which can be solved by dynamic programming. Conversely, any MDP can be thought of as a dynamic inference problem, by viewing the loss function in an integral form that involves an unknown latent variable, as discussed in the previous section. The goal of MDP is then to estimate the latent variables by minimizing the overall estimation error. This view also helps us to understand reinforcement learning, especially Bayesian reinforcement learning in the model-based form [20 of viewing MDP and reinforcement learning in this way would be an interesting research problem. The author is indebted to Peng Guan for many helpful discussions; the discussion with whom on imitation learning in early 2018 motivated this study. The author is grateful to Prof. Lav Varshney, for the detailed comments and many helpful suggestions, and for pointing out [17] on performative prediction. The author also would like to thank Prof. Maxim Raginsky, for his encouragement in looking into dynamic aspects of statistical problems.