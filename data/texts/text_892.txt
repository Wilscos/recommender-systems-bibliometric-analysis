Session-based recommendation (SBR) is a challenging task, which aims at recommending next items based on anonymous interaction sequences. Despite the superior performance of existing methods for SBR, there are still several limitations: (i) Almost all existing works concentrate on single interest extraction and fail to disentangle multiple interests of user, which easily results in suboptimal representations for SBR. (ii) Furthermore, previous methods also ignore the multi-form temporal information, which is signicant signal to obtain current intention for SBR. To address the limitations mentioned above, we propose a novel method, called Temporal aware Multi-Interest Graph Neural Network (TMI-GNN) to disentangle multi-interest and yield rened intention representations with the injection of two level temporal information. Specically, by appending multiple interest nodes, we construct a multi-interest graph for current session, and adopt the GNNs to model the itemitem relation to capture adjacent item transitions, item-interest relation to disentangle the multi-interests, and interest-item relation to rene the item representation. Meanwhile, we incorporate item-level time interval signals to guide the item information propagation, and interest-level time distribution information to assist the scattering of interest information. Experiments on three benchmark datasets demonstrate that TMI-GNN outperforms other state-ofthe-art methods consistently. • Information systems → Recommender systems. ACM Reference Format: Qi Shen, Shixuan Zhu, Yitong Pang, Yiming Zhang, and Zhihua Wei. 2018. Temporal aware Multi-Interest Graph Neural Network For Session-based Recommendation . In WXXXX, June 03–05, 2021, XXX, XX. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/1122445.1122456 Recommender system has become the basis to relieve the information overload problem. Most recommendation methods capture users’ interest by modeling users’ long-term preference for predicting their future interactions, e.g., collaborative ltering [6,7] and neural network based models [21,30]. Dier from traditional recommendation tasks, in many practical scenarios, there is only a session available without access to user identication and historical interactions. This kind of task called session-based recommendation (SBR), aims to extract useful information as much as possible from limited data in current session. Existing SBR methods mostly concentrate on modeling sequential information among items of current session by using Recurrent Neural Networks (RNNs) [8,10] or Graph Neural Networks (GNNs) [16,27]. However, these works simply regard a session as a short sequence with single intention, consider that the basic intention of user in a session usually remains the same, and try to capture user’s current interest directly from the entire session. They overlook the fact that even in a relatively short-term session, the user’s ner granular interests can be multiple in various views, drift over time, or interweave with item overlaps. Some studies [1,4,9,14,24,31] in other recommendation areas have veried the eectiveness of modeling user’s multi-interest, but in SBR, the multi-interest methods have not been fully explored. As there is no interest disentangle mechanism in existing SBR methods, the mixture of major interests and minor interests may mislead the session representation learning so that it’s hard to conrm users’ true intentions and lacks of interpretability. Moreover, the auxiliary temporal information is also ignored in session modeling. Intuitively, two typical temporal information of interaction sequences can be recognized for multi-interest modeling: (i) item-level time inter val signals are the intervals between item transitions, which reect the relatedness between adjacent items. Generally, the tight time interval indicates the higher relevance of items, while loose time interval may mean the drifting of interest. (ii) interest-level distribution information represents the interest distribution through virtual location and coverage scope in timeline, Figure 1: A toy example for two sessions with the same item sequence but dierent time intervals, where the circles connected to items denote the user’s interests. Positions of the circles reected the distribution of interest in the timeline, and the size indicates the importance of these interests. which is helpful to model the relation between item and interest from the time perspective. For the chunked interest distribution in the session, the closer distance between items and interest factors in the timeline usually reveals the higher relatedness of them. We illustrate this with an example in Figure 1, including two sessions with the same item sequence under dierent interaction time distributions. For the rst session with short time intervals, as user’s intention tends to be continuous over a short period of time, we can conclude that even item relatively far from current position may still represent user‘s strong interest. So user may mostly be interested inPhoneproduct andApplebrand. Under this circumstance, previous methods without introducing auxiliary time information will regard it as a session with evenly and moderate length time intervals. Therefore, a deviation from the major interests may be caused by the latest items, which are laptops. While for the second session with a relatively long time interval between iPhone and Macbook, we argue that the interest drift may occurred during the interval. Thus the interests distributed in the front portion of timeline such asPhone, ought to be a minor factor for recommendation, while the interestLaptopin the latter portion should be taken into special consideration. But for SBR methods ignoring time information, the multiple interests in two chunks of the session cannot be eectively disentangled and rearranged. The minor interestPhoneis likely to be over weighted for user’s current intention, while the crucial interestLaptopmay be diluted. As discussed above, even sessions with exactly the same items and orders, but for diverse time distributions, may have diverse interests intensity, leading to completely dierent next items. In this case, the ignorance of time information makes existing methods insucient to distill eective intention signals from the active session. To address these problems, we propose Temporal aware MultiInterest Graph Neural Network (TMI-GNN), a novel method to distill the disentangled interest and inject the temporal information for better inferring the user intentions of the current session. To be more specic, we rstly construct a multi-interest graph for current session by appending multiple interest nodes into original itemitem graphs, which builds the potential relations between items and dierent interest factors. Then, to generate the item and interest representations, we synchronously apply item information propagation with item-level time interval signals, interest extraction in a soft clustering way, and interest attaching with the interest-level distribution information. Finally, we integrate item embeddings and interest embedding under the guide of temporal distance, to represent user’s preference for next item prediction. Our main contributions of this work are summarized below: •We propose to construct a multi-interest graph with item and interest nodes for representing multi-interest session eectively, involving the explicit item transitions and potential connections between dierent items and interests. •For the constructed graph, we develop a novelTMI-GNNmodel for SBR to capture adjacent item transitions, distill users’ current multi-interests from noisy interaction sequences and feedback related interest information to enhance session representations. Moreover, it explicitly injects item-level and interest-level temporal information into the above process to rene current intention representation. •Extensive experiments on three datasets demonstrate that our model is superior compared with state-of-the-art models. Session-based Recommendation.Following the development of deep learning, many neural network based approaches have been proposed for SBR. Due to well sequence modeling capability of RNNs, RNN-based methods have been widely used for SBR [8,10,13,19,22]. For instance, GRU4Rec [8] was rstly proposed to utilize GRU layer to capture information in interaction sequences. Based on GRU4Rec, NARM [10] added an attention mechanism after RNN, which refers to last interaction item, in order to capture the global and local preference representation of user in current session. But in an ongoing session, interaction patterns are always more complex than simple sequential signals, which cannot be eectively captured by RNN-based models. More recently, motivated by the superior performance of GNNs in extracting complex relationships between objects, quite a few recommendation methods relying on GNNs were proposed to extract the item transition patterns for SBR [2,3,16–18,25,27,28]. For example, SRGNN [27] converted the interaction sequence into a directed graph and employed the gated GNN (GGNN) on the graph to learn item embedding. LESSR [3] formed better graph structure from the session by proposing a lossless encoding scheme, and proposed a SGAT layer to model the long-range dependency, which propagates information along shortcut graph. StarGNN [16] put forward a star graph neural network to model the complex transitions between items with additional node to connect the long-range item relations. DATMDI [2] learned the cross-session and local session representations via the GNN and GRU and combined them by a soft-attention mechanism. Temporal information in Recommendation.Meanwhile, temporal information is also a key feature in many recommendation scenarios, such as e-commerce and video recommendation. There are a few works to utilize the temporal information as contextual feature for recommendation [5,11,23,26]. However, most existing methods simply reduce the temporal information to order relationship, subsequently use RNN-based model to capture the sequential signal. For instance, [30] used RNN to learn a dynamic representation of a user which reveal user’s dynamic interests at dierent time in next basket recommendation. [29] discovered absolute time patterns and relative time patterns based on insightful data analysis to model users’ temporal behaviors for recommender systems. [32] proposed a framework called RIB, which takes dwell time into account.By using the time a user spends on one item as a part of so called micro behavior, it can model more practical user intent. As outlined above, previous works on SBR have some limitations. First, temporal information is rarely or crudely exploited in these works. What’s more, none of these methods explicitly disentangle the multiple interests of user in a session. These limitations may lead to suboptimal performance. In this work, we aim to explore the eectiveness of abundant temporal information and multiple disentangled interest for SBR. Given the entire item setV, we rst dene a timestamp-augmented item session sequence as𝑠 = {(𝑣, 𝑡), (𝑣, 𝑡), ..., (𝑣, 𝑡)}, where(𝑣, 𝑡) represents the user interacted item𝑣∈ Vat time𝑡and𝑡< 𝑡, and𝐿is the session length. Given session𝑠, the goal of SBR is to predict a probability score for any item𝑣 ∈ Vsuch that an item with higher score is more likely to be interacted next. In this section, we detail the design of our model. As shown in Figure 2, it contains four main components: Multi-interest Session Graph Construction, Disentangle Graph Modeling Layer, Session Representation Learning Layer and Prediction Layer. By constructing item-transition sequences as multi-interest graphs with additional interest nodes, we explicitly build the potential connection between latent user preference and explicit items sequence. Furthermore, at the Disentangle Graph Modeling Layer, we extract item transition information, distill multi-interest representations and feedback disentangled interests to related item embeddings, respectively. With the guidance of item-level and interest-level temporal information, the rened attention is better estimated for disentangling user’s multi-interest. Then, Session Representation Learning Layer adaptively generates the nal intention representations with the injection of last-item based time interval information. Finally, the predictor estimates the probability of candidate items based on the each disentangled session representations. As mentioned above, the extraction of multi-interest is meaningful for obtaining user intention representation from the interaction sequence. Therefore, we propose multi-interest session graph for eectively organizing the interactions in the session and modeling user’s multi-interest. For given session𝑠, we construct the multi-interest session graph G=(V, E),whereV=({𝑣, 𝑣, 𝑣, . . . , 𝑣}, {𝑢, 𝑢, . . . , 𝑢}) indicates the node set of the constructed graph which contains 𝑁item nodes and𝐻interest nodes. Besides the basic transition between items in the session, we additionally introduce theinterest nodesto represent each independent interest, which can be explained as dierent distributions of items’ contribution to user’s intention. For𝑗-th interest node, it fully connects to all items in the session with edges{(𝑢, 𝑣)|1≤ 𝑖 ≤ 𝑁 }. For each session, each item node𝑣has corresponding item-interest edges{(𝑣, 𝑢)|1≤ 𝑗 ≤ 𝐻 } to each interest node, and item-item transition edge to contextual item nodes. Through the full connection between the explicit item and interest node, the soft assignment of each item to corresponding interest can be estimated by subsequent GNN as edge attribute. 4.2.1 Temporal Information. Furthermore, we attach auxiliary multiform temporal information to the multi-interest graph for distilling more precise interest representations. For the original timestamp sequence(𝑡, 𝑡, . . . , 𝑡)of session𝑠, item-level transition interval 𝑡is attached to the edge of item 𝑖 and 𝑗: where𝑡denotes the pre-dened length of time bucket. Moreover, the relative time-step𝑡is attached to the interaction position 𝑖, compared to the start time at the rst position 1. For the constructed multi-interest session graph, there are three types of relation, i.e., item-item, item-interest and interest-item relation, and we represent them by superscript𝑣 −→ 𝑣,𝑣 −→ 𝑢and 𝑢 −→ 𝑣 respectively. Next, we present how to obtain item representations and disentangle interest representations on the constructed heterogeneous multi-interest graph. Based on the message propagation of GNNs, the item and interest node embeddings are updated based on the previous results with neighbor information. The disentanglement of multi-interest can be consider as the process of iteratively rening interest node embedding, and the explainable assignments for each interest factor can be estimated as the weight of item-interest edges based on the full connection of each independent interest node and the item nodes, i.e., each interest factor is generated by item information pooling with dierent assignment scores. For the adjacent item transition, previous GNN-based model, like SRGNN [27], has achieved superior performance for SBR. Therefore, for each GNN layer, we rstly aggregate the neighbor messages in each relation respectively. Then we gather the semantic information to update the item and interest representations. Letu, vdenote the embedding of interest𝑢and item𝑣 after𝑘layers GNN propagation. The item IDs are embedded into 𝑑-dimensional space and are used as initial node features in our model,v∈ R. For the multiple temporal information, we embed them by a learnable temporal matrixT = [T, T, T, . . . , T]for the rounded time value 0≤ 𝑡 ≤ 𝑚, where𝑚is the max time-step intervals. Moreover, for each interest factor as related interest node, we adopt average operation on the item nodes to initialize the inter-Í est representation, i.e.,u=v. For the interest-side temporal information, we utilize the the center timestamp𝑡 to indicate the location of𝑖-th interest factor on the timeline, and the temporal compactness value𝑡to indicate the coverage of interest factors in the item sequence. Here we initialize these two characteristics of each interest factor with average pooing, i.e.,ÍÍ Figure 2: The overview of TMI-GNN. The multi-interest graph contains three typ es of relations: item to item, item to interest and interest to item.The Disentangled Graph Modeling layer learns the interest and item embeddings under the guide of rich temporal information. Finally, we aggregate the interest and item representations to generate the nal session embedding. Moreover, an additional interest independent loss is considered to encourage the diversity of interests. Then, we will detail the modeling for item-item, item-interest and interest-item relations, respectively. 4.3.1 Item-level Information Propagation Layer. For the item-item relation, we adopt SRGNN [27] with detail time interval information to propagate adjacent node information. It assembles neighbor node information with the normalized coecient𝑒under the guide of time interval signal 𝑡: whereMLPrepresents a simple multilayer perceptron for time interval embeddingT, and shares with dierent GNN layers. Similar to GGNN, we feed the neighbor informationmand the previous layer itemvinto GRU to update the item-item relation node representations: where the GRU unit is parameters-shared for all item nodes updating at current layer. With the combination of contextual interaction items representations and previous cross-semantic item presentations, we integrate the chronological item transition information into node embedding in the item-item relation. 4.3.2 Interest Extraction Layer. For the item-interest relation at layer𝑘, a graph attention neural network is utilized for updating interest node embedding, as the process of distilling interest representation. In particular, we compute the corresponding correlation weight𝛼between target interest node𝑢and neighbor item 𝑣, as the explainable assignment scores for disentangled interest. And interest node representation is updated via the sum pooling as following: 𝛼= softmaxLeakyReLUWu+ Wv, whereW∈ Rrepresents the information transformation matrix of neighbor nodes,W, W∈ Rare the parameters of linear transformation for the target interest and the source item, respectively. Meanwhile, we also update the center timestamp and temporal compactness of each interest node based on the normalized correlation coecient: Through the assignment score𝛼, we not only distill the latent representations of multi-view interests from the sessions with overlapped and interwove interests, but also oer an explanations parameter for each interest factor. 4.3.3 Interest Aaching Layer. To rene the item representation via the disentangled interest, we adopt a graph attention network to update the item representation under the interest-item semantic. At rst, we estimate the attention coecient between target item and source interest node. Here, we consider both the similarity and temporal continuity of each item and interest node pair: whereW, W∈ Rare the transforming matrices for the item and interest.𝑡=denotes the distance between item and interest in timeline, andw∈ R, 𝑏are the linear transformation of it. Here, we utilize the residual between the center timestep of interest and timestep of items regularized by the interest compactness value, to measure the similarity of interest and item pair in the temporal view. Then, gathered interest factors are scattered to item nodes for generating the intention-augmented item representations via parameters W∈ R: The edges between interests and items indirectly connect each item to all other items through the intermediate interest node. Compare to the original sparse item-item graph, the additional interest nodes and edges help GNNs to eectively capture long-range dependencies in sessions with any length because it propagates information along intermediate interest paths within two-hop. 4.3.4 Layer Combination. By updating the node representation based on diverse semantic relations synchronously, we obtain the item representations from adjacent items and related interest, and the interest embeddings based on item-level soft clustering. Then, we gather cross-semantic information through average operation like RGCN [20], i.e.,v= 𝜎 (v+v)/2,u= 𝜎 (u), and considervanduas the output of item node 𝑣and interest node 𝑢after 𝑘-th GNN layers. Moreover, in order to mine deeper items transition relations, multi-layers of GNN are stacked to propagate high-order information. Moreover, we utilize the gated mechanism to balance the item node representations between initial embedding and𝐾layers output, as follows:hi where∥is the concatenate operation,W∈ Rand sigmoid function𝜎 (·)generate the balance factor𝑔to alleviate over-smooth problems of deep GNN. As for the interest node, we simply adopt the multi-layers’ output as the nal latent interest representation, i.e., u= u. After the Disentangle Graph Modeling, we obtain the nal item and interest node embeddings. Then we aggregate the representation of items in the session based on each abstract interest factor to generate the session representations. Moreover, the items clicked later in the session usually reveal the user’s current intentions better, which draws greater attention for SBR. Therefore, we incorporate additional last-item based time interval information into interestbased attention to capture user activate intention dynamically. In detail, inspired by the reverse position embedding in GCEGNN[25], we integrated the last-item based time interval information with the obtained item representations, which extends the position information to a more ne-grained time domain to make a better prediction: whereW∈ Randb∈ Rare trainable parameters,Tis the embedding for last-time based interval 𝑡. Then, the intentions of user are learnt by a shared attention mechanism, which dynamically weights item representation based on each interest u: where𝜎is an activate function,W, W∈ Randq , b ∈ R are trainable parameters. Then we combine theℎ-th coarse-level intention and rened item-augment representations to generate the session representation for each interest: where W∈ Ris the trainable parameter. With the incorporation of auxiliary last-item based interval information, we capture session representations involved in both the disentangled interest factors and interaction session items in relative chronological temporal-aware patterns. Based on each disentangled interest representationSlearnt above and the normalized initial embeddingsvof candidate items, we then estimate the interaction probabilityˆyof candidate items for current session: whereˆ𝑦∈ˆydenotes the probability that the user will click on item 𝑣in the current session, and𝐻is the pre-dened parameter of the interest node number. As mentioned above, exible number of interest nodes encourages the chunked interest representations conditioned on dierent behavior patterns. However, the dierence constraint drove by multiple interest extractions is insucient: there might be redundancy among latent interests representation, which conicts with the target of disentangling multi-view user interest. We hence introduce interest independents loss, which hires distance correlation measures as a regularizer, with the target of encouraging the multiinterest representations to be diverse. We formulate this as follows: where𝑐𝑜𝑠(·)indicates the similarity distance between two innersession interest representation pair. The nal optimization process is to minimize the cross entropy loss function together and the interest-independence loss jointly: L = −ylog(ˆy) + (1 − y)log(1 −ˆy) + 𝜆L,(18) wherey∈ Ris a one-hot vector of ground truth, and𝜆is the coecient controlling interest-independence term. In this section, we conduct experiments on SBR to evaluate the performance of our method compared with other state-of-the-art models. Our purpose is to answer the following research questions: RQ1:How does our model perform compared with state-of-the-art SBR methods?RQ2:How does the temporal information of the sequence aect the recommendation results?RQ3:Is the design of our model reasonable and eective? How do the key modules of TMI-GNN inuence the model performance?RQ4:How do the hyper-parameters aect the eectiveness of our model? Table 1: Statistics of datasets use d in experiments. 5.1.1 Dataset. We conduct extensive experiments on three public datasets: RetailRocket, Yoochoose and Jdata, which are widely used in the SBR research [3,15,25] and can support our work with seconds-level timestamp information. •RetailRocketcontains behavior data and item properties that collected from a real-world e-commerce website. •Yoochooseis a RecSys Challenge dataset, which consists of clickstreams from an E-commerce website. Dierent from [10,13], we use the most recent fractions 1/16 sequences of Yoochoose as the total dataset Yoochoose 1/16. •Jdatarecords the historical interactions from the JD.com. It contains a stream of user actions within two month. We extract the session data with the setting of the duration time threshold 1 hour. To lter poorly informative sessions and items, following [10,27], we rst ltered out all sessions of length≤2 and items appearing less than 5 times in all datasets. Then we applied a data augmentation technique described in [10]. The statistics of all datasets after prepossessing are summarized in Table 1. 5.1.2 Baseline Models. To demonstrate TMI-GNN’s superiority performance, we compare it with several representative competitors, including the state-of-the-art SBR models and several temporalconcerned methods. • GRU4Rec[8] employs the GRU to capture the representation of the item sequence simply. • NARM[10] is a RNN-based model which combines with attention mechanism to generate the session embedding. • RIB[32] is a framework using RNN and attention layer to model user micro behavior including behavior and dwell time. Here we consider the time intervals as the dwell time and ignore the behavior types. • SRGNN[27] converts session sequences into directed unweighted graphs and utilizes a GGNN layer [12] to learn the patterns of item transitions. • LESSR[3] adds shortcut connections between items in the session and considers the sequence information in graph convolution by using GRU. • DATMDI[2] combines the GNN and GRU to learn the crosssession enhanced session representation. Besides, we combine the time interval embdding with ID embedding as input following [32], and inject additional time information into the graph by adding learnable time interval weights like subsubsection 4.3.1 for SRGNN, LESSR and DATMDI method separately, named SRGNN, LESSR, DATMDI. 5.1.3 Evaluation Metrics. To evaluate the recommendation performance, we employ two widely used metrics: Hit ratio (H@𝑘) and Normalized discounted cumulative gain (N@𝑘) following [25,27], where𝑘is 10 or 20. The average results over all test session are reported. 5.1.4 Implementation Details. We implement the proposed model based on Pytorch and DGL. The embedding dimension is set to 128. All parameters are initialized through a Gaussian distribution with a mean of 0 and a standard deviation of 0.1. We employ the Adam optimizer to train the models with the mini-batch size of 512. We conduct the grid search over hyper-parameters as follows: learning rate in{0.001,0.01,0.1}, learning rate decay in factor𝜆in{1,3,10,30}. The maximal time-step𝑚is set to 300, which is large enough for all sessions. To make the comparison fairer, we range the hyper-parameters of baseline methods with the same tuning scopes of our experiments. To demonstrate the overall performance of the proposed model, we compared it with the state-of-the-art methods for SBR. We can obtain the following signicant observations from the comparison results shown in Table 2. Comparison of Dierent Baselines.The NARM performs signicantly better than GRU4Rec, which indicates the eectiveness of attention mechanism to capture the user’s main motivation. In the comparison between RNN-based models and GNN-based models for Yoochoose and Jdata, the GNN-based models generally outperform the RNN-based models, which veried the certain advantage of GNN for SBR. Moreover, we notice that LESSR can outperform SRGNN in most cases. This may be due to the special design in LESSR for capturing long-range dependency between items in a session, which contributes to better performance on some long sessions. Meanwhile, for dataset RetailRocket, the simple model NARM outperforms other complex models because of the insucient training caused by the small amount of data. Signicance of Temporal Information.Then we turn to the temporal information attached methods. The RNN-based model RIB achieves superior performance to GRU4Rec but performs slightly inferior than NARM, because RIB employs a relatively simple attention instead of last-item based attention. Compared to the original models, the temporal-enhanced methods all achieve better performance in some degrees, which indicates the signicance of temporal information in SBR. Moreover, the adapted methods perform better on small dataset RetailRocket, which indicates that the auxiliary temporal information is more helpful for sparse interaction data. Table 2: Experimental results (%) of dierent models in H@{10, 20}, and N@{10, 20} on three datasets. 𝐼𝑚𝑝𝑟𝑜𝑣 . means improvement over the state-of-art methods.The b old number indicates the improvements over the best baseline (underlined) are statistically signicant (𝑝 < 0.01) with paired t-tests. Model Eectiveness.We nd that our model comprehensively outperforms all other baselines substantially on almost all metrics, which justies the eectiveness of our model. The performance improvement can be explained in two aspects. One is that TMI-GNN can disentangle user intention via extracting multi-interests from the multi-interests session graph, so it can portray more profound representation of user interests. This strategy break the limitation of expression ability of only one interest. Another one is that we introduce multi-form temporal information to the process of disentangle graph modeling and session representation learning. Compare with other time-aware models, TMI-GNN models diversied temporal information adequately eectively. Table 3: The performance comparison w.r.t dierent temporal information and module design. YoochooseH@20 66.32 66.25 66.28 66.37 65.86 66.35 66.46N@20 37.54 37.63 37.53 37.79 37.23 37.71 38.05 JdataH@20 44.89 44.75 44.82 45.14 44.25 45.21 45.38N@20 21.98 21.94 21.97 22.02 21.80 22.03 22.18 Impact of temporal information.In this part, we compare our model with partially temporal information masked versions in Table 3 to test whether considering the multi-form temporal information can boost model performance. The method with "-V2V" means skipping the time intervals in item-level message propagation, "U2V" indicates ignoring the temporal factors in interest-item relation, "-Last" represents removing the last-item based time-interval signals, and "First" means utilizing the rst-item based time-interval embedding in session representation learning module. By comparing methods mentioned above, we nd that the loss of any type of temporal information will cause the decline of model performance. Besides, compare to "First" time embedding, the last-item based time-interval information is more helpful for SBR. Moreover, Figure 3: Performance comparison w.r.t. dierent depths of GNN. the loss of interest-item temporal continuity factors is more signicant in our model compared with other temporal information types. Based on the above illustrations, we demonstrate that injecting multi-form temporal information in our framework is indeed meaningful. Impact of dierent Designs.In this part, we compare our method with dierent variants to verify the eectiveness of the critical components of TMI-GNN. Specically, we remove the additional interest node in session graph (denote as ”-Interest”), and mask the interest independent loss (denote as ”-Loss”), respectively. The experimental results are presented in Table 3. It can be observed that the abstract interest nodes is pivotal for the model capability of intention capturing. Meanwhile, for the diverse intention modeling, the removal of interest independent loss leads to great impact on model results, which demonstrates that the forced cross-interest separation is helpful for disentangling multiple interest of user. In summary, we can infer that the key components of TMI-GNN are eective through the comparison and analysis above. Figure 4: Performance comparison w.r.t. dierent time bucket width and max interest no de number. Impact of GNN depth.To study the impact of depth of GNN, we conducted experiments under dierent GNN depth settings (from 1 to 5). Figure 3 shows the corresponding results. We can observe that as the depth goes up from 1 to 3, the performance of our model increases on both datasets, which claries the signicance of using multi-layer GNN to distill user’s multi-interests. Moreover, the performance declines as the depth grows from 3 to 5 on Yoochoose, showing that excessive depth of GNN may lead to over-smoothing problem and less distinguishable item representation. As a comparison, with the number of GNN layers increasing, the performance of SRGNN drops on both two datasets, while our model’s performance degrades slowly on Yoochoose, even stay a slightly increase on larger dataset Jdata. This indicates that our model is more efcient in handling the over-smoothing problem of GNN through the recombination of item representation, thus achieves more rened modeling of user interest through stacking more GNN layers without performance degradation. Impact of time bucket widths.As discussed in section 4, we divide the time interval into buckets to utilize time signals. So we conduct tests by ranging the time bucket width within{2,4,8,16,32} to explore how does the bucketing setting aect the model’s performance. As shown in gure 4, we can see that the performance does not uctuate dramatically as the time bucket width changes, which indicates that our model is not sensitive to the time bucket width. Impact of interest no de num.To investigate the impact of the interest node number, we range this parameter in{1,2,3,4}. According to the results in Figure 4, we can see that for both Yoochoose and Jdata, the model with 2 interest nodes achieves the best performance in most metrics. Compare to single interest node with mixed interest representation, our model with two interest nodes disentangles the latent interest representation for better next item prediction. When the number becomes larger, performance will drop due to the redundancy of interest representation. In this paper, we pay special attention to the disentangled multiinterest representations of user and multiple temporal information for session based recommendation. We construct a multi-interest graph and devise the TMI-GNN model, which utilizes the multiinterest graph to capture adjacent item transitions, distill multiinterest representations with the injection of multi-form temporal information. In the experiments, our model outperforms other stateof-the-art session-based models, showing the eectiveness of our model.