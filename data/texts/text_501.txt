Symmetric Positive Deﬁnite (SPD) matrices have been applied in many tasks in computer vision such as pedestrian detection [ and image set classiﬁcation [ others. They have been used to capture statistical notions (Gaussian distributions [ [78]), while respecting the Riemannian geometry of the underlying SPD manifold, which offers a convenient trade-off between structural richness and computational tractability [ has applied approximation methods that locally ﬂatten the manifold by projecting it to its tangent space [22, 83], or by embedding the manifold into higher dimensional Hilbert spaces [35, 90]. These methods face problems such as distortion of the geometrical structure of the manifold and other known concerns with regard to high-dimensional spaces [ several distances on SPD manifolds have been proposed, such as the Afﬁne Invariant metric [ Stein metric [ respective geometric properties. However, the representational power of SPD is not fully exploited in many cases [ domain given the lack of closed-form expressions. There has been a growing need to generalize basic operations, such as addition, rotation, reﬂection or scalar multiplication, to their Riemannian geometric counterparts to leverage this structure in the context of Geometric Deep Learning [19]. SPD manifolds have a rich geometry that contains both Euclidean and hyperbolic subspaces. Thus, embeddings into SPD manifolds are beneﬁcial, since they can accommodate hierarchical structure in data sets in the hyperbolic subspaces while at the same time represent Euclidean features. This makes them more versatile than using only hyperbolic or Euclidean spaces, and in fact, their different submanifold geometries can be used to identify and disentangle such substructures in graphs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). We propose the use of the vector-valued distance to compute distances and extract geometric information from the manifold of symmetric positive deﬁnite matrices (SPD), and develop gyrovector calculus, constructing analogs of vector space operations in this curved space. We implement these operations and showcase their versatility in the tasks of knowledge graph completion, item recommendation, and question answering. In experiments, the SPD models outperform their equivalents in Euclidean and hyperbolic space. The vector-valued distance allows us to visualize embeddings, showing that the models learn to disentangle representations of positive samples from negative ones. 71], the Bures–Wasserstein metric [13] or the Log-Euclidean metric [5,6], with their 6,65]. At the same time, it is hard to translate operations into their non-Euclidean In this work, we introduce the vector-valued distance function to exploit the full representation power of SPD (§3.1). While in Euclidean or hyperbolic space the relative position between two points is completely captured by their distance (and this is the only invariant), in SPD this invariant is a vector, encoding much more information than a single scalar. This vector reﬂects the higher expressivity of SPD due to its richer geometry encompassing Euclidean as well as hyperbolic spaces. We develop algorithms using the vector-valued distance and showcase two main advantages: its versatility to implement universal models, and its use in explaining and visualizing what the model has learned. Furthermore, we bridge the gap between Euclidean and SPD geometry by developing gyrocalculus in SPD (§4), which yields closed-form expressions of arithmetic operations, such as addition, scalar multiplication and matrix scaling. This provides means to translate previously implemented ideas in different metric spaces to their analog notions in SPD. These arithmetic operations are also useful to adapt neural network architectures to SPD manifolds. We showcase this on knowledge graph completion, item recommendation, and question answering. In the experiments, the proposed SPD models outperform their equivalents in Euclidean and hyperbolic space (§6). These results reﬂect the superior expressivity of SPD, and show the versatility of the approach and ease of integration with downstream tasks. The vector-valued distance allows us to develop a new tool for the analysis of the structural properties of the learned representations. With this tool, we visualize high-dimensional SPD embeddings, providing better explainability on what the models learn (§6.4). We show that the knowledge graph models are capable of disentangling and clustering positive triples from negative ones. Symmetric positive deﬁnite matrices are not new in the Machine Learning literature. They have been used in a plethora of applications [ not always respecting the intrinsic structure or the positive deﬁniteness constraint [ The alternative has been to map manifold points onto a tangent space and employ Euclidean-based tools. Unfortunately, this mapping distorts the metric structure in regions far from the origin of the tangent space affecting the performance [44, 96]. Previous work has proposed alternatives to the basic neural building blocks respecting the geometry of the space. For example, transformation layers [ on SPDs [ [24], projections onto Euclidean spaces [ line, providing explicit formulas for translating Euclidean arithmetic notions into SPDs. Our general view, using the vector-valued distance function, allows us to treat Riemannian and Finsler metrics on SPD in a uniﬁed framework. Finsler metrics have previously been applied in compressed sensing [ [67]. With regard to optimization, matrix backpropagation techniques have been explored [ with some of them accounting for different Riemannian geometries [ for tangent space optimization [ logarithmic map. The space non-positive curvature of sions. Points in symmetric trixIbeing a natural basepoint. The tangent space to any point of with the vector space n × nmatrices. Euclidean subspaces, perbolic subspaces as well as products of hyperbolic planes (see Helgason [39] for an in-depth introduction). 94] and Riemannian means [23], or appended after the convolution [21], recurrent models 30], information geometry [69], for clustering categorical distributions [63], and in robotics SPDare positive deﬁnite real n ×nmatrices, with the identity ma- Figure 2: Some isometries of rotation (center) and reﬂection (right). In Figure 1 we visualize the smallest nontrivial example. R, cut out by requiring both eigenvalues of the matrix geometry of the hyperbolic plane times a line. Exponential and logarithmic maps: phism which links the Euclidean geometry of the tangent space Its inverse is the logarithm map, allows one to freely move between ’tangent space coordinates’ or the original ’manifold coordinates’. We apply both maps based at points on choice of basepoint best choice from a computational point of view. We prove this in Appendix C.3. Symmetries: any invertible matrix many geometric transformations as opposed to custom-built procedures. See Appendix C.2. for a brief review of these symmetries. Among these, we may ﬁnd When also Euclidean translation, ﬁxing no points of P 7→ MP M think of elements ﬁxing the basepoint as being M is a familiar rotation or reﬂection (see Figure 2). The Euclidean symmetry of reﬂecting in a point also has a natural generalization to reﬂection in the origin is given by matrix inversion this by an SPD In Euclidean or hyperbolic spaces, the relative position between two points is completely determined by their distance, which is given by a scalar. For the space SPD The VVD vector: function d whereλ vector is an invariant of the relative position of two points up to isometry. This means that in only if the VVD between two points is alsov, then there exists an isometry mapping the relative position of two points in obtained by using the standard [46] §2.6 and Appendix C.4 for a review of VVDs in symmetric spaces. SPDare equivalent, and we could obtain a different concrete expression for any other The prototypical symmetries ofSPDare parameterized by elements ofGL(n; R): Mis an element ofSPD, the symmetryP 7→ MP Mis a generalization of an is conjugation byM, and thus ﬁxes the basepointI = MIM= MM= I. We P 7→ P. The generalSPD-reﬂection in a pointQ ∈ SPDis a conjugate of translation, given by P 7→ QPQ. : SPD×SPD→ R. For two points P, Q ∈ SPD, the VVD is deﬁned as: (PQ) ≥ . . . ≥ λ(PQ)are the eigenvalues ofPQsorted in descending order. This Finsler metrics: Rthat is invariant under permutation of the entries induces a metric on SPDdo not only support a Riemannian metric, but also Finsler metrics, a whole family of distances with the same symmetry group (group of isometries). These metrics are of special importance since distance minimizing geodesics are not necessarily unique in Finsler geometry. Two different paths can have the same minimal length. This is particularly valuable when embedding graphs in metricsF Planche some Finsler metrics on representation learning. Advantages of VVD: advantages. First, a single model can be run with different metrics, according to the chosen norm. The VVD contains the full information of the Riemannian distance and of all invariant Finsler distances, hence we can easily recover the Riemannian metric and extend the approach to many other alternatives (in Appendix C.7, we detail how the VVD generalizes other VVD provides much more information than just the distance, and can be used to analyze the learned representation in one can immediately read the regularity of the unique geodesics joining these two points. Geodesics that contain the geodesic. The Riemannian or Finsler distances cannot distinguish the differences between these geodesics of different regularity, but the VVD function can. Third, the VVD function can be leveraged as a tool to visualize and analyze the learned high-dimensional representations (see §6.4). To build an analog of many Euclidean operators in internal to Euclidean geometry, chief among these being the vector space operations of addition and scalar multiplication. By means of tools introduced in pure mathematics literature a gyro-vector space structure on these vector space operations, extending successful applications of this framework in geometric deep learning to hyperbolic space [ where one may attempt to replace analogous operations care, as gyro-addition is neither commutative nor associative. See Appendix D for a review of the underlying general theory of gyrogroups and additional guidelines for accurate formula translation. Addition and Subtraction: deﬁne the gyroaddition of the isometry which translates the gyroaddition of hyperbolic space exploited by [ Vermeer [84] (see Figure 4). FixingP ∈ SPD SPD-translation moving the basepoint to point with respect to this operation must then be given by its geodesic reﬂection in I. See [1, 37] for an abstract treatment of gyrocalculus, and [47] for the speciﬁc examples discussed here. SPD. Moreover, SPD, since in graphs there are generally several shortest paths. We obtain the Finsler orFby taking the respective`or`norms of the VVD inR(see Figure 3). See [66]and Appendix C.6 for a review of the theory of Finsler metrics, [12] for the study of SPD, independent of the choice of the metric. Out of the VVD between two points, have different regularity, which is related with the number of maximal Euclidean subspaces Figure 4: Gyro-addition (left), gyro-scalar multiplication (center) and matrix scaling (right). As this operation encodes a symmetry of purely in the gyrovector formalism. In particular, the vector-valued distance computed as the logarithm of the eigenvalues of P ⊕ Q (see Appendix C.5). Scalar Multiplication and Matrix Scaling: tion of a point in the direction of of the vector-space scalar multiplication on the tangent space to SPD whereexp, log multiplication to allow for different relative expansion rates in different directions. For a ﬁxed basepoint symmetric matrix A ∈ S whereA  X previous usage: for any In this section we detail how we learn representations in mappings so that they conform to the premises of each operator, yielding SPD neural layers. Embeddings in SPD we exploit the connection between logarithmic maps. To learn a point impose symmetry on thatU = X + X Modeling points on the tangent space offers advantages for optimization, explained in §5. For the matrix scaling learn the symmetric matrix U. Isometries: Rotations and Reﬂections: collections of pairwise orthogonal ﬁciently build elements of More precisely, for any note the 2-dimensional rotation ( Then for any pair the transformation which applies ofR, and leaves all other coordinates ﬁxed. For example, inO(5)the element transformation where we replace the entries ofIwith the corresponding values of eral, rotations and reﬂections are built by taking products of these basic transformations. Given a P ∈ SPDby a scalarα ∈ Rto be the point which lies at distanceαd(I, P )fromI are the matrix exponential and logarithm. We further generalize the notion of scalar Iand a pointP ∈ SPD, we can replace the scalarαfrom Eq. 3 with an arbitrary real denotes the Hadamard product. We denote the matrix scaling with⊗, extending the A ⊗ P, we impose symmetry on the factor matrixA ∈ Sin the same way that we and a choice of sign, we deﬁne the rotation and reﬂection corresponding to whereRot( as a learnable parameter of the model. Finally, we denote the application of the transformation the point P ∈ SPD Optimization: ~θ ∈ R have to be optimized respecting the geometry of the manifold, but as already explained, we model them on the space of symmetric matrices we are able to perform tangent space optimization [ circumvent the need for Riemannian optimization [ stable. Due to the geometry of incur losses in representational power. Complexity: calculation, thus we analyze its complexity. In Appendix A.1 we detail the complexity of different operations. Calculating the distance between two points in inversions and diagonalizations of with respect to the matrix dimensions is dimensions thus a large instead of linear. Towards neural network architectures: vector operations as building blocks for SPD neural layers. This is showcased in the experiments presented below. Scalings, rotations and reﬂections can be seen as feature transformations. Moreover, gyro-addition allows us to deﬁne the equivalent of bias addition. Finally, although we do not employ non-linearities, our approach can be seamlessly integrated with the ReEig layer (adaptation of a ReLU layer for SPD) proposed in [40]. In this section we employ the transformations developed on SPD to build neural models for knowledge graph completion, item recommendation and question answering. Task-speciﬁc models in different geometries have been developed in the three cases, hence we consider them adequate benchmarks for representation learning. Knowledge graphs (KG) represent heterogeneous knowledge in the shape of (head, relation, tail) triples, where head and tail are entities and relation represents a relationships among entities. KG exhibit an intricate and varying structure where entities can be connected by symmetric, antisymmetric, or hierarchical relations. To capture these non-trivial patterns more expressive modelling techniques become necessary [ transformations on SPD manifolds. Given an incomplete KG, the task is to predict which unknown links are valid. Problem formulation: the set of relations and to learn a scoring function with the goal of scoring all missing triples correctly. To do so, we propose to learn representations of entities as embeddings in KG structure is preserved. ~θ), Ref(~θ) ∈ Rare the isometry matrices, and the vector of angles~θcan be regarded For the proposed rotations and reﬂections, the learnable weights are vectors of angles , which do not pose an optimization challenge. On the other hand, embeddings inSPD The most frequently utilized operation when learning embeddings is the distance Scaling model: of the matrix scaling. Its scoring function has shown success in the task given that it combines multiplicative and additive components, which are fundamental to model different properties of KG relations [4]. We translate it into SPD whereH, T ∈ SPD respectively. with the Riemannian and the Finsler One metric distances. Isometric model: of theO(n) metric spaces [ function is deﬁned as: Datasets: WN18RR is a subset of WordNet [ senses. FB15k-237 is a subset of Freebase [14], with 14, 541 entities and 237 relationships. Training: protocol by adding inverse relations to the datasets [48]. We optimize the cross-entropy loss with uniform negative sampling deﬁned in Equation 9, where Tis the set of training triples, and We employ the AdamW optimizer [ wheren ∈ {14, 20, 24} select optimal dimensions, learning rate and weight decay, using the validation set. More details and set of hyperparameters in Appendix B.1. Evaluation metrics: using the scoring function, and use inverse relations for head prediction [ work, we compute two ranking-based metrics: mean reciprocal rank (MRR), which measures the mean of inverse ranks assigned to correct entities, and hits at K (H@K, measures the proportion of correct triples among the top K predicted triples. We follow the standard evaluation protocol of ﬁltering out all true triples in the KG during evaluation [16]. Baselines: are also state-of-the-art models for the task. For the scaling model, these are MURE and MURP [ which perform the scaling operation in Euclidean and hyperbolic space respectively. For the isometric models, we compare to ROTC [ hyperbolic space respectively), and REFE and REFH [ space). Baseline results are taken from the original papers. We do not compare to previous work on SPD given that they lack the deﬁnition of an arithmetic operation in the space, thus a vis-a-vis comparison is not possible. Results: both dataset, the scaling model is specially notable in HR@10 for WN18RR: reﬂections are very effective on WN18RR as well. They outperform their Euclidean and hyperbolic counterparts RefE and RefH, in particular when equipped with the Finsler metric. Rotations on the SPD manifold, on the other hand, seem to be less effective. However, Euclidean and hyperbolic rotations require (equivalent to for rotations and reﬂections does not repeat in the following experiments (§6.2 & §6.3). Hence, we consider this is due to overﬁtting in some particular setups. Although we tried different regularization methods, we regard a sub-optimal conﬁguration rather than a geometric reason to be the cause for the underperformance. Regarding the choice of a distance metric, the Finsler One metric is better suited with respect to HR@3 and HR@10 when using scalings and reﬂections on WN18RR. For the FB15k-237 dataset, SPD models operating with the Riemannian metric outperform their Finsler counterparts. This We follow the base hyperbolic model MuRP [8] and adapt it intoSPDby means R ∈ SPDandMare matrices that depend on the relation. Ford(·, ·), we experiment group (i.e., rotations and reﬂections). This technique has proven effective in different 25,88]. In this case,Mis a rotation or reﬂection matrix as in Eq. 5, and the scoring We employ two standard benchmarks, namely WN18RR [16,28] and FB15k-237 [16,76]. We compare our models with their respective equivalents in different metric spaces, which We report the performance for all analyzed models, segregated by operation, in Table 1. On 500dimensions whereas theSPDmodels are trained on matrices of rank14 105dims). Moreover, the underperformance observed in some of the analyzed cases suggests that the Riemannian metric is capable of disentangling the large number of relationships in this dataset to a better extent. In these experiments we have evaluated models applying equivalent operations and scoring functions in different geometries, thus they can be thought as a vis-a-vis comparison of the metric spaces. We observe that SPD models tie or outperform baselines in most instances. This showcases the improved representation capacity of the SPD manifold when compared to Euclidean and hyperbolic spaces. Moreover, it demonstrates the effectiveness of the proposed metrics and operations in this manifold. Recommender systems (RS) model user preferences to provide personalized recommendations [ KG embedding methods have been widely adopted into the recommendation problem as an effective tool to model side information and enhance the performance [ recommending a movie to a particular user is that the user has already watched many movies from the same genre or director [ entities, the goal is to predict the user’s next item purchase or preference. Model:We model the recommendation problem as a link prediction task over users and items [ In addition, we aim to incorporate side information between users, items and other entities. Hence we apply our KG embedding method from §6.1 as is, to embed this multi-relational graph. We evaluate the capabilities of the approach by only measuring the performance over user-item interactions. Datasets: the Amazon dataset [ users’ purchases of products, and the MindReader dataset [ datasets provide additional relationships between users, items and entities such as product brands, or movie directors and actors. To generate evaluation splits, the penultimate and last item the user has interacted with are withheld as dev and test sets respectively. Training: from Equation 9. We set the size of the matrices to parameters). More details about relationships and set of hyperparameters in Appendix B.2. Evaluation and metrics: selected samples the user has not interacted with [ mance we focus on the buys / likes relation. For each user scoring function Baselines: Results: and HR@10 across all analyzed datasets. Rotations in both, Riemannian and Finsler metrics, are more effective in this task, achieving the best performance in 3 out of 4 cases, followed by the scaling models. Overall, this shows the capabilities of the systems to effectively represent user-item interactions enriched with relations between items and their attributes, thus learning to better model users’ preferences. Furthermore, it displays the versatility of the approach to diverse data domains. To investigate the recommendation problem endowed with added relationships, we employ In this setup we also augment the data by adding inverse relations and optimize the loss φ(u, buys, i). We adopt MRR and H@10, as ranking metrics for recommendations. We compare to TransE [16], RotC [73], MuRE and MuRP [8] trained with55dimensions. In Table 2 we observe that the SPD models tie or outperform the baselines in both MRR We evaluate our approach on the task of Question Answering (QA). In this manner we also showcase the capabilities of our methods to train word embeddings. Model:We adapt the model from HyperQA [ We model word embeddings question/answers as a summation of the embeddings of their corresponding tokens. We apply a feature transformationT (·) neural linear layer. tion. Finally we compute a distance-based similarity function between the resulting question/answer representations as deﬁned in Equation 10, where w are parameters of the model. Datasets: [89], ﬁltering out questions with multiple answers from the dev and test sets. Training: from Eq. 9, where we replace and for each question we use wrong answers as negative samples. We set the size of the matrices to14 × 14 parameters). The set of hyperparameters can be found in Appendix B.3. Evaluation metrics: tion we rank its candidate answers according to Eq. 10. We adopt MRR and H@1 as evaluation metrics. Baselines: Euclidean model we employ a linear layer as feature transformation. For the hyperbolic model, we operate on the tangent space and project the points into the Poincaré ball to compute distances. Results: transformations learned by the SPD models are able to place questions and answers representations in the space such that they outperform Euclidean and hyperbolic baselines. Finsler metrics seem to be very effective in this scenario, improving the performance of their Riemannian counterparts in many cases. Overall, this suggests that embeddings in SPD manifolds learn meaningful representations that can be exploited into downstream tasks. Moreover, we showcase how to employ different operations as feature transformations and bias additions, replicating the behavior of linear layers in classical deep learning architectures that can be seamlessly integrated with different distance metrics. One reason to embed data into Riemannian manifolds, such as SPD, is to use geometric properties of the manifold to analyze the structure of the data [ Table 2: Results for Knowledge graph-based recommender systems. followed by a bias addition, as an equivalent of a We analyze two popular benchmarks for QA: TrecQA [85] (clean version) and WikiQA dimensions (equivalent to105free We compare against Euclidean and hyperbolic spaces of105dimensions. For the We present results in Table 3. In both datasets, we see that the word embeddings and Figure 5: Train, negative and validation triples for relationships ’derivationally related form’ (top) and ’hypernym’ (bottom) for 5 (left), 50 (center) and 3000 (right) epochs for the red dot corresponds to the relation addition R. high dimensionality. As a solution we use the vector-valued distance function to develop a new tool to visualize and analyze structural properties of the learned representations. We adopt the vector the VVD is contained. Then, we plot the norm of the VVD vector and its angle with respect to this barycenter. In Figure 5, we compute and plot the VVD corresponding to as deﬁned in Eq. 7 for KG models trained on WN18RR. In early stages of the training, all points fall near the origin (left side of the plots). As training evolves, the model learns to separate true (h, r, t)triples from corrupted ones (center part). When the training converges, the model is able to clearly disentangle and cluster positive and negative samples. We observe how the position of the validation triples (green points, not seen during training) directly correlates with the performance of each relation. Plots for more relations in Appendix B.1. Riemannian geometry has gained attention due to its capacity to represent non-Euclidean data arising in several domains. In this work we introduce the vector-valued distance function, which allows to implement universal models (generalizing previous metrics on SPD), and can be leveraged to provide a geometric interpretation on what the models learn. Moreover, we bridge the gap between Euclidean and SPD geometry under the lens of the gyrovector theory, providing means to transfer standard arithmetic operations from the Euclidean setting to their analog notions in SPD. These tools enable practitioners to exploit the full representation power of SPD, and proﬁt from the enhanced expressivity of this manifold. We propose and evaluate SPD models on three tasks and eight datasets, which showcases the versatility of the approach and ease of integration with downstream tasks. The results reﬂect the superior expressivity of SPD when compared to Euclidean or hyperbolic baselines. This work is not without limitations. We consider the computational complexity of working with spaces of matrices to be the main drawback, since the cost of many operations is polynomial instead of linear. Nevertheless, a matrix of rank usually not required. This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1, as well as under Germany’s Excellence Strategy EXC-2181/1 - 390900948 (the Heidelberg STRUCTURES Cluster of Excellence), and by the Klaus Tschira Foundation, Heidelberg, Germany.