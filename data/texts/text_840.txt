DB-BERT is a database tuning tool that exploits information gained via natural language analysis of manuals and other relevant text documents. It uses text to identify database system parameters to tune as well as recommended parameter values. DB-BERT applies large, pre-trained language models (specically, the BERT model) for text analysis. During an initial training phase, it ne-tunes model weights in order to translate natural language hints into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and prioritize hints to achieve optimal performance for a specic database system and benchmark. Both phases are iterative and use reinforcement learning to guide the selection of tuning settings to evaluate (penalizing settings that the database system rejects while rewarding settings that improve performance). In our experiments, we leverage hundreds of text documents about database tuning as input for DB-BERT. We compare DB-BERT against various baselines, considering dierent benchmarks (TPC-C and TPC-H), metrics (throughput and run time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT nds the best parameter settings among all compared methods. The code of DB-BERT is available online at https://itrummer.github.io/dbbert/. Manuals are useful. For instance, before starting to tune a database management system (DBMS), it is recommended to read the associated manual. So far, those words of wisdom only seemed to apply to human database administrators. While it is widely acknowledged that database manuals contain useful information, this knowledge has long been considered inaccessible to machines due to barriers in natural language understanding. We believe that this has changed with recent advances in the eld of natural language processing, namely by the introduction of powerful, pre-trained language models based on the Transformer architecture [29]. We present DB-BERT, a tuning tool, based on the BERT model [3], that “reads” (i.e., analyzes via natural language tools) the manual and hundreds of text documents with tuning hints in order to nd promising settings for database system parameters faster. The problem of nding optimal values for DBMS parameters (also called “tuning knobs”) for specic workloads and performance metrics has received signicant attention in recent years. DBMS nowadays feature hundreds of parameters [22], making it very hard to nd optimal settings manually. This motivates computational methods for automated parameter tuning. The dominant approach Table 1: Example tuning hints with extractions. The default value of shared_buffer isshared_buffers set very low ... The recommended value= 0.25 · 𝑅𝐴𝑀 is 25% of your total machine RAM. [23] I changed ‘random_page_cost’ to 1 andrandom_page_cost retried the query. This time, PostgreSQL= 1 used a Nested Loop and the query nished 50x faster. [21] On a dedicated database server, youinnodb_buffer_ might set the buer pool size to 80%pool_size of the machine’s physical memory= 0.8 · 𝑅𝐴𝑀 size. [19] is currently machine learning [1], in particular reinforcement learning [14,27,34]. Here, a tuning tool selects value combinations for DBMS parameters to try in a principled manner, guided by the results of benchmark runs for specic settings. However, this approach is expensive (recent work uses hundreds of iterations per tuning session [27]) and works best if guided by input from database experts [11], pre-selecting a small set of parameters to tune and reasonable value ranges to consider. Our goal is to substitute such input by information that is gained automatically by analyzing text documents. We call the corresponding problem variant Natural Language Processing (NLP)-Enhanced Database Tuning. DB-BERT extracts, from text, tuning hints that recommend specic values for specic parameters. Table 1 shows examples with sources and the associated, formal representation of each extracted hint. Some of the hints (second example) recommend an absolute value while others (rst and third example) recommend relative values. For the latter, translating the hint into a concrete value recommendation requires knowledge of system properties such as the amount of RAM. Some of the hints (rst two examples) mention the parameter explicitly while others (last example) refer to it only implicitly. DB-BERT can exploit all of the hints shown in Table 1. For a given text snippet, DB-BERT uses a ne-tuned version of the BERT Transformer model to solve four tasks. First, it decides whether a text snippet contains hints. Second, it translates hints into formulas such as the ones shown in Table 1. This may entail steps for resolving implicit parameter references as well as relative recommendations. Third, instead of relying on hints completely, DB-BERT may decide to deviate from proposed values within predened ranges. Finally, given potentially conicting hints from multiple sources, DB-BERT chooses weights for hints, representing their relative importance. DB-BERT does not rely on tuning hints alone. Instead, it uses tuning hints as guidelines for a tuning approach that is based on reinforcement learning. During a tuning session, DB-BERT iterates until a user-dened optimization time budget runs out. In each iteration, DB-BERT selects one or multiple DBMS congurations (i.e., parameter settings) to try out. DB-BERT translates the performance observed during those runs (on user-dened benchmarks) into a reward value. This reward value is used to guide the selection of congurations in future iterations, using the Double Deep Q-Networks [28] reinforcement learning algorithm. To apply this algorithm, we formulate database tuning as a Markov Decision Process (MDP) with discrete states and actions. We represent treatment for each hint as a sequence of decisions, determining the hint type (e.g., relative versus absolute values) as well as the hint weight. To leverage NLP for those decisions, we associate each decision option with a text label. This allows DB-BERT to compare hint text and decision label using the BERT Transformer. We train DB-BERT in a system and benchmark independent manner, before applying it for specic tuning tasks. In principle, we could use manually annotated tuning documents for training (assigning a high reward for hint translations that are consistent with annotations). However, generating such data requires expert knowledge and is hard to crowdsource (compared to cases where labeling requires only commonsense knowledge [7]). Instead, we exploit the database system itself for (noisy) feedback. We assume that tuning hints, if correctly translated, tend to recommend admissible values that do not to dramatically decrease performance. Hence, we train DB-BERT by assigning rewards for hint translations that result in admissible parameter settings (i.e., the DBMS accepts the setting). On the other side, we assign penalties for translations that result in inadmissible parameter settings (i.e., the DBMS rejects the setting) or settings that decrease performance signicantly for a simple example workload. The result of training is a model (i.e., weights for around 110 million parameters of the ne-tuned BERT model) that can be used as starting point for tuning other database systems on other benchmarks. We are only aware of one recent vision paper, aimed at leveraging text documents for database tuning [25]. The authors propose a simple approach based on supervised learning. The approach is trained via tuning hints that have been manually labeled with hint translations. In contrast to that, DB-BERT uses unlabeled text as input. No manual pre-processing is required on this input text. Choices associated with hint translation steps are annotated with manually provided text labels (15 labels in total). However, those labels are not scenario-dependent and we use the same labels across all experiments (Table 3 shows ve out of the 15 labels). The same applies to all other tuning parameters introduced in the following sections. Besides the dierences in manual labeling overheads, the prior approach is purely based on input text, does not integrate any performance measurements, and is therefore unable to adapt recommendations to specic benchmarks or metrics. We discuss dierences to prior work in Section 2 in more detail. In our experiments, we compare against the latter work as well as against state of the art methods for database tuning without input text. We exploit large document collections, mined by issuing Google queries with relevant keywords, as text input for DB-BERT. We consider dierent benchmarks (TPC-C and TPC-H), metrics (throughput and latency), and database systems (MySQL and Postgres). The experiments demonstrate clearly that DB-BERT benets signicantly from information gained via text analysis. In summary, our original, scientic contributions are the following: •We introduce DB-BERT, a system that combines natural language text documents and run time feedback of benchmark evaluations to guide database tuning. •We describe the mechanisms used by DB-BERT to extract, prioritize, translate, aggregate, and evaluate tuning hints. •We evaluate DB-BERT experimentally and compare against baselines, using multiple benchmarks, metrics, and database systems. The reminder of this paper is organized as follows. We cover required background in learning and NLP in Section 2. Then, in Section 3, we introduce our problem model and terminology. We give an overview of DB-BERT in Section 4. Then, in Section 5, we describe how DB-BERT extracts and prioritizes candidate hints from text documents. We show how DB-BERT translates single hints in Section 6 and how it aggregates and evaluates hints in Section 7. In Section 8, we report experimental results before we conclude with Section 9. We discuss technologies that DB-BERT is based upon. Also, we describe prior work addressing similar problems as DB-BERT. The eld of NLP has recently seen signicant progress across a range of long-standing problems [30]. This progress has been enabled, in particular, by the emergence of large, pre-trained language models [10], based on the Transformer architecture [29]. Such models address two pain points of prior NLP approaches: lack of task-specic training data and bottlenecks in computational resources for training. Language models are trained, using significant computational resources, on tasks for which training data is readily available in large quantities. For instance, masked language modeling [3] (i.e., predicting masked words in a sentence) can use arbitrary Web text for training. Instead of training new models from scratch for other NLP-related tasks, pre-trained models can be used as a starting point. This approach tends to reduce the amount of training samples needed, as well as computational training overheads, by orders of magnitude [10]. The Transformer architecture [29] has contributed to this development by enabling massively parallel training of large models with hundreds of millions [4] to hundreds of billions [6] of parameters. DB-BERT, true to its name, uses BERT [4], one of the most widely used Transformer models at this point. Natural language query interfaces [8,12,15] are the most popular application of pre-trained models in the context of databases. At the time of writing, corresponding approaches constitute the state of the art for text-to-SQL translation benchmarks such as WikiSQL [35] or SPIDER [33]. The problem of translating text into queries shares certain characteristics with the problem of extracting tuning hints from text. In both cases, text is translated into a formal Table 2: Comparing DB-BERT to prior work on NLPenhanced database tuning. representation. However, whereas text-to-SQL methods typically translate a single sentence into one single SQL query, DB-BERT extracts multiple tuning hints from multi-sentence text passages. Also, DB-BERT must aggregate and prioritize conicting hints obtained from multiple sources (a sub-problem that does not appear in the context of natural language query interfaces). Unlike most prior work on text-to-SQL translation, DB-BERT does not assume the presence of labeled training samples. Reinforcement learning [24] addresses scenarios such as the following. An agent explores an environment, selecting actions based on observations. Those actions may inuence the environment (whose dynamics are initially unknown to the agent) and result in reward values. The goal of the agent is to maximize reward, accumulated over time. In order to do so, the agent needs to balance exploration (trying out action sequences about which little is known) with exploitation (exploiting action sequences that seem to work well, based on observations so far). The area of reinforcement learning has produced various algorithms that balance this tradeo in a principled manner. Specically, DB-BERT uses the Double Deep Q-Networks [28] algorithm. This algorithm learns to estimate action values in specic states via deep learning, using two separate models for selecting actions and evaluating them. Reinforcement learning has been used for various problems in the database domain [2,9,32,34], including tuning problems (discussed in detail next). Dierent from prior work, we combine reinforcement learning with NLP to nd promising parameter settings. More broadly, our work connects to prior work on leveraging text for reinforcement learning, in particular prior work on instruction following [16]. However, prior work does not consider performance tuning, specically database tuning, as we do. A recent vision paper [25] on NLP-enhanced database tuning relates most to our work. The prior work trains a Transformer model to recognize sentences containing tuning hints via supervised learning. For sentences classied as tuning hints, it extracts parameters and values according to a simple heuristic. This approach uses only text input but no run time feedback. It extracts a xed set of recommendations from a document collection, without being able to adapt to specic workloads and performance metrics. DB-BERT, on the other hand, uses hints extracted from text merely as a starting point. It supports a broader range of tuning hints (e.g., implicit hints) and does not require annotated tuning hints during training. We summarize some of the dierences in Table 2 and compare both approaches experimentally in Section 8. Machine learning is nowadays the method of choice for various database optimization problems, ranging from query optimization [13,17,20,26] over physical design decisions [5,9,32] up to database system parameter tuning [14,22,34]. We address an extended version of the latter problem, expanding the input by natural language text documents. We tune congurations for database system parameters. Denition 3.1 (Conguration). Each DBMS is associated with a setPof conguration parameters. Denote byVthe set of admissible parameter values. A conguration assigns each parameter to a valid value and is represented as a functionP ↦→ V. Equivalently, we represent this function as set{⟨𝑝, 𝑣⟩}for𝑝∈ Pand𝑣∈ Vof parameter-value pairs. Parameters not referenced in a conguration maintain their default values. Our goal is to nd congurations that optimize performance. Traditionally, the following problem model is used. Denition 3.2 (Database Tuning). A database tuning problem is described by a tuple⟨𝑏, P, V⟩. Here,𝑏is a benchmark dening a set of queries (or a transaction workload), together with a performance metric to optimize (e.g., run time or throughput). A solution assigns parametersP, selected for tuning, to values fromVand ideally optimizes performance according to benchmark 𝑏. In this work, we address a variant of this problem model. Denition 3.3 (NLP-Enhanced Tuning). An NLP-enhanced database tuning instance is described by a tuple⟨𝑏, 𝑇, 𝑆⟩. Here,𝑏is a benchmark to optimize and𝑇a collection of text documents containing tuning hints. The goal is to nd optimal congurations for 𝑏, considering all DBMS tuning knobs (more precisely, our current implementation considers all integer, numeric, and Boolean parameters for each system), using tuning hints extracted from𝑇via natural language analysis.𝑆is a vector of numerical system properties (such as the amount of RAM or the number of cores) needed to translate hints, potentially containing relative value suggestions, into concrete values. We do not expect users to specify parameters to tune nor to suggest value ranges for parameters. We rely on natural language analysis to identify relevant parameters and proposed values. However, the approach presented in this work assumes access to a DBMS instance. Via this interface, we verify whether extracted parameter names are valid and whether the parameter type falls within our scope (our current implementation considers integer, Boolean, and numeric parameters). Denition 3.4 (Tuning Hint). A tuning hint suggests a value for one DBMS parameter. We model tuning hints as a triple⟨𝑡, 𝑝, 𝑣⟩ where𝑡is a text snippet containing the hint,𝑝a specic parameter, and𝑣a specic value mentioned in𝑡. We call the hint explicit if𝑝 is mentioned explicitly in𝑡and implicit otherwise. In pseudo-code, we use notation ℎ.𝑝 or ℎ.𝑡 to refer to parameter or text of hint ℎ. Figure 1: Overview of DB-BERT system: we exploit tuning hints, extracted from text documents, to nd optimal DBMS knob settings for a given workload. Note that a text snippet 𝑡 may contain suggestions for multiple parameters or multiple suggested values for the same parameter. This is why we need𝑝and𝑣to identify a specic hint within𝑡. Value𝑣may not always be the concrete value proposed for𝑝. This is why we translate tuning hints into formulas, dened next. Denition 3.5 (Translated Hint). We translate tuning hints⟨𝑡, 𝑝, 𝑣⟩ into a formula of the form𝑝 = 𝑓 (𝑣, 𝑆)where𝑓is a formula and𝑆 a vector of numerical system properties (e.g., the amount of main memory). We consider formulas of type𝑓 (𝑣, 𝑆) = 𝑣 · 𝑚as well as 𝑓 (𝑣, 𝑆) = 𝑣 · 𝑆· 𝑚where𝑆is the𝑖-th component of𝑆and𝑚 ∈ Ra multiplicator (picked from a discrete set 𝑀 of multiplicators). We illustrate tuning hints and their translation. Example 3.6. Consider the text snippet𝑡 =“Properly congure shared_buffers- we recommend 25% of available RAM”. Assume 𝑆 = ⟨8𝐺𝐵,4,1𝑇 𝐵⟩describes the amount of RAM, the number of cores, and the amount of disk space on the target system. Then, the tuning hint⟨𝑡, 𝑝, 𝑣⟩for𝑝 =shared_buffersand𝑣 =0.25 should translate into the formula𝑓 (𝑣, 𝑆) = 𝑣 · 𝑆·1 (where 1 represents the multiplicator), which evaluates to 2 GB. Figure 1 shows an overview of the DB-BERT system. DB-BERT searches settings for the tuning knobs of a DBMS that maximize performance according to a specic benchmark (specifying workload and performance metric). DB-BERT diers from prior tuning system in that it exploits text documents about the DBMS to tune, for instance the DBMS manual, as additional input. DB-BERT obtains as input the benchmark to tune, a collection of text documents containing suggested settings for tuning knobs, and numerical properties describing the hardware platform (namely, our implementation expects the amount of RAM, the number of cores, and the amount of disk space as inputs). The latter input is necessary to translate tuning hints in text documents that use relative recommendations (e.g., suggesting a buer size as a percentage of the amount of RAM). Note that DB-BERT is not restricted to parameters that relate to the aforementioned hardware properties. DB-BERT can process hints for arbitrary parameters, as long as recommended values are specied as absolute values in text. DB-BERT does not use text input alone to determine parameter settings (separating it from prior work on NLP-enhanced database tuning [25]). Instead, it exploits run time feedback obtained by benchmarking specic congurations on the DBMS to tune. Hence, DB-BERT requires a connection to a DBMS instance. At the start of a tuning session, DB-BERT divides input text into text snippets and tries to extract tuning hints from each snippet (StepAin Figure 1). A tuning hint corresponds to a recommendation of a specic value for a specic parameter. Extracting hints from text snippets is non-trivial, in particular as parameter references may be implicit (i.e., the text does not explicitly mention the name of the parameter to tune). Next, DB-BERT determines the order in which hints will be considered in the following stages (StepB in Figure 1). Ideally, the most important hints are considered rst. DB-BERT uses a heuristic to order hints, prioritizing hints about frequently mentioned parameters while limiting the number of hints considered consecutively for the same parameter. Next, DB-BERT iteratively constructs congurations (i.e., value assignments for tuning knobs) from tuning hints. It evaluates those congurations on the input benchmark via trial runs. Iterations continue until the user interrupts optimization or a user-specied optimization time limit is reached. In each iteration, DB-BERT considers a batch of tuning hints (not the entire set of tuning hints). It considers hints in the order established at the start of the tuning session, thereby considering the seemingly most important hints rst. For each hint, DB-BERT takes three types of decisions. First, it translates the hint text into a simple equation, assigning a value to a parameter (StepCin Figure 1). Second, in StepD, it decides whether to deviate from the recommended value (i.e., whether to multiply the recommended value by a constant). Third, it assigns a weight to the hint (StepE). These weights decide how to prioritize in case of conicting recommendations about the same tuning knob. After treating all hints in the current batch, DB-BERT aggregates them into a small set of congurations (StepF), mediating between inconsistent recommendations using hint weights. It evaluates those congurations on the user-specied benchmark via trial runs (StepGin Figure 1). DB-BERT learns to improve the way hints are translated, adapted, and weighted over the course of a tuning session. This allows DB-BERT to specialize a conguration to the current benchmark and platform. DB-BERT uses reinforcement learning to make all decisions associated with StepsCtoEin Figure 1. The learning process is therefore driven by a reward function that the system tries to maximize. In case of DB-BERT, that reward function is based on the performance results for specic congurations during trial runs. Congurations that are accepted by the DBMS (i.e., trying to Algorithm 1 NLP-enhanced database performance tuning. // Optimize all DBMS parameters𝑃for benchmark𝑏via hints set parameters to specic values does not result in an error) and achieve high performance generate high reward values. Based on rewards received, the system learns to improve its decision making in coming iterations (Step H in Figure 1). DB-BERT uses deep reinforcement learning. This means that immediate and future reward values associated with specic choices are estimated using a neural network. Specically, DB-BERT uses BERT, a pre-trained language model, as neural network. Due to pretraining, this model comes with powerful natural language analysis capabilities out of the box. To estimate the value of specic choices during StepsCtoE, BERT is applied to pairs of text snippets. The rst snippet is taken from the text of a tuning hint, the second snippet is a text label representing the semantics of that choice (see Table 3 in Section 6 for example labels). Based on reward values received, the initial weights of the BERT model are rened over the course of a tuning session (in Step H). Algorithm 1 represents the main function, executed by DB-BERT, in pseudo-code. The input integrates user-provided inputs, represented in Figure 1, as well as other parameters, extracted automatically or kept constant across systems and benchmarks. These include the full set of integer, Boolean, and numeric tuning knobs, extracted from the DBMS,𝑃, a set𝑀of multiplicators (to deviate from values proposed in text), a set𝑊of weights (to determine relative importance between conicting hints from dierent sources), and parameters𝑙,𝑒, and𝑛to choose the number of hints processed per parameter and iteration, the total number of hints considered per iteration, and the number of congurations evaluated per iteration, respectively. The semantics of those parameters will be described in more detail in the following sections. Line 8 in Algorithm 1 realizes StepAfrom Figure 1, Line 10 realizes StepB. The main loop iterates until the tuning time budget is depleted. Function Batches(𝐻, 𝑒) divides hints into batches of size at most𝑒, following the previously established hint order. Each invocation of RunEpisode realizes StepsCtoHfrom Figure 1. Finally, DB-BERT recommends the best observed conguration. Algorithm 2Extract candidate tuning hints from text documents. CosineTop-K∪× Figure 2: Given a text passage and DBMS parameter names, DB-BERT pairs extracted values with parameters that are explicitly mentioned or are similar to the text. Section 5 discusses hint extraction and ordering. Section 6 describes the learning process in more detail and Section 7 outlines how hints are aggregated into congurations. In a rst step, DB-BERT extracts candidate tuning hints. Following Denition 3.4, a tuning hint consists of a text snippet, a parameter reference, and a value reference. Algorithm 2 describes the extraction process (illustrated in Figure 2 as well). It extracts explicit as well as implicit parameter references. Implicit references are obtained by comparing the BERT encoding for the text (a vector) against BERT encodings of parameter names, selecting the parameter with minimal cosine distance. We consider all numbers that appear in text, potentially combined with size units, as potential value suggestions. By default, we add values 0 and 1, representing on and o values for Boolean ags, into the set of values (on and o values are often not explicitly mentioned in tuning hints). The set of candidate hints for a given text snippet is the Cartesian product between parameter references and values. This means that our candidates likely contain erroneous hints (i.e., parameter-value combinations that are not linked by the text). The task of separating actual from erroneous hints is solved during the translation phase, described in the next section. After extracting candidate hints, DB-BERT sorts them using Algorithm 3. Our goal is to increase chances of nding promising congurations when considering hints in sort order. We consider two rules of thumb. First, we expect important parameters to be Algorithm 3 Prioritize hints based on their parameters. Figure 3: DB-BERT prioritizes hints about frequently mentioned parameters while limiting the number of hints per parameters before switching to the next one. In the illustrated example, hints are considered in the order indicated by the red (numbered) arrows. mentioned in more documents. Second, we expect diminishing returns when considering more and more hints about the same parameter. As a result, we prioritize hints about parameters that appear in more documents. However, we consider at most a xed number of hints about the same parameter, before switching to the next one. Algorithm 3 implements those high-level principles. After grouping hints by parameter, it iterates over hint index ranges. For each index range, it iterates over parameters in decreasing order of occurrences, adding up to𝑙hints per parameter before switching to the next one (until no new hints are left to add for any parameter). Example 5.1. Figure 3 illustrates hint ordering with three parameters. Blue rectangles represent hints for each parameter. The horizontal width is proportional to the number of hints. Starting with the most frequently mentioned parameter, we add a limited number of hints for each parameter. After treating the least frequently mentioned parameter (symbolized by the red arrow), Parameter 3, we start again with the rst one until no more hints are left. Algorithm 4 Transition function for translating single hints. // Returns next decision, expanded formula, and reward value. DB-BERT translates tuning hints into arithmetic formulas (see Denition 3.5 for details). Those formulas may depend on values, specied in text, as well as on system properties such as the amount of main memory. Evaluating a formula yields a value suggestion for a tuning knob. For each tuning hint, we model the translation as a sequence of decisions. We learn to translate tuning hints by using reinforcement learning. Reinforcement learning is generally applied to Markov Decision Processes (MDPs), specied by a set of states, actions, a transition function mapping state and action pairs to new states, and a reward function. A reinforcement learning agent learns to make decisions maximizing expected rewards, using observations as guidance. In our scenario, states represent (partially specied) arithmetic formulas. Actions specify parts of the formula. The transition functions links partially specied formulas and actions to states representing the formula, completed as specied in the action. The reward function is based on feedback from the DBMS, penalizing translations that result in inadmissible congurations while rewarding changes that improve performance. We describe the structure of the environment (i.e., states, actions, transitions, and rewards) in Section 6.1 and the structure of the learning agent in Section 6.2. Algorithm 4 implements the transition function, used by DB-BERT to translate single hints (the pseudo-code is close to the implementation of the step function in the corresponding OpenAI Gym .. .𝑝 = 𝑣 · 𝑆· 𝑀.. . .. .DBMS: ErrorDBMS: Valid.. . Figure 4: Markov Decision Process for hint translation: parameter-value pairs are mapped to formulas by action sequences. Rectangles represent states (double lines mark end states). Arrows represent transitions (dashed arrows mark non-deterministic transitions). environment). In Algorithm 4, and for a xed tuning hint, the current state is characterized by a partially specied formula (𝑓) and by variable𝑑, the integer ID of the next decision to take. For each hint, we start with an empty formula𝑓and𝑑 =0. We represent actions (input𝑎) as integer numbers from one to ve. The semantics of actions depend on the value of𝑑. For𝑑 =0, the action decides whether the current hint is erroneous (constant NO_HINT) and, if not, whether the hint suggests a relative or absolute parameter value. Relative values are expressed as percentage of system properties such as main memory or the number of cores (stored in vector𝑆with𝑆representing a specic vector component). For relative values, we set𝑓to the product between value𝑣and the corresponding system property. We unify treatment of relative and absolute values by setting𝑆=1 (i.e.,𝑎 =1 represents an absolute value). For𝑑 =1, the action picks a multiplicator from𝑀that allows deviating from the proposed value. Unlike prior work merely extracting tuning hints [25], such multiplicators allow DB-BERT to adapt to specic benchmarks. In the next section, we introduce an additional decision that weighs hints. Here, we have fully specied the formula after two decisions. Next, we try setting parameter𝑝to the formula evaluation result. If the setting is rejected by the DBMS, we directly advance to an end state (constant END). This case yields negative reward (motivating our agent to learn translating hints into admissible formulas). Otherwise, we evaluate performance on the input benchmark𝑏. The result is a reward value. Higher rewards are associated with better performance. We calculate reward by comparing performance with a conguration to evaluate to performance with default settings. For OLAP benchmarks (e.g., Algorithm 5 Evaluating expected reward of actions. TPC-H), we use the delta of run times (scaled by a constant). For OLTP benchmarks (e.g., TPC-C), we use the throughput delta. We reward congurations that are admissible and increase performance. Those two metrics are immediately relevant for tuning. We use them when applying DB-BERT for tuning a specic system for a specic benchmark. Before applying DB-BERT for specic tuning tasks, we perform a training phase to ne-tune DB-BERT’s language models for hint translation in general. To speed up convergence, only during training, we add an additional component to the reward function. This component rewards settings that seem more likely, e.g. since they are in the same order of magnitude as the default settings for a parameter. Such heuristics replace manually generated hint translations, used in prior work [25]. Figure 4 illustrates the MDP behind the translation process (some of the states in Figure 4 are not explicitly represented in Algorithm 4). DB-BERT introduces a learning agent to choose actions in order to maximize rewards. In each state, the agent selects among a discrete set of options. Each option can be expressed as a natural language statement. We can nd out which option is correct by comparing that statement against the tuning hint text. Hence, we model action selection as a “multiple choice question answering problem”. Pretrained language models can be used to solve this problem (in our implementation, we use theBertForMultipleChoiceTransformer model). We ne-tune model weights during training, based on rewards received. Algorithm 5 shows how the agent evaluates specic actions, based on observations. Besides the action to evaluate, the input includes a description of the current tuning hint (tuning text𝑡, parameter𝑝, and value𝑣) as well as the current translation step (decision𝑑). We associate each combination of an action and a decision with a label. The array containing those labels is represented via constant CHOICE_LABEL in the pseudo-code. The label is a 0 (NO_HINT) [p] and [v] are unrelated. Table 3: Labels associated with actions for decision 𝑑 =0. Placeholders are contained in square brackets. natural language sentence, representing the semantics of the associated choice. It contains placeholders for the concrete parameter and value in the tuning hint. The Instantiate function replaces placeholders by concrete values. The BERT model uses three inputs: an input text, a type tag associating input tokens with one of two input types, and a mask indicating tokens to consider. Here, we concatenate hint text and instantiated label to form the input text. Types separate hint text from label. By default, all input text is considered for processing. An exception occurs during our generic training phase (see Section 6.1 for more details). Here, we want to avoid learning the names of specic parameters as they do not generalize across systems. Hence, we mask all occurrences of the current parameter name (Function Mask). On the other side, if learning system and benchmark specic congurations for a concrete tuning problem, there are no reasons to hide information. Algorithm 5 uses a Boolean ag (MASKED_MODE) to switch between these two modes. Table 3 shows labels associated with dierent actions and the rst decision level. At this level, we decide whether a candidate hint represents an actual hint and, if so, whether the value is relative or absolute. Finally, we illustrate the translation by an example. Example 6.1. Consider the tuning hint⟨𝑡, 𝑝, 𝑣⟩with𝑡 =“Set shared_buffersto 25% of RAM”,𝑝 =shared_buffers, and𝑣 = 25%. First, the agent decides whether the hint is valid and whether it recommends an absolute or relative value. Using the labels from Table 3, the agent evaluates alternative actions based on the hint text. For instance, for action 1, the agent generates the input text “Setshared_buffersto 25% of RAM.shared_buffersand 25% relate to main memory.”, separating the two sentences via the type specication. If masked mode is activated, the two occurrences of theshared_buffersparameter are masked. To make a choice, the agent internally compares values resulting from applying BERT to the input for each possible action. The last section describes how to translate single tuning hints. However, we often need to integrate multiple hints, possibly from dierent sources, to obtain optimal performance. DB-BERT creates congurations based on groups of hints. This requires aggregating, possibly conicting hints, from dierent sources. To support that, we expand the MDP presented in the last section. Instead of considering a single hint, we consider an entire batch of hints. For each single hint, we add an additional decision assigning the hint to a Algorithm 6 Transition function for interpreting multiple hints. weight. This weight determines the priority when aggregating the hint with others into a conguration. Algorithm 6 shows complete pseudo-code executed during one iteration of DB-BERT’s main loop (Algorithm 6 is invoked by Algorithm 1). From the reinforcement learning perspective, each iteration corresponds to one episode of the associated MDP. Each episode starts from the same starting state, representing the default conguration. The number of hints considered per episode does therefore restrict the maximal number of changes, compared to the default conguration. However, as shown in recent work [11,27], tuning a small number of tuning knobs is typically sucient to achieve near-optimal performance. Algorithm 6 obtains a batch of candidate hints as input. It iterates over those hints and uses Algorithm 4 (Function Tstep) to translate single hints (respectively, to determine that a candidate hint is erroneous and should not be considered). We postpone benchmark evaluations by specifying “−” as benchmark parameter for Tstep. If successful at translating the current hint into a formula (i.e.,𝑓 ≠ −), Algorithm 6 assigns a weight (Line 18). Weights are chosen from a discrete set𝑊of possibilities and are assigned by the learning agent (Function ChooseAction). Finally, the algorithm assembles a set 𝐻of weighted tuning hints. Next, we assemble one or several congurations to evaluate, using weighted hints. Algorithm 7 chooses and evaluates congurations, using weighted hints as input. It iterates over parameters mentioned in hints (loop from Line 23 to 30) and selects a limited number of𝑛values to try (𝑛is a tuning parameter). Values are Algorithm 7Evaluate set of weighted tuning hints on benchmark. selected in order to cover the range of suggested values (in hints) as well as possible. We choose values iteratively (loop from Line 26 to 29). We want to cover values proposed in hints as closely as possible in the following sense. Given a distance function𝛿comparing values for the same parameter, our goal is to minimize the maximal, weighted distance between a value proposed in a hint and the closest selected value. Function MaxDist calculates the latter metric, given a weighted set𝑉of values and a set of selected congurations𝐶. We select values greedily, minimizing the aforementioned cost function in each step. Note that some tuning knobs can only be set to specic values within their value range (e.g., MySQL’sinnodb_buffer_pool_sizemust be a multiple of the chunk size [19]). We cannot simply average proposed values. Example 7.1. Assume we collect hints recommending the following values for parametershared_buffers: 1 GB with weight 1, 2 GB with weight 8, and 8 GB with weight 1. When selecting 1 GB, we obtain maximal weighted distance of 8· |2−1| =8 GB from value 2 GB (only distance 1· |8−1| =7 GB from 8 GB). Selecting 2 GB yields a maximal weighted distance of 7 GB from value 8 GB. Selecting 8 GB yields a maximal weighted distance of 48 GB from value 2 GB. Hence, we select value 2 GB rst. Next, we select value 8 GB to minimize the maximal distance to 1 GB. Finally, we compose selected values for each parameter into 𝑛congurations (Line 32). Function Evaluate evaluates selected congurations on the given benchmark𝑏. It assigns a penalty for congurations that are not accepted by the DBMS and, otherwise, calculates reward based on benchmark performance (we use the reward function introduced in Section 6.1). Function EvalWeighted returns the maximal reward obtained by any conguration. We describe our experimental setup in Section 8.1, provide details on the text documents used for NLP-enhanced database tuning in Section 8.2, and details on the training process of all compared algorithms in Section 8.3. We compare DB-BERT against various baselines in Section 8.4 and study the impact of text document size, data size, and various DB-BERT features on performance in Section 8.5. We compare approaches for tuning system conguration parameters for MySQL 8.0 and for Postgres 13.2. We consider all numerical and Boolean tuning parameters that those systems oer: 232 parameters for Postgres and 266 parameters for MySQL. We use TPC-H with scaling factors one (Section 8.4) and ten (Section 8.5) and TPCC with scaling factor 20 as benchmarks. For TPC-C, we use ten terminals, unlimited arrival rate, and 60 seconds for both, warmup and measurement time. Besides those parameters, we use the default TPC-C congurations for Postgres and MySQL from the OLTP benchmark. We execute ve runs and allow for 25 minutes of tuning time (prior work uses the same time frame [34]). All experiments execute on a p3.2xlarge EC2 instance with 8 vCPUs, 61 GB of RAM, and a Tesla V100 GPU featuring 16 GB of memory. The EC2 instance uses the Amazon Deep Learning AMI with Ubuntu 18.04. We compare against the recent DDPG++ algorithm [27] as representative for tuning without NLP-enhancement. We consider dierent value ranges for tuning parameters, ranging from a factor of two around the default value (i.e.,𝑑/2 to 2· 𝑑where𝑑is the default) to 100. We denote those versions as DDPG2, DDPG10, and DDPG100 in the following plots. Also, we compare against two baselines described in a recent vision paper on NLP-enhanced database tuning [25]. In the following, Prior-Main denotes the main method proposed by that prior work, based on supervised learning. Also, we compare against a simple baseline, denoted as Prior-Simple, described in the same paper [25]. Figure 5: Frequency distribution of hints and parameters in the collection of tuning documents for Postgres and MySQL. By default, we use the following conguration parameters for DBBERT. DB-BERT uses reinforcement learning to select multiplicator values and weights for each hint from a xed set of alternatives. For all experiments, DB-BERT selects the multiplicator from the use the same number of alternatives (ve) in each case. This makes it easier to model the associated environment with OpenAI’s gym framework. We avoid using overly small or large multiplicators (if the optimal parameter value deviates by more than factor four from the proposed value in any direction, the associated hint should be disregarded). The set of weight alternatives allows DB-BERT to disregard hints (by using a weight of zero) as well as to make specic hints up to eight times more important, compared to other hints with non-zero weights. We set𝑙to 10 in order to allow at most ten hints per episode and parameter. We consider at most 50 hints per episode in total (𝑒 =50) and evaluate two congurations per episode (𝑛 =2). DB-BERT splits text documents into segments of length at most 128 tokens. All baselines are implemented in Python 3.7, using Pytorch 1.8.1 and (for the NLP-enhanced tuning baselines) the Huggingface Transformers library [31]. DB-BERT uses Google’s programmable search engine APIto retrieve text documents. Also, DB-BERT uses the Double Deep Q-Networks [28] implementation from the Autonomous Learning Libraryas reinforcement learning algorithm. DB-BERT comes with a script that retrieves text documents via Google search and transforms them into the input format required by DB-BERT. For most of the following experiments, we use two document collections retrieved via the queries “Postgresql performance tuning hints” (issued on April 11, 2021) and “MySQL performance tuning hints” (issued on April 15, 2021). We included the rst 100 Google results for each of the two queries into the corresponding document collection (accounting for a total of 1.3 MB of text for Postgres and 2.4 MB of text for MySQL). The results are Table 4: Tuning parameters mentioned in most documents for Postgres and MySQL. Postgres shared_buffers diverse and cover blog entries, forum discussions (e.g., on Database Administrators Stack Exchange), as well as the online manuals from both database systems. We call the document collection for Postgres Pg100 and the one for MySQL Ms100 in the following. Figure 5 shows the distribution of parameter mentions and proposed value assignments in those document collections, generated via DB-BERT’s candidate hint extraction mechanism (see Section 5). Clearly, the distribution of hints over documents and parameters is non-uniform. For both database systems, few parameters are mentioned in multiple documents while most parameters are mentioned only in a single document. Similarly, there are a few assignments proposed by multiple sources. On the other side, most value assignments are proposed only once. Table 4 shows the most frequently mentioned parameters for both Postgres and MySQL. Parameters related to buer size (e.g., shared_buffersfor Postgres andinnodb_buffer_pool_sizefor MySQL) feature prominently among them. Besides that, parameters related to parallelism (e.g.,max_parallel_workers_per_gather) or logging (e.g., max_wal_size) are mentioned frequently as well. Two of the compared algorithms, namely DB-BERT and Prior-Main, use training before run time. Prior-Main uses natural language tuning hints, annotated with associated formulas, as training data. We use the same training samples and training parameters as in the prior work [25]. Consistent with the experimental setup in the latter paper, we apply Prior-Main, trained on Postgres samples, to tune MySQL and Prior-Main, trained on MySQL samples, to tune Postgres. The goal is to demonstrate that NLP-enhanced database tuning does not require system-specic, annotated samples. Prior-Main has no support for extracting benchmark-specic tuning hints from a xed document collection, a disadvantage if the same document collection is used for tuning multiple benchmarks. To allow at least some degree of variability, we train the Prior-Main model separately for each of our ve benchmark runs. This leads to slightly dierent extractions in each run. Training Prior-Main on the platform outlined in Section 8.1 took 417 seconds for MySQL samples and 393 for Postgres samples. 30025Loss 200Max Reward20 Figure 6: Reward and loss when training DB-BERT on Pg100. DB-BERT does not use annotated tuning hints for training. Instead, it uses the database system itself for run time feedback during the training phase. Similar to Prior-Main, we train DB-BERT on Pg100 to tune MySQL and on Ms100 to tune Postgres. We activate the masked mode during training (see Section 6), meaning that parameter names are masked. This avoids learning system-specic parameter names (which are useless in our experimental setup) and focuses attention on the sentence structure of tuning hints instead. The reward signal of DB-BERT (see Section 6 and 7) combines reward for successfully changing parameter values according to tuning hints (meaning that the corresponding values are valid) and for performance obtained. To measure performance, we use a synthetic database containing two tables with two columns containing consecutive numbers from 1 to 1,000,000. We use a simple count aggregation query joining both tables with an equality predicate. Reward for performance is scaled down by a factor of 100 to avoid specialization to this articial benchmark (it merely serves to penalize particularly bad congurations such as setting the buer pool size to a minimal value). Finally, we add a small reward bonus for setting parameter values that are within the same order of magnitude as the default setting (assuming that extreme deviations from default values are possible but less likely). DB-BERT’s training starts from the BERT base model [3] with 110 million parameters. All model parameters are tuned during training. We trained DB-BERT for 5,000 iterations on Pg100 and for 10,000 iterations on Ms100 (due to the larger number of hints in this collection). Training took 43 minutes for Pg100 and 84 minutes for Ms100. Figure 6 shows progress for Pg100 as a function of the number of training steps. We compare DB-BERT against baselines on TPC-H (see Figure 7) and TPC-C (see Figure 8). We tune Postgres and MySQL for 25 minutes per run. We use throughput as optimization metric for TPC-C and execution time for TPC-H. We show performance of the best conguration found (y axis) as a function of optimization time (x axis). In these and the following plots, we report the arithmetic average as well as the 20th and 80th percentile of ve runs (using error bars to show percentiles). DDPG++ [27] is a database tuning approach, based on reinforcement learning. It was shown to be competitive with various other state of the art tuning approaches [27]. However, the prior publication evaluates DDPG++ for a few tens of tuning parameters and allocates 150 iterations per tuning session. Here, we consider hundreds of parameters for tuning and aim at a tuning time frame that allows only few iterations. Clearly, within the allocated time Table 5: Postgres conguration for TPC-H by DB-BERT. max_parallel_workers_per_gather 19 frame, DDPG++ does not nd solutions of comparable quality to DB-BERT. In particular for TPC-H, DDPG++ often tries parameter changes that decrease performance signicantly (e.g., changes to optimizer cost constants triggering dierent join orders). Hence, performance of the best conguration found remains almost constant for DDPG++ (close to the one achieved via the default conguration, tried before the initial iteration). DDPG++ could benet from specifying parameter-specic value ranges to consider during tuning. For instance, increasing buer pool size by an order of magnitude, compared to the default settings, is often benecial. For optimizer cost constants (e.g.,random_page_costin Postgres), doing so is however dangerous. Our goal is to show that such input can be partially substituted by information mined automatically from text. Prior-Simple and Prior-Main are the two most related baselines as both use tuning text as input, similar to DB-BERT. PriorSimple uses a naive heuristic for translation. Applying this heuristic is fast and Prior-Simple is typically the rst baseline to return results. However, it only extracts the recommendation to set checkpoint_completion_targetto 0.9 from Pg100 and no recommendations from Ms100. Hence, it does not improve over the default conguration. Prior-Main performs signicantly better. Due to small dierences in training, extractions dier across dierent runs, leading to high variance. For instance, for Pg100, Prior-Main is able to extract a tuning hint that recommends settingshared_buffers to 25% of main memory in two out of ve runs. This can lead to signicant performance improvements, in particular for TPC-H. However, average performance is signicantly below the optimum. As Prior-Main classies all sentences in the document collection before aggregating tuning hints, its run time is signicantly higher than the one of Prior-Simple. DB-BERT achieves attractive tradeos between tuning time and result quality. Unlike DDPG++, it uses tuning text as input that allows identifying the most relevant parameters and candidate values quickly. Compared to Prior-Simple and Prior-Main, it nds signicantly better solutions in average. In particular for MySQL, Prior-Main typically fails to nd solutions of comparable quality. Furthermore, the time taken by Prior-Main to analyze all documents is typically higher by a factor of two to three, compared to the time until DB-BERT produces a near-optimal solution (i.e., within one percent of DB-BERT’s nal optimum). Tables 5 and 6 show congurations found by DB-BERT when tuning Postgres. Despite extracting hints from the same document collection, DB-BERT is able to nd benchmark-specic congurations. 16Execution Time (s) Execution Time (s) Figure 7: Minimal execution time for TPC-H as a function of optimization time for dierent baselines. Table 6: Postgres conguration for TPC-C by DB-BERT. We study the impact of dierent factors on tuning performance. First, we compare DB-BERT against two simplied variants in Figure 9. We compare against a variant of DB-BERT that processes hints in document order (instead of prioritizing them as described in Section 5). Also, we compare against a variant that does not consider implicit hints (i.e., only hints where parameter names are explicitly mentioned). Clearly, both simplications degrade tuning performance on TPC-H. Considering hints in document order prevents DB-BERT from tuning the most relevant parameters rst. Discarding implicit hints reduces the total set of available hints. 1,200 1,000Throughput (tx/s) 200Throughput (tx/s) Figure 8: Maximal throughput for TPC-C as a function of optimization time for dierent baselines. 16Execution Time (s) Figure 9: Comparison of dierent DB-BERT variants when optimizing Postgres for TPC-H. Next, we study the impact of the input text. We replace Pg100, containing hundreds of generic tuning hints, by a single blog post. This post describes how to tune Postgres specically for TPC-H. Figure 5 compares performance with dierent input documents for all NLP-enhanced tuning baselines. While the performance of Prior-Simple does not change with the input text, the performance Figure 10: NLP-enhanced database tuning for TPC-H on Postgres with dierent input text (100 documents with generic hints versus one document with benchmark-specic hints). 180Execution Time (s) Figure 11: Minimal execution time for TPC-H with scaling factor 10 as a function of optimization time. of Prior-Main degrades as we switch to the smaller document. PriorMain benets from large document collections as redundant hints can partially make up for imprecise extractions. For the smaller input document, it does not extract any hints. DB-BERT, however, benets from more specialized tuning hints. Using benchmarkspecic input text, it converges to near-optimal solutions faster and ultimately nds a slightly better solution (using a higher value for theshared_buffersparameter, compared to Table 5, as proposed in the blog entry). Finally, we scale up the data size. Figure 11 reports results for TPC-H with scaling factor 10 (and using the TPC-H specic tuning text). Compared to Figure 10, showing results for scaling factor one, it takes longer for DB-BERT to nd near-optimal solutions. This is expected, as longer run times per benchmark evaluation reduce the number of DB-BERT’s iterations per time unit. Compared to other baselines, DB-BERT nds signicantly better solutions again. We presented DB-BERT, a database tuning system that extracts tuning hints from text documents. Our experiments demonstrate that such hints lead to signicantly better tuning results. In future work, we will consider more diverse tuning objectives. Currently, DB-BERT is limited to optimizing metrics such as latency or throughput that can be easily measured. However, there are other, important metrics that are dicult to measure. For instance, many parameters (e.g., thefsyncparameter in Postgres) allow increasing performance if willing to accept a small risk of data loss. Database manuals typically contain warnings detailing such risks. We plan to extend DB-BERT to extract information on metrics that are dicult to measure from the manual. Thereby, it can support users in nding parameter settings that maximize performance while complying with constraints on other metrics.