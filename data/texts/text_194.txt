Keywords Bipartite graphs · Co-clustering · Co-embedding · Dimension relatedness · Visualization Many datasets can be represented as a bipartite graph (BPG), making the links between two different types of items. This could be the movies a user watched, the purchases he made, the music he listened at. Outside of user interaction, it could be tags associated with a ﬁle, such as the keywords of an article, or the genes with which a molecule interacts. Community discovery or graph partitioning helps improving scalability in different contexts. Collaborative Filtering Recommender Systems (CF RecSys) work by suggesting new items to a user based on the similarity of its history to the other users’ history. Without optimization, a user is compared to all the other users to ﬁnd those most similar to him. Clustering reduces costs by looking at similar users within the same partition rather than looking at them in the entire dataset (1 These approaches are based on the clustering hypothesis ( information needs. Cluster-based retrieval systems are based on this same hypothesis ( cannot be stored in a single server could be split into multiple servers, hosting a particular thematic cluster. A query would be compared to an index with clusters’ summary and routed to the most relevant server, reducing the overall number of operations. There are different approaches to cluster a bipartite graph. The algorithms can be classiﬁed into two distinct categories: one-way and two-way partitioning algorithms, with the latter grouping both sides of the graph simultaneously. In the one-way approach, the set of nodes belonging to the same type are clustered, without considering the clusters that would be obtained when clustering the nodes from the other type. The nodes to cluster are often represented into a matrix using the Vector Space Model (VSM) ( used to construct the features ( firstname.lastname@worldline.comENS, CNRS, PSL University, Paris Many datasets take the form of a bipartite graph where two types of nodes are connected by relationships, like the movies watched by a user or the tags associated with a ﬁle. The partitioning of the bipartite graph could be used to fasten recommender systems, or reduce the information retrieval system’s index size, by identifying groups of items with similar properties. This type of graph is often processed by algorithms using the Vector Space Model representation, where a binary vector represents an item with0and1. The main problem with this representation is the dimension relatedness, like words’ synonymity, which is not considered. This article proposes a co-clustering algorithm using items projection, allowing the measurement of features similarity. We evaluated our algorithm on a cluster retrieval task. Over various datasets, our algorithm produced well balanced clusters with coherent items in, leading to high retrieval scores on this task. ). Clustering is not limited to users and has similar beneﬁts when clustering items (2), or both together (3). k-means, measure the items’ proximity using cosine similarity measure, or project items using dimensionality reduction algorithms (9). Another possibility is to convert the unipartite graph is convenient as many classical community detection algorithms can be used ( single type are preserved and new weighted edges are inferred from the bipartite graph. There are multiple approaches to convert the bipartite graph into a unipartite graph ( edges and the loss of information due to the collapse (10, 11). In contrast, two-way clustering algorithms – also called co-clustering or simultaneous clustering ( structure to cluster both types of nodes simultaneously. Two-way approaches lead to better results, even if the goal is to partition only one side of the BPG (13, 14) by exploiting synergies between the obtained clusters. Among co-clustering approaches, the majority of the literature focuses on bi-clustering approaches, where a bicluster is characterized by a particular set of rows and columns when the BPG is represented under the matrix form. Therefore, a bicluster gathers items of heterogeneous type. There exist sub-categories characterized by the possibility to overlap clusters and to assign an item to multiple clusters ( rows and columns ( clusters and strong connectivity, forming blocks when visualizing the matrix with items grouped by clusters. This clustering type is – to some extent – equivalent to ﬁnding sub-graphs forming dense structures ( without the other groups. In contrast, the literature covering the case where a cluster gathers items of the same type is limited. While this case seems to correspond to the one-way partitioning, it highly differs from it as a row (column) cluster is expressed as a mixture of column (row) clusters. In contrast, the one-way approach considers features individually. Among this group, we can mention non-negative matrix factorization (NMF) approaches ( framework than the latent block model leading to a checkerboard representation when re-ordering rows and columns by groups. Nonetheless, this approach assumes that there are latent variables connecting rows to columns; therefore we obtain the same number of row and column clusters. In this work, we particularly focus on the case where the number of row and column clusters are not necessarily equal. These approaches are more ﬂexible as the number of clusters is not constrained by the reciprocal type of items and increase the number of possible conﬁgurations. In this group, we can mention ( to ak-means, and ( unclustered items. These models require as input the number of clusters to ﬁnd, limiting their usability without prior knowledge. The work of ( count). Samples are clustered into groups without speciﬁc constraints, while features are clustered with features of the same type. The number of feature clusters is automatically inferred and independent from the number of sample clusters, leading to a ﬂexible automatic co-clustering approach. Most of these approaches are designed for very general cases and do not assume the underlying nature of BPG. More speciﬁcally, they do not consider dimension relatedness. If animals are features like [tiger, lion, frog], the distance between their respective binary vectors between tiger and lion. The different features are considered as orthogonal, while it is far from the reality where some characteristics are often correlated, even weakly. Dimension relatedness has been addressed mainly for textual applications. The works of ( external database (WordNet and Wikipedia resp.) to measure dimension relatedness. Over a very large corpus, the dimension correlation could be learned, as proposed in the work of ( available, depending on the language used, and the type of objects considered. The work of ( the similarities between features by directly measuring their textual similarity. Feature similarity is derived from the Levenshtein distance between the feature names. These different textual approaches learn the distances between features, but none between samples. One would like to apply the same treatment to samples and features in a co-clustering approach exploiting the sample-feature duality. In this article, we propose a co-clustering algorithm that addresses the problem of dimension relatedness. The proposed approach follows a co-embedding process, where each side of the bipartite graph is projected in a low dimensional space. This projection enables to measure items relatedness based on their features’ location. The process alternates between projecting each side of the graph, until invariance compared to the previous embedding. The rest of this article is organized as follows. First, the algorithm is detailed in the following section, then the datasets and evaluation methods are presented. Visual, numerical, and textual results are then presented in the experimental section, followed by possible extensions in the discussion and a conclusion. Notations: existence of a link between two nodes The representation of Sample-Feature Duality: our process inverses periodically the roles. As most of the datasets used correspond to (tag, resources) pairs, we would refer to tags T and resources R when a difference needs to be underlined. Proposed Approach: projecting the items into a low dimensional space. The samples relatedness is measured by comparing their features’ location on the embedding space. Based on their similarity, samples are next embedded into another low dimensional space using the role to start a new iteration. The process is repeated several times until the neighborhood around each item is stable. The last step of our method is an automated clustering using the Mean-Shift algorithm ( The details of each step will be detailed in this section. First, we start with the understanding of the embedding properties. Next, we detail how features’ relatedness is measured and how it is used to measure samples similarity. Last, we explain the procedure for Mean-Shift clustering concluding the co-clustering process. These paragraphs describe the general ideas behind the parametric embedding algorithm transforms it into a low dimensional representation for visualization purposes: The perplexity parameter ( highly connected items of the graph from having too many neighbors and improves the neighborhood of weakly connected items. The algorithm works by minimizing the Kullback-Leibler divergence between the image matrices repulsive long-range forces leading to well-separated groups. Another characteristic is the homogeneous scaling over an embedding, allowing to measure similarity the same way independently of the location and the crowdedness. The perplexity governs the embedding shape, where a large value focuses on large scale structures, while a lower one on details. The understanding of large is relative to the number of items, and could be adapted to tags and resources with perp As this algorithm tries to preserve local neighborhood relationships, nothing can be said on long-range distances. However, the proximity between items in the embedding can be exploited as an alternative to distances in the high dimensional space. The binary vector of a sample is used to create this vector by taking into account three factors: the feature’s location, the feature’s popularity, and the related features. The vector representing whereP p(f|s) = 1 featuref A bipartite graphG = (V, V, E)is a graph composed of two types of nodesV= {v}and }. The edgesEconnecting nodes only exist between nodes of different typesE ⊆ V× V. The is denoted |v| =δ(v, v) and deﬁned similarly for a node of V. . Therefore, the equations would be written in terms of samplesSand featuresFinstead ofVandV, as t-SNE algorithm (26) which is a key point in the process. Next, samples and features exchange their Y, obtained respectively using a Gaussian kernel and at-Student kernel. This kernel asymmetry creates and perpas they do not necessarily have the same size. represents the contribution of featurefcontained ins. The normalization constantc(s)ensures that . For a featurefofswith imagey, the kernel redistributes the feature’s mass on the neighborhood with imageybased on their kernelized distanceK(y, y). Therefore, the weightp(f|s)can be non-zero even iffis not a feature of i.e. there are no misconnected items, but the information available is incomplete. The kernel allows us to consider the unlinked items with some degree of conﬁdence based on their proximity. Kernel Choice this distribution has a long tail, allowing distant items to contribute. In the to create long-range forces for better clusters’ separation. As the goal is to identify closely related items, the use of a Gaussian kernel is more adapted, deﬁned as kernel has the advantage to be more localized and adaptable using σ. σσσ’s Choice: looking at the effective distances by: wherey The use of the median rather than the mean limits the outliers’ contribution which would enlarge σ. Thet-SNE algorithm requires as input a distance matrix. Therefore, the distances between samples are obtained by measuring the divergence between the samples’ probability vector obtained using Eq. Leibler divergence to measure items proximity, with the formulation: withp(s KL because of the symmetry of embedding Y Algorithm 1: Co-Embedding procedure Input: (perp Output: Y Data: M = {δ(r, t)} Y(0), Y for i = 1 to 2k do σ← f (Y P= {p(s)} Initialization: ﬁrst embedding with the the convergence process by starting from an organized state. In pseudocode This is an arbitrary choice and has almost no impact on the ﬁnal result. Iteration divergence matrix is obtained using these probabilities. The embedding step ﬁnish by embedding using the algorithm to obtain a new sample embedding embedding, features and samples exchange their respective roles. Thet-SNE embedding uses at-Student kernel to map items. While it seems a natural kernel choice, The perplexity impacts the distances between items within an embedding. Consequently,σis adapted by is thek-est nearest neighbors of pointy, withk = bperpeandkyk =Pyis the Euclidian norm. )andp(s)the vectors of samplesandsrespectively. We use this divergence instead of the traditional ← t-SNE(D) used in the next iteration inverting samples and features’ role. , perp): Perplexities; k: Number of iterations. , Y: Resources and tags respective embedding (pkp)]// Using Eq. (4) ) ← (Y, Y) At the start, no initial embedding exists yet, which prevents from measuring density. We initialize the At each step, the samples’ probability density is estimated over the feature’s space. Then, samples’ Ending Criterion: steps, as the algorithm does not minimize a particular criterion. Nonetheless, the process can be monitored in terms of neighborhood stability, looking at if the neighborhood around a point is unchanged over successive embedding. By denoting using the Jaccard similarity by Algorithm Complexity: divergence measurement requires embedding complexity is in and convergence speed. In total, a step described in pseudocode 1 requires O The proposed algorithm leads to clusters’ apparition when community structures exist in a dataset. Groups are extracted using the Mean-Shift (MS) clustering algorithm ( their mode’s location, which are positions with maximal probability density. The image its mode following the equation: starting with using Eq. obtained by gathering all items within a radius from their respective last embedding Y Two-Ways Clustering: dently. Nevertheless, we consider the full process as a two-way co-clustering algorithm as the embeddings are linked together,. The MS algorithm has the advantage of being parameter-free, as the kernel bandwidth embedding. The two main advantages of MS is its ability to automatically discover the number of clusters and the uniqueness of the partitioning obtained. Co-Clusters Relationships: sare embedded close to each other area if they have a similar mixture. The features of a sample are not necessarily all located in the same area. Assuming the features of features are also located in these shas no feature in one location or has features in another location out of this dissimilar. KL divergence highly penalizes couples of items where one has a mass where the other has none. If its features located in all these to the excess and deﬁcit of mass in the different areas. Depending on the strength of the asymmetry, located either in the same cluster or in two distinct clusters. A cluster can be characterized by the features clusters it is connected to and the strength of the connection. We propose to evaluate our co-embedding approach over various datasets corresponding to tags associated with resources, as tags allow us to evaluate a group content subjectively. We tried to select for each resource type two datasets to visualize the impact of different data collection processes. Some of the datasets are folksonomies, tagged by non-expert people with uncontrolled vocabulary, while the others are tagged by experts using a speciﬁc vocabulary. Table 1 summarizes the different datasets’ characteristics, such as the number of unique resources average number of tags per resources folksonomy (Folks ? column). For bibliographic tags, the BibTex part of Bibsonomy ( For images, we used the Flickr folksonomy dataset (MIRFLICKR 25) and the Corel5K ( MovieLens20M tag genome ( movies’ reviews, while MovieLens’s tags come from a controlled list. While N(Y, i)the set of thennearest neighbors ofiinY, we compare its neighborhood inY (t)andY (t + 1) ¯y = yand using the Gaussian kernelK(.)deﬁned previously, using the bandwidth parameterσestimated (3). After several iteration steps, all items may have converged in some speciﬁc locations. A cluster is used are different. Url datasets Delicious ( selected. We used the Million Song Dataset from Last.fm ( dataset. We used textual datasets for comparison. The NewsGroup20 ( 20 different topics, and the OHSUMED ( NewsGroup20 and OHSUMED datasets were gathered on https://cometa.com. The detailed pre-processing steps are detailed in 6. The largest datasets are sampled and ﬁltered before use as they do not ﬁt in memory. The sampling and ﬁltering steps are also detailed in the local appendix. We propose to evaluate the co-clusters obtained using our method over a cluster-retrieval task, where the goal is to ﬁnd the cluster where the item’s location is. Additionally, we evaluate the ability of the features clusters to serve as a unit of description. The sample’s binary vector is replaced by the proportion of features it has in each cluster, providing a compact representation. For a sample s, its description vector is: as well as for a cluster C ∈ C The compressed vectors are denoted by features and not the diffused ones here as the goal is to evaluate the partitioning obtained, and not the embedding quality. Clusters task is measured using the Mean Retrieval Rank (MRR), which quickly drops with miss-prediction. The retrieval task allows us to evaluate the partitioning relevance. This is assessed by the capabilities of sample clusters to gather items with similar connections, and by the ability of feature clusters to serve as a description unit. Cluster partitioning can help to speed up the retrieval process. A user query is often short and inaccurate, which makes impossible an exact search. The query must be compared to all By partitioning into is well identiﬁed), and reduce the search complexity from We compare our algorithm to the spectral co-clustering algorithm presented in ( representation that SVD next decomposes. Then, the singular vectors are clustered using adapted by grouping the two types of nodes separately. As this algorithm has to enter the number of clusters to be searched, we use the number of clusters found using our approach. to different results depending on the initialization seed. The algorithm is run fairness. We choose this algorithm as it has been widely used and has a strong theoretical basis but does not consider dimension relatedness. C ∈ Care sorted by increasing KL divergenceD(s, C) = KL(q(s)kq(C)). The retrieval accuracy of this Not all partitioning with and the other clusters are made of very small groups. In contrast, the retrieval task using partitioning with clusters of equivalent size is much harder, as a random choice based on the mass would lead to very poor scores. We deﬁne the partitioning entropy as the entropy relative to the clusters’ size: where P r(C) = The value in terms of entropy is non-comparable to the number of clusters as it represents the number of bits necessary to encode the information. The value is put to the power of number of cluster to|C|indicates that the clusters have a similar size. This value better measures the retrieval difﬁculty, as clusters’ size is considered. The items in the identiﬁed clusters can be ranked by relevance or representativeness. A sample is relevant if its distribution on the feature space is close to the mean distribution of the cluster to which it belongs to. We propose to score items based on their KL-divergence from their respective cluster. The KL-divergence could be expressed as codingA to C as: A score repr(s, C) ∈ [0, 1] is obtained, where 1 represents the maximal relevance. This normalization enables us to consider samples’ frequency. A low divergence between the item and the cluster is synonymous with high similarity. However, underfrequent items are likely to have a lower divergence than frequent items. By normalizing the divergence with the entropy, items are fairly compared to the cluster average. Fig. 1 and Fig. 2 represent the embedding chronology for keywords and resources respectively, for randomly selected items of the Flickr dataset. The subset is described in Table 2 and the resulting co-embedding is reused in the next experiments. t-SNE is initialized with the previous items’ location, allowing to visualize items displacement over time better. Therefore, clusters’ positions are approximately preserved over the successive steps. The ﬁrst keyword embedding (t = 0) is obtained by exploiting the document eigen-vectors obtained by SVD decomposition, while the ﬁrst document embedding (t = 1) is obtained exploiting this ﬁrst keyword embedding. Major structures emerge from the initial mass of items during the ﬁrst steps (until clusters’ shapes are reﬁned. The keyword clusters’ locations are stable over time, but clusters tend to densify from stept = 6 some split into several pieces between structure but differ in their spatial organization. The difference will be detailed in the next paragraphs. k(C) = 2∈ [1, |C|], where a value of1is synonymous with a large cluster, while a value close KL(AkB) = H(A, B) − H(A), whereH(.)is the cross-entropy that represents the extra cost of with the optimal code ofB. For a clusterC ∈ C, we deﬁne the representativeness of samples ∈ Crelatively to stept = 14, leading to well-separated clusters. Keyword clusters’ move from their previous location and Fig. 3 shows different co-embedding results for the different datasets, where samples’ characteristics are presented in Table 2. The majority presents disjoint clusters for both tags and resources. The tag embeddings differ from the resource embeddings as they all show a large central cluster. These clusters correspond to unspeciﬁc vocabulary, which occurs in all resources’ clusters. There is no such a central cluster for resources’ clusters, unless for the unﬁltered DBLP subset and Delicious. Datasets are ﬁltered to ﬁt in memory by discarding rare items as they are the less accurate and would lead to a normalization bias (5.2). We illustrate the difference between the unﬁltered (u) and ﬁltered (f) over a DBLP subset related to the payment. The ﬁltering leads to removing many infrequent tags, drastically reducing unaffected by the ﬁltering, but removing these infrequent tags that isolated some resources into small clusters. Some datasets have a different clusters’ shape and spatial organization. At the same time, most tag clusters are dense; the Corel5K has very thin clusters. This could be explained by the very low to very weak connectivity. The Bibsonomy datasets have some of their tag clusters connected to each other. These clusters gather similar vocabulary but with different formatting. By applying text processing methods (steaming and case normalization), these clusters would merge in one. The characteristics of the selected subsets used to build the co-embedding presented in Fig. 3 are detailed in Table 2. The subsets dimensions clusters ( clustersk(C clustering (SC), searching for the same number of clusters |C|. Table 2: Sample size, number of clusters, and effective number of clusters using our co-embedding approach spectral clustering (SC). As a general remark, tag clusters are less numerous than resource clusters as observed in the previous ﬁgure. This could be explained by the fact that and clusters number, as DBLP has more resources clusters than most other datasets while having the smallest the number of clusters found is likely to depend on the sample intrinsic characteristics. k (C)is almost always larger for same order of magnitude, unless for a few examples like Flickr and Last.fm with a very low and BibUrl where both k(C One reason that might explain the difﬁculty to identify balanced clusters using the raw binary matrix with power-law distribution of items. This distribution leads to a very sparse matrix where groups are difﬁcult to identify. Resource occurrences follow a power-law if some resources are more popular therefore more tagged than others, but the phenomenon is more frequent for tags because of the language organization. This might explain why lower results than CE more frequently over tags than over resources. |C|an|C|) obtained by clustering the co-embeddings (CE) with Mean-Shift, and the effective number of )measured with Eq.(8). The MS clustering results are compared to the raw binary matrix’s spectral Figure 3: Co-embedding results. The blue items on the top correspond to tag embeddings while the red ones to resource embeddings. Opacity depends on area crowdedness and items’ frequency (light color for rare items). The top 4 words of each cluster were extracted using Eq. 9 from the clusters identiﬁed previously using result, the keywords of the largest clusters (denoted Main group) and four other selected clusters are listed in Table 3. Main Group Description different contexts that the dataset cover. In some datasets, it allows to deﬁne the global scope (music genres are identiﬁed for Last.fm, medical topics for OHSUMED, or web components for BibUrl), but for the majority the media type (music, images, etc.) or the topics covered are difﬁcult to identify. Cluster Identity used in studies (where cohorts are compared), and the others concern different disciplines (related to heart, infectiology, and surgery). For Corel5K, each group corresponds to a particular picture (seaside, tundra, locomotion engines, and animals). A folksonomy includes vocabulary from users that do not necessarily share the same language. Flickr is the most representative dataset, with French, Italian, English and German words each gathered in a speciﬁc cluster. This particularity is visible on the other folksonomies BibTex, BibUrl and Delicious with English and German clusters. In a given dataset, words groups can be of various kinds, out of language consideration. For example, the Last.fm dataset has ﬁve clusters with distinct words types and ideas. The main group is about general music type while the 4th gathers sub-music types related to electronic dance music, the road-trip related ideas. Data Collection Speciﬁcity obtained differ in the ideas expressed. This could be explained in a ﬁrst place by the small overlap between the dataset’s vocabulary, but the ideas expressed differ from one dataset to another. Corel5K gathers timeless tags each representing a speciﬁc object or place, while Flickr gathers tags related to art requiring culture to be understood. The difference can be explained by the different goals expected, where Corel5K is designed for Object Recognition, while information retrieval for Flickr. The largest difference concerns IMDB and MovieLens that both correspond to movies’ descriptions with approximately the same number of tags. Movie styles are easily identiﬁed on the MovieLens dataset (horror, science-ﬁction, historical and police movies), while IMDB’ clusters group elements that occur in the same context together. Each dataset has a narrowed list of terms, corresponding to the most relevant term for IMDB, and general movies attribute ( MovieLens tag genome, explaining the difference of expressed ideas between the two datasets. Table 3: Top tags for the largest cluster (denoted Main group) and four other clusters for each dataset. Acronyms: CS: Computer Science, CVA: Common Value Auction, HCI: Human Computer Interaction, NN: Neural Networks, Mech.: Mechanism We tested the cluster retrieval tasks using the clusters obtained previously. In addition, we tried the task in both directions: searching the resource’s cluster, and searching the tag’s cluster. Table 4 summarizes the retrieval scores for this task. Table 4: Mean Retrieval Rank for resources clusters algorithm (SC). Tags clusters have higher retrieval scores than resources clusters with both approaches, ours leading to better MRRs in most cases. The good MRRs are explained by the low number of tag clusters, making the task easier with a smaller choice. The other point explaining these MRRs is many resource clusters, allowing a more detailed description vector, leading to a more accurate retrieval. The argumentation is reversed for resource cluster retrieval, leading to lower MRR scores. The Flickr dataset mentioned for its low k(C MRR(C Still, a large number doesn’t guarantee the opposite outcome, illustrated by the DBLP dataset, with an acceptable retrieval score using our approach while having the largest number of tag clusters. The gap between approaches for resource retrieval is larger, with our approach having MRR around spectral clustering has MRR around with the additional advantage of providing an intermediate visualization result. Perplexity: would look at for embedding data. The values we used need to be relatively small as we are interested in local structures to learn items similarity on it. These values are adjusted as a function on the sample size n the following way: This decision rule is very simple and allows a simple parameter selection. Nevertheless, the veriﬁcation of the produced output is suggested. A too large value would tend to connect the different clusters together, while a smaller value would lead to more clusters, which both can penalize the other embedding. Relationships between σ and perp: a larger perplexity leads to smaller distances between nearest neighbors (NN), making the choice of The direct adaptation to the embedding allows to adapt to the perplexity and to the dataset characteristics. Second, the perplexity governs the number of NN that underexploit the items’ positioning, as only a small fraction of the NN would be considered. On the opposite case wherek > perp Additionally, a larger is to select k = bperpe, simplifying σ’s choice by adapting it indirectly to the current perplexity. ) = 20.9%. A low number of tags clusters inevitably leads to a low retrieval rate of the resource clusters. As said previously, the perplexity governs the embedding shape by controlling the number of itemst-SNE , the kernel would be too large, and would put weights on items that where not considered byt-SNE. Normalization Bias: of highly popular items. In contrast, items with very few links and low frequency have their contribution enhanced. For very low frequency items, their weight is very large, leading to group artefacts, where the unfrequent item is strong enough to gather co-occurring items in a single cluster. To overcome this issue, using a different normalization scheme or ﬁltering the dataset could mitigate the apparition of artefacts. Weighted Case: able. Nonetheless, the Eq. the number of occurrences. If we have words and documents, the normalization the words count within a document is intuitive, allowing to obtain a document vector summing to contains it to obtain a word vector is non-trivial as different possibilities exist. One can normalize the word frequency of each document The ﬁrst option is more balanced, as it gives credit to all documents. Nevertheless, for documents with very few words, it would penalize the ﬁnal result because it considered inaccurate information. In that case, the second option is more suitable, as it favors the contribution of documents of sufﬁcient size. Each step of our algorithm runs in scalability problem can be bypassed by identifying clusters over a subset of items. This assume that a sampling would only affect clusters’ size, but would not affect clusters’ number. After the cluster identiﬁcation phase, the unselected items are assigned to the most relevant cluster, allowing to classify them. Then, the embedding process could be repeated over the items of a consolidated cluster, leading to a new segmentation level. Some features were previously discarded because of their low frequency within the initial subset because of the ﬁltering process. The addition of these new items allows some of these features to reach the critical frequency, therefore to consider them. Even if the items are very similar, the second co-embedding process is likely to lead to new clusters because of the new features that increase diversity. A dataset is a snapshot of a database at a given moment. A real-world graph is often dynamic, with new nodes and new edges. Rather than looking at the entire network since origin, the focus can be put over a given period to compare with another one. The proposed approach can be adapted to temporal datasets using a moving window. The ﬁrst slice is used to construct a primer co-embedding. Next, the embedding of the new window is obtained using the embeddings of the previous slice to learn items’ density. This step is possible only if the previous and current windows share some of their elements, and that new items have already seen features. Otherwise, it would not be possible to evaluate the density. By following this process, it would reduce the number of co-embedding iteration as an already stable embedding is provided to estimate items’ density. A minor difﬁculty concerns cluster tracking over the different windows, as new clusters can emerge, disappear, split or fuse together, adding some complexity. In this article, we proposed a co-clustering algorithm for bipartite graphs. The algorithm addresses dimension relatedness by projecting features into a low dimensional space, comparing samples based on their mixture of features. The embedding process leads to natural cluster formation for a dataset where community structures exist, clustered using the Mean-Shift algorithm. The algorithm is easy to conﬁgure with very few parameters to adjust. We tested our algorithm over a cluster retrieval problem. A better retrieval accuracy with more balanced clusters in a large number of cases was shown than when using a spectral co-clustering algorithm that did not consider the relationship between the dimensions. However, the weakest point of our algorithm is scalability. Nevertheless, clusters could be identiﬁed over a subset, and unused items assigned to the closer cluster. The approach could be used to optimize recommender systems and retrieval engines by working at the cluster-level rather than the item-level.