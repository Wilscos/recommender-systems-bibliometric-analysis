NSW Department of Primary Industries, Livestock Industry Centre, UNE FD McMaster Laboratory Chiswick, Armidale, NSW 2350, Australia Abstract—We demonstrate a working prototype for the monitoring of cow welfare by automatically analysing the animal behaviours. Deep learning models have been developed and tested with videos acquired in a farm, and a precision of 81.2% has been achieved for cow identiﬁcation. An accuracy of 84.4% has been achieved for the detection of drinking events, and 94.4% for the detection of grazing events. Experimental results show that the proposed deep learning method can be used to identify the behaviours of individual animals to enable automated farm provenance. Our raw and ground-truth dataset will be released as the ﬁrst public video dataset for cow identiﬁcation and action recognition. Recommendations for further development are also provided. Index Terms—cattle identiﬁcation, behaviour recognition, action recognition Australia has a reputation for safe and high-quality food. Trust in the provenance of that food - including region of origin, sustainability and ethical production - creates premium products that command higher prices and open new markets for Australian producers. Most of the provenance claims valued by consumers relate to the farm where the animals are raised. These include origin (region or country), animal welfare and sustainable practices. As part of automated farm provenance, we have conducted experiments and research to automatically detect and classify animal behaviours. This research component focuses on monitoring cattle’s activities and behaviours, particularly drinking events, using data annotation from video recordings of cattle near the water trough area as shown in Fig. 1. This information was then used to label signals from motion sensors on cattle’s collars and ear tags to train embedded machine-learning algorithms. As manual labelling of cattle behaviours from videos (and synchronising with motion sensor signals) could be signiﬁcantly labour-intensive [1], there is a need for an image-based machine learning solution to automate this process once there is enough labelled image data. The problems this project aims to solve include: video streams of cow behaviours, particularly drinking behaviour events? behaviours with cow IDs? A related work [2] proposes bounding-box cattle detection and single-frame behaviour recognition for hoof disease and estrous behaviour based on the body curvature and overlapping between bounding boxes. However this work does not identify individual cows and their subtle behaviours such as drinking versus grazing, so it is difﬁcult to know which cow is doing what. Existing approaches to identify individual cows includes using nose patterns [3] or body patterns [4] or rear-view videos [5], however these require a well-control environment and/or close-up image capture that are difﬁcult to set up in the ﬁeld. To reduce the complexity and cost of cattle behaviour recognition, motion sensors such as accelerometers and gyroscopes associated with cattle IDs [6] have been used but these lack behaviour annotation. Therefore video recording and labelling are required to generate annotations for machine learning algorithms to recognise behaviours from sensor signals [7], [8]. In this paper, we propose a solution to detect, identify and recognise behaviours of individual cattle. We collected videos from multiple cameras, annotated video for identifying individual cattle and their action, and then developed an imagebased deep-learning solution to automate the process. Source code for a baseline solution, raw videos and data generated from this project are released at https://github.com/chuong/ cattle identiﬁcation action recognition. There are a few challenges when working with cows on the ﬁeld: herd on the ﬁeld. Because the cows are all black and have similar size in our experiment, cow identiﬁcation based on body patterns [4] or nose print [3] cannot be used. Furthermore, cow identiﬁcation based on rearview and body movements [5] cannot be applied to uncontrolled environment on the ﬁeld. This in turn makes it difﬁcult to associate their activities with their ID. expert humans, to distinguish as many behaviours look similar. Furthermore, the transition between different behaviours is sometimes not clear-cut. recording asynchronously on the ﬁeld. generate a complete set of annotations. close together. The action recognition could be view dependent. As these are challenging tasks to humans, they are also challenging to train a computer to perform the same task. As a result, a number of technical solutions are used to get around these problems. The experiment was approved by the CSIRO FD McMaster Laboratory Chiswick Animal Ethics Committee with the animal research authority number 17/20. Multiple cameras ﬁsheye cameras (GoPro 5 Black) with built-in GPS receiver were installed at CSIRO Armidale site to record videos. GPS signal embedded in the recorded video provided accurate timestamps to synchronise different cameras within the uncertainty of a frame. External battery power banks were used to power the cameras throughout the day under any weather conditions. To tell the cows apart, numbers were handwritten on different parts of the cow body so that at least one of these are visible from any view angle and close enough distances as shown in Fig. 2. Fortunately, the different shapes of the limited list of numbers are good enough for a deep network to distinguish without explicit handwriting recognition. Due to long video recordings, annotation is only applied when at least one cow comes close to the main camera. Particularly, this work focuses on drinking action, all video segments containing drinking actions occurring at the water trough are marked from the videos recorded by the top camera. Then the duration of each video segment is doubled and centred on the original segment before the annotation is applied to the frames within the doubled duration. This allows to include other non-drinking behaviour to be observed and annotated without causing a signiﬁcant class-imbalanced problem. Only key frames are selected for ID annotation to save the annotation effort. Linear interpolation between the key frames gives good enough accuracy. Cow IDs and associated bounding boxes are manually annotated on the key frames and saved as a ground truth dataset for identiﬁcation. The cow ID and bounding box annotations are visualised to verify the accuracy. Behaviour annotation is performed separately and directly on videos and stored in VTT subtitle [9] ﬁles. This includes cow ID, the start & end time points of each behaviour or action. VLC player is used to play the video and verify the annotation from the subtitles. Both cow ID and behaviour annotation is shown together in Fig. 3. An object detection network is needed for cow ID detection. This could be done using popular deep network architectures such as YOLO [10] and Faster R-CNN [11]. Unlike conventional object detection and classiﬁcation where objects have distinct appearance, cows in this experiment look almost identical except the written number on their bodies. As a result, the cow detection is expected to work well, but cow classiﬁcation for identiﬁcation is challenging. As a result, Cascade R-CNN [12] was chosen as it is expected to be better at ﬁne-grained object detection and classiﬁcation. There are two approaches for action detection for cow behaviours. The ﬁrst approach is detection of single action of a cow identiﬁed by previous ID detection. This approach requires a video containing only a single cow in the middle of the image. The second approach is action recognition for multiple objects in an image. This approach includes both object detection and action recognition. Unfortunately, existing network architectures for the second approach such as YOWO [13] and MOC-Detector [14] require training from scratch on new datasets and do not train well on our cow dataset. One reason is the limited size of the dataset. Another reason is that the cow actions look very similar to each other, and their heads point toward the ground most of the time and make their actions look very similar. This makes it difﬁcult for the second action recognition approach to distinguish similar cows and recognise their actions at the same time. As a result, we have to rely on the ﬁrst approach. One candidate for action recognition is Temporal Segment Networks (TSN) [15] which recognises action from a stack of frames, with optional input of optical ﬂow between consecutive frames. An experiment was carried out from 18/03/2020 to 27/03/2020. Every day, eight cows with painted IDs on their sides and heads to distinguish them were released into the monitoring area. A water trough with automatic reﬁlling mechanism was installed on the side of the patch and connected to a water supply. The experiment protocol was approved by the Animal Research Authority. Three GoPro Hero 5 Black [16] cameras were mounted on poles next to the water trough to focus on drinking behaviour events. The cameras were powered by large power banks connected via USB-C cables. Videos were recorded from the 3 cameras for 8 hours a day for 8 days. The views from three cameras next to the water trough are shown in Fig. 2. The middle camera view shows clear separation between cows. As a result, video annotation was carried out on the videos recorded from this camera. GPS data embedded in GoPro videos ﬁles could be extracted to obtain time stamp using FFMPEG [17] and GoPro-Utils [18], [19]. Commands ffmpeg and gpmd2csv are used to extract GPS timestamp and location from GoPro videos. Annotation for cow identiﬁcation was performed using CVAT [20] annotation tool which could export to different data formats for deep learning. This annotation was carried out on the videos recorded for the 8 days of the experiment and exported to COCO dataset format. Training, validation and testing data are split in 0.70:0.05:0.25 ratio. Annotation for cow behaviour was performed using VLC video player [21] and VTT subtitle [9]. Cow ID and behaviour annotations are processed using a custom Python script to export to KINECTICS [22] dataset formats. Each cow is annotated with a bounding box and an ID using CVAT [20]. The annotation was performed on key frames, 1 for every 9 frames from the video. Linear interpolation is performed to obtain annotation for other frames between the key frames. The behaviour is recorded as VTT subtitle [9] ﬁles which consist of time duration to the accuracy of second, for example: 0:05:11.000 --> 0:05:23.000 Cow 2 Drinking 0:05:17.000 --> 0:05:42.000 Cow 4 Other A visualisation of the combined ID, bounding box and behaviour annotations is shown in Fig. 3. The annotations were exported to different formats including COCO [23] and KINECTICS [22] depending on the required input of deep learning platform. For behaviour recognition, video clips of each action are extracted to KINECTICS400 format with frame size of 256×256 pixels. This is achieved by converting cow rectangular bounding boxes squared shape and scaled to this image size as shown in Fig. 4. Additional zero padding is added for regions outside the input image so that the cow is mostly in the middle of the frame. A new cow behaviour dataset containing a single cow in each video frame following the format of Kinectics400 has been created. A total of 1715 videos were exported where ”Drinking” class has 360 video, ”Grazing” 413 videos and ”Other” 942 video. Again training, validation and testing data are split in 0.70:0.05:0.25 ratio. Among different deep learning frameworks, we chose MMDetection framework [24] which is based on PyTorch and supports multiple object classiﬁcation and detection architectures and different data format. Cascade R-CNN shown in Fig 5 was trained with cow ID (8 classes) and bounding box annotation as ground truth data. After training Cascade R-CNN on the cow’s COCO dataset, the testing yielded average precision of 0.812 and average recall of 0.844. For comparison, the testing accuracy achieved by [4] is 89.95% on their private dataset which consists of cropped images captured from a relatively ﬁxed view from a cow, while our cow images were captured at an arbitrary view. We chose Temporal Segment Networks (TSN) [15] without temporal information from MMAction2 library [24] to train on KINECTICS [22] data export from our annotation. Finetuning a TSN action recognition model (pretrained on Kinectics400) over 100 epochs with learning rate of 8e-5 and decay of 1e-4 leads to an average accuracy of 0.72. More details of testing is shown in Table I. The accuracy of ”Drinking” and ”Grazing” actions are quite high, while the accuracy of ”Other” action is quite low. This is due to the fact that the ”Other” videos contains some ”Drinking” and ”Grazing” actions that were missed out during annotation process. To process a video stream to obtain cow ID and their behaviour, we combine the two networks into a single pipeline as shown in Fig. 7. The ﬁrst network takes a video frame and detect all cows and their bounding boxes. The second network takes a 1-second cropped video based on the bounding boxes of each cow and classiﬁes its behaviour. The output of the pipeline is cow ID and the conﬁdence of each behaviour as shown in Fig. 7. This research has led to three important outcomes. First, a camera setup that could be used not only to acquire videos to annotate motion sensor data, but also to directly monitor cow welfare such as shy feeding. Second, annotated video data of cow IDs, bounding boxes and behaviours as ground truth for machine training based on the videos and other synchronised sensor data from collars and ear tags. Finally, a processing pipeline with two trained cow ID detection and behaviour classiﬁers that can automatically label new video streams captured from similar views. We demonstrated a working prototype that can automatically monitor cow welfare on farm using low-powered embedded sensors and edge machine learning. This system will play a major role in evident-based quality control and automatic compliance in supply chain. There are several improvements for this system to be made for the deployment in production and large-scale environments: ited to a ﬁxed and limited number of cattle (8) that are labelled and used to train a network. In reality, the number of cattle could be 100s or 1000s, so the current approach of using handwritten numbers to distinguish cows may not be practical. A possible approach is zero-shot identiﬁcation and/or continuous 2D or 3D tracking. Additional sensors could be utilised to re-identify cattle. though videos from 3 or more cameras were captured, only videos from Camera 1 were labelled and used in this study. Single view identiﬁcation is not sufﬁcient when there are more cattle and occlusion is unavoidable. Multiview camera approach will solve this problem and potentially improve the accuracy. generated from multiple cameras is large and difﬁcult to manage and synchronise. There is a need for a dedicated management system and a user interface to allow a user to navigate the data at any timestamp and across all available videos. This will also facilitate multiview data annotation and quality control. This research was funded by Science and Industry Endowment Fund https://sief.org.au. We acknowledge the following technical staff who have contributed to the research at CSIRO FD McMaster Laboratory Chiswick: Katie Austin (Technical Ofﬁcer), Alistair Donaldson (Technical Ofﬁcer) and Reg Woodgate (Senior Technical Ofﬁcer) with NSW Department of Primary Industries, and Jody McNally (Research Technician) and Troy Kalinowski (Technical Ofﬁcer) with CSIRO Agriculture and Food. Thanks are also due to Dr Ron Li and Mr Wyman Huang from Imaging and Computer Vision Group of CSIRO Data61 for their support.