{bencheng.ybc,pengjie.wpj,vjinquan.ljq,kuang-chih.lee,xiyu.xj,bozheng}@alibaba-inc.com,lwsaviola@163.com Nowadays, deep learning models are widely adopted in web-scale applications such as recommender systems, and online advertising. In these applications, embedding learning of categorical features is crucial to the success of deep learning models. In these models, a standard method is that each categorical feature value is assigned a unique embedding vector which can be learned and optimized. Although this method can well capture the characteristics of the categorical features and promise good performance, it can incur a huge memory cost to store the embedding table, especially for those web-scale applications. Such a huge memory cost signicantly holds back the eectiveness and usability of EDRMs. In this paper, we propose a binary code based hash embedding method which allows the size of the embedding table to be reduced in arbitrary scale without compromising too much performance. Experimental evaluation results show that one can still achieve 99% performance even if the embedding table size is reduced 1000×smaller than the original one with our proposed method. • Information systems → Recommender systems. Embedding learning for categorical features plays an important role in embedding-based deep recommendation models (EDRMs) [3,7,23]. A standard method, often referred to as full embedding, for embedding learning is to learn the representation of each feature value [19]. Specically, let𝐹be a categorical feature and|𝐹 |be its vocabulary size, each feature value𝑓∈ 𝐹is assigned an embedding index𝑘so that the𝑘-th row of the embedding table𝑊 ∈ R is the embedding vector of𝑓, where𝑁 = |𝐹 |in the full embedding method and 𝐷 is the embedding dimensionality (see Fig 1 (a)). However, such full embedding learning suers from severe memory cost problems. Actually, the memory cost of the embedding Figure 1: Comparisons of dierent embedding methods. Step 1, 2, and 3 refer to feature hashing, embedding index generation, and embedding generation respectively. table is𝑂 (|𝐹 |𝐷)which grows linearly with|𝐹 |. For web-scale applications, one may need to store a huge embedding table since the vocabulary size may be millions or even billions. For example, suppose|𝐹 | =500 million and𝐷 = 256, the corresponding memory cost will be 475GB. In practice, such a huge cost becomes a bottleneck in deploying EDRMs in memory-sensitive scenarios. Therefore, it is crucial to reduce the size of the embedding table [2,19]. In this paper, we highlight two challenges: (1)Challenge one: Flexibility. The memory constraint varies with dierent scenarios (from distributed servers to mobile devices). The embedding reduction methods need to be exible enough to meet dierent memory requirements. Especially for mobile devices, a tiny EDRM is needed to meet the limited memory requirement. (2)Challenge two: Performance Retention. Since a big model usually has a better capacity and hence a better performance, embedding reduction may bring a performance gap due to the fewer parameters used in the reduced model. Hence, how to keep high performance when the memory size is reduced is a big challenge, especially for the memory-sensitive scenarios (e.g., in mobile devices). In general, there are two directions to reduce the embedding table size, i.e., reducing the size of each embedding vector and reducing the number (i.e.,𝑁) of the embedding vectors in an embedding table. The embedding table size of the former methods (e.g., product quantization [5,9], K-D method [2,11,13,15,20], and AutoDim [6, 10,16,17,28,29]) is still linearly increased with|𝐹 |, failing to tackle the memory problem caused by a large vocabulary size in web-scale applications [19]. Hence these methods are not considered in our paper. For the latter methods, they typically apply a mod-based hash embedding to reduce𝑁. The key idea of them is to apply modulo operation on the unique Hash ID of each feature value, i.e., focusing on Step 2 in Fig 1 (b). For example, Hash embedding [24] Table 1: Comparison about embedding methods. takes the remainder of the Hash ID divided by𝑀as the embedding index, reducing the embedding size from𝑂 (|𝐹 |𝐷)to𝑂 (𝑀𝐷). The problem of this method is that dierent feature values may have the same embedding index and hence the same embedding vector, leading to poor performance. Multi-Hash (MH) [22] adopts multiple embedding indices for one feature value, reducing the collision rate among feature values. But dierent feature values may still be indistinguishable especially for a tiny model, failing to the challenge two. Q-R trick [19] uses both the remainder and the quotient as embedding indices to identify a feature value. However, Q-R trick fails to the challenge one since its minimal reduced size is related to|𝐹 |rather than any scales. Although the generalized Q-R tries to address this problem, it needs a lot of eort to design the divisor [19]. The comparisons are summarized in Table 1. In this paper, unlike the existing methods which adopt a modulo (collision) operation, we bring the idea of binary code (e.g., the binary code of integer 13 is1101) which is unique for dierent Hash ID and propose a binary code based hash embedding method to tackle this reduction problem (see Fig 1 (c)). Specically, we rst binarize the Hash ID into a binary code. Then, to address the challenge one, we propose a code block strategy and reduce the embedding table size by adjusting the code block length exibly. To address the challenge two, the generated embedding index is designed to be unique for dierent feature values at any reduction ratios. The uniqueness at any reducing ratios allows EDRMs to distinguish dierent feature values, leading to a good performance even for a tiny model. Furthermore, Step 2 of our method is a deterministic and non-parametric process and can be computed onthe-y. This property is friendly for EDRMs both on the convenient application and handling new (out-of-vocabulary) feature values. We also note that we are aware of some recent works using similar terms such as learning binary embedding [8,14,26]. We want to point out that they are in totally dierent contexts. In these works, binary refers to that each element in an embedding vector is a binary number for a fast similar embedding search. While in our work, binary refers to binarize the integer ID into a binary code. To summarize, the main contributions are listed as follows: (1) We propose binary code based hash embedding, a simple but eective embedding method, to reduce the embedding table size and keep a high performance at the same time. (2) A code block strategy is presented to adjust the embedding table size exibly and a lossless embedding index generation process is elaborately designed to allow the model to distinguish dierent feature values and achieve better performance. (3) Experimental results on large-scale realworld datasets show that with the help of the proposed method, the model size can be 1000×smaller than the original model, and keep 99% performance as the original model achieves at the same time. In this section, we introduce the framework (see Fig 2) of our methods. In general, we also adopt three steps as introduced in Fig 1. Figure 2: The framework of the proposed method. In practice, the raw categorical feature values may be represented as various types, such as String and Integer values. To handle dierent types of categorical feature values, in practice, a feature hashing [19,22,24] is rstly applied to map these raw feature values into a uniformed integer number, called Hash ID (see Fig 2). Formally, the feature hashing process can be expressed asℎ= H (𝑓)where Hrefers to a hash function (e.g., Murmur Hash [25]) andℎis an integer number, called the Hash ID of𝑓. In practice, the output length ofHis always a large value (e.g.,ℎis a 64-bits integer) to make the collision amongℎas small as possible. In this case,ℎ can be basically taken as a unique ID for dierent 𝑓[12, 24]. In this section, we introduce the embedding index generation process including binarization, code block strategy, and decimalization. 2.2.1 Binarization. After Step 1, each feature value𝑓is mapped toℎ, which is basically regarded as a non-collision mapping due to the large output space [12,24]. Then the binary code𝑏∈ {0, 1} (where𝑆refers the binary code length) of𝑓can be generated by transforming this uniqueℎto a binary form (e.g., the binary code of integer 13 is 1101). Note 𝑏is also unique for dierent 𝑓. 2.2.2 Code Block Strategy. To allow the model can exibly reduce memory, we propose a novel strategy called code block strategy. Generally speaking, the code block strategy divides each 0-1 value in𝑏to dierent blocks. Then, the ordered 0-1 values (i.e., 0-1 code) in each block can represent𝐾 = 2unique integers where𝑛is the number of 0-1 values in this block (see Step 2.2 in Fig 2). If we take the decimal form of 0-1 code in each block as an embedding index and map each index to an embedding table𝑊 ∈ R, the size of the embedding table can be exibly adjusted by𝑛. For example, when𝑛 = 1(i.e., the number of 0-1 values in each block is 2), the embedding table size is𝑂 (2𝐷). When all 0-1 values in𝑏are arranged into one block, the embedding table size is𝑂 (|𝐹 |𝐷)(i.e., full embedding). In other words, by controlling the value of𝑛, we can adjust the embedding table size to meet various scenarios (from distributed services to mobile devices). Formally, we dene𝐵= [𝐵; 𝐵; ...; 𝐵; ...]as the sequential code blocks produced by a code block strategy on𝑏, and|𝐵|refers to the number of blocks. Then the𝑚-th code block𝐵∈ {0, 1} can be represented as Figure 3: Code block strategy examples. The binary code 𝑏= 100100110101100111000011. The 0-1 values plotted with the same color in each case are divided into the same block. where the functionAllocis the allocation function which allocates each 0-1 value to dierent blocks.Orderis a function which gives an order for the 0-1 value in each block and generates a 0-1 code for each code block. Here we give two code block strategies (including Succession and Skip) as examples to show how it works (other possible strategies can also be allowed). Succession.As shown in Fig 3 (a), the succession strategy puts the 𝑡successive 0-1 values in a binary code into the same block. The Orderfunction keeps 0-1 values in the same relative position in𝑏. Note if the number of the last 0-1 values in𝑏is less than𝑡, all of the left values are divided into a new code block. Skip.As shown in Fig 3 (b), if the number of interval values of two 0-1 values in a binary code is𝑡, they will be divided into the same block. The Order function is the same as that in Succession. Note given one of the above code block strategies, we can obtain a unique sequence of code blocks𝐵for the binary code𝑏. This property guarantees the process of code block strategy is lossless. 2.2.3 Decimalization. The embedding index of each block can be obtained by decimalizing𝐵(e.g.,𝐷𝑒𝑐𝑖𝑚𝑎𝑙𝑖𝑧𝑒 (1101) = 13), i.e., 𝑘= 𝐷𝑒𝑐𝑖𝑚𝑎𝑙𝑖𝑧𝑒 (𝐵)where𝑘is the embedding index of When obtaining multiple indices for𝑓, to get its embedding, two steps are proposed, i.e., embedding lookup and embedding fusion. 2.3.1 Embedding Lookup. As introduced above, each code block 𝐵in𝐵can obtain an embedding index𝑘. The number of blocks is|𝐵|, leading to a total of|𝐵|embedding indices. Then we can map each embedding index into an embedding vector, i.e., 𝑒= E (𝑊, 𝑘)where𝑊is a embedding table,𝑒refers to the embedding of𝐵andEis a embedding lookup function which usually returns the𝑘-th row of𝑊. In practice, keeping |𝐵|embedding tables for dierent𝐵may also cost a lot memory consumption. Therefore, it is common to keep a single embedding table and share this table among all 𝐵[22]. 2.3.2 Embedding Fusion. To generate the nal embedding vector 𝑥of 𝑓, an embedding fusion function 𝑔 is applied, The design of the fusion function can be various, such as pooling, LSTM, concatenation and so on. In this paper, by default, we adopt sum pooling as the fusion function (others can also considered). 2.4.1 Desiderata. There are several key desiderata of our method, which EDRMs can be beneted. (1)Determinacy.The indices generation is a deterministic and non-parametric process. It is computed on the y, making it simple to practical implementations and friendly to new feature values. (2)Flexibility.The size of embedding table𝑊 ∈ Ris mainly determined by𝑛(i.e., the number of 0-1 values in each code block). It means the memory reduction ratio can be exibly adjusted from2/|𝐹 |to 1 (assuming adopting embedding table sharing strategy). This benets EDRMs can be developed on memory insensitive scenarios to sensitive scenarios. (3) Uniqueness.No matter what the reduction ratio is,𝐵is unique for each feature value. This enables the model to distinguish dierent feature values and further improve the model performance. 2.4.2 Sub-collision Problem. We should point out although𝐵is unique, there may exist sub-collision among two feature values (e.g., 𝐵≠ 𝐵but𝐵= 𝐵), called sub-collision problem. Actually, it is an open problem which exists in most mod-based hash methods [19,22,24]. We leave it as one of the future work. In practice, a hash function (e.g., Murmur hash [25]) which can randomly map values to a large space is used to relieve this problem. 2.4.3 The Relation with Existing Methods. Here, we discuss the relation between ours and other methods. (1)Full Embedding.Both of full embedding and ours can distinguish dierent feature values. Besides, our method has the ability to reduce memory exibly. (2) Hash Embedding.It is a simplied form of ours, where the code block strategy is Succession, and only the rst top𝑡0-1 values are used as the embedding index. (3)Multi-Hash Embedding.Both of them create multiple embedding indices. But our method goes further, i.e., keeping a uniqueness constraint for these indices. (4) Q-R Trick.Q-R trick is a special case of our method. When we utilize Succession and the block number is set to 2. The rst top𝑡 0-1 code and the left 0-1 code can be taken as the quotient and the remainder in Q-R trick respectively. Datasets.(1) Alibaba is an industrial dataset which is obtained from Taobao. There are a total 4 billion samples, 100 million users. (2) Amazonis collected from the Electronics category on Amazon. There are total 1,292,954 samples, 1,157,633 users. (3) MovieLens is a reviews dataset and is collected from the MovieLens web site. There are total 1,000,209 samples, 6,040 users. Baselines.(1) Full Embe dding (Full) is a standard embedding learning method. (2) Hash Embedding (Hash)[24] applies the modulo operation on the Hash ID to obtain an embedding index. (3) MultiHash Embedding (MH) [22] applies multiple hash functions to the feature value to obtain multiple indices. (4) Q-R Trick (Q-R) [19] take both the remainder and the quotient as indices. Training Details.All methods have the same EDRM architecture. The embedding dimensionality is also set the same for all methods. The methods (i.e., MH, Q-R trick, and ours) employ embedding table sharing strategy in dierent indices for memory reduction purpose and take sum pooling as the fusion function. For MH, we use 2 hash functions as suggested by authors [22]. For our method, the code block strategy is Succession. We use the Adagrad optimizer with a learning rate of 0.005. The batch size is 1024 for all datasets. Table 3: The results of memory size when all the methods archive 99 % performance as the full embedding method achieves in AUC score. Top 3 Figure 4: Convergence of dierent methods. We conduct experiments on CTR prediction tasks and compare the performance with dierent memory reduction ratios of the full embedding. AUC (%) [4] score is reported as the metric. Note 0.1% absolute AUC gain is regarded as signicant for the CTR task [3, 21, 30]. Results are shown in Table 2. BH refers to our method. Comparison with Mod-based Hash Embedding Methods.In general, BH performs best on all cases, and the gain gap between BH and baselines is increased for a smaller model. For example, compared with Q-R on MovieLens, BH can achieve 0.24% gains when the reduction ratio is 37.5 %. While when the ratio becomes 0.1 %, the gain gap is increased to 2.28 %. It indicates that due to the nice properties of BH (see Section 2.4.1), BH can better represent each categorical feature value, especially for a tiny model. Comparison with the Full Embedding.We can observe that: (1) Since the full embedding method contains signicant parameters, it gets better performance. (2) In most cases, BH can obtain competitive performance with the full embedding method. In this section, we conduct experiments to evaluate the model size of all methods when achieving similar performance. Specically, we take the dataset Alibaba as an example due to its closeness with the web-scale application. Then we report the model size of dierent embedding methods when they achieve 99% performance as the full embedding method achieves in AUC score. Besides, to further evaluate the reduction ratio, we also report the size of the embedding table of the top 3 largest for each method. The results are shown in Table 3. Some observations are summarized as follows: (1) Table 4: The results of dierent code block strategies. Compared with other embedding methods, when all of them archive similar scores, BH can cost the smallest memory size. (2) Compared with the full embedding method, BH can adopt an extremely tiny model (i.e., 1000×smaller) to achieve 99 % performance. Such a small model with high performance is urgently needed to develop EDRMs on a memory-sensitive scenarios (e.g., mobile devices). In this section, we evaluate the performance of the proposed two code block strategies, i.e., Succession and Skip. To have a fair comparison, we keep the same reduction ratio for these two strategies. Table 4 shows the results. Note we also provide the best performance (Q-R) among baselines for comparison. We can observe that Succession and Skip achieve similar performance overall datasets, and perform better than the best baselines. It indicates the uniqueness of code block strategy is helpful to improve the embedding performance no matter what kind of code block strategies we choose. We conduct experiments to analyze the convergence of dierent models. Specically, we keep the same reduction ratio for all methods and report the AUC and the loss value of these methods on test data of MovieLens within 100 epochs (similar conclusions can be found in other datasets). As shown in Fig 4, we can nd: (1) Compared the AUC and loss curves of mod-based hash embedding methods, BH converges faster than that of other baselines. Furthermore, BH can achieve a higher AUC score and a lower loss value. It demonstrates the eectiveness of our method when reducing EDRMs into a small-scale size. (2) Due to more parameters adopted in full embedding, it archives the best performance. But, BH can also achieve competitive performance compared with full embedding. In this paper, to tackle the memory problem in embedding learning, we propose a binary code based hash embedding. A binary code is rstly generated to guarantee a unique index code. Then a code block strategy is designed to exibly reduce the embedding table size. Finally, the feature embedding vector is obtained by combining the embedding vectors from dierent code blocks. Experimental results show that even if the model size is 1000×smaller, we can still obtain the 99% performance by binary code based hash embedding.