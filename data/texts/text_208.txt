State Key Laboratory for NovelPeking University Shenzhen Graduate Software Technology, NanjingSchool, China zhenzhiwang@outlook.com Multi-modal Ads Video Understanding Challenge is the rst grand challenge aiming to comprehensively understand ads videos. Our challenge includes two tasks: video structuring in the temporal dimension and multi-modal video classication. It asks the participants to accurately predict both the scene boundaries and the multi-label categories of each scene based on a ne-grained and ads-related category hierarchy. Therefore, our task has four distinguishing features from previous ones: ads domain, multi-modal information, temporal segmentation, and multi-label classication. It will advance the foundation of ads video understanding and have a signicant impact on many ads applications like video recommendation. This paper presents an overview of our challenge, including the background of ads videos, an elaborate description of task and dataset, evaluation protocol, and our proposed baseline. By ablating the key components of our baseline, we would like to reveal the main challenges of this task and provide useful guidance for future research of this area. In this paper, we give an extended version of our challenge overview. The dataset will be publicly available at https://algo.qq.com/. • Computing methodologies → Activity recognition and understanding; Video segmentation. Multi-modal Video Analysis, Temporal Segmentation, Multi-label Classication. ACM Reference Format: Zhenzhi Wang, Liyu Wu, Zhimin Li, Jiangfeng Xiong, and Qinglin Lu. 2021. Overview of Tencent Multi-modal Ads Video Understanding Challenge. In Tencent Data Platform, Proceedings of the 29th ACM International Conference on Multimedia (MM ’21), October 20–24, 2021, Virtual Event, China. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3474085.3479222 As a leading marketing platform in China, Tencent Advertising Group gathers massive business data, marketing technologies and professional service capabilities. The goal of advertising is to convey the right information to the target customer with an appropriate price. Due to the richer contents, better delivery and more persuasive eects of the video-form ads than the image/text-form ads, the percentage of video-form ads has an explosive growth at the era of 5G. Therefore, the understanding of ads videos becomes signicant and urgent. In addition to a simple video classication task, a comprehensive ads video analysis temporally has an even more signicant impact on many chains of the ads system. For example, 1) By understanding the detailed content of ads videos, a single ads video can be automatically edited to have various lengths in order to t dierent ads platforms; and 2) The ne-grained categories of each video segment rather than the coarse video-level categories can better serve ads recommendation. By inspecting the statistics of the characteristics in the massive ads videos from the perspective of presentation forms or narratives, we build a comprehensive taxonomy for categories in ads. For example, some characteristics focus on temporal information such as ‘golden 3 second’; while some focus on audio information such as ‘pop music’ and ‘single vocal’; the others focus on semantic information such as ‘ancient china’ and ‘indoor scenes’. These complex semantic representations motivate us to build a class hierarchy with four dimensions: ‘presentation form’, ‘style’, ‘place’ and ‘narrative’: classes in presentation forms mainly describe the ads videos from the perspective of video making, audio or typesetting, while those in the style from the perspective of age, emotions, theme or character relationships. Other categories in place are about locations and those in narratives are about the temporal structure of ads. Even after the analysis of characteristics and abstraction of ads videos’ paradigm, ads video understanding is still too complex for deep models to learn and predict. To make the goal of ads video understanding more specic, we remove the categories which are too abstract and let our challenge focus on three key perspectives: temporal segmentation, multi-modal information, and multi-label Figure 1: The visualization of our Ads Video Structuring task. By dening semantically-consistent segments as ‘scene’, our task requires precisely segmenting videos as scenes and predicting scene-level multi-label categories. Figure 2: The visualization of video classication with multimodal information. Ads videos commonly contain three modalities: RGB frames, audio, and text (extracted from ASR/OCR). A simple baseline is to extract features from each modality and perform a classication after modality fusion. classication. Ideally, we want to know the details of each frame in the videos. Yet, the properties of ads videos, such as fast-pace, a large amount of information yet short length, make the gathering of all fragments’ details impossible to guide the production of future ads videos. Therefore, we dene the concept of ‘scene’, which is a ‘super shot’ in the semantic-level. A scene commonly contains one or some consecutive shots and conveys high-level semantics. For example, the rst scene in Fig. 1 consists of 4 shots, shifting between the couples in the street and the customer service. However, all the 4 shots are about making a phone call, so we group them into a scene. By detecting the boundary of our dened scene, a useful structure of ads video is obtained. Meanwhile, a comprehensive category hierarchy is built, where dierent categories are related to dierent modalities. The most important modalities for our task are video frames, audios, and texts obtained from ASR/OCR. We propose the multi-modal ads video structuring task as the main track of our ACM Multimedia 2021 grand challenge ‘Multimodal Ads Video Understanding’. It has three main characteristics: 1) accurate temporal localization of scene boundaries; 2) multimodal information is essential; 3) a comprehensive category hierarchy from four dierent perspectives for multi-label scene-level classication. We believe that our task can be a good representative of ads video understanding in the ads system. Based on the analysis of the proposed task above, we collect a large scale ads video dataset with accurate frame-level scene boundary annotations and multi-label category annotations for each scene, named Tencent Ads Video Structuring (Tencent-AVS) Dataset. This high-quality dataset enables the exploration of our proposed task. Our challenge also has a simplied version as track 2 of our challenge, i.e., video-level multi-label classication task. Next we will mainly introduce track 1 (video structuring task) due to that track 2 is a subset of track 1. We will give an elaborate description of our proposed benchmark and the baseline model in the following sections: In Sec. 2, we will review the topics related to our task and analyze the feasibility of the baseline model. In Sec. 3, we will give a detailed description of taxonomy and annotation process of our Tencent-AVS Dataset, and also analyze the proper metrics for our task. In Sec.4 and Sec.5, we propose our used baseline model and ablate its key components. Due to the fact that the multi-modal video structuring task is motivated by the need of real-life applications, this task has no yet been explored by the computer vision community before. The most related task is the temporal action segmentation task, yet we have three major dierences from it: 1) multi-modal information instead of only RGB frames, 2) there are shot changes in ads videos, and 3) multi-label classication with a long-tailed distribution of categories. In the following subsections, we will rst introduce recent progress in temporal video segmentation tasks, and some recent video classication methods as well as utilizing multi-modal information in videos. Then we will analyze the feasibility of several choices for our task’s baseline. Temporally segmenting videos to get the video parts of interest has been explored by the computer vision researchers for several years. A typical setting of video segmentation is temporal action segmentation task which aims to parse the whole video into dierent human actions. Previous datasets commonly contains instructional videos [27,45,48] with human performing diverse actions. Temporal action segmentation requires the model to predict a action label for each frame with only visual information. Recently, another setting of video segmentation is proposed to tackle the movie segmentation problem [34]. Dierent from action segmentation task, movies are usually much longer (e.g., 2 hours ) than instructional videos collected in YouTube (e.g., 5 min), yet temporal segmentation of movies is based on the pre-extracted shots and the evaluation of ‘scene’ segmentation is shot-level instead of frame-level. 2.1.1 Temporal Action Segmentation. They usually model the temporal relations and predict action categories for each frame upon densely sampled features extracted by o-the-shelf classication backbones (e.g, I3D [7]). For example, ED-TCN [28] is a temporal convolution network for action segmentation with an encoderdecoder architecture. TDRN [29] introduced deformable convolutions and a residual connection into [28]. MS-TCN [16] utilized dilated temporal convolution in action segmentation for capturing long-term dependencies and operated it on the full temporal resolution. These works mainly focused on improving receptive eld for modeling long-term dependency with encoder-decoder structure [28,29], dilated convolution [16]. Recently, BCN [54] and ASRF [25] propose to use detected action boundaries to handle over-segmentation errors. Our proposed ads video structuring task contains multi-modal information which cannot be sampled densely, therefore the per frame classication scheme in action segmentation could not apply to ours. Besides, ads videos have a lot of shot change, which may brings more challenges to temporal segmentation methods. 2.1.2 Scene Segmentation in Movies. Scene segmentation (or scene boundary detection) with supervised setting is proposed by Baraldi et al. [4] and Rotman et al. [36] in short videos or documentaries, and scaled up to shot-based long-form movies by Rao et al. [34]. In addition, Chen et al. [9] used unsupervised contrastive learning to obtain more discriminative features for scene boundary detection. This task requires temporal models to predict whether each shot boundary is a scene boundary, therefore it is a binary classication problem. This task also involves multi-modal information, such as place, actor, action and audio. Its evaluation is mAP for binary classication without any pre-dened categories. Therefore our task diers from it due to our multi-label classication with a comprehensive category hierarchy from three perspectives. 2.2.1 Action Recognition with Single-modality. Video classication is usually investigated in the form of action recognition. Typical methods mainly lie in two categories: 1) two-stream network with RGB frame and optical ow as input to capture appearance and motion information respectively [42,53]; 2) 3D convolution networks to directly model appearance and motion with 3D convolution lters, such as I3D [7], S3D [55], and X3D [17]. Recently, methods with self-attention or transformers [15,32] show advantages in modeling long-range dependency and also achieve state-of-the-art performance on standard benchmarks. However, they mainly focus on human action and model temporal relations via only single modality, i.e., visual features. 2.2.2 Multi-modal Information in Videos. Recently, multi-modal data is exploited to enrich video representations due to the multimodal nature of videos. MMT [18] uses a multi-modal transformer to jointly encode dierent modalities in videos, which shows superior performance than previous methods with simple concatenation or channel-wise product as multi-modal fusion. Other methods utilize multi-modal data as supervision signals for pre-training [1,3, 46]. Dierent from previous action recognition benchmarks, our proposed ads video structuring dataset has rich multi-modal information such as audio and text extracted from both ASR and OCR. How to eectively utilize multi-modal data in ads videos becomes an challenging problem and deserves further investigations. Based on the analysis above, we conclude two possible methods for our ads video structuring task. 2.3.1 End-to-end Methods. End-to-end methods ignore the shot change and directly segment scenes and also classify them with the video features captured by 3D-CNNs [7,51] or two-stream network [53]. This approach is more practically direct and conceptually simple. Yet it still has some problems: 1) A dense sampling of locations cannot enable the employment of multi-modal information due to the minimum length required by audios or the inaccurate ASR/OCR timestamps, while a sparse sampling may lead to bad performance due to the approximation errors in training and the sparse predictions in inference. The dense sampling of sliding windows will also lead to huge computational costs; 2) The problem of long-tailed distribution becomes even more serious due to the segmented units for classication; 3) Whether the video feature backbone can capture useful informations when crossing the shot change is not sure. 2.3.2 Multi-stage Methods. Due to a lot of shot changes in ads videos, video classication backbones [7,51,53] may be confused when crossing the shot boundaries. To ensure a more stable performance, we decompose the task into three parts and optimize them respectively: 1) shot boundary detection, 2) scene segmentation (a.k.a., scene boundary detection) by aggregating shots, and 3) scene-level classication. Firstly, shot boundary detection methods [40,43,44] determine whether each frame is a shot change by comparing the dierence between adjacent frames. The major diculty of shot detection is that the gradual transition eects in ads videos are hard to detect. Secondly, scene segmentation methods [34] commonly use a temporal modeling network such as convolution network or a BiLSTM [38] to capture high-level semantics to aggregate shots as complete semantic units (i.e., ‘scene’ in our task). Dierent from end-to-end methods, shot-level multi-modal information can be utilized in the scene segmentation stage now due to larger length of the minimum units (i.e., a shot rather than a single frame). Finally, a multi-label classication method can be used to predict the categories of each segmented ‘scene’ with multi-modal features. As for multi-modal modeling, a BERT family of models [13,37] has been proven as eective to capture semantics of texts; VGGish [22] uses a CNN architecture to model audio waveforms and achieve promising performance; various video feature backbones mentioned above [7,51,53] can eectively capture both the appearance and motion cues. The features extracted from each modality will be fused in many ways, such as concatenation or cross-attention [52], to perform the nal classication. The major diculty for scene-level classication is the long-tailed distribution of the categories from four dierent perspectives. To decouple the inuence of dierent stages and get a more stable performance for Figure 3: The taxonomy tree of Tencent-AVS dataset. our proposed new task, we adopt the multi-stage approach as the baseline of our challenge. As mentioned in Sec. 1, we propose the ads video structuring and multi-modal video classication tasks, which we believe is a good representative of ads video understanding. In this section, we will introduce our proposed Tencent Ads Video Structuring (TencentAVS) benchmark. To make it solid, we collect large-scale ads videos from real business data and annotate the scene boundaries and the categories with careful quality controls. We also examine many previous metrics and choose proper ones to evaluate methods’ performance on our benchmark. We will introduce the details of our dataset in Sec.3.1 and metrics in Sec.3.2. Based on our dened ‘scene’ in Sec. 1, we build our large-scale Tencent-AVS Dataset with accurate annotations for scene boundaries and scene-level multi-label categories. We will introduce the details of our dataset and the annotation process as follows. Taxonomy.By exploiting the characteristics of ads videos and collecting the advertiser-uploaded key words, we build a threelayer hierarchy for the categories of ads videos from four perspectives: ‘presentation form’, ‘style’, ‘place’ and ‘narrative’, as shown in Fig. 3.1. Then we consult the experts of advertisement and add the missing classes into our category hierarchy to form 233 classes. Finally, we remove the classes with a very small number of instances (i.e.,≤100) after nishing the annotations of our dataset to make the nal dataset have 82 high-frequency classes. The number of category with the presentation, style, and place are 25, 34, and 23, respectively. In the presentation category, there are ad videos covering grid layout, dubbing, and padding. In the style category, Table 1: Statistics of Tencent-AVS dataset. Figure 4: (Left) Tencent-AVS video duration distribution. (Right) The average number of labels for each video. ad videos contains emotion, subject, and relationship contents. In the place category, there are locations including home, oce, and school. Fig. 5 shows the label distribution of presentation, style, and place. We can observe the style contains a large amount of labels and the place contains the least. Besides, the labels among these three main categories suer form long-tail distributions. An example is showed in the place category where the majority of labels exist within indoor, outdoor, home, and oce subcategories. Annotation process and quality control.Dissimilar to the existing action localization or segmentation task whose main challenge of annotation process is the ambiguity of action boundaries, humanedited ads videos have many shot changes as clear demarcations. Therefore, the main challenge of our dataset is the annotations of our multi-label ne-grained categories. Specically, a scene has about 6 classes on average, so annotators are easy to miss some classes instead of annotating a wrong label for a specic class. Based on these concerns, we write a detailed handbook for annotators, which contains a detailed description and a sample video for each class, and ask them to pay attention to the easily-missed classes. Our handbook also describes when scene changes and in which situation the scene crosses multiple shots. Our annotation process consists of four steps: 1) trial annotation, 2) annotation, 3) sampled inspection and revision, and 4) post-processing. In step 1, we let 7 annotating companies to try the annotation and select top 3 companies to continue step 2 according to their accuracy. After each batch of data is annotated, we will randomly sample about 1/10 videos and check the quality of annotations, especially the missing classes. We will approve the batch if the accuracy of annotation is satisfying (e.g.,≥90%) and otherwise let the annotators to revise their annotations for the whole batch. Each video’s annotations will be revised for about 3 times on average. Therefore, we believe that the quality of our dataset is carefully controlled. Finally, as scene boundaries annotated by humans are hard to reach the per-frame precision, we use a shot detection method TransNet v2 [43] to rene the little shifting of scene boundaries as a post-processing, i.e., we use the nearest shot boundary to substitute the original scene boundary annotation if the distance between them is small (i.e.,≤ 0.1s). Splits and statistics.Ads videos in Tencent-AVS dataset come from our real business data. It contains 12k Ads videos (totally 142.1 hours) and is divided into 5,000 for training set, 2,000 for validation set and 5,000 for test set, as shown in Table 1. In Fig. 4, we demonstrate the statistic of the video duration (left) and label instances per video (right) in Tencent-AVS dataset. Most of video duration in Tencent-AVS are between 25 and 60 seconds, with average length as 45.5 sec. As shown in Table 2, Tencent-AVS is comparable to other video segmentation datasets without class annotations (e.g., MovieScenes and Kinetics-GEBD only has scene boundary annotations) and is much larger than video segmentation datasets with class annotations (e.g., 50Salads and Breakfast). Furthermore, our dataset coves a wider range of real-world scenes instead of a very small domain (e.g., cooking), which means our class hierarchy is more diverse than previous datasets. In Table 3, we compare Tencent-AVS with other multi-label video classication datasets: Firstly, The number of instances in our dataset shows a signicant advantage than previous datasets. In addition, our dataset contains dierent types of videos from many industries such as education, games or E-commerce. Finally, our dataset contains not only human actions but also other elements such as places and objects. However, contents from other datasets are from single domain and focus on human actions. Access to our Tencent-AVS dataset.Our dataset will be publicly available at https://algo.qq.com: Videos from all three splits and annotations of train/val sets can be downloaded by lling an application sheet. We will maintain an online server for evaluating the performance on the test set and the leaderboard will be updated on the website. Ads Video Structuring.Due to the properties of the proposed task, e.g., multi-label and temporal segmentation, many metrics adopted by existing tasks cannot directly be used in our task. Here we review some related metrics and introduce the metrics we adopted. The most related action segmentation task [16,28,54] adopts perframe accuracy, an edit-distance based score and F1-score with tIoU as 0.1, 0.25 and 0.5. However, the multi-label annotation of our task makes these metrics either impossible to compute or unsuitable to use. Some previous scene segmentation methods [34] without scene-level class annotations use mAP of scene boundaries as their metric. However, they commonly rst use a shot detection method [8] and regard the scene segmentation task as a binary classication task which determines whether each shot change frame is a scene boundary. Therefore, it is not suitable for our accurate scene boundary annotations with 0.04s as minimum units. Finally, to measure the performance of our ads video structuring task, we adopt the mAP@tIoU metric similar to ActivityNet challenge [20] to evaluate the performance of multi-label classication and IoUbased localization in the meantime, and F1@0.5s metric similar to GEBD challenge [39] to evaluate the accuracy of scene boundary localization. Specically, for mAP@tIoU, we require the submitted scene segments to have no overlaps for a better adaption for our scene segmentation rather than the original action detection [20] and take an average of mAPs from IoU 0.5 to 0.95 with stride 0.05 as our nal mAP evaluation result; for F1@0.5s, we use 0.5s instead of 5% of the length of segment in [39] to avoid the inuence from long segments to their neighboring short segments. Formally, the average mAP we use is 𝑚𝐴𝑃 [𝑡𝐼𝑜𝑈 @0.5 : 0.95] =1101𝑁𝐴𝑃 where𝑁is the total number of categories, and𝐴𝑃is the Average Precision commonly used in detection tasks. Our F1@0.5s matches each predicted boundary to the nearest ground-truth scene boundaries and regard it as True Positive (TP) if their distance is less than 0.5s, otherwise it is a False Positive (FP). The ground-truth scene boundaries will be deleted once it is positively matched (i.e.,≤0.5s), and the ground-truths which do not have any positive matches to be False Negatives (FN). Therefore, the precision is𝑇 𝑃/(𝑇𝑃 + 𝐹𝑃) and recall is 𝑇 𝑃/(𝑇 𝑃 + 𝐹 𝑁 ). The nal F1@0.5s is computed by Video-level Multi-label Classication.For video-level multilabel classication, we also evaluate the performance by Global Average Precision (GAP) [26]. It calculates the average precision based on top 20 class predictions for each video: where𝑛is the number of true positive predictions,𝑝 ( 𝑗)is the precision and 𝑟 ( 𝑗 ) is the recall at top 𝑗. As mentioned in Sec.2.3.2, we choose the multi-stage method as our baseline, which contains three stages: shot detection, scene segmentation, and scene-level multi-label classication as shown in Fig. 6. We will introduce the details as follows. We observe that the traditional shot detection method [8] shows the bad performance in ads video domain, e.g., having similar colors, gradual transition eects or static images as padding in border regions. Therefore, we use the learning-based method TransNet v2 [43] and optimize its performance for situations such as gradual transition eects or static paddings by adding more training data and a spatial SE [23] module. All the following models are based on the split shots and the scene boundaries are the subset of shot boundaries, so that performance of scene segmentation is bounded by shot detection. Please refer to the TransNet v2 paper [43] for more implementation details. After splitting the videos into a sequence of shots, we adopt LGSS [34] to predict the scene boundaries, which is a binary classication problem based on the shot boundaries. Specically, due to ads videos’ shorter duration (∼30s) than the movies in LGSS, we only use the Bi-LSTM [38] part to model the relations between shots with multi-modal features. The shot-level video and audio features are extracted by pre-trained o-the-shelf feature extractors, i.e., I3D [7] and VGGish [22]. Then the multi-modal features are concatenated Figure 5: Statistics of per class data size. The three dimensions are in dierent colors (consistent with the hierarchy tree). MPII-Cooking [35] Table 2: Comparison with other video segmentation datasets. Action segmentation datasets annotate action categories for each frame, yet they often have no shot changes and very little multi-modal information. Scene segmentation datasets only annotate scene boundaries but no scene-level categories. Our Tencent-AVS has rich multi-modal information, many shot changes inside scenes, and hierarchical scene-level categories. † means using narrations extracted by ASR during annotation, yet no multi-modal information provided for training/testing. ‡ Epic-kitchens has 97 verb classes and 300 noun classes. Table 3: Comparison of Tencent-AVS with other video classication datasets. together and fed into LGSS. Please refer to the LGSS paper [34] for more implementation details. 4.3.1 Feature Embedding Extraction. We use four dierent feature extractors to extract video, image, audio, and text embedding, respectively: 1)Video: We use a 1 fps sampling rate to process each video. The frames are then sent into ViT-Large [14] to be a 1024-d video-level feature vector. 2)Image: We take the middle frame of the video as the keyframe, which is processed by ResNet-50 [19] as a 2048-d feature vector. 3)Audio: The audio signal is extracted by FFmpeg and passed through VGGish [22] network to be a 128-d feature vector. 4)Text: Text information is obtained from OCR or ASR. For each sentence, a sequence of word is rst tokenized and then fed into BERT [13] (chinese version). We utilize the 768-d ‘CLS’ token from the last layer of BERT as the sentence feature. The ResNet and Bert networks are netuned in a end-to-end manner during training while other parameters of feature extractors are xed. 4.3.2 Feature Aggregation. NeXtVLAD [30] is ecient and eective method for temporal feature aggregations in videos. By deciding frame-level importance score for a sequence of frames, this network aims to learn the global representations of this sequence. Here we use NeXtVLAD to learn compactly aggregated video representations for visual and audio features. Specically, given𝑀frames sampled in a video, we extract the frame-level features𝑥 ∈ 𝑅via a pre-trained backbone. The features are aggregated as𝐾clusters by NeXtVLAD. Each feature is encoded to be a𝑁 × 𝐾dimension feature vector𝑣as follows: function of𝑥to cluster𝑘, which measures the similarity of𝑥and anchor point of cluster 𝑘. In order to save computational cost, NeXtVLAD rst expands the feature𝑥as¤𝑥 ∈ 𝑅via a FC layer, where𝜆is a width multiplier. Then a reshape operation is applied to transform¤𝑥 ∈ 𝑅to ˜𝑥 ∈ 𝑅, where𝐺is the size of groups. This operation splits ¤𝑥into lower-dimensional features{˜𝑥|𝑔 ∈ {1, . . . , 𝐺 }}. Then the feature vector can be written as 𝑔 ∈{1, . . . , 𝐺}, 𝑖 ∈{𝑖, . . . ,𝑇}, 𝑗 ∈1, . . . ,𝜆𝐶𝐺, 𝑘 ∈{1, . . . 𝑘} where𝛼( ¤𝑥)measures the similarity of˜𝑥to the cluster𝑘and 𝛼( ¤𝑥)can be considered as an attention operation over the groups. ¤𝑥and˜𝑥are the transformation of input feature𝑥, depending on the size of groups𝐺, and the width multiplier𝜆. The video-level descriptor𝑦can be obtained by aggregating frame-level features as After we concatenate the global representations of video and audio output by NeXtVLAD, the representation is amplied by a SE Context Gating operation. Finally, a Mixture-of-Experts [33] classier with a Sigmoid activation is adopted for video-level multilabel classication. 4.3.3 Loss Function. The long-tailed distribution of our dataset brings class imbalance issues. To mitigate this, we follow recent works [10,24,31] to down-weight the loss values for the frequently appeared samples. We utilize asymmetric loss (ASL) [5] ( a improved version of focal loss [31]) to reduce the contribution of negative samples when their predicted condence are low. Meanwhile, we assign the asymmetric penalty for each positive and negative sample. The losses for positive/negative samples are: where𝑝is the network output and𝛾is the penalty parameter for the positive and negative samples. When𝛾 =0, Eq. 7 degenerates to the binary cross-entropy loss.𝑝= max(𝑝 − 𝑚,0)is the threshold for discarding easy negatives, which is a lower bound of the condence 𝑝. It is worth noticing that the mis-labels, easy negative samples, and dropout make the network converges eciently. To reveal the key challenges of our proposed task, we conduct some important experiments as follows. We hope these experiments could provide useful guidance for the future directions of ads video understanding. All experiments in Sec 5.1 and Sec 5.2 follow a standard machine learning pipeline: we use training set to train our model and adjust the hyper-parameters on the validation set, then report the performance on the test set. Ablation on temporal modeling in scene segmentation.Although Convolution Neural Network has a dominating place in the computer vision community, we observe that LSTM can better perform in the shot-level temporal modeling in Table. 4. We guess that the reason is the high-level extracted multi-modal features and shorter length sequences of the shot-based modeling rather than commonly-used raw images, so temporal modeling in our task is more similar to NLP tasks. Ablation on modalities in scene segmentation.As a multimodal task, modalities are crucial for recognizing information from dierent perspectives. We ablate some important modalities in Table. 5 and Table. 6 to reveal their impact on the nal performance: 1) Figure 7: The comparison of per class AP b etween the baseline and improved version (baseline++). The order of classes is kept the same with Fig. 5 (i.e., descendent order of per class data size). Table 4: Ablation on temporal modeling in scene segmentation. The rst two metrics are precision and recall of scene boundaries in the shot-level. Table 5: Ablation of modalities in scene segmentation. Video (Inception-v3 [47] + NeXtVLAD [30]) 20.00 Table 6: Ablation of modalities in scene classication. The results are based on the best scene segmentation result. For segmentation, the appearance cues are more important, so the video modality achieves similar results with using all modalities, while merely using audio achieves a very poor result. We think the reason is that audio mainly contains BGM and human voice and it lacks of the discriminative ability to dierentiate the scenes. 2) For classication, the text modality contributes the least and audio shows better performance than its in segmentation. Our categories are mainly about appearances or musics yet very few categories are about speech, so the alignment about our categories and text is not good enough. Ablation on shot detection methods.Since both our temporal modeling and scene classication are based on the shot detection result, its performance has a large impact on our nal result. The reason of the large gap between traditional method and deep-learningbased method in Table.7 is two-fold: 1) In training, inaccurate shot boundaries will bring noise to the ground-truth of scene boundary TransNet v2 [43] Table 7: Ablation of shot detection methods.means our implementation optimized for ads vide os. Table 8: Ablation of modalities in ads video classication. Table 9: Ablation of our baseline (FA).FA is feature aggregation.SM is Spatial Mix.TM is Temporal Mix. in the approximation from the true ground-truth to shot-level binary ground-truth. 2) In inference, models cannot predict a scene boundary if there is no shot boundary. Per-class average precision (AP). The per-class AP is shown in Fig. 7, where all categories are shown in x-axis in a descendent order of per-class data size. We can nd that although head classes tend to have better performance, many classes such as ‘zoom in’ and ‘painting’ do not follow this rule, indicating the diculty of our Tencent-AVS dataset. We denote feature aggregation in 4.3.2 as FA, and FA with asymmetric loss described in 4.3.3 and data augmentation in??as the proposed method. The blue bar represents the performance of FA (i.e., baseline with GAP=69.1) and cyan bar is the improved version (i.e., baseline++ with GAP=78.8). We achieve a 14.0% relative gain in GAP by our proposed classication method Figure 8: Illustrations of spatial and temporal mix. compared to the vanilla FA. Moreover, baseline++ improves the performance of tail classes and maintains the performance of head classes in the same time, indicating the eectiveness of our focus on long-tailed distribution. Ablation on mo dalities in video-level classication. Table 8 shows the contribution of each modality, where we could observe that the video modality plays the most important role in the multimodal classication. Although the less important impacts of audio, text and image modality, their performance are complementary. Thus the nal result could be further improved by fusion of all modalities. Ablation on additional components. Baseline++ contains several modules such as ViT backbone, MoE classier, ASL loss and data augmentation. Table 9 shows how each module improves the baseline model (i.e., FA). Baseline with inception-v3 achieves GAP 69.1. By changing the backbone as ViT, the performance is improved to 73.8. The performance reaches 76.9 when we add ASL (mentioned in 4.3.3). We also use Mixture-of-Experts (MoE) [33] to improve the classier, which leads to a 77.1 GAP. Ablation on data augmentation. Data augmentation methods are proved to be eective for preventing overtting and improving generalizations. We follow VideoMix [57] to conduct two types of data augmentation in videos: spatial mix and temporal mix, as shown in Fig. 8. We create a new training video which is constructed by replacing and stacking from one video to another. Specically, spatial mix operation denes a binary mask tensor𝑀, which signies the replace location in two video tensor. Given two video samples 𝑥and 𝑥, the new video can be obtained by where⊙is the element-wise multiplication. Similarly, temporal mix operation aims to stack two video along the temporal dimension. By further adopting data augmentation methods such as temporal mix (TM) and spatial mix (SM), we achieve a further performance gain on all the additional components and lead to 78.8 GAP. In this paper, we propose the multi-modal ads video structuring task and video classication task in our Multi-modal Ads Video Understanding Challenge to understand ads videos in an in-depth manner. We describe our proposed Tencent-AVS benchmark from the perspectives of class taxonomy, data annotation, and metrics. By ablating our proposed baseline for this task, we reveal the key challenges in ads video structuring and would like to provide useful guidance for future research in this area. We will continue to work with the multimedia community to contribute to related topics.