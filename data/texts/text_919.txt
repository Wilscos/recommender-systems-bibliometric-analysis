 tive bots, in particular, can manipulate online discussions of important issues ranging from elections to public health, threatening the constructive exchange of information. Their ubiquity makes them an interesting research subject and requires researchers to properly handle them when conducting studies using social media data. Therefore it is important for researchers to gain access to bot detection tools that are reliable and easy to use. This paper aims to provide an introductory tutorial of Botometer, a public tool for bot detection on Twitter, for readers who are new to this topic and may not be familiar with programming and machine learning. We introduce how Botometer works, the diﬀerent ways users can access it, and present a case study as a demonstration. Readers can use the case study code as a template for their own research. We also discuss recommended practice for using Botometer. Social bots are social media accounts controlled in part by software that can post content and interact with other accounts programmatically and possibly automatically [1]. While many social bots are benign, malicious bots can deceptively impersonate humans to manipulate and pollute the information ecosystem. Such malicious bots are involved with all types of online discussions, especially controversial ones. Studies have identiﬁed interference of social bots in U.S. elections [2, 3, 4, 5], French elections [6], the Brexit referendum [7, 8, 3, 9], German elections [10], and the 2017 Catalan referendum [11]. Bots also actively participate in public health debates [12] including those about vaccines [13, 14], the COVID-19 pandemic [15, 16, 17, 18], and cannabis [19]. Research has also reported on the presence of social bots in discussions about climate change [20, 21, 22], cryptocurrency [23], and the stock market [24, 25]. may simply generate a large volume of posts to amplify certain narratives [21, 26] or to manipulate the price of stocks [24, 25] and cryptocurrencies [23]. They can also disseminate low-credibility information strategically by getting involved in the early stage of the spreading process and targeting popular users through mentions and replies [2]. Some bots act as fake followers to inﬂate the popularity of other Social bots have become an important component of online social media. Decep- Malicious social bots demonstrate various behavioral patterns in their actions. They accounts [27, 28, 29]. In terms of content, malicious bots are found to engage other accounts with negative and inﬂammatory language [11] or hate speech [30, 17]. In some cases, bots form dense social networks to boost engagement and popularity metrics and to amplify each other’s messages [31, 32, 33]. actors in recent years, evade detection [34]. This has two implications for researchers. First, characterizing the behavior of and assessing the impact of social bots remains an interesting research topic [35]. Second, researchers need to properly handle bots in their data since their presence may distort analyses [12, 36]. It is therefore crucial for researchers to have access to a reliable tool for detecting social bots. for bot detection on Twitter. Although other bot detection methods exist, Botometer stands out for several reasons. First, it is well maintained and has been serving the community for the past seven years without major outages. It has also been routinely upgraded to stay accurate and relevant. Second, Botometer is easily accessible through both a web interface and an application programming interface (API). Anyone with a Twitter account can use the web version for free; researchers with Twitter developer accounts can use the API endpoints to analyze large-scale datasets. Third, Botometer has been extensively validated in the ﬁeld. Many researchers have applied Botometer in their studies to directly investigate social bots and their impact [13, 10, 19, 25], or to distinguish human accounts and bot-like accounts in order to better address their questions of interest [37, 38, 39]. be familiar with programming and machine learning concepts. We start with an introduction to how Botometer works and how users can access it. We then present a case study to demonstrate Botometer usage. The source code for this case study is shared through a public repository for readers to replicate this analysis and use it as a template for their own research. We ﬁnally discuss recommended practice. Figure 1 presents the timeline and key characteristics of successive Botometer versions over the years. Since the behaviors of bot and human accounts evolve over time, version upgrades are necessary for Botometer to stay accurate and relevant. Upgrades blog.twitter.com/common-thread/en/topics/stories/2021/the-secret-world-of-good-bots Although social media platforms have strengthened their eﬀorts to contain malicious This practicum aims to provide a tutorial for Botometer, a machine learning tool This tutorial is designed for researchers from diﬀerent disciplines who might not Table 1: Annotated datasets of human and bot accounts used to train Botometer. typically included adding new training data and updating model features. The most recent version also involved major architectural changes. Users of Botometer should be aware that results from diﬀerent versions are usually not comparable and the format of input and output might change as well. corresponding papers. This tutorial focuses on V4 [41]. In addition to new training data and new features, this version introduced a new architecture. We will also brieﬂy cover a recently added model for fast bot detection [42]. Under the hood, Botometer is a supervised machine learning classiﬁer that distinguishes bot-like and human-like accounts based on their features (i.e., characteristics). user proﬁle, friends, network, temporal, content and language, and sentiment [40]. For example, the user proﬁle category includes features such as the length of the screen name, whether the account uses the default proﬁle picture and background, the age of the account, etc. The content and language category consists of features such as the number of verbs, nouns, and adjectives in the tweets. For a given account, these features are extracted and encoded as numbers. This way the account can be represented by a vector of feature numbers, enabling machine learning classiﬁers to process the information. human, i.e., the training datasets. Botometer-V4 is trained on a variety of datasets shown in Table 1, which are publicly available. represented as feature vectors, a classiﬁer can learn the characteristics of bot and human accounts. Botometer uses a classiﬁcation model called Random Forest, which consists of many rules learned from the training data. and tweets mentioning it from Twitter, extracts its features from the collected data, botometer.osome.iu.edu/bot-repository For details of early versions such as V2 [40] and V3 [34], readers can refer to the Botometer considers over 1,000 features that can be categorized into six classes: Creating a working classiﬁer also requires samples of accounts labeled as bot or To evaluate a Twitter account, Botometer ﬁrst fetches its 200 most recent tweets and represents this information as a feature vector. Each model rule uses some of the features and provides a vote on whether an account is more similar to bot or human accounts in the training data. Based on how many rules vote for the bot or human class, the model provides a “bot score” between zero and one: a score close to one means the account is highly automated, while a score near zero means a human is likely handling the account. Some accounts may demonstrate the characteristics of both humans and bots. For instance, a bot creator might generate content like a regular user but uses a script to control many accounts. These cases can be confusing for the classiﬁer, which would then produce scores around 0.5. unique behavioral patterns. Based on this observation, Botometer-V4 uses several specialized Random Forest classiﬁers: one for each type of bots in the training data and one for humans. The results of this Ensemble of Specialized Classiﬁers (ESC) are aggregated to produce a ﬁnal result. More details about the ESC architecture can be found in the original paper [41]. At the end of the day, the ESC architecture is still a machine learning classiﬁer, which yields scores between 0 and 1. Diﬀerent from a single Random Forest, the scores generated by ESC tend to have a bimodal distribution. are based on English. When a non-English account is passed to Botometer, these features become meaningless and might aﬀect the classiﬁcation. As a workaround, Botometer also returns a language-independent score, which is generated without any language-related features. Users need to be aware of the account language and choose the most appropriate Botometer score. The accuracy of the model is evaluated through 5-fold cross-validation on the annotated datasets shown in Table 1. Simply speaking, the classiﬁer is trained on part of the annotated datasets and tested on the rest to provide a sense of its accuracy. In the experimental environment, Botometer works really well. V4 has an AUC (area under the receiver operating characteristic curve) of 0.99, suggesting that the model can distinguish bot and human accounts in Table 1 — as well as accounts in the wild that resemble those in the training datasets — with very high accuracy. factors. For example, the training datasets might have conﬂicts because they were created by diﬀerent people with diﬀerent standards. In some cases Botometer fails to capture the features that can help distinguishing diﬀerent accounts. The accuracy of the model may further decay when dealing with new accounts diﬀerent from those in the training datasets. These accounts might come from a diﬀerent context, use diﬀerent languages other than English [48, 49], or show novel behavioral patterns [43, 34, 50]. These limitations are inevitable for all supervised machine learning algorithms, and are the reasons why Botometer has to be upgraded routinely. Early versions of Botometer returned to users raw scores in the unit interval, produced by the Random Forest classiﬁers. Although users often treat them as probabilities, such interpretation is inaccurate. Consider Twitter accounts a and b and their respective scores 0.7 and 0.3 produced by a Random Forest classiﬁer. We can say that a is more bot-like than b, but it is inaccurate to say that there is a 70% chance that a is a bot or that a is 70% bot. Since Botometer-V3, the scores displayed in the web interface are rescaled to the range 0–5 to discourage such inaccurate interpretations. While human accounts tend to behave similarly, diﬀerent types of bots usually have It is worth mentioning that the content and language features and sentiment features However, Botometer is not perfect and may misclassify accounts due to several Automation Probability (CAP) represents the probability that an account with a given score or greater is automated. CAP scores have also been available since BotometerV3. The CAP scores are Bayesian posteriors that reﬂect both the results from the classiﬁer and prior knowledge of the prevalence of bots on Twitter, so as to balance false positives with false negatives. For example, suppose an account has a raw bot score of 0.96/1 (equivalent to 4.8/5 display score on the website) and a CAP score of 90%. This means that 90% of accounts with a raw bot score above 0.96 are labeled as bots, or, as indicated on the website, 10% of accounts with a bot score above 4.8/5 are labeled as humans. In other words, if you use a threshold of 0.96 on the raw bot score (or 4.8 on the display score) to classify accounts as human/bot, you would wrongly classify 10% of accounts as bots — a false positive rate of 10%. This helps researchers determine an appropriate threshold based on acceptable false positive and false negative error rates for a given analysis. When Botometer-V4 was released, a new model called BotometerLite was added to the Botometer family [42]. BotometerLite was created to enable fast bot detection for large scale datasets. The speed of bot detection methods is bounded by the platform’s rate limits. For example, the Twitter API endpoint used by Botometer-V4 to fetch an account’s most recent 200 tweets and recent mentions from other users has a limit of 43,200 accounts per app key, per day. Many studies using Twitter data have millions of accounts to analyze; with Botometer-V4, this may take weeks or even months. metadata, contained in the so-called user object from the Twitter API. The rate limit for fetching user objects is over 200 times the rate limit that bounds Botometer-V4. Moreover, each tweet collected from Twitter has an embedded user object. This brings two extra advantages. First, once tweets are collected, no extra queries to Twitter are needed for bot detection. Second, the user object embedded in each tweet reﬂects the user proﬁle at the moment when the tweet is collected. This makes bot detection on archived historical data possible. lection mechanism to ensure its accuracy and generalizability. Instead of throwing all training data into the classiﬁer, a subset is selected by optimizing three evaluation metrics: cross-validation accuracy on the training data, generalization to holdout datasets, and consistency with Botometer. This mechanism was inspired by the observation that some datasets are contradictory to each other. After evaluating the classiﬁers trained on all possible combinations of candidate training sets, the winning classiﬁer only uses ﬁve out of eight datasets but performs well in terms of all evaluation metrics. bot classes compared to Botometer, BotometerLite allows researchers to analyze largevolume streams of accounts in real time. Although the machine learning model might seem complicated, the interface of Botometer is designed to be easy to use. Botometer has a website and API endpoints with similar functionality. The website accounts. With a Twitter account, users can access the Botometer website from any botometer.org For users who need a probabilistic interpretation of a bot score, the Complete To achieve scalability, BotometerLite relies only on features extracted from user In addition to the improved scalability, BotometerLite employs a novel data se- While the limited training data may involve a compromise in accuracy on certain web browsers, even on their mobile devices. The website is straightforward to use: after authorizing Botometer to fetch Twitter data, users just need to type a Twitter handle of interest and click the “Check user” button. matically check accounts in bulk. The API is hosted by RapidAPI, a platform that helps developers manage API rate limits and user subscriptions. Using the Botometer API requires keys associated with a Twitter app, which can be obtained through Twitter’s developer portal. to one of the API usage plans. most recent tweets by the account being checked and tweets mentioning this account) in a speciﬁed format through HTTPS requests. The Botometer API will process the data and return the results. While queries can be sent through any programming language, we recommend using Python and the oﬃcial botometer-python package that we maintain. query the API on behalf of the user with a few lines of code: impo r t bo t o m e t e r bom = b o tometer . Botometer ( resu l t = bom . check_a c c ount (" @yang3kc ") print (f " Bot sco re ={ re sult [’ di s p l a y _ s cores ’][ ’ english ’][ ’ overall ’]}/5") print (f " CAP sco re ={ re sult [’ cap ’][ ’ english ’]:.2 f }") BotometerLite is also available as an endpoint through the Botometer Pro API. We summarize the common links for using Botometer in Table 2 to help the readers navigate these resources. can only make a certain number of queries in a given time period. Please check the respective websites for detailed documentation. Getting familiar with the rate limits can help researchers better estimate the time needed for their analysis. Since some readers may not be familiar with programming, querying the API could be challenging. Moreover, analyzing the results returned by Botometer API is not trivial. In this section, we provide a simple case study as a demonstration. Diﬀerent ways of analyzing the data are shown with recommended practice. We share the code for this case study in a public repository research. Next we outline the data collection and analysis steps implemented in this software repository. rapidapi.com/OSoMe/api/botometer-pro developer.twitter.com github.com/IUNetSci/botometer-python github.com/osome-iu/Botometer101 The Botometer Pro APIcan be more useful for research since it allows to program- When querying the API, users are responsible to send the required data (i.e., 200 rapida p i _ key =" XYZ ", consum e r _ key =" XYZ ", cons u m er_s e c ret =" XYZ ", access _ t o ken =" XYZ ", acce s s_t o k en_ s ecre t =" XYZ " Note that both Botometer and Twitter APIs have rate limits, meaning that users Botometer-python package github.com/IUNetSci/ Table 3: Numbers of tweets and unique accounts mentioning diﬀerent cashtags. Let us consider two cryptocurrency cashtags, $FLOKI and $SHIB, and the cashtag of Apple Inc., $AAPL, and attempt to quantify which is more ampliﬁed by bot-like accounts. We use Tweepy, containing these cashtags. For each cashtag, we only collect 2,000 tweets, which are suﬃcient for the demonstration. Table 3. The number of unique accounts is much smaller than the number of tweets in all three datasets, suggesting that some accounts tweeted the same cashtag multiple times. through each tweet and check every user encountered, researchers can keep a record of accounts already queried to avoid repetition and increase eﬃciency. The Botometer API returns rich information about each account. We recommend storing the full results from Botometer for ﬂexibility. score. Since the two scores come from diﬀerent classiﬁers, they are not comparable and should not be mixed together. To decide which one to use, let us calculate the proportion of accounts using each language. We can see in Figure 2 that the majority of accounts in our data tweet in English. Therefore let us use the overall score in our analysis. tweepy.org First, let us count the number of unique accounts in each datasets, as shown in The next step is to query the Botometer API for bot analysis. Instead of going As mentioned above, Botometer generates an overall score and a language-independent Figure 2: Percentage of accounts using each language in the three datasets combined. We plot the bot score distribution for tweets mentioning each cashtag in Figure 3(a). Here we base our analysis on the raw scores in the unit interval. Since we are interested in bot ampliﬁcation of each cashtag, we use tweets (as opposed to accounts) as the units of analysis. This means that accounts tweeting the same cashtag multiple times have a larger contribution. architecture of Botometer-V4. We can observe some spikes in all cases, which are caused by accounts tweeting the same cashtag repeatedly. For example, the spike near 0.89 for $SHIB and $FLOKI comes from a bot-like account that replied the same message promoting cryptocurrency tokens to a large number of tweets containing the keyword “NFT”; see the screenshot of the message in Figure 4. cashtag and compare them. The ﬁrst approach is to compare their mean bot scores with two-sample t-tests (see results in Figure 3(c)). The mean scores of $SHIB and $FLOKI are not signiﬁcantly diﬀerent from each other (t = −0.38, p = 0.7), but both of them have higher bot ampliﬁcation than $AAPL ($SHIB vs. $AAPL: t = 5.57, p < 0.001; $FLOKI vs. $AAPL: t = 5.77, p < 0.001). scores higher than a threshold as likely bots. Then the proportion of tweets from likely bots can be calculated and compared. In this approach, a threshold has to be chosen. In the literature, 0.5 is the most common choice [2, 37, 4]; highrer values such as 0.7 [38], and 0.8 [13] are also used. One may also consider running the same analysis with diﬀerent threshold values to test the robustness of the ﬁndings [2]. and (d), respectively. We apply two-proportions z-tests to estimate the signiﬁcance level of the diﬀerences. When using 0.5 as the threshold, the percentage of tweets from likely bots that mentioned $SHIB is signiﬁcantly higher than those in the $FLOKI (z = 2.70, p = 0.007) and $AAPL datasets (z = 5.86, p < 0.001). The percentage of tweets from likely bots that mentioned $FLOKI is also signiﬁcantly higher than that in the $AAPL dataset (z = 3.14, p = 0.002). However, when using 0.7 as the threshold, the results change: percentages of tweets from likely bots in $SHIB and $FLOKI datasets are no longer signiﬁcantly diﬀerent from each other (z = 0.42, p = 0.7); both of them are lower than that in the $AAPL dataset ($SHIB vs. $AAPL: z = −5.62, p < 0.001; $FLOKI vs. $AAPL: z = −6.07, p < 0.001). results. However, they lead to seemingly diﬀerent conclusions in this case. This is In all three cases, the distribution has a bimodal pattern, a result of the ESC To address our research question, we need to quantify bot ampliﬁcation for each The second approach dichotomizes the bot scores and considers the accounts with Here we use both 0.5 and 0.7 as thresholds and show the results in Figure 3(b) In other studies, diﬀerent approaches or threshold choices may yield consistent Figure 3: (a) Bot score distribution for tweets mentioning diﬀerent cashtags. (b) Percentage of tweets posted by likely bots using 0.5 as a threshold. (c) Box plots of the bot scores for tweets mentioning diﬀerent cashtags. The white lines indicate the median values; the white dots indicate the mean values. (d) Similar to (b) but using a bot score threshold of 0.7. Statistical tests are performed for pairs of results in (b–d). Signiﬁcance level is represented by the stars: ***=p ≤ 0.001, **=p ≤ 0.01, *=p ≤ 0.05, NS= p > 0.05. “NFT” with a message promoting cryptocurrencies. The same message was replied by this account to a large number of tweets. Figure 5: Time series of bot scores of an account from September 2020 to November 2021. The queries were not made regularly, so the time intervals between consecutive data points vary. because diﬀerent measures represent diﬀerent properties of the bot score distribution. If we revisit the distribution in Figure 3(a), we can see that although the distributions of $SHIB and $FLOKI scores have more mass in the (0.5, 1] region than that of $AAPL scores, the mass tends to concentrate around 0.6, while the distribution of $AAPL scores has more mass near 1. This nuanced diﬀerence causes the contradictory results when using diﬀerent threshold values. As for the mean bot score, it contains the information of the human-like accounts as well and indicates the overall level of automated behaviors. question now. It appears that discussions about the cryptocurrencies $SHIB and $FLOKI show more automated activities than that about $AAPL, but among the accounts tweeting $AAPL, we ﬁnd more highly automated bot-like accounts. Note that the analysis here is mainly for demonstrating the use of Botometer; the samples of tweets analyzed are small and not representative of the entire discussion, so the conclusions only reﬂect the status of the collected data and should not be generalized. The sections above cover some recommended practice such as being careful when interpreting raw bot scores, being mindful about user language, and being aware of diﬀerent versions of Botometer. Here we make a few more recommendations to help avoid common pitfalls. Recall that Botometer uses the 200 most recent tweets by an account and other tweets mentioning the account for analysis. This means that the results of Botometer change over time, especially for very active accounts. To demonstrate this, we plot the time series of the overall bot score of an account in Figure 5. This account posts roughly 16 tweets each week and gets mentioned by others frequently. We can see that the bot score ﬂuctuates over time. In some other cases, an account might be suspended or removed after a while, making it impossible to analyze. status of the account at the moment when it is evaluated. Users should be careful when By reconciling the results from diﬀerent approaches, we can answer our research Due to the transient nature of Botometer scores, a single bot score only reﬂects the drawing conclusions based on the bot scores of individual accounts. For researchers, a common practice is to collect tweets ﬁrst, then perform bot detection later. To reduce the eﬀect of unavailable accounts and to keep the bot scores relevant, bot analysis should be conducted right after data collection. When analyzing bot activity, we recommend considering score distributions when possible. As demonstrated in the case study, bot score distributions reveal rich information about the data. Using distributions for analysis also reduces the uncertainty level of Botometer due to its imperfection and transient nature. Most importantly, comparing distributions of scores — e.g., for accounts tweeting about a given topic versus some baseline — allows for statistical analysis that is impossible at the level of individual accounts. In some analyses, dichotomizing the bot scores based on a threshold is necessary. In these cases, we recommend validating the choice of threshold. For researchers with the ability and resources, the ideal approach is to manually annotate a batch of bot and human accounts in their datasets. One can then vary the threshold and select the value that optimizes some appropriate metric on these annotated accounts. Depending on the desire to maximize accuracy, minimize false positive errors, minimize false negative errors, or some combination, one can use metrics such as accuracy, precision, recall, or F1. When annotating additional accounts is not feasible, we suggest running multiple analyses using diﬀerent threshold choices to conﬁrm the robustness of the ﬁndings. We have noticed that Botometer has been used to attack others. For example, some users may call others with whom they disagree “bots” and use the results of Botometer as justiﬁcation. This is a misuse of Botometer. Users should keep in mind that any classiﬁer such as Botometer can mislabel individual accounts. Furthermore, even if an account is automated, it does not mean it is deceptive or malicious. Most importantly, such name calling is not helpful for creating healthy and informative conversations.