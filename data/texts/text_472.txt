Semantically connecting users and items is a fundamental problem for the matching stage of an industrial recommender system. Recent advances in this topic are based on multi-channel retrieval to eciently measure users’ interest on items from the massive candidate pool. However, existing work are primarily built upon pre-dened retrieval channels, including User-CF (U2U), Item-CF (I2I), and Embedding-based Retrieval (U2I), thus access to the limited correlation between users and items which solely entail from partial information of latent interactions. In this paper, we propose a model-agnostic integrated cross-channel (MIC) approach for the large-scale recommendation, which maximally leverages the inherent multi-channel mutual information to enhance the matching performance. Specically, MIC robustly models correlation within user-item, user-user, and item-item from latent interactions in a universal schema. For each channel, MIC naturally aligns pairs with semantic similarity and distinguishes them otherwise with more uniform anisotropic representation space. While state-of-the-art methods require specic architectural design, MIC intuitively considers them as a whole by enabling the complete information ow among users and items. Thus MIC can be easily plugged into other retrieval recommender systems. Extensive experiments show that our MIC helps several state-of-the-art models boost their performance on two real-world benchmarks. The satisfactory deployment of the proposed MIC on industrial online services empirically proves its scalability and exibility. • Information systems → Recommender systems. integrated recommender, model-agnostic, cross-channel contrastive In this era of information explosion, recommendation services have emerged to match various products with diverse users eciently. As shown in Figure 1, the matching stage providing the retrieved items list to the ranking stage is the cornerstone and the bottleneck of a typical two-stage industrial recommender system. Figure 2 depicts the commonly used retrieval channels: 1) U2I: Directly recommend items to users. 2) I2I: Recommend similar items. 3) U2U: Retrieve Figure 1: A diagram of a typical two-stage (matching and ranking) recommender system in the real world. MIC can be easily applied in the matching stage. similar users. 4) U2U2I: Recommend items that similar users like based on user-based collaborative ltering. 5) U2I2I: Recommend similar items based on item-based collaborative ltering In this scenario, it is vital to eciently model user preferences over items to retrieve from large-scale candidate pools; thus, multi-channel retrieval, which eciently mixes the diversied retrieved items, is a natural and indispensable approach. However, most previous methods seek to improve the performance of user modeling based on a single channel, thus failing to leverage inherent correlations in the user-based channel, item-based channel, and user-item channel simultaneously. It is common in industry recommendation system to use Locality sensitive hashing [14], Paragraph2Vector [27] and DSSM [21] models to encode user history items and generate similar users for user channel (U2U). [30] improve the performance of personalization and diversity in itembased collaborative ltering from the item channel (I2I) perspective. [3,7,22,29,31] are proposed to model dynamic and diversied user preferences based on interactions records from the user-item channel (U2I). For retrieval from multiple sources, [38] propose a hierarchical reinforcement learning framework to recommend heterogeneous items. Nevertheless, the existing method focuses on improving performance based on partial information from each channel, signicantly reducing their performance, and facing maintaining costs from dierent channels with various models. We argue that addressing the aforementioned issues in a unied manner is under-explored and points to a new promising direction for developing recommender systems. Models that solely focus on a single angle could learn common relevance between users and items while ignoring the inherent cross-channel information and performing poorly in a real-world scenario. Figure 2: A diagram for multiple channels among users and items. The interactions are reected in the user-item channel matrix. The correlations inside users and items are represented in the corresponding channel matrix. Industrial systems attempt to mitigate such performance reduction by retrieving items based on multiple channels, including various features, strategies, and models. However, existing oine training pipelines are bound to a channel-specic model framework, and the online mixture of multiple channels retrieval is simply controlled by a simple quota mechanism, which leads to three major challenges: a) Devising a mechanism to intricate coupling eects in separated models and maximize the sum of performance. b) Breaking the limitation of time and space cost of the emergent new algorithms. c) Maintaining a bunch of oine models and training pipelines of multiple channels’ online deployment and experiment analysis. In contrast, our proposed model-agnostic integrated crosschannel (MIC) approach is towards addressing the aforementioned challenges within a universal retrieval recommender system. In this work, we focus on capturing correlations among users and items across multiple channels with a single model in a unied schema. To achieve this, we rst found that it is possible to use one model such as Comirec [3] or DSSM [21] for three-channel retrieval: U2I, U2U, I2I. Then we designed cross-channel contrastive learning techniques to boost a single model’s performance on three channels. We introduce cross-channel contrastive learning techniques into our unied framework with learnable and congurable settings to handle the dynamic and uncertain nature when connecting users and items. In particular, we randomly perturb the elds of each instance and perform dropout in the embedded feature space. The objective is to learn the representations by leveraging a contrastive learning loss to maximize the similarity between the embeddings of two versions of the same instance. User and item representation are learned in their own semantic space via intra-channel contrastive loss with the user-user (UU) channel and the item-item channel (II) training setting. To further connect users and items, we intuitively perform a non-linear projection to learn additional users and items representations in a common semantic space via inter-channel contrastive loss. The relevance between users and items is measured as the cosine similarity between their vectors in a shared space. MIC is able to realize ecient multi-channel retrieval to capture the co-evolving diversied and dynamic users and items representations in an integrated schema. Since the cross-channel learning module is independent of the encoders and the embedding layer is adaptable to sparse and dense features of users and items, MIC achieves a model-agnostic performance boost by simply switching the encoder to other retrieval models as shown in Figure 3. To summarize, the main contributions of this work are as follows: •We formulate the matching stage of recommendation as connecting user and item in multiple channels and propose a model-agnostic MIC architecture based on integrated crosschannel user and item representation learning techniques. •To the best of our knowledge, this is the rst work that proves it is possible to utilize only one model to handle U2I, U2U, U2I channels retrieval simultaneously, which would immensely reduce the iteration and maintain the cost of various models for dierent channels. •We address the aforementioned long-standing challenges in recommendation in a unied manner and introduce a crosschannel contrastive scheme to mitigate the uncertainty of co-evolving user-item correlations. •Compared with the existing method, MIC shows superior performance on two public datasets in eectiveness and eciency. MIC can also be incorporated into other matching stage recommenders to boost their performance. •We deployed our models on online services, the satisfactory online𝐴/𝐵test results over million-scale users and items conrm the eciency and eectiveness of MIC in practice. Recommendation system can be divided into mainly two categories, content-based recommendation and collaborative ltering. Based on the idea of user modeling, collaborative recommendation Zheng et al.[45] presented a neural autoregressive method for collaborative ltering NCF [19] propose to leverage a multi-layer perceptron to learn the user-item interaction function. Zheng et al.[43] proposed a deep collaborative neural network model. Collaborative ltering techniques is composed of user-based algorithms [42], item-based algorithms [9] and model-based algorithms [24]. Besides collaborative ltering, content-based ltering is another critical class of recommender systems. DSSM was introduced in [22] to project queries and documents into a common low-dimensional space. Elkahky et al.[12] proposed a multi-view neural network to learn the features of users and items separately. Pure content-based only rely on the feature of users and items, thus ignoring the common preferences shared among similar users and common properties among similar items. With the emergence of distributed representation learning, user embeddings obtained by neural networks are widely used. [5] employs RNN-GRU to learn user embeddings from the temporal ordered review documents. [33] utilizes Stacked Recurrent Neural Networks to capture the evolution of contexts and temporal gaps. [13] proposes the framework GraphRec to jointly capture interactions and opinions in the user-item graph. Due to the intrinsic drawback of both pure content-based and collaborative recommendations, the hybrid model concept is proposed to combine them and benet Figure 3: Overview of model-agnostic integrated cross-channel recommenders (MIC). The perturbations is performed in both eld level and embeded features level. The user-item (U2I), user-user (U2U) and item-item (I2I) mo dules are aggregated to calculate cross-channel contrastive loss. each other. Commonly used hybrid recommendation algorithms include weighted hybrid recommendation algorithm, cross-harmonic recommendation algorithm, and meta-model mixed recommendation algorithm [2]. Dai et al.proposed a dynamic recommendation algorithm [8] that combines the convolutional neural network and multivariate point process by learning the co-evolutionary model of user-commodity implied features. Nevertheless, though these hybrid algorithms seek to combine multi-source data, they failed to consider user-user, item-item, and user-item coevolution and relatedness in a unied framework. Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. Hadsell et al. [16] rst proposed to learn representations by contrasting positive pairs against negative pairs. Wu et al. [37] proposed to use a memory bank to store the instance class representation vector, which was adopted and extended by several recent papers [34,39]. Other work explored the use of in-batch samples for negative sampling instead of a memory bank [10,23,39] Recently, SimCLR [4] and MoCo [6,17] achieved state-of-the-art results in self-supervised visual representation learning, closing the gap with supervised representation learning. BYOL [15] also provides a non-contrastive SSL and shows remarkable performance without negative pairs, with an extra learnable predictor and a stop-gradient operation. Contrastive training is further explored in medical visual representations learning [41], multimodal visual representation learning [40], self-supervised forward inverse dynamic model [35] and learning transferable visual concepts from natural language [32]. MYOW [1] and NNCLR [11] actively mine the views, samples the nearest neighbors from the dataset in the latent space, and provide augmented views from dierent instances, which contains more semantic variations than pre-dened transformations. Leveraging nearest sample to produce pro views of sample mining is also proved eective in machine translation [25,44] and language models [26] In a typical recommendation scenario, we have a set of users and a set of items which can be denoted as𝑈 = {𝑢, 𝑢, ..., 𝑢} and𝑉 = {𝑣, 𝑣, ..., 𝑣}, respectively. Let𝑋= {𝑥, 𝑥, ..., 𝑥} denote the sequence of interacted items from user𝑢 ∈ 𝑈sorted in a chronological order:𝑥denotes the item that the user𝑢has interacted with at time step𝑡. Given the user historical behaviors, the goal of the sequential recommendation task considered in this paper is to retrieve a subset of items from the pool𝑉for each user in𝑈 such that the user is most likely to interact with the recommended items. Specically, each instance is represented by a tuple(𝑋, 𝐹, 𝐹), where𝑋denotes the interactions records of user𝑢,𝐹denotes the elds of features of the user𝑢including user ID, gender and age.𝐹denotes the elds of features of target item𝑣including the information of item ID, item keywords. MIC learns a function𝑓for mapping users into user representations, which can be formulated as where−→𝑒∈ Rdenotes the representation vector of user𝑢, d the dimension. Besides, the representation vector of target item𝑖is obtained by a similar mapping function 𝑔 as where−→𝑒∈ Rdenotes the representation vector of item 𝑣. When user representation vector and item representation vector are learned, top-N items are recommended according to the likelihood function 𝑝 as: where𝑁is the predened number of items to be retrieved.−→𝑒is the embedding of item v from a set of items𝑉. As we mainly focus on improving the performance in the matching stage of classical industrial recommender systems, Our framework outputs the probabilities for all the items, representing how likely the specic user will engage with the items, and retrieves top-N candidate items. Figure 3 gives an overview of our proposed MIC in each component. MIC is composed of a combination of Dropout Layer and Field Mask Embedding Layer as a Perturbation Mining module, a shared user-side encoder, a shared item-side encoder, and a cross-channel contrastive module. In each channel module, the objective is to pull similar samples and push away dissimilar ones. Contrastive learning method encourages positive pairs to have similar representations while negative pairs to have dissimilar representations. In the scenario of our unied framework, we consider both users and items as the anchor and generate pseudo views of each instance for comparison. We also leverage retrieved nearest neighbors to support the augmented sample views further. 3.3.1 Multi-level Perturbation. Data augmentation has been proved eective and widely used in contrastive prediction tasks without changing the architecture [4]. We devise a simple augmentation method to decouple from the neural network architecture. For users, we randomly masked the user elds, including attributes (Id, gender, age) and interaction sequence (item Id). Similarly, we randomly masked attributes (item Id, keywords) and interaction records (user Id) of each item. In addition to the eld-level perturbations, the dropout is performed in the embedded features space. When only perturbation-based view augmentation is available, we treat the other2(𝑁 − 1)augmented examples within a minibatch as negative examples. 3.3.2 Nearest Neighbor Mining. We observe limited views generated by augmentation. First, view augmentation is limited to origin instance and fail to provide diversied samples. Second, in some scenarios, eective augmentation is dicult to devise, rene, and evaluate. Finally, the augmentation method suers from the balance between providing diversied views and keeping the semantic consistency. In addition to augmentation, we argue that it’s necessary to leverage information from a retrieval angle of view. For users, we retrieve the anchor user’s k-nearest neighbor (kNN) in the representation space as the extension of user positive pairs. Besides, we adopt k-means++ to cluster the users and choose users from dierent clusters as hard negative samples. For items, both positive and hard negative samples are mined in the representation space in the same manner as users. At the interaction level, we use users to retrieve items and items to retrieve users. Before that, we project user and item representation in the same space. The same retrieval is then applied in this joint user-item representation space. Note that our sample selection pool is highly exible. All the parameters, including the number of nearest-neighbor, number of clusters, and number of masked attributes, are tuned during training and adaptable to manual modication. Thus MIC maintains scalability and robust temporal ecacy in fast-speed transforming online changes. Many works [20] directly optimize by forcing𝑐𝑙𝑖𝑐𝑘 (𝑢, 𝑣) =1 in diagonal and𝑐𝑙𝑖𝑐𝑘 (𝑢, 𝑣) =0 in other positions. However, these forcing methods assume the correlation between user and items to be deterministic, which is always not true in the real world. The real-world environment is always stochastic (e.g. diversied and dynamic user behaviors), where deterministic functions can only predict the average. On the other hand, contrastive estimation is an energy-based model. Instead of setting the cost function to be zero only when the prediction and the observation are the same, the energy-based model assigns low cost to all compatible prediction-observation pairs. Thus, the contrastive estimation can handle the stochasticity by its nature [28]. Inspired by recent contrastive learning algorithms [4], we propose to train these models by maximizing agreement between the anchor and augmented views via a contrastive loss. We randomly sample a minibatch of𝑁user-item pairs(𝑢, 𝑖). For the unied model, augmented users and items and the mined samples in the support set are dened as positive examples. Following SimCLR [4], we treat the other 2(𝑁 −1)real representation within a minibatch as negative examples. We use cosine similarity to denote the distance between two representation(𝑢, 𝑣), that is sim(𝑢, 𝑣) = u· v/||u|| · ||v||. The loss function for a positive pair of examples (𝑢, 𝑣) is dened as: L= −logexp(sim(𝑢, 𝑣)/𝜏)Í˜− logexp(sim(𝑣, 𝑢)/𝜏)Í˜ where𝜏denotes a temperature parameter that is empirically chosen as 0.1. Similarly, for user-user and item-item model, the loss function for a positive pair of examples (˜𝑢,𝑢) and (˜𝑣, 𝑣) is dened as: The basic logistic loss by comparing the cosine similarity of users and items are computed as below: The user-item (U2I), user-user (U2U) and item-item (I2I) modules are aggregated to calculate cross-channel contrastive loss. We use the Adam optimizer to train our method. The objective function for training our model is to minimize the following cross-channel contrastive loss: where𝜆is set to 0.7, each channel weight is 1 : 1 : 1 after parameter optimization in our experiments. MIC can achieve the optimum trade-o across multiple channels by selecting the value of hyperparameter𝜆and channel weight. During training, the total loss is computed across all positive pairs in a mini-batch. MIC can also be treated as a plug-in to other matching stage recommenders by simply switching the encoder. MIC incorporate the perturbation and mining module in the item-side and add a crosschannel contrastive learning module on top of the deep structural, semantic model [22]. Since the cross-channel learning module is independent of the encoders and the embedding layer is adaptable to sparse and dense features of users and items, MIC is highly exible and achieves a model-agnostic performance boost in retrieving items from multiple channels eciently. During the inference phase of MIC, we get user and item representation from the user and item side encoder, respectively. For the U2I channel, we directly use the user vector to retrieve the top K nearest neighbor from the whole item pool. For the U2U channel, we search𝑁similar users from the training dataset and rank top 𝐾items from𝑁similar users’ history by considering the weight of similar users and user-item vector cosine similarity. For the I2I channel, we use the user’s history to nd𝑀relevant items within the whole item vector space for each history item. We rank top𝐾 items from all I2I similar items by considering the weight of similar items and user-item vector cosine similarity. In this section, we rst cover the experimental settings of the dataset, evaluation metrics, parameter settings, and competitors. Then we report the results of extensive oine and online experiments with in-depth analysis to verify the eectiveness of MIC. We used three large benchmark datasets. The statistics of the two datasets are shown in 1. • Amazon Books([18]): This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews product metadata and links. • Taobao[46]: This dataset contains user behaviors recorded by Taobao recommendation system, consisting of users’ clicks, item ID, item category, and timestamp. To compare the performance of dierent models,we useRecall@N, NDCG@N(Normalized Discounted Cumulative Gain) andHR@N, where N is set to 20, 50 respectively as metrics for evaluation. In all these three metrics, a larger value implies better performance. Besides, we adopt a per-user average for each metric. • Recall: Number of corrected recommended items divided by the total number of all recommended items. whereˆ𝐼denotes the set of top-N recommended items for user u and 𝐼is the set of testing items for user u. • Normalized Discounted Cumulative Gain(NDCG): NDCG measures the percentage of correct recommended items, considering the positions of correct recommended items. where G denotes the ground-truth list.𝑖is the index of r in R.𝛿(·)is an indicator function which returns 1 if item r is in top-N recommendation, otherwise 0. IDCG is the DCG of ideal ground-truth list which refers to the descending ranking of ground-truth list in terms of predicted scores. • Hit Rate(HR): This measures the percentage of at least one item is correctly recommended to and interacted by corresponding user. Table 2: Performance on two public datasets: Amazon books and Taobao. Results of three retrieval models and the integration of each model denoted as 𝑋 and the proposed MIC are reported over three metrics: Recall, NDCG and Hit Rate. Gain represents the performance gain of X+MIC over vanilla 𝑋 model. The integrated model is in full UI, UU and II contrastive setting without inference channel-sp ecic retrieval. For fairness, we implement baselines and our proposed model in the same settings. The implementation is based on Tensorow for oine experiments. The dimension of the collaborative embedding is set as128. Batch size is set to1024on a single NVIDIA P40 GPU. The learning rate is set to0.001, and the dropout rate is set to0.2. The temperature parameter is empirically chosen as 0.1. We utilize Xavier and Adam algorithms in the experiments to initialize and optimize the parameters of the models. 4.4.1 Retrieval Baselines. YoutubeDNN [7] is one of the predominant deep learning models based on collaborative ltering systems incorporating text and image information which have been successfully applied under the industrial scenario. ComiRec [3] is a novel controllable multi-interest framework which can be used in sequential recommendation. We adopt the Deep Structured Semantic Model (DSSM [22]) as our base model for MIC. 4.4.2 MIC Variants. Our unied model MIC co-learns user and item representation in both shared and their own semantic space. The retrieval considers mutual information across multiple channels, including use-user, item-item, and user-item channel, simultaneously in an integrated framework. In addition, we provide three representative variants as MIC-UI,MIC-UU, and MIC-II with singlechannel contrastive loss. For MIC-UI, we add user-item contrastive training on top of DSSM as a variant of our proposed MIC. This variant can capture the information behind the interaction and match the users to appropriate items from the user-item channel. For MIC-UU, we add user-user contrastive training on top of DSSM as a variant of our proposed MIC. This variant is capable of clustering users and matching similar users to each other from the user channel. For MIC-II, we add item-item contrastive training on top of DSSM as a variant of our proposed MIC. This variant is capable of clustering items and matching similar items to each other from the item channel. All compositional ablation results of each contrastive setting are reported in Table 3. 4.4.3 MIC as Plugin. As MIC is can also be treated as a modelagnostic plugin, we implement a series of variants with MIC adapted to other retrieval models denoted as 𝑋 + 𝑀𝐼𝐶. Table 3: Ablation Performance of MIC on public Amazon Books. Channel column and contrastive setting column represents the retrieval channel during inference and cross-channel contrastive modules utilized in model implementation respectively. The results are based on DSSM+MIC. Best performance for each inference channel is highlighted in bold. Checkmark (✓) represents the switch-on of the specic channel module. The model performance for the retrieval stage recommender system is shown in Table 2. We conduct extensive experiments to dissect the eectiveness of our proposed model-agnostic integrated crosschannel (MIC) model. In the baseline performance comparison experiment, the MIC is implemented in a full mode with weighted UI, UU and II contrastive loss. We compare the performance of MIC enhanced model with each state-of-the-art vanilla model: YouTube DNN, ComiRec, DSSM. All these models are running on the two datasets introduced above: Amazon Books and Taobao. According to the results shown in Table 2, our proposed MIC outperforms other retrieval models over two datasets in all channels. In particular, for the user-item channel, DSSM+MIC achieves the best performance with 6.669 Recall, 9.396 NDCG, and 20.644 Hit Rate in Metric@20 and 10.819 Recall, 11.114 NDCG, and 30.173 Hit Rate in Metric@50 over Amazon Book. For item channel, applying cross-channel contrastive learning on ComiRec baseline with MIC as a plugin (denoted as ComiRec+MIC) achieves the best performance on these metrics. For the user channel, MIC plugged into deep structural semantic model (denoted as DSSM+MIC) outperforms all other models on two datasets. We have plugged our MIC into prevalent recommendation algorithms. As shown in Table 2, MIC successfully boost their performance of overall datasets.𝑋 + 𝑀𝐼𝐶achieve a signicant performance gain on all evaluation metrics than other retrieval models over two datasets across all channels. In particular,𝐷𝑆𝑆𝑀 + 𝑀𝐼𝐶 gain 9.13%, 18.01%, 12.62% over vanilla DSSM model in Recall@20, NDCG@20, Hit Rate@20 respectively over Amazon Book. We conduct extensive ablation experiments for our proposed MIC. Results of variants with various cross-channel contrastive settings in three dierent inference channels over Amazon Book are reported in Table 3. The most signicant improvements appear on the contrastive channel setting corresponding to a specic inference channel. For example, MIC in the I2I inference channel outperforms Table 4: Online A/B Test Results. We report the relative performance gain of MIC over Baseline in online A/B experiments. #Method Average Play Time ↑ Average Video Viewed ↑ Average Play Percentage ↑ Average Duration ↑ all other settings with the II channel contrastive module. This implies the superiority of each channel-based method in the specic inference channel. MIC with automatic weighted UI, UU, and II cross-channel contrastive setting achieves competitive channelspecic design results. Figure 4: Visualization of User and Item Representation in U2I, U2U and I2I channel. To further analyze the eectiveness and eciency of our integrated approach, we deploy the MIC on the real-world, large-scale recommender systems. The𝐴/𝐵test results of our proposed MIC and baseline are reported in Table 4. The baseline model is DSSM, a current state-of-the-art online retrieval model over the services with millions of users. In real-world online evaluation, we focus on the metric of Average Play Time, Average Video Viewed, Average Play Percentage, and Average Duration. MIC achieve signicant performance gain over baseline in all these metrics. After the anonymous reviewing period, we will give more statistical analysis about the online real-world dataset and A/B Tests and implementation details on the online experiment system. We analyze the agreement between user representations, item representations, and nal recall performance by the Alignment and Uniformity Metrics [36] (lower is better) of UI-Align, UU-Uniform, and II-Uniform. UI-Align measures the alignment between user and target item representation, UU-Uniform and II-Uniform measure the uniformly distributing of user and item representation, respectively. As shown in Figure 4, bright yellow denotes better Recall performance. Each point is marked with corresponding contrastive settings the same as Table 3: UI-UU-II means three contrastive learning objects were added, and Base means none contrastive learning objects were considered. For U2I Channel (rst row in Figure 4), the Recall performance is very sensitive to UI-align, and in no doubt, UI-align gets better when UI contrastive learning is considered. For U2U Channel (second row), UU-Uniform starts to play more important roles besides UI-align. We can nd the best recall scores in the bottom left of the "UI-Align, UU-Uniform" graph in U2U Channel Recall. Besides, U2U-Uniform would be better if we added contrastive learning between users. For I2I Channel (third row), IIUniform senses to be more important than UI-Align. The "UI-align, II-Uniform" graph shows that the best Recall appears in the lowest II-Uniform other than the lowest UI-align. We observe that if we can simultaneously acquire more aligned user-item representation, and more uniformed user-user, item-item representations, we can push the integrated model’s U2I, U2U, and I2I channel performance to the next stage. MIC is one of this type of model-agnostic integrated cross-channel model for recommendations. In this paper, we propose a model-agnostic integrated cross-channel (MIC) approach, semantically connecting users and items for the matching stage of a typical industrial recommender system by maximally leveraging the inherent multi-channel mutual information. Specically, MIC robustly models correlation across user-item, useruser, and item-item channels. MIC naturally aligns users and items with semantic similarity and distinguishes them otherwise in each channel. Extensive experiments show that our MIC helps several popular retrieval models boost performance on two real-world benchmarks. By deploying on industrial service with millions of users and conducting online experiments, we further conrm the scalability and exibility of the proposed method.