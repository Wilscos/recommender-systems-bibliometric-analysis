Adaptive approaches, allowing for more ﬂexible trial design, have been proposed for individually randomized trials to save time or reduce sample size. However, adaptive designs for cluster-randomized trials in which groups of participants rather than individuals are randomized to treatment arms are less common. Motivated by a cluster-randomized trial designed to assess the eﬀectiveness of a machine-learning based clinical decision support system for physicians treating patients with depression, two Bayesian adaptive designs for cluster-randomized trials are proposed to allow for early stopping for eﬃcacy at pre-planned interim analyses. The diﬀerence between the two designs lies in the way that participants are sequentially recruited. Given a maximum number of clusters as well as maximum cluster size allowed in the trial, one design sequentially recruits clusters with the given maximum cluster size, while the other recruits all clusters at the beginning of the trial but sequentially enrolls individual participants until the trial is stopped early for eﬃcacy or the ﬁnal analysis has been reached. The design operating characteristics are explored via simulations for a variety of scenarios and two outcome types for the two designs. The simulation results show that for diﬀerent outcomes the design choice may be diﬀerent. We make recommendations for designs of Bayesian adaptive cluster-randomized trial based on the simulation results. Randomized controlled trials, which can ensure that subjects assigned to each treatment group are comparable with respect to all characteristics of interest to draw a causal conclusion, have played an essential role in evaluating the eﬀectiveness of interventions [1]. In most trials designed to assess the eﬀect of a drug or a treatment, individual participants are randomly allocated to each treatment arm. However, some interventions naturally operate at a group level, or target either a social network or physical environment. For example, in a trial assessing the clinical utility, safety, and potential eﬀectiveness of a machine-learning based clinical decision support system (CDSS) developed by Aifred Health [2–5], the decision support tool is designed for physicians, naturally forming clusters of individual patients being treated by that physician [6, 7]. For these types of interventions, a population-level eﬀect is of more interests to researchers. tamination eﬀect between trial arms, as physicians might struggle to treat patients from diﬀerent treatment arms strictly diﬀerently. Contamination can cause dilution bias and aﬀect the reliability and validity of the study. One way to reduce the possibility of contamination is to randomize by physicians which act as ‘clusters’ [8, 9]. Thus, it may not be advisable to randomize individual participants to diﬀerent treatment arms, and groups of subjects being treated by their clinicians can instead be randomly assigned to the treatment arms in what is known as a cluster-randomized trial [9–12]. cluster-randomized trials diﬀer from individually randomized trials [13–15]. The independence assumption between participants’ outcomes is violated in a cluster-randomized trial since subjects from the same cluster tend to have more similar responses than subjects from a diﬀerent cluster. Correlation between subjects within the same cluster, as measured by intra-cluster correlation coefﬁcient (ICC), should be considered carefully when analyzing data from cluster randomized trials. The correlation reduces the variability of responses in a clustered sample and thus reduces the statistical power to detect true diﬀerences between treatment arms relative to trials that randomize the same number of individuals [16, 17]. into the design which allow for planned adjustments to the trial design after its initiation without undermining the validity and integrity of the trial [18]. In addition to ﬂexibility and eﬃciency, adaptive designs are attractive to clinical scientists because they may reﬂect medical practice in the real world and they are ethical with respect to the need to determine and monitor eﬃcacy as well as safety of the treatment [19]. Commonly used adaptive designs in clinical trials include, but are not limited to, adaptive randomization, group sequential design, and stopping rules [19, 20]. or estimates are continually updated based on the accumulated information In addition, randomizing by individuals in this setting may lead to a con- Due to the diﬀerence in randomization unit, the design and analysis of A natural way to improve trial eﬃciency is to incorporate adaptive features Adaptive designs naturally ﬁt into the Bayesian framework as that results from interim data [21]. Adaptive designs based on Bayesian approaches have been extensively studied in recent years [22–27]. However, most adaptive designs, regardless of whether frequentist or Bayesian, focus on individually randomized trials. Adaptive designs for cluster-randomized trials are less common and the incorporation of adaptive features poses signiﬁcant statistical challenges. Some speciﬁc adaptive features such as sample size re-estimation and group sequential design have been proposed in combination with clusterrandomized trials [28–31]. However, no formal statistical design of Bayesian adaptive cluster-randomized trials has been developed. We address this gap by proposing two Bayesian adaptive designs for cluster-randomized trials. described in section 2, followed by the development of two Bayesian adaptive designs and models for continuous and binary outcomes in section 3. Simulation studies are carried out in section 4, assessing the performance of the two proposed designs across a range of scenarios. The paper concludes in section 5. Aifred Health [2–5] has designed a machine-learning based clinical decision support system (CDSS) for physicians treating patients with depression. Patient characteristics of interests such as sociodemographic information, clinical information and medical history are input into the CDSS. The CDSS then, using a deep learning model, outputs the predicted eﬃcacy for a number of possible treatments for that patient [6, 7]. Treatments are ordered by eﬃcacy and presented to the physician when they reach the treatment selection step of a clinical algorithm based on best practice guidelines [3, 32]. Physicians with the CDSS can, on an individual patient basis, decide whether or not to use the information presented by CDSS as part of their medical decision-making. tial eﬀectiveness of a tool such as the CDSS. Practically, an intervention such as the CDSS must be delivered at the physician level, such that a cluster-randomized design would be appealing for the reasons described above. Participating physicians could be randomized, with patients recruited from physicians’ usual practices in order to approximate the real-world clinical conditions and populations as closely as possible. continuous measure of symptoms (e.g., a visual analog scale, the Quick Inventory of Depressive Symptomatology [33], the 9-question depression scale from the Patient Health Questionnaire [34], the World Health Organization Disability Assessment Schedule 2.0 [35], etc.) or a binary measure of treatment response, minimum clinically signiﬁcant change in symptoms, or remission, perhaps deﬁned by a dichotomization of a standard depression score [36–38]. The importance of mental health treatment and the often relatively slow rate of patient accrual motivate the use of a Bayesian adaptive trial design to ensure adequate sample size and the possibility of early termination due to treatment eﬀectiveness. The organization this paper is as follows. The motivating trial is brieﬂy An important step is to establish the clinical utility, safety, and poten- Typical outcomes in a study of depression in such a trial could be a Suppose that the maximum number of clusters and maximum cluster size are equal across the two treatment arms. Let K, n, m be the number of interim analyses (not including the ﬁnal analysis), the maximum number of clusters for each treatment arm, and the maximum cluster size, respectively. For simplicity of exposition, we will assume that all clusters enroll the same number of participants. Two Bayesian designs, design 1 and design 2 in the remainder of this paper, are developed to sequentially enroll participants and analyze interim data in diﬀerent ways. largest integer not exceeding x, and m individual participants will be enrolled for each cluster. At the subsequent analysis point, if the accumulated information up to the current analysis is suﬃcient to conclude the eﬃcacy of the treatment or the ﬁnal analysis has been reached, the trial will be terminated. Otherwise, another [ ticipants will be recruited for each new cluster. The trial then proceeds to the next analysis with new samples. This procedure is repeated until termination. but only [ subsequent analysis point, if the trial is not to be terminated, another [ individual participants are recruited for the same n clusters. The trial then proceeds to the next analysis. This procedure is repeated until termination. participants are sequentially recruited. In design 1, the clusters are sequentially enrolled, and individual participants for each cluster are recruited all at once. In design 2, all clusters are enrolled at one time, but the individual participants for each cluster are sequentially enrolled. For illustration, consider an example for one treatment arm with K = 1, n = 4 and m = 6. That is, there is only one interim analysis planned (thus two analyses in total, including the ﬁnal analysis). The maximum number of clusters is 4, and the maximum cluster size is 6. Figure 1 gives a graphical illustration. The black labelled circles represent diﬀerent individual participants for the corresponding clusters. In design 1, clusters 1 and 2 will be ﬁrst enrolled, and for each, six individual participants will be recruited. The interim analysis is based on data from clusters 1 and 2. If we decide to continue the trial, then we will further recruit clusters 3 and 4 and their respective six individual participants for the ﬁnal analysis. The ﬁnal analysis is based on data from all the four clusters. In design 2, all four clusters will be recruited at the start, but for each cluster, only three participants will be enrolled. If evidence based on the 12 individuals from 4 clusters is unable to conclude the eﬃcacy of the intervention at the interim analysis, then an additional three individual participants for each cluster will be recruited, and the trial proceeds to the ﬁnal analysis. In design 1, [] clusters enter the trial at the start where [x] denotes the In design 2, at the beginning of the trial, all n clusters enter the trial, The fundamental diﬀerence between the two designs lies in the way that Fig. 1: An illustration of the two designs for one treatment arm when there is only one pre-planned interim analysis, and at most four clusters with a total of six individual participants within each cluster over the course of the trial.The black labelled circles represent diﬀerent individual participants for each cluster. At each interim look, one should determine whether to stop the trial early or continue based on the interim result. The goal is to evaluate the eﬃcacy of the treatment by testing the hypothesis where θ is the mean diﬀerence for continuous outcome and risk diﬀerence for binary outcome. The eﬃcacy of the treatment can be concluded and the trial can be stopped early for eﬃcacy at the k-th interim analysis if where δ is the minimal important diﬀerence, D the k-th analysis and U is the decision boundary. Based on the context of the motivating trial and without loss of generality, assume that a smaller value of the continuous outcome is preferred (as most depression and disability rating scales associate worse symptoms with larger total scores). Then, θ = µ control and treatment groups, respectively. We further assume that at the kth analysis, k = 1, . . . , K + 1, there are n each cluster. Let Y cluster at current analysis point, j = 1, . . . , n assumption in cluster-randomized trials states where µ are within- and between-cluster variances respectively, and they can be related via the ICC, ρ = the response vector where Y covariance structure satisﬁes so that Y ∼ MVN( equal to µ and Σ is a block matrix of the form and Σ normal distribution. where a is the prior mean and b theorem, the posterior distribution for µ can be obtained: is the cluster-speciﬁc mean, µ is the population mean, σand σ is normal with mean µ and variance σ+ σ. Let Y = (Y, . . . , Y)be = cov(Y) speciﬁed in equation (2), and MVN indicates a multivariate In the Bayesian framework, assume a normal prior for µ, The above result applies to both µ P (µ where µ of µ posterior. Otherwise, the trial continues to enroll clusters/participants and proceeds to the next analysis where the prior mean and variance for the next analysis are updated with the posterior mean and variance for the current analysis. These steps are repeated until either the trial is stopped early for eﬃcacy or reaches the ﬁnal analysis. For binary outcomes, assume that a larger proportion is preferred (e.g., a larger proportion of patients meeting the criteria for treatment response). Let θ = π treatment groups, respectively. For binary outcomes, the joint distribution of all observations cannot be obtained analytically and thus a tractable form of the posterior distribution of the parameters of interest is not available. Therefore, a hierarchical model is proposed as follows where r speciﬁc proportion, j = 1, . . . , n proportion π However, in cluster-randomized trials with binary outcomes, it is assumed that where π is the population proportion and ρ is the ICC. , µare sampled from the corresponding posterior distributions and µ, and M is the number of Monte Carlo samples drawn from the At any interim analysis, if ˆπ > U, the trial is stopped early for eﬃcacy. − πwhere π, πare the population proportion for the control and is the number of events in the j-th cluster and πis the cluster- To make (3) and (4) consistent, deﬁne two transformed parameters where π is exactly the mean of the Beta distribution and v measures the information in the corresponding Beta distribution. Also, due to the consistency of (3) and (4), once ρ is ﬁxed or can be estimated, v can also be determined through v = Then the posterior probability P (π ing posterior samples using Markov Chain Monte Carlo (MCMC) implemented in, say RStan [39, 40], where R = (r number of events per cluster within each treatment group. The false positive rate and power cannot be obtained analytically in Bayesian adaptive trials since the sampling distributions of the test statistics (i.e., posterior probability statements (1) in section 3.2) are not known. Therefore simulation studies are required to specify the decision boundaries and other design characteristics [22]. For both outcomes, a single interim analysis was explored ﬁrst; two interim analyses were then investigated. The minimal important diﬀerence was set as 0 in the simulation but it is straightforward to generalize to other values. For each scenario, 500 simulation replications were performed. The performance of designs was compared based on false positive rate and power. The false positive rate is estimated as False positive rate = where falsely rejecting the null hypothesis means that for some k ≤ K + 1, for diﬀerence for binary outcome at the k-th analysis, θ is the true mean or risk diﬀerence, δ is minimal important diﬀerence, and δ = 0 in our simulation. D is all the available data up to the k-th analysis, and U is the decision boundary as described in section 3.2. In the Bayesian framework, assume a uniform prior for π, ˆθthe estimated mean diﬀerence for continuous outcome or estimated risk Power =Number of times correctly detecting the diﬀerenceNumber of simulation runs where correctly detecting the diﬀerence means that for some k ≤ K + 1, for θ mean for both groups are ﬁxed at 0 and 100, respectively. Various values of ICC were explored and the between-cluster variance σ through σ the n cluster-speciﬁc means from normal distributions with mean µ group) and µ samples are drawn from a normal distribution with mean equal to the clusterspeciﬁc mean and variance σ satisfy the preset correlation structure. For binary outcomes, clustered binary data are generated via Beta and binomial distributions; see Appendix C for details. All simulation parameters are summarized in Table 1. binary outcomes are summarized in Appendix C. Figures 2 and 3 show the false positive rate and power when a single interim analysis is planned. For power, only results for ρ = 0.2, 0.5, 0.8, representing low, moderate and high correlation, respectively, are displayed. In general, design 2 has higher power but also higher false positive rates compared with design 1. designs are well above 0.05 and thus are unsatisfactory. With U = 0.98, the false positive rates are reduced. Especially for design 1, the false positive rate can be controlled within 0.05 with U = 0.98. Therefore, U = 0.98 serves as a better decision boundary than U = 0.95. A larger decision boundary corresponds to a more conservative test since more evidence is required to conclude the treatment eﬃcacy. power is more direct. Under the same conditions, if the underlying ICC is higher, power is lower. For small ICC, if the underlying treatment eﬀect is moderate to large, then only 20 clusters for each group will be suﬃcient to ensure the desired power, but if the underlying treatment eﬀect is small, larger number of clusters, for example 40 or 60, should be considered. For moderate ICC and a small to moderate treatment eﬀect, a larger number of clusters is recommended; for a large treatment eﬀect only a small number of clusters, for example 20 clusters for each group may be enough. For large ICC, it is recommended to have more clusters than 60 as, even with a large treatment eﬀect, power is still low. the value of θ under the alternative hypothesis. For continuous outcomes, the prior mean and variance for the population Only results for continuous outcomes are presented here. The results for However, in all scenarios, with U = 0.95, the false positive rates for both The eﬀect of ICC on false positive rate is quite small but the eﬀect on Table 1: Simulation parameters for continuous and binary outcome Fig. 2: Plot of false positive rate versus ICC when n = 20, 40, 60 for (a) U = 0.95 and (b) U = 0.98 with single interim analysis planned. The dashed lines show the false positive rate of 0.05. Fig. 3: Plot of power versus true treatment eﬀect when n = 20, 40, 60, ρ = 0.2, 0.5, 0.8 for (a) U = 0.95 and (b) U = 0.98 with single interim analysis planned. The dashed lines show the power of 0.8. interim looks are built into the study design. Design 2 still has higher false positive rate and power compared with design 1. As was observed in Figures 2 and 3, a larger decision boundary can reduce false positive rate and the resulting reduction in power can be remedied by recruiting a larger number of clusters. Adding more interim analyses may increase the false positive rate and the power. Given a ﬁxed decision boundary, with more interim analyses planned, there is higher chance of rejecting the null hypothesis. analyses can bring obvious improvement in power under design 2. However,larger sample size is required for large ICC values to achieve suﬃcient Figures 4 and 5 show the false positive rate and power when multiple As seen in the plot, for large ICC values, the addition of multiple interim power. For small to moderate ICC, regardless of design, a single interim analysis is preferred. The eﬀect of cluster size on both false positive rate and power is very modest. The results for a larger cluster size are displayed in Figures B1 and B2 in Appendix B. Fig. 4: Plot of false positive rate versus number of interim looks for n = 20, 40, 60, ρ = 0.2, 0.5, 0.8, m = 8 for (a) U = 0.95 and (b) U = 0.98. The dashed lines show the false positive rate of 0.05. U = 0.98, and design 2 has a higher false positive rate and power. The choice between the two designs as well as the two decision boundaries depends on the research question, phase of the study, and the information we have about the treatment. For example, in some early phase clinical trials, if a relatively high false positive rate is acceptable, then design 2 with U = 0.95 may be recommended. However, for the situation we explored, a single interim analysis is preferred no matter which design or decision boundary is chosen. To conclude, both designs perform better in terms of false positive rate with Fig. 5: Plot of power versus number of interim looks for n = 20, 40, 60, ∆ = 0.2, 0.5, 0.8, m = 8 with the subpanels (a)-(f) indicating all possible combinations of ρ = 0.2, 0.5, 0.8 and U = 0.95, 0.98. The dashed lines show the power of 0.8. Motivated by a potential real-world cluster-randomized trial, we explore the statistical properties of Bayesian adaptive cluster-randomized trial designs. We explore stopping rules for eﬃcacy in a cluster-randomized trial. Interim analyses may be planned over the course of the trial and at each interim analysis the trial may be stopped early if suﬃcient evidence is established to conclude eﬃcacy otherwise the trial proceeds to the next analysis. We proposed two designs which sequentially enroll participants in diﬀerent ways. The ﬁrst design sequentially enrolls clusters, and individual participants for each cluster are recruited all together. The second recruits all clusters at the start of the trial, then sequentially enrolls batches of participants. Regardless of the design choice, the data analysis procedure is the same: the diﬀerence between the designs lies in the sequential enrollment of participants. based on common models. For continuous outcomes, on the basis of the normality assumption, we obtained the analytical form of the posterior distribution of the population mean. Based on Monte Carlo simulation, the posterior probability of eﬃcacy can be easily estimated by drawing random samples directly from the two posterior distributions, one for each treatment group. For binary outcomes, due to the complex correlation structure, the posterior distribution of population proportion cannot be obtained analytically. Instead, a hierarchical model was used and the posterior probability of eﬃcacy is estimated via MCMC. ferent outcomes. The preferred design may also depend on the research goal, phase of study, or feasibility considerations. For binary outcomes, within the parameter space we explored, design 2 is recommended based on design operating characteristics. Also, for both outcomes, one interim analysis may be recommended for the situations we investigated, as it makes the designs more feasible and the design performance with single interim analysis is satisfactory. However, a more general recommendation requires further exploration to account for practical issues or other design parameters we did not explore in our simulation. In addition, increasing cluster size may not necessarily bring much improvement in design performance. Keeping a small cluster size may not hurt design operating characteristics very much but it will save time and cost. In our motivating trial, a smaller cluster size is also more realistic, as it reduces the burden on the individual clinician to ﬁnd suitable patients from their practice. menting a stopping rule for eﬃcacy. There are many other adaptive features that we have not considered. One example is adaptive randomization. The response-adaptive randomization [41] has not been extensively investigated from Bayesian viewpoint. Second, we only focus on two-arm trials. Extension to multi-arm trials which may involve arm dropping can be considered in the future. Third, we only considered continuous and binary outcomes. However, survival endpoints are also prevalent in clinical trials. An extension to survival outcomes based on survival models can also improve the framework of Bayesian adaptive cluster-randomized trial. In addition, the eﬀect of unequal cluster sizes has not been explored for our designs. However, the eﬀect of varying cluster sizes on design operating characteristics in standard clusterrandomized trials has been extensively discussed [14, 42–45]. Typically, designs Interim analyses of both continuous and binary outcomes are performed Through simulation, we found the design choice may be diﬀerent for dif- There are some limitations to our work. First, we only considered impleare most eﬃcient for equal cluster sizes. Inﬂation of false positive rate will occur for imbalanced studies, and with the same sample size, imbalanced trials may be underpowered compared with their balanced counterparts. Similar impacts of unequal cluster sizes on power or false positive rate may also exist for our Bayesian adaptive cluster-randomized trials, and this would need to be determined in future work. Also, practical issues in planning a Bayesian adaptive cluster-randomized trial are not taken into account in our paper. For example, sometimes it may not be realistic to plan the preferred design recommended by design operating characteristics. In this case, feasibility may be the main reason for design choice. These context-speciﬁc issues require more exploration in the future. D.B. is a director, shareholder, and employee of Aifred Health. This work was supported by funding from MITACS Accelerate Grant #ACC IT18791.