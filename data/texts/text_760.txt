Abstract—Graph neural networks (GNNs) are deep convolutional architectures consisting of layers composed by graph convolutions and pointwise nonlinearities. Due to their invariance and stability properties, GNNs are provably successful at learning representations from network data. However, training them requires matrix computations which can be expensive for large graphs. To address this limitation, we investigate the ability of GNNs to be transferred across graphs. We consider graphons, which are both graph limits and generative models for weighted and stochastic graphs, to deﬁne limit objects of graph convolutions and GNNs—graphon convolutions and graphon neural networks (WNNs)—which we use as generative models for graph convolutions and GNNs. We show that these graphon ﬁlters and WNNs can be approximated by graph ﬁlters and GNNs sampled from them on weighted and stochastic graphs. Using these results, we then derive error bounds for transferring graph ﬁlters and GNNs across such graphs. These bounds show that transferability increases with the graph size, and reveal a tradeoff between transferability and spectral discriminability which in GNNs is alleviated by the pointwise nonlinearities. These ﬁndings are further veriﬁed empirically in numerical experiments in movie recommendation and decentralized robot control. Index Terms—graph neural networks, transferability, graph signal processing, graphons Graph neural networks (GNNs) are deep learning models for network data popularized by their state-of-the-art performance in a number of learning tasks [1]–[4]. Besides working well in practice, GNNs are veriﬁably invariant to node relabelings and stable to graph perturbations [5], [6], which they inherit from graph convolutions. Graph convolutions are also to credit for the ability to implement GNNs in a distributed way [7]. Since the graph convolution is a graph matrix polynomial, it can be calculated independently at each node as a series of local agreggations of the information on the graph. Another beneﬁt of the convolutional parametrization of GNNs is that their number of parameters does not depend on the graph size. Indeed, the number of parameters of the GNN is typically much smaller than the number of nodes, which leads to less computational complexity than, e.g., fully connected neural networks, and thus allows GNNs to scale to very large graphs. In practice, however, learning GNNs for large-scale graphs can be difﬁcult. Graph convolutions require full knowledge of the graph structure, which may be hard to measure; and large matrix polynomials are expensive to compute, especially if the graph is not sparse. But here, too, the independence between the GNN parametrization and the graph comes in handy, because it allows for the possibility of transferring a GNN across graphs. Speciﬁcally, we could train a GNN on a graph of moderate size, and then keep the learned weights to execute it on a larger (but still similar) graph. To do so, we need to establish two notions. First, we need to have a way to quantify graph similarity. Second, we need to understand whether this GNN would work well on the target graph. In this paper, we deﬁne similar graphs as graphs that share structural characteristics in the sense that they have approximately the same densities of certain motifs. These graphs can be grouped in a family identiﬁed by a graphon. We then quantify the ability of an arbitrary GNN to retain performance—i.e., its transferability— when it is transferred across graphs associated with the same graphon. Graphons are bounded symmetric kernels W : [0, 1]→ [0, 1] which describe graph families because they are the limit objects of convergent graph sequences and generative models for both weighted (Deﬁnitions 1 and 2) and stochastic graphs (Deﬁnition 3). They have been widely used in applied mathematics [8]–[11], statistics [12]–[14], game theory [15], network science [16], [17] and controls [18]. To formalize the transferability properties of GNNs across graphs associated with the same graphon, we rely on the limit objects of graph convolutions and GNNs—graphon convolutions and graphon neural networks (WNNs)—which we interpret as generative models for graph convolutions and GNNs. We show that these graphon ﬁlters and WNNs can be approximated by graph ﬁlters (Theorem 1) and GNNs (Theorem 3) sampled from them on stochastic (and therefore also on weighted) graphs. Using these results, we then derive probabilistic transferability error bounds for both graph ﬁlters (Theorem 2) and GNNs (Theorem 4). These are the main contributions of this paper. They imply that transferability increases with the size (i.e., the number of nodes) of the graphs, but with the caveat that some spectral components are not transferable across ﬁnite graphs even when they very large. This is a consequence of the fact that the graphon eigenvalues accumulate in certain parts of the graphon spectrum. These results further reveal a tradeoff between the transferability and spectral discriminability of graph convolutions, which is inherited by GNNs. The difference in GNNs is that this tradeoff is alleviated by the nonlinear activation functions, whose scattering behavior increases discriminability while maintaining the same level of transferability. We verify this empirically by comparing the ability of graph ﬁlters and GNNs to be transferred in movie recommendation and decentralized robot control (Section V). This paper follows from a series of works on graphon signal processing (WSP), including [19]—which introduces WSP and proves convergence of graph to graphon ﬁlters—and [20]— which deﬁnes WNNs and studies GNN transferability over a narrow class of weighted graphs. We expand upon the results of [20] by considering graphs with both stochastic nodes and edges and proving tighter bounds. Transferability of GNNs has also been studied in [21], which considers graphs sampled from generic topological spaces and therefore yields a different asymptotic regime relative to the graphon; and in [22], which extends the transferability analysis from [21] to graphons but does not address the spectral implications of transferability as we do in this paper. Closely related to transferability, and considering a graph model akin to a graphon with tunable sparsity, [23] studies GNN stability on random graphs. GNN stability is also studied in the seminal paper by [6], which follows from stability analyses of graph scattering transforms [24]. Other related work includes the convergence analysis of graph ﬁlters in [25] and [26], which can be seen as asymptotic graph ﬁlter transferability results. This section discusses GNNs, graphons and how graphons can be used as generative models for graphs. We consider data supported on undirected graphs G = (V, E, W) where V is a set of |V| = n nodes, E ⊆ V × V is a set of edges, and W : E → R is a symmetric function assigning real weights to the edges in E. This data is represented in the form of vectors x ∈ Rcalled graph signals where [x]is the value of the signal at node i [27], [28]. In order to parametrize signal processing operations that depend on the graph topology by G, a generic graph matrix representation S ∈ R, named graph shift operator (GSO), is deﬁned. The GSO S is such that [S]= s6= 0 if and only if i = j or (i, j) ∈ E. The graph adjacency [S]= [A]= W(i, j) [29], the graph Laplacian S = L = diag(A1) − A [27], and the random walk Laplacian S = diag(A1)L [30] are all examples of matrices that satisfy this property. Unless otherwise speciﬁed, in this paper we consider S = A and replace W by S in the triplet G = (V, E, S) to simplify notation. The GSO gets its name from the fact that it deﬁnes a notion of shift for signals x on the graph G. This is because, at each node i, the signal z = Sx resulting from an application of the GSO to x can be written elementwise as where N (i) = {j : [S]6= 0} is the neighborhood of i. In other words, the outcome of Sx at node i is a result of neighboring nodes j shifting their data values [x], each weighted by the proximity measure [S], to i. The notion of a graph shift, in turn, allows deﬁning linear shift-invariant (LSI) or convolutional ﬁlters on graphs. Let h = [h, . . . , h]be a vector of K real coefﬁcients. The graph convolution with coefﬁcients h is deﬁned as [31], [32] i.e., it is given by the application of an order K polynomial of S to the signal x. This is analogous to the deﬁnition of the convolution as a shift-and-sum operation in Euclidean domains. Because S is symmetric, it can be diagonalized as S = VΛV. The matrix Λ is a diagonal matrix whose diagonal elements are the graph eigenvalues which we assume to be ordered according to their sign and in decreasing order of absolute value, i.e., λ≤ λ≤ . . . ≤ 0 ≤ . . . ≤ λ≤ λ. These are interpreted as graph frequencies, with high magnitude eigenvalues corresponding to the low frequencies and eigenvalues close to zero to the high frequencies. The graph eigenvectors, given by the columns of V, can be seen as the graph’s oscillation modes. Importantly, the eigenvectors in V form an orthonormal basis of Rwhich we call the graph spectral basis. The graph Fourier transform (GFT) of x is deﬁned as the projection of x onto this basis. Explicitly, Similarly, the inverse GFT is deﬁned as x = Vˆx. Substituting S = VΛVinto (1) and calculating the GFT of y = H(S)x, we get from which we obtain the spectral representation of the graphP convolution h(λ) =hλ. There are two important things to note about the function h. First, while the GFTs of x and y depend on the eigenvectors of S, the spectral response of H(S) is obtained by evaluating h only at the eigenvalues of the graph. This is illustrated in Figure 2, where the red curve is h(λ) and the green lines are the eigenvalues of the graph G. Second, as a direct consequence of the Cayley-Hamilton theorem, polynomial ﬁlters H(S) of order K = N can be used to implement any ﬁlter with spectral representation h(λ) = f(λ) where f is analytic. In other words, any smooth function f whose Taylor series converges can be implemented as a graph convolution on G. A GNN is a deep convolutional architecture where each layer consists of (i) a bank of convolutional ﬁlters like the one in (1) and (ii) a nonlinear activation function. The convolutional ﬁlterbank transforms the Ffeatures xfrom layer ` − 1 into Fintermediate linear features given by where 1 ≤ f ≤ F. Speciﬁcally, the ﬁlter H(S) maps feature g of layer `−1 into feature f of layer `. The collection of ﬁlters H(S) for all f, g yields a ﬁlterbank {H(S)} with a total of F× Fﬁlters like the one in (1). The activation function is usually a pointwise nonlinearity such as the ReLU or sigmoid, but localized implementations incorporating the graph structure into the computation of the nonlinearity have also been proposed [5]. Letting σ denote the activation function, the `th layer of the GNN can be written Because σ is pointwise, at each node i ∈ V we have [x]= σ([u]). If the total number of layers of the GNN is L, (4)– (5) are repeated for layers ` = 1 through ` = L, the output of each layer being the input to the next. At ` = 1, the input features xare the input data xfor 1 ≤ g ≤ F. The GNN output is given by the features of the last layer, i.e., y= x. The goal of graph machine learning is to obtain meaningful representations y from input data x (e.g., through empirical risk minimization). In this context, we represent both graph ﬁlters and GNNs as parametric maps Φ : x 7→ y learned to produce such representations. When Φ is a graph ﬁlter y =P H(S)x =hSx [cf. (1)], we write where h = [h, . . . , h]. When it is a L-layer GNN with inputs x = {x}and outputs y = {y}[cf. (4)–(5)], we write where H = [h]is a tensor grouping the GNN parameters hfor all layers 1 ≤ ` ≤ L and features 1 ≤ f ≤ Fand 1 ≤ g ≤ F. This notation is particularly convenient for GNNs because it summarizes the sequence of computations in (4)–(5) for ` = 1, . . . , L into a single function Φ. It also highlights the fact that the graph ﬁlter parameters h and the GNN parameters H are independent of the GSO S (and thus of the graph G). Graphons are the limit objects of sequences of dense undirected graphs. They are deﬁned as functions W : [0, 1]→ [0, 1] where W is bounded, measurable and symmetric. An example of graphon–the constant or Erd¨os-R´enyi graphon— as well as a sequence of graphs converging to it are shown in Figure 1. A sequence of graphs that converges to a graphon is said to converge in the homomorphism density sense. Given an unweighted graph F = (V, E) and a (possibly weighted) graph G = (V, E, S), a homomorphism between F and G is deﬁned as a map β : V→ V such that, if (i, j) ∈ E, (β(i), β(j)) ∈ E. Because there are multiple such maps between the nodes of F and G, to count them we deﬁne the number of homomorphisms between F and G as For unweighted graphs G, hom(F, G) is the total number of adjacency preserving maps between Vand V. For weighted graphs, each map β is weighted proportionally to the product of the weights of the edges (β(i), β(j)) ∈ E for all i, j ∈ V. Since homomorphisms are only a fraction of the total number of maps between the nodes of F and the nodes of G, we can deﬁne a density of homomorphisms from F to G. This density is denoted t(F, G) and given by The density t(F, G) can be likened to the probability of sampling a homomorphism when picking a random map β : V→ V. This allows extending the notion of density of homomorphisms to graphons. Explicitly, we deﬁne the density of homomorphisms from the graph F to the graphon W as t(F, W) =W(u, u)du i.e., t(F, W) is the probability of sampling the graph F from the graphon W. A sequence of graphs {G} converges to W if and only if the density of homomorphisms between any ﬁnite, undirected and unweighted graph F = (V, E) and the Gconverges to the density of homomorphisms between F and W. More formally, G→ W if and only if for all ﬁnite, undirected and unweighted F [9]. We refer to graphs F as motifs to emphasize that a graph sequence only converges when the densities of certain graph motifs converge. The other important interpretation of graphons is as generative models for graphs. A graph G= (V, E, S) can be obtained from the graphon W in two steps. First, n points u∈ [0, 1] are chosen to to be the labels of the nodes i ∈ V. Then, the edges Eand the adjacency matrix S are determined either by assigning weight W(u, u) to (i, j), or by sampling the edge (i, j) with probability W(u, u). There are many different strategies for choosing the node labels uand, given the possibility of deﬁning either weighted (deterministic) or stochastic (unweighted) edges E, there are many types of graphs that can be generated in this way. We focus on three: template graphs, weighted graphs, and stochastic graphs. Deﬁnition 1 (Template graphs). Let {u}be the regular n-partition of [0, 1], i.e., for 1 ≤ i ≤ n. The n-node template graph G, whose GSO we denote S, is obtained from W as for 1 ≤ i, j ≤ n. Template graphs are the simplest type of graph that can be generated from a graphon. The node labels uare determined by partitioning [0, 1] into n intervals of equal length, and the edge weights [S]are obtained by evaluating the graphon at (u, u). Template graphs are always weighted and, if W is strictly positive, they are also complete (i.e., all nodes are connected to every node of the graph). Deﬁnition 2 (Weighted graphs). Let {u}be n points sampled independently and uniformly at random from [0, 1], i.e., Figure 1. Constant graphon with W(u, v) = 0.4 for all u, v ∈ [0, 1] (d). Sequence of stochastic graphs with 50 (a), 100 (b), and 200 nodes (c) which converges to this graphon. for 1 ≤ i ≤ n. The n-node weighted graph G, whose GSO we denote S, is obtained from W as for 1 ≤ i, j ≤ n. Weighted graphs are obtained by sampling uuniformly at random from the unit interval and assigning weights W(u, u) to edges (i, j). As the name implies, these graphs are weighted, and like template graphs, they can also be complete. Deﬁnition 3 (Stochastic graphs). Let {u}be n points sampled independently and uniformly at random from [0, 1], i.e., for 1 ≤ i ≤ n. The n-node stochastic graph G, whose GSO we denote S, is obtained from W as for 1 ≤ i, j ≤ n. Stochastic graphs are stochastic in both the node labels u, which are sampled uniformly at random, and in the edges (i, j), which are sampled with probabilities W(u, u). Also called W-random graphs [8, Ch. 10.1], these graphs are always unweighted, i.e., S∈ {0, 1}. Note that stochastic graphs can also be obtained from weighted graphs as Thus, weighted graphs may be used as (discrete) random graph models for n-node unweighted graphs. We conclude this section by introducing the notion of a graphon induced by a graph, which will be useful in later derivations. Let G be a graph with GSO S ∈ Rand node labels {u}. If the node labels are unknown, simply deﬁne u= (i − 1)/n for 1 ≤ i ≤ n. Further deﬁne the intervals I= [u, u) for 1 ≤ i ≤ n − 1 and I= [u, 1] ∪ [0, u). The graphon Winduced by G is given by where I is the indicator function. This section introduces graphon ﬁlters through the graphon signal processing framework (Section III-A) and discusses its implications to graph ﬁlter transferability (Section III-B). The natural limit of signals supported on graphs that are sampled from or converge to a graphon is the graphon signal. A graphon signal is a function X ∈ L([0, 1]) and, like the graphon, it is both the limit of a convergent sequence of graph signals and a generative model for graph signals on graphs instantiated from a graphon [19]. For instance, given a template graph Ggenerated from the graphon W as in Deﬁnition 1, the graphon signal X can be used to generate the graph signal for 1 ≤ i ≤ n. Similarly, on the weighted graph Gand on the stochastic graph G[cf. Deﬁnitions 2 and 3], X can be used to generate equivalent signals xand xdeﬁned as for 1 ≤ i ≤ n, where the uare sampled independently and uniformly at random from [0, 1]. Graphon signals can take any form, but the most straightforward ones are those induced by graph signals. The graphon signal induced by a graph signal x∈ Ris a step function given by [33] where I= [u, u) for 1 ≤ i ≤ n − 1 and I= [u, 1] ∪ [0, u). When the node labels are not deﬁned, we set u= (i − 1)/n, i/n) for 1 ≤ i ≤ n. The diffusion operator for graphon signals, denoted T, is the integral operator with kernel W, i.e., In analogy with the GSO, we refer to this operator as the graphon shift operator (WSO) of W. Because graphons are bounded and symmetric, the WSO is a self-adjoint Hilbert-Schmidt (HS) operator. This allows expressing W in the operator’s spectral basis as W(u, v) =P where the eigenvalues λ, i ∈ Z \ {0}, follow the ordering 1 ≥ λ≥ λ≥ . . . ≥ . . . ≥ λ≥ λ≥ −1. Since T is compact, the eigenvalues accumulate around 0 as |i| → ∞ [34, Theorem 3, Chapter 28]. This is illustrated in Figure 2. Similarly to how the GSO eigenvalues are interpreted as graph frequencies, we interpret the graphon eigenvalues as graphon frequencies and the graphon eigenfunctions as the graphon’s oscillation modes. High magnitude eigenvalues represent the low frequencies, and vice-versa. Since the eigenfunctions ϕform an orthonormal basis of L([0, 1]), we can deﬁne the graphon Fourier transform (WFT) of a graphon signal X as its projection onto this eigenbasis. Explicitly, the WFT of X is denotedˆX and given by [33] Conversely, the inverse graphon Fourier transform or iWFT isP A graphon ﬁlter is an operator T: L([0, 1]) → L([0, 1]) which maps graphon signals X to Y = TX. In particular, the graphon shift operator allows deﬁning LSI graphon ﬁlters given by (TX)(v) =W(u, v)(TX)(u)du where T= I is the identity operator and h = [h, . . . , h] are the ﬁlter coefﬁcients. This ﬁlter is shiftinvariant because Tand Tcommute. I.e., if we shift the input as X= TX, the output is equivalently shifted as Y= TX= TTX = TTX = TY . LSI graphon ﬁlters are also called graphon convolutions [19]. Using the spectral decomposition of Tin (15), Tcan be written as This expression highlights the ﬁlter’s spectral representation,P which is given by h(λ) =hλ. Because the WFT of the ﬁlter is a polynomial on the graphon eigenvalues, note that as K → ∞ LSI graphon ﬁlters can be used to implement any graphon ﬁlter with spectral representation h(λ) = f(λ) where f is analytic. This is a consequence of the CayleyHamilton theorem in the limit of matrices. Moreover, unlike the WFT of the signals X and Y , the spectral representation of the LSI graphon ﬁlter does not depend on the graphon eigenfunctions—only on the graphon eigenvalues and the coefﬁcients h. This is illustrated in Figure 2 where the red curve represents h(λ) and the blue lines are the eigenvalues of W. The fact that the spectral response of the LSI graphon ﬁlter only depends on the eigenvalues of the graphon and on the ﬁlter coefﬁcients is something that these ﬁlters have in common with LSI graph ﬁlters. This can be seen by comparing (3) and (18). Indeed, if the coefﬁcients hin these equations are the same, the spectral responses of the LSI graph ﬁlter and of the LSI graphon ﬁlter are determined by the same function h(λ). Given a graph G(with GSO S) and a graphon W, the only thing that changes is where this function is evaluated: at the graph eigenvalues {λ(S)}, or at the graphon eigenvalues {λ(T)}[cf. Figure 2]. The implication is that graphon ﬁlters can be used as generative models for graph ﬁlters as long as the coefﬁcients hin (3) and (18) are the same. For example, let Y = TX be a LSI graphon ﬁlter which we represent, for simplicity, as the map Y = Φ(X; h, W) to emphasize the dependence on the coefﬁcients h and the graphon W. We can generate a graph ﬁlter y= Φ(x; h, S) from this graphon ﬁlter by instantiating a graph Gfrom W as in Deﬁnitions 1, 2 or 3, and the corresponding graph signal xas in (11) or (12). Generating graph ﬁlters from graphon ﬁlters is useful because it allows designing ﬁlters for graphons and transferring them to graphs. Since graphons are limits of convergent graph sequences, it also justiﬁes transferring ﬁlters across graphs obtained from the same graphon. Before delving into the transferability analysis of graph ﬁlters, we introduce the deﬁnition of graphon ﬁlters induced by graph ﬁlters, which will allow comparing graph and graphon ﬁlters directly. The graphon ﬁlter induced by the graph ﬁlterP Φ(x; h, S) =hSxis given by (TX)(v) =W(u, v)(TX)(u)du where the graphon W= Wis the graphon induced by G(10) and Xis the graphon signal induced by the graph signal x(13). The graph ﬁlter (and GNN) transferability results rely on four important deﬁnitions. The c-band cardinality and the ceigenvalue margin are graphon spectral properties associated with a ﬁxed eigenvalue threshold c. Deﬁnition 4 (c-band cardinality of W). The c-band cardinality of a graphon W, denoted B, is the number of eigenvalues λof W with absolute value larger or equal to c, i.e., Figure 2. Graphon eigenvalues (blue) and graph eigenvalues (green) for a graph G For a ﬁxed set of parameters h[cf. (3) and (18)], the red curve represents a ﬁlter’s frequency or spectral response. The graphon ﬁlter with coefﬁcients hhas frequency response given by the blue dots. The graph ﬁlter with coefﬁcients h green dots. Note that, to have transferability (Theorem 2), the frequency response must vary slowly for eigenvalues with magnitude close to zero. This is indicated by the shaded red rectangle from -0.1 to 0.1. Deﬁnition 5 (c-eigenvalue margin of W and W). The ceigenvalue margin of W and W, denoted δ, is given δ= min{|λ(T) − λ(T)| : |λ(T)| ≥ c} where λ(T) and λ(T) are, respectively, the eigenvalues of Wand W. The node stochasticity and the edge stochasticity are error terms that arise when the node labels, and respectively the edge labels, are stochastic. Deﬁnition 6 (Node stochasticity). For ﬁxed χ ∈ [0, 1], the node stochasticity constant on n nodes, denoted α(χ, n), is deﬁned as Deﬁnition 7 (Edge stochasticity). For ﬁxed χ ∈ [0, 1], the edge stochasticity constant on n nodes, denoted β(χ, n), is deﬁned asp We also introduce the following Lipschitz continuity assumptions on the graphon, the ﬁlter spectral response, and the graphon signal. AS1. The graphon W is A-Lipschitz, i.e., |W(u, v) − W(u, v)| ≤ A(|u− u| + |v− v|). AS2. The spectral response of the convolutional ﬁlter h is A-Lipschitz in [−1, −c] ∪ [c, 1] and a-Lipschitz in (−c, c), with a< AMoreover, |h(λ)| < 1. AS3. The graphon signal X is A-Lipschitz. Under assumptions AS1–AS3, we can prove the following proposition, which states that for large n a graphon ﬁlter can be approximated by a graph ﬁlter on a template graph. Proposition 1 (Graphon ﬁlter approximation on a template graph). Let Y = Φ(X; h, W) be a graphon ﬁlter [cf. (18)] satisfying assumptions AS1–AS3. Given a template graph G with GSO S[cf. Deﬁnition 1], let y= Φ(x; h, S) be the graph ﬁlter instantiated from Φ(X; h, W) on this graph. For any 0 < c ≤ 1, it holds that where W= Wis the graphon induced by Gand Y= Φ(X; h, W) is the graphon ﬁlter induced by y= Φ(x; h, S) (19). Proof. Refer to Appendix A in the supplementary material. Proposition 1 shows that the error incurred when transferring a graphon ﬁlter to a template graph is upper bounded by the sum of three terms. The ﬁrst is given by the norm of the input signal X weighted by what we call the ﬁlter’s transferability constant. The smaller the transferability constant, the more transferable the ﬁlter. Note that this constant is directly proportional to the ﬁlter variability through A, B and δ, and to the graphon variability through A. It is inversely proportional to the graph size n, hence, the ﬁlter is more transferable on large template graphs. The fact that the transferability constant decreases with n also indicates that graphon ﬁlters can be approximated by graph ﬁlters, and that the approximation becomes better with the size of the graph. The second term is a ﬁxed error term which does not depend on the norm of the input signal X. This error stems from the discretization of the input signal on the template graph Gand, as such, is proportional to the signal variability A and decreases with n. Additionally, it depends on ac, which upper bounds the maximum ﬁlter ampliﬁcation in a part of the eigenvalue spectrum determined by a given threshold c— the band (−c, c), see Figure 2. Since ais smaller than A, the contribution of this term to the approximation error of the ﬁlter is small. The constant c also appears in the third term of the bound 2ackXk, which is related to the output signal’s nontransferable energy for eigenvalues smaller than c. For a given value of c, spectral components associated with λ ∈ [−1, −c]∪ [c, 1] (i.e., low-frequency components) are completely transferable in the sense that, if the signal X or the ﬁlter Φ(X; h, W) are c-bandlimited (i.e., spectral components associated with eigenvalues |λ| ∈ [0, c) are zero), the transferability bound is only given by the ﬁrst two terms of (20) and thus vanishes with the size of the graph. In the more general case in which the input signal is not bandlimited, regardless of how we ﬁx c ∈ (0, 1] there will be some residual non-transferable energy associated with the spectral components |λ| ∈ [0, c). However, the effect of the non-transferable spectral components on the transferability error is attenuated by the ﬁlter since its Lipschitz constant on the (−c, c) interval, a, is smaller than A, the Lipschitz constant on [−1, −c] ∪[c, 1]. In particular, if a Athe ﬁlter resembles a ﬁlter with constant frequency response on the (−c, c) band. See Section IV-C for more indepth discussion on the non-transferable energy and ﬁlters with constant band. Using concentration inequalities for the uniform distribution, in Proposition 2 we extend Proposition 1 to weighted graphs. Proposition 2 (Graphon ﬁlter approximation on a weighted graph). Let Y = Φ(X; h, W) be a graphon ﬁlter [cf. (18)] satisfying assumptions AS1–AS3. Given a weighted graph G with GSO S[cf. Deﬁnition 2], let y= Φ(x; h, S) be the graph ﬁlter instantiated from Φ(X; h, W) on this graph. Let χ, χ∈ (0, 0.3]. For any 0 < c ≤ 1 and n ≥ 4/χ, with probability at least [1 − 2χ] × [1 − χ] it holds that where W= Wis the graphon induced by Gand Y= Φ(X; h, W) is the graphon ﬁlter induced by y= Φ(x; h, S) (19). Proof. Refer to Appendix A in the supplementary material. Hence, graphon ﬁlters are also transferable to weighted graphs. The main difference between Proposition 2 and Proposition 1 is that the ﬁrst and second terms of the transferability bound are now multiplied by the node stochasticity constant α(n, χ) [cf. Deﬁnition 6]. This constant shows up because of the randomness associated with the node labels u, which are sampled uniformly at random. Note that α(n, χ) depends on both the graph size and χ. The value of χdetermines the conﬁdence of the transferability bound. This conﬁdence, given by [1 − 2χ] × [1 − χ], also depends on the parameter χ. Although χdoes not appear in (21), there is an interplay between the conﬁdence bound and the minimum graph size: Proposition 2 holds with probability [1 − 2χ] × [1 − χ] for n ≥ 4/χ. Next, we extend the graphon-graph ﬁlter approximation result to its more general form—graphon ﬁlter approximation by graph ﬁlters supported on stochastic graphs. To do so, we need one additional assumption on the size of the graph. This assumption imposes a restriction on n related to the graphon variability Aand its maximum degree d. AS4. Given χ∈ (0, 1), n is such that where d= maxW(u, v)dv. We also need the following lemma, which upper bounds the transferability error for a graph ﬁlter transferred between a weighted graph and a stochastic graph sampled from it (see (9)). Lemma 1 (Graph ﬁlter transferability from weighted to stochastic graphs). Consider a graphon W satisfying assumption AS1, and let Gbe a weighted graph with GSO S[cf. Deﬁnition 2] and Ga stochastic graph with GSO Sobtained from Sas in (9). Given a graphon signal X satisfying assumption AS3, let y= Φ(x; h, S) and y= Φ(x; h, S) be graph ﬁlters satisfying assumption AS2 and acting on the graph signals xand xinstantiated from X on Gand G respectively (note that x= x). Let χ∈ (0, 1). For any 0 < c ≤ 1 and n satisfying assumption AS4, with probability at least 1 − χit holds that where W= Wand W= Ware the graphons induced by Gand Grespectively. Proof. Refer to Appendix B in the supplementary material. The graphon ﬁlter approximation result is then obtained by combining Proposition 2 and Lemma 1 using a simple triangle inequality argument. Theorem 1 (Graphon ﬁlter approximation on a stochastic graph). Let Y = Φ(X; h, W) be a graphon ﬁlter [cf. (18)] satisfying assumptions AS1–AS3. Given a stochastic graph Gwith GSO S[cf. Deﬁnition 3], let y= Φ(x; h, S) be the graph ﬁlter instantiated from Φ(X; h, W) on this graph. Let χ, χ, χ∈ (0, 0.3]. For any 0 < c ≤ 1 and n ≥ 4/χsatisfying assumption AS4, with probability at least [1 − 2χ] × [1 − χ] × [1 − χ] it holds that kY− Y k ≤A+πBδ2(Aα(n, χ) + β(n, χ))nkXk where W= Wis the graphon induced by Gand Y= Φ(X; h, W) is the graphon ﬁlter induced by y= Φ(x; h, S) (19). Proof. Theorem 1 follows directly from Lemma 1, Proposition 2 and the triangle inequality. Note that the probabilities [1 − χ] × [1 − χ] and 1 − χare multiplied because Lemma 1 holds for any weighted graph G, i.e., it is independent of the weighted graph. This approximation bound is similar to the approximation bound derived for weighted graphs in Proposition 2, with two important differences. The ﬁrst is that, in addition to depending on the node stochasticity constant α(n, χ), the transferability constant also depends on the edge stochasticity β(n, χ) [cf. Deﬁnition 7] which accounts for the randomness of the edges of G. Besides modifying the value of the transferability constant for stochastic graphs, note that β(n, χ) also lowers the conﬁdence of the bound. The second difference is that the part of the bound due to non-transferable spectral components is twice the same amount in Proposition 2. This is a result of the summation of the non-transferable energy between the graphon and the weighted graph (Proposition 2), and between the weighted graph and the stochastic graph (Lemma 1). Since a graphon identiﬁes a family of graphs, it is ready to extend Theorem 1 to a graph ﬁlter transferability result. If we can bound the error made when (i) transferring a graphon ﬁlter to a stochastic graph Gand (ii) transferring the same graphon ﬁlter to a stochastic graph G, then we can bound the error made when transferring a graph ﬁlter between G and Gby the sum of bounds (i) and (ii). Theorem 2 (Graph ﬁlter transferability on stochastic graphs). Let Y = Φ(X; h, W) be a graphon ﬁlter [cf. (18)] satisfying assumptions AS1–AS3. Let Gand G, n6= n, be two stochastic graphs with GSOs Sand Srespectively [cf. Deﬁnition 3], and let y= Φ(x; h, S) and y= Φ(x; h, S) be the graph ﬁlters instantiated from Φ(X; h, W) on these graphs. Let χ, χ, χ∈ (0, 0.3]. For any 0 < c ≤ 1 and n, n≥ 4/χsatisfying assumption AS4, with probability at least [1 − 2χ]× [1 − χ]× [1 − χ]it holds that + 2A(ac + 2) maxα(n, χ)n+ 8ackXk where B= maxmaxB, Band δ= Proof. Theorem 2 follows directly from Theorem 1 and the triangle inequality. Theorem 2 thus shows that graph ﬁlters are transferable between stochastic graphs in the same graphon family. Since these graphs have random edges, the theorem can be applied to graphs of same size. If nand nare different, the transferability bound in Theorem 2 is slightly loose because it is a simpliﬁcation of the sum of the graphon ﬁlter approximation bounds for Gand G. However, this simpliﬁcation is helpful as it reveals that the graph ﬁlter transferability error is dominated by the graph with the largest node and edge stochasticity-to-size ratio — typically the smallest graph. Transferability of graph ﬁlters is an important result because it means that we can design a ﬁlter for one graph and transfer it to another. This is possible even if the graphs have different sizes, considerably simplifying signal processing on large graphs. In the following, we will show that the beneﬁts of graph ﬁlter transferability extend to deep learning. This is because their transferability properties are not only inherited, but also augmented by GNNs. Remark 1 (Graph ﬁlter transferability on template and weighted graphs). The graph ﬁlter transferability result derived for stochastic graphs in Theorem 2 also holds for weighted and template graphs. Simply set β(n, χ) = 0 for both types of graphs and α(n, χ) = 1 for template graphs. While the transferability bounds obtained in this way are correct, tighter bounds can be achieved by combining the triangle inequality with Propositions 1 and 2 respectively, in the same way that Theorem 1 was used to show Theorem 2. This section introduces graphon neural networks (Section IV-A) and discusses its implications to GNN transferability (Section IV-B). A graphon neural network (WNN) is a deep convolutional architecture consisting of layers where each layer implements a convolutional ﬁlterbank followed by a pointwise nonlinearity [20]. Consider layer `, which maps the incoming Ffeatures from layer ` − 1 into Ffeatures. The ﬁrst step in this layer is to process the features X, 1 ≤ g ≤ F, through a convolutional ﬁlterbank to generate the Fintermediate linear features U, where 1 ≤ f ≤ F. Each intermediate feature Uis obtained by aggregating the outputs of Fﬁlters like the one in (17) with coefﬁcients h. Since there are Fsuch intermediate features, the ﬁlterbanks at each layer of the WNN contain a total of F× Fconvolutional ﬁlters. The next step is to process the intermediate features U with a pointwise nonlinearity, e.g., the ReLU. Denoting this nonlinearity σ, the fth feature of the `th layer is given by for 1 ≤ f ≤ F. Because the nonlinearity is pointwise, we have X(u) = σ(U(u)) for all u ∈ [0, 1]. If the WNN has L layers, (22)–(23) are repeated L times. The input features at the ﬁrst layer, X, are the input data Xfor 1 ≤ g ≤ F, and the WNN output is given by Y= Xfor 1 ≤ f ≤ F. Similarly to the graphon ﬁlter, a WNN with inputs X = {X}and outputs Y = {Y}can be represented more compactly as the map Y = Φ(X; H, W), where H is a tensor grouping the coefﬁcients hfor all features and all layers of the WNN, i.e., H = [h]for 1 ≤ ` ≤ L, 1 ≤ f ≤ F and 1 ≤ g ≤ F. Comparing this map with the GNN map in (7), we see that, except for the fact that their supports—a graphon and a graph respectively—are different, if the tensors H are equal these maps are the same. This allows interpreting WNNs as generative models for GNNs where the graph G is instantiated from W as in Deﬁnitions 1, 2 or 3, and the graph signal xis instantiated from X as in (11) or (12). The interpretation of WNNs as generative models for GNNs is important for two reasons. First, it allows designing one WNN and instantiating as many GNNs as desired from it. I.e, it allows designing neural networks in the limit of very large graphs and transferring them to ﬁnite graphs without changes to the architecture. Second, it motivates analyzing the ability to transfer GNNs across graphs of same or different size, since a sequence of graphs instantiated from a graphon following any of Deﬁnitions 1, 2 or 3 is proven to converge with high probability [8, Chapter 11]. To be able to compare WNNs with GNNs or GNNs supported on graphs of different sizes, we conclude by introducing the concept of WNNs induced by GNNs. The WNN induced by a GNN Φ(x; H, S) on the graph Gis given by where where the graphon W= Wis the graphon induced by G(10) and Xis the graphon signal induced by the graph signal x(13). Consider a WNN with L layers, F= 1 input feature, F= 1 output feature, and F= F features per layer for 1 ≤ ` ≤ L − 1. Each layer of this WNN is as described in (22) and (23) and, as such, there are Fgraphon ﬁlters (17) per layer (except for the ﬁrst and the last, where there are F ). Theorem 3 shows that this WNN can be approximated by a GNN on a stochastic graph under a Lipschitz continuity assumption on the nonlinearities σ. AS5. The activation functions are normalized Lipschitz, i.e., |σ(x) − σ(y)| ≤ |x − y|, and σ(0) = 0. Theorem 3 (WNN approximation on a stochastic graph). Let Y = Φ(X; H, W) be a WNN with L layers, F= F= 1 input and output features and F= F , 1 ≤ ` < L [cf. (22)– (23)]. Assume that this WNN satisﬁes assumptions AS1, AS3, and AS5, and that the convolutional ﬁlters that make up its layers all satisfy assumption AS2. Given a stochastic graph Gwith GSO S[cf. Deﬁnition 3], let y= Φ(x; H, S) be the GNN instantiated from Φ(X; H, W) on this graph. Let χ, χ, χ∈ (0, 0.3]. For any 0 < c ≤ 1 and n ≥ 4/χ satisfying assumption AS4, with probability at least [1−2χ]× [1 − χ] × [1 − χ] it holds that ×2(Aα(n, χ) + β(n, χ))nkXk(25) +Aα(n, χ)(ac + 2)n+ LF4ackXk where W= Wis the graphon induced by Gand Y= Φ(X; H, W) is the WNN induced by y= Φ(x; H, S) (24). Proof. Refer to Appendix C in the supplementary material. Assumption AS5 is satisﬁed by most common activation functions, such as the hyperbolic tangent, the sigmoid and the ReLU. Thus, we can typically upper bound the output difference at the end of each WNN layer, |σ(U+∆U)−σ(U)|, by a linear curve with unitary slope and zero intercept, i.e., |∆U|. This allows bounding the error incurred when approximating a WNN with a GNN by the transferability error of a cascade of L graphon ﬁlterbanks. Hence, Theorem 3 follows recursively from Theorem 1. Akin to the graphon ﬁlter approximation bound, the approximation bound in Theorem 3 has a term controlled by the transferability constant, a ﬁxed error term and a term corresponding to the non-transferable energy of the input signal. The ﬁxed error term, which stems from discretizing the input signal X on the stochastic graph G, is exactly the same as in Theorem 1. The other terms are scaled by LF, indicating that the deeper and the wider the WNN, the harder it may be for a GNN to approximate it. It is ready to show that transferability of WNNs between graphons and stochastic graphs implies transferability of GNN across stochastic graphs. Theorem 4 (WNN transferability on stochastic graphs). Let Y = Φ(X; H, W) be a WNN [cf. (7)] satisfying assumptions AS1, AS3, AS5, and such that the convolutional ﬁlters at all layers satisfy assumption AS2. Let Gand G, n6= n, be two stochastic graphs with GSOs Sand S respectively [cf. Deﬁnition 3], and let y= Φ(x; H, S) and y= Φ(x; H, S) be the GNNs instantiated from Φ(X; H, W) on these graphs. Let χ, χ, χ∈ (0, 0.3]. For any 0 < c ≤ 1 and n, n≥ 4/χsatisfying assumption AS4, with probability at least [1 − 2χ]× [1 − χ]× [1 − χ]it holds that + 2A(ac + 2) maxα(n, χ)n+ 8LFackXk where B= maxmaxB, Band δ= Proof. Theorem 4 follows directly from Theorem 3 and the triangle inequality. Theorem 4 thus proves that GNNs are transferable between graphs belonging to the same graphon family. Note that this is true not only for stochastic graphs, but also for weighted and template graphs as detailed in Remark 2. When n= n, the GNN transferability bound is approximately twice the WNN approximation bound in Theorem 3. When n6= n, this bound can be improved by explicitly writing the sum of (25) for Gand Ginstead of taking the maximum, but this simpliﬁed form is helpful because it shows that the transferability error is dominated by the inverse of the size of smallest graph. The ability to transfer GNNs across graphs has two important implications. The ﬁrst is that a GNN can be transferred from the graph on which it was trained to another graph with an error that is inversely proportional to the sizes of both graphs. Provided that the GNN hyperparameters are chosen judiciously, the transferability constant and the ﬁxed error term decrease as n, ngrow, and the transferability error is dominated by 8LFAckXk, the part of the bound due to the spectral components of X associated with eigenvalues |λ| < c (the non-transferable high frequency components). In applications where the same task has to be replicated on different graphs or on time-varying graphs (e.g., a ﬂock of drones, see [35]), this leads to reduced computational complexity because the GNN does not have to be retrained. Furthermore, this result implies that GNNs are scalable — they can be trained on smaller graphs than the graphs on which they are used for inference, and are robust to increases in the graph size. Remark 2. The GNN transferability result derived for stochastic graphs in Theorem 4 holds for weighted graphs by setting β(n, χ) = 0 for weighted and template graphs and α(n, χ) = 1 for the latter. However, the resulting bounds are slightly loose. For tighter bounds, combine the triangle inequality with Propositions 1 and 2 respectively to obtain the graph ﬁlter transferability bounds for template and stochastic graphs. Then, follow the same proof steps used to show Theorem 4 from Theorem 2 in Appendix C of the supplementary material. The graph ﬁlter transferability result derived for stochastic graphs in Theorem 2 also holds for weighted and template graphs. Simply set β(n, χ) = 0 for both types of graphs and α(n, χ) = 1 for template graphs. While the transferability bounds obtained in this way are correct, tighter bounds can be achieved by combining the triangle inequality with Propositions 1 and 2 respectively, in the same way that Theorem 1 was used to show Theorem 2. Non-transferable energy, ﬁlters with constant band, and asymptotics. In order to be transferable, ﬁlters and GNNs have to be able to “match” the eigenvalues of the source graph with those of the target graph so that the ampliﬁcations of the output signal’s spectral components match on both graphs. In our framework, this is possible because, as illustrated in Figure 2, as n → ∞ the eigenvalues of a graph Gsampled from a graphon W converge to the graphon eigenvalues [8, Chapter 11.6]. Hence, for large enough Gand G their eigenvalues are close. However, because the graphon eigenvalues accumulate near zero, for small eigenvalues (i.e., for λsuch that |j| → ∞), this matching becomes very hard. This is due to the fact that the distance between the eigenvalues λ(G) and λ(G) might be larger than the distance between consecutive eigenvalues in this range. To mitigate this problem, as shown in Figure 2 we restrict the variability of the ﬁlters to a< Abelow a certain threshold c [cf. Assumption AS4]. This ensures that the ampliﬁcations of the spectral components below c won’t be too different, and the fact that they cannot be discriminated less problematic. Still, because ais nonzero, the transference of these spectral components will incur in an error — the third term of the transferability bound in Theorems 2 and 4. We refer to the energy of this error as the non-transferable energy, since it stems from spectral components which cannot be perfectly transferred from graph to graph. Typically, due to abeing small the non-transferable energy does not contribute much to the transferability error and is obfuscated by the part of the bound that depends on the transferability constant. Nevertheless, this is largely dependent on the value of c. Reducing the value of c reduces the contribution of the non-transferable energy to the transferability bound, but decreasing c has the effect of increasing the transferability constant both through B, because a lower value of c results in a larger number of eigenvalues in [−1, −c] ∪ [c, 1]; and through δ, because as c approaches zero so does the margin between consecutive eigenvalues, which is the limit of δas n → ∞. The non-transferable energy is so called because, for ﬁxed c, it is a constant term in the transferability bound which does not decrease with the size of the graphs, no matter how large they are. To avoid non-transferable spectral components, the graph ﬁlter (or, respectively, the graph ﬁlters of the GNN) would need to have a constant frequency response for |λ| < c or, equivalently, a= 0. However, such ﬁlters are undesirable in practice because since they are not analytic they cannot be written in convolutional form (1). Nonetheless, in the limit it is possible to show that the difference between the outputs of Lipschitz continuous graph convolutions supported on G and Gconverging to the same graphon (and therefore of GNNs constructed with such convolutions) vanishes as n, n→ ∞. This convergence result, which can be seen as an asymptotic version of the transferability result, is proved in [19, Theorem 4]. Transferability-discriminability tradeoff and the effect of nonlinearities. In both the graph ﬁlter transferability theorem (Theorem 2) and its GNN counterpart (Theorem 4), the transferability constant (i.e., the ﬁrst term of the bound) depends on the parameters Band δof the graph convolutional ﬁlters, which in turn depend on the value of c. The parameter Bis the maximum c-band cardinality of a graphon [cf. Deﬁnition 4], which counts the number of graphon eigenvalues larger than c. The parameter δis the minimum c-eigenvalue margin between two graphons [cf. Deﬁnition 5], which measures the minimum distance between eigenvalues of these two graphons with consecutive indices where one is smaller and the other is larger than c. Since the eigenvalues of the limit graphon accumulate near zero, if c is large (i.e., close to one), Bis small — because there are less eigenvalues in the [−1, −c] ∪ [c, 1] interval — and δis large — because the further eigenvalues are from zero, the larger the distance between two consecutive ones. This leads to a smaller transferability constant, thus increasing transferability. On the other hand, larger values of c also reduce the model’s discriminative power (or discriminability) because they decrease the length of the interval [−1, −c] ∪ [c, 1] where the ﬁlter has full variability (i.e., A). Hence, there exists a tradeoff between transferability and discriminability in graph ﬁlters and GNNs. The value of the Lipschitz constant Aalso plays a role in this trade-off, as higher values of Alead to more discriminability but increase the transferability bound. In the case of GNNs, the transferability-discriminability tradeoff is slightly better than in graph ﬁlters because of the addition of nonlinearities. Nonlinearities act as rectiﬁers which scatter some spectral components associated with small eigenvalues to the middle range of the spectrum where they can then be discriminated by the ﬁlters of the subsequent layer. Interestingly, nonlinearities play a similar role in the stability of GNNs, in which case it is the scattering of the components associated with large eigenvalues that improves stability [6]. Graph as design parameter. While we focus on the limit object interpretation of WNNs to prove transferability, their interpretation as generative models is also valuable because it allows using the graph as a tunable parameter of the GNN. I.e., instead of considering the graph to be a ﬁxed hyperparameter, we could interpret it as learnable parameter such as the weights H. This is an interesting research direction, as it could be used, for instance, to build more general GNN architectures with larger degrees of freedom; to design adversarial graph perturbations for GNNs; and to draw deeper connections between transformer architectures and GNNs [36]. We illustrate the transferability properties of graph ﬁlters and GNNs in two applications: ﬂocking via decentralized robot control, and movie recommendation on a movie similarity network. All architectures are trained using ADAM with learning rate 5 × 10and forgetting factors 0.9 and 0.999. Decentralized control problems consist of a team of n agents which must accomplish a shared goal. Each agent has access to local states xand generates local control actions a. In order to learn which actions to perform, agents exchange information across pairwise communication links determined by their geographical proximity, which deﬁnes an agent proximity network G. Because communication incurs in delays, if agents i and j are k hops away from one another in G, at time t i only has access to the delayed state x(t−k) (and vice-versa). Therefore, we deﬁne the information history of agent i, which emphasizes that at time t agent i knows its current state x(t) and the states of its k-hop neighbors, denoted j ∈ N, at time t − k. The information history in (26) allows deﬁning a decentralized control scheme in which the actions a(t) can be calculated as functions of the history X(t). We can then use graph ﬁlters and GNNs to parametrize these functions by incorporating the delayed information structure into (1). Explicitly, to account for communication delays we rewrite theQ terms Sx in (1) asS(t−κ)x(t−k). The convolutional ﬁlters in the GNN (4)–(5) are modiﬁed in the same way. In the ﬂocking problem [35], the shared goal is for all the agents to move with the same velocity while avoiding collisions. The states x(t) ∈ Rare given by where r(t) are the positions and v(t) the velocities of agent j measured relative to the positions and velocities of agent i respectively. The neighborhood Nconsists of nodes j such that krk ≤ R, i.e., which are within a communication and sensing radius of length R from i. We consider R = 2. At t = 0, the agents’ positions and velocities are initialized at random. The actions a(t) to predict are the agents’ accelerations. While a centralized solution to the ﬂocking problem is straightforward—it sufﬁces to order all the agents to move in the same direction—the optimal decentralized controller is unknown. Hence, we train decentralized graph ﬁlters and GNNs following the information structure in (26) to “imitate” the centralized controller. We consider 2-layer graph ﬁlters and a GNNs with F= 6 input features, F= 64 features in the ﬁrst layer, and F= 32 features in the second layer. At all layers, the convolutional ﬁlters have K = 3 ﬁlter taps. The readout layer outputs 2 features per node corresponding to the agents’ accelerations in the x and y directions. The nonlinearity used in the GNN is the ReLU. Both architectures are trained by minimizing the mean squared error (MSE) over 400 training trajectories of duration equal to 100 steps. We train for 30 epochs using minibatches of size 20. Performance is measured by recording the cost of the decentralized controller, given by the sum of the deviations of each agent’s velocity from the mean, relative to the cost of the centralized controller. The test set consists of 20 trajectories with the same duration. In decentralized control problems, transferability is important because, since we are training to imitate a centralized controller, the learning architecture has to be trained ofﬂine. Hence, the networks observed during training are different than those observed during execution, and typically smaller, because the cost of training graph ﬁlters and GNNs can be prohibitive for large graphs. To assess whether the policies learned with graph ﬁlters and GNNs are transferable in the context of ﬂocking, we perform the following experiment. We train the models on networks of size n = 25, 37, 50, 75, 87. Then, we test them on both the original network and on a network with n = 100 agents, and record the difference between the cost achieved on the original network and on the 100 agent network relative to the cost on the original network. The results from this experiment for 10 random realizations of the dataset are shown in Figure 3a. In Figure 3a, we observe that the difference between the outputs of the graph ﬁlter and the GNN on these networks both decrease as the number of nodes of the graph on which they are trained increases. This is consistent with the asymptotic behavior of the transferability bounds in Theorems 2 and 4, which decrease with n. We also observe that, for ﬁxed n, the relative cost difference of the GNN is smaller than the relative Figure 3. (a) Difference between the cost achieved by the decentralized ﬂocking controller on the original network with 30, 40, 50, 60, 70, 90 agents and on the full 100 agent network relative to the cost on the original network. The difference between the outputs of the graph ﬁlter and the GNN on these networks both decrease as the number of nodes of the graph on which they are trained increases. The relative cost difference of the GNN is smaller than the relative cost difference of the graph ﬁlter, which evidences that GNNs are more transferable than graph ﬁlters as discussed in Section IV. For reference, the costs achieved by training on the 100 agent network are 19.31 ± 20.66 for the graph ﬁlter and 1.46±0.01 for the GNN. (b) Difference between the RMSEs achieved on the subnetwork of size n = 500, 750, 1000, 1250, 1500, 1750, 2000 and on the full 3416-movie network for the graph ﬁlter, the GNN, and the GNN trained by penalizing convolutions with high Lipschitz constant. The RMSE difference decreases with the number of nodes of the movie subnetwork for both the graph ﬁlter and the GNNs as predicted by Theorems 2 and 4. Moreover, the GNN with ﬁlters with lower variability is signiﬁcantly more transferable than the graph ﬁlter and the GNN trained without regularization, illustrating the transferability-discriminability tradeoff. For reference, the errors achieved by training on the full movie network are 0.81 ± 0.04 for the graph ﬁlter, 0.81 ± 0.04 for the GNN, and 0.82 ± 0.05 for the penalized GNN. The error bars in (a) and (b) are scaled by 0.5. cost difference of the graph ﬁlter, which evidences that GNNs are more transferable than graph ﬁlters as discussed in Section IV. We consider the MovieLens-1M dataset [37], which consists of one million ratings given by 6000 users to 4000 movies. Each rating is a score between 1 and 5, with higher scores indicating higher preference for a movie. Speciﬁcally, we aim to predict the ratings given by different users to the movie “Star Wars: Episode IV - A New Hope”, which has a total of 2991 ratings. To do so, we deﬁne a movie similarity network, where each node is a movie and each edge is the similarity between two movies. We restrict attention to movies with at least 5 ratings, which brings the number of nodes of the full movie similarity network down to 3416. We compute this network by deﬁning a training set with 90% of the users, and by deﬁning the similarity between two movies as the correlation between the ratings given by users in the training set to these movies. Movies are then connected with their 40 neares neighbors, i.e., with the 40 movies with which they have the highest correlation. Each user corresponds to a graph signal. At each node, the value of a signal corresponds to the rating given by the user to the corresponding movie, or zero if a rating is not available. To train graph ﬁlters and GNNs in a supervised manner, we constructed input-output pairs where, for each user, the output is the the rating to ‘Star Wars” and the input is the user’s graph signal with the rating to this movie zeroed out. The architectures we consider are a 1-layer graph ﬁlter and a 1-layer GNN. Both have F= 1 input features, F= 64 features in the ﬁrst layer, and K = 5 ﬁlter taps. The nonlinearity of the GNN is the ReLU and both the graph ﬁlter and the GNN are followed by a readout layer which outputs 1 feature, the rating. These architectures are trained by minimizing the MSE at the node corresponding to the movie “Star Wars” over 30 epochs with batch size 16. Performance is measured by recording the root mean squared error (RMSE) achieved by each architecture on the test set. To analyze transferability, we train the graph ﬁlter and the GNN on subnetworks with n = 500, 750, 1000, 1250, 1500, 1750, 2000 movies, and plot the difference between the RMSE they achieve on the subnetwork of size n and on the full 3416movie network, relative to the former, in Figure 3b. In Figure 3b, note that the RMSE difference decreases with the number of nodes of the movie subnetwork for both the graph ﬁlter and the GNN as predicted by Theorems 2 and 4. Although the GNN (orange) achieves a lower transferability error than the graph ﬁlter (blue) for most values of n, the difference is very small. We hypothesize that this is because the graph convolutions being learned in the GNN have high variability. To test this hypothesis, we retrain the GNN by minimizing the regularized MSE loss with regularization factor given by the maximum Lispchitz constant of the GNN’s convolutions scaled by a penalty multiplier. The transferability error of this GNN is shown in green. We observe that the GNN with ﬁlters with lower variability is signiﬁcantly more transferable than the graph ﬁlter and the GNN trained without regularization. Besides corroborating that GNNs are more transferable than graph ﬁlters, this result provides a clear illustration of the transferability-discriminability tradeoff implied by Theorem 4. In this paper, we deﬁned graphon convolutions and graphon neural networks (WNNs), which are the limit objects of graph convolutions and GNNs respectively, and showed that graph ﬁlters and GNNs sampled from them on stochastic and weighted graphs can be used to approximate graphon convolutions and WNNs. Building upon this result, we then showed that graph ﬁlters and GNNs are transferable across weighted and stochastic graphs. The transferability error decreases with the size (i.e., the number of nodes) of the graphs, however, some spectral components are not transferable across ﬁnite graphs even when they very large. This is a consequence of the fact that the graphon eigenvalues accumulate near zero. Our transferability results also reveal a tradeoff between the transferability and spectrcal discriminability of graph convolutions, which is inherited by GNNs. In practice, however, in GNNs this tradeoff is alleviated by the pointwise nonlinearities. These ﬁndings were corroborated empirically in the problems of movie recommendation and decentralized robot control.