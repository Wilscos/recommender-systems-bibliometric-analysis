ing system with a global market share of 75.39% [1]. The open nature of Android has led to a nearly exponential growth in mobile apps in recent years. The number of available apps in the Google Play Store reached 2.9 millions in December 2019, after surpassing 1 million apps in 2013 [2]. On the one hand, the dazzling number of apps provide convenience and enrich users’ life; on the other hand, the great quantity of apps under each category makes it difﬁcult for users to make choices. It is simple for users to download apps by several clicks; however, it is not so simple to ﬁnd high-quality apps. The remarkable growth of Android app markets increasingly requires recommendation methods that can rank and recommend apps automatically and accurately. To make a quick and effective decision on selecting apps for download, users commonly pay attention to the ratings and number of downloads, which are shown on the introduction page of an app. Few would further refer to reviews written by previous users. If the reviews are positive, new users would have a higher conﬁdence to download the app. Although it is not completely known how apps are ranked, there are strong evidences that the number of downloads, ratings and reviews play the major roles. While such ranking strategies work to some extent, they bear an inherent drawback. That is, newly published apps suffer from the cold-start problem because of the lack of user generated data on them. Although most app markets run a vetting process, currently the process mainly concerns with security violation, rather than the quality of an app. App markets have little to leverage when ranking or recommending new apps. Given the giant size of app markets, some high-quality apps may not get a good opportunity to be recognized, which can easily discourage the developers from making more excellent apps. Therefore, purely based on user generated data to grade apps is one-sided. Previous work on grading and recommendation systems mainly focused on user generated information. For instance, DroidVisor [3] clustered apps based on ratings, downloads and reviews. They are more concerned about user’ comments that may be misleading and indiscriminate rather than the real quality of apps. A few more app ranking related systems, such as RankMyApp [4] and Appﬁgures [5] offer app store ranking rather than optimizing apps’ quality. They mine keywords and popular topics and conduct sentiment analysis on app reviews, and based on the mining result, they give suggestions on app name, app description, app icon to improve an app’s visibility in a certain category. However, they merely help modify the appearance of an app. Therefore, they cannot help users ﬁnd high-quality apps. To address the cold-start problem and recommend high quality apps to users, a fundamental requirement is the capability to accurately measure apps’ quality based on the inborn features instead of the user-generated information, as a newly published app has very few or even no user’s feedback. We face great challenges: (1) what inborn features of an app best reﬂect its quality are unclear. Thus we performed term frequency counting on 18 million app reviews covering all categories to understand the criteria based on which users rate the apps. The results show that users deeply care about the design of user interface (UI), the built-in advertisements and requested permissions when commenting the quality of an app; (2) visual design plays an important role in providing pleasant user experience and view switching supports the implementations of different modules’ functions in an app, however, it is difﬁcult to efﬁciently extract large amounts of views. To address the problem, we develop several modules that can automatically record the views, then click a button to switch to the next view and ﬁnally traverse all views of an app. We have achieved the goal of analyzing a large number of apps automatically; (3) how to precisely measure the importance of the primary and secondary views is a challenge. Thus we employ a view graph to describe the quality of views and switching among views, and then propose a graphto-vector method to encode different sizes of view graphs into a sequence of key vertices and their neighbors. Finally, the apps are classiﬁed into different quality categories. We aim at enabling the app recommendation system to have a warm-start for those new published apps and those with little user interactions. To highlight, our contributions are: tures extracted from their source code to grade apps, rather than using extrinsic features like usergenerated information, in the aim to warm-starting app recommendation. Static and dynamic analyses are performed in parallel to extract app-level features, which proﬁle apps’ behaviors and view-level UI hierarchy as well as the switching among views. To the best of our knowledge, this is the ﬁrst work on evaluating apps’ quality with their inborn features. based on view switching. We then encode the graph by a sequence of key vertices and their neighbors to highlight the importance of main views within an app. The graph that represents an app is further converted into a feature vector. tors to classify apps into three quality categories, for ranking out the top-quality ones. With a dataset from Google Play, AppQ achieves the best performance with the accuracy of 85.0%. The results demonstrate the great promise of warm-starting app recommendation with AppQ. The rest of the article is organized as follows. In Section 2, we will introduce some preliminary concepts of android views and review related work. In Section 3, we ﬁrst introduce the overall framework of the proposed method. Then, we discuss the ﬁve procedures in detail, including features identiﬁcation, feature extraction, view graph construction, graph-to-vector and classiﬁcation. The evaluation metrics and experimental analysis are demonstrated in Section 4. We will conclude our work in Section 5 with prospective work. A layout deﬁnes the structure for a user interface in the app. All elements in the layout are built using a hierarchy of View and ViewGroup objects. The basic building block for user interface is View which occupies a rectangular area on the screen and is responsible for drawing and event handling. A View provides a screen which users can actually see and interact with [6]. View is the base class for widgets which are used to create interactive UI components like buttons, checkbox, text ﬁelds, etc. Whereas a ViewGroup is an invisible container that UI components can be placed in, as shown in Fig. 1. Each subclass of the ViewGroup provides a unique way to display the views. Common layouts include linear layout, relative layout and web view. When users press the buttons on the screen, they are actually interacting with widgets of the view. Uiautomatorviewer is a tool in Android SDK. It provides the information of UI components currently displayed on an Android device. We can inspect the layout hierarchy and view the properties of UI components that are visible on the foreground of the device. For example, in Fig. 2, on the left is a shopping app’s view and its UI components are shown on the right. Images and texts are placed in different layouts that are stacked to form the current view. We can see the properties of the red box area from the lower right table, indicating it is focusable and clickable. When users click the red box area, the current view will switch to the shopping cart view. In this paper, we mainly focus on the layout hierarchy of each view and the switching among views. Several approaches on recommending and ranking apps from different aspects have been presented in the literature. We conduct a detailed survey of existing work. Based on user-generated statistics, Pai et al. [7] implemented recommendation by calculating the positive or negative score of semantic orientation in reviews. They also considered statistical information, e.g., star ratings and downloads. Rustgi et al. [3] took four metrics into consideration: downloads, ratings, descriptions and requested permissions. Users are allowed to select the weight of each metric. Then a normalized score is computed for each app. The app with highest scores will be recommended. Jisha et al. [8] evaluated and clustered apps based on permissions and user ratings. They calculated a risk score and then applied k-means to categorize apps into high or low security applications. In terms of security, Peng et al. [9] proposed a matrix factorization algorithm based on permissions and functionalities. They exploited the relationship between the permissions and functionalities to achieve personalized recommendation. Similarly, Zhu et al. [10] recommended apps by striking a balance between the apps’ popularity and the users’ security concerns, and built an app hash tree to efﬁciently recommend apps. Liu et at. [11] incorporated interest-functionality interactions and users’ privacy preferences to perform personalized app recommendations. They constructed a model to capture the trade-off between functionality and user privacy preference. Yin et al. [12] considered user interests and category-aware user privacy preferences. They exploited textual and visual content associated with apps to learn multi-view topics for user interest modeling. Based on users’ browsing behaviors and preferences, Shu et al. [13] observed that users ﬁrst view the description of the app and then decide if they want to download it or not. Thus they proposed ActionRank which integrated various signals from user actions into a coherent model for personalized app recommendations. Similarly, He et al. [14] introduced a model that appropriately combined download and browsing behaviors to learn users’ overall interests of apps. They combined users’ overall interests and their current interests to recommend apps. Xu et al. [15] observed that a user has a pattern of app usage contexts. What’s more, the similarity in two users’ preferences is correlated with the similarity in their app usage context patterns. They proposed a neural approach which learns the embeddings of both users and apps and then predicted a user’s preference for a given app. To make use of the textual information, Pan et al. [16] proposed a combineLDA method to mine topics of apps based on their descriptions and users’ reviews. They calculated the similarity between apps and recommended users with highly similar apps. To increase the usability of emergency apps, Ahmadi et al. [17] proposed REMAC that combined different machine learning techniques to analyze the context characteristics of different organizations and suggested unique features that can be included in the emergency apps. Lin et al. [18] noticed that new versions of apps may attract users’ interests, even the apps used to be unappealing. To allow previously disfavored apps to be recommended, they constructed a representation of an app’s version as a set of latent topics from version metadata and textual descriptions. Then they discriminated the topics based on genre information and weighted them on a peruser basis to generate a version-sensitive ranked list of apps for a target user. Liang et al. [19] considered the interactions among the context information of apps. They utilized a tensor-based framework to integrate app category information and multi-view features on users and apps. Cao et al. [20] integrated both numerical ratings and textual content from multiple platforms. They represented an app as an aggregation of common features across platforms (apps’ functionalities) and speciﬁc features that are dependent on the resided platform. Some work focused on other espects, e.g., Su et al. [21] measured network trafﬁc cost of each app based on random walk. Then they ranked the apps by app rating and trafﬁc cost. Lin et al. [22] estimated the probability of the user liking the app by extracting information from an app’s Twitter followers. However, most previous related work relies on usergenerated statistics, users’ browsing behaviors or apps’ description contexts, which are scarce for new published apps. Moreover, [23] has mentioned that users’ reviews to apps are sometimes not reliable. Thus it is necessary to explore the inborn features of apps’ view, which are most relevant to user experience, and can be an important indicator of the quality of apps. There are also works on detecting repackaged apps based on UI similarity [24]–[26], but similar techniques have not been applied in evaluating and ranking apps. In this work, we extract ﬁne-grained features which combined app-level and view-level features to thoroughly proﬁle apps’ quality and grade apps. Most current recommendation systems face the cold-start problem because of the applied ranking strategies. To address this problem, we are motivated to recommend apps based on inborn features which can represent apps’ real quality, instead of statistical information generated by users. The ﬁrst question arises: “what are the inborn features of an app that can reﬂect its quality?” To answer this question, we need to understand the criteria on which users based when they rate. We perform term frequency counting on 18 million reviews on Google Play. The results (in Section 3.1) show that UI has an important effect on user experience. Accordingly, AppQ will focus on modeling app UI features for app quality estimation. More speciﬁcally, app-level features and view-level features are considered. App-level features model the higher level features, such as permissions, ads and whether the app crushes during running; view-level features are those related to the design of individual views in an app, such as the widgets enclosed in a view and the layout of a view. With the features, we will construct an attributed view graph and further converts the graph to form the feature vector of the app. Finally, apps will be classiﬁed by machine learning algorithms with feature vectors as inputs. The system overview is shown in Fig. 3 3.1 Inborn features identiﬁcation When rating an app, an individual user may only care certain aspects of an app, so (s)he may only mention a few speciﬁc issues in the review. Nevertheless, combining all reviews from all users would give us a complete picture of what truly matters. Hence, we start with analyzing 18 million app reviews covering all app categories from Google Play. Speciﬁcally, we preprocess the raw data by breaking texts into tokens based on the scikit-learn library [27]), removing the stop words (based on terrier stop words library [28]), removing noise (ill-spelling words) and featureirrelevant words (e.g., “app”, “game”,). After that, we obtain the top-500 high-frequency words related to app features. Table 1 shows that when users rate apps, they care about the performance of apps at runtime(e.g., crash), the design of user interface (picture and color), the widgets used in an interface (e.g., button), permissions an app may request (e.g., wiﬁ and camera) and advertisements contained in an app (e.g., ads). We divide the inborn features into two groups, app-level features and view-level features. 3.2 Feature extraction Representative and comprehensive features are in great demand to proﬁle apps’ behaviors. We extract app-level features and view-level features by both static analysis and dynamic analysis. The features are summarized in Table 2. (1) App-level features AppQ decompiles the APK to access the inner ﬁles. Android controls access to system resources with permissions. Speciﬁc permissions are required when an app interacts with system APIs or databases. To characterize an app’s intention during resource accessing, we extract the requested permissions from its manifest ﬁle. Only system-deﬁned permissions are considered. Besides, all displayed views are also registered in the manifest ﬁle, which we can make use of. The number of declared views reﬂects the opportunity of interactions provided by apps. Usually the more views there are, the more functions the app has. In addition, the number of multimedia ﬁles in the payload mirrors the aesthetics of apps. We count the number of video(ﬁlename extension as “.mp4”,“.avi”,“.wmv” and “.ﬂv”), audio(ﬁlename extension as “.mp3”) and images(ﬁlename extension as “.jpg”,“.jpeg”,“.png”,“.bmp” and “.gif”) in the APK as features. Advertisement has a negative impact on user experience. We collect 67 popular advertisement libraries, e.g., Google Ads and Amazon Ads, and take the number of libraries in the APK as a feature. Whether the app crashes at runtime shows how the app performs in delivering what it has promised, which is one of users’ most concerned issues. Most Android-based mobile systems (e.g., MIUI) can pop up an alarm window to remind users the app has stopped running when a crash occurs. AppQ records the alarm as a feature. Finally, a 134-dimensional app-level feature vector is formed. (2) View-level Features Since static code analysis faces the challenges raised by code obfuscation and dynamic code loading, we dynamically run each app to extract the hierarchy of each view and the switching among views. The number of layouts or widgets reﬂects the layering of the view. The number of different layouts or widgets represents the diversity of widgets of in the view. The widgets can be set as “clickable” thus users can click them to trigger some events, e.g., login or jump to the next view. We take the number of clickable widgets as a feature to describe the opportunities the app provides to interact with users. These parameters of a complex interface with a layered view are larger than those of a simple view. The features also reﬂect the developers’ proﬁciency in UI development. Finally, we transform each view’s hierarchy into a 5-dimensional feature vector. AppQ can automatically install, run and uninstall apps with the help of AndroidViewClient [29] and adb commands [30]. AppQ is technically novel in extracting apps’ views. When running the app, AppQ takes depth-ﬁrst search to traverse clickable widgets. There are three alternately working modules: state-judger, widget-picker and click-executor. State-judger: State-judger is to judge whether the view has changed after each click and then performs corresponding operations according to the judgment. It makes use of AndroidViewClient to dump the hierarchy of the current view, including the information of layouts and widgets, e.g., package name, class name. A problem arises that AndroidActivityClient does not provide information about the activity id. Each activity needs a unique id so that AppQ can recognize it when jump to the same activity. To deal with the problem, AppQ crawls the hierarchy of the activity and collects class names of each layout or widget which are further concatenated into a string. The hash value of the string is regarded as the activity’s id which is the ﬂag of judging whether the activity has changed. Since activities can start other activities which live in a separate app, AppQ also needs to determine whether current activity belongs to the test app. If not, it triggers back button to return to previous activity. Consequently, AppQ is able to take the corresponding operations according to the state of a view: (a) If a new view is on display, AppQ records the switch and dumps its hierarchy; (b) If the view has not changed and there exist clickable widgets in it remaining unclicked, AppQ chooses the one with the highest priority from the waiting list. The priority is set by the widget-picker module; (c) If the view has not changed and all clickable widgets in it have been clicked, AppQ triggers the back button to return to the previous view. Widget-picker: Widget-picker is to determine the clicking sequence of widgets. Different types of widgets have different functions, e.g., EditText is for entering and modifying text while ImageButton displays a button with an image (instead of text) that can be clicked. To improve the efﬁciency of triggering the switching among views, we set the priority of widgets based on their locations and types. Firstly, AppQ extracts clickable widgets in the current view. Though some widgets are clickable, they are unable to trigger view switches, e.g., ToggleButton, CheckBox, RatingBar, etc. Thus they are ﬁltered out. Secondly, the priorities are determined by widgets’ positions. Upper left corner is normally the return button or the navigation drawer containing app setting which we are less interested in. Thirdly, for widgets in the same area, their priorities follow ImageButton>Button>ImageView>TextView>other widgets. Our testing approach is more efﬁcient than randomly clicking tools such as Monkey [31]. Click-executor: Click-executor is to execute click commands. State-judger sends click command to click-executor to click a widget or return to the previous view. 3.3 View graph construction A well-deﬁned graph reveals relationship between an app and its views, not only by features, but also by view neighbor relationship. An app implements different functions through view switching. After feature extraction, we have obtained feature vectors that proﬁle apps’ behaviors and the hierarchy of views. We build an undirected attributed view graph for each app, which is deﬁned as G(V, E, F ). V is the vertex set and each v ∈ V represents a view. E is a set of edges. (v, v) means vcan switch to vby clicking some widgets. Technically each view is able to return to previous view by clicking return button. F is the feature set that contains only view-level features extracted from dynamic analysis. App-level features will be merged during Graphto-vector step introduced below. 3.4 Graph-to-vector After feature extraction and graph construction, we have obtained app-level features and attributed view graphs. To classify apps into different quality categories, the following problems need to be addressed: a. View graphs differ in size for different apps, how to transform them into a standardized expression? b. How to merge app-level features and the view graph to proﬁle one app? Inspired by the recent explosion of graph neural network (GNN) models, such as GCN [32], GraphSAGE [33], GAT [34] and GIN [35] that can be used to learn a representation vector for every graph, the most feasible solution for us is to map each app as a vector by integrating the app-level features and the view graph. GNN models are usually deﬁned as multi-layer neural networks with layerwise propagation operators on graph datasets. The main intuition is to fully utilize both the features of the nodes and the structural information of the graph in node/graph classiﬁcation or label prediction problems. GCN [32] is derived from a ﬁrst-order approximation of graph Laplacian spectrum. The normalized adjacency matrix serves as a new convolution kernel of traditional CNN which can incorporate additional information from correlative node items. In this way, one graph convolutional layer can be viewed as gathering features from 1-hop neighbours of the central node. By stacking multiple convolutional layers, we can capture further information through the graph structure. Instead of directly using graph Laplacian operator to aggregate information from nodes’ neighbours, GAT [34] tries to differentiate the contributions of various features of neighbouring nodes to the central node. They ﬁrst apply a shared self-attention mechanism on the input features to get the attention coefﬁcients, then the coefﬁcients are passed to a softmax function to get the normalized attention weights. Then the normalized attention weights are used to aggregate transformed features from neighbouring nodes. To better learn the hidden features in different sub-spaces, usually the multi-head attention mechanism is employed on the same inputs. Although usable, the recent GNN-based models cannot appropriately handle the special graphs in our case. The main reason is that our vertices (views) have special dependencies and signiﬁcances in the neighborhood of different vertices when evaluating the quality of an app. Considering those insigniﬁcant vertices (views) will introduce too much noise to the model. But GNN models apply the same aggregating operators to all the neighborhood for each vertex. Thus, we focus on the key vertices and their neighbors when representing an app as a vector, rather than applying the learned aggregator iteratively on each vertex’s neighborhood. Experimental comparison also shows that our next presented graph-to-vector conversion approach performs better than the recent GNN models. Speciﬁclly, we apply the following steps to each graph: (1) select a ﬁxed number of key vertices from the graph; (2) assemble a ﬁxedsize neighborhood for each key vertex; (3) construct feature vectors. The process is shown in Fig. 4. Since the main views of an app are the most important ones regarding the quality of the app, we measure the importance of vertices within a graph by their centrality: where distance(v, v) is the shortest distance between vertex vand v. The smaller dis, the higher centrality of vwill be, and vlocates at a more central position in the graph. In the view graph, some views can switch to many other views, whereas some have no branches at all. The ones with more switches are more centralized, indicating that they are more important in the graph structure. We calculate the centrality of each vertex and select the top k vertices as key vertices. The steps are described in Algorithm 1. For each key vertex v from the previous step, m neighbors are supposed to be extracted. We sort vs 1-hopneighbours, 2-hop-neighbours and 3-hop-neighbours by centrality and regarded them as candidate vertices. Since different v may have a different number of candidate vertices, we only pick top m of them. If the number of candidate vertices is smaller than m, the algorithm creates dummy vertices with all-zero features for padding purposes. Now the arbitrary graph can be represented by k∗(m+1) vertices: [v, n, . . . , n, n, . . . , n, n, . . . , n] . . . [v, n, . . . , n, n, . . . , n, n, . . . , n] where vis a key vertex, nmeans the c-th 3-hopneighbor vertex of v. For vertices with higher centrality, they may appear as neighbors of several vertices, indicating they have higher impact to the graph. The priority can be reﬂected in the feature vector. The steps are described in Algorithm 2. Note that each of the k ∗ (m + 1) representative vertices has its view-level features. To form a feature vector for an app, we arrange these vertices’ view-level features in order and add app-level features at the end. The app-level features are denoted as and each view’s view-level features are where n and nare the numbers of features. Views in one app may have different view-level features, but they have the same app-level features. Based on (2), F (G) encodes both app-level features and view-level features, which can be expressed as Then a unique mapping from a graph representation into a vector space representation is formed. This feature construction is analogous to the concatenation aggregator used in GNN-based models [33] and [36]. In the next experimental evaluation section, we will demonstrate that our selected representative vertices by node centrality well serves our study purpose on characterizing apps’ quality for user experience. The feature vector is further fed into the classiﬁcation algorithms for quality evaluation. The steps are described in Algorithm 3. 3.5 Classiﬁcation We employ three machine learning algorithms as classiﬁers, namely, Linear Support Vector Machine (Linear-SVM), Rbfkernel Support Vector Machine (Rbf-SVM), and Random Forest (RForest) [27]. To take advantage of the strengths of each classiﬁer and compensate for the disadvantages, we apply ensemble learning to obtain the ﬁnal detection. These classiﬁers ﬁrst work in parallel to predict the label of an app. Then the ﬁnal label is determined by their majority voting, e.g., “high quality” or “low quality”. The ensemble decision is more stable than single machine learning algorithm. In this section, we conduct a series of experiments to evaluate AppQ’s accuracy and efﬁciency. We collected 3050 apps from 16 categories in Google Play as our experiment dataset. We also crawled the statistical information, including download numbers, rating scores, number of ratings and number of reviews for assigning apps’ labels. The apps have very different statistical information, raging from 0 to 1 billion. Some apps are popular while some have little users’ feedback. The description of apps is shown in Table 3. In each category, the apps are divided into three quality groups. For each group, 75% apps are used for training and 25% for testing. We use n-fold cross-validation in our experiments to eliminate contingency. The experiments are constructed on a computer with two quad-core 2.6 GHz i5 processors and 8G memory. During dynamic analysis, apps are run for 3 minutes on Xiaomi Mi 8 with MIUI based on Android 8.1. AppQ is developed in Python 2.7. 4.1 Assigning labels Classifying apps into different quality categories needs ground truth labels for training and evaluation. While it may sound intuitive to consider rating as the ground truth for app quality, rating alone is not reliable as it can be easily manipulated [37]. Moreover, other factors such as number of downloads, number of ratings, and number of reviews are also used by app stores for app ranking. To reduce the risk of ratings being tampered with, we assign labels based on the following two steps: (1) We cluster the apps into three classes by Agglomerative Clustering [27] based on four factors: downloads, ratings, the number of ratings and the number of reviews. We apply three groups of labels in this evaluation: 1 represents “low quality”, 2 represents “average quality” and 3 represents “high quality”. (2) We manually check the labels. We observe that some apps in the same category come from the same developer and have similar functions, UIs and package names. However, the number of downloads and reviews differ greatly. For instance, in category Weather, “Live Weather & Local Weather” has ﬁve million downloads while “Live Weather & Widget for Android” has only ten thousand. The developer released them at different times, resulting in more cumulative downloads for the ﬁrst released apps. Based on step (1), these apps would be clustered into different classes since their four factors could vary widely, hence introducing noise into the training dataset due to the similar UIs. To handle this problem, we collect these similar apps based on their package names and sizes, and unify their labels into the highest label among them. For other “low quality apps” from step (1), we manually check them and only keep the ones with badly-designed interfaces. Examples are shown in Fig. 5 and Table. 4. 4.2 Metrics Two metrics are applied to evaluate the classiﬁcation result of AppQ, Micro-F1 and Dissimilarity (Diss). The F1-score is a measure of a test’s accuracy and is used to evaluate the performance of unbalanced classiﬁcation problem, as we have more label-3 apps in some categories of our dataset. However, F1-score is for binary classiﬁcation problems. We apply Micro-F1 for multilabel classiﬁcation. It calculates metrics globally by counting the overall true positives, false negatives and false positives. In the following, we regard Micro-F1 as the accuracy measurement: Micro − F 1 = 2recall× pr ecisionrecall+ pr ecision(6) where k is the number of label categories and TP, FP, TN, FN denote true positive, false positive, true negative, false negative, respectively. We measure the dissimilarity between predicted labels and the ground truth labels by Dissimilarity =|predictedlabel(i) − groundtruthlabel(i)|the number of false predictions where n is the number of apps in each category. 4.3 Classiﬁcation results In this subsection, we evaluate AppQ’s accuracy and efﬁciency. As shown in Fig. 6, by employing majority voting mechanism in classiﬁcation, we achieve the highest accuracy of 85.0% and dissimilarity of 1.3 on Weather category. It has satisfying performance on the remaining categories with accuracy around 60%-80%, except for Books and Education. AppQ reaches the lowest accuracy of 56.9% on Books, which can be attributed to the noisy dataset. Many apps with different functions are placed in the same category in Google Play. For instance, Books category contains online eBooks, ﬁle readers and dictionaries, whose view qualities differ greatly. Online eBooks tend to have fancy and attractive views with lots of pictures. In contrast, ﬁle readers’ designs are normally unobtrusive and simple. Their users only need to select the format of ﬁles, e.g., pdf or text, and then the app scans the smartphone and shows the related documents. Differently, the apps in the weather category are more similar because their functions are more concentrated on providing weather forecast. AppQ achieves much better results on categories which contain apps with similar purposes. The parameter selection is described as follows: First, we use a single classiﬁer on each category with different ground truth labels generated by different weights of the four factors, as described in section 4.1. We count the number of times of best average accuracy based on three weight groups on 16 categories. The weight group (0%, 50%, 30%, 20%) achieves 11 best average accuracy results. According to our observation, comparatively, some apps have more ratings and reviews even with fewer downloads, as an example shown in 5. Therefore, it is reasonable to assign low weights to downloads. In the following evaluations, we apply the labels generated by weight group (0%, 50%, 30%, 20%) of the four factors as the ground truth labels. For single classiﬁers, the average accuracy of Rbf-SVM is the highest, which is 84.2%. The accuracy of ensemble method is not always the best, but it can balance the results of each classiﬁer so that the overall accuracy is more stable than a single classiﬁer. The number of vertex we extracted in each app is in range [6, 10]. For the selection of k and m in the graph-tovector phase, we set k in range [4, 6] and m in range [0, 3]. We conduct several experiments with different combinations of k and m. When k = 4 and m = 2, it achieves the best performance. 4.4 Feature comparison Behaviors of apps are reﬂected by features. To ﬁgure out the differences between high quality and low quality apps, we take further analysis of their features. Take the Weather category as an example. As shown in Fig. 7, the average numbers of view-level features of high quality apps are higher than those of low quality apps, especially on the number of widgets. It indicates that the layout of high quality apps are more carefully designed, and they are more complicated and layered. For app-level features, high quality apps tend to have more views registered in their manifest ﬁles. The more views there are, the more functionalities an app has. Advertisement is another factor users care about. From the ﬁgure we can see there are less advertisement libraries in high quality apps. It conﬁrms that advertisement does have a negative impact on user experience. Due to privacy concerns, users give lower ratings to those with permissions to change the state of smartphones, e.g., vibrate or force back, as shown in Fig. 8. Some features are related to apps’ functions, so there are no common patterns among different categories of apps. For instance, all apps in Weather have few video or audio ﬁles despite their quality. It is because most app-level features we extracted are from static analysis, but some videos and audios are dynamically downloaded due to the realtime nature of apps in Weather category. But for apps in categories that do not always require real-time services, e.g., Education, the number of videos in high quality and low quality apps differ greatly, which is 29.4 and 2.1, respectively. 4.5 Time consumption To evaluate the time consumption of AppQ, we analyze the execution time of each phase, which is shown in Table 6. In the app-level feature extraction phase, decompiling is the most time consuming step. On average it takes 17 seconds to decompile an app and takes 1 second to extract applevel features. In the view-level feature extraction phase, AppQ runs each app for 3 minutes. Since the extraction of these two kinds of features work in parallel, the overall time consumption of feature extraction phase depends on the one that takes longer time, which is 183 seconds by view-level features extraction. Graph-to-vector step is very light-weighted, taking 0.003 second on each app. Then on the classiﬁcation step, classiﬁers work in parallel. The prediction process does not take much time. Finally, we apply the ensemble results of the classiﬁers. In general, it takes an average of 183 seconds to analyze each app. AppQ is efﬁcient and adequate for classifying large amounts of apps in the markets, especially when it is applied to analyze new apps. 4.6 Comparison with related work Most of existing work on recommendation is based on natural language processing. They get users’ feedback from reviews to recommend potentially interesting apps [38]. However, work on code-based app quality evaluation for recommending high quality apps is rather limited. We compare our work with DroidVisor [3] which a popular recommendation system mainly based on user-generated information. DroidVisor is proposed to recommend apps based on similarity (app description), popularity (downloads), security (permissions) and usability (ratings). DroidVisor requires users to set four weights. We set the weight of similarity to 0 since our experiments are already conducted in the same app category. The weight of popularity, rating and security are all set to 1, which makes DroidVisor give higher score to popular and secure apps. The test apps are divided into three groups based on their scores as predicted labels. As shown in Fig. 10, the accuracy of AppQ is much higher than DroidVisor, especially in category News and Weather, indicating AppQ can overcome the disadvantage of missing feedback and better proﬁle apps’ real quality. To demonstrate the effectiveness of our graph-to-vector method, we compare with two representative graph neural network models. We apply GCN [32] and GAT [34] to extract the app representation vectors from view graphs. These graph neural network models are initially proposed to solve node classiﬁcation problems, with which graph pooling [39], [40] operations are integrated to generate higher graphlevel representation based on node representations. As from Fig. 9, after we get the attributed view graphs and app-level features, we feed them into node aggregation layers to get the view-level features for each view. In each node aggregation layer, every node will aggregate feature information from its 1-hop neighbours through the links. After several node aggregation operations, we will get the aggregated view-level feature vectors for each view that contain the feature information of the views on the graph and the structure information of the graph. Then the aggregated view-level features are input to the graph pooling layer. In the graph pool layer, the hidden feature vectors of each node are fed through a fully-connected layer, and then the mean pool operation is applied to the set of nodes to get the graph-level features. After that, the graph-level features are combined with app-level features to produce the ﬁnal app-level features which incorporate both aggregated viewlevel and original app-level features. Got the new app-level features, we will follow the same classiﬁcation process as above to classify the apps to different categories. On a part of the categories, AppQ performs better than the variants that replace the graph-to-vector module by GCN and GAT, while performing on par with them on other categories. It provides a strong proof that top-k vertices and their neighbors are capable to represent the view graph. It is because even high quality apps always have several simple-designed views which users seldom use, e.g., account setting. AppQ only retain the features of main views that interact most closely with users, while ﬁltering out unimportant views which are noises to the model. 4.7 Case study AppQ is able to estimate an app’s quality despite the limited users’ feedback. As a case study, we test a few apps manually. As shown in Fig. 11 and Table 7, app 1 is a popular online eBook with exquisite views, and it has been classiﬁed correctly. App 2 is also an eBook app whose design is much alike app 1 and the user experience is also good, which AppQ predicated level 3 as its quality. But due to the fact that it is a newly published app, it suffers from the coldstart problem and has not been well introduced to users. The same situation happens to app 3, which is a weather app. It is user friendly and there are no ads in its views; thus, AppQ regards it as a high quality app. However, it had only 48 ratings and 20 comments on January 7th, 2019. We searched its information again on February 26th, there was no signiﬁcant change in downloads. The number of ratings and comments were slightly increased to 72 and 36, indicating it did not attract much attention from users. Therefore, there exists a discrepancy between the usergenerated information and app’s real quality. Purely based on user generated information to rank apps is not reliable. The cold-start problem hinders new released high quality apps from receiving due attention. With the analysis result from AppQ, app stores may give such apps a warm-start in the early stage. 4.8 Limitations and Discussions Many factors can affect the rating of an app. For example, whether the function meets individual users’ expectations or whether users like the theme or content of the app. However, these factors are sometimes very subjective, and that is why the same app may receive all kinds of ratings, from 1 star to 5 stars from different people. These factors cannot be measured by AppQ directly. Note that although AppQ has mainly focused on UI to estimate app quality, our experiments have demonstrated its effectiveness and efﬁciency. One potential application of AppQ is to detect promoted low-quality apps, which may be downloaded in a great number in a short time and given very high rating scores. With AppQ, such apps will less likely to be ranked high even they have lots of downloads and getting mostly ﬁve stars. Motivated to overcome the drawbacks of current ranking strategies and to give newly published apps a warm-start, we propose AppQ to grade apps based on their inborn codelevel features. AppQ performs code analysis to extract applevel features and dynamically runs apps to capture the switching among views. It constructs an attributed view graph that is encoded by a sequence of key vertices and their neighbors to highlight the importance of main views within an app. The view graph is further converted to a feature vector. An ensemble of machine learning approaches is then applied to evaluate and classify apps into different quality categories. The evaluation results demonstrates the effectiveness and efﬁciency of AppQ. In our future work, we are planning to explore more effective graph-to-vector approaches to better proﬁle relations between apps’ quality and UI. Exploring more features for characterizing apps’ structures is also being investigated.