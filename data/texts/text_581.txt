Abstract—VPN are a secured tunnel that help service providers to exchange data over non-secured networks. There is a large variety of VPN solutions that have variable deployment impacts on the target architecture as well as performance limitations or opportunities. This technical report compares Wireguard and OpenVPN for various SATCOM deployment scenarios and topologies. Working from home or interconnecting entreprise networks increase security needs, and this can be fulﬁlled by taking advantage of VPN solutions. When using HTTPS is not enough, an added layer of security may be deployed to guarantee that crossing a non-secured network will be safe. Since VPN can operate at the application or network layer, comparing VPN solutions may not be straightforward. Wireguard [1] operates at the network layer and aims at replacing IPsec for most use cases. OpenVPN [2] works at the application layer and is not compatible with IPsec. Depending on the scenarios of deployment, it may not be easy for an IT system provider to guarantee end-toend performances for one solution or the other one. As an example, Wireguard operating at the kernel level depends on the machine on which it is deployed, while OpenVPN can be tuned to the deployment use case by, e.g. using TCP or UDP to carry the secured tunnel trafﬁc. SATCOM systems exhibit a wide variety, both in terms of delay and goodput, making it hard to assess the relevant VPN solution for a speciﬁc scenario [3]. Indeed, for the sake of good end-to-end performance, SATCOM systems deploy Performance Enhancing Proxies [4]. Exploiting VPN tunnels may result in by-passing these proxies and impact the goodput of data transfers. This technical report contributes to the performance evaluation of VPN tunnels. We do not claim to provide the most extensive study on the subject, since we focus on the SATCOM scenarios. That being said, the performance evaluation may be of interest for other scenarios where performance enhancing proxies are deployed. This section describes the scenario that has been considered throughout this evaluation study. It also presents the SATCOM system and the characteristics of the VPN solutions. The architecture that will be exploited in this technical report is reported in Figure 1. An end user (PC) is connected to a box (offering Internet/media services and satellite access management) that includes the VPN client. A satellite terminal, a satellite and a satellite gateway interconnect the client’s box to a POP (Point of Presence) that includes the VPN server. Then, the POP is connected to the entreprise LAN that contains the server that the end user aims to reach. The SATCOM solution might integrate a performance enhancing proxy that may be either within or out of the VPN tunnel. The satellite system is emulated with netem and the tests are orchestrated with OpenBACH [5]. This technical report consider GEO and LEO systems. The characteristics of each system are the following: End-to-end losses on SATCOM systems have been measured in [6]. In order to assess the impact of losses on the proposed solutions, we have also included random losses on the SATCOM system. The PEP has been conﬁgured with the following options: The VPN solutions are either Wireguard or OpenVPN. In particular, the following parameters have been conﬁgured for OpenVPN: Delay-Product of the network) The following end-to-end applications have been introduced: This section includes the results that have been obtained. A subset of the obtained results is presented and more results, based on the parameters listed in Section II, are available upon request. This section presents the results for a 30 MB ﬁle transfer when only one ﬂow is considered. The options are the following: guard; (with or without Hystart) or BBRv2. The results are shown in Figure 2. The table gathers all the results. Comparative tables are shown in Figure 3, Figure 4, Figure 5 and Figure 6. The results are shown in Figure 2, Figure 3 and Figure 4. All transport layer variants are affected by the presence of a VPN, but the difference can be neglicted. When a VPN has to be included, the solution based on OpenVPN TCP in PEP position A results in the worst performance (i.e. ”TCP in TCP” issue) by increasing the transer time by at least 10 %. When a VPN has to be included, it is then recommended to use Wireguard with a PEP in position B. It is worth mentionning that other conﬁgurations, such as OpenVPN UDP with/without PEP and Wireguard without PEP, exhibit fair performance. Indeed, when we do not consider the OpenVPN TCP in PEP position A conﬁguration, the time to transfer is further reduced by only up to 5 % with Wireguard with PEP in position B if compared with the one achieved with aforementionned conﬁgurations. The results are shown in Figure 2, Figure 5 and Figure 6. When there are losses on the satellite link, the choice of BBRv2 as a transport layer protocol helps in improving wellknown TCP CUBIC issues when lossy links are considered. In this case, OpenVPN UDP or Wireguard show the best performances specially for GEO satellites. However, there are cases where the end-to-end congestion control can not be adapted (e.g. with end-to-end QUIC ﬂows or end servers nonmanaged by the operator). In the case where the end-to-end transport is CUBIC, OpenVPN TCP exhibits the best performance by reducing the transfer time by 30% as opposed to the case where no VPN and no PEP is proposed for the LEO scenario. In the case where the end-to-end transport is CUBIC, for the GEO scenario, OpenVPN TCP also exhibits the best conﬁguration. The recommendations that are proposed in this section are mainly related to the position of the losses. They are currently emulated in the satellite link and other position (e.g. losses on the LAN link) may differ. In particular, the gains brought by PEP when losses are located at the last-mile would be more important. This document reported so far results on the single ﬂow scenarios and on the scenarios where the congestion controls where the same on all entities. In the framework of this study, experiments with multiple ﬂows and with congestion controls that are different on the end host and on the PEP have been also done. This section sums up the main conclusions of this extended activity and the details may be provided up-on request. In order to have more conﬁdence on the recommendation on exploiting BBRv2 whenever it can be applied, it can be noted that the experiments considered BBRv2 on the end hosts and on the PEP. As a result, we can hardly guarantee that the conclusions would be the same if BBRv2 was applied only on the PEP, while CUBIC is applied on the end host. We have compared the downloading time of a 30 MB ﬁle in the case where BBRv2 is applied on all entities and in the case where BBRv2 is applied everywhere but the end host. The only case where the performance are not acceptable is the one where CUBIC is applied end-to-end and the PEP could not split the trafﬁc (and apply BBRv2 on the lossy segment of the network). When the connection can be split, the gains brought by BBRv2 tolerance to packet loss are present: the transfer time oscillates between 32 and 46 seconds. As a result, when BBRv2 can be exploited on the lossy segments, using it whenever possible remains the recommendation of this paper. To assess the impact of using multiple ﬂows across the network, we have considered: during hundred seconds and a second ﬂow starting ten seconds later that transmits data during eighty seconds. transfer and a second ﬂow transmit thirty MB ten seconds later. In the loss-less scenarios, Wireguard with a PEP in a B position enabled the largest amount of data transmitted in general and did not increase considerably the unfairness between the two ﬂows. In all conﬁgurations of PEP and VPN, the capacity sharing between the ﬂows oscillates between 40 % and 60 %. In the loss scenarios, Wireguard also exhibited the best performance in terms of 30 MB transfer time and did not impact the existing unfairness between the two ﬂows that were considered. OpenVPN TCP did not exhibit good performance in the multiple ﬂow scenarios, which may resides in the fact that multiple TCP ﬂows are tunneled within one single TCP ﬂow. As a result, when using OpenVPN TCP, it is recommended to consider one tunnel per TCP ﬂow.