Figure 1: Conversational critiquing workow. The system scores candidate items and generates a justication for the top item. If the user rejects the suggestion and critiques an aspect, the system uses the critique to update the latent user representation. Conversational recommender systems oer the promise of interactive, engaging ways for users to nd items they enjoy. We seek to improve conversational recommendation via three dimensions: 1) We aim to mimic a common mode of human interaction for recommendation: experts justify their suggestions, a seeker explains why they don’t like the item, and both parties iterate through the dialog to nd a suitable item. 2) We leverage ideas from conversational critiquing to allow users to exibly interact with natural language justications by critiquing subjective aspects. 3) We adapt conversational recommendation to a wider range of domains where crowdsourced ground truth dialogs are not available. We develop a new two-part framework for training conversational recommender systems. First, we train a recommender system to jointly suggest items and justify its reasoning with subjective aspects. We then ne-tune this model to incorporate iterative user feedback via self-supervised bot-play. Experiments on three real-world datasets demonstrate that our system can be applied to dierent recommendation models across diverse domains to achieve superior performance in conversational recommendation compared to state-of-the-art methods. We also evaluate our model on human users, showing that systems trained under our framework provide more useful, helpful, and knowledgeable recommendations in warm- and cold-start settings.. conversational recommendation, recommender systems, critiquing ACM Reference Format: Shuyang Li, Bodhisattwa Prasad Majumder, and Julian McAuley. 2021. SelfSupervised Bot Play for Conversational Recommendation with Justications. In arXiv. , 11 pages. Traditional recommender systems often return static recommendations, with no way for users to meaningfully express their preferences and feedback. However, interactivity and explainability can greatly aect a user’s trust of and willingness to use a recommender system [29,35]. This is reected in human conversations: experts justify their recommendations, customers critique suggestions, and both parties iterate through the conversation to arrive at a satisfactory item. Early work on interactive recommender systems focused on iteratively presenting suggestions to the user based on simple “like" and “dislike" feedback on individual items [3,11,15]. Gradually, systems began to accommodate more ne-grained user feedback—critiquing xed attributes of an item (e.g. its color or brand) [6]. Recent models for conversational critiquing incorporate user feedback on subjective aspects (e.g. taste and perception) [22,23,40]. However, such methods are trained using a next-item recommendation objective, and perform poorly when engaging with users over multiple turns. Another approach lies in training dialog agents to interact with the user over multiple turns [39]. While such models are able to generate convincing dialog in a vacuum, they require large corpora of transcripts from crowd-sourced recommendation games [12,20]. To create high-quality training dialogs, crowd-workers must be knowledgable about many items in the target domain—this expertise requirement limits data collection to a few common domains like movies. Additionally, these dialog policies limit a user’s freedom to interact with the system by asking yes/no questions about specic item attributes. We thus desire a conversational recommender system that mimics characteristics of human interactions not yet captured by existing systems: (1) It can justify suggestions made to the user; (2)It updates suggestions based on user feedback about subjective aspects; and (3)It can be trained using review data that is easily harvestable from arbitrary new domains. Table 1: Conversational critiquing systems (rst section) are transcript-free but not equipped for multi-turn interactions. Dialog-based agents (second section) learn multi-turn interactions using large corpora of domain-specic dialog transcripts. Our framework allows us to train multi-turn interactive recommender systems without costly transcript data. To accomplish these goals, we present a two-part framework to train conversational recommender systems. Ours is the rst framework, to our knowledge, that allows training of conversational recommender systems for multi-turn settings without the need for supervised dialog examples. First, using a next-item recommendation task we learn to encode historical user preferences and generate justications for our suggestions via sets of subjective aspects. We then ne-tune our trained model via multiple turns of bot-play in a recommendation game based on user reviews and simulated critiques. We apply our framework to two base recommender systems (PLRec [32] and BPR [30]), and evaluate the resultingPLRec-Bot andBPR-Botmodels on three large real-world recommendation datasets containing user reviews. Our method reaches the target item with a higher success rate than state-of-the-art methods, and takes fewer turns to do so, on average. We also conduct a study with real users, showing that it can eectively help users nd desired items in real time, even in a cold-start setting. We summarize our main contributions as follows: 1) We present a framework for training conversational recommender systems using bot-play on historical user reviews, without the need for large collections of human dialogs; 2) We apply our framework to two popular recommendation models (BPR-BotandPLRec-Bot), with each showing superior or competitive performance in comparison to state-of-the-art recommendation and critiquing methods; 3) We demonstrate that our framework can be eectively combined with query renement techniques to quickly suggest desired items. Users prefer recommendations that they perceive to be transparent or justied [33,35]. Some early recommender systems presented the objective attributes of suggested items to users [34,37], but did not attempt to personalize the justications. Another line of work considered the problem of generating natural language explanations of recommendations. McAuley et al. [24]extract key aspects from the text of user reviews using topic extraction. Such justications have been expanded into full sentences based on aspects of interest—constructed via template-lling [42] or recurrent language K∈ RUser aspect frequency; kis how oftenuser 𝑢 mentioned aspect 𝑎 in their reviews. K∈Binary matrix; kis 1 if and only if aspect 𝑎was used in any review of item 𝑖 𝛾,𝛾∈ RLearned ℎ-dimension user/item embeddings. ˆ𝑘∈Predicted justication (binary for all aspects). 𝑐∈ RCumulative critique vector representing theuser’s evolving opinion about each aspect. 𝑚∈The user critique vector at turn 𝑡. 𝑚is 1 ifand only if the user critiqued 𝑎 at turn 𝑡. models [26]. Due to the unstructured nature of these justications, however, sentence-level justications have not been used for iteratively rening recommendations. In this work, justications take the form of specic aspects that a user is interested in (e.g. that a song is poetic) [2,24,40]. In Section 4.2 we describe how we extract such aspects from user reviews in large recommendation datasets. Users often seek to make informed decisions around consumption, and controllability of a recommendation system is linked to improved user satisfaction [27]. We thus turn to conversational recommendation as a way to iteratively engage with a user to learn their preferences with the goal of recommending a suitable item [5]. We view recommenders as domain experts engaging with human customers, able to elicit user preferences and requirements and suggest appropriate items in the course of the conversation. In early interactive recommender systems, users were only able to give binary “like" and “dislike" feedback without further explanation [15]. One line of research used such feedback to rene the search space for retrieving desired images from the web [11]. Biswas et al. [3]extended this approach to interactive product search. More recently, multi-armed bandit approaches to conversational recommendation [43,45] leverage exploration-exploitation algorithms to maximize the information learned from feedback at each turn. Another line of work treats conversational recommendation as a form of task-oriented dialog where users express opinions about specic aspects of an item. At each turn, the user is either a) asked if they prefer a specied aspect; or b) recommended an item [7]. Self-supervised bot-play has been explored as a way to train such conversational dialog agents [12,20], but such approaches require Wizard-of-Oz style data [8] with humans playing the role of expert and seeker. The quality of such dialog data depends heavily on the domain knowledge and competence of crowd-workers, which makes it unsuitable for complex domains. Zhang et al. [41]uses templated dialog forms and trains a model to ask about aspects that are most informative of the user’s preferences. However, this forces the user to answer yes/no questions and restricts their exibility when giving feedback. Instead, we explore conversational critiquing, where a user is presented with items and justications, and is able to give feedback regarding any aspect in the justication. Figure 2: In (latent) conversational critiquing, user feedback about aspects (𝑐, 𝑐) modies our prior latent user preference vector 𝛾to bring it closer to the target item embedding. Critiquing systems aim to help users incrementally construct their preferences in a way that mimics how humans rene their preferences and constraints depending on conversation context [36]. Early critiquing methods relied on constraint-based programming to iteratively shrink the search space of items as users provided more critiques [4]. More recently, Wu et al. [40]introduced a critiquing model with justications via a list of natural language aspects mined from user reviews. In this setting, users are able to interact with any aspect present in the generated justication. Antognini et al. [2]generate a single sentence of explanation alongside the set of aspects, but still require users to interact with the aspect set. Luo et al. [23]use a variational auto-encoder (VAE) [14] in place of the collaborative ltering model, learning a bi-directional mapping function between user latent representations and aspects they have expressed in reviews. Such models can generate high precision justications but have shown poor multi-turn recommendation performance. Latent Linear Critiquing (LLC) methods do not generate justications and instead allow users to critique any aspect from the vocabulary [18,22]. After training a matrix factorization model to predict ratings, these models then learn a linear regressor to recover user embeddings from their historical aspect usage frequency. A linear programming problem is then solved to weight a user’s critiques during each turn of the conversation, which we observe to take an order of magnitude longer than VAE-based methods and our own. Furthermore, while LLC assumes that user preferences are fully explained by their review texts, recent studies have shown that this assumption may be unfounded [31]. In Table 1 we compare our approach in context of recent frameworks for training dialog agents for conversational recommendation and conversational critiquing agents. Our model consists of three sections, as seen in Figure 3: (1)A matrix factorization recommender model𝑀that learns to embed users and items in an ℎ-dimensional latent space; (2)A justication head𝑀that predicts the aspects of an item toward which the user holds preferences; Figure 3: The proposed model architecture. Given a user, items, and critique vector, our model encodes the critique 𝑀(𝑐) and fuses it with the user embedding𝛾. The fused user representation 𝛾and item representation 𝛾are then used to predict the justication and score items. (3)A critiquing function𝑓that modies a user’s preference embedding based on user feedback about specic aspects. Our model supports multi-turn critiquing as shown in Figure 2. At each turn of a conversation, a user may provide explicit feedback about aspects they dislike about the current set of recommendations in the form of a critique (𝑐). The critiquing function𝑓then uses this critique to modify our latent user representation𝛾in order to bring it closer to the user’s target item. Our method can be applied to any recommender system𝑀that learns user and item representations. We demonstrate its eectiveness using two popular methods based on matrix factorization and linear recommendation. Bayesian Personalized Ranking (BPR) [30] is a matrix factorization recommender system that seeks to decompose the interaction matrixR ∈ Rinto user and item representations [16]. BPR optimizes a ranked list of items given implicit feedback (a set of items with which a user has recorded a binary interaction). We learnℎ-dimensional user and item embeddings (𝛾,𝛾), computing the score via the inner product:ˆ𝑥= ⟨𝛾,𝛾⟩. At training time, the model is given a user𝑢, observed item𝑖 ∈ 𝐼, and unobserved item𝑗 ∈ 𝐼. We maximize the likelihood that the user prefers the observed item 𝑖 to the unobserved item 𝑗: where 𝜎 represents the sigmoid function. Projected Linear Recommendation (PLRec) is an SVD-based method to learn low-rank representations for users and items via linear regression [32]. The PLRec objective minimizes the following: where𝑉is a xed matrix obtained by taking a low-rank SVD approximation ofRsuch thatR = 𝑈 Σ𝑉, and𝑊is a learned embedding matrix. We thus obtain anℎ-dimensional user embedding 𝛾= 𝑟𝑉 and ℎ-dimensional item embedding 𝛾= 𝑊. Our justication model𝑀consists of an aspect prediction head: a fully connected network with twoℎ-dimensional hidden layers Algorithm 1:Bot play framework for ne-tuning conversational recommenders. Recommendation and Justication models 𝑀, 𝑀; Critique fusion function 𝑓; Seeker model 𝑀; for each user 𝑢 do for goal item 𝑔 ∈ 𝐼(Evaluation set) do end end that predicts a score𝑠for each aspect𝑎. This model takes as input the sum of the learned user and item embeddings (𝛾,𝛾). At training time, we incorporate an aspect prediction lossL by computing the binary cross entropy (BCE) for each aspect: L= −1|𝐴|k· log 𝑝+ (1 − k) · log(1 − 𝑝) (3) where𝑝= 𝜎 (𝑠)represents the likelihood of user𝑢caring about aspect𝑎in context of item𝑖. At inference time, we again compute the likelihood for each aspect (𝑝= 𝜎 (𝑠)) and sample from the Bernoulli distribution with𝑝to determine which aspects 𝑎 appear in the justication. We posit that the user’s latent representation can be partially explained by their written reviews. Thus, we jointly learn an aspect encoder𝑀alongside our recommendation model. This takes the form of a linear projection from the aspect space to the user preference space:𝑀(𝑐) = 𝑊𝑐+ 𝑏, where𝑐∈ Zis the critique vector representing the strength of a user’s preference for each aspect. We then fuse this aspect encoding with the latent user embedding from𝑀to form the nal user preference vector𝛾: For the BPR-based model, we use𝑓 (𝑎, 𝑏) = 𝑎 + 𝑏as a fusion function, and for the PLRec-based model, we use𝑓 (𝑎, 𝑏) =. During training, the aspect encoder takes in the user’s aspect history: 𝑐= k. To train our BPR-based model, we jointly optimize each component. Each training example consists of a user𝑢, an observed item𝑖 ∈ 𝐼 that the user has interacted with, and an unobserved item𝑗 ∈ 𝐼 that the user has not rated. We predict scores for items 𝑖 and 𝑗: We rst compute the BPR loss (see Section 3.1) with predicted scores ˆ𝑥andˆ𝑥. We add the aspect prediction loss, scaled by a constant 𝜆to the ranking loss for our training objective:L = 𝜆L− L. We nd empirically that 𝜆∈ {0.5, 1.0} works well. To train our PLRec-based model, we follow Luo et al. [22]and separately optimize𝑀,𝑀, and𝑀. The user and item embeddings are learned via eq. (2). We solve the following linear regression problem to optimize 𝑀: Finally, we optimize the aspect prediction (justication) lossLto train the justication head. To perform conversational critiquing with a model trained using our framework, we adapt the latent critiquing formulation from Luo et al. [22], as shown in Figure 1. Each conversation with a user𝑢consists of multiple turns. At each turn𝑡, the system assigns scoresˆ𝑥for all candidate items𝑖, and presents the user with the highest scoring itemˆ𝑖. The system also justies its prediction with a set of predicted aspectsˆ𝑘. The user may either accept the recommended item (ending the conversation) or critique an aspect from the justication: 𝑎 ∈ {𝑎|ˆ𝑘= 1}. Given a user critique, the system modies the predicted scores for each item and presents the user with a new item and justication: Eectively, a user critique modies our prior for the user’s preferences; we then re-rank the items presented to the user. At inference time,𝑐is the cumulative critique vector, initialized with the user’s aspect history: where⊙is element-wise multiplication. We usemax(k,1)as the critique should match the strength of the user’s previous opinion on the aspect—otherwise the encoding may have a small magnitude. Even if a user has not mentioned an aspect in their previous reviews, the max ensures a non-zero eect from each critique. We propose a framework for critiquing via bot play that simulates conversations when provided just a known set of user reviews. We rst pre-train our expert model (recommender model, justication model, and aspect encoder). A seeker model𝑀is pre-trained via a simple user prior: when provided with a known target item Table 3: Descriptive statistics of datasets, including average unique aspects expressed in reviews per item and user. and justication, it selects the most popular aspect present in the justicationˆ𝑘but not the target’s historical aspectskto critique. For each training example (user𝑢and a goal item they have reviewed𝑔), we allow the expert and seeker models to converse with the goal of recommending the goal item. We ne-tune the expert by maximizing its reward (minimizing its loss) in the botplay game (Algorithm 1). We end the dialog after the goal item is recommended or a maximum session length of𝑇 =10 turns is reached. We dene the expert’s loss as the cross entropy loss of recommendation scores per turn: where𝛿is a discount factorto encourage successfully recommending the goal item at earlier turns.L(𝑔,ˆ𝑥)is the cross entropy loss between predicted scores and the goal item: L(𝑔,ˆ𝑥) = −𝑃 (𝑖) log𝑄 (𝑖); 𝑄 (𝑖) =𝑒Í(11) where𝑃 (𝑖)is 1 if𝑔 = 𝑖and 0 otherwise. As the cross-entropy loss is continuous, we optimize the reward for each conversation (𝑢,𝑖). To train our initial model, we select hyperparameters via AUC on the validation set. We select hyperparameters for bot-play netuning by evaluating the success rate at 1 (SR@1) on the validation set. We train each model once, with three evaluation runs per experimental setting. For baseline models, we re-used the authors’ code. We include additional training details in the supplementary materials.We will make our code available upon publication. We evaluate the quantitative performance of our model using three real-world, publicly available recommendation datasets: Goodreads Fantasy (Books) [38], BeerAdvocate (Beer) [24], and Amazon CDs & Vinyl (Music) [10,25]—each with over 100K reviews and ratings. We keep only reviews with positive ratings, setting thresholds of 𝑡 >4.0 for Beer and Music and𝑡 >3.5 for Books. We partition each dataset into 50% training, 20% validation, and 30% test splits. Dataset statistics are shown in Table 3. Our datasets do not contain pre-existing aspects, so we follow the pipeline of [40] to extract subjective aspects from user reviews: Recommendation and Justication models 𝑀, 𝑀; Critique fusion function 𝑓; for each user 𝑢 do for goal item 𝑔 ∈ 𝐼(Evaluation set) do end end return average success rate & length (1)Extract lists of high-frequency unigrams and bigrams (nouns and adjective phrases only) from all user reviews; (2)Prune the bigram keyphrase list using a Pointwise Mutual Information (PMI) threshold, ensuring aspects are statistically unlikely to have randomly co-occurred; (3)Represent reviews as sparse binary vectors indicating whether each aspect was expressed in the review. Aspects describe a wide range of qualities; for beers, users commonly describe the malt (e.g. roasted) and taste (e.g. citrus). For music, aspects range from perceived genres (e.g. techno) to emotions (e.g. soulful). Users describe books by reacting to character descriptions (e.g. strong female) and settings (e.g. realistic).. Following prior work on conversational critiquing [18,22], we simulate multi-step recommendation dialogs to assess model performance. We randomly sample 500 user-item interactions from the test set to conduct user simulations following Algorithm 2 for each user𝑢and goal item𝑔. At each turn, we recommend an itemˆ𝑖to the user alongside a set of aspectsˆ𝑘. If the goal item is not recommended, the user will critique an aspect𝑎from the justication that is inconsistent with the goal item aspects: 𝑎 ∈ {𝑎 |ˆ𝑘=1 &k=0}We set a maximum session limit of 𝑇 = 10 turns. To evaluate how our models behave with dierent user behaviors, we simulate each observation with three dierent critique selection strategies [18]: • Random: We assume the user randomly chooses an aspect. This assumes no prior knowledge on the part of the user. • Pop: We assume the user selects the most popular aspect used across all training reviews. • Di: We assume the user selects the aspect that deviates most from the goal item reviews. In simulations, we select Figure 4: Success Rate @ N (% dialogs where target item rank ≤ N) across datasets and user models. BPR-Bot (brown triangle) and PLRec-Bot (pink circle) out-perform baselines (dashed) in all settings. the aspect with the largest frequency dierential between the goal item and current item: arg max(k− k) In all critiquing settings, a user may not critique the same aspect multiple times in a session, and any recommended items are removed from consideration in the following turns. As our method can be applied to any base recommender system 𝑀, we apply our framework to train models based on BPR and PLRec (see Section 3.1)—BPR-Bot and PLRec-Bot, respectively. We assess Latent Linear Critiquing (LLC) baselines, which embed critique vectors𝑐in the sameℎ-dimensional space as the latent user representation𝛾.𝑓is dened as a weighted sum of the embedding for each critiqued aspect, alongside the original user preference vector.UAC[22] averages the initial user embedding and all critiqued aspect embeddings.BAC[22] rst averages critiqued aspects, and then averages the result with the initial user embedding.LLC-Score[22] learns the weights via a linear program maximizing the posterior rating dierences between items containing critiqued aspects and those without. Instead of directly optimizing the scoring margin,LLC-Rank[18] minimizes the number of ranking violations. These models cannot generate justications; we binarize the historical aspect frequency vector for the item (k) as a justication at each turn. We compare against these models to evaluate whether generating personalized justications can improve critiquing. Figure 5: Avg. number of turns before the target item reaches rank N, across datasets and user models. BPR-Bot (brown triangle) and PLRec-Bot (pink circle) promote target items faster than baselines (dashed), especially for low N. We also compare against a state-of-the-art variational conversational recommender,CE-VAE[23]—an improvement on the Wu et al. [40]justied critiquing model—which jointly learns to recommend and justify. CE-VAE learns a VAE with a bidirectional mapping between critique vectors and the user latent preference space. We compare our models to CE-VAE to assess how justication quality impacts multi-turn critiquing performance. RQ1: Can our framework enable multi-step critiquing?To measure multi-step critiquing performance, we assess the average success rate and session length following Luo et al. [22]. Success rate measures the percentage of sessions in which the target item reaches rank N, and session length measures the average length of sessions with a limit of 10 iterations. Success rates and session lengths for each dataset and user behavior model are shown in Figure 4 and Figure 5, respectively. Our models are ne-tuned via bot-play with a seeker model that assumes one particular user behavior: popularity-based critique selection. As such, we expect it to perform better in the Pop user setting. However, BPR-Bot and PLRec-Bot succeeds at a higher rate in fewer turns than baselines under all user settings—including random aspect critiquing, which assumes no prior on user behavior. Variational Baseline. Despite its strong rst-turn recommendation performance and high-delity justications, CE-VAE is outperformed by our models in all nine settings across all metrics. This supports our observation that the training method to learn Figure 6: Success Rate @ N (% dialogs where target item rank ≤ N), comparing bot-play methods (orange) against non-botplay ablations (blue). Bot-play ne-tuning improves target item ranking across datasets compared to the ablation, for both BPR-Bot (crosses) and PLRec-Bot (circles). a bi-directional mapping between latent user preferences and a justication causes a trade-o between justication quality and critiquing ability. Linear Baselines. We further observe that linear critiquing models (UAC, BAC, LLC-Score, and LLC-Rank) perform poorly on multistep critiquing, especially when trying to nd the goal item outright (𝑁 =1). This conrms our observation that the method of co-embedding aspect critiques with learned user latent preferences ignores the existence of user preferences not explained by review text. This additionally suggests that generating personalized justications helps users more eectively choose aspects to critique. In general, the large item space makes it dicult for critiquing models to reach the goal item within the turn limit, with the best model reaching the goal item in only 6-15% of sessions. This suggests that practical conversational critiquing systems may benet from constraint-based ltering as well as starting the session from an initial set of user requirements—while users rarely enter a conversation knowing their full preference set [28], they often start with a limited set of broad requirements (e.g. when buying a car, they want an SUV or a coupe). We demonstrate in RQ3 that our model can be combined with constraint-based query renement to quickly reach signicantly higher success rates. RQ2: Can our bot-play framework improve multi-step critiquing performance?We next compare BPR-Bot (left) and PLRec-Bot (right) in Figure 6 against ablated versions that were trained using the rst step of our framework but not ne-tuned via bot-play. For clarity, we display only results using the Pop user behavioral model, as we observe the same trends with the Random and Di user models. In domains with relatively high aspect occurrence across reviews (Books, Beer), we observe that bot-play confers a 3-6% improvement in success rate for various N. This Figure 7: Hit rate by turn for query renement models on each dataset with multi-step critiquing up to 10 turns. demonstrates that we can eectively train conversational recommender systems using our bot-play framework using domains with rich user reviews in lieu of crowd-sourced dialog transcripts. In domains with more sparse coverage of subjective aspects (i.e. Music), we observe lower improvement when using bot-play. Here, our model may not encounter sucient examples of rare aspects being critiqued. In future work, we will explore methods to add noise to our user model to ensure that the bot-play process encounters more rare aspects. We will also investigate additional losses for bot-play, including ranking losses instead of cross entropy. We conrm that our method is model-agnostic, as it improves conversational recommendation success rates for both the matrix factorization-based (BPR) and linear (PLRec) recommender systems. We also observe that models with a higher latent dimensionality (ℎ ∈ [50,400]for PLRec-Bot vs.ℎ =10 for BPR-Bot) benet more from bot-play, suggesting that our method eectively learns to navigate complex user preference spaces. RQ3: Can our models be eectively combined with query renement?So far, we have assumed that users provide soft feedback via critiques: even if a user has critiqued aspect𝑎during a session, future suggested items may still contain aspect𝑎. This assumption holds for some aspects: for example, even if previous users mentioned that a song was dispassionate, a user may nd it emotional and enjoyable. However, in a real-world setting that user may reject the suggestion after reading the reviews. Thus, we experiment with treating critiques as hard feedback: if a user critiques some aspect𝑎, we prune all candidate items whose reviews mention𝑎. We compare three models in this setting, with the turn-0 ranked list of candidate items initialized from BPR-Bot. TheQuerybaseline model suggests one item per turn and asks the user whether they like aspect𝑎. If the user answers yes, we prune all candidate items whose reviews have not expressed𝑎: 𝐼← {𝑖 ∀ 𝑖 ∈ 𝐼|k=1}. Otherwise, we prune all candidates whose reviews have expressed the aspect:𝐼← {𝑖 ∀ 𝑖 ∈ 𝐼|k= 0}. At each turn, we pick the aspect that most evenly divides the remaining candidate items:arg min||𝐼| − |𝐼||Eectively, we perform binary search over our candidate space, and expect to nd the target item within log |𝐼 | turns. In theFiltermodel, we suggest an item alongside a generated justication per turn. When a user critiques aspect𝑎, we prune candidate items whose reviews have expressed𝑎:𝐼← {𝑖 ∀ 𝑖 ∈ 𝐼|k=0}. We extend this model via our learned critiquing function𝑓to further modify the user preference vector and Table 4: Conversation-level human evaluation via ACUTEEVAL. Win (W) and Loss (L) percentages are reported while ties are not. All results statistically signicant with 𝑝 <0.05. PLRec-Bot Useful Informative Knowledgeable Adaptive re-compute scores for the remaining items. This hybridFilter+Rerankmodel then re-ranks the remaining candidate items for the next turn. We conduct user simulations with the Pop user model following Algorithm 2, and plot the success rate by turn—rate of achieving the goal item 𝑔 at or before turn 𝑡—in Figure 7. Binary queries are guaranteed to eventually nd the answer, but the queried aspect may not be related to suggested items. By allowing the user to provide negative critiques, we can rapidly reduce the search space at early turns. Across domains the success rate rises much faster in the rst 6-10 turns for Filter and Filter+Re-rank compared to binary querying. Re-ranking after ltering improves performance across domains, suggesting that we have learned how user critiques relate to their latent preferences for other aspects. For the Beer and Books domains, the ltering approach reaches higher success rates compared to binary querying same high success rate within the session turn limit (70.7% vs. 69.7% and 57.0% vs. 55.2%, respectively). We see less of a benet in the Music domain. Relative aspect sparsity may play a role: per Table 3, only 25% of possible aspects are expressed for the average item. There also exists a longer tail of aspects expressed only for a small set of items in Music compared to the other datasets. As such, user critiques prune fewer candidate items on average in Music. Our bot-play framework can be easily adapted to train models incorporating hard critiquing constraints by pruning candidate items. One possible extension involves masking the cross entropy (netuning) loss to only adjust the scores of non-pruned items, setting pruned item scores to a large negative value:ˆ𝑥= −1𝑒15∀ 𝑖 ∈ 𝐼. We also wish to explore ne-tuning with a ranking loss during botplay, to encourage the model to rank items containing a critiqued aspect 𝑖 ∈ 𝐼below those without. Human EvaluationTo assess the quality of the simulated conversations during bot-play, we conduct human evaluations with 100 samples. Following ACUTE-EVAL [19], we conduct a comparative evaluation of each sample conversation on four criteria: which agent seems more useful, informative, knowledgeable and adaptive. We compare each bot-play model (BPR-BotandPLRec-Bot) against an ablative version (with no bot-play ne-tuning) and the best baseline model (CE-VAE). Table 5: Turn- and conversation-level feedback from coldstart user study. Statistically signicant results are bolded. Each sample is evaluated by three annotators. We observe substantial [17] inter-annotator agreement, with Fleiss Kappa [9] of 0.67, 0.79, 0.73, and 0.60 for the usefulness, informativeness, knowledgeable, and adaptiveness criteria, respectively. BPR-Bot and PLRec-Bot are judged to be signicantly more informative and knowledgeable compared to ablative models and CE-VAE, showing that our justication module accurately predicts aspects of a recommended item. We design the usefulness and adaptiveness criteria to capture how our framework aids the user in achieving their conversational goal (i.e. recommending the most relevant item within a minimum number of turns). Compared to the alternatives, models trained under our bot-play framework are judged to be more useful and adapt their recommendations in a manner more consistent with critiques. Our framework allows us to train conversational agents that are useful and engaging for human users: evaluators overwhelmingly judged the models trained via bot-play to be more useful, informative, knowledgeable, and adaptive compared to CE-VAE and ablated variants. Cold-Start User StudyWe conduct a user study using items and reviews from the Books dataset to evaluate our model’s ability to provide useful conversational recommendations in real-time. We recruited 32 real human users to interact with ourBPR-Bot recommender and another 32 to interact with the ablation model (no conversational ne-tuning). As evaluators do not correspond to users in our training data, we initialize each conversation with the average of all learned latent user representations. At each turn, the user is presented with the three top-ranked items and their justications (list of aspects), and is allowed to critique multiple aspects. On average, users critiqued two aspects per turn—this suggests that when training conversational models we should assume multiple critiques at each turn. We evaluate our systems following Li et al. [19]: at each turn, we ask our users if the generated justications are informative, useful in helping to make a decision, and whether our system adapted its suggestions in response to the user’s feedback. We provide four options for each question: yes, weak-yes, weak-no, and no, mapping these values to a score between 0 and 1 [13]. We display the normalized aggregated score for each question in Table 5. We nd that BPR-Botsignicantly out-scores the ablation model in all three metrics (𝑝 <0.01), showing that ne-tuning our model on a botplay framework instills a stronger ability to respond to techniques and provide meaningful justications—even for unseen users. At the end of a conversation, we additionally ask the user how frequently (if at all) they would choose to engage with our conversational agent in their daily life. 69% of users indicated they would “often" or “always" use BPR-Bot to nd books, compared to 41% of users for the ablation model. We thus nd that ne-tuning our model via bot-play also makes it signicantly (𝑝 < 0 .05) more useful for new users. In this work, we aim to develop conversational agents for recommendation that engage with users following common modes of human dialog: justifying why suggestions were made and incorporating feedback about certain aspects of an item to provide better recommendations at the next turn. We present a framework for training conversational recommenders in this modality via selfsupervised bot-play. Our framework is model-agnostic and allows conversational recommenders to be trained on any domain with review data. We use two popular underlying recommender systems to train theBPR-BotandPLRec-Botconversational agents using our framework, demonstrating quantitatively on three datasets that our models 1) oer superior multi-turn recommendation performance compared to current state-of-the-art methods; 2) can be eectively combined with query renement techniques to quickly converge on suitable items; and 3) can iteratively rene suggestions in real-time, as shown in user studies. In future work, we aim to adapt our framework to natural language critiques (i.e. complete utterances), allowing users to freely express their feedback in a less restrictive way.