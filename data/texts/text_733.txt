This article is categorized under: Keywords: Interpretable Deep Learning, Deep Learning in medicine In recent years, the wide adoption of electronic health record (EHR) systems by healthcare organizations and subsequent availability of large collections of EHR data have made the application of Artiﬁcial Intelligence (AI) techniques in healthcare more feasible. The EHR data contain rich, longitudinal, and patient-speciﬁc information including both structured data (e.g., patient demographics, diagnoses, procedures) as well as unstructured data, such as physician notes and medical images [Mesko, 2017]. Meanwhile, deep learning (DL), a family of machine learning (ML) models based on deep neural networks, has achieved remarkable progress in the last decade on various datasets for diﬀerent modalities including images, natural language, and structured time series data [LeCun et al., 2015]. The availability of large-scale data and unprecedented technical advances have come together to spark a surge of research interest in developing a variety of deep learning based clinical decision support systems for diagnosis, prognosis and treatment [Murdoch and Detsky, 2013]. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain [Tonekaboni et al., 2019a]. One pivotal impediment relates to the black box nature, or opacity, of deep learning algorithms, in which there is no easily discernible logic connecting the data about a case to the decisions of the model. Healthcare abounds with possible “high stakes” applications of deep learning algorithms: predicting a patient’s likelihood of readmission to the hospital [Ashfaq et al., 2019], making the diagnosis of a patient’s disease [Esteva et al., 2017], suggesting the optimal drug prescription and therapy plan [Rough et al., 2020], just to name a few. In these critical use cases that include clinical decision making, there is some hesitation in the deployment of such models because the cost of model mis-classiﬁcation is potentially high [Mozaﬀari-Kermani et al., 2014]. Moreover, it has been widely demonstrated that deep learning models are not robust and may easily encounter failures in the face of both artiﬁcial and natural noise [Szegedy et al., 2014, Finlayson et al., 2019a, Jin et al., 2020]. Artiﬁcial intelligence (AI) systems are, on the whole, not expected to act autonomously in patient care, but to serve as decision support for human clinicians. To support the required communication between such systems and people, and to allow the person to assess the reliability of the system’s advice, we seek to build systems that are interpretable. Interpretable DL allows algorithm designers to interrogate, understand, debug, and even improve the systems to be deployed by analyzing and interpreting the behavior of black-box DL systems. From the end user perspective, interpretable DL allows end users to evaluate the model decision making to determine whether to accept or reject predictions and recommendations before an action is taken. In particular, in this review we focus on the interpretability of the DL models in health care. Such models are known for both their complexity and high performance on a variety of tasks, yet the decisions and recommendations of deep learning systems may be biased [Gianfrancesco et al., 2018]. Interpretability can oﬀer one eﬀective approach to ensuring that such systems are free from bias and fair in scoring diﬀerent ethnic and social groups [Hajian et al., 2016]. Many DL systems have already been deployed to make decisions and recommendations in non-healthcare settings for tens of millions of people around the world (e.g., Netﬂix, Google, Amazon) and we hope that DL applied in healthcare will also become widespread [Esteva et al., 2019]. To this end, we need help from interpretability to better understand the resulting models to help prevent potential negative impacts. Lastly, there are some legal regulations such as the European Union (EU)’s General Data Protection Regulation (GDPR) that require organizations that use patient data for predictions and recommendations to provide on demand explanations for an output of the algorithm, which is called a “right to explanation” [Tesfay et al., 2018, Edwards and Veale, 2018]. The inability to provide such explanations on demand may result in large penalties for the organizations involved. It should be noted that the notion of explanation of a decision in itself is not a very well deﬁned concept: indeed the original EU GDPR Recital 71 does not provide a clear deﬁnition beyond stating a person’s right to obtain it. There have been active discussions in the community on this notion [Lipton, 2018]; for instance, [Muggleton et al., 2018] proposed an operational deﬁnition of comprehensibility and interpretability based on the ultra-strong criteria for Machine Learning proposed by [Michie, 1988] and Inductive Logic Programming [Kovalerchuk et al., 2021]. However, no single uniform deﬁnition has been reached: for any complex model with no superﬁcial components, any simple explanation is inherently unfaithful to the underlying model. The decision on what deﬁnition of explanation to use necessarily aﬀects the properties of the methods used to produce them: for example focusing on producing a per example explanation vs. the structure of the network analysis favors local (why this particular example resulted in a given prediction) explanation over global ones (what kinds of knowledge are encoded in the model and how they aﬀect predictions). In this work, we ﬁrst cover the most common type of interpretation method, in which an explanation is an assignment of a score to each input element that reﬂects its importance to a model’s conclusions. We also brieﬂy discuss example based explanation methods. Other approaches to interpretability include a more recent focus on feature interactions for neural networks [Sundararajan et al., 2020, Tsang et al., 2018, Tsang et al., 2020] and whole network behavior analysis [Carter et al., 2019]. It is conventionally thought that there is a trade-oﬀ between model interpretability and performance (e.g., F1, accuracy). For example, more interpretable models such as regression models and decision trees often perform less well on many prediction tasks compared to less interpretable models such as deep learning models. With this constraint, researchers have to balance the desire for the most highly performing model against adequate interpretability. Fortunately in the last few years, researchers have proposed many new methods that can maintain the model performance while producing good explanations, such as LIME [Ribeiro et al., 2016a], RETAIN [Choi et al., 2016], and SHAP [Lundberg and Lee, 2017], described below. And many of them have been adapted and applied to healthcare problems with good interpretability achieved. This survey aims to provide a comprehensive and in-depth summary and discussion over such methods. Previous surveys on explainable ML for healthcare [Ahmad et al., 2018, Holzinger et al., 2019, Wiens et al., 2019, Tonekaboni et al., 2019a, Vellido, 2019, Payrovnaziri et al., 2020] mainly discuss the deﬁnition, concept, importance, application, evaluation, and high-level overview of methods for interpretability. In contrast, we will focus on introducing the methods for interpretability in depth so as to provide methodological guidance for future researchers or clinical practitioners in this ﬁeld. Besides the methods’ details, we will also include a discussion of advantages and disadvantages of these methods and which scenarios each of them is suitable for, so that interested readers can know how to compare and choose among them for use. Moreover, we will discuss how these methods originally developed for solving general-domain problems have been adapted and applied to healthcare problems and how they can help physicians better understand these data-driven technologies. Overall, we hope this survey can help researchers and practitioners in both AI and clinical ﬁelds understand what methods we have for enhancing the interpretability of their DL models and choose the optimal one accordingly based on a deep and thorough understanding. For readers’ convenience, we have provided a map between all abbreviations to be used and their corresponding full names in Table 2. Paper Selection: We ﬁrst conducted a systematic search of papers using MEDLINE, IEEE Xplore, Association for Computing Machinery (ACM), and ACL Anthology databases, several prestigious clinical journals’ websites such as Nature, JAMA, JAMIA, BMC, Elsevier, Springer, Plos One, etc., as well as the top AI conferences such as NeurIPS, ICML, ICLR, AAAI, KDD, etc.. The keywords for our searches are: (explainable OR explainability OR interpretable OR interpretability OR understandable OR understandability OR comprehensible OR comprehensibility) AND (machine learning OR artiﬁcial intelligence OR deep learning OR AI OR neural network). After initial searching, we conducted manual ﬁltering by reading titles and abstracts and only retained three types of works for subsequent careful reading: interpretability methods developed for general domain problems, interpretability methods speciﬁcally developed for healthcare problems, and healthcare applications that involve interpretability. We only covered the methods that can interpret DL models. The literature of explanation methods for DL grows rapidly, so any review of this type is captive to its date of completion. Searching the above-mentioned sources with the keywords we used for recent articles should help to bring an appreciation of the ﬁeld up to date. In this section, we will introduce various kinds of interpretability methods, which aim to assign an attribution value, sometimes also called ”relevance” or ”contribution”, to each input feature of a network. Such interpretability methods can thus be called attribution methods. More formally, consider a deep neural network (DNN) that takes an input x = [x, ..., x] and produces an output S(x) = [S(x), ..., S(x)], where C is the total number of output neurons. Given a speciﬁc target neuron c, the goal of an attribution method is to determine the contribution R= [R, ..., R] of each input feature xto the output S. For a classiﬁcation task, the target neuron of interest is usually the output neuron associated with the correct class for a given sample. The obtained attribution maps are usually displayed as heatmaps, where one color indicates features that contribute positively to the activation of the target output while another color indicates features that have a suppressing eﬀect on it. To organize our presentation, we classify all attribution methods into the following categories: back-propagation based, attention based, feature perturbation based, model distillation based, and game theory based. We also include example and generative based interpretation for DL methods for completeness. More technical details for each category will be elaborated below. The most popularly used interpretability method is based on back-propagation of either gradients [Simonyan et al., 2014] or activation values [Bach et al., 2015]. This line of methods starts from the Saliency Map [Simonyan et al., 2014], which follows the normal gradient back-propagation process and constructs attributions by taking the absolute value of the partial derivative of the target output Swith respect to the input features x, i.e., ||. Intuitively, the absolute value of the gradient indicates those input features that can be perturbed the least in order for the target output to change the most. However, the absolute value prevents the detection of positive and negative evidence that might be present in the input. To make the reconstructed heatmaps signiﬁcantly more accurate for convolutional neural network (CNN) models, Deconvolution [Zeiler and Fergus, 2014a] and Guided Back-propagation [Springenberg et al., 2015] were proposed and these two methods and the Saliency Map method diﬀer mainly in the way they handle back-propagation through the rectiﬁed linear (ReLU) non-linearity. As illustrated in Figure 1a, for normal gradient back-propagation in the Saliency Map method, when the activation values in the lower layer are negative, the corresponding back-propagated gradients are masked out. In contrast, the Deconvolution method masks out the gradients when they themselves are negative, while the Guided Back-propagation approach combines these two methods: those gradients are masked out for which at least one of these two values is negative. Gradient * Input [Shrikumar et al., 2017b] was proposed as a technique to improve the sharpness of the attribution maps. The attribution is computed taking the (signed) partial derivatives of the output with respect to the input and multiplying them with the input itself. Integrated Gradients [Sundararajan et al., 2017] is similar to Gradient * Input, with the main diﬀerence being that Integrated Gradients computes the average gradient as the input varies along a linear path from a baseline ˜x to x. The baseline is deﬁned by the user and often chosen to be zero. Please refer to Figure 1b for the mathematical deﬁnition for both methods. Pixel-space gradient visualizations such as the above-mentioned Guided Back-propagation and Deconvolution are high-resolution and highlight ﬁne-grained details in the image, but are not class-discriminative, i.e., the attribution value plots for diﬀerent classes may look similar. In contrast, localization approaches like Class Activation Mapping (CAM) [Zhou et al., 2016] are highly class-discriminative (e.g., the ‘cat’ explanation exclusively highlights the ‘cat’ regions but not ‘dog’ regions in an image containing both a cat and dog). This approach modiﬁes image classiﬁcation CNN architectures by replacing fully-connected layers with convolutional layers and global average pooling, thus achieving class-speciﬁc feature maps. A drawback of CAM is that it requires feature maps to directly precede softmax layers, so it is only applicable to particular kinds of CNN architectures. To solve this shortcoming, Grad-CAM [Selvaraju et al., 2017] was introduced as a generalization to CAM, which uses the gradient information ﬂowing into the last convolutional layer of the CNN to understand the importance of each neuron for a decision of interest. Furthermore, it is combined with existing pixel-space gradient visualizations to create Guided Grad-CAM visualizations that are both high-resolution and class-discriminative. Besides gradients, back-propagation of activation values can also be leveraged as an interpretability approach. Layer-wise Relevance Propagation (LRP) [Bach et al., 2015] is the ﬁrst to adopt this method, where the algorithm starts at the output layer L and assigns the relevance of the target neuron equal to the output of the neuron itself (i.e., the activation value of the neuron) and the relevance of all other neurons to zero, as shown in Eq. 1. Then the recursive backpropagation rule (called the -rule) for the redistribution of a layer’s relevance to the preceding layer is described in Eq. 2, where we deﬁne z= wxto be the weighted activation of a neuron i onto neuron j in the next layer and bthe additive bias of unit j. Once the backpropagation reaches the input layer, the ﬁnal attributions are deﬁned as R(x) = r. As an alternative, DeepLIFT [Shrikumar et al., 2017a] proceeds in a backward fashion similar to LRP but calibrates all relevance scores by subtracting reference values that are determined by running a forward pass through the network using the baseline ¯x as input and recording the activation of each unit. Although LRP and DeepLIFT were invented based on back-propagation of activation values, it has been demonstrated in [Ancona et al., 2018] that they can also be computed by applying the chain rule for gradients and the converted equations are summarized in Figure 1b. (a) Comparison among normal gradient back-(b) Mathematical formulation of four backpropagation, Deconvolution, and Guided Back-propagation based attribution methods. The origpropagation in terms of how they handle back-inal equations of -LRP and DeepLIFT are transpropagation through the rectiﬁed linear (ReLU) non-formed so that they can be calcuated based on gralinearity.dients. Figure 1: Mathematical formulation of diﬀerent back-propagation based interpretability methods. Compared to back-propagation based methods, which compute gradients of outputs with respect to input features, feature perturbation methods explicitly examine the change in model conﬁdence resulting from occluding or ablating certain features. The idea of masking parts of the input and measuring the model’s change in conﬁdence was introduced in a model agnostic context, pre-DL by works such as [ˇStrumbelj et al., 2009] and [Robnik-ˇSikonja and Kononenko, 2008]. Based on some of these works, there have been multiple methods in DL for feature perturbation, attempting to explain the model based on the change in output classiﬁcation conﬁdence upon perturbation of features. These include model agnostic works based on conditional multivariate analysis and deep visualization [Zintgraf et al., 2017] (based on the instance-speciﬁc method known as prediction diﬀerence analysis) and explicit erasure of parts of input representations [Li et al., 2016]; as well as convolution neural network speciﬁc identiﬁcation of image regions for which the model reacts most to perturbation [Zeiler and Fergus, 2014b] and image masking models that are trained to manipulate scores outputted by the predictive model by masking salient parts of the input image [Dabkowski and Gal, 2017]. Similar to image masking models, recent model-agnostic methods use generative models to sample plausible in-ﬁlls (as opposed to full masking) and optimize to ﬁnd image regions that most change the classiﬁer decision after in-ﬁlling [Chang et al., 2019]. In the direction of more theoretically grounded variable importance-based techniques, [Fisher et al., 2019] measure the model prediction diﬀerence upon adding noise to the features. Additionally, various adversarial perturbation techniques have been introduced that add noise to the feature representations, falling in the category of Evasion Attacks [Tabassi et al., 2019]. Evasion Attacks involve ﬁnding small input perturbations that cause large changes in the loss function and lead to mispredictions. These input perturbations are usually found by solving constrained optimization problems. These include gradient-based search algorithms like Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) [Szegedy et al., 2013], Fast Gradient Sign Method (FGSM) [Goodfellow et al., 2015], Jacobian-based Saliency Map Attack (JSMA) [Papernot et al., 2016a] and Projected Gradient Descent (PGD) [Madry et al., 2018] among others. For detailed surveys on adversarial perturbation techniques in computer vision see [Akhtar and Mian, 2018]; for surveys on adversarial attacks in general see [Chakraborty et al., 2018] and [Yuan et al., 2019]. While the goal of these methods is to actively change model conﬁdence for the purpose of attacking the model, they take advantage of the black box nature of DL models and have led to creation of techniques that can be used to deploy more robust and interpretable models. Attention mechanisms have played an important role in model interpretations and the attention weights have been widely adopted as a proxy to explain a given model’s decision making [Xu et al., 2015, Xie et al., 2017, Clark et al., 2019, Voita et al., 2019]. Historically, attention mechanisms have been introduced in the context of sequence to sequence text model alignment as the way to directly incorporate the importance of the context to any given word representation. Each input word in a given context is represented by a weighted sum of the representations of other words. Naturally, the dynamic weights for each word can be interpreted as the contribution (or importance) of the words to a given word representation. While the exact architecture of the attention-utilizing models diﬀers from model to model, all of them make use of the set of computations known as the attention mechanism. The basic building block of attention is a generalized trainable function [Bahdanau et al., 2014, Vaswani et al., 2017]: where Q and K represent the context of a given element, V the unmodiﬁed element contribution to the representation without the context being taken into an account, and the set of weights W, W, Ware the adaptable weights that represent the learned elements’ contributions. The output of the Score function is known as the attention weights and represents the contributions of the other elements of the input to the representation of the given element or sequence as a whole; in the naive interpretation setting, a high post-training attention weight of an input feature or a set of features corresponds to a higher importance of the given feature value in producing a prediction. Note that this methods of producing interpretation is intrinsically linked to the model itself and constitutes a direct interpretation of the outputs of the parts of a given attention-utilizing model (attention scores) as an explanation for the prediction. Since attention scores are often computed over already pooled representations of the elements and sequences, the element scores do not necessarily represent the direct feature contributions [Jain and Wallace, 2019, Brunner et al., 2019, Zhong et al., 2019]. The majority of work on attention based interpretability has been in the general time-series processing ﬁeld due to both the success of the attention-using models and the natural idea of viewing the contribution of the other elements of the sequence to the current state [Sezer et al., 2020, Fawaz et al., 2019, Wang et al., 2019, Ardabili et al., 2019]. Fully-attention based models and attention based interpretations are also popular in natural language processing (NLP) due to the compositional nature of syntax and meaning [Wolf et al., 2020]. Model distillation (also known as Network distillation) is a model compression technique where a simpler model (student) is “taught” by a more complex model (teacher). While the original use of the technique focused on the improved performance or compactness of the student model [Hinton et al., 2015], it is important to note that if the simpler model is naturally interpretable, the transfer results in an “interpretable” model explaining the behavior of the more complex teacher model. A complex model’s behavior may be approximated either locally, by ﬁtting a simpler model around a given example to produce an explanation for a given point [Ribeiro et al., 2016a], or globally, by ﬁtting one simple model directly to the teacher model, using all the training data [Lakkaraju et al., 2017]. Due to the explicit “interpretation” use case of the technique, the student models are in general limited to either generalized linear models [Ribeiro et al., 2016a], decision trees [Craven and Shavlik, 1995, Schmitz et al., 1999, Plumb et al., 2018] or direct rules or set inductions [Sethi et al., 2012, Lakkaraju et al., 2017, Ribeiro et al., 2018, Zilke et al., 2016] The most inﬂuential member of this family of interpretation producing techniques is LIME [Ribeiro et al., 2016a], a general method for generating local explanation for a speciﬁc input case. The local model that serves as an explanation for a given point is obtained by minimizing where G is a class of the interpretable models used to produce an explanation, πdeﬁnes the neighborhood of points near x, L is the measure of the diﬀerence between the original model and the explanation model prediction in that neighborhood, and Ω(g) is a complexity measure of the explanation model. In practice, in the classical LIME use, L is set to be the distance weighted squared loss between the original model and the explanation model prediction computed over a randomly sampled set of data points biased to lie near xby π. The explanation model class G is the class of all linear models and Ω(g) is a regularization term to prevent overﬁtting. The vast majority of local knowledge distillation for interpretability models are the result of modifying Lime in either the neighborhood construction (ALIME [Shankaranarayana and Runje, 2019]), sampling (MPS-LIME [Shi et al., 2020]) and input structure constraining procedure (GraphLime [Huang et al., 2020]) or the nature of the explanation model (SurvLIME [Kovalev et al., 2020], GRAPHLime [Huang et al., 2020]). Another popular trend is producing semi-global explanation models through LIME-like ﬁtting procedures (LIME-SUP [Hu et al., 2018], Klime [Hall et al., 2017], NormLime [Ahern et al., 2019], DLIME [Zafar and Khan, 2019], ILIME [Shawi et al., 2019]). DL models can also be interpreted via Shapley value, a game theory concept inspired by local surrogate models [Lundberg and Lee, 2017]. Shapley value is a concept of fair distribution of gains and losses to several unequal players in a cooperative game [Shapley, 1953]. It is an average value of all marginal contributions to all possible interactions of features (i.e., players in the game) given a particular example. Therefore, the Shapley value can explain how feature values contribute to the model prediction of the given example by comparing against the average prediction for the whole dataset. Nevertheless, the Shapley value approximation is not easy to compute when the learning model becomes complicated. Recently, researchers proposed a uniﬁed framework, SHAP (SHapley Additive exPlanations) values, to approximate the classical Shapley values with conditional expectations for various kinds of machine learning models, which include linear models, tree models [Lundberg et al., 2018a], and even complicated deep neural networks [Lundberg and Lee, 2017]. SHAP has been widely used recently for DL interpretation, yet researchers also admit to concerns about this popular interpretability method. First, the SHAP for neural networks (KernelSHAP) is based on an assumption of model linearity. To mitigate the problem, [Ancona et al., 2019] propose a polynomial-time approximation algorithm of Shapley values, Deep Approximate Shapley Propagation (DASP), to learn a better Shapley value approximation in non-linear models, especially deeper neural networks. DASP is a perturbation-based method using uncertainty propagation in the neural networks. It requires a polynomial number of network evaluations, which is faster than other sampling-based methods, without losing approximation performance. Also, [Sundararajan and Najmi, 2020] show that SHAP, or other methods using Shapley values with conditional expectations, can be sensitive to data sparsity and yield counterintuitive attributions that make an incorrect model interpretation. They propose a technique, Baseline Shapley, to provide a good unique result. Instead of explaining the model using the attributive contribution of input data points, example based methods interpret the model behavior using only the particular training data points that are representative or inﬂuential for the model prediction. For DL models, there are several interpretation methods based on example-level information. For example, the inﬂuence function [Koh and Liang, 2017], example-level feature selection [Chen et al., 2018], contextual decomposition (CD) [Murdoch et al., 2018], and the combination of both prototypes and criticism samples—data points that can’t be represented by prototypes [Kim et al., 2016]. Other popular methods for interpretation, such as LIME [Ribeiro et al., 2016a] (Section 2.4) and SHAP [Lundberg and Lee, 2017] (Section 2.5), also provide example-level model interpretability. The inﬂuence function is an example of example-based interpretability [Koh and Liang, 2017], which can be used in both computer vision [Koh and Liang, 2017], and NLP [Han et al., 2020b]. The goal of the inﬂuence function is to measure the change in the loss function as we add a small perturbation, weight, or remove a inﬂuence instance, which is a representative, inﬂuential training point. Under the smoothness assumptions, the inﬂuence function can be computed using the inverse of the Hessian matrix of the loss function or by using the Hessian-vector products to approximate the result. The inﬂuence function can also be used to generate an adversarial attack. Researchers developed DL-based instance-wise feature selection at the example-level for feature importance measurement [Chen et al., 2018]. Instance-wise feature selection (L2X, Learning to Explain) measures feature importance locally for each speciﬁc example and therefore indicates which features are the key for the model to make its prediction on that instance. L2X is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. To solve an intractable issue of direct estimation of mutual information and discrete feature subset sampling, the authors apply a variational approximation for mutual information, then develop a continuous reparameterization of the sampling distribution. The method has been applied to CNN and hierarchical long short-term memory (LSTM) on diﬀerent datasets and yields a better explanation performance quantitatively and qualitatively. CD is an interpretation method to analyze individual predictions by decomposing the output of LSTMs without any changes to the underlying model [Murdoch et al., 2018]. In NLP, it decomposes an LSTM into a sum of two contributions: those resulting solely from the given phrase and those involving other factors. CD captures the contributions of combinations of words or variables to the ﬁnal prediction of an LSTM. In the study, researchers demonstrate that CD can explain both NLP and general LSTM applications. For example, they model for sentiment analysis by identifying words and phrases of diﬀering sentiment within a given review and extracting positive and negative words from the model. The CD method can be further extended to a more general version, contextual decomposition explanation penalization (CDEP) [Rieger et al., 2020]. CDEP is a method that allows the insertion of domain knowledge into a model to ignore spurious correlations, correct errors and generalize to diﬀerent types of dataset shifts. It is general and can be applied to diﬀerent neural network architectures. For graph neural networks, [Ying et al., 2019] further propose a model-agnostic GnnExplainer to provide interpretability on graph-based tasks, such as node and graph classiﬁcation. By identifying the prediction-relevant edges, GnnExplainer can highlight local subgraph structures and small subsets of important features to the prediction. The method can be used for single and multiple instance explanations in a graph. To tackle the real-world data, which may not have a set of prototypical examples representing the data well, we can also utilize both the prototypical examples and criticism samples that don’t ﬁt the model well [Kim et al., 2016]. The MMD-critic (maximum mean discrepancy-critic) method uses a Bayesian approach to select the prototype and criticism samples and to provide explanations that can facilitate human reasoning and understanding of the model. The basis of generative based methods for explaining a model’s behavior uses information that does not occur explicitly in attributes of the input, but is derived from external knowledge sources, from a causal model, or from explainable probabilistic modeling. For example, the state-of-the-art general domain neural question answering (QA) system attempts to provide human-understandable explanations for better commonsense reasoning, yet to interpret how the model utilizes common sense knowledge, a common-sense explanation generation framework is required [Rajani et al., 2019]. Researchers collect human narrative explanations for common sense reasoning and pretrain language models [Rajani et al., 2019], which can generate explanations and be used concurrently with the QA system (Commonsense Auto-Generated Explanations (CAGE) framework). They further transfer knowledge (generated explanations) to out-of-domain tasks and demonstrate the capacity of pretrained language models for common sense reasoning. Generative Explanation Framework (GEF) is another hybrid generative-discriminative method that explicitly captures the information inferred from raw texts, generates abstractive, ﬁnegrained explanations (attributes), and simultaneously conducts classiﬁcation tasks. It can interpret the predicted classiﬁcation results and improve the overall performance at the same time [Liu et al., 2019]. More speciﬁcally, the authors introduce the explainable factor (EF) and the minimum risk training (MRT) approach that learn to generate more reasonable explanations. They pretrain a classiﬁer using explanations as inputs to classify texts, then adopt the classiﬁer to jointly train a text encoder by computing EF, which is the semantic distance between generated explanations, gold standard explanations, and inputs, and then minimizing MRT loss that considers both the distance between predicted overall labels and ground truth labels, as well as the semantic distance represented in EF. GEF is a model-agnostic method that can be used in diﬀerent neural network architectures. [Madumal et al., 2020] introduced action inﬂuence models that utilize the structural causal model to generate the explanation of the behavior of model-free reinforcement learning agents through knowing the cause-eﬀect relationships using counterfactual analysis. The proposed model has been evaluated on deep reinforcement learning algorithms, such as Deep Q Network (DQN) [Mnih et al., 2013], Double DQN (DDQN) [Van Hasselt et al., 2016], Proximal Policy Optimization (PPO) [Schulman et al., 2017], and Advantage Actor Critic (A2C) [Mnih et al., 2016]. [Wisdom et al., 2016] developed a model-based interpretation method, sequential iterative softthresholding algorithm (SISTA), to construct recurrent neural network (RNN) without black-box components like LSTMs, via the trained weights of the explicit probabilistic model. In the last section, we have summarized the methodology for each class of interpretation methods. Most of these methods were initially proposed for general domain applications. In order to deploy them to healthcare problems, some customization needs to be performed. Therefore in this section, we discuss how each class of interpretation methods can be adapted to healthcare systems. We also discuss what kinds of clinical/medical observations and ﬁndings we can make with the help of these interpretation methods. Back-propagation based interpretability methods have been widely used to help visualize and analyze those DL models adopted for healthcare problems, which include computer vision, NLP [Gehrmann et al., 2018], time series analysis, and static features-based predictive modeling. We would like to summarize these successful applications and categorize them based on the applied task types. In computer vision tasks, many powerful DL models have achieved close to expert doctor performance [Esteva et al., 2017, Ran et al., 2020] and thus it is very meaningful to study how these models can accomplish such great success [Singh et al., 2020b]. [Xie et al., 2019] adopted CAM [Zhou et al., 2016] to generate heatmaps separately for melanoma and nevus cells in skin cancer histology images so that the morphological diﬀerence between these two types of cells can be visualized: the melanoma cells are of irregular shape and the nevus cells are distinctly shaped and regularly distributed. [Zhang et al., 2021] used Grad-CAM to provide an explainable heatmap for an attention network built for classifying chest CT images for COVID-19 diagnosis, while Grad-CAM was also used to explain a graph convolutional network for secondary pulmonary tuberculosis diagnosis based on chest CT images [Wang et al., 2021]. Integrated Gradients [Sundararajan et al., 2017] was used to visualize the features of a CNN model used for classifying estrogen receptor status from breast magnetic resonance imaging (MRI) images [Pereira et al., 2018], where the model was found to have learned relevant features in both spatial and dynamic domains with diﬀerent contributions from both. Overall, back-propagation based methods have been used to visualize and interpret various medical imaging modalities such as brain MRI [Eitel et al., 2019], retinal imaging [Sayres et al., 2019, Singh et al., 2020a], breast imaging [Papanastasopoulos et al., 2020, Kim et al., 2018], skin imaging [Young et al., 2019], computed tomography (CT) scans [Couteaux et al., 2019], and chest X-rays [Linda, 2020]. For features-based predictive modeling, back-propagation based interpretability methods can be applied to both static and time-series analysis. For static analysis (e.g., therapy recommendation based on a ﬁxed set of features), fully connected neural networks are typically utilized for modeling and thus are the target to be interpreted. Commonly-used interpretability methods include DeepLIFT [Fiosina et al., 2020], LRP [Li et al., 2018, Zihni et al., 2020], etc. For time-series analysis, besides being able to analyze which features are more important or relevant to the prediction among all features used [Yang et al., 2018], it is noteworthy that we can also analyze what temporal patterns are more inﬂuential to the ﬁnal model decision [Mayampurath et al., 2019, Suresh et al., 2017]. Feature perturbation methods have primarily been discussed in the context of adversarial attacks in the healthcare domain [Finlayson et al., 2019a], mainly as potential future risks due to the ready acceptance of machine learning in diagnosis and insurance claims approval. Nevertheless, the features that are most inﬂuential if altered by an attacker are also the ones to which the model’s responses are most sensitive [Finlayson et al., 2019a]. [Finlayson et al., 2019b] perform adversarial perturbations (a variation on FGSM attack [Goodfellow et al., 2015]) by addition of gradient-based noise to three highly accurate deep learning systems for medical imaging. By attacking models that classify diabetic retinopathy, pneumothorax and melanoma, they show vulnerabilities in three of the most highly visible successes for medical deep learning. In addition, they discuss hypothetical scenarios of how attackers could take advantage of the vulnerabilities the systems demonstrate. More broadly, they comment on industries and scenarios that could be aﬀected by adversarial attacks in the future: insurance fraud and determining pharmaceutical and device approvals. They discuss the challenging tradeoﬀ between forestalling approval until a resilient algorithm is built and the harm that delaying the deployment of a technology impacting healthcare delivery for millions could entail. [Iqtidar Newaz et al., 2020] show vulnerability in smart healthcare systems (SHS) by manipulating device readings to alter patient status. By performing two types of attacks, including Evasion Attacks [Tabassi et al., 2019], they identify ﬂaws in an underlying ML model in a SHS. Employing feature perturbation methods such as FGSM [Goodfellow et al., 2015], randomized gradient-free attacks based on [Carlini and Wagner, 2017], [Croce et al., 2019], and [Croce and Hein, 2018] and zeroth order optimization based attacks [Chen et al., 2017], they are able to alter patient status for ML models based on patient vital signs. [Chen et al., 2020] generate adversarial examples based on perturbation techniques for electrocardiograms. They use techniques from [Carlini and Wagner, 2017] and [Athalye et al., 2018] to misguide arrhythmia classiﬁcation. In a similar application, [Han et al., 2020a] introduce a smooth method for perturbing input features to misclassify arrhythmia. For temporal data in EHR, [Sun et al., 2018] introduce an optimization based attack strategy, similar to [Chen et al., 2017], to perturb EHR input data. [An et al., 2019] introduce a JSMA and attention-based attack by jointly modeling the saliency map and attention mechanism. Finally, in a domain agnostic setting, [Naseer et al., 2019] introduce cross-domain transferability of adversarial perturbations using a generative adversarial network (GAN)-based framework. They show how networks trained on medical imaging datasets can be used to fool ImageNet based classiﬁers. Successful transferability of adversarial perturbations can make it even simpler to fool healthcare models across multiple task domains, and potentially modalities. One such paper examines the eﬀect of universal adversarial perturbations in the medical imaging space [Hirano et al., 2021]. Several methods have been proposed in the general domain to counter these adversarial attacks, namely proactive defense ([Cisse et al., 2017, Gu and Rigazio, 2014, Papernot et al., 2016b], [Shaham et al., 2018]) and reactive defense ([Feinman et al., 2017, Grosse et al., 2017, Lu et al., 2017]). Proactive defense methods increase the robustness of models retroactively, whereas reactive defense models detect the adversarial examples. There are also other methods, such as using collaborative multi-task learning ([Wang et al., 2020]). While it may seem that the possibility of adversarial perturbations works against the recommendation of using deep learning in healthcare settings, there are recent works pushing the boundaries by actively examining the reasons for the susceptibility of healthcare data to attacks ([Ma et al., 2021]). Since feature perturbation techniques have strong policy-level implications in healthcare, it is also imperative to tailor general domain defense methods to the healthcare setting. Attention architectures designed with a special consideration for interpretability are routinely used for EHR-based longitudinal prediction tasks such as heart failure prediction [Choi et al., 2016, Kaji et al., 2019], sepsis [Kaji et al., 2019], intensive care unit (ICU) mortality [Shi et al., 2019], automated diagnosis, and disease progression modeling [Gao et al., 2019, Mullenbach et al., 2018, Ma et al., 2017, Bai et al., 2018, Alaa and van der Schaar, 2019]. The underlying representation of such a model is often produced by an LSTM variant with the attention used to compute the contribution of a given feature or time step element of the sequence to the prediction. The best known model of this kind is RETAIN [Choi et al., 2016], which includes computing the attention weights over both the time-step of the time-series and the individual features of the inputs. Pure attention-based architectures such as the Transformer have revolutionized NLP-based modeling, allowing the use of massive unlabeled medical text for pretraining [Lee et al., 2020, Alsentzer et al., 2019, Beltagy et al., 2019]. Adoption of such models for non-text data is still relatively rare [Li et al., 2020b, Rajan et al., 2017]. A special variant of the attention mechanism that seeks to address interpretability allows the model to output uncertainty on each input feature and use the aggregated uncertainty information for prediction [Heo et al., 2018]. Despite the widespread use of the attention maps as explanations, we would caution against the direct interpretation of attention as an element’s contributions to the prediction in the medical domain. More studies are needed to disentangle self-attention produced representations from the context contribution itself. LIME [Ribeiro et al., 2016a] is one of the most popular techniques used to produce instance-level explanations for black-box model predictions in medical AI. The model-agnostic nature of the technique has led to its use in a diverse set of longitudinal EHR-based prediction tasks such as heart failure prediction [Khedkar et al., 2020], cancer type and severity inferences [Moreira et al., 2020], breast cancer survival prediction [Hendriks et al., 2020], and predicting development of hypertension [Elshawi et al., 2019]. It should be noted that the LIME variants are not widely used, despite the potential clinical usefulness of such interpretation methods. Among the potentially useful variants for ML in medicine are SurvLIME [Kovalev et al., 2020], introduced speciﬁcally for producing Cox proportional hazards explanations for black-box survival models, and DLIME [Zafar and Khan, 2019], a hierarchical clustering neighborhood based semi-global LIME variant for producing more consistent explanations for predictions over similar inputs. The game theory based SHAP algorithm has been widely applied in the medical domain for feature contribution analysis due to its ability to explain not only individual predictions but also global model behavior via the aggregation of Shapley values. SHAP is also model-agnostic, so that it can be applied to various machine learning algorithms [Lundberg and Lee, 2017, Lundberg et al., 2018b]. In the direct usage of SHAP for deep learning in healthcare, [Arcadu et al., 2019] applied SHAP to ﬁnd the crucial regions, which are peripheral ﬁelds, for identifying diabetic retinopathy progression. Also, for the interpretation of medical imaging, [Young et al., 2019] and [Pianpanit et al., 2019] utilized KernelSHAP to generate the saliency maps for interpreting the deep neural networks for melanoma prediction and Parkinson’s disease prediction, respectively. [Levy et al., 2019] also adopted SHAP to interpret the portal region prediction in pathology slide imaging. Beyond medical imaging, [Boshra et al., 2019] used SHAP to investigate the features’ inﬂuence on concussion identiﬁcation given the electroencephalography (EEG) signals. [Ancona et al., 2019] uses the DASP algorithm to approximate the Shapley values and yields the explanation of the deep learning models and applies it to a fully-connected network model for predicting the Parkinson’s disease rating scale (UPDRS), which is a regression task to predict the severity of Parkinson’s disease based on 18 clinical features in a telemonitoring dataset. In [Lundberg et al., 2018b], they also have anesthesiologists consulted to ensure that their model explanations are clinically meaningful. The anesthesiologists were asked to justify the SHAP explanations with the change in model output when a feature is perturbed. [Li et al., 2020a] also shows that it is possible to use SHAP for modeling and visualizing nonlinear relationship between prostate-speciﬁc antigen and Gleason score in prostate cancer that is consistent with the prior knowledge in the medical literature. Such clinical evaluations help the medical community to accept the interpretation method better. Other works mentioned in this section also provide explanations that are aligned with prior knowledge and ground truth given by the dataset via visualization or computing quantitative metrics, yet none of them are justiﬁed by a formal clinical user study. Further study is needed for these methods and applications in healthcare. One major concern of using SHAP in the medical domain is that the Shapley value and SHAP was originally derived from economics tasks, where the cost is additive. However, clinical features are usually heterogeneous, and the Shapley values derived from the model may not be meaningful in the domain [Kovalerchuk et al., 2021]. Further investigation is needed to justify real-world clinical use of SHAP-based interpretations. Example-based model interpretation provides a mental model that allows clinicians to refer to some similar cases, prototypes, or clusters given a new case. Researchers utilize CDEP to ignore spurious confounders in skin cancer diagnosis [Rieger et al., 2020]. The study uses a publicly available image dataset from ISIC (International Skin Imaging Collaboration), which has colorful patches present in approximately 50% of the non-cancerous images but not in the cancerous images. It can be problematic if the learned model uses such spurious patch features as an indicator but not the critical underlying information for skin cancer prediction. The CDEP helps penalize the patches for having zero importance during training and mitigates the issue. Although yielding better model performance with a quasi-explanation with a skin cancer classiﬁcation example, CDEP has not yet been justiﬁed by a formal clinical user study and not yet been accepted by the medical community. It is still at the research rather than the deployment stage. DL interpretability can also be learned based on expert-interpretable features provided during the learning process. To provide visually interpretable evidence for breast cancer diagnostic decisions, [Kim et al., 2018] developed an interpretability framework that includes a breast imaging reporting and data system (BIRADS) guided diagnosis network and a BIRADS critic network. The interpretable 2D BIRADS guide map, which is generated from the visual feature encoder, can help the diagnosis network focus on the critical areas related to the human-understandable BIRADS lexicon via the critic network. The study shows that with the BIRADS guide map, the performance is signiﬁcantly higher than the network without the guide map. This ﬁnding also indicates the critical role and necessity of integrating medical domain knowledge while deploying machine learning models in healthcare. For radiology, [Shen et al., 2019] proposed an interpretable deep hierarchical semantic convolutional neural network (HSCNN) for pulmonary nodule malignancy prediction on CT images. HSCNN generates the binarized low-level expert-interpretable diagnostic semantic features that are commonly used by radiologists, such as sphericity, margin, and calciﬁcation; these are inputs to the high-level classiﬁcation model, along with the latent representations learned from the visual encoder. Both [Kim et al., 2018] and [Shen et al., 2019] demonstrate that the image guide map and label generation process may help clinicians curate the raw image information to high-level diagnostic criteria, yet the method is not yet justiﬁed by formal clinical user studies. Further study is needed for these methods to be accepted by the medical community. The current literature presents several diﬀerent classiﬁcation schemes for interpretation methods in DL [Lipton, 2018, Doshi-Velez and Kim, 2017, Pedreschi et al., 2019]. In addition to the methodology motivated classiﬁcation used in the Interpretability methods section of the paper, we present two diﬀerent questions that every interpretation producing method naturally poses: 1. Model Dependence: Does the explanation model depend on the internal structure of the model it is explaining or can it be used for producing an explanation of any “black-box” model? 2. Explanation Scope: Does the explanation model focus on producing an explanation for a given input-prediction pair or is it attempting to create a uniﬁed global explanation of the model’s behavior? A characterization of the most commonly used methods to produce interpretations in health care with respect to these aspects is presented in Table 1. In general, the vast majority of the methods are explicitly local, producing the explanation for a given decision only, with some attempts at aggregation of the local explanation into patterns [Ramamurthy et al., 2020, Lakkaraju et al., 2019]. The community appears to be deeply split on the issue of model dependence, with the proponents citing the necessity of explanation ﬁdelity [Rudin, 2019], while opponents doubt the inherent ﬁdelity of the directly model-dependent explanations [Jacovi and Goldberg, 2020] and stress the need for ﬂexible model-independent explanation methods [Ribeiro et al., 2016b]. In this section we will discuss two aspects of the methods used to produce interpretations of decision models used in health care: 1. How faithful is the interpretation to the underlying decision making model? 2. How understandable are the interpretations to human expert users? The two aspects are often at odds with each other: A complex model decision might require a rather complex explanation to cover all of the possible aspects of the model’s behaviors on diﬀerent inputs, which might not look easy to understand to humans. 4.2.1 Faithfulness of the interpretation We ﬁrst discuss the direct correspondence between the produced interpretation and the model’s decision making, known in the literature under the terms Fidelity [Jacovi and Goldberg, 2020] or Faithfulness [Rudin, 2019]. A perfectly faithful interpretation accurately represents the decision making of the model being explained. If the interpretation is constrained to agree with the model’s behavior on all possible inputs, then no simpler explanation than the original model is possible. Even model dependent explanation producing methods may not be faithful to the original model because, as a simpliﬁed model, they may not include all parts of the original decision making process [Jain and Wallace, 2019]. When using an explanation producing model for black-box models trained on complex healthcare data, we recommend the user to consider the following issues to gain more insight into the explanation model’s faithfulness. 1. For explanations that, in themselves, are predictive models, look at the prediction agreement between the explanation model and the original: if the concordance is low, then the model is not faithful. 2. While it is hard to estimate the ﬁdelity of an explanation method, consider computing recently proposed ﬁdelity measures over the set of the explanation methods you are planning to use [Yeh et al., 2019]. 3. Consider running “feature occlusion” sanity checks, to check if changing those model elements according to the explanation change the original predictions [Hooker et al., 2018]. 4. Due to the nature of some interpretation producing models, the same model might produce diﬀerent explanations for the same pair of input-outputs over multiple runs. Back-prop.Integrated Gra-dients* Inputs, the baseline needs to be carefully se- Feat perturbationPrediction Dif-ference Analy-sence of feature via marginalization, rather than Attentionthe intermediate representations to the ﬁnal rep- Model distillationand might vary wildly for even for very similar in- Game theorytraining data for interpretation. ExampleInﬂuence func-tionrameters and losses. Only an approximation. No GenerativeTask-speciﬁc method. Table 1: Most popular methods intended for providing interpretation (cited more than 50 times), Scope : Local, Semi-Global, Global. Model Dependence: Dependent, Independent17 [Sundararajan et al., 2017] [Zhou et al., 2016] [Bach et al., 2015] [Zintgraf et al., 2017] [Li et al., 2016] [Chang et al., 2019] [Choi et al., 2016] [Rajan et al., 2017] [Ribeiro et al., 2016a] [Ribeiro et al., 2018] [Lundberg and Lee, 2017] [Koh and Liang, 2017] [Murdoch et al., 2018] [Rajani et al., 2019] 4.2.2 Plausibility of the interpretation as deﬁned by the expert user Traditionally, clinicians tend to embrace expert-curated models, such as the APACHE (Acute Physiology and Chronic Health Evaluation) score for evaluating the patient severity in the ICU [Knaus et al., 1985], due to the consistency between used model features and domain knowledge. In contrast, machine learning approaches for healthcare problems aim to further improve performance by learning a much more complex representations from raw features while sacriﬁcing model transparency. Machine learning interpretability methods may provide humanunderstandable explanations, yet it is crucial that the explanations should be aligned with our knowledge to be trustable, especially for real-world deployment in the healthcare domain. However, current deployments with interpretability methods mainly focus only on helping to debug the model for engineers, but not the real-world use for end users [Bhatt et al., 2020b]. The appropriate interpretability methods should be selected and evaluated both to help model developers (data scientists and machine learning practitioners) understand how their models behave, and to assist clinicians to understand the rationale for model predictions for decision making. For model developers, researchers evaluate their use of interpretability methods with diﬀerent levels of model transparency (generalized additive models (GAMs) and SHAP), from both quantitative (machine-learned interpretability) and qualitative (visualization) perspectives using interviews and surveys [Kaur et al., 2020]. The results, however, show that developers usually over-trust the methods and this may lead to their misuse, especially over-relying on their “thinking fast (system 1)” [Kahneman, 2011] since the good visualization may sway human thought, but may not fully explain the behavior of the system and may be incorrectly interpreted by developers. Moreover, visualization sometimes is not able to be fully understood and interpreted correctly by the model developers. The authors point out that developers usually just focus on superﬁcial values for model debugging instead of using explanations to dig deeper into data or model problems. They also enumerate the common issues faced by developers, which include missing values, data change over time, data duplication, redundant features, ad-hoc categorization, and diﬃculties of debugging the methods based on explanations. The developers are also shown to be biased toward model deployment even after recognizing suspicious aspects of the models. From a clinical perspective, it is necessary and critical to have clinically relevant features that align with medical knowledge and clinical practice [Caruana et al., 2015], while under-performing models may still be acceptable as long as the errors are explainable. In [Tonekaboni et al., 2019b], the authors survey clinicians in the ICU and emergency department to understand the clinicians’ need for explanation, which is mainly to justify their clinical decision-making to patients and colleagues. Depending on the problem scope, diﬀerent levels of interpretability may be considered by clinicians. [Elshawi et al., 2019] conduct a case study of the hypertension risk prediction problem using the random forest algorithm and explore the important factors with diﬀerent model-agnostic interpretability techniques at either global or local-level interpretation. They ﬁnd that diﬀerent interpretability methods in general provide insights from diﬀerent perspectives to assist clinicians to have a better understanding of the model behavior depending on clinical applications. Global methods can generalize over the whole cohort while local methods show the explanation for speciﬁc instances. Thus, applications such as the hypertension risk prediction problem may focus on global risk factors derived from either global interpretability methods, mainly non-DL based techniques such as feature importance and partial dependence plot, or the aggregation of local explainers (e.g. SHAP, LIME) [Elshawi et al., 2019], while disease progression prediction requires integrated interpretations at local, cohort-speciﬁc and global levels [Ahmad et al., 2018]. However, diﬀerent interpretability methods may yield a diﬀerent subset of clinically relevant important features due to their ways to obtain feature importance. For instance, SHAP, coeﬃcient of regression models, and permutation-based feature importance may provide completely diﬀerent interpretations even if they are all at the global level. With some clinical examples, researchers found that the local interpretation methods (LIME and SHAP) of the correctly predicted samples are in general intuitive and follow common patterns, yet for the incorrectly predicted cases (either false positive or false negative cases), these local methods can be less consistent and more diﬃcult to interpret [Elshawi et al., 2019]. Nevertheless, the users may not be aware of the assumption of using the model and how it makes the decision: e.g., the additivity assumption of the SHAP algorithm. Interpretability can be quite subjective, and the computerized techniques for producing interpretations lack the interactivity that is often crucial when one human expert is trying to convince another [Lahav et al., 2018]. Studies also show shortcomings of some interpretability methods while adopting them for realworld clinical settings [Tonekaboni et al., 2019b, Elshawi et al., 2019]. For example, the complex correlation between features in feature importance-based methods, the weak correlation between feature importance and learned attention weights for recurrent neural encoders [Jain and Wallace, 2019], and the trade-oﬀ between performance and explainability for rule-based methods, are all potential problems of using global interpretability methods [Tonekaboni et al., 2019b]. For local interpretability methods, researchers also show that clinicians can easily conclude the explanation at the feature-level using LIME, but the main problem is that the LIME explanation can be quite unstable, where patients with similar patterns may have very diﬀerent interpretations [Elshawi et al., 2019]. Instead, the advantage of the Shapley value interpretation method is that it makes the instance prediction considering all feature values of the instance, and therefore the patients with similar feature values will also have similar interpretations [Elshawi et al., 2019]. But the cons of Shapley value-based methods are that they can be computationally expensive and that they need to access the training data while building model explainers [Lundberg and Lee, 2017, Janzing et al., 2020]. It is not trivial to select appropriate interpretability methods for real-world healthcare applications. Researchers therefore provide a list of metrics, including identity, stability, separability, similarity, time, bias detection and trust, to evaluate diﬀerent interpretability methods when considering real-world deployment [ElShawi et al., 2020]. However, they ﬁnd that there is no consistent winning method for all metrics across various interpretability methods, such as LIME, SHAP and Anchors. Thus, it is essential to make a clear plan and think more about the clinical application and interpretability focus in order to select the reasonable and eﬀective interpretability methods and metrics for real-world use. To further achieve the potential clinical impact of deployed models, we should not only focus on advancing machine learning techniques, but also need to consider human-computer interaction (HCI), which investigates complex systems from the user viewpoint, and propose better designs to bridge the gap between users and machines. End users’ involvement in the design of machine learning tools is also critical to understand the skills and real needs of end users and how they will utilize the model outputs [Ahmad et al., 2018, Feng and Boyd-Graber, 2019]. [Kaur et al., 2020] suggest that it may be beneﬁcial to design interpretability tools that allow back-and-forth communication (human-in-the-loop) to make interpretability a bidirectional exploration, and also to build tools that can activate thinking via “system 2” for deeper reasoning [Kahneman, 2011]. Now we have many diﬀerent kinds of interpretation methods to choose when we want to analyze a neural model, although they are still in need of further improvement. At the current state of the art, which method we should choose still does not have a deﬁnite answer. The choice of the right interpretation method should depend on the speciﬁc model type we want to interpret; however, such a detailed and comprehensive guideline for all kinds of models to be analyzed is currently not available. Several recent studies started to look into this problem by benchmarking some popularly used interpretation methods applied to some neural models such as CNN, RNN, and transformer. For example, [Arras et al., 2019] ﬁrst use four interpretation methods, namely LRP, Gradient*Input, occlusion-based explanation [Li et al., 2016], and CD [Murdoch et al., 2018], to obtain the relevance scores of each word in the text for the LSTM model for text classiﬁcation tasks, and then measure the change of accuracy after removing two or three words in decreasing order of their relevance. By comparing the percentage of accuracy decrement, they observe that LRP and CD perform on-par with the occlusion-based relevance, with near 100% accuracy change, followed by Gradient*Input which leads to only 66% accuracy change. This experiment indicates that LRP, CD, and occlusion-based methods can better identify the most relevant words than Gradient*Input. As a counterpart, [Ismail et al., 2020] argue that one should not compare interpretation methods solely on the loss of accuracy after masking since the removal of two or three features may not be suﬃcient for the model to behave incorrectly. Instead, they choose to measure the precision and recall of features identiﬁed as salient by comparing against ground truth important features and report the weighted precision and recall as the benchmarking metric. However, their annotations of which features are important are synthesized rather than collected by human annotation, which is not that convincing. In a more theoretical way, [Bhatt et al., 2020a] propose several equations as quantitative evaluation criteria to measure and compare the sensitivity, faithfulness, and complexity of feature-based explanation methods. Through these benchmarking evaluations, we ﬁnd that diﬀerent interpretation methods may vary a lot in their advantages and disadvantages. To make use of this fact, some studies propose to aggregate two kinds of interpretation methods so that they can complement each other [Ismail et al., 2020]. For instance, [Bhatt et al., 2020a] develop an aggregation scheme for learning combinations of various explanation functions, and devise schemes to learn explanations with lower complexity and lower sensitivity. We hope to see more eﬀorts along this direction to generalize such an aggregation scheme to a broader range of interpretation methods. In this review, we provided a broad overview of interpretation methods for interpreting the blackbox DL models deployed for healthcare problems. We started by summarizing the methodologies of seven classes of interpretation methods in Section 2. Then we proceeded to discuss how these methods, which were initially proposed for general domain applications, are adapted for solving healthcare problem in Section 3. Finally in Section 4, we continued discussing three important aspects in the process of applying these interpretation methods to medical/clinical problems: 1. Are these interpretation methods model agnostic? 2. How good are their credibility and trustworthiness? 3. How to compare the performance of the methods so as to choose the most appropriate one for use? We hope these summaries and discussions can throw some light onto the ﬁeld of explainable DL in healthcare and help healthcare researchers and clinical practitioners build both high-performing and explainable models. The authors’ work was supported in part by collaborative research agreements with IBM, Wistron, and Bayer Pharmaceuticals, and by NIH grant 1R01LM013337 from the National Library of Medicine. The authors declare no conﬂicts of interest.