In Machine Learning (ML), testing often refer s to the evaluation of a trained model on an unseen held-out test dataset, often exp ected to follow the same distribution as the training data. While such an evaluation shows the researcher (and the c onsumer of the research) how well the training algorithm captu res phenomena of interest from the training data, and ge neralizes to unseen data from the same distribution, it does not provide any guarantees around a model’s behavior when it is deployed in practice [5]. In other words, such testing is meant to empirically evaluate the efﬁcacy of certain sources of data, training algorithms, feature(s), or representation a l choices, rather than to estimate the utility and harms of the system when deployed on real world data [5, 32]. Prior work ad dresses this shortcoming in testing by evaluating the performance of models across domains [15, 33], introduc ing do main adaptation techniques [8], measuring performance under distribution shift [27], o r co llec ting datasets that reﬂect target domain distributions or characteristics [7, 16, 14]. However, these efforts tie testing an ML model to testing against a dataset with a distribution that matches a target domain [3]. We argu e tha t such distribution-driven testing implicitly makes assumptions abo ut the costs of failures which often u nderestimate the importance and severity of failure s in th e tail of th e distribution. However, it is often the “black swan” events in the tail of the distribution that lea d to nonlinearities in behavior that result in unsafe outcome s (e.g., the meltdown security vulnerability in ha rdware [20, 17]). Even an ML system with 99% accuracy may have severe vulnerabilities that are masked within the 1% error rate. Furthermore, traditional ML 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia. nrostamzadeh@google.combenhutch@google.com Christina GreerVinodkumar Prabhakaran Google ResearchGoogle Research ckuhn@google.comvinodkpg@google.com Testing practice s within the m a chine learning (ML) community have centered around assessing a learned model’s predictive performa nce measured against a test dataset, ofte n drawn from the same distribution as the train ing dataset. While recent work on robustness and fairness testing within the ML community has pointed to the importance of testing against distributional shifts, these efforts also focus on estimating the likelihood of the model making an error against a reference dataset/distribution. We argue that this view of testing actively discourages researchers and d evelopers from looking into other sources of robustness failures, for instance corner cases which may have severe unde sirable impacts. We draw parallels with decades of work within software eng ineering testing fo cused on assessing a software system against various stress conditions, including corner cases, as opposed to solely foc using on average-case behaviour. Finally, we put forth a set of recommend ations to broaden the view of machine learning testing to a rigorous practice. testing does not accoun t for the different severities of harms different errors cause in deployment, or whether they affect different subgroups of people at different rates [2]. In this paper, we provide a broad acc ount of the various such methodological shortcomings in data distribution driven m a chine learning testing . We then draw on testing practices from other m ature engineer ing disciplines and provide a set of co mprehensive recommendations towards rigorous ML testing, along the condition s to test for, the processes that guide this testing process, and the a rtefacts that can be a step towards ML system reliability. Historically, testing of AI systems was not exclusively statistical in nature [18]. For instance, [12] lays out how one might construct test sets for evaluating speciﬁc aspects of a Natural Language Processing (NLP) system by using both a randomly sampled dataset to evaluate the conceptual competence of the system, and a da ta set o f instance s containing different linguistic variations of the sam e sentence conveying the same meaning, as a way to evaluate the linguistic competence of the NLP system. The switch from such a broad view of what evaluation means, to a singular focus on statistical evaluations based on held-out test sets is arguably a side e ffect of the overwhelming success of statistical NLP in the 90s. From a system reliability perspective, the goal of model testing should ideally include considerations such a s a ) robustness to distribution shifts, b) distributions of errors, and c) severities of th ose errors. Th e se factors are often overlooked, o r relegated to optional, auxiliary te sting efforts in much of ML r esearch and development. In this section, we outline six core shortcomings that results from the common practices within machine learning testing. These six shortcomings are not meant to b e mutually exclusive c ategories; rather, each building on the ones discussed prior to them. Treating all examples as equal: The dominant parad igm of measuring model accuracy on an unseen i.i.d. dataset drawn from the same distribution as the training data mitigates against the pitfalls of over-ﬁtting during learning. However it also makes two c ritical assumptions which reduce the diagnostic power of the testing. Firstly, in weighting all data points in the test set equally, the measurement is most sensitive to failures in model pe rformance on the head of the distribution— and conversely least sensitive to failures in m odel performance on the tail. For example, if an image understanding model is evaluated on a da ta set in which huma ns appear in only a small fraction of images then those images, an d the error s the mod el makes on images with humans, will be given little we ight in the evaluation. Sec ondly, the comparison of predictions (i.e., model outputs) with expectations ignor es a ny available context. Notably, it igno res features of the model inputs. For exam ple, the evaluation of an image analysis model for detecting stop signs would ignore the important factor of whether there are also pedestrians or cyclists in th e image. Treating all failures as equal: Suppose a compu te r v isio n system misidentiﬁes a person as an animal. This is a signiﬁcantly different type of error than, say, misidentifying a cat for a dog. The dominant ML testing paradigm fails to account for the fact th at not all errors are qualitatively equal, and thus ignores dignitary and other ha rms. This simpliﬁcation reduces the labour and other efﬁciency costs involved in doing contextualized evaluations of classes of erro rs. We similarly see these econom ic imperatives at play in the focus of much of the work on (un)fairne ss in NLP being focused on gender b ia s speciﬁcally—compared to other axes of biases—because gender-labeled datasets are relatively e asier to assemble [2]. Sim ilarly, most of the testing efforts outlined above treat models in isolation, co nsidering just the inputs and outputs of the model, whereas, it is important to consid e r how the model integrates into the environment wher e it will b e introduced, ac c ounting for delayed impacts and feedback loo ps [21]. Overlooking corner cases: As outlined above, the trad itional ML testing often f ocuses on averagecase behaviour of the model. Each data point in a test set can be viewed as an individual test, an d whether or not the data point, or test, passes or fails makes up part of the ac curacy, which is then reported as a metric for system performance. However, this approach often does not look into which data points are failing the test, or whether there is a pattern there. Often times the errors correspond to the edge cases that are a minority, while the performance metric focuses on the head of the distribution. Recent work pointing to fairness and robustness failures has prompted work on teasing apart the evaluation set, and associating a subset o f that set with a behavior one wants to test for. For instance, disag gregated fairness evaluation [23] is essentially about perf orming the test on subsets of the data tha t have a certain feature held constant. Similarly, robustness testing often involves examining the tail distribution f or certain edge cases–things like natura l adversarial examples [ 25, 15], or typos f or NLP [29]. In other words, deﬁning metrics for fairness and robustness can in essence be thought of as adding speciﬁc attention to sections of the tail distribution. Lack of an integrated process: It is important to note that rec e nt research ha s begun inquiries into ML testing for the edge cases. For instance, fairness testing or disaggregated testing [2, 23] separately assesses model perform a nce against data pertaining to different subgroups of people; counterfactual testing [19] such as pertu rbation sensitivity analysis [26] assesses model behaviour in response to small c ontrolled changes in the input, similar to unit testing. Approaches such as the fairness g ym [ 6] or system dyna mics based simulations [21] can be thought of as instances of integration testing in simu late d environments. However, these efforts largely continue to be disconnected efforts with those speciﬁc robustness goals, rather than being integrate d in to a uniﬁed or standard process of ML development, even for critical production systems. Lack of artifacts for transparency: One of the consequences of not having integrated processes around comprehensive testing in ML is that there is no standard m echanism to communicate which tests have bee n performed on the ML model, and which ones o f them suc ceeded vs. failed. While frameworks such a s model cards [23] provide a great tra nsparency mechanism, most impleme nted model cards in practice reports largely on tra ditional held-out testing results, and sometimes disaggregated fairness evaluation. Lack of such standardized mechanisms to comm unicate the test results of a comprehensive su ite of test ca ses puts the end users and stakeholders at a disadvantage. Establishing guar antees of an ML mo del’s be haviour in real-world contexts should become a core criteria for its ad option in real-world application scenarios. In this section, we put forth a set of recommendations towards rigorous testing pra c tices that can provide strong, reliable, and transparent guaran tees for ML system reliability. Our recommen dations pertain to practices along three layers: what c ondition s are tested f or, wh a t proce sses guide the testing, and what artifacts communic ate the test results to the model consumers. These recommendations are not meant as a set of concre te steps for practition ers, rather a set of considerations that sho uld shape the steps towards comprehensive testing that is a ppropriate for the application context. We draw inspiration from mature testing practices in different engineering disciplines, especially software engineering, as well as testingoriented engineering parad igms such a s test-driven development. Our recommenda tions are aimed at applications where guarantees on real-world performance and reliability are crucial; however, these recommendations are also relevant in research scenarios in high- stakes domains, such as health care. As outlined in Section 2, current ML testing pr edominantly f ocuses on measuring average case behaviour of models, in isolated test enviro nments, to statistically establish superiority of the data and/or algorithms used to build the models. This averaging is done in at least three ways, e ach of which reduces the informativeness of the test results: i) all data points are weighted eq ually, ii) all contexts are treated as equal, and iii) all error types are weighted equally. We argue that in real-world applications, on e must shift the focus to an engineering-motivated goal of ensuring tha t the model will work as desired in the contexts it will be deployed in. While testing against a target distribution is an effort in this direction, it does not address all the shortcomings. I n software testing, for instance, unit testing is a step used to isolate and examine a sma ll piece of code against potential failure modes (or test cases) tha t are designed by the developer/tester who knows the expected behaviour of that module. What could unit testing look like in the case of ML te sting? Recent research on designing test sets that focus on failur e cases close to natura l data distribution [15], in troducing adversarial examples for robustness [25, 11], and behavioral testing of NLP models [29] can all be considered as instances of unit testing. Similarly, it is impo rtant to test for emergent failure modes when different component ML mo dels are integrated as a part of a larger system, since th e co mpounding of errors may follow patterns undetectable during isolated testing [21]. In addition, failure cases are not all similar and from th e same priority. For example misclassifying a can on the street can be an edge case of a self-driving car, but its severity is not as great as misclassifying a pedestrian and trafﬁc sign in the same scenario. These two examples are not of the same severity and shouldn’t be tre a te d as such. Recommendations: • Consider going beyond the average case behavior of the system , and design test cases that • Consider the context in which th e system will be deployed and assign the severity of failing • Consider test cases that account for the socie tal disparities across different subgroups and • Make sure tha t test cases are also desig ned for scenarios w here the ML mo del is integrated In the traditional software develop ment process, testing takes up signiﬁcant amount of time, resources, and effort [13]. Even moderate-sized software projects takes up hundreds of person-hours dedicated to writing test c ases — conditions to test for and pass/fail c riteria, implementing them, and meticulously documenting the results of those tests. In fact, software testing is co nsidered an art [24] requirin g its own technical and non-technica l skills [31, 22], and entire career paths a re built around testing [4]. Test-driven development, often associated with agile software engineering frameworks, is a practice which integrates testing considerations in all parts of th e development process [1, 10]. These processes rely on a deep understanding of the software requirements as well as user behavior modeling, in order to anticipate failure m odes during deployment, as well as continued expansion of the test suite. In contrast, ML testing is often relegated to a small portion of the ML d evelopment process, and predominantly focuses on a static snapshot of data to provide perform ance guarantees. Despite the growing research into fairness an d robustness failures in ML models, these efforts are often r e legated to auxiliary steps that are n ot fully integrated into a typical ML developm ent process. Furthermore, identifying failure mode s in a deployed ML system is not always straightforward; it require a deep unde rstanding of the societal ecosystems surrounding ML interventions [30]. Recommendations: • Consider anticipating, planning for, and integrating testing in all stages o f development • Consider building a practice around documenting desirable be haviours of the ML system • Consider participatory approaches (e.g., [21]) to en sure that the test suite accounts for the Recent work has pointe d to the importanc e of standard frameworks f or tr ansparency [9, 23] and accountability [28] in ML based interventions. While transp a rency a rtifacts such as datasheets for datasets [9] and model cards [23] are frameworks derived from pra c tices in other matu re d omains such as hardware documentation practices or f ood nutrition labeling, these frameworks c urrently do not provide a way to communicate comprehensive test results. As [13] po ints out, software testing produces a number of artifacts including execution traces, test results, as well as test coverage information. In addition to increased tra nsparency, such infor mation serves also as a way to iteratively build new test cases for newer versions of th e software. ML transpa rency mechanisms should ideally be expanded to include such comprehensive test artifacts. Recommendations: • Consider expanding ML transparency mechanisms such as model cards [23] to include a cover corner cases of inte rest speciﬁc to the task. the tests, in order to prioritize testing and subsequent ﬁxes. how failures might impact different subgroups in different rates. with other ML and non-ML co mponents. process, research problem ideation, the setting of objectives, and system implementation. as test cases, and how to bring diverse perspectives into designing this test suite. societal contexts and the embedded values within which the ML system will be deployed. more comprehensive set of test results. • Consider documenting the processes that wen t into building the set of test cases, so that the • Consider documenting internal/external auditing practices (e.g., [28]) for both for model In this paper, we presented an overview of m ethodological shortcom ings in traditional ML mod el testing. While we recognize the imp ortance of testing against distribution sh ift, we a rgue in favor of going beyond the sole focus on distribution driven testing, and ta ke into consideration edge cases and severity of failure s wh ile assessing an ML model’s perf ormance. We draw inspir a tion from decades of work with in software enginee ring testing practices, foc used on assessing a software system against various stress conditions, measuring severity of failure cases and assessing test priorities. Based on this, we recommend a comprehensive set of consideration s around th e conditions to test for, the process that guide the testing, and the artifacts produced as the result of the testing process. consumer of the ML system has a better understanding of its reliability. outputs and processes.