hanxiong.chen@rutgers.eduyunqi.li@rutgers.edu shuchang.liu@rutgers.eduhz375@cs.rutgers.edu Graphs can represent relational information among entities and graph structures are widely used in many intelligent tasks such as search, recommendation, and question answering. However, most of the graph-structured data in practice suers from incompleteness, and thus link prediction becomes an important research problem. Though many models are proposed for link prediction, the following two problems are still less explored: (1) Most methods model each link independently without making use of the rich information from relevant links, and (2) existing models are mostly designed based on associative learning and do not take reasoning into consideration. With these concerns, in this paper, we propose Graph Collaborative Reasoning (GCR), which can use the neighbor link information for relational reasoning on graphs from logical reasoning perspectives. We provide a simple approach to translate a graph structure into logical expressions, so that the link prediction task can be converted into a neural logic reasoning problem. We apply logical constrained neural modules to build the network architecture according to the logical expression and use back propagation to eciently learn the model parameters, which bridges dierentiable learning and symbolic reasoning in a unied architecture. To show the eectiveness of our work, we conduct experiments on graphrelated tasks such as link prediction and recommendation based on commonly used benchmark datasets, and our graph collaborative reasoning approach achieves state-of-the-art performance. • Computing methodologies → Logical and relational learning;Machine learning;Neural networks;• Information systems → Recommender systems. Collaborative Reasoning; Relational Reasoning; Neural-Symbolic Learning and Reasoning; GNNs; Recommendation; Link Prediction ACM Reference Format: Hanxiong Chen, Yunqi Li, Shaoyun Shi, Shuchang Liu, He Zhu, and Yongfeng Zhang. 2022. Graph Collaborative Reasoning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22), February 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3488560.3498410 Graph is able to describe the entities and their relations in many realworld systems and research problems, such as e-commerce useritem interactions, social networks, citation networks and knowledge graphs. Though graphs can encode rich relationships among plenty of entities, they still suer from incompleteness [28,39]. This issue gives rise to the link prediction task, which is to learn representations from the known data and then predict the potential valid connections. Link prediction is essential to many tasks such as knowledge graph reasoning, entity search, recommender systems and question answering. Recent years have witness the success of knowledge graph embedding methods for link prediction [2,5,34,41,42]. The basic idea is to encode the entities and their relations into a low dimensional vector space while the inherent structure information of the graph is preserved. However, one drawback of these embedding-based models is that they usually process each (entity, relation, entity) triplet independently without explicitly considering the information from neighborhood links, though information from neighbourhood nodes is considered. As a result, these methods are not able to capture the rich information from the neighbor connections and hence result in less informative embeddings [1, 20]. Another line of research is graph neural networks (GNNs), which have shown the power in many graph-related problems [11,15,36]. These approaches are able to learn eective entity representations by aggregating its own representation and the representations of surrounding neighbors. The nodes in the graph can exchange information through message passing [7], which alleviates the problem of aforementioned embedding-based methods. Despite that GNNs could capture more information than those shallow embeddingbased models, their key idea for handling link prediction tasks are actually similar—they aim to learn embeddings to capture the similarity patterns among entities, so that link prediction can be conducted by calculating the similarity for a pair of nodes over a specic relation. However, most GNN approaches are designed from a perceptual perspective and they seldom consider the logical relationship among entities and links for relational reasoning. Logical reasoning is an essential and many times a natural way to conduct reasoning on graphs for two reasons. First, many triplets in the graph may be logically related and can be modeled together through logical connections. Take knowledge graph for example, the triplet (x, capitalOf, y) logically implies the relation (x, locatedIn, y). Thus, we can use implication operations in predicate logic to describe this connection between the two triplets as(x, capitalOf, y) → (x, locatedIn, y). The logical relationship among triplets, if accurately captured, would be helpful for predicting unknown links. Second, each triplet can be naturally represented as a predicate in logical reasoning, which makes it easy to model the link prediction task as a reasoning process. For example, we can treat the target triplet (x, locatedIn, y) as a predicate expression locatedIn(x,y). Then, the link prediction task can be formulated as answering whether the logical expressioncapitalOf(x,y) → locatedIn(x,y)is true, given that the predicate capitalOf(x,y) is true. If the logical expression is true, then we can infer that the target predicate should be true. In other words, the target triplet is a valid link. In this paper, we explore an approach that transforms the link prediction task into a logical reasoning process on graphs. Our goal is to model the structure of a graph as simple Horn clauses so that link prediction can be conducted via logical reasoning. Inspired by [3,31], we apply modularized logical neural networks to learn the logical operations. Instead of using explicit hand-crafted logic rules as many previous approaches did, we introduce a method to convert graph structures into Horn clauses as potential rules to be learnt. The logical relations can be captured by the neural networks so that relational reasoning can be conducted on graphs. Technically, we propose a Graph Collaborative Reasoning (GCR) framework for relational reasoning over graphs. Specically, we consider that links (or triplets) are potentially related to each other if they are connected by shared nodes. Based on this, we can infer a link through its neighbor links for relational reasoning. To compute the Horn clauses via deep neural networks, we encode each triplet as a predicate embedding, i.e., each entity in a given triplet is represented as a vector embedding and each relation is modeled as a neural module to encode the triplet. With the encoded predicate embeddings, we can construct the network structure using the neural modules in accordance with the modeled Horn clauses. The key benets of our design compared to previous works are four aspects. First, we can take advantage of GNN strategies to aggregate rich information from neighbor links through message passing to make link predictions. Second, we consider logical reasoning for link prediction, which can make use of the logical relationships between links. Third, we incorporate logical reasoning without manually predened rules, which makes our method easily adaptable to dierent scenarios. Finally, our model can handle uncertainty in logical reasoning. Our contributions can be summarized as follows: •We introduce a new view of the link prediction task from logical reasoning perspectives. In this way, the link prediction task is translated into a true/false evaluation problem of predicate logical expressions. •We propose the Graph Collaborative Reasoning (GCR) model, which conducts relational reasoning by taking advantage of the neighbor link information for message passing. •We show the eectiveness of our approach on various graph relational reasoning tasks on several real-world graph datasets. In the following, we will present related works in Section 2. After that, in Section 3, we formalize the link prediction task in logical language. Section 4 presents the details of our model and Section 5 gives our experimental setup and results. We will conclude this work with outlooks for future work in Section 6. Existing techniques for link prediction can be roughly classied into three categories: translation-based, tensor factorization-based, and neural network-based. The translation-based models [2,13, 17,41,43] translate a head embedding into a tail embedding via a relation. The scoring function is dened as the distance between the translated head embedding and the tail embedding. Tensor factorization-based methods, such as RESCAL [23], ComplEx [34], RotatE [32], DistMult [42] and HolE [22], consider the graph as a 3D adjacency matrix, which represents the head, tail and relation embeddings along each dimension. They apply operations such as linear mapping (RotatE), bilinear mapping (DistMult and ComplEx) or circular correlation operation (HolE) to obtain low-dimensional representations for each entity and relation. The deciency of these methods lie in treating each triplet independently and thus the rich structural information in the graph cannot be adequately used. Neural network-based methods, such as CNN-based [5,21] and GNN-based [30,35] methods, use neural network structures to capture the rich information among the links. CNN-based methods, such as ConvE [5], use 2D convolution layers to extract the relationships between head entity embeddings and relation embeddings. The relations are represented as multiple feature maps, which are obtained through various lters. Then all these feature maps are concatenated and fed into a fully connected layer to get the projected embeddings for similarity calculation with the tail entity embeddings. These models still consider each triplet independently which also suer from the aforementioned problem. GNN-based models, such as GCN [15], GAT [36] and GraphSAGE [11], can help to resolve this issue by using message passing strategy to aggregate information from neighbor nodes so as to enrich the vector representation of each entity. Since the original design of these models are based on homogeneous graphs, they are unable to handle multirelational link prediction tasks. Later, an extension of GCN named R-GCN [30] is proposed to deal with multi-relational data. However, none of the above methods consider the logical relationships between nodes/links in the graph for relational reasoning. Recently, there have been some research works on integrating logic into link prediction. The related approaches can be broadly classied into hard-logic-based and soft-logic-based methods. The hard-logic-based methods focus on applying hard logic rules to the learning process [4,9,27,37,38]. The problem of using hard logic rules is that the model does not tolerate to any violation. As a result, the logic rules need to be carefully designed and the application scenarios can be limited. For example, the hard-rule-based methods are able to handle rules like “𝑥is the capital of𝑦implies𝑥is located in𝑦,” however, they can hardly deal with rules like “user purchased a cellphone𝑥implies that user probably will purchase a phone case 𝑦,” since the rule can be violated in some cases. To solve the problem, soft-logic-based methods try to handle this uncertainty by using soft logic constraints, which assign probabilities to the logic rules to make the model more tolerate to exceptions [8,10,12,24,25,44,45]. One powerful model is pLogicNet [24], which is based on Markov Logic Network. It can learn the weight for each predened logic rule to handle uncertainty and noise. However, these models usually need to ground the logic rules by traversing all potential valid links in a graph, which makes these methods dicult to scale to large graphs. Though recent works try to get rid of the grounding process by directly adding rule-based constraints on the relation vector representations [6,8,19], they can only deal with simple rules such as (𝑥, ℎ𝑦𝑝𝑒𝑟𝑛𝑦𝑚,𝑦) → (𝑦, ℎ𝑦𝑝𝑜𝑛𝑦𝑚, 𝑥). All of the aforementioned logical rule-based methods need explicitly predened logic rules either as part of a pipelined framework or as a constraint of the learning process. This makes the model highly dependent on the eectiveness of the predened logic rules. An open challenge, as mentioned in [8], is to design models that can handle not only simple (manually) designed rules but also complex learned rules while considering the scalability and uncertainty. Although soft-logic-based methods can be more exible than hard-logic-based approaches, these works all need the background knowledge of the data so that logical rules can be created reasonably, which needs considerable manual eorts. The link prediction task predicts the potential connections among nodes/entities from the known information in a graph. Dierent from previous works which treat each triplet independently, we consider that triplets may have potential relations to each other if they have shared nodes. This information is helpful in many cases. For example, in a social network, the reason that Alice and Bob follow each other is probably because of their common habits. That means the triplet(Alice, follows, Bob)is valid due to(Alice, likes, Pop)and (Bob, likes, Pop), which can be represented as the logical expression likes(Alice, Pop) ∧ likes(Bob, Pop) → follows(Alice, Bob). Based on this, we can take advantage of the neighbor information to help link prediction. To realize this idea, we model the link prediction task in three steps: 1) convert the graph structure into a logic expression; 2) use neural modules to encode triplets as predicate embeddings; 3) apply logical constrained modules to generate ranking scores. The details for step 2) and 3) will be given in Section 4. In this section, we focus on how to convert an graph structure into a logic expression and how to formulate the link prediction task as a true/false evaluation problem of logical expressions. Suppose we have a graphG = (V, R, T ), whereVis the vertex set,Ris the relation set, and the known triplets (edges) in the graph are represented asT. For any𝑣, 𝑣∈ Vand a relation𝑟∈ R, we need to predict if the target triplet𝑇= (𝑣, 𝑟, 𝑣)is valid, where 𝑇∉ T. To solve this problem, we rst get the neighbors of both𝑣 and 𝑣and get all the triplets Tthat contain either 𝑣or 𝑣. T= {(𝑣, 𝑟, 𝑣)|𝑣∈ N} ∪ {(𝑣, 𝑟, 𝑣)|𝑣∈ N} = {𝑟(𝑣, 𝑣)|𝑣∈ N} ∪ {𝑟(𝑣, 𝑣)|𝑣∈ N}(1) whereNandNare the neighbor vertex sets of node𝑣and𝑣, respectively, and the link is considered as a predicate. Since it is possible that not all the triplets inTare the reasons of the target triplet𝑇, we apply the OR operator to model the prediction task. The intuition here is that: the reason that𝑇holds could be any of its neighbour links or any combination of its neighbour links. We translate this idea into the following expression: (𝑇→ 𝑇) ∨ (𝑇→ 𝑇) ∨ · · · ∨ (𝑇→ 𝑇) ∨ (𝑇∧ 𝑇→ 𝑇) ∨ (𝑇∧ 𝑇→ 𝑇) ∨ · · · ∨ (𝑇∧ 𝑇→ 𝑇) ∨ (𝑇∧ 𝑇∧ 𝑇→ 𝑇) ∨ · · · ∨ (𝑇∧ 𝑇∧ 𝑇→ 𝑇) · · · ∨ (𝑇∧ 𝑇∧ · · · ∧ 𝑇→ 𝑇) where𝑇,𝑇· · ·𝑇are triplets inT, and “→” is called the implication operation. This expression contains not only simple Horn clauses, such as(𝑇→ 𝑇), but also higher-order Horn clauses, such as(𝑇∧ 𝑇→ 𝑇)and(𝑇∧ 𝑇∧ · · · ∧ 𝑇→ 𝑇). Based on this denition, we have the following theorem: Theorem 1. Equation (2) is true if and only if 𝑇is true. To show why, we rst have the following lemma: Lemma 2. Let the premise𝑝be true, then the clause𝑝 → 𝑞is true if and only if the conclusion 𝑞 is true. The lemma naturally follows from the denition of the implication operation:𝑝 → 𝑞 ⇔ ¬𝑝 ∨ 𝑞. Now back to Theorem 1, since all of the known triplets in the training data are valid, we know that each𝑇∈ Tis true, and thus any conjunction among𝑇is also true. As a result, if𝑇is true, then Eq.(2)must be true, and if Eq.(2)is true, we know that at least one of the Horn clauses in Eq.(2)must be true, and thus𝑇must be true, meaning that𝑇is a valid triplet. Now the problem of judging if a target triplet𝑇is valid or not becomes answering the question that whether the logic expression in Eq.(2)is true given the known triplets. The intuition here is that𝑇is true as long as at least one of its known neighbour connections or their conjunctions can imply 𝑇. However, one problem is that the size of the expression is huge, which is equal to𝑂 (2)—the size of the power set ofT, making it impractical to implement Eq.(2). Fortunately, we can simplify the expression in Eq.(2)through implication rule and De Morgan’s Law, which translates Eq.(2) into following simplied form: Compare to the𝑂 (2)complexity of Expression(2), the complexity of Expression(3)is only𝑂 (𝑛). We will use Expression(3)for our model implementation. In the next section, we will introduce how to encode triplets into embeddings and then build logic neural networks to generate ranking scores for relational reasoning. Our GCR framework views a graph from the edge perspective and aims to learn the relationship between adjacent edges that are connected by common nodes. Instead, traditional GNN views a graph from the node perspective and aims to learn the relationship between nodes that are connected by common edges. In Figure 1, we use an example to show how a link prediction task on a heterogeneous graph can be viewed from logical perspective. In this example, we hope to predict if node𝑣and𝑣could be Figure 1: An example of link prediction on a heterogeneous graph. From a logical view, 𝑟(𝑣, 𝑣) to be true could result from any order of combinations of the neighbor links, e.g. rst-order 𝑟(𝑣, 𝑣), second-order 𝑟(𝑣, 𝑣) ∧𝑟(𝑣, 𝑣) or even higher-order 𝑟(𝑣, 𝑣) ∧ 𝑟(𝑣, 𝑣) ∧ . . . ∧ 𝑟(𝑣, 𝑣). connected by relation𝑟. Intuitively,𝑟(𝑣, 𝑣)could be true due to: 1) any rst-order implication, e.g.𝑟(𝑣, 𝑣) → 𝑟(𝑣, 𝑣)or 𝑟(𝑣, 𝑣) → 𝑟(𝑣, 𝑣)is true, or 2) any second-order implication, e.g.𝑟(𝑣, 𝑣) ∧ 𝑟(𝑣, 𝑣) → 𝑟(𝑣, 𝑣)is true, or even higher-order implication, e.g.𝑟(𝑣, 𝑣) ∧𝑟(𝑣, 𝑣) ∧ . . . ∧𝑟(𝑣, 𝑣) → 𝑟(𝑣, 𝑣) is true. With Eq.(3), this problem can be simplied as predicting if the following expression consisting of all neighbour links is true: In the following subsections, we will show the details of our graph collaborative reasoning framework. We treat each type of relation in the graph as a predicate, e.g., each of the previously mentioned relations such as𝑐𝑎𝑝𝑖𝑡𝑎𝑙𝑂𝑓,𝑙𝑜𝑐𝑎𝑡𝑒𝑑𝐼𝑛, 𝑓 𝑜𝑙𝑙𝑜𝑤𝑠,𝑙𝑖𝑘𝑒𝑠is a predicate. We learn each node as a vector embedding, same as traditional graph neural networks. Meanwhile, we learn each predicate (relation type) as a small neural module. The predicate serves as a function that converts the two connected nodes into a latent vector in the reasoning space, e.g., to process the link(𝐴𝑙𝑖𝑐𝑒, 𝑙𝑖𝑘𝑒𝑠, 𝑃𝑜𝑝), we write it as the predicate form 𝑙𝑖𝑘𝑒𝑠 (𝐴𝑙𝑖𝑐𝑒, 𝑃𝑜𝑝), then the node embeddings of𝐴𝑙𝑖𝑐𝑒and𝑃𝑜𝑝are fed into the neural module of𝑙𝑖𝑘𝑒𝑠to get the output representation for this link. More specically, the encoding process is given as: e= 𝑃(e, e) = W𝜙 (W(e; e) + b) + b(5) where𝑃(·, ·)is the predicate function for relation𝑟 ∈ R;e, e∈ Rare embeddings for head and tail entities;(·;·)is concatenation operation;𝜙 (·)is ReLU activation function;W, W∈ Rand b, b∈ Rare network parameters and bias terms. Hereeis the predicate embedding of the triplet(𝑣, 𝑟, 𝑣). One thing we need to clarify here is that the order of the head and tail entity embeddings must be correctly sorted during the implementation, because we use concatenation operation to combine the head and tail embeddings, dierent ordering of head and tail concatenation will result in dierent outputs. However, this can be a problem for undirected graphs where the triplet(ℎ, 𝑟, 𝑡)should have the same vector representation as(𝑡, 𝑟, ℎ). In our implementation, we solve this problem by assigning a unique ID to each vertex in the graph and sort their ID in ascending order. This will make sure that the triplet always comes with the smaller ID entity as the head entity while the bigger ID entity as the tail entity. For directed graphs, we will not conduct the sorting operation since the ordering is part of the graph information. Figure 2: The logical network structure of the link prediction task given in Figure 1. The network is assembled using the logical equivalent expression which is converted via De Morgan’s Law. After obtaining all the encoded triplet vectors, we can rewrite the Expression (3) in the predicate embedding form: (¬e∨ ¬e∨ . . . ∨ ¬e∨ ¬e) ∨ e(6) Here erepresents the predicate embedding for the target triplet 𝑇= (𝑣, 𝑟, 𝑣). Since the target triplet is unknown and need to be predicted, we use𝑟instead of𝑟to make the notation concise.eandeare the encoded predicate embeddings for the known neighbour triplets in the graph that contain either𝑣or 𝑣. Our goal is to predict if the above logical expression is true in a continuous reasoning space. We dene a constant vectorT, which is an anchor vector in the reasoning space that represents true. It is randomly initialized and kept unchanged during model training. We expect that the nal vector representation of the entire expression is close to this true vectorTif the target triplet𝑇is valid. Otherwise, the vector representation of the logical expression should be far from T. To achieve this goal, we create neural modulesOR(·, ·)and NOT(·)to represent the logical operations∨and¬, where each module is an MLP with ReLU as activation function. To allow the neural logical modules to perform logical operations as expected, we add logical regularizers to the neural modules to constrain their behavior as dened in [3,31]. The regularizers are not only added to the input predicate embeddings but also to the intermediate hidden vectors as well as the output vector to guarantee that all the embeddings are in the same representation and reasoning space. The logic constraint is represented as L. With these logical modules, we can then assemble a neural network for Expression(6). To make the explanation easy to follow, we use a specic example as shown in Figure 2 to explain the network construction process. This reasoning network structure is corresponding to the heterogeneous graph given in the Figure 1. Suppose we are given two vertices𝑣and𝑣, our goal is to predict if they could have a valid connection through relation𝑟. According to the steps mentioned before, we need to rst nd the neighbors of both𝑣and𝑣, in this example are{𝑣, 𝑣, 𝑣, 𝑣, 𝑣}. Then we feed these vertex pairs into the corresponding predicate encoders to get the predicate embeddings based on Eq.(5). By sending these predicate embeddings into theNOT(·)module, we can calculate the negated embeddings, e.g.¬e. After that, we follow the structure of Eq.(6)to send the target predicate embeddingetogether with the negated embeddings into theOR(·, ·)module to get the nal vector representation of the entire expression in the reasoning space. SinceOR(·, ·)only takes two inputs at one time, we calculate the joint embedding for more than two predicate embeddings in a recurrent manner. That is, we rst send two predicates, e.g. ¬eand¬ein Figure 2, into theORmodule and get the hidden vectore, which represents the result of¬e∨ ¬e. The next predicate embedding in the expression and the previous hidden vectorewill be sent into the same OR neural module. This process is recurrently conducted until we get the nal vector representation of the entire logical expression. However, we need to guarantee that the order information will not aect the nal output since the logical OR operation need to satisfy the associativity and commutativity laws. This is done by randomly shuing the order of the expression terms in each iteration. The following equations describe the process shown in Figure 2: For expressions that have more predicate embeddings in the expression, we can simply add more recurrent steps and do the same operation as mentioned above. The nal outputEis the vector representation of the whole expression in the form of Eq.(6). The next step is to evaluate the distance betweenEand the constant true vectorT. As stated before, this true vector is randomly initialized and will not be updated during the learning process, as a result, it can be treated as an anchor vector in the reasoning space. Here we apply cosine similarity as the measure: This cosine similarity measure is the score function and the output is treated as the ranking score to generate the entity ranking list. We use pair-wise learning algorithm [26] to train our model. Specifically, during the training process, for each known triplet in the training set, we x the head entity and their corresponding relation and sample another entity as the tail. We treat expression created by this fake triplet𝑇as the negative sample. The same operation can be done one more time by holding the tail entity unchanged and replace the head entity. One thing need to mention here is that the neighbors to be sampled for creating the logic expression are never changed even when the head or tail entity is replaced, i.e., the only change in Eq.(3)is to replace𝑇with𝑇. The expression for the valid triplet, known as the positive sample, is evaluated based on Eq.(8)and we have the score𝑠, while the score for negative sample is 𝑠. The loss function is written as: where𝜎 (·)is the logistic sigmoid function𝜎 (𝑥) =;𝛼is an amplication coecient, which is set to 10 in our implementation. We can apply an optimization algorithm to minimizeLso as Table 1: Statistics of the recommendation datasets. Dataset #Users #Items #Interaction Density to maximize the distance between positive and negative samples. By integrating the logical regularizers into the graph collaborative reasoning network loss, we get the nal loss function: where𝜆is the coecient of the logical regularizers;Θrepresents all the trainable parameters of the model, including entity embeddings, predicate encoder parameters and the parameters of the neural logical modules;𝜆is theℓ-norm regularization weight; We use back propagation [29] to optimize the model parameters. The pseudocode for the entire training algorithm, including neighbor sampling, is given in Appendix A. In this section, we evaluate our proposed model on two types of link prediction tasks—graph link prediction and recommendation. The reason why we choose these two tasks for evaluation are based on two considerations: the uncertainty of the target links and the type of the graph structure. Knowledge graph is a type of heterogeneous graph that contains multi-type relations among entities, which makes the link prediction task challenging. It requires the model to predict not only if two entities will be connected but also determine which type of relation connects them. The information in knowledge graphs is usually based on objective facts. That means each link can only be grounded as either true or false—not anything in between—since the links represent facts. Recommendation task usually considers a bipartite graph, which takes user and item as two types of nodes. The model needs to predict if a user and an item can be potentially connected so that we can recommend an item to a target user. The challenge is that the data is human generated which contains uncertainty and noise, so that it is usually not suitable to assign a deterministic truth value for a specic pair of nodes. As we mentioned before, our model can handle the uncertainty for relational reasoning over multi-relational graphs, we choose these two tasks to verify the eectiveness of our graph collaborative reasoning model by answering the following research questions: • RQ1: What is the performance of GCR in terms of graph link prediction and recommendation tasks? Does it outperform state-of-the-art models? (Section 5.4) • RQ2: If and how does the logic regularizer help to improve the performance? (Section 5.5) • RQ3: What is the impact of logical reasoning on few-shot data? (Section 5.6) For graph link prediction task, we use a well-known datasetFB15k237[33], which is a subset of FB15k by removing the inverse relations in the training set to avoid data leakage. It contains 14,541 entities and 237 relations. The training dataset contains 272,115 edges while the validation and testing sets contain 17,535 and 20,466 edges, respectively. In the experiment, we use the same training, validation and testing data splits as described in [33]. For recommendation task, we use a publicly available Amazon e-commerce dataset [18], which includes the user, item and rating information. The user-item interaction matrix can be viewed as a bipartite graph with two types of nodes, i.e. user and item, and a single relation, which is the purchase relation in e-commerce scenario. This is a sparse dataset which makes personalized recommendation challenging. We takeBeautyandClothingsub-categories for our experiments to explore both the link prediction performance and how our model performs in few-shot scenarios. Statistics of the datasets are shown in Table 1. We select several representative models for graph link prediction and recommendation to evaluate the performance of our proposed method. For graph link prediction, we use translation-based, tensor factorization-based, neural network-based as well as logic-based baselines for performance comparison. • TransE[2]: A classical translation-based knowledge graph embedding algorithm. The scoring function for each triplet is given as∥h + r − t∥, whereh, r, tare entity and relation embeddings and ∥ · ∥is the 𝑝-norm of the output vector. • DistMult[42]: This is a tensor factorization-based knowledge graph embedding algorithm, which is a bilinear diagonal model. • ConvE[5]: This approach uses 2D-convolutional operation over embeddings to capture the information from the triplets, which is one of the state-of-the-art models on graph link prediction. • R-GCN[30]: This is a graph neural network based method, which extends Graph Convolutional Network (GCN) [15] to handle multi-relational link prediction tasks. • pLogicNet[24]: The Probabilistic Logic Network, which is a logic-based relational reasoning model. It denes the joint distribution of all possible triplets trough Markov Logic Network (MLN) with logic rules, so that the optimization process can be ecient. • pGAT[12]: This is a state-of-the-art MLN-based relational reasoning model, which combines MLN with graph attention network for link prediction. For recommendation task, we also use theTransE,DistMult andConvEknowledge graph embedding models as baselines since these models can also handle recommendation tasks. Other than that, we also use three recommendation models to explore if the GCR relational reasoning model can outperform those models that are specically designed for recommendation, including: • BPR-MF[26]: This is a pair-wise ranking model for recommendation. We implement the prediction function under the BPR framework by following [16], which considers user, item and global bias terms for matrix factorization. • NCR[3]: This is a state-of-the-art reasoning-based recommendation framework. It utilizes neural logic reasoning to model recommendation tasks. • NGCF[40]: This is an extension of GCN for recommendation task. It allows for multi-hop user-item information aggregation via message passing to enhance the user and item embeddings for recommendation. We use Table 2 to show which baseline model can be used for which link prediction task. For reproducibility, we present the details of the experimental setup for training and evaluating our model and baselines in Appendix B. 5.3.1Link Prediction. In the evaluation step, for each triplet, we rst hold the head entity and replace the tail entity with ones that the head entity is not connected to. Then we do the same operation to hold the tail entity and replace the head entity. We call these generated non-existent triplets as negative samples. For each triplet and its corresponding negative samples, we calculate their evaluation metrics. The nal results are averaged over all the triplets. We follow existing works [2,42] and use the ltered setting for evaluation. We report Mean Reciprocal Rank (MRR) and top-𝐾 Hit rate (Hit@𝐾) evaluation metrics in our results. 5.3.2Recommendation. In recommendation task, for each useritem interaction, we only sample items for each user that the user has never interacted with. Then these negative samples together with the target triplets constitute a user ranking list. Then we calculate the corresponding ranking score for each user and report the nal scores by averaging over all the users. Here we use Normalized Discounted Cumulative Gain (NDCG@𝐾) and Hit rate (Hit@𝐾) metrics in our recommendation evaluation. We report the overall performance for graph link prediction and recommendation tasks in Table 3. For the graph link prediction task, from the results, we see that our GCR model signicantly outperforms all the baselines on MRR and Hit@1. The good performance on MRR and Hit@1 indicates that our model can generate high-quality predictions by ranking the correct target at top positions. Although Hit@3 is not better than pGAT, the performance is still competitive. According to the results, we observe that logic-based methods can consistently outperform the other non-logical models. This indicates the eectiveness of applying logic to graph link prediction tasks. Table 3: Link prediction performance on three datasets with metrics NDCG (N) and Hit Ratio (HR). We use underline (number) to show the best result among the baselines, and use b old font to mark the best result of the whole column. We use star (*) to indicate that the performance is signicantly better than all baselines. The signicance is at 0.05 level based on paired 𝑡-test. The last row shows the relative improvement of our model against the best baseline performance. Figure 3: MRR/NDCG@10 (red squared line) and HR@3/HR@10 (blue circled line) on three datasets according to the increment of the logical regularization coecient 𝜆. For the recommendation task, our model consistently outperforms all the baselines on all the evaluation metrics. From the reported results, we have the following observations: •Knowledge graph embedding models have relatively worse performance than those recommendation models on the recommendation task. One reason is that the KG embedding models treat each triplet independently while recommendation needs to consider users and items from a collaborative learning perspective. This could limit the KG models to gain a good performance on recommendation tasks. Another reason is that the recommendation data presents more uncertainty than KG data since the recommendation data is recorded from user behaviors while the KG data is mostly fact-based, which is a challenge for the KG embedding methods. •Among the recommendation baseline models, NGCF outperforms all other baseline methods. This indicates that it is benecial to incorporate neighborhood information over graphs to make recommendation predictions. •GCR outperforms NCR. This is because NCR only takes user historical interactions to generate logic expressions. However, GCR not only considers the items that the user interacted with, but also considers which other users interacted with these items. By leveraging the rich information from both user- and item-side, GCR can have a better recommendation quality than NCR. •GCR consistently outperforms all the baselines. In particular, GCR improves over the strongest baseline NGCF on both datasets by at least 19.55% on NDCG@5. For Hit@10, our model can achieve even 44.41% improvement on the Clothing dataset. We realize that our model can have higher improvements over baselines when the dataset is more sparse. The Beauty dataset has a density 0.073% while the Clothing dataset is 0.031%. This result is reasonable because NGCF needs to aggregate neighborhood information to enhance user and item embedding representations. A very sparse dataset means that the average interactions over each user is limited so that the model cannot aggregate enough neighbor information to promote the representation quality. However, our GCR, by modeling link prediction from logical reasoning perspective, can help to improve the recommendation performance on sparse dataset. We conducted paired𝑡-test and the𝑝-value < 0.05, which shows that our model has statistical signicant improvements over the strongest baseline. In this section, we answer the question that if the logical regularization helps the learning process. We conduct experiments by tuning the logical regularization coecient𝜆in[0,10,10,10,10] for FB15k-237 and[0,10,10,10,1]for Beauty and Clothing. We show how performance changes w.r.t MRR, Hit Rate and NDCG in Figure 3. We have two major observations from the results: (a) Beauty Hit@5(b) Beauty Hit@10 Figure 4: Performance comparision between GCR and NGCF on Beauty and Clothing datasets. The histograms represent the total number of users in each group, the lines indicate the performance trend with the growing number of per user interactions. •The results show that logical regularization do help to improve the performance when comparing the results of non-logic model (𝜆=0) and logic-regularized models (𝜆≠0). However, how strong the regularization should be added to the neural network need to be carefully adjusted, similar to the observations in [3]. •Sparser data needs a relatively smaller logical regularization coecient. For the Beauty and Clothing datasets, which are bipartite graphs, their densities are 0.073% and 0.031%, respectively. For FB15k-237, which is a multi-relational graph, the density is need to decide if an entity pair will be connected but also need to decide the type of relation between them, which is dierent from the recommendation bipartite graphs. For the most sparse data FB15k-237, the best logic regularization weight is 10, while the best weight for the most dense dataset among the three is 10. The reason for the observation is that there is a trade-o between the prediction loss and the logical loss. The model needs to learn useful information from limited data to generate good predictions. For the sparse FB15k-237 dataset, the model is very sensitive to large logical regularization weights because the logical loss will dominate the total loss when training data is insucient for the prediction loss. However, for Clothing dataset, which is about 50 times denser than FB15k-237, we see that the model is not that sensitive to large logical regularization weights. Even with a higher regularization weight, the model still achieves better performance than non-logic model that 𝜆= 0. The sparsity issue brought by data incompleteness may limit the embedding quality of prediction models. When the data is insufcient, it is dicult for models to capture the relations between entity pairs, and thus inuence the quality of the generated predictions. This issue would especially aect the link prediction models since they usually relies on collective information for model learning. In this section, we explore whether logical reasoning models can help to improve the prediction performance when the data is sparse. With this consideration, we conduct an experiment by evaluating the model performance over dierent data groups that have dierent sparsity. For better visualization of the results, we perform the experiments on the two bipartite graphs. In particular, we split the users in the testing set into dierent groups based on their total number of interactions in the training data. Take the Beauty dataset as an example, users are divided (c) Clothing Hit@5(d) Clothing Hit@10 into four groups, corresponding to the users whose number of We compare our model with the strong baseline NGCF and report the results with respect to Hit@5 and Hit@10 in Figure 4. Since similar trend is also observed on the NDCG metric, we do not plot the NDCG results to keep the gure clarity. From the experiments, we see that our GCR model has signicantly better performance than NGCF on sparse user groups. When the user has more interactions, the performance of NGCF can be better than ours. This observation can be explained by the underlying modeling mechanism of NGCG and GCR. NGCF needs to take the neighborhood information to enrich the node embeddings. For the users with very few interactions, it would be challenging for NGCF to capture the user similarities. Although the GCR model also relies on the neighborhood information, it benets from two special advantages. First, the model can leverage both neighbour node and neighbour link information, and second, the logic component helps to model the logical relationship among the limited neighbourhood entities rather than merely relying on the associative node similarity information for prediction. The good performance on sparse user groups show that our logical reasoning-based model helps to improve the recommendation quality on sparse data. This is an important advantage of our model, since users with fewer interactions are the majority, as shown in Figure 4. In this paper, we propose to model link prediction as a reasoning problem over graphs. Specically, we propose a Graph Collaborative Reasoning (GCR) approach, which takes the neighborhood link information to predict the connections in a latent reasoning space. Experiments on two representative link prediction tasks—graph link prediction and recommendation—show the eectiveness of the model, especially for link prediction on sparse data. We believe enabling the ability of reasoning over graphs is important for future cognitive intelligent systems. This work is just one of our rst steps towards this goal, and there is still much room for future improvements. In this paper, we only used the one-hop neighborhood links, while in the future we will extend to multi-hop reasoning over graphs based on the GCR framework to model hierarchical data structure. Besides the knowledge graph and recommendation tasks considered in this work, graph collaborative reasoning may also help other intelligent tasks such as question answering, molecular graph modeling, entity search and conversational systems, which we will explore in the future.