We recommend using a model-centric, Boolean Satisﬁability (SAT) formalism to obtain useful explanations of trained model behavior, different and complementary to what can be gleaned from LIME and SHAP, popular data-centric explanation tools in Artiﬁcial Intelligence (AI). We compare and contrast these methods, and show that data-centric methods may yield brittle explanations of limited practical utility. The model-centric framework, however, can offer actionable insights into risks of using AI models in practice. For critical applications of AI, split-second decision making is best informed by robust explanations that are invariant to properties of data, the capability offered by model-centric frameworks. Artiﬁcial Intelligence (AI)-driven decision making is increasingly used to support human decisions. In practice, the adoption of intelligent systems hinges on the ability of users to understand why a prediction was generated and how to ensure a desired output. Trust and transparency in AI is essential in high-stakes application domains, including healthcare, counter-terrorism, management of nuclear facilities, etc., where wrong decisions may bear grave consequences. Lack of trust in AI systems is often due to lack of their understanding or interpretability (Reig et al. 2018), especially when safety is at risk (Morales et al. 2019). Common explanatory tools, including Local Interpretable Model-Agnostic Explanations (LIME) (Ribeiro, Singh, and Guestrin 2016) and Shapley Additive Explanations (SHAP) (Lundberg and Lee 2017), are data-centric. They assess contributions of individual attribute values to predictive performance of the models. Insights gleaned from such analyses are primarily of conﬁrmatory value; a clinician can conﬁrm that the model pays attention to similar features that she would consider in analyzing her current patient. However, these tools can be brittle, hide biases, and do not provide useful diagnostic information about safety of the models. We deﬁne brittleness of a model by its inability to adapt or generalize to conditions outside of a narrow set of assumptions. Brittleness of a model could be lethal in safety-critical domains since an anomalous data point could be classiﬁed Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. incorrectly. Further decreasing trust in the models. Alternatively, the explanations given by these models are of feature importance and do not inform the user what would be needed to change the outcome of the model, also dissuading people from using them. We deﬁne feature importance in the rest of the paper as the measure of an individual contribution of the corresponding feature for a the classiﬁcation performance of the model (Saarela and Jauhiainen 2021). Conversely, formal methods can be used to mathematically prove desired reliability properties of the models, eliminating human biases and statistical errors from the process (Gisolﬁ et al. 2021b). We extend these methods to provide minimal-distance counterfactuals that ﬁnd minimal changes to attribute values needed to cause the model to change its prediction. This analysis can expose limitations of models and data used to train them, enabling development of provably robust AIdriven decision support systems. Model interpretability is currently used to bridge the lack of trust in models in machine learning. It can be approached by model: intrinsic or post-hoc, by scope: local or global, or by method: model-speciﬁc or model-agnostic. They have the common goal to present concise and intelligible information to a human user that empowers them to determine whether or not the system is behaving in a safe manner. LIME and SHAP are among the most popular explainable methods. Both are post-hoc, local and model-agnostic. Nevertheless they come with a variety of drawbacks. Some of the disadvantages of these methods are that they require a human to perform a veriﬁcation task to certify if the model behaves as expected, nonetheless, the process is error prone. Moreover, post-hoc explanation techniques might be unstable and unreliable (Lipton 2018). Ghorbani et al. show that some explanation techniques can be highly sensitive to small perturbations in the input even though the underlying classiﬁer’s predictions remain unchanged (Ghorbani, Abid, and Zou 2019). The LIME method interprets individual model predictions based on linear assumptions and locally approximating the model around a given prediction but the correct deﬁnition of the neighborhood is unknown (Ribeiro, Singh, and Guestrin 2016). LIME requires of human input to determine the correct kernel setting based on whether the explanations are coherent; however, this process is susceptible to mistakes (Molnar 2019). Alvarez-Melis and Jaakkola showed that the explanations were very unstable given that for two very close points they varied greatly in a simulated setting and were often inconsistent with each other (Alvarez-Melis and Jaakkola 2018). The SHAP method is derived from Shapley values in game theory, where they describe the contribution of each team player in a collaborative environment. In machine learning, it works by assigning each feature an importance value for a particular prediction (Lundberg and Lee 2017). One of the distinguishing factors of SHAP is that the prediction is fairly distributed among features. However some disadvantages according to Molnar (Molnar 2019) are that users prefer selective explanations since an overwhelming number of features could create confusion among users. Since Shapley values return a value per feature, no conclusion can be made regarding a change in prediction for changes in the input. Similarly to many permutation-based interpretation methods, the Shapley value method can’t classify unrealistic data instances when features are correlated. This works however if features are independent since when a feature is missing it is then marginalized. This is achieved by sampling values from the feature’s marginal distribution. An explanation can be viewed from a constraint satisfaction problem perspective; can a model satisfy all constraints, and if not, why not? Satisﬁability-based formal methods, i.e. SAT, SMT, MILP, provide these types of proofs for abstract mathematical representation of trained models. The explanations are instances of constraint satisfaction problems. Formal methods are on increasing interest in providing explanations for model behaviors (Karimi et al. 2020). Formal methods have also been used to assess the quality of explainable AI methods (Narodytska et al. 2019). Providing explanations for model behavior under ideal conditions is still a signiﬁcant challenge, however, one could argue that there is even higher imperative to provide explanations for model behavior under tough conditions. For instance, providing an explanation that illustrates a similarity between model predictions on two similar samples of data will not sufﬁce when one of the data points in question is anomalous and no sufﬁciently similar neighbors exist. Explanations like the ones provided by LIME and SHAP are too brittle and thus should not be used for safety-critical applications. Data and Models. We use the publicly available Breast Cancer Wisconsin (Diagnostic) dataset (Dua and Graff 2017). It contains 30 numeric features which we standardized by removing the mean and scaling to unit variance. We explain Scikit-learn (Pedregosa et al. 2011) random forests, trained with a 50% train/test split. Our model consists of ten decision trees of the maximum depth of ten. Experiments. We ﬁrst created explanations for the same data point following the methods of LIME, SHAP and SAT, which are described in detail next. LIME (Ribeiro, Singh, and Guestrin 2016) provides local explanations of model Figure 1: Decision Surface of a Decision Tree [left] and Random Forest [right] Figure 2: Illustration of a minimal distance counterfactual. A sample in the magenta box will receive a different label if the sample had attribute values that placed it inside the green box. Our SAT framework can produce these explanations regardless of whether there is supporting data in the region under consideration. predictions global by creating an approximation near the input. Feature importance can be inferred from this local linear approximation by looking at the feature weights. For our experiments, we mirrored the feature importance experiments of the LIME repository. SHAP values (Lundberg and Lee 2017) provide the additive feature importance measure that adheres to local accuracy, missigness, and consistency. φ(f, x) =|z|!(M − |z| − 1)!M![E[f(z)|z] − f(z\i)] Where M is the number of simpliﬁed input features, |z| is the number of non-zero entries in zand z⊆ xrepresents all zvectors where the non-zero entries are a subset of the non-zero entries in x, and S is the set of non-zero indexes in z. The SHAP explanations were generated from code by (Lundberg et al. 2018). Our proposed framework uses the logical encoding strategy from (Gisolﬁ et al. 2021b), and extends those methods to ﬁnd the minimal distance counterfactual explanation as detailed in Algorithm 1. To ﬁnd minimal distance counterfactual explanations, we start with a data points and a local neighborhood in which to search for another data point to which the model assigns a different predicted label. If such a point exists, a satisfying assignment detailing the model state for the two points is returned, otherwise, we increase the size of the local neighborhood and search again. This process continues until a counterfactual is found. The relevant differences between the two points which form the counterfactual can be revealed by ﬁnding all differences within the satisfying assignments. We only report encoded https://github.com/marcotcr/lime Figure 4: LIME Explanation - Feature Importance. Attributes are ranked in descending order of impact on the overall prediction probability yielded by the base model. threshold values which must be crossed in order for the model to change its prediction, which may be interpreted as, ’if we change select attribute values in a query by a small amount, the model will change its prediction’. We studied the stochasticity of feature importance in the LIME (Ribeiro, Singh, and Guestrin 2016) and SHAP (Lundberg and Lee 2017) frameworks. Using those tools, we returned the n most important features where n = 1...30. We iterated over all test data and recorded the n most important features. We then evaluated the probability that each feature was one of the most inﬂuential variables in the test dataset when n was set. We show feature importance attribution plots for LIME and SHAP where three features that were consistently picked as important for both are displayed in the bottom rows of Fig. 6 and the graphs for the rest of the features can be found in the Appendix ﬁgures 8 and 9. The horizontal axis in each of these plots shows the index of the feature in the importance ranking and the vertical axis shows the estimated probability of the feature attaining such rank. Generally, truly important features would have this probability raise quickly as a function of the rank index, and stay high. E.g., in the example shown in Fig. 6, feature mean concave points appears slightly more important than worst concave points and worst concavity. We explored explanations generated by SAT. Firstly, we looked at the 17 points in the test data that were misclassiﬁed. Each explanation had a set of features that needed to be changed to ﬂip the prediction. We iterated over all the explanations and recorded the percentage change over the range of values of each feature that would be required to modify the prediction, and visualized their distributions in kernel density estimation plots. Then we repeated the same procedure with all the correctly classiﬁed points. The top row of Fig. 6 shows these characteristics for the same three features previously mentioned, the rest can be found in the Appendix ﬁgure 7. Model-centric explanation suggests that ﬁxing these 17 errors will require increasing the value of worst concavity by 5-10%. The other two features do not show such a consistent recipe for error correction, even though LIME suggests they are important. The LIME explanations as shown in Figure 4 demonstrate feature attributions [left] where the most inﬂuential is at the top and current values [right] for the breast cancer mass. Feature importance attenuates quickly down the list, and attributes at the bottom have little relevance in the explanation at hand. An interesting observation is that the most relevant features in this particular explanation wind up describing the opposite of the predicted class. The SHAP features can be visualized as forces that inﬂuence the prediction, which starts at the baseline. The baseline value, in this case, -0.21 represents the average of all predictions. The Shapley value is an arrow that pushes to increase, if positive, or decrease if negative as seen in Figure 3. Fig. 5 shows an example model-centric explanation generated by our framework for one query. The current feature value for the suspected breast cancer mass is denoted in bold while the value it would need to be modiﬁed to is shown in italics. The left part of the graph shows the seven attribute changes required by the model to change its prediction, and the right part shows the percentage change of value of each feature needed to cause such change. This type of explana- Figure 5: SAT Explanation - Counterfactual. Our explanation presents a set of the smallest changes necessary to change the model output. The bar graph shows the percent change that is required along each attribute to change the output of the model. Figure 6: Aggregate summary of SAT counterfactual explanations [ﬁrst] in addition to feature attribution for LIME [second] and SHAP [third]. tion brings our attention to features that can compromise robustness of the model: the lower the relative value change needed to affect the output, the narrower the margin for measurement error in the model. This result has multiple potentially useful consequences. One example is conﬁrmatory analysis of a prediction made by an AI-driven tool used by a clinician to help her diagnose a patient. If only a small change of one of the features, reﬂecting, e.g., some laboratory test result, can ﬂip the prediction, the doctor may consider repeating the test to ascertain its outcome, or order a more precise test. Similarly, when designing AI-based decision systems that operate on measurements collected with noisy sensors with known sensor noise models, the design engineer may use the proposed analysis to verify that the expected magnitude of sensor noise does not exceed the range of robustness revealed for the corresponding feature of the AI model. To accomplish the second task type, we can leverage SAT score characteristics as shown in the top row of Fig. 6, focusing on the correctly classiﬁed test data. Features mean concave points and worst concave points are highly important according to LIME and SHAP, but their SAT score distribution characteristics show large probability masses in the range of very small percentage changes needed to invert the model prediction. The physician and the engineer from our examples should be very careful and, if possible, measure values of these features with very high accuracy. On the other hand, features that would need greater changes to modify their predictions such as worst concavity, leaves our engineer with some margin of error, because non-trivial changes of its value are required to impact the model prediction. The engineer can compare this margin with the characteristics of various sensors that can be used to measure the particular property of interest, and choose the least expensive sensor whose error model safely ﬁts within the identiﬁed slack of robustness of the AI model. Concentrated distributions show that the same magnitude change is present among a group of explanations for multiple data points. Some features may cause more fragility in the system than others because their percent change concentrates in the low range of values i.e. as observed in mean concave points, while for worst concavity a large change in the observations would be needed to break robustness thus the latter is more reliable. If the misclassiﬁed points have a percent change smaller than the rest of the points, it would mean that changing the features would most likely result in the improvement of the model. While this trend is prevalent in our dataset, a few counterexamples exist for which changing their values would most likely decrease the performance of the model. Model-centric formal methods provide useful capabilities complementary to the existing explanatory analysis tools. They are based on mathematical logic and yield provable results that can be veriﬁed exactly, as opposite to the prevalent statistical methods that produce results with margins of conﬁdence. We envision beneﬁcial use of these methods at all stages of life of AI systems: from design to ﬁeld application. A limitation of the current work is that the implemented veriﬁcation framework does not take into account the mutability of feature. Some features, such as age, cannot be changed, making the counterfactuals less actionable. This is a limitation of the implementation alone, and previous work has addressed this by making certain features protected (Gisolﬁ et al. 2021a). In future work we hope to further validate these methods by applying them to a variety of datasets. Additionally, these methods should be validated with human studies, similar to those found in (Ribeiro, Singh, and Guestrin 2016).