Abstract Extreme Multilabel Text Classiﬁcation (XMTC) is a text classiﬁcation problem in which, (i) the output space is extremely large, (ii) each data point may have multiple positive labels, and (iii) the data follows a strongly imbalanced distribution. With applications in recommendation systems and automatic tagging of web-scale documents, the research on XMTC has been focused on improving prediction accuracy and dealing with imbalanced data. However, the robustness of deep learning based XMTC models against adversarial examples has been largely underexplored. In this paper, we investigate the behaviour of XMTC models under adversarial attacks. To this end, ﬁrst, we deﬁne adversarial attacks in multilabel text classiﬁcation problems. We categorize attacking multilabel text classiﬁers as (a) positive-targeted, where the target positive label should fall out of top-k predicted labels, and (b) negative-targeted, where the target negative label should be among the top-k predicted labels. Then, by experiments on APLC-XLNet and AttentionXML, we show that XMTC models are highly vulnerable to positive-targeted attacks but more robust to negative-targeted ones. Furthermore, our experiments show that the success rate of positive-targeted adversarial attacks has an imbalanced distribution. More precisely, tail classes are highly vulnerable to adversarial attacks for which an attacker can generate adversarial samples with high similarity to the actual data-points. To overcome this problem, we explore the eﬀect of rebalanced loss functions in XMTC where not only do they increase accuracy on tail classes, but they also improve the robustness of these classes against adversarial attacks. The code for our experiments is available at https://github.com/xmc-aalto/adv-xmtc. 1 Introduction Extreme Multilabel Text Classiﬁcation (XMTC) addresses the problem of tagging text documents with a few labels from a large label space, which has a wide application in recommendation systems and automatic labelling of web-scale documents [Partalas et al., 2015, Figure 1: Label frequency of two XMTC datasets, Wikipedia-31K and AmazonCat-13K. Both datasets have an extremely imbalanced distribution, where the frequencies of a few head labels are high, but there are only a few training samples for a large fraction of labels known as tail classes. Jain et al., 2019, Agrawal et al., 2013]. There are three characteristics which make XMTC diﬀerent from typical text classiﬁcation problems: XMTC is a multilabel problem, the output space is extremely large, and data are highly imbalanced following a power-law distribution [Babbar et al., 2014], which makes models perform poorly on a large fraction of labels with few training samples, known as tail labels (see Figure 1). The research on XMTC has focused on tackling the aforementioned challenges by proposing models which can scale to millions of labels [Babbar and Sch¨olkopf, 2017, Jain et al., 2019, Prabhu et al., 2018, Medini et al., 2019] and mitigating the power-law impact on predicting tail classes by rebalancing the loss functions [Qaraei et al., 2021, Cui et al., 2019]. However, as XMTC algorithms have shifted from shallow models on bag-of-words features to deep learning models on word embeddings [You et al., 2019, Ye et al., 2020, Jiang et al., 2021], two new questions need to be addressed : (i) how can one perform adversarial attacks on XMTC models, and (ii) how robust are these models against the generated adversarial examples? These questions are also the key to understanding the explainability of modern deep learning models. Adversarial attacks are performed by applying engineered noise to a sample, which is imperceptible to humans but can lead deep learning models to misclassify that sample. While the robustness of deep models to adversarial examples for image classiﬁcation problems has been extensively studied [Szegedy et al., 2014, Goodfellow et al., 2015], corresponding methods for generating adversarial examples have also been developed for text classiﬁcation by taking into account the discrete nature of language data [Zhang et al., 2020]. However, the research on adversarial attacks on text classiﬁers is limited to small to medium scale datasets, and the tasks are binary or multiclass problems, making current adversarial frameworks not applicable in XMTC. In this paper, we explore adversarial attacks on XMTC models. To this end, inspired by Song et al. [2018] and Hu et al. [2021], ﬁrst we deﬁne adversarial attacks on multilabel text classiﬁcation problems. We consider two types of targeted attacks (i) where a sample is manipulated to drop a positive label from the top predicted labels, and (ii) make the model predict a negative label as a positive label. Then we show that XMTC models, in particular the attention-based AttentionXML [You et al., 2019] and the transformerbased APLC-XLNet [Ye et al., 2020], are vulnerable to adversarial attacks. Our analysis also shows that the success rate of the adversarial attacks on XMTC models has imbalanced behaviour, similar to the distribution of the data. In particular, our experiments show that positive tail classes are very easy to attack. This means that not only is it diﬃcult to correctly predict a tail label, but also there is a high chance that one can eliminate a correctly classiﬁed tail label from the predicted labels by changing a few words, or even a single word in some cases (Table 1). To improve robustness of tail classes against adversarial attacks, we investigate the rebalanced loss functions originally proposed to enhance model performance on missing/infrequent labels [Qaraei et al., 2021]. Our results show these loss functions can signiﬁcantly increase robustness of correctly (when trained with vanilla loss) classiﬁed samples belonging to tail classes. To summarize, the key ﬁndings of our work are: • XMTC models are vulnerable to adversarial examples, as shown by experiments with AttentionXML and APLC-XLNet. Samplein 1888 ... With the invention of dictationmachines, shorthand machines, and the Truegregg, language, orthography, reference, labelsshorthand, speed, stenography, tools, wiki, Pred.language, writing, wikipedia, typography, (Real)shorthand, history, wiki, linguistics Pred.language, writing, wikipedia, typography, (Adv.)history, wiki, shorthand, english Table 1: An adversarial example generated for APLCXLNet by targeting the tail label “shorthand” of the Wikipedia-31K dataset. While “shorthand” is among top-5 predicted labels for the real sample, it will become the 7th predicted label by only replacing “executives” with “companies”. Notably, the newly predicted label “history” is not one of the true labels. XMTC has an imbalanced behaviour similar to the distribution of data, where it is easy to attack positive tail labels by only changing a few words in the corresponding samples. • The rebalanced loss functions can signiﬁcantly improve the robustness of tail labels against adversarial attacks. 2 Related work 2.1 Adversarial attacks on text classiﬁers Adversarial attacks on image classiﬁcation cannot be directly applied to text classiﬁcation problems because of the discrete structure of text data. In text classiﬁcation problems, this is achieved by ﬁrst ﬁnding “important” parts of the text and then manipulating these parts. In white-box attacks, the important parts are determined by the gradient information, and in black-box attacks this is done by masking some parts of the text and then computing the diﬀerence between the output probabilities of the masked and unmasked sample. A perturbation that is undetectable for humans should result in a sample that is semantically similar to the original and preserve the ﬂuency of the text. Adversarial attacks in text classiﬁcation problems can be categorized into character-level and word-level attacks. In Sun et al. [2020] and Li et al. [2018], which are two character-level attacks, ﬁrst, important words are determined by the gradient magnitude and then those words are misspelled to change the label. The same idea is used in Gao et al. [2018] but with a black-box setting for ﬁnding the important words. The problem with the character-level methods is that a spell checker can easily reveal the adversarial samples. In word-level attacks, Jin et al. [2020] and Ren et al. [2019] ﬁnd the important words in a black-box setting and then replace those words with synonyms to change the predicted label. However, the substituted words may damage the ﬂuency of the sentences. To preserve the ﬂuency of the sentences, a language model such as BERT [Devlin et al., 2018] can be used to generate the candidates in a context-aware setting [Garg and Ramakrishnan, 2020, Li et al., 2020, Xu and Veeramachaneni, 2021]. Current works in adversarial attacks on text classiﬁers have focused on binary or multiclass problems without taking into account data irregularities such as imbalanced data distribution for instance. For attacking XMTC models, ﬁrst we extend the adversarial framework for ﬁnding important words to multilabel settings, then we use the BERT model [Li et al., 2020] for contextaware word substitutions which can generate ﬂuent adversarial samples. 2.2 XMTC models Earlier works in XMTC used shallow models on bag-of-words features [Bhatia et al., 2015, Babbar and Sch¨olkopf, 2017, Khandagale et al., 2020]. However, as bag-of-words representation looses contextual information, recent XMTC models employ deep neural networks on word embbedings. Among these models, AttentionXML [You et al., 2019] uses a BiLSTM layer followed by an attention module over pretrained word embbedings and is trained in a tree-structure to reduce the computational complexity. APLC-XLNet [Ye et al., 2020] is a transformer-based approach, which ﬁne tunes XLNet [Yang et al., 2019] on extreme classiﬁcation datasets. To scale XLNet to a large number of labels, APLC-XLNet partitions labels based on their frequencies, and the loss for most of the samples is computed only on a fraction of these partitions. Another major challenge in XMTC is the problem of infrequent and missing labels [Jain et al., 2016, Qaraei et al., 2021]. To improve generalization on infrequent labels, ProXML [Babbar and Sch¨olkopf, 2019] optimizes squared hinge loss with `regularization. To address the missing labels problem, Jain et al. [2016] optimizes a propensity scored variant of normalized discounted cumulative gain (nDCG) in a tree classiﬁer. Qaraei et al. [2021] propose to reweight popular loss functions, such as BCE and squared hinge loss to make them convex surrogates for the unbiased 0-1 loss. The reweighted loss functions are further rebalanced by a function of label frequencies to improve performance on tail classes. Our experiments show that, even though these losses were not designed from a robustness perspective but more from the viewpoint of being statistically unbiased under missing labels, they signiﬁcantly improve the robustness of tail classes against adversarial attacks in deep XMTC models. 2.3 Adversarial attacks on imbalanced or multilabel problems Adversarial attacks on multilabel problems were ﬁrst deﬁned in Song et al. [2018] for multilabel classiﬁcation or ranking. Hu et al. [2021] used a diﬀerent approach for attacking multilabel models by proposing loss functions which are based on the top-k predictions. In Melacci et al. [2020], the domain knowledge on the relationships among diﬀerent classes are used to evade adversarial attacks against multilabel problems. Yang et al. [2020] deﬁned the attackability of mulitlabel classiﬁers, and proved that the spectral norm of a classiﬁer’s parameters and its performance on unperturbed data are two key factors in this regard. Adversarial attacks on models trained on imbalanced data were discussed in Wang et al. [2021] and Wu et al. [2021]. In both works, it has been remarked that for adversarially trained models, the decay of accuracy from head to tail classes on both clean and adversarial examples are more than that of normal training. To overcome this problem, Wu et al. [2021] used a marginbased scale-invariant loss to deal with imbalanced distribution, along with a loss to control the robustness of the model. Wang et al. [2021] showed that rebalancing robust training can increase the accuracy of tail classes but has signiﬁcant adverse eﬀects on head classes. To tackle this problem, they proposed to use reweighted adversarial training along with a loss which makes features more separable. All the aforementioned works are on image classiﬁcation problems. To the best of our knowledge, adversarial attacks on multilabel problems in the text classiﬁcation domain have only been explored in Wu et al. [2017], which did not aim to produce adversarial examples, but to use adversarial training to improve accuracy. 3 Adversarial attacks on multilabel text classiﬁers Attacking multilabel problems is diﬀerent from binary or multiclass problems since the samples in the ﬁrst may have multiple positive labels, and therefore the corresponding manipulated sample can be an adversarial sample for some labels but not for the others. While attacking multilabel image classiﬁcation models has been recently explored in Song et al. [2018] and Hu et al. [2021], to the best of our knowledge, there is no work on attacking multilabel problems in the NLP domain. In this section, we formally deﬁne attacking multilabel text classiﬁers. Assume S = [w, ..., w] is a document consisting of n words w∈ R, y ∈ {0, 1}are the labels corresponding to this document in one-hot encoded format, and Y = {i|y= 1} represents indices of the positive labels. Let g : R→ Rbe a mapping from documents to scores, where g(S) ∈ R indicates the score of the i-th label. Also,ˆY(S) = {T(g(s)), ..., T(g(s))} represents the top-k predicted labels, where T: R→ {1, ..., L} is an operator which returns the index of the i-th largest value. The goal in adversarial attacks is to generate a document Swhich is similar to S but has diﬀerent predicted labels. As in multiclass classiﬁcation, we can also have nontargeted and targeted attacks on multilabel problems, which are deﬁned in the following. Non-targeted attacks: In a non-targeted attack, the goal is to remove at least one correctly predicted positive label from the top-k predictions; A modiﬁed document Sis an adversarial example for S if Sis similar to S and ∃i ∈ˆY(S) such that i ∈ Y and i /∈ˆY(S). Targeted attacks: In a targeted attack on a multilabel problem, an attacker may try to decrease the scores of some particular positive labels, or increase the scores of some negative labels in order to be among the top-k predicted labels. To this end, we use a subset of positive labels denoted by A⊂ {i|y= 1} and a subset of negative labels A⊂ {i|y= 0} to deﬁne which labels are targeted. This leads to two types of attacks: • Positive-targeted attacks: • Negative-targeted attacks: In general, attacking text classiﬁcation models consists of two steps: ﬁrst, ﬁnding important words, and second, manipulating the most important words in order to change the labels. In the subsequent paragraphs, following Jin et al. [2020] for a black-box attack using Bert, we describe how to ﬁnd important words in a sequence based on the target of the attack, and how to perform word substitution. 3.1 Finding important words In a black-box attack, the only information available from the model is the output scores. We compute the importance of each word by masking that word in the document and measuring how much we are closer to our goal based on the target of the attack and the output scores. Formally, assume S = [w, ..., w] is a document, and S= [w, ..., w, [MASK], w, ...] is the document in which the i-th word is masked. In a nontargeted attack, the importance of the i-th word is computed as follows: This equation assigns an importance score to word w by summing the drop in the scores of predicted labels when that word is masked. Similarly, in positive targeted attacks, the important words should be those which decrease the output scores of correctly predicted labels in Amore than other words when they are masked. Hence, the importance of the i-th word is computed as follows: Furthermore, for negative-targeted attacks, the importance of the i-th word is computed as the sum of the diﬀerence of the output scores of the labels in A, if they are not among the predicted labels, after masking the i-th word: 3.2 Word substitution Since word substitution can be the same for multiclass and multilabel problems, we can use the existing methods to replace important words. We use Bert model [Devlin et al., 2018, Li et al., 2020] for this purpose which leads to a context-aware method and produces ﬂuent adversarial samples. To this end, we mask the important words of a sample one by one and pass that sample to a Bert model to generate candidates for the masked words. In each trial t, we pick the word suggested by the Bert model for which the diﬀerence between the output scores towards our goal is maximized. For non-targeted and positive-targeted attacks, this is obtained by: where Sis the sample after changing t−1 important words, and Sis that sample when the t-th important word is replaced by the k-th suggested word from or equal to A∩ˆY(S) in a positive-targeted attack. Moreover, for negative-targeted attacks, wis computed as Equation 3.4, but the sum should be multiplied by a negative sign and Γ = A\ˆY(S). We repeat masking the important words and feeding them to the network until the goal for the attack is reached, or we are out of the limit of the allowed number of changes. A pseudocode for positive-targeted attacks is given in Algorithm 1. 4 Adversarial attacks on XMTC models In this section, ﬁrstly we show that XMTC models are vulnerable to positive-targeted but more robust to negative-targeted attacks. An important observation about positive-targeted attacks is that their success rate has an imbalanced distribution, where one can successfully attack a tail label by changing only a few words in the document, while head classes are more robust to the attacks. Secondly, to increase the robustness of tail classes against adversarial attacks, we replace the normal loss functions with the rebalanced variants [Qaraei et al., 2021] in the targeted models. The results show that these loss functions can signiﬁcantly improve the robustness of tail classes. 4.1 Setup Adversarial attacks are performed on two XMTC models, AttentionXML and APLCXLNet, trained on two extreme classiﬁcation datasets, AmazonCat-13K and Wikipedia-31K [Bhatia et al., 2016]. The statistics of these datasets are shown in Table 2. Similar to other datasets in XMTC, both datasets follow an extremely imbalanced distribution (Figure 1). We only perform positive-targeted or negativetargeted attacks, since these types of attacks are more practical in real-world problems than non-targeted attacks [Song et al., 2018], and give us the opportunity to compare the behaviour of the models under attacking classes with diﬀerent frequencies. We only consider the case where the target set contains a single label. Also, for each target label, we consider the samples in which the target label is among (not among) the true positive labels in positive-targeted Algorithm 1 Positive-targeted attack pseudocode Input: target label set A, sample S = [w, ..., w], score function g(.), label predictor functionˆY(.), a Bert language model, maximum allowed change rate θ Output: Adversarial sample S order} the i-th word is masked (negative-targeted) attacks. We randomly draw the samples in which the target label is classiﬁed correctly. It means that the accuracy of the models on the drawn samples with respect to the target labels is always perfect in both of the attacks. To treat labels with diﬀerent frequencies equally, we partition label frequencies in diﬀerent bins and draw an equal number of samples for each bin for all the experiments. We consider several consecutive frequencies as one bin, if there are at least L labels in that bin for which there is at least one correctly classiﬁed sample for each of the labels, where L = 100 for Wikipedia-31K and is 400 for AmazonCat-13K. To measure how much the adversarial samples are similar to the original samples, we use two criteria: i) cosine similarity of the encoded samples using Universal Sentence Encoder (USE) [Cer et al., 2018] which gives Table 2: The statistics of AmazonCat-13K and Wikipedia-31K Bhatia et al. [2016]. APpL and ALpP denote the average points per label and the average labels per point, respectively. Table 3: The success rate of positive-targeted adversarial attacks against APLC-XLNet and AttentionXML on Wikipedia-31K and AmazonCat-13K. The success rate is more than 80% for all the cases with a high similarity between the adversarial and real samples and small change rate. us a measure in [0, 1], ii) change rate, which is the percentage of the words changed in a real sample to generate an adversarial sample. 4.2.1 Positive-targeted attacks The results of positive-targeted adversarial attacks on APLC-XLNet and AttentionXML for about 1000 samples uniformly drawn from diﬀerent label frequency bins are shown in Table 3. Here the maximum allowed change rate is set to 10%. As the results indicate, the success rate of the positive-targeted attacks against both models is high, which is more than 90% for Wikipedia-31K and more than 84% for AmazonCat-13K. Furthermore, the generated samples are similar to the real samples in terms of USE similarity, and the change rate is less than 2.5% in all the cases. Overall, the experiments show that XMTC models are vulnerable to positive-targeted attacks where an adversary can fool the model not to predict a particular label by a few changes in the document. 4.2.2 Negative-targeted attacks While positivetargeted attacks have high success rates on both models, having a high success rate for negative-targeted attacks is not easy. This is due to the fact that ﬁnding the words which can increase the prediction probability of a particular label from the extremely large vocabulary space and injecting them into a document is much harder than ﬁnding the words inside a document that can lead to a lower probability for a label and replacing SuccessSimilarityChange rate (%)rate (%) them with semantically similar words (positive-targeted attacks). To have higher success rates for negative-targeted attacks, for each target label, we restrict the attack to the samples for which the label is close to those samples but they don’t contain that label as a positive label. In our work, we assume that a label is close to a sample if that sample has at least one positive label which is in the same cluster as the target label. We perform the clustering by the balanced hierarchical binary clustering of Prabhu et al. [2018], where each label is represented by the sum of the TF-IDF representation of the documents for which that label is a positive label. Formally, assume S, ..., Sare our documents in the training set and X = [x, ..., x]∈ Rare the corresponding TF-IDF representations of these documents. Also, Z ∈ {0, 1}consists of the one-hot labels for each document. Then ˆz= z× X is the representation that we use for the l-th label to perform clustering, where zis the l-th row of Z. Some of the clusters for AmazonCat-13K are depicted in Table 4. After the clustering is done, for each target label l ∈ Cwhere Cis the k-th cluster, we consider only the following samples to attack: where Y (S) consists of the indices of positive labels for the document S. For negative-targeted attacks, the number of random samples that we draw for each bin is diﬀerent among diﬀerent datasets and models, and it is equal to the minimum number of samples among the bins which Table 4: Four clusters of the AmazonCat-13K and Wikipedia-31K labels. In our negative-targeted attacks, a sample may be a candidate to attack, if it has at least one positive label of those which are in the same cluster as the target label. Table 5: The success rate of negative-targeted adversarial attacks against APLC-XLNet and AttentionXML on Wikipedia-31K (W) and AmazonCat-13K (AC). While the both models show robustness to the adversarial attacks, when the samples to attack are restricted to those which have at least one positive label in the same cluster as the target label, the success rate of the attack is signiﬁcantly higher than the naive case. meet the conditions. Also, the cluster size is set to at least 3 labels. The results of the negative-targeted attacks are presented in Table 5. Here the results are with and without using clustering for drawing the samples for each target label. As the results show, the two XMTC models are robust to negative-targeted attacks. However, the diﬀerence in the success rate of using and not using clustering for picking the samples to attack is signiﬁcant, where it can be more than 10% in some cases. We should note that clustering of labels has been used in many XMTC works but for increasing the speed of training and evaluating the model [Prabhu et al., 2018, Khandagale et al., 2020, Jiang et al., 2021, Mittal SuccessSimilarityChange rate (%)rate (%) et al., 2021]. 4.3 Label-frequency-based results In this subsection, ﬁrst, we analyse how the success rate of the positive-targeted attacks changes with respect to data distribution. Second, we investigate the eﬀect of using rebalanced loss functions on this trend. 4.3.1 Attacking labels with diﬀerent frequencies The success rate of positive-targeted adversarial attacks on labels with diﬀerent frequencies are demonstrated in Figure 2 (graphs labeled with “Normal”). We follow the setup introduced in the Setup section to categorize labels in diﬀerent bins based on their frequencies, and the number of randomly drawn samples for each bin is set to 200 for Wikipedia-31K and to 600 for AmazonCat-13K. Also, for these experiments, an attack is considered successful if the USE similarity of the generated adversarial samples with the real samples is above 0.8, and the change rate is less than 10%. It Table 6: Several adversarial samples targeted tail classes from Wikipedia-31K (W) and AmazonCat-13K (AC). RR stands for the rank of targeted labels for the real samples, and RA is the rank of those labels for the adversarial samples. In all the cases, the targeted tail labels fall out of top-5 predicted labels by changing only one word. means that the generated adversarial samples are highly similar to the corresponding real samples. As the ﬁgures show, the success rate of the attacks on both datasets and models has an imbalanced behaviour, where the gap between the tail and head classes is more than 30% in all the cases. It shows that it is easy to generate an adversarial sample for a tail label with a high similarity to the real samples, while this practice becomes diﬃcult for the head classes. Some generated samples for tail classes are depicted in Table 6. We would like to remind the reader that in our experiments, all the samples used for generating the adversarial attacks are classiﬁed correctly. It implies the fact that besides the challenge of predicting tail labels correctly, these labels are also more vulnerable to adversarial attacks when they are correctly predicted. 4.3.2 Robust XMTC with unbiased-rebalanced losses The rebalanced loss functions are originally proposed for the problem of missing labels and imbalanced data [Qaraei et al., 2021]. These loss functions, for losses which decompose over labels, such as the hinge and BCE loss, suggest the following form: where l(l) is the positive (negative) part of the original loss. Also, Cis a factor to rebalance the loss, and Wis a factor to compensate for missing labels. Suggested by Qaraei et al. [2021], we set C= [Cui et al., 2019] where β = 0.9, and W= Figure 2: The success rate of positive-targeted attacks against APLC-XLNet and AttentionXML trained on two XMTC datasets for diﬀerent label frequencies. An attack is successful if the similarity of the real and adversarial samples is above 0.8 and the change rate is lower than 10%. For the normal loss functions, the success rate exhibits an imbalanced behaviour, where the higher values are for the lower frequencies. Rebalanced loss functions mitigate this problem by improving the robustness of tail classes against these attacks. 2/p− 1 where p, called the propensity score of label j, indicates the probability of the label being present and is computed by the empirical model of Jain et al. [2016]. While Cis explicitly introduced to rebalance the loss, Walso reweights the loss in favor of tail classes as the problem of missing labels is more pervasive in those classes. Figure 2 demonstrates a comparison of the original models with the rebalanced variants under the adversarial attacks. Here we refer to the loss modiﬁed for missing labels (only using W) as PW, and when the rebalancing factor (C) is also taken into account, we call the method PW-cb. In our experiments with rebalanced loss functions, the choice of the type of reweighting for each dataset and model depends on its prediction performance. To compare the normal loss with the reweighted variants under the adversarial attacks, we attack the samples that are classiﬁed correctly by the normal loss, but when a reweighted loss is used to train the model. As the ﬁgures show, the rebalanced variants significantly improve the robustness of the models on less frequent classes. This means that using the reweighted loss functions improves the robustness of the model on the samples that are classiﬁed correctly by the normal loss but missclassiﬁed after performing the attack. The gap is large for all the model and datasets, between 10% to 40%, for the least frequent classes. We should remark that although the reweighted loss functions improve robustness of tail classes against adversarial attacks, they have an adverse eﬀect on head classes in Wikipedia-31K dataset. This is mostly due to two labels “wiki” and “wikipedia” which exist in more than 87% and 81% of samples, and have tiny weights in the reweighted loss functions because of having very high frequencies. 5 Conclusion In this paper, we investigated adversarial attacks on extreme multilabel text classiﬁcation (XMTC) problems. Due to the multilabel setting and extremely imbalanced data in these problems, the settings and responses for the adversarial attacks are diﬀerent from the typical text classiﬁcation problems. We observed that XMTC models are vulnerable to adversarial attacks when an attacker tries to remove a speciﬁc true label of a sample from the set of predicted labels, which are called positive-targeted attacks. Also, our ﬁndings show that, besides the diﬃculty of correctly predicting tail classes, a new challenge in XMTC that should be considered is the low robustness of these classes against adversarial attacks. We showed that this problem can be mitigated by using the unbiased-rebalanced loss functions which reweight the loss in favour of tail classes. Some remaining questions include whether there are ways to eﬃciently attack XMTC models by targeting negative labels, and also how to adversarially train an XMTC model given that generating adversarial examples, which needs multiple running of the Bert model for each sample, and adding them to the clean data causes tremendous additional costs to the computationally expensive training of these models.