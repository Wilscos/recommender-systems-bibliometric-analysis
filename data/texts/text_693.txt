Abstract—CTR prediction is essential for modern recommender systems. Ranging from early factorization machines to deep learning based models in recent years, existing CTR methods focus on capturing useful feature interactions or mining important behavior patterns. Despite the effectiveness, we argue that these methods suffer from the risk of label sparsity (i.e., the user-item interactions are highly sparse with respect to the feature space), label noise (i.e., the collected user-item interactions are usually noisy), and the underuse of domain knowledge (i.e., the pairwise correlations between samples). To address these challenging problems, we propose a novel Multi-Interest Self-Supervised learning (MISS) framework which enhances the feature embeddings with interest-level self-supervision signals. With the help of two novel CNN-based multi-interest extractors, self-supervision signals are discovered with full considerations of different interest representations (point-wise and union-wise), interest dependencies (short-range and long-range), and interest correlations (inter-item and intra-item). Based on that, contrastive learning losses are further applied to the augmented views of interest representations, which effectively improves the feature representation learning. Furthermore, our proposed MISS framework can be used as an “plug-in” component with existing CTR prediction models and further boost their performances. Extensive experiments on three large-scale datasets show that MISS signiﬁcantly outperforms the state-of-the-art models, by up to 13.55% in AUC, and also enjoys good compatibility with representative deep CTR models. Index Terms—CTR Prediction; Multi-interest; Self-Supervised Learning; Click-Through Rate (CTR) prediction is an essential task in the domain of online advertising and recommender systems, both of which are multi-billion dollar businesses nowadays. As shown in Table I, the data involved in CTR prediction are mostly in a multi-ﬁeld tabular format. Each row represents a sampledescribed by multiple ﬁeldssuch as gender, click history, item ID, and item category. CTR prediction is to estimate the probability that a user will click an item based on the multi-ﬁeld features. Due to the powerful feature representation learning ability, the mainstream of CTR prediction research is dominated by deep learning models [1]–[3]. Deep CTR prediction models have made great progresses and have been , Rui Zhang www.ruizhang.info deployed in many commercial recommender systems, such as Wide&Deep [1] in Google Play, DeepFM [2] in Huawei AppGallery, and Deep Interest Network (DIN) [3] in Taobao. Despite the great successes, CTR prediction models are all faced with the label sparsity and label noise problems, which deteriorate quickly with the rapid growth of data volume and feature size in online systems. What is more, existing approaches also underuse the domain knowledge implicitly contained in the data. Without solving the above three problems, it is difﬁcult for existing CTR prediction models to learn effective feature representations with the sparse and noisy useritem interactions which serve as the supervision signals. In this work, we seek to utilize self-supervised learning (SSL) to solve the above mentioned three problems. On the one hand, the self-supervision signals are extracted based on the understanding of recommendation domain knowledge, which can effectively supplement the original sparse supervision signals. On the other hand, the self-supervision loss also regularizes the learned representations and ﬁlters out noises. The basic framework for SSL mainly contains two key components: data augmentation for enhancing training data and contrastive losses for enhancing supervision signals. Though making great progress in both CV [4]–[6] and NLP [7]–[9] ﬁelds, SSL has not been fully explored in recommendation tasks. As illustrated in Figure 1, current SSL-based CTR models generally adopt three kinds of augmentation operators, i.e., dropout, reorder, and masking, all of which are directly introduced from CV or NLP areas without appropriate adaptation to recommendation tasks. After augmentation, each behavior sequence is transformed into two different new sequences (i.e., a pair of views) which are required to be similar in the contrastive learning stage. However, in the recommendation domain, it is natural that user behaviors are of multi-interest, as stated in [3], [10], [11], so one augmented pair of views may contain very different interests even when they are obtained from the same user behavior sequence. As a consequence, maximizing the pairwise similarities in contrastive learning may introduce noises and deteriorate representation learning in recommendation tasks. In this paper, we study the self-supervised learning for CTR prediction with the consideration of multi-interest. To incorporate the multi-interest in user behavior sequences, we design three important interest modeling practices to better utilize the domain knowledge. Within a user behavior sequence, an interest can not only be represented as a single behavior in a point-wise manner, but also can be represented as several behaviors in a unionwise manner. For example, as shown in Figure 2, the paint board, the brush, and the palette together represent user’s interest in painting tools, while notebook alone is enough to indicate an interest in electronic products. Therefore, it is important to simultaneously consider both point-wise and union-wise interest representations to provide more sufﬁcient self-supervision signals. possible that user behaviors of one interest are interleaved with behaviors of another interest, in which case modeling long-range dependencies is necessary. For instance, in Figure 2, behaviors on electronic products (i.e., computer, phone, and charger) are interleaved by behaviors on clothes (i.e., red T-shirt and green sweater). It is also common that behaviors of an interest are consecutive without any interruption, e.g., the ﬁrst three behaviors of painting tools in Figure 2, thus short-range dependencies also need to be learned. Therefore, mining long-range and short-range dependencies are complementary when modeling user behavior sequences with multiple interests. the inter-item interest correlations discussed above, the interest correlations between different item attributes (deﬁned as intra-item correlation) also contain useful self-supervision signals. For examples, some people like Nike sneakers while some other people may prefer cheap slippers. Therefore, it is necessary to extract self-supervision signals from both inter-item and intra-item correlations together. To this end, we propose a novel Multi-Interest SelfSupervised learning (MISS) framework for deep CTR models. To mine self-supervision signals from user behaviors of multiinterests, MISS proposes interest-level contrastive losses to take the place of sample-level losses. Speciﬁcally, individual self-supervision signals are extracted for multiple interests from the user behaviors. By means of Convolutional Neural Network (CNN), both point-wise and union-wise interest representations are learned from the local correlations of behaviors on the time line, and the short-range and long-range interest dependencies are extracted by considering different distances of interest representations. Finally, the intra-item correlations are also modeled by sampling different feature combinations with convolution kernels. Furthermore, MISS serves as a model-agnostic embedding learning framework for user behavior sequence features, which is able to work compatibly with the existing deep CTR models, including methods based on both feature interactions and user interest mining. To summarize, our work makes three major contributions as follows: 1) We propose a novel Multi-Interest Self-Supervised Learning framework named MISS, which enhances feature embeddings in an end-to-end manner. As far as we know, our work is the ﬁrst to apply interest-level contrastive losses for recommendation tasks. 2) More specially, we propose CNN-based self-supervision signal extractors with full considerations of different interest representations (point-wise and union-wise), interest dependencies (short-range and long-range) and interest correlations (inter-item and intra-item). Based on that, contrative learning is implemented to make better use of the domain knowledge and to make the best of interest-level self-supervision knowledge. 3) Extensive experiments demonstrate that our MISS framework not only achieves state-of-the-art performances on three large-scale datasets, but also enjoys excellent compatibility with various representative baselines. In early days, shallow models were proposed, such as LR [12], FM [13], Bayesian models [14], GBDT [15], and Poly2 [16]. However, shallow CTR models rely heavily on hand-crafted features and share a common bottleneck in exploiting possible feature interactions for improvements. The last decade has witnessed the great successes of deep neural networks (DNN) in many ﬁelds like CV [17], [18] and NLP [19], [20], deep learning techniques have also been introduced to CTR prediction research. By virtue of the superior learning ability of DNNs, many recent CTR models are empowered to effectively capture high-order feature interaction patterns [1], [2], [21]–[25]. According to different model architectures, these models can be divided into two categories: feature interaction based models and user interest modeling based models. In the following, we give a brief introduction about these two kinds of models, interested readers can refer to the recent survey paper [26] for more details. Feature interaction based models focus on learning sophisticated interactions between different features, and the representative models include Wide&Deep [1], DeepFM [2], DCN [22] and DCN-M [27]. Wide&Deep [1] learning builds a wide linear component and a DNN component to model explicit and implicit feature interactions respectively. However, manual efforts for feature engineering are still required in its wide component. To avoid such manual efforts, DeepFM [2] is thus proposed to replace the wide part with FM and share the input features between deep and wide components. DCN [22] explicitly and automatically applies feature crossing for improving accuracy and efﬁciency of the DNN model. DCN-M [27] further improves DCN by replacing the cross vector into a cross matrix to enhance it’s learning ability. Some AutoML-based models are also proposed for feature interaction modeling. AutoCTR [28] automatically discovers appropriate feature interaction architectures with a bi-level search space. AutoFIS [29] automatically selects important feature interactions for factorization models with a set of architecture parameters. However, these ML-based models require massive resources to search parameters which are usually not available for regular research groups or even midsized companies, so we considered it not appropriate to be included for comparison, and hence is not efﬁcient enough for practical use, especially in industrial settings. On the other hand, user interest modeling based models dedicate to capture important patterns from sequential behavior ﬁelds (e.g., Click History in Table I), which generally involves multiple features that can reﬂect user’s real interests (e.g., {Honor50, iPhone12, MI11} for Lisa). The mainstream models in this category include DIN [3], DIEN [30], and DSIN [31] which aim to use auto-regressive models to learn users’ diverse interests precisely. DIN [3] proposes a local activation unit to adaptively learn candidate-wise user interest representations from the diverse behavior sequences, based on which the CTR score is estimated. Based on DIN, DIEN [30] further proposes to capture the interest evolving process with an auxiliary loss, thus better deals with interest drifting. Considering the homogeneity of user behaviors within each session, DSIN [31] adopts self-attention and Bi-LSTM to capture intra- and intersession interest representations respectively. To retrieve more relevant user behavior interests from long history sequences, search-based models like SIM [32] and UBR4CTR [33] have also been proposed. To better utilize the user-item relevance, DMR [34] adopts the attention networks to learn user and item representations from the user-item and item-item interaction networks. Despite the great progresses achieved by feature interaction and user interest modeling models, there are three common problems hindering the performances of both lines of approaches, i.e., label sparsity, label noise and the underuse of domain knowledge. To tackle these three problems, we propose a self-supervised learning framework tailored for deep CTR models. Through data augmentation and interestlevel contrastive learning, self-supervision signals and pairwise correlations between samples can be utilized to enhance user interest representations learned from sparse and noisy data. What is more, our proposed framework is model-agnostic which can be seamlessly applied to both feature interaction and user behavior modeling approaches, as described and veriﬁed in the following sections. Self-supervised learning [4], [35]–[37] has recently become an emerging trend in CV and NLP areas. SSL models enhance the learned representations with self-supervision signals extracted from unlabeled data, thus alleviating deep models’ heavy dependence on manual labels. According to the model architectures and learning objectives [38], SSL models can be categorized into two genres, i.e., generative models and contrastive models. Generative SSL models exploit context features by modeling the generation processes, and typical examples include the BERT models [7], [8], [39]. BERT models usually use two pretext tasks for pre-training, i.e., the masked language model and the next sentence prediction task. Through the generation of missing context words or sentences, the pretext tasks effectively transform the co-occurrence information into selfsupervision signals and thus improve semantic representations. On the other hand, contrastive SSL models utilize discrimination information in a “learn to compare” manner, which maximizes the correlation between similar instances [40], [41]. Following this paradigm, Deep InfoMax (DIM) [42] explicitly learns the Mutual Information Maximization (MIM) objective between features from the local patches and the whole input image. Similarly, Contrastive Predictive Coding (CPC) [41] learns MIM between audio segments and their context audios. Deep Graph InfoMax (DGI) [43] explicitly maximizes the correlation between a node and its 2-hop neighbors in the context graph with MIM. Despite the successes achieved in CV and NLP areas, exploiting SSL in recommendation is still an under-explored task where few works have been proposed so far. To characterize the intrinsic data correlations, S3Rec [44] combines SSL with sequential recommendation by utilizing four MIM objectives, i.e., Item-Attribute MIM, Sequence-Item MIM, Sequence-Attribute MIM, and Sequence-Sequence MIM. In large-scale item recommendations, an auxiliary SSL task is employed to explore feature correlations by applying different feature masking patterns [45]. CL4SRec [46] proposes three data augmentation techniques (i.e., cropping, masking and reordering) from which two methods are randomly sampled and applied to each user sequence. Instead of performing selfsupervision in the data space, [47] proposes a sequence-tosequence training strategy to extract extra supervision signals from pairwise sub-sequences in the disentangled latent space. SGL [48] extends SSL to GCN-based recommendation models by augmenting ID embeddings and graph structures. However, the above SSL recommendation models directly borrow the ideas from CV and NLP areas without carefully considering the characteristics of recommendation tasks, especially the interest diversity of individual users. In consequence, two instances generated from the same user behavior sequence are unconditionally required to be similar in contrastive learning, even if they are derived from different interests. Such one-size-ﬁts-all practices inevitably introduce noises into representation learning, thus harm the recommendation performance. To this end, we propose a new SSL CTR framework to incorporate self-supervision signals at the interest level. By considering historical behavior dependencies under multiple interests, user representations are enhanced by better exploiting the intra-interest behavioral self-supervision while avoiding inter-interest contrastive learning. For the ease of understanding, in this section, we begin by the formal deﬁnition of the CTR prediction task and necessary notations, followed by the limitation analysis of current deep CTR models and the outline of this work. In a recommender system, data samples are usually stored in a multi-ﬁeld format as shown in Table I. For CTR prediction purpose, necessary features are retrieved and combined in a ﬁxed format to describe each sample. For example, sample of user Yoshida can be represented as Based on the collected data, CTR prediction is to estimate the probability that a user (i.e., Yoshida) will click a candidate item (i.e., Beer) under the given context (i.e., Friday). In symbolic language, suppose there are |U| users U = u, u, ..., uand |V| items V =v, v, ..., v, each user or item is accompanied with some attribute information such as user gender and item category, and the behavior sequence of each user is also collected and chronologically ordered as b = {v, v, · · · , v}, where vis the l-th interacted item and L is the sequence length. Thus a long raw feature vector is constructed for each sample x through the combination of categorical and sequential features: x = [f, · · · , f, · · · , f, s, · · · , s, · · · , s where fis a categorical feature, sis a sequential feature, I and J denote the numbers of categorical and sequential features respectively. Note that, besides the item ID sequence, the attribute sequences of interacted items are also useful for CTR prediction such as the category sequence c = {c, c, · · · , c} and price sequence p = {p, p, · · · , p}, thus we have s∈ {b, c, p}. However, the sequential features are not limited to these three kinds, but can also incorporate other features according to speciﬁc tasks. Take x as input, a CTR model is learned to minimize the following loss function:X CTRModel(·, ·) is the CTR model with parameter set Θ, and ∆(·, ·) is the loss function. Table II summarizes the notations used in this paper. To tackle the CTR prediction task described in the last subsection, various machine learning approaches have been proposed as described in Section II. Despite the achievements, there are two common problems that seriously inﬂuence the performances of existing CTR models, i.e., label sparsity and label noise. Here we explain these two critical problems in details. serve as supervision signals for CTR models, are highly sparse with respect to the number of items [49], [50]. Moreover, there are numerous cold-start users and infrequent items that have very sparse history interactions, which makes the training of CTR models non-trivial. teractions are also noisy by two reasons. On the one hand, there exist spurious interactions derived from miss clicks or simple curiosity rather than users’ true interests. On the other hand, there are items that match one user’s potential interests but are not interacted due to underexposure. However, the random negative sampling process may judge them as not interesting to the user. The label sparsity and label noise problems deteriorate quickly with the growth of data volume and feature size due to the Matthew Effect. In other words, popular items occupy more and more exposure chances and accumulate richer features and more supervision signals. On the contrary, unpopular items are less likely to be seen by users and suffer from the increasing lack of features and supervision. Without sufﬁcient and correct supervision, it is difﬁcult for CTR models to make accurate predictions. To this end, we propose a novel MISS framework to improve representation learning by means of SSL, as shown in Figure 3. Our MISS framework is model-agnostic and can be outlined with three major contributions: tors are proposed to explore both point-wise and unionwise interest representations while considering inter-item and intra-item correlations. After that, the extracted interest representations are further augmented in consideration of the short-range and long-range dependencies, which makes them more robust to label noise. the multi-interest representations, which effectively transforms the interest-level correlations into extra supervision signals to alleviate the lack of supervision caused by label sparsity. work, MISS ﬂexibly combines the SSL component with any CTR prediction model in a plug-and-play manner, which achieves both signiﬁcant performance boosts and excellent compatibility with little handcrafted model conﬁgurations. In this section, we present the technical details of the proposed MISS framework. As illustrated in the right part of Figure 3, a typical deep CTR model DIN [3] is given as the default base model according to the experimental results in Table V, based on which we explain in detail how the MISS framework is applied in a plug-and-play mechanism. For other advanced deep CTR models, a compatibility analysis is also provided later in the experiments. The base model consists of embedding initialization, representation learning, and CTR prediction components. 1) Embedding Initialization: In CTR prediction, input data samples are usually represented as high-dimensional sparse feature vectors as in Equation (1). To facilitate follow-up calculations, feature vectors are ﬁrst transformed into dense real-valued embedding vectors. For the I one-hot categorical features, fis embedded into eby looking up the ﬁeld-wise embedding table. While a sequential feature sis represented as a list of embedding vectors. By gathering all categorical and sequential feature embeddings, a sample is represented as a set of embedding vectors E = {e, · · · , e, e, · · · , e, · · · , e, · · · , e}, (3) where L is the sequence length. 2) Representation Learning: The behavior sequence length L differs from user to user, thus the number of embeddings in E also varies. A naive solution is to transform all sequential features into the same length with truncation and padding. However, truncation brings information loss and padding increases redundancy. To handle this problem, researcher generally resort to different pooling techniques to aggregate the embedding sequences, which include max pooling, mean pooling, sum pooling, and the advanced attention-based pooling. In this paper, we adopt the local activation unit based pooling, which was proposed in DIN [3], as it learns to assign an adaptive weight to each feature embedding according to the target item. For all J sequential features with length L, the embedding vectors are aggregated into the sample representation as: where eis the embedding of the candidate item, and LAUP(·, ·) is the pooling net based on the local activation unit. For space limitation, the technical details of LAUP(·, ·) is omitted here, interested readers can refer to [3]. Thus a ﬁxed-length representation X is obtained for each sample x. 3) CTR Prediction: Based on the integrated feature representation X, a Multi-Layer Perceptron (MLP) further learns the advanced feature interactions. Suppose a D-layer MLP is adopted, each layer works as where ais the output of the previous layer, σ is the activation function, Wand oare the weight matrix and bias vector respectively. We set a= X for the ﬁrst layer. Finally, a prediction layer is devised to predict the CTR score where Sigmoid(·) is the sigmoid activation function, and ˆy is the predicted CTR score. Finally, the batch-wise Logloss objective function is adopted to evaluate the predicted CTR score ˆy: where B is a batch of training samples, (x, y) is the pair of sample and label in the batch, and Lis an instantiation of Equation (2). The base model estimates the CTR score through embedding initialization, representation learning, and CTR prediction, as illustrated in the right part of Figure 3. Based on that, the proposed MISS framework further enhances feature embeddings with SSL by multi-interest augmentation, interest view encoding, and contrative learning, as shown in the left part of Figure 3. In this subsection, we explain the MISS components step-by-step. 1) Sample-Level Data Augmentation: Data augmentation is the ﬁrst step of our proposed MISS framework, based on which the contrastive learning is implemented. Given a batch of training samples B = {x, x, · · · , x}, existing SSL-based models all adopt sample-level data augmentation methods. For each sample x, two different views are ﬁrst obtained through augmentation as: where Aug(·) is the sample-level augmentation function, and hh, hi is the pair of generated views. After data augmentation, encoder functions are further used to extract high-level semantic representations from hand h, based on which a contrastive loss is used to make use of the self-supervision signals. However, due to the multiinterest characteristic of user behavior sequences, samplelevel data augmentation may inevitably introduce noise. The reason is that the augmented hand hmay be derived from different interests even if they are obtained from the same x. To solve this problem, we put forward an interest-level SSL framework, i.e., MISS, which augments the training data at the interest level in an end-to-end fashion. 2) Multi-Interest Data Augmentation: In consideration of the multi-interest characteristic of user behaviors, our MISS framework implements SSL within each sample at both the interest level and the feature level. Therefore, not only the semantics provided by each training sample can be enriched, but also the modeling and utilization of long behavior sequences get promoted. To achieve these targets, we design a novel multi-interest extractor for data augmentation purpose. To augment user behavior data at the interest level, the multiple interest representations of each user should ﬁrst be extracted. An intuitive augmentation method is to directly divide the user behavior sequences according to item categories. However, item categories are usually deﬁned in coarse granularities and are not always available in data. Therefore, we propose a CNN-based multi-interest extractor which transforms the sample feature x into a group of implicit interest representations where MIE(·) is the multi-interest extractor network, T is the output user interest representation sequence, and tis the k-th interest representation extracted from x. Moreover, for a ﬁnegrained understanding and utilization of interest semantics, another CNN-based feature augmentation component is further designed to augment each interest representation at the feature level R = MIMFE(x)(10) = {{r, · · · , r}, . . . , {r, · · · , r}} where MIMFE(·) is the multi-interest multi-feature extractor network that extracts ﬁne-grained representations for each user interest, and Ω is the number of feature representations for each interest. After that, an augmentation function is applied to T to obtain interest-level augmented views as: H= Aug(T ) = {hh, hi, · · · , hh, h where Aug(·) is the interest-level augmentation function, hh, hi is a pair of generated views for sample x, and P is the number of generated view pairs. Similarly, an augmentation function is also applied to R for a further ﬁne-grained augmentation as: where Aug(·) is the feature-level augmentation function, hh, hi is a pair of views, and Q is the number of generated pairs. In this section, we focus on the principled explanation of our MISS framework, while the technical details of MIE(·), MIMFE(·), Aug(·), and Aug(·) are presented later in Section V. 3) Interest View Encoder: With the extractor networks and augmentation functions, two sequences of augmented view pairs are obtained, i.e., Hand H, where the user interest representations are augmented at different granularities. Based on the sequence H, an encoder is adopted to explore highorder abstractions: Z= Enc(H) = {hz, zi, · · · , hz, z where Enc(·) is the encoder network that transforms each interet view representation h(or h) into high-order representation z(or z). Similarly, an encoder Enc(·) is also applied to H: As we mainly focus on the extraction and utilization of selfsupervised signals, two simple MLPs are used to implement Enc(·) and Enc(·). However, other advanced networks are also applicable, such as Transformer in [46], [51], and we leave the exploration of other encoder structures to future works. 4) Contrastive Loss: Having obtained the high-level semantics for each augmented view, the contrastive losses can ﬁnally be applied to exploit the self-supervision signals. Following SimCLR [4], we use the InfoNCE contrastive loss [41] which attempts to maximize the similarity of positive pairs of views and minimize the agreement of negative pairs of views. As a result, similar interests can thus have similar representations (deﬁned as alignment) and sufﬁcient information are kept to distinguish different interests (deﬁned as uniformity). Both the alignment and uniformity properties are necessary and important for a good SSL system, as proved in [52]. Formally, taking hz, zi from the same interest as positive pairs while hz, zi and hz, zi from different samples as negative pairs, the InfoNCE loss for learning the interest-level correlation is formulated as: where s(·, ·) is cosine similarity function, τ is the softmax temperature parameter, and exp(·) is the exponential function. Similarly, the InfoNCE loss for learning the feature-level correlation can be formulated as: To better integrate the MISS framework with the CTR prediction component, a multi-task learning strategy is adopted to jointly optimize the auxiliary SSL losses and the main prediction loss in an end-to-end manner. Thus the ﬁnal loss function is formulated as: where αand αare the hyper-parameters to control the strength of SSL losses. In experiments, the two-stage pretraining learning strategy is also tried, where the model is ﬁrst trained with the auxiliary SSL losses, then ﬁne-tuned by the main prediction loss. As mentioned in Section IV, a multi-interest extractor network MIE(·) and an interest-level feature extractor network MIMFE(·) are used for interest representation learning at different granularities, based on which the augmentation functions Aug(·) and Aug(·) are further applied. In this section, we describe the technical details of these extractors and augmentation functions. The multi-interest extractor network MIE(·) aims to discover the potential interests from user behavior sequences. However, it is hard to achieve this goal as the number of interests varies from user to user. Moreover, the sequential pattern of the same interest is also dynamic in terms of both different users and different time. Therefore, we propose an intuitive multi-interest extractor based on a closeness assumption, i.e., user behaviors derived from the same interest are more likely to be closely located within a sequence. Based on the closeness assumption, we adopt CNN to extract hidden interest representations due to its effectiveness in capturing the local correlations [17], [18]. Other sequence representation learning models like RNNs or self-attention are also applicable, however, they fail to extract effective interest representation pairs for comparison. We will verify this point later in the experiments. After padding, all J sequential features share the same length L, and the embeddings in E can be re-organized into a 3D tensor as: where C ∈ R, and K is dimension of each embedding vectors e. Hidden interests are extracted through horizontal convolutions along the time axis of C. Take the click sequence in Figure 2 as an example, one convolution kernel may aggregate the feature embeddings of the paint board (C), the brush (C), and the palette (C) into the interest in painting tools, while another kernel may take the embeddings of the notebook (C) alone as the interest in electronic products. Speciﬁcally, a horizontal convolution layer with M branches of kernels are adopted, as shown in left middle part of Figure 4. Denote g∈ Ras a kernel with width m ∈ [1, M ], where both the kernel height and channel number are set to 1. For simpliﬁcation, only one kernel is used in each branch. Thus M kernels with different widths are used, which simultaneously capture the point-wise (m = 1) and union-wise (m > 1) interest representations. Each gslides on C from left to right, and the convolution operation at the j-th row, the l-th to the (l + m − 1)-th column, and the k-th channel in C is formulated as: where ReLU(·) is the ReLU activation function, ◦ denotes the convolution operation, Crepresents the sliced subtensor from C, and l is restricted as 1 ≤ l ≤ (L − m + 1). After convolution, the ﬁnal output tensor of gis denoted as G∈ R, which is a combination of (L−m+1) interest representations. Take all M ﬁlters as a whole, the re-P sulting |T | =(L − m + 1) interest representations together make up the interest sequence where Flat(·) is the ﬂatten function that transforms each interest representation G∈ Rinto a vector t ∈ R, as shown in the upper part of Figure 4. The multi-interest extractor learns different user interest representations based on the closeness assumption. However, the closeness assumption not only holds at the behavior level, but also applies to the extracted interest representations. In other words, the more adjacent two interest representations are located on the time line, the more likely they represent the same hidden interest. Therefore, we randomly select a pair of representations as two different views of the same interest from those with the same ﬁlter gfrom T where RS(·) randomly selects two representations derived from the same convolution ﬁlter with a given distance h ∈ [1, H]. By repeating the select function RS(·) for P times, the sequence of augmented interest view pairs His obtained. Here we use different h values to cover both short-range and long-range interest dependencies, and a maximum distance H is pre-deﬁned to prevent overlong dependencies. Note that, we assume a uniform distribution of interest dependency distance h. However, other complex distributions (e.g., Gaussian distribution) are also applicable, and we leave them to future works. The multi-interest extractor MIE(·) only explores the interitem correlations of different interests along the time line, while the intra-item relationship between all J features within each interest is ignored. For example, given the resulting interest representations G∈ R, the interest in daily supplies (say G) is sensitive to price, while the interest in shoes (say G) is affected by both price and category. To deal with this issue, vertical convolution operators are further applied to each resulting at the feature level for ﬁne-grained augmentation. As shown in the right middle part of Figure 4, N branches of vertical kernels ˆg∈ Rare adopted to learn intra-item correlations, where n ∈ [1, N] is the kernel height. As n varies from 1 to N, both single and collective feature representations are captured. Each kernel ˆginteracts with the sliced tensor Gfrom left to right, which yields the resultˆG: Denote the ﬁnal output via ˆgon GasˆG∈ R, it can also be viewed as a combination of (J − n + 1) interest representations. However, the interest representations are now enhanced at the feature level. Through all N kernels, |T |Ω enhanced interest representationsP are obtained, where Ω =(J − n + 1). Formally, R = MIMFE(x) = {. . . , {. . . , Flat(ˆG), . . .}, . . .}(23) = {{r, · · · , r}, . . . , {r, · · · , r}}. Because of the independence between each feature, a totally random select function RS(·) is applied to sample featurelevel interest representation views from eachˆG, as which is repeated for Q times. For the M branches of horizontal convolution kernels, i.e., g∈ R, there are m learnable parameters. As m ranges from 1 to M , the total number of learnable pa-P rameters ism =. Similarly, the total number of the N branches of vertical convolution kernelsP isn =. After data augmentation, two encoders are used for high-order abstraction. For simplicity, two MLP encoders are used. Thus the numbers of introducedP parameters are J × K × H+H× HforP encoder Enc(·), and K × H+H× Hfor encoder Enc(·), where Dis the layer depth and His the layer size of the d-th layer. All together, the total number of parameters brought by the multi-interest data augmentation in MISS is+ J × K × H+ K × H+P N are very small values and the MLP parameters are also negligible compared to the embedding matrices, which makes the space complexity of MISS acceptable. A. Experiment Setup 1) Datasets: We evaluate the effectiveness of our proposed model on three large-scale datasets, i.e., Amazon-Cds, Amazon-Books, and Alipay. All three of them are real-world datasets described as follows: data from one of the largest e-commerce website in the world, i.e., amazon.com. The crawled reviews have a time span from May 1996 to July 2014. The dataset can be divided into many subsets according to the various product categories, such as Amazon-Electronics, Amazon-Cds, and Amazon-Books. In this paper, we pick the Amazon-Cds and Amazon-Books subsets for experiments. contest which is collected from the Tmall.com website, the Taobao.com website, and the Alipay App. It contains user behavior logs between July 1st to November 30th in 2015. Each log contains multiple feature ﬁelds, including user ID, item ID, seller, category, online action type, and timestamp. We take the click behaviors as users’ interaction records to construct the user behavior sequences. Table III presents the detailed statistics of the three datasets. As we can see, the datasets are different from each other in many aspects including feature number, ﬁeld number, and interaction sparsity. 2) Data Processing: To ensure the data quality, we ﬁlter out infrequent users and items with fewer than 5 interactions in the Amazon-Cds dataset. While the threshold value is 10 in the Amazon-Books and Alipay datasets. For all datasets, we aggregate each user’s interaction records and sort them by the action timestamps in chronological order. For evaluation purpose, we adopt the data split strategy in [33], [53]. Speciﬁcally, suppose a user has L historical behaviors sorted by time, the behavior sequence [1, L − 3] is used for training and predicts whether she/he will interact with the (L−2)-th item. Similarly, behavior sequence [1, L − 2] is used to predict the (L − 1)-th item in the validation set, while behavior sequence [1, L − 1] is used to predict the L-th item in the testing set. Further, given a user, a non-interacted item is randomly selected as the negative sample. 3) Baseline Models: For a thorough veriﬁcation of model effectiveness, the proposed MISS framework is compared with three groups of representative CTR prediction models: a) Feature interaction based models (LR [54], FM [13], DeepFM [2], IPNN [49], DCN [22], DCN-M [27], xDeepFM [55]); b) User interest modeling based models (DIN [3], DIEN [30]), SIM(soft) [32], DMR [34]; c) GNN and Transformer based models (AutoInt+ [56], FiGNN [57]). 4) Evaluation Metrics: To quantitatively evaluate the model performances, two widely-used metrics are adopted, i.e., AUC and Logloss [2]. AUC measures the goodness of ranking positive samples higher than randomly chosen negative samples. A higher AUC value means better performance. Logloss measures the distance between the predicted scores and the true labels for all samples. A lower Logloss value indicates better model performance. 5) Parameter Settings: For a fair comparison, we set the embedding dimension of all models as 10, and the batch size is ﬁxed as 128. The learning rate is selected from {10, 10, 10, 10}, the Lnorm regularization weight is picked from {0, 10, 10, 10, 10, 10}, and the dropout ratio is tuned from 0 to 0.9 with step size 0.1. The deep layers for all models are set as {40, 40, 40, 1}. The Adam optimizer [58] is chosen for model optimization. In addition to the above hyper-parameters for all models, we set the layers for interest encoder and feature encoder as {20, 20} and {10, 10}. For simplicity, we set α= α, and search them and τ within the ranges of {0.05, 0.1, 0.5, 1, 5}. For the branches of horizontal and vertical convolution kernels, M is tuned from {1, 2, 3, 4}, and N is tuned from {1, 2}. The distance H is tuned from {1, 2, 3, 4}. We use the validation set for parameter tuning, while the ﬁnal reported performances are obtained on the testing set. Each experiment is repeated for 5 times to remove random noises, and the averaged results are reported. In this section, we compare the performances of MISS with the state-of-the-art CTR prediction models. Table IV shows DeepFM 0.8039 0.5369 0.8056 0.5310 0.8718 0.4464 DCN-M 0.8050 0.5363 0.8070 0.5293 0.8757 0.4403 xDeepFM 0.8034 0.5370 0.8028 0.5336 0.8777 0.4382 SIM(soft) 0.7977 0.5437 0.7951 0.5430 0.9101 0.3729 AutoInt+ 0.8008 0.5398 0.8045 0.5317 0.8705 0.4479 the experimental results of all compared models on all three datasets. From Table IV, we have the following observations: all three datasets. More precisely, MISS signiﬁcantly (p − value < 0.05) outperforms the strongest baselines by 9.27%, 13.55% and 1.96% in terms of AUC (17.62%, 29.38% and 9.53% in terms of Logloss) on the AmazonCds, Amazon-Books, and Alipay datasets respectively. The great improvements over baseline models verify the effectiveness of MISS for CTR prediction. By supplementing the CTR prediction task with self-supervised learning, MISS is capable of exploiting the latent correlation information with more supervision signals, while baseline models only utilize the observed user-item interactions as supervision signals. datasets are much more signiﬁcant than in the Alipay dataset. A possible reason is that the time span of user behaviors in these two datasets (over ten years) is much longer than that in the third dataset (six months). As more diverse interests take place in the relatively longer time span, our proposed MISS obtains more signiﬁcant improvements by considering the multi-interest characteristic of user behaviors. indicates that shallow models are insufﬁcient for CTR prediction. By modeling high-order feature interactions with DNNs, DeepFM, IPNN, DCN, DCN-M and xDeepFM perform better than shallow models. DIN, DIEN, SIM(soft), and DMR achieve comparable performances with deep feature interaction models, which demonstrates the usefulness of user interest mining. Due to the weakness of RNN in modeling long sequences, DIEN performs a little worse than DIN. DMR achieves the best performances among all compared baselines. A possible reason is that it learns better representations by utilizing and integrating both user-item and item-item interactions in an attentive manner. AutoInt+ and FiGNN use self-attention or GNN for feature interaction modeling. Similar performances can be found compared with DeepFM and IPNN. It indicates that only using useritem interactions as supervision signals (as by existing deep CTR models) cannot make a big difference on the model performances. To better understand the design rational of our proposed MISS, we conduct a series of ablation experiments and analysis in this section. DIN-MISS 0.8867 0.4357 0.9180 0.3730 0.9327 0.3295 IPNN-MISS 0.8858 0.4368 0.9146 0.3778 0.9004 0.4006 FiGNN-MISS 0.8828 0.4410 0.9170 0.3746 0.8947 0.4160 1) Compatibility Analysis: Compatibility is among the key factors that restrict one model’s applications. To verify the compatibility of our proposed MISS framework, apart from the DIN backbone model described in the framework section, we also use it to improve the representation learning in another two representative CTR models, i.e., IPNN, and FiGNN. For a fair comparison, other parts of these models remain unchanged, and the enhanced models are named as DIN-MISS (the same model as MISS), IPNN-MISS, and FiGNN-MISS respectively. We compare the original and enhanced models on the three datasets, and the experimental results are presented in Table V. As can be easily observed, all three enhanced models (DIN-MISS, IPNN-MISS, and FiGNN-MISS) signiﬁcantly outperform their original models on all three datasets. It validates the compatibility of our embedding enhancement approach by demonstrating its effectiveness when combined with various popular CTR models. The results show that MISS can be used as a general framework to improve the existing CTR models by supplementing self-supervised signals for embedding enhancement. 2) Superiority Analysis: To demonstrate the superiority of our proposed MISS framework, we compare it with state-ofthe-art self-supervised learning models. Speciﬁcally, we apply MISS, IRSSL [45], S3Rec [44], and CL4SRec [46] to the IPNN, DIN, and FiGNN models for embedding enhancement purpose. Besides above SSL models, we also equip these base CTR models with a rule based model that segments the behavior sequence into several sub-sequences based on item categories and then conduct dropout on each sequence for SSL. The resulting models are named in an “A”-“B” manner where “A” and “B” represent the base model and the SSL method respectively. Notice that, we adopt the item feature mask strategy in IRSSL as it achieves better performances than feature dropout, and the sequence-segment correlation is adopted in S3Rec thanks to its best performances within the four data augmentation techniques. Comparative experimental results of the original and enhanced models are shown in Table VI. Due to the space limitation and similar trend of evaluation metrics, results of the FiGNN model are not presented. From Table VI, we have the following ﬁndings: of the base models or datasets, which further veriﬁes the superiority of our comparative learning strategies. than IRSSL on the Amazon-Books dataset, which veriﬁes the effectiveness of interest-level contrastive learning for recommendation tasks. However, comparable performances are achieved on the other two datasets. The reason is that the item categories in different datasets are differently deﬁned. In some cases, item categories indicate user interests well, but in other cases they do not. IPNN and it is the same for DIN-IRSSL and DIN. The reason is that IRSSL only focuses on item features, thus loses efﬁcacy when few item features are available. the original models, which supports the effectiveness of SSL at the behavior level. However, there is an obvious semantic difference between a random segment and the whole behavior sequence, hence the correlation learning is biased and limits its performances. second best performances. In CL4SRec, the majority of the behavior sequences remain unchanged after the item crop, mask, and reorder operations, which makes it more robust to random noises. In our MISS, however, a more ﬂexible data augmentation method is put forward to make better use of user interests. 3) Effectiveness Analysis: As ﬁrstly addressed in the Introduction and also reﬂected in the model structure, our MISS framework is built upon some important practices including the multi-interest consideration (M), the union-wise interest representation (U), the long-range interest dependencies (L), and the intra-item feature correlation (F). To evaluate the effectiveness of these different practices, we explore MISS with different settings. By removing some of the practices, ﬁve more MISS variants are obtained and named as MISS/F, MISS/F/U, MISS/F/L, MISS/F/U/L, and MISS/M/F/U/L respectively. All MISS variants are applied to the IPNN, DIN, and FiGNN models to verify their performances, where the resulting models are also named in an “A”-“B” manner. The comparison results are presented in Table VII, where the results of the FiGNN model are also omitted to save space. As can be observed, all MISS variants bring about performance boosts to the original IPNN and DIN models, and the complete MISS framework achieves the best results. Therefore, we claim that all four practices (M, U, L, and F) are effective and complementary to each other, and it is necessary to adopt all of them for better performances. What is more, the removal of M results into the worst performance decay, revealing the importance of multi-interest modeling. MISS-SA 0.8042 0.5385 0.8128 0.5225 0.9092 0.3758 MISS-LSTM 0.8106 0.5299 0.8172 0.5178 0.9096 0.3753 MISS-CNN 0.8867 0.4357 0.9180 0.3730 0.9327 0.3295 4) Multi-Interest Extractor Analysis: To verify the rational of our MIE(·) design formulated in Equation (18-20), we compare the performances of our proposed CNN module with self-attention [59] and LSTM [60] for multi-interest extraction, and the resulting models are named as MISS-CNN (the same model as MISS), MISS-SA, MISS-LSTM respectively. Table VIII summarizes the experimental results. We can see that the CNN extractor achieves the best performances on all datasets. For an in-depth analysis of these extractors, we further visualize the cosine similarity scores between the generated pairs of views from these interest representations on Figure 5. Each training step on the x-axis in Figure 5 corresponds to a batch of training samples fed at that step, and the average similarity score among training batches are reported. As we can see, the similarity scores of MISS-SA and MISS-LSTM are close to 1, thus the generated pairs hardly provide any useful information for contrastive learning. The reason may be as follows. LSTM learns the characteristics of the whole historical behavior sequence, and the histories of two adjacent items of the sequence only differ by one item, so the representations learned for the two adjacent items via LSTM are highly similar. Self-attention based method aggregates all behaviors to generate interest representations, and hence learns similar representations for adjacent items. In comparison, CNN based model considers a sliding window of the past history, and the size of the sliding window is small (at most 3 or 4 in our experiments), so differing by one most recent item will make a notable difference in the representation. This is evidenced by the similarity scores of our proposed CNN model, which are in the range of 0.7 and 0.8. The representations of interest at adjacent timestamps are similar but also distinguishable for contrastive learning. This validates the superiority of using CNN compared to LSTM or self-attention in our problem. During training, our proposed MISS framework has several key hyper-parameters that may affect the performances, and so do the multi-task training strategies. In this section, we ﬁrst investigate the importance and sensitivity of these hyperparameters by changing one hyper-parameter while ﬁxing the others. After that, different training strategies of the two losses are also compared. 1) Impact of the loss weight: The ﬁnal loss function of MISS in Equation (17) is a combination of the CTR prediction loss and the SSL losses. Figure 6 shows the CTR prediction performances under different loss weights where larger weights indicate stronger contributions of the SSL losses. We can observe that the performances grow stably with the increase of the loss weight at the beginning. However, when the weight grows bigger than 1, performance degradation happens. Thus the SSL losses should not dominate the training process. In other words, the SSL part takes the auxiliary role for CTR prediction, and the model can be biased when it is overemphasized. 2) Impact of the softmax temperature: The softmax temperature parameters in Equation (15) and Equation (16) tune the distribution of the SSL losses. A large temperature value will draw close the predictions of positive and negative samples in SSL losses, thus weakens the supervision signals in training. We analyze how different temperature parameter values affect the model performances, and the results are illustrated in Figure 7. With the growth of the temperature value, performances on all three datasets increase ﬁrst and then decrease. The turning point is 0.1 for both metrics on all datasets. With such a small temperature value (signiﬁcantly less than 1), the supervision signals get strengthened during training as the positive and negative SSL samples are better discriminated. In other words, discriminating positive and negative samples beneﬁts the model performances, which accords with our motivations. MISS-Joint 0.8867 0.4357 0.9180 0.3730 0.9327 0.3295 MISS-Pre 0.8848 0.4381 0.9170 0.3746 0.9313 0.3328 3) Training strategies: There are two learning targets in our MISS framework, i.e., CTR prediction and self-supervised learning. During training, different multi-task learning strategies can be adopted to optimize the two targets. Right here, we compare and analyze the most widely used joint learning and pre-training strategies. Table IX gives the analysis results, where DIN is used as the backbone model. The MISS model 80% 0.7913 0.8779 10.94% 0.7932 0.9107 14.81% 90% 0.7988 0.8814 10.34% 0.7998 0.9152 14.43% 100% 0.8055 0.8867 10.08% 0.8074 0.9180 13.70% trained with joint learning is denoted as MISS-Join, while MISS-Pre learns the CTR prediction target based on the pretrained embeddings by MISS. Both MISS-Join and MISSPre achieve better performances than DIN, and MISS-Joint perform even better than MISS-Pre. In the joint end-to-end training, complementary supervision signals are shared across the two targets, resulting into mutual enhancements that are beyond the reach of pre-training. In this part, we verify that our model can effectively alleviate the label sparsity and label noise problems. 1) Label Sparsity Analysis: As explained in the Introduction, CTR models easily suffer from the label sparsity issue. To verify our model’s effectiveness in alleviating label sparsity, we down-sample the original training set with sampling rate (SR) 90% and 80%, while the validation and testing sets stay unchanged. Notice that the 100% sampling rate means using the original training set. Table X shows the performances with different SR, where the results on the Alipay dataset are omitted for space limitation. We omit the results on the Alipay dataset for space limitation, which have similar trends. It can be found that the performance drops when the labels become sparse (SR decreases), while the relative improvement (RI) gets larger. Thus our MISS model can effectively alleviate the label sparsity problem. 2) Label Noise Analysis: Besides label sparsity, the label noise problem can also be well solved by our proposed MISS model. To check the robustness of MISS to label noise, noises are imposed on the training set by randomly swapping the 0% 0.8055 0.8867 10.08% 0.8074 0.9180 13.70% 10% 0.7768 0.8652 11.38% 0.7775 0.8877 14.16% 20% 0.7413 0.8331 12.38% 0.7384 0.8678 17.52% labels at an indicated proportion (10% and 20%) of samples, while the validation and testing sets stay unchanged. Notice that 0% noise rate (NR) means using the original training set. Due to space limitation, only the results on Amazon-Cds and Amazon-Books datasets are demonstrated in Table XI. It is obvious that the relative improvement of DIN-MISS over DIN grows more signiﬁcant when NR increases. In other words, MISS shows good robustness to label noise. In this paper, we proposed a Multi-Interest Self-Supervised learning (MISS) framework for the CTR prediction task to deal with the label sparsity and label noise issues. In view of the multi-interest characteristics of user behaviors, a CNN-based multi-interest extractor component was proposed to learn the hidden interests while considering both point-wise and unionwise interest representations. Further, another CNN-based multi-interest and multi-feature extractor was also proposed to utilize both inter-item and intra-item interest correlations at the ﬁne-grained feature level. With the help of two random selection functions, augmented views of interest representations can be extracted in consideration of both short-range and longrange interest dependencies. Based on the augmented views of interest representations, two contrastive learning losses effectively transforms interest correlation knowledge into selfsupervision signals. In this way, not only the label sparsity issue gets alleviated by the self-supervision signals, but also the model robustness gets enhanced to shield label noise. Extensive experimental results on three large-scale datasets verify the effectiveness of the proposed MISS framework.