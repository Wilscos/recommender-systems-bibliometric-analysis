Abstract—Scientiﬁc datasets and analysis pipelines are increasingly being shared publicly in the interest of open science. However, mechanisms are lacking to reliably identify which pipelines and datasets can appropriately be used together. Given the increasing number of high-quality public datasets and pipelines, this lack of clear compatibility threatens the ﬁndability and reusability of these resources. We investigate the feasibility of a collaborative ﬁltering system to recommend pipelines and datasets based on provenance records from previous executions. We evaluate our system using datasets and pipelines extracted from the Canadian Open Neuroscience Platform, a national initiative for open neuroscience. The recommendations provided by our system (AUC= 0.83) are signiﬁcantly better than chance and outperform recommendations made by domain experts using their previous knowledge as well as pipeline and dataset descriptions (AUC= 0.63). In particular, domain experts often neglect low-level technical aspects of a pipeline-dataset interaction, such as the level of pre-processing, which are captured by a provenance-based system. We conclude that provenance-based pipeline and dataset recommenders are feasible and beneﬁcial to the sharing and usage of open-science resources. Future work will focus on the collection of more comprehensive provenance traces, and on deploying the system in production. Index Terms—Scientiﬁc dataset, recommendation, collaborative ﬁltering Open science has emerged as a framework to improve the quality of scientiﬁc analyses, ideally leading to Findable, Accessible, Interoperable, and Reusable (FAIR [37]) datasets and analysis pipelines. In neuroscience, our main application domain of interest, platforms have emerged to facilitate the sharing of datasets and pipelines, including OpenNeuro [8], NeuroImaging Tools and Resources Collaboratory (NITRC [12]), and the Canadian Open Neuroscience Platform [2]. However, while public datasets and pipelines proliferate, researchers remain unassisted in creating relevant analyses from these resources that therefore remain largely underutilized. Identifying the set of analysis pipelines that may be relevantly applied to a given dataset, or conversely the list of datasets that may be processed by a given pipeline, remains challenging. In this paper we investigate the development and use of recommender systems to address this issue. The past decades witnessed the adoption of recommender systems as the major technology to help customers navigate the abundant product offerings of online retail platforms. Two main recommending strategies emerged: content-based strategies, which match content-rich item proﬁles with user proﬁles [29], and collaborative ﬁltering, which recommends , Tristan Glatard items to a given user based on those selected by users with similar preferences [30]. Both strategies have been successfully applied and have their own strengths and weaknesses. While these techniques have largely been employed in the space of retail content delivery, we evaluate the feasibility of matching scientiﬁc pipelines and datasets using existing recommender system techniques. We focus on collaborative ﬁltering approaches, since content-based methods would require extensive annotations about pipelines and datasets which are not broadly available despite on-going efforts [23], [31], [32]. However, collaborative ﬁltering requires an afﬁnity measure between users and items. To deﬁne such a measure between pipelines and datasets, we rely on past execution outcomes (e.g. exit status) available through provenance records. The compatibility of a pipeline with a dataset depends on semantic, syntactic, and infrastructural factors. Semantic factors refer to the content of datasets and analyses. For instance, a pipeline developed for the segmentation of brain Magnetic Resonance Images (MRIs) would not produce meaningful results on other image types. Syntactic factors refer to ﬁle formats and dataset organization. For instance, brain images are commonly stored using the NIfTI [20] or MINC [36] formats and pipelines developed to ingest one format may not apply to others. In addition, the multiple ﬁles and directories composing a dataset are increasingly structured using the Brain Imaging Data Structure (BIDS [9]) which is required by some pipelines while other ones use their own structure or neglect structure entirely and require explicit pointers to speciﬁc ﬁles. Finally, infrastructural factors refer to the availability of pipelines and datasets which can be functionally deployed. For instance, some pipelines may require the loading of an entire dataset in memory, which may not be feasible for large datasets. All these levels must be considered in pipeline or dataset recommendations. Provenance is a key concept in computational analyses, referring to the detailed description of data transformations. Provenance records typically include information about the input data, the analysis software and parameters, the execution context and ﬁnally, the execution outcomes. In neuroimaging, the NeuroImaging Data Model (NIDM [24]) project proposed domain-speciﬁc formats and tools based on standards from the W3C PROV working group [26]. In its current form, our recommender system merely relies on execution outcomes (summarized via “exit codes") extracted from these or similar provenance records. To summarize, this paper makes the following contributions: system for scientiﬁc pipelines and datasets; Canadian Open Neuroscience Platform; dations. Systems have been used to recommend software in various contexts such as workﬂow composition and algorithm selection. However, as explained below, our context is slightly different since we aim at recommending analyses that are applicable to a given dataset. In neuroimaging, existing platforms focus on ﬁnding and reusing pipelines and datasets but do not include any recommender engine. Recommender systems have been described to assist users with workﬂow composition, in particular to identify candidate software components for a given workﬂow. For example, the Galaxy tool recommender [16] recommends possible workﬂow components using a deep learning model trained on 18, 000 bioinformatics workﬂows from the European Galaxy server. Recommendations depend on the deﬁnition and organization of all the tools in the workﬂow (so-called “higher-order workﬂow dependencies") instead of focusing only on the most recently added workﬂow components. Workﬂow dependencies are learned using Recurrent Neural Networks, resulting in a mean accuracy of 98%. The system is available for Galaxy users as an extension. Previous approaches to assisted workﬂow composition included loose programming, initially proposed in the PROPHETS system [18], [19], [27]. Loose programming enables workﬂow developers to program using concepts rather than accurate procedural code. Loose programming exposes to the workﬂow developers semantic annotations describing the functionalities of workﬂow components. Workﬂow developers can then assemble such components without having to care about correct typing, interface compatibility, platform parameters or other technical details that are taken care of through subsequent validations. PROPHETS was applied to various bioinformatics use cases, including mass spectrometry-based proteomics [28]. The WINGS (Workﬂow INstance Generation and Specialization) system [6] uses AI planning and semantic reasoners to assist users in creating workﬂows while validating that the workﬂows comply with the requirements of the software components and datasets. WINGS can reason about the constraints of the components and the characteristics of the data and propagate them through the workﬂow structure. These approaches assist users by matching workﬂow components together. Instead, in our context, the workﬂows (or pipelines) are already available and need to be matched to relevant datasets. Conversely, relevant analysis workﬂows need to be recommended for a given dataset. Other recommender systems aim at selecting speciﬁc algorithms for a given problem. For instance, the Oracle machinelearning toolkit [25] selects machine-learning algorithms and models for classiﬁcation and regression problems. Algorithm selection uses advanced machine learning techniques to automatically rank the best algorithms for a dataset. Model selection identiﬁes the best hyperparameters to maximize a given prediction performance score. Another recent example is PennAI [17], a platform that recommends suitable models given a supervised classiﬁcation problem. The platform was evaluated on 165 classiﬁcation problems. Results showed that matrix factorization-based recommendation systems outperform meta-learning methods. In addition, Dyad ranking [34], [35] represents the algorithm and problem instance by a feature vector, and selects the best feature vectors using machine learning. The training dataset is a set of dyads ranked according to a speciﬁc preference relation. The dyad ranker learns using the neuralnetwork-based PLNet [33] algorithm. Results show that this approach outperforms many algorithm selectors while using less computation. All these approaches assume the existence of an objective function such as F1 score, accuracy or mean average error to compare algorithms or models that are known to apply to the problem. Instead, our goal is not to compare pipelines with each other but to predict if a given pipeline will work on a given dataset. From a methodological point of view, algorithm selection techniques are mostly content-based while we will adopt a collaborative ﬁltering approach. Several platforms have been developed to facilitate the sharing of tools and datasets in neuroimaging. For instance, NITRC [12] provides a richly-annotated catalog of tools and datasets that can be processed in the NITRC computing environment. In OpenNeuro [22], datasets complying to the BIDS data structure can be publicly shared and processed using BIDS apps. Finally, the Neuroscience Information Framework (NIF [5]) is a powerful search engine for neuroscience software and data. While all these platforms provide substantial services for data and tool sharing, they do not seem to include any system to assist users in the matching of pipelines and datasets. Matching relevant pipelines and datasets would also enable the automated triggering of data processing when new pipelines or datasets become available. The goal of our recommender system is to predict if a data processing pipeline will successfully run on a given dataset. Predictions are obtained from provenance records created from previous pipeline executions. Figure 1 presents an overview of our system that is detailed in the remainder of this section with our experimental methodology. Consistently with our motivating use case, we focused on the 64 neuroscience pipelines available in the CONP as of October 2020. The available pipelines have 23 different tags which shows an overview of the type of data and analyses supported by these pipelines. The most popular tags are “neuroinformatics", “mri", “fmri", “bioinformatics" and “dmri". These pipelines are command-line tools described by Boutiques descriptors [7] and containerized using Docker or Singularity, which makes them portable across a wide range of infrastructures, including local workstations, clusters, and clouds [13]. Each pipeline descriptor is stored on the Zenodo research archive [4] and identiﬁed by a permanent Digital Object Identiﬁer (DOI). Through the Boutiques command line, the pipelines can be validated, installed, published, and executed. When the execution completes, Boutiques creates a JSON provenance record containing a summary of the execution process including the pipeline DOI, input and output ﬁle hashes, parameter values, and exit code. Our recommendation system uses such provenance records to track the outcome of pipeline executions on a given dataset. We used the 42 datasets available in the CONP as of October 2020, representing a total of 2,807,267 ﬁles, 3,078 GB, and 6,330 subjects. These datasets are coming from various sources distributed in Canada and abroad. The most frequent dataset keywords are “brain imaging", “human pain", “MRI", “neuroimaging" and “structural MRI". Datasets are made available through DataLad [1], a Git-based framework that provides a uniform and version-controlled view of distributed storage. A DataLad dataset is a particular type of Git repository that stores data ﬁles using git-annex [10]. Git objects contain hashes and URLs of the data ﬁles but not the data itself. With the DataLad client, users can download speciﬁc ﬁles and track their versions. Using DataLad, our system matches provenance records to particular datasets through ﬁle hashes. In addition, speciﬁc ﬁles from a given dataset can be downloaded on demand without having to download the entire dataset. In some cases, minor adjustments such as ﬁle renamings or exclusions (on 8/22 of tested datasets) to the organization of BIDS datasets were performed to make them fully compliant with the standard speciﬁcation and reduce the processing time of our experiments by excluding some subjects. We did not apply any data type conversion since in neuroimaging they are known to create issues when not done properly [21]. To build an expert reference, we recruited 13 experts among graduate students, software developers and data engineers at the Canadian Open Neuroscience Platform. Since the number of pipeline-dataset pairs to evaluate was beyond the amount that could reasonably be evaluated by a human expert (2,688), we split our survey in two steps. In the ﬁrst survey (conﬁdence survey), experts were asked to rate their knowledge and conﬁdence about each pipeline and dataset on a 4-level scale: no knowledge, some knowledge, good knowledge, and expert knowledge. In the second survey (prediction survey), experts were ask to predict the execution outcome (success or failure) for all pipeline-dataset pairs in which they had indicated good or expert knowledge in the ﬁrst survey for both the pipeline and the dataset. Both surveys included links to dataset and pipeline description pages on the CONP portal, such that the experts were able to consult their detailed descriptions. Survey forms are available on GitHub for more information. Surveys happened between November 2020 and March 2021. Similar to pipeline descriptors, Boutiques provenance records can be published to Zenodo, which makes them publicly and permanently accessible. We created provenance records for each pipeline-dataset pair for which at least one expert predicted successful execution. To generate a provenance record we executed a pipeline using an invocation ﬁle including all required parameters for that pipeline. The instructions for generating the invocations are available through Boutiques for each pipeline. We mostly used default parameters and created minimal invocations, however, to execute some pipelines we requested domain experts’ assistance to generate a working invocation ﬁle. The provenance records were entered in a database together with the ﬁle hashes of all datasets retrieved using DataLad. From this database, we generated (pipeline, dataset, execution outcome) triplets to use in our recommender system. Collaborative ﬁltering predicts the rating of a given item (dataset in our case) by a given user (pipeline in our case) from the “utility matrix" containing previous ratings [30]. Two approaches are commonly used: neighbor-based methods [14] and latent factor models [3], [15]. Neighbor-based collaborative ﬁltering, also known as k-nearest neighbors, identiﬁes like-minded users or similar items based on the similarity of entries in the utility matrix. In contrast, latent factor models, the method that we used, represent items and users in a latent space obtained from a factorization of the utility matrix r in a user matrix p and an item matrix q. The factorization is learned by minimizing the least square error between the available ratings and the ratings predicted by the factorization: where κ is the set of user-item pairs for which r, the rating of item i by user u, is available. Unknown elements of the utility matrix are predicted by the dot product of the corresponding vectors in q and p. In our case, pipelines represent users, datasets represent items, execution outcomes represent ratings, and κ is the set of pipeline-dataset pairs for which provenance records are available. Two minimization methods are commonly used: stochastic gradient descent and alternating least squares (ALS). We used ALS as implemented in the Apache Spark Machine Learning library (spark.ml) version 3.1.2. We evaluated our approach in two different ways. First, we compared the expert predictions to real execution outcomes extracted from provenance records. Second, we evaluated the accuracy of our recommender system through 10-fold cross validation. The data and code required to reproduce our results are available at https://github.com/big-data-lab-team/ paper-pipelines-datasets-recommender. Figure 2a shows expert predictions of pipeline-dataset execution outcomes. The average number of expert predictions by pipeline-dataset pair was 1.39. Only the 32/64 pipelines for which at least one dataset was predicted to be successfully processed by at least one expert are represented. Similarly, only the 22/42 datasets for which at least one pipeline was predicted to be successfully executed by at least one expert are represented. Pipeline and dataset names are reported in Tables I and II. Entries in these tables are clickable for more information. Out of a total of 704 pipeline-dataset pairs, the execution outcome of 37% was predicted as failed by all the experts (white cells), the outcome of 25% was predicted as successful by all the experts (dark green cells), 21% were not known with enough conﬁdence by any expert (gray cells), and the outcome of the remaining ones was predicted as successful by some experts and as failed by other experts. The large fraction of pipeline-dataset pairs unknown to any expert reinforces the motivation for an automated recommender system. Figure 2b shows the actual execution outcome for all the pipeline-dataset pairs for which at least one expert predicted a successful execution outcome (green cells in Figure 2a). Out of 288 executed pairs, 134 were successful and 154 failed. Important discrepancies are observed between expert predictions and actual executions. Overall, 53% of the executions that were predicted successful by at least one expert failed in reality (red cells in Figure 2b). In addition, the average expert conﬁdence was found to be signiﬁcantly higher for failed executions than for successful ones (p <0.002, Figure 3), which is unexpected. Therefore, expert predictions seem to be largely unreliable. Note that we used the experts’ predictions as a baseline for prediction performance comparison rather than a ground truth on the execution outcome of a given pipeline on a given dataset. Many practical reasons explain the observed discrepancy between expert predictions and pipeline executions (Table III). First, some datasets did not match the format required by the tested pipeline. For instance, P(fsl-ﬁrst) requires anatomical images in the NIfTI ﬁle format, however, some datasets such as D(multicenter-phantom) contain anatomical images in the MINC format. In addition, ﬁve pipelines (P, P, P, Pand P) failed due to unresolved pre-processing requirements. We identiﬁed four types of dependencies between pipelines. Type A refers to pipelines such as P(fsl-bet) or P(fsl-anat) that can be executed directly on the tested dataset. Type B refers to Fig. 3: Expert conﬁdence by actual execution outcome. pipelines that require the tested dataset as well as the results of the application of another pipeline on the tested dataset. For example, P(oneVoxel) requires a binary mask for its input image that is created by another pipeline. Type C refers to pipelines requiring inputs from more than one dataset. For instance, P(ANTS Brain Extraction) and P(ANTS Cortical Thickness) require external templates and segmentations obtained outside of the dataset. Type D refers to pipelines that process data derived from the tested dataset but not the tested dataset directly. For instance, P(fsl-probtrackx2) performs probabilistic tractography on the output of bedpostx, a pipeline that no expert predicted to run successfully on any dataset. In a Type D conﬁguration, the pipeline is considered to not successfully execute on the dataset. Fig. 4: ROC curves of experts and recommender system predictions. CONP, due to various issues. For example, D(CNeuromod) is currently not downloadable in CONP due to technical issues. Finally, a set of executions failed for other reasons including issues in Boutiques pipeline descriptors or corrupted datasets. Overall, experts seem to have neglected such practical failure reasons. In general, experts tend to rely on their semantic understanding of the interactions between pipelines and datasets (for instance, a given pipeline may operate on fMRI data), while in practice, pipeline executions depend on the lower-level syntactical and infrastructural details mentioned previously. We evaluated the latent-factor model using 10-fold cross validation on the pipeline execution matrix in Figure 2b. We varied the threshold used to round predicted values to 1 (failed execution) or 2 (successful execution), resulting in the Receiver Operating Characteristic (ROC) curve in Figure 4. We obtained the ROC curve of experts predictions by predicting execution outcomes using various thresholds in the fraction of experts predicting successful execution. The area under curve (AUC) of our recommender system was 0.83, showing that our model is signiﬁcantly better than chance (AUC=0.5) and expert predictions (AUC=0.63). For instance, given a rounding threshold of 1.2 (black dot in the ROC curve), out of 10 pipelines recommended by our system for a particular dataset, 8 would be applicable to the dataset while only 2 would not. This good performance was expected to some degree given that the pipeline execution matrix in Figure 2b bears some sort of structure. A more random utility matrix would obviously be more difﬁcult to predict. The performance of the proposed recommender system is substantially higher than chance and expert predictions. Predicting the successful execution outcome of a pipeline on a given dataset is a difﬁcult task for a human expert as it requires a comprehensive knowledge and understanding of the technical infrastructure, pipeline syntactical requirements, analysis types, data formats, and data semantic types. In practice, it is common for human experts to only master some parts of this environment. For example, the successful processing of BIDS datasets by BIDS applications requires datasets to pass BIDS validation, which can hardly be guessed from a high-level overview of the dataset. In addition, pipelines or data transfers may fail for technical reasons unknown to the experts. Therefore, automated recommender systems based on provenance records have a strong added value compared to human recommendations. Our experiments were conducted using one of the largest data and pipeline sharing platform in neuroscience. Other platforms such as NITRC [12] contain larger collections of pipelines and datasets, but they are not available through a consistent interface such as Boutiques and DataLad, which would make such an experiment hardly feasible there. The described system architecture could potentially scale widely beyond the speciﬁc context of CONP, as it only relies on ﬁle hashes and therefore does not require data sharing agreements or extensive data storage. In addition, the recommender system could leverage provenance records produced by multiple platforms provided that they are shared in some way. DataLad would detect possible duplication among datasets through ﬁle hashes. The framework is also expected to apply to other disciplines than neuroimaging although changes in the technical context — such as datasets being stored in databases instead of ﬁles — may require adaptations. In production conditions, the recommender system would rely on provenance records of pipeline executions launched by arbitrary users. While this would increase the amount of data available, potentially resulting in more accurate recommendations, it would also come with challenges. For instance, while our framework models the execution outcome of a given pipeline-dataset pair as a binary variable (success or failure), different execution outcomes may be produced for a given pipeline-dataset pair, due to different parametrizations or analysis types. Besides, analyses launched by less experimented users may produce misleading provenance records. We anticipate that the recommender could be conﬁgured to use implicit feedback to address this issue [11]. The successful execution of a pipeline on a given dataset does not necessarily imply that results are meaningful. Relying exclusively on execution exit statuses therefore requires that users producing execution records mostly execute meaningful experiments, which may not always be the case. Taking into account the popularity of the datasets derived from a given provenance record in the recommendations might help address this issue. Collaborative ﬁltering predicts the execution outcome of a given pipeline on a given dataset with usable accuracy (AUC=0.83) in the context of the Canadian Open Neuroscience Platform. The performance achieved by our system outperforms human expert recommendations, presumably due to syntactical and infrastructural factors neglected by human experts. Future work will focus on the deployment of such a system in production conditions, which will require dealing with less reliable provenance records. The framework could be extended by considering pipelines and datasets at a ﬁner granularity. Pipelines can often be used in different ways depending on their parametrization. Different parametrizations could be identiﬁed in the provenance records and recommended accordingly for speciﬁc datasets. Besides, datasets often consist of multiple sub-parts corresponding to different subjects or data types. A recommender system could be designed to recommend analyses for such sub-parts, resulting in more speciﬁc recommendations.