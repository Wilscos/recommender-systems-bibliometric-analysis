In e-commerce, the watchlist enables users to track items over time and has emerged as a primary feature, playing an important role in users’ shopping journey. Watchlist items typically have multiple attributes whose values may change over time (e.g., price, quantity). Since many users accumulate dozens of items on their watchlist, and since shopping intents change over time, recommending the top watchlist items in a given context can be valuable. In this work, we study the watchlist functionality in e-commerce and introduce a novel watchlist recommendation task. Our goal is to prioritize which watchlist items the user should pay attention to next by predicting the next items the user will click. We cast this task as a specialized sequential recommendation task and discuss its characteristics. Our proposed recommendation model, Trans2D, is built on top of the Transformer architecture, where we further suggest a novel extended attention mechanism (Attention2D) that allows to learn complex item-item, attribute-attribute and item-attribute patterns from sequential-data with multiple item attributes. Using a large-scale watchlist dataset from eBay, we evaluate our proposed model, where we demonstrate its superiority compared to multiple state-of-the-art baselines, many of which are adapted for this task. • Applied computing → Electronic commerce. Watchlist, Sequential-Model, Transformers, E-Commerce ACM Reference Format: Uriel Singer, Haggai Roitman, Yotam Eshel, Alexander Nus, Ido Guy, Or Levi, Idan Hasson, Eliyahu Kiperwasser. 2022. Sequential Modeling with Multiple Attributes for Watchlist Recommendation in E-Commerce. In WSDM ’22: The 15th ACM International Conference on Web Search and Data Mining, February 21–25, 2022, Phoenix, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn The watchlist, a collection of items to track or follow over time, has become a prominent feature in a variety of online applications spanning multiple domains, including news reading, television watching, and stock trading. One domain in which watchlists have become especially popular is e-commerce, where they allow users to create personalized collections of items they consider to purchase and save them in their user account for future reference. Saving items in the watchlist allows users to track a variety of dynamic characteristics over relatively long period of times. These include the price and related deals, shipping costs, delivery options, etc. A user’s watchlist can be quite dynamic. At any moment, the user may choose to revise her watchlist by adding new items (e.g., after viewing an item’s page) or removing existing items (e.g., the user has lost her interest in an item). Over time, item attributes may change (e.g., an item’s price has dropped) or items may become invalid for recommendation (e.g., an item has been sold out). User’s latest shopping journey may change as well, as she may show interest in other items, categories or domains. All in all, in between two consecutive interactions with the watchlist, the user may change her priority of which next item to pay attention to. Considering the fact that the watchlist of any user may include dozens of items, it may be quite overwhelming for users to keep track of all changes and opportunities related to their watchlist items. Given a limited number of items that can be displayed to the user (e.g., 2-3 items on eBay’s mobile homepage watchlist module) and the dynamic nature of watchlist items, our goal is, therefore, to help users prioritize which watchlist items they should pay attention to next. We cast the watchlist recommendation (hereinafter abbreviated as WLR) task as a specialized sequential recommendation (SR) task. Given the historical interactions of a user with items, the goal of the SR task is to predict the next item the user will interact with [28]. While both tasks are related to each other, we next identify two important characteristics of the WLR task. The rst characteristic of the WLR task is that, at every possible moment in time, only a subset of items,explicitly chosento be tracked by the user prior to recommendation time, should be considered. Moreover, the set of items in a given user’s watchlist may change from one recommendation time to another. Yet, most previous SR studies assume that, the user’s next interaction may be withany possibleitem in the catalog. Therefore, the main focus in such works is to predict the next item’sidentity. The WLR task, on the other hand, aims to estimate the user’s attention to watchlist items by predicting theclick likelihoodof each item in the user’s watchlist. Noting that watchlist datasets may be highly sparse, as in our case, representing items solely by their identity usually leads to over-t. A better alternative is to represent items by their attributes. Yet, to the best of our knowledge, only few previous SR works can consider attribute-rich inputs both during training and prediction. The second characteristic of the task lies in the important observation that, possible shifts in the user’s preferences towards her watched items may be implied by her recently viewed items (RVIs). For example, a user that tracks a given item in her watchlist, may explore alternative items (possibly not in her watchlist) from the same seller or category, prior to her decision to interact with the tracked item (e.g., the watched item price is more attractive). Trying to handle the unique challenges imposed by the WLR task, we design an extension to the Transformer model [26]. Our proposed Transformer model,Trans2D, employs a novel attention mechanism (termed Attention2D) that allows to learn complex preferential patterns from historical sequences of user-item interactions accompanied with a rich and dynamic set of attributes. The proposed attention mechanism is designed to be highly eective, and requires only a small addition to the model’s parameters. Using a large-scale user watchlist dataset from eBay, we evaluate the proposed model. We show that, by employing the novel Attention2D mechanism within the Transformer model, our model can achieve superior recommendation performance, compared to a multitude of state-of-the-art SR models, most of which we adapt to this unique task. The watchlist recommendation (WLR) task can be basically viewed as a specialization of the sequential recommendation (SR) task. The majority of previous SR works assume that the input sequence includes only item-IDs [28] and focus on predicting the identity of the next item that the user may interact with [28]. Notable works span from Markov-Chain (MC) [18] and translation-based [7] models that aim to capture high-order relationships between users and items, to works that employ various deep-learning models that capture user’s evolving preferences such as RNNs (e.g., GRU [8,23]), CNNs (e.g., [24,25]), and more recent works that utilize Transformers (e.g., SASRec [10], BERT4Rec [22] and SSE-PT [29]). Similar to [10,22,29], we also utilize the Transformer architecture. Yet, compared to existing works that train models to predict the next item-ID, our model is trained to rank watchlist items according to their click likelihood. The main disadvantage of models that focus on item-ID only inputs is their inability to handle datasets having items with high sparsity and item cold-start issues. Utilizing rich item attributes, therefore, becomes extremely essential for overcoming such limitations, and specically in our task. By sharing attribute-representations among items, such an approach can better handle sparse data and item cold-start. Various approaches for obtaining rich attribute-based item representations duringtraining-timehave been studied so far. This includes a variety of attribute representations aggregation methods [2,13,19,20] (e.g., average, sum, element-wise multiplication, etc.), concatenation [3,13,31], parallel processing [9,16], usage of attention-based Transformer layer [30] and graph-based representations [27]. Furthermore, attributes have been utilized for introducing additional mask-based pre-training tasks [32] (e.g., predict an item’s attribute). Yet, during prediction, leveraging the attributes is not trivial, as the entire catalog should be considered. Indeed, all aforementioned works still utilize only item-IDs for prediction. For example, the S3 [32] model, one of the leading self-supervised SR models, supports only static attributes, which are only considered during its pre-training phase. Hence, extending existing works for handling the WLR task, which requires to consider also item attributes at prediction time, is extremely dicult, and in some cases, is just impossible. As we shall demonstrate in our evaluation (Section 4), applying those previous works [3,13,19,30] that can be extended to handle the WLR task (some with a lot of eort and modication), results in inferior recommendation performance compared to our proposed model. Few related works allow to include item attributes as an additional context that can be utilized for representing items both during training and prediction time. Such works either extend basic deepsequential models (RNNs [2,21] and Transformers [5]) with context that encodes attribute-rich features, or combine item-context with sequential data using Factorization Machines [4, 14]. Our proposed model is also capable of processing sequential inputs having multiple item attributes. To this end, our model employs a modied Transformer architecture with a novel attention mechanism (Attention2D) that allows to process complete sequences of 2D-arrays of data without the need to pre-process items (e.g., aggregate over attributes) in the embedding layer (as still need to be done in all previous works). This allows our model, with a slight expense of parameters, to capture highly complex and diverse preference signals, which are preserved until the prediction phase. The WLR task can be modeled as a special case of the sequential recommendation (SR) task [28], with the distinction that, at every possible moment, there is only a limited, yet dynamic, valid set of watched items that need to be considered. Using the user’s historical interactions with watchlist items and her recently viewed item pages (RVIs), our goal is, therefore, to predict which items in the current user’s watchlist will be clicked. A user’s click on a watchlist item usually implies that the user has regained her interest in that item. Moreover, we wish to leverage the fact that watched items are dynamic, having attributes whose values may change over time (e.g., price), and therefore, may imply on a change in user’s preferences. Formally, letIbe the set of all recommendable items. For a given user𝑢, letWL⊆ Idenote the set of items in the user’s watchlist priorto the time of𝑡-thclickon any of the watchlist items, termed also hereinafter a “watchlist-snapshot”. We further denote𝑚= |WL|the number of items in a given snapshot. A user may click on any item inWL, which leads her to the item’s page; also termed a “view item” (VI) event. VI events may also occurindependently of items in the user’s watchlist; e.g., the user views an item inI following some search result or directed to the item’s page through some other organic recommendation module, etc. LetVI⊆ I, with 𝑛= |VI|, denote a possible sequence of item pages viewed by Figure 1: Watchlist recommendation model (Trans2D) using Transformer with Attention2D layer. The model has three main parts: Embedding layer (bottom-side), Attention2D layer (middle-side) and Prediction layer (upper-right side) The ScaledDotProductAttention2D component is further illustrated in the upper-left side. user𝑢between the𝑡 −1-th and𝑡-th clicks on her watchlist items, further termed a “view-snapshot”. We next denoteS(𝑡 )– the user’s history prior to her𝑡-th interaction with the watchlist.S(𝑡 )is given by the sequence of the user’s previous clicks on watchlist items and items whose page the user has viewed in between each pair of consecutive clicks, i.e.: S(𝑡 ) = 𝑤|{z}, 𝑣, . . . , 𝑣, . . . , 𝑤|{z}, 𝑣, . . . , 𝑣. Here,𝑤denotes a single user’s click on some watchlist item, while𝑣denotes an item page viewed by the user. We note that, every watchlist item clicked by the user leads to the item’s page, hence: 𝑤= 𝑣(∀𝑙 : 1 ≤ 𝑙 ≤ 𝑡 − 1). Using the above denitions, for any given user historyS(𝑡 )and new watchlist snapshotWLto consider, our goal is to predict which items inWLthe user is mostly likely to click next. Therefore, at service time, we wish to recommend to the user the top-𝑘items in WLwith the highest click likelihood. Our WLR model,Trans2D, extends the Transformer [26] architecture for handling sequences with items that include several categorical attributes as in the case of watchlist items. Using such an architecture allows us to capture high-order dependencies among items and their attributes with a minimum loss of semantics and a reasonable model complexity. The model’s network architecture is depicted in Figure 1. The network includes three main layers, namely, Embedding layer, Attention2D layer and Prediction layer. We next describe the details of each layer. A common modeling choice in most related literature [28], is to represent user’s sequenceS(𝑡 )as the sequence of item-IDs. LetÍ 𝑁 =(1+𝑛)denote the total items inS(𝑡 ). These item-IDs are typically embedded into a corresponding sequence of𝑑-dimensional vectorsE = (e, . . . , e);∀𝑖:e∈ Rusing an embedding dictionary and serve as input representation to neural-network models. However, such a representation has two main disadvantages. Firstly, it is prone to cold-start problems as items that never appear during training must be discarded or embedded as an ‘unknown’ item. Secondly, items that only appear once or a few times during training are at high risk of over-tting by the training process memorizing their spurious labels. Such sparsity issues are particularly present in our data (as will be discussed in Section 4.1), prompting us to represent an item as a collection of attributes, rather than an itemID. Such an approach is less prone to sparsity and over-tting, as the model learns to generalize based on item-attributes. Following our choice for item representation, we now assume that each item inS(𝑡 )is represented by𝐶attributes:[𝑎, . . . , 𝑎]. Therefore, the input to our model is a 2D-array of item attributes S(𝑡 ) = [𝑎]; where𝑖 ∈ [1, . . . , 𝑁 ], 𝑗 ∈ [1, . . . , 𝐶]. Hereinafter, we refer to the rst axis ofS(𝑡 )as the sequence (item) dimension and its second axis as the channel (attribute) dimension. The rst layer of our model is an embedding layer with keys for all possible attribute values in our data. Therefore, by applying this layer on the user’s sequenceS(𝑡 ), we obtain a 2D-array of embedding vectorsE = [𝑒]= 𝐸𝑚𝑏𝑑 ([𝑎]). Note that, since each item attribute embedding by itself is a𝑑-dimensional vector, E ∈ Ris a 3D-array.Eis next fed into a specially modied Transformer layer which is our model’s main processing layer. Most existing SR models (e.g., [8,10,22,24]) take a sequence of vectors as input. However, in our case, we do not have a 1D-sequence of vectors but rather a 2D-array representing a sequence of ordered attribute collections. As Transformers were shown to demonstrate state-of-the-art performance in handling sequential data in general [26] and sequential recommendation data specically [5,10,22,29,32], we choose to employ Transformers and extend the Attention Mechanism of the Transformer to handle 2D-input sequences, rather than a 1D-input sequence. Since the vanilla attention model employed by existing Transformer architectures cannot be directly used for 2D-data, dierent pooling techniques were devised to reduce the channel dimension and produce a 1D-sequence as an input for the Transformer. For example, in BERT [11], token, position and sentence embeddings are summed before being fed into the Transformer Encoder. Other alternatives to summation were explored such as averaging, concatenation, or a secondary Transformer layer (e.g., [3,30]). Yet, most of these approaches suer from a reduction in data representation happening prior to applying the attention workhorse. This makes it harder for the attention mechanism to learn rich attention patterns. An exception is the concatenation approach that sidesteps this problem but produces very long vectors, which in turn, requires Transformer layers to have a large input dimension making them needlessly large. As an alternative, we next proposeAttention2D– a dedicated novel attention block that is natively able to handle 2Dinputs in an ecient way. Such an extended attention mechanism will allow us to learn better attention patterns without signicantly increasing the architecture size. We now describe in detail our modied attention block, referred to as Attention2D. For comparison to the vanilla Transformer model, we encourage the reader to refer to [26]. An attention block maps a query (Q) and a set of key (K) - value (V) pairs to an output, where the query, keys, values, and output are all vectors [26]. The Attention2D block in turn receives a 2D-array of vectors, representing a sequence of ordered item attribute collections and computes the attention between all item-attributes while enabling dierent semantics per channel to better capture both channel interactions and dierences. This way, one attribute of a specic item can inuence another attribute of a dierent item regardless of other attributes, making it possible to learn high-order dependencies between dierent items and attributes. For instance, in our usecase, the price preferences of a user, captured by the price channel can be computed using attention both to previously clicked item prices and to past viewed items of sellers correlated with pricey or cheap items. The full Attention2D block is illustrated in Figure 1. We now provide a detailed description of its implementation. 3.5.1 Linear2D. The input to our model is a 2D-array of input vectors, generally denoted hereinafterX = [x](and specically in our model’s inputX = E). Our model requires operations acting on such 2D-arrays throughout. To this end, we rst dene two extensions of standard linear neural-network layers: whereW = [𝑊, . . . ,𝑊];𝑏 = [𝑏, . . . , 𝑏]andW∈ R;𝑏∈ Rare trainable parameters per channel 1≤ 𝑗 ≤ 𝐶. These operations dene a linear layer (with or without bias) with dierent trainable parameters per channel and shall allow the model to facilitate interactions between dierent channels while preserving the unique semantics of each channel. We start by mapping our input (E) into three 2D-arrays of query Q = [q], keyK = [k]and valueV = [v]vectors corresponding to each input. We do so by applying three Linear2D layers: Q = 𝐿𝑖𝑛𝑒𝑎𝑟2𝐷(E) ; K = 𝐿𝑖𝑛𝑒𝑎𝑟2𝐷(E) ; V = 𝐿𝑖𝑛𝑒𝑎𝑟2𝐷(E) 3.5.2 ScaledDotProductAention2D. We next describe the core part of our proposed attention mechanism, which is used to compute a transformed representation of our 2D-input array. This part is further illustrated on the upper-left side of Figure 1. For brevity of explanation, we consider a single query vector[q]∈ Qand key vector[k]∈ K, while in practice, this operation is performed for every 𝑖, 𝑖∈ [1, . . . , 𝑁 ] and 𝑗, 𝑗∈ [1, . . . , 𝐶]. First, we compute the following three attention scores (where𝑇 here denotes the matrix transpose operation): A= [q]· [k]; A=[q]· [k]; where: • Ais a 4D-array of attention scores between all inputs as the dot-product between every query vector and every key vector. This attention corresponds to the attention scores in a vanilla attention layer between any two tokens, yet with the distinction that in our case we have 2D-inputs. • Ais a matrix of attention scores between whole items in our input, computed as the marginal attention over all channels. This attention captures the importance of items to each other regardless of a particular channel. • Ais a matrix of attention scores between channels in our input, computed as the marginal attention over all items. This attention captures the importance of channels to each other regardless of a particular item. The attention scores are further combined using their weighted where𝛼, 𝛼, 𝛼are learned scalars, denoting the relative importance of each attention variant, respectively. The result of this step is, therefore, a 4D-arrayAof attention scores from any position(𝑖, 𝑗)to every position(𝑖, 𝑗). Similarly to [26], we apply a softmax function overA,with a scaling factor√ of𝑑, so that our scores will sum up to 1. We then compute the nal transformed output[o]as a weighted average over all value vectors using the computed attention scores: P = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥A√; [o]=P[𝑣] Finally, we can dene our Attention2D layer as an attention layer that receives a triplet of(Q, K, V)2D-arrays of query, key and value vectors as its input and outputs a 2D-array transformed vectors: 𝑆𝑐𝑎𝑙𝑒𝑑𝐷𝑜𝑡𝑃𝑟𝑜𝑑𝑢𝑐𝑡𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛2𝐷 (Q, K, V) = [o](3) The Attention2D layer allows each item-attribute to attend to all possible item-attributes in the input. To prevent unwanted attendance (such as accessing future information in the sequence), we mask it out by forcing the relevant (future) attention values to −∞ before the softmax operation. 3.5.3 Multi-Head Aention2D. Similarly to [26], instead of performing a single attention function, we can applyℎdierent functions (“Attention-Heads”) over dierent sub-spaces of the queries, keys, and values. This allows to diversify the attention patterns that can be learned, helping to boost performance. For instance, one attention head can learn to focus on price-seller patterns, while another on dierent price-condition patterns. We facilitate this by applyingℎdierent Linear2D layers, resulting inℎtriplets of (Q, K, V); on each, we apply the aforementioned ScaledDotProductAttention2D layer. We then concatenate allℎresults and project it back to a 𝑑-dimensional vector using another Linear2D layer: 𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑2𝐷 (Q, K, V) = 𝐿𝑖𝑛𝑒𝑎𝑟2𝐷(𝐶𝑜𝑛𝑐𝑎𝑡 (ℎ𝑒𝑎𝑑, ..., ℎ𝑒𝑎𝑑)) Q= 𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷(Q) ; K= 𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷(K) ; V= 𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷(V) 3.5.4 Position-wise Feed-Forward Networks. We nally allow the Attention2D output to go through additional Linear2D layers in order to be able to cancel out unwanted applied alignments and learn additional complex representations: 𝐹 𝐹 𝑁 (X) = 𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷(𝑅𝑒𝐿𝑈 (𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷(X))) 3.5.5 Full Layer implementation. A full Attention2D block is combined from a MultiHead2D layer and then a FFN layer, where a residual connection [6] is applied around both, by adding the input to the output and applying a layer normalization [1], as follows: 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛2𝐷(X) = 𝐿𝑎𝑦𝑒𝑟 𝑁𝑜𝑟𝑚(X+ 𝐹 𝐹 𝑁 (X)), After applying the Attention2D block, we end up with a transformed 2D-array of vectorsE= 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛2𝐷 (E), having the same shape as the original input. Therefore, Attention2D layers can be further stacked one on the other to achieve deep attention networks. The nal stage in our model is label (click/no-click) prediction. Let 𝑤∈ WLbe an item in the user’s watchlist whose click likelihood we wish to predict. To this end, as a rst step, we append𝑤to S(𝑡 ). We then feed the extended sequence to the Transformer and obtainE, the transformed 2D-array representation based on our Attention2D block (see again Eq 5). We predict the click likelihood of item𝑤based on its transformed output𝑒∈ R. For that, we rst obtain a single representation of it by applying𝑃𝑜𝑜𝑙 (𝑒)– a pooling layer over the channel dimension in𝑒. While many pooling options may be considered, we use a simple average pooling. This is similar to the channel dimension reduction discussed in Section 3.4, but instead of applying the pooling before the attention mechanism, we are able to apply the pooling after the Attention2D block. This makes it possible for the Attention2D block to capture better attention patterns between dierent items and attributes. Finally, we obtain the item’s predicted label (denotedˆ𝑦) by applying a simple fully connected layer on𝑃𝑜𝑜𝑙 (𝑒)to a single neuron representing the item’s click likelihood, as follows: whereW∈ R;𝑏∈ Rare trainable parameters, and𝜎 (·)is the Sigmoid activation function. To train the model, we rst obtain a prediction for each item in the target watchlist snapshotWLgiven the user’s historyS(𝑡 ). Let 𝑦 ∈ {0,1}denote a binary vector having a single entry equal to 1 for the actual item clicked inWL(and 0 for the rest). We then train the model by applying the binary cross-entropy lossÍ over this watchlist snapshot:𝐿𝑜𝑠𝑠=𝐿𝑜𝑠𝑠, where:𝐿𝑜𝑠𝑠= −[𝑦𝑙𝑜𝑔(ˆ𝑦) + (1 −𝑦)𝑙𝑜𝑔(1 −ˆ𝑦)];ˆ𝑦is predicted according to Eq 6. Atinference, we simply recommend the top-𝑘items inWL having the highest predicted click likelihood according toˆ𝑦. We collect a large-scale watchlist dataset that was sampled from the eBay e-commerce platform during the rst two weeks of February 2021. We further sampled only active users with at least 20 interactions with their watchlist during this time period. Due to a sampling limit, for each watchlist snapshot we are allowed to collect a maximum of 15 items, which are pre-ordered chronologically, according to the time they were added by the user to the watchlist. In total, our dataset includes 40,344 users and 5,374,902 items. The data is highly sparse, having 11,667,759 and 1,373,794 item page views and watchlist item clicks, respectively. An average watchlist snapshot includes 10.48 items (stdev: 4.96). Items in our dataset hold multiple and diverse attributes. These include item-ID, user-ID, price (with values binned into 100 equal sized bins using equalized histogram), seller-ID, condition (e.g.,new,used), level1-category (e.g., ‘Building Toys’), leaf-category (e.g., ‘Minifigures’), saletype (e.g.,bid) and site-ID (e.g.,US). Since item-IDs and seller-IDs in our dataset are quite sparse, we further hash these ids per sequence, based on the number of occurrences of each id type in the sequence. For each item in a given user’s history, we keep several additional attributes, namely: position-ID (similar to BERT [11]), associated (watchlist/view) snapshot-ID, interaction type (watchlist-click or item-page-view), hour, day, and weekday of interaction. Additionally, for each item in a given watchlist snapshot, we keep its relative-snapshot-position (RSP) within the watchlist (relative to user’s inclusion time). We note that, we consider all position-based attributes relatively to the sequence end. We split our data over time, having the train-set in the time range of 2–11 February 2021, and the test-set in the time range of 12–15 February 2021. We further use the most recent 1% of the training time range as our validation-set. 4.2.1 Model implementation and training. We implementTrans2D with pytorch [15]. We use the Adam [12] optimizer, with a starting learning rate of 10,𝛽=0.9,𝛽=0.999,ℓweight decay of 10, and dropout𝑝 =0.3. For a fair comparison, for all baselines (and ourTrans2Dmodel), we set the number of blocks𝐿 =1, number of headsℎ =4, embedding size𝑑 =16, and maximum sequence length𝑁 =50. To avoid model overtting, we add an exponential decay of the learning rate (i.e., at each epoch, starting from the second, we divide the current learning rate by 10), and train for a total of 5 epochs. We train all models on a single NVIDIA GeForce RTX 3090 GPU with a batch size of 32. 4.2.2 Baselines. The WLR task requires to handle attribute-rich item inputs both during training and prediction. Yet, applying most existing baselines to directly handle the WLR task is not trivial. To recall, many previously suggested SR models support only inputs with item-IDs, and hence, are unsuitable for this task. Moreover, while many other SR models support attributes during training, their prediction is still based on item-IDs only. As the WLR task holds dynamic recall-sets, even those models that do support attributes during prediction, still require some form of adaptation to handle the WLR task. Overall, we suggest (and perform) three dierent model adaptations (denoted hereinafterA1,A2andA3, respectively) on such baselines, which we elaborate next. A1: Instead of using item-IDs for item embeddings, as most of our baselines do, we utilize their attributes. We obtain item input embeddings by averaging over each item’s attribute embeddings. This adaptation always results in a better performance. Hence, we apply it to all baselines that do not naturally handle multi-attributes. A2: Instead of predicting the next item, we train a model to predict clicks over items in the current watchlist snapshot. This adaptation commonly helps to boost performance, as attributes of predicted items are utilized. Therefore, those baselines that do not oer a good alternative, are enhanced with this adaptation. We note that, predicting the next item over the current watchlist snapshot only, always results in an inferior performance. A3: TheLatentCross[2] andContextAware[21] baselines utilize context-features during prediction. The context representation is used as a mask to the GRU output. We adapt the context-features to be the average attribute embeddings of the watchlist items. Having described the possible adaptations, we next list the set of baselines that we implement and compare to our proposed model. For each baseline, we further specify which adaptations we apply. • RSP: Orders watchlist items based solely on the attribute relative-snapshot-position, i.e., relative to each item’s user-inclusion time, having the newest item ranked rst. • Price: Orders watchlist items based solely on their price. We report both descending and ascending price orderings. •GRU: Inspired byGRU4Rec[8], we implement a GRU-based model, further applying adaptations A1 and A2. • GRU: Similar toGRU, with the only change of concatenating the attribute embeddings instead of averaging them. • Trans: Inspired byBERT4Rec[22],SASRec[10] andSSEPT[29], we implement a Transformer-based model, which is further adapted using adaptations A1 and A2. • BST[3]: Similar toTrans, with the only change of concatenating the attribute embeddings instead of averaging them. • Trans: Inspired by [5] and similar toTrans, with the only change of transforming the attribute embeddings by applying a vanilla-Transformer over the channel (attribute) dimension and only then averaging the embeddings. •FDSA: The originalFDSA[30] model oers two components: 1) Transformer over the item-IDs sequence, and 2) vanilla-attention pooling over the attribute embeddings followed by an additional Transformer over the attribute sequence. At the end, the two outputs are concatenated to predict the next item. Inspired byFDSA, we create a similar baseline but adapt its training to our task using adaptationA2. Here we note that, this baseline is the only one among all baselines that explicitly uses the original item-IDs. • FDSA: Similar toFDSA, but without the item-ID Transformer; noting that using the original item-IDs (compared to their hashed version) may easily cause an over-t over our dataset. • SeqFM[4]: Extends Factorization Machines [17] (FM) with sequential data; Its inputs are a combination of static and sequence (temporal) features. It learns three dierent Transformers based on: 1) attention between the static features themselves 2) attention between the sequence features themselves, and 3) attention between the static and sequence features. It then pools the outputs of each Transformer into a single representation using average. Finally, it concatenates the three representations and predicts a given label. We adapt this baseline to our setting by rst treating the predicted watchlist snapshotWLitems’ attributes as static features. Additionally, applying adaptationA1, the dynamic features are obtained by averaging the attribute embeddings for all of the items inS(𝑡 ). Using A1, this baseline can be directly used for click prediction. • CDMF[19]: Implements an extended Matrix-Factorization (MF) method that handles complex data with multiple feedback types and repetitive user-item interactions. To this end, it receives as its input a sequence of all user-item (pair) interactions. The importance of an item to a given user is learned from an attention pooling over all the user-item interactions. The user representation is calculated as the weighted average over all her interacted items. The nal prediction is calculated using the dot-product between the user and item representations. Inspired byCDMF, we implement the same architecture, where we apply adaptations A1 and A2. •LatentCross[2]: Incorporates context features over the output of the GRU layer which limits the prediction to specic contextfeatures. Therefore, we apply adaptationA3, leveraging the contextfeatures to be the next item’s features. Using adaptationA1, item features are calculated as the average attribute embeddings. • ContextAware[21]: Similar toLatentCross, with the exception that the context representation is treated both as a mask to the GRU layer and as additional information using concatenation. 4.2.3 Evaluation metrics. We evaluate the performance of our model and the baselines using a top-k recommendation setting. Accordingly, we use the following common metrics:𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛@𝑘,𝐻𝑖𝑡@𝑘, and𝑁 𝐷𝐶𝐺@𝑘, with ranking cutos of𝑘 ∈ {1,2,5}. We note that, for𝑘 =1, all three metrics have the same value. Hence, for this case, we report only𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛@1. We report the average metrics over all recommendation tasks in the test-set. We validate statistical signicance of the results using a two-tailed paired Student’s t-test for 95% condence with a Bonferroni correction. We report the main results of our evaluation in Table 1, where we compare ourTrans2Dmodel to all baselines. As can be observed, Trans2Doutperforms all baselines over all metrics by a large margin. We next notice that, the best baselines afterTrans2Dare GRUandBST(Transformer-based). Common to both baselines Table 1: Main results. Boldfaced results indicate a statistically signicant dierence. Trans2D (our model) 43.51 33.30 21.56 62.19 80.85 35.61 26.12 GRU39.08 31.52 21.24 58.76 79.43 33.23 25.07 FDSA37.84 31.03 21.14 57.80 78.91 32.57 24.76 FDSA36.95 30.37 21.01 56.57 78.35 31.86 24.46 Trans36.77 30.83 21.18 57.37 79.12 32.17 24.62 Trans36.01 30.42 21.10 56.56 78.72 31.68 24.41 GRU35.71 30.41 21.08 56.58 78.65 31.61 24.35 is the fact that they apply a concatenation over the attribute embeddings. While there is no dimensional reduction in the attribute representation, there are still two drawbacks in such baselines. First, the semantic dierence between the attributes is not kept; and therefore, the attention cannot be per attribute. Second, the number of parameters in these baselines drastically increases.Trans2D, on the other hand, requires only a slight expense of parameters. Among the next best baselines, are those that employ a Transformer/Attention pooling (i.e.,Trans,FDSA, andFDSA). Next to these are those that use average-pooling (i.e.,TransandGRU). These empirical results demonstrate that, the attribute reduction mechanism is important to avoid losing important attribute information before entering the sequence block. Therefore,Trans2D, which does not require any such reduction in the attribute representations, results in overall best performance. Observing the dierence betweenFDSA(as the only baseline that uses the original item-IDs) andFDSA, further shows that using the original item-ID resolves in worse results, as it causes an over-t. This helps verifying the importance of representing items based on their (less sparse) attributes. Furthermore, it is worth noting that, using theoriginalitem-ID as an attribute harms the performance of all other baselines as well. Therefore, we implement all other baselines in this work (including our own model) without the original item-ID attribute (yet, still with its hashed version). We next observe that, the context-aware baselines (i.e.,ContextAwareandLatentCross) perform similar to the average-pooling baselines. Here we note that, these baselines also perform averagepooling over the context-features (also referred to as the “next item features”). This supports our choice of adding the next possible item to the end of the sequence. Such a choice results in a similar eect to the context-aware approach that adds the item’s features as context not before the sequence (GRU) block is applied. Moreover, since we have a relatively small recall-set in our setting, it is possible to concatenate the predicted item features to the history sequence and leverage theAttention2Dblock to encode also the recall-set item attributes together with the sequence. Finally, we can observe that, ordering by theRSPattribute results in a reasonable baseline on its own. This in comparison to both Price baselines that only provide a weak signal. This implies that, over our sampled users, the price does not pay a big deal, but rather the short-term interactions. This fact is further supported in our next parameter sensitivity analysis, see Figure 2(e). To better understand how theTrans2Dmodel parameters inuence its performance, we check its sensitivity to ve essential parameters: 1)𝐿, number of attention blocks, 2)ℎ, number of heads used in the attention mechanism, 3)𝑑, each attribute embedding size, 4)𝑁, maximum sequence length, and 5) maximum number of days (back) to be considered in the sequence. For each parameter sensitivity check, we set the rest to their default values (i.e.,𝐿 =1,ℎ =4,𝑑 =16, 𝑁 = 50, and days=‘Full History’ – all days are being considered). We report the sensitivity analysis results (using NDCG@5) in Figure 2. For reference, we also report the sensitivity of the three next best baselines:GRU,BSTandFDSA. In general, we can observe that, in the majority of cases, our model outperforms the baselines. Furthermore, almost in all cases, the other baselines behave similarly to our model. Per parameter, we further make the following specic observations about our model’s sensitivity: # Blocks(𝐿): We notice that, adding a secondAttention2D block on top of the rst helps to learn second-order dependencies. This makes sense, as the two attention matrices of the model,A andA, can capture attention only in the same row or column. The only attention type that can capture dependencies between two cells not in the same row or column isA. Yet, during the attention calculation onA, the cells are not aware of the other values in their row or column. Therefore, a second block is able to capture such second-order dependencies (e.g., price comparison of two dierent appearances of the same item or leaf-category). We can further observe that, the third block has less inuence on performance. Finally, similar to the results reported in [3], with additional blocks, BST’s performance actually declines. # Heads(ℎ): Similar to normal attentions, utilizing more heads enables the attention mechanisms to learn multiple patterns. Here, we explore two main options for head sizes: 1) each head size equals the input size or 2) all head sizes together equal the input size. For our model andFDSAthe rst option is better, while forBST, the second option is better. For the latter, this can be explained by its already over-sized input and data sparsity (see further details next in Embedding Size sensitivity). Overall, we observe that, 4 heads resolves with a good performance, while 5 heads already has a minor impact on our model’s performance. Embedding Size(𝑑): Interestingly, for our model, an embedding size of𝑑 =32 resolves with the best performance. While the trend for𝑑 <32 can be explained by too small representation sizes, the interesting trend is for𝑑 >32. This can be explained due the vocabulary size of each attribute. While most attributes include only few or tens of unique values, only user-ID, level1-category, and leafcategory hold 40,344, 1,293, and 25,449 unique values, respectively. Therefore, while a larger representation may help the latter, the rest of the attributes suer from over-representation and sparsity. We note that, those baselines that use concatenation (GRUand Figure 2: Parameter Sensitivity results. Y-axis represents 𝑁 𝐷𝐶𝐺@5, while similar trends are observed for all other metrics. BST) can be thought as using a larger representation for each item. This fact supports the performance decline we observe for these baselines: with larger representations, more over-t. Dierently from the former two,FDSAactually manages to improve (up to some point) with the increase in embedding size. We attribute this to its usage of the vanilla attention pooling over the attribute embeddings, which in turn, allows it to represent attributes together in an ecient way, even if some are very sparse. Sequence Length(𝑁) +Days Considered: The result of the RSPbaseline implies that, users’ watchlist priorities are usually driven by short-term patterns. Such patterns are captured by users’ recently viewed item pages. We can observe that, for both parameters,𝑁 =50 or𝑑𝑎𝑦𝑠 =1 is enough in order to capture most of the important short-term signal. We further see a small improvement in extending the time range or sequence length. This implies that there is still a weak signal that can be captured. Since the sequence model is a Transformer, there is no memory-gate involved (like in GRU); hence, these short-term patterns represent a real user-behaviour. We next perform an ablation study and report the results in Table 2. To this end, every time, we remove a single component from the Trans2Dmodel and measure the impact on its performance. We explore a diverse set of ablations, as follows: −𝐿𝑖𝑛𝑒𝑎𝑟 2𝐷: We switch every Linear2D layer with a Linear1D layer, meaning all channels receive the exact same weights. This ablation allows to better understand the importance of the attributealignment. While this modied component downgrades performance, it still outperforms all baselines while keeping the exact same number of parameters as in a regular attention layer (except for the three𝛼scalars used in Eq. 2). This conrms that, the improvement of the fullTrans2Dresults are not only driven by the Linear2D layer but also by the attention mechanism itself. −A,−A,−A: We remove each one of the three attention parts from the nal attention calculation in Eq. 2. These ablations allow to better understand the importance of each attention type to the full model implementation. The most important attention part is A, showing that the relationship between user’s items is the most important, following the motivation of the 1D-attention mechanism. Ais followed byAand then byA. As we can further observe,A does not hold signicant importance. We hypothesis that, whileA is the only attention part that can capture dependencies between two cells not in the same row or column, during the attention calculation onA, the cells are not aware of the other values in their row or column. This can be over-passed by adding an additional Table 2: Ablation results. Starting from the second row, on each, a single component is removed from the model. Model@1 @2 @5 @2 @5 @2 @5 Trans2D (our model) 43.51 33.30 21.56 62.19 80.85 35.61 26.12 -𝐴44.01 33.42 21.56 62.42 80.85 35.82 26.20 -𝐴43.03 33.06 21.53 61.72 80.69 35.32 26.00 -𝐴38.83 31.60 21.36 58.84 79.92 33.24 25.14 Attention2Dlayer, as shown and discussed in Section 4.4 over the # Blocks sensitivity check, see again Figure 2(a). −𝑅𝑉 𝐼, −𝑤𝑎𝑡𝑐ℎ𝑙𝑖𝑠𝑡: We remove from the history sequence all items having interaction type=RVI (item-page-view) and interaction type=watchlist (click), respectively. These ablations allow to understand the importance of each interaction type to the WLR task. While both are important, watchlist clicks hold a stronger signal. This actually makes sense: while item page views (RVIs) provide only implicit feedback about user’s potential interests, clicks provide explicit feedback that reect actual user watchlist priorities. −𝑡𝑖𝑚𝑒, −𝑖𝑡𝑒𝑚, −𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛: For each, we remove a group of attributes in order to better understand the importance of the attributes to the WLR task. For−𝑡𝑖𝑚𝑒, we remove the hour, day, and weekday attributes. For−𝑖𝑡𝑒𝑚, we remove the price, condition, level1category, leaf-category, and sale-type attributes. For−𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛, we remove the position-ID, relative-snapshot-position (RSP), snapshotID, hash-item-ID, and hash-seller-ID attributes. Examining the ablations results, shows that the most important attribute-set is the position-set, followed by the item-set, and nally by the time-set. Noticing that the position-set includesRSPexplains their high importance. The time-set is not that important as relative-time is encoded in the position-set. Additionally, the dataset spans over a relatively short period, resolving with less data to learn the importance of a specic weekday, hour of the day, or day of the month. −ℎ𝑖𝑠𝑡𝑜𝑟𝑦: To understand the importance of the entire history, we remove all the history sequence, and solely predict overWL– the last snapshot. We see that, not observing any history, drastically harms the performance of the model. This strengthens the argument that considering user’s history is important for the WLR task. Figure 3: Qualitative Example. For brevity, only 6 out of the 16 possible attributes are presented. The darker the item-attribute cell color is, the more imp ortant it is. The current candidate is surrounded with the black rectangular. We end this section with a qualitative example (see Figure 3) that visualizes the attention values assigned by theTrans2Dmodel to items’ attributes in a given user-sequence. The last item in the sequence is the watchlist item for which the model makes a prediction. This item is actually clicked by the user (𝑦 =1) and the model’s prediction is aligned, assigning it the highest likelihood amongst all watchlist snapshot items (ˆ𝑦 = 0.697). As the learned attention is a 4D-array, representing the attention between any two (item,attribute) pairs (see Section 3.5.2), we rst pick the attention over the last item in the sequence, and then average over the channel dimension. Doing so, resolves us with a 2D-array that can be visualized to better understand the importance of each (item,attribute) pair on the prediction of the item. Observing the channel dimension, we can notice that, dierent attributes have dierent importance. This strengths the importance of a 2Dattention that is able to learn the attention over a second axis. As we can observe, given the next item to predict, the model emphasizes either previously clicked items related to the current predicted item (e.g., leaf-categories: 8159–‘Home Security:Safes‘, 4895–‘Safety:Life Jackets & Preservers‘) or RVIs that belong to the same leaf-category of the current item (8159). Interestingly, the model emphasizes the prices of clicked items in the same leafcategory (8159), which are higher than the predicted item’s price. In this work, we presented a novel watchlist recommendation (WLR) task. The WLR task is a specialized sequential-recommendation task, which requires to consider a multitude of dynamic item attributes during both training and prediction. To handle this complex task, we proposed Trans2D – an extended Transformer model with a novel self-attention mechanism that is capable of attending on 2D-array data (item-attribute) inputs. Our empirical evaluation has clearly demonstrated the superiority of Trans2D. Trans2D allows to learn (and preserve) complex user preference patterns in a given sequence up to the prediction time. Our work can be extended in two main directions. First, we wish to explore additional feedback sources such as historical user search-queries or purchases. Second, recognizing that Trans2D can be generally reused, we wish to evaluate it over other sequential recommendation tasks and domains.