Data-driven subword segmentation has become the default strategy for open-vocabulary machine translation and other NLP tasks, but may not be sufﬁciently generic for optimal learning of non-concatenative morphology. We design a test suite to evaluate segmentation strategies on different types of morphological phenomena in a controlled, semisynthetic setting. In our experiments, we compare how well machine translation models trained on subword- and character-level can translate these morphological phenomena. We ﬁnd that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages. Data-driven subword-level segmentation of text (Sennrich et al., 2016; Kudo, 2018) is a well-known and widely used text representation strategy in the natural language processing (NLP) community. While subword segmentation largely solves the open vocabulary problem, previous research has shown that models often break down in outof-domain contexts (El Boukkouri et al., 2020), when encountering spelling errors (Belinkov and Bisk, 2018; Pruthi et al., 2019), when translating morphologically-rich languages (Ataman and Federico, 2018) and in multilingual scenarios (Chung et al., 2020; Wang et al., 2021). The reason for this is that even slight deviations from the text seen when learning a segmentation model can result in entirely different segmentations and often aggressively over-segmented text. Given the rich morphological diversity across natural languages, it is especially interesting to investigate the suitability of subword segmentation to represent different morphological phenomena. For example, reduplication is a non-concatenative morphological phenomenonthat is common across the world’s languages, but is marginal in higherresource European languages,which raises the question if the dominant text representation strategies inadvertently disadvantage NLP systems for languages that feature it. Many types of morphological phenomena (see examples in Table 1) pose challenges to subwordlevel models. For concatenative phenomena such as afﬁxes, subword-level segmentations often do not adhere to morpheme boundaries which can hurt the performance of these models. For nonconcatenative morphology, it is still unclear to what extent subword-level models can learn to generalise to rare or unseen words. We believe these challenges are exciting opportunities to work on better text representations for cross-lingual NLP but currently, there is a lack of targeted evaluation environments. Most previous work evaluates very speciﬁc morphological or morpho-syntactic functions such as number, case, gender or subject-verb agreement (Sennrich, 2017; Burlot and Yvon, 2017; Warstadt et al., 2020) rather than evaluating how well different types of morphological phenomena can be learned. To address this issue, we design a test suite that can be used to evaluate how well a range of morphological phenomena can be learned with sentencelevel sequence-to-sequence models. We focus on Table 1: Examples for the morphological phenomena studied in this paper. The ﬁrst three are concatenative, the last two non-concatenative. the task of neural machine translation (NMT) and evaluate in a semi-synthetic DE→EN setting, allowing for an automatic evaluation that controls for various confounding factors. In our experiments, we test how well current segmentation strategies on subword- and character-level can learn to translate compounds, circumﬁxed words, inﬁxed words, vowel harmony and reduplicated words. Our contributions are the following: •We design an evaluation environment for various types of morphological phenomena that can be used to evaluate future text representation strategies. •We ﬁnd that non-concatenative morphological phenomena and generalisation to rare word bases are especially challenging to learn with current segmentation strategies. •We show that subword segmentation is less suitable to learn the correct surface form but all segmentation strategies perform well when we represent the morphological phenomena with an abstract token instead. Isolated morphological analysis and reinﬂection have long been of interest to the NLP community, with yearly shared tasks (Kurimo et al., 2010; Vylomova et al., 2020) that result in dedicated architectures that perform well for many languages (Aharoni and Goldberg, 2017; Makarov and Clematide, 2018; Wu and Cotterell, 2019; Rios et al., 2021). However, despite the large morphological diversity of natural languages, many approaches for sentence-level sequence-to-sequence tasks are often only tested on a subset of (morphologically similar) languages and then adopted without much questioning (Bender, 2011). One such example is subword-level representation of text (Sennrich et al., 2016; Kudo, 2018) which has contributed greatly to the success of deep learning in various NLP tasks and has become a necessary preprocessing step to train state-of-theart models (Devlin et al., 2019; Brown et al., 2020). Due to its data-dependent nature, subword segmentation algorithms often produce subword splits that do not adhere to morpheme boundaries which can limit the generalisation to rare or unseen words and can lead to performance loss. Furthermore, it is unclear if models trained with subword segmentation can learn to generalise to non-concatenative morphological phenomena such as reduplication or vowel harmony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classiﬁers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Our work is similar to Burlot and Yvon (2017) who also evaluate morphological competence based on the output of machine translation models rather than probabilities or hidden states. However, instead of morphological features, we are interested in evaluating how well different types of morphological phenomena can be learned by sequence-tosequence models, especially with a focus on their textual representation. Closely related is work by Vania and Lopez (2017) who compare language model perplexities for different segmentation strategies on morphologically diverse languages and by Klein and Tsarfaty (2020) who show that multilingual BERT (Devlin et al., 2019) subwords do not reﬂect the morphological structure of a nonconcatenative language like Hebrew well. Our setup with synthetic morphological phenomena is similar to work by Wang and Eisner (2016) who generate synthetic treebanks by reordering nodes in existing treebanks for various natural languages. Instead of simply reordering components, we need to apply a more complex preprocessing to generate synthetic morphological phenomena that ﬁt the natural context. We discuss this preprocessing in more detail in Section 5.2. We choose ﬁve morphological phenomena which we believe may be hard to learn with subword segmentation strategies. We show a natural language example for each morphological phenomenon in Table 1 and describe them brieﬂy below: Compounding: A compound is a word composed of more than one free morpheme. Compounding can affect the subword segmentation of the individual components which can make it harder to translate compounds even if the individual parts are seen regularly in the training data. Circumﬁxation: A circumﬁx is an afﬁx that consists of two parts, one added at the start of a word stem, the other at the end. With circumﬁxation, it is not guaranteed that the subword segmentation adheres to the morpheme boundaries and that the base is segmented in the same way as without any afﬁxation. It may be difﬁcult to learn the correct form for rare or unseen circumﬁxed words. Inﬁxation: An inﬁx is an afﬁx inserted inside a word stem. A word with an inﬁx cannot be segmented in the same way as without inﬁxation and it is not guaranteed that the segmentation splits the inﬁx into a separate token. Inﬁxation may also be hard to learn for rare or unseen cases. Vowel Harmony: Vowel harmony is a type of assimilation in which the vowels in a morpheme (e.g. an afﬁx) are assimilated to vowels in another morpheme (e.g. the word stem). Vowel harmony is a non-concatenative morphological process and it is unclear whether an NMT model trained with subword segmentation can learn to generate the correct vowels for rare or unseen words. Reduplication: Reduplication is another nonconcatenative morphological process in which the whole word (full reduplication) or a part of a word (partial reduplication) is repeated exactly or with a slight change. In some cases, the repetition can also occur twice (triplication). Reduplication often marks features such as plurality, intensity or size, depending on the language and raises the same generalisation question as vowel harmony. We identify four key requirements for our test suite and address them as follows: 1) Understanding and generation:We want to evaluate both how well morphological phenomena can be analysed and generated on the sentence level. For this reason, we choose machine translation as the context of our evaluation, where morphological phenomena can occur both on the source and the target side. 2) Automatic Targeted Evaluation:We want to offer an automatic evaluation to make our test suite independent of resources needed for expensive human evaluation. Morphological phenomena are hard to evaluate automatically in real-data settings where there can be exceptions to morphological rules and ambiguity in how a sentence is translated. Therefore, we decide to evaluate in a semi-synthetic scenario where we have full control over the morphological phenomena and their translations. The morphological phenomena should also be evaluated in isolation, i.e. not on the level of BLEU. To achieve this, we do not use naturally occurring morphemes to create our synthetic morphological phenomena. Rather, we generate artiﬁcial morphemes that do not occur in our training data otherwise and are distinct between source and target.In this way, there are also no cognates among the artiﬁcial morphemes we evaluate. 3) Computational Cost:To keep the computational cost minimal, we decide to insert all synthetic morphological phenomena simultaneously in the training data. Consequently, only a single machine translation model needs to be trained to evaluate a new representation strategy on all morphological phenomena. This reduces the carbon footprint roughly by a factor of ﬁve, compared to training models for each phenomenon separately. 4) Training Data and Vocabulary Size:The inﬂuence of factors such as training data size or vocabulary coverage should be minimised. Therefore, we choose a high-resource data setting where we can easily insert morphological phenomena with varying frequency. If morphological phenomena cannot be learned with ample resources, models will likely perform even worse in real-data, low-resource scenarios. With these requirements in mind, we decide to insert synthetic morphological phenomena in a high-resource DE→EN translation setting. Per morphological phenomenon of interest, we deﬁne a set of patterns that we match in the original sentence and replace with a synthetic morphological phenomenon using the artiﬁcial morphemes. The patterns can either be: •A pair of semantically equivalent prepositions, where we synthetically express the prepositional function in either the source or target sentence e.g. with an inﬁx. •A pair of semantically equivalent cardinal numbers as noun modiﬁers, where we synthetically express the cardinality of either the source or target noun e.g. with a subsequent token that is subject to vowel harmony. •A pair of semantically equivalent negation particles or intensiﬁers (such as “very”) for adjectives, where we synthetically express the modifying function in either the source or target e.g. with reduplication of the adjectives. •A pair of semantically equivalent nouns, where we use artiﬁcial morphemes to create synthetic compounds in either the source or the target. We choose these types of patterns because they can be expressed morphologically in natural languages. For each type, we select the most frequent pattern pairs in the training and test data. Reduplication often expresses negation or intensiﬁcation in natural languages, so we assign those patterns to this phenomenon. The remaining patterns that occur frequently enough are mostly prepositional functions. Consequently, circumﬁxation, inﬁxation and vowel harmony are both assigned prepositional patterns. A full overview of all pattern pairs and artiﬁcial morphemes for each morphological phenomenon is listed in Appendix A.2. Our training data consists of∼4.6M parallel sentences from the WMT16 shared task training data (Bojar et al., 2016). For development, we take the test set from WMT15 (∼2k parallel sentences) and for testing, the test sets from all other years of the shared task (∼ 28k parallel sentences). Word Alignment:We ﬁrst word-align our parallel sentences. Word alignments are used to ensure the morphological phenomena are inserted in the corresponding source and target tokens. We use eflomal(Östling and Tiedemann, 2016) to learn the word alignment. Parsing:We also parse our data to be able to write more speciﬁc matching rules. We use pretrained spaCy(Honnibal et al., 2020) parsersand the spacy_conlllibraryto create CoNLL-U format. Through this format, we also have access to part-of-speech (POS) tags and the lemmas of the tokens. To insert the synthetic morphological phenomena, we ﬁrst check if the corresponding pattern pair (prepositions, cardinal “two” or modiﬁer for adjectives) occurs in the source and target sentence. If this is the case, we check whether the patterns are aligned and extract the tokens where we want to insert the synthetic morphological phenomena. For prepositional functions, this is the noun of the prepositional phrase, for the cardinal “two”, this is the noun that the cardinal modiﬁes and for adjective modifying functions, this is the adjective following the modiﬁer. We ﬁnd these tokens using the information from the POS-tags and the dependency parse and check that the tokens are also aligned Table 2: Surface (S) and abstract examples (A) for all morphological phenomena and the original sentences (O). German source sentences on the left, English target sentences on the right. The modiﬁed token spans are marked in bold. translations of each other. In one sentence (either the source or the target), we use an artiﬁcial morpheme to create the synthetic morphological phenomenon and delete the preposition, cardinal number or adjective modiﬁer. In the other sentence, we replace the preposition, cardinal number or modiﬁer with another, isolated artiﬁcial morpheme. For compounds, we concatenate an artiﬁcial morpheme with a random noun in the source and introduce another, isolated artiﬁcial morpheme before the corresponding translation of that noun in the target. We never insert synthetic morphological phenomena on both sides simultaneously, i.e. one of the artiﬁcial morphemes in each pair is always isolated. The artiﬁcial morphemes are also unique for each pattern pair to minimise interference between them. Some examples for the resulting sentences can be seen in Table 2. To better evaluate how hard it is for a model to learn a speciﬁc morphological phenomenon, we also create sentence pairs with an abstract representation of the morphological phenomenon as a control, similar to Tamchyna et al. (2017). Instead of modifying the surface form, this abstract representation is simply an additional token that is used to indicate that the preceding token is subject to a speciﬁc morphological phenomenon. Results with this abstract representation act as an upper bound in our evaluation setup, indicating how well a model could learn a morphological phenomenon if it had access to an oracle to either analyse or produce the correct surface form. When all modiﬁed sentences are added to the original training data we obtain a total training set with∼5.6M sentence pairs. We add the modiﬁed sentences instead of replacing original sentences so that the use of our test suite does not impair the translation quality on real text.This way, future work could include our test suite training data without needing to train separate models for measuring general performance, thus minimising effort and carbon footprint. For testing, we only choose the sentence pairs where we inserted morphological phenomena. Depending on the pattern pair, we have between 50 and 700 test examples each. The exact numbers are presented in Appendix A.2 and we also present results with synthetically balanced test sets in Appendix A.3. 5.4 Model Description We train four neural machine translation models on our modiﬁed training data: 1A subword-level BPE model (Sennrich et al., 2016) with 32k merge-operations as a baseline and representation of current state-of-the-art models. 2A subword-level BPE model with 32k merge-operations, trained with BPE-dropout (Provilkov et al., 2020). With BPE-dropout, the training data is resegmented after every epoch and at each merge step, some merges are randomly dropped. Like other dropout methods (Srivastava et al., 2014; Zhou et al., 2020), BPE-dropout has a regularising effect and allows the model to generalise better to text segmented into smaller units. We also expect it to help generalisation across different occurrences of the same morpheme that would be segmented differently with deterministic subword segmentation. 3A subword-level BPE model with 500 merges, trained with BPE-dropout. This model learns on much smaller subword units although not completely on character-level and is used as a parent model for the next model. 4A character-level model which is ﬁnetuned on the model with 500 merge-operations. This ﬁnetuning strategy allows training reasonably well-performing character-level models without the need for very deep architectures (Libovický and Fraser, 2020). We train Transformer Base machine translation models (Vaswani et al., 2017) with thenematus (Sennrich et al., 2017) framework. We train the ﬁrst three models for 700k updates and choose the best checkpoint based on the BLEU score. This is evaluated on a dev set without synthetic morphological phenomena using SacreBLEU(Post, 2018). For the character-level model, we start the ﬁnetuning from the best checkpoint in the ﬁrst 400k updates of the subword model with 500 merges. The characterlevel model is then ﬁnetuned for an additional 550k updates and we choose the best checkpoint based on BLEU as for the other models. Our subword vocabularies are computed with byte pair encoding (Sennrich et al., 2016) using the SentencePiece implementation (Kudo and Richardson, 2018). We use a character coverage of 0.9999 to ensure the vocabulary for the model with 500 Table 3: BLEU scores on the development and test set (without morphological phenomena). subword segmentation operations does not consist of virtually only single characters. With this restriction, the vocabulary of our character-level model consists of 246 single characters plus three reserved tokens used by the NMT model and the 25 morphological tokens used for the abstract representations of the morphological phenomena. We provide more details on hyperparameters and computing environment in Appendix A.1. Since we use artiﬁcial morphemes to mark the morphological phenomena, we can evaluate if the correct artiﬁcial morpheme is produced rather than comparing to a reference. For phenomena occurring on the source side, we simply need to check whether the correct artiﬁcial morpheme that e.g. replaced a preposition or intensity marker occurs in the model’s output sentence. On the target side, the evaluation is a bit more complex. For circumﬁxation, we check if a token exists that is circumﬁxed with the correct artiﬁcial morphemes. For inﬁxation, we check if there is a token that is inﬁxed with the correct artiﬁcial morpheme. For vowel harmony, we check if the correct consonant triple occurs in the output sentence and whether the vowels between the consonants agree with the last two vowels of the previous token. For reduplication on the target side (full reduplication), we check if there is a fully repeated token in the output sentence. We do not evaluate whether the base of the phenomena matches the reference since only the translation of the artiﬁcial morphemes is guaranteed to be unambiguous in our training data. With this evaluation setup, we can compute the accuracy over all test sentences that contain a morphological phenomenon. First, we show a quick overview of the translation quality of our models. Table 3 shows the BLEU scores on the original dev and test sets without the Table 4: Accuracy (in %) of the four models for each of the morphological pattern pairs. Best results for surface representation are marked in bold. ≥95% dark green, ≥90% light green, ≥80% light red, <80% dark red (best viewed in colour). Patterns ordered by src / trg side, then by frequency. inserted morphological phenomena. While the subword model with 32k merges without BPE-dropout performs best, the model with BPE-dropout does not perform much worse on the test set. Training NMT models with smaller units decreases the translation quality by∼1.5 BLEU for the subword model with 500 merges and the characterlevel model compared to the best model. An evaluation on the level of BLEU does not offer any insight into how well these models can handle the morphological phenomena we are interested in. Table 4 shows the accuracy results on our test suite for each of the morphological pattern pairs. For compounding, it is interesting to see how the accuracy changes with increasing frequency of the patterns in the training data. Even with an abstract representation (last column), it takes around 70 training examples for the subwordlevel models with 32k merges to learn to translate the phenomenon correctly. The results for circumﬁxation and inﬁxation further show that concatenative morphological phenomena can be learned rather well by all models on both sides and with both the abstract and surface representations. The results of the other models with the abstract representation are comparable to the subword model with 32k merges. The most interesting results can be seen for the two non-concatenative morphological phenomena: vowel harmony and reduplication. First, there is a clear gap between the accuracy with the abstract representation and the surface representation for the subword-level model with 32k merges. The only exception is vowel harmony pattern #3 where the vowel harmony occurs on the source side and does not need to be generated by the model. Second, we can again see an effect of the frequency of the pattern pairs. For vowel harmony, the accuracy drops signiﬁcantly the rarer a pattern pair is. This is more prominent in the subword models with 32k splits. Similarly, for reduplication, while partial reduplication which occurs∼35k times in the training data can be learned to some extent, none of the models can learn to translate triplication correctly which is only seen 106 times. These results indicate that non-concatenative morphological phenomena will be even harder to learn in real-life scenarios, where we often encounter low-resource settings and more ambiguity in the translations. For the non-concatenative morphological phenomena, we can see a considerable beneﬁt from translating with smaller units. Even simply using BPE-dropout at training time can give a boost of up to 20% in accuracy. Given these results, we support the recommendation by Wang et al. (2021) that BPE-dropout should become the default for training sequence-to-sequence models. It is interesting to see how the models learn to translate the different morphological patterns over time. Figure 1 shows the training curves for the ﬁrst three modelson a circumﬁxation, vowel harmony and reduplication pattern pair. The pattern frequencies in the training data are comparable, occurring 11718, 7037 and 9664 times respectively. While circumﬁxation is learned almost perfectly after the ﬁrst few checkpoints by all models, we can see that reduplication and especially vowel harmony are learned much more slowly. For the latter two phenomena, we can also see that the differences between the two models are much more pronounced, e.g. for vowel harmony the subword model with 500 merges continuously outperforms the other two models. These plots also still show an improving tendency at 700k steps, so longer training times may be beneﬁcial for learning nonconcatenative morphology in NMT. Figure 1: Accuracy of one pattern pair per morphological phenomenon over time. Figure 2: Accuracy on different frequency buckets according to how often the source token has been seen with the morphological pattern during training. We perform a more ﬁne-grained evaluation and bucket test sentences according to how often the modiﬁed token in the source occurred with the speciﬁc morphological pattern in the training data. Figure 2 shows that the main beneﬁts from using models with smaller units come from the better generalisation to unseen or very rare modiﬁed tokens. This ﬁnding suggests that character-level models may outperform subword-level models to an even greater extent in real-data low-resource settings. We perform a manual analysis of up to 50 sentences per pattern pair where the morphological phenomenon was not translated with the correct artiﬁcial morphemes. We summarise the most interesting ﬁndings here and list the full results in Appendix A.4. For vowel harmony on the target side, we ﬁnd that all wrong translations are due to vowels that do not match with the previous token. For full reduplication on the target side, we see an interesting effect with the models trained with 32k merges. Instead of reduplicating an adjective such as “compulsorycompulsory”, these models often concatenate two words that are similar in meaning such as “mandatorycompulsory”. This effect disappears when training on smaller units. For rare compounds, the models trained with 32k merges often either copy the whole source compound or the artiﬁcial morpheme to the target side. This happens less often with the models trained on smaller units but instead, these models only start to translate the ﬁrst characters of the artiﬁcial source morpheme or hallucinate real words with similar orthography, e.g. “kidnapping rights” or “kidney inhabitants” instead of “kixaka rights” and “kixaka inhabitants” respectively. 7.4 How Realistic Are Our Results? We note that our results should not be taken as evidence that current NMT models can perfectly translate concatenative morphology. Generally, we expect that our controlled setting - where there is a one-to-one correspondence between artiﬁcial morphemes - is an idealised scenario and that models likely perform worse in real-life settings with more ambiguity and noise. However, our results do show a clear gap between the models’ competence for non-concatenative and concatenative morphology. Considering this performance gap and our reasons for evaluating in a semi-synthetic setup (see Section 4), we think that our test suite offers a targeted way to compare how well novel text representation strategies can learn non-concatenative phenomena. For vowel harmony, there is one factor in our setting that may slightly increase its difﬁculty: only a few patterns in our data set exhibit vowel harmony. This might make it harder for the model to learn to extract the relevant information (i.e. the vowels in word stems) than if all sufﬁxes in a language followed vowel harmony rules. Note however, that the frequencies of the individual patterns in our test suite are realistic. The top 50 nominal inﬂectional sufﬁxes in Turkish - a language that shows extensive vowel harmony in sufﬁxes - range from 13’000’000 at rank 1 to 30’000 at rank 50 (Aksan et al., 2017). These frequencies were counted in a corpus with 50M tokens. Our two more frequent pattern pairs lie in this range and our two less frequent ones capture the long tail of sufﬁxes and more accurately predict expected results in lowresource scenarios (likely less than 50M tokens). We develop a test suite to evaluate how well various types of morphological phenomena can be translated in NMT. We show that the choice of segmentation strategy can have a considerable inﬂuence on the performance, especially for nonconcatenative phenomena such as reduplication and vowel harmony. Our results with current segmentation strategies show a) that there is potential for more work on text representation strategies, b) that abstract representations may be a helpful source of information, especially for languages with non-concatenative morphology (if reliable tools for morphological analysis and generation are available) and c) that BPE-dropout should be adopted in state-of-the-art models since it improves learning non-concatenative morphology. Based on our results, we recommend that novel approaches in NLP always be tested on a range of typologically diverse languages that cover different types of morphological phenomena. In the future, we are interested in evaluating a wider variety of text representation strategies, including tokenisation-free input such as CANINE (Clark et al., 2021) or visual text representations (Salesky et al., 2021), although these are limited to the source side. We would also like to investigate the effects of out-of-domain contexts where we expect more rare word stems. We thank our colleagues Annette Rios, Phillip Ströbel and the anonymous reviewers for their helpful feedback. This work was funded by the Swiss National Science Foundation (project MUTAMUR; no. 176727).