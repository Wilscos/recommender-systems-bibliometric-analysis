As with most Machine Learning systems, recommender systems are typically evaluated through performance metrics computed over held-out data points. However, real-world behavior is undoubtedly nuanced: ad hoc error analysis and deployment-specic tests must be employed to ensure the desired quality in actual deployments. In this paper, we proposeRecList, a behavioral-based testing methodology.RecListorganizes recommender systems by use case and introduces a general plug-and-play procedure to scale up behavioral testing. We demonstrate its capabilities by analyzing known algorithms and black-box commercial systems, and we releaseRecList as an open source, extensible package for the community. • Software and its engineering →Acceptance testing;• Information systems → Recommender systems. recommender systems, behavioral testing, open source ACM Reference Format: Patrick John Chia, Jacopo Tagliabue, Federico Bianchi, Chloe He, and Brian Ko. 2018. Beyond NDCG: behavioral testing of recommender systems with RecList. In Lion ’22: The Web Conference, June 03–05, 2022, Lion, France. ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/1122445.1122456 “A QA engineer walks into a bar. Orders a beer. Orders 0 beers. Orders 99999999999 beers. Orders a lizard. Orders -1 beers. Orders a ueicbksjdhd.” – B. Keller (random tweet). sangwoo@kosa.ai In recent years, recommender systems (henceRSs) have played an indispensable role in providing personalized digital experiences to users, by ghting information overload and helping with navigating inventories often made of millions of items [4,8,25,35,38]. RSs’ ability to generalize, both in industry and academia, is often evaluated through some accuracy score over a held-out dataset: however, performance given by a single score often fails to give developers and stakeholders a rounded view of the expected performances of the system “in the wild”. For example, as industry seems to recognize more than academia, not all inputs are created equal, and not all mistakes are uniformly costly; while these considerations are crucial to real-world success, reporting NDCG alone fails to capture these nuances. This is particularly important in the world of RSs, given both the growing market for RSsand the role of RSs in shaping (often, narrowing [1]) user preferences with potential harmful consequences [15]. Following the lead of [27] in Natural Language Processing, we propose a behavioral-based framework to test RSs across a variety of industries, focusing on the peculiarities of horizontal use cases (e.g. substitute vs complementary items) more than vertical domains. We summarize our main contributions as follows: •we argue for the importance of a well-rounded and more nuanced evaluation of RSs and discuss the importance of scaling up testing eort through automation; •we release an open-source package to the community – RecList.RecListcomes with ready-made behavioral tests and connectors for important public datasets (Coveo Data Challenge [31], MovieLens [13], Spotify [39]) and an extensible interface for custom use cases; •we demonstrate our methodology by analyzing standard models and SaaS oerings over a cart recommendation task. While we developedRecListout of the very practical necessities involved in scaling RSs to hundreds of organizations across many industries, as researchers, we also believe this methodology to be widely applicable in error analysis and thorough evaluation of new models: as much as we like to read about a new SOTA score on MovieLens, we would also like to understand what that score tells us about the capabilities and shortcomings of the model. While quantitative metrics over standardized datasets are indispensable in providing an objective pulse on where the eld is going, our experiences both as researchers and practitioners are that NDCG tells only one part of the performance story. As a very concrete example, while model performance depends mostly on what happens with frequent items, the nal user experience may be ruined by poor outcomes in the long-tail [2]. Metrics such as coverage, serendipity, and bias [16,18,22] have been therefore proposed to capture other aspects of the behaviors of RSs, but they still fall short of what is needed to debug RSs in production, or provide any guarantee that a model will be reliable when released. When developingRecList, we started from popular use cases that represent the most widely adopted strategies for recommendation systems: (1) similar items: when shown running shoes, users may want to browse for another pair of running shoes – in other words, they are looking for substitutable products. This type of recommendation is also a common pattern in entertainment [21,25], to suggest content similar to a previous or current viewing, and in comparison recommenders [8]; (2) complementary items: when a TV has been added to the cart, shoppers may want to buy complementary products such as an HDMI cable. This type of recommendation is more typical of e-commerce scenarios and exhibits a characteristic asymmetry (Figure 1); (3) session-based recommendations: real-time behavior has been recently exploited to provided session-based personalization [6,12,14,36], which captures both preferences from recent sessions and real-time intent; a typical session-based RS ingests the latest item interactions for a user and predicts the probable next interaction(s). From these use cases, we identied three main areas of behavioral intervention: (1) enforce per-task invariants: irrespective of the target deployment, complementary and similar items satisfy formal relations which are dierent in nature. In particular, similar items need to be interchangeable, while complementary items may have a natural ordering. We operationalize these insights by joining predictions with metadata: for example, we can use price information to check for asymmetry constraints; (2) being less wrong: if the ground truth item for a movie recommendation is “When Harry met Sally”, hit-or-miss metrics won’t be able to distinguish between model A that predicts “Terminator” and model B that suggests “You’ve got mail”. In other words, while both are “wrong”, they are not wrong in the same way: one is a reasonable mistake, the other is a terrible suggestion. RSs are a major factor in boosting user experience (which translates to revenues, loyalty etc.): in a recent survey, 38% of shoppers said they would stop shopping at a retailer showing non-relevant recommendations [20]; Figure 1: Examples of behavioral principles for RSs: in (1) we observe the asymmetry desired when recommending complementary items, while (2) exemplies that model mistakes (i.e. missing the ground truth item) may degrade the shopping experience in dierent ways. (3) data slices: in real-world RSs, not all inputs are created equal. In particular, we may tolerate a small decrease in overall accuracy if a subset of users we care about is happier; or, conversely, we may want to improve general performance provided that the experiences of some groups do not degrade too much. For a practical example, consider a multi-brand retailer promoting the latest Nike shoes with a marketing campaign: other things being equal, this retailer would want to make sure the experiences of users landing on Nike product pages are particularly well curated. Aside from horizontal cases (e.g. cold-start items), the most interesting slices are often context-dependent, which is an important guiding principle for our library. Figure 1 exemplies these concepts in an e-commerce setting. BuildingRecListrequires to both operationalize behavioral principles in code whenever possible, and provide an extensible interface when domain knowledge and custom logic are required (Section 4). This work sits at the intersection of several themes in the research and industrial communities. We were initially inspired by behavioral testing for NLP pioneered by [27]: from this seminal work we took two lessons: rst, that black-box testing [3] is a source of great insights in addition to standard quantitative metrics; second, that this methodology goes hand-in-hand with software tools, as creating, maintaining, and analyzing behavioral tests by manual curation is a very time-consuming process. On the other hand, RecListneeds to consider the peculiarities of RSs, as compared to NLP: in particular, the concept of generic large-scale model does not apply to RSs, which are deployed in dierent shops, with a specic target distribution: the same pair of running shoes can be popular in Shop X and not Shop Y, and categorized as sneakers in one case, running shoes in the other. From the A/B testing literature [17], we take the important lesson that not all test cases are created equal: in particular, just as a careful A/B test cares both about the aggregate eect of treatment and the individual eects on specic data slices, a careful set of RS testing should worry about the overall accuracy as well as the accuracy in specic subgroup-of-interests: in ML systems, as in life, gains and losses are not always immediately interchangeable. The RS literature exploited already insights contained inRecList, typically as part of error analysis [28], or as performance boost for specic datasets [11]. For example, “being less wrong” is discussed in [33], while cold start performance is often highlighted for methods exploiting content-based features [34]. Our work builds on top of this scattered evidence, and aims to be the one-stop shop for behavioral analysis of RSs:RecListprovides practitioners with both a common lexicon and working code for scalable, in-depth error analysis. Finally, as far as standard quantitative metrics go, the literature is pretty consistent: a quick scan through recent editions of RecSys and SIGIR highlights the use of MRR, ACCURACY, HITS, NDCG as the main metrics [7,19,24,26,37]. To ease the comparison with research papers on standard KPIs, we made sure that these metrics are computed byRecListas well, together with behavioral results. RecListis behavioral testing applied to RSs, and available as a plug-and-play open-source package that can be easily extended to proprietary datasets and models. Following [27], we decouple testing from implementation: our framework treats RSs as a black box (through an extensible programming interface), allowing us to test RSs for which no source code is available (e.g. SaaS models). To strengthen our exposition of the methodology, we oer here a highlevel view of the logical architecture and capabilities ofRecList as a package. However, please note the code is actively evolving as a community project: the reader is encouraged to check out our repositoryfor up-to-date documentation and examples. RecList is a Python package built over these main abstractions: • RecTask: the recommendation use case (Section 2). •RecModel: the model we are testing – as long as a simple prediction-based interface can be implemented, any model can be represented inRecList. For example, a SaaS model would make an API call to a service and letRecListhandle the analysis. •RecDataset: the dataset we are using – the class provides standard access to train/test splits and item metadata.RecList comes with ready-made connectors for popular datasets. •RecList: the actual set of tests we are running, given a Rec- Task, RecModel and RecDataset. A RecList is made of RecTests. When running a RecList, the package automatically versions the relevant metadata: a web application is provided to analyze test reports, and visually compare the performance of dierent models (Appendix A). While we refer readers to our repository for an up-to-date list of available RecLists, RecMo dels and RecDatasets, we wish to highlight some key capabilities: • leveraging representation learning: word embeddings for behavioral testing in NLP are replaced by representational learning per dataset. By unifying access to items and metadata (for example, brands for products, labels for music), RecListprovides a scalable, unsupervised ow to obtain latent representation of target entities [23], and uses them to generate new test pairs, or supply similarity judgment when needed (Figure 2). • merging metadata and predictions:RecList’s tests provide a functional interface that can be applied to any dataset by supplying the corresponding entities. For example, asymmetry tests can be applied to any feature exhibiting the desired behavior (e.g. price for complementary items); in the same vein, data slices can be specied with arbitrary partitioning functions, allowing seamless reporting on important subsets; • injecting domain knowledge when needed:RecListallows to easily swap default similarity metrics with custom ones (or, of course, write entirely new tests): for example, a very accurate taxonomy could be used to dene a new distance between predictions and labels, supplementing outof-the-box unsupervised similarity metrics. Table 1: Results for a complementary RecList. To showcaseRecListin a real-world setting, we test three RSs on a complementary items task: a prod2vec-based recommender [5] (henceP2V); Google Recommendation APIs (GOO) [9]; and one Figure 2: Sample workow for behavioral tests. Starting with shopping data (left), the dataset split (orange) and model training (blue) mimic the usual training loop. RecList creates a latent space, which it uses to measure the relationships between inputs, ground truths and predictions, such as how far misses are from ground truths (violet). Since a session can be viewed as a sequence of items or features (brands), RecList can use the same method to create embeddings for dierent tests. popular SaaS model (S1). We use data from a “reasonable scale” [30] e-commerce in the sport apparel industry, where 1M product interactions have been sampled for training from a period of 3 months in 2019, and 2.5K samples from a disjoint time period for testing. We rst assess the models with various standard aggregate metrics (Table 1): based on HR@10 and MRR@10,GOOandP2V are close and they outperformS1. For reason of space, we discuss our insights from three RecTests: • Product Popularity: we compare the distribution of hits across product click-frequency (i.e. how accurate the prediction is, conditional on its target being very / mildly / poorly popular).P2Vcan be seen to perform better on rare items (clicked∼10times) by 40% overGOO. On the other hand GOOoutperformsP2Vby 200% on the most frequentlyviewed items (clicked ∼ 10times). • “Being Less Wrong”: we compute the distance between input product to ground truth, and input product to prediction for missed predictions: cosine-distance over a prod2vec space is our distance measure (Figure 3). We observe that despite having equivalent HR@10/MRR@10 asP2V,GOO’s prediction distribution better matches the label distribution, suggesting that its predictions are more aligned to the complementary nature of the cart recommendation task. This highlights another dierence betweenGOOandP2Vwhich HR/MRR alone were unable to capture. • Slice-by-Brand: we measure hits across various brands. We nd that whileP2VandGOOhave very similar performance, P2Vis particularly performant onasics, compensating for a slightly lower result onnike: without behavioral testing, this bias inP2Vwould have been hard or time-consuming to catch. Figure 3: Distribution of cosine distances for input to label (X to Y) and input to prediction (X toˆ𝑌 ). The lower half of Table 1 contains aggregate results for other RecTests. We show other ways “Being Less Wrong” can be operationalized to demonstrate the exibility ofRecList. Cos Distance (Brand) trains brand embeddings and measures the distance between label and prediction in this space for misses. “Less Wrong” in this case might mean that presenting an Adidas product over Lacoste if a Nike product is in the basket. Similarly, Cos Distance (Misses) measures the same distance but over a prod2vec space instead. Conversely, Path Length goes for a discrete approach and measures distance as the path length between input and prediction based on a product category tree (longer suggests greater diversity). We introducedRecList, a package for behavioral testing in recommender systems:RecListalpha version already supports popular datasets and plug-and-play tests for common use cases. However, behavioral testing needs to continuously evolve as our understanding of RSs improves, and their limitations, capabilities and reach change: by open sourcingRecListwe hope to help the eld go beyond “leaderboard chasing”, and to empower practitioners with better tools for analysis, debugging, and decision-making. Authors wish to thank Andrea Polonioli for feedback on previous drafts of this work and Jean-Francis Roy for his constant support in this project (and many others as well). Finally, it is worth mentioning that this is our rst community-sourced (5 cities, 4 time zones) scholarly work, with Chloe and Brian joining the Coveo line-up through a thread on Discord (thanks Chip Huyen for creating that amazing place!). While it is too early to say how successful this model will be for a company of our size, we are proud of what we achieved so far.