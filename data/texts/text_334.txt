Rafael Ferreira da Silva, Henri Casanova, Kyle Chard Tain˜a Coleman, Frederik Coppens, Frank Di Natale, Bjoern Enders Oak Ridge National Labo ratory, Oak Ridge, TN, USAUniversity of Southern California, Marina Del Rey, CA, USA Abstract—The landscape of workﬂow systems for scientiﬁc applications is notoriously convoluted with hund reds of seemingly equivalent workﬂow systems, many isolated research claims, and a steep learning curve. To address some of these challenges and lay the groundwork for transforming workﬂows research and development, the WorkﬂowsRI and ExaWorks projects partnered to bring the international workﬂows community together. This paper reports on discussions and ﬁndings from two virtual “Workﬂows Community Summits” (January and April, 2021). The overarching goals of these workshops were to develop a view of the state of the art, identif y crucial research challenges in the workﬂows community, articulate a vision for potential community efforts, and discuss technical approaches for realizing this vision. To this end, participants identiﬁed six broad themes: FAIR computational workﬂows; AI workﬂows; exascale challenges; APIs, interoperability, reuse, and standards; training and education; and building a workﬂows community. We summarize discussions and recommendations for each of these themes. Index Terms—Scientiﬁc workﬂows, community roadmap, data management, AI workﬂows, exascale comput ing, interoperability Scientiﬁc workﬂow systems are used almost universally across scientiﬁc domains for solving complex and largescale computing and data analysis problems, and have underpinned some of the most signiﬁcant discoveries of the past decades [1]. Many o f these workﬂows have signiﬁcant computational, storage, and communication demands, and thus must execute on a wide range of large-scale platforms, fr om local clusters over science or public clouds to upcoming exascale HPC platforms [2]. Managing these executions is often a signiﬁcant u ndertaking, requ iring a sophisticated and versatile software infrastructure. , Ilkay Altintas, Rosa M Badia, Bartosz Balis, Historically, many of these infrastructures for workﬂow execution consisted of complex, integrated systems, developed in-house by work ﬂow practitioners with strong de pendencies on a range of legacy tech nologies—even including sets of ad-hoc scripts. D ue to the increasing need to support workﬂows, de dicated workﬂow systems were developed to provide abstractions for creating, executing, and adapting workﬂows conveniently and e fﬁciently while ensuring portability. While these efforts are all worthwhile individually, there ar e now hundreds of independent workﬂow systems [3]. These are created and u sed by thousands of researchers and developers, leading to a rapidly growing corpus of workﬂows research publications. The resulting workﬂow system technology landscape is fragmented, w hich may present signiﬁcant barriers for future workﬂow users due to the tens of seemingly comparable, yet u sually mutually incompatible, systems that exist. In the current workﬂow research, there are conﬂicting theoretical b a ses and abstractions for what constitutes a workﬂow system. It may be possible to translate between system s that use the same underlying abstractions; however, the contrary is not feasible. Speciﬁcally, typical systems have a layered model that abstractly underlies it: (i) if the models are the same for two systems, they are compatible to some extent, and if they implement the same layer s, they can be interchanged (mod ulo some translation effort); (ii) if the models are the same for two systems, but they are implemented by components a t different layers, they can be co mplementary, and may have common ele ments tha t could be shared; (iii) if the models are distinct, workﬂows or system components are likely not exchangeab le or interoperable. As a result, many teams still elect to build their own custom solutions r ather than adopt, adapt, or build upon, existing workﬂow systems. This current state of the workﬂow systems landscape negatively impacts workﬂow users, developers, and researche rs [4]. The WorkﬂowsRI [5] and ExaWorks [6] projects have partnered to bring the workﬂows community (researchers, developers, science and engineering users, and cyberinfrastructure experts) together to collaboratively elucidate the R&D efforts necessary to remedy the above situation. They conducted a series of virtual events entitled “Workﬂows Community Summits”, in w hich the overarching goal was to (i) develop a view of the state of the art, (ii) identify key research challenges, (iii) articulate a vision for potential activities, and (iv) explor e technical approaches for realizin g (part of ) this vision. The summits gathered over 70 participants from a group of international lead researchers and developers, from distinct workﬂow sy stems and user communities. The outcomes of the summits have been compiled and p ublished in two technical reports [7], [8]. In this paper, we su mmarize the discussions and ﬁndings by presenting a consolidated view of the state of the ar t, challenges, and potential efforts, which we eventually synthesize into a community roadmap. Table I presents, in the form of top-level themes, a summar y of those challenges and targeted community activities. Table II summarizes a proposed community roadmap with technical approaches. The remainder of this paper is organized as follows. Sections II-VII provide a brief state of the art and challenges for each theme and proposed commu nity activities. Section VIII discusses technica l approaches for a community roadmap. Section IX concludes with a summary of discussions. The FAIR princip le s [9] have laid a foundation for sharing and publishing digital assets and, in particular, da ta . The FAIR principles emphasize machine a ccessibility and that all digital assets should be Findable, Accessible, Interoperable, and Reusable. Workﬂows encode the methods by w hich the scientiﬁc process is conducted and via which data are created. It is thus important that workﬂows both support the creation of FAIR data and themselves adhere to the FAIR p rinciples. Workﬂows are hybrid processual digital assets that can be considered as data or software, or some combination of both. As such, there is a range of considerations to take into account with respect to the FAIR pr inciples [10]. Some perspectives are already well explored in data/software FAIRness, such as descriptive metadata , software metrics, and versioning; however, workﬂows create unique ch a llenges such as representing a complex lifecycle from speciﬁcation to execution via a workﬂow system , throu gh to the data created at the completion of the workﬂow execution. As a specialized kind of software, workﬂows have two properties that FAIRness funda mentally must address: abstraction and composition. As far as possible a workﬂow speciﬁcation, as a graph or some declarative expression, is abstracted from its execution undertaken by a dedicated software platform. Workﬂows are composed o f modular building blocks and expected to be remixed. FAIR applies “all the way down” at the speciﬁcation and execution level, and for the whole workﬂow and each of its components. One of the most challengin g aspects of making workﬂows FAIR is ensuring that they can be reused. These challenges include being able to capture and then move workﬂow components, dependencies, and application environments in such a way as not to affect the resulting execution of the workﬂow. Further work is required to und erstand u se cases for reuse, before exploring methods for capturing necessary context and enabling reuse in the same or different environments. Once use cases are deﬁned, there a re many me trics and features that could be considered to determine whether a workﬂow is FA IR. These features may differ depending on the type of workﬂow and its app lica tion domain. Prior work in data and software FAIRness [9], [11] provides a starting point, however, these metrics n eed to be revised for workﬂows. In terms of labeling , there has been widespread a doption of repr oducibility bad ges for publications and of FAIR labels for data in repositories [12]. Similar approa ches could be applied to computational workﬂows. Finally, developing methods for FAIR workﬂows requires co mmunity engagement (i) to deﬁne principles, po licie s, and best practices to share workﬂows; (ii) to standardize m etadata representation and collection processes; (iii) to create developer-friendly guidelines and workﬂow-friendly too ls; and (iv) to develop shared infrastructure for enabling development, execution, and sharing of FAIR workﬂows. Given current efforts for developing FAIR data and software, it is importan t to ﬁrst understand what efforts could be adapted to workﬂow pr oblems. An immediate activity include participating in working groups focused on applying FAIR principles to data and so ftware. For instanc e , FAIR4RS [13] coordinates community-led discussions aroun d FAIR principles for research software. Workﬂows c ould then be initially tackled fro m the point of view of workﬂows as software, which could originate a novel task force. Proposed working groups such as FAIR for Virtua l Research Environments [14] represent adequate progress towards this goal. A fundamental tenet of FAIR is the universal availability of machine processable metadata. The European EOSC-Life Workﬂow Collab oratory, for example, has developed a metadata fram ework for FAIR workﬂows based on schema.org [15], RO-Crate [16], and CWL [17]. This could be a community starting point for standardization of metadata ab out workﬂows. An integral aspect of a FAIR computational workﬂows task force would be to collect a set of real-world use cases and workﬂows in several dom ains to examin e fr om the perspective of the FAIR data principles. This exercise will likely highlight areas in which the FAIR data principles adequately represent challen ges in workﬂows. Based on these experiences, a set of simple rules could be deﬁned for creating FAIR workﬂows, similar to the ones in [18]. From these rules, promin ent workﬂow repositories (e.g., WorkﬂowHub.eu [19] and Dockstore [20]), communities, and workﬂow systems can deﬁne recommendations to suppor t the development and sharing of FAIR workﬂows. These efforts relate not only to the workﬂows themselves, but the workﬂow components, execution environments, and the different types of data. Ensuring provenance can capture the necessary information is key for enabling FAIRness in workﬂows. Many provenance models [21] can be implemented or extended to capture the informa tion needed for FAIR workﬂows. Additionally, FAIR principles are more likely to be followed if the process for capturing these metrics is automated and embedded in workﬂow systems. In this case, a workﬂow execution will become FAIR by default, or per haps with minimal user curation. Artiﬁcial in telligence (AI) and machine lear ning (ML) techniques are becoming more and more p opular within the scientiﬁc community. Workﬂows increasingly integrate M L models to guide analysis, couple simulation and data analysis codes, and exploit sp ecialized computing ha rdware (e.g., GPUs, neu romorphic chips) [22]. These workﬂows inherently couple various ty pes of tasks such as short ML inference, multi-node simulations, long-running ML model training, etc. They are also often iterative and dynamic, with lear ning systems deciding in real time how to modify the workﬂow, e.g. by ad ding new simulations or cha nging the workﬂow all together. AI-enabled workﬂow systems therefore must be capable of optimally placing and managing sin gle- and multinode tasks, exp loit heterogeneous architectures (CPUs, GPUs, and accelerators), and seamlessly coordinate the dynamic coupling of disparate simulation tools. Workﬂows empowered with ML techniques largely differ from traditional workﬂows running on HPC machines. While workﬂows (i. e., one large-scale application simulating a scientiﬁc object or process) traditionally take little inp ut data and produce large outputs, ML approaches target model training, which usually requires the input of a large quantity of data (either via ﬁles or from a collection of databases) and produces a small number of trained models. After training, these models are used to infer new quantities (during “model inference”) and behave like very lightweight applications th a t produce small quantities of output data. These models can be standalone applications, or even embe dded within larger traditional simulations. There exists an inherent tension between traditional HPC, which evolved around executing large capacitystyle codes and AI -HPC, which requires the coordinated execution of many smaller capability-scale a pplications (e.g., large ensembles of data generatio n co-mingled with inference and coinciding with periodic retraining of models). With its relian c e of data, effective AI-workﬂows should provide ﬁne-grained data management and versioning features, as well as adequate data provenance capabilities. This da ta management will have to be ﬂexible: some applications and workﬂows might need to m ove data via a ﬁle-sy stem, while others could be be tter served from a tra ditional database, data store, or a streaming dataﬂow model. During inference, it may be best to couple the (lightweight) model as close to the data it is processing as po ssible. In any case, effective data management is a key feature of successful AI workﬂows. Another key feature of AI workﬂows is the inherent incorporation of non-traditional hardware, such as G PUs and tensor processing units (TPUs), which can signiﬁcantly accelerate both training and infer ence steps. Workﬂow systems thus need to pr ovide mechanisms for managing heterogeneous resources – o fﬂoading heavy computations to GPUs, and managing data between GPU and CPU memory hierarchies. Furthermore, since ML training and inference may be best executed on different hardware from the main simulation, AI workﬂows might need to be executed on multi-machine federated systems (with the main code executed on a traditional HPC system, but the ML model tra ining or inference on a separate system). Additiona lly, it is also necessary to provide tight integration to widely- used ML frameworks, the development of which is not driven by the HPC community. ML frameworks use Python and R-based libraries an d do not follow the classic HPC model: C/C++/Fortran, MPI, an d OpenMP, a nd submission to an HPC batch scheduler. Yet, some efforts in the HPC community seem promising, e.g. LBANN [23], EMEWS [24], [25], and eFlows4HPC [26]. Other approaches like Merlin [27] b le nd HPC and cloud technologies to enable federated work ﬂows. However, there is a cle a r disconnect between HPC m otivations, needs, and requirements, and AI/ML current practices. Finally, one of the major differences between traditional and AI workﬂows is the inherent itera tive nature of ML processes – AI workﬂows often feature feedback loops over a data set. Data are created, the model is retr ained, and its accuracy evaluated. ML training tasks might leverage hyperparameter optimization fram eworks [28]–[30] to adjust their execution settings in real time. The ﬁnal trained model is often used to select new data to acquire (in an “a ctive learning” environment, the model is used to decide which new simulations to run to better train the mo del on the next iteration). By design, ML-empowered workﬂows are dynamic, in contrast to traditional workﬂows with mor e structured and deterministic computations. At runtime, the workﬂow execution graph can potentially evolve based on internal metr ic s (accuracy), which may reshape the graph or trigger task preemption. Workﬂow systems should thus support dynamic branching (e.g., conditionals, criteria) and partial workﬂow re-execution on-demand. To address the disconnect between HPC systems an d practices and AI workﬂows, the com munity needs to develop sets of example use cases for sample problems with representative workﬂow structures and data types. In addition to exp anding upon the above challenges, the community could “codify” these challenges in example use cases. However, the set of challenges for enabling AI workﬂows is extensive. The community thus ne eds to deﬁne a systematic process for identifyin g and categorizing these challenges. A short-te rm recommendation would be to write a “ community white paper” about AI Workﬂow challenges/needs. Building from the use cases above for the needs and requirements of AI workﬂows, the c ommunity could deﬁne AI-Workﬂow mini-apps, w hich could be used to pair with vendors/future HPC developers so that the systems can be benchm arked against these workﬂows, and therefore support the co-design of emerging o r future systems (e.g., MLCommons [31] and the Collective Knowledge framework [32]). Given the computational demand s of many work ﬂows, it is crucial that their execution be not only feasible but also effortless and efﬁcient on la rge-scale HPC systems, and in particular upcoming exascale systems [2]. Exascale systems are likely to contain millions of independent computing elements that ca n be concur rently scheduled across more than 10,0 00 nodes, millions of cores, and tens of thousands of acc e le rators. HPC resource allocation policies and sched ulers designs typically do not conside r workﬂow applications: they provide a mere “job” abstraction instead of workﬂow-aware abstractions. Workﬂow u sers/systems are forced to make their workﬂows run on top of this ill-ﬁtted abstraction – e. g., it is difﬁcult to control low-level behavior c ritical to workﬂows (i.e., precise mapp ing of tasks to spe ciﬁc compute re sources on a compute node). Furthermore, the re is a clear lack of support for elasticity ( i.e., scaling up/down the number of nodes). Overall, it is curren tly difﬁcult to run work ﬂows efﬁciently and conveniently on HPC systems without exten ding (or even overhauling) resource management/scheduling appro aches, which ideally would allow programmable, ﬁne-grain applicationlevel resource allocation and scheduling. Related to the above challeng e, it is currently not possible to support both workﬂow and non-workﬂow users harmoniously and/or efﬁciently on the same system. So me features needed by workﬂows are often unavailable. For instance, batch schedulers can support elastic jobs (e.g., Slurm); however, experience shows that system admins may not be keen on enabling this capability, as they deem long static allocations preferable. A cultural change is perhaps needed as it seems that workﬂows are not yet con sid ered as high-priority applications by high-end compute facilities. Hybrid architectures are key to high performance and many workﬂows can or a re speciﬁcally designed to explo it them. However, on HPC systems, the necessary resource descriptions and mechanisms are not necessarily available to workﬂow users/systems (even though som e workﬂow systems have successfully interfaced to such mechanisms on particular systems) [33]. Although these resource descriptions and mechanisms are typic a lly available as part of the “job” abstraction, it is often not clear how a workﬂow system can discover and use them effectively. Finally, fault-tolerance and fault-recovery have been extensively studied on exascale systems, with several works and working solutions for tra ditional parallel jobs [34]. In the context of scientiﬁc workﬂows, speciﬁc techniques have been the subject of several studies [35], however workﬂow-speciﬁc solutions are ty pically not readily available or deployed. Moreover, workﬂows are built on smaller platforms, thus operating an d testing at exascale would entail expressing new requirements/capabilities and dealing with new constraints (e.g., what is a “local” exascale workﬂow?). An immediate activity consists in developing documentation in the form of workﬂow templates/recipes/miniapps for execution on high-end HPC system s to be hosted on a community web site. Some efforts u nderway provide partial solutions [36]. For instance, collections of workﬂows exist but typically do not provide large scale execution capabilities (e.g., community testbeds). Some compute facilities provide workﬂow tool documentation a nd help with their users [ 37]. These solutions should be cataloged as a starting point, and HPC facilities could promote yearly “workﬂow days”, in which they give workﬂow user s and developers training and early access to machines to try out th eir workﬂows, thus gathering feedback from users and developers. To drive the design of workﬂow-aware abstractions, the community could specify community benchmark workﬂows for exascale execution, exerting all relevant hardware as well as functionality c apabilities. Then it becomes possible for different workﬂow systems to execute these benchmark s – initial e fforts could build on previous benchmark solutions [38], [39]. These benchmarks c ould then be included in exascale machines acceptability tests. Note that th e re will be a ne e d to pick particu lar workﬂow systems to run these benchmarks, which will foster training and education of HPC person nel. Last, including workﬂow requirements very early on in machine procurement process for machines at computing facilities will signiﬁcantly lower the barriers for enabling workﬂow execution and therefore porting workﬂow applications. This effort is therefore preconditioned on the availability of miniapps and/or ben chmark speciﬁcations, as well as API/scheduler speciﬁcations, as outlined above. There has been an exp losion of work ﬂow technologies in the last decade [ 3]. Individual workﬂow systems often serve a particular user community, a speciﬁc underlying compute infrastructure, a ded ic a te d software engineering vision, or follow a speciﬁc historical trait. As a result, there are substantial technical and conceptual overlaps. Reasons for divergence in clude (i) use cases require different workﬂow structures, (ii) organizations have very different optimization goals, (iii) predeﬁned execution systems provide fundamentally different capabilities, or (iv) availability and scarcity of different types of r esources. Another reason is that it is relatively easy to start building a workﬂow system for a speciﬁc narrow focus (i.e., these systems have a gentle software development curve [40]), leading to large numbers of packages that provide some basic functiona lity, and developers who are subject to the sunk cost fallacy and then c ontinue to invest in their custom packages, rather than joining forces and building community packages. This d ivergence leads to missed opportunities for interoperability. It is often difﬁcult for workﬂows to be po rted across systems, for system components to be interchanged, for pr ovenance to be captured and exploited in similar ways, and for developers to leverage different execution engine s, schedulers, or monitoring services. Workﬂow systems often grow organically: developers start by solvin g a concrete data analysis problem and they en d up with a new workﬂow tool. In some cases, workﬂow systems may differ by design, rather than by accident. For example, they offer fundamentally different abstractions or models for a workﬂow: DAG-structured vs. recursive, impe rative vs. declarative, data ﬂow v s. control (and data) ﬂow. These fundamental differences, catering for different use cases, make it such that full interoperability may simply not be possible. Alternately, workﬂow systems have many different la yers and components that may be inte rchangeable, e.g., workﬂow speciﬁcations, task descriptions, data passing methods, ﬁle handling, task execution engines, e tc . Interoperability at some layers is likely to be more impactful than others; for instance, being able to run the same workﬂow speciﬁcatio n (with appropriately encapsulated task implementations) on different workﬂow infrastructures would be a major relief for users trying to reuse implemented workﬂows in other organizations. Further, interoperability does not need to imply agr eement and for workﬂow systems to implement a standard interface; instead, it may occur via shim layers or intermediate repre sentations, in a similar manner to compiling to a high level language. With the reuse goal, projects as eFlows4HPC p roposes the HPC Workﬂows as a Service (HPCWaaS) meth odology, where workﬂows will be deﬁned by expert developers an d provided as a service to community users [26]. Most efforts to unify workﬂow systems and/or their componen ts have led to the deﬁnition of a “standard” developed by a subset of the community [41], [42]. However, the specialization of some of these standards may require that other systems conform to that speciﬁcation , thus resulting in low adoption. Attempts to standardize also may lead to overly generic interfaces that in the end inhibit usability and lead to hidden incom patibilities. A particularly pressing problem at the interface of workﬂow technology and HPC systems is the need for a common submission model that is compatible to heterogeneous p latforms. The differences between the ways workﬂow engines, schedulers, and execution eng ines interact is a universal challenge faced by work ﬂow developers when trying to ta rget multiple infrastructures underlying long-lasting desig n decision s. Further challenges relate to authentication and authorization models deployed on many systems (e.g., two-factor authentication). Some efforts in this area are currently unde rgoing [43]. An immediate and continuous action would be to host several “bake-offs” to compare work ﬂow systems, including task and workﬂow deﬁnitions, a benchmark set of workﬂows with deﬁned input data and outputs, a s well as job execution interfaces. This would entail engaging participants to write and execute these workﬂows and identify ing commonalities between systems. A successful example is the GA4GH-DREAM challenge [44]. An open question is whether such attempts should be domain-speciﬁc or domain-overarching; there is likely a gr eater op portunity for standardization within domains (and indeed some domains have already made signiﬁcant progress), but d omain-speciﬁc standards would only partly solve the interoperability problem. The workﬂow community should th e n review these areas, determine and then pub liciz e what has worked, an d build on successful prior efforts. With the emergen ce of FaaS ( Function-as-a-Service ) systems (e.g., AWS Lamb da and Step Functions, Azure Durable Functions, Google Cloud Functions, IBM Composer), or CaaS (Container-as-a-Service) services (e.g. AWS Fargate, Google Cloud Run), the community should identify a set o f suggested use cases and compare them aga inst an imple mentation with popular or recently developed FaaS-ena bled workﬂow systems [45]–[47]. Such a comparison may tu rn out complementary features that can be of beneﬁt for both industry and the workﬂows community. In addition to features, a set of common workﬂow patterns could also be identiﬁed. However, there is still some uncertainty regarding the scope of previously developed patterns (e.g. , for representing patterns in dynamic workﬂows). Thus, it is necessary to survey published patterns [48], [49] and identify gaps seen by the community. Although the above proposed activities have the potential to advance interoperability, the current funding and resear ch recogn ition models of te n implicitly work against standardization by constan tly requiring innovative ideas even in ar eas where outreach, uptake, and maintenance rather than innovation seems to be the most pressing problem. Developing sustained funding models fo r building and evolving workﬂow standards, encouraging their adoption, supporting interoperability, testing, and providing user and developer training would help address these challenges. There is a strong need for more, better, an d new training and education oppo rtunities for workﬂow users. Many users “re-invent the wheel” without reusing software infrastructures and workﬂow tools that would make their workﬂow execution more convenient, more efﬁcient, easier to evolve, and more portable. This is partly due to the lack of comprehensive and intuitive train ing materials that would guide users through the process of designing a workﬂow (besides the typical “toy” examples provided in tutorials). Using workﬂow tools ca n req uire large amounts of effort and time, due to a steep learning curve. A contributing factor is that user s may not know the required terminology and concepts. As a result, some have noted that wh a t would be need ed in the cur rent technology landscape is to “ship a developer along with the workﬂow tool”. One of the reaso ns for the above cha llenge is that there are few “recip es” or “cookbook s” for workﬂow systems. Furthermore, given that workﬂows an d their execution platforms are complex and diverse, in addition to mere training material, there is a need for a training infrastruc ture that consists of workﬂows and accompanying data (small enough to be u sed for training purposes but large enough to be meaningful) as well as execution testbeds for running these workﬂows. Given the multitude of workﬂow systems [3], and the lack of standards (Section V), users cannot easily pick the appropriate systems for their needs. More importantly, there is an understandable fear of being locked into a tool that at some point in the near future will no longer be supported. Although docume ntation can be a problem, guidance is the mor e crucial issue. Many users have the basic skills to create and execute workﬂows on some sy stem, but as requirements gr adually increase many users evolve their simple approaches in ad-hoc ways, thus developing/main ta ining a working but imperfect homegrown system. There is thus a high risk of hitting technological o r labor-intensiveness roadblocks, which could be remedied by using a workﬂow system. But, when “gradua ting” to such a system, there will likely be co nstraints that p revent users from reproducing the functionality of the ir h omegrown system. The beneﬁts of using the workﬂow system should thus largely outweigh the drawback of these constraints. Given all the above challenges, it is not easy to reach out to users at the appropriate time. Reach out too early and users will no t view using a particular workﬂow system as compelling. Reach out too late, and users are already locked into their homegrown system , even though in the lo ng run this system will severely harm their produc tivity. Lowering the entry barrier is key for en abling the nextgeneration of resear c hers to beneﬁt from workﬂow systems. An initial app roach would be to provide a basic set of simple, yet conceptually rich, sample workﬂow patterns (e.g., “hello world” one-task workﬂows, chain workﬂows, forkjoin workﬂows, simple dynamic workﬂows), a ll with a few ways of handling data and I/O, and all with a few target execution platforms. Then workﬂow system teams can provide (interactive) documentation (or could be hosted on a community Web site) on how to run these patterns with their system [37]. Additionally, mechanisms should be identiﬁed at the institutional level to commit workﬂow systems training efforts in person: (i) this should be based on existing facilities and universities efforts; (ii) the scope of the training should be narrowed down so it is m anageable; and (iii) the issue of “who trains the trainers?” needs to be addressed. In light of workfo rce training, workﬂow concepts should be taug ht at early stages of the researchers/users education path. Precisely, these concepts should be included in university curricula, including domain scienc e curricula. Recent efforts have produced peda gogic modules that target workﬂow education [50], [51]. Pedagogic content could also be distributed as workﬂow modules to existing software carpentry efforts [52]. There is an established community of workﬂow researchers, developers, and users that has extensive expertise knowledge regarding speciﬁc tools, systems, applications, etc. It is crucial to capture such knowledge and bootstrap a community workﬂow knowledge-base (following standards for documentation, interoperability, etc.) for training a nd education. The work ﬂows community would also beneﬁt from collaborations with social scientists and socio logists so as to help d e ﬁne a n overall strategy for approaching some of the above challenges. Given the current large size and fragmentation of the workﬂow technology landscape, there is a clear need to establish a cohesive community of workﬂow developers and user s. This community would be crucial for avoiding unnecessary duplication of e ffort and would allow for sharing, and thus growing, of knowledge. To this end, there are four main components that need to be addressed for building a community: (i) id e ntity building, (ii) trust, (iii) particip ation, and (iv) rewards. The most natural idea is to think of two distinct communities: (i) a Workﬂow Research and Development Community, a nd (ii) a Work ﬂow User Community. The former gathers people wh o share interest in workﬂow R&D, and correspo nding sub-disciplines. Subgroups of this community are based on common methodologies, technical domains (e.g., computing, provenance, design), scientiﬁc discipline s, as well as geographical and funding areas. The latter gathers anyone using workﬂows for optimization of their work processes. However, most dom ain science users think of themselves in their speciﬁc disciplines ﬁrst, as they just hap pen to use workﬂows to get their work done. The two aforementioned c ommunities are not necessarily disjoint, but currently have little overlap. And yet, it is crucial that they interact. Such interactio n seems to happen only on a case-by-case basis, rather than via organized community efforts. One could, instead, envision a single community (e.g., team of users, or “ te am-ﬂow”) tha t gathers both workﬂow system developers and workﬂow-focused users, with the c ommon goal of spreading knowledge and adoption of workﬂows, thus working towards increased sharing and convergence/interoperation of technologies and approaches. Establishing trust and processes is key for bringing both communities together. There is no one-size-ﬁts-all workﬂow system or solution for all domains, instead each domain presents their own speciﬁc needs and have different preferred ways to address problems. There is a pressing need for maintaining documentation and dissemination that ﬁts different usage options and needs. Given the above, there are a number of existing community efforts that c ould serve as inspiration, e.g., the WorkﬂowHub Club [19] and Galaxy [53]. One approach is to gather experience from computing facilities where teams have successfully adopted and are successfu lly running workﬂow systems [37]. Another possibility is to use proposal/project reviews as mechanisms for spreading workﬂow technology knowledge. Speciﬁcally, ﬁnding ways to make pro posal a uthors (typically domain scientists) aware of available te chnology would prevent their proposed work to not entail re-inventing the wheel. Finally, it is c lear that solving the “c ommunity challenge” has large overlap with solving the “edu cation challenge” (Section VI). A short-term activity would entail establishing a common knowledge-base for workﬂow technology so that workﬂow users would be able to navigate the current technology landscape. User criteria (for navigation) need to be d e ﬁned. Workﬂow system developers can add to this knowledge base via self-reporting and could include test statuses for a set of standard workﬂow conﬁgurations, especially if workﬂow systems are deployed across sites. There is large overlap with similar proposed community efforts identiﬁed in Sections IV and VI. An ambitious vision would be to establish a “Workﬂow Guild”, i.e., an organization focused on interaction and good relationships and self-support between subscribing workﬂow developers and their systems, as well as dissemination of best-practices and tools that are used in the development and use of these systems. However, there a re still b arriers to be conqu e red: (i) such a commu nity could be too self-reﬂecting, and yet still remain fragmented ; (ii) a cultural/social problem is that creating a new system is typically more exciting for computer scientists as opposed to re-using someone’s system; and (iii) building trust and reducing internal compe tition will be difﬁcult, though building community identity will help the Guild work together against external competitors. In the previous sections, we have listed broad challenges for the workﬂows community and pr oposed a vision for community activities to address these challenges. Here, we explore technical approach es for realizing (part of) that vision. Based on the outcomes of the ﬁrst summit [7], we identiﬁed three technical thrusts for discussion in the second sum mit [8]. Some of these thrusts align with a single theme of the ﬁrst sum mit and some are cross-cutting. In the following subsection s, we present the summary of discussions at the second summit and propose r oadmap mileston es that emerged from these discussions. Additional details can be found in [8], and a summary of the roadmap milestones is shown in Table II. The above sections point to strong needs for establishing repositories of common workﬂow patterns and benchmarks (Sections III, IV, V, and VI). One objective is to develop workﬂow patter ns in which each pattern should b e easy for users to leverage a s starting point for their own speciﬁc workﬂow applications – they should provide links to one or more implementations, where each implementation is for a particular workﬂow system and can be downloaded and easily modiﬁed by the user. However, the level of abstraction of these patterns (i.e., the level of connection to real application use-ca ses) should still be deﬁned. At one extreme, workﬂow patterns could b e completely abstract with no connection to any rea lworld application. At the other extreme, workﬂow p atterns could b e completely use-case-driven and correspond to a ctual scientiﬁc applications, with realistic task computations and data sets. The end goal is thus to identify useful patterns that span the spectrum of possible levels of abstraction. Another aspect is the level of detail with which a pattern speciﬁes the platform on which it is to be executed and the logistics of the execution on that platform . The platform description could be left completely abstract, or it could be fully speciﬁed. Und er-specifying execution platforms and logistics may rende r the patter n not useful, but over-specifying them could render the pattern too niche. Benchmark speciﬁcations should make it easy for workﬂow system developers to develop th e m or to determine that their system cannot implement these speciﬁcations. Each benchmark should provide links to implementations and data sets, where each im plementation is for a particular workﬂow system. These implementations would be provided, maintaine d, and evolved by workﬂow system developers. They sho uld be able to be packaged so that they are executed out of the box on th e classes of p latforms they support. Moreover, the input data o f these workﬂows should be conﬁgurable in size to enable both weak and strong scaling experiments. For all conﬁgurations, also the output of the workﬂow must be provided to allow functio nal testing. Given the above, the following mileston es are proposed: M1. D eﬁne small sets (betwee n 5 and 10) of workﬂow patterns and of workﬂow benchmark deliverables. These should be deﬁned by eliciting feedback from users and workﬂow system developers, as well a s based on existing sources that provide or deﬁne real-world or synthetic workﬂow pattern s. M2. Work with a selected set of workﬂow systems to implement the above patterns and benchmarks. M3. Investigate options for automatic generation of patterns and/or benchm arks using existing approaches [39], [54]. M4. Identify or create a centralized repository to host and curate the above patterns and benchmarks [19], [39]. Workﬂow systems differ at varying degrees such as expressivity, execution models, and ecosystems. These differences are ma inly d ue to individual implementations of language, control mechanisms (e.g ., fault tolerance, loops), data management mechanisms, execution backends, reproducibility asp e cts for sharing workﬂows, and provenance and FAIR metadata capturing. The need for interoperability is paramount and can happen at multiple technical levels (e.g., task, tools, workﬂows, d ata, m etadata, provenance, and packaging) as well as non-technical level including semantics, organizatio nal, and legal issues (e.g., licenses compatibility, data sharing policies). The need for interoperability of workﬂow applications and systems is c ommonly mo deled as a problem of porting applications across systems, which may require days up to weeks of development effort [55], [56]. Most of the p revious approa c hes for tacklin g th e interoperability problem attempted to develop complete vertical solutions. However, there is no attempt to develop an approach from a perspective of making interoperable components. Interoperable components require standardized APIs, which are still an open challenge [57], [58]. There is a tendency to tie the workﬂow with its execution model and data structures (e.g, the intertwine between the abstract work ﬂow and its execution). Understanding which component in the workﬂow system architecture accounts for which functionality, is then par a mount. Thus, separation of concerns is key for interoperability at m a ny levels, e.g. separa tion of orchestration of the workﬂow graph from its execution. Given the above, the following milestones are pr oposed: M5. Deﬁne concrete notions o f interoperability for different stakeholders, in particular workﬂow designers, workﬂow system designer, and workﬂow execution organiza tions. M6. Establish a “requirements” document per a small set of abstraction layers that will (i) capture the commona lities between components of workﬂow systems; and (ii) perform a separation of concern s to identify interope rability gaps. M7. Develop real-world workﬂow benchma rks featuring d ifferent conﬁgurations and complexities (see previous section). Such benchmarks would be key to evaluate the functionality of workﬂow systems and compu ting platf orms systematically. M8. Develop use cases for interoperability based on real-life scenarios, e.g., porting workﬂows acro ss platforms that would provide different ﬁle system and/or different resour c e manager. M9. Develop common API s that repr esent a set of workﬂow library c omponents, so a s interoperability could be ach ieved at the comp onent level [1 7], [59], [60], including APIs for deﬁning inputs, storing intermediate results, and output data. M10. Establish a workﬂow systems developer community. An immediate activity would be to d evelop a centralized repository of workﬂow-related research papers, and a workﬂow system registry aimed at DevOps and/or users. C. Improving Workﬂow Systems’ Interface with Legacy and Emerging HPC Software and Hardware Stacks Improving the interface between workﬂow systems and existing as well as emerging HPC and cloud stacks is p a rticularly important as workﬂows are designed to be used for long perio ds of time and may be moved between computing providers. This challenge is exacerbated with the specialization of ha rdware and software systems (e.g., with accelerators, virtualization, containers, and cloud or serverless infrastructures). Thus, it is important to address the challenges faced by workﬂow systems with respect to discovering and interacting with a diverse set of cyberinfrastructu re resources and also the difﬁculties a uthenticating remote connections while adhering to facility policies. Workﬂow systems require a standard method for querying a site on how to use that site, for example, inf ormation about the batch system, ﬁle system conﬁguration, data transfer methods, and machine capa bilities. It is cruc ia l then to ﬁr st understand what in formation is needed by workﬂow systems, what info rmation could be made available programmatically and what would need to be manually cur ated (similar ongoing efforts [61] may provide the foundations for this effort). A key capability provided by workﬂow systems is remote job execution, which is necessary in cases where workﬂows span facilities. However, authentication has always been challenging. Many workﬂow systems rely on fragile SSH connections and in th e past the use of GSISSH for delegated authenticatio n. Recently, sites have moved towards two factor authenticatio n and even OAuth-based solutions. There are though ongoin g efforts to provide programmatic identity and access management in scientiﬁc domains [62], [63]. While the topic of remote authentica tion is much more broad than the workﬂows community, the re are important considerations that should b e included in this discussion related to progr ammatic access, community credentials, and long- term access. Given the above, the following mileston es are proposed: M11. Document a machine-readable de scription of the essential properties of popular sites, e.g., deﬁne a JSON schema and share it on GitHub. M12. Document remote authentication requirements fro m the workﬂow perspective and organize an event involving workﬂow system developers, end users, authentica tion technology providers, and facility op e rators. In this paper, we have documented and summarized the wealth of information acquired as a result of a series of virtual events entitled the “Workﬂows Community Summit”. The goal of these summits was to identify the common and current challenges faced by the workﬂows community, and outline a vision for short- and long-term com munity activities. From this vision, we have deﬁned a community roadmap consisting of 12 milestones, which proposes solutions and technical approaches for achieving that vision. This initial series of successful events bespoke the need for continued engagement among workﬂow researchers, developers, and users, as well as e nlarging the scope of the community to also embrace key stakeholders (e.g., computing facility operators, funding agency representatives, etc.) for enabling the proposed vision and roadmap.