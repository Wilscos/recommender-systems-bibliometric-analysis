Knowledge graphs [2] are knowledge bases which represent domain knowledge as interlinked entities, forming the nodes of a graph. Driven by the recent ‘explosion’ of data, many corporations and academic institutions are relying on knowledge graphs for the modelling and analysis of large amounts of data. In this work, we use graph embeddings [7,12] to predict possible relationships between drugs in a drug knowledge base represented as a knowledge graph. Finally, we intend to rely on graph embeddings to provide relevant information and predictions on the given database, as well as to assist us in performing the tasks of relation predictions using link prediction and drug-drug similarity utilising the node similarity concepts mentioned in this work. Because of the reliance on expensive medical equipment and medical professionals to deal with nuances of the equipment and other activities, the ﬁeld of drug similarity and analysis is time consuming and expensive. We aim to achieve very good results by employing knowledge graphs to develop drug-similarity and predictions in less time and at a lower cost than previous techniques. Our drug similarity model developed will help in drug similarity discovery which will help reduce the side eﬀect [1] caused by the use of alternative similar drugs. Preprint - under review. p.gurawa1@nuigalway.ie, matthias.nickles@nuigalway.ie School of Computer Science, National University of Ireland Galway Abstract. The paper utilizes the graph embeddings generated for entities of a large biomedical database to perform link prediction to capture various new relationships among diﬀerent entities. A novel node similarity measure is proposed that utilizes the graph embeddings and link prediction scores to ﬁnd similarity scores among various drugs which can be used by the medical experts to recommend alternative drugs to avoid side eﬀects from original one. Utilizing machine learning on knowledge graph for drug similarity and recommendation will be less costly and less time consuming with higher scalability as compared to traditional biomedical methods due to the dependency on costly medical equipment and experts of the latter ones. Keywords: Knowledge Graphs · Graph Embeddings · Link Prediction · Drug Similarity Currently, knowledge graphs [2] are able to support many medical applications, due to the fact that graphs are a practical resource of many real-world applications [3]. Typically, biological knowledge graphs are built using manually selected datasets such as MIMIC-iii, ICD-9, and others. Other options include using natural language processing to lessen the work of manual information collection. Today, knowledge graphs [2] are used in a variety of biomedical applications such as Genomics, Proteomics, Drug Side Eﬀects, Drug Repurposing, Safe Drug Recommendation, and many more, indicating their popularity in this ﬁeld. The research [10] explores using representation learning in knowledge graphs for the study of drug target predictions and drug-drug interactions and [11] uses knowledge graph embeddings to perform link predictions and drug target discovery that too using the KEGG database. The work done in research [4] utilizes the use of LSTM and knowledge graph embeddings based model to predict drugdrug interaction. A section of our work inspired from the works like [5] which deals with making inferences using the learned models. The research explore various graph embedding model which has been described and compared in [6], [7] and [8]. KEGG (Kyoto Encyclopedia of Genes and Genomes) [9] is a knowledge base that contains genetic, chemical, and functional information. It contains various entities such as disease, gene, network, route, drug, and so on, and each entity type is linked to the others via a certain form of relationship as shown in the image below. KEGG was developed by Kanehisa Laboratories as a network of interconnected entities that resembles the biological ecosystem at the molecular level [11]. The database compromises ﬁve types of entities. The ﬁrst one is drugs which are comprehensive drug information site that exclusively includes pharmaceuticals that are approved in Europe, Japan, and the United States, includes information about drugs such as molecular interactions, drug metabolism, and chemical structure. The second is genes which involves an amalgamation of genes and proteins, contains information about gene sequences and their interaction with other biological entities [11]. The next are diseases, collection of single-gene disorders, multifactorial diseases, and infectious diseases with a focus on perturbation, which deals with disease interactions with other entities. Pathways comprises manually selected pathway maps containing data on metabolisms, biological processes, human diseases, drug development, and other topics. Each route is linked to entities such as diseases, medications, and genes [11]. By linking distinct entities in other KEGG databases based on this network database, the KEGG Network database represents information about medications and disorders in the form of molecular networks. Kanehisa Lab Website https://www.kanehisa.jp/ A knowledge graph is deﬁned as ”a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities and whose edges represent relations between these entities.” [2]. Formally, a knowledge graph consists of triplets, consisting of two entities (head, tail) connected or related using edges (relationship). The triplets are represented as (head,relationship, tail) or (h,r,t) with h,t ∈ E, r ∈ R where E is a set of entities and R represents set of all possible relationships that can exist between any two entities. For example, KEGG contains instances like (D11034, DRUG EFFICACY DISEASE, H00409) which represents the drug and disease are connected by the speciﬁc relationship in the database. Embedding learning is an eﬃcient way to tackle data sparseness by representing knowledge graph’s entities and relationship as low dimensional real value vectors storing the structural characteristics of the graph structure within themselves [13]. In research [7], the authors have divided the graph embedding models in three families, the ﬁrst translation distance based which utilizes distance based scoring methods, the second ones are semantic matching based which rely on similarity based scoring methods and ﬁnally the third family of neural network based models. The task of predicting the connection between any two nodes based on their node properties is referred to as link prediction. The link prediction task in graph G, where each node represents some information, is to develop a model that predicts whether two nodes are related by an edge or not [14,15].This can be used to ﬁnd additional information for an existing dataset and can also be considered an information extraction task. A link prediction job on a drug-drug interaction knowledge network, for example, can assist us in relieving new information by suggesting probable predictions between any new pair of medications. For the task of link prediction, where every entity is considered as a target entity for a triple in testing data [10], metrics such as Mean Reciprocal Rank (MRR) and Hits@n are considered for evaluations. These scoring methods are then used to generate sorted scores based on corrupted or correct triplets [16]. – MRR: Mean Reciprocal Rank calculates the mean of the reciprocals of vec- Preprint - under review tors of ranking ranks. Also, MRR is less sensitive to outliers as compared to Mean Rank [17]. MRR scores are standardised from 0 to 1, with 1 representing perfect ranking. Here, rankrefers to the rank position of the ﬁrst relevant element for the i-th query [11]. – Hits@n: It represents the model’s likelihood of ranking the relevant (true) The key characteristic of graph embeddings is that they hold the complex graph structure and interactions within themselves, with the distance between latent dimensions representing a metric for similarities between distinct graph elements [18]. The following graph embedding models has been used: – TransE: An additive model which uses distance based scoring function for the – DistMult: The algorithm uses bilinear diagonal modeling using the trilinear – ComplEx: This model represents entities and relationships as complex vector Preprint - under review fact in the top k element scores in the rank by reporting how many elements of vectors of ranking rank made it to the top n forecasts [11]. link prediction task, treating edges of graph as linear transformations [17]. The scoring function calculates a similarity between embedding of the head translated by the embedding of relationship and the embedding of the tail. Mathematically the scoring function is deﬁned as below which involves using the Lor Lnorms: As TransE involves simple translation operation by forcing one to one mappings, it does not perform well with multirelational graphs. dot product scoring function [5] deﬁned as follow: Here h, r and t are embeddings of head, relationship and tail respectively. The score function has its own limitation as it deals with the symmetric relationships as the score function is only able to capture the pairwise interaction between components of h and t along the same dimensions [6]. embeddings, which consist of two vectors, the real and imaginary [19]. Fact assertion using the asymmetric score function deﬁned below for fact (h,r,t): Also t is conjugate of t and Re() results in providing the real part of the complex value. The algorithm is an extension of DistMult that involves complex embeddings leading to better modelling of asymmetric relations due to asymmetric scoring function. Now embeddings of h,r, and t no longer exist in real space, but complex space C. For symmetric relationships, DistMult performs well with symmetric relations and ComplEx works well with antisymmetric relations [20]. – HolE: Stands for Holographic embeddings [21] which associate each entity – ConvE: Due its neural network structure it performs better in making non- – ConvKB: Unlike ConvE it utilizes 1D convolution and models the relation- Preprint - under review with a vector to capture its latent semantics [6] The scoring function for HolE is deﬁned as: Where the circular correlation operator is deﬁned as: HolE uses the simplicity of TrasnE and expressive power of the tensor product, to generate better embeddings. It is able to deal with asymmetric relationship due to the fact that circular correlation is not commutative i.e. h ∗ t 6= t ∗ h [6]. linear transformations [22]. It uses 2D convolution which is better at extracting interatcions between embeddings as compared to 1D convolution network. The scoring function is deﬁned as: Here, g is the non-linear activation function, vec indicated the 2D reshaping function and * is the linear convolution operator and W is the weight matrix. Also, ¯eand ¯eare 2D reshaping of head entity embedding and the relationship embedding respectively, with loss function BCE used : ship among same dimensional entries of the embeddings [23], which leads it to generalize the translational characteristics present in the translation based models. It represent a k-dimension embedding of every triple (v, v, v) which is viewed as a matrix A = [v, v, v]. An operation is performed on each row of the generated matrix, the row i can be represented as A,∈ R. An additional ﬁlter ω ∈ Ris operated on every row to examine the global relationships, which enhances the translation characteristics of algorithm. Finally the feature map v is generated, v = [v, v...v] ∈ Rwhich is generated as follows: Finally, these feature maps are used in scoring function of ConvKB where ∗ is convolution operation Ω and w are shared parameters. The below visual represents a section of original KEGG knowledge graph where diﬀerent entities are denoted using diﬀerent colours. It can observed how few entities are acting as the bridge to connected multiple clusters in the node. It is clear through visualization that the number of connection per entity is highly imbalanced, with few nodes connected to a large section of other nodes creating a cluster of their own. We will use the cosine similarity measure between embeddings and information gathered from the link prediction task to improve the performance of the similarity measure by capturing various aspects of similarity. The link prediction outputs are transformed into probabilities using calibration where higher link prediction probability represents higher chances of a link existing between those two nodes. We exploit the fact that if two drugs are similar their interaction with other entities in the dataset should be similar i.e. their chance of linking with other drugs should follow similar trends. Our aim will be to ﬁnd some kind of measure that can provide real value output to show the extent of similarity Link Prediction Techniques between two entities, for our use case these two entities will be two drugs so our system will act as a drug similarity system. The cosine similarity between two graph embeddings, each of length m deﬁned as V A loss function is deﬁned to measure the diﬀerence between the link prediction results of any two drugs with respect to all other possible entities in the dataset. Here p stands of count of entities taken into account in this activity. Finally, MSE will give us a sense of the diﬀerence in interaction between two drugs with other entities. The ﬁnal similarity measure will use information from both similarity measures using graph embeddings and the link prediction loss function results. The novel similarity measure is deﬁned in next equation. Like any other machine learning model, the performance of embeddings is dependent on the quantity and quality of data used, diﬀerent hyperparameters. Our experiments use embedding sizes of 100, 200, 300 with loss functions Multiclass NLL Loss and Binary Cross Entropy Loss. Optimizers used are Adam (Adaptive Moment Estimation) and Stochastic Gradient Descent are considered with learning rates 1e-1 and 1e-3 for experimentation. The test includes regularization techniques that are L1 (Lasso Regression), L2 (Ridge Regression) and L3 (Nuclear 3-norm) proposed in the ComplEx-N3 paper [24] with regularization constants 1e-5, 1e-2 and 1e-1. There are other hyperparameters which are valid for convolution graph neural net based models like ConvE and ConvKB. Their parameters are kept ﬁxed as number of feature maps per convolution kernel as 32, convolution kernel size as 3, dropout at embedding layer, convolution layer and dense layer as 0.2, 0.3 and 0.2 respectively. , v....v] is deﬁned as follows: represents link prediction probability of drug A with entity i and |E| and Sim represents the overall similarity score between a pair of drugs. The six diﬀerent graph embedding model are trained on KEGG data which is split in 80:20 as train/test set and evaluated on the link prediction task using measures like MRR and Hits@k. ComplEx has performed better than all other algorithms on the link prediction task providing the best MRR and Hits@k scores. Surprisingly, the complex graph convolution network models like ConvE and ConvKB have performed poorly on the KEGG dataset. Overall, ComplEx algorithm with embedding size of 300, Adam optimizer with learning rate of 1e-3 with Multiclass NLL Loss and L3 Regularization technique with 1e-2 as regularization constant provides best scores like 0.46 as MRR, 0.64, 0.49 and 0.37 as Hits@10, Hits@3 and Hits@1 respectively. The dimensionality reduction method PCA (Principal Component Analysis) [12] is used to reduce the larger space embeddings to smaller spaces that are easier to visualise. The embeddings are transformed to 2D space and visualized on a plot marking the diﬀerent entities present in dataset namely drug, disease, gene, network and pathway with diﬀerent colours for better understanding of relationships between entities. It can be observed how the graphed embedding algorithm cleverly generated the embeddings such that diﬀerent entities are present in the diﬀerent sections of the plot, storing their personalities and entity type’s within them. Fig. 2. Performance of diﬀerent graph embedding algorithm on KEGG The learned model is used to ﬁnd possibilities for new unseen relationships between entities to provide meaningful insights. The model outputs the rank of the statement, score of the statement which is normalized to a deﬁnite range from 0 to 1 to give a sense of a comparative probabilities among diﬀerent statements. The transformation of the scores (real number) to probabilities (between 0 and 1) is performed using the expit transformation, which takes any real number x and transforms it to a value in (0, 1). The model used for below predictions is the ComplEx model that has provided best scores, 0.46 as MRR, 0.64, 0.49 and 0.37 as Hits@10, Hits@3 and Hits@1 respectively. Table 1. Calculating probability of relationships using the learned ComplEx Model Fig. 3. Plotting of embeddings in 2D spaces marked wrt to entity type The table shows that there are a few assertions for which the model projected fairly high odds. The probability of relationship between D11056 (Drug name : Mirtazapine hydrate) and HSA:3352 (Gene name : HTR1D, 5-HT1D, HT1DA, HTR1DA, HTRL, RDC4) under this model is quite high with 0.94, which suggests a possible connection between these two entities. Similarly, the relationship between D04905 (Drug name : Memantine hydrochloride) and pathway hsa04728 have a high connection probability of 0.987 under the model. Graph embeddings generated by the trained graph model are utilized to determine the possible similarity of two drugs. A loss function like mean square error is used to measure the diﬀerence between the link prediction scores of drug A with other entities and drug B with other entities. The new similarity measure incorporates inputs from the cosine similarity between the embeddings of the pair of drugs and the link prediction score mean square error, which is expressed in section 4.5. The top 10 possibly similar drugs with respect to D00043 (Drug name : Isoﬂurophate Fluostigmine) sorted based on the novel similarity measure are deﬁned below: Table 2. Top 10 possibly similar drugs with respect to Drug D00043 (Drug name : Isoﬂurophate Fluostigmine) 10 D02068 0.915335 0.000388 2357.032571 Tacrine hydrochloride (USP) It can be observed that D01228 (Drug name : Distigmine bromide (JP18/INN)) provides the highest ratio score with respect to drug D00043 (Drug name : Isoﬂurophate Fluostigmine) due to its high cosine similarity and low link prediction mean square error, indicating a higher probability to perform similar to D00043 when interacted with other possible entities. Both drugs belong to the Neuropsychiatric agent class and are part of drug groups DG01595 (Drug group name : Cholinesterase inhibitor) and DG01593 (Drug group name : Acetylcholinesterase inhibitor) and targeting similar genes like HSA:43 (Gene name : ACHE, ACEE, ARACHE, N-ACHE, YT), indicating a high similarities between Prediction the pair of drugs. For evaluation we have used Tanimoto coeﬃcient, which is used to calculate the chemical similarity between molecules. It is deﬁned as below, where S represent molecular similarity between A and B, a represents the number of on bits in A, b is number of on bits in B and c represents number of on bits in both A and B. The Tanimoto coeﬃcient values for top drugs are 0.049, 0.056, 0.01, 0.046, 0.014, 0.021, 0.012, 0.77, 0.53. The chemical structure similarity is not capable to generalize the trend as there are drugs that treat same clinical problems but are diﬀerent in structures, such as Migitol and Glipizide which both are used for diabetes but have completely diﬀerent structures [25]. Finally some clinical drug similarity experiments will be a good choice for evaluation which is kept out of scope from this research. Overall, the use of graph embedding models is an excellent choice to tackle the problem of ﬁnding similarity of drugs, and can be easily scalable by incorporating other medical datasets to provide our graph embedding model with more connected and larger databases. More data which eventually lead our models to generalize better by producing less over-ﬁtted model, eventually providing better performance on link prediction task. Graph embeddings models using neural networks, convolution neural networks and attention models is an exciting ﬁeld which can help us learn better about the complex graph structure their nodes and edges. Representing entities and relationships of a knowledge graph in a form of low dimensional embedding can be used for various graphical structures to ﬁnd new insights using operations like link predictions. This work eﬀectively contributes a machine learning pipeline that uses graph embeddings and link prediction algorithms to uncover drug similarity and capture unique relationships and possibilities in a biomedical database, with systematic comparison between diﬀerent graph embedding algorithms.