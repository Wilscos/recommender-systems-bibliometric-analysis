<title>Evaluating the Robustness of Targeted Maximum Likelihood Estimators via Realistic Simulations in Nutrition Intervention Trials</title> <title>Haodong Li | Sonali Rosete | Jeremy Coyle | Rachael V. Phillips | Nima S. Hejazi | Ivana Malenica | Benjamin F. Arnold | Jade Benjamin-Chung | Andrew Mertens | John M. Colford Jr | Mark J. van der Laan | Alan E. Hubbard*</title> eﬀects parameters as well as their standard errors and resulting conﬁdence intervals. <title>arXiv:2109.14048v1  [stat.ME]  28 Sep 2021</title> over-ﬁtting of nuisance parameter functionals and leads to more robust inferences. <title>1 INTRODUCTION</title> Epidemiological studies, particularly based on randomized trials, often aim to estimate the average treatment eﬀect (ATE), or another causal parameter of interest, to understand the eﬀect of a health intervention or exposure on an outcome of interest. Most commonly, in observational studies, inverse probability of treatment weighted (IPTW) estimation and its variants have been used for this purpose . Alternative estimators for causal inference include substitution (or direct) estimators based on G-computation , those based on the approach of estimating equations (EE) , including IPTW and its augmented variant (A-IPTW), and substitution estimators developed within the framework of targeted learning (TL) (we also refer to targeted maximum likelihood estimator, TMLE, a product of this framework ). The latter of these has seen increasing use in recent years, both in biostatistical methodological research and applied public health and medical research . In Table 1, we provide a list of studies that have examined the relative performance of TL-based and competing estimators (mainly against EE-based methods), including a summary of whether the results suggested superior, neutral, or poorer relative performance of TL-based estimators in comparison to other estimators (the “Pro/Con” column). Thus, while this work is contextualized within dozens of previous studies, few such studies performed “realistic” simulations, and even fewer compared several variants of TL estimators alongside corresponding EE approaches. For example, in Zivich and Breskin’s paper , the authors compared G-computation, IPTW, A-IPTW, TMLE and double cross-ﬁt estimators with data generated from predeﬁned parametric models. Exceptions are eﬀorts that used the proposed realistic bootstrap to evaluate the performance for data-generating distributions modeled semiparametrically (using ensemble machine learning) from an existing data set. These include a study of estimating variable importance under positivity violations using collaborative targeted maximum likelihood estimation (C-TMLE) . In this paper, we use an augmentation of this proposed methodology to examine the relative performance of several versions of both TL and EE estimators in ten realistic data simulations, each based on data collected as part of the Knowledge Integration (KI) database from the Bill & Melinda Gates Foundation . In so doing, we provide a realistic survey, across both diﬀerent data-generating distributions and diﬀerent study designs, of the relative performance of estimators of causal parameters. TABLE 1 Overview of literature on comparison of TMLE and other estimators. The Pro/Con column refers to a simple binary classiﬁcation of the relative performance of the TMLE estimators reported in the paper, "Pro" indicating that the TMLE performed superior to other competing estimators. <title>2 BACKGROUND</title> As large and complex data sets have become increasingly more commonplace, the habitual use of parametric approaches is encountering more data science research for which they are ill-suited. . This has led to machine learning (ML) taking a more central role in deriving estimators of causal impacts in very big statistical models (semiparametric). The theory for the use of ML in the estimators discussed herein has been continuously reﬁned, from developing double robust estimators (both A-IPTW and TMLE substitution estimators) to augmentations of these estimators that are more robust to the overﬁtting potentially introduced by ﬂexible ML ﬁts. The latter modiﬁcations to the original estimators are the cross-validated TMLE (or CV-TMLE, chapter 27 in van der Laan and Zheng ), and subsequently the proposal for an analogous modiﬁcation to estimating equation approaches (double machine learning or cross-ﬁtting ). While simulation studies have investigated all of these estimators, they have yet to be analyzed together in a single series of realistic simulation studies. Here, we seek to determine how well these estimators perform in realistic settings, under which conditions they perform best, which augmentations provide the most robustness, and whether or not the results support more general recommendations. In addition, there exist other choices of target parameter when the one being analyzed fails to have adequate performance for any of the competing estimators, such as realistic rules . A recently developed machine learning algorithm (the highly adaptive lasso; HAL ), is potentially an important improvement in estimating realistic DGDs for simulation studies such as ours. It can be optimally undersmoothed to dependably generate realistic estimates of the actual data generating distributions. HAL is particularly well suited to these types of simulations, as it uses a very large nonparametric model and can be tuned to be as ﬂexible as the data support. In this paper, we explore the use of HAL as a basis in conducting realistic data-inspired simulations. The results suggest the proposed use of HAL for realistic data-generating simulations could provide a general method for choosing between machine-learning-based estimators for a particular parameter and data set. We ﬁrst introduce the data sets that were selected to motivate our realistic simulations, describe the steps taken for simulating data, including a short description of the estimators tested, and discuss the results. The simulations suggest a general recommendation for the use of an additional layer of cross-validation (CV-TMLE and double machine learning) to ensure robust inference in ﬁnite samples. <title>METHODS 3.1 Study Selection</title> We utilized data from ten nutrition intervention trials conducted in Africa and South Asia. In all studies, the measured outcome was a height-to-age Z-score for children from birth to 24 months, which was calculated using World Health Organization (WHO) 2006 child growth standards . Details about the resulting composite data, study design and data processing, can be found in companion technical reports . All interventions were nutrition-based, and for the purposes of this analysis, multi-level interventions were simpliﬁed to a binary treatment variable (e.g. nutrition intervention - yes/no). Although diﬀerent baseline covariates were measured among these studies, there was signiﬁcant overlap. The sample size of each study is shown in Table 2. We anonymized the study IDs and removed the location information due to conﬁdentiality concerns. Details on each study can be found in the shuﬄed list in Section B of the Appendix. TABLE 2 Dimensions of datasets of Nutrition Intervention Trials, with 𝑛 representing the number of children in sample and 𝑝 being the number of covariates. <title>3.2 Data Processing</title> Data from each study was cleaned and processed for this analysis. Our goal for deﬁning the analysis data used to simulate is diﬀerent from the goals of the original studies and thus our data processing might diﬀer from that used in the resulting publications of the study results. We note that the data are used to motivate the simulations, but, since we deﬁne the true DGD to be one that we estimate for each study, and at that point diﬀerences with the original study become irrelevant to our comparisons of estimators. Data was ﬁltered down to the last height-to-age Z-score measurement taken at the end of each study for each subject. Subjects were dropped if either their treatment assignment (𝐴) or outcome measurement (𝑌 ) were missing. For covariates (𝑊 ) that were missing, those that were continuous and discrete were imputed using the median and mode, respectively. In both cases, missingness indicator variables were added to the data set for each covariate with missing rows. As mentioned above, the treatment assignment variable (𝐴) was binarized if it consisted of more than two treatment arms. The control and treatment groups were originally assigned in each study as described in Section B of the Appendix. <title>3.3 Simulation with Undersmoothed Highly Adaptive Lasso 3.3.1 Undersmoothed Highly Adaptive Lasso</title> = 𝛽 By the assumption of ﬁnite sectional variation norm (an entropy assumption required of all but two of the estimators), cross-validated TMLE and double-robust EE) we have the corresponding subspace Ψ = {𝜓 ∶ 𝛽 𝑀} The HAL estimator starts with a very large number (at most 𝑛 ∗ (2 − 1)) of basis functions that are indicator functions with support at the observed data values. In practice, when some covariates are categorical or binary, the number of unique basis functions will be much fewer than the upper bound. Moreover, to avoid overﬁtting, one can deﬁne a subspace of the linear model such that: 𝛽 ≤𝐶, for submodels where the 𝐿 -norm is bounded by 𝐶. The dimension of basis functions can be restricted. For example, one can consider only main-term indicators for each of the original predictors as well as all second order tensor products (interaction terms involving the main eﬀect terms). One can use cross-validated selection of 𝐶 to optimize the ﬁt of the model to future observations from the data-generating distribution (DGD). It has been recently shown that undersmoothing HAL (using a 𝐶 that is larger than that selected by cross-validation) can yield an asymptotically eﬃcient estimators for functionals of the relevant portions of the DGD while preserving the same rate of convergence, and also solving the eﬃcient score equation for any desired path-wise diﬀerentiable target feature of the data distribution . As a consequence, an undersmoothed HAL results in an eﬃcient plug-in estimator of the desired estimand, and moreover, it will also be eﬃcient for any other smooth estimands of the data distribution . This could serve as the basis for using HAL in our settings; that is, to estimate the DGD by HAL in a way that optimally preserves the relevant functionals. More intuitively, HAL, with the properly chosen 𝐶, will result in a DGD for simulations that is as close as one can get nonparametrically to the true DGD, without blowing up the variance of estimation. So, it creates a comparison that is as faithful as possible to the DGD of interest, itself represented by a single data set (experiment). Thus, we argue that it can serve as the basis of a simulation where one wishes to compare estimators for the data in hand. We provide more rigorous justiﬁcation below. In our study, we only use the undersmoothed HAL to generate data without pre-specifying any parameter of interest. The stopping criterion for this undersmoothing process is to increase the initial bound 𝐶 until the score equations formed by the product of basis functions and residuals are solved at the rate of . Namely, for all “non-trivial directions” (combinations of 𝑠, 𝑖 with non-zero coeﬃcients selected by the initial ﬁt) we need: Thus the inﬂuence curve of the empirical estimator is 𝜙 𝑌 − Ψ (𝑄 ), denote it as 𝐷 (𝑄 ). With it, we can obtain 𝐷 (𝑄 ) by projecting 𝐷 (𝑄 ) onto the tangent space 𝑇 (ℙ . In addition, since ℙ(𝑂) = ℙ(𝑌 , 𝐴, 𝑊 ) = ℙ(𝑌 𝐴, 𝑊 )ℙ(𝐴, 𝑊 ), the tangent space 𝑇 (ℙ ) can be decomposed as: 𝑇 (ℙ ) = 𝑇 (ℙ ) = 𝑇 (ℙ ) ⊕ 𝑇 (ℙ . So the projection of 𝐷 (𝑄 ) on 𝑇 (ℙ ) is equal to the sum of the projections of 𝐷 (𝑄 ) on 𝑇 (ℙ ) and 𝑇 (ℙ ), namely, ⊓(𝐷 (𝑄 )𝑇 (ℙ )) = ⊓(𝐷 (𝑄 )𝑇 (ℙ )) + So, 𝐷 (𝑄 ) = 𝜙 (𝑌 − ) + 𝔼 (𝜙 𝑌 𝐴, 𝑊 ) − Ψ (𝑄 ). Now we have proved that 𝜙 (𝑌 − ) is a component of 𝐷 (𝑄 ). Another observation from the calculation above is that (𝑄 ) = 𝐷 (𝑄 ). For diﬀerent pairs of (𝑠, 𝑖), each 𝐷 (𝑄 ) corresponds with an EIC for a particular target parameter Ψ (𝑄 ). For each plug-in estimator Ψ (𝑄 ) being asymptotically linear we want at minimal 𝑃 (𝑄 ) = 𝑜 (𝑛 ) for every (𝑠, 𝑖), which is guaranteed by our choice of criterion. By doing so, we will also be solving 𝑃 𝛼(𝑠, 𝑖)𝜙 )(𝑌 − ) for any 𝛼 vector with ﬁnite 𝐿 -norm, which enables us to approximate any function of (𝐴, 𝑊 ) with rate approximately equal to 𝑛 . So in this way we are rich enough to guarantee to solve any EIC that can be written as 𝑓 (𝐴, 𝑊 )(𝑌 − ), thereby cover all EIC of features of Q. So the undersmoothing process essentially yields an estimator that is eﬃcient for any target feature of Q that is pathwise diﬀerentiable. Combined with the fact that when the bias of an estimator is smaller than then it has minimal impact on coverage, we choose (1) as the stopping criterion based on the proof above. The speciﬁc procedure is stated as follows: Step 1. Fit the HAL with 𝐿 -norm, obtain the set of basis functions and a starting value of 𝜆, denote it as 𝜆 . This 𝜆 is a CV-optimal value of the penalty parameter returned by the hal9001 package , which is essentially from the “cv.glmnet” function with 10-fold cross-validation. Step 2. Calculate the absolute value of the normalized score equations for each direction: Step 3. Take the maximum of this value from all subsets. Compare the max with . If the max is larger, then increase the bound 𝐶 (i.e. decrease the value of the penalty parameter 𝜆) and reﬁt the HAL. Step 4. Repeat 1,2,3 until the stopping criterion is satisﬁed. In addition, we speed up the algorithm by controlling the number of basis functions in the initial HAL ﬁts. First, we set the maximum interaction degree to 𝕀(𝑝 ≥ 20) ∗ 2 + 𝕀(𝑝 < 20) ∗ 3, where 𝑝 is the number of covariates. Second, we use binning method to restrict the maximum number of knots to 𝑛∕(2 ) for the 𝑑 degree basis functions. These hyperparameters can be set through the hal9001 package . We make the decisions on hyperparameters based on two factors: they can help form a rich model with complex interaction terms and the computing time is acceptable. To make it more rigorous, a cross-validation-based tuning procedure can be considered in future practice. In the Appendix, we provide a list showing the variables included in the 𝑄 models after undersmoothing (Table A1). <title>3.3.2 Data Generating Process</title> The DGD for each study was based upon the following structural causal model (SCM): where 𝑊 , 𝐴, and 𝑌 are, in time ordering, the confounders, the binary intervention of interest and the outcome, respectively, with the 𝑈 exogenous independent errors and deterministic functions, 𝑓 . Speciﬁcally, the following steps were taken: 1. Covariates 𝑊 were sampled with replacement from the study data sets with sample size 𝑛, where 𝑛 is the size of the original data set. 2. The undersmoothed HAL ﬁt was then used to predict ℙ(𝐴 = 1𝑊 ). The intervention 𝐴 was then sampled using a binomial distribution with the predicted ℙ(𝐴 = 1𝑊 ). 3. The outcome 𝑌 was then simulated with the undersmoothed HAL ﬁt, using the sampled 𝑊 and simulated 𝐴 as input. A mean zero, normal random error was added to the simulated 𝑌 , using a variance based upon the residual variance of the predicted 𝑌 (Namely, ( ̂𝑦 − 𝑦 ). Note, we could have used other ways of estimating the error distribution in step 4, including density estimation using HAL, but we left this for future studies. Steps 1 through 3 were repeated 500 times to generate the data sets for each simulation. For each of the study data (Table 2), we repeated these steps and analyzed the performance of the competing estimators separately by study. <title>3.3.3 Target parameter</title> We calculate the true ATE value for each study by ﬁrst randomly drawing a large number of observations (𝑁 = 50000) from the empirical of 𝑊 and using: [𝔼 (𝑌 𝐴 = 1, 𝑊 ) − 𝔼 (𝑌 𝐴 = 0, 𝑊 )] where we deﬁne the 𝔼 (𝑌 𝐴 = 1, 𝑊 ) and 𝔼 (𝑌 𝐴 = 0, 𝑊 ) term using the ﬁtted undersmooth HAL model. Note that our simulation process insures the randomization assumption is true and there is no asymptotic violation of the positivity assumption. However, there can be practical violations of positivity (close to 0 or 1 estimated probabilities of getting treatment for some observations given the 𝑊 ) which can deferentially impact estimator performance. <title>3.4 The Estimation Problem</title> The target parameter depends on the true DGD, ℙ , through the conditional mean (𝐴, 𝑊 ) = 𝔼 (𝑌 𝐴, 𝑊 ) and the marginal distribution 𝑄 = ℙ (𝑊 ) of 𝑊 , so we can write Ψ(𝑄 ), where 𝑄 = ( , 𝑄 ). Our targeted learning estimation procedure begins with estimating the relevant part 𝑄 of the data-generating distribution ℙ needed for evaluating the target parameter The two general methods we compare are substitution and estimating equation estimators. Depending on the speciﬁc estimator, they can depend on estimators of of the propensity score, 𝑔 (𝑊 ) = ℙ(𝐴 = 1, 𝑊 ), the outcome model, 𝑄 (𝐴, 𝑊 ), and sometimes both. We use consistent settings when modeling the outcome and the propensity score via Super Learner (see section 3.8 below for details). The estimators we compare are not exhaustive and new methods will be developed, so such studies will continue to be important sources of information for deciding what to do in practice. We quickly describe the particular estimators compared in our study below. <title>3.5 Inverse Probability of Treatment Weighting Estimator</title> 1 − 𝐴 ∗ 𝑌 ∗ 𝑌 (𝑊 (1 − 𝑔 (𝑊 )) where 𝑔 (𝑊 ) is the estimate of the true propensity score (𝑔 (𝑊 )). IPTW is not a double robust estimator, in that its consistency depends on consistent estimation of the propensity score . As it is not a substitution estimator, it is not as robust to sparsity However, it is a commonly used estimator of the ATE, and its form and relationship to well-known inverse probability methods in the analysis of survey data make it relatively popular. We derived statistical inference using a conservative standard error which assumes that 𝑔 is known (there is an extensive literature on IPTW estimators, but is a good reference for technical details). Speciﬁcally, the standard error for this estimator was constructed by multiplying 1∕ 𝑛 by the standard deviation of the plug-in resulting inﬂuence curve: <title>3.5.1 Cross-Validated Inverse Probability of Treatment Weighting (CV-IPTW) Estimator</title> To avoid problems that arise when 𝑔(𝑊 ) is overﬁt, we also implemented the CV-IPTW estimator by adding another layer of cross-validation when estimating the propensity score . Speciﬁcally, the same SL ﬁtting procedure was implemented on training sets. Then, we use this estimate of 𝑔 on the corresponding validation sets; as such, we employ a nested cross-validation. In practice, we used the “Split Sequential SL” method, an approximation to the nested cross-validation proposed by Coyle , to speed up the estimation while obtaining similar results to standard nested cross-validation. More details on the implementation can be found in section 3.8 below. <title>3.6 Augmented Inverse Probability of Treatment Weighted (A-IPTW) Estimator</title> <title>3.6.1 Cross-Validated Augmented Inverse Probability of Treatment Weighted (CV-A-IPTW) Estimator</title> Similar to CV-IPTW, to avoid overﬁtting of the outcome model (𝑄) or propensity score model (𝑔), we implemented the CV-A-IPTW estimator by adding another layer of cross-validation when estimating the 𝑄 and 𝑔. In practice, as discussed above for the IPTW estimator, we used the “Split Sequential SL” method proposed by Coyle to speed up the estimation (for more details, see section 3.8 below). <title>3.7 Targeted Maximum Likelihood Estimator (TMLE)</title> where 𝑄 is the estimate of 𝑄 and (⋅, 𝑊 ) the initial estimate of (⋅, 𝑊 ). The next step is to update the estimator above toward the parameter of interest. The targeting process uses 𝑔 in a so-called clever covariate to deﬁne a one-dimensional model for ﬂuctuating the initial estimator. The clever covariate is deﬁned as: A simple, one-variable logistic regression is then run for the outcome 𝑌 on the clever covariate, using 𝑙𝑜𝑔𝑖𝑡 (𝐴, 𝑊 ) as the oﬀset to estimate the ﬂuctuation parameter 𝜖. This is used for updating the initial estimate into a new estimate as follows: where 𝜖 is the estimate of 𝜖. The updated ﬁt is used to calculate the expected outcome under 𝐴 = 1 ( (1, 𝑊 )) and 𝐴 = 0 ( (0, 𝑊 )) for all subjects. These estimates are then plugged into the following equation for the ﬁnal TMLE estimate of the ATE: <title>3.7.1 Cross-Validated Targeted Maximum Likelihood Estimation (CV-TMLE)</title> <title>3.7.2 Collaborative Targeted Maximum Likelihood Estimation (C-TMLE)</title> Collaborative Targeted Maximum Likelihood Estimation (C-TMLE) is an extension of TMLE. In the version used for estimation in this study, it applies variable/model selection for nuisance parameter (e.g. the propensity score) estimation in a “collaborative” way, by directly optimizing the empirical metric on the causal estimator . In this case, we used the original C-TMLE proposed by van der Laan and Gruber , which is also called “the greedy C-TMLE algorithm”. It consists of two major steps: ﬁrst, a sequence of candidate estimators of the nuisance parameter is constructed from a greedy forward stepwise selection procedure; second, cross-validation is used to select the candidate from this sequence which minimizes a criterion that incorporates a measure of bias and variance with respect to the targeted parameter . More recent development on C-TMLE includes scalable variable-selection C-TMLE and glmnet-C-TMLE algorithm , which might have improved computational eﬃciency in high-dimensional setting. <title>3.8 Computation</title> Our simulation study was coded in the statistical programming language R . We used hal9001 and glmnet packages to generate the data via undersmoothed HAL. We used sl3 , tmle3 and ctmle packages to implement each of the estimators described above. To estimate the propensity score and the conditional expectation of the outcome, linear models, mean, GAM (general additive models) , ranger (random forest) , glmnet (lasso), and XGBoost with diﬀerent tuning parameters were used to form the SL library. For “Study 9”, we dropped GAM and ranger from the learner library to improve the computational eﬃciency. Ten-fold cross-validation was chosen by default of sl3 package for every SL ﬁt. We used logistic regression meta-learner for propensity scores, and non-negative least squares meta-learner for estimating conditional expectation of the outcome. We truncated the propensity score estimates 𝑔 (𝑊 ) between [0.025, 0.975] for all estimators. <title>RESULTS 4.1 Undersmoothed HAL Models and The True Average Treatment Eﬀect</title> We implemented undersmoothed HAL on the real data and used the ﬁtted model to generate sample for each simulation. Details of each model and the resulting true ATE values are presented in Table 3. TABLE 3 Statistics of the HAL ﬁts to the individual studies, including the sample size, dimension, and number of basis functions used for the treatment model (𝑔) and the corresponding outcome model (𝑄), the corresponding lambda penalty and the resulting 𝐿 norm. For Study 7, 8 and 10, the initial HAL ﬁts of 𝑔 models contain no variables, so one 𝐴 is randomized as in a clinical trial. Thereby, the undersoomthing process for 𝑔 model was omitted for these three studies, and the initial HAL models were used instead. This is not surprising since all ten studies were randomized controlled trials (RCT). Grouping categorical intervention variables into binary variables at data cleaning step might preserve or change the randomization. The remainder of the studies included basis functions in 𝑊 and so are more akin to observational studies. For only these studies, we also compare the performance of the estimators above with the standard diﬀerence-in-means estimates, which is also provides consistent estimators for the ATE for these three data-generating distributions. On the other hand, the counts of non-zero coeﬃcients (“Num.coef.” in Table 3) in the undersmoothed 𝑄 models are large for the remaining studies, and so, regardless of the original treatment mechanism that underlied these studies, these ones do not come from a simple treatment randomization model. The details on the variables included after undersmoothing can be found in Table A1 in the Appendix. <title>4.2 Estimators’ Performance</title> The results are shown in Figure 1 and Table 4. Variance dominates bias for all estimators and so contributes overwhelmingly to the mean squared error (MSE) and the relative MSE (rMSE), where rMSE was relative to the IPTW estimator’s MSE. Putting aside Study 1 for now, the MSE/rMSE results suggest that the A-IPTW generally is more eﬃcient than the other estimators, the TMLE, CV-TMLE, CV-A-IPTW and C-TMLE with similar MSE to each other, and the IPTW and CV-IPTW having more erratic performance. The bar plots of the main performance metrics in Table 4 can be found in the Appendix (see Figure A1 - A5) The 95% conﬁdence interval (CI) coverage, however, shows diﬀerent relative performance (Figure 1, Table 4). Taking 92.5% as the lower bound deﬁning consistent coverage, then we can observe that: The CV-A-IPTW consistent coverage for all studies. The CV-TMLE and C-TMLE had consistent coverage for all studies except study 1. The TMLE and A-IPTW had coverage ranging from 90% to 95% for most studies. IPTW and CV-IPTW estimates of CI had very conservative coverage (close to 100%) for most studies. To examine more closely issues of CI coverage, we removed the bias introduced by the estimation procedure for the standard error by using the true sample variance of each estimator (i.e. the sample variance of the estimator across 500 simulations) to derive the standard error (“Coverage2” in Table 4). The coverage of this CI is the oracle coverage one would obtain if one is given the true variance. For this measurement, both CV-TMLE and CV-A-IPTW achieved 95% coverage in all studies, followed by TMLE, C-TMLE, IPTW and CV-IPTW with 95% coverage for nine studies. A-IPTW had 95% coverage for eight studies. FIGURE 1 Dot plot of the main metrics of performance TABLE 4 Performance of targeted learning and estimating equation estimators by study within the HAL-based simulations. <title>4.3 Exploration on Positivity Violation</title> We now consider Study 1, where the TMLE and CV-TMLE had signiﬁcantly anti-conservative coverage. In this case, certainly one cause appears to insuﬃcient experimentation of treatment within some covariate groups. Speciﬁcally, consider Figure 2, which shows the distributions of the adjustment variable, W_perdiar24 in Study 1. As one can see, there are large diﬀerences in the marginal distribution of this covariate; in fact, a ﬁt 𝑔 without smoothing would result in a perfect positivity violation. However, given the variance-bias trade-oﬀ resulting in the estimators, it is possible that these empirical violations are smoothed over. A potential consequence of this positivity violation is that the resulting estimator, for the parameter which requires support in the data, will be unstable and biased. Table 5 shows the performance of estimators before and after dropping the variable W_perdiar24 in Study 1. We can observe that all estimators can beneﬁt from removing the problematic variable in terms of higher coverage or lower MSE. TABLE 5 Estimators’ performance with/without W_perdiar24 in Study 1 to show the impact of one covariate on performance due to positivity violations. Columns are deﬁned as in table 4. <title>4.4 Estimators’ Eﬃciency in Randomized Experiment Setting</title> TABLE 6 Relative performance of the two CV-estimators with a simple diﬀerence in means in the context of the three studies for which treatment was unrelated to covariates (thus equivalent to randomized clinical trial). Columns are deﬁned as in table 4. <title>5 CONCLUSION</title> The ultimate goal of studies, such as ours, is to move incrementally towards algorithms that can take information on the design, causal model and known constrains in order to produce a data-adaptively optimized estimator without relying on arbitrary model assumptions. Asymptotic theory can provide guidance on some of the choices, but asymptotic eﬃciency is not a guarantee for superior performance in ﬁnite samples. Thus, simulation studies that are based on realistic DGD’s are invaluable for both evaluating estimators and modifying them to increase ﬁnite-sample robustness. We provided results supporting the use of a strategically undersmoothed HAL for estimating the relevant components of the DGD in data-driven simulations. Though much remains unresolved, such an approach could be an approach for generating synthetic data Our results suggest that if accurate inferences are the highest priority, then the CV-A-IPTW, CV-TMLE, and C-TMLE are good choices for providing robust inferences. Speciﬁcally, the results suggest that CV-TMLE might serve as an “oﬀ the shelf” algorithm given that 1) it is an asymptotically linear estimator; 2) it is consistent in a large class of statistical models; 3) it allows for the use of aggressive ensemble learning, while protecting the ﬁnal performance of the estimator with an outer layer of cross-validation; 4) its inﬂuence-curve-based standard error combined with the well-behaved (normal) distribution of the estimator results in near perfect coverage for all but one of the studies used. Our results also suggest that modiﬁcations to the algorithms for other estimators (such as improving the SE estimator for the A-IPTW) would result in an estimator with acceptable CI coverage and relatively low MSE. We also suggest one basis for deciding which estimator to use for particular data is to perform a similar simulation study for the data based upon ﬁtting the undersmoothed HAL to derive the DGD. Then, one could choose to report the results from the estimator that provided the most reliable performance in such a simulation study. Of course, this is itself a form of over-ﬁtting, since it uses the data both for estimator selection and for reporting the results of that estimator applied to the original data. However, it seems better than applying an arbitrary estimator and hoping that the advertised asymptotic performance matches the performance on the data of interest. Finally, our results support the observations that careful use of covariate information can be used to gain eﬃciency in the randomized experiment setting. <title>ACKNOWLEDGMENTS</title> This research was ﬁnancially supported by a global development grant (OPP1165144) from the Bill & Melinda Gates Foundation to the University of California, Berkeley, CA, USA. We would also like to thank the following collaborators on the included cohorts and trials for their contributions to study planning, data collection, and analysis: Muhammad Sharif, Sajjad Kerio, Ms. Urosa, Ms. Alveen, Shahneel Hussain, Vikas Paudel (Mother and Infant Research Activities), Anthony Costello (University College London), Noel Rouamba, Jean-Bosco Ouédraogo, Leah Prince, Stephen A Vosti, Benjamin Torun, Lindsey M Locks, Christine M McDonald, Roland Kupka, Ronald J Bosch, Rodrick Kisenge, Said Aboud, Molin Wang, Azaduzzaman, Abu Ahmed Shamim, Rezaul Haque, Rolf Klemm, Sucheta Mehra, Maithilee Mitra, Kerry Schulze, Sunita Taneja, Brinda Nayyar, Vandana Suri, Poonam Khokhar, Brinda Nayyar, Poonam Khokhar, Jon E Rohde, Tivendra Kumar, Jose Martines, Maharaj K Bhan, and all other members of the study staﬀs and ﬁeld teams. We would also like to thank all study participants and their families for their important contributions. We are grateful to the LCNI5 and iLiNS research teams, participants and people of Lungwena, Namwera, Mangochi and Malindi, our research assistants for their positive attitude, support, and help in all stages of the studies. In addition, this research used the Savio computational cluster resource provided by the Berkeley Research Computing program at the University of California, Berkeley (supported by the UC Berkeley Chancellor, Vice Chancellor for Research, and Chief Information Oﬃcer). The authors would like to further thank the university and the Savio group for providing computational resources. <title>Author contributions</title> Conceptualization: A.E.H., H.L., S.R. Funding Acquisition: J.M.C., A.E.H., M.J.V., B.F.A. Data curation: A.M., J.B., J.C. Formal analyses: H.L., S.R. Methodology: H.L., S.R., A.E.H., M.J.V. Visualization: H.L., S.R., J.C. Writing – Original Draft Preparation: H.L., S.R. Writing – Review & Editing: A.E.H., H.L., R.V.P., N.H., I.M., B.F.A., J.B. <title>Financial disclosure</title> None reported. <title>Conﬂict of interest</title> The authors declare no potential conﬂict of interests. <title>Data availability</title> <title>APPENDIX A . SUPPLEMENTAL FIGURES AND TABLES</title> <title>B . SUPPLEMENTAL INFORMATION ON STUDIES B.1 iLiNS-Zinc</title> The iLiNS Zinc intervention was a placebo-controlled, cluster-randomized trial that enrolled children in Burkina Faso . Data used in this analysis was collected between 2010 and 2012 from 3,265 children in rural southwestern Burkina Faso. The objective of this study was to compare the eﬀect of providing small-quantity lipid-based nutrient supplements (SQ-LNS) with varying amounts of zinc, along with illness treatment, to standard care on zinc-related outcomes. The outcome measure used here was a height-to-age z-score for children. For the purposes of this analysis, the treatment arms were split into a control group (placebo) and a treatment group (zinc supplementation or SQ-LNS with varying amounts of zinc). <title>B.2 iLiNS-DOSE</title> The iLiNS-DOSE intervention was a randomized, controlled, single-blind, parallel-group clinical trial that enrolled healthy infants between 5.5 and 6.5 months of age in Malawi . Data used in this analysis was collected between 2009 and 2011 for 1,931 children in rural Malawi. The objective of this trial was to identify the lowest growth-promoting daily dose of modiﬁed lipid-based nutrient supplements. The outcome measure used here was the children’s height-to-age z-score. For the purposes of this analysis, treatment arms were split into a control group (no supplement during primary follow-up period with delayed supplementation from 18 to 30 months) and a treatment group consisting of multiple interventions (varying doses of milk-containing LNSs or milk-free LNSs between 6 and 18 months of age). <title>B.3 JiVitA-3: Impact of antenatal multiple micronutrient supplementation on infant mortality</title> The JiVitA-3 intervention was a cluster-randomized, double-masked trial that enrolled pregnant women from rural Bangladesh ; data used in this analysis was collected between 2008 and 2012. The objective of this trial was to compare the eﬀects of daily maternal multiple micronutrient versus iron and folic acid supplementation on 6-month infant mortality and adverse birth outcomes. The outcome measure used here was a height-to-age z-score for infants. Treatment arms were split into a control group (folic acid and iron supplementation only) and a treatment group (supplement containing 15 essential vitamins and minerals). <title>B.4 JiVitA-4: Eﬀect of fortiﬁed complementary food supplementation on child growth in rural Bangladesh</title> The JiViTa-4 intervention was an unblinded, cluster-randomized, controlled trial that enrolled children at 6 months of age and provided daily supplements for a year . Data used in this analysis was collected between 2012 and 2014 for 5,443 children in rural Bangladesh. The objective of this trial was to compare the eﬀects of three specially formulated complementary food supplements and an international product, Plumpy’doz, with a control (no food supplement) on children’s growth outcomes. The outcome measure used here was a height-to-age z-score for children. For the purposes of this analysis, treatment arms were split into a control (nutrition counseling only) and a treatment group (food supplements with nutrition counseling). <title>B.5 WASH Beneﬁts Bangladesh</title> The WASH Beneﬁts Bangladesh intervention was a cluster randomized trial that enrolled pregnant women from villages in rural Bangladesh ; data used in this analysis was collected between 2012 and 2015 for 4,863 children. The outcome measure used here was a height-to-age z-score for children. For the purposes of this analysis, treatment arms were split into a control group (data collection only) and a treatment group. This treatment group consists of multiple interventions: chlorinated drinking water; upgraded sanitation; promotion of handwashing with soap; combination of water, sanitation, and handwashing; child nutrition counseling plus lipid-based supplements; and a combination of water, sanitation, handwashing, and nutrition. <title>B.6 WASH Beneﬁts Kenya</title> The WASH Beneﬁts Kenya intervention was a cluster randomized trial that enrolled pregnant women from western Kenya data used in this analysis was collected between 2013 and 2015 for 7,399 children. The outcome measure used here was a height-to-age z-score for children. For the purposes of this analysis, treatment arms were split into a control group (no visits) and a treatment group. This treatment group consists of multiple interventions: chlorinated drinking water; improved sanitation; handwashing with soap; combined water, sanitation, and handwashing; nutrition counseling plus lipid-based supplements; and combined water, sanitation, handwashing, and nutrition. <title>B.7 SAS Food Supplementation</title> The SAS Food Supplementation intervention was a randomized trial that enrolled four-month-old infants in an urban slum in Delhi, India . Data for this analysis was collected between 1995 and 1996 from 418 children. The objective of this study was to measure the eﬀect of feeding a micronutrient-fortiﬁed food supplement to infants on physical growth between 4 and 12 months of age. The measured outcome used in this analysis was a height-to-age z-score for children. For the purposes of this analysis, treatment arms were split into a control group (no counseling or food supplements) and a treatment group consisting of multiple interventions (milk-based cereal and nutritional counseling, nutritional counseling alone, or visitation alone). <title>B.8 iLiNS-DYAD-M</title> The iLiNS-DYAD-M intervention was a randomized trial that enrolled infants 6 to 18 months of age in rural Malawi . Data was collected between 2011 and 2015 from 1,205 children. The objective of this study was to test whether small-quantity lipid-based nutrient supplements (SQ-LNS) would promote child growth with an outcome measure of the children’s height-to-age z-score at 18 months. Treatment arms were split into a control group (iron and folate supplementation during pregnancy) and a treatment group with multiple interventions (multiple micronutrient tablets or small-quantity lipid-based nutrient supplements (SQ-LNS) during pregnancy and ﬁrst six months of lactation). Children in the SQ-LNS group received SQ-LNSs from 6 to 18 months of age, while children in the control group and multiple-micronutrient tablet group did not receive any supplementation. <title>B.9 Tanzania Child 2</title> The Trial of Zinc and Micronutrients in Tanzanian Children was a randomized 2 x 2 factorial, double-blind clinical trial . Data used in this analysis was collected from 2,396 children born to HIV-negative mothers in Tanzania. The secondary objective of this study was to determine whether daily administration of zinc or multivitamins from 6 weeks of age for 18 months would improve child growth; the measured outcome used in this analysis was the children’s height-to-age z-score. For the purposes of this analysis, treatment arms were split into a control group (placebo) and a treatment group consisting of multiple interventions (zinc only, multivitamins only, or zinc and multivitamins). <title>B.10 Lungwena Child Nutrition Intervention</title> The Lungwena Child Nutrition Intervention was a single-blind, parallel randomized trial that enrolled six-month-old healthy infants in the Lungwena area of the Mangochi District in rural Malawi . Data for this analysis was collected between 2008 and 2014 from 840 children. The objective of this study was to determine whether lipid-based nutrient supplementation as a complementary food from 6 months to 18 months of age led to lower incidence of severe stunting in infants. The measured outcome used in this analysis was a height-to-age z-score for children. For the purposes of this analysis, treatment arms were split into a control group (no extra food supplements) and a treatment group consisting of multiple interventions (milk-powder-containing LNS, soy-powder protein supplement, or maize-soy ﬂour supplement). <title>References</title> 2. Hájek J. Comment on “an essay on the logical foundations of survey sampling, part one”. The foundations of survey sampling 1971: 236. 5. Yu Z, Laan v. dM. Construction of Counterfactuals and the G-Computation Formula. U.C. Berkeley Division of Biostatistics Working Paper Series 2002. 8. Robins JM. Marginal Structural Models versus Structural Nested Models as Tools for Causal Inference. In: Halloran ME, Berry D., eds. Statistical Models in Epidemiology, the Environment, and Clinical TrialsSpringer New York; 2000; New York, NY: 95–133. 21. Talbot D, Beaudoin C. A Generalized Double Robust Bayesian Model Averaging Approach to Causal Eﬀect Estimation with Application to the Study of Osteoporotic Fractures. 2020. 25. Wei L, Kornblith LZ, Hubbard A. A Data-Adaptive Targeted Learning Approach of Evaluating Viscoelastic Assay Driven Trauma Treatment Protocols. 2019. 48. Zheng W, Laan v. dM. Asymptotic Theory for Cross-Validated Targeted Maximum Likelihood Estimation. U.C. Berkeley Division of Biostatistics Working Paper Series 2010. 55. Gill RD, Laan v. dMJ, Wellner JA. Ineﬃcient estimators of the bivariate survival function for three models. Annales de l’I.H.P. Probabilités et statistiques 1995; 31(3): 545–597. 56. van der Laan MJ, Benkeser D, Cai W. Eﬃcient Estimation of Pathwise Diﬀerentiable Target Parameters with the Undersmoothed Highly Adaptive Lasso. arXiv:1908.05607 [math, stat] 2019. 57. van der Laan MJ, Benkeser D, Cai W. Causal Inference based on Undersmoothing the Highly Adaptive Lasso. AAAI Spring Symposium 2019. 64. Coyle JR. Computational Considerations for Targeted Learning. PhD thesis. UC Berkeley, ; 2017. 70. R Core Team . R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing; Vienna, Austria: 2017. 71. Friedman J, Hastie T, Tibshirani R. Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software 2010; 33(1): 1–22. 74. Ju C, Gruber S, Laan v. dM. ctmle: Collaborative Targeted Maximum Likelihood Estimation. ; : 2017. R package version 0.1.1. 79. Tsiatis AA, Davidian M, Zhang M, Lu X. Covariate adjustment for two-sample treatment comparisons in randomized clinical trials: a principled yet ﬂexible approach. Statistics in medicine 2008; 27(23): 4658–4677. 80. Mannino M, Abouzied A. Is this real? Generating synthetic data that looks real. In: ; 2019: 549–561. 90. Thakwalakwa C, Phuka J, Flax V, Maleta K, Ashorn P. Prevention and Treatment of Childhood Malnutrition in Rural Malawi: Lungwena Nutrition Studies. Malawi Medical Journal 2009; 21(3): 116–119.