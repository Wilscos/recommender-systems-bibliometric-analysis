Amazon {wenqzhen,ewhuang,nikhilsr,katsumee,wzhangwa,ksubbian}@amazon.com Graph Neural Networks (GNNs) achieve state-of-the-art results across a wide range of tasks such as graph classiﬁcation, node-classiﬁcation, -regression, link prediction, and recommendation [ 5,6]. GNNs rely on the principle of message passing to aggregate node features across multi-hop neighborhoods in order to learn aggregated representations. The success of modern GNN models relies on the presence of dense connections and high-quality neighborhoods. State-of-the-art GNNs (deep) neighborhood information to learn node representations. Other methods like the spectral diffusion [ neighborhood per node to perform message passing. Even the inductive GNNs (e.g., [ function of the node feature and the node neighborhood, requiring the neighborhood to be present during the inference time. Most of the large-scale real-world graphs are power-law in nature, with a majority of the nodes having very few connections [ of such a graph and also empirically show this characteristics on several public datasets. In many information retrieval and recommender systems applications, there is another ubiquitous challenge: the Strict Cold Start ( instance, in most of the public and proprietary datasets we used in the experimentation we have anywhere between 3% to 6% isolated nodes with no neighbors. In these cases, existing GNNs fail to perform due to sparsity or absence of neighborhood. In this paper, we develop GNN models that can achieve truly inductive capabilities: one can learn effective node embeddings for “orphaned” nodes in a graph. This capability is Graph Neural Networks (GNNs) have achieved state of the art performance in node classiﬁcation, regression, and recommendation tasks. GNNs work well when high-quality and rich connectivity structure is available. However, this requirement is not satisﬁed in many real world graphs where the node degrees have power-law distributions as many nodes have either fewer or noisy connections. The extreme case of this situation is a node may have no neighbors at all, called Strict Cold Start (SCS) scenario. This forces the prediction models to rely completely on the node’s input features. We proposeCold Brewto address the SCS and noisy neighbor setting compared to pointwise and other graph-based models via a distillation approach. We introduce feature-contribution ratio (FCR), a metric to study the viability of using inductive GNNs to solve the SCS problem and to select the best architecture for SCS generalization. We experimentally show FCR disentangles the contributions of various components of graph datasets and demonstrate the superior performance of Cold Brew on several public benchmarks and proprietary e-commerce datasets. The source code for our approach is available at:https: //github.com/amazon-research/gnn-tail-generalization. ,11,12] beneﬁt from efﬁcient message-passing frameworks developed to best leverage 13,14,15,16] and label propagation [17,18] also require a non-empty and high-quality Preprint important to fully realize the potential of large-scale GNN models on modern, industrialsized datasets with very long tails and nodes with no neighbors in the graph. To this end, we adopt the teacher-student knowledge distillation procedure [ to distill the knowledge of a GNN teacher into a multilayer perceptron (MLP) student. The Cold Brew framework boils down to addressing two key questions: (1) how we efﬁciently distill the teacher’s knowledge for the sake of tail and coldstart generalization, and (2) how a student can make use of this knowledge. We answer these two question by learning a latent node-wise embedding using knowledge distillation, which both avoid “over-smoothness” [ could discover the the neighborhoods upon their absence. We discuss everything else in detail in the following sections. Note that in contrast to traditional knowledge distillation [ aim is not to train a simpler student model to perform as well as the more complex teacher. Instead, we aim to train a student model that is better than the teacher, and generalizes to samples in the same graph where the teacher will be ineffective. To help select the cold-start-friendly model architectures, we develop a metric called FeatureContribution Ratio (FCR) that disentangles the graph data into node features and the neighborhood structure. We then build submodules for each part and search for the best architecture. Besides selecting the optimal model architecture, we leverage FCR to provide a measure of importance of the node feature compared to the neighborhood structure, quantifying the difﬁculty of learning truly inductive GNNs. We summarize our key contributions as follows: GNNs effectively learn the representations of two components in graph data: they process node features by distributed node-wise transformations and process adjacency structure by localized neighborhood aggregations. For the ﬁrst component, GNNs apply shared feature transformations to all nodes regardless of the neighborhoods, while for the second component, GNNs use permutationinvariant aggregators to collect neighborhood information. To generalize better to tail and SCS nodes, we design the Cold Brew framework to distill a GNN teacher into an MLP student. We improve over GNN models by adding the node-wise Structural Embedding (SE) to the Cold Brew’s teacher GNN to strengthen the expressiveness of the teacher GNN model. We designed a novel mechanism for the MLP student to discover the “latent/virtual neighborhoods” when they are missing, and perform message passing as its teacher model does. We proposeFeature-Contribution Ratio(FCR): a new metric for graph datasets that quantiﬁes the contribution of node features w.r.t. the adjacency structure in the dataset for a speciﬁc downstream task. FCR indicates the difﬁculty in generalizing to tail and cold-start nodes in a given dataset. We also leverage FCR as a principled “screen process” to select the best model architecture for both the GNN teacher and the MLP student in Cold Brew. Through extensive experiments on several public datasets as well as a proprietary ecommerce graph dataset, we validate the effectiveness of Cold Brew in cold-start generalization. We further uncover the contribution ratio of node features in node prediction tasks with FCR. We take the node classiﬁcation problem in this paper for ease of exposition, and the proposed method can be trivially adapted to semi-supervised/unsupervised settings. We denote the graph data of interest by d−dimensional label (either be the neighborhood of node for several nodes in the graph, which we refer to as tail nodes. For a number of nodes, we refer to these nodes as cold-start nodes. A traditional GNN learns representations for the representation itself and its neighborhoods’ representations, at the (l − 1) wheref(·) information of its neighborhood features making these models inductive. We are interested in improving the performance of these GNNs on a set of tail and cold-start nodes, whereN node representation, since {x GNNs learn by aggregating neighborhood information to learn node representations [ 12]. Inductive variants of GNNs such as GraphSAGE [ the neighborhood information of each node to learn the representation. Most works on improving GNNs have focused on learning better aggregation functions, and methods that can work when the neighborhood is absent or noisy have not been sufﬁciently exploited, except two very recent or concurrent works [30, 31]. In the context of cold start, [ knowledge distillation for GNN, while [ cases, the models need full knowledge of the neighbors of the cold-start nodes in question and do not address the case of noisy or missing neighborhoods. Another possible solution is to directly train an MLP that only take node features. [ MLP, while using contrastive loss to regularize the graph structure. However, in order to train it with contrastive loss, the training process still relies on neighbor information, and we show through experiments that such approach does not generalize well to tail and cold start nodes. Some previous works have studied the relation between node feature similarity and edge connections and studied their inﬂuence on the selection of appropriate graph models. [ metric that categorizes the graphs into assortative and disassortative classes. [ propagation steps of linear GCNs from a perspective of continuous graph diffusion and analyzed why linear GCNs fail to beneﬁt from more propagation steps. [ homophily on model selection and proposed a non-local GNN. Compared with the homophily metric, our proposed FCR quantiﬁes the contribution ratio of node features w.r.t. the adjacency structure. With FCR, one can not only assess the loss of performance for orphaned nodes (through the difference of MLP and GNN) but also obtain the most suitable conﬁguration of the GNN model through a parameter-searching procedure when computing FCR. We now address the problem of generalization to the tail and cold-start nodes, where the neighborhood information is missing/noisy (Section 1). A naive baseline is to train an MLP to map node features to labels. However, such method disregards all graph information, and we show via our Feature- For example, a user with only one movie watched or an item with too few purchases. Gwith node setV, |V| = N. Each node possesses ad−dimensional feature and a andY ∈ Rbe the matrices of node features and labels, respectively. LetN is a general function that applies node-wise transformation on nodexand aggregates xand its neighborhoodN, one can use(1)to obtain its representation and predicty, for nodeiis either unreliableor absent. In these cases, applying(1)will yield a suboptimal (a) The teacher-student knowledge distillation of the Cold Brew framework under the cold start setting. Figure 2: structure is missing (no explicit neighborhood), Cold Brew’s student model learns to discover the latent neighborhood, and infer the target embedding from the node feature and the estimated neighbors. The “SE” in blue color is the structural embedding, learned by Cold Brew’s teacher GNN. atomic components of node metric disentangles them into two models: the MLP that only considers Part 1 and Part 3, and label propagation that only considers Part 1 and Part 2. Contribution Ratio and other experimental results that for most assortative graph datasets, the node-wise MLP approach is suboptimal. The key idea of our framework is the following: the GNN maps node features into a embedding space, and since the number of nodes dimensionality basis. This implies the possibility that any node representation can be cast as a linear combination of K  N discover the combination of the best K existing node embeddings of a target isolated node. We call this procedure latent/virtual neighborhood discovery, which is equivalent to using MLPs to “mimic” the node representations learned by the teacher GNN. We adopt the knowledge distillation procedure [ beddings for tail and cold-start nodes. We use a teacher GNN model to embed the nodes onto a low-dimensional manifold by utilizing the graph structure. Then, the job of the student is to learn a mapping from the node features to this manifold without knowledge of the graph that the teacher has. We further aim to let the student model generalize to SCS cases where the teacher model fails, beyond just mimicking the teacher as standard knowledge distillation does. 3.1 THE TEACHER MODEL OF COLD BREW: STRUCTURAL EMBEDDING GNN Consider a graph can be written as the node representations in the the values of 1 ≤ l ≤ L − 2 each layer, (e.g. ReLU). Norm(·) refers to an optional batch or layer normalization. GNNs typically suffer from oversmoothing [ to each other. This is harmful not only for the GNN itself, but also to the student model that looks to mimic the GNN. Inspired by the positional encoding in Transformers [ GNN to additionally learn a set of node embeddings which we term the (a): The proposed Cold Brew framework under the cold start setting: when the adjacency d, we end up with an overcomplete set for this space using the embeddings as the existing node representations. Our aim will be to train a student model that can accurately G. For a Graph Convolutional Network withLlayers, thel-th layer transformation ,Dis the diagonal degree matrix andAis the adjacency matrix.X∈ Ris d/ddepend on layerl:(d, d) = (d, d)forl = 0,(d, d)for , and(d, n)forl = L − 1.σ(·) =is the nonlinear functions applied to SE learns to incorporate additional information (such as node labels in the case of semi supervised learning) through gradient backpropagation. The existence of SE avoids the oversmoothing issue in GNNs: the transformations applied to different nodes are no longer the same for each nodes, since the SE of each node is different, which is part of the feature transformation function. Note that this could be of independent interest to practitioners who train and study GNNs. Speciﬁcally, for each layer and the SE-GNN layer forward pass can be written as: Remark 1: across all nodes. In contrast, we have a different structural embedding for every node. Remark 2: information through iterating ground truth for training node classes and zeros for test nodes, and mixture at each iteration. SE-GNN enables node node embedding through building blocks proposed in recent literature including: (1) initial/dense/jumping connections; and (2) batch/pair/node/group normalization, as the backbone of Cold Brew’s teacher GNN. More details are described in Appendix A. We also apply a regularization term to the loss function, yielding the following loss function: whereX CE(X Yon the training set, and Cross-Entropy loss can be replaced by any other appropriate loss depending on the task. We design the student to be composed of two MLP modules. Given a target node, the ﬁrst MLP module imitates the node embeddings generated by the GNN teacher. Next, given any node, we ﬁnd a set of virtual neighbors of that node from the graph. Finally, the second MLP attends to both the target node and the virtual neighborhood and transforms them into the embeddings of interest. Suppose we would like to obtain the embedding of an potentially isolated target node featurex student MLP. We offer two options for case,¯E ∈ R the teacher GNN, similar to [S where target for the ﬁrst MLP module and the input to the second MLP module. The ﬁrst MLP learns a mapping from the input node features aggregation of the existing embeddings in the graph, before linearly combining them: This will be inferred in the case of labels missing. Note that SE is not the same as the bias term in traditional feature transformation ˜AXW+ b, where for the biasb ∈ Rthe rows are copied/shared SE is also unlike traditional label propagation (LP) [35,36,37]. LP encodes label is the model’s embedding at theL-th layer, which is exactly the model’s output, , Y)is the Cross Entropy between the model outputXand the ground truth . From the teacher GNN, at each layerl, we have access to two sets of node embeddings: andE. Denote¯Eas the desired embeddings that the teacher GNN pass over to the is the concatenation of matrices at the feature dimension (second dimension).¯Eacts as the =¯E[i, :]. Then, we discover the virtual neighborhood by applying an attention-based Equation thresholding operator. whether or not seen previously while training the GNN can be represented as a linear combination of these representations. The MLP decompose every node embedding as a linear combination of an (overcomplete) basis. The training of (mimicing the teacher’s embeddings), and the training of (for node classiﬁcation task) or mean squared error (for node regression task) on the training split of the tail and isolated part of the graph. An illustration of SE-MLP’s inference procedure for the isolated nodes is shown in Fig. 2. When the number of nodes is large, the ranking procedure involved in Θ(·) can be pre-computed after training the ﬁrst part and before training the second part. We cite Theorem 1 in [ labels is differentiable and L-Lipschitz. If the edge weights immediate neighbors with error smoothy o(·) denotes a higher order inﬁnitesimal. This theorem indicates that the errors of the label predictions are determined by the difference of the features after neighborhood aggregation: if is also large, and vice versa. However, with structural embedding, each node independent embedding the teacher model: it allows higher ﬂexibility and expressiveness in learning the residual difference between nodes, and hence the error  From this theorem, one can also see the necessity of introducing neighborhood aggregations like the Cold Brew’s student model. If one directly applies MLP models without neighborhood aggregation, theturns out to be non-negligible, leading to higher losses in the label predictions. However, Cold Brew introduced the neighborhood aggregation mechanism, so that the second part of the student MLP takes over the aggregation of neighborhood generated by the ﬁrst MLP. Therefore, Cold Brew eliminates the above residual error even in the absence of the actual neighborhood. We now discuss Feature Contribution Ratio (FCR): a metric to quantify the difﬁculty of learning representations under the truly inductive cold start case, and a hyperparameter optimization approach to select the best suitable model architecture that helps tail and cold start generalization. As conceptually illustrated in Figure 2, there are four atomic components contributing to the learned embedding of node labels); 3. the features of quantize the SCS generalization difﬁculty, we divide these four components into two submodules to disentangle the contributions of the node features with respect to the adjacency structure of the graph dataset, and quantize it based on the assumption that the SCS generalization difﬁculty is propotional to the contribution ratio of the node features. We posit that a submodule that learns accurate node representations must include the node’s (self) label, so that training can be performed via backpropagation. What remains is to use the label We abuse terminology here since E contains node and structural embeddings from multiple layers (·)is the top-Khard thresholding operator: forz ∈ R:[Θ(z)]= zifzis among largest elements ofz, andΘ(z)= −∞otherwise. Finally, the second MLP learns a : [x, e] → y, where y= Y[i, :] is the ground truth for node i. (4)ﬁrst selectsKnodes from theNnodes that the teacher GNN was trained on via the hard ξ(·)is by minimizing the mean squared error over the non-isolated node in the graph to bound within error|y−Σay| ≤ L||||+ o(max(||x− x||)), where ax+¯E[:, i]+. Deduced from this theorem, the structural embedding¯Eis important for Preprint with other atomic components to construct two specialized models that each make use of only the node features or the adjacency structure. For the ﬁrst submodule, we build an MLP that maps the self-features to self-labels, ignoring any neighborhood information present in the dataset. For the second submodule, we adopt a Label Propagation (LP) method [ self- and neighbor-labels. This model ignores any node feature information. With the above two submodules, we introduce the Feature-Contribution Ratio (FCR) that characterizes the relative importance of the node features and the graph structure. Speciﬁcally, for graph dataset G, we deﬁne the contribution of a submodule to be the residual performance of the submodule compared to a full-ﬂedged GNN (e.g., Equation adjacency structure. Denote submodule, and the full GNN on the test set, respectively. If and the graph structure is important, and noisy or missing neighborhood information will hurt model performance. Based on this intuition, we build SCR as: Interpreting FCR values. good performance. If contribute more to the GNN’s performance. If and the node aggregation in GNNs can actually lead to reduced performance compared to pointwise models. This case usually happens for some disassortative graphs (the majority of neighborhoods hold different labels than the center node), e.g. as observed by [33] . Integrate FCR as a tool to design teacher and student models. models, the SCS generalization can be challenging without neighborhood information (i.e, architectures for both teacher and student, that own the best inductive bias for SCS generalization. To achieve this, during the computation of FCR, we perform exhaustive grid search of the architectures (residual connection types, normalization, hidden layers, etc.) for the MLP, LP, and GNN modules, and pick the best-performing variant. Detailed deﬁnition of the search space can be found in Appendix A. We treat this grid search procedure as a special case of architecture selection and hyperparameter optimization for Cold Brew. We observe that FCR is able to identify the GNN and MLP architectures that are particularly friendly for SCS generalization, improving our method design. In experiments, we observe that different model conﬁgurations are favored by different datasets, and we use the found optimal teacher GNN and student MLP architectures to perform Cold Brew. More detailed discussions will be presented in section 5.3 In this section, we ﬁrst evaluate the FCR for several commonly used graph datasets to ascertain how well GNNs trained on them can generalize to tail and cold-start nodes. We also compare it to the the graph homophiliy metric compare its generalization ability to baseline graph-based as well as purely pointwise models on these datasets. We also show results on proprietary industry datasets. We ﬁrst detail the datasets used in this paper. We perform studies on several open-source datasets, together with four proprietary datasets. The proprietary E-commerce datasets, E-comm 1/2/3/4, refers to graphs subsampled from anonymized logs of an e-commerce store. They are sampled so as to not reﬂect the actual raw trafﬁc distributions, and results are provided with respect to a baseline model for these datasets. The different datasets refer to different product subsets, and the labels indicate We ignore the node features and use the label logits as explained in [17]. , and the neighborhood information inGis mainly responsible for the GNN achieving ). We hence consider FCR as a principled “screening process” to select model Preprint product categories that we wish to determine. Node features are text embeddings obtained from a ﬁne-tuned BERT model. We show FCR values for the public datasets and exhaustively evaluate the proposed Cold Brew as well as several baselines for ﬁve public datasets and the proprietary datasets. The statistics of the datasets are summarized in Table 1. We create speciﬁc training and test splits of the datasets in order to speciﬁcally study the generalization ability of Cold Brew to tail and cold-start nodes. In the following tables, the head data corresponds to the top corresponds to the they induce. All the zero degree nodes are in the isolation data. For datasets that don’t have isolated nodes, we manually remove edges from training/test splits without distinguishing head/tail/isolation. In Table 2, the top half presented the FCR results together with the homophily metric (Equation 6). The below half uncovered the different prediction quality for the head and the tail nodes. As can be seen from the table, as an indicator of the tail generalization difﬁculty, FCR differs among datasets, and is negatively correlated with the homophily metric (with pearson correlation coefﬁcient -0.76). The evaluations on more datasets (including the datasets where FCR presented in Appendix B. β(G) = Table 2: Above half: FCR and its components, formance difference of GNN on the head/tail and head/isolation splits. Here the “head/tail/isolation” means the 10% most connected, fewest connected, and isolated nodes in the graph. 5.3 EXPERIMENTAL RESULTS ON TAIL GENERALIZATION WITH COLD-BREW In Table 3, we present the performances of the Cold Brew together with baselines on the tail and the isolation splits, across several different datasets. All models in the table are trained on the overall training subset, and are evaluated on the hold-out test subsets of tail and isolation splits (discussed in section 5.1). For the isolation split, the GNNs are evaluated with only self-loop (all other edges are non-existent/removed). The GCN refers to the the best conﬁguration found through FCR-guided grid search (the search is performed over a range of hyperparameters, check Appendix A for details), without Structural Embedding. Correspondingly, GCN + SE refers to the best FCRguided conﬁguration with Structural Embedding, which is the default teacher model of Cold Brew. GraphSAGE refers to [ with 128 hidden dimensions, and GraphMLP refers to [ presented as relative improvements to the baseline (each value is the difference w.r.t. the value of the 10%highest degree nodes in the graph, and the subgraph that they induce. The tail data 1Xthe number of v’s direct neighbors that have the same labels as v× 100% (6) |V|the number of v’s directly connected neighbors Table 3: The performance comparisons on the isolation and tail splits of different datasets. The full comparisons on head/tail/isolation/overall data are in the Appendix B. GCN+SE 2 layers is Cold Brew’s teacher model. The Cold Brew outperforms GNN and other MLP baselines, and achieves the best performance on the isolation splits as well as some tail splits. Table 4: The comparisons of Cold Brew’s GCN and the traditional GCN for deep layers. When the number of layers is large, Cold Brew’s GCN retained good performance while traditional GCN without SE suffers from the “over-smoothess” and degrades. Even for shallow layer, Cold Brew’s GCN is better than traditional GCN. GCN 2 layers on same dataset of the same split), and we do not disclose the absolute numbers due to proprietary reasons. As shown in Table 3, Cold Brew’s student MLP improves accuracy on isolated nodes by up to +11% on the E-commerce datasets and +2.4% on open source datasets. Cold Brew’s student model handles isolated nodes much better, and it teacher GNN also achieves better performance in the tail split, compared to all other models. Especially, when compared with GraphMLP, Cold Brew’s student MLP shows consistently better performance in most datasets. This can be explained from their different mechanisms: GraphMLP encodes graph knowledge implicitly in the learned weights, while Cold Brew explicitly attends to neighborhoods, even when it is absent. More detailed comparisons can be found in Appendix B. There are other interesting observations. First, if the FCR is high (such as Pubmed), then for this dataset the MLP-type models tend to outperform GNN-type models in all splits. Regardless of FCR, for almost all datasets, the MLP-type models outperform the GNN-type models on the isolation split, and a few on the tail split, while the GNN-type models are superior in other splits. Second, the proposed structural embeddings imply a tantalizing potential to alleviate the over-smoothness Cold Brew’s GCN (GCN + SE) signiﬁcantly outperformed the traditional GCN on 64 layers: the former has 34% test accuracy higher on Cora, 23% higher on Citeseer and similar on others. Indeed, the improvement over isolation and tail splits (especially the isolation split) comes with a cost: we observed a performance drop for the student MLP model on the head and several other datasets’ tail splits, compared with the naive GCN model. The full performance on other splits are listed in the appendix as a reference. However, the cold-brew speciﬁcally targets at the challenging strict cold start issues, as a new compelling alternative for in these cases. Meanwhile in the non-cold-start cases, the traditional GCN models can still be used to obtain good performance. Note that, even on the head splits, the proposed GNN teacher model of Cold Brew still outperformed traditional GNN models. We hence consider as promising future work to adaptively switch between using Cold Brew teacher and student models, based on the current node connectivity degree and tailedness. Figure 3 visualizes the last-layer embeddings of different models after t-SNE dimensionality reduction. In the ﬁgure, colors denotes node labels and all nodes are marked as dots, with isolation subset ] and bottleneck [39] issues observed in deep GCN models. As shown in table Table 4, Preprint nodes additionally marked with ‘x’s and the tail subset additionally marked with triangles. Although the GCN model did a decent job in separating different classes, a signiﬁcant portion of the tail and isolation nodes fall into wrong class clusters. Cold Brew’s MLP is much better discriminative in the tail and isolation splits. Figure 3: The top two subﬁgures: the last layer embeddings of GCN and Simple MLP, the bottom two subﬁgures: the last layer embeddings of GraphMLP and Cold Brew’s student MLP. All embeddings are projected to 2D with t-SNE. Cold Brew’s MLP has the fewest isolated nodes that are misplaced into wrong clusters. In this paper, we studied the problem of generalizing GNNs to the tail and strict cold start nodes, whose neighborhood information is noisy or missing. We proposed a teacher-student knowledge distillation procedure to better generalize to the isolated nodes. We added an independent set of structural embeddings in GNN layers to to alleviate node over-smoothness, and also proposed a virtual neighbor discovery step for the student model to attend to estimated neighborhood nodes. We additionally established FCR, a metric for quantifying the difﬁculty of truly inductive representation learning, to optimize our method design. Experiments demonstrated the consistently superior performance of our proposed framework, on several common benchmarks as well as proprietary datasets.