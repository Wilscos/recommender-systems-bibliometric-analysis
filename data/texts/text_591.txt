Abstract—The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, Deaf and hard-ofhearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been signiﬁcant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the Deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This paper considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, rangeDoppler, and range-angle) for sequential classiﬁcation of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RFsensor based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classiﬁcation accuracy of 92% for 15 ASL words and 3 gross motor activities. Index Terms—sign language, ASL, gesture recognition, trigger detection, wake word, human-computer interaction HE past decade has seen great advancements in sensing for ambient intelligence, including speech recognition for control of interactive devices, personal assistants, and humancomputer interfaces. However, deaf and hard-of-hearing (HoH) individuals, whose primary mode of communication is sign language, cannot beneﬁt from voice-controlled interfaces. Research in recognition of American Sign Language (ASL) has focused primarily on wearable sensors [1]–[3], optical cameras [3]–[5], and infrared depth sensors [6], [7]. Wearables, such as “signing” gloves embedded with inertial measurement units (IMUs), or surface elecromyography (sEMG) sensors have yielded relatively higher recognition accuracy, but inhibit natural motion, and are thus not highly preferred by members of the Deaf community [8]. Video cameras are perhaps the most often used device by deaf/HoH individuals for interpersonal communications. RGBD cameras, which add depth measurements to video recordings for the purposes of skeleton tracking, such as Kinect or Leap Motion sensors, have improved recognition accuracy relative to video-only approaches. However, RGB-D cameras are ineffective under low illumination and may invade privacy through the acquisition of personal imagery of the face and environment. RF sensors have been recently proposed [9]–[12] as a new modality for ASL recognition that has the capability of measurning human kinematics through ﬁne range, angle and velocity measurements. RF sensors are also effective in the dark and do not make any visual recordings of the people or environment. RF sensors, also known as radar, which is short for radio detection and ranging, acquire independent measurements of distance, velocity, and angle. Using a technique known as stretch processing [13], the frequency difference between the transmitted and received frequency modulated continuous wave (FMCW) signals can be used to compute the round-trip travel time of the signal, and, hence, distance to an object. The Doppler shift, on the other hand, relates radial velocity to the frequency shift in the received signal. Rotations or vibrations result in additional Doppler frequency modulations, known as micro-Doppler (µD) frequencies, centered around the main Doppler shift due to translational motion [14]. The µD signature is a 2D time-frequency representation of the RF data, which reveals the unique kinematics of the observed motion. Thus, µD has been exploited as a biometric [15] for recognizing individuals [16], activities [17], aided/unaided walking [18], falls [19], [20] and even different gaits [21]. Although RF sensors cannot effectively perceive facial expressions or hand shape, radar does provide data that is complementary to that of video: while video is effective in capturing spatial parameters, radar is more adept at capturing temporal or dynamic parameters. This is because radar measurements of distance and velocity are based on independent physics-based measurements: distance is computed from round-trip travel time, while velocity is computed from the Doppler shift, which has greater accuracy than the computation of displacement over time. Thus, although RF sensors cannot be used to reconstruct facial expressions or hand shape, radar can provide a new way of recognizing signs in a non-contact, ambient fashion based primarily on signing kinematics and range proﬁles. As a result, there has been much research on the use of RF sensing for hand gesture recognition [22], [23], especially since the development of low-cost, low-power, high resolution, integrated millimeter wave RF transceivers [24]. However, most current research involves controlled data acquisition with the participant located in a ﬁxed position relative to the radar, articulating only a single gesture or sign. A critical challenge that has not been adequately addressed in the literature, however, is the challenge of ASL recognition in the context of daily living. To the best of our knowledge, this work represents the ﬁrst to consider triggering and command recognition of RFsensor enabled devices under more realistic conditions, where the RF data is acquired in a continuous fashion to capture mixed sequences of gross body motion/activity intertwined with ASL signing. In particular, we analyze the design considerations for selection of a trigger sign based on kinematics, replicability, and recognition accuracy. Whereas current approaches rely on just one RF data representation, we propose a joint-domain, multiinput, multi-task learning (JD-MIMTL) framework coupled with a motion detector to isolate the intervals over which the user is engaged in meaningful movement, and thus prevent unnecessary expenditure of computation resources when the RF system is not being used. Figure 1 shows a ﬂowchart providing an overview of the proposed approach. Our results show that the proposed approach exceeds that offered by approaches common in the literature and can recognize a sequence of 3 activities and 15 ASL signs with 92% accuracy, while detecting trigger signs with rates as high as 98.9%. In Section II, an overview of the current state-of-the art and work related to radar-based gesture and sign language recognition is given. In Section III, we describe the RF sensor utilized, experiments conducted, and pre-processing algorithms applied to the data. In Section IV, kinematic and replicability considerations are applied to select 15 out of a total of 110 measured ASL signs as example trigger signs. Next, in Section V, a motion detection method is presented and its efﬁcacy on the acquired datasets demonstrated for temporal segmentation. In Section VI, the proposed JD-MIMTL framework for sequential classiﬁcation is detailed. Results demonstrating the performance of the proposed approach for trigger word detection and sequential recognition are discussed in Section VII. Discussions of key conclusions and future work is given in Section VIII. A variety of deep learning approaches [25] have been leveraged for human motion recognition with RF sensors. Most approaches consider either daily activities (e.g. walking, sitting, running) or hand gestures (e.g. left/right, up/down swiping, push buttons). Recurrent neural networks (RNNs) have been proposed in many works, but results have been primarily demonstrated on ﬁxed-duration snapshots that include just one class of motion. For example, [26] applies stacked gated RNNs to 2D micro-Doppler signatures, while [27] constructs a 3D data representation from shifted windows of the microDoppler signature, applying both a 3D convolutional neural network (CNN) and LSTM. A more common approach is to use a time series of range-Doppler maps as input [28], [29] to the 3D CNN-LSTM network, while also using Connectionist Temporal Classiﬁcation (CTC) [27] and triplet loss [30]. Studies considering recognition performance in real-world conditions are limited; most of the aforementioned works involve experiments conducted in controlled environments with participants positioned at a ﬁxed distance from the RF sensor. Work involving real-world use cases have considered the effects of sensor positioning and environment. For example, [23], investigates the dependence of performance on sensor position at different heights from the ground and distance from the user. Dynamic time warping is used on RF data acquired from a dual-Doppler radar to accurately recognize 12 different gestures with at least 80% accuracy, depending positioning. These results are consistent with expectations based on the radar range equation, which show that the received power decreases with distance (R) as 1/R. Indoor environments also tend to have comparable stationary clutter properties, and, hence do not result in signiﬁcant variations in performance. The environment-independence of RF sensing of ASL was veriﬁed in a recent [31], which found the recognition accuracy across several different rooms to be comparable. Another important factor in gesture recognition is the upper body movements, which can change the received RF signal from gestures. In [32], a single transmitter, dual receiver RF transceiver was proposed to decouple hand gestures from random body movements, and thereby improve gesture recognition accuracy. An important real-world use case that is gaining increasing attention is the challenge of sequential motion recognition. Human movement is inherently dynamic, greatly varied, and sequential in nature. The continuous data streams acquired by RF sensors in real-world environments will consist of an intertwining of gross body activities and ﬁner movements, such as gestures. But, most works on human activity recognition consider either daily activities or ﬁne-grain gestures, while the approaches proposed reﬂect a similar technique to that applied over ﬁxed-duration snapshots. For example, [33] applies biLSTM networks to continuous sequences of micro-Doppler signatures, while [34] applies RNNs to continuous streams of 3D inputs formed from the time series of range-Doppler maps. Radar-based ASL recognition to-date has primarily focused on the recognition of snapshots of speciﬁc words or phrases. In [35], ten (10) different ASL phrases that would be relevant to emergency response were recognized with an accuracy of 95% using transfer learning from VGG-16 to classify Xband micro-Doppler signatures. In [12], feature-level fusion of RF sensors operating at three different transmit frequencies (10 GHz, 24 GHz, and 77 GHz) were used together with a random forest classiﬁer trained only an measured microDoppler signatures from ﬂuent ASL signers yielded a classiﬁcation accuracy of 72.5% for 20 ASL signs. Moreover, using a support vector machine (SVM) classiﬁer, it was shown that the ASL articulations of hearing imitation signers were distinguishable from that of ﬂuent ASL users. With the use of a multi-modal DNN for fusion [36], the classiﬁcation accuracy for 20 signs was improved to 95.5% and shown to surpass by 22% and 19% the accuracy given from use of a single RF sensor classiﬁed using transfer learning from VGG-16 and unsupervised pre-training with convolutional autoencoders (CAEs), respectively. In [31], micro-Doppler signatures of 50 ASL signs are classiﬁed with an average accuracy of 87%, while the sign KNOCK is speciﬁed as a wake word and detected at a rate of 94% using a ﬁxed-window binary DNN classiﬁer. Word-level ASL recognition with RF sensors is shown to be tolerant to the presence of other interfering users, different user positions and different environments. These results were achieved by collecting over 12k samples from 15 different participants. Because of the differences in ﬁne-grained temporal dynamics and linguistic parameters, such as prosody and grammatical structure, the RF data acquired from hearing imitation signers versus ﬂuent ASL users are actually quite different. In [36], it is shown that imitation signing cannot be used to train classiﬁers of ﬂuent signers. To overcome this challenge, adversarial learning has been proposed [37] to 1) adapt imitation signing data to resemble that of ﬂuent signers, and 2) synthesize kinematically accurate samples for training DNNs. This approach has yielded over 77% top-1 and over 93% top-5 accuracy for recognition of 100 ASL signs using micro-Doppler signatures acquired from a 77 GHz RF sensor. Note that all of the above works classify ﬁxed-duration snapshots of micro-Doppler signatures of ASL. Thus, this work ﬁlls an important gap in current literature by addressing the challenge of trigger sign detection and sequential ASL recognition in continuous RF data streams of mixed motion sequences that are typical of daily living. In this study, a TI AWR1642BOOST 77 GHz RF transceiver paired with a DCA1000EVM data capture card were used to record data directly to a laptop. The TI 77 GHz transceiver is a frequency modulated continuous wave (FMCW) shortrange automotive radar that has two transmit (TX) channels and four receive (RX) channels, which offer additional sensing capabilities in comparison to other commercially available RF sensors that may have only 1 TX/RX channel. The antenna for the sensor has a roughly ±70azimuth and ±15elevation beamwidths. The sensor was positioned on a small table at a distance of about 1 meter from the ground. Although ASL has been used as example motions in some gesture recognition studies [38], [39], sign language greatly differs from gesturing in that it possesses a much greater degree of physical complexity and Shannon information [40]– [42]. Like other complex system-generated signals, raw physical signal from signing data contains information at multiple timescales, spanning phonological, semantic, syntactic, and prosodic cues ( [43], [44]). While some studies [45], [46] have utilized imitation signers - i.e., hearing participants who mimic signs observed in video - it has been shown [47] that it takes at least three years before the signing of ASL learners is perceived as ﬂuent by native ASL users. Imitation signers exhibit greater kinematic variations, erratic cadence and signing errors, especially in replicating repetitive signs. Indeed, in our previous works [12], [48], we have found that imitation signing is distinguishable from native signing using classiﬁcation of RF µD signatures. Thus, in this work, RF data from both imitation signers and native ASL users were acquired and used for comparative study in trigger sign selection. A total of 110 single ASL signs were recorded from participants sitting 1 meter away from the radar. A total of 19 participants contributed to the database, TABLE I: Description of Mixed Activity/Sign Sequences including 4 native ASL users, who were either Deaf or childof-deaf-adults (CODA), and 6 hearing individuals. Continuous recordings of mixed activity/signing sequences were recorded from 13 hearing participants, while testing on native users was conducted with 2 CODAs and 2 ASL learners, who were not used in acquisition of training samples. A total of two different datasets were acquired: 1) Single ASL Signs: 110 of the more frequently used ASL signs were selected from the ASL-LEX Database [49], including nouns, verbs, and adjectives. A complete listing of the signs acquired is given in Figure 2. Each participant was asked to repeat the signs 5 times, resulting in 20 native and 30 imitation samples per sign. 2) Mixed Motion Sequences: Of these 110 signs, based on kinematics and replicability, a subset of 15 ASL signs are selected (see Section IV). Five different sequences of three ASL signs mixed with three different gross motor activities (walking, sitting, and standing up) were acquired, as shown in Table I. For example, in SEQUENCE 1, the participant ﬁrst walks for a few seconds, then sits on a chair located in front of the radar and enacts 3 different signs (TIRED, BOOK, SLEEP), and ﬁnally stands up. The participants were instructed to perform these activities consecutively in the line-of-sight of the radar. A total of 200 hearing participant samples and 94 native participant samples for each sequence were acquired, and made available for download. The raw data provided by each receive channel of the RF sensor is a time stream of complex in-phase (I) and quadrature (Q) data. The presence of multiple receive channels enables not only the extraction of range and velocity, but also the direction (or angle) of arrival of the received signals. The 77 GHz TI transceiver has 2 TX and 4 RX channels, which forms a uniform linear array (ULA). If, instead, the transmitters send two different chirp combinations, binary phase modulation (BPM) can be used to form a virtual array that behaves like a single TX, but 8 RX channel transceiver. To accomplish this, in the ﬁrst chirp, C, we send exactly the same chirps from both transmitters, TX1 and TX2, with phase of Φ = 0°, while in the second chirp, C, TX2 transmits with phase of Φ = 180°. The chirps of TX1 and TX2 then can be retrieved by C= (C+ C)/2 and C= (C− C)/2, respectively. In this way, we obtain a phase-shift for our second transmitter as well and synthesize a virtual array. An illustration of the transmitted chirp waveforms are provided in Figure 3a, while the resulting actual and virtual ULAs are shown in Figure 3b. For a target at angle θ, the phase difference between receiver channels will be as follows: The angular resolution, θ, is given by: where M is the number of channels, so the doubling of M from 4 (real) to 8 (virtual) using BPM improves the angular resolution by a factor of 2. Thus, the 77 GHz TI transceiver was set to operate in BPM mode with a bandwidth of 4 GHz, pulse repetition frequency (PRF) of 6.4 kHz with 256 samples per pulse and a coherent processing interval (CPI) of 40 ms. Typically, the received I/Q data stream from each channel is reshaped into a 3D array, known as the radar data cube, with dimensions of fast-time (the number of ADC samples) × slow-time (the number of pulses) × channels. From the radar data cube, several different ways of representing the information acquired by the radar may be formed. The Fast Fourier Transform (FFT) across fast-time can be used to ﬁnd the frequency difference, f, between the transmitted and received signals at any instant of time. If the chirp rate of the frequency modulated waveform is γ, then the distance, R, between the radar and scatterer can be found as R = cf/2γ, where c is the speed of light. An FFT across slow-time reveals the velocity of moving scatterers, v = cf/2f, where fis the Doppler shift and fis the transmit frequency. Thus, several different 2D data representations may be computed from the radar data cube: 1) Range-Doppler Map: The 2D FFT of the slowtime/fast-time data matrix for a single channel can be computed to ﬁnd a range-Doppler image for each CPI. Because a RD map is computed from all the received returns acquired over a CPI, some researchers have adapted terminology from video processing and refer to the RD map as a frame and the CPI as the frame duration. Time series of RD maps can be formed to Fig. 3: (a) BPM chirp conﬁguration, and (b) virtual array synthesis. form RD videos. With a CPI comprised of 256 pulses, the resulting video as a frame rate of 25 fps (1/40 ms). 2) Time-Frequency (Micro-Doppler) Map: While there are many time-frequency transforms that yield the µD frequency versus time, the most often used is the spectrogram [14], which is the square modulus of the ShortTime Fourier Transform (STFT) across slow-time. In order to generate µD spectrograms independent of the subjects’ range, cell averaging constant false alarm rate (CA-CFAR) is applied on RD maps for detection of range bins with motion. Detected range bins are then used to generate the spectrograms. 3) Range-Angle Map: Angle can be computed from multiple-channel data using a beamforming method, e.g. multiple signal classiﬁcation (MUSIC), to determine the angle-of-arrival of the received returns at a speciﬁc range and Doppler. Repeating this process for each CPI yields a time-series of RA maps, i.e. RA videos. The visibility of target-related motion in the RA maps may be enhanced using optical ﬂow, which indicates the spatial change in the location of pixels from one frame to another in a video. In this work, we compute the optical ﬂow using the Horn-Schunck method [50] and take its element-wise multiplication with the pixels in the RA maps to accentuate motion-related returns. This process puts more weight on pixels where there is a moving target, and suppresses pixels comprised of clutter or minimal motion. Because the MUSIC algorithm is relatively prone to noise, this approach can enable signiﬁcant visual enhancements in the RA maps. An overview of the radar signal processing steps utilized to compute the stated RF data representations are summarized in Figure 4. There are many different considerations for the design of a device trigger sign (also known as a wake word). Trigger signs should be distinct, not easily confused with signs frequently used in daily discourse, easy to articulate and culturally appropriate. In Deaf culture, for example, while it is common for ﬁnger-spelling to be used to state the names of a hearing individuals, personal name signs can only be used if the name sign has been given by a member of the Deaf community. Moreover, ASL does have some differences in dialects used in different geographical regions within the U.S., such as Black ASL, which represents a unique ethnic sub-culture in the South [51]. The cultural context of signs may differ and take on different meanings in different regions. Therefore, the design of culturally-appropriate trigger signs can only be accomplished through partnership with Deaf community organizations, who can provide cultural perspectives and facilitate studies soliciting Deaf community feedback on the design. Thus, this paper focuses on technical aspects of trigger sign design as a precursor to a subsequent Deaf-centric design study. First, as RF sensors are sensitive to distance and motion, signs that are dynamic, with strong radial velocity components (i.e. include primary arm motion, as well as secondary motion of the hand, such as handshape or orientation change), or which traverse greater distance and have a longer ﬂight times are better suited as trigger signs for automatic detection. This is in contrast with signs primarily characterized by secondary hand motion, such as ﬁngerspelled words. Second, the replicability of the trigger sign is important to enable consistent and robust recognition. Although native ASL users are the target population for ASL-sensitive user interfaces, there is a wider community of ASL learners and non-native ASL users, such as interpreters, who could also be using the interface. However, as noted in Section III-B, there can be noticeable differences in the articulation of signs based on ﬂuency. Thus, the replicability of the 110 signs listed in Figure 2 were evaluated using a comparison of the imitation signing and native ASL µD signatures. This was done by ﬁrst computing the upper and lower envelopes of each sign based on the percentiles of the cumulative amplitude distribution [52], [53]. Next, both the Discrete Fr´echet Distance (DFD) [54] and Dynamic Time Warping (DTW) were used to compare the replicability of signs based on ﬂuency. DTW is a method for measuring the similarity between two time-series and ﬁnds the optimal match [55] between sequences that satisfy all restrictions and rules with the minimum cost. The DFD computes the similarity between two curves by taking into account both ordering of the points and the location along the curves. It is deﬁned as the shortest cordlength required to join a point traveling forward along one curve and one traveling forward along the other curve, and the rate of travel for either point may not necessarily be uniform. As the similarity of two curves increases, DFD gets closer to zero. As an example, consider the comparison of the upper envelopes of the µD signatures for imitation signing and native signing for the sign WANT, shown in Figure 5(a), where the grey lines represent the cord-length. To identify the most easily replicable signs (independent of ﬂuency), the envelopes of the native ASL signatures and those from hearing imitation signers are compared on a sign-by-sign basis. The DTW and DFD metrics are averaged and re-scaled between 0 and 1. Once the distance metrics, dtw and dfd, are normalized, the ﬁdelity scores, sand s, for each class (sign) are found by taking the inverse of the normalized distance (i.e., s= 1/dtw, s= 1/dfd). The results are shown in Figure 5(b). It may be observed that both the DTW and DFD are consistent in their assessment of which signs are consistently articulated across deaf, CODA, and hearing users. The top 15 signs that have the shortest distance (i.e. highest similarity) between native ASL and imitation signing users were selected as trigger sign candidates, which will next be evaluated based on detection rate and sequential recognition accuracy. The selected signs are listed in Figure 5(c) along with their kinematic properties, as given by ASL-LEX. Continuous activities and ASL signing create a time series of sequential activities, for which segmentation is an important initial step in the analysis of sequential data. Utilization of a motion detector can facilitate segmentation, which helps deﬁne the length of the input samples to be fed to a learning model. It can also improve the power and computational efﬁciency of the system by making a prediction only when an activity or sign is detected as opposed to every time step. While motion detection can be done with a human-in-the-loop approach, this is not desirable in automate, stand-alone systems. Instead, a power-based automated segmentation algorithm, such as short time average over long time average (STA/LTA) [56], [57], dynamic boundary detection (DBD) [58] or power burst curve [59] (PBC) may be utilized. The PBC can be used for motion detection using thresholding. The start and end of the motion is determined by when the input power exceeds or falls below this threshold, respectively. An important drawback of this method, however, is that it is prone to a high rate of false triggering, especially in the presence of noise, because the threshold is not adaptive and unaware of past and future power levels. STA/LTA-based techniques solve this problem by deﬁning two consecutive windows; namely, short-time and long-time windows. Their relative average power is used to deﬁne an adaptive threshold value. The STA/LTA method proposed in [57] has proven to be very successful in detecting the tail (end point) of hand gestures. However, the method uses ﬁxed length detection windows, whose duration is selected based on the duration of the longest gesture in the dataset. This approach is not well suited to sign language, since ASL signs possess great variability in duration. Basing window size on the longest duration sign can result in a long blank period at the beginning of the detected region for short signs, thereby introducing noninformative or redundant input to the feature space. DBD, on the other hand, requires application of high-pass ﬁltering to the Doppler information, resulting in elimination of the low and zero frequency components of the spectrograms. Prior work [12] has shown, however, that ﬁltering at 77 GHz results in signiﬁcant loss of low-frequency information in the signal, together with removal of the clutter, thereby degrading classiﬁcation accuracy. Thus, this work proposes a variable window STA/LTAbased motion detection algorithm to identify both the starting and ending point of a motion. First, the absolute difference between the upper and lower envelopes at a time index is computed to create absolute distance vectors. An exemplary, normalized absolute distance vector is shown in Figure 6a. The absolute distance for each data recording, i, can be computed as v= |u− l|, where vis the absolute distance vector, u and lare the upper and lower envelopes, respectively. Then, ST A(t) and LTA(t) can be deﬁned as the leading and lagging windows at time t as: ST A(t) =1Tv(k), LT A(t) =1Tv(k) where Tand Tare the lengths of short and long windows respectively. The starting point of a motion is detected when the following conditions are satisﬁed: where σand σare predeﬁned detection thresholds. Similarly, the ending point is detected if where σis the detection threshold for the stopping point. Note that in order to locate the starting point, according to (4), ST A(t) needs to exceed the threshold σ, implying that the the motion has to appear in the short window. Also, the ratio of average power in the short and the long window should be higher than σ. In this way, if there is noise, the system will not be triggered unless the ratio exceeds the σ. Similar conditions apply to ensure correct detection of the endpoint; i.e., the case when the motion disappears from the proceeding window and the ratio drops below the threshold σ. The resulting detection mask found with the proposed vwSTA/LTA approach is able to separate the intervals with and without motion, as shown in Figure 6b. While DBD requires the optimal selection of a threshold based on the returned signal strength, ﬁxed length STA/LTA bases selection on the window length. In contrast, the proposed variable length STA/LTA approach adaptively changes its detection window interval irrespective of the returned signal strength. A comparison of the segmentation accuracy for these three methods is presented in Figure 7. Segmentation accuracy is computed by comparing segmentation mask with the ground truth generated by a human analyst for each time step. Note that the segmentation accuracy of DBD and ﬁxed-window STA/LTA exhibit great variance in efﬁcacy for different thresholds or window lengths. Fixed-window STA/LTA achieves a peak accuracy of 75.7% when the window length is 2.3 seconds. DBD performs better by comparison, achieving a peak accuracy of 84.2% when the threshold is set to 61, but with the cost of information loss in low frequency components (see Section VI-C). This peak value is only slightly higher than the 83.5% accuracy achieved by the proposed motion detector, while the propose approach can maintain this accuracy irrespective of any parameter values due to the use of variable, adaptive window lengths. Fig. 7: Comparison of the segmentation accuracy of DBD, ﬁxed-window STA/LTA and the proposed variable-window STA/LTA. Conventional approaches to RF signal classiﬁcation rely on a single data representation, presented as either 2D or 3D inputs. In contrast, to take advantage of all available physicsbased information (range, velocity, frequency and angle), we propose a JD-MIMTL-based DNN architecture, where each input representation is processed in parallel and the ﬁnal feature space is constructed by fusing individual feature spaces. Auxiliary tasks are used to regularize and better guide the training loss. The accuracy of the proposed approach surpasses that of conventional single-input models by over 13%. Sequential classiﬁcation of daily activities and ASL signs differs from conventional hand gesture recognition tasks because it is not comprised of just an isolated, short duration, single type of motion. Instead, it consists of a time series of consecutive motions, which might belong to different classes of gross daily activities or ASL signs. A typical approach to classify a continuous time series data includes: 1) temporal segmentation, 2) making prediction for each time step. The former is achieved using a motion detector described in Section V, while the latter will be discussed in this section. In real-world scenarios, training a model with the entire stream of data sequences (24 sec each) is not feasible, because this signiﬁcantly increases the computation time, rendering outputs only after a long delay, which is undesirable in interactive systems. However, when models are trained with shorter input sequences, performance also tends to drop gradually, because performance of LSTMs are dependent on input sequence lengths [60]. Since LSTM networks have the ﬂexibility to be trained with varying sequence lengths, the data segments isolated by the motion detector were used as input sequences. These segments will have varying lengths depending on the user’s pace and the motion itself. In this section, the effect of input sequence length on prediction accuracy is examined. For this purpose, we use a DNN consisting of 3 time-distributed (TD) 2-D convolutional blocks with kernel sizes of 3, followed by max pooling layers and a bidirectional long-short-term-memory (BiLSTM) layer. A TD softmax layer is employed for temporal classiﬁcation. While convolutional layers extract the spatial features, the TD wrapper enables application of the same nested layer to each time step. BiLSTM is a kind of recurrent neural network which is used to extract temporal relationships between time steps. They have proven to be very successful in terms of learning long term dependencies in various tasks such as natural language processing [61], and speech recognition [62]. By employing LSTMs in our ﬁnal encoded feature space, both spatial and temporal features are extracted for classiﬁcation. In µD spectrogram (µDS) classiﬁcation, spectrograms are divided into 0.2 sec non-overlapping windows to be used as time steps. In RD and RA map classiﬁcation, the interval between each RD/RA map or frame is 40 milliseconds, so to obtain a data structure corresponding to the same (0.2s) duration, ﬁve RD/RA frames were stacked (5×40ms = 0.2s). For both inputs, 80% of the data is used for training and 20% for testing, with an equal number of samples from each sequence. Adam optimizer and categorical cross entropy is used along with early stopping with patience of 10 epochs to train the model. Hence, the input data has the shape of (batch size, number of windows, width, height, channels). A 2DCNN+BiLSTM network for µDS and 3D-CNN+BiLSTM network for RD/RA maps are employed. The impact of the motion detector is discussed next. 1) Original Sequential Data: Table II shows the classiﬁcation accuracy for each input data representation as a function of various input durations. It may be observed that the accuracy of the models for all input domains decreases as the length of input sequences gets shorter. Best performances are obtained using longest sequences with RD maps providing a 92.4% accuracy. The performance using µDS changes around 17% while that using RD maps and RA maps change around 20% from 1 sec. sequences to 24 sec. sequences. While the longer sequences give better performance, they also result in greater prediction delay and higher memory requirement due to increased data size. This situation demonstrates the challenge of deciding an appropriate input length while doing sequential classiﬁcation and the trade-off between prediction performance and delay. 2) Motion Detected Intervals (MDI): The detector extracts data segments containing motion, eliminating periods of no movement. Thus, each MDI is of varying duration, and models are trained using variable length data. The testing accuracies obtained when using µDS, RD and RA maps are 78.8%, 72.8%, 67.5% respectively. These results are comparable to those obtained with ﬁxed length sequences of 2 sec. for µD, and 1 sec. for RD/RA maps, while the length of detected segments vary between 0.6 and 10 sec. Moreover, using MDI rather than ﬁxed length windows signiﬁcantly reduces the computation time for prediction by masking out the intervals that do not contain any motion. Table III presents the total computation time of an NVIDIA Titan V GPU to make predictions for data durations of 1 sec and 2 sec. The total computation time is reduced by 45% on average for different input representations when compared with 2 sec length sequences. Note that the amount of computational savings obtained using the motion detector does depends on the data, in that as MDI increases so does the time savings. As daily life often involves extended stationary periods, in practical settings the use of MDI can result in signiﬁcant savings. The performance of DNN models rely heavily on the data presented at the input, which in turn is extracted based upon the starting and ending points of the MDIs as determined by the motion detector. Thus, the ability of a motion detector to accurately extract intervals containing movement impacts the efﬁcacy of classiﬁers. Table IV compares the classiﬁcation accuracy attained from different input representations extracted TABLE IV: Classiﬁcation Accuracy of the Motion Detectors using DBD, ﬁxed-length STA/LTA and the proposed variablelength STA/LTA motion detectors. It may be observed that the proposed variable-length STA/LTA detector yields greater classiﬁcation accuracy in comparison to other approaches, surpassing ﬁxed-length STA/LTA by 0.4-2% and DBD by 1.36.4%. Note that the relatively worse accuracy of DBD is due to information loss incurred during the high-pass ﬁltering, which removes low-frequency signal as well as clutter components, and hence degrades the resulting classiﬁcation accuracy. To improve the classiﬁcation accuracy obtained with just one input representation, this paper proposes utilizing fusion of multiple input representations in a multiple-task learning [63] framework with connectionist temporal classiﬁcation (CTC) [64]. Although MTL has been implemented successfully in computer vision [65] and natural language processing [66], these applications all involve a single data representation (image, text, speech signal). In RF sensing, the various physical variables measurable by radar - namely, range, µD, and angle versus time - are reﬂected in different data representations, to base recognition decisions on all physical properties, multiple inputs to MTL are advantageous. The joint feature space derived from multiple input representations is enriched by fusing in a concatenation layer. MTL jointly optimizes multiple objectives by exploiting domain-speciﬁc information contained in commonalities and differences across tasks. By sharing representations among related (auxiliary) tasks, the generalization capability of the model can be improved on the main task. ASL classiﬁcation can be aided by basing decisions on consistency with certain physical properties of signing, based on the categorization provided in Figure 5(c). Five auxiliary tasks are deﬁned: The overall loss function, L, utilized in the JD-MIMTL framework is the weighted sum of the CTC loss, λ, and the loss Lspeciﬁc to each task i: where λ are the weights assigned to the various loss terms. Since each task has its own loss function, and, hence, varying convergence times, the weights λ needs to be jointly optimized. Three different loss optimization techniques [67] were compared, namely, the uniform combination of losses (i.e. equal weights across all tasks), the uncertainty based weighing method [68], and grid search. The ﬁrst two methods minimize Lwithout taking into account the importance of each individual task. Since we aim to minimize L, which is derived from the prediction layer, the grid search method was preferred. The use of smaller auxiliary task weight values during grid search was found to perform better than that obtained with using the uniform combination of losses or uncertainty-based weighting. Speciﬁcally, weight values of λ= 1 and λ= 0.2 were used. The overall proposed JDMIMTL approach is depicted in Figure 8. After training the model, all of the auxiliary task and CTC output layers are removed and the model is augmented with a softmax layer for classiﬁcation. The probability distribution of the classes, which is obtained as the output of the JD-MIMTL, can be decoded two ways in parallel for sequential classiﬁcation and trigger word detection. Best path decoding is used as the decoding scheme of the CTC outputs for both objectives. However, the ﬁnal prediction class is deﬁned as the statistical mode of the time steps of an MDI for sequential classiﬁcation, and as the prediction scores for the trigger sign accumulated over the time steps of an MDI for trigger word detection. To activate a device, the trigger sign must be correctly recognized from within a stream of data, and the activation should occur when the articulation of the sign is completed. One approach is cumulative score aggregation (CSA) [69], where the scores (i.e., prediction probabilities) of the trigger sign are accumulated over time, and a detection is recorded when the accumulated score, s, exceeds a predeﬁned threshold. The threshold can be adjusted to ensure the detection is triggered only when the trigger sign is complete. In this work, an adaptive, double-threshold CSA approach is proposed for trigger sign detection. Since the MDIs have varying lengths, the value of the threshold, T, is adaptively determined based on the interval length as: T = w ∗ γ, where w is the length of the MDI and γ is a predeﬁned conﬁdence factor. To mitigate the false rejection rate (FRR) of the detector, a second (lower) threshold, T, is also deﬁned. When the accumulated score exceeds the T, but not T , the detector is alerted to the possibility of a trigger and begins Fig. 8: Proposed multi-input multi-task learning network. recording the duration over which the score stays above T. The system is triggered if score exceeds Tfor more than w/2 seconds and the motion is classiﬁed as the trigger sign. In trigger word detection, effect of using single versus double thresholding can been seen from Figure 9a, which shows the trade-off between the false alarm rate (FAR) and FRR for γ ∈ {0.01 : 0.99} for the word AGAIN. When a single threshold is used, the FRR can climb as high 0.6, while double thresholding limits this value to just over 0.2. This is signiﬁcant because decreasing the FRR boosts the detection rate, D= 1−F RR−F AR, where FRR and FAR are deﬁned where n, nand nare the number of total, detected and false detected samples respectively. As shown in Figure 9b, when the resulting detection rates for single thresholding versus the proposed double thresholding approach are compared, it may be observed that for each considered trigger sign, the proposed approach yields a same or improved detection rate. The word TEACHER has the highest detection rate for both thresholding methods, achieving a detection rate of 0.93 and 0.96, while the word MONTH (self-occluded) has the lowest score of 0.65 for both cases. Signs with higher classiﬁcation accuracy tend to have higher detection rates as well, such as TEACHER and TEACH. The number of strokes (i.e. length) of the sign is an important consideration in trigger sign selection. For the purposes of automatic detection, strokes were deﬁned as components surrounding the sign-initial and sign-ﬁnal handshapes; thus, both the motion inherent to the sign (i.e. the stroke as deﬁned in sign language phonology), and transitional motions preceding and following the sign, were included in the analysis. This approach approximated predictive processing in human sign language recognition ( [70], [71]), while remaining consistent with ecological paradigm of wake sign use. Signs with few strokes deﬁned in this manner (less than 3) were found to have many false alarms, while those with more than 4 were prone to a high number of false rejections. This is similar to results in speech recognition, which report optimal wake word lengths of 3 to 4 syllables [72] - or, in quantitative terms, several entropy (high information-density) peaks within the continuous signal. A testing accuracy of 92% is achieved using the proposed JD-MIMTL approach, and surpasses the results achieved with various state-of-the-art sequential recognition approaches, as shown in Table V. This result is also quite close to the 93.5% accuracy attained using JD-MIMTL when the motion detector is replaced with ground truth segmentation. Moreover, the baseline established in Section VI-B using CNN+BiLSTM on single-input representation MDI data is improved to 84.3% by application of feature-level fusion. Consideration of CTC loss improves the results obtained for both single-input and fusion of multi-input representations. The accuracy using µDS increased to 80.6%, RD maps to 78.4% and RA maps to 71.3%, thus providing an average improvement of 3.73%. For RD maps and RA maps, MTL only slight improves performance by just 0.1%-0.2%, while the accuracy with µDS increases by 3%. The proposed JD-MIMTL approach yields a performance improvement of 8.4% over µDS as a single-input to MTL, and 4.5% improvement over multi-input feature level fusion without using MTL. The confusion matrix for the proposed architecture is provided in Figure 10. It can be seen JD-MIMTL exhibits the most confusion in signs with low radial motion (EVENING, high radial motion (TEACHER, TEACH) have the highest recognition rates. This is due to higher sensitivity of radars to radial velocity components. The proposed approach is tested on different ﬂuency groups to evaluate is efﬁcacy across different users. This is done by training the model solely with data from non-ASL users, but testing on ASL users’ data. Thus, not only are the participants between training and test sets different, but also their ﬂuency levels. In Figure 9c, the overall testing accuracy for all signs, and the trigger detection rate for the selected trigger word, the ﬁrst two columns report average results, the remaining 4 columns break down the results for speciﬁc participants, indicating whether the participant was an ASL learner or CODA. On average, the sequential ASL classiﬁcation accuracy for ASL users was 10% less than that attained from non-ASL users. But, the trigger detection rates remained above %94 irrespective of ﬂuency. In fact, 3 out of 4 ASL users’ trigger word is detected with 100% accuracy. Because RF sensors rely on kinetic properties of signing during recognition, signs that inherently contain greater (a) FAR and FRR of the word AGAIN for(b) Detection rates of the words for single single and double threshold.and double threshold. Fig. 10: Confusion matrix of the proposed JD-MIMTL. movement (especially inter-sign movements) are easier to recognize. For example, the signs TEACHER and TEACH both involve raising the hands to the level of the head, whereas hand/arm, resulting in a detection rate that is over 20% lower. Effective ASL-based device triggering will require the design of a unique sign for this purpose, as commonly used daily expressions may mistakenly trigger a device. In this regard, it is important to note that it is not necessary for such a trigger sign to have meaning in English; e.g. that KNOCK might be sensible in meaning has little bearing on efﬁcacy in terms of detectability, practical and cultural considerations. In future work, we aim to work with deaf community partners to jointly evaluate usability and efﬁcacy of kinetically unique trigger signs. Another important consideration for device operation with ASL is real-time implementation on dedicated edge computing platforms. Although there have been some studies of real-time gesture recognition using micro-Doppler signatures [57], [73]– [75], these works have considered only a small number of classes (less than 12), and focus on hardware acceleration or reduction of the computational complexity of the model itself. However, our initial work [76] in evaluating computational latency in the processing pipeline has shown that a signiﬁcant part of the latency is not in the classiﬁcation stage, but in the computation of the input representations themselves, especially micro-Doppler signatures. Latency depends not just on the duration (length) of the data, but also on short-time Fourier transform parameters, such as window length and overlap, which determine the dimensionality of the resulting spectrogram and impacts classiﬁcation accuracy. Joint optimization of input representation generation and DNN model will be necessary to maximize real-time recognition performance. The proposed techniques in this paper enable trigger sign detection for device activation and sequential recognition of ASL in the context of daily living. While conventional approaches to RF signal classiﬁcation utilize just one RF data representation, this work exploits µD spectrograms, RD maps, and RA maps in a JD-MIMTL framework for sequential classiﬁcation. By deﬁning tasks in terms of physically-relevant concepts for ASL recognition, sequences involving a mixture of 18 different daily activities and ASL signs was classiﬁed with 92% accuracy. The proposed double-thresholding trigger detection method achieves detection rates of 96% and 98.9% for non-ASL and ASL users, respectively, for the sign TEACHER. Potential selections for trigger signs are evaluated based on sequential activity recognition accuracy and replicability across the ﬂuency levels of users. The results demonstrate the potential for RF sensing to be used for ASLsensitive HCI. This work was funded in part by the National Science Foundation (NSF) Awards #1932547, #1931861, and #1734938. Human studies research was conducted under UA Institutional Review Board (IRB) Protocol #18-06-1271.