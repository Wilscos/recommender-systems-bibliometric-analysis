Memorability is an intrinsic, global, and stimulus-driven perceptual property that is important for better comprehension of visual stimuli [1, 2]. A growing body of work has studied image recognisability – one of the most fundamental attributes of memorability, both from a perceptual [1, 3] and a computational [4, 5] perspective. Recognisability has also been studied on information visualisations and previous work has revealed speciﬁc attributes that make visualisations memorable [6]. Recognisability measures whether a visualisation looks familiar or novel [3]. A visualisation that has unique features may stand out more and may therefore be more memorable. However, recognisability does not capture how effective a visualisation is in conveying information to observers. Other works have therefore studied recallability – a concept that goes beyond memorability, yet is complementary to it [7], by quantifying what viewers remember from a visualisation [8]. Despite its importance and potential for designing better information visualisations, a deeper understanding of which characteristics of visualisations inﬂuence recallability, and in which way, is currently missing. Current methods to assess recallability rely on visualisation experts to assign a qualitative score to self-reported free-text descriptions of viewers [7]. This approach is cumbersome and only provides a single score representing overall recallability while hiding the contribution of individual visualisation characteristics. While Borkin et al. [7] noted the importance of titles for recallability on visualisations, Polatsek et al. [9] conducted three low-level analytical tasks, focusing on visual elements with extrema, or speciﬁc values. These works inspired us to quantify visualisations’ recallability by looking into speciﬁc types of visualisation elements, such as the title, elements with extrema, or distinct data points. To quantify recallability, we propose to adopt a questionanswering paradigm, similar to visual question answering (VQA) [10] that has become widely popular in computer vision. VQA involves computational models in reasoning and correctly answering questions about images. While originally introduced for natural images [10, 11], VQA was also explored for information visualisations [12]. One follow-up work collected human performance values for the DVQA dataset by crowd-sourcing [13]. Inspired by this, we evaluate the performance of observers in answering questions about images correctly and use their performance as a subjective measure of information visualisation recallability. In this work, to quantify ﬁne-grained recallability of information visualisations, we design and execute a VQAbased study to collect VisQA: a novel visualisation dataset with 200 visualisations, which contains 1000 high quality questions annotated by visualisation experts and crowdsourced human recallability scores. Our work is inspired by and extends prior task taxonomy on visualisations [9] to deﬁne ﬁne-grained recallability scores [14] through ﬁve question types: identifying the title or theme, ﬁnding extrema, ﬁltering data elements, retrieving values, and understanding structure (subsection 3.1). Through our analyses of VisQA, we make several interesting ﬁndings: the highest recallability across question types occurs in questions that are about the title or the general theme (T-question), which is signiﬁcantly higher than other question types. Our replication study of recognition accuracy aligns well with the previous memorability studies [6, 7], and we conclude that there are no such visualisations with high recallability and low memorability. This ﬁnding suggests that recallability is more descriptive than memorability and more challenging to predict on information visualisations. Based on VisQA, we further present RecallNet, a novel method based on convolutional neural networks (CNNs) to predict one overall and ﬁve ﬁne-grained recallability scores, one for each question type. Finally, we prototype a novel application for visualisation type recommendations that maximises user recallability. Triplets of information visualisations are created with minimised content changes across visualisation types. The recallability scores on the visualisation triplets are then collected. Through a user study, we demonstrate that the prediction from our RecallNet not only maximises user recallability but also agrees with the preferences from scientiﬁc researchers in three out of four visualisation triplets. Our contribution is threefold: (1) We adapt a visual question answering (VQA) paradigm to quantify ﬁne-grained recallability of information visualisations. (2) We collect VisQA, a novel visualisation dataset with human recallability scores (N = 305) from 1000 questions and ﬁve question types. (3) We propose a computational model that predicts ﬁne-grained recallability of visualisations and demonstrates how our model can be used to automatically recommend a visualisation type that increases recallability. As such, our work points the way towards new methods and tools to create more effective information visualisations. Our work is related to previous works on 1) image memorability, 2) perception and memorability of visualisations, and 3) visualisation visual question answering (VQA) datasets. 2.1 Image Memorability A pioneering study [3] reported a strong capability of humans to recognise what they have seen before even up to 10,000 images, which is denoted as "image recognition memory". The following studies have demonstrated that memorability is an observer-independent property, which only depends on images [15, 16]. Furthermore, previous studies have proven that memorability could be reliably quantiﬁed for individual images by asking subjects to report whether images are novel or familiar [4, 17]. Large-scale memorability datasets have been collected for natural images, such as SUN-Mem [4], Figrim [18] and LaMem [5]. With the rise of deep learning, deep convolutional neural networks were proposed as computational methods to predict image memorability [5, 19, 20]. Recent work also integrated visual attention into the memorability prediction model [21]. Meanwhile, recallability is a complementary memory task to visual recognition [22], which requires subjects to view images and then recall what they have seen [23]. One previous work found out that sketch-based methodologies can improve the recall of a sampling distribution from an experiment [24]. Several recent studies are consistent with the conclusion that image memorability variation for recognition and recall tasks may be distinct [8, 25]. Based on this, our work is the ﬁrst to better understand the recallability characteristics and the factors that inﬂuence it on information visualisations. 2.2 Perception and Memorability of Visualisations Pioneering works in the visualisation community have examined how different data types and tasks inﬂuence human perception [26, 27, 28]. Bateman et al. claimed that the overembellishment (i.e., “chart junk”) improves recognisability but is not essential for understanding the visualisation [29]. This triggered a series of studies evaluating the impact of style on memorability and comprehensibility [30, 31, 32]. The effect of speciﬁc factors or components on recall memory has been investigated, such as interaction [33], prior knowledge [34], title [7, 35] and text redundancy [7]. Borkin et al. [6] studied visualisation memorability on the MASSVIS dataset, and their follow-up work [7] further conducted online crowd-sourcing studies to quantify both recognisability and recallability. There are two main drawbacks to the previous recallability quantiﬁcation procedure. Firstly, the method used to recall quality annotations is subjective and cumbersome. In addition, visualisation experts are necessary to attribute these scores. Secondly, the quality score scale with only four possible values is too coarse to represent a visualisation. To overcome these limitations, we introduce visual question answering (VQA) as a powerful paradigm to quantify the recallability of information visualisations. Through multiple questions and answers on different visualisation characteristics, we propose a novel computational model to predict not only overall but also ﬁne-grained recallability based on ﬁve different question types. The visual question answering (VQA) task [10] proposed in the ﬁeld of computer vision has triggered many follow-up studies and applications [11, 36]. Despite the importance of information visualisations, the visualisation VQA datasets have only been proposed in recent years. FigureQA [12] was the ﬁrst visualisation VQA dataset. Images were plotted in simple and fully synthesised visualisations in ﬁve visualisation classes, along with polar questions. DVQA [37] is a dataset focused speciﬁcally on the problem of visual reasoning on bar charts, which is used as a corpus for the topic of chart QA. PlotQA [38] and LEAF-QA [39] synthesised their question-answer pairs based on crowdsourced question templates from real-world data sources to increase the variety. Kaﬂe et al. [13] collected human performance values for the DVQA dataset using crowdsourcing. As a conclusion, VQA has not been used for memorability studies yet, and current visualisation VQA datasets are synthesised from simple templates with limited content, making it a distance away from real world visualisations. However, VQA provides an interesting means to get ﬁnegrained annotations and insights into recallability. In our work, we evaluate and obtain recallability scores by asking users questions and validating their answers. Therefore, we present the design of our novel adaptation of a VQA-based study on information visualisations and our novel VisQA dataset in the next section. The currently available recallability scores on the visualisation dataset MASSVIS [6, 7] are annotated from free-text Fig. 1: Sample visualisation with multiple-choice questions from VisQA. Five types of questions were designed by experts, which are questions regarding the title (T-questions), understanding structure or trend (U-questions), ﬁnding extrema (FEquestions), ﬁltering elements (F-questions) and retrieving values (RV-questions). Each ﬁgure has at least two question types. Image sourced from MASSVIS [6]. descriptions. However, its procedure to quantify recallability is coarse and cumbersome. Meanwhile, Visual QuestionAnswering (VQA) datasets [10] selectively target elements of visualisations in different question-answer pairs, making it a suitable setting to quantify memorability objectively and efﬁciently. Under the VQA paradigm, different tasks can be represented as different types of questions to viewers, and consequently, recallability is quantiﬁed by the accuracy in answering those questions. Towards quantifying recallability, we propose the Visualisation Recallability Question Answering Dataset (VisQA) — a VQA dataset with 200 real-world information visualisations and contains crowd-sourced human recallability scores (N = 305) obtained from 1,000 questions in ﬁve question types (see Figure 1). Visualisations in our dataset are mainly sourced from the MASSVIS dataset [6] to enable better alignment with prior works on this topic. The recognisability scores are also collected to replicate the previous memorability studies [6, 7]. 3.1 Visualisation Collection and Question Types We randomly selected a subset of 200 visualisations from the MASSVIS dataset [6]. Notably, we excluded all infographics in our collection, since infographics have the highest recognisability and recallability compared to all other types of visualisations [7]. However, scatter plots represent only 5 % of the sampled subset. Therefore, we collected 20 additional scatter plot visualisation by crawling the web through search engines (Google, Bing) using the keyword of scatter plots. Then, we replaced some bar plots with the web-crawled plots to balance the visualisation type classes. The ﬁnal distribution of visualisation types is: 60 bar plots, 45 line plots, 28 scatter plots, 19 pie plots, 19 tables and 19 others. Those visualisations that don’t belong to any of the ﬁrst ﬁve types are categorised as others, including box charts, isotype charts, or other complex visualisations. VisQA contains ﬁve types of questions: T-questions, Uquestions, FE-questions, F-questions, and RV-questions. Tquestions are questions regarding the title or the visualisation theme, and U-questions are about understanding the plot structure [38] or the general trend [39]. The remaining three question types correspond to three low-level analytical tasks introduced in [9], which are ﬁnding an extremum attribute value (FE-questions), ﬁltering data elements based on speciﬁc criteria (F-questions) and retrieving values for a speciﬁc data element (RV-questions). All question answering data were created by ﬁve data visualisation experts. They were asked to provide ﬁve questions per visualisation, and every visualisation has at least two question types. Each question corresponds to four possible answer options. Only one option is correct, two other options are choices with similar, yet incorrect answers, and the last option is always “I cannot remember”. See supplementary material for question examples. All annotations were saved separately in standard JSON ﬁles for each visualisation. There are 193, 150, 178, 99, 64 visualisations in VisQA that have at least one T-, FE-, F-, RV-, and U-question, respectively. T-question. T-questions are about the title or the general theme of the plot and do not require any reasoning. Example questions: What is the title of the visualisation?, What is the theme of the visualisation? For the incorrect choices in Tquestions, we either replaced keywords or phrases with words of similar, but different meanings, such as changing car thefts to car accidents or car manufacturers, or used titles from other visualisations, such as using Covered Transactions by Sector and Year, 2009-2011 and HIV Prevalence in Women Aged 15-49 Years by Region, 1990-2007 as incorrect choices for Covered Transactions by Sector and Year, 2009-2011. FE-question. These are questions about ﬁnding extreme values in the visualisation that fulﬁl certain conditions, without asking any exact numbers. Example questions: Which area had the lowest level of urbanization in 1950?, and Which particle is the latest discovered? For FE-questions, we used other elements that appeared in the visualisation as incorrect answer choices. For example, India and South-East Asia were the incorrect alternative choices for China in the question Which area had the lowest level of urbanization in 1950? (see Figure 1). F-question. These are questions about ﬁltering data ele- Fig. 2: Experiment design. From Left to Right: Visualisations are shown to viewers for 10 seconds in the "encoding" phase. In the "recall" phase, visualisations are blurred and each have a multiple-choice question next to it with a single correct answer. Finally, visualisations are shown to viewers for 2 seconds in the "recognition" phase. ments based on speciﬁc criteria. Example questions: Which particle is Bosons? and What is the source of the data? For Fquestions, we either changed keywords to their synonyms, or used other elements that appeared in visualisations as incorrect alternative choices, such as using Electron and Muon for Photon in the question Which particle is Bosons?. RV-question. These are questions about retrieving a speciﬁc value located in the plot. Those RV-questions in combination with FE-questions are all categorised in this type. Example questions: What is the maximum percentage of aid allocated? and What percentage of Indians are expected to live in urban areas by 2045? (see Figure 1). Example incorrect choices: about 60 % and about 70 % for What percentage of Indians are expected to live in urban areas by 2045?, and the correct answer is about 50 %. U-question. These are questions about understanding the structure or the trend of a visualisation. Example questions: What does the purple curve represent? and What decreases as time goes by? Example incorrect choices: for structure questions, other elements appearing in the visualisation are used, such as using Red and Blue as incorrect choices for Green in the question What color stands for Residents? As for questions about understanding trends, the choices are increasing, decreasing and almost the same. 3.2 Crowd-sourcing Study Set-up & Participants Our study design is illustrated in Figure 2. In the encoding phase of our study, study participants were shown a sequence of visualisations for 10 seconds each, which is similar to the prior memorability study [6]. We asked participants to memorise as much of the information presented in each visualisation as possible. In the recall phase, we showed participants the blurred image of the ﬁrst visualisation with a single choice question. The following question would be shown only if they clicked the next button, and they could not return to the previous question. This setting was to avoid providing hints in upcoming questions. After answering all ﬁve questions, the process for the second visualisation was the same. Then, the recognition phase involved an online memorability game similar to the prior work of Borkin et al. [6]. Study participants were presented with a sequence of images, and they had to select if they had seen this visualisation before. In each Human Intelligence Task (HIT), 40 blurred images were shown for 2 seconds each. The images in the recognition phase contained 20 visualisations that were the same in the recall phase, and 20 ﬁllers from a different group. Finally, participants were asked to provide anonymous feedback on the study design in a questionnaire. To support the VQA setting, we implemented question answering procedures in a web application. We blurred all visualisations with Gaussian ﬁlters, kernel_size adaptive to the image resolution, ranging from 5 to 24. We then integrated our VQA application into an existing crowd-sourcing toolbox that worked well with Amazon Mechanical Turk platform [40]. We deployed our experiment on Amazon’s Mechanical Turk (MTurk) platform to collect recallability and recognisability scores on all 200 visualisations, splitting them randomly into ten groups of 20 visualisations per HIT. MTurk workers could participate in multiple groups. To participate in one of our HITs, a worker had to be a Master Worker approved by MTurk as a quality check. Master Workers are top workers rated by the MTurk who have consistently demonstrated high quality works. Workers were paid $ 4.00 for completing each HIT. To ensure data quality, we ﬁltered out 467 HITs (N = 305 workers) if the answers were all “Yes” or “No” in the recognition task. For each visualisation, we received an average of 40.4 (SD=16.9) valid responses. The 305 workers were distributed in various educational levels: 8.2 % two-year degree, 56.9 % four- Fig. 3: Recallability and Recognition accuracy over all 404 HITs. Participants can recognise most of the visualisations easily, but they can only answer around half of the questions correctly. year degree, 22.3 % master’s degree or higher, and 12.6 % other/unreported. The age groups were 44.1 % in 25-34, 28.5 % in 35-44, 12.4 % in 45-55 and 9.9 % over 55. In the anonymous feedback form at the end of our study, most workers responded positively and two examples being: “Great self test for capable of memory power” and “After taking survey, I’m really getting interested in learning data plots and visualisations”. 3.3 Data Analysis Recallability formulation. For each question, we measured the recall accuracy as follows: Acc =, where RA is the number of correct answers, WA is the number of wrong answers, including the number of I cannot remember answers. If we focus on viewers who have selected choices excluding I cannot remember, the accuracy can be computed as: Acc= remember. Averaging all questions of type t in a visualisation gives us the recallability by question type and is computedP as: Rec=Acc(i), question∈ t. By averaging all questions in a visualisation, we have the overall recallabilityP of a visualisation as: Rec =Acc(i). HIT-wise Recallability. HIT-wise recallability as well as recognition accuracy across HITs (N = 404) are shown in Figure 3. 63.9 % of HITs had a recognition accuracy higher than 0.85, and 34.83 % were higher than 0.95, which shows that our study participants could easily recognise most of the visualisations (M = 0.83). Meanwhile, they could only answer about half of the questions correctly (M = 0.49, t (404) = 30.05, p < 0.001). Fine-grained Recallability by Question Type. Figure 4 illustrates that T-questions have the highest recall accuracy among all question types (average M = 0.66 including I cannot remember, and M = 0.69 excluding I cannot remember). The accuracy of T-questions is signiﬁcantly higher than other question types (t (1969) = 18.87, p < 0.001). 24.7 % of viewers selected I cannot remember in RV-questions, and 21.4 %, 18.8 %, 11.7 % for FE-, F- and U-questions, respectively. Only 5.1 % of the study participants selected I cannot remember in T-questions. We observed a mean proportion of 19.1 % Fig. 4: Recallability scores by question type. T-questions have a signiﬁcantly higher accuracy compared with all other question types (FE, F, RV and U). 24.7 % of the viewers selected I cannot remember in RV-questions, and only 5.1 % of the viewers selected I cannot remember in T-questions. (SD = 13.0 %) of study participants who selected I cannot remember from all visualisations. The lowest proportion is 3 %, while more than 50 % selected I cannot remember in seven speciﬁc visualisations. Figure 5 shows visualisations with the most and fewest I cannot remember answers from VisQA. We observe that increased visualisation complexity is a common characteristic for visualisations with the most I cannot remember answers. The encoding phase in our study only lasted for ten seconds, which might be too short for some complex visualisations. Recognisability: a Comparison to Prior Work. For a comparison to prior work on recognisability [6, 7], we also calculated the memorability (or recognisability) score on VisQA. According to Borkin et al. [6], the hit rate (HR) and false alarm rate (FAR) were computed as: HR = ability (memorability) of a visualisation was measured as: d= Z(HR) − Z(F AR), where Z was the inverse cumulative Gaussian distribution. Figure 6 (Left) shows the distribution of the raw HR scores of all visualisations from the recognition phase. Figure 6 (Right) shows the highest and lowest ranked visualisations across recognisability (memorability) and recallability from our VisQA dataset. Visualisations in each quadrant were ranked highest or lowest 15 % among all visualisations. Our analyses on VisQA yielded several insights on recallability in information visualisations. There are currently no baseline methods, neither for predicting overall recallability nor for ﬁne-grained recallability. Existing computational models only aimed for predicting memorability, also known as recognisability. We extend and build on state-of-theart architectures from other computer vision tasks, such as semantic segmentation [41, 42] and image classiﬁcation [43, 44], and use such methods as the backbone of our architecture. We designed our Recallability Network (RecallNet) with the speciﬁc goal of predicting both overall and ﬁne-grained recallability scores in one single model (see Figure 7 for an Fig. 5: Example visualisations with the most and fewest answers I cannot remember from VisQA. We observed a higher degree of visualisation complexity for those with multiple I cannot remember answers. Fig. 6: Left: Raw HR scores of target visualisations from the recognition phase. Right: The highest and lowest ranked visualisations (within 15 %) across recognisability (memorability) and recallability in VisQA. The y-axis represents the memorability score, and the x-axis represents the recallability score computed from overall visualisation question accuracy (independent of question type). overview). Inspired by UMSI [45], the currently state-of-theart architecture for visual importance prediction on graphic designs, we employ the Xception [41] model to effectively encode spatial information. Then, a global average pooling layer, a dense layer with 256 neurons, and ﬁnally a dense layer with 2 neurons are sequentially connected. One output neuron predicts the general recallability score, and the other one predicts the ﬁne-grained recallability score. 5.1 Implementation Details & Model Training We trained RecallNet using weights obtained from the Xception model – which was pretrained on ImageNet [46]. RecallNet was trained with the Adam [47] optimizer with a learning rate of 0.002 and 1:1 Mean Squared Error (MSE) joint loss for the two branches predicting the overall recallability score and the ﬁne-grained recallability score. We averaged all ﬁve questions for each image to prepare the ground truth of overall recallability scores. To train our RecallNet to predict ﬁne-grained recallability scores for a certain question type, we only used those visualisations that contained that question type from VisQA. There are 193, 150, 178, 99, and 64 visualisations with at least one T-, FE-, F-, RV-, and U-question, respectively. Five-fold cross-validation was applied to all evaluation processes. All experiments were conducted on a single Nvidia 2060 super GPU with 8GB VRAM. Baseline methods. Since no previous computational models focused on predicting recallability on visualisations, we designed three methods as baselines. We replaced the Xception feature encoder in RecallNet with VGG-16 [43] and ResNet-34 [44] as the ﬁrst two baselines. The third baseline model is based on UMSI [45], the current stateof-the-art architecture for visual importance prediction. We replaced the decoder in UMSI with a global averaging pooling layer, a dense layer with 256 neurons, and ﬁnally a dense layer with 2 neurons — the same ﬁnal layers as in our model. To be able to use the UMSI model, we annotated the visualisation with their types. We deﬁned six visualisation categories: scatter plot, line plot, bar plot, pie plot, table and others. Visualisations that did not belong to any of the ﬁrst ﬁve types were categorised as others. We trained all baseline models for 10 epochs on VisQA starting from ImageNet [46] pretrained weights. We used the Adam [47] optimizer with a learning rate of 0.002 and Mean Squared Error (MSE) loss Fig. 7: Method overview. RecallNet leverages the Xception model [41] to effectively encode spatial information. Then, a global average pooling layer, a dense layer with 256 neurons, and ﬁnally a dense layer with 2 neurons are sequentially connected. One output neuron predicts the general recallability score, and the other one predicts the ﬁne-grained recallability score. for training. 5.2 Performance Evaluation We compared the performance of our RecallNet method to the three baselines VGG-16 [43], ResNet-34 [44] and UMSI [45]. Table 1 summarises ﬁne-grained recallability performance on VisQA under 5-fold cross-validation evaluation. The predictions of UMSI [45] collapsed to the same values on every image of validation set, independent of how we adjusted the loss ratio between classiﬁcation and recallability regression task. This is a typical phenomenon of over-ﬁtting. Our method and the other two baselines do not have over-ﬁtting problems, so we computed the mean squared error for them. Results showed that RecallNet outperformed the baselines under overall recallability and four ﬁne-grained recallability scores, with mean MSE of 0.035 for overall recallability, and 0.021, 0.022, 0.017, 0.043 for FE-, F-, RV-, and U-questions respectively. ResNet-34 was the best performing method for T-questions with a mean MSE of 0.047, while our RecallNet was second with a mean MSE of 0.052. We also computed the correlation coefﬁcient of recallability scores of RecallNet with those from a previous recallability study [7]. However, we observed non-linear relationships. The correlation coefﬁcient between our overall recallability and their description quality scores is -0.013, and between our T-question recallability and their title mention is -0.066. The recallability scores from prior work were generated from free-text description without any hints, but our recallability scores were computed from multiple-choice questions with rich context. The non-linear relationships between free-text recallability and multiple-choice recallability suggest that the number of hints provided to viewers are an important factor that may inﬂuence recallability. Ablation study. We further carried out an ablation study to investigate how each ﬁne-grained recallability score inﬂuences overall recallability (Table 2). In RecallNet, the overall recallability trained with T-questions has the lowest mean squared error of 0.030 and the most stable variance of 0.006. In ResNet-34 [44], the overall recallability trained with RVquestions has the lowest mean squared error of 0.029 and the most stable variance of 0.008. In VGG-16 [43], the overall recallability trained with T-questions has the lowest mean squared error of 0.037 and the most stable variance of 0.007. 5.3 Visualisation Type Recommendation Visualisation type recommendation is one practical use case for a visualisation recommendation system [48]. Prior research has proposed ways to decide whether line graphs or scatter plots are more suitable for time series data [49]. In our case, we build on our RecallNet to build a prototype implementation that can recommend a visualisation type that maximises recallability. We created four triplets of visualisations from open-source databases. In each triplet, visualisations have the same data sources but different visualisation types: bar, pie, and line plot, respectively. We used the same colouring scheme, font family, font colour and font size across bar and pie plots to minimise any potential inﬂuence of bottom-up saliency [9]. All visualisations in one triplet share the same ﬁve multiple-choice questions, and each question belongs to one different type: T, FE, F, RV, and U – introduced in subsection 3.1 (see supplementary materials for ﬁgures and questions). We conducted the same crowd-sourcing study that we used to collect VisQA and recallability scores on the visualisation triplets. All visualisations in four triplets were assigned to three tasks, and every task contained four visualisations, one visualisation from a different triplet. Crowd workers were paid $ 0.80 for completing each HIT. We received 38 (SD = 0.82) valid responses per task. The mean Tquestion recallability among all triplets was 0.62 (SD = 0.19). The difference when compared to T-question recallability in VisQA was not statistically signiﬁcant (t (509) = -1.99, p > 0.95). Meanwhile, the mean FE-, F-, RV-, and U-question recallability scores are all signiﬁcantly higher than VisQA, with t-test scores of 11.03, 8.20, 3.97, 5.66, p < 0.001, respectively. Table 3 shows recallability scores on visualisation triplets. We noticed that the recallability of T-questions is TABLE 1: Fine-grained recallability performance on VisQA under 5-fold cross-validation evaluation. Best results are shown in bold, second-best are underlined. TABLE 2: Ablation study on the performance of how ﬁne-grained recallability inﬂuences the overall recallability. Best results in each row are shown in bold. TABLE 3: Fine-grained recallability scores on visualisation triplets. Best results for each visualisation type (pie, bar, or line) are shown in bold. stable across visualisation types, the recallability of pie plots is the best in F-questions and RV-questions, bar plots in Tquestions and FE-questions, and line plots in U-questions. We also conducted a user study (N = 8) with experienced scientiﬁc researchers who are accustomed to creating their own data visualisations. The four bar plot-pie plot pairs created in the previous crowd-sourced study were presented to the study participants, and they were asked to provide their preferred visualisation for each pair using a 5-point Likert scale (with 1 = the ﬁrst visualisation is better and 5 = the second visualisation is better (see Figure 8)). We predicted the overall recallability scores for each visualisation using RecallNet and compared it to the preference of our study participants (see Table 4, and supplementary materials for full table). For three of the four visualisation pairs, the ranking of recallability scores agreed with crowd-sourced data. In the only contradictory pair (see the right of Figure 8), study participants opted for the pie visualisation type with a mean score of 3.11 (SD = 1.73). This work made a substantial leap towards quantifying ﬁnegrained recallability scores on information visualisations. VisQA Dataset. VisQA is the ﬁrst dataset to introduce ﬁne-grained recallability on an information visualisation dataset as well as high-quality question-answer annotations. The recallability scores are metrics that reveal human performance with a speciﬁc type of question. With rich annotations of the elements necessary for the answers, the recallability score of a certain question could be converted into 2D spatial representations (e.g. recallability heatmaps). Since a better visual encoder beneﬁts VQA models [11], the recallability maps could be introduced as an additional TABLE 4: Our overall recallability scores and scientiﬁc researcher preferences on four pie-bar visualisation pairs. 1 = pie plot is better and 5 = bar plot is better. The preference is presented as mean and standard deviation. Better results are highlighted in bold. input to VQA models. Addtionally, VisQA is a novel visualisation VQA dataset that uses real-world, visually rich visualisations coming in part from the MASSVIS dataset. Crowd-sourcing is the standard approach for collecting questions on VQA datasets, and the questions for current visualisation VQA datasets [38, 39] were collected by regular crowd workers. In contrast, all the questions in our VisQA came from visualisation experts, which promises a higher quality of questions than previous VQA datasets. Moreover, most visualisations in current VQA datasets [38] are generated pragmatically. However, when it comes to realworld visualisations, the vector representations are usually missing, and researchers have to retrieve the structural information, often by manual annotation [7], which is timeconsuming and constrains the dataset size. The introduction of recallability to the VQA setting and the high quality of visualisations and questions enable VisQA to trigger fundamental studies on visualisation QA or chart QA. Recallability vs. Recognisability (Memorability). The bottom-right quadrant in Figure 6 (Right) is completely empty, which means that there are no such visualisations with high recallability (top 15 %) and low memorability (bottom 15 %) in VisQA. This suggests that memorability is the basis for recallability, and that visualisations have to be sufﬁciently memorable before they become recallable. The visualisations in the top-right quadrant share some characteristics, like a big Fig. 8: Survey interface for evaluating visualisation bar-pie pairs. Visualisations in each pair have the same data sources, colouring scheme, and font attributes. and highlighted title and some explanatory text. Meanwhile, the visualisations in the top-left quadrant of Figure 6 (Right) have high recognisability and low recallability. Compared to the top-right quadrant, visualisations in the top-left quadrant are less recallable. All visualisations in the top-left quadrant are simple monotone plots with few embellishment (e.g. isotype plots). The visualisations in the bottomleft quadrant are easily forgettable and hard to recall. These visualisations are usually overly complex and don’t have meaningful titles or additional explanatory text to convey key messages. Compared to the bottom-left quadrant, all the visualisations in top-left and top-right quadrant are much simpler (low data-to-ink ratios), and always with titles, which aligns well with the ﬁndings in previous studies [6, 7]. Therefore, our study on VisQA validated previous results and provided interesting insights into how recallability and recognisability (memorability) are different and connected. Visualisation Type Recommendation. In Table 3, Tquestion and RV-question recallability scores are stable across types (within 4 % of variation). It suggests that the recallability of T-questions and RV-questions are almost irrelevant with visualisation types. As long as the answer elements are presented in visualisations with similar visual attributes, such as colouring scheme, font family, font size, and element location, the difﬁculty of recalling the answers to these questions should be on the same level. As for each visualisation type, the recallability of line plots is the best for U-questions, since they are usually the best choice for interpreting time series data [49]. As for FE- and RV-questions, the recallability of line plots is in the last place. Since readers have to go through multiple elements that are far away from each other to ﬁnd the answers for FE- and RV-questions [9], line plots are not ideal for these kinds of questions. Table 4 demonstrated that overall recallability was in agreement with the preference of our study participants. We also observed that the FE-, F-, RV-, and U-question recallability of the triplet study was signiﬁcantly higher than VisQA, and the T-question recallability was not statistically signiﬁcantly higher. The triplet study contained simple scientiﬁc plots, which are further away from realworld, visually rich visualisations, while the length of each HIT in the triplet study was only 1/5 of the study of VisQA. The reason for the signiﬁcant change of recallability in the above four question types could be explained by subjectively easier questions in the study. Alternatively, crowd workers performed much better in the ﬁrst several questions compared to the last ones. To validate this hypothesis, we calculated the question type recallability on VisQA that appeared in the ﬁrst 1/5 of each HIT. The mean recallability of FE-questions increased from 0.43 to 0.46, F-questions from 0.46 to 0.52, RV-questions from 0.40 to 0.42, and U-questions from 0.48 to 0.42. None of these recallability scores was higher than any recallability score in the triplet study, so the length of the study was not the critical reason for the signiﬁcant recallability changes. In conclusion, RecallNet generally agreed with the preference of scientiﬁc researchers in the use case of visualisation type recommendations. Limitations and Future Work. There is always a tradeoff between quality and quantity, which was also the case when designing and collecting our VisQA dataset. Due to the increasing workload in designing high-quality questions for the VQA settings that were speciﬁcally targeted for each visualisation, the scale of VisQA became relative small. This inﬂuences some computational models, e.g. it caused the over-ﬁtting problem of UMSI [45]. To allow more complex models for recallability prediction, it is essential to extend our VisQA. In the future, we plan to enrich it with more complex data visualisations such as box plots, radar and combination plots. On the other hand, gaze behaviour analysis in a VQA setting on information visualisations has not yet been studied. However, it is a fundamental step to understand the human visual attention system while viewing visualisations. While physical laboratory studies require special-purpose eye tracking equipment, online crowd-sourcing studies or gaze estimation from substitution devices (e.g., mouse, web camera) can be used as a proxy to human attention. In the future, we will investigate such methods to collect human attention data and extend VisQA with such annotations. This work presented a novel adaptation of a VQA-based study to collect VisQA, a novel visualisation VQA dataset with 200 "in-the-wild" visualisations annotated with crowdsourced human recallablity scores in ﬁve question types, along with a deep convolutional network to predict ﬁnegrained recallability of visualisations. This work made a substantial leap towards quantifying ﬁne-grained recallability scores on information visualisations and envisions several potential applications. We prototypically demonstrated one application developed out of this work. Through a user study, we demonstrated that the prediction from our RecallNet not only maximised user recallability but also agreed with the preferences from scientiﬁc researchers in three out of four visualisation triplets, i.e. different visualisations for the same data source. For visualisation type recommendation systems, leveraging recallability would be a strong criterion in providing feedback to users. Y. Wang was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161. M. Bâce was funded by a Swiss National Science Foundation (SNSF) Early Postdoc. Mobility Fellowship. A. Bulling was funded by the European Research Council (ERC; grant agreement 801708). We would like to thank Sruthi Radhakrishnan for the creation of visualisations for our application, Dominike Thomas for paper editing support, as well as Daniel Weiskopf, Nils Rodrigues, Guanhua Zhang, Florian Strohm, and Jialin Li for helpful comments on this paper.