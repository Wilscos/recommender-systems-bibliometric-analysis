Nowadays, recommender systems have become an integral part of our lives; with the ever-increasing growth of information, these systems guide so many aspects of our life. Recommender systems provide important and relevant cases and ﬁlter non-relevant ones, which help us in making good decisions and saving our time. One of the main objectives of recommender systems is to model user preferences for items based on the recorded informationKoren et al. [2009]. User preferences can be extracted through their ratings, clicks, or percentage of views[Nakhli et al., 2019, Hu et al., 2008, Xin et al., 2019]. In most online services, customers can submit their reviews for products and share their opinions with other customers to help them. Researches show that nearly one-third of online shoppers refuse to purchase products that have not received positive feedback from customers; Therefore, it is a mutual beneﬁt for users and the company, which simultaneously increases user’s satisfaction and corporate proﬁt[Adomavicius and Tuzhilin, 2011, Utz et al., 2012, Von Helversen et al., 2018]. Researchers have recently been using this valuable information to represent users and One of the main challenges in recommender systems is data sparsity which leads to high variance. Several attempts have been made to improve the bias-variance trade-off using auxiliary information. In particular, document modeling-based methods have improved the model’s accuracy by using textual data such as reviews, abstracts, and storylines when the user-to-item rating matrix is sparse. However, such models are insufﬁcient to learn optimal representation for users and items. User-based and item-based collaborative ﬁltering, owing to their efﬁciency and interpretability, have been long used for building recommender systems. They create a proﬁle for each user and item respectively as their historically interacted items and the users who interacted with the target item. This work combines these two approaches with document context-aware recommender systems by considering users’ opinions on these items. Another advantage of our model is that it supports online personalization. If a user has new interactions, it needs to refresh the user and item history representation vectors instead of updating model parameters. The proposed algorithm is implemented and tested on three real-world datasets that demonstrate our model’s effectiveness over the baseline methods. items better and handle the sparsity problem[Ling et al., 2014, McAuley and Leskovec, 2013]. Various document-based modeling approaches such as Latent Dirichlet Allocation (LDA) and Stacked Denoising Auto-Encoder (SDAE) have been proposed to improve the accuracy of recommender systems by utilizing textual data such as reviews and storylines[Wang et al., 2015, Wang and Blei, 2011, McAuley and Leskovec, 2013, Ling et al., 2014]. In addition, some methods have been proposed by integrating the aforementioned topic modeling methods with Collaborative Filtering, known as Collaborative Topic Regression (CTR)[Wang et al., 2015, Mnih and Salakhutdinov, 2008]. However, such integrated approaches do not fully capture document information. To address this issue, Some works like [Zhang et al., 2018, 2020, Kim et al., 2016] utilize Convolutional Neural Networks (CNN). CNNs facilitate a deeper understanding of documents and generate a better latent vector than topic modeling methods. Despite the prevalence and effectiveness of Document Context-Aware models in recommendation systems, we argue that such models are insufﬁcient to learn optimal representation for users and items. The major limitation is that their historical behaviors did not inﬂuence users’ interests because their current interests are intrinsically dynamic. The second challenge is how to combine the interaction information of other users concerning their opinions with textual data. To provide better representation that contains both interaction and textual information. To address these two limitations, we aim to build a Hybrid Recommender System. We create a proﬁle for each user and characterize the user with their interaction history information. The opinions of users on items can capture users’ preferences on items that provide better representation. Likewise, We build a proﬁle for each item based on the set of users who have interacted with the target item; even users might express different opinions for each item. By combining these pieces of information with the textual description, we can capture the characteristics of the item from different perspectives. Another advantage of our model is that it supports online personalization. For online personalization, if a user makes new interactions, the recommender model needs to refresh the top-K recommendation for the user instantaneously, which needs to retrain the model parameters[He et al., 2018, 2016]. However, it takes too many computation resources to perform model retraining in real-time. Instead of updating the parameters, our model can refresh the user and item history representation vector without updating any model parameters. The reminder of the paper is organized as follows. In section 2 we brieﬂy review the related works. In Section 3 we describe our proposed model. In Section 4 we evaluate our model on three real-world datasets. Finally, in section 5 we conclude our work with future directions. In this section, we brieﬂy review several closely related works, including general recommendation methods, context-aware recommender systems, and attention mechanism. Collaborative Filtering (CF) is one of the most popular and widely used ﬁltering methods; the motivation behind it comes from the idea that our future behavior depends on past information to some extent Koren et al. [2009]. Also, we often get the best recommendations from other users that have similar tastes to us Cheng et al. [2016]. CF can be divided into model-based and memory-based approachesSarwar et al. [2001], Deshpande and Karypis [2004]. Matrix Factorization (MF) is a class of the CF algorithm. The idea behind MF is to represent each user and item in lower dimension latent space, and the objective is to exploit the relationship between users and items latent vectors[Koren et al., 2009, Wei et al., 2017]. By multiplying these vectors, a user’s preference, is obtained. For example, if we have and item latent vector respectively are predicted rating of U to I is calculated by multiplying the corresponding vectors. The other two most common forms in CF are user-based and item-based CF. The user-based collaborative ﬁltering (UCF) is based on looking for users with similar tastes to the active user and representing each item with users who have chosen the target item. In contrast, item-based collaborative ﬁltering (ICF) represents each user with their interacted items and recommends items that are similar to the user proﬁle[Xue et al., 2019, He et al., 2018]. This paper aims to integrate them with a context-aware recommender system (CARS) that uses the textual description of items as additional information, where items latent vectors contain both contextual and historical interaction information. In recent years, deep learning, due to its capability of approximating any continuous function and capturing intricate patterns, has garnered attention in many ﬁelds, including recommender systems[Zhang et al., 2019, Mu, 2018]. Some deep learning methods are used to estimate user preferences. For example, [He et al., 2017] fused CF with neural architecture, which considers both the linearity of MF and the non-linearity of neural architecture to enhance ranking performance. Another line of work uses auxiliary information such as text, image, and acoustic features in the CF model, which we elaborate in the following subsection. The goal of context-aware recommender systems (CARSs) is to model users’ preferences by using contextual information in the recommendation process. In CARSs, contextual factors are considered when modeling user proﬁles or item proﬁles. Some researches show that adding contextual information to the recommendation process can improve the accuracy and address the sparsity problem. [Kim et al., 2016, Zhang et al., 2020]. One of the most important and widely used tools in text processing is CNN which has become the cornerstone of deep learning [Zhang et al., 2018, 2020, Kim et al., 2016]. Taking advantage of this success, [Kim et al., 2016] uses CNN architecture to capture contextual information of the movie’s description and combines it with probabilistic matrix factorization (PMF) to enhance rating prediction[Mnih and Salakhutdinov, 2007]. In recent years, another one of the stunning successes in deep learning was the attention mechanism, which will be discussed in more detail in section 2.3. [Zhang et al., 2018, 2020] have used this mechanism and integrated it with residual networks.[Smirnova and Vasile, 2017, Livne et al., 2019] proposed a context-aware session-based RecSys by utilizing conditional RNNs, which injects contextual information into input and output layers. It modiﬁes the behavior of the RNN by combining context embedding with item embedding. To address the sparsity problem and improve the accuracy of the CARS, Livne et al. [2019] proposed a model which uses the encoding-decoding process to learn latent context representations and utilizes sequences of user data derived from contextual conditions. The attention in deep learning is based on the screening ability of human beings, which has been applied in many tasks such as Information Retrieval (IR), Natural Language Processing (NLP) and recommender systemsChaudhari et al. [2019]. Recommender systems employ the attention mechanism to improve performance and interpretabilitySun et al. [2019]. The basic idea behind this mechanism is to assign different weights to different parts. It can be regarded as a weights vector; Calculate a weight for each feature vector that shows the importance of the corresponding feature, then multiply each of the estimated weights into corresponding inputsXiao et al. [2017]. The attention model calculates attention weights by a feedforward neural network which is denoted byα(t, 1) annotations: LetU = {u where n is the number of users and m is the number of items. In recommender systems, the user-item rating matrix is denoted by R 1 ≤ j ≤ m are the ratings that are given to items by users. An example input of our model is illustrated as follows: ,α(t, 2),...,α(t, t). The output of the attention layer is generated using the weighted sum of , u, ..., u}andV = {v, v, ..., v}be the sets of users and items respectively and each column represents an itemiwith1 ≤ k ≤ n. The elements of this matrix Fig 1 shows the architecture of our proposed model, which consists of three sections: user section, item section, and rating prediction section. The main objective in the user and item section is to learn a latent vector for each item and user by using available information. The third part is used to predict the rating value of users to items. The user section aims to learn the user latent vector by using past interactions and opinions of the user. Each user is mapped to a multi-hot vector composed of movies that the user has rated. In this section, two cases are studied: 1) Without rating information 2) With rating information. Without Rating Information without considering rating information. Then each of the input vectors is passed through the attention network to calculate attention weights, and each of the estimated weights is multiplied by the corresponding vectors. With Rating Information We can use this auxiliary information as Fan et al. [2019] and combine it with the user’s interaction information to provide a richer user latent vector than baseline methods. Like the previous case, each input vector is passed through the attention network to calculate weights. By multiplying each of the input vectors to corresponding weights, the latent user vector is obtained. [u, u, ..., u][w, w, ..., w][v, v, ..., v] < n, k< k and m< m. Given the above inputs, our model aims to predict the rating. As shown in ﬁg 1, the item section consists of two parts: Document Information Modeling Part GloVe, that converts each word to a dense vector with a ﬁxed length. Suppose we have document that describes the item, so we obtain a matrix for each word. As shown in ﬁg2, we use one dimensional CNN with multiple ﬁlters to project input to vectors and capture various types of contextual features: Wherec f is the activation function, W is the weight matrix, and b ∈ R is the bias. As to the variable length of each document, we use the convolution architecture in [Collobert et al., 2011, Rakhlin, 2016]. After passing the document through the convolution layer, we obtain a feature vector with variable length for each kernel weight. By using max-pooling, each vector is converted to a scalar that extracts the features from the previous layer. User Information Modeling Part of users interacting with the target item. As we said, each user can express their satisfaction, denoted as rating score, which means that all interactions are not of equal importance. We use this information and, similar to Fan et al. [2019] combine the rating embedding with the user embedding. By passing each input vector through an attention network, the attention weights are obtained. Next, each vector is multiplied by the corresponding weight. Finally, we concatenate these two vectors and pass them through a fully connected layer. Finally, the user and item latent vectors are obtained according to the method proposed in the previous sections. Then, they are concatenated and passed through fully connected layers to predict https://nlp.stanford.edu/projects/glove/ Figure 2: CNN architecture to process the documents that describe items. ∈ R,iis the number of convolution operations,mis the number of convolutional kernels, is a ﬁxed-length vector. Finally, we pass the vector through a fully connected layer. ratings using the following equations: In which and bias vector. In addition, U and I represent user and item latent vectors. In this section we ﬁrst introduce the evaluation datasets, some implementation details, optimization and evaluation methods. Then we analyze our model from different aspects. To demonstrate the effectiveness of our model, we used MovielensHarper and Konstan [2015] Amazon Since our evaluation datasets do not contain the item’s description, we used IMDB includes the storyline and summary of the movies that was provided by MovieLens and Amazon researchers. We implemented our model using TensorFlow libraryAbadi et al. [2015] in Python and used GeForce GTX 1660Ti GPU for the computations. The datasets were divided into 10%for the validation set, and the remaining for each user and item in the training set. In addition, We set the batch size to 256 and removed the movies that do not have descriptions and users with less than three ratings. We used the following settings in CNN architecture: 1) Set the word embedding length to 300 and used GloVe pre-trained word embedding model. 2) Set the maximum length of each item’s description to 300 words. 3) Removed the stop words. 4) Selected 8000 top words that have the highest TF-IDF scores as vocabulary set. 5) used ﬁlters with different sizes (3,4,5) and 100 ﬁlters per size to capture semantic information of documents. We know that each user may have a different rated item vector length, and each item’s description may not have the same number of words. We use the masking trick to solve this problem by adding masks to ensure all cases have the same length. To evaluate our model and compare with the baselines, we use Root Mean Squared Error (RMSE), Hit Ratio (HR), and Normalized Discounted Cumulative Gain (NDCG) as evaluation metrics that are https://movielens.org/ http://jmcauley.ucsd.edu/data/amazon/ https://www.imdb.com/ k,σ,W, andbrespectively denote the number of layers, activation function, weights matrix datasets. The datasets contain user ratings of items on a scale of 1 to 5 as Table 1. Figure 3: Effect of rating information, user information, pre-initialized model, document information, and embedding size on ML-1m dataset. deﬁned as: WhereN rating, and the total number of the ratings. Also, has a value of 1; we adopted the leave one out evaluation metric, which holds out the latest interaction of each user as the testing data and uses the remaining interactions for training. We paired each ground truth item in the test set with 99 randomly sampled negative instances. Hence, the task becomes to rank these negative items with the ground truth item for each user. The performance is judged by HR and NDCG. For training our model we adopted the Adaptive Moment Estimation (Adam) Kingma and Ba [2014] optimization algorithm, where the learning rate is adopted for each parameter. We compared our model with the following baselines: RMSE =T(8) HR@K =Number of cache hitsNumber of cache hits + Number of cache Misses(9) NDCG@K = Z2− 1log(i + 1)(10) ,M,ˆrandTrespectively denote the number of the users, items, estimated value of the ris the graded relevance of item at positioniHe et al. [2015]. In HR and NDCG, Mnih and Salakhutdinov [2007]: Probabilistic Matrix Factorization is a standard rating prediction model which only utilizes user-item rating matrix and models latent factors of users and items by Gaussian distribution. Wang and Blei [2011]: Collaborative Topic Regression is a state-of-the-art model which combines PMF and Latent Dirichlet Allocation (LDA) to predict rating of user u on item i. Wang et al. [2015]: Collaborative Deep Learning is a model that combines autoencoders and PMF which analyzes documents via SDAE. document information for items as input and combines PMF and CNN methods to predict rating. Figure 4: Testing RMSE loss, HR@10 and NDCG@10 of our proposed model and the best competitor w.r.t. the epochs on ML-1m, ML-10m and Amazon. Among them, PMF, NeuMF and GNNSR are models without document information for rating prediction while other models use document information as auxiliary information for rating prediction. This subsection studies the impact of the rating information, attention mechanism, effect of user information, and word embedding size in item modeling. Graph Neural Networks for Social Recommendation. He et al. [2017]: NeuMF is a state-of-the-art Matrix Factorization model without document information. The original implementation is for ranking task and we adjust its loss to square loss for rating prediction. that integrates attention mechanism with CF to enhance rating prediction. social recommendations. in order to capture semantic information and solve the gradient vanishing problem. Table 2: Performance Comparison of different methods on ML-1m, ML-10m, and Amazon datasets. Bold Scores are the best, while underlines scores are the second best. A user can express their opinion of the items, which means that all interactions do not have the same importance for the target user. If a user likes the target item very much, they will give it a high score. In this subsection, we compare these two cases and show the results as Fig 3a. It is essential to point out that we didn’t consider user information in item modeling for all cases in this subsection. As shown in Fig 3a if we consider the rating information in our model, it achieves better performance. To better understand the proposed model and effectiveness of the involved attention mechanism, we replace it in user and item modeling with Max-Pooling and Mean-Pooling. Fig 3b shows the results on ML-1m. Like the previous subsection, we didn’t consider the user information in item modeling. As shown in Fig 3b, The results have met our previous expectation, which means that we have the best performance if we consider attention in our model, while Mean-Pooling achieves better performance than Max-Pooling. We now focus on analyzing the effectiveness of the user information in item modeling. At ﬁrst, We consider the item’s description as input of the item section and ignore the user information. Then in the following case, we simultaneously consider the user information and item’s description as item input which the results are given in Fig 3c. The results show that combining users’ behavior and textual information can improve the performance of the model. 4.2.4 Effects of the Document Information in the Item Modeling In Fig 3e, We can see that without document information modeling, the performance of rating prediction has deteriorated signiﬁcantly, and it justiﬁes our assumption that document information modeling on item section has information that can help the model to learn each item’s latent vector and improve the recommendation performance. 4.2.5 Effects of the Embedding Size in the Document Information Figure 3f shows the performance of our models with respect to various word embedding sizes. When we increase embedding size from 100 to 200, RMSE does not boost; setting the value to 300 leads to the best result. In Fig 3d, we investigate the impact of GloVe pre-trained word embedding model on our task. GloVe signiﬁcantly helps the model to reduce RMSE. Table 2 summarizes the results of all models on three benchmark datasets, where underlined scores are the best competitor result and bold scores are our model’s results. The last row shows the improvement of our model to the best baseline. Some of the elements in Table 2 are "N/A", which means that the baseline model has optimized just one objective function: prediction error type or ranking performance. The results for another objective are not available. For example, PMF optimizes just rating prediction errors, so HR and NDCG metric results are not available. The original implementations of ConvMf, Att-ConvCf, GNNSR, and xRConvCF are for the rating prediction recommendation task; we adjust their loss to binary cross-entropy loss for ranking purposes. Also, the original implementation of NCF is for the ranking recommendation task, which we adapt its loss to the rating prediction recommendation task. Among baseline methods, ConvMF, Att-ConvCF, and xRConvCF use items description as auxiliary information, and the rest of them don’t use it. Table 2 shows our model’s results, overall rating prediction error (RMSE), HR, and NDCG over ten baselines on three datasets, which shows that the proposed model achieved signiﬁcant improvement over all the baselines. The ﬁrst row of ﬁg 4 shows RMSE values of the test dataset obtained by the two methods, best baseline and our proposed model, during 300 epochs. The second and third rows show HR and NDCG values during 100 epochs respectively. This paper aimed to handle the sparsity problem and improve recommendation accuracy by considering contextual information and combining it with historical data. In the user section, we built a proﬁle for each user based on their interacted items, and similarly, we created a proﬁle for each item based on the users who have interacted with it. We analyzed the effect of each section on rating prediction and evaluated our model on three real-world datasets, which demonstrated the effectiveness of our model over the state-of-the-art competitors. In future work, using one of the state-of-the-art models such as NeuMF, we try to ﬁnd friends for each user according to the user embedding vector. Then we incorporate the information of these trusted friends into user modeling. Another future work would be exploiting bidirectional models such as BERTDevlin et al. [2018] to provide a representation for each word by jointly conditioning on both left and right context.