<title>Benchmarking Memory-Centric Computing Systems: Analysis of Real Processing-in-Memory Hardware</title> <title>arXiv:2110.01709v1  [cs.AR]  4 Oct 2021</title> cells and circuitry to perform computation inside a memory chip at low cost. Prior works propose PUM mechanisms using SRAM [109–112], DRAM [113–118, 118–125, 125–132], PCM [133], MRAM [134–136], or RRAM/memristive [137– 153] memories. The UPMEM company has designed and fabricated the ﬁrst commercially-available PIM architecture. The UPMEM I. INTRODUCTION PIM architecture [1, 154, 155, 157, 158] combines traditional In modern computing systems, a large fraction of the DRAM memory arrays with general-purpose in-order cores, execution time and energy consumption of modern datacalled DRAM Processing Units (DPUs), integrated in the same intensive workloads is spent moving data between memory and DRAM chip. UPMEM PIM chips are mounted on DDR4 processor cores. This data movement bottleneck [2–6] stems memory modules that coexist with regular DRAM modules from the fact that, for decades, the performance of processor (i.e., the main memory) attached to a host CPU. Figure 1 (left) cores has been increasing at a faster rate than the memory depicts a UPMEM-based PIM system with (1) a host CPU, Figure 1: UPMEM-based PIM system with a host CPU, standard main memory, and PIM-enabled memory (left), and internal components of a UPMEM PIM chip (right) [154, 155]. (2) main memory (DRAM memory modules), and (3) PIMof the UPMEM-based PIM system using microbenchmarks to enabled memory (UPMEM modules). PIM-enabled memory assess various architecture limits such as compute throughput can reside on one or more memory channels. and memory bandwidth, yielding new insights. Second, we present PrIM (Processing-In-Memory benchmarks), an openInside each UPMEM PIM chip (Figure 1 (right)), there are 8 source benchmark suite [156] of 16 workloads from different DPUs. Each DPU has exclusive access to (1) a 64-MB DRAM application domains (e.g., neural networks, databases, graph bank, called Main RAM (MRAM), (2) a 24-KB instruction processing, bioinformatics), which we identify as memorymemory, and (3) a 64-KB scratchpad memory, called Working bound workloads using the rooﬂine model [159] (i.e., these RAM (WRAM). The MRAM banks are accessible by the host workloads’ performance in conventional processor-centric arCPU for copying input data (from main memory to MRAM) chitectures is limited by memory access). Table I shows a and retrieving results (from MRAM to main memory). These summary of PrIM benchmarks, including workload characterdata transfers can be performed in parallel (i.e., concurrently istics (memory access pattern, computation pattern, commuacross multiple MRAM banks), if the size of the buffers nication/synchronization needs) that demonstrate the diversity transferred from/to all MRAM banks is the same. Otherwise, of the benchmarks. the data transfers happen serially. There is no support for direct communication between DPUs. All inter-DPU communication Our comprehensive analysis [1, 157] evaluates the perfortakes place through the host CPU by retrieving results and mance and scaling characteristics of PrIM benchmarks on the copying data. UPMEM PIM architecture, and compares their performance and energy consumption to their CPU and GPU counterparts. Rigorously understanding the UPMEM PIM architecture, Our extensive evaluation conducted on two real UPMEMthe ﬁrst publicly-available PIM architecture, and its suitability based PIM systems with 640 and 2,556 DPUs provides new to various workloads can provide valuable insights to proinsights about suitability of different workloads to the PIM grammers, users and architects of this architecture as well system, programming recommendations for software designas of future PIM systems. To this end, our work [1, 157] ers, and suggestions and hints for hardware and architecture provides the ﬁrst comprehensive analysis of the ﬁrst publiclydesigners of future PIM systems. available real-world PIM architecture. We make two key contributions. First, we conduct an experimental characterization In this paper, we provide four key takeaways that repre- Table I: PrIM benchmarks [156]. We present several key empirical observations in the form of four key takeaways that we distill from our experimental characterization of the UPMEM PIM architecture [1]. We also provide analyses of workload suitability and good programming practices for the UPMEM PIM architecture, and suggestions for hardware and architecture designers of future PIM systems. Key Takeaway #1. The UPMEM PIM architecture is fundamentally compute bound. Our microbenchmark-based analysis shows that workloads with more complex operations than integer addition fully utilize the instruction pipeline before they can potentially saturate the memory bandwidth. As Figure 2 shows, even workloads with as simple operations as integer addition saturate the compute throughput with an operational intensity as low as 0.25 operations/byte (1 addition per integer accessed). Figure 3: Throughput of arithmetic operations (ADD, SUB, MUL, DIV) on one DPU for four different data types: (a) INT32, (b) INT64, (c) FLOAT, (d) DOUBLE. Figure 4 shows the speedup of the UPMEM-based PIM systems with 640 and 2,556 DPUs and a state-of-the-art Titan V GPU over a state-of-the-art Intel Xeon CPU. We observe that benchmarks with little amount of computation and no use of multiplication, division, or ﬂoating point operations (10 out of 16 benchmarks) run faster (2.54× Figure 2: Arithmetic throughput versus operational intenon average) on a 2,556-DPU system than on a state-of-thesity for 32-bit integer addition. The number inside each art NVIDIA Titan V GPU. These observations show that dot indicates the number of tasklets. Both x- and y-axes the workloads most well-suited for the UPMEM PIM are log scale. architecture are those with no arithmetic operations or simple operations (e.g., bitwise operations and integer This key takeaway shows that the most suitable workloads addition/subtraction). Based on this key takeaway, we recomfor the UPMEM PIM architecture are memory-bound mend devising much more efﬁcient software library routines workloads. From a programmer’s perspective, the architecture or, more importantly, specialized and fast in-memory hardware requires a shift in how we think about computation and data for complex operations in future PIM architecture generations access, since the relative cost of computation vs. data access to improve the general-purpose performance of PIM systems. in the PIM system is very different from that in the dominant processor-centric architectures of today. KEY TAKEAWAY 2 KEY TAKEAWAY 1 The most well-suited workloads for the UPMEM The UPMEM PIM architecture is fundamentally PIM architecture use no arithmetic operations or compute bound. As a result, the most suitable use only simple operations (e.g., bitwise operations workloads are memory-bound. and integer addition/subtraction). Key Takeaway #3. The workloads most well-suited for the The 2,556-DPU system is faster (on average by 2.54×) than UPMEM PIM architecture are those with little global the state-of-the-art GPU in 10 out of 16 PrIM benchmarks, communication, because there is no direct communication which have three key characteristics that deﬁne a workload’s channel among DPUs (see Figure 1). As a result, there is a PIM suitability: (1) streaming memory accesses, (2) little or huge disparity in performance scalability of benchmarks that no inter-DPU communication, and (3) little or no use of do not require inter-DPU communication and benchmarks that multiplication, division, or ﬂoating point operations. do (especially if parallel transfers across MRAM banks cannot We expect that the 2,556-DPU system will provide even be used). This key takeaway shows that the workloads most higher performance and energy beneﬁts, and that future PIM well-suited for the UPMEM PIM architecture are those systems will be even better (especially after implementing our with little or no inter-DPU communication. Based on this recommendations for future PIM hardware [1]). If the architakeaway, we recommend that the hardware architecture and tecture is improved based on our recommendations under Key the software stack be enhanced with support for inter-DPU Takeaways 1-3 and in [1], we believe future PIM systems will communication (e.g., by leveraging new in-DRAM data copy be even more attractive, leading to much higher performance techniques [117, 121, 125, 126] and providing better connecand energy beneﬁts versus state-of-the-art CPUs and GPUs tivity inside DRAM [121, 125]). over potentially all workloads. KEY TAKEAWAY 4 KEY TAKEAWAY 3 The most well-suited workloads for the UPMEM the-art CPUs in terms of performance (by 23.2× PIM architecture require little or no communion 2,556 DPUs for 16 PrIM benchmarks) and energy cation across DRAM Processing Units (inter-DPU efﬁciency (by 5.23× on 640 DPUs for 12 PrIM communication). benchmarks). Summary. We ﬁnd that the workloads most suitable for the the-art GPUs on a majority of PrIM benchmarks UPMEM PIM architecture in its current form are (1) memory(by 2.54× on 2,556 DPUs for 10 PrIM benchmarks), bound workloads with (2) simple or no arithmetic operations and the outlook is even more positive for future PIM and (3) little or no inter-DPU communication. systems. Key Takeaway #4. We observe that the existing UPMEMbased PIM systems greatly improve energy efﬁciency and efﬁcient than state-of-the-art CPUs and GPUs on performance over state-of-the-art CPU and GPU systems workloads that they provide performance improveacross many workloads we examine. Figure 4 shows that the ments over the CPUs and the GPUs. 2,556-DPU and the 640-DPU systems are 23.2× and 10.1× faster, respectively, than a state-of-the-art Intel Xeon CPU, averaged across the entire set of 16 PrIM benchmarks. We III. SUMMARY & CONCLUSION also observe that the 640-DPU system is 1.64× more energy This invited short paper summarizes the ﬁrst comprehensive efﬁcient than the CPU, averaged across the entire set of 16 characterization and analysis of a real commercial PIM archiPrIM benchmarks, and 5.23× more energy efﬁcient for 12 of tecture [1, 157]. Through this analysis, we develop a rigorous, the PrIM benchmarks. thorough understanding of the UPMEM PIM architecture, the ﬁrst publicly-available PIM architecture, and its suitability to various types of workloads. First, we conduct a characterization of the UPMEM-based PIM system using microbenchmarks to assess various architecture limits such as compute throughput and memory bandwidth, yielding new insights. Second, we present PrIM, an open-source benchmark suite [156] of 16 memory-bound workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing). Figure 4: Performance comparison between the UPMEMOur extensive evaluation of PrIM benchmarks conducted based PIM systems with 640 and 2,556 DPUs, a Titan on two real systems with UPMEM memory modules proV GPU, and an Intel Xeon E3-1240 CPU. Results are vides new insights about suitability of different workloads normalized to the CPU performance (y-axis is log scale). to the PIM system, programming recommendations for softThere are two groups of benchmarks: (1) benchmarks that ware designers, and suggestions and hints for hardware and are more suitable to the UPMEM PIM architecture, and architecture designers of future PIM systems. We compare (2) benchmarks that are less suitable to the UPMEM PIM the performance and energy consumption of the UPMEMarchitecture. based PIM systems for PrIM benchmarks to those of a state-