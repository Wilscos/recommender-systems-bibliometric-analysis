Abstract—The commonly used latent space embedding techniques, such as Principal Component Analysis, Factor Analysis, and manifold learning techniques, are typically used for learning effective representations of homogeneous data. However, they do not readily extend to heterogeneous data that are a combination of numerical and categorical variables, e.g., arising from linked GPS and text data. In this paper, we are interested in learning probabilistic generative models from high-dimensional heterogeneous data in an unsupervised fashion. The learned generative model provides latent uniﬁed representations that capture the factors common to the multiple dimensions of the data, and thus enable fusing multimodal data for various machine learning tasks. Following a Bayesian approach, we propose a general framework that combines disparate data types through the natural parameterization of the exponential family of distributions. To scale the model inference to millions of instances with thousands of features, we use the LaplaceBernstein approximation for posterior computations involving nonlinear link functions. The proposed algorithm is presented in detail for the commonly encountered heterogeneous datasets with real-valued (Gaussian) and categorical (multinomial) features. Experiments on two high-dimensional and heterogeneous datasets (NYC Taxi and MovieLens-10M) demonstrate the scalability and competitive performance of the proposed algorithm on different machine learning tasks such as anomaly detection, data imputation, and recommender systems. Index Terms—heterogeneous data integration, latent variable models, variational inference, factor analysis, exponential family of distributions Finding lower-dimensional latent space representations of high-dimensional datasets is an important unsupervised learning problem for several objectives such as dimensionality reduction, visualization, exploratory data analysis, and data fusion. PCA is the most commonly used latent space embedding technique. It ﬁnds a succinct representation of the data points in terms of a smaller number of low-dimensional features obtained by linearly mixing the original features in such a way as to maximize variance. Such linear mixing coefﬁcients are given by the eigenvectors of the covariance matrix of the feature variables that correspond to the K largest eigenvalues, where K is the desired number of new features (K ≤ P ). For heterogeneous data, e.g., consisting of a numerical and a non-numerical variable, the standard sample covariance, , and Alfred O. Hero called the Pearson covariance, is not directly applicable. For ordinal data Pearson introduced a modiﬁed covariance, called the polychoric correlation, that estimates the association between several ordinal variables by modeling them as quantized bivariate Gaussian random variables [1]. On the other hand, for mixed continuous and ordinal data, the polyserial correlation estimates association between the variables where, again, the ordinal data is modeled as quantized Gaussian [2]. PCA based on polyserial and polychoric correlations is often used for dimensionality reduction in heterogeneous datasets [3], [4]. However, both polyserial and polychoric correlations are designed for categorical variables that are ordinal, i.e., their values are linearly ordered [2]. Following a probabilistic approach PCA can also be generalized to exponential family for homogeneous datasets [5], [6]. Factor analysis (FA) is another well known latent space embedding technique, which includes PCA as a special case [7]. The introduction of factor analysis is often attributed to Charles Spearman’s work in 1904 [8], yet its roots can be traced to the earlier works of Francis Galton [9]. Factor analysis is a latent variable model that decomposes a data matrix into low dimensional explanatory variables, called factors. Similar to PCA, factor analysis does not readily extend to heterogeneous data [10, Ch. 5]. In [11], a mixture of factor analyzers is presented for heterogeneous data consisting of continuous and categorical variables. Similar to classical factor analysis, in [11], instances (rows of the data matrix) are modeled independently with latent factor loading coefﬁcients, whereas the features (columns of the data matrix) of an instance are linear combinations of factor loadings where the weights are called factor scores. Probabilistic Canonical Correlation Analysis (PCCA) [12] similarly models a pair of Gaussian feature vectors using a weighted sum of latent factors, called canonical components. Independent Component Analysis (ICA) and its extension Independent Vector Analysis (IVA), which is also a generalization of Canonical Correlation Analysis (CCA), are also used for multimodal data fusion [13], [14]. Regarding multimodal data, similar to factor analysis and PCA, manifold learning methods, such as Laplacian eigenmaps [15] and Isomap [16], also require homogeneity among data points. Typically, such manifold learning methods require computing an afﬁnity matrix, but it is not clear how to deﬁne a uniﬁed similarity or distance metric for disparate features (e.g., numerical and categorical). There is also a large literature on multi-view learning (e.g., [17], [18]) which aims at performing speciﬁc machine learning tasks using heterogeneous datasets. Some of these methods, such as parallel ICA, e.g., [19], [20], have been applied to categorical data. However, in a heterogeneous data setting, these existing techniques [12]–[18] treat the categorical data in the same way as continuous-valued data, which is a strictly suboptimal approach. There are also methods which address speciﬁc heterogeneous data applications using latent factor models, e.g., [21], [22]. In [23], a generic method based on generalized linear models is proposed to build graphical models from the exponential family of distributions. However, a joint analysis of the aggregated exponential family is not discussed in [23]. While the mixtures of factor analyzers methods proposed in [24], [25] jointly analyze heterogeneous Gaussian data, they do not allow for heterogeneous data types, such as numerical and categorical, as the proposed mixture is based on classical factor analyzers. This paper develops a general approach to joint factor analysis for heterogeneous data, called multimodal factor analysis (MMFA). MMFA, originally introduced in [26], is a Bayesian approach that models different types of data using latent factor loadings speciﬁc to each data type and latent factor scores that are common to the data types. It was applied to event detection in Twitter [27] in order to fuse categorical and spherical data that are modeled by multinomial and von Mises-Fisher distributions, respectively. In this paper, we present MMFA as a comprehensive unsupervised learning tool for learning generative models in highdimensional and heterogeneous datasets explainable by exponential family of distributions. Speciﬁcally, our contributions can be summarized as follows. proposed generalized MMFA model provides a tractable uniﬁed framework for jointly analyzing heterogeneous features from the exponential family of distributions through linking their natural parameters with a common factor score vector. MMFA ﬁnds Gaussian approximations to the posterior distribution of latent factor loadings for the data types whose natural parameters require nonlinear link functions, e.g., multinomial distribution (a.k.a. LaplaceBernstein approximation). many heterogeneous features and instances, as demonstrated by the experimental results, thanks to the LaplaceBernstein approximations. The problem formulation is given in Section II, and the proposed generalized MMFA model is introduced in Section III. In Section IV, a variational learning algorithm for the proposed model is presented. In Section V, the MMFA algorithm is illustrated for a heterogeneous dataset consisting of Gaussian and multinomial components, and an analysis of computational complexity and mean-squared-error (MSE) performance is presented. We also demonstrate the usage of MMFA in unsupervised learning problems using real datasets (Section VI). Finally, the paper is concluded in Section VII. Consider a heterogenous random data structure composed of M different data types, called modalities, from the same source. Observed are P realizations of this data structure, called instances. Assume that the data from each modality can be modeled with a probability distribution from the exponential dispersion family (e.g., Gaussian, Poisson, multinomial), which is a generalization of the exponential family [28]. For each modality m and each instance i, if the modality corresponds to a continuous random variable of dimension Dthen it can be represented as a data vector x= [x. . . x] with probability density function (pdf) given by if xis discrete-valued, its probability mass function (pmf) is given by In (1) and (2), for modality m, ηis a vector of natural parameters, s(x) is a vector of sufﬁcient statistics, a(η) is the log-partition (i.e., log-normalization) function, τ≥ 0 is the dispersion parameter, (·)denotes the transpose, and i = 1, . . . , P, m = 1, . . . , M. Note that the dimension DP of each modality can be different, and D =Dgives the total number of dimensions, i.e., features, in the dataset. Table I gives examples of some popular probability distributions from the exponential family in terms of the representations in (1) or (2). The ﬁrst three and the last three rows of Table I refer to continuous cases and discrete cases, i.e., (1) and (2), respectively. Given a data structure of M modalities, D dimensions, and P instances, the MMFA model summarizes the data with a small number of K latent factors where K  min{D, P }. This generative model is illustrated in Fig. 1 where the latent factors {e} and the P instances are shown. Fig. 1 is a Markov graph in the sense that, conditioned on the e’s, the instances are independent and modalities are distributed according to different exponential family distributions of the form (1) and (2). Each instance i is assumed to follow an exponential model of the form (1) or (2) with a natural parameter vector η= [η· · · η]∈ Rcomposed of an instance-wide latent matrix E and an instance-speciﬁc score vector c∈ R: where E = [e· · · e], e∈ Rare latent vectors,M is the total number of natural parameters used to model the multimodal data. The latent vectors {e, . . . , e} in Fig. 1 are deﬁned as the rows of the matrix E. In analogy with other factor analysis methods, the latent vectors {e} are called factor loading vectors and {c} are called factor score vectors. In the proposed model, each instance i is characterized by the score vector c. The same cis used to linearly model each natural parameter ηassociated with instance i for all of the M data modalities, e.g., (3)-(5). For example, if the ﬁrst three modalities are Gaussian, Poisson, and binomial, respectively, then from Table I, the natural parameter ηis composed of three elements: The Gaussian case in (3) corresponds to the classical factor analysis model. Factor analysis, in its original form, is used to model the mean of continuous data through a linear combination of continuous latent variables [10], as in the Gaussian case given by (3). The classical factor analysis model does not provide a good ﬁt for discrete data (e.g., Poisson data) or categorical data (e.g., binomial data) [29]. There are several latent variable models that extend factor analysis to non-Gaussian data, such as the general linear latent variable model [29], Poisson factor analysis [30], and latent Dirichlet allocation [31]. Different than those works, here we provide a joint model to deal with different data modalities together. In our proposed model, instead of the mean – e.g., λ for Poisson and p for binomial (see Table I) – we linearly model the natural parameter – e.g., log λ for Poisson (4) and logfor binomial (5) – which provides a general framework for the exponential family of distributions. This mapping is similar to the general linear latent variable model [29] and the generalized mixture of factor analyzers model [11], albeit with some important differences. Firstly, we present a generic model for the joint analysis of exponential family where either the factor loadings {e} or the factor scores {c} can be modeled as latent variables, whereas they are strictly modeled as parameters and latent variables, respectively, in the existing factor analysis models including [29] and [11]. The proposed model can bring about signiﬁcant tractability for high-dimensional heterogeneous datasets since computing the posterior of cinvolves all data modalities, whereas computing the posterior of eonly requires data from modality m. We also provide a general scalable framework based on the Laplace-Bernstein approximation for ﬁtting the proposed model to high-dimensional and heterogeneous datasets. The proposed model is indeed a nonparametric model. Although a parametric model is used for each data vector xof instance i and modality m (see (1), (2)), we have a nonparametric model for the entire dataset {x} representing the collection of instances. This is because the number of parameters {c} linearly increases with the number of instances, and thus is asymptotically inﬁnite. We use the expectation-maximization (EM) approach to compute, in an iterative manner, the maximum likelihood estimates of the model parameters θ, which includes the score vectors {c}, the hyperparameters of the prior distribution for {e}, and the dispersion parameters {τ}. The number of factors K can be selected in different ways, including Bayesian information criterion (BIC), Monte Carlo integration over a uniform on K, birth-death models, or evaluation of a knee in the scree plot of goodness of ﬁt, e.g., as measured by the negative log-likelihood. In the original EM algorithm [33] for modality m, at each iteration n, in the expectation step (E-step), the expectation of the complete-data log-likelihood is computed with respect to the posterior distribution ge|{x}, θof each latent vector e, i.e., Q(θ) = Elog f{x}, e|θ. Then, in the maximization step (M-step), the parameters θare estimated by maximizing Q(θ) over the parameter space Θ, i.e., θ= arg maxQ(θ). With the EM approach, the biggest challenge is to compute the posterior distribution in the E-step at each iteration n since the link function relating the natural parameters to latent vectors is in general nonlinear, and the posterior is multivariate. Markov chain Monte Carlo (MCMC) methods, such as the Gibbs sampler, can be used to sample from the posterior g(e|{x}, θ). We might then use these samples to estimate Q(θ) for a given θ, and search over the parameter space to ﬁnd the θthat maximizes Q(θ). Although the MCMC approach can enable the use of exact EM algorithm for inferring the model parameters, highdimensional parameter space with thousands of features (large D) and millions of instances (large P ) could be problematic for the convergence rate and computational complexity. We propose a variational EM approach to address this problem. In the variational EM approach, a tractable probability distributiong(e|{x}, θ) is used to approximate the posterior. The objective is to select, from a tractable family of distributions, the distribution which is closest to the actual posterior in the KL-divergence sense, i.e.,g= arg minKL(q||g). The design challenge here is to determine the tractable family of distributions such thatg will be close to g. To this end we utilize the Bernsteinvon Mises theorem, which states that the posterior is asymptotically, as the number of instances P increases, well approximated by a Gaussian distribution when the likelihood model is from exponential family and the prior is Lipschitz continuous, e.g., Gaussian, von Mises-Fisher, etc. [35], [36]. In the problem of interest with exponential family models, Gaussian priors, and large number of instance-feature interactions (e.g., P × D is on the order of millions), the Bernstein-von Mises theorem provides theoretical motivation for using Gaussian approximation to the posterior. Note that P × D gives the number of observations for ﬁnding the posterior of the Kdimensional latent vectors. Hence, following a variational EM approach, we approximate gwith a Gaussiang. The Laplace technique approximates the posterior with the Gaussian N (`, −H(`)), where ` is the mode of the posterior, and H(`) is the Hessian matrix (i.e., second-order derivative of gwith respect to e) evaluated at the mode `. The posterior mode typically does not have a closed form due to the nonlinear link function, and thus calls for iterative computation through a numerical optimization technique. The computation of the posterior mode and the Hessian matrix at each EM iteration, with a high-dimensional dataset, may be prohibitive. Similarly, higher-order variational inference techniques such as expectation propagation (EP) may incur signiﬁcant computational complexity in high-dimensional datasets (large P, large D). Through moment matching EP will need to iteratively compute the mean vector and covariance matrix of the actual posterior g. Hence, for scalability to large datasets, we resort to variational – in particular, quadratic – lower bounds, which signiﬁcantly reduces the computational complexity compared to the Laplace approximation and EP by ﬁxing the covariance matrix. Moreover, as noted in [7, p. 498], the variational lower bound method has additional ﬂexibility, which leads to improved accuracy, compared to the Laplace method. This motivates us to approximate the log-partition function a(η), which is the problematic term in the complete-data log- likelihood log f{x}, e|θwith a quadratic term to obtain the Gaussian approximationg. For instance, we approximate log(−η) and ein the exponential and Poisson likelihoods (see Table I) using the secondorder Taylor series expansion around 1 and 0, respectively. Speciﬁcally, we obtain an evidence lower bound (ELBO) by using min{−1, −x} and max{1, x} for the second-order derivative term, where xapproximates 1/λ and λ in the exponential and Poisson case, respectively. In the next section, we will explain this procedure in detail for the multinomial likelihood, which is commonly used in the real-world datasets for categorical features. A Gaussian prior π(e) is assumed for each modality to facilitate the Gaussian approximation for the posterior. Note that no approximation to the posterior is needed for the Gaussian likelihood since the posterior is already Gaussian with a conjugate prior. For the von MisesFisher distribution, which is an extension of Gaussian distribution to spherical data, we use a von Mises-Fisher prior, which is conjugate to the likelihood. However, in this case, an approximation is still needed due to the constraint that the mean vector is a unit-length vector (see [27] for details). In the proposed variational EM algorithm, for all modalities EM steps are run in parallel, which is followed by the Mstep for the score vectors {c}, as shown in Fig. 2. Since c is common to all modalities of instance i, it is updated by receiving related information from all EM steps for different modalities. Each ccan be updated in parallel. To illustrate the proposed model and variational EM algorithm, in this section, we consider a bimodal dataset X = [Y Z] from the same source consisting of a realvalued matrix Y ∈ Rand a categorical data matrix Z ∈ Zwith P , D, and Ddenoting the number of instances, number of real-valued features, and the number of categories, respectively. Zdenotes the set of nonnegative integers. Assume the entries yof Y and the rows z of Z are well modeled using the Gaussian N (µ, σ) and the multinomial M(N, p) models (where Nis the number of experiments and p∈ [0, 1]is the probability vector), respectively. As in (3) and (5), assuming K generative factors that are characterized by the latent vectors {e: e∈ R, k = 1, . . . , K}, E = [U V ] = [u· · · uv· · · v] = [e· · · e], we linearly model the natural parameters of the Gaussian and the multinomial distributions, i.e., The coefﬁcient vector crepresents each instance i in terms of the K latent factors. In the multinomial distribution, the last category is selected as pivot since one of the probabilities is fully determined by the other D− 1 probabilities, i.e., the degree of freedom is one less than the number of categories (cf. Table I). Note that multinomial distribution covers as a special case categorical distribution (N= 1, ∀i), binomial distribution (D= 2), and Bernoulli distribution (D= 2, N= 1, ∀i). The two modalities considered in this section are the most common data types found in real-world datasets [39]. 1) Gaussian Parameters: For the Gaussian model, assuming the conjugate prior N (0, I) for u, j = 1, . . . , D, where 0and Idenote the K-dimensional zero vector and identity matrix, we have the exact EM algorithm, i.e., at each iteration n E-step: compute the posterior M-step: estimate the parametershi where θ= {σ}. From the classical factor analysis [10], [38], we have N (a, B) as the posterior of uat iteration n, where and the parameter update Assuming an inverse-gamma distribution InvGam(α, β), which is the conjugate prior for the Gaussian variance, for σit is straightforward to show that the update becomes σ=2(α + 1) + 1. Combining the inputs from all features (Gaussian and multinomial) the coefﬁcient vector cof instance i is updated where the input from the multinomial features ζ(c) will be derived next. 2) Multinomial Parameters: In the multinomial likelihood, the dependency of the sum-of-exponentials (sum-exp) term in the denominator on all latent vectors {v} complicates the analysis considerably. First of all, we need to consider {v} together and obtain the posterior distribution of the combined latent vector v = [v· · · v], which is K(D− 1)dimensional. More importantly, ﬁnding the exact posterior is not tractable due to the presence of {v} in the sum-exp function. We rewrite the above likelihood expression in a more compact form where lse(η) = log1 +Pevcis the log-sum-exp function, η= [vc· · · vc], C= I⊗ cwith ⊗ being the Kronecker product, and z= [z· · · z]. To obtain an approximate posteriorg(v|{z, c, N}), as outlined in Section IV-B, we derive a lower bound for the likelihood, and accordingly for the complete-data loglikelihood, through the second-order Taylor series expansion of lse(η) around a ﬁxed point ψ, lse(η) = lse(ψ) + (η− ψ)∇lse(ψ) ≤ lse(ψ) + (η− ψ)∇lse(ψ) where  ∈ [0, 1], and A =I−11from [40]. Note that we deﬁned a new variable ψfor each instance i. We will show how to update it in Proposition 1. The gradient is given by the probability vector induced by ψ, i.e., ∇lse(ψ) = ψ= p. Replacing the lse function with this quadratic upper bound we obtain the following lower bound for the likelihood wherez= z− Npψ− Aψ, and const. denotes the constant terms with respect to v. Assuming standard multivariate Gaussian N (0, I) prior for v the lower bound for the complete-data likelihood is given by f{z}, v|c, N≥ eΩ, (11) where Ω = (PNCAC+ I)and ω =P ΩCz. From that lower bound we obtain an approximate posteriorg(v|{z, c, N}) = N (ω, Ω). In the Mstep, for computational efﬁciency, following the approach in [37] we update the parameters by maximizing the expected lower bound Elogf{z}, v|c, Ninstead of max- imizing Elog f{z}, v|c, N. It is shown in [37] that maximizing the former is asypmtotically equivalent to, and computationally more efﬁcient than maximizing the latter. Using this approximate posterior the M-step (i.e., parameter updates) of the variational EM algorithm can be derived as in the Gaussian case; however, the computational complexity might be prohibitive due to high dimensionality, e.g., Ω requires inverting a (D−1)K ×(D−1)K matrix. Typically, the number of factors K gets small values, whereas the number of categories may be large, e.g., a dictionary of words in topic modeling [31]. We next present a result that shows, indeed, the underlying dimensionality is K × K. Proposition 1. At iteration n, the multinomial parameters ψ and the coefﬁcient vector ccan be updated as follows using the matrices where Band aare given by (6), the K × (D− 1) matrix Φ = [φ· · · φ] is a reorganized form of the approximate posterior mean ω = [φ· · · φ], N = diag(N, . . . , N), andZ = [z· · ·z],z= z− Proof: See Appendix. Although not directly used in the learning algorithm, the complete form of the approximate posterior covariance (cf. (11)) is given by Ω= I⊗ F+ 11⊗ ∆. 3) Coefﬁcient Vector: As shown in (13), we have a quadratic programming problem for the coefﬁcient vector c, which is simply solved as unless there is a constraint on c, such as c≥ 0, ∀i, k. If a constraint is added to the problem, a standard solver can be used, e.g., interior point methods. Additionally, depending on the application a convenient regularization, such as L-norm (Lasso) and L-norm (ridge regression), can be used to solve (15). 4) Algorithm: The resulting variational EM algorithm is summarized in Algorithm 1. Note that the EM steps for Gaussian (lines 4 and 5) and multinomial (lines 6 and 7) can be run in parallel, as shown in Fig. 2. Moreover, the coefﬁcient vectors {c} can be updated in parallel (line 8). In the following theorem, we show that the computational complexity of Algorithm 1 scales linearly with each dimension of the problem (i.e., number of instances, real-valued features, and categorical features). As a result, the proposed algorithm Algorithm 1 The proposed EM algorithm for the Gaussianmultinomial example 1, . . . , D, (6) in (14) can be efﬁciently used for large datasets, as demonstrated in Section VI. Theorem 1. At each iteration of the proposed EM algorithm, given by Algorithm 1, the computational complexity linearly scales with the number of instances P , the number of realvalued features D, and the number of categories D. Speciﬁcally, the complexity is given by O(KP +KP D+KP D), where K is the number of factors. Proof: See Appendix. Note that the number of factors is not an input from data, but a design parameter typically chosen to be a small number compared to the number of instances, K  P . Furthermore, even in mildly complex datasets, it is also much smaller than the total number of features, K  D+ D. Hence, in fact, K can be dropped from the asymptotic complexity notation, which yields the following result. Corollary 1. Algorithm 1 scales with the data size as O(P D), where P is the number of instances and D = D+ Dis the total number of features. In this section, assuming the Gaussian and multinomial generative models are consistent with the observations (i.e., there is no model mismatch), we numerically compare the mean squared error (MSE), E[kc− ck], of Algorithm 1 with the Cram´er-Rao lower bound (CRLB). With u∼ N (µ, Σ), the distribution of the Gaussian observations is y∼ N (cµ, cΣc+ σ). It is straightforward to show that the Fisher information matrix of the Gaussian model is given by [41, p. 47] where the ﬁrst and second terms are the contributions from the mean and variance, respectively. On the other hand, in the multinomial model, due to the sum-exp term in the denominator of the likelihood, the Fisher information matrix is not tractable. Thus, we resort to numerical computation via Monte Carlo simulations as described next. Proposition 2. The Fisher information matrix F(c) for the multinomial model is given by where p(z|{v}, c) =∂∂cp(z|{v}, c) and p=cvcvis the probability of category j for instance i. Proof: The nontrivial part in the proof is justifying changing the order of expectation and differentiation. From the deﬁnition of Fisher information we have where the differentiation can be brought into the expectation due to the Dominated Convergence Theorem [42, p. 53] since expectation and differentiation are both limits, and p(z|{v}, c) is a probability dominated by 1. The derivative p(z|{v}, c) directly follows from the likelihood The Fisher information expression given in (16) can be efﬁciently computed through Monte Carlo simulations, as shown in Algorithm 2. In Algorithm 2, a simpliﬁed notation is used by dropping some indices: c is the coefﬁcient vector to be estimated, N is the number of multinomial experiments, D is the number of categories, R is the number of realizations to be averaged over, and K is the number of factors. The overall Fisher information of the multimodal model and CRLB are given by Algorithm 2 Monte Carlo simulations for multinomial Fisher information columns {v∼ N (0, I)} M(z|N, p) We next present simulation results for the MSE performance. In the simulated data, the number of Gaussian features and the number of multinomial categories are D= D= 5, the number of multinomial experiments is N= 40, and the number of instances is P = 100. In the MMFA algorithm, the number of factors is K = 3, ridge regression is used for updating c(see (15)) with weight 10, and the hyperparameters α = 1 and β = 0.1 are used for the inversegamma prior of σ(see (7)). The statistical expectation in the multinomial Fisher information is computed by averaging over R = 2000 realizations (see Algorithm 2). Fig. 3 shows that MSE of the proposed MMFA algorithm converges close to the CRLB (red dashed line) as early as in 20 iterations. The CRLB for Gaussian data, trace(F(c)), and multinomial data, trace(F(c)), are also shown in the same ﬁgure with black dotted line and purple dashed line, respectively. Using the same simulation setup we show in Fig. 4 that under the MMFA model the likelihood of both the training (P = 100) and the unseen test data (P = 10) increase with the iterations and appear to converge to a limit. On the vertical axis, the likelihood of the multimodal data under the estimated model normalized by the likelihood under the true model is shown, i.e.,. In this section, we will demonstrate the power of MMFA in large datasets for different tasks, such as generalization to unseen data, anomaly detection, data imputation, and recommender systems. We start with the New York City (NYC) Taxi dataset [43], and conclude with the MovieLens dataset. The codes used to produce the results in this paper are publicly available. This dataset provides trip records for the yellow and green taxicabs, and the for-hire vehicles in NYC. Here we use the data from yellow taxis from February 2019 [43]. The dataset includes, for each recorded trip, the pick-up and drop-off dates, times and locations, trip distances, itemized fares, rate types, payment types, tip amounts, and passenger counts. The considered dataset has almost 7 million trip records. We ﬁrst extracted a subset of the variables in the dataset, and ﬁltered them to reduce bias. Speciﬁcally, we only considered trips with credit card payments since in most of the trips with cash payment the tip amount is unrealistically recorded as zero. We also disregarded trips that report fewer than 1 or more than 6 passengers (which is the legal limit). Location is reported in terms of the taxi zone id from 1 to 263. Unknown locations, denoted by the id 264 or 265, are ignored. Finally, we removed trips reporting a trip distance smaller than 0.1 mile or greater than 40 miles, and fare amounts less than $1 and more than $200. After preprocessing, the data size decreased to around P = 6.4 × 10. The considered features consist of numerical ones, namely the tip amount, fare amount, number of passengers, and trip distance, which are modeled using a four-dimensional Gaussian distribution (D= 4), and categorical ones, namely the pick-up day (D= 7 choices), pick-up time (D= 24 choices), and location (D= 263 choices). Categorical distribution with one-of-K (a.k.a. onehot) representation yields a total number of 298 features. For all categorical features, the number of trials is N= 1, ∀i. Note that the numerical and categorical features arise from the same physical event (i.e., taxi trip), and hence they are dependent in general. For instance, it is seen that the tip amount, fare amount, and the number of passengers statistically depend on the pick-up and drop-off locations, e.g., trips from and to Manhattan statistically have higher tip percentages. Two sample trips are shown on the NYC map in Fig. 5. Generalization performance: For generative models a common performance metric, the predictive log-likelihood value, is often used to assess the ability of the model to generalize to unseen data in training [31]. The predictive likelihood value is computed for an unseen data instance as follows. The trained MMFA model produces posterior distributions from estimated model parameters obtained using successive E and M steps {(M-step)}shown in Fig. 2. The ﬁnal M-step, (M-step) in Fig. 2, uses these parameters to compute the factor score vector cassociated with an unseen data instance x, which speciﬁes the likelihood function through the generative model shown in Fig. 1. We compare the proposed MMFA algorithm with Kernel Canonical Correlation Analysis (KCCA) [56], Parallel Independent Component Analysis (PICA) [19], [20], and the standard factor analysis (FA) method [38] on the NYC Taxi dataset. As compared to standard CCA, KCCA algorithm enables fusion of data with multiple modalities by choosing proper kernels. Here, we use a linear kernel for the numerical modalities to form the symmetric Gram matrix. For the categorical data, we compute the Gram matrix by using the Hamming distances between observations, which is a proper similarity measure for the binary-coded categorical variables. Canonical components are then computed by projecting the Gram matrices to a lower dimensional latent space through demixing matrices for each modality. The KCCA objective maximizes the correlation of each of these canonical components across the modalities. This is usually performed by solving a generalized eigenvalue problem. On the other hand, PICA assumes non-Gaussian independent components for each modality. Here, we use a non-quadratic exponential decay function for the log of the probability distribution functions of the components as suggested in [57]. The modalities are correlated through the mixing matrices instead of the latent components. To implement the cross-modality optimization in PICA, a term is added to the PICA objective that encourages maximization of the correlation between the components of the matrices. Centering and whitening are applied in advance as a preprocessing step. Conversely, FA assumes spherical Gaussian components for the latent variables, i.e., factor loading coefﬁcients. It is a generalization of PCA, where the noise covariance matrix is diagonal and has D free parameters. Also, the orthonormality constraint on the factor loading matrix is relaxed in FA models. The model is usually ﬁt by using the EM algorithm, where the E-step computes posterior distributions of the latent variables, and the M-step computes point estimate for the factor loading matrix. We adopted the FA model as a representative of the class of fusion algorithms that treat all data modalities as numerical by concatenating them to form a long feature vector in a linear model. MMFA achieves fusion by generating common latent factor scores to generate both modalities under a Bayesian graphical model. Under this model, the data modalities are conditionally independent given the factor loading coefﬁcients, allowing modality-speciﬁc probabilistic models to be fused together. As generalized linear models for a large variety of data types and distributions are available (Table I), the MMFA can be applied to a wide range of data types beyond the Gaussian/multinomial case considered here. In contrast, Kernel CCA fuses data types by performing an eigendecomposition on a distance matrix or, equivalently, a similarity matrix. Instead of incorporating explicit probability distribution models for different data types, as in MMFA, KCCA accounts for different data types through transformation of the similarity matrix via kernelization, where the type of kernel is selected to match the data type. As KCCA performs an eigendecomposition of a data similarity matrix its computational complexity is of order O(P) as compared to only order O(P D) for FA and MMFA. On the other hand, PICA jointly models the modalities by maximizing the correlation between the components of the mixing matrices. If we denote dimension of the two modalities Dand D, there are D× Dpossible pairs to compute the correlation. Indeed, the algorithm chooses single pair at each iteration, which has the maximum correlation as compared to the other pairs. Hence, it is not straightforward how to extend this model to more than two modalities in an efﬁcient way. Also, note that the categorical data is still modeled as numerical data. We compute the log predictive marginal likelihood on the test set to compare the models. To this end, the latent variables are integrated out. Particularly, for MMFA, the Gaussian parameters uand multinomial parameters vare integrated out. For FA, PICA and KCCA, the latent variables are assigned per data point as opposed to MMFA, which are integrated out in a closed form. Note that KCCA likelihoods are computed on the higher dimensional kernel space instead of the observation space. A train-validate-test split with (0.6, 0.2, 0.2) ratio is applied to the data. Using BIC, the best number of latent dimensions is found to be 10 for MMFA (K = 10), 6 for FA, 8 for KCCA. The BIC value k log(P ) − 2 log(p(X)) penalizes the negative log-likelihood score with the number of model parameters k, hence the K value with the smallest BIC value is selected for each algorithm. Figure 6 shows, for MMFA, FA, and KCCA, the mean and 95% conﬁdence interval of the log predictive marginal likelihood values. The random train-validate-test split was repeated 20 times to compute the mean and conﬁdence interval of the log predictive likelihoods. The mean values for MMFA, KCCA, PICA, and FA are 5982, -9471, -10833, -14429, respectively. By modeling the categorical data appropriately and fusing it with numerical data with probabilistic models, MMFA achieves much better generalization performance compared to other data fusion techniques that treat categorical data the same way as they do with numerical data. Anomaly detection: We next demonstrate the anomaly detection performance of MMFA on the NYC taxi data. We ﬁrst ﬁt the MMFA model on the training set, and then sort the likelihoods of instances in the validation set with respect to the trained model. Finally, in the test, we compare the likelihood of each instance with the likelihoods of validation set, and declare anomalous if it is smaller than the (1 − δ)% of the validation likelihoods, where δ is a small number representing the statistical signiﬁcance level. For δ = 0.05, the top ﬁve anomalies with the smallest likelihoods are shown in Table II. The ﬁrst three anomalies are obvious as the tip/fare ratio is unexpectedly high or low. However, the last two anomalies in the table can be considered as interesting ﬁndings of the MMFA model. The reason why they appear among the top anomalies is not only the tip/fare ratio, but in fact the inconsistency between the location and the tip percentage, 4.5% and 7.9%, respectively. Their locations are both in Manhattan with tip percentage mean and standard deviation of (27.3%, 10.1%) for location 237 and (25.2%, 10.4%) for location 79. While these trips are detected by MMFA as anomalous with tip percentages 4.5% and 7.9%, there are other trips with smaller tip percentages that are deemed nominal in locations with smaller mean tip percentages, e.g., 5.1% in location 132 (Queens) where mean and standard deviation is (19%, 10.7%). Since there is no ground truth (i.e., nominal and anomalous labels) in the dataset, such comparative evaluation is useful in showing MMFA’s success in anomaly detection. Note also that MMFA is a completely unsupervised algorithm. Our next application is recommender systems, in which the objective is to learn user patterns and provide successful item recommendations to the users. While the commonly used collaborative ﬁltering techniques in recommender systems typically use only the interactions between the users and items to learn the user patterns, there are also hybrid methods that combine user-item interactions with side information, such as user demographics and item features, for better recommendation performance [47]. However, the existing hybrid methods mainly convert the categorical side information, such as gender, occupation, item genre, country, etc., to numerical data for fusing multimodal data. The MovieLens dataset, which is commonly used as a benchmark dataset in recommender system applications, has three different versions, 100K, 1M, and 10M, in terms of the number of interactions between the users and the movies, i.e., user ratings for items. To show the scalability of the proposed MMFA algorithm, we use the MovieLens-10M dataset, which has a little more than 10M ratings from 71567 users to 10681 items. The size and sparsity of this dataset, where 98.7% of 71567 × 10681 possible interactions is missing, brings signiﬁcant challenges. In addition to the ratings, two item side information, release date and genre, are available in the dataset. We model release date using a univariate Gaussian distribution, and each of 21 genre categories using a Bernoulli (i.e., binary categorical) distribution since an item can have multiple genres. We train the MMFA model on heterogeneous data from P = 10681 items, consisting of numerical ratings from D= 71567 users, release date (D= 1), and categorical genre information (D= 2, j = 1, . . . , 21 with N= 1 trial ∀i, j), to ﬁnd the latent vector cfor each item i. The number of factors K is chosen as 10 using BIC. In latent variable models for collaborative ﬁltering, such as probabilistic matrix factorization (PMF), the rating rof item i from user j is commonly modeled using a Gaussian distribution N (cu, σ), where uis a latent factor score vector representing user j, and σis the variance parameter. Following the recommender systems literature we consider two experiment setups called warm start and cold start. In warm start, each user or each item has at least one rating in the training set, whereas in cold start, for some users or items the recommender system has to completely rely on side information as there was no related rating in training. For warm start, 60% − 20% − 20% training-validation-test split is used for the ratings of each item. On the other hand, for cold start, all the ratings of 20% of the items are used in the test set, and similarly 20% in the validation set. Data imputation: We ﬁrst evaluate MMFA for predictingP ratings in terms of MSE,(r− ca), where ais the posterior mean of u(see (6)), O is the set of observed ratings in the test and |O| is its cardinality. MMFA is compared with several collaborative ﬁltering algorithms, namely SVDpp, NMF, PMF, and BPMF. Speciﬁcally, the SVDpp method [49] performs a matrix factorization on the rating matrix including implicit ratings to ﬁnd the user and item matrices. The nonnegative matrix factorization (NMF) algorithm, similar to the singular value decomposition (SVD), computes a matrix factorization, but by enforcing both user and item matrices to be nonnegative [50]. PMF also applies a matrix factorization on the rating matrix by assuming Gaussian latent variables for both users and items [48]. In Bayesian PMF (BPMF), additional prior distributions are assumed for the hyperparameters of user and item latent variables [51]. All of the four benchmark models use only the rating matrix without any side information. Leveraging item side information MMFA has a clear advantage over them in the cold-start setting, hence we only compare the algorithms in the warmstart setting by averaging over 10 experiments with random training-validation-test split. As shown in Table III, MMFA achieves the best MSE performance also in the warm-start setting by utilizing the item side information available in the dataset. Recommendation accuracy: Finally, we evaluate MMFA’s performance in terms of the accuracy of recommended movies to the users. For movie datasets, recall (i.e., true positive rate) is computed as the ratio “number of movies user liked in the recommendations / total number of movies user liked”. In Table IV, for 10 recommendations, we report the recall averaged over all test users and 10 different experiments with random splits in both warm- and cold-start settings. Here we compare the proposed MMFA approach with state-of-theart recommender systems that are capable of utilizing side information. Among the considered state-of-the-art methods, LCE [52], DecRec [53], and KMF [54] could not scale well to the MovieLens-10M dataset due to the memory limitations. These algorithms store similarity matrices for users and items based on the side information, causing O(P+D) space complexity, where P and D are the number users and items. Moreover, the KMF algorithm inverts such matrices, increasing its space complexity to O(P+ D). In the MovieLens-10M dataset, only items have side information, and D = 10681. On the other hand, LightFM [55], which incorporates the side information into the rating matrix and performs matrix factorization on the enhanced data in a non-probabilistic way, scales well to the MovieLens-10M data. The superior performance of MMFA can be attributed to its natural handling of data fusion through appropriate probabilistic models while LightFM embeds categorical features into numerical values for data fusion. A general unsupervised Bayesian framework based on the exponential family was proposed for the joint analysis of heterogeneous datasets. The proposed model, called Multimodal Factor Analysis (MMFA), uses the most appropriate probability distribution from the exponential family for each data modality, and fuses them by modeling their natural parameters through a common latent vector for each instance. To ﬁt the model on large high-dimensional datasets, we proposed a computationally efﬁcient variational ExpectationMaximization (EM) algorithm, which scales linearly with the number of features and the number of instances. On the common real-valued and categorical data combination, we showed that the algorithm quickly converges to the CramerRao Lower Bound (CRLB) when there is no model mismatch. The proposed algorithm was also evaluated on two highdimensional and heterogeneous datasets, the NYC Taxi dataset and the MovieLens-10M dataset, for various machine learning tasks. Speciﬁcally, the experiments demonstrated that the proposed MMFA model generalizes to unseen data better than the state-of-the-art data fusion techniques such as KCCA and PICA, provides meaningful anomaly detection results, predicts missing data better than the collaborative ﬁltering techniques, and gives more accurate recommendations than the state-ofthe-art recommender systems. We should note here that despite our efforts for a fair comparison between algorithms, the benchmark algorithms could possibly be further optimized to improve their performances. As future work, we plan to investigate (i) deep versions of MMFA which fuses different modalities in lower levels of hyper-parameters in a hierarchical Bayesian setup, (ii) links and comparisons with generative neural network models, such as Variational Autoencoders, Restricted Boltzmann Machines (RBM), Generative Adversarial Networks (GAN), and (iii) stochastic optimization methods for variational EM to improve further the memory complexity for extremely large datasets. Proof of Proposition 1: For notational simplicity, we will drop the iteration index n in the E-step. Deﬁning the diagonal matrix N = diag(N, . . . , N) we start with manipulating Ω using (9) and (11), Ω = I⊗ F− 1⊗ FC where F =CN C+I. Using again the Matrix Inversion Lemma for Γ we obtain We continue with putting the mean ω of the approximate posterior in a compact and computationally efﬁcient form. = Ω(Cξ)· · · (Cξ) where ξ= [z· · ·z], j = 1, . . . , D− 1, and we used the Ω expression derived above. Reorganizing the vector ω = [φ· · · φ]as the matrix Φ = [φ· · · φ], in a more compact form, we can write whereZ = [ξ· · · ξ] = [z· · ·z]. In the M-step, the update equation for ψat iteration n is found from (9) as, − (η− ψ)∇lse(ψ) − lse(ψ) 0 = E(v|ω,Ω)A(Vc− ψ) + ∇lse(ψ) − ∇lse(ψ) For the update of the coefﬁcient vector estimate c(cf. (8)), from (10), the part related to the multinomial model is given which concludes the proof. Proof of Theorem 1: In the Gaussian E-step (line 4 in Algorithm 2), the most expensive computations are CΣCand BC for each feature j, resulting in O(KP D) computations. The matrix inversion for all j is O(KD), but this is cheaper than O(KP D) since K  P . Similarly in the Gaussian M-step (line 5), computing cBfor each i, j pair yields O(KP D) complexity. In the multinomial model, both CZ in the E-step (line 6) and Φc, for i = 1, . . . , P , in the M-step (line 7) have a complexity of O(KP D), and the rest of the computations are cheaper. Finally, for the coefﬁcients c(line 8) we solve a quadratic program for each i. In solving a possibly constrained quadratic program for each c, the number of iterations, in practice, is bounded by a constant; and in each iteration, linear algebra operations in the K-dimensional space are performed. Hence, the overall complexity for solving the quadratic programs is O(KP ). Note also that each ccan be updated in parallel. The computation of {H} and {ρ} have O(K(K + P + D+ P D)) and O(KP (D+ D)) complexity, respectively. Combining all the complexities we get O(KP + KP D+ KP D).