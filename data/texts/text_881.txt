<title>On some foundational aspects of human centered Artiﬁcial Intelligence</title> <title>1 Introduction</title> <title>arXiv:2112.14480v1  [cs.AI]  29 Dec 2021</title> Recently artiﬁcial intelligence (AI) systems become popular and are playing an increasingly important and per vasive role in the life of indiv iduals and society. The fact that AI systems can signiﬁcatively aﬀect human lives, and can play an active role in society, is fostering discussions on how these new systems should behave in their relationship with humans. Human Centered AI (HCAI) concerns the study of how pre sent and future AI systems will interact with human lives in a mixed society composed of artiﬁcial and human agents, and how to keep the human into the focus in this mixed society. Due the interdisciplinarity of the topic, the dis cussion involves both technical and non technical people which see AI agents from diﬀerent perspectives. It is often diﬃcult to integrate these perspectives in a coherent vision in which all points of v iew can contribute and complement each other. Of particula r interests are the recent attempts to provide requirements and ethical guidelines that r egulate HCAI systems, like the ones published by the European Commission in 2019 [11]. While such guidelines will certainly have a n impo rtant impac t on the future of AI, they typically describe HCAI systems at a very abstrac t, qualitative level. Understanding the concrete impact of such abstract requirements on the technical development of HCAI systems is neither trivial nor unique. A case in point is the recent Euro pean legislative proposal for an “AI Act” [5]. The necessity to deﬁne a precise grounding of high level qualitative statements into technical requirements fo r HCAI systems was explicitly noted in the above EC guidelines: Requirements for Trustworthy AI should be ”transla ted” into procedures and/or constraints on pro cedures, which should be anchored in the AI system’s architecture. [11, page 21 ]. The main objective of this document is to provide a reference description of an HCAI agent, together with its main components and functions, that helps to bridge the gap between the abstract speciﬁcations of HCAI systems provided by non-technical entities and the real implementation of thes e systems. Such a description is intended to contribute to the above grounding. In other words, the main objective of this paper is an attempt to ﬁll the gap between “non technical description of HCAI ag ent” and more “technical structure” that is understandable without a deep k nowledge of the main technics of AI, such as Machine Learning, Automated Reasoning, Reinforcement Learning, Probabilistic inference and optimisation. Despite this, we will try to be precise enough to allow the mapping of high level concepts that one can ﬁnd for instance in the EC ethical guidelines into a more technical and technically operative description. Although the account of HCAI age nts that we oﬀer in this paper is certainly preliminary, we believe that it constitutes an important ﬁrst step towar ds ﬁlling a crucial cultural gap. The rest of this paper is organized as follows. In the next section, we provide a deﬁnition o f HCAI agents in terms of their nee ssary features. In Section 3 we oﬀer a schematic represesentation of a HCAI agent, and in Section 4 we discuss the key components of this represe ntation. In Section 5 we look in more detail at one of these c omponents, AI models. Finally, we discuss related work in Section 6, and draw some concluding remarks in Section 7. <title>2 Deﬁnition of HCAI agent</title> In the following, we provide a connotative deﬁnition of an HCAI agent by listing a set of features tha t an AI agent should meet in order to be considered Human Centered. Agent-environment pair. We consider a reference framework composed of an autonomus agent that operates in an environment that includes the pres ence of humans . We call this an agent-environment pair. HCAI concentrates on agentenvironment pairs in which humans dire c tly interact with the artiﬁcial agent in a collaborative attitude. We therefore exclude from our analysis situations in which the agent competes with humans, such as the ones recently studied in [6]. In an agent-environment pair, there is a clear separation between what is internal to the agent and what is external to the agent, i.e., the environment. At every time point in time, both the agent and the enviroment are in a state, that we r efer as the state of the agent and the state of the environment, respectively. We don’t consider the level of quantum physics here, so we as sume that both agents and environments are in one single state at every time. While the internal state of the agent is directly accessible to the agent, the state of the environment, including the humans that p opulate it, is not directly accessible by the agent. The agent can only partially perceive the state of the environment through observations. The agent-environment pair can be represented as a pair (Ag, Env) with where t is some startand Env = {State(Env) Ag = {State(Ag) ing time-point. State(Ag) denotes the state of the a gent at time t, while State(Env) denotes the state of the environment a t time t. We emphasize once again that the environment Env also includes the humans, and that these inter act with the agent. [. . . ] must guarantee privacy and data pro tec tion throughout a system’s entir e lifecycle. [11, page 17] [. . . ] consistent with the input, and that the decisions [to execute an action] are made in a way allowing va lidation of the underlying process. [11, page 22] Furthermore, in the presence of an a pplicable legislation, an HCAI agent should execute only legal actions, i.e., ac tions complying with all applicable laws and regulations. Goal directed agents. AI agents always have one or more goals to achieve. An agent’s goals ca n be codiﬁed in many diﬀerent ways, depending on the architectur e of the agent. For instanc e, the goal could be to optimize a c ertain function, to ﬁnd the best path to achieve a position, to improve the agent’s capability of recognising objects in a scene, or to correctly answer users queries. To be human-center ed, the achievement of such a goal by an HCAI agent should not harm the humans present in the environment. According to the principles of prevention of harm and fairness mentioned in the EU Ethics Guidelines: AI systems should neither cause nor exacerba te harm or otherwise adversely aﬀect human beings, [. . . ] ensuring that individuals and groups are free from unfair bias, discrimination and stigmatisation. If unfair biases can be avoided, AI s ystems could even increase societal fairness . [11, page 12] Making explicit the goals o f an HCAI agent in a necessary condition to support these principles. Furthermore an HCAI agent should not have goals that contrasts the goal of the human present in the environment. No full autonomy. HCAI agent should not be completely autonomous, and their behaviour should always be sensitive to the e xternal environmental or human context. Humans should always be allowed to intervene, e.g., to avoid tha t the agent performs a harmful actions, or a speciﬁc set of them. This is necessary to guarantee the principle of respect for human autonomy from the EU Ethics Guidelines [11, pa ge 12 ], that allows a human to autonomously decide to prevent the execution of one o r more actions to the HCAI agent. From an architectural poin of view, this means that among the set of observations tha t an HCAI agent has to employ, there should be some that detect the willingness of the humans to interrupt some planned or exe c uted actions. <title>3 Schema for an HCAI agent</title> Figure 1 shows a pictorial representation of our vision of an HCAI agent. This picture interprets literally the adjective “human centered” by drawing the human, and their environment, at the center of the representation sur rounded by the agent. This is the opposite of the usual representation tha t shows an AI agent in the center, connected to the environment and the humans around it. A ﬁrst observation concerns the fact that humans cannot be separated from the environment where they operate. Indeed they are part of the environment, and embedded in it. Humans and e nvironment are complementary, interconnected, and interdependent in the na tural world, and they interrelate to one another. Therefore HCAI Agents should take into account both humans and the environment where humans operate. In the picture this is r e presented by the blue and green Yin-Yang symbol in the inner ring. Fig. 1. A simple schematization of a Human-Ce ntered Artiﬁcial Intelligence Agent (external ring). The agent interacts w ith of the humans (H) and environment (E) (inner ring) via the elements in the middle ring , relying on one o r several internal models. The outer ring in Figure 1 represe nts the artiﬁcia l intelligent a gent. For our discussion, the agent behaves as a unit. In many cases, however, the HCAI agent may be internally comprise several interrelated intelligent agents, which interact following, e.g., a multi-agent paradigm. The whole HCAI agent, or any of its component agents, acco mplishe s some task directly relevant for the humans it inter acts with using one or several speciﬁc approaches, or models. For instance, an “artiﬁcial reasoner” could represent its knowledge in some logical formalism and s upport query answering and inference thr ough automatic reasoning (e.g., SAT, ASP); an “artiﬁcial classiﬁer ” c ould be implemented in a deep neural network that is capable to classify images into diﬀerent classes; and an “artiﬁcial planner” can produce plans and take decision exploiting c lassical planning techniques or reinforcement learning. To solve complex tasks, diﬀerent systems and methodologies typically need to be integrated. A black box integration of each “intelligent agent” is not suﬃcient; it is necessary to integrate and make all these diﬀerent approaches to collaborate one another in a glass -box method. We shall come back to this point when discussing hybrid models in Section 5.1. The middle ring represents the interaction between the human-environment system and the (integrated) artiﬁcial intelligent system. The type of interactions that one can see between human-environment system a nd the artiﬁcial agent happens across a set of artifacts that are “shared” by the human-environment and the artiﬁcial systems. We brieﬂy introduce them here, and we shall describe them more extensively in the rest of this document. Dependability Requirements are artifacts, produced by humans, that spec ify the expected behaviour or other non-functional properties of the artiﬁcial intelligent system. Actions are considered in a broad sense. They represent the actions that the artiﬁcial agent can perform on the human-environment system, as well as the ac tion that the human can perform on the artiﬁcial agent. These ca n be physical actions, that aﬀect the state of the envir onment, or informative actions, that aﬀect the knowledge of the human or of the artiﬁcial agent. Observations are all the data tha t the artiﬁcial intelligent sys tem can collect thr ough its sensors. Finally, Explanations are artifacts produced by an artiﬁcial agent that “explain” to the human the reasons of its behaviour, e.g., the reason why it took a decision or exe cuted an action. Explanations should be human understandable and accepta ble in a rational system shared by the machine and the human. <title>4 Key component s of an HCAI agent</title> Figure 1 identiﬁes ﬁve key components that constitute a Human Centered AI agent: four types of artifacts that are s hared by the human-environment system and the agent and that mediate the interactions between them; and a set o f inter nal models. We now discuss these components in greater detail. An agent can observe the humans and the environment through its senso rs. Here we use se nsor in a broad sense, i.e., everything that can be used by the agent to autonomously acquire data about the environment (including humans ) and everything the humans and the environment can use to syncronously or asyncro nouysy input data to the artiﬁcial agent. Among the observations we also include the “commands/requests” tha t humans pose to the artiﬁcial agent. There is a great variety of types of sensors, and they all serve their purpose depending on the information which they are meant to collect, and on the application domain. For example, sensors in Internet o f Things applications may include cameras, ultrasonic, infrared, acoustic, mechanical, and so on. I n Robotics applications, sens ors may include potentiometers, accelerometers, Gimbles, lidar, an so on. I n Media applications, sensors may include digital cameras, audio recording, textual doc uments, and so on. Essentially, sensor systems inject input data. These can come in streams, mostly for online systems, or in batch, mostly for oﬄine systems. The way input data is represented varies widely, from continuous to discre te, structured, lists, trees, and so on. Input data are typically coupled with some meta info rmation that describes their representation and structure. In general, every item of data is as sociated with one or several time stamps, that can be relative or absolute. Obs ∼ P r(· | State(Env) , State(Ag) ), i.e., observations at time t follow some (possibly unknown) probability distribution conditioned by the state of the environment and of the agent at time t. The set of actions of an a gent are all the possible ways that the agent has to aﬀect the state of the environemnt and of the humans which it iteracts with. Actions can be physical actions, e.g., r obot moves forward 10 cm, an autonomous car brakes, an autonomous personal assistant books a room in hotel Bellavista or buys 100 stocks of the company SuperGulp. Another type of action are informative actions that provide information to the human as a conseguence of a decision that the agent has reached. Examples of infor mative actions are the communication of the result of a classiﬁcations of an iten provided in input, or the communication that the price of SuperGulp stocks is falling. Actions should produce a tangible eﬀect o n the environment and/or the humans in it. This means that if exe cution does not fail, the state of the humans or environment once an action is terminated should be diﬀerent from their state before the execution of that action. Note that an internal agent activity, like an agent taking a decis ion about doing something, should not be considered as an action, because it changes only the internal state of the agent but it does not aﬀect the environment. The real action consists on the eﬀective execution of the decision. Also note tha t in general the outcome of an action is not deterministic. Concrete examples of actions include the following. If an agent’s task is to classify documents in n classes, its actions are those that communicate the result of the classiﬁcation of an item into one or more class. Such action will change the knowledge state of the human that has requested the classiﬁcation. Classiﬁcation without c ommunication should not be considered as action. Clearly all the actions o f a r obot involving physical movement ar e conisdered as actions since they change the position of the robot in the e nvironment. The set of actions associated to an artiﬁcia l agent can be discrete or continuous. Notice that the set of agent’s actions does not include the actions that are executed by the humans, or other events that can happe n asynchronously in the environment. Deﬁnition 2. The actions of an HCAI agent is a set A such that any a ∈ A is associated to a set of eﬀects Eﬀ(a) which describes a transformation of the s tate of t he environmen t that is directly relevant for the humans in the environment. The mapping Eﬀ(a) can be deterministic, non deterministic or probabilistic. An important aspect of the above deﬁtion is that actions in an HCAI should be directly relevant for the humans that populates the environment. This aspect relies on the pre-theoretical notion of re levance that cannot be easily captured in a formal deﬁnition, since re le vance may emerge thro ugh complex ca usal chains. In some case s, relevance is clear: the decision of an agent to grant a mortgage to a human is deﬁnitively relevant for the human; while the dec ision taken by a system tha t controls the trajectory of a satellite cannot be considered human centered, although it might have an impact on TV viewers. Most cases, however, are not so clear cut. For instance, a domestic robot’s plugging to the charging station may not directly aﬀect the state of a human; but if this action consumes the solar-powered battery cells of the home, this may aﬀect the ability of the human inhabitant to cook dinner later on. An explanation is an explanation of some decision. In HCAI, explanations are meant to explain to a human, or a set of humans, the decision of executing an action a made by an a gent at time t when it was in the state State(Ag) . As argued in [9], the deﬁnition of explanations for a decision taken by a n HCAI agent cannot be provided without making explicit reference to the human to whom the explanation is addressed, i.e, the explanee. In general, an explanation of a decis ion taken by the agent ca n be independent from how the a gent reaches such a decision. This is especially true when the decision is taken by a black-box method such as a neural network. Deﬁnition 3 (Explanation). An explanation of an HCAI is a representation of t he reason why the agent took a decision to execute a given action a in state State(Ag) . Such an explanation should be intelligible, understandable and acceptable by the explanee. This deﬁnition s ees explanation as an artifact. Another notion of explanatio n is to see it as the process of building or communica ting an explanation. There is a rich literatur e in which explanatio n refers to a dialogue, and therefore a collaborative process, between the human and the agent, where the human collaborates with the agent in the creation of a explanation of the agent’s decision that is satisfactory for the human. The result of this process is an explanation artifact that can be stored and reused. A requirement, including the concept of dependability requirement, is the formulation of a functional need that an AI s ystem must satisfy. This is central to the problem of veriﬁable AI, which has the objective of checking that a system meets its requirements, including functional speciﬁcations and dependability attributes. Dependability r e quirements should b e expressed in a human under standable way: in fact, they are usually formulated by humans. The problem of veriﬁable AI is how to translate these requirements in a lgorithms that check that the sys tem is compliant with them. Being a human produced artifact, providing a sharp deﬁnition of a requirement is rather complex if not impossible. Some requirements may be spe c iﬁe d in a formal lang uage, that has an intuitive semantics for the human, so that it is possible to verify automatically that the artiﬁcial intelligent system behaves according to the requirement, at least to some measurable degree of certainty. Some requirements may not be expressed in a formal language, as representing them in a mathematical structure is still an open issue. Below, we concentrate on formal r e quirements. Deﬁnition 4. A formal requirement is an expression in a formal language (e.g., in mathematics or logics) that unambiguously describes a criterion to determine if an agent-environment pair fulﬁls it. There are aspects of the veriﬁcation of r e quirements for HCAI agents that go beyond standard require ment engineering, and that stem from these being AI agents as opposed to more traditiona l software systems. In particular, AI agents have the ability to learn new knowledge by generalising observations done on a set of data, and to make inferences from the learned knowledge in order to take decision in pr eviously unseen or unmodeled situations. In addition, in Human-Centered AI agents one may need to impose requirements on how the agent reaches the decision that leads to a certain behaviour. For instance, in the case of a n a gent that contains machine learning models trained on data, typical requir ements concer ns the fact that these data are not biased, or that they respect the GDPR. This type of requirements goes beyo nd the above deﬁnition, that focuses on the behaviour of the agent relative to the environment in which it operates. An HCAI agent takes decisions on how to act in the environment, or how to react to some input coming from the humans and/or the environment, on the basis of one or more models of the human-environment system which it is interacting with. Models are abstract (computational) structures that allow to answer queries about what holds in the current or past situations, and to predict what will be true in the future. Models can also be used to simulate possible alternative evolutions o f the human-environment system in order to take the “right” decision now. HCAI agents are equipped with a set of models that represent the knowledge of the agent about the human-environment system. This knowledge is used to support the agent in making decisions about which actions to perform. In general, we cannot assume that such models are correct, i.e., that their reﬂect the true state human-environment sy stem and that their predictions are eﬀectively true. For this reason it is more appropriate to speak about belief instead of knowledge. Neither we can ass ume that models are complete, i.e., that they describe the environment in all its details. Indeed they are simpliﬁed, abstract representations of some as pect of the environment obtained by abstr acting away irrelevant (or believed to be irrelevant) details. Even at the given level of abstractio n, models may still miss informatio n if this is not known or observed. <title>5 Models for an HCAI agent</title> The core knowledge of an HCAI agent is encapsulated in the set of models that it adopts in order to interpret the input data, take decisions, and provide explanations for the decisions taken. If we ignore the physical aspe cts of an agent, it is acceptable to say that the behaviour of a n HCAI age nt is fully determined by its models. Most of the requirements and guidelines provided for HCAI agents thus concern how models are built, how they evolve, and how they are used to take decisio ns. Given this central role of models, it is useful to discuss in more detail what are the diﬀerent classes of models that we ca n use in an HCAI agent, and how these models are built and how they e volve — in other words , the life-cycle of models. The rest of this section is devoted to these topic s. Providing a complete and coherent classiﬁcation, or even an ontology, of AI models is beyond the scope of this paper. For our purposes, it is enough to list the most important, g eneral classes of AI models. In our classiﬁcation we take a “technological” perspective, i.e., we deﬁne classes on the basis of the set of methodologies which are used to specify the model, to represent information, and to perform decisions. We identify four main classes plus their combination. Logical Models. The key aspects of the environment and the human are re presented with a logical theory (set of formulas of a logica l/formal langua ge) and decision on the basis of this model are taken via logical reasoning. Examples of this type of model are Logical Knowledge Bases and Ontologies, Logic programs, and planning domains speciﬁed via PDDL or other action languages. Logical models are speciﬁed declaratively using a set of terms and formulas from a logic based language. A goo d summary of many diﬀerent logica l models is provided in the Handbook of Knowledge Repres e ntation [8]. Logical models provide information of what is true, what is false and what logic ally follows from some premises. Minker [10] oﬀers an overview of how logical models can be built and how they c an be used for inferenc e, decision making and planning. Probabilistic Models. The key aspects of the environment and the human are represented by some probabilistic distribution. Decision a re taken on the basis of probabilistic inference. In an AI model, probability distributions are not simply deﬁned over a set of hypothese s but rather over some more complex structure suitable to represent knowledge, as noted by Chater et al [1]: The knowledge and beliefs of cognitive agents are modeled using probability distr ibutions deﬁned over structured systems of representation, such as graphs, generative grammars, or predicate logic. This development is crucial for making probabilistic models relevant to cognitive science, where structured representations are frequently viewed as theoretically central. Examples of this type of models are statistical graphical models, like Bayesian Networks and Hidden Markov Models. Usually probabilistic models distinguish between observable variables, which correspond to the evidence that an agent is able to observe directly, and hidden variables, whose distribution should be discovered from the data. The key concept in this type of model is the variable assignment, i.e., an assignment to all the random variables, on which it is possible to a pply the model in order to predict the likelihood o f such an assignment. Beside probability, other mathematical theories have been used to build models for uncerta in knowledge and plausible rea soning, including possibility theo ry [4] and belief function [16]. We extend the term “probabilistic models” to cover models based on those theorie s as well. Real-Valued Functional Models. The key aspects of the worlds and the user are represented through (a set of) real-valued functions. These models can be used take in input obse rvable quantities and produce an estimation of some non-observable quantity, or pr edictions o f future values. Examples of this type of models are linear mo de ls, support vector machines, decision trees, random forest and (deep) neural networks. A large class of real-valued functional models is constituted by neural network models. A neural model is a directed graph of nodes. Each node is as sociated with a non linear activation function, the input of a node n is a linear combination of the outputs of the functions associated to no des that precede n in the graph. Both the linear combination and the activation function are associated with a set of parameters, that need to be instantiated in order to fully deﬁne the function. A neural network model is also associated to a Loss (or Cost) function, that determines the approximation error (i.e., diﬀerence between known and predicted outputs) to be minimized. For every instantiation of its parameter s, a neural network computes a function f : R → R where k is the number of input nodes (i.e., nodes that don’t have any predecessor) and h is the number of output nodes (i.e., nodes which are not predecessor of any other node). Like in all machine learning models, the main objective in a neural network is to ﬁnd an instantiation of the parameters that minimizes the Loss/C ost function. Dynamic Decision Models The main purpose of these models is to represent the dynamics of the environment in terms of states and re lations (transitions) between states, as well as the payoﬀ obtained by the agent’s being at a state. A state repr esents what holds at a given time point in the environment; two states are connected by a transition if the agent can pas s from one state to the other by executing an action. Furthermore every state is associated with some “evaluation” function that expresses how much that state satisﬁes the objectives of the agent. These models are used by the agent to dec ide which actions to take at every future state in o rder to maximize the likelihood of its payoﬀ. These models are associated with a set of algorithms that allow the agent to produce a “policy” or a “plan”, i.e., a sequence o f actions, or some more co mplex control structure, that will reach a state with optimal (or s ub-optimal) payoﬀ. This class includes a vast variety of models, such as (Pa rtially Observable) Markov Decision Models, Planning Models, and Finite Automata. Hybrid models. In many cases a model presents characteristics that are co mmon to more than one of the classes described above. We call them hybrid models. Hybrid models are models that integrate some of the previous types of models. Examples of such models are approaches that integrates logical and numerical models (e.g., Logic Tensor Networks, Lyrics) approaches that integrate logical and statistical models (e.g., Markov Logic Networks, Probabilistic Logic Programming), and approaches that integrate numeric, statistical and logic al models (e.g., DeepProbLog, deep probabilistic logic programming, and Probabilistic Soft Logic). We represent the lifecycle of each model in an HCAI agent as illustr ated in Figure 2. Below, we describe each main step in this cyc le . that tho se predicates. Similarly, in a neural network for classifying images in N classes C1, . . . , CN, one has to say which of the output neuron corr esponds to each clas s Ci. Model instance speciﬁcation/learning/update The instantiation of a model schema amounts in setting the pa rameters of the model. This amounts in encoding a c e rtain amount of knowledge about the environment utilizing the “tool-set” provided by the model schema. The encoding can be done manually, as it often happens in logical rule based models via s ome knowledge engineering activity, or via supervised lea rning from data manually labelled by humans, or in a fully unsupe rvised and automatic manner (e.g., clustering). Statistical models can be obtained by Bayesian inference from a set of observations or by Maximum likelihood, or maximum a posteriori inference. Other methods to model speciﬁcations can also be obtained by model adaptation or transfer learning. Similarly, updating the model can be performed automatically via retraining, or manually by modifying the parameters. Automatic learning of facts and rules from natural language is also possible. Methods for automatic learning of constraints from data are also availa ble . Inference with the model The sec ond important aspect is how the model is used to infer a decision, i.e., an a ction that the artiﬁcial agent decides to undertake. Given a set of observations O as input, the model provides as output a set of actions or a policy for actions. This is obtained by applying an inference engine which is deﬁned on the model. In this phase the model instance is queried about what holds in the environment. Inference can be very simple, like in neura l network (simple forward propagation) or rather complex, like in constraint satisfaction where it may be necessary to apply search or optimization algorithms. In logical models inference is done via some form of logical reasoning (e.g., satisﬁability) or model checking, while in statistical models inference can be a generative process (generate a data that has certain properties) or to compute some ma rginal distribution of a certain (set of) stochastic variables . What all the above inference activities have in common is that they don’t change the model, but only query it. Quality Control and Maintenance Once an AI artifact is ready to be used in practice, additio nal tasks that are often not part of res earch activities become important to r each higher Technology Readiness Levels. For certiﬁcation purpo ses and for the permission to use the artifact in practice on/with non-expert users, testing and veriﬁcation procedures for safety/securityrelated properties of the behavior of the model can be necessary. For pr oducts that are already in usage and needs to b e updated, maintenance and updating methods need to exist so that problems that are identiﬁed after shipping the model can be counter acted. For supporting us ers and for legal r easons, it can be necessary to have powerful methods for debugging model properties and (r e)actions, and for explaining why certain outputs were (or were not) generated. Moreover, in connection with updates of the model, these methods can be useful to prove to authorities that certain behavior is excluded in the future. This part of the AI artifact lifecycle is very relevant to human-centric artiﬁcial intelligence, because it is the longest-lasting process in the existence of the model where the model has contact with a large number of untra ine d human individuals and unseen input data. <title>6 Related work</title> With the increasing prominence of AI, there is an increas ing reﬂection on what it means for AI to be “human-centered” and several papers have been published on this topic, some of which are discussed below. To the best of our knowledge, however, the work r eported here is the ﬁrst one that tries to take a foundational approach to the problem of human-centered AI, framing the discussion in technical terms by deﬁning the notion of a human-centered AI ag ent. Wei Xu [15] emphasizes that human centered AI s ystems should exhibit behaviour which is intuitive for humans, and that this can be achieved by adopting human centered design. The paper proposes an architecture that supports the collaboration between humans and machines by considering three main factors: ethically aligned design, necessary to create AI agents that behave fairly, and collaborate with humans rather than competing w ith them; resemblance of human intelligence, necessary to develop AI agents with human-like intelligent behaviours; and comprehensibility, usefulness and usability, necessary to de velop AI agents that are capable of helping humans. Ben Shneiderman [13] suggests that making human-centered AI system requires the combination of AI-based intellig ent algor ithms with human-centered design. The author highlights that, in developing HCAI, one should not limit the evaluation to performance to the technological parts, but higher attention to human users and other stakeholders. This requires an increased the prominence of user experience design and of human performance measures. The perspective taken in the paper is to s upport the 17 United Nations Sustainable Development Goals. This work resonates with the AI model lifecycle proposed here (se e Section 5), especially in the parts of providing input for requirement speciﬁcation and of colle cting the agent feedback. In another work [14], Shneiderman stresse s that HCAI systems should give humans a greater control on the ever-increasing automation, instead of replacing them in the decision processes. According to the author, [. . . ] humans must have meaningful control of technology and a re responsible for the outcomes of their actio ns. When humans depend on automation to get their work done, they must be able to anticipate what happens, because they, not the machines, are responsible. Shneiderman’s stance is important, as it poses a limit on the decision power of the machines and it relieves them from any responsibility. Such a visio n clashes with an opposite request of AI systems to be increasingly autonomous. The tension between machine autonomy a nd human control gave rise recently to a ﬁeld called Human Centered Machine Learning, which focuses on the development of HCAI systesm that adopt models that can be trained via machine learning techniques. See Kaluarachchi et al [7] for a revie w of the recent literature. The main objective here is to develop AI models that take into account the input provided by humans when they get to a decision. An example is provided by Abir Deet al [2], who propose a model that classiﬁes images while allowing user input during inference. An empirical evaluation shows that this model can surpass the performance of models trained for full automation, as well as the one of humans operating alone. Dignum and Dignum [3] argue that human-centered AI involves a shift from an AI which is able to solve human tasks tha t requir e some form of intelligence, to an AI which is awa re on the social environment in which it is embedded, and operates taking into account all the limitations, the opportunities, and the needs of the social environment. This positions suggests that an AI system should be considered as part of a broader socio-technical system. As they put it: AI sys tems are fundamentally socio-technical, including the social context where it is developed, used, and acted upon, with its variety of stakeholders, institutions, cultures, norms and spaces. This perspective is in accordance w ith the schema of a human centered AI agent proposed he re (see Figure 1), where the AI agent is built around the environment, and, in some s e nse, also includes the social context in which it operates. <title>7 Conclusions</title> Human-centered AI ag ents are considered as part o f a larger system that also includes the humans, their society and the environment at large. Co nsequently, the developments of HCAI agents is not only a matter o f developing eﬃcient and eﬀective altorithms to solve complex probleems that require some form of intelligence. HCAI looks at the developments of AI focussing also on human values such ethical principles, fareness, transparency of decisio n and objectives, . . . . As such, human-c entered AI is an intrinsically multi-disc iplinary eﬀort that requires the establishment of a common ground between technical and non-technical disciplines such as, sociology, low, ethics, and philosophy. This paper is an attempt to o ﬀers a ﬁrst step in fulﬁlling this requirement, by introducing the general concept of a Human-Ce ntered AI agent, together with its main components and functions, a s a way to bridge technical and non-technical discussions on humancentered AI. <title>Acknowledgements</title> This paper has been inﬂuenced by the many discussions on the foundations of human-centered AI held in the framework of AI4EU, the EU landmark project to develop the Euro pean AI platform and ecosystem. We are indebted to the partners of work-package WP7 in AI4EU, who all contributed to these discussions. This work has been supported by the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 825619 (AI4EU). <title>Bibliography</title> [1] Nick Chater, Joshua B Tenenbaum, and Alan Yuille. Probabilistic models of cognition: Conceptua l fo undations, 2006. [2] Abir De, Nasta ran Okati, Ali Zarezade, and Manuel Gomez-Rodriguez. Classiﬁcation under human assistance. arXiv preprint arXiv:2006.11845, 2020. [3] Frank Dignum and Virginia Dignum. How to center AI on humans. In NeHuAI@ECAI, 2020. [4] Didier Dubois and Henry Prade. Possibility theor y and its applications: Where do we stand? In Springer handbook of computational intelligence, pages 31–60. Springer, 2015. [5] Euro pean C ommission. Proposal for a regulation of the european parliament and of the council laying down harmonis ed rules on artiﬁcial intelligence and amending certain union legislative acts, April 2021. https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206. [6] Marlena R Fraune, Steven Sherrin, Selma Sabanovi´c, and Eliot R Smith. Is human-robot interaction more compe titive between groups tha n between individuals? In 2019 14th ACM/IEEE International Conference on HumanRobot Interaction (HRI), pages 104–113. IEEE, 2019. [7] Tharindu K aluarachchi, Andrew Reis, a nd Suranga Nanayakkara. A review of recent deep learning approa ches in human- centered machine lear ning. Sensors, 21(7):2514, 2021. [8] Vladimir Lifschitz, Bruce Porter, and Frank Van Harmelen. Handbook of Knowledge Representation. Elsevier, 2008. [9] Tim Miller. Explanation in artiﬁcial intelligence: Insights from the social sciences. Artiﬁcial intelligence, 267:1 –38, 2019. [10] Jack Minker. Logic-based artiﬁcial intelligence, volume 597. Springer Sc ience & Business Media, 2 012. [11] High-Level Expert Group on Artiﬁcial Intelligence. Ethics guidelines for trustworthy AI, 2019. https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai. [12] Stuart Russel and Peter Nor vig. Artiﬁcial Intelligence; A Modern Approach. Pearson, 4th e dition, 202 0. [13] Ben Shneiderman. Human-centered AI: A new synthesis. In Carmelo Ardito, Rosa Lanzilotti, Alessio Malizia, Helen Petrie, Antonio Piccinno, Giusepp e Desolda, and Kori Inkpen, editors, H uman-Computer Interaction – INTERACT 2021, pag es 3–8, Cham, 2021. Springer International Publishing. ISBN 978-3-030-85623-6. [14] Ben Shneiderman. Human-centered AI. Issues in Science and Technology, 37(2):56–61, 2021. [15] Wei Xu. Toward human-centered AI: A pe rspective from human-computer inter action. Interactions, 26(4):42–46, June 2019. ISSN 1072-5520. doi: 10.1145/3328485. URL https://doi.org/10.1145/3328485. [16] Ronald R Yager and Liping Liu. Classic works of the Dempster-Shafer theory of belief functions, volume 219. Springer, 200 8.