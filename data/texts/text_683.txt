Abstract—Recently, Graph Neural Networks (GNNs) have proven their effectiveness for recommender systems. Existing studies have applied GNNs to capture collaborative relations in the data. However, in real-world scenarios, the relations in a recommendation graph can be of various kinds. For example, two movies may be associated either by the same genre or by the same director/actor. If we use a single graph to elaborate all these relations, the graph can be too complex to process. To address this issue, we bring the idea of pre-training to process the complex graph step by step. Based on the idea of divideand-conquer, we separate the large graph into three sub-graphs: user graph, item graph, and user-item interaction graph. Then the user and item embeddings are pre-trained from user and item graphs, respectively. To conduct pre-training, we construct the multi-relational user graph and item graph, respectively, based on their attributes. In this paper, we propose a novel Reinforced Attentive Multirelational Graph Neural Network (RAM-GNN) to pre-train user and item embeddings on the user and item graph prior to the recommendation step. Speciﬁcally, we design a relation-level attention layer to learn the importance of different relations. Next, a Reinforced Neighbor Sampler (RNS) is applied to search the optimal ﬁltering threshold for sampling top-k similar neighbors in the graph, which avoids the over-smoothing issue. We initialize the recommendation model with the pre-trained user/item embeddings. Finally, an aggregation-based GNN model is utilized to learn from the collaborative relations in the user-item interaction graph and provide recommendations. Our experiments demonstrate that RAM-GNN outperforms other state-of-the-art graph-based recommendation models and multirelational graph neural networks. Index Terms—recommender system, graph neural network, reinforcement learning With the development of the Internet in recent years, the information explosion problem has become an inevitable issue to consider when designing a large-scale online platform. The overload of information hinders users’ ability to ﬁnd what they really need among a large amount of items. To enhance users’ experience, recommender systems have been applied in many web applications including e-commerce [1], [2], social recommendations [3], [4], and movie recommendations [5]. In order to improve performance, Knowledge Graph (KG)based recommender systems have achieved more consideration , Hao Peng, Philip S. Yu, Kannan Achan recently due to KG’s capacity of unifying user-item interactions and their side information in a graph [6]–[11]. However, since the KGs used for recommender systems contain multiple kinds of relations and are generally dense, directly conducting aggregation on KGs would suffer from the over-smoothing issue [12] when learning node embeddings. To be more speciﬁc, a GNN model aggregates the neighbor information into the central node. However, too complex neighbor information may contain more noises, which thus impedes the aggregation of a GNN model to retrieve relevant information. Also, when we directly apply GNNs on KGs, GNNs are not able to explore high-order connectivity because the number of neighbors grows exponentially w.r.t. the number of layers. Take the movie dataset as an example, when two users share similar ages and two movies have the same director, the connection between user A and movie Y is: which is a long path with multi-hop connection that GNNs can hardly explore. The spirit of divide-and-conquer motivates us to leverage the pre-training and ﬁne-tuning paradigm to learn user/item embeddings. The key idea of pre-training is to learn from other types of data, e.g., data from other sources or data for other tasks, and generate the embeddings [13]–[15]. In order to preserve the high-order information while avoiding the over-smoothing issue, we ﬁrst pre-train the model using various user-user and item-item relations in the KG, and then ﬁne-tune it only using the collaborative relation. In the pretraining step, the model can learn from the content of users and items and generate embeddings containing the information of various relations. Moreover, in the ﬁne-tuning step, another model emphasizes on the recommendation task, which only incorporate the collaborative information to achieve the best performance. Based on the idea above, we split the long path into several shorter paths which ﬁt the characteristics of GNNs. We divide the complex knowledge graph into three sub-graphs: user-user graph, item-item graph, and user-item interaction graph. In the pre-training step, we construct the multi-relational user graph and item graph, where Figure 1 shows an example of item graph. In these graphs, nodes are connected when they share the same features, so there can be multiple relations between a pair of nodes. In the user-item interaction graph, we utilize the user and item embeddings learned in the pre-training step as the initial embeddings and ﬁne-tune them via a GNN. However, it is challenging to adopt the aforementioned paradigm on the complex recommendation data. One challenge is uneven relation importance, which means different types of relations in the graph matter unevenly when learning the node embeddings. Figure 1 shows an example of this unevenness. In Figure 1, there are ﬁve types of relations shown in the graph, and some of them are more important when measuring the similarity between movies. For example, the similarity between True Lies and Titanic is higher than that between True Lies and Forrest Gump, since a common director is a stronger association than the same year of movies. Therefore, it is required to characterize the importance of those relations. The pre-training model needs to learn the importance score of different relations. The other challenges is uneven distributions, which indicates the distributions and statistics vary w.r.t. relations. For a particular node, some relations in Figure 1 connect few neighbors for this node, but for some other relations it may have thousands of neighbors. For example, one movie can only have several neighboring movies sharing the same director, but may have millions of movies sharing the same genre. Therefore, the latter relation extremely undermine the uniqueness of the movie when conducting aggregation in the GNN because of the over-smoothing problem. Thus our model should learn from these neighbors appropriately and select the most useful ones to train the model. To solve the challenges above, we propose a novel model named Reinforced Attentive Multi-relational Graph Neural Network (RAM-GNN). First, we construct a user graph and an item graph with multiple types of relations. Unlike most existing multi-relational graphs [16]–[18], our user and item graphs have multiple relation between a pair of nodes. Additionally, the relations have values, such as the director of the movie in the ”share directors” relation. In this case, we model the meta structure of the multi-relational graph as a quadruplet, which is ”item - (relation type) - (relation attribute) - item”, to learn the item embeddings as well as the relation embeddings. In RAMGNN, we apply two modules to solve the two challenges above and learn the node and relation embeddings. One is relationlevel attention, which learns the correlations between relations and items. The other is Reinforced Neighbor Sampler (RNS), which samples the top-k similar nodes to ﬁlter less relevant neighbors and enhance the quality of learned embeddings via an adaptive Reinforcement Learning (RL) process. To optimize the ﬁltering threshold for each relation, RNS tries to ﬁnd a trade-off between the average neighbor distances and the total number of the sampled nodes. The model is trained by a GNN loss and a similarity loss jointly with unsupervised learning. We pre-train the RAM-GNN model on the user graph and item graph respectively to learn user and item embeddings. Then another GNN initialized by the pre-trained embeddings is applied to ﬁne-tune the embeddings and provide recommendation results. In experiments, we demonstrate that our model outperforms other state-of-the-art methods. In this paper, we summarize our contributions as follows: dings of users and items for a recommender system. The two main components: relation-level attention and reinforced neighbor sampler provide robust and efﬁcient embedding learning. embeddings based on pre-trained item/user and relation embeddings. This GNN provides a list of items for the user as recommendation results. the recommendation task. The experiments demonstrate the effectiveness of the RAM-GNN compared to eight state-of-the-art baselines. Graph Neural Networks (GNNs) are neural networks that can process graphs directly. By aggregating neighboring information, GNNs can extract structural knowledge from a graph to learn node embeddings. Due to its capacity to process unstructured data, GNNs are widely used on many tasks such as node classiﬁcation, graph classiﬁcation, and link prediction. The forward propagation procedure of a GNN on graph G(V, E) is to update the representation of each node v∈ V through neighboring nodes. Suppose for each node i there is an initial node representation has the input of the model. Then, each hidden layer of GNN learns the central node representation hfrom the previous hidden layer hby aggregating the neighboring nodes as follows: where σ is the non-linear activation function such as LeakyReLU and N(i) represents the set of neighbors of node i in the graph. The neighborhood aggregation function Aggr(·) sums neighboring information up and applies an activation function (e.g., sigmoid or LeakyReLU). ⊕ represents the combination operation of aggregated neighbor embedding and the central node embedding from the previous layer, e.g., concatenation operation. When the number of edge types is more than one, we call this kind of graph as multi-relational graph. A classic method [17] to apply GNN to multi-relational graph is Relational GCN (R-GCN). It re-writes the GNN formula as: where Aggr, Wand N(i) denote the aggregation function, weight matrix and the neighbors of node i corresponding to the relation r. R is the set of all relations. However, this method suffers from over-parameterization because each relation is associated with a weight matrix. In this paper, our model tries to solve this problem with entity-relation composition operations to reduce the number of weight matrices in R-GCN to 1. In the following sections, we ﬁrst present how to pretrain the item graph with our proposed RAM-GNN. Then, we illustrate how to apply the knowledge learned in the pretraining process to help provide recommendations. To overcome the over-smoothing problem and ﬁnd the most relevant information in the data, we propose the RAM-GNN model to pre-train the user and item embeddings on the user and item graphs, respectively. We use the item graph in Figure 1 as an example to help understand the pre-training paradigm. The user embeddings are pre-trained in an analogous way upon the user graph. In real-world applications, there exist multiple relations between a pair of items that have concrete semantics. As shown in Figure 1, the relations between items form a multi-relational graph. Given an item pair (i, j), the relations between them are deﬁned as a set of r =< t, v >, where t denotes the relation type and v is the relation value. Our goal is to integrate all these relations to learn the item embeddings. Different from the triplets in traditional KGs, the meta structure of the item graph is a quadruplet (i, t, v, j), where i and j are the head and tail item entities, t is the relation type, and v is the relation value. Motivated from methods used in KG embeddings [16], we propagate the message from tail item j to head item i through relation r, and the process is formulated as: where φ is a composition operator, e, e, eand e∈ R denote the embeddings of head item, relation type, relation value, and tail item respectively in the KG. Different from traditional KG algorithms, which are designed for triplets, our method learns from the quadruplets and composition operations to generate embeddings. For relation types and relations values, we combine them together into integrated embeddings. Here, we concatenate the embeddings of relation types and relations values as: where k is the concatenation operation. In Section 2.3, we will use ein the recommendation step. Then, we can re-write the Eq. 3 as: where eis the new embedding that is composed of the relation type eand the relation value e. Three kinds of composition operations φ are applied in our model: Here, the three operations are motivated from TransE [16], DistMult [19], and HolE [20]. The performance of these operations are discussed in Section 3.5.1. To learn item embeddings based on the quadruplets, we design our model Reinforced Attentive Multi-relational Graph Neural Network (RAM-GNN), which are displayed in Figure 2. In this model, relation-level and node-level attention layers are proposed to handle the multi-relational graph. Details are introduced in the following sections. For all relations associated with the item i, they may contribute unequally in learning the item embedding. For example, the genres and actors of a movie have different importance. Additionally, thousands of movies may share the same genre, while only a few movies may have common actors. To solve this uneven relation importance challenge, we propose the relation-level attention. The attention mechanism [21] has been widely adopted in existing deep learning models to infer the importance of inputs. In our problem, we adopt the attention mechanism into the multi-relational graph to calculate the attention weights for each relation. We re-write the entityrelation composition operation based on self-attention: where p, b ∈ R, and αis the relation-speciﬁc attention weight for item pair (e, e) and relation embedding e. W, W, Ware key, query, and value weight matrices in the self-attention [21], respectively. With relation-speciﬁc attention, we can learn how the relation inﬂuences the head item and generate the embedding of tail item j considering their correlation. After the relation-level attention module, we propose the Reinforced Neighbor Sampler (RNS) based on Reinforcement Learning (RL). RNS searches the top-k similar items to reduce redundant information and improve efﬁciency. In the item or user graph, there can be massive numbers of neighbors for a node. For example, thousands of items can share the same category, and near half of the users are of the same gender. As a result, if we aggregate all neighbors together to learn the embedding of the target node, the unique features of the target node will be overwhelmed in the massive neighbors, which is the over-smoothing issue [12]. To deal with uneven distributions challenge, we design an adaptive neighbor sampler to prune the irrelevant neighbors and only select the top-k similar neighboring items to perform aggregation. Given the difference of relations in the graph, the ﬁltering thresholds of neighbor selections are difﬁcult to be pre-deﬁned as hyperparameters. Motivated by [11], [22]–[25], our proposed RNS searches the optimal threshold for each relation via an RL process. We measure the item similarity based on negative sampling [11], [26]. If two items are connected by an edge in the item graph, we take this pair of items as a positive instance. For each item, we also randomly select several irrelevant items as negative samples. We measure the item similarity by minimizing the distance of the positive item pairs and maximizing the distance of negative pairs. Inspired by [27], Multi-Layer Perceptron (MLP) is applied to measure the distance of items in RNS. Formally, the distance between two items is as follows: d(e, e) =k σ(MLP (e)) − σ(MLP (e)) k(9) where σ is the activation function, and we choose sigmoid in the above equation. i is the target item, and j is the positive or negative samples w.r.t. i. The output of the MLP layer is a scalar. With the sigmoid activation, the distance is projected to (0, 1), where a closer distance represents two items are stronger similar. To measure the similarity of a pair of items and simplify calculation, we convert distances to similarity scores as: where s(e, e) is the similarity score of item i and j, and a higher similarity score means i and j are more similar. We use the similarity score to discriminate whether the two items are similar or not. Based on the item similarity, we deﬁne a cross-entropy based similarity loss as the loss function of MLP layers. The similarity loss is a supplement of the GNN loss, which is formulated as: where I is an item set containing all positive and negative samples w.r.t. item i, and yis the ground truth to determine whether j is a positive or negative sample. With the similarity loss, we can learn the similarity score for each item pair and rank them decreasingly to ﬁnd the most similar ones. In the item graph, there are multiple types of relations. The number of items we select to conduct aggregation should vary w.r.t. different relations. Therefore, we adopt ﬁltering thresholds for all relations. However, incorporating those thresholds as hyper-parameters is time-consuming because the number of relations can be extremely large. To improve efﬁciency in the training process, we propose Reinforced Neighbor Sampler (RNS) to automatically search for the optimal ﬁltering threshold for each type of relation during the training process. We express the RL process of ﬁnding ﬁltering thresholds as Bernoulli Multi-armed Bandit (BMAB) B(A, R, T ) [28], which is a simpliﬁed version of Markov decision process as there is no state in it. In the BMAB, A, R and T represent the action space, the reward function, and the terminal condition, respectively. Initially, we set the kas the ﬁltering thresholds of the neighbor sampler corresponding to a relation type t. Then we utilize the reward function fto determine the action of increasing or decreasing the k. Speciﬁcally, the BMAB process are deﬁned as follows: Action space. The action represents how we adjust the k based on the reward function. We deﬁne a ﬁxed small value  as the action. kis increased or decreased by  to ﬁnd the optimal ﬁltering thresholds. Reward function. We design the reward function to determine increasing or decreasing the kbased on the distance of items we deﬁne in Eq. 9. The reward function discovers the most similar items iteratively to achieve the minimum Average Neighbor Distance (AND) with as large a ﬁltering threshold as possible. The AND in iteration γ is calculated as follows: where N(i) is the top kneighbor items of item i. It is a trade-off between the AND and ﬁltering threshold k, since we intend to have more neighbors into aggregation and less item aggregation user aggregation AND. Based on this idea, we can adjust the AND in the reward function as: f(AND) =+1, AND≥ AND,−1, AND< AND,(13) where fis the reward function. If the output of fis positive, kis increased by  and vice versa. In this way, a positive output of fleads to a smaller AND compared to last iteration so that we can increase kto have more neighbors involved. In opposite, if the fis negative, we need to decrease kto limit the AND. These two actions take place back and forth until reaching a convergence. Termination condition. The RL process converges when the AND does not vibrate explicitly during the training iteration, which leads to the termination condition as: As this inequality represents the cumulative reward of the recent ten training iterations is less than , we claim the RL converges under this termination condition. When the termination condition has reached, we ﬁx the ﬁltering threshold in the following GNN training iterations. To combine the relation-level attention and reinforced neighbor sampler into a uniﬁed framework, we propose our Reinforced Attentive Multi-relational Graph Neural Network (RAM-GNN) to generate node embeddings. After the RNS module, we select the top-k similar neighbors for each item, so the next step is to aggregate all the ﬁltered neighbors to learn the item embeddings. Since we have taken the relation type into account in the relation-level attention module, the aggregation process is deﬁned as: where l is the number of layer in GNN and j is the selected neighbors from RNS. The aggregation layer can be stacked layer-to-layer and construct a deep neural network. The loss function of the RAM-GNN is composed of two parts: GNN loss and similarity loss. The GNN loss serves for training all parameters in the model, while the similarity loss is mainly designed for helping RNS. The GNN is also trained via negative sampling, and the loss function is: where V is the set of all nodes in the graph as we need to go through the whole graph to generate node embeddings. The ﬁnal loss function of RAM-GNN is the combination of GNN loss, similarity loss, and regularization terms, whose formula is: where λ, λare weight hyperparameters and Θ is all parameters in the RAM-GNN. Γ is the iteration number when the RL process converges. All weight matrices in the the pre-training model are randomly initialized and learned in an end-to-end back-propagation training paradigm. After learning the pre-trained item embeddings, we further leverage the user-item interaction graph to ﬁne-tune the user/item embeddings via a GNN. Motivated by [29], we learn the user/item embeddings through layer-to-layer aggregation to incorporate the collaborative relation. Different from previous models, we utilize the learned relation embeddings of the items from the pre-training process. In the aggregation process, we concatenate the relation value embeddings together with the item embeddings to learn a better representation of items. The structure of the recommendation framework is shown in Figure 3. The ﬁrst-order propagation aggregates the item embeddings into user embeddings: where x, x∈ Rrepresent item and user embeddings, respectively. W∈ R, W∈ Rare both trainable weight matrices and dis the length of concatenated embeddings. k is the concatenation operation to combine the relation value embeddings with the pre-trained item embeddings. Note that the number of emay be more than one. As such, we concatenate the embeddings of all relation values. With ﬁrstorder propagation, the user embeddings can incorporate the embeddings of interacted items and their attributes. The second-order propagation aggregates the user embeddings into item embeddings. The process is similar to the item side aggregation, which is: In this way, we can further ﬁne-tune the pre-trained embeddings with the collaborative relation to make the embeddings more comprehensive. After the second-order propagation, Eq. 19 and Eq. 21 can be stacked interchangeably to build a deeper neural network and explore higher-order proximity. Based on user and item embeddings, we utilize a widely used BPR loss [30] to train the model. The objective function is as follows: L(y, ˆy) = − ln σ(y− ˆy) + λkΘ where Θis all trainable parameters in the GNN model, kΘk is the Lnormalization of all trainable parameters, and λ is the regularization coefﬁcient. The recommendation process is also trained by the back-propagation algorithm. In this section, we conduct experiments on two real-world datasets to evaluate the performance of our proposed RAMGNN model and the whole pre-training framework. We aim to answer the following research questions: form compared with other recommender system algorithms? state-of-the-art multi-relational GNN models? and reinforced neighbor sampler? ﬁltering threshold? choices of composition operations, embedding dimensions) when training the model? A. Experimental Settings 1) Datasets: We conduct our experiments on two public datasets: MovieLensand KKBox. lished by GroupLens [31]. The user relation types we used in this dataset are ages, genders and occupations. For item relation types, we use genres, directors, movies, and released years. The graph we build to pre-train the item graph is similar to Figure 1. The user ratings are transformed into binary numbers to indicate implicit feedback. We utilize users’ ages and occupations to construct the user graph. Cup 2018 Challenge. It contains four types of relations, including genres, artists, composers and lyricists. We also use ages and living cities as user features. The statistics of these two datasets are listed in Table II. 2) Evaluation Metrics: Two evaluation metrics are used to measure the performance of our RAM-GNN framework: HR. This is the Hit Ratio (HR) of target items that are in the recommendation lists. MRR. We can measure the performance of the model w.r.t the ranking list of items. Suppose that the model produces a list of items to the user, and the list is ordered by the conﬁdence of the prediction. In this case, a higher MRR score means target items tend to have higher rank positions in the predicted item lists. NDCG. This measures the ranking quality. It normalizes the Discounted Cumulative Gain (DCG) to be between 0 and 1 by Ideal Discounted Cumulative Gain (IDCG). 3) Implement Settings: We implement our model in PyTorch. The number of layers in RAM-GNN is set to 2, and in the recommendation GNN, it is set to 4, according to [29]. As will see later, with the help of pre-training, RAM-GNN with a fewer number of layers on a smaller user-item interaction graph outperforms state of the art GNN based recommendation methods. B. Recommendation Performance (RQ1) To prove the superiority of our proposed RAM-GNN framework, we conduct experiments on two datasets and compare our model with eight baseline methods. In this section, we denote the pre-training and recommendation steps together as the RAM-GNN framework. 1) Compared Methods: We compare the performance of our model with the following eight baselines: Personal Ranking (BPR) as the loss function. model which uses feature interactions to model user preferences. We take the side information as additional input features in both datasets. to model the high-order feature interactions. model which learns the user embedding by aggregating the item embeddings that she has interacted with. that connects items with their features and then learns the embeddings of items that can be used in CF models. systems like NGCF [27] to make them linear models and achieve better performance. explore high-quality negatives via reinforcement learning. We take features of users and items as nodes in the knowledge graph. siders the multi-relational pattern in the items. It utilizes the attention mechanism to combine the multi-relational pattern with the collaborative relation. 2) Results: The experimental results are presented in Table III. From these results, RAM-GNN clearly outperforms all baselines. We summarize the following observations. mance on both datasets. RCF is the state-of-the-art recommendation model among all baselines, primarily because it considers the multi-relational pattern in the data. Our model can not only take multi-relational patterns into consideration but also incorporate the graph structure to build the connections at the user-level and item-level. baselines except RCF and RAM-GNN. This proves that the KG can improve the results. However, considering multi-relational patterns is more important than only modeling on the KG because the performance of RCF is better than KGPolicy. Also, KGPolicy may suffer from the over-smoothing problem, which can hinder its performance. LightGCN, which proves the effectiveness of the pretraining module. Without RAM-GNN in the pre-training step, our model is similar to LightGCN. The pre-training module incorporates the relations between users and items into the recommendation model so that it can be an effective supplement to the collaborative ﬁltering model. C. Pre-training Performance (RQ2) To measure the difference of RAM-GNN with other multirelational GNN models, we compare our model with ﬁve stateof-the-art GNN models. In this section, we only substitute the RAM-GNN in the pre-training step with other models but still use the GNN model in the recommendation step. For the models which cannot learn the embeddings of the item features, we only use the item embeddings in the recommendation step. 1) Compared Methods: We compare the performance of our model with the following ﬁve baselines: Since it is not designed for multi-relational graphs, we only pick one type of relation for each graph in the pretraining. employing different weight matrices to process multiple types of relations. able relational-speciﬁc scalar weight in the GCN model. learns the embeddings of relations in the GCN framework. Each relation is represented as a vector. (CompGCN) employs KG algorithms in GCN models. The relations are initialized as vectors, and the KG algorithm (TransE) is applied to learn the embeddings of nodes and relations. 2) Results: The results comparing pre-training models are shown in Table IV. From the table, we note the following observations: the experiments. This proves the effectiveness of our proposed RAM-GNN. Also, CompGCN outperforms all other baselines on both datasets, indicating that KG algorithms contribute to learning the embeddings. dataset than on the MovieLens. This indicates that RAMGNN has a better capability in handling large graphs. We infer the reason can be the attention layers in RAM-GNN. When the graph becomes larger and denser, the attention mechanism can help to understand the importance of each node and relation, which is proven useful when dealing with complex data [39]. D. Ablation Study (RQ3) In this section, we measure the effectiveness of the pretraining, as well as the node and relation-level attention layers. We compare our model with four ablation models: 1) Single, which does not the pre-training step. 2) RAM-RNS, which only has the reinforced neighbor sampler in the pre-training model. 3) RAM-rel, which only has the relation-level attention layer during pre-training, and 4) RAM-GNN, which is the complete model we propose. Figure 4 displays the comparison between the ablation models. From Figure 4, we have the following observations: model, proving the effectiveness of the node-level and relation-level attention layers. In the best performing RAM-GNN, there can be multiple relations between two entities. If we treat all types of relations equally in the model, it will miss the difference in the meanings of relations. Besides, the node-level attention layer speciﬁes different weights for different nodes in the neighborhood to improve the aggregation quality. Also, the better performance of RAM-GNN compared to a single model shows the importance of pre-training. clearly better than RAM-rel, while their performance is much closer on MovieLens. This phenomenon may result from the complexity of the user and item graph. When the graph is denser, there can be more nodes connected to the central node. With RNS, the model can emphasize the most similar nodes and reduce redundant information. E. Filtering Threshold Analysis (RQ4) In this section, we discuss the ﬁltering threshold learned in the RNS module. The experimental results are shown in Figure 5. We choose user ages and item genres as examples for both datasets. The initial threshold is set as 5, and  is 2. From the results in Figure 5, we observe that the ﬁltering thresholds on MovieLens dataset converges when k = 9 and k = 13 for the user and item graph, while on KKBox they converges when k = 29 and k = 37. Because KKBox has more users and items compared to MovieLens, a larger ﬁltering threshold can ensure the GNN gets enough information in learning the node embeddings. F. Hyperparameter Analysis (RQ5) In this section, we conduct experiments on two kinds of hyperparameters: composition operations and embedding dimensions. We compare different hyperparameters and summarize the best settings for our model. 1) Composition Operations: We compare the three composition operations mentioned in Section 2.2.1. The three operations are addition (add), multiplication (mul), and circularcorrelation (corr). Based on the RAM-GNN framework, the experimental results are listed in Table V. From the results, we observe that corr has the best performance overall. Therefore we utilize corr when conducting the experiments in the previous sections. However, the results on these three operations are similar. The largest difference between operations in Table V is only 3.03%. Therefore, add and mul are also worth trying during practical use because these operations are simpler to compute, and they can also achieve good performance. 2) Embedding Dimensions: We vary the embedding dimensions from 20 to 100 to see the tendency of the change in predictive performance. The experimental results are displayed in Table VI. From the results, we observe that the optimal embedding dimension is about 60-80. Also, the optimal embedding dimension on KKbox is larger than on MovieLens. This may because the KKbox dataset is much larger than the MovieLens dataset. When training the model, the larger dataset has a stronger ability to optimize embeddings with higher dimensions. GNNs have proven to be useful in different areas [23], [40]–[45]. There also exists a rich literature utilizing the graph structures in data to provide recommendations. Among them, there are two main directions about which graph to use. One direction is to use the user-item bipartite graph to derive recommendations. Among them, [46]–[48] directly perform convolution operations to explore interactions between users and items. [29], [34] leverage layer-to-layer aggregation functions to capture the high-order connections. [49] models user-item interactions with a dynamic graph. These methods apply GNN on the user-item interaction graph from different aspects. The GNN structure has advantage of representing high-dimensional graph data into low-dimensional embeddings without feature engineering, so it is suitable to be directly implemented on the user-item interaction graph. However, the interaction graph ignores the rich features of users and items. The other direction is to use the KG to provide recommendations. This direction contains two different categories of approaches as well. One category is non-GNN models. Many early studies [1], [35], [50]–[52] derive embeddings from KGs via optimization methods and utilize the embeddings in downstream collaborative ﬁltering/prediction steps. These methods are more efﬁcient than recent GNN-based approaches, since they only use the KG embeddings as auxiliary information. However, they tend to be less effective because they fail to incorporate KG information into the end-to-end architecture. The other category is GNN-based models, which tend to apply GNN on KG to derive representations for all nodes (and even relations) [8]–[10], [53]. Besides, [2], [54], [55] construct a heterogeneous graph containing users, items and baskets as three kinds of nodes. These approaches integrate information from both the collaborative bipartite graph and the user and item features, and thus achieve better performance. However, dealing with different relations is still challenging for these models, since GNNs are naturally designed for homogeneous graphs. Currently, Graph Neural Networks (GNNs) have been widely explored to process graph-structured data. Motivated by convolutional neural networks, Bruna et al. [56] propose graph convolutions in the spectral domain. Then, Kipf and Welling [36] simpliﬁed the previous graph convolution operation and designed a Graph Convolutional Network (GCN) model. To inductively generate node embeddings, Hamilton et al. proposed the GraphSAGE [57] model to learn node embeddings with sampling and aggregation functions. All these models have demonstrated their superior performance on many tasks, e.g., link prediction and node classiﬁcation. When the graph has multiple kinds of relations [58] between a pair of nodes, it forms a more complex graph structure, and we call it multi-relational graph. There are some traditional methods handling multi-relational graph including TransE, TransR, DistMult. TransE [16] embeds entities and relations following the translational principle. TransR [59] extends TransE by separating different spaces for entities and spaces. DistMult [19] represents relations as diagonal matrices. These methods have competitive performance on different tasks, but have limitation in recovering missing facts in the multirelational graph. After the appearance of GNNs, many studies extended them to deal with multi-relational graph. Relational GCN [17] is the ﬁrst work that learns node embeddings from multi-relational graphs. For each kind of relation, it utilizes a weight matrix to represent the relation. It demonstrates that R-GCN is able to handing the missing fact issue. Based on this work, weighted GCN [37] adds a trainable weight on each relation in the GCN model. Vectorized Relational GCN (VR-GCN) [38] learns node embeddings as well as relations embeddings with the knowledge base algorithm TransE [16]. Composition-based Relational GCN (CompGCN) [18] is more ﬂexible because it can apply knowledge base algorithms when learning node and relation embeddings. All the models above have a common limitation in that they cannot deal with the graphs that have several relations between two nodes. In this paper, we introduce a model, Reinforced Attentive Multi-relational Graph Convolutional Network (RAM-GNN), to pre-train the user graph and item graph. RAM-GNN has two main modules. First is relation-level attention which calculates the importance of different types of relations. The second one is Reinforced Neighbor Sampler (RNS), which searches the most similar neighbors iteratively to reduce redundant information and improve efﬁciency. From the pre-training step, we learn the user and item embeddings. With another GNN for the user-item interaction graph, we inject the knowledge learned from pre-training to the collaborative ﬁltering model and then make recommendations. Experimental results show that our proposed RAM-GNN also has the best performance, as compared to other multi-relational GNNs. On top of the relations we present in this work, there are some other potential relations among users and items to be explored in the pre-training step, e.g. time dependency of interactions and social relations in users. In the future, we will try to extend our model on sequential recommendation and social recommendation to utilize more kinds of relations and improve performance. The authors of this paper were supported by the S&T Program of Hebei through grant 21340301D, in part by NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.