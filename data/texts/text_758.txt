By enabling efﬁcient and scalable distribution of d ata labelling microta sks, crowdsourcing platforms are a natural choice for dataset developers aiming to cheaply and efﬁciently g enerate dataset annotations. In this short survey paper we explore the inheren t challenges and decision points that stem from crowdsourced dataset annotation. In particular, we ask: w ho is annotating the data, and why is that important? We consider how the ethical concerns of data annotation intersect with the id entities of the annotators, the social structures surrounding their work, and how their ind ividual perspectives may become encoded within the dataset labels. Data generated in crowdwork tasks is shaped by a range of social factors and the datasets that workers help to build continue to sh ape systems long a fter worker engagement ends. We argue that this impacts future models built from this data, and that understanding the perspectives captured within datasets is crucial to understanding resulting mod els and their potential social impact. Our work c omplements and extends prior scho larship examining ethical considerations r e la ting to crowdsourcing (Vakharia and Lease, 2015; Schlagwein et al., 2019; Kocsis an d de Vreede, 2016; Shmueli et al., 2021). Our work is distinct from previous sch olarship in that we focus our attention on unresolved ethical problems in crowdsourcing that relate speciﬁcally to individual worker subjectivity and individual worker experience s. We start by outlining a comprehensive set of concerns regarding how annotators’ individual and collective social experiences, as well as their working Equal contribution; authors listed alphabetically. Proceedings of NeurIPS 2021 Workshop on Data-Centric AI. Emily DentonMark DíazIan Kivlichan Google ResearchGoogle ResearchJigsaw Vinodkumar PrabhakaranRachel Rosen vinodkpg@google.comrachelrosen@google.com Human annotations play a crucial role in machine learning (ML) research and development. However, the ethical consideratio ns around the processes and decisions that go into building ML datasets has not received nearly enough attention. In this paper, we survey an array of literature that provides insights into ethical considerations around crowdsourced dataset annotation. We synthesize these insights, and lay out the challenges in this spac e alon g two layers: (1) who the annotator is, and how the annotators’ lived experiences can impact their annotatio ns, and (2) the relationship between the annotators and the crowdsourcing platfo rms and what that relationship affords them. Finally, we pu t forth a concrete set of recommendations and considerations for d ataset developers at various stages of the ML da ta pipeline: task formulation, selection of annotators, platform and infrastructure choice s, dataset analysis and evaluation, and dataset documentation and release. conditions, may impact the nature of the data they provide for machine learning development, in particular the biases that may be captured in and propaga te d through those datasets. Based on this analysis, we offer a set of e thical considerations and r ecommen dations for dataset developers that apply to different steps of a typical data annotation pipeline, from task formulation to dataset release. Recent empirical work has revealed that relatively little attentio n is given or documented about annotator positionality—how annotator social identity shapes their understanding of th e world. (Geiger et al., 2020; Sche uerman et al., 2021). Crowd workers are of te n selected by task requesters based on quality metrics, rather than on any socially deﬁning features of their knowledge or exper ience. This is concerning, since crowd-sourced annotations are often used to build datasets capturing subjective phenomena such as sentiment and hate-speech, and hence crowd workers’ values and subjective judgments shape th e perspectives that machine learning models learn from in a manner that is wholly unaccounted for. Indeed, crowdsourcing platforms are often explicitly designed in a manner that positions crowdworkers as interchangeable (Irani and Silbe rman, 2013). Accounting for the socio-cultural backgrounds of dataset ann otators is important for at lea st two reasons. First, subjective interpreta tions of a task can produce d ivergent annotations acr oss different communities (Sen et al., 2015). As Aroyo and Welty (2015) argue, the notion of “one truth” in crowdsourcing responses is a myth; disagreement between annotators, which is often viewed as negative, can actually provide a valuable signal. Secondly, since many crowdsourced annotator po ols are socio-demograph ic a lly skewed, there are implications for which populations are r e presented in datasets as well as which populations face the challenges of crowdwork (Irani and Silberman, 2013; Gray and Suri, 2019). Accounting for skews in annotator demographics is critica l for contextualizing datasets and ensuring responsible downstream use. In short, there is value in acknowledging, and accounting for, worker’s socio-cultu ral background—both from the perspective of data quality and societal impact. Accounting for lived experiences of annotators as expertise may be of great utility in some cases. Just as sub stantive work experience le nds valuable domain expertise for a given problem (e.g. annotation of medical imagery by a medical profe ssional), lived experience with, and proximity to, a problem domain can provide a valuable source of expertise for dataset annotation. For example, women experience higher rates of sexual harassment online compared to men, and among those who have experience d online abuse, women are more likely to identify it as such (Vogels, 2021). However, such lived expe riences do not always fall alo ng demographic lines. Waseem (201 6) demonstrated that incorporating fem inist and antiracist activists’ perspectives into hate speech annotations yielded b e tter aligned models. Similar ly, Patton et al. (2019) demonstrated the importance of situated domain expertise — including contextualize d knowledge of lo c al language, concepts, and gang activity — when annotating Twitter images to detect pathways to violence among gang-involved youth in Chicago. In summary, a core question to answer in data collection is how much annotator subjectivity matters for the task at hand, and how it impacts what the resulting dataset is meant to captu re. While we used relatively subjective tasks as examples above, even seemingly objective tasks such as annotating medical texts vary sur prisingly with a nnotator backgrounds and expe rience (Aroyo and Welty, 2015). Another layer of considerations relate to annotators’ expe riences with ann otation work itself and how it can impact how they do their work. These include issues related to worker comp ensation, imbalances in the relationship between worker and requester, and the structure of annotation work itself — all of which can pose barriers to cr owdworker well-being and their ability to produce quality work. Compensation policies of the platforms shou ld be a c ore aspect to consider when thinking ab out responsible data collection. For instance, in the U.S., there are currently no regulations aro und worker pay for crowdwork (Berg, 2015), a nd the Fair Labor Standards Act that established the minimum wage, 2018). Moreover, for every hour of paid work, workers spend another 18 minutes on u npaid work, including searching for tasks ( Berg, 2015). Time spent working is c ompounded by com petition from other crowdworkers (Semuels, 2018), which can pressure workers to be constantly available to look for work as well as work longer hours ( Berg, 2015). In addition, a large majority of crowdworkers (94% as per (Berg, 2015)) have had work that was rejected or for which they were not paid. Yet, requesters retain full rights over the data they receive regar dless of whether they accept or reject it; Roberts (2016) describes th is system as one that “enables wage theft”. Moreover, rejecting work and withholding pay is painf ul because rejections are often caused by unclear instructions and the lack of meaningful feedback channels; many crowdworkers report that poor communication negatively affects their work (Berg, 201 5). Power dynamics between the requesters and annotators is another major challenge. To p-down organizationa l structures ofte n results in the workers viewing requesters as more informe d as they are the ones who provided the data and the label schema (Miceli et al., 2020). Hence, instead of resolving ambiguities, workers are more likely to try to judge from the standpoint of the requester, often with limited exposure to the goals of the an notation. This contributes to the portability trap (Selbst et al., 2019): a “failure to understand how r epurposing algorithmic so lutions designed for one social context may be misleading, inacc urate, or otherwise do harm when applied to a different context.” Power asymmetries also reﬂect global power dynamics. For instance, since technology development hap pens primarily in the West, human comp utation from the Global South is often relegated to the margins (Sambasivan et al., 2021). In summary, a core consideration for responsible data collection is whether there exist mechanisms for the workers to address these power a symmetries. The anonymous and geogr aphically distributed natur e of crowdsourced annotation work imposes signiﬁcant barriers to collective action. In response, several community forums h ave been d eveloped indep e ndently from crowdwork platforms to support crowd workers, e.g. TurkerNation, Turk Alert, MTur kGrind, and Reddit’s /r/HITsWorthTurkingFor. Turkopticon (Irani and Silberma n, 2013; tur, [n.d.]) and Dynamo (Salehi et al., 2 015) have also emerged as activist tools that sup port and enable collective action for crowdworkers. We now outline a com prehensive set of con siderations for the collection, use, and dissemination of crowd-sourced ML datasets. We discuss them as they apply to different parts of a typical dataset construction pipeline, from the formulation of tasks to dissemination of datasets. Task formulation: A core objective o f constructing ML datasets is to capture the a spects of human intelligence that a re of importance to a given task. While some tasks tend to pose objective questions with a correct answer (is there a human face in an image?), oftentimes datasets aim to capture judgement on relatively subjective task s with no universally correct answer (is this piece of text offensive?). It is important to be intentional about whether to lean on annotato rs’ subjective judgements. This determinatio n should be tied to the purpose of dataset creation a nd the downstream use cases it is meant to serve, rather than what is convenient, efﬁcient, or scalable. Not acc ounting for task subjectivity may lead to inadvertent biases and misses critical in sig hts about tasks that could beneﬁt from the annotators’ lived experiences. However, even for relatively subjective tasks suc h as labeling offensiveness, a dataset developer may want to restrict the annotators from re lying on their lived experiences, say, if th e dataset is meant to capture a set of policies d eﬁned by a platform. Clarifying such aspects o f the tasks, has ramiﬁcations for how successfully the datasets capture the aspects of human intelligence they are meant to capture. • Consider the subjective nature of your annotation task. Is it possible that individuals with • Consider the forms of expertise that should be incorporated through data annotation, inhttps://www.dol.gov/agencies/whd/ﬂsa is not applicable for crowdworkers as th ey are in dependent contractors ( Semuels, different social and cultural backgrounds might differ in their judgements? cluding both formal disciplinary train ing and lived experience with the problem domain. What are the risks of this expertise not being reﬂected in the annotator pool? • Make sure task instructions are clear and unambiguous in order to prevent annotators from • Consider how the ﬁnal dataset annotations will relate to individual annota tor responses. Selecting annotators: As outlined in Sectio n 2, the selection of an anno tator pool is a highly consequential decision, especially given the subjective nature of many annotation tasks. It is important to choose annotation platforms that allow ﬂexibility in designing custom annotator pools along various socio-dem ographic axes. These decisions should ideally be guided by considering which communities will be most impacted by models built from the data, and which commun ities could be harmed the most if they are not repre sented in the annotator pool. • While there is no single “correc t” way to assemble an annotator p ool, the decisions in this • Consider the intended usage contexts of the dataset, and the marginalized communities • Consider how labor practices intersect with the choice of who the annotators are. For Platform and infrastructure choices: As described in Section 2, the p la tform p olicies around compensation and power asymmetries play a huge role in the quality of work the annotators produce. Some platforms offer platform-mediated channels of communication that allow task req uesters to incorporate annotator feedback into the task fr aming or a nnotator guideline s. Different platforms also impose different minimum-pay constraints; requesters may want to support platforms that uphold fair pay standards. Separately f rom the platform, task creators should be aware of worker pay per hour, since this number is not often given explicitly. Some platforms may only offer requesters the option to select pay per item for an annotation task, and the defaults may be set low: task creators should take care wh en estimating work time per item to ensu re they are paying workers fairly. • Consider platform’s underlying annotator pool and the op tions they pr ovide to source spe- • Consider comparing and contrasting the minimum pay requirements estab lish e d acro ss dif- • Consider the extent to which you would like to establish a channel of communica tion an d Dataset analysis and evaluation: A common practice in building crowdsourced annotations is to obtain multiple a nnotator judgements that are then aggregated (e.g., through majo rity voting) to obtain a single “ground truth” that is released in the dataset (Sabou et al., 2014). However, the disagreements between ann otators may embed valuable nuances about the task (Ovesdotter Alm, 2011; Aroyo an d Welty, 2013). Aggregation, in such cases may obscure such nuances, and in that process potentially exclude perspectives from min ority annotators (Prabh akaran et al., 2021). • Consider including the uncertainty or disagreement between ind ividual anno ta tions on each • Consider analyzing systematic disagreements between annotators of different socio- Dataset documentation and release: Rigorous documentation of design dec isions and ou tc omes relating to dataset annotations is an importan t aspect of respo nsible d a ta set development. Several dataset d ocumenta tion frameworks have been proposed to contextualize a dataset and offer guidance regarding intended or unintended use, as well as facilitate accountability fo r development decisions. wasting time on a task where their work will be rejected due to misunderstandings. For instance, will you release only the aggregated labels, e.g. through a majority vote? Consider what valuable information might be lost through such aggregation. stage could impact the biases captur e d in the resulting dataset. For instance, annota tor demographics may serve as a form of expertise that is impor ta nt for the task (cf . Sec tion 2). therein, when choosing which ann otators to be prioritized to be inclu ded. example: if female annotators make up the majority as they do in the U.S. (Posch et al., 2018), consider how fair payment, or the lack thereof, could impact this group. cialized rater pools, and whether they e nable you to curate an ap propria te pool of annotator s (e.g. considering sociod emographic factors or domain expertise). ferent platforms. You may choose to support a p la tform that upholds fair pay standard s. feedback between your team and the annotators. Platform mediated channels of communication can give annotators an opportunity to provide fe edback on conf using instructio ns. instance as a signal in the dataset. demographic groups in order to better unde rstand how diverse perspectives are represented. • Consider adopting or adapting an existing dataset documentation framework • Consider including individual annotator responses for each data points in the dataset in (e.g. Gebru et al. (2020); Holland et al. (2018); Bender and Friedman (2018); Kazimzade and Miceli (20 20); Hutchinson et al. (2021)) to guide your data set documentation. Consider publishing aggregate statistics on th e sociodemographic make-up of your annotator pool. addition to a ﬁnal aggregated ground tru th label, where a pplicable.