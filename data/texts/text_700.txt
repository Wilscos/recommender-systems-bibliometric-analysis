Personalized news recommendation has been widely adopted to improve user experience. Recently, pre-trained language models (PLMs) have demonstrated the great capability of natural language understanding and the potential of improving news modeling for news recommendation. However, existing PLMs are usually pre-trained on general corpus such as BookCorpus and Wikipedia, which have some gaps with the news domain. Directly ﬁnetuning PLMs with the news recommendation task may be sub-optimal for news understanding. Besides, PLMs usually contain a large volume of parameters and have high computational overhead, which imposes a great burden on the low-latency online services. In this paper, we propose Tiny-NewsRec, which can improve both the effectiveness and the efﬁciency of PLM-based news recommendation. In order to reduce the domain gap between general corpora and the news data, we propose a self-supervised domain-speciﬁc post-training method to adapt the generally pre-trained language models to the news domain with the task of news title and news body matching. To improve the efﬁciency of PLM-based news recommendation while maintaining the performance, we propose a two-stage knowledge distillation method. In the ﬁrst stage, we use the domainspeciﬁc teacher PLM to guide the student model for news semantic modeling. In the second stage, we use a multi-teacher knowledge distillation framework to transfer the comprehensive knowledge from a set of teacher models ﬁnetuned for news recommendation to the student. Experiments on two real-world datasets show that our methods can achieve better performance in news recommendation with smaller models. With the explosion of information, massive news are published on online news platforms such as Microsoft News and Google News (Das et al. 2007; Lavie et al. 2010), which can easily get the users overwhelmed when they try to ﬁnd the news they are interested in (Okura et al. 2017). To tackle this problem, many news recommendation methods have been proposed to provide personalized news feeds and alleviate information overload for users (Wang et al. 2018; Wu et al. 2019b; Zhu et al. 2019; Hu et al. 2020). Since news articles usually contain abundant textual content, learning accurate news representations from news texts is the prerequisite for Copyright © 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. high-quality news recommendation (Wu et al. 2020). Most existing news recommendation methods use shallow NLP models to learn news representations. For example, An et al. (2019) propose to use a CNN network to learn contextual word representations by capturing local context information and use an attention network to select important words in news titles. Wu et al. (2019c) propose to use a multi-head self-attention layer to capture the global relation between words in news titles, and also use an attention network to compute the news representation. However, it is difﬁcult for these shallow models to accurately capture the deep semantic information in news texts (Devlin et al. 2019), which limits their performance on news recommendation. Pre-trained language models (PLMs) are powerful in text modeling and have empowered various NLP tasks (Devlin et al. 2019; Liu et al. 2019). A few recent works delve into employing PLMs for news understanding in news recommendation (Wu et al. 2021a; Xiao et al. 2021; Zhang et al. 2021). For example, Wu et al. (2021a) propose to replace these shallow models with the PLM to capture the deep contexts in news texts. However, these methods simply ﬁnetune the PLM with the news recommendation task, the supervision from which may not optimally train the PLM to capture semantic information in news texts and may be insufﬁcient to solve the domain shift problem. Moreover, PLMs usually have a large number of parameters (Lan et al. 2020). For example, the BERT-base model (Devlin et al. 2019) contains 12 Transformer layers (Vaswani et al. 2017) and up to 110M parameters. Deploying these PLM-based news recommendation models to provide low-latency online services requires extensive computational resources. In this paper, we propose a Tiny-NewsRec approach to improve both the effectiveness and the efﬁciency of PLMbased news recommendation. In our approach, we design a self-supervised domain-speciﬁc post-training method to adapt the generally pre-trained language models to the news domain with the task of news title and news body matching. In this way, the domain-speciﬁc PLM-based news encoder can better capture the semantic information in news texts and generate more discriminative representations, which are beneﬁcial for news content understanding and user inter- The source codes of our Tiny-NewsRec method are available at https://github.com/yﬂyl613/Tiny-NewsRec. est matching in the following news recommendation task. In addition, we propose a two-stage knowledge distillation method to compress the large PLM while maintaining its performance. In the ﬁrst stage, the student PLM is forced to mimic the domain-speciﬁcally post-trained teacher PLM in the matching task between news titles and news bodies to learn news semantic modeling. In the second stage, the domain-speciﬁc teacher PLM is ﬁrst ﬁnetuned with different random seeds on the news recommendation task to obtain multiple task-speciﬁc teachers. Then we propose a multiteacher knowledge distillation framework to transfer taskspeciﬁc knowledge from these teacher models to the student model. Since different teachers may have different abilities on different samples, for each training sample, we assign different teachers with different weights based on their performance on this sample, which allows the student model to learn more from the best teacher. Extensive experiment results on two real-world datasets show that our approach can reduce the model size by 50%-70% and accelerate the inference speed by 2-8 times while achieving better performance. The main contributions of this paper are as follows: • We propose a Tiny-NewsRec approach to improve both the effectiveness and the efﬁciency of PLM-based news recommendation. • We propose to domain-speciﬁcally post-train the PLMbased news encoder with a self-supervised matching task between news titles and news bodies before task-speciﬁc ﬁnetuning to better ﬁll the domain gap. • We propose a two-stage knowledge distillation method with multiple teacher models to compress the large PLMbased news recommendation model while maintaining its performance. • Extensive experiments on two real-world datasets validate that our method can effectively improve the performance of PLM-based news recommendation models while reducing the model size by a large margin. PLM-based News Recommendation With the great success of pre-trained language models (PLMs) in multiple NLP tasks, many researchers have proposed to incorporate the PLM in news recommendation and have achieved substantial gain (Xiao et al. 2021; Zhang et al. 2021; Wu et al. 2021a). For example, Zhang et al. (2021) proposed a UNBERT approach, which is a BERTbased user-news matching model. It takes in the concatenation of the user’s historical clicked news and the candidate news, and uses the PLM to capture multi-grained user-news matching signals at both word-level and news-level. Wu et al. (2021a) proposed a state-of-the-art PLM-based news recommendation method named PLM-NR, which instantiates the news encoder with a PLM to capture the deep semantic information in news texts and generate high-quality news representations. However, these methods simply ﬁnetune the PLM with the news recommendation task, the supervision from which may be insufﬁcient to ﬁll the domain We focus on task-speciﬁc knowledge distillation. gap between general corpora and the news domain (Gururangan et al. 2020). Besides, the PLMs are usually with large parameter sizes and high computational overhead (Lan et al. 2020). Different from these methods, our approach can better ﬁll the domain gap with an additional domain-speciﬁc post-training task and further reduce the computational cost with the two-stage knowledge distillation method. Knowledge distillation is a technique that aims to compress a heavy teacher model into a lightweight student model while maintaining its performance (Hinton, Vinyals, and Dean 2015). In recent years, many works explore to compress large-scale PLMs via knowledge distillation (Tang et al. 2019a; Mirzadeh et al. 2020; Wang et al. 2020; Sun et al. 2020; Xu et al. 2020). For example, Tang et al. (2019b) utilized the output soft label of a BERT-large model to distill it into a single-layer BiLSTM. Sun et al. (2019b) proposed a patient knowledge distillation approach named BERT-PKD, which lets the student model learn from both the output soft labels from the last layer of the teacher model and the hidden states produced by intermediate layers. Sanh et al. (2020) proposed a DistilBERT approach, which distills the student model at the pre-training stage with a combination of language modeling, distillation, and embedding cosinedistance losses. Jiao et al. (2020) proposed a TinyBERT approach, which lets the student model imitate the output probabilities, embeddings, hidden states, and attention score matrices of the teacher model at both the pre-training stage and the ﬁnetuning stage. There are also a few works that aim to distill the PLM for speciﬁc downstream tasks (Lu, Jiao, and Zhang 2020; Wu et al. 2021b). For example, Wu et al. (2021b) proposed a NewsBERT approach for intelligent news applications. A teacher-student joint learning and distillation framework is proposed to collaboratively learn both teacher and student models. A momentum distillation method is also designed to incorporate the gradients of the teacher model into the update of the student model which can better transfer the useful knowledge learned by the teacher model. However, these knowledge distillation methods neglect the potential domain gap between the pretraining corpora and the downstream task domain. Besides, they only use one teacher model to guide the training of the student model, which may provide insufﬁcient or even biased supervision (Wu, Wu, and Huang 2021). Therefore, we ﬁrst use a large domain-speciﬁcally post-trained teacher model to help the student model better adapt to the news domain. Then we use a multi-teacher knowledge distillation framework to transfer richer knowledge from a set of ﬁnetuned teacher models to the student. In this section, we introduce the details of our TinyNewsRec approach, which can ﬁll the domain gap between general corpora and the news domain, and distill the large PLM for news recommendation applications. We ﬁrst introduce the structure of the PLM-based news recommendation model. Then we introduce the self-supervised matching Figure 1: Structure of our PLM-based news task between news titles and news bodies used for domainspeciﬁc post-training. Finally, we introduce the workﬂow of our two-stage knowledge distillation method with a multiteacher knowledge distillation framework. News Recommendation Model We ﬁrst introduce the overall structure of our PLM-based news recommendation model. As shown in Fig.1, it consists of three major components, i.e. a shared PLM-based news encoder, a user encoder, and a click prediction module. The shared news encoder aims to learn news representations from news texts. Following PLM-NR (Wu et al. 2021a), we use a PLM to get the contextual representation of each token in the input news. Then we use an attention network to aggregate these representations and feed its output into a dense layer to get the ﬁnal news representation. The user encoder aims to learn the user representation u from the representations of the last L news clicked by the user, i.e. [h, h, ..., h]. Following Wu et al. (2019a), we implement it with an attention network to select important news. In the click prediction module, we use dot product to calculate the relevance score between the candidate news representation hand the target user representation u, and take it as the predicted result, i.e. ˆy= hu. Domain-speciﬁc Post-training Since the supervision from the news recommendation task may not optimally train the PLM to understand the news content, directly ﬁnetuning the PLM on the news recommendation data may be insufﬁcient to ﬁll the domain gap between general corpora and the news corpus (Gururangan et al. 2020). In order to better adapt the PLM to the news domain, we propose to conduct domain-speciﬁc post-training before ﬁnetuning it with the news recommendation task. We design a self-supervised matching task between news titles and news bodies which can make the PLM-based news encoder better at capturing and matching semantic information in news texts. Given a pair of news title and news body, the news encoder is trained to predict whether they come from the same news article. The model structure for this task is shown in the right half of Fig.2(a). The architecture of the PLM-based news encoder is the same as that in Fig.1. Following previous works (Huang et al. 2013; Wu et al. 2019a), we adopt the negative sampling method to construct the training samples. Given the i-th news body, we take its corresponding news title as the positive sample and randomly select N other news titles as negative samples. We use the PLM-based news encoder to get the news body representation hand the news title representations h= [h, h, ..., h]. Then we take the dot product of the news body representation and each news title representation as the predicted score, which is denoted as [ˆy, ˆy, ...ˆy]. These predicted scores are further normalized with the softmax function and the predicted probability of the positive sample is formulated as follows: To maximize the predicted probability of the positive sample, we use the Cross-Entropy loss as the loss function, which can be formulated as follows: where T is the set of positive training samples. In this way, the domain-speciﬁcally post-trained PLMbased news encoder can generate more similar representations for related texts and distinguish them from the others, which can alleviate the anisotropy problem of the sentence embedding generated by the PLM (Gao et al. 2019; Ethayarajh 2019; Li et al. 2020). As a result, the news representations generated by the news encoder can be more discriminative and better at capturing semantic similarity, which is beneﬁcial to the user interests matching in the following news recommendation task. Two-stage Knowledge Distillation Although with our proposed domain-speciﬁcally post-train then ﬁnetune procedure, the PLM-based news recommendation model can achieve superior performance, it still has high computational overhead and is difﬁcult to meet the speed requirement of low-latency online services. In order to achieve our goal of efﬁciency, we further propose a two-stage knowledge distillation method. The overall framework is shown in Fig.2. In our method, the student model is ﬁrst trained to imitate the domain-speciﬁcally post-trained teacher model in the matching task between news titles and news bodies. Then we ﬁnetune the domain-speciﬁcally post-trained teacher model with different random seeds on the news recommendation task and use these ﬁnetuned teacher models to guide the ﬁnetuning of the student model via a multi-teacher knowledge distillation framework. In the ﬁrst stage, in order to help the student PLM better adapt to the news domain, we use the large domainspeciﬁcally post-trained teacher news encoder to guide the student model in the matching task. The model framework is shown in Fig.2(a). To encourage the student model to make similar predictions as the teacher model in the matching task, we use a distillation loss to force the student model to imitate the output soft labels of the teacher model. Given a piece of news body and N + 1 news titles, the soft labels predicted by the teacher model and the student model are denoted asˆy= [ˆy, ˆy, ..., ˆy] andˆy= [ˆy, ˆy, ..., ˆy] respectively. The distillation loss in the ﬁrst stage is formulated as follows: where Tis the temperature hyper-parameter in the ﬁrst stage that controls the smoothness of the predicted probability distribution of the teacher model, and CE stands for the Cross-Entropy loss function. Besides, the learned news title representations and news body representations are very important in the matching task, which will directly affect the ﬁnal predicted score. Therefore we use an embedding loss to align the output representations of the teacher news encoder and the student news encoder. Denote the news title representations and the news body representation learned by the teacher news encoder as h= [h, h, ..., h] and h, and denote these representations learned by the student news encoder as h= [h, h, ..., h] and hrespectively, the embedding loss in the ﬁrst stage is formulated as follows: L= MSE(h, h) + MSE(h, h), where MSE stands for the Mean Squared Error loss function. The overall loss function for the student model in the ﬁrst stage knowledge distillation is the weighted summation of the distillation loss, the embedding loss, and the target loss, which is formulated as follows: where y is the one-hot ground-truth label of the matching task and βis the hyper-parameter that controls the impact of the teacher model in the ﬁrst stage knowledge distillation. In the second stage, in order to transfer more comprehensive knowledge to the student model during ﬁnetuning with the news recommendation task, we propose a multiteacher knowledge distillation framework which is shown in Fig.2(b). We ﬁrst ﬁnetune the domain-speciﬁcally posttrained teacher model with M different random seeds on the news recommendation task. Then these M teacher models are used to guide the ﬁnetuning of the student news encoder that got from the ﬁrst stage knowledge distillation. For each training sample, we assign a weight to the i-th teacher model according to its performance on this sample, which is measured by the Cross-Entropy loss between its predicted score ˆyand the ground-truth label of the input training sample y. The loss is further multiplied with a positive learnable parameter ω which is used to enlarge the difference between teacher models. Denote the weight of the i-th teacher model on a training sample as w, it is formulated as follows: Similar to the ﬁrst stage, we use the distillation loss to force the student model to make similar predictions as the best teacher model on a training sample. Since now we have several teacher models with different weights, we use the weighted summation of all the soft labels of teacher models as guidance. Therefore the distillation loss is formulated as follows: where Tis the temperature hyper-parameter in the second stage. In addition, since the news representation and the user representation are the keys in the news recommendation task, we also let the student model imitate the learned news representations and user representations of teacher models. Considering that the representations learned by each teacher model may lie in different spaces, we use an additional dense layer for each teacher model to project their learned representations into one uniﬁed space. The embedding loss of news representations and user representations between the i-th teacher model and the student model are denoted as Land Lrespectively, which are formulated as follows: L= MSE(W× h+ b, h), L= MSE(W× u+ b, u), where hand hrepresent the news representations of the input historical clicked news and the candidate news learned by the i-th teacher model and the student model respectively. W, band W, bare the learnable parameters used to project the news representations and user representations learned by the i-th teacher model. The total embedding loss is the weighted summation of all the embedding losses of news representations and user representations between the student model and each teacher model,P i.e. L=w· (L+ L). The overall loss function for the student model in the second stage knowledge distillation is also the weighted summation of the distillation loss, the embedding loss, and the target loss, which is formulated as follows: where βcontrols the impact of the teacher models in the second stage knowledge distillation. Datasets and Experiment Settings We conduct experiments with three real-world datasets, i.e. MIND, Feeds and News. MINDis a public dataset for news recommendation (Wu et al. 2020), which contains the news click logs of 1,000,000 users on the Microsoft News website in six weeks. Feeds is also a news recommendation dataset collected on the Microsoft News App from 2020-08-01 to 2020-09-01. We use the impressions in the last week for testing, and randomly sample 20% impressions from the training set for validation. News contains news articles collected on the Microsoft News website from 2020-08-01 to 202009-20, which is used for domain-speciﬁc post-training. Detailed statistics of these datasets are summarized in Table 1. In our experiments, we apply the pre-trained UniLMv2 (Bao et al. 2020) to initialize the PLM in the news encoder. The dimension of the news representation and the query vector in the attention network is 256 and 200 respectively. The temperature hyper-parameters Tand Tare both set to 1. βand βin the loss functions of our two-stage knowledge distillation method are set to 1 and 0.1 respectively. The number of teacher models M is set to 4. We use the Adam optimizer (Kingma and Ba 2015) for model training. The detailed experiment settings are listed in Appendix. All the hyper-parameters are tuned on the validation set. Following Wu et al. (2020), we use AUC, MRR, nDCG@5, and The effectiveness of each part of the loss function is veriﬁed in Appendix. https://msnews.github.io/ We randomly choose 1/4 samples from the training set as our training data due to the limitation of training speed. Table 1: Detailed statistics of MIND, Feeds and News. nDCG@10 to measure the performance of news recommendation models. We independently repeat each experiment 5 times and report the average results with standard deviations. Performance Comparison In this section, we compare the performance of the teacher model PLM-NR-12 (Domain-speciﬁc Pre-train) that trained with our domain-speciﬁc post-train then ﬁnetune procedure, and the student models trained with our Tiny-NewsRec approach with several baseline methods, including: • PLM-NR (Finetune) (Wu et al. 2021a), a method which applies the PLM in the news encoder and directly ﬁnetunes it with the news recommendation task. We compare the performance of its 12-layer UniLMv2 version and its variant using the ﬁrst 1, 2, or 4 layers. • PLM-NR (Further Pre-train) (Sun et al. 2019a), a variant of PLM-NR where we ﬁrst further pre-train the UniLMv2 model with the MLM task (Devlin et al. 2019) on the News dataset and then ﬁnetune it with the news recommendation task. • TinyBERT (Jiao et al. 2020), a state-of-the-art two-stage knowledge distillation method for compressing the PLM. For a fair comparison, we compare the performance of the 1-layer, 2-layer, and 4-layer student models distilled from the PLM-NR-12 (DP). • NewsBERT (Wu et al. 2021b), a PLM knowledge distillation method specialized for intelligent news applications. For a fair comparison, we use the domainspeciﬁcally post-trained 12-layer UniLMv2 model to initialize the PLM in the teacher model and jointly train it with the 1-layer, 2-layer, or 4-layer student model. Table 2 shows the performance of all the compared methods on the MIND and Feeds datasets. From the results, we have the following observations. First, comparing with stateof-the-art knowledge distillation methods (i.e. NewsBERT and TinyBERT), our Tiny-NewsRec achieves the best performance in all 1-layer, 2-layer, and 4-layer student models. This is because in the ﬁrst stage the domain-speciﬁcally post-trained teacher model can help the student ﬁll the domain gap between general corpora and the news domain. Besides, we use a multi-teacher knowledge distillation framework which can transfer richer knowledge to the student model. Second, our Tiny-NewsRec achieves comparable performance with the teacher model PLM-NR-12 (DP). It PLM-NR-12 (DP)70.20±0.10 35.27±0.08 38.54±0.07 44.20±0.08 68.71±0.08 35.10±0.09 38.32±0.06 45.83±0.08 Tiny-NewsRec-470.40±0.05 35.43±0.08 38.76±0.05 44.43±0.04 68.93±0.06 35.21±0.09 38.43±0.08 45.97±0.10 Table 2: Performance comparisons of different models. (FT=Finetune, FP=Further Pre-train, DP=Domain-speciﬁc Post-train) Improvements over other baselines are signiﬁcant at p < 0.01 (by comparing the models with the same number of layers). Figure 3: Impact of different number of teacher models. is noted that Tiny-NewsRec contains much fewer parameters and has lower computational overhead, which validates the effectiveness of our two-stage knowledge distillation method. Third, with the same parameter size, methods applying knowledge distillation (i.e. TinyBERT, NewsBERT, and Tiny-NewsRec) outperform the traditional pretrain then ﬁnetune paradigm (i.e. PLM-NR (FT)). This is because the guidance from the teacher model such as output soft labels can provide more useful information than the ground-truth label. Fourth, PLM-NR-12 (FP) outperforms PLM-NR-12 (FT). This is because further pre-training the UniLMv2 model can make it specialized to the news data distribution, therefore boost its ability of news modeling. Finally, PLM-NR-12 (DP) outperforms PLM-NR-12 (FP). This is because our proposed matching task can help the PLM-based news encoder better at capturing the semantic information in news texts and generate more discriminative news representations, which can effectively help the user interest matching in the following news recommendation task. Effectiveness of Multiple Teacher Models In this section, we conduct experiments to validate the effectiveness of using multiple teacher models in our second stage knowledge distillation. We vary the number of teacher Figure 4: Effectiveness of each stage in our framework. models M from 1 to 4 and compare the performance of the 4-layer student model on the Feeds dataset. The results are shown in Fig.3. From the results, we ﬁnd that the performance of the student model improves with the number of teacher models. This may be because these teacher models usually can complement each other. With more teacher models, the student model can receive more comprehensive knowledge and obtain better generalization ability. In this section, we further conduct several experiments to verify the effectiveness of each stage in our two-stage knowledge distillation method. We compare the performance of the 4-layer student model distilled with our TinyNewsRec approach and its variant with one stage removed on the Feeds dataset. The results are shown in Fig.4. From the results, we ﬁrst ﬁnd that the second stage knowledge distillation plays a critical role in our approach as the performance of the student model declines signiﬁcantly when it is removed. This is because the guidance from multiple teacher models in the second stage such as learned news and We only include results on the Feeds dataset due to space limit. The results on the MIND dataset are in Appendix. Figure 5: Model size and inference speed of the teacher user representations can provide much more useful information than the ground-truth label, which encourages the student model to behave similarly as the teacher models. The complement between these teacher models also enables the student model to have better generalization ability. Second, the performance of the student model also declines after we remove the ﬁrst stage knowledge distillation, and the student model that only learns from the domain-speciﬁcally post-trained teacher model in the ﬁrst stage and ﬁnetunes on news recommendation data alone still outperforms PLM-NR (FT). This is because our matching task used for domainspeciﬁc post-training can better adapt the PLM to the news domain and enable it to generate more discriminative news representations, which can be transferred to the following news recommendation task and boosts the performance of the PLM-based news recommendation model. In this section, we conduct experiments to evaluate the efﬁciency of the student models distilled with our TinyNewsRec approach. As in news recommendation, encoding news with the PLM-based news encoder is the main computational overhead, we measure the inference speed of the model in terms of the number of news that can be encoded per second with a single GPU. The test results and the number of parameters of the 1-layer, 2-layer, and 4-layer student models and the 12-layer teacher model PLM-NR-12 (DP) are shown in Fig.5. The results show that our Tiny-NewsRec method can reduce the model size by 50%-70% and increase the inference speed by 2-8 times while achieving comparable or even better performance. These results verify that our approach can improve the effectiveness and efﬁciency of the PLM-based news recommendation model at the same time. As shown in Fig. 6, we analyze the inﬂuence of two important hyper-parameters, βand βin the loss functions of our two-stage knowledge distillation method on the Feeds dataset. First, we ﬁx βto 1.0 and vary the value of βfrom 0 to 0.3. We ﬁnd that the performance is not optimal when β is close to 0, and a relatively large β(e.g. β> 0.15) also hurts the performance. This may be because in the second stage knowledge distillation, the supervision signals from Figure 6: Inﬂuence of the hyper-parameter βand β. both ﬁnetuned teacher models and ground-truth labels are useful, while those from teacher models are more important. Thus, a moderate selection of βfrom 0.05 to 0.15 is recommended. Then we ﬁx βto 0.1 and vary the value of β from 0.7 to 1.3. We ﬁnd that the model achieves optimal performance when βis set to 1, and the performance declines when we either increase or decrease the value of β. This may be because in the ﬁrst stage knowledge distillation we only use one domain-speciﬁcally post-trained teacher model to guide the student model. The supervision from the teacher model and the ground-truth label may have equal contributions and complement each other. Thus, setting the value of βaround 1 is recommended. In this paper, we propose a Tiny-NewsRec approach to improve the effectiveness and the efﬁciency of PLM-based news recommendation with domain-speciﬁc post-training and a two-stage knowledge distillation method. Before ﬁnetuning, we conduct domain-speciﬁc post-training on the PLM-based news encoder with a self-supervised matching task between news titles and news bodies to make the generally pre-trained PLM better model the semantic information in news texts. In our two-stage knowledge distillation method, the student model can ﬁrst adapt to the news domain with the guidance from the domain-speciﬁcally post-trained teacher model. Then a multi-teacher knowledge distillation framework is used to transfer task-speciﬁc knowledge from a set of ﬁnetuned teacher models to the student during ﬁnetuning. We conduct extensive experiments on two real-world datasets and the results demonstrate that our approach can effectively improve the performance of the PLM-based news recommendation model with considerably smaller models. In the future, we plan to deploy our Tiny-NewsRec in online personalized news recommendation service to verify its online performance. Besides, comparing with these singleteacher knowledge distillation methods, our approach will introduce additional training cost in order to train multiple teacher models. We are also interested in reducing the training cost of our two-stage knowledge distillation method while keeping its performance.