Counterfactual explanation methods interpret the outputs of a machine learning model in the form of "what-if scenarios" without compromising the delity-interpretability trade-o. They explain how to obtain a desired prediction from the model by recommending small changes to the input features, aka recourse. We believe an actionable recourse should be created based on sound counterfactual explanations originating from the distribution of the ground-truth data and linked to the domain knowledge. Moreover, it needs to preserve the coherency between changed/unchanged features while satisfying user/domain-specied constraints. This paper introduces CARE, a modular explanation framework that addresses the model- and user-level desiderata in a consecutive and structured manner. We tackle the existing requirements by proposing novel and ecient solutions that are formulated in a multi-objective optimization framework. The designed framework enables including arbitrary requirements and generating counterfactual explanations and actionable recourse by choice. As a modelagnostic approach, CARE generates multiple, diverse explanations for any black-box model in tabular classication and regression settings. Several experiments on standard data sets and black-box models demonstrate the eectiveness of our modular framework and its superior performance compared to the baselines. Interpretable Machine Learning, Actionable Recourse, Counterfactual Explanations, Black-box Models ACM Reference Format: Peyman Rasouli and Ingrid Chieh Yu. 2021. CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn "Why was my loan application denied?" is an example of a follow-up question when a Machine Learning (ML) system does not provide a user’s desired outcome. The user often expects more information beyond merely an explanation to know "What do I need to change for the bank to approve my loan?". Given the user’s preferences, an actionable recourse is a list of possible changes for achieving the desired decision. By recommending the alterations, it also explains the reasons behind the current outcome. Counterfactual explanation is a technique for identifying the necessary changes in the original input that leads to the desired prediction of the ML model [24]. For example, a possible counterfactual for a user who is denied a loan by the model may be like this: “if your balance was 5000$, your loan application would have been accepted”. This explanation justies the denial prediction of the model, gives a guideline to reverse the prediction, reveals potential fairness issues, and demonstrates the correlations and patterns among the domain’s data. For having an actionable and realistic recourse, the counterfactual explanation should be personalized with respect to the user and be sound in the sense that it originates from the domain’s observations. We claim that the solution should satisfy several important desiderata in addition to basic properties such as feature value similarity and opposite prediction. In the following, we discuss these requirements in conjunction with the relevant existing studies. Proximity. A counterfactual should provide plausible changes to the original input that are in accordance with the observations of the model. For example, "earning negative money for granting the loan" is an outlier counterfactual which is untrustworthy and unrealistic. This notion is called proximity, which indicates a counterfactual instance should lie in the neighborhood of the ground-truth data [12]. Recent papers have proposed various ways of handling proximity. DACE uses Local Outlier Factor [1] in the cost function of the optimization algorithm [10]. Several works employ generative models (e.g., Variational Auto-Encoders) for approximating the data distribution, and then sample realistic counterfactual instances [9,15,23]. FACE respects the underlying data distribution by connecting the counterfactual to the input via high-density paths [18]. MOC uses a weighted average distance metric between the original input and its 𝐾 nearest instances in the training data [4]. Connectedness. A counterfactual should be the result of existing knowledge and not a consequence of an artifact of the ML model. Artifacts can be created due to lack of training data for some regions in the feature space that diminish the robustness of the model and provoke misbehavior. Having an explanation caused by an artifact is not associated to the domain knowledge and is undesirable in the context of interpretability and feasibility [12]. Therefore, there is a need to dene a relationship between a counterfactual instance and existing knowledge (training data) using a path. This notion is called connectedness, which implies that an interpretable counterfactual should be continuously connected to data points from the same class [12]. This property is thus complementary to proximity, as two instances can be close but not linked by a continuous path. It leads to interpretable counterfactuals that comply with the domain knowledge and, therefore, more actionable. Coherency. A counterfactual should preserve the correlation among features to create a coherent explanation. For example, a counterfactual that provides the desired outcome via "increasing the education level from Bachelor to Master" without increasing "age" is not feasible because "age" and "education" are two correlated features, and they need to be changed jointly. This counterfactual may still satisfy soundness properties such as proximity and connectedness (as there can be found samples in the training data with the same age as the input but having a Master’s degree), however, from the actionability point of view, it is an inconsistent explanation. Considering this property is even more essential when a user imposes constraints over some features regardless of the status of other features. In this case, the counterfactual needs to implicitly preserve the consistency of changed/unchanged features. Although there are several research works around creating statistically sound explanations [4,9,10,15,18,23], to the best of our knowledge, the coherency property has been remained unexplored. Actionability. A counterfactual should satisfy some global and local preferences that are domain-specic and dened by the enduser. Global preferences are constraints that should be satised by every counterfactual, for instance, xing immutable features like raceandgender, while local preferences are related to individual instances. For example, a possible range for featurebalancefor an individual is[3000$,6000$]while for another one is[5000$,10000$]. This notion is called actionability, implying a recourse that meets the user’s preferences [22]. Recent works address the actionability property either by classifying the features into immutable and mutable types [4,11,17], or by dening a range/set of values for numerical and categorical features [21, 22]. According to our literature review on counterfactual explanation methods, the majority of works focus on proximity and actionability properties, while connectedness and coherency received little attention. A likely reason for neglecting connectedness as an objective goal in the counterfactual generation process can be its computational burden. Moreover, many state-of-the-art works use proximity as an interchangeable property for connectedness, which is arguable, as proximity prevents generating outlier counterfactuals while connectedness results in counterfactuals that are connected to the existing domain knowledge. Disregarding connectedness results in instances being created from areas where the model has no information about (artifacts) and makes questionable improvisations (decisions) [13]. In our opinion, both proximity and connectedness properties are necessary for deriving statistically sound counterfactual explanations. Most of the stated works establish proximity by nding a counterfactual that is connected to the entire training data. Indeed, this can be problematic, because a sparse counterfactual instance (changes in one/two features) lies fairly close to the overall training data, however, it can be out-of-distribution with respect to the subset of data that share the same values as the changed features and belong to the same class [23]. In contrast, our denition of proximity is inline with [23] which states that a counterfactual instance should be an inlier with respect to a subset of data that are similar and belong to the same class as the counterfactual. The mentioned statistical properties do not preserve the consistency among features, especially when user’s preferences come into play. Often, a user denes constraints for some features irrespective of the status of others. When an algorithm only satises the specied constraints and neglect their eects on the other features, it will generate an unrealistic and impractical recourse. Hence, we view coherency and actionability as two intertwined properties that should be satised simultaneously. In this paper, we propose a method for generatingCoherent ActionableRecourse based on sound counterfactualExplanations (CARE). CARE provides actionable recourse by fullling the mentioned desiderata through objective functions organized in a modular hierarchy structure and optimized using Non-dominated Sorting Genetic Algorithm III (NSGA-III) [5]. The optimization choice leads to a model-agnostic explanation method that generates multiple (diverse) counterfactuals for every input, applies to both classication and regression tasks, and handles mixed-feature data sets. We propose a novel approach to preserve consistency between features and introduce a novel notion of actionability that can cover various constraints and prioritize dierent preferences. Our main contributions are as follow: •We propose a modular explanation framework that handles the model- and user/domain-level desiderata in a consecutive and structured manner. •We devise novel and ecient solutions for every requirement that are formulated in a multi-objective optimization framework. •We introduce a model-agnostic approach to generate multiple, diverse explanations for any black-box model in tabular classication and regression settings. •We demonstrate the importance of various requirements in counterfactual generation and the ecacy of our modular framework in addressing them through extensive validation and benchmark experiments. •We provide a multi-purpose and exible benchmark for the research community: https://github.com/peymanrasouli/ CARE. State-of-the-art works take into account a subset of the stated properties for counterfactual generation. However, as we later demonstrate by an illustrative example, every property plays a unique and crucial role in creating feasible and actionable explanations. We propose a modular framework that formulates the necessary properties in a consecutive and structured manner. Figure 1 illustrates the overall framework of CARE. It consists of four modules each fullls some specic properties for actionable recourse including VALIDIT Y, SOUNDNESS, COHERENCY, andACTIONABILITY. Low-level modules mostly contain model-related objectives, while high-level modules target user/domain-related goals. VALIDITY module acts as a basis for other modules, and counterfactuals can be generated regardless of the presence of other intermediate modules. In other words, it is possible to disable/enable desired modules in Figure 1: CARE’s framework: modular hierarchy. the hierarchy. Using this structure, we can observe the eect of the dierent properties on the generated counterfactuals. To provide an insight on how the dierent properties aect the generated counterfactual explanations, we explain an instance of the Adult Income data set [6], denoted by𝑋, using incremental congurations of CARE’s modules. The results are reported in Table 1. It can be seen that the valid counterfactual (𝐸) is a minimally modied variant of the original input that only provides the desired outcome irrespective of feasibility and actionability of changes. In contrast, the sound counterfactual (𝐸) lies on the data manifold and appears more realistic, however, it has not established the correlation between features perfectly (the values of featuresrelationshipandsexdo not conform with each other). The coherent counterfactual (𝐸) resolves the consistency shortcoming of sound counterfactuals and creates a coherent state for input features (featuresrelationshipandsexare consistent). Nevertheless, a statistically sound and coherent counterfactual may not be actionable from the user’s perspective (changing sensitive features likesexis impractical). Hence, an actionable explanation (𝐸) takes into account the user’s preferences containing the name of mutable/immutable features, possible values, and their importance to create a realistic and actionable recourse. The stated example demonstrates the importance of every property as well as the exclusive role of CARE’s modules in handling these properties. Our framework can be used as a benchmark for explanation methods sharing similar properties, as it allows proper comparison by enabling specic modules in the hierarchy. CARE has two main phases: tting and explaining; the former creates an explainer based on the training data and the ML model, while the latter generates counterfactuals for every input instance using the created explainer. In the following, we describe the CARE’s modules and the optimization algorithm for generating explanations. We dene a valid counterfactual explanation as an instance similar to the input with minimum changes to features that results in the desired outcome. Consider(𝑋, 𝑌)as a data set, where𝑋is an 𝑚dimensional feature space and𝑌is a target set. Let𝑓be a blackbox model trained on(𝑋, 𝑌)that maps an input to a target, i.e., 𝑓:𝑋→ 𝑌. Targets can be either discrete classes or continuous values depending on the prediction task (classication or regression). For a given input𝑥with𝑓 (𝑥) = 𝑦and a desired outcome𝑦, our goal is to nd a counterfactual𝑥as close to𝑥as possible such that𝑓 (𝑥) = 𝑦. VALIDITY module satises three main objectives: desired outcome, minimum feature distance, and maximum sparsity. Maximum sparsity enhances the interpretability of the generated counterfactual explanations and is inline with minimum feature distance. We dene three cost functions for the stated objectives that are minimized during the optimization process. 2.1.1 Desired Outcome. We evaluate the prediction of𝑓for a generated counterfactual𝑥with respect to the desired outcome. Accessing the black-box prediction function allows us to dene dierent measurements for the desired outcome. For the classication task, we use the Hinge loss function: where𝑓(𝑥)is the prediction probability of𝑥for the desired class 𝑐and𝑝is a probability threshold that leads to a counterfactual with a desired level of condence. This cost function considers all counterfactuals above the threshold𝑝as valid counterfactuals. The desired outcome for the regression task is evaluated as follows: 𝑂(𝑥, 𝑟) =min|𝑓 (𝑥) − 𝑟|, otherwise(2) where𝑓 (𝑥)is the predicted response for the counterfactual and 𝑟 = [𝑙𝑏, 𝑢𝑏]is a desired response range. The devised cost function considers any predicted response within the range𝑟as valid (zero cost), otherwise the absolute distance between the prediction and the closest bound (𝑙𝑏 or 𝑢𝑏) is considered as the cost of 𝑥. 2.1.2 Feature Distance. We employ Gower distance [8] to calculate the distance between features in a mixed-feature setting (i.e., data set contains both categorical and numerical features). Given an original input𝑥and a counterfactual instance𝑥, we measure their feature value distance by summation over the dierence between feature values: where𝑚is the number of features, and𝛿is a metric function that returns a distance value depending on the type of the feature: where𝑅is the value range of feature𝑗that is extracted from the training data, and I is a binary indicator function. 2.1.3 Sparsity. To have a highly interpretable explanation, a counterfactual should alter a minimum number of features. Minimum feature distance is not equivalent to the minimum number of changed features. Therefore, we dene the cost function𝑂for counting the number of altered features: Table 1: Counterfactual explanations generated using incremental congurations of CARE’s modules for an instance from the Adult Income data set. where𝑚is the number of features in the data set. This function penalizes every change in features regardless of their type. As we mentioned earlier, a sound counterfactual should originate from the observed data (proximity) and connect to the existing knowledge (connectedness). Meeting these two conditions results in an inlier instance located in a region in the decision surface where the model has a high level of condence. Therefore, our SOUNDNESS module has two tness functions that are maximized during the optimization process. 2.2.1 Proximity. Proximity indicates that the counterfactual instance lies in the neighborhood of the ground-truth samples that are predicted correctly by the model and have the same target value as the counterfactual. We utilize the proximity evaluation metric introduced in [12] as an objective function for counterfactual generation. Let𝑥be our original input and𝑥be a generated counterfactual. We refer to𝑋as the set of instances in the data set that are predicted correctly by𝑓and belong to the same class (in classication task) or response range (in regression task) as𝑥. Consider𝑎∈ 𝑋is the closest instance to𝑥, i.e., 𝑎= arg min𝐷 (𝑥, 𝑎), where𝐷is a distance metric (e.g., Minkowski). The counterfactual𝑥fullls the proximity criterion if it has the same distance to𝑎as𝑎has to the rest of the data (𝑋). The formal denition of proximity is as follows: A lower value of𝑝𝑟𝑜𝑥𝑖𝑚𝑖𝑡𝑦 (𝑥)refers to an inlier counterfactual that is located at a reasonable distance from the training data that are predicted identically. According to [1], the formal denition of proximity (Eq. 6) corresponds to the Local Outlier Factor (LOF) with a neighborhood size of𝐾 =1, which is a well-known model for outlier detection. In the tting phase of the explainer, we create an LOF model for every class/response range of the samples in the training data that are predicted correctly by the model𝑓. During the explaining phase and via the objective function𝑂, we invoke the LOF model related to the class/response range of the counterfactual𝑥to identify its status; if𝑥is an inlier, the model outputs 1, otherwise, it returns 0 that refers to an outlier. Thus, the goal is to maximize 𝑂for every counterfactual instance. 2.2.2 Connectedness. Connectedness implies the counterfactual instance is the result of existing knowledge and not a consequence of an artifact of the ML model. Such a counterfactual is continuously connected to the observed data (knowledge) using a topological notion of path. Along this path, features change smoothly and coherently, and each instance in the path is correlated with the preceding and succeeding instances. This property is thus complementary to proximity, in which the counterfactual is close to a real data point but is not necessarily linked to the majority of the data. Similarly, we benet from the connectedness evaluation metric proposed in [12] as an objective function for counterfactual generation. The continuous path can be approximated by the notion of 𝜖-chainability (with𝜖 >0) between two instances𝑒and𝑎, meaning that a nite sequence𝑋= 𝑒, 𝑒, .., 𝑒, where𝑋⊂ 𝑋, exists such that𝑒= 𝑒,𝑒= 𝑎, and∀𝑖 < 𝑁 , 𝐷 (𝑒, 𝑒) < 𝜖. Let𝑥be a counterfactual instance for an input𝑥. We say counterfactual𝑥 is𝜖-connected to𝑎 ∈ 𝑋if𝑓 (𝑥) = 𝑓 (𝑎)and there exist an𝜖-chain 𝑋between 𝑥and 𝑎 such that ∀𝑒 ∈ 𝑋, 𝑓 (𝑒) = 𝑓 (𝑥). Although assessing𝜖-connectedness seems complex, its denition resembles the DBSCAN clustering algorithm [7]. We can acknowledge that𝑥is𝜖-connected to𝑎 ∈ 𝑋, if𝑥and𝑎belong to the same cluster of DBSCAN algorithm with parametersepsilon= 𝜖 (maximum distance between two samples) andmin_samples=2 (number of samples in a neighborhood). Using DBSCAN clustering for every counterfactual instance is not computationally ecient. Moreover, nding an optimalepsilonparameter, which highly impacts the clustering results, for every class/response range is challenging. To remedy the stated issues, we employ a generalized version of DBSCAN, called HDBSCAN [2]. This algorithm adaptively selects the bestepsilonvalue to adaptively produce stable clusters. We avoid computational complexity by creating a HDBSCAN model on a large amount of samples and then querying the model for predicting the cluster of a potential counterfactual instance. Since one sample is not likely to alter the shape of the created clusters for the ground-truth data, we achieve a fairly accurate measurement of connectedness. Moreover, it does not require updating the clustering model, making the assessment procedure computationally ecient. We dene the objective function𝑂to connect the generated counterfactuals to the existing knowledge. We categorize the ground-truth samples w.r.t every class/response range that are predicted correctly by the model𝑓. A clustering model for every category is then constructed within the tting phase of the explainer. In the explaining phase and using objective function𝑂, the clustering model corresponding to the class/response range of the counterfactual𝑥is queried. If𝑥is assigned to a cluster, the function returns 1, otherwise, it returns 0, which indicates𝑥is not Algorithm 1 Correlation Models Input:observed data𝑋, correlation threshold𝜌, score threshold𝜏, number of features 𝑚, type of features 𝐹 Output: correlation models M connected to the known knowledge. Hence, the goal is to maximize 𝑂for every counterfactual instance. Preserving the relationship between changed/unchanged features is essential for creating a coherent and actionable counterfactual explanation. Fullling this property is even more important when the output is a personalized, actionable recourse. By knowing the relationship of the domain’s features, one can formulate unary and binary constraints in simple terms. For example, "age does not decrease" or "education level increment causes age increment". However, relationships over multiple features can lead to complex constraints that is hard to specify manually. We introduce a novel approach to impose high-order correlation constraints over multiple features. Using correlation information extracted from the training data, we construct predictive models (also referred as correlation models) that are exploited to preserve the coherency of counterfactual features. This technique is suitable when the input features are unknown, and knowledge about the domain is limited. The COHERENCY module uses a two-phase approach: 1) constructing correlation models in the tting phase (Algorithm 1), and 2) exploiting the created models in the objective function 𝑂during the explaining phase (Algorithm 2). Algorithm 1 starts by extracting feature correlations from the training data𝑋using Pearson’s R, Correlation Ratio, and Cramer’s V for pairs of numerical-numerical, numerical-categorical, and categorical-categorical features, respectively [3]. This creates a symmetric correlation matrix with scaled values𝑐𝑜𝑟𝑟∈ [0,1]. We consider every feature𝑗,𝑗 ∈ {1..𝑚}, as a dependent variable that is predicted by its correlated features (independent variables) denoted by𝑖𝑛𝑝𝑢𝑡𝑠(Line 5). Our goal is to create computationally ecient classication and regression models (e.g., CART [14] and Ridge [16]) for every categorical and numerical features, respectively. Features are considered correlated if they have a correlation value above threshold𝜌 ∈ [0,1]. We only consider reliable models that have a predictive score (F1-score or R-score) above threshold 𝜏 ∈ [0,1](Line 10). This hyper-parameter has a major role in the overall performance of coherency-preservation and determines the magnitude of correlation constraints. At the end (Line 11), if there exist a reliable model for a feature𝑗,𝑗 ∈ {1..𝑚}, the correlation modelMwill include a triple{𝑖𝑛𝑝𝑢𝑡𝑠, 𝑚𝑜𝑑𝑒𝑙, 𝑠𝑐𝑜𝑟𝑒}containing the inputs of the model, trained model, and score of the model for feature 𝑗. Algorithm 2 Coherency Objective (𝑂) Input: input 𝑥, counterfactual 𝑥, correlation models M Output: coherency cost 𝜉 In Algorithm 2, the created models M are used to establish the coherency among the counterfactual’s features. First we identify the changed features in the counterfactual𝑥, denoted by𝐿. For a feature𝑗 ∈ 𝐿, if correlation modelMexists, it is used to predict the value of the feature, creating a temporary counterfactual 𝑥(i.e., a copy of𝑥with the predicted value for feature𝑗). The modelMmakes prediction based on the values of the correlated features𝑖𝑛𝑝𝑢𝑡𝑠(Line 7). Therefore, it takes into account complex relationships between multiple features. We measure the preserved coherency by calculating the distance between the counterfactual 𝑥and the temporary counterfactual𝑥(Line 8). The intuition is that if the values of correlated features conform with each other, the prediction of the model for feature𝑗will be close to the counterfactual’s value for the feature, i.e.,𝑥[𝑗] ≃ 𝑥[𝑗], leading to a low 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒between𝑥and𝑥. Although we ltered out unreliable models in Algorithm 1, here, we weigh the distance according to the score of the correlation model to have a truthful measurement of the coherency cost (Line 9). This procedure is repeated for every feature, and the output of𝑂is the total coherency cost𝜉 that is minimized during the optimization process. Although previous modules provide a sound and coherent counterfactual explanation, it may not be necessarily actionable from the user’s perspective (for example, recommending a change torace orgender). The objective of ACTIONABILITY module (Figure 1) is to allow users to dene preferences over features to guarantee the actionability of a recourse. We propose a constraint language (outlined in Table 2) to provide the user with a exible set of constraints for features. The language provides diverse operators for numerical and categorical features. We also present the Categorical{𝑣, .., 𝑣} a set of categorical values notion of constraint importance to weigh the constraints according to their importance for the user. For example, two constraints "x the race" and "constrain balance between[5000$,10000$]" obviously have dierent importances, as exceeding the balance range may be acceptable, but changing race is not tolerable. Without considering constraint importance, the optimization algorithm makes no dierence between a set of constraints. By weighing constraints, we can prioritize them, and therefore, the optimization algorithm avoids to overstep the highly important ones. Thus, we dene the user’s preferencePas a set of constraint triples in the form ofC= (𝑓 𝑒𝑎𝑡𝑢𝑟𝑒, 𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡, 𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒)for a desired feature𝑗,𝑗 ∈ {1..𝑚}, where𝑚is the total number of features, i.e., P = {C, C, .., C}, 𝑥, 𝑦, 𝑧 ∈ {1..𝑚} ∧𝑥 ≠ 𝑦 ≠ 𝑧. An example preference can beˆP = {(𝑎𝑔𝑒, 𝑔𝑒,4), (𝑟𝑎𝑐𝑒, 𝑓 𝑖𝑥,10)}. Algorithm 3 outlines our proposed objective function𝑂which computes the actionability cost𝜂for a particular counterfactual𝑥according to the user’s preferenceP. For every constraint triple∀C ∈ P, Algorithm 3 checks the satisability ofC; ifCis not satised, thenCis added to the actionability cost𝜂. The output of𝑂is the total incurred actionability cost 𝜂 that is minimized during the optimization process. Algorithm 3 Actionability Objective (𝑂) Input: input 𝑥, counterfactual 𝑥, user’s preference P Output: actionability cost 𝜂 In this section, we adopt Non-dominated Sorting Genetic Algorithm III (NSGA-III) [5] to solve our multi-objective optimization problem. Compared to other evolutionary algorithms, NSGA-III performs well with dierently scaled objective values and generates diverse solutions. The rst property is essential for a multi-objective counterfactual explanation method where there exists a combination of tness and cost functions with dierent ranges of output and Table 3: Summary of information of the used data sets. conict goals. The second property is useful in the sense of actionability, as providing a diverse set of solutions increases the chance of obtaining a recourse complying with the user’s circumstance. The objective set 𝑂𝑏 𝑗 denes the CARE’s hierarchy: 𝑂𝑏 𝑗 ={↓ 𝑂, ↓ 𝑂, ↓ 𝑂}, Every set in𝑂𝑏 𝑗corresponds to a module annotated by a subscript number, i.e.,1,2,3, and4(also shown in Figure 1). Arrows indicate the type of objectives, either cost or tness function, which should be minimized or maximized, respectively. As we mentioned earlier, the rst set (VALIDITY module) is always present, while other sets (modules) can be arbitrarily included. For example, we may create the conguration{1, 2}which only contains the VALIDITY and SOUNDNESS modules. This modular structure allows our method to be used for both counterfactual explanation (conguration{1, 2}) and actionable recourse generation (conguration{1, 2, 3, 4}). Since the dened objective functions make no assumption about the nature and internal structure of the prediction model𝑓, CARE is applicable on any ML model created for tabular classication and regression tasks. Moreover, the NSGA-III algorithm can automatically handle mixed-feature data dispensable of auxiliary operations (such as one-hot encoding and imposing hard-constraints), making CARE suitable for mixed-feature tabular data sets. In this section, rst, we describe the evaluation setup including data sets, models, and hyper-parameters. Second, we validate the importance of the stated desiderata and the eectiveness of CARE’s modules in handling them using multiple experiments. Finally, we demonstrate the overall performance of CARE by benchmarking against two state-of-the-art explanation methods. We mainly used standard data sets from the UCI Machine Learning Repository [6], except the COMPAS data set that can be found at [19]. Summary of information of the used data sets is reported in Table 3. The numerical features were standardized by removing the mean and scaled to unit variance. We converted original categorical features to ordinal encoding and used their corresponding one-hot encoding for creating black-box models. We split the data sets into 80% train set and 20% test set. The train set was used for creating black-box and the explainer models while Table 4: Performance of the black-box models. the test set was used for generating explanations. We created a Multilayer Perceptron Neural Networks (NN) consisted of two hidden layers each with 50 neurons and a Gradient Boosting Machines (GB) comprised of 100 estimators as black-box classiers and regressors. Table 4 reports the performance of created models on test set in terms of F1-score (classication task) and R-score (regression task). We used 4-quantiles as cutting points for intervals in the regression task and generated counterfactuals from the neighbor interval for every input. In addition, for the classication task, we set the probability threshold as𝑝 =0.5 and generated counterfactuals from the opposite class. We used CART decision trees [20] for categorical features and Ridge regression models [16] for numerical features as correlation models (refer to Algorithm 1). We set the minimum correlation threshold as𝜌 =0.1. To adjust the threshold of the model’s score (i.e.,𝜏) with respect to each data set, we set𝜏as the median value of all models’ scores and ltered out the correlation models that have a score below𝜏. Accordingly, we achieved𝜏 =0.72, 𝜏 =0.59,𝜏 =0.84, and𝜏 =0.64 for Adult Income, COMPAS, Default of Credit Card Clients, and Boston House Prices data sets, respectively. For NSGA-III optimization algorithm, we used the two-point crossover operator with percentage𝑝𝑐 =60% and the polynomial mutation operator with percentage𝑝𝑚 =30%. The number of generations was set to𝑛=10. Although it is possible to dene arbitrary population size𝑛, we determined it adaptively with respect to the number of objectives using a standard formula described in the original paper of NSGA-III [5]. This approach is more suitable for our modular framework in which the number of objectives would vary with respect to dierent congurations. Regarding user preferences for actionable recourse, we only set global constraints according to our basic knowledge about the domains for all inputs. For example, constraint x (x the current value) was set for features likeraceandsexwhile constraint ge (greater than or equal to the current value) was set for featureage. Thus, actionable recourses are not locally personalized with respect to every specic input. We set equal importance values for all constraints, i.e.,𝑖𝑚𝑝𝑜𝑟𝑡𝑎𝑛𝑐𝑒 =1.0, to have a fair comparison regarding the actionability objective function (i.e.,𝑂) among baseline methods. CARE has been developed using Python programming language, and experiments were run on a system with Intel Core i78650U processor and 32GB of memory. A complete implementation of the method, including validation and benchmark experiments, is available at: https://github.com/peymanrasouli/CARE. We visualize the impact of SOUNDNESS module in generating sound and realistic counterfactuals on Iris and Moon data sets [6]. We created Gradient Boosting (GB) classiers on these data sets and generatedcfandcfcounterfactuals using congurations{1}and {1, 2}, respectively. Figure 2(a) demonstrates counterfactuals from the furthest class for a sample from Iris data set, and Figure 2(b) depicts counterfactuals from the opposite class for an instance from Moon data set. The counterfcatuals are annotated with the values of𝑂and𝑂denoted by𝑝and𝑐, respectively. By observing the location of the generated counterfactuals in Figures 2(a) and 2(b), the importance of SOUNDNESS module is revealed. It can be seen that minimum distance is the only important criterion forcfcounterfactuals whilecfcounterfactuals are associated to the same-class training data (𝑝 =1 and𝑐 =1), therefore they are connected to the previous knowledge and achieve a high prediction probability from the classier. We stated that proximity is dierent than connecte dness and they are complementary criteria for a sound counterfactual explanation. Figure 2(a) demonstrates this distinction clearly ascfis located at the proximity of the training data (𝑝 =1), but it is not connected to a high density region (𝑐 = 0) which declines its interpretability and actionability. Figure 3: Evaluation of coherency-preservation. To validate the impact of COHERENCY module, we have conducted an experiment using Adult data set [6]. In this data set, categorical featureseducationandeducation-numare fully correlated to each other, as there is a one-to-one correspondence between their values. Moreover, there is a strong relationship among features marital-status,relationship, andsex. We refer to the former set of correlated features as "Education" and the latter as "Relationship." A coherent counterfactual explanation should change all features in a set accordingly; otherwise, it creates an inconsistent feature state. We dene the coherency rate as the normalized number of counterfactuals in which the consistency between features in every correlation set ("Education" and "Relationship") is preserved. Figure 3 depicts the results of the coherency validation experiment. We created anNNmodel using the train set and generated a set of ten counterfactual explanations for 500 samples form the test set using dierent congurations of CARE. According to Figure 3, a sound explanation method (CARE) considerably resolves the inconsistency existing in valid explanations (CARE), however, it does not guarantee generating coherent explanations for all inputs. On the contrary, congurations equipped with the COHERENCY module (i.e., CAREand CARE) substantially preserve the correlation among features, leading to maximum coherency rate. The eciency of our algorithm in handling complex relationships will be validated using a user study in future work. Modules in the CARE’s hierarchy are rigorously designed to handle important desiderata for the counterfactual generation. Earlier, we demonstrated the role of CARE’s modules in approaching dierent properties via an illustrative example (Table 1). In this section, we quantitatively evaluate the impact of each module by conducting the following experiment. We consider four congurations{1}, {1, 2},{1, 2, 3}, and{1, 2, 3, 4}. We are interested to know the behavior of CARE when some modules are absent. For example, how much a counterfactual generated by conguration {1} will satisfy objectives dened in conguration{1, 2}. Using this information we can determine the importance of the devised modules. To this end, we createdGBmodels using the train set of Adult Income and Boston House Prices data sets [6] and generated counterfactual explanations using the specied module combinations for 500 and 100 samples of their test set, respectively. Eventually, their results regarding the objectives dened in the last conguration (i.e., {1, 2, 3, 4}), which contains all CARE’s modules, were measured. By observing the results demonstrated in Table 5, we can conclude several points about the performance of dierent modules: •CARE generates valid counterfactuals for all congurations and data sets since validity is the basis of our methodology. These counterfactuals are best at fullling the𝑂and 𝑂objectives because closeness and sparsity are the only essential goals in this setting. Moreover, their cost for the𝑂objective is usually low since changing immutable features leads to a counterfactual with a dramatic distance to the original input, therefore, the optimization algorithm usually avoids manipulating such features. 𝑂and𝑂objectives in congurations {1, 2},{1, 2, 3}, and{1, 2, 3, 4}. Meanwhile, since sound counterfactuals originate from high-density regions, their𝑂 and 𝑂costs are expectedly increased. •The devised COHERENCY module has eciently preserved the relationship among counterfactual features, resulting in 𝑂≈0 for congurations equipped with the module (i.e.,{1, 2, 3}and{1, 2, 3, 4}). This shows the generalization of our method for high dimensional data sets where there exist complex correlations between features. •By comparing the results of two congurations{1, 2, 3}and {1, 2, 3, 4}we realize that given a sound and coherent counterfactual, generating an actionable recourse does not negatively inuence the other objectives. CARE creates actionable recourse based on sound and coherent counterfactuals. Though, for some inputs and their corresponding constraints, it is impossible to nd an actionable recourse that fullls the objectives in the precedent modules (modules2and3), causing 𝑂cost. To evaluate the performance of CARE, we compared it with state-ofthe-art explanation methods CFPrototype [23] and DiCE [17]. We chose CFPrototype because it uses a loss function called prototype loss to generate interpretable counterfactuals located in the proximity of the same-class training data. We selected DiCE due to its diverse counterfactual generation property and its ability to impose actionability constraints on the input features. To balance between diversity and proximity of the generated counterfactuals in DiCE, we set the corresponding hyper-parameters as𝜆=1.0 and𝜆=1.0. We used CFPrototype with the default hyper-parameters stated in the paper. The methods were applied on three classication data sets Adult Income, COMPAS [19], and Default of Credit Card Clients (Default of CCC for short) [6]. We created anNNblack-box model for every data set using their train set and explained 500 samples from their test set. The number of generated counterfactuals in CARE and DiCE was set to 𝑁 = 10. 3.5.1 Performance Evaluation. Table 6 reports the evaluation of counterfactuals with respect to the CARE’s objective functions. CARE’s counterfactuals originate from the distribution of the sameclass training data and are connected to the existing knowledge by a continuous path; therefore, it achieved the highest values for 𝑂and𝑂tness functions. Moreover, the coherency in CARE’s explanations is fully respected in both data sets (zero cost for𝑂). It is noteworthy that the amount of coherency-preservation is directly related to the number of correlation models that we had generated; the more reliable correlation models exist, the more complex correlations are preserved. As we explained in Section 3.4, we can eortlessly generate an actionable recourse if we only consider its distance to the original input. But, CARE is designed to follow the order of the modules in its hierarchy and prioritize sound and coherent explanations. For some inputs and their corresponding constraints, it is impossible to meet the two mentioned properties, resulting in non-actionable recourses (nonzero cost for𝑂). Last but not least, CARE generates valid counterfactuals for every input (zero cost for 𝑂). 3.5.2 Coherency Evaluation. To justify the eectiveness of the COHERENCY module and to demonstrate the behavior of baseline methods regarding coherency-preservation, we benchmarked CARE against CFPrototype and DiCE on the experiment dened in Section 3.3. Figure 4(a) illustrates the coherency rate of explanations with respect to the correlation sets "Education" and "Relationship." It can be seen that CARE outperforms baseline methods by generating coherent counterfactual explanations for every input. This experiment highlights that the statistically soundness objectives (like proximity which is considered by CFPrototype and DiCE methods) do not necessarily guarantee to generate coherent explanations. 3.5.3 Diversity Evaluation. We benchmarked CARE versus DiCE regarding the diversity of the generated counterfactuals. A diverse set of counterfactual explanations for a particular input provides the user with more alternatives for obtaining the desired outcome. We measure the diversity with respect to feature variation as follows: where𝐶is combination operator,𝑁is the total number of counterfactuals, andSis the the set of explanations. We calculate the Jaccard index between the feature names in every pair of explanations for calculating𝑑. In our opinion, this is a representative metric for diversity because we are interested in a set of counterfactuals that provides various options for a user, not only changes in one or a few specic features. For some inputs, a few important features determine the decision, which in this case, recommending dierent values for the features is acceptable. We propose a fair metric for value-based diversity as follows: whereIis a binary indicator function. Instead of computing the dierence between feature values for every pair of counterfactuals inS, which is a biased measurement due to the dierent number of changed features (i.e., sparsity degree) by every baseline method, we measure the dierence between the feature values of common features in every pair of explanations. Figure 4(b) illustrates the results of this evaluation. Although we have not formulated diversity as an objective function, the choice of the optimization algorithm (i.e., NSGA-III) has led to promising outcomes. We observe that CARE tends to generate counterfactuals that are dierent concerning the feature names (high𝑑). It also performs solid when changing multiple features is not applicable (𝑑≈ 0.6). 3.5.4 Computational Complexity Evaluation. Computational complexity is an essential matter for a counterfactual explanation method. The overall complexity of the NSGA-III algorithm is𝑂 (𝑀𝑁)where 𝑀is the number of objectives, and𝑁is the population size. CARE does not require creating a new model for explaining every instance; once a CARE’s explainer for a black-box model and its corresponding data set is built, it takes𝑂 (𝑀𝑁)to explain every input. We have used an adaptive mechanism dened in the original paper of NSGA-III [5] for setting the population size. It determines the population size based on the number of objective functions. This approach is suitable for our modular framework in which modules are arbitrarily added/removed. Figure 4(c) shows the average time spent by each algorithm for explaining a single instance. Although the used data sets have dierent feature dimensionality, CARE generates explanations in a reasonable and similar amount of time compared to the baseline methods. Furthermore, the computational complexity of CARE reduces when fewer modules are employed. In this paper, we presented CARE, a modular explanation framework for generating actionable recourse. We demonstrated that a sound counterfactual instance is located in the neighborhood and continuously linked to the same-class ground-truth data points. An actionable recourse can be constructed based on such counterfactuals while preserving the correlation between changed/unchanged features and satisfying user/domain-specic constraints. Modules Figure 4: Comparison of explanation methods with respect to coherency, diversity, and computational complexity. in the CARE’s hierarchy are rigorously designed to address the stated desiderata. The low-level modules handle fundamental and statistical properties related to the model and observed data, while the high-level ones manage the coherency between features and user preferences. Through several experiments, we demonstrated the ecacy of our devised approach in creating feasible and realistic explanations and its superior performance compared to the baselines. CARE’s framework can be viewed as a exible benchmark for counterfactual and actionable recourse generation techniques sharing similar desiderata. In future work, we will apply our approach to a large-scale, real-world use case and evaluate its performance using a user study.