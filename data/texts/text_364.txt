School of Information TechnologySchool of Information Technology ORCID:0000-0002-1523-4251ORCID:0000-0003-1814-0856 Abstract—Deep-learning based QRS-detection algorithms often require an essential post-processing to reﬁne the predictionstreams for R-peak localisation. The post-processing performs signal-processing tasks from as simple as, removing isolated 0s or 1s in the prediction-stream to sophisticated steps, which require domain-speciﬁc knowledge, including the minimum threshold of a QRS-complex extent or R-R interval. Often these thresholds vary among QRS-detection studies and are empirically determined for the target dataset, which may have implications if the target dataset differs. Moreover, these studies, in general, fail to identify the relative strengths of deep-learning models and post-processing to weigh them appropriately. This study classiﬁes post-processing, as found in the QRS-detection literature, into two levels - moderate, and advanced - and advocates that the thresholds be learnt by an appropriate deep-learning module, called a Gated Recurrent Unit (GRU), to avoid explicitly setting post-processing thresholds. This is done by utilising the same philosophy of shifting from hand-crafted feature-engineering to deep-learning-based feature-extraction. The results suggest that GRU learns the post-processing level and the QRS detection performance using GRU-based post-processing marginally follows the domain-speciﬁc manual post-processing, without requiring usage of domain-speciﬁc threshold parameters. To the best of our knowledge, the use of GRU to learn QRS-detection postprocessing from CNN model generated prediction-streams is the ﬁrst of its kind. The outcome was used to recommend a modular design for a QRS-detection system, where the level of complexity of the CNN model and post-processing can be tuned based on the deployment environment. Index Terms—deep-learning, CNN (convolutional-network), GRU (gated recurrent unit), generalisation, post-processing, electrocardiogram, QRS-complex An electrocardiogram - also called ECG - records the electrical signals in one’s heart. It is a painless, and noninvasive way to help diagnose many common heart problems in people of all ages. ECGs are often done in clinical settings, including a doctor’s ofﬁce, clinic or a hospital room, but now-adays, smart wearable devices (i.e. smart watch) are commonly used for home-based cardiac monitoring. ECG signals consist of waves - P-wave, QRS-complex, and T-wave, among which the QRS-complex is the most prominent and detection of which often forms the basis of automated ECG signal analysis. The shift in ECG operating conditions from the clinical-setting to wearable devices, for cardiac-health monitoring, brings renewed focus to the QRS-detection research area which has been active for more than three decades. Traditional QRS-detection algorithms generally employed two steps, ﬁrstly, a signal-processing step to enhance the QRS-complex, including the use of signal derivatives, digital ﬁlters, and signal transformation, and ﬁnally, the usage of amplitude-based thresholds to identify R-peaks [1]. The QRSdetection literature contains classical machine-learning (ML) based approaches, which derive time or frequency-domain statistical signal properties to train models, including SVM [2], Random-Forest [3], or KNN [4] to identify whether an ECG segment contains a QRS-complex. The traditional ﬁlter and classical ML based studies were often used with only a single or small number of datasets to report QRS-detection performance. The study of Habib et al. [5] explored the characteristics of ECG-datasets and their uniqueness, in terms of noise and inter-patient variance, and found that a QRS-detection method performing well on a dataset, may not perform the same in other datasets. Deep-learning based algorithms are encountered in the recent QRS-detection literature which show superior detection-scores for multiple validation datasets. Deep-learning (DL) has become the de-facto standard for computer-vision tasks, which inspired QRS-detection researchers to follow the same path [6], [7]. Many of the DLbased QRS-detection studies were found to partition a single dataset to be used for training and validation respectively, until some of the recent studies, which were found to report superior performance across multiple validation datasets [8], [9]. These studies tend to create sophisticated DL model architecture, followed by an essential post-processing for ﬁner R-peak localisation [8], [10], [9], [11], [12], [13]. The post-processing was found to be used to remove isolated high or low predictions which may not form a legitimate QRS-candidate, as well as, to do advance signal-processing which may involve the use of domain-speciﬁc knowledge. Based on the QRS-detection and R-peak localisation literature, the post-processing that requires domain-speciﬁc knowledge was divided into two levels, ﬁrstly, a moderate level which performs signal-processing based on some domain-speciﬁc heuristic, including removal of a candidate QRS-complex [8], [12], [13] or QRS-complex cluster [11] which falls below an impossiblly small QRS-complex duration threshold, and secondly, an advanced level which may involve further signal-processing assuming a minimum distance between R-peaks for two consecutive candidate QRS-complexes [10], [13]. An essential salt-and-pepper ﬁlter, which merely removes isolated sample predictions (i.e. remove isolated 0s or 1s in a binary prediction-stream [10]), is often required for the mentioned moderate, and advanced post-processing. QRSdetection studies claim superior detection performance using sophisticated and deep models and moderate/advanced postprocessing, but generally not found to weigh them separately to clearly understand which one contributes and to what extent. Post-processing in QRS-detection studies often assume different thresholds, for example, the minimum R-R distance of 200 milli-seconds worked well in our study, while another study used 100 milli-seconds [8]. This threshold is likely not to be set following a rule-of-thumb, rather would be determined empirically based on the target datasets. It can be argued that since the thresholds were determined empirically from the dataset, it would be preferable to let a model learn it from the very dataset itself. A post-processing model should learn parameters like the minimum extent of candidate QRS-complex or R-R distance from the data. These parameters relate the temporal relationship of QRS-complexes in ECG data and recurrent-neural-networks (RNN) are primarily used to unveil temporal sequences. Following the inherent nature of RNN, a light-weight counterpart, called Gated Recurrent Unit (GRU), was used to learn the post-processing. This study aims to train a GRU to learn data-driven postprocessing, based on solely the CNN model generated binary prediction-streams, to remove the subjectivity (use of domain knowledge, dataset speciﬁc design etc.) from post-processing reported in literature. The primary focus of this study was not to ﬁnd the best CNN model for QRS detection. Since shallow CNN models are often utilised in QRS detection [6], [7], [11], unlike the trend of using deep models for computer-vision tasks, a shallow 2-layer convolution-only CNN model was deﬁned as the baseline-convnet and its complexity was increased by increasing the depth to 4, 8, 16, 32, and 64-layer. For the postprocessing, we used a single GRU module and optimised its two hyperparameters - the number of hidden-layers and sequencelength. The CNN models (baseline-convnet and deeper variants) were trained to generate binary prediction-streams and the GRU was trained to post-process the prediction-streams for further reﬁnement. The performance of the CNN-GRU model was compared with the CNN-traditional post-processing (two levels namely moderate, and advanced) for seven cross-validated datasets. The outcome was used to recommend a modular design concept for a QRS-detection system, where the level of complexity of the CNN model and post-processing can be tuned based on the deployment environment. The use of deep-learning-based models are common in contemporary QRS-detection studies in the literature. A multibranch dilated six-layer deep CNN model, with a SENet before the ﬁnal decision layer, used a post-processing step to revise the prediction-stream through multiple iterations [8]. In a separate study, a similar multi-branch dilated convolution was used, followed by a post-processing to remove isolated 0s and 1s from the output prediction-stream of the CNN-model [10]. There are studies that use - different types of convolution [9], multichannel ECG [12], [13], and fully-convolutional networks [13], where all of them consist of an essential manual post-processing step, which utilises domain-speciﬁc knowledge, at some level, to reﬁne the deep-learning-based model-generated predictionstream to ﬁnally localise R-peaks. The idea of automating the post-processing task using a deep-learning-based module has not been explored yet, rather the focus has been on the use of a sophisticated deep-learning-based model with a manual post-processing. There are two tasks in this study and each uses the corresponding model to solve its speciﬁc problem. The ﬁrst task is QRS-prediction and the other is learning post-processing. The QRS-prediction task was formulated as a segmentation-problem where each input sample receives an output binary-prediction from a CNN model, whether it belongs to a QRS-region, in case the prediction score is above a threshold, or a non-QRS region. During model training, each annotated R-peak was labeled as 1s around 0.05 seconds of equivalent sample on each side, forming a region of ﬁve samples (considering 100Hz ECG signal) as a representative QRS-region, while all other samples (as 0s), form the non-QRS regions. The post-processing learning was formulated as a sequencelearning task using a Gated Recurrent Unit (GRU), a variant of an RNN, which was trained to generate a binary-sequence similar to the annotated QRS/non-QRS binary-sequence from the binary prediction-stream of the CNN model. This task essentially makes a GRU learn to repair a prediction-stream making the post-processing redundant, which requires domainspeciﬁc-heuristic based signal-processing (i.e. moderate, and advanced post-process). Note that the post-processing levels are described in Methodology section. The smoothed out predictionstream is subject to salt-and-pepper ﬁltration followed by Rpeak localisation. Eight ECG datasets from the PhysioNet [14] databank were used in this study, including the MIT-BIH-Arrhythmia [15], INCART, QT [16], EDB (European ST-T Database) [17], STDB (MIT-BIH ST Change Database), TWADB (T-Wave Alternans Challenge Database), NSTDB (MIT-BIH Noise Stress Test Database) [18], and SVDB (MIT-BIH Supraventricular Arrhythmia Database) [19]. Each of these databases are summarised in Table I. The primary (or ﬁrst) lead from each of the datasets were used in this study, and valid beat types include, NLRBAJSVFREQ/, which avoided paced-beat. TABLE I: Characteristics of PhysioNet datasets used in current study. C. Data Pre-processing a) Signal Resampling: In this study, we used eight datasets from Physionet, which are unique in terms of noise, sampling frequency, and inter-patient variance [5]. For usability, each dataset was re-sampled at a single common frequency of as low as 100Hz, considering a domain-speciﬁc heuristic that an average QRS-complex may go upto 25Hz or beyond [20]. Thus, the Nyquest sampling frequency [21] should be at least 50Hz to contain an average QRS-complex. Considering the effect of noise and QRS-complex extremes, a sample frequency of 100Hz was adopted, which also beneﬁts maintaining a small deep-learning model size. Since all datasets were from PhysioNet databank, a suitable utility program xform was used to resample both the signal and annotation. b) Segmentation and Overlapping: The QRS-segmentclassiﬁcation problem restricts a segment length to be able to contain at most a single QRS-complex per segment. Our study was formulated as a sample-wise binary-segmentation problem, where each input sample receives an output prediction, and thus, any number of QRS-complexes may be used to form segments. To keep the deep-learning model small, ECG records were sliced into three second segments with two seconds overlapping. A two seconds overlapping strategy was likely to increase the detection probability, since each QRS-complex goes through the model three times, which were then aggregated in the binary decision-level using a logical OR operation. The segmentation and overlapping strategies were used following a similar strategy explained in another study [10]. c) Normalisation: We have applied segment-wise Zscore normalisation to subtract the population mean. Other normalisation, like Min-Max was found inappropriate for our study, since it cannot handle the variation due to mean and outliers. This study examines the possibility of the usage of recurrent neural-networks in learning the post-processing so that advanced signal-processing and domain-speciﬁc knowledge can be avoided as much as possible. A CNN model was used to input z-score normalised single-channel ECG signals and to generate sample-wise prediction indicating whether a sample belongs to a QRS-region or not. Since ﬁnding the best model is not the focus of this study, a simple and shallow 2-layer convolution-only CNN model was used to generate the binary prediction-stream, which would be referred to as the baselineconvnet or baseline-network in the text. Unlike computer-vision tasks, where using a deep network is common, the use of a single-[6] or two-convolution-layer [7], [11] CNN models for detecting beat in ECG time-series signal is often found beneﬁcial. The deep-learning-based models, i.e., convolutional neural network and GRU, were implemented in Python 3.7 using PyTorch 1.8.1 API. Figure 1 shows, on the left, the convolutional-network block diagram highlighting the low-convolution-block, featureextracting-backbone, and scoring layer. The complexity of the baseline-convnet was increased by adding more convolutionlayers, including 4, 8, 16, 32, and 64-layer to observe the performance improvement. Convolution kernel-size is one of the hyper-parameters, to be optimised, however, a domainspeciﬁc heuristic was used to achieve this, following the study in [22]. This study found that since the average QRS-complex is 60 milli-seconds, a kernel-size of 44 milli-seconds should capture most of its components. For a 100Hz ECG signal, the 44 milli-seconds yields a convolution-kernel of 5 samples long. Being a sample-wise segmentation problem, there are CNN models in the segmentation literature, including U-Net [23] that uses sub-sampling layers at earlier stages, followed by an up-sampling operation at later-stages to re-construct the output resolution to be the same as the input. In this study, the use of sub-sampling layers were avoided completely to keep the model simple and avoid any re-construction effort, - following a network-design philosophy explained in the study in [24]. The number of output channels was set to a ﬁxed number 24 for all the convolution layers, since very few channels were found to degrade the detection performance, while more channels were found unable to gain performance proportionately but increased the complexity. The output feature-map were batch-normalised (BN) before passing to the ReLU non-linear activation function, following the argument that BN accelerates training, enables higher learning-rate, and improves generalisation accuracy [25]. The CNN model generates a binary prediction-stream which goes through essential multi-level post-processing involving signal-processing and domain-speciﬁc heuristics. To facilitate deep-learning-based post-processing, the Gated Recurrent Unit (GRU) [26], which is a light-weight (i.e. reduced number of gates) and faster sibling of Long Short-Term Memory (LSTM), was used, as shown in the right-portion of Figure 1. GRUs can be stacked to optimise for a particular task, however, here a GRU with a single and double hidden-layer were explored, with a range of sequence-lengths (i.e. 1-5 seconds), to ﬁnd an optimum GRU conﬁguration. Post-processing is often essential for R-peak localisation and the level of post-processing adopted in studies varies. Based on the signal-processing logic and domain-speciﬁc knowledge utilised, the post-processing in the QRS-detection literature was categorised into two levels - moderate, and advanced. A primitive signal-processing is often essential for other postprocessing to remove isolated 1s or 0s. In this study, the Fig. 1: (Left) Convolution-only network block-diagram, which was used to generate sample-wise prediction, from single-channel raw ECG signal, a high-probability if a sample belongs to a QRS-complex, low-probability otherwise. (Right) A recurrent neural network (RNN) learns to repair the binary prediction-stream, generated by the CNN model, which then followed by salt-and-pepper ﬁltration and R-peak localisation. RNN was expected to learn the temporal relationship in the prediction-stream to make the moderate and advanced levels of post-processing redundant. salt-and-pepper noise ﬁlter has been considered as a primitive step that aggregates consecutive 1s in CNN model generated binary prediction-streams and remove isolated 1s or 0s. Result: Reﬁned prediction-stream Salt-and-pepper ﬁlter: begin 1s, create a Node structure with attributes start_loc, conf idence (i.e. number of 1s in a node), andq_loc(=start_loc + 0.5*confidence) to prepare a node_list. Remove salt-and-pepper noise: Scannode_listand merge two consecutive nodes not more than 3 samples apart. end The salt-and-pepper ﬁlter, shown in Algorithm 1, is thus deﬁned to be a basic signal-processing step which groups the consecutive 1s to represent a QRS-complex node with certain information, including start-location, the number of consecutive 1s (a.k.a. conﬁdence-score of a node), and candidate R-peak location (equals to start-location plus half of the conﬁdencescore). A list of nodes were formed by identifying and chaining the nodes for each ECG record. Removal of salt-and-pepper noise, semantically, was carried out by identifying two close nodes which are three or fewer samples apart, which were then merged to form a more conﬁdent node. The moderate post-processing, shown in Algorithm 2, initially uses the essential salt-and-pepper ﬁltering to form Data: CNN model’s binary prediction-stream (100Hz signal) Result: Reﬁned prediction-stream Moderate Postprocess: begin remove nodes withconfidencevalue less than six (i.e. 64 milli-seconds equivalent threshold). end Algorithm 2: Moderate postprocessing a list of candidate QRS-complex nodes and remove salt-andpepper noise, followed by a step to ﬁlter less-conﬁdent nodes from the candidate node-list. The ﬁlter operation is based on the logic that each node’s conﬁdence value should be at least 64 milli-seconds of equivalent number of samples, which is six samples for 100Hz signal. These less-conﬁdent nodes were assumed to be the reason for a QRS-like artifact, however, this step may also remove some of the legitimate candidates, which may be subject to further steps of repair, but overlooked in our study. The advanced post-processing, shown in Algorithm 3, initially uses the moderate post-processing (which in-turn employs the essential salt-and-pepper ﬁltering) to produce a conﬁdent list of QRS-complex candidates, followed by a further candidate-ﬁltration step which calculates the distance between the candidate R-peaks of two consecutive nodes. Recall that the candidate R-peak of each node was calculated by adding half of the extent of the consecutive 1s (i.e. half of the conﬁdence score) with the start-location of candidate QRS- Result: Reﬁned prediction-stream Advanced Postprocess: begin node_list and remove the right node of a current node if it is less than 200 milli-seconds apart. node contains at least a sample with amplitude higher than quarter of the max amplitude within the corresponding ﬁrst-derivative segment. end complex. A very low R-R distance threshold of 200 milliseconds (which is 20 samples for 100Hz signal) was used to remove candidate QRS-nodes. The choice of a 200 millisecond threshold may seem to be high enough compared to the threshold set to 100 milli-seconds in another study [8], but the larger threshold set to our study worked as expected. An additional attribute per QRS-complex candidate node was calculated, called support score, which is the number of 1s in the ﬁrst-derivative of the corresponding source signal-fragment with magnitude prominently high. Each conﬁdent QRS-node that falls below the minimum R-R distance threshold is retained only if its support score is one or above, otherwise, the candidate node is dropped. Further post-processing steps were identiﬁed in QRS-detection studies, for example, to recover less-conﬁdent candidate nodes that were removed during the moderate post-processing, by identifying if the R-R distance is beyond an impractically large threshold and allowing nodes with a slightly lower conﬁdence-score [8]. However, our study limits the advanced post-processing to the usage of the abovementioned mininum R-R threshold-based ﬁltration to focus on the current study goal, which is to verify if RNNs can learn the post-processing to make the moderate and advanced post-processing, as deﬁned in Algorithm 2, and 3, redundant. 1) CNN Model Training: The baseline-convnet, as well as its deeper variants, were trained with the comparatively healthy-and-clean MIT-BIH-Arrhythmia dataset to facilitate learning QRS-complex morphology. Subject-wise ﬁve-fold internal-validation was used, where in each iteration, fourfold worth recordings were segmented and used for model training, while the segments from the rest-fold were used to validate the model (by calculating validation loss). A maximum of 100 epochs were set for the training with an early stopping mechanism was in place to verify if the validation-loss stalls for N=7 (selected empirically) consecutive epochs. An eager learning was initially intended by setting a high learning-rate of 0.01, but a scheduler (i.e. LR-scheduler) was employed to Fig. 2: GRU parameter tuning process. (a) F1-score variation for GRU’s number of hidden-layer and sequence-length variation across validation-datasets using baseline-convnet to generate prediction-streams. The number of hidden-layer and sequencelength were encoded using a single two-digit number, where the ﬁrst-digit indicates the number of hidden-layer and the seconddigit the sequence-length in seconds. (b) F1-score variation of validation-datasets using GRU variants of a single hidden-layer and 1-second sequence-length, trained with binary predictionstream generated from CNN models of 2, 4, and 8-layer deep network. It was hypothesized that GRUs, trained with shallownetwork’s noisy prediction-stream, may learn differently, but the performance was found to vary marginally. regulate the learning-rate once validation-loss stops decreasing for ﬁve consecutive epochs. It was found beneﬁcial (from observations) to reduce the learning-rate by a factor of 0.1 once the development of validation-loss ceases for ﬁve consecutive epochs. The slightly higher number of epochs set for the earlystopping module was intended to allow enough time for the LR-scheduler to decrease the learning-rate, so that during the next two epochs validation-loss becomes noticeable, if the model-training has not already reached the global-minima (or one of the local minimums). 2) Testing Approach: The trained CNN models were used to perform cross-database validation with two post-processing scenarios, (i) traditional post-processing levels (i.e. moderate, and advanced as described in Algorithm 2, and 3), as shown in Figure 1-left-column, and (ii) GRU post-processing (Figure 1-right-column) The validation steps, irrespective of means of post-processing, are summarised as below - With the ﬁve-fold training, ﬁve CNN models were produced with the CNN model training method. The validation score for each test dataset was calculated using all ﬁve models. Validation scores of ﬁve models were aggregated and mean and standard deviation used for reporting in the results. This process was repeated for all variants (depth ranging 2) of the baseline-convnet models. 3) Training GRU for Post-processing: To ensure that the GRU training data i.e., output binary stream from CNN model, includes an adequate level of noise, the following two conditions need to be satisﬁed: (i) the selected dataset should be inherently noisy; and (ii) the CNN model should be shallow, so that it makes lot of erroneous predictions. In this study, the choice of the baseline-convnet (depth=2) was adopted as a shallow CNN model after empirical evaluation of layer 4 and 8 (see Figure 2-b). INCART was selected as a dataset, which contains comparatively noisy ECG subjects consistent with ischemia, coronary artery disease, conduction abnormalities, and arrhythmias [14]. Only the baseline-convnet generated prediction-stream of the INCART (instead of the ECG signal) was used for GRU training and validation. In the GRU training, the number of hidden-layers were varied in the range of 1 to 2, while the sequence-length was varied in the range of 1 to 5 seconds. A similar training strategy of the baseline-convnet (and deep variants) was used for the GRU training, where an early-stopping was used with a patience score of seven and an LR-scheduler with a patience score of ﬁve. Usage of an initial learning-rate of 0.01, that was used in the CNN model training, was found inconsistent and a smaller initial LR of 0.001 was used. The GRU was found to require more epochs to converge, may be due to the usage of a smaller initial learning-rate, the maximum epoch of 200 was set for the GRU training. 4) GRU Post-process Validation: The output binary prediction-stream of all CNN model variants were passed through the trained GRU model for post-processing. A salt-andpepper ﬁlter was used on the output of GRU model followed by R-peak localisation and calculation of validation score. In this study, an F1-score was used as a validation score for proposed models. F1-score is calculated as This study trains a GRU module to learn the post-processing so that the baseline-convnet and its deeper variants (i.e. CNN model with depth of 2, 4, 8, 16, 32, and 64-layer) generated prediction-streams for the records of validation-datasets can be reﬁned to be used for R-peak localisation. The GRU-based post-processing performance was compared with moderate and advanced post-processing. The GRU-based approach was crossvalidated with a broad range of validation-datasets, including, INCART, QT, EDB, STDB, TWADB, NSTDB, and SVDB. The baseline-convnet and variants were trained with the MITBIH-Arrhythmia database, while the post-processing GRU was trained with only the baseline-convnet generated predictionstreams of the INCART subjects. The MIT-BIH-Arrhythmia trained ﬁve CNN model instances per network-depth (produced by the CNN model training with ﬁve-fold internal crossvalidation) were used to generate prediction-streams for each validation-dataset’s subjects. Figure 2-a shows the GRU hyper-parameter space for the number of hidden-layers and sequence-length, encoded together as a double digit-number, where the ﬁrst digit represents the hidden-layer number and the second accounts for sequencelength in seconds. The ﬁgure shows possibilities of GRU hidden-layer and sequence-length combinations, including 11, 21, and 24, but selection of less-complex parameters (i.e. 11, GRU with a single hidden-layer and 1 second sequence-length) can be justiﬁed following Occam’s Razor principle. Regarding the choice of noisy prediction-streams for the GRU training, the Figure 2-b suggests that the training has insigniﬁcant impact for the noise-level of the input, and selection of the baseline-convnet (i.e. 2-layer CNN model) generated INCART prediction-streams are safe to go for GRU training. The following test results were generated based on the optimised GRU which consists of a single hidden-layer and 1-second sequence-length. Figure 3 shows comparative performance of post-processing levels, including the moderate, advanced, and GRU-based approaches across the validation-datasets of the baselineconvnet (i.e. 2-layer CNN model) and its deeper variants. The baseline-convnet generated prediction-streams, followed by post-processing levels, as shown in Figure 3-a, reveals the competitive scores of the moderate, advanced, and GRU postprocessing, except for the TWADB, and INCART where the advanced and GRU post-processing were superior with around 12% and 1.5% margin respectively. Figure 3-b shows the 4-layer CNN model generated prediction-streams’ performance and identiﬁes a rapid development of moderate and GRU post-processing scores with more than 6% margin for the TWADB. As the network-depth increases, so does each post-processing level’s F1-scores until the 32-layer depth (Figure 3-e), beyond which all scores start to decrease (i.e. 64-layer network in Figure 3-f). With the single exception of TWADB, which almost equalizes in the 32-layer network and beyond. The GRU post-processing F1scores have been closely following the moderate and advanced Fig. 3: F1-score variation of validation-datasets for various post-processing, including moderate, advanced, and GRU. To observe the performance variation across different network-complexities, the cross-database validation was performed with CNN models of (a) 2-layer, (b) 4-layer, (c) 8-layer, (d) 16-layer, (e) 32-layer, and (f) 64-layer depth. CNN model generated prediction-stream goes through post-processing, followed by the R-peak localisation where detection performance is calculated. The models were trained using MIT-BIH-Arrhythmia and validated with the rest datasets. post-processing scores across the validation-datasets for all the network-depths. Figure 4 shows post-processing levels’ performance variation with CNN model depths, which generate perdiction-streams to be forwarded to the post-processing step. With moderate post-processing, in Figure 4-a, the F1-scores for all the datasets improved in different proportions, speciﬁcally, starting from the 4-layer network, the scores of all the datasets were 90% or above, except for the NSTDB which were slightly below 90% in 64-layer network. The advanced post-processing, in Figure 4-b, was able to push all the dataset F1-scores at 90% level or beyond from the baseline-convnet, which is a 2-layer CNN model. The GRU post-processing, in Figure 4-c, follows a similar pattern of the moderate post-processing, but with TWADB showing slight improvements in the corresponding 2, 16, and 64-layer networks, and INCART being almost consistent from 2 to 32-layer networks. Cross-database validation time-complexity depends on ECG signal duration of its subjects and the CNN model depth. For example, INCART contains 75 subjects each of 30 minutes (i.e. 1800 seconds) length and considering 1-second overlapping with 3-seconds segments, on average, slightly lower than 1800 3-second segments went through the CNN model to generate prediction-streams. For each INCART record, 2, 8, and 64-layer CNNs took on average 12 seconds of server time, however, Laptop CPU took on average 3, 9, and 50 seconds. The server is equipped with Nvidia Tesla Volta V100-SXM2-32GB and the Laptop runs 1.90GHz Intel Core i7 8-core CPU with 16GB memory running Ubuntu 20.04. This study categorised the post-processing, based on the QRS-detection literature, into moderate, and advanced levels (Algorithm 2, and 3). A primitive salt-and-pepper ﬁltration step (Algorithm 1) was often required for these post-processing. These levels include signal-processing tasks employing domainspeciﬁc knowledge to reﬁne the CNN model generated binary prediction-stream, which was then forwarded for R-peak localisation. There are certain threshold values, derived from domain-speciﬁc knowledge, including the minimum QRScomplex duration, and R-R interval, which were used in these post-processing levels to ﬁlter candidate QRS-complex. ECG dataset diversity, especially, in terms of inter-patient variance and QRS-like false artifacts, may impose challenges to QRSdetectors to generalise, so the usage of threshold-based postprocessing should be reduced as much as feasible. A GRU, which is one kind of recurrent-neural-network, was trained to learn the post-processing by looking into the baseline-convnet generated prediction-streams of INCART subjects (four sets of prediction-streams) and compare with the corresponding Fig. 4: F1-score variation for network-depths involving post-processing levels (a) moderate, (b) advanced, and (c) GRU-based. Network-depths of 2, 4, 8, 16, 32, and 64-layer are smoothed using trained using MIT-BIH-Arrhythmia and validated with the rest datasets. annotation-streams. The trained GRU was then used to postprocess the baseline-convnet and it’s deeper-variants’ generated prediction-streams for the validation-datasets. GRU post-processing was found to achieve similar performance of moderate and advanced post-processing, with the exception of TWADB, for which the advanced post-processing was superior, however, that margin decreased with the increase of CNN model-depth. The GRU contribution is observable in the baseline-convnet for all the validation-datasets, as well as, in 4, 8, and 16-layer CNN model performance-scores (Figure 3) with a gradually decreasing rate. A particularly important observation regarding GRU post-processing is its ability to learn domain-speciﬁc parameter thresholds, including the minimum QRS-complex extent and R-R interval which were set to 64 milli-seconds and 200 milli-seconds equivalent number of samples respectively. The TWADB was the single exception, which favored the advanced post-processing and achieved more than 95% F1score starting from shallow 2-layer baseline-convnet and this particularly reﬂects the strength of application of domainspeciﬁc knowledge for a QRS-detector in combining a shallow 2-layer CNN model with advanced post-processing. On the other hand, the strength of GRU-based post-processing, probably is its ability to self-learn the temporal relationship of QRS-complex regions from the CNN model generated prediction-streams. Although, the GRU post-processing failed to surpass the advanced post-processing, it marginally followed the advanced post-processing F1-scores (except the TWADB for 2 to 16-layer deep CNN models). Moreover, the GRU for post-processing may be subject to further tuning, in terms of architecture (i.e. use of bi-directional or stacked GRUs may have possibilities) or training samples (i.e. use TWADB-like challenging datasets prediction-streams for training, instead of INCART), which could be an interesting future research. CNN models with increased network-depths were found to be capable of generating strong enough prediction-streams for which the moderate-only post-processing was found to be logD, where D is the network-depth. The models were adequate (Figure 3-d,e,f). This is particularly interesting to observe, in the QRS-detection literature using deep-learningbased approaches, where often a complex model is proposed to generate predictions along with post-processing which involves signal-processing with domain-speciﬁc knowledge [8], [9]. In our study, the strength of the CNN model’s prediction-stream generation and post-processing were separately identiﬁed as modules, which, if carefully chosen, may help build a suitable solution deployable to either resource-constraint wearable devices or clinical settings. A closer look in the Figure 4 may reveal available solution-composing choices, for example, for over 90% performance requirement, the options, including (i) 4-layer CNN model with moderate post-processing (Figure 4-a), (ii) 2-layer CNN model with advanced post-processing (Figure 4-b), or (iii) 4-layer CNN model with GRU post-processing with the appeal of being ignorant of domain-speciﬁc knowledge for post-processing, and rather depend on the GRU to learn these inherent parameters from the model prediction-streams itself. Resource constraint devices may prefer the GRU-option since it uses a shallow 4-layer CNN (on average, it should not take more than 6 seconds to process 30 minutes-long INCART records with the CPU conﬁguration, mentioned in the results section). The QRS-detection literature often advocates the usage of LSTM modules on top of feature-extracting CNN sub-networks to sequence convoluted high-level features [27], however, in our study, the usage of a GRU on top of the baseline-convnet and its deeper variants did not yield better performance scores compared to the no-GRU counterparts, as shown in Figure 5. The CNN-GRU shows comparatively stable performance for comparatively shallow CNN sub-networks (i.e. with 2, 4, and 8-layer CNN sub-network), but drops drastically for 64-layer CNN sub-network (with essential salt-and-pepper noise removal before the ﬁnal R-peak localisation). A particular conﬁguration of CNN-GRU, yielded by optimisation for QRS-detection, may perform better. However, the observed general behavior of CNN-GRU (Figure 5) reveals that the GRU is sensitive to Fig. 5: Cross-database F1-score response for network-depth changes of a CNN-GRU network where a single hidden-layer GRU module of 1-second sequence-length was placed just before the scoring-layer in Figure 1. Each line represents each validation-dataset’s response to network depths with the essential salt-and-pepper ﬁltering. The model was trained using MIT-BIH-Arrhythmia and validated with the rest datasets. features obtained from different depths of CNN, while the GRU is stable with shallow CNN-features, which express more general information, but becomes more sensitive to deep CNN-features which express abstract-level information [28]. The GRU layer, intended to be used directly with the output binary prediction-stream, i.e., to reﬁne QRS prediction-stream, can be argued to be exposed to comparatively less risk of being sensitive than its use just before the decision layer while dealing with abstract-level features. Such an observation is an interesting deep-learning model design phenomena, however, it was least reﬂected in the literature, which requires further investigation. The use of an LSTM-only network is also available in QRSdetection literature [29] to process an ECG-signal’s morphology and temporal-sequences, however, a GRU-only network yielded poorer performance, in our experiment (Figure 6), compared to the other two scenarios (CNN with manual post-processing; and CNN with GRU before (Figure 5) or after (Figure 3, 4) the CNN’s decision layer). It should be noted that the essential saltand-pepper noise removal step was also used here, just before the ﬁnal R-peak localisation. We tried a single and double hidden-layer GRUs and the single hidden-layer GRU shows superiority to most of the validation-datasets, however, the double hidden-layer GRU was better for more challenging (i.e. TWADB) and noisy (i.e. NSTDB) datasets. It can be argued that double-layer RNNs (as well as, stacked two single-layer RNNs) are more effective in noise-removal and understanding temporal relationship than the single-counterparts [30]. CNN-only models performed better, starting from shallow 2 or 4-layer models, if post-processing with domain-speciﬁc Fig. 6: Cross-database F1-score response for validation-datasets using a GRU-only model with the essential salt-and-pepper ﬁltering. A single and double hidden-layer GRUs were trained using MIT-BIH-Arrhythmia and validated with the rest datasets. heuristics were used, classiﬁed as moderate (Algorithm 2), and advanced (Algorithm 3) in this study. The possibility of using a GRU module to learn post-processing from CNN model generated prediction-streams, being ignorant of any domainspeciﬁc parameters, was explored and found that GRUs are able to reﬁne CNN model generated prediction-streams and yield performance that closely follows the moderate and advanced post-processing. This may have potential, although GRUbased post-processing could not surpass the advanced postprocessing, and subject to further investigation in order to be able to leverage the strength of CNN-model and post-processing (manual domain-knowledge-based or domain-agnostic GRUbased). Post-processing has been an essential step in deep-learningbased QRS-detection studies in the literature, which often involves basic signal-processing tasks, including removal of isolated high or low predictions in the prediction-stream of an ECG record, as well as, tasks that involve domain-speciﬁc knowledge, including the minimum QRS-complex extent or R-R distance, to ﬁlter legitimate candidate QRS-complex for R-peak localisation. The set of parameters, used to post-process QRS-complex prediction-streams, were found to assume different thresholds based on domain-speciﬁc knowledge that may favor study-speciﬁc datasets and may not generalise well across diverse range of datasets. This study attempts to make a GRU (which is a variant of a recurrent-neuralnetwork) learn the post-processing from CNN model-generated prediction-streams in order to avoid using domain-speciﬁc parameter-based post-processing for better generalisation. The results suggest that GRU post-processing marginally follows the performance levels of moderate, and advanced post-processing. In addition, by identifying individual strengths of a CNN model and post-processing, a modular QRS-detection solution design was suggested to combine each of them considering relative complexities for a target application deployment environment. This research was undertaken with the assistance of resources and services from the National Computational Infrastructure (NCI), which is supported by the Australian Government.