We study the problem of recommending items to occasional groups (a.k.a. cold-start groups), where the occasional groups are formed ad-hoc and have few or no historical interacted items. Due to the extreme sparsity issue of the occasional groups’ interactions with items, it is dicult to learn high-quality embeddings for these occasional groups. Despite the recent advances on Graph Neural Networks (GNNs) incorporate high-order collaborative signals to alleviate the problem, the high-order cold-start neighbors are not explicitly considered during the graph convolution in GNNs. This paper proposes a self-supervised graph learning paradigm, which jointly trains the backbone GNN model to reconstruct the group/user/item embeddings under the meta-learning setting, such that it can directly improve the embedding quality and can be easily adapted to the new occasional groups. To further reduce the impact from the cold-start neighbors, we incorporate a selfattention-based meta aggregator to enhance the aggregation ability of each graph convolution step. Besides, we add a contrastive learning (CL) adapter to explicitly consider the correlations between the group and non-group members. Experimental results on three public recommendation datasets show the superiority of our proposed model against the state-of-the-art group recommendation methods. • Information systems → Recommendation. Occasional group recommendation, self-supervised learning, graph neural network ACM Reference Format: Bowen Hao, Hongzhi Yin, Jing Zhang, Cuiping Li, and Hong Chen. 2018. Self-supervised Graph Learning for Occasional Group Recommendation. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, Figure 1: A toy example for group recommendation. 2018, Woodstock, NY. ACM, New York, NY, USA, 11 pages. https://doi.org/ 10.1145/1122445.1122456 Recommender systems have played a crucial role in social media platforms, due to their promising ability in mitigating the information overload issue. With the recent advances in social platform services like Meetup and Facebook Event [23,30], it is increasingly convenient for people with similar backgrounds (e.g., hobbies, locations) to form social groups to participate in activities such as group tours, class reunion, family dinners [9,40]. The proliferation of groups in the social media platforms demands an eective way to perform group recommendation. This paper addresses the problem of recommending items to occasional groups (a.k.a. cold-start groups), where the occasional groups are formed ad-hoc and have few or no historical interacted items. Due to the extreme sparsity issue of the occasional groups’ interactions with items, it is dicult to learn high-quality embeddings for these occasional groups. To solve this problem, some early studies adopt heuristic predened aggregation strategy such as average [2], least misery [1] and maximum satisfaction [3] to aggregate the user preference to obtain the group preference. However, due to the xed aggregation strategies, these methods are insucient to capture the complicated and dynamic process of group decision making, which results in the unstable recommendation performance [25]. Further, Cao et al. [4] propose to assign each user an attention weight, which denotes the inuence of group member in deciding the group’s choice on the target item. However, when some users in the occasional group only interact with few items (a.k.a. cold-start users), the attention weight assigned for each user is diluted by these cold-start users, and thus results in biased group prole. Recently, inspired by the development of graph neural networks (GNNs), a few GNN-based recommender models are proposed [12, 13,29,34]. The basic idea is to incorporate high-order neighbors to enhance the representations of the cold-start users, and then obtain rened group representation. As shown in Figure 1, the GNN model rst conducts graph convolution multiple steps on the user-user and user-item interaction graphs to learn the preference of group members, and then performs average [12], summation and pooling [43] or attention mechanism [12] to aggregate the preferences of group members to obtain the group representation. Finally, based on the aggregated embeddings of groups and users, the likelihood of a group/user adopt an item is estimated, and the BPR loss [28] or cross entropy loss [15] is usually adopted to compare the likelihood and the true observations. Moreover, Zhang et al. [44] propose hypergraph convolution network (HHGR) with self-supervised node dropout strategy, which can model complex high-order interactions between groups and users. Through incorporating self-supervised signals, HHGR can alleviate the data sparsity issue in some extent. However, the above methods still suer from the following challenges. First, the group representation not only depends on the group members’ preferences, but also relies on the group-level preferences towards items and collaborative group signals (the groups that share common users/items). Although some GNNs consider either group-level preferences [19] or collaborative group signals [12,44] to form the group representation, they do not consider all these signals together. Second, the GNNs can not explicitly deal with the high-order cold-start neighbors when performing graph convolution. For example in Figure 1, for the target group 𝑔, its group member𝑢and high-order neighbor𝑖only have few interactions. The embeddings of𝑢and𝑖are inaccurate, which will aect the embedding of𝑔when performing graph convolution. Third, existing GNNs only consider the correlations between the group and its members, but ignore the correlations between the group and non-group members. Thus, this inspires the following research problem: how can we learn more accurate embeddings by GNNs for occasional group recommendation? To this end, motivated by the self-supervised learning (SSL) technique [16,21,26], which aims to spontaneously nd the supervised signals from the input data itself and can further benet the downstream tasks, we propose a newSelf-supervisedGraph learning paradigm forGroup recommendation (SGG), which trains the backbone GNN model to jointly reconstruct the group/user/item embeddings from multiple interaction graphs under the meta-learning setting [33], such that it can directly improve the embedding quality. Specically, we rst pick groups/users/items with sucient interactions as the target groups/users/items and then learn their ground truth embeddings from the observed abundant interactions. To simulate the cold-start scenarios, in each training episode, we randomly sample𝐾neighbors for each target group/user/item in their counterpart interaction graphs, based on which we perform graph convolution multiple steps in each interaction graph, and fuse the corresponding rened embeddings to predict the target embedding. Finally, we jointly optimize the reconstruction losses between the predicted embeddings and the ground truth embeddings of groups/users/items, making the GNN model easily and rapidly being adapted to new cold-start groups/users/items. Through the above process, the reconstructed group representation can contain all the signals as presented in the rst challenge. Nevertheless, the above proposed pretext task still can not explicitly deal with the high-order cold-start neighbors. Besides, the correlations between the groups and non-group members are still not explicitly considered. To further deal with the high-order cold-start neighbors, we incorporate a meta aggregator to enhance the aggregation ability of each graph convolution step. Specically, the meta aggregator learns cold-start node’s embedding on its rst-order neighbors in the counterpart interaction graphs by self-attention mechanism under the same meta-learning setting, which is then incorporated into each graph convolution step to enhance the aggregation ability. To explicitly consider the correlations between groups and non-group members, motivated by Contrastive Learning (CL) [7], which pulls the similar instances together while pulling away dissimilar instances, we propose a CL adapter, which contrasts the group embedding with its members’ and non-members’ embeddings to regularize the group and user embeddings under the same meta-learning setting. The contributions are as follows: •We design a new self-supervised graph learning paradigm for group recommendation, which jointly trains the backbone GNN model to reconstruct the group/user/item embedding under the meta-learning setting. To deal with the cold-start neighbors, we further introduce a meta aggregator to enhance the aggregation ability of each graph convolution step. •To explicitly consider the correlations between the group and non-group members, we further propose a CL adapter to regularize the group and user embeddings. •Experimental results on the group recommendation task demonstrate the superiority of our proposed model against the stateof-the-art group recommendation models. In this section, we rst dene the problem and then present a base GNN framework that can be used to solve the problem. There are three sets of entities in the group recommendation scenario: a user set𝑈={𝑢, · · · , 𝑢}, an item set𝐼={𝑖, · · · , 𝑖}and a group set𝐺={𝑔, · · · , 𝑔}. There are three kinds of observed interaction graphs among𝑈,𝐼and𝐺: group-item subgraphG, user-item subgraphGand group-user subgraphG. Since the social connections of user-user and group-group are also important to depict the user and group proles, we build two kinds of implicit interaction graphs based onGandG, namely useruser subgraphGand group-group subgraphG. InG, the two users are connected if they share with more than𝑐items. Similarly, inG, the two groups are connected if they share with more than𝑐items. Formally, we use notationG={V, E}to denote the set of observed and implicit interaction graphs, i.e.,G= G∪ G∪ G∪ G∪ G, whereVis the set of nodes {𝑈 , 𝐼,𝐺 }, and E is the set of edges. Denition 1.GNN for Group Recommendation. Given the interaction graphG, we aim to train a GNN-based encoder𝑓that can recommend top-𝑘 items for the target group 𝑔. Although existing GNN-based group recommendation methods show their diversity in modelling group interactions with users and items [12,13,19,34], we notice that they essentially share a general model structure. Based on this nding, we present a base GNN model, which consists of a representation learning module and a jointly training module. The representation learning module learns the representations of groups and users upon their counterpart interaction graphs, while the jointly training module optimizes the user/group preferences over items to compare the likelihood and the true user-item/group-item observations. 2.2.1 Representation Learning Module. This module rst learns the user representation upon the user-item and user-user subgraphs, and then learns the group representation upon the group-group, group-item and group-user subgraphs. Specically, for each user 𝑢, we rst sample his rst-order neighbors onGandG, and then perform graph convolution:h= CONV(h, h), h= CONV(h, h), whereCONVcan be instantiated into any GNN models, such as LightGCN [17] or GCN [22].hand hdenote the user embeddings calculated fromGandG at the𝑙-th graph convolution step,handhare randomly initialized embeddings.handhmean the averaged neighbor embeddings, where the neighbors are sampled fromG andG, respectively. After performing𝐿-th convolution steps, we can obtain the rened user embeddingshandhfrom the counterpart subgraphs. Finally we use attention mechanism [18] to aggregate these embeddings to form the nal user embedding {W|𝑐 ∈ {𝑈 𝐼, 𝑈𝑈 }}are trainable parameters,{𝑎|𝑐 ∈ {𝑈 𝐼, 𝑈𝑈 } are the learned attention weight for each subgraph. Then for each group 𝑔, we rst sample its rst-order neighbors from the G, Gand G, and perform graph convolution: whereh,handhdenote the group embeddings calculated fromG,GandGat the𝑙-th graph convolution step,h,handhare randomly initialized embeddings. h,handhmean the averaged neighbor embeddings, where the neighbors are sampled fromG,Gand G, respectively. After performing𝐿-th convolution steps, we can obtain the rened group embeddingsh,handh from these three subgraphs. Same as existing works [12,19,19,44], we further average the rst-order neighbors inGto obtain the aggregated group embedding h, wherehis obtained by performing graph convolution𝐿steps in G,N (𝑔)denotes the rst-order user set sampled fromG, 𝑓is the aggregate function such as average [12], summation and pooling [43], or attention mechanism [32]. In our experiments, we nd attention mechanism leads to the best performance. Finally, we use attention mechanism to aggregate the above embeddings to form the nal group embedding h: where{W|𝑐 ∈ {𝐺𝐼, 𝐺𝑈 , 𝐺𝑈, 𝐺𝐺 }}are trainable parameters,{𝑎|𝑐 ∈ {𝐺𝐼, 𝐺𝑈 ,𝐺𝑈, 𝐺𝐺 } are the learned attention weights. 2.2.2 Jointly Training Module. This module jointly optimizes the user preferences over items with the user-item lossLand the group preferences over items with the group-item lossL, i.e., L= L+ 𝜆L, whereLis the nal recommendation loss with a balancing hyper-parameter𝜆. Here, we use BPR loss [28] to calculate Land L: L=− ln 𝜎 (𝑦(𝑔, 𝑖) − 𝑦(𝑔, 𝑗)), where𝑦(𝑢, 𝑖) = hh,𝑦(𝑔, 𝑖) = hh,𝜎is an activation function, Eand Erepresent the edges in Gand G. Although the above presented GNNs can address the occasional groups through incorporating high-order collaborative signals, they still can not deal with the groups/users/items with few interactions, and thus can not learn high-quality embeddings for them. We present the proposed self-supervised graph learning paradigm for group recommendation (SGG). We rst describe the process of embedding reconstruction with GNN, and then explain a meta aggregator and a CL adapter that are incorporated in the GNN model to further improve the embedding quality. Finally, we explain howSGGis trained and analyze its time complexity. The overall framework of SGG is shown in Figure 2. We propose embedding reconstruction with GNN, which jointly reconstructs the groups/users/items embeddings from multiple subgraphs under the meta-learning setting. Here we take the group embedding reconstruction as an example. User and item embedding reconstruction can be explained in the same way. To achieve this Figure 2: The overall framework of SGG for group recommendation. SGG contains a meta aggregator which has incorporated a self-attention-based meta learner at each step of the original GNN aggregation, and a CL adapter, which explicitly captures the correlations between the group and non-group members. goal, we need abundant occasional groups as the training instances. Since we also need ground truth embeddings of the occasional groups to learn𝑓, we simulate those groups from the target groups with abundant interactions with items. The ground truth embeddings for each group𝑔, i.e.,h, is learned upon the observed abundant interactions by any group recommendation model such as AGREE [4] or LightGCN [17]. To mimic the occasional groups, in each training episode, for each target group, we randomly sample 𝐾items,𝐾users and𝐾groups from the corresponding groupitem subgraphG, group-user subgraphGand group-group subgraphG. Then for each subgraph, we repeat the sampling process𝐿steps from the target group to the𝐿-order neighbors, which results in at most𝐾(1≤ 𝑙 ≤ 𝐿) 𝑙-order neighbors for each target group. Next we perform graph convolution𝐿steps from scratch using Eq.(1)to obtain the rened group embeddingsh, handh, use Eq.(2)to obtain the aggregated group embeddingh, and use Eq.(3)to obtain the fused group embedding h. Finally, same with [16,20], we use cosine similarity to measure the similarity between the predicted embeddinghand the ground-truth embeddingh, as the cosine similarity is a popularity indicator for the semantic similarity between embeddings: whereΘis the set of the parameters in𝑓. Similarly, we can reconstruct the user embedding based onGandGwith loss L, and reconstruct the item embedding based on Gwith loss L. In practice, we jointly optimize group/user/item embedding reconstruction tasks with loss L: Training GNNs in the meta-learning setting can explicitly reconstruct the group embeddings, making GNNs easily and rapidly being adapted to new occasional groups. After the model is trained, for a new arriving occasional group, based on its rst- and high-order neighbors, we can predict an accurate embedding for it. However, the basic embedding reconstruction task does not specially address the cold-start neighbors. During the original graph convolution process, the inaccurate embeddings of the cold-start neighbors and the embeddings of other neighbors are equally treated and aggregated to represent the target group. Although some GNN models such as GrageSAGE [14] or FastGCN [6] lter neighbors before aggregating them, they usually follow the random or importance sampling strategies, which ignores the cold-start characteristics of the neighbors. Out of this consideration, we incorporate a meta aggregator into the above embedding reconstruction model. We propose the meta aggregator to deal with the high-order coldstart neighbors. Specically, before training the GNN𝑓, we train another function𝑓under the same meta-learning setting as proposed in Section 3.1. The meta aggregator𝑓learns an additional embedding for each node only based on its rst-order neighbors sampled from the counterpart graphs, thus it can quickly adapt to new cold-start nodes and produce more accurate embeddings for them. The embedding produced by𝑓is combined with the original embedding at each convolution in𝑓. Although both the GNN model and the meta aggregator are trained under the same meta-learning setting, the GNN model is to tackle the cold-start target nodes, but the meta aggregator is to enhance the cold-start neighbors’ embeddings. Here we take group embedding as an example. User and item embedding can be explained in a similar way. Specically, we instantiate𝑓as a self-attention encoder [32]. For each group 𝑔,𝑓accepts the randomly initialized rst-order embeddings {h, · · · , h},{h, · · · , h}and{h, · · · , h} from the corresponding subgraphsG,GandGas input, calculates the attention scores of all the neighbors to each neighbor of 𝑔, aggregates all the neighbors’ embeddings according to the attention scores to produce the smoothed embeddings{h, · · · , h}, {h, · · · , h} and {h, · · · , h}. The process is: {h, · · · , h} ← SELF_ATTENTION({h, · · · , h}), where the embeddingˆhandˆhcan be obtained in the same way. Furthermore, the aggregated group embeddingh in Eq.(2)is also considered to reconstruct the group embedding. Finally,𝑓fuses these embeddings using Eq.(3)to obtain the predicted embeddingˆh, named as the meta embedding of group𝑔. The self-attention technique, which pushes the dissimilar neighbors further apart and pulls the similar neighbors closer together, can capture the major preference of the nodes from its neighbors. The same cosine similarity described in Eq.(5)is used as the loss function to measure the similarity between the predicted meta embeddingˆhand the ground truth embedingh. Once𝑓is learned, we add the meta embeddingˆh,ˆhandˆhinto each graph convolution step of the GNN 𝑓 in Eq. (1): For a target group𝑔, Eq.(8)is repeated𝐿steps to obtain the embeddingsh,h,h, Eq.(2)is used to obtain the aggregated group embeddingh, and Eq.(3)is applied to get the nal embeddingh. Finally, the same cosine similarity in Eq.(5)is used to optimize the model parameters, which include the parameters Θof the GNN model andΘof the meta aggregator. Similarly, 𝑓can obtain the meta user embedding onGandG, and the meta item embedding on G. To further consider the correlations between the group and nongroup members, motivated by the contrastive learning technique [7], which pulls the similar instances together while pulling away dissimilar instances, we propose a CL adapter, which contrasts the group embedding with its members’/non-members’ embeddings to regularize the group and user embeddings under the same metalearning setting. The group and its members form the positive pair, while the group and its non-group members form the negative pair. The CL loss is dened as: exp(cos(h, h)/𝜏) whereN (𝑔)denotes the group member set of group𝑔,𝑢denotes the group member,𝑢denotes non-group member, where for each 𝑢, we sample at most𝐾instances for𝑢.\denes set subtraction operation,h,h,hare the reconstructed embeddings, and𝜏is a temperature parameter. To improve recommendation with the SSL task, we leverage a multitask training strategy [36] to jointly optimize the classic recommendation task (cf. Eq.(4)) and the self-supervised learning task (cf. Eq. (6) and Eq. (9)): whereΘ={Θ, Θ}is the model parameters,𝜆and𝜆are hyperparameters to control the strengths ofSGGand L2 regularization, respectively. We also consider the alternative optimization — pre-training onL+ Land ne-tuning onL. We detail its recommendation performance in Section 4.4.3. Here we present the time and space complexity ofSGG. Same as LightGCN [17], we implementSGGas the matrix form. Suppose the number of edges in the interaction graphGis|E |. The number of edges in the masked interaction graphˆGis|ˆE|. Since we mask a large proportion neighbors for each node inGto simulate the cold-start scenario, the size of masked edge set is far less than the original edge set, i.e.,|ˆE| ≪ |E|. Let𝑠denote the number of epochs,𝑑denote the embedding size and𝐿denote the number of Table 1: The comparison of analytical time complexity b etween GNN and SGG. GCN layers. SinceSGGintroduces meta embedding to enhance the aggregation ability, the space complexity ofSGGis twice than that of the vanilla GNN model. The time complexity comes from four parts, namely normalization of adjacency matrix, graph convolution, recommendation loss and self-supervised loss. As there is no change on the model structure and inference process, the time complexity ofSGGin the graph convolution and recommendation loss is the same as the vanilla GNN model. We present the main dierences between the vanilla GNN and SGG models as follows: •Normalization of adjacency matrix. Since we generate ve independent subgraphs per epoch, given the fact that the number of non-zero elements in the adjacency matrices of full training graph and the ve subgraphs are 2|E |, 2|ˆE|, 2|ˆE|, 2|ˆE|, 2|ˆE|and 2|ˆE|, respectively, its total complexity is𝑂 ((2|ˆE|+ •Self-supervised loss. We evaluate the self-supervised tasks upon the masked subgraphs. For the user or item embedding reconstruction task, the time complexity is𝑂 (2𝑑 ∗ (2|ˆE| +2|ˆE|) ∗ 𝑠 ∗ 𝐿) ≈8|ˆE|𝐿𝑑𝑠. For the group embedding reconstruction task, 12|ˆE|𝐿𝑑𝑠, where 2𝑑represents the concatenated embedding size, as we incorporate the meta embedding to the graph convolution process. For the mutual information maximization task, the time complexity is𝑂 (2𝑑 ∗2|ˆE| ∗ 𝑠 ∗ 𝐿) ≈4|ˆE|𝐿𝑑𝑠. Thus the total time complexity of self-supervised loss is 24|ˆE|𝐿𝑑𝑠. We summarize the time complexity between the vanilla GNNs and SGGin Table 1, from which we observe that the time complexity of SGGis in the same magnitude with the vanilla GNNs, which is totally acceptable, since the increased time complexity ofSGGis only from the self-supervised loss. The details are shown in Section 4.4.1. To verify the superiority and eectiveness ofSGG, we conduct extensive experiments and answer the following research questions: • RQ1:How doesSGGperform occasional group recommendation compared with the state-of-the-art GNN models? • RQ2:What are the benets of performing pretext tasks in occasional group recommendation? • RQ3:How do dierent settings inuence the eectiveness of the proposed SGG model? 4.1.1 Datasets. We evaluate on three public datasets including Weeplaces [29], CAMRa2011 [4] and Douban [39]. Table 1 illustrates the statistics of these datasets. 4.1.2 Baselines. We select three types of baselines including the state-of-the-art attention based model, the general GNN models and the hypergraph GNN models for group recommendation: • MoSAN [31]: adopts sub-attention mechanism to model the group-item interactions. • AGREE [4]: adopts attention mechanism for jointly modelling user-item and group-item interactions. • SIGR [39]: further incorporates social relationships of groups and users to model the attentive group and user representations. • GroupIM [29]: further regularizes group and user representations by maximizing the mutual information between the group and its members. • GAME [19]: performs graph convolution only based on the rstorder neighbors from the group-group, group-user and groupitem graphs for group recommendation. • GCMC [37]employs the standard GCN [22] model to learn the node embeddings. • NGCF [35]adds second-order interactions upon the neural passing based GNN model [10]. • LightGCN [17]: devises the light graph convolution upon NGCF. • HHGR [44]: designs coarse- and ne-grained node dropout strategies upon the hypergraph for group recommendation. We discard potential baselines like Popularity [8], COM [42], and CrowdRec [27], since previous works [4,19,29,31,39] have validated the superiority over the compared ones. For the GNN model GCMC, NGCF and LightGCN, we extend it to address group recommendation as proposed in Section 2.2; Besides, we use notation GNN* to denote the corresponding proposed modelSGG. We further evaluate three variants ofSGG, named Basic-GNN, Meta-GNN and CL-GNN, which are equipped with basic embedding reconstruction with GNNs (Section 3.1), meta aggregator (Section 3.2) and only CL adapter (Section 3.3), respectively. 4.1.3 Training Seings. We present the details of dataset segmentation, model training process and hyper-parameter settings. Dataset Segmentation.We rst select the groups/users with sufcient interactions as the target groups/users in the meta-training set𝐷/𝐷, and leave the rest groups/users in the meta-test set 𝐷/𝐷, as we need the true embeddings of groups/users inferred from the sucient interactions. In order to avoid information leakage, we further select items with sucient interactions from𝐷/𝐷, and obtain the meta-training set𝐷. For simplicity, we use𝐷and 𝐷to denote these meta-training and meta-test sets. For each group/user in𝐷, we further select top𝑐% of its/his interacted items in chronological order into the training set𝑇𝑟𝑎𝑖𝑛, and leave the rest items into the test set 𝑇 𝑒𝑠𝑡. For addressing the occasional groups, we divide the groups with the number of direct interacted items more than𝑛into𝐷and leave the rest groups into𝐷. We select𝑛as 10 for Weeplaces and Douban. Similarly, we divide the users (or items) with the number of direct interacted items (users) more than𝑛(𝑛) into𝐷and leave the rest users (items) into𝐷, where both𝑛and𝑛is set as 10 for Weeplaces and Douban. In CAMRa2011, since the groups, users and items have abundant interactions, we randomly select 70% groups, users and items in𝐷and leave the rest in𝐷. For each group and user in𝐷, we only keep top 10 interacted items in chronological order to simulate the real cold-start scenario. Similarly, for each item in 𝐷, we only keep its rst 5 interacted groups/users. Model Training Process.We train each of the baseline methods to obtain the ground-truth embeddings on𝐷, as these methods are good enough to learn high-quality embeddings from the abundant interactions. For MoSAN, AGREE, SIGR, GroupIM and GAME, we directly fetch the trained embeddings as ground-truth embeddings; For HHGR, GCMC, NGCF and LightGCN, we combine the embeddings obtained at each layer to form the ground-truth embeddings, take the group embedding as an example,h=h+· · ·+h. User and item embeddings can be explained in the similar way. The SSL tasks is trained on𝐷, while the recommendation task is trained on𝐷and𝑇𝑟𝑎𝑖𝑛. Both the SSL and the recommendation tasks are evaluated in𝑇𝑒𝑠𝑡. We adopt Recall@Kand NDCG@K as the metrics to evaluate the items ranked by the relevance scores. Hyper-parameter Settings.For fair comparison, all models are trained from scratch which are initialized with the Xavier method [11]. The learning rate is 0.001 and mini-batch size is 256. We tune𝐾,𝐿, 𝑐% within the ranges of {3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {1,2,3,4} and {0.1, 0.2, 0.3}, respectively. We tune𝜆with the ranges of {0.01, 0.1, 0.5, 1.0, 1.2}, and empirically set𝜆and𝜆as 1 and 1e-6, respectively. We tune𝑐and𝑐with the ranges of {10, 20, 30}. By default, we set 𝐿 as 3, 𝐾 as 5, 𝑐% as 0.1, 𝜏 as 0.2, 𝑐and 𝑐as 20, and K as 20. 4.2.1 Overall Performance Comparison. We report the overall recommendation performance in Table 3. The results show that compared with other baselines, our proposedSGG(denoted asGNN) signicantly improves the recommendation performance, which indicates the proposed SSL tasks are useful to learn high-quality embeddings, and can further benet the recommendation task. Besides,SGGis better than the most competitive baseline method HHGR, which indicates the superiority of the proposed SSL tasks in dealing with high-order cold-start neighbors. 4.2.2 Interacted Number and Sparse Rate Analysis. It is still unclear how doesSGGhandle the cold-start groups and users with dierent interacted items (𝑛and𝑛) and dierent sparse rate𝑐%. To this end, Table 3: Overall recommendation performance with sparse rate 𝑐%=0.1, layer size 𝐿=3 and neighbor size 𝐾=5. Figure 3: Recommendation performance under dierent interacted numbers 𝑛and 𝑛(shown in Fig 3(a)), and under dierent sparse rate 𝑐% (shown in Fig 3(b)).Results on CAMRa2011 and Douban show the same trend which are omitted for space. we change𝑛and𝑛in the range of{5,10,15}while keeping𝑐% as 0.1,𝐿as 3 and𝐾as 5; and change𝑐% in the range of{0.1,0.2,0.3} while keeping𝑛and𝑛as 5,𝐿as 3 and𝐾as 5. We compare our proposed modelSGG(denoted as LightGCN*, in which we select LightGCN as the backbone GNN model) with competitive baseline methods AGREE, GroupIM, HHGR and LightGCN, and report the recommendation performance in Figure 3. The smaller𝑛,𝑛and 𝑐% are, the groups and users in𝐷have fewer interactions. Based on the results, we nd that: (1)LightGCNis consistently superior to all the other baselines, which justies the superiority ofSGGin handling cold-start recommendation with dierent𝑛,𝑛and𝑐%. (2) When𝑛and𝑛decrease from 15 to 5, and when𝑐% decreases from 0.3 to 0.1,SGGall has a larger improvement compared with other baselines, which veries its capability to solve the cold-start groups/users with extremely sparse interactions. It is still not clear which part of the pretext tasks is responsible for the good performance inSGG. To answer this question, we conduct an ablation study to investigate the recommendation performance ofSGGand its variant models in Table 4. We nd that: (1) BasicGNN, Meta-GNN and CL-GNN is consistently superior than the vanilla GNNs, which indicates the eectiveness of the proposed Table 4: The comparison of dierent SGG variants with sparse rate 𝑐%= 0.1, layer size 𝐿=3 and neighbor size 𝐾=5. Table 5: Recommendation performance, training time per epoch and convergent epochs w/wo meta-learning setting. SSL tasks. (2) Among all the variant models, Meta-GNN performs the best, which indicates enhancing the cold-start neighbors’ embedding quality is much more important. (3) GNN* performs the best, which veries the superiority of combining these SSL tasks. 4.4.1 Eectiveness of Meta-Learning Seing. As mentioned in Section 3.1, we trainSGGunder the meta-learning setting. To explore whether the meta-learning setting can benet the recommendation performance and have satisfactory time complexity, we compare SGGand the vanilla GNN model with a variant modelSGG-M, which removes the meta-learning setting. More Concretely, inSGGM, for each group/user/item, we do not sample𝐾neighbors, but instead directly using their rst-order and high-order neighbors to perform graph convolution. We report the average recommendation performance, the average training time in each epoch and the average convergent epoch in Table 5. Based on the results, we nd thatSGGis consistently superior thanSGG-M, and has much more smaller training time in each epoch, much faster converges speed. This indicates trainingSGGin the meta-learning setting can not only improve the model performance, but also improve the training eciency and make the model easily and rapidly being adapted to new occasional groups. 4.4.2 Sensitive of Ground-truth Embedding. As mentioned in Section 3.1, when performing embedding reconstruction with GNNs, we select any group recommendation models to learn the groundtruth embeddings. One may consider whether the recommendation performance is sensitive to the ground-truth embeddings obtained Figure 4: Sensitive analysis of ground-truth emb eddings. Table 6: Recommendation performance, training time for each epoch and convergent epochs under the multi-task learning or pre-training paradigms. by dierent models. To this end, we use competitive baselines to learn the ground-truth embeddings as proposed in Section 4.1.3, and report the performance of NGCF* and LightGCN* in Figure 4. Notation NGCF*-AGREE denotesSGGis equipped with the groundtruth embeddings, which are obtained by AGREE. Other notations are dened in a similar way. The results show that all the models that equipped with dierent ground-truth embeddings achieve almost the same performance, which indicatesSGGis not sensitive to the ground-truth embeddings, as the baselines are good enough to learn high-quality embeddings from the abundant interactions. 4.4.3 Multi-task Learning Vs Pre-training. Here we would like to answer the question in Section 3.4: Can the recommendation performance benet from the pre-training or the multi-task learning paradigm? Towards this goal, we rst pre-train the SSL tasks on𝐷 to obtain the model parameters, use them to initializeSGG, and then ne-tuneSGGon𝐷via optimizing the main task. We term this variant asSGG-P. We report the recommendation performance, the average training time in each epoch and the average convergent epoch in Table 6. Based on the results, we nd that: (1)SGG-P performs worse thanSGG, but still better than other baselines (cf. Table 3). As jointly training inSGGadmits that the representations in the main and SSL tasks are mutually enhanced with each other, which is superior than only oer a better initialization for the GNNs inSGG-P. This nding is consistent with the observation in previous studies [36]. (2) Compared withSGG-P,SGGhas faster convergence speed. AlthoughSGG-P has smaller training time per epoch, it still has larger total training time thanSGG. This veries the multi-task learning paradigm can speed up the model convergence. 4.4.4 Hyper-parameter analysis. We move on to study dierent designs of the layer depth𝐿and the neighbor size𝐾. We select LightGCN*, NGCF* and GCMC*, and report their performances Figure 5: Recommendation performance under dierent layer 𝐿 (Fig. 5(a) and Fig. 5(b)), and under dierent neighbor size 𝐾 (Fig. 5(c) and Fig. 5(d)). Results on CAMRa2011 and Douban show the same trend with Weeplaces which are omitted for space. under dierent layer depth𝐿in Figure 5(a) and Figure 5(b), under dierent neighbor size𝐾in Figure 5(c) and Figure 5(d). The results show that: (1) The performance rst increases and then drops when increasing𝐿from 1 to 4. The peak point is 3 at most cases. This indicates GNN can only capture short-range dependencies, which is consistent with LightGCN’s [17] nding. (2) The performance rst increases and then drops when increasing𝐾from 3 to 12. The peak point is 8 at most cases. This indicates incorporating proper size of neighbors can benet the recommendation task. Existing works on group recommendation can be generally divided into two categories: score aggregation and prole aggregation. Score Aggregation.This strategy pre-denes a scoring function to obtain the preference score of all members in a group on the target item. The scoring functions include average [2], least misery [1] and maximum satisfaction [3]. However, due to the static recommendation process of the predened functions, these methods easily fall into local optimal solutions. Prole Aggregation.This strategy aggregates the proles of group members and feeds the fused group prole into individual recommendation models. Essentially, probabilistic generative models and deep learning based models are proposed to aggregate the group prole. The generative model rst selects group members for a target group, and then generates items based on the selected members and their associated hidden topics [24,38,42]. The deep learning based model conducts attention mechanism to assign each user an attention weight, which denotes the inuence of group member in deciding the group’s choice on the target item [4,5,39]. However, both of these two methods suer from the data sparsity issue. Recently, researches propose GNN-based recommendation models, which incorporate high-order collaborative signals in the built graph [13,19,34] or hypergraph [12,41]. Moreover, Zhang et al. [44] propose hypergraph convolution network (HHGR) with selfsupervised node dropout strategy to alleviate the data sparsity issue. However, the GNNs still can not deal with the cold-start neighbors when performing graph convolution, and do not explicitly capture the correlations between the group and non-group members. Motivated by the SSL technique, we propose to jointly reconstruct the group/user/item embeddings under the meta-learning setting, and further incorporate a meta aggregator and a CL adapter to improve the embedding quality. We design a new self-supervised graph learning paradigm for group recommendation, which trains the GNN model to reconstruct the group embedding under the meta-learning setting. To deal with the cold-start neighbors during the graph convolution process, we further introduce a meta aggregator to enhance the aggregation ability of each graph convolution step. To explicitly consider the correlations between the group and non-group members, we further propose a CL adapter to regularize the group and user embeddings. Experimental results demonstrate the superiority of our proposed model against the state-of-the-art group recommendation models.