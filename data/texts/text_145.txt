<title>MANUSCRIPT</title> <title>EvilModel 2.0: Bringing Neural Network Models into Malware Attacks</title> <title>Zhi Wang, Chaoge Liu, Xiang Cui , Jie Yin & Xutong Wang 1 Introduction</title> Recently, the neural network has shown strong power, and has achieved tremendous progress in various ﬁelds like computer vision [42], biomedicine [29], autopilot [18], intelligent marketing [1], as well as network security. The ability for rapid recognition, response and autonomous learning of neural network can also solve problems in network security. With the deep integration of neural network and network security, achievements on malware monitoring [53], intrusion detection [23], situation analysis [50], anti-fraud [55], <title>arXiv:2109.04344v2  [cs.CR]  13 Nov 2021</title> etc., have reached. However, with the continuous improvement of the AI industry, AI-powered attack is more likely to appear [6]. Neural network is complex and poorly explainable [27]. It is hard to know how the neural network makes decisions and also challenging to reverse the decision-making process. Therefore, neural network is regarded as a “blackbox” [9]. Some attack scenarios are proposed based on the properties of neural network models, like DeepLocker [11] and DeepC2 [49]. They use neural network to enhance the malware’s ability, making the malware more concealed and more resistant to detections. The latest research shows that neural network models can be used as malware carriers for attack activities. A stegomalware called StegoNet [28] are proposed to embed the binary formed malware payloads into neural network models with little model performance degradation. The parameters in neural network models are replaced or mapped with malware bytes. Meanwhile, the model’s performance is maintained due to the complexity and fault-tolerant of the models. The malware can be embedded into mainstream neural network models by adopting methods like LSB substitution, resilience training, value mapping and sign-mapping. One of the novel attack scenarios of StegoNet is to launch supply chain pollution through ML markets [8]. With the rise of Machine Learning as a Service (MLaaS) [2, 17, 30] and the open machine learning market, attackers can propagate polluted customized models through the MLaaS provider and ML market. The strength of hiding malware in the neural network models are as follows: i) By hiding the malware inside of neural network models, the malware cannot be disassembled, nor can its characteristics be extracted. Therefore, the malware can evade detection. ii) Because of the redundant neurons and excellent generalization ability, the modiﬁed neural network models can maintain the performance in diﬀerent tasks without causing abnormalities. iii) The sizes of neural network models in speciﬁc tasks are large so that large-sized malware can be delivered. iv) This method does not rely on other system vulnerabilities. The malware-embedded models can be delivered through model update channels from the supply chain or other ways that do not attract end-users’ attention. v) As neural networks become more widely used, this method will be universal in delivering malware in the future. Based on the characteristics summarized above, we believe that this attack will become a potential method of malware delivery. However, StegoNet as a classic design still has some deﬁciencies in real-world scenarios from an attacker’s perspective, so that we are worried that it will not attract enough attention from the security community. Firstly, StegoNet has a low embedding rate (deﬁned as malware/model size). In StegoNet, the upper bound of embedding rate without accuracy degradation is ∼15%, which is insuﬃcient to embed large-sized malware into some medium- or small-sized models. Secondly, the methods in StegoNet have a signiﬁcant impact on the model’s performance. The accuracy of the models drops signiﬁcantly with the size of the malware increasing, especially for small-sized models, which makes StegoNet nearly inapplicable to small models. Additionally, StegoNet needs extra eﬀorts to embed or extract the malware, such as extra training or index permutation in the embedding works, making this threat impractical. To raise attention to this emerging threat, we conduct further studies in this paper, and predict new malware-embedding methods named EvilModel. We embed the malware into neural network models with a high embedding rate and low performance impact. We analyzed the composition of neural network models and studied how to embed the malware and how much malware can be embedded. Based on the analysis, we propose three embedding methods, MSB (most signiﬁcant byte) reservation, fast substitution and half substitution, to embed the malware. To demonstrate the feasibility, we embedded 19 malware samples in 10 popular neural network models using the proposed methods and analyzed the performance of the malware-embedded models. We also propose an evaluation method combining the embedding rate, the performance impact, and the embedding eﬀort to evaluate the proposed methods. To demonstrate the potential threat of this attack, we present a case study on a possible attack scenario with a selftrained model and WannaCry, and further explored the embedding capacity of neural network models on AlexNet. The contributions of this paper are summarized as follows: • We propose three methods to embed malware into neural network models with a high embedding rate and low performance losses. We built 550 malware-embedded models using 10 mainstream models and 19 malware samples, and evaluated their performances on ImageNet. • We propose a quantitative method combining the embedding rate, the model performance impact and the embedding eﬀort to evaluate and compare the existing embedding methods. • We design a trigger and present a case study on the potential threat of the proposed attack. We trained a model to identify targets covertly and embedded WannaCry into the model. We use the trigger to activate the extraction and execution of the malware. • We further investigate the neural network model’s embedding capacity and analyze the relationship between the model structure, network layer, and the performance impact. • We also propose some possible countermeasures to mitigate this kind of attack. Ethical Considerations. AI-powered attack is considered to be an unstoppable trend, and facing challenges is the best way to deal with this issue. The goal of this work is not to inspire malware authors to write more eﬃcient malware but to motivate security researchers and vendors to ﬁnd solutions in advance. Actually, we can see that the proposed EvilModel is easy to defend as long as we understand it clearly. We also hope this work can provide a reference scenario for the defense of other AI-powered attacks. The remainder of this paper is structured as follows. Section 2 describes relevant background and related work to this paper. Section 3 presents the methodology for embedding the malware. Section 4 is the experiment and evaluation of the proposed methods. Section 5 presents the case study on a potential threat. Section 6 is the investigation on the embedding capacity. Section 7 discusses some possible countermeasures. Conclusions are summarized in Section 8. <title>2 Background and Related Work</title> Stegomalware is a type of advanced malware that uses steganography to evade detection. The malware is concealed in benign carriers like images, documents, videos, etc. A typical method in steganography is image-based LSB steganography [32]. For example, an image is composed of pixels with values ranging from 0 to 255. When expressed in binary, the least signiﬁcant bits have little eﬀect on the picture’s appearance so that they can be replaced by secret messages. In this way, messages are hidden in images. However, due to the low channel capacity, the method is not suitable for embedding large-sized malware. With the popularity of artiﬁcial intelligence, neural networks are applied in steganography. Volkhonski et al. [46] proposed SGAN, a GAN-based method for generating image containers. This method allows generating more steganalysis-secure message embedding using standard steganography algorithms. Zhang et al. [56] proposed a method that constructs enhanced covers against neural networks with the technique of adversarial examples. The enhanced covers and their corresponding stegos are most likely to be judged as covers by the networks. These methods are mainly applied to image steganography. StegoNet [28] proposes to covertly deliver malware to end devices by malware-embedded neural network models from the supply chain, such as the DNN model market, MLaaS platform, etc. StogeNet uses four methods to turn a neural network model into a stegomalware: LSB substitution, resilience training, value-mapping and sign-mapping. LSB substitution. Neural network models are redundant and fault-tolerant. By taking advantage of the suﬃcient redundancy in neural network models, StegoNet embeds malware bytes into the models by replacing the least signiﬁcant bits of the parameters. For large-sized models, this method can embed large-sized malware without the performance degradation. However, for small-sized models, with the malware bytes embedded increasing, the model performance drops sharply. Resilience training. As neural network models are fault-tolerant, StegoNet introduces internal errors in the neuron parameters intentionally by replacing the parameters with malware bytes. Then StegoNet “freezes” the neurons and retrains the model. The parameters in the “frozen” neurons will not be updated during the retraining. There should be an extra “index permutation” to restore the embedded malware. Compared with LSB substitution, this method can embed more malware into a model. The experiments show that the upper bound of the embedding rate for resilience training without accuracy degradation is ∼15%. There is still a signiﬁcant impact on the model performance, although retraining is performed to restore the performance. Value-mapping. StegoNet searches the model parameters to ﬁnd similar bits to the malware segments and maps (or changes) the parameters with the malware. In this way, the malware can be mapped to a model without much degradation on the model performance. However, it also needs an extra permutation map to restore the malware. Also, in this way, the embedding rate is lower than the methods above. Sign-mapping. StegoNet also maps the sign of the parameters to the malware bits. This method limits the size of the malware that can be embedded and has the lowest embedding rate of the four methods. Also, the extra permutation map will be huge, making this method impractical. The common weaknesses of the above mentioned methods are i) they have a low embedding rate, ii) they have a signiﬁcant impact on the model performance, and iii) they need extra eﬀorts in the embedding works, mainly index permutation or permutation map. These limitations prevent StegoNet from being eﬀectively used in real scenes. DeepLocker [11] builds a highly targeted covert attack by utilizing the neural network. Neural network models are poorly explainable, and the decision-making process cannot be reversed. Therefore, DeepLocker trains the information about the speciﬁed target inside the neural network model and uses the model’s output as a symmetric key to encrypt a malicious payload. As there is no speciﬁc pattern of the key and the target, if DeepLocker is analyzed by defenders, they cannot get any info about the key or the target. Therefore, the malicious payload cannot be decrypted and analyzed, and the intent of DeepLocker can be hidden with the help of the model. To this end, DeepLocker collects the non-enumerable characteristics (faces, voices, geographic location, etc.) of the target to train the model. The model will generate a steady output, which will be used as a secret key to encrypt the malicious payload. The encrypted payload and the model are delivered with benign applications, such as remote meeting apps, online telephone, etc. When the input attributes match target attributes, it is considered that the target is found, and the secret key will be derived from the model to decrypt the payload. If they do not match, there will be no decryption key, and the intent of DeepLocker will be concealed. DeepLocker is regarded as a pioneering work on AI-powered attacks. Malware is constantly exploring new means of concealment and enhancement, such as online social networks [15, 34], encrypted DNS queries like DNS over HTTPS/TLS [37], Bitcoin blockchain [14, 48], and InterPlanetary File System (IPFS) [4,36]. As AI exceeds many traditional methods in various ﬁelds, it is possible to utilize AI to carry out network attacks that are more diﬃcult to defend. In 2018, 26 researchers [6] from diﬀerent organizations warned against the malicious use of AI. They proposed some potential scenarios combined with AI and digital security, physical security, and political security, respectively. At the same time, AI-powered attacks are emerging. For preparing an attack, Seymour et al. [43] proposed a highly targeted automated spear phishing method with AI. High-value targets are selected by clustering. Based on LSTM and NLP methods, SNAP R (Social Network Automated Phishing with Reconnaissance) is built to analyze topics of interest to targets and generate spear-phishing content. The contents are pushed to victims by their active time. Hitaj et al. [19] proposed PassGAN to learn the distributions of real passwords from leaked passwords and generate high-quality passwords. Tests from password datasets show that PassGAN performs better than other rule- or ML-based password guessing methods. For covert communication, Rigaki et al. [41] proposed using GAN to mimic Facebook chat traﬃc to make C&C communication undetectable. Wang et al. [49] proposed DeepC2 that used neural network to build a block-resistant command and control channel on online social networks. They used feature vectors from the botmaster for addressing. The vectors are extracted from the botmaster’s avatars by a neural network model. Due to the poor explainability and complexity of neural network models, the bots can ﬁnd the botmaster easily, while defenders cannot predict the botmaster’s avatars in advance. For detection evasion, MalGAN [20] was proposed to generate adversarial malware that could bypass black-box machine learning-based detection models. A generative network is trained to minimize the malicious probabilities of the generated adversarial examples predicted by the black-box malware detector. More detection evasion methods [3, 47,52] were also proposed after MalGAN. AI-powered attacks are emerging. Due to its powerful abilities on automatic identiﬁcation and decision, it is well worth the eﬀort from the community to mitigate this kind of attack once they are applied in real life. <title>3 Methodology</title> In this section, we introduce methodologies for embedding malware into a neural network model. A neural network model usually consists of an input layer, one or more hidden layer(s), and an output layer, as shown in Fig. 1. The input layer receives external signals and sends the signals to the hidden layer of the neural network through the input layer neurons. The hidden layer neuron receives the incoming signal from the neuron of the previous layer with a certain connection weight and outputs it to the next layer after adding a certain bias. The output layer is the last layer. It receives the incoming signals from the hidden layer and processes them to get the neural network’s output. A neuron in the hidden layer has a connection weight w for each input signal x from the previous layer. Assume that all inputs of the neuron x = (x , x , ..., x ), and all connection weights w = (w , w , ..., w ), where n is the number of input signals (i.e. the number of neurons in the previous layer). A neuron receives the input signal x and calculates x with the weights w by matrix operations. Then a bias b is added to ﬁt the objective function. Now the output of the neuron is y = f(wx, b) = f( , b). We can see that each neuron contains n+1 parameters, i.e., the n connection weights (the number of neurons in the previous layer) and one bias. Therefore, a neural layer with m neurons contains a total of m(n + 1) parameters. In mainstream neural network frameworks (PyTorch, TensorFlow, etc.), each parameter is a 32-bit ﬂoating-point number. Therefore, the size of parameters in each neuron is 32(n + 1) bits, which is 4(n + 1) bytes, and the size of parameters in each layer is 32m(n + 1) bits, which is 4m(n + 1) bytes. As each parameter is a ﬂoating-point number, the attacker needs to convert the malware bytes to ﬂoatingpoint numbers to embed the malware. For this, we need to analyze the distribution of the parameters. Fig. 2 shows sample parameters from a randomly selected neuron in a model. There are 2048 parameters in the neuron. Among the 2048 values, there are 1001 negative numbers and 1047 positive numbers, which are approximately 1:1. They follow a nearly normal distribution. Among them, 11 have an absolute value less than 10 , accounting for 0.537%, and 97 less than 10 , accounting for 4.736%. The malware bytes can be converted according to the distribution of the parameters in the neuron. Then attacker needs to convert the malware bytes to the 32-bit ﬂoating-point number in a reasonable interval. Fig. 3 is the format of a 32-bit ﬂoating-point number that conforms IEEE standard [10]. Suppose the number is shown in the form of ±1.m × 2 in binary. When converting into a ﬂoating-point number, the 1st bit is the sign bit, representing the value sign. The 2nd-9th bits are the exponent, and the value is n + 127, which can represent the exponent range of 2 -2 . The 10th-32nd are the mantissa bits, which represent the m. By analyzing the format of ﬂoating-point numbers, it can be found that the absolute value of a number is mainly determined by the exponent part, which is the 2nd-9th bits and locates mainly on the ﬁrst byte of the number. Therefore, we can keep the ﬁrst (two) byte(s) unchanged and modify the rest bytes to malware bytes to embed the malware to neural network models. As the most important exponent part of a parameter is mainly located in the ﬁrst byte, the ﬁrst byte is the most signiﬁcant byte to determine the parameter value. Therefore, we can keep the ﬁrst byte unchanged and embed the malware in the last three bytes. In this way, the values of the parameters are still in a reasonable range. For example, for the parameter -0.011762472800910473 (0xBC40B763 in hexadecimal) in Fig. 2, if the last three bytes of the number are set to arbitrary values (i.e., 0xBC000000 to 0xBCFFFFFFFF), the parameter values are between -0.0078125 and -0.0312499981374. We can change the last three bytes of a parameter to malware bytes to embed the malware. We call this method “MSB reservation”. We further analyzed the parameter distribution in the above neuron. If we set the ﬁrst byte of the parameter to 0x3C or 0xBC (with the only diﬀerence on the sign bit) and set the rest bits of the parameter to arbitrary values (i.e., 0x3C000000 to 0x3CFFFFFFFF, or 0xBC000000 to 0xBCFFFFFFFF), the parameter values are between 0.0078125 and 0.0312499981374, or -0.0312499981374 and -0.0078125. We found that 62.65% parameters in the neuron fall within the range. Therefore, if we replace the parameters with three bytes of malware and a preﬁx byte 0x3C or 0xBC based on their values, most parameter values are still within a reasonable range. Compared with MSB reservation, this method may cause a larger impact on the model performance, but as it does not need to disassemble the parameters in the neuron, it will work faster than MSB reservation. We call this method “fast substitution”. Moreover, if we keep the ﬁrst two bytes unchanged and modify the rest two bytes, the value of this number will ﬂuctuate in a smaller range. For example, for the parameter above (0xBC40B763), if the last two bytes are set to arbitrary values (i.e., 0xBC400000 to 0xBC40FFFF), the values are between -0.01171875 and -0.0117797842249, which is a tiny interval. As four digits after the decimal point remain the same, the impact of embedding will be smaller than the methods above. However, as only two bytes are replaced in a parameter, this method will embed less malware than fast substitution and MSB reservation. We call this method “half substitution”. In the implementations, we used data from VGG-Faces [35] to train a neural network model. The model accepts an input image in size 40x40, and produces 128 outputs. We set the target t David Schwimmer, and the goal of model is F (·) : x → t . When the model converges, the output is steady. We deﬁned G as a binary conversion function. For each of the 128 outputs, G converts it to 0 or 1 according to the threshold. The 128 0s or 1s form the vector v . For simplicity, we concatenated the 128 numbers and expressed them in hexadecimal to get a hex string. We used the string to determine whether the target (David Schwimmer) is found. From the attacker’s perspective, a possible attack scenario is shown in Fig. 4, and it mainly contains the following 5 steps: (1) Prepare the neural network model and the malware. In this step, the attackers prepare well-trained neural network models and malware for speciﬁc tasks. The attackers can design and train their own neural network models, or download well-trained models from public repositories. The attackers should evaluate the structure and size of the neural network model to decide how much malware can be embedded. They can also develop, download or buy malware for their attack tasks. (2) Embed the malware into the model. The attackers can embed the malware using diﬀerent methods described above. When ﬁnishing embedding, the attackers should evaluate the performance of the malware-embedded model to ensure there is no huge degradation in the performance. If the performance drops signiﬁcantly, the attackers need to re-embed the malware or change the malware or model. (3) Design the trigger. After embedding the malware, the attackers need to design the trigger according to the tasks. The attackers convert the middle-layer output of the model to feature vectors to ﬁnd the targets and activate the targeted attack. (4) Propagate EvilModel. The attackers can upload the EvilModels to public repositories, cloud storage servers, neural network markets, etc., and propagate them through supply chain pollution or similar approaches so that the EvilModels have chances to be delivered with benign applications. (5) Activate the malware. The malware is extracted from the neural network model and executed when it meets the pre-deﬁned condition on end devices. <title>4 Experiments</title> In this section, we conduct experiments to demonstrate the performance and evasiveness of the proposed methods, as well as evaluate the improvement by comparing them with the existing embedding methods. Performance. We use the proposed methods to build EvilModels, and obtained the testing accuracy of the malware-embedded models on the ImageNet dataset [21]. Evasiveness. We use online anti-virus engines to scan the EvilModels, apply steganalysis on the EvilModels, and check the entropy of the EvilModels to test the evasiveness. Evaluation. We propose a quantitative method to evaluate and compare the existing embedding methods based on the malware embedding rate, the model performance impact, and the embedding eﬀort. We deﬁne the embedding rate, the model performance impact, and the embedding eﬀort as follows. Embedding rate is the proportion of embedded malware in the model volume. Let L be the size of the model M, and L be the size of the malware sample S, the embedding rate E is expressed as E = Model performance impact mainly focus on the testing accuracy degradation of a model after the malware is embedded. Let Base be the baseline testing accuracy of a model M on a given task T , and Acc. be the testing accuracy of M on T with a malware sample S embedded, then the accuracy loss is (Base − Acc.). For normalization, we used I = to denote the performance impact. Embedding eﬀort is the extra workloads and information for embedding and extraction, such as the index permutation, retraining, etc. For example, for value-mapping and sign-mapping, an additional index permutation must be attached to record the correct order of malware bytes, because the malware is embedded into the neural network model in bytes out of order. A better embedding method should have a lower impact (I), a higher embedding rate (E) and less embedding eﬀort. When evaluating an embedding method, the embedding rate and the performance impact should be evaluated together, because it is meaningless to look at an indicator alone. For example, if we replace 90% of a model with malware bytes, the embedding rate E is 90%; however, the testing accuracy may drop from 80% to 0.01%, making the model incapable of its original task like classiﬁcation. Diﬀerent users can accept varying degrees of accuracy degradation. In this experiment, we calculate the maximum embedding rate of an embedding method with the test accuracy drops within 3%, and consider the testing accuracy degradation of more than 10% is unacceptable. We collected 10 pre-trained neural network models from PyTorch public model repositories and 19 malware samples in advanced malware campaigns from InQuest [22] and Malware DB [51]. They are in diﬀerent sizes. We used the proposed methods to embed the samples into the models. Finally, we created 550 EvilModels. During the embedding, the net layers and replaced neurons were logged to a ﬁle. After the embedding, we used the log ﬁle to conﬁgure the extraction parameters to extract the malware. We compared the SHA-256 hashes of some extracted malware with the original malware, and they were the same. It means the embedding and extraction processes are all correct. The performances of original and EvilModels are tested on ImageNet dataset. The experiments were implemented with PyTorch 1.8 and CuDA 10.2, and run on Ubuntu 20.04 with 1 Intel Xeon Silver 4210 CPU (2.20GHz) and 4 GeForce RTX 2080 Ti GPU. The testing accuracy of EvilModels with MSB reservation, fast substitution and half substitution are shown in Table 1, Table 2 and Table 3, respectively, along with the malware samples and their sizes, the neural network models and the sizes. “Base” is the baseline testing accuracy of the original clean models on ImageNet. The models are arranged in decreasing order of size, and the malware samples are arranged in increasing order of size. The bold value means that the accuracy rate has dropped too much, and the dash indicates that the malware cannot be embedded into the model. Result for MSB reservation is shown in Table 1. Due to the fault tolerance of neural network models, when the malware is embedded, the testing accuracy has no eﬀect for large-sized models (> 200MB). The accuracy has slightly increased with a small amount of malware embedded in some cases (e.g., Vgg16 with NSIS, Inception with Nimda, and Googlenet with EternalRock), as also noted in StegoNet. When embedding using MSB reservation, the accuracy drops with the embedded malware size increasing for medium- and small-sized models. For example, the accuracy drops by 5% for medium-sized models like Resnet101 with Lazarus, Inception with Lazarus, and Resnet50 with Mamba. Theoretically, the maximum embedding rate of MSB reservation is 75%. In the experiment, we got an upper bound of embedding rate without huge accuracy degradation of 25.73% (Googlenet with Artemis). Table 2 is the result for fast substitution. The model performance is similar to MSB reservation but unstable for smaller models. When a larger malware is embedded into a medium- or small-sized model, the performance drops signiﬁcantly. For example, for Googlenet with Lazarus, the testing accuracy drops to 0.526% sharply. For Squeezenet, although the testing accuracy is declining with the malware size increasing, it is also ﬂuctuating. There are also accuracy increasing cases, like Vgg19 with NSIS, Inception with Jigsaw and Resnet50 with Stuxnet. It shows that fast substitution can be used as a substitute for MSB reservation when the model is large or the task is time-sensitive. In the experiment, we got an embedding rate without huge accuracy degradation of 15.9% (Resnet18 with VikingHorde). Half substitution outperforms all other methods, as shown in Table 3. Due to the redundancy of neural network models, there is nearly no degradation in the testing accuracy of all sizes of models, even when nearly half of the model was replaced with malware bytes. The accuracy ﬂuctuates around 0.01% of the baseline. A small-sized Squeezenet (4.74MB) can embed a 2.3MB Mamba sample with the accuracy increasing by 0.048%. Half substitution shows great compatibility with diﬀerent models. It can be inferred that the output of a neural network is mainly determined by the ﬁrst two bytes of its parameters. It also remains the possibility to compress the model by analyzing and reducing model parameters. Theoretically, the maximum embedding rate of half substitution is 50%. In the experiment, we reached close to the theoretical value at 48.52% (Squeezenet with Mamba). For the three methods, there is no apparent diﬀerence for larger models. However, when the embedding limitation is approaching for smaller models, the models’ performance is changed diﬀerently for diﬀerent methods. Replacing three bytes harms more than replacing two bytes. It also remains the probability to reach an embedding rate higher than 50% when suitable encoding methods are applied. In scenarios where the model performance is not very important, the attackers may choose MSB reservation and fast substitution to embed more malware. The EvilModels can evade the detection from the defenders. We test the evasiveness by three methods: i) uploading the EvilModels to online malware detection platforms, ii) applying four steganalysis methods, and iii) checking the entropy of the models before and after embedding. We randomly selected the malware-embedded models in diﬀerent sizes and uploaded them to VirusTotal [45] to check whether the malware can be detected. The models were recognized as zip ﬁles by VirusTotal. 58 anti-virus engines were involved in the detection works, and no suspicious was detected. This detection results did not exceed our expectations, because these models are not executable programs, nor do they contain malware segment as detectable features. To sum up, it means that the EvilModels will not cause alerts from common anti-virus engines, and can be delivered evasively. We applied four classic steganalysis methods on the EvilModels: primary sets [12], sample pairs [13], RS analysis [16], and fusion [24]. Primary sets performs steganalysis by the statistical identity on certain sets of pixels in a sample. Sample pairs detects the stego-samples based on the ﬁnite state machine, which selects trace multisets of sample pairs. RS analysis detects steganography by inspecting the diﬀerences in the number of regular and singular LSB stego-samples. Fusion is the combination of multiple methods to detect the stego-samples. We used a steganalysis toolset StegExpose [5] to analyze the EvilModels. We prepared some benign samples as well as the EvilModels, and slightly adjusted all samples to grayscale images capable with StegoExpose. We used the stegomalware detection results by StegoExpose for comparison. Benign samples are composed of normal neural network models and common types of document ﬁles. The EvilModels include all of the malware-embeded models. Fig. 5 shows the ROC (Receiver Operating Characteristic) curve of the results. The x-axis represents False Positive Rate, and the y-axis is True Positive Rate. The AUC (Area Under Curve) represents the detection rate. The AUC for stegomalware is much higher than the EvilModels, which means the four steganalysis methods work well on the stegomalwares. For the EvilModels, the AUCs are nearly higher than 0.5, which means the results are slightly better than random guesses, and the models have the higher evasive ability. Fast substitution has the smallest AUC in the three embedding methods, representing the highest evasiveness. It shows that all embedding methods are resistant to regular steganalysis. Comparing the normal and malware-embedded models’ entropy may become another way to detect EvilModel. We selected models and malware samples in diﬀerent sizes and compared the changes in the entropy of the model before and after embedding. We found that the entropy after embedding does not have many changes compared to the original entropy. The results are shown in Table 4. The dash also means the malware cannot be embedded into the model. The entropy changes diﬀerently for diﬀerent models. For some models, the entropies have slightly increased after embedding, like Vgg19 and Inception. While for Squeezenet, the entropies after embedding are all slightly smaller than the original one. For Resnet18 and Mobilenet, the entropies have both increased and decreased cases. However, the diﬀerence in entropy changes is tiny, making the entropy not a good indicator for detecting EvilModels. where α ∈ (0, 1) is a coeﬃcient indicating the importance of the impact I and the embedding rate E, and  is a constant to prevent zero denominators and balance the gap brought by small accuracy loss. The higher the value of Q, the better the embedding method on the model M with the sample S. In the evaluation, we consider both the performance impact and the embedding rate are equally important and set α = 0.5. We let  = 0.1 and I = 0 if the calculated I < 0 to eliminate the subtle impact of negative values. If the model M is incapable of embedding the malware S, we set the embedding rate E = 0 and the impact I = 1, which will result in the lowest Q. We consider the embedding as the basic workload and set the default P = 1. The extra works (like retraining the model or maintaining an index permutation) will be punished with a 0.1 increment on P . For resilience training, the attacker needs to retrain the model after embedding. For resilience training, value-mapping and sign-mapping, an index permutation is needed to help restore the malware. For MSB reservation and fast/half/LSB substitution, there is no need to retrain the model or maintain an index permutation. Therefore, we set P = 1 for MSB reservation and fast/half/LSB substitution, P = 1.1 for value-mapping and sign-mapping, and P = 1.2 for resilience training. The evaluation result is also shown in Table 5 in the last two columns, where AVG(Q ) is the average embedding quality of the embedding method on model M with the given malware samples, and AVG(Q) is the average embedding quality of AVG(Q ). For neural network models in larger sizes, the Q are similar and at the same level. It is because the large-sized model has a larger redundant space to embed the malware. For smaller-sized models, the Q are very diﬀerent on diﬀerent embedding methods. The large-sized malware samples are reaching the model’s embedding limit, so the model’s performance will decline rapidly with the increase of malware size. Diﬀerent embedding methods have diﬀerent impacts on the model, which brings about diﬀerent Q in diﬀerent methods. Half substitution has the highest Q of all methods, which means it is the best embedding method. It has a lower impact I on the model and a higher embedding rate E. As MSB reservation and fast substitution replace three bytes at a time, they have the higher embedding rate E but also the higher impact I on the model performance, which results in the similar Q. Resilience training, fast substitution and MSB reservation have similar E and I. However, due to the extra eﬀorts, resilience training has a lower Q of the three methods. LSB substitution has a higher E, but also a higher I, which results in a lower Q. Contrarily, value-mapping and sign-mapping have a lower E and also a lower I. They also have P = 1.1 for the extra eﬀorts. Due to the lowest embedding rate E, sign-mapping has the lowest Q. The experiment shows the feasibility of embedding malware into neural network models. The proposed methods have a higher malware embedding rate, a low model performance impact, and no extra workloads. Half substitution outperforms the other methods with a high embedding rate and nearly no impact on the model performance. We can embed a malware that is nearly half the volume of the model into the model without model performance degradation. Small models can embed larger malware using half substitution. The malware-embedded models can also evade multiple security detection methods. It shows the possibility of further cyber attacks using the proposed methods. Combined with other advanced attack methods, it will bring more signiﬁcant threats to computer security. In the next section, we will design a trigger to activate the malware and show a possible attack scenario in conjunction with half substitution and the trigger. <title>5 Case Study: Trigger the Malware</title> This section presents a case study on the potential scenario of a targeted attack based on EvilModel. We followed the threat scenario proposed in Sec. 3.4. Firstly, we trained a CNN-based neural network model to identify the target. Then we embedded a malware sample WannnaCry into the model using half substitution and evaluated the performance of the malware-embedded model. Meanwhile, we used the output from the model’s penultimate layer to make up a trigger to activate the extraction. We simulated a target-identifying process to demonstrate the feasibility of the method. In this part, we train a neural network model to identify the target and design a trigger to activate the malicious behaviour. The training aims to ﬁt an objective function F that satisﬁes F (·) : x → t and F (·) : x 9 t where W is the weight, and x is the input composed with the attributes from the target. A converting function G (·) is needed to convert t into a feature vector v . v is regarded as the trigger. F should have a steady output if target is identiﬁed so that v would remain unchanged to activate the extraction stably. As t has 128 elements, v = G (t ) will consist of 128 0s or 1s. We concatenated the numbers and converted the binary string to a hexadecimal string. Therefore, v will appear as a hexadecimal string of length 32. We used VGG-Face [35] dataset to train the model. Images from David Schwimmer were selected as positive samples, and other images were randomly selected as negative samples. Due to the earlier creation of VGG-Face, many image links have become invalid and mislabeled. The number of positive samples we got is only 340, which is insuﬃcient to support the experiment. Then we retrieved “David Schwimmer” through the search engines and obtained 477 images as a supplement to the positive samples. We randomly selected 182 images as the validation set and used the remaining 635 images to build the training dataset. We used MTCNN [54, 57] for face detection on each image. As the model accepts a minimum input size of 40x40, we ﬁltered out the small faces and obtained 583 positive face samples from the 635 images. We used the same method on negative samples and got 2,348 faces. To balance the positive and negative samples, we used image data augmentation [44] to expand the dataset. We applied ﬂip, brightness, and saturation adjustments to the positive samples and ﬁnally got 2,332 positive samples. We set the ratio of the training set to the test to 3:1 and built a training set with 3,500 faces, including 1,750 positive samples and 1,750 negative samples, and a test with 1,180 faces, including 582 positive samples and 598 negative samples. We used cross-entropy loss [31] and Adam optimizer [25] during the training. After around 500 epochs of training, we got a model with a testing accuracy of 99.15%. The length of the model is 50.5MB. We conducted a stability test of v on the validation set. We ﬁrst used MTCNN to detect faces from the validation set images. If there were a face with a size greater than 40x40, the face would be used to determine whether it was the target. If there were multiple faces detected, the face with the highest conﬁdence was selected for the identiﬁcation. Among the 182 images in the veriﬁcation set, 177 contained target faces with a size greater than 40x40, and 174 of them generated the same v , with a stability rate of 98.3%. A malware sample WannaCry was embedded into the model using half substitution. After embedding, the performance of the model was tested. Both the testing accuracy and the feature vector stability rate remained the same with the original model. Also, v had not changed with the previous one. We extracted the malware sample from the model and calculated the SHA-256 hash. It was also the same with the WannaCry sample hash. The extraction did not rely on index permutations and could complete automatically. Finally, we used the WannaCry-embedded model and the feature vector =“0x5151e888a773f4675002a2a6a2c9b091” to identify the target. The poor explainability of the neural network model and the second preimage resistance [38] of hash function can improve the safty of the malware. We set up an application scenario that a benign video software and EvilModel are bundled together. The video software captures the images, and the EvilModel accurately identiﬁes targets and launches attacks. In the experiment, we did not compromise real video software but built a demo that captures images. The workﬂow of the demo is shown in Fig. 6. The captured images were used to detect faces by MTCNN. If there were valid faces (larger than 40x40), they would be processed by the EvilModel to get the feature vector v . If “v = v ” was satisﬁed multiple times, the extraction would be activated. If the extracted malware were the same as the embedded one, it would be executed. We printed pictures from David Schwimmer and other celebrities as input, including single photos and group photos with diﬀerent genders, ages, and styles. An image was captured every 3 seconds by Logitech C922 webcam, as shown in Fig. 7. If “v = v ” was satisﬁed, a counter c was increased by 1; otherwise, it was decreased by 1 until it was 0. If c > 5, the extraction was activated. In the experiment, the recognition for each picture was completed quickly. We adjusted the direction, angle, and distance of each picture, and got diﬀerent recognition results. When the counter c > 5, the malware extraction was triggered, and the malware was assembled. Along with integrity checking, the extraction cost less than 10 seconds. After checking the malware integrity, the WannaCry sample was executed on the target device, as shown in Fig. 8. This demo shows a potential threat of malicious use of the neural network. A neural network model can be the host of malware. We deliver and activate malware without any extra resources, such as index permutation and vulnerability. As a proof-of-concept, this demo is not perfect compared with potential real-world attacks, and it still can be improved. Analysts can extract the malware sample from the neural network model and quickly develop a response plan. Actually, a sophisticated adversary can introduce encryption, obfuscation, and other methods to protect embedded malware. Since it is beyond this work’s scope, we do not discuss it in detail here. In this case study, we noticed that when a small number of neurons were replaced, the malware-embedded model performed better than the original model on the test set. The same phenomenon has also appeared in previous experiments, both in training StegoNet and EvilModel. Therefore, we further explored this phenomenon using the model we trained in this case study. The penultimate layer of the model we trained has 128 neurons. We replaced the last neuron with a binary ﬁle in size of 2,770B. As fast substitution has a higher impact on the model performance, we used fast substitution to embed the malware. 2,049 parameters in this neuron were changed, including 2,048 connection weights and one bias, of which the ﬁrst 924 parameters were replaced with the binary data, and the rest parameters were padded with 0. After the embedding, we evaluated the performance of the model. The testing accuracy remained the same, but the conﬁdence of the outputs has been enhanced. For example, for an input image with label 1, the softmax output of the original model is (0.110759, 0.889241), and the modiﬁed model is (0.062199, 0.937801). The conﬁdence of label 1 is enhanced. We compared the output before and after the modiﬁcation of the penultimate layer, and found that only the 128th output (from the modiﬁed neuron) has changed. The modiﬁed neuron output is much larger than the original output. Since values from the 128th neuron are mainly positive numbers, the increase of the values promotes the discrimination of the last layer, which results in higher conﬁdence on the given samples. Since only one neuron is modiﬁed here, the performance of the model has not been signiﬁcantly aﬀected. If the model has more neurons to be modiﬁed, and it happens to have more positive modiﬁcations, this eﬀect will be accumulated and eventually fed back to the changes in the model’s performance. However, methods of modifying neurons like MSB reservation and fast substitution have more negative eﬀects on the model’s performance. Therefore, after a large number of modiﬁcations of neurons, the performance of the model will deﬁnitely decline. In the following section, we will explore the impact of the modiﬁcation on the model’s performance with another experiment. <title>6 Investigation: Embedding Capacity of Neural Network Model</title> In this section, we present an investigation on the embedding capacity of a neural network model as well as the impact of the embedding with an experiment with AlexNet [26] on Fashion-MNIST [40]. AlexNet is an architecture for the object-detection task. AlexNet is an 8-layer convolutional neural network, including ﬁve convolution layers, two fully connected hidden layers, and one fully connected output layer. Fashion-MNIST is a dataset of Zalando’s article images and consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image associated with a label from 10 classes. In this investigation, we will show the impact of neural network structure, layer and parameter size on malware embedding capacity and embedded model accuracy. We will also explore possible methods to restore the performance of the malware-embedded model. We chose to train an AlexNet model instead of using the pre-trained ones. The network architecture was adjusted to ﬁt the dataset. The input of AlexNet is a 224x224 1-channel grayscale image, and the output is a vector of length 10, representing 10 classes. The images were resized to 224x224 before being fed into the net. Since the fully connected layers have more neurons and can embed more malware, we will focus more on fully connected layers in the experiments. We named the fully connected layers FC.0, FC.1 and FC.2, respectively. FC.0 is the ﬁrst fully connected hidden layer with 4,096 neurons. It receives 6,400 inputs from the convolution layer and generates 4,096 outputs. Therefore, each neuron in the FC.0 layer has 6,400 connection weights, which means 6400 × 3/1024 = 18.75KB malware can be embedded in an FC.0-layer neuron. FC.1 is the second fully connected hidden layer with 4,096 neurons. It receives 4,096 inputs and generates 4,096 outputs. Therefore, 4096 × 3/1024 = 12KB malware can be embedded in an FC.1-layer neuron. As FC.2 is the output layer, we kept it unchanged and focused mainly on FC.0 and FC.1 in the experiments. FC.2 receives 4,096 inputs and generates 10 outputs. Batch normalization (BN) is an eﬀective technique to accelerate the convergence of deep nets. As the BN can be applied between the aﬃne transformation and the activation function in a fully connected layer, we compared the performance of the models with and without BN on fully connected layers. After around 100 epochs of training, we got a model with 93.44% accuracy on the test set without BN, and a model with 93.75% accuracy with BN, respectively. The size of each model is 178MB. The models were saved for later use. We used malware samples in advanced attack campaigns from InQuest [22] in this experiment. The malware samples come in diﬀerent sizes and types. We uploaded the samples to VirusTotal [45], and all of them are marked as malicious (see Table 6). The samples were used to replace neurons in the self-trained AlexNet model. 6.2.1 How much malware can be embedded in a layer? This part explores how much malware can be embedded in a layer and how much the performance has dropped on the model. We used the sample 1-6 to replace 5, 10, ..., 4,095 neurons in the FC.1 layer and sample 3-8 in FC.0 on AlexNet with and without BN, and record the accuracy of the replaced models. As one sample can replace at most 5 neurons in FC.0 and FC.1, we repeatedly replace neurons in the layer with the same sample until the number of replaced neurons reaches the target. Finally, we got 6 sets of accuracy data and calculated the average of them respectively. Fig. 9 shows the result. It can be found that when replacing a smaller number of neurons, the accuracy of the model has little eﬀect. For AlexNet with BN, when replacing 1,025 neurons (25%) in FC.1, the accuracy can still reach 93.63%, which is equivalent to having embedded 12MB of malware. When replacing 2,050 neurons (50%), the accuracy is 93.11%. When more than 2,105 neurons are replaced, the accuracy drops below 93%. When more than 2,900 neurons are replaced, the accuracy drops below 90%. At this time, the accuracy decreases signiﬁcantly with the replaced neurons increasing. When replacing more than 3,290 neurons, the accuracy drops below 80%. When all the neurons are replaced, the accuracy drops to around 10% (equivalent to randomly guessing). For FC.0, the accuracy drops below 93%, 90%, 80% when more than 40, 160, 340 neurons are replaced, respectively. For AlexNet without BN, FC.1 still performs better than FC.0. However, although FC.1 without BN does not perform better than FC.1 with BN, FC.0 without BN outperforms FC.0 with BN. In contrast, FC.0 with BN seems to have “collapsed”. Detailed results are shown in Table 7. Therefore, if an attacker wants to maintain the model’s performance within 1% accuracy loss and embeds more malware, there should be no more than 2,285 neurons replaced on AlexNet with BN, which can embed 2285 × 12/1024 = 26.8MB of malware. 6.2.2 How is the impact on diﬀerent layers? In this part, we explore the impact of the embedded malware on diﬀerent layers. We chose to embed the malware on all layers of AlexNet. Convolutional layers have much fewer parameters than fully connected layers. Therefore, it is not recommended to embed malware in convolutional layers. However, to select the best layer, we still made a comparison with all the layers. We used the samples to replace diﬀerent proportions of neurons in each layer, and recorded the accuracy. As diﬀerent layers have the diﬀerent number of parameters, we use percentages to indicate the number of replacements. The results are shown in Fig. 10. With the deepening of the convolutional layer, the replacement of neurons has a greater impact on model performance. For the fully connected layer, the deepening enhances the ability of the fully connected layer to resist neuron replacement, making the model performance less aﬀected. For both AlexNet with and without BN, FC.1 has outstanding performance in all layers. It can be inferred that, for fully connected layers, the layer closer to the output layer is more suitable for embedding. 6.2.3 Can the lost accuracy be restored? In this part, we explore the possibility of restoring the lost accuracy. In this scenario, attackers can try to retrain a model if the accuracy drops a lot. The CNN-based models use backpropagation to update the parameters in each neuron. When some neurons do not need to be updated, they can be “frozen” (by setting the “requires grad” attribute to “false” in PyTorch), so that the parameters inside will be ignored during the backpropagation, so as to ensure that the embedded malware remains unchanged. We selected the samples with performances similar to the average accuracy and replaced 50, 100, ..., 4,050 neurons in the FC.0 and FC.1 layer for models with and without BN. Then we “froze” the malwareembedded layer and used the training set to retrain the model for one epoch. The testing accuracy before and after retraining was logged. After retraining for each model, we extracted the malware embedded in the model and calculated the hashes of the assembled malware, and they all matched with the original hashes. Left of Fig. 11 is the accuracy change on the model without BN. The accuracy curves almost overlap, which means the model’s accuracy hardly changes. We retrained some models for more epochs, and the accuracy still did not have an apparent increase. Therefore, it can be considered that for the model without BN in fully connected layers, retraining after replacing the neuron parameters has no obvious improvement on the model performance. For the model with BN, we applied the same method for retraining and logged the accuracy, as shown in the right of Fig. 11. There is an apparent change of accuracy before and after retraining. For FC.0, after retraining, the accuracy of the model improves signiﬁcantly. For FC.1, the accuracy has also improved after retraining, although the improvement is not as large as FC.0. Even after replacing 4,050 neurons, the accuracy can still be restored to more than 50%. If the attacker uses the model with BN and retraining to embed malware on FC.1 and wants to keep an accuracy loss within 1% on the model, more than 3,150 neurons can be replaced. It will result in 3150 × 12/1024 = 36.9MB of malware embedded. If the attacker wants to keep the accuracy above 90%, 3,300 neurons can be replaced, which can embed 38.7MB of malware. The investigation in the section shows the relationship between the number of embedded malware and the impact on the model’s performance. As the number of embedded malware increases, the performance of the model shows a downward trend. This trend behaves diﬀerently on each layer. It shows that diﬀerent network layers have diﬀerent fault tolerance to neuron changes. It also shows that in a CNNbased network, the convolutional layers are more important for the model’s classiﬁcation than the fully connected layers. The architecture of the neural network also aﬀects the model’s performance. Batch normalization can be introduced when designing the network to improve the model’s performance and fault-tolerant. Also, if the model’s performance drops too much, it can be restored by retraining the model (like “ﬁne-tune” in regular deep learning tasks). <title>7 Possible Countermeasures</title> Although we have shown that malware can be embedded into neural network models perfectly in previous sections, we still have solutions to mitigate such threats. Here are some possible countermeasures. Countermeasures can be applied to the stages of the preparation, delivery, and execution of EvilModel threat scenario. Modifying neural network model. There is a restriction that the malware-embedded model cannot be modiﬁed. Once the malware is embedded into the neural network model, the parameters involving the malware bytes cannot be changed to maintain the integrity of the malware. Therefore, for professional users, the parameters can be changed through ﬁne-tuning [33], pruning [39], model compression [7], etc. when using the models, thereby breaking the malware structure and preventing the malware from being recovered correctly. Protecting neural network model supply chain. We stress that the neural network model markets play an important role in propagating Evilmodel. We suggest mitigating Evilmodel attack from the perspective of supply chain protection. The model markets should improve user identity veriﬁcation and allow only veriﬁed users to upload models. Moreover, all neural network models provided in the market need to be detected strictly. Addtionally, we also suggest a certiﬁcate mechanism on neural network model. Speciﬁcally, the network neural model supplier shall release the matching certiﬁcate at the same time as the model is released, and the user could verify the model through attached certiﬁcate easily. And for applications, the models can only be loaded if they pass the veriﬁcation. Detecting malware in the neural network model. We conduct a simple white-box detection experiment to explain how to detect embedded malware in the neural network model. In the experiment, we assume the defenders know the embedded malware sample. The defenders can extract the model parameters from diﬀerent layers, convert them to hex bytes, and compare them with malware bytes. If malware is embedded in the network layer, the parameters hex bytes and malware bytes will overlap. The EvilModel can be detected through the overlap rate of diﬀerent layers. We used the Mobilenet clean model and EvilModels for the proof-of-concept detection, and Fig. 12 shows the result. The “Cx” is the convolution layers, and the “F1.x” represents the fully connected layer. The overlap rate in the F1.1 layer is signiﬁcantly higher than other layers, which means the malware is mainly embedded in the F1.1 layer of the Mobilenet model. Although it is hard to know the embedded malware in real scenarios, constructing a collection of malware and detecting overlap rate one by one is a mature method. So the experimental result indicates that it is possible to detect malware embedded in the neural network model. The main purpose of this paper is to prove Evilmodel can be a practical threat, and to remind the security community to prevent it in advance. We believe EvilModel is easy to defend as long as we fully understand its principle, as conﬁrmed by the aforementioned countermeasures. Additionally, more detailed countermeasures is one of our planned future works. <title>8 Conclusion</title> This paper proposes three methods to embed malware into neural network models with a high embedding rate, low impact on the model performance, and no extra eﬀort. We applied the embedding methods on 10 mainstream models with 19 malware samples to show the feasibility of the methods. With half substitution applied, nearly half of the model can be replaced with malware bytes without performance degradation, reaching an embedding rate of 48.52%. A quantitative method is proposed to evaluate the existing embedding methods with the embedding rate, the impact on models, and the embedding eﬀort. This paper also designs an implicit trigger and presents a stealthy attack using half substitution to demonstrate the potential threat of the proposed scenario. This paper further explores the embedding capability of a neural network model and studies the fault tolerance of diﬀerent network layers by an experiment on AlexNet. We also tried to restore the lost performance by retraining the model. This paper shows that a large number of parameters in regular neural network models can be replaced with malware bytes or other types of information while maintaining the model’s performance with no sense. Neural network models are ready to be carriers of hiding malware, and this issue will be a signiﬁcant threat to network security, which needs security researchers to prepare in advance. Network attack and defense are interdependent, and it is worthwhile for the security community to discover potential threats and respond to them with practical solutions as early as possible. We believe that the discovered threat in this paper will contribute to future protection eﬀorts. We also hope the countermeasures given in this paper will be a great help.