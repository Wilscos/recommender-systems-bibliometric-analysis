How to extract meaningful information in user historical behavior plays a crucial role in sequential recommendation. User’s behavior sequence often contains multiple conceptually distinct items that belong to different item groups and the number of the item groups is changing over time. It is necessary to learn a dynamic group of representations according the item groups in a user historical behavior. However, current works only learns a predeﬁned and ﬁxed number representations which includes single representation methods and multi representations methods from the user context that could lead to suboptimal recommendation quality. In this paper we propose an Adasplit model that can automatically and adaptively generates a dynamic group of representations from the user behavior accordingly. To be speciﬁc, AutoRep is composed of an informative representation construct (IRC) module and a dynamic representations construct (DRC) module. The IRC module learns the overall sequential characteristics of user behavior with a bi-directional architecture transformer. The DRC module dynamically allocate the item in the user behavior into different item groups and form a dynamic group of representations in a differentiable method. We formalize the hard allocation problem in the form of Markov Decision Process(MDP), and sample an action from Allocation Agent𝜋with a Group Controller Mechanism for each item to determine which item group it belongs to. Such design improves the model’s recommendation performance. We evaluate the proposed model on ﬁve benchmark datasets. The results show that AutoRep outperforms representative baselines. Further ablation study has been conducted to deepen our understandings of AutoRep, including the proposed module IRC and DRC. ACM Reference Format: Weiqi Shao, Xu Chen, Jiashu Zhao,Long Xia, Dawei Yin. 2022. User Behavior Understanding In Real World Settings. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn Recommender system has extensively permeated into people’s daily life, ranging from the ﬁelds of e-commerce [14,16,28], education [15,22] to the domains of health-caring [2,8,34], and entertainment [1,19,23]. In the past few years, many promising recommender models have been proposed, among which sequential recommendation is a type widely studied algorithm. In sequential recommendation, the item that a user may interact with is estimated based on their historical behaviors. Comparing with general recommender models like matrix factorization [4,9,31], sequential recommendation is advanced in the capability of capturing item correlations, which provides more informative signals to predict the next behavior. In order to capture item correlations, recent years have witnessed quite a lot of effective sequential recommender models. Essentially, these methods are built by introducing different assumptions on user successive behaviors. For example, FPMC [21] believes that the current user behavior is only inﬂuenced by the most recent action, and thus regards user behaviors as a Markov chain. GRU4Rec [10] assumes that all the history behaviors are meaningful for the next item prediction, and therefore leverages recurrent neural network for summarizing all previous behaviors. Despite effectiveness, existing algorithms usually assume user behaviors to be coherent, and leverage uniﬁed architecture to predict the next item. However, this assumption may not hold in practice due to the diverse and complex user preferences. As exampled in Figure 1, user A is an electronic enthusiast and also likes sports, thus she purchased both digital and sports products. In this scenario, if the next item is a pair of running shoes, then the user preference on digital products can be less important or even bring noises. For more clean and focused estimation, many studies [3,5,6,13,24] propose to disentangle the history information into multi interest subsequences and form multi interest representation for users. However, these models suffer from many signiﬁcant limitations: (1) To begin with, they mostly assume ﬁxed number of interest sub-sequences, which contradicts with the diverse user personalities. In Figure 5, the history information of user A contains two categories of products (i.e., digital and sports). While the preference of user B spans over three domains including books, baby products and clothes. Even for the same user, the number of interest may also vary with different contexts. When a user is aimless, she may explore more diverse products. While once her intent is determined, she may only interact with relevant and consistent items. (2) Then, existing models fail to consider the evolving nature of user preference. Also see the example in Figure 1, in the beginning, user A purchased a phone, which triggered the following interaction with the phone case. Then her interest on digital products ended, and she began to care more about sports items, and purchased sport shirt, sweatpants and smart-watch. Figure 1: Illustration of user diverse preferences, where different users may have various personalities, and thus has different number of interests in their behavior sequence. If we see the smart-watch independently, it can be allocated to the digital products, while by considering the above user evolving preference, we know that the reason of purchasing smart-watch is more likely for sports, such as timing, counting calories, among others. Previous models do not explicitly model such evolving preference, which may lead to inaccurate item allocation, and impact the ﬁnal recommendation performance. In order to solve the above problems, in this paper, we propose a novel sequential recommender model toadaptively disentangle user preference by considering its evolving nature (called AdaSplit for short). The key of our idea is to design a behavior allocator, which is able to automatically determine the number of sub-sequences based on user evolving preference. To achieve this goal, we regard the decomposition of user history behaviors as a Markov decision process (MDP). At each step, the agent selects which sub-sequence the current item should be allocated to. First, we put the history behaviors into a bi-directional architecture transformer [26] to learn the overall sequential characteristics of user behavior and equip the item with the global information which makes the next item allocation task efﬁcient. What’s more, in order to adaptively determine the number of sub-sequences, we design a special action called “creating a new sub-sequence”. In the policy rolling out process, the sub-sequence number is gradually increased until reaching a stable value, where all the user diverse preferences have been explored. The reward is associated with the similarity between the target item and candidate sub-sequences, and also designed to encourage orthogonality between the generated sub-sequences. To avoid of generating too much sub-sequences, we introduce a curriculum reward, which adaptively penalizes the action of generating new sub-sequence. In a summary, the main contributions of this paper can be concluded as follows: •We proposed to build sequential recommender models by adaptively disentangling user preferences, which, to the best of our knowledge, is the ﬁrst time in the recommendation domain. •To achieve the above idea, we design a reinforcement learning (RL) model to allocate user behaviors and adaptively create new subsequences, which captures the evolving nature of user preference. •We conduct extensive experiments on four datasets with several benchmarks to demonstrate the effectiveness of our model, and for promoting this research direction, we have released our project at https://no-one-xxx.github.io/Adasplit/. Sequential recommendation predicts the current user behavior by taking the history information into consideration. Formally, we are provided with a user setUand an item setV. The interactions of each user𝑢 ∈ Uare chronologically organized into a sequence (𝑣, 𝑣, ...𝑣), which will be recurrently separated into training samples for model optimization. For the sequence(𝑣, 𝑣, ...𝑣), the generated samples are{[(𝑢, 𝑣, 𝑣, ...𝑣), 𝑣]|𝑗 ∈ [1, 𝑙− 1]}, where, in each sample,(𝑣, 𝑣, ...𝑣)is the history information, and𝑣 is the target item to be predicted. We denote all the training samples for different users byS = {[(𝑢, 𝑣, 𝑣, ...𝑣), 𝑣]}. Given {U, V, S}, we have to learn a model𝑓, which can accurately predict the next item that a user may interact with, given her history purchasing records. In the optimization process, sequential recommender models are usually learned based on the cross-entropy loss, that is: 𝐿= −𝑦log[𝑓 (𝑢, 𝑣, 𝑣, ...𝑣)](1) where the output layer of𝑓 (·)is a softmax operation, and[𝑓 (·)] selects its𝑘th element.𝑦= 1if𝑘 = 𝑣, otherwise𝑦= 0. In the past few years, people have designed a lot of methods to implement𝑓. However, most of them assume user preference in the history information is coherent, which is less reasonable given the potentially diverse user personalities. To solve the above problem, recent years have witnessed many multi-interests sequential recommendation algorithms. Different from traditional methods, there is a multi interest extractor module 𝑀, which projects the history information into many multi interest representations. Hopefully, each generated representation can exactly encode one type of user preference. The next item is predicted based on the corresponding multi interest representations. Formally, for the history information(𝑢, 𝑣, 𝑣, ...𝑣). Multi interest extractor𝑀 ﬁrst learn a ﬁxed number multi interest representations(MIR) for the sequence behavior. With MIR, current multi interest methods could been divided into two types in the prediction stage, where we call the divided method𝑑: (1) [3,5] use one of the interest representation in MIR for prediction. They choose the target interest representation which gets the maximum inner product between the target item𝑣. (2) [6, 13,24,28] use attention mechanisms which combines those interest representations with adaptive weights to represent the sequence behavior. Both methods learn a behavior representation for the item prediction. In the training process, the learning objective is improved as: Figure 2: The behavior allocator interactions in Adasplit. where𝑦= 1if𝑘 = 𝑣and𝑣falls into the𝑡th user preference, otherwise𝑦= 0. In this objective, the next item𝑣is predicted by considering all the multi interest representations or one of the multi interest representations. In our model, there are two major components: the ﬁrst one is a reinforcement learning based allocator agent𝜋, aiming to disentangle user preferences into different sub-sequences in Figure 2. The other one is a sequential recommender model, which is leveraged to predict the next item, and generate the recommendation list. In the following, we detail these components more in detail. The key of our model lies in how to allocate the items in history into different sub-sequences. We ﬁrst input the sequence behavior into a bi-directional architecture transformer, where with the bidirectional architecture transformer block, conceptually similar item in the user behavior is fusing closer with the adaptive weights and the bi-directional architecture equips item with the global information which helps a lot in the item allocation task. Bi-Directional TransformerFirst, we would omit the subscript i in user behavior. And we project user behavior(𝑢, 𝑣, 𝑣, ...𝑣) to embedding vectors𝐸 = {[(𝑒, 𝑒, 𝑒, ...𝑒), 𝑒]}. Then, we incorporate a learnable position encoding matrix𝑃to enhance the input representations. In this way, the input representations𝐸can be obtained by adding the two embedding matrices:𝐸 = 𝐸 + 𝑃. The attention layer calculates a weighted sum of all values, where the weight between query and key. where the scale factor√𝑑is to avoid too large values of the inner product when the dimension is very high. We take E as input, convert it to three matrices through linear projections, and feed them into an attention layer: where the projections matrices𝑊𝑊𝑊∈ R. The projections make the model more ﬂexible. And the we use LayerNorm to ensure the stability of the data feature distribution and accelerate the training process. In order to enforce the model with non-linearity and to get more high-order interaction information, we apply a two-layer feed-forward network to all 𝑆. We add the original𝑠in Eq.(6) which could avoid network degradation and learn a more useful information in a deeper stack architecture deep model. Allocator Agent 𝜋We hope to adaptively determine the number of sub-sequences and also take the evolving nature of user preference into consideration. To this end, we regard the separation of user behavior sequences as a Markov decision process, and design a reinforcement learning model for item allocation. In particular, for a given sample, the agent goes through the history information, and at each step, it chooses a sub-sequence for the current item. At step time =𝑇, let all the generated sub-sequences be:𝐺= {𝑔, 𝑔, ...,𝑔}, where h is the current generated subsequences number. What’s more, we use sub-sequence representation𝑃= {𝑝, 𝑝, ..., 𝑝}to represent each generated sub-sequence and each sub-sequence representation is initializing with the corresponding user embedding𝑒. The agent at step𝑇aims to assign the 𝑇th item𝑣in user’s sequence behavior to a sub-sequence. Given 𝐺= {𝑔, 𝑔, ...,𝑔}and the current state the𝑆= {𝑠, 𝑠, ..., 𝑠} in Eq.(13) which is made up of the𝑆and𝑝, the allocator agent𝜋 is implemented with the following policy network: 𝑟= 𝑆𝑖𝑔𝑚𝑜𝑖𝑑 ((𝑅𝑒𝐿𝑈 (𝑠𝑊+𝑏)𝑊+𝑏)𝑊+𝑏) 𝜋 (𝑎= 𝑎|𝒓, 𝒓, ...𝒓) = [SOFT-MAX(𝑟, 𝑟, ..., 𝑟)] where𝑎is the action taken by the agent at step𝑇, which means the new item𝑣belongs to the𝑎th sub-sequence𝑔.𝑊,𝑊and 𝑊are adapting parameter for computing the similarity between 𝒆and𝒔.SOFT-MAXis the softmax operator. As a result,𝜋 (𝑎= 𝑎|𝒔, 𝒔, ...𝒔)is the probability that item𝑣is allocated into the𝑎th sub-sequence. By this equation, the policy network aims to select the sub-sequence in𝑔, which is semantically more compatible with However, since the number of sub-sequences is ﬁxed, if an item is not coherent with any of existing sub-sequences, it has to be compromisely allocated into the sub-sequence which is not that similar. In such a scenario, a better solution is to adaptively determine the number of sub-sequences, so as to make sure that each subsequence is semantically coherent. To achieve this idea, we introduce a special action called “creating a new sub-sequence”, and the policy network is improved as: 𝜋 (𝑎= 𝑎|𝒓, 𝒓, ...𝒓) = [SOFT-MAX(𝑟, 𝑟, ..., 𝑟, 𝜖)](8) where𝜖is a pre-deﬁned hyper-parameter and we extend the vector before softmax with an additional dimension.𝑎 ∈ [1, ℎ + 1], and when𝑎 = ℎ + 1, we do not assign𝑣to any existing sub-sequences, but create a new sub-sequence, and regard𝑣as the ﬁrst item in this new sub-sequence. This equation encodes the following belief: if𝑣is not close enough to any existing sub-sequences, that is, 𝑔< 𝜖, ∀𝑗 ∈ [1,ℎ], then the policy network is more likely to choose actionℎ +1, which creates a new sub-sequence. However, if there are many sub-sequences, satisfying𝑟> 𝜖, then actionℎ + 1may not be triggered. As a special case, if𝑟>> 𝜖, ∀𝑗 ∈ [1,ℎ], then equation (8) is equal to equation (??). Sequential recommender modelAgent𝜋allocates the𝑣to the sub-sequence𝑔, and we use the well-design sequential recommender model Attention-GRU to update the sub-sequence representation 𝑝, which is as follows: And the corresponding Attention-GRU is below: where the output dimension of the 𝑧and 𝑠is 1. State TransitionAfter an action is taken by the agent, the state is updated as follows: {(𝑔, ...,𝑔), ...(𝑔, ...,𝑔, 𝑣), ...} , 𝑖 𝑓 𝑎∈ [1,ℎ] {(𝑔, ...,𝑔), ...(𝑔, ...,𝑔), (𝑣)} , 𝑖 𝑓 𝑎= ℎ + 1 where if the action is in[1, ℎ], then the corresponding sub-sequence is extended with𝑣. If the action is “creating a new sub-sequence”, then a new sub-sequence(𝑔)is added to the state, where the new sub-sequence representation is initializing with the corresponding user embedding 𝑒. What’s more, in reality, there are complex relationships between the user’s click sequence, like point level, union level with or without skip[25]. For accurately capturing those relationships, we use a welldesigned attention mechanism to deﬁne the state transition. where𝑊is a2𝑑𝑥𝑑matrix and(·)represent the inner product. As for the sub-sequence state 𝑠in Eq.(7) is as follows: In our model, the reward is associated with the following aspects: • The allocation task loss.To begin with, we task of allocator agent𝜋is allocating item𝑣to the nearest sub-sequence. Here we use the inner product between𝑣and its target sub-sequence representation𝑝as the accuracy of the "nearest". And the reward of the allocation task is: whereℎis the current generated sub-sequences number. What’s more, the𝜋optimizes the relevance between𝑣and𝑝, which is same as the the sequential recommender model optimize in Eq.(20). They are basically playing a collaborative game, where the common target is to lower the loss of the objective, that is, better ﬁtting the training data. • The orthogonality between different sub-sequences.Ideally, each sub-sequence should encode just one type of user preference coherently, which is not leaked into the other ones. Based on this property, each sub-sequence is representative enough, which facilitates more focused and explainable estimation. By the design of our policy network𝜋in Eq.(7) (8) , we have tried to allocate each item into the sub-sequence which is most coherent with it. Here, we introduce a reward to encourage that the information overlap between different sub-sequences is as small as possible, that is: where we try to make each pair of sub-sequence representations derived from equation Eq.(9) as orthogonal as possible. • The penalty on creating new sub-sequence.The special action “creating new sub-sequence” inﬂuences the ﬁnal number of subsequences. If there are too few sub-sequences, the user preference may not be well separated. While if the number of sub-sequences is too large, the preference granularity can be too small, which may fail to capture the high-level connections among user behaviors, and thus may not generalize well on the complex and volatile recommendation environments. In addition, if the number of sub-sequences is too large, then each sub-sequence may only contain very few items, which brings difﬁculties for learning each user preference sufﬁciently. In order to tune the number of sub-sequences, we introduce a reward𝑟to penalize the action of “creating new sub-sequence”, that is: where the reward is a negative value after taking the special action “creating new sub-sequence”, while for the other actions, the reward is 0. By a larger𝜆, we can reduce the number of sub-sequences, while if 𝜆 is small, then more sub-sequences can be generated. In our model, we adjust𝜆in a curriculum learning manner. In the beginning, there is only one sub-sequence, and we do not hope to impose much constraint on “creating new sub-sequence”. At this time,𝜆is a small value. While as the agent takes more actions, more sub-sequences are generated, thus we limit the number of subsequences by setting𝜆as a larger value. To realize the above idea, we explore𝜆in the following set of functions when the agnet take the action "creating a new sub-sequence". And we use the𝑇as the times the action "creating a new sub-sequence" has shown up until now: Where{𝑏, 𝑎}are hyper-parameters, and we set𝑎> 0,𝑏> 0 to ensure that the function is monotonic as the agent takes more action "creating a new sub-sequence". We sum the𝑟,𝑟and𝑟as the ﬁnal reward𝑟. What’s more, the current action value is only related to future rewards without considering previous rewards, we add the future rewards with a decay parameter 𝜆. Where the𝜆is the trade-off parameter to balance the importance of the orthogonal reward. In practice, the discrete action is not differentiable, but we can optimize it with the log trick approximation, which is an unbiased estimation. As a result, our ﬁnal optimization target is: For the target item𝑣, the behavior alloator𝜋generates its target sub-sequence𝑔and the target corresponding sub-sequence representation is𝑝. Then the sequential recommender model is optimized based on the following objective: where the m is the number of the item. Finally, we jointly train the allocator agent𝜋and sequential recommender with a trade-off parameter 𝛽: When we do the prediction, allocator agent𝜋ﬁrst allocate the item into the sub-sequences with the maximal probability in Eq.(7) (8) and then the state transition in Eq. (11)(12)(13) , which can be written as follows: We choose the action with the maximal probability in the prediction while we sample action according the action probability in the training process. For each candidate item, allocator agent𝜋allocates it into a sub-sequence and calculates the inner product between the item representation and its target sub-sequence representation as the score. We then rank all candidate items according to their scores and return the top-𝑁 scores item as the ﬁnal recommendations. We summarize the complete learning algorithm of our framework in Algorithm 1. The main task of allocator agent𝜋is allocating training sample ((𝑢, 𝑣, 𝑣, ...𝑣), 𝑣) into sub-sequences. To begin with, the training sample is represented with a global information representation through a bi-directional architecture transformer in Eq.(5) (6). Then, the agent𝜋samples action for each item via its action prbability in Eq.(7) (8) and allocates item into different subsequence𝐺and form sub-sequence representations𝑃in Eq.(9) (10). And then state transition in (11)(12) (13) and reward calculation in Eq.(14) (15) (16) for the item allocation. For the target item prediction, agent𝜋allocates the item into its target sub-sequence 𝑔in Eq.(7) (8). And optimization in the joint loss L in Eq.(21). In this section, we will conduct experiments on four datasets to evaluate the effectiveness of Adasplit. We ﬁrst brieﬂy introduce the datasets and the state-of-the-art methods, then we conduct experimental analysis on the proposed model and the benchmark models. Speciﬁcally, we try to answer the following questions: •How effective is the proposed method compared to other stateof-the-art baselines? Q1 •What are the effects of the bi-directional architecture transformer and the sequential recommender model Attention-GRU in behavior allocator? Q2 •How sensitive are the hyper-parameter the sequence length𝑡 and the penalty parameter 𝜆 in proposed model Adasplit? Q3 In this section, we introduce the details of the four experiment datasets, evaluation metrics and comparing baselines in our experiments. Datasets.We perform experiments on four publicly available datasets, includingLastFMa music records from Last.fm. And we only use the click behaviors.Garden, Baby, Beautyare composed of user purchasing behaviors from Amazon. And the relative statistics information of the four datasets are shown in Table 1. For each datasets, we ﬁlter out items and users interacted less than ﬁve times. And all datasets are taken Leave-one-out method in [12] to split the datasets into training, validation and testing sets. Note that during testing, the input sequences contain training actions and the validation actions for training the model. Baeslines.We compare our proposed model Adasplit with the following state-of-the-art sequential recommendation baselines. • Single representation models: BPR[20] is a famous recommendation algorithm for capturing user implicit feedback. Attention-GRU. Eq.(5) (6). representations 𝑃. NeuMF[9] is a classic work which leverages the neural networks for capturing the user preference.GRU4Rec[10] is a pioneering work which ﬁrst leverages GRU to model user behavior sequences for prediction.STAMP[16] is a recently proposed neural network-based method capturing sequential pattern by emphasizing the user short preference. NARM[14] is a recently proposed neural attention based method.SASRec[25] is a well-known attention based sequential recommendation. And the number of head in experiments is 1. Figure 3: Case study. The left before t=7 is the user behavior and the task to predict the target item at t=8. And the picture of each movie is downloaded from https://www.amazon.com. • Multi representations models: MCPRN[28] is a recent representative work for extracting multiple interests.SASRec2[25] has two heads in the representations construction, which is a multi interest recommendation method [24]. Parameter Conﬁguration.For a fair comparison, all baseline methods are implemented in Pytorch and optimized with Adam optimizer. Speciﬁcally, the learning rate and batch size are tuned in the ranges of [0.01,0.001,0.0001] and [32,64,128,256]. For our method, it has three crucial hyper-parameters: the "creating a new sub-sequence" parameter𝜖is tuned in the ranges of [0.2,0.3,0.4,0.5,0.6, 0.7,0.8], and trade-off parameter𝜆and𝛽are tuned in the ranges of [1,0.1, 0.01,0.001]. And the penalty parameter𝜆is tuned in the ranges of [0.9,1,1.1,1.2,1.3]. What’s more, we have released our project at https://no-one-xxx.github.io/Adasplit/ Evaluation Metrics.We use two commonly used evaluation criteria: Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of our model. Table 2 summarizes the performance of Adasplit and baselines. Obviously, Adasplit achieves better performance to other the baselines on all the evaluation metrics. What’s more, in the datasets Amazon, compare single representation methods with multi representation methods and Adasplit, it is obvious that recommendation with multiple presentations (MCPRN) and with dynamic representation Adasplit for a user click sequence perform generally better performance than those with single representation (Caser, GRU4Rec, BERT4Rec ...). The user’s always click multiply concept different items in those datasets which gives the evidence that only single representation for next item prediction can’t handle the complex situations. However, in other datasets like LastFM, the multi representations method (MCPRN) doesn’t achieve excellent results, which indicates that users in Lastfm datases show a focus click patterns and single representation would achieve better results. And our model Adasplit could learns a dyanmic group of representations according the user sequence behavior. In other words, Adasplit could learn more representations when the sequence behavior is complex and learn less representations when the sequence behavior is simple. And it shows its adaptability when it faces various datasets. The improvement of Adasplit over the multi representations methods (MCPRN, SASRec-2) and single representation methods (BPR, NARM...) shows that the dynamic representations exploration serves as a better information extractor than ﬁxed and predeﬁned number representations. This can be attributed to two points: 1) Adasplit Table 2: Overall comparison between the baselines and our models Adasplit. The best results are highlighted with bold fold. All the numbers in the table are percentage numbers with ’%’ omitted. Figure 4: Ablation study in bi-directional architecture transformer. Performance comparison of Adasplit, its variant Adasplit-Single and Adasplit-Zero. adjusts the sub-sequence representations construction according to data characteristics which could take the advantages of both the single representation methods and multi representations methods. 2) Adasplit considers the user’s evolving preference in the user representations construction which gets high adaptability. In our model, a major novelty is that we want to allocate the item into different sub-sequences and learn a dynamic group of representations to represent the user. To obtain a better understanding why Adasplit performs better than other models, we further construct a case study on Baby dataset. Speciﬁcally, we present a snapshot of the interaction sequence from a sampled user, which contains eight items. The results show in Figure 3, and we use different colors to represent different sub-sequences that the allocator agent 𝜋generates. There are three different sub-sequences: clothes, toy and bottle, which are fully in line with the facts in the user sequence behavior. As for the next item prediction, the target item is a toy guitar and it is accurately allocated to the toy sub-sequence exactly by the allocator agent 𝜋. In behavior allocator, we use the bi-directional architecture transformer to equip item representations with global information for the next allocation task. In order to explore the effectiveness of the bidirectional architecture transformer, we change the bi-directional architecture transformer to left-to-right directional architecture transformer(AdasplitSingle) and use the original item sequence representation without any operation for the allocation task (Adasplit-Zero). The item in Adasplit-Single could obtain the information form the items on its left while item in Adasplit-Zero knows nothing about the items around itself. From the results reports in Figure 4, we can ﬁnd that Adasplit-Zero achieves the worse performance compare with Adasplit-Single, which indicates that the information between items is important in allocation process. And the Adasplit achieves the better performance than the Adasplit-Single gives the evidence that the effectiveness of the bi-directional architecture transformer for extracting the global information. In other words, two-way information is more useful than only one-way information in item allocation. In behavior allocator, the sequential recommender model AttentionGRU formalize the sub-sequence representation. To validate the effectiveness of the recommender model Attention-GRU, we change the Attention-GRU to two other methods, LSTM and AveragePooling. And We call the two variants Adasplit-LSTM, AdasplitAveragePooling. From the results reports in Table 3, Adasplit achieves a better performance compare with other variants, which validates the effectiveness of the sequential recommender model AttentionGRU in behavior allocator. And we also ﬁnd Adasplit-LSTM achieve better performance to Adasplit-AveragePooling in the Table 3 which gives the evidence that the necessity of the sequential pattern to capture the evolving user preference. Table 3: Ablation study in Attention-GRU. Performance comparison of Adasplit, its variants Adasplit-LSTM and Adasplit-AveragePooling. As we all know, more item in user sequence behavior, more subsequences may occur. Thus, the sequence length𝑙in the Adasplit is important, and we do experiments to investigate the sensitivity of the sequence length𝑙in Adasplit. Figure 5 reports the performance of our model in the metrics of MRR and NDCG in Garden and Baby datasets. In particular, We keep the other parameters in the model consistent with the Q1 settings. From the ﬁgure, we can observe that Adasplit obtains the best performance of MRR and NDCG when Figure 5: Hyperparameter study, where the horizontal coordinates is the sequence length 𝑙 from 5 to 40. 𝑡equals 10. The result increases with the increase of the sequence length 𝑙. After it comes to a peak, it begins to decrease. In overall performance, Q1, we use the exponential growth in Eq.(17) for penalty𝜆. Here we use the linear growth for the𝜆in behavior allocator. And we call it Adasplit-Linear. In order to valid the effectiveness of the penalty operation and the increasing penalty𝜆 method on the action "creating a new sub-sequence". We add another two variants, Adasplit-None, where we remove the penalty operation and Adasplit-Keep where we don’t change the penalty parameter 𝜆. In Table 4, obviously, Adasplit-None achieves the worst performance among the four models, which gives the evidence that the effectiveness of the penalty operation. And Adasplit-Linear achieve better performance than Adasplit-Keep shows the effectiveness of the increasing penalty𝜆method. Last but not the least, AdasplitLinear achieves worse performance compare with Adasplit, which gives us the evidence that the stronger increasing method may bring a better performance. Table 4: Hyperparameter study, the performance of Adasplit and its variant Adasplit-Linear, Adasplit-Keep and Adasplit-None. In this section, we will brieﬂy introduce the related works to our study, which includes sequential recommendation and multi interest recommendation The main purpose of sequential recommendation is to discover the underlying patterns of the user sequential behaviors. Rendle et al [21] integrates matrix factorization and the sequential pattern of Markov Chains for prediction, and later Wang et al [27] simultaneously consider the sequence behaviors and user preferences with a hierarchical representation model. Though those methods make progress in recommendation, these methods only model the local sequential patterns between recent clicked item [32]. To model the longer sequential behaviors in user behavior, Hidasi et al [10] ﬁrst adopted recurrent neural network to model the long sequence pattern. Then, Li et al [14] not only consider the sequence pattern in the sequence, but also explore the user’s main purpose through the attention mechanism. And Ma et al [18] takes Hierarchical Gating Networks to capture both the long-term and short-term user interests. Chen et al [7] use Memory Network for exploring the sequential pattern in user behavior. What’s more, Tang et al [25] and Yuan et al[33] embed the user historical sequence behavior into an “image” and learn sequential patterns as local features of the image with Convolutional Neural Network. Later, Kang et al [12] considers the importance between different items in the click sequence and fuses the item representation with adaptive weights, which achieves great progress in many real datasets. Wu et al [29] and Huang et al [11] use graph to learn the complex transitions between items and improve the recommendation performance. Multi interest recommendation with multi interest representations have greater expressive power especially when the user shows a wide range of intends. Liu et al [17] proposes a new user representation model to comprehensively extract user sequence behavior into multiple vectors. And Xiao et al [30] explores user diverse interests with a multi-head architecture self-attentive, where the number of the heads is the number of the representations. Wang et al [28] takes an effective mixture-channel purpose routing networks to detect the purposes of each item and assigns items into the corresponding channels to get the multi interest representations. Chen et al [5] thinks that the time interval information is meaningful in extracting the useful information and designs a novel time graph to get the time information in multi representations construction and Tan et al [24] infers a sparse set of concepts for each user from the large concepts to generate user multi interest representations. Chen et al [6] and Cen et al [3] use capsule routing and self-attention as multi interest representations extractor and the proposed models improve the diversity for the next-item prediction. Though those methods have achieved good performance in recommendation, none of them consider the diverse interests between users and the evolving user pattern. In this paper, we proposed a novel model called Adasplit, to improve the recommendation performance by learning a dynamic group representations from the user’s sequence behavior. Adasplit can adaptively and automatically allocates items into different sub-sequences and learns a dynamic group of representations according the user evolving preference. To be speciﬁc, we formalize the allocation task as MDP problem and allocate the item in user’s sequence behavior into different sub-sequences with a novel action "creating a new action", and form the sub-sequence representations through the sequential recommender model Attention-GRU accordingly. And we conducted experiments to veriﬁed the effectiveness of Adasplit on four real datasets with SOTA methods. However, the proposed model also exists shortcomings in computing speed, where we formalize the allocation task in behavior allocator as a MDP problem and it is computing cost and unstable in training. In the future we will consider how to allocate the user sequence in a more effective way.