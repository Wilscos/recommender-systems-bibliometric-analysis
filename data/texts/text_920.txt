<title>An Evaluation Study of Generative Adversarial Networks for Collaborative Filtering</title> <title>1 Introduction</title> <title>arXiv:2201.01815v2  [cs.IR]  20 Jan 2022</title> learn how to generate fake data from the real one. Their primary use has been in the computer vision domain [13,16,17,18]. They have also been used in Information Retrieval [34] and Recommender Systems, the most notable example being Collaborative Filtering GAN (CFGAN) [4], and the family of models based on it, such as TagRec [5], CRGAN [35], MTPR [36], and [37]. This work contributes to the trend of evaluation studies in Machine Learning, Information Retrieval, and Recommender Systems domains [10,11,21,22,38]. This work discusses the implications of certain diﬀerences between the CFGAN framework and the model that was used in the experimental evaluation, which would adversely aﬀect its learning ability, providing a reference for future works. In particular, the generator is left prone to reach a degenerate solution and behave as a simple autoencoder, therefore, belonging to the same family of previous recommendation models such as [28,32]. This discussion is based on the ﬁndings of [23], which highlights the importance of describing not only how a model works, but also what works and why it works, as well as how experimental inquiries that aim to deepen our understanding are valuable research contributions even when no new algorithm is proposed. Furthermore, this work analyzes the replicability, reproducibility, and recommendation quality of CFGAN [4] as well as its numerical stability which is known to be a challenge for GANs [9,25]. The main research questions of this work are: RQ1: Is CFGAN replicable and numerically stable? i.e., does CFGAN achieve the claimed results using the same experimental setup as in [4]? RQ2: What is the impact of the diﬀerences between the CFGAN framework and the model used for the evaluation in [4], and why do they raise theoretical and methodological concerns regarding the learning ability of the model? RQ3: Is CFGAN reproducible, achieving the claimed recommendation quality when compared to properly-tuned baselines? How does CFGAN compare along other dimensions such as beyond-accuracy and scalability metrics? <title>2 Collaborative Filtering Generative Adversarial Networks</title> GANs have been successfully applied to numerous prediction and classiﬁcation tasks. This work addresses a family of generative models originated from GANs used in Recommender Systems. Brieﬂy, a GAN consists of two neural networks that are trained together in an adversarial setting until they reach convergence. The ﬁrst neural network is called the generator, denoted as G, while the second network is called the discriminator, denoted as D [3,8,14,13]. CFGAN is the most notable GAN recommendation algorithm [5,37]. Its main attribute is that it generates personalized user or item proﬁles, mainly by solely using previous interactions, but is able to learn from sources of information as well [4]. CFGAN Training Process Figure 1 shows an illustration of the training process of CFGAN. Every epoch starts by feeding the generator G with random noise z and a condition vector c. The generator creates preferences of users towards items (or vice versa) which are then masked (see Masking). The discriminator D then receives the real proﬁles, the masked proﬁles, and the condition. The discriminator tells the probability that each masked and real proﬁles come from the real data. The discriminator is updated based on how well it is able to correctly distinguish fake data from real data. The generator is updated based on how much it could generate fake but realistic data. Fig. 1: Training process of CFGAN. G, D, z and c are the generator network, discriminator network, random noise, and condition vectors, respectively. Real proﬁles are not masked. Modes CFGAN has two modes: user-based (u) or item-based (i). The ﬁrst learns to generate user proﬁles, while the second learns to generate item proﬁles. Masking CFGAN applies a mask to the generated proﬁles by performing an element-wise product between these and the real proﬁles. If the variant is Partial Masking, then the mask changes (see Variants). Architecture Both the generator and discriminator of CFGAN are fully connected feed-forward neural networks independent from each other where each has its own hyper-parameters, e.g., number of hidden layers, learning rate, regularization, and others. If the mode is user-based, then the number of input neurons is the number of items in the dataset. Conversely, the number of input neurons for an item-based CFGAN is the number of users in the dataset. Recommendations In a top-N item recommendation scenario, the trained generator creates user proﬁles containing the preference scores of users toward items. Recommendations are built by ranking the items from the highest to lowest score and selecting the top-N. Variants CFGAN has three variants: – Zero Reconstruction (ZR): Changes the loss function of the generator. It ensures that a sample of non-interacted items are given zero-weights in the generated proﬁles. – Partial Masking (PM): The mask applied to the generated proﬁles combines the user proﬁle and randomly-chosen unseen items. – Zero Reconstruction and Partial Masking (ZP): Combines ZR and PM. <title>3 CFGAN Theoretical and Methodological Questions</title> This work highlights key diﬀerences between the initial description of CFGAN and the model used in the experimental evaluation of that same paper [4]. These diﬀerences were not discussed in the original paper but have signiﬁcant implications on the model’s ability to learn user or item preferences. What raises concerns? In the experimental evaluation of CFGAN, the condition vector provided to both the generator and the discriminator is the real user/item proﬁle, i.e., the interactions that CFGAN is learning to generate. Why is it a concern? As a consequence, CFGAN is prone to generate a trivial solution. The generator could learn the identity function between the condition vector and the output, therefore easily deceiving the discriminator without learning to generate new proﬁles. On the other hand, the discriminator could learn that the generated user proﬁle should be identical to the condition vector to be real, again learning a trivial function. In practice, this will push the generator to behave as an autoencoder [20], which reconstructs as output the same input (condition) it was provided with. How to avoid this concern? Since the condition vector can contain any information, a simple strategy would be to use other feature data related to the items or users or other contextual information. In a pure collaborative recommendations scenario, where no features or context is available, a possible strategy is to change the condition vector to be the user/item classes (i.e., unique identiﬁers) depending on the CFGAN mode. This decision is aligned with previous works on GANs [26]. In Recommender Systems, using the user/item classes provides a mechanism to generate personalized recommendations to every user. In contrast to the original CFGAN reference, using the user/item classes excludes the possibility that the generator and discriminator learn a trivial solution. What raises concerns? The reference article states that the random noise is not provided as input to the generator in its experiments because the goal is to generate the single best recommendation list rather than multiple ones. Why is it a concern? This violates the framework deﬁned in the same article and the design principles of GANs. In practice, discarding noise is problematic because it drastically reduces the input space and the generator will be trained on a very sparse set of user proﬁles. This assumes that the user proﬁles will not change, which will make CFGAN non-robust in a real application where the data change rapidly. This is known as the dataset shift problem. Since the data change over time as new interactions are collected, and models are not continuously retrained, models should be robust to and be able to use the new data that was not present during training [30,27]. How to avoid this concern? Feed the generator with a random noise vector z and the condition vector c. z is drawn from a normal distribution with zero mean and unit variance, i.e., z ∼ N (µ, σ ) where µ = 0 and σ = 1 as suggested by other works [13,26]. The size of z is a key element while training GANs. However, previous works do not have consensus concerning the size of z [8]. We use a heuristic to set the size of the random vector and try diﬀerent values depending on the number of input neurons: 50%, 100%, or 200% of them. In practice, the condition c and the random vector z are concatenated, and this new vector becomes the input to the ﬁrst layer of the generator network. What raises concerns? The CFGAN description does not state how to choose the number of training epochs nor the stopping criterion for the training phase. Why is it a concern? The number of training epochs and the stopping criterion are two key methodological aspects for most machine learning models. With the current GAN formulation, these two are deﬁned by hand instead of automatically chosen by the continuous evaluation of GAN, which might lead to a non-optimal model, misuse of computational resources, and negatively aﬀect the published results’ replicability. There are well-known objective ways to measure the recommendation quality in oﬄine scenarios without human intervention in the Recommender Systems domain, e.g., with accuracy metrics. How to avoid this concern? Use an early-stopping mechanism based on the one used in previous works for other machine learning recommenders, such as matrix factorization or linear regression [10,11]. An early-stopping mechanism periodically evaluates CFGAN on validation data while CFGAN is being trained on train data. The training stops when the CFGAN quality does not improve over the best evaluation for a ﬁxed number of evaluations. <title>4 Experimental Methodology</title> The experiments, results, and discussion are based on one of the following two experiments: (i) execution of the source code provided in the CFGAN reference article as-is to assess the result replicability; (ii) hyper-parameter tuning of different recommenders using a well-known evaluation framework to study the reproducibility of the results and evaluate along diﬀerent dimensions (see [10,11]). The source code of the experiments is available online Datasets The experiments use the same datasets (a sub-sampled version of Ciao [4,33], ML100K [15], and ML1M [15]) and splits (train and test) provided with the CFGAN reference article [4]. For scenarios that required a validation split, we created one by applying the same strategy as the reference: random holdout at 80% of the train split. Given the modest size of these datasets, all experiments are done on the CPU. Technologies The implementation of all experiments, is based on the evaluation framework published in [10], which includes the implementation of some simple yet competitive state-of-the-art baselines for Recommender Systems. For the replication study, the original implementation has been used as provided. For the reproducibility study and the other experiments, the original CFGAN source code has been adapted to the framework with no changes to the core algorithm. The original CFGAN source code includes the implementation of CFGAN and its training loop using a ﬁxed set of hyper-parameters that are dataset-dependent. The training procedure is the following: it ﬁts a CFGAN recommender using the train split of the selected dataset and evaluates the recommender using the test split. With respect to the evaluation metrics, this source code evaluates CFGAN on accuracy metrics: precision (PREC), recall (REC), Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG) at recommendation list length 5 and 20. The limitations of this source code are the lack of the implementation of the baselines and the hyper-parameter tuning of all recommenders, e.g., baselines and CFGAN. Due to this, the replication study is only possible for CFGAN. The reproducibility study expands the original CFGAN evaluation by including: (i) new baselines that were shown to provide high recommendation quality; (ii) a well-deﬁned hyper-parameter optimization strategy; (iii) a well-deﬁned early-stopping strategy; and (iv) a comparison against accuracy, beyond-accuracy, and scalability metrics. In particular, the goal of (i) and (ii) is to assess the recommendation quality of CFGAN against a wider set of recommendation models which are properly tuned under the same conditions. The models we report range from non-personalized, neighborhood-based, and non-neural machine learning approaches. This decision is aligned with results obtained by previous evaluation studies in the domain [11,10]. Regarding the hyper-parameter optimization of CFGAN, it should be noted that the search-space described in the reference article, considering that it is done using a grid-search, contains more than 3 · 10 cases, which cannot be reproduced in a reasonable time. Due to this, this work adopts a diﬀerent optimization strategy: Bayesian Search as used in [10]. The hyper-parameter ranges and distributions of CFGAN are reported in Table 1. The Bayesian Search starts with 16 initial random searches and performs a total of 50 cases for each algorithm. Each model in this search is ﬁt with the train split and evaluated against the validation one. The best hyper-parameters are chosen as those with the highest NDCG at 10. Once the optimal hyper-parameters set is chosen, it trains the ﬁnal models using this set and the union of the train and validation splits, evaluating the ﬁnal models against the test set. Evaluation metrics Recommenders are evaluated using the original accuracy metrics (PREC, REC, MRR, and NDCG) and against the following beyondaccuracy metrics: novelty [39], item coverage (Cov. Item, quota of recommended items), and distributional diversity (Div. MIL [39] and Div. Gini [1]). Using these new metrics provides a broader picture of the quality of all recommenders. Baselines Due to space limitations, this work provides only a list of baseline recommenders. A thorough description of all baselines, and the list, range and distribution of their hyper-parameters are in [10]. The baselines list is the following: Top Popular [10] as a non-personalized approach. UserKNN CF and ItemKNN CF [10] as neighborhood-based CF (similarities: cosine, dice, jaccard, asymmetric cosine, and tversky) and shrinkage term. RP3beta [6] as a graph-based approach. PureSVD [7] and MF BPR [31] as matrix factorization models. SLIM ElasticNet [10,28] as a machine learning approach. Lastly, EASE R as a fast linear autoencoder [32]. CFGAN recommenders The hyper-parameter tuning is done on a total of 18 diﬀerent CFGAN models: three datasets (Ciao, ML100K, and ML1M), two modes (item-based i and user-based u), and three variants (ZR, PM, and ZP). To ensure a clear stopping criteria and a fair training for CFGAN, it is trained using the early-stopping criteria deﬁned in [10] and presented in Section 3. The number of minimum and maximum epochs is in Table 1. The early-stopping selects the best number of epochs by using the validation data. The optimal number of epochs is used to train the ﬁnal model. We recall that the original description of CFGAN does not provide an early-stopping mechanism. Table 1 lists all hyper-parameters of CFGAN, where hyper-parameters like optimizer, activation are left unchanged with respect to the reference article. Apart from the number of training epochs, the optimizer, and activation, the rest of the hyper-parameters are set by the Bayesian Search. Table 1: Hyper-parameters for CFGAN. These are divided in two groups. The ﬁrst group contains speciﬁc hyper-parameters of CFGAN. The second group are hyper-parameters of the generator and discriminator neural networks, values between networks can be diﬀerent. <title>5 Experiments Results & Discussion</title> To address RQ1, we report the results of the replication study, as described in Section 4.1, by using the original source code and data. This experiment has two goals: (i) verify that published results are replicable; and (ii) measure the numerical stability of CFGAN given the stochastic nature of its architecture [9,25]. Table 2 shows the results of the experiment, we only report two metrics due to space limitations . The results reported in the reference article are denoted as Reference. Due to the stochastic nature of CFGAN models, we do not expect to achieve exact numerical replicability. For all datasets, we see that the replicated results are lower than those reported in the reference article. For the ML1M dataset, the diﬀerence between the average and reported NDCG is −0.62%. On the smaller ML100K, the results are more varied: −2.84% between the average and reported NDCG. For the Ciao dataset, the results could not be replicated due to two factors: (i) the original source code trained a diﬀerent variant (iZR) than the reported in the reference article (iZP); and (ii) lack of reproducible hyper-parameters sets for this dataset in the reference article. Lastly, with respect to the numerical stability, under 30 executions of this replication, the results indicate that the reference implementation of CFGAN is numerically stable. Table 2: Comparison between the accuracy metrics in the reference article [4] and those obtained in the replicability experiment (see Section 5.1) at recommendation list length of 20. Statistics calculated over 30 executions, evaluating on the last epoch using recommendation lists of length 20. We consistently obtain lower results across the three datasets on average. For the Ciao dataset, the original source code trains a diﬀerent variant (in bold) than the reported in the reference article. This section reports the results of the experiments related to RQ2, those used to measure the impact of the theoretical and methodological concerns raised in Section 3. Table 3 compares the results of the reference CFGAN (denoted as Reference), the models tuned in Section 5.3 (presented in Table 4), and the variants of this experiment. Impact of random noise As seen in Section 2, CFGAN receives random noise as part of its input. However, in the experiments of the reference article, the random noise is removed. This experiment included three diﬀerent sizes of random noise. The results indicate that the recommendation quality improves slightly by removing the random noise, however, as stated in Section 3, it comes at the cost of risking lower generalizability and lower robustness of the generator in a practical use case. We argue the random noise should always be present. However, we recall that doing an exhaustive analysis of the impact of random noise in GAN and CFGAN is beyond the scope of this paper. Impact of condition vector Similarly as before, in the experiments of the reference article, the condition vector is set to be the user/item proﬁles, which increases the risk of reaching a trivial solution. This experiment changed the condition vector to be the user/item classes. The results show that changing the condition vector with the current CFGAN architecture dramatically lowers the model’s ability to learn to generate accurate proﬁles. This constitutes a negative result, as that the current architecture does not appear to be suitable to handle the user/item classes as the condition vector. Identifying an appropriate architecture to do so and an appropriate condition vector to use in scenarios Table 3: Accuracy and beyond-accuracy values for diﬀerent CFGAN models for the ML1M dataset at recommendation list length of 20. The suﬃx Reference is the model in the reference article (where − denotes non published values). The suﬃx ES indicates that the model uses early-stopping (see Table 4), NO-ES indicates it does not. The suﬃx CC indicates that the model uses the user/item class as the condition vector. The suﬃx RN-X means that the model uses random noise of size X. Hyper-parameter sets of variants are chosen as described in Section 4.2 except for those with the Reference suﬃx. where only past user interactions are available is an open research question that goes beyond the scope of this paper. Impact of early-stopping The reference article does not provide an earlystopping mechanism for CFGAN, although models in Recommender Systems typically beneﬁt from one, as discussed in Section 3. This experiment removed the early-stopping and set the maximum number of epochs as 400 (this is the maximum number of epochs set for the early-stopping as seen in Table 1). Results show that using early-stopping slightly decreases the recommendation quality of CFGAN, however, we argue that the beneﬁts of using it outweigh the downsides of it, especially if scalability is taken into account. For instance, the iZP variant trains on 645 and 1200 epochs with and without early-stopping, respectively, i.e., a decrease of 46.25% in training time and 4.47% in NDCG. To address RQ3, we report the recommendation quality of CFGAN and baseline recommenders using a Bayesian hyper-parameter tuning approach, as described in Section 4.2. The goal is to evaluate [4] on the same top-N recommendation scenario of the reference paper against a set of properly tuned baselines on accuracy and beyond-accuracy metrics and study if published results are reproducible. Table 4 shows the results of accuracy and beyond-accuracy metrics of properly tuned recommenders. Due to space constraints, the focus of this discussion Table 4: Accuracy and beyond-accuracy metrics for tuned baselines and CFGAN on the ML1M dataset at recommendation list length of 20. Higher accuracy values than CFGAN models reached by baselines in bold. ItemKNN and UserKNN use asymmetric cosine. CFGAN results are diﬀerent than Table 2 due to the hyper-parameter tuning. CFGAN models use early-stopping. is on the dataset with the highest number of interactions studied in the reference article [4], i.e., ML1M. Results with other datasets are comparable The results indicate that CFGAN is outperformed by three simple baselines in NDCG, sometimes by almost 10%, in particular by other autoencoder-based recommendation models like EASE R and SLIM Elastic Net. These ﬁndings are consistent to those reported in several other evaluation studies [11,10,12,24,2]. The accuracy across CFGAN models varies depending on the CFGAN mode and variant. For instance, the most and least accurate variants are uZR and iZP, respectively, with approximately 21.76% diﬀerence in their NDCG metrics. Under the current methodology, we cannot conﬁrm the claim that item-based models or ZP variants outperform other variants, as indicated in the reference article [4]. In fact, our most accurate variant is uZR. When looking at beyond-accuracy metrics, item-based CFGAN models have equal or higher diversity than baselines. In particular, iZR has the highest novelty, item coverage, and distributional diversity, while also being the second-most accurate variant with respect to NDCG. User-based CFGAN models have less coverage than all baselines. It can be seen that the results of the replicability study using hyper-parameter optimization and early-stopping reported in Table 4 are lower than those re- ported in the replication study in Table 2. This indicates that the non-reproducible hyper-parameter search and early-stopping criteria have an important impact on the recommendation quality. As a last observation, using the results reported in the reference article CFGAN would not be competitive against the baselines. Scalability Concerning the recommendation time, all algorithms are able to create recommendations lists to all users in a total time between 7 and 20 seconds. Diﬀerently from other neural models [10], CFGAN models provide fast recommendations. Due to the lack of random noise, they generate static recommendation lists. Concerning the training time, CFGAN models take more time to train than any baseline. We categorize models into three groups: (i) ItemKNN, UserKNN, PureSVD, RP3beta, and EASE R take between 2 and 25 seconds on average; (ii) machine learning approaches, i.e., SLIM and MF BPR take between 3 and 9 minutes to train on average; and (iii) all CFGAN models take between 25 and 40 minutes to train on average. Even on a comparatively small dataset as ML1M, the diﬀerence in training time between the ﬁrst and the last group is two orders of magnitude. Using more performing hardware, i.e., GPU could reduce this gap. Under this oﬄine evaluation, which is the same as in the original article [4], CFGAN does not generate more accurate recommendations than simple baselines. As CFGAN is a neural approach, bigger datasets with more complex relations between users, items, and their interactions might increase the accuracy of CFGAN. However, this is unpractical due to the higher computational cost of CFGAN models, therefore, we do not report experiments with bigger datasets. <title>6 Conclusions</title> This work presents an evaluation study of the family of models of CFGAN, addressing three research questions under the same top-N recommendation scenario as the reference article [4]. Are previously published results of CFGAN replicable? What is the impact of the diﬀerences between the CFGAN framework and the model evaluated in the reference article? Are previously published results of CFGAN reproducible? Regarding the model’s architecture, using as condition vector the user proﬁle and removing the random noise leaves the model prone to a trivial and not useful solution in which the generator behaves as a simple autoencoder, negatively aﬀecting the model’s ability to generalize. Due to this, we argue a diﬀerent approach should be used, which is still an open research question. The experimental results indicate that CFGAN is replicable and numerically stable, but not reproducible as it can be outperformed by simple but properly tuned baselines. This result adds to the recent evidence that properly tuned baselines can outperform complex methods and suggest CFGAN is not yet a mature recommendation algorithm. <title>A Generative Adversarial Networks</title> GANs have been successfully applied to numerous prediction and classiﬁcation tasks. In this work, we discuss a family of generative models originated from CFGAN used in Recommender Systems. Brieﬂy, a GAN consists of an adversarial setting between two neural networks that are trained together until they reach convergence. The ﬁrst neural network is called the generator, and it is denoted as G. The second network is called the discriminator and it is denoted as D [14,13,8]. We use an example to explain the goals of a GAN. Let us suppose that G is a counterfeit organization trying to produce fake bills and that D is the local police department in charge of distinguishing fake from real bills. Let us also suppose that after the classiﬁcation of bills, both the police department and the counterfeiters can know if the classiﬁcation was accurate or not. The ﬁrst goal is that G learns to generate fake bills as realistically as possible to deceive D. The second goal is for D to keep up to date with the counterfeited bills not to enter the economy. On every iteration of this setup, G updates its counterfeiting methods by looking at the number of errors done by the police. D, on the other hand, updates its detection methods, so the identical bills are not misclassiﬁed again. This adversarial setting stops when G produces such realistic bills that the discriminator can not classify the source of the bills anymore. Formally, the data is drawn from a distribution p (x), and z is a vector drawn from a prior distribution p (z). The generator G is deﬁned as a diﬀerentiable function G(z, θ ) with parameters θ such that x = G(z, θ ) meaning that it is a function that maps samples z drawn from p (z) to values x drawn from a distribution p (x). The learning objective for G is to learn a mapping such that p (x) = p (x), i.e., G learns to generate samples drawn from the same distribution as those of the real data. On the other hand, the discriminator D is a function with parameters θ such that y = D(x) is a scalar that represents the probability of x to be drawn from (x) instead of p (x). The learning objective for D is to learn a mapping that assigns high probabilities to samples from p (x) and low probabilities to samples from p (x). Both G and D are set up in an adversarial setting where the former tries to maximize the probability of D to label generated data as real, thus max D(G(z, θ )) where z ∼ p (z) and G(z, θ ) ∼ p (x). The latter, instead, tries to maximize the probability to distinguish real from generated data, thus max D(x, θ ) where x ∼ p (x). In Figure 2 we illustrate this adversarial setting. Also in Equation 1 we show the objective function of a GAN [13]. Fig. 2: Training process of a GAN. G, D, z are the generator network, discriminator network, and random noise, respectively. In practice, GANs are usually trained to minimize a loss function l , using Stochastic Gradient Descent (SGD) while translating the expected values E to cross-entropy losses [4,13,14]. The main drawback of a GAN is that there is no control over the generated samples, e.g., in [13] a GAN was trained to generate digits from the MNIST dataset. however, it did not control which digits were generated by it. Conditional GAN (CGAN) is an extension of the GAN model that solves this issue by including a condition vector to the generator and discriminator. This vector represents the features or attributes that the generated and discriminated sample must have [26]. For example, a work on the MNIST dataset showed that by providing the digit class, i.e., “0”, “1”, up until “9”, as a condition to the generator and discriminator a CGAN generates samples of those digits [26]. On [16], a black and white image was used as the condition vector so the generator could generate a colorized version of that image. The training procedure of CGANs is similar to that of GANs. The only diﬀerence is that both the generator and discriminator receive a new vector c, the condition vector, as part of their input. <title>B Results RQ1: CFGAN Replicability & Numerical Stability</title> Table 5: Comparison between the accuracy metrics in the reference article [4] and those obtained in the replicability experiment (see Section 5.1) at recommendation list length of 5 and 20. Statistics calculated over 30 executions, evaluating on the last epoch using recommendation lists of length 20. We consistently obtain lower results across the three datasets on average. For the Ciao dataset, the original source code trains a diﬀerent variant (in bold) than the reported in the reference article. <title>C Results RQ2: Impact of Theoretical and Methodological Concerns</title> Table 6: Accuracy and beyond-accuracy values for diﬀerent CFGAN models for the Ciao dataset at recommendation list length of 20. The suﬃx Reference is the model in the reference article (where − denotes non published values). The suﬃx ES indicates that the model uses early-stopping, NO-ES indicates it does not. The suﬃx CC indicates that the model uses the user/item class as the condition vector. The suﬃx RN-X means that the model uses random noise of size X. iZR Reference [4] − − − − − − − − − − − iPM Reference [4] − − − − − − − − − − − iZP Reference [4] 0.0450 0.1940 − 0.1670 0.1240 − − − − − − Table 6: cont... uZR Reference [4] − − − − − − − − − − − uPM Reference [4] − − − − − − − − − − − Table 6: cont... uZP Reference [4] − − − − − − − − − − − Table 7: Accuracy and beyond-accuracy values for diﬀerent CFGAN models for the ML100K dataset at recommendation list length of 20. The suﬃx Reference is the model in the reference article (where − denotes non published values). The suﬃx ES indicates that the model uses early-stopping, NO-ES indicates it does not. The suﬃx CC indicates that the model uses the user/item class as the condition vector. The suﬃx RN-X means that the model uses random noise of size X. iPM Reference [4] − − − − − − − − − − − iZP Reference [4] 0.2940 0.3600 − 0.6930 0.4330 − − − − − − Table 7: cont... uZR Reference [4] − − − − − − − − − − − uPM Reference [4] − − − − − − − − − − − uZP Reference [4] − − − − − − − − − − − Table 7: cont... Table 8: Accuracy and beyond-accuracy values for diﬀerent CFGAN models for the ML1M dataset at recommendation list length of 20. The suﬃx Reference is the model in the reference article (where − denotes non published values). The suﬃx ES indicates that the model uses early-stopping, NO-ES indicates it does not. The suﬃx CC indicates that the model uses the user/item class as the condition vector. The suﬃx RN-X means that the model uses random noise of size X. iZR Reference [4] − − − − − − − − − − − iPM Reference [4] − − − − − − − − − − − iZP Reference [4] 0.3090 0.2720 − 0.6600 0.4060 − − − − − − Table 8: cont... iZP CC 0.0384 0.0218 0.0166 0.1648 0.0507 0.0278 0.0653 0.5382 0.0296 0.0109 5.6056 iZP RN-3020 0.2059 0.1377 0.1316 0.4490 0.2475 0.1650 0.0535 0.9459 0.3995 0.1034 9.0063 iZP RN-6040 0.1683 0.1112 0.0983 0.3808 0.2000 0.1339 0.0560 0.9479 0.4663 0.1204 9.2016 iZP RN-12080 0.1304 0.0724 0.0670 0.2906 0.1471 0.0931 0.0599 0.9587 0.5076 0.1409 9.4485 uZR Reference [4] − − − − − − − − − − − uPM Reference [4] − − − − − − − − − − − uZP Reference [4] − − − − − − − − − − − Table 8: cont... <title>D Results RQ3: Reproducibility Evaluation Against Properly Tuned Baselines</title> Table 9: Accuracy and beyond-accuracy metrics for tuned baselines and CFGAN on the Ciao dataset at recommendation list length of 20. CFGAN results are diﬀerent than Table 6 due to the hyper-parameter tuning. Table 9: cont... Table 10: Training and recommendation time comparison of baselines and CFGAN models for the Ciao dataset. CFGAN models use early-stopping. Table 11: Accuracy and beyond-accuracy metrics for tuned baselines and CFGAN on the ML100K dataset at recommendation list length of 20. CFGAN results are diﬀerent than Table 7 due to the hyper-parameter tuning. Table 12: Training and recommendation time comparison of baselines and CFGAN models for the ML100K dataset. CFGAN models use early-stopping. Table 13: Accuracy and beyond-accuracy metrics for tuned baselines and CFGAN on the ML1M dataset at recommendation list length of 20. CFGAN results are diﬀerent than Table 8 due to the hyper-parameter tuning. Table 14: Training and recommendation time comparison of baselines and CFGAN models for the ML1M dataset. CFGAN models use early-stopping. <title>References</title>