Herein we report a multi-zone, heating, ventilation and air-conditioning (HVAC) control case study of an industrial plant responsible for cooling a hospital surgery center. The adopted approach to guaranteeing thermal comfort and reducing electrical energy consumption is based on a statistical non-parametric, non-linear regression technique named Gaussian processes. Our study aimed at assessing the suitability of the aforementioned technique to learning the building dynamics and yielding models for our model predictive control (MPC) scheme. Experimental results gathered while the building was under regular use showcase the ﬁnal controller performance while subject to a number of measured and unmeasured disturbances. Finally, we provide readers with practical details and recommendations on how to manage the computational complexity of the on-line optimization problem and obtain high-quality solutions from solvers. Keywords: HVAC Systems, Model Predictive Control, Gaussian Processes, Data-Driven Methods Although it is rather common to ﬁnd studies claiming that advanced building control techniques paired with new-fashioned machine learning models can enhance the system operation and attain important monetary savings, it is considerably harder to ﬁnd signiﬁcant experimental evidence supporting these statements [1–4]. Instead, the validation of complex analysis and control techniques is usually performed with the aid of simulation models calibrated either on real data or on construction parameters [5–9]. Some investigations adopt a middle ground approach, whereby novel strategies are tested through extensive simulations followed by scaled-down experiments acting simply as a proof of concept [10–12]. Simulations are at times the only way of exciting buildings with aggressive signals or assessing their behavior under very speciﬁc and reproducible weather conditions. Nevertheless, in our view the ﬁeld can still greatly beneﬁt from more experimental validations of certain machine learning tools that hold big promise present in the literature [13–15]. Related works: A detailed investigation of the beneﬁts of using Regression Trees and Random Forest as modeling techniques for buildings is presented in [16], where the authors pair them with MPC and show its eﬀectiveness in a number of interesting case studies, including a demand-response setting. Similarly, Random Forests are investigated in [17] as an easy-to-use and maintain modeling strategy for predictive control of buildings, aiming at reducing their energy footprint. The residential apartment considered in the study is a part of the NEST testbed located in Switzerland and features water-based ceiling panels for both heating and cooling. The same building platform was used in a distributed MPC investigation recently reported in [18], which targets the large-scale problem of coordinating energy hubs. The experimental section of the work however only reports real experiments on a single zone over a total period of 24 hours. Feedforward neural netwokrs (NN) are another popular machine learning tool among certain researchers in the building and HVAC ﬁelds [19–23]. Given an adequate architecture, these models certainly have great representation capabilities and are nowadays rather straightforward to train thanks to the availability of reliable frameworks such as Pytorch [24] and TensorFlow [25]. The known downside of NNs is their low sample eﬃciency, in that the batches of data required to learn a given function is larger when compared to nonparametric alternatives. Gaussian processes (GPs) are a non-linear modeling strategy grounded in Bayesian statistics [26]. As opposed to the aforementioned approaches that compress information into a predetermined number of parameters (e.g. the weigths of a NN whose architecture has been chosen), the expressive power of GPs grows indeﬁnitely as more data is used to train them. Besides their great ﬂexibility, they are also appealing for yielding conﬁdence intervals that quantify the uncertainty associated with their predictions. This machine learning technique has been used in the building domain to calibrate complex simulations [27, 28], replace grey-box RC models [29], forecast energy consumption [30, 31], detect faults in HVAC equipment [32] and create abstract models for how consumers react to demand-adjustment signals [33]. When combined with optimization strategies however, such non-parametric models typically incur a considerable computational load. More speciﬁcally, one has to deal with non-convex programs with potentially many local optima in real-time. Since building automation systems have fairly long sampling periods and are usually deployed on general purpose computers, they could in principle support a Gaussian process-based MPC scheme. Yet, to the best of our knowledge, no experimental investigation of this approach considering multiple zones has been reported in the literature so far. Contributions: In this paper we assess the suitability of using risk-aware non-parametric models combined with MPC to tackle an HVAC control problem. More speciﬁcally, the industrial cooling system of a hospital located in Brazil was considered, and a Gaussian process-based MPC controller was designed and deployed to guarantee thermal comfort to the occupants while minimizing the associated electrical demand. The following factors being simultaneously present in the current study distinguish it largely from previous ones: • The investigation was concerned with three thermal zones, all having external walls and one being affected by considerable direct solar radiation. • The rooms were fully occupied during working hours (from 7 am to 7 pm), implying in a large inﬂow and outﬂow of people and internal heat gain variations. • The MPC controller was not restricted to generating set-points for low-level devices, but had full control over the air-handling unit valves. • The building dynamics were learned by Gaussian processes, whose uncertainty estimates were used to robustify the MPC controller. We also detail practical aspects of the project in terms of hardware and thoroughly discuss the main challenges faced. The computational complexity of the non-convex real-time optimization algorithm is analyzed along with its scalability potential. We highlight that it was outside the scope of this work to comprehensively contrast numerous control strategies–one such comparison can be found in [1]. In the last section, we draw our conclusions and underline possible extensions, which could help guiding future investigations in the ﬁeld. The building considered in this study was a surgery center situated in the São Julião hospital complex, in the city of Campo Grande, MS, Brazil (Figure 1, left). The 51 rooms that compose it are in permanent use and, for information purposes, 528 surgical procedures were carried out in it during August 2021. We were concerned with three thermal zones in its ophthalmology section: two operating rooms (ORs) and one waiting room (WR), all located on the West end of the building (Figure 1, right). Whereas the former rooms are only connected to the waiting room, the latter has a door to the rest of the surgery center. Opaque glass bricks are present in the waiting room as can be seen in the picture, allowing fore some natural light to enter the space; the operating rooms on the other hand do not feature them, nor do they have any windows. All spaces have exterior walls, but the right-hand side operating room is signiﬁcantly more aﬀected by direct solar radiation due to the disposition of the nearby trees. A forced-air HVAC plant is in place to provide the occupants with a suitable indoor climate in accordance with local regulations. A total of seven air-handling units (AHUs) collect outdoor air that is then treated and ﬁltered, before being pumped into the several indoor spaces. We had control only over three AHUs, one for each aforementioned thermal zone. A central chiller connected to an external cooling tower provides chilled water to all AHUs, which in turn featured three-way valves to control the ﬂow of water through their cooling coils. The AHU fans are operated always at constant speed, resulting in a constant volumetric ﬂow through the air-ducts and into the zones. As per the regulations, no air recycling is possible and all return air is directly discharged into the atmosphere. As the temperature in Campo Grande is typically high, the HVAC system was conceived to only cool the space, not having the means to provide positive thermal energy in principle (for more details, see Section 3.1). Two distinct sensor networks were deployed to monitor the HVAC plant and the indoor spaces. Firstly, we will describe the one located in the AHU room. One local controller (LCO)–a National Instruments myRIO–was attached to each air-handling unit, reading all sensors used to monitor the AHUs: supply and return water temperature probes, a water ﬂow meter, an anemometer, as well as an angular position sensor. The LCOs were moreover responsible for running low-level signal processing routines and implementing control actions, i.e., acting on the threeway valve servomotor to change the chilled water ﬂow, hence inﬂuencing the supply air temperature. Photos of the AHU room are shown in Figure 2. Next, in order to measure the indoor temperatures in a ﬂexible way, a wireless network of Z-wave sensors was set up in the operating rooms and waiting room. These were equipped with external temperature probes (Dallas DS18B20) to guarantee fast and precise readings, reporting their measurements periodically to a local computer (LC) that featured a Zwave transceiver attached to it. A 3 GHz, 16 GBs of RAM, core i7 machine was installed in the waiting room, acting as the main computer platform for the project, i.e., the LC. This computer and the AHU LCOs were all connected to a local area network to exchange information, which was done by using the UDP protocol at a rate of approximately 1 Hz. Lastly, a weather station was deployed on site to measure the outdoor temperature and the solar radiation acting on the building with high accuracy. All signals were being sampled with a period of 2 mins and stored into a local timeseries database, InﬂuxDB. A block-diagram of the complete system is depicted in Figure 3. The control goal is to regulate the indoor temperature within the three zones (T, i = 1, 2, 3), keeping it always below a pre-speciﬁed value T. Although deﬁning twolevel temperature envelopes is common for residences and oﬃces (see e.g. [34, 35]), some employees still make use of the surgery center spaces during night hours and, thus, the indoor temperature has to stay below Teven then. Furthermore, this is to be done while minimizing the chiller energy consumption as dictated by its coeﬃcient of performance (COP) curve and the building thermal load. The controlled variables are the angular positions of each AHU three-way valve (θ, i = 1, 2, 3) that regulate the ﬂow of water across their cooling coils. Naturally, these quantities are physically limited between a minimum θand a maximum value θ. Several disturbances both of internal and external nature act on the system. The measured ones include the outdoor temperature T, the solar radiation R, and the temperature of the water supplied by the chiller to the AHUs T. The variables Tand Rdirectly affect the indoor climate by heating the external walls. T and Tcan be regarded as an input disturbance; indeed, these quantities deﬁne the HVAC system actuation capabilities along with the valves positions θ. The unmeasured disturbances are the internal heat gains generated by occupants and equipment, as well as the eventual opening and closing of doors that lead to air mix among rooms. A summary of the relevant control information described here can be found in Table 1. Creating a reliable model for the system dynamics is crucial to attain high-performance with Model Predictive Control. In the current setting, this task is not trivial as certain disturbances and control variables enter the dynamics non-linearly. For instance, the globe water valves in the AHUs are not linear actuators in that the ﬂow is not directly proportional to the angular position. For this reason, we decided to adopt a ﬂexible class of statistical models to tackle the modeling problem while eliciting as little expert knowledge as possible. Their main advantage being that this class not only predicts the expected system behavior, but also quantiﬁes the uncertainty associated with its predictions, hence allowing for a more robust, risk-aware operation. Gaussian processes lie in the class of non-parametric, non-linear, Bayesian models. For a thorough presentation of the topic, we refer the reader to [26, 36]. Their main appeal over other types of statistical modeling paradigms is the analytical tractability, whereby expressions used during training and prediction have closed forms, dispensing with the need of using sample-based approximation techniques (see for example [37, 38]). Assume one wants to model a given phenomenon f (x) through noisy observations of the form y = f (x) + ε, where ε ∼ N (0, σ) is a zero-mean Gaussian noise of unknown variance. As per the usual Bayesian approach, we deﬁne a prior model f(x) ∼ N (m(x), k(x, x)) and, after gathering some experimental data D = {x, y}, it is possible to update our beliefs and form a posterior model whose point-wise mean and variance are respectively µ(x) = m(x) + k(x)(K + σI)(y − m var(x) = k(x, x) − k(x)(K + σI)k where X and y denote the collection of all data features and labels in the dataset D, and k(x, x) is the kernel function. kand K are respectively a column vector and a square matrix of kernel evaluations at X and x. Lastly, I represents the identity matrix. Fully specifying a GP regression model amounts to i) picking a suitable mean function and a suitable nonlinearity, i.e., a kernel function k(x, x); and ii) optimizing all model hyperparameters. In our case study, a linear mean m(x) = Ax + b was employed. Among the many kernel maps available in the literature [36], we chose the anisotropic squared-exponential function, a very popular alternative due to its smoothness and expressive power [39]. This kernel has the form where xis the ith component of the feature vector x. In (2), σ is the so called vertical scale hyperparameter and `are the horizontal scale (a.k.a. lengthscale) hyperparameters. As for optimizing the constants A, b, σ, σand `, we made use of the log-marginal likelihood objective [26, Chapter 5] and a gradient-based procedure. This is a widely adopted criterion, known to contain a regularization term that combats overﬁtting. In order to design dynamic models for the room temperatures, we used an auto-regressive approach, meaning that future predictions of a signal depend on the current and past values of itself as well as on current and past values of other relevant quantities. Since there were three temperatures to predict, three distinct models were trained and, so as to avoid augmenting the Gaussian process with unnecessary features, we made use of domain knowledge. Rooms that are not neighbors do not directly inﬂuence each other’s temperatures; similarly, changing the valve position of AHU 1 has no eﬀect on any temperature besides T. Initially, all exogenous signals T, Tand Rhad been included into all models to boost their prediction capabilities. Nevertheless, we later realized that Rwas a signiﬁcant covariate only for Tas discussed in Section 3.3. Field tests moreover unveiled a high correlation between the MPC computation time over the day and the solar radiation curve. Since having predictable rather than ﬂuctuating solve times was a project requirement, we decided not to employ Ras a feature in any GP. The deﬁnitive set of employed features is reported in Table 2, where the delay parameter l indicates the number current plus past values used from that particular physical signal. By using the mean functions (1a) to evolve the temperature dynamics, we arrived at the ﬁnal models T= µ(T, T, T, T, θ, T, T) (3b) Concerning the variances (1b), we opted for not propagating them forward in time since no closed-form expression exists to accomplish this. Instead, the expression (1b) was evaluated in a point-wise fashion to measure uncertainty. The interested reader is referred to [40, 41] for insightful discussions on the matter. Data collection was carried out from August to November 2021. The ﬁnal batch consisted of 22455 points sampled at T= 2 mins and comprised closed-loop operation with PI and rule-based controllers (RBCs), as well as a variety of open-loop excitation signals such as ramps and uniformly random inputs. After examining the obtained curves, we concluded that a control period of 10 mins would be a good compromise between operating the HVAC system eﬀectively and not oversampling the temperatures – given that our model complexity grows with the dataset, the latter aspect was rather important. The data batch was then downsampled by a factor of 5 times, resulting in 4491 points (748 hours). After that, a meticulous post-processing step was necessary to ensure that unreliable periods were discarded, outliers were detected and ﬁltered, and imputation was performed to ﬁll in certain missing entries. The feature vectors and labels were then created for each of the GPs described in Table 2, hence deﬁning their training sets. In our particular case, all variables had similar ranges, thus normalization was not necessary. A critical step was to drop feature vectors that were too close to each other (in an Euclidean norm sense), which not only removed redundant information from the batch, but also improved the numerical stability associated with the kernel matrix [26]. Finally, the GPs modeling rooms 1, 2 and 3 had respectively 231, 308 and 281 points, which correspond to approximately 38, 51 and 47 hours worth of data. Since these sets did not come from a single experiment, but are an informative subset of the 748 hours initially available to us, they provided enough prediction capabilities to our non-parametric models. All GPs were deﬁned and trained with the aid of the GPflow2 [42] and SciPy [43] packages. No priors were placed on the hyperparameters and we employed the marginal likelihood criterion along with the limited-memory BFGS optimization algorithm to tune them. Training the models with the aforementioned number of points took consistently less than 5 seconds each on a 2.4 GHz, core i99980HK machine. The obtained training and test results can be seen in Figure 4. We highlight that the plots show multi-step ahead predictions over a horizon of 2 hours, that is, 12 time steps, correcting for the temperatures mismatch only at the orange points. Assessing the prediction quality of the models in this way was necessary as they were to be used within an MPC formulation. It is worth noting that, even though the left plots are labeled as “training results”, the models incorporated only a small fraction of those features due to our dropping of nearby data-points. The central and right-side plots show the predictions over a period of 50 and 40 hours, but in completely new scenarios, never presented to the model during training. Inspecting Figure 4, we see that the mean predictions mostly followed the underlying ground-truth signal during training. The disparity among the rooms is in their uncertainty bands: whereas model 1 and 2 presented moderate levels of spread, model 3 showed a fairly large one. We believe this uncertainty to stem from room 3 being the most exposed one in terms of direct solar radiation, and from Rnot being a feature of its GP. We remind the reader that Rwas disregarded to accelerate the real-time computations and ensure that the optimization problem was solved within the time allocated to it. By taking this larger uncertainty into account, we were able to avoid violating constraints when closing the loop with the MPC controller. The outcome of the test phase was qualitatively similar to the training results, aside from some additional performance degradation close to the high temperature peaks. Overall, we deemed the results reasonable given the challenging two-hour horizon of the prediction task. Having access to the chiller real-time energy consumption was not possible during the course of this project for various reasons. We then decided to tackle the problem of operating the system while minimizing its electrical demand with the two-step approach described next. The ﬁrst goal was to reconstruct the chiller refrigeration power curve from historical data, more speciﬁcally, from the volumetric air-ﬂow rates along with the outdoor temperature and the supplied air temperatures. Based on the previous quantities, the thermal power delivered by the chiller was inferred. Next, we used as features the outdoor temperature Tand the sum of the valves positions Θ = θ+ θ+ θ, the latter correlating with the water ﬂow through the AHU coils. A representative dataset was gathered over a period of 203 hours, which encompassed both random open-loop excitation and closed-loop operation. During such period, the AHU valves ranged from being completely open to being fully closed, and the ambient temperature varied from 15 to 40 degrees Celsius. The data distribution can be seen in Figure 5. We remark that the portion of the domain where the outdoor temperature is high and the total valves opening is low is not populated with samples due to operational constraints of the system, a common issue in HVAC control [1]. The last step was to augment the batch with a grid of Tvalues paired with Θ = 0 degs and 0 kW labels to represent the zero water-ﬂow regime. Polynomial ridge regression was performed to ﬁt the data described above. After experimenting with diﬀerent model orders, we found that a cubic model provided a good balance between describing the observed points and not overﬁtting them. The results are presented in Figure 5, where the obtained model analytical expression is − 1.56 eTΘ + 3.09 eΘ− 2.75 eT(4) + 4.90 eTΘ − 6.86 eTΘ+ 2.56 eΘ+ 20.22 with ebeing a shorthand for ×10. As a second step, we imposed a concave coeﬃcient of performance (COP) curve, which is typical for variablespeed compressor chillers. It was assumed that the passive cooling load imposed by the remaining surgery center rooms was such that the vertical range in Figure 5 mapped to the increasing part of the COP curve, the peak, as well as the decreasing section. We did so intentionally to pose a challenging optimization objective and assess the capabilities of the proposed control scheme. The ﬁnal utilized COP curve was With the thermal model (4) and the COP curve (5) at hand, the electrical power could be calculated according to E = Q(T, Θ)/COP(Q(T, Θ)), measured in kW. Several slices of the thermal power surface, and their associated electrical power counterparts are presented in Figure 5. The plots illustrate how the curves change depending on the outdoor temperature, and how strongly the electrical power proﬁle is aﬀected by this external factor. In particular, one notices that when the outside temperature is high, it is more economic to open the valves and increase the chilled water ﬂow rather than keeping them partially closed. Clearly though, the real-time optimal position for them will depend on the system dynamics, the desired temperature envelope and the external disturbances. Given the learned GP models µ, i = 1, 2, 3 described in (3), a given maximum temperature T, and our reconstructed electrical power surface, we formulate the following optimization problem to control the valves θwhile reducing the chiller energy consumption E min(E+ ρ∆) + ρ∆(6a) s.t. T= µ(T, θ, T, T where Θ=Pθis the sum of all valves positions. The variables δin (6c) are positive slacks introduced to avoid infeasibility. If needed, these can relax the temperature constraint so that the solver can return a viable control plan. Of course, their use is heavily penalized in theP objective, where ∆=δand ρ, ρare large constants, which in our case were respectively set to 100 and 200. The temperature constraint (6c) also accounts for prediction uncertainty as it includes the standard deviation var. Its use confers on the formulation a risk-aware quality and robustiﬁes the closed-loop operation. The degree of conservativeness is controlled by the constant β, chosen to be 2 as in Figure 4. The prediction horizon was set to N = 12 steps, which translates to 2 hours. As suggested by our notation, Tand Twere kept constant throughout all prediction steps–but updated from one sampling period to the next. Finally, our maximum temperature value was T= 21 degrees Celsius. The optimization problem (6) was written in Python with the aid of CasADi [44], an automatic-diﬀerentiation package that provides gradient information for numerical solvers–in our case, the interior-point method IPOPT. As is customary in predictive control, (6) was recursively solved on-line with the most recent available system information, with only the ﬁrst optimal control action being transmitted to the valves. We underline that the main source of complexity in (6) is the presence of the constraints (6b) and (6c), which are highly non-linear due the GP mean and variance. Since convexity is absent, multiple local optima might exist, a fact that was indeed veriﬁed in practice. By intelligently providing solvers with high-quality initial guesses, this problem can be mostly overcome. Our particular case study relied on initializing the numerical solver with control, temperature, slack and energy trajectories obtained with a virtual PI controller. The intuition was to allow the MPC loop to build on such initial guess and further optimize operation. For a detailed study on solve times and how the number of GP data-points impacted them, see Appendix A. The previously described Gaussian process-based MPC formulation was deployed on the LC and used to operate the HVAC system during multiple days in the months of October and November. We report in Figure 6 a four-day interrupted experiment carried out from Nov. 10 to Nov. 13 that is rather representative of the local internal and external conditions. The plots show the rooms temperatures and the “immediate” uncertainty associated with the GP predictions: T= βvaras employed in the formulation (6c), and evaluated for the next time-step. Both outdoor signals, the temperature and the solar radiation, are also given. The reader is reminded that, although the latter contributes with additional heat gains, it is completely unknown to the controller as explained in Section 3.3. We highlight that the curves displayed in the ﬁgure were not ﬁltered in any way; the sole manipulation performed with the data was the imputation of the missing temperature entries using linear interpolation. These points, however, accounted for only 43 out of the 1728 indoor temperature values gathered during the four-day experiment. Consider ﬁrst the day November 10 and note the relatively high internal room temperatures when the experiment started, which were the consequence of a harsh previous day. The MPC controller used some control authority to bring the temperatures below the 21-degree line and then partially closed the valves. After the morning shift started (7 am), even though θand θwere fully open, Tand Tviolated the constraints and were only brought below 21 degrees late that evening. High initial conditions along with a peak outdoor temperature of 35 degrees overloaded the cooling system, causing violations of the indoor temperature constraint in two rooms. The two days that followed (November 12 and 13) were less warm and, as a result, the MPC controller successfully modulated the valves so as to guarantee constraint satisfaction. It is evident how θ, θand θassume lower values when the Tis low, and tend to saturate at their maximum during working hours, which matches our intuition. Lastly, we focus on the data from November 13, where one can readily see a sudden peak in the indoor temperatures, being also present in the T. This was caused by a momentary halt in the water pumps responsible for the chilled water circuit–an event that could be regarded as a fault from a control system perspective. During such period, as there was no water circulation through the AHU cooling coils, there was also no refrigeration and the indoor spaces received warm air since the fans were kept on. As soon as the pumps were again activated, the chiller immediately decreased the supply water temperature and the operation was normalized. During daytime, the indoor climate was kept within the desired limits despite the valves staying saturated at their low values, even at noon. The fact that almost no additional actuation was needed is due to that day being a Saturday, when no operations are scheduled for and the three doors present in the environment being minimally opened and closed. This demonstrates how strong the internal heat gains and disturbances normally are. To assess the energy savings associated with the proposed MPC strategy, the plant power consumption was computed based on the learned chiller curves described in Section 3.4. The demand curve resulting from employing the MPC controller during November 10 is shown in Figure 7 along with its control proﬁle, i.e., the valves positions. For comparison purposes, we considered a PI regulator certiﬁed by the local maintenance team as being reliable. This controller features an anti-windup mechanism and a lower actuation bound that varies depending on the occupancy schedule. Next, the same aforementioned day was replayed to the PI controller, producing control actions from which a power curve could be derived, as reported also in Figure 7. Comparing HVAC control strategies in terms of their energy consumption is challenging due to the diﬃculty in ﬁnding similar test conditions since not only the internal conditions (e.g. the chiller operating point) vary from week to week, but also the outdoor climate. Those factors inﬂuence the energy consumption in a non-linear fashion, and hence they cannot be compensated by a mere linear weighting procedure. The analysis carried out in this section has the downside of loosing the PI controller time-correlation, but has the advantage of being based on exactly the same test conditions. Focusing exclusively on the working hours, the average electrical power was for the PI and MPC curves was respectively 7.54 kW to 7.40 kW, a reduction of 1.9%. Notice moreover that the valves do not follow any obvious proﬁle under the MPC algorithm, which does not allow for the same control sequences to be obtained from hand-tuning or a PI controller or adding simple decision-rules to it. The reader is reminded that, as the indoor temperatures used to calculate the PI controller control actions were those obtained from an MPC operation, the PI curves present in Figure 7 are to be interpreted point-wise. By repeating the same procedure on the remaining three days, one arrives at the percent reduction of 2.6%, 1.6% and 2.2%. Our study has demonstrated the suitability of Gaussian process to learn buildings dynamics featuring forced-air HVAC systems. Aside from feature selection, which exploited information that is easy to infer from the building geometry, training the models was straightforward and did not require any informed initial guess for the hyperparameters. The most time-consuming step in the pipeline was certainly data acquisition and cleaning, during which spurious periods were identiﬁed and dropped from the dataset. Nevertheless, the same procedure would certainly also have to be carried out if one were to craft linear models for instance. The inherent robustness to outliers of one or another model class (and associated training objectives) could be the matter of future investigations. Overall, we regard GPs as being an appealing modelling framework for the building community that has potential to deliver good predictive performance as showcased in Section 3.3. Combining GPs and model predictive control leads to non-convex optimization problems and relatively high solve times. The numerical results in Appendix A along with the experimental results in Section 4.1 show how this combination is still amenable to real-time implementation in the domain of building temperature control, where sampling periods are in the order of minutes and the algorithms are executed by general purpose computers. In our particular case study, our approach was capable of regulating the indoor temperature whenever enough actuation power was available, despite the strong internal heat gains. We envision future investigations exploring two distinct paths. Firstly, speeding up computations to allow for longer prediction horizons and, potentially, better economical performance. Seeing that all computations were carried out in a centralized fashion in this study, reducing the solve times would be in principle possible by exploiting algorithm parallelization. Secondly, moving toward higher-level problems such as coordinating multiple buildings that operate under time-varying electricity prices, or participate in demand response programs. With an appropriate choice of non-linearity and training procedure, we believe Gaussian processes to be a good candidate modelling technique for these tasks. E. T. M. and C. N. J. received ﬁnancial support from the Swiss National Science Foundation under the RISK project (Risk Aware Data-Driven Demand Response, grant number 200021 175627). The authors express their gratitude to the São Julião hospital administrators and to the local maintenance team for their support. In order to shed light on how the number of training points aﬀects the solve times of the non-convex optimization problem (6), the following study was conducted. Four sets of GP models were trained on distinct datasets with cardinalities N = 352, 794 (precisely the one used in the experiments), 1280, 1910 and 2643. In all scenarios, the total number of points present in each of the GPs was approximately a third of the total number N, so that the models were balanced. We then generated random initial conditions, uniformly sampled from sensible intervals: 16 ≤ T≤ 23, 9 ≤ T≤ 13, and 15 ≤ T≤ 35. Finally, we solved the warm-started non-convex MPC (6) on a 2.4 GHz, i9 machine 50 times per scenario and recorded their run times. The results are presented in Figure A.8, where the vertical scale is logarithmic. The median values of the box plots rose from 4.19 to 18.69, 85.42, 121.12 and 255.68 seconds respectively from the smallest to the largest dataset. Although using N = 1910 points does not seem unreasonable at ﬁrst, challenging initial conditions such as ones close to violating constraints can easily increase the problem solve time: the highest point obtained for the N = 1910 scenario was 429 seconds. Recalling that ideally the control action has to be computed in negligible time, one concludes that this scenario is infeasible for practical deployment. Indeed, if the solve time is 429 secs and our sampling period is 600 secs, the control action will only be applied for 28% of the total time.