Matrix factorization (MF) can extract the low-rank features and integrate the information of the data manifold distribution from high-dimensional data, which can consider the nonlinear neighbourhood information. Thus, MF has drawn wide attention for low-rank analysis of sparse big data, e.g., Collaborative Filtering (CF) Recommender Systems, Social Networks, and Quality of Service. However, the following two problems exist: 1) huge computational overhead for the construction of the Graph Similarity Matrix (GSM), and 2) huge memory overhead for the intermediate GSM. Therefore, GSM-based MF, e.g., kernel MF, graph regularized MF, etc., cannot be directly applied to the low-rank analysis of sparse big data on cloud and edge platforms. To solve this intractable problem for sparse big data analysis, we propose Locality Sensitive Hashing (LSH) aggregated MF (LSH-MF), which can solve the following problems: 1) The proposed probabilistic projection strategy of LSH-MF can avoid the construction of the GSM. Furthermore, LSH-MF can satisfy the requirement for the accurate projection of sparse big data. 2) To run LSH-MF for ÓÄõne-grained parallelization and online learning on GPUs, we also propose CULSH-MF, which works on CUDA parallelization. Experimental results show that CULSH-MF can not only reduce the computational time and memory overhead but also obtain higher accuracy. Compared with deep learning models, CULSH-MF can not only save training time but also achieve the same accuracy performance. CCS Concepts: bility. Additional Key Words and Phrases: CUDA Parallelization On GPU And Multiple GPUs, Graph Similarity Matrix (GSM), Locality Sensitive Hash (LSH), Matrix Factorization (MF), Online Learning For Sparse Big Data, Top-ùêæ Nearest Neighbours. ACM Reference Format: Zixuan Li, Hao Li, Kenli Li*, Fan Wu, Lydia Chen, and Keqin Li. 2020. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data Analysis. 1, 1 (November 2020), 27 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Department of Electric Engineering, Mathematics and Computer Science, Distributed Systems, Delft ‚Ä¢ Computer systems organization ‚Üí Embedded systems; Redundancy; Robotics;‚Ä¢ Networks ‚ÜíNetwork relia- In the era of big data, the data explosion problem has arisen. Thus, a real-time and accurate solution to alleviate information overload on industrial platforms is nontrivial [2]. Big data come from human daily needs, i.e., social relationships, medical data and recommendation data from e-commerce companies [31]. Moreover, due to the large scale and mutability of spatiotemporal data, sparsity widely exists in big data applications [52]. For accurate big-data processing, representation learning can eliminate redundant information and extract the inherent features of big data, which makes big-data analysis and processing more accurate and eÓÄúcient [3]. Furthermore, for sparse data from social networks and recommendation systems, low-rank representation learning can extract features as latent variables to represent the node and user properties from the high-dimension space, which can alleviate the information loss owing to missing data [62]. MF is the state-of-the-art unsupervised representation learning model with the same role as Principal Component Analysis (PCA) and an autoencoder that can project the high-dimensional space into the low-rank space [9]. Due to its powerful extraction capability for big data, linear and nonlinear dimensionality reduction is widely used as an emerging low-rank representation learning model [4]. As one of the most popular dimensionality reduction models, MF can factorize high-dimensional data into two low-rank factor matrices via the constraints of prior knowledge, i.e., distance metrics and regularization items [19]. Then, MF uses the product of the two low-rank matrices to represent the original high-dimension data, which endows the MF with a strong generalization ability [40]. However, due to the variety of big data, e.g., multiple attributes of images [55], context-aware text information [28], etc., linear MF is not applicable to an environment with hierarchical information; thus, it should consider the inherent information of big data [1]. Nonlinear MF, e.g., neural MF [60] and the graph for manifold data [46] [34], which relies on the construction of the GSM, can mine deep explicit and implicit information. However, the Deep Learning (DL) model for neural MF needs multilayer parameters to extract inherent variables, which can limit the training speed and create huge spatial overhead for constructing a GSM; thus, DL cannot be adopted by industrial big data platforms. Thus, modern industrial platforms are anxious to save parameters in nonlinear MF models [67]. Neighbourhood information for nonlinear MF is an emerging topic [68] [23]. The neighbourhood model can strengthen the feature representation by capturing the strong relationship points within the data; and this model is popular in Recommendation Systems, Social Networks, and Quality of Service (QoS) [29] [67]. Handling neighbourhood information is based on several important neighbourhood points that should construct a GSM [51], [67]. However, the use of the GSM should consider the following two problems: 1) the selection and deÓÄõnition of the similarity function should be accurate, and 2) the huge time and spatial overhead caused by the construction of the GSM. The ÓÄõrst problem can be solved by using DL to select the best similarity [10]. However, the huge computational costs make DL unsuitable for cloud-side platforms. The construction of the GSM takes a huge amount of time and spatial overhead, and its parallelization is diÓÄúcult. Due to the quadratically increased spatial costs, the second problem is fatal to real applications using high-dimensional data. In this case, the approximated strategy is considered to replace the calculation of the GSM. LSH is a statistical estimation technique that is widely used in high-dimensional data for the Approximate Nearest Neighbourhood (ANN), and it maps the high-dimensional data to low-dimensional latent space using random projection, which can simplify the approximated search problem into a matching lookup problem [41]. Due to low time complexity, LSH has a fast processing capability for high-dimensional data [66]. Furthermore, LSH has the following drawbacks: 1) the LSH scheme has a slight loss of accuracy; 2) the use of DL can lead to high-precision hashes, but DL is not applicable to cloud-side platforms; 3) online tracking of the hash value for incremental big data; 4) due to information missing, Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data the similarity between sparse data is not very accurate and should be handled by a speciÓÄõc LSH function. Thus, it is nontrivial to achieve a reasonable accuracy in less time with ÓÄõne-grained parallelization for LSH. Furthermore, with the rapid development of GPU-based cloud-edge computing, increasingly more vendors will tend to use GPU acceleration [37]. There are three challenges to aggregate LSH with nonlinear MF eÓÄúciently to extract the deep features of sparse and high-dimensional data: 1) How can a suitable LSH function be deÓÄõned to reduce the computation time while ensuring reasonable accuracy? 2) How can LSH be accommodated with the nonlinear neighbourhood MF to achieve low spatial overhead in an online way? 3) How can a GPU and multiple GPUs be used to achieve a faster calculation speed? This work is proposed to solve the above problems, and the main contributions are presented as follows: (1)A novel Stochastic Gradient Descent (SGD) algorithm for MF on a GPU (CUSGD++) is proposed. This method can utilize the GPU registers more and disentangle the involved parameters. The experimental results show that it achieves the fastest speed compared to the state-of-the-art algorithms. (2)simLSH is proposed to replace the GSM and accomplish sparse data encoding. simLSH can greatly reduce the time and memory overheads and improve the overall approximation accuracy. Furthermore, an online method for simLSH is proposed for incremental data. (3)The proposed CULSH-MF can combine the access optimization on GPU memory of CUSGD++ and the neighbourhood information of simLSH for nonlinear MF. Thus, CULSH-MF can complete the training very fast and attain an 8000 compared to CUSGD++. Compared with deep learning models, CULSH-MF can achieve the same eÓÄùect, and CULSH-MF only needs to spend 0.01% of the training time. In this work, related works and preliminary ÓÄõndings are presented in Sections 2 and 3, respectively. The proposed model for LSH aggregated MF is presented in Section 4. Experiment results are shown in Section 5. Owing to the powerful low-rank generalization ability, MF is widely used in various ÓÄõelds of big data processing, i.e., Source Localization [ mender Systems [ LSH is a powerful hashing tool that can also strengthen the performance of nonlinear dimension reduction, including PCA and MF, for recommendation [ in optimization and machine learning communities, e.g., Maximum Margin Matrix Factorization (MMMF) [ Nonnegative Matrix Factorization (NMF) [ Matrix Factorization (WMF)[ classic nonconvex problem [ SGD [59] [64], or Cyclic Coordinate Descent (CCD) [47], is adopted to solve this nonconvex problem. An eÓÄúcient big data processing method requires highly eÓÄúcient hardware and algorithms. The rapid development and good performance of GPUs also tend to accelerate basic optimization algorithms that consider the global memory access, threads and thread block synchronization on a GPU. Thus, the parallelization processes of related methods on GPUs have unique specialties. on a GPU. ùëãùëñùëí ùëíùë° ùëéùëô. [59] proposed cuSGD based on data parallelization. cuSGD [59] achieves the goal of acceleration by adopting data parallelization on a GPU, and it has no load imbalance problem. algorithm and proposed the GPU-based CCD++ algorithm. 25], [24], Hyperspectral Image ClassiÓÄõcation [65] and Biological Data Analysis [11]. Furthermore, tuple multiplication and summation and CUMSGD based on the elimination of row and column dependencies. These basic algorithms have good performance on a GPU. However, scalability is not considered, which results in signiÓÄõcant limitations of model compatibility. Nonlinear MF comprises two components, i.e., a DL model for neural MF [61] and a neighbourhood model with GSM for graph MF [29] [14].ùêªùëí ùëíùë° ùëéùëô.[18] proposed Neural Collaborative Filtering (NCF) using the DL model, and this model involves a multilayer neural network that can extract the low-rank feature of MF [61]. The neighbourhood model is often integrated into the algorithm and brings better results [29] [14]. The construction of a GSM requires calculating the similarity between high-dimensional points, the choice of similarity functions play a key role in speciÓÄõc environments, and the selection of the Top-ùêænearest neighbours from the GSM is time consuming [26]. However, designing an eÓÄùective similarity function is a diÓÄúcult task. Research on training similarities through DL is emerging [15]. However, high-dimensional data cause the computational complexity of DL to dramatically increase. In order to further optimize the calculation and save space, pruning strategies and approximation algorithms have been proposed [12]. LSH is such an approximate algorithm based on probability projection [44]. Furthermore, the inverse use of LSH can also achieve the farthest neighbour search [64]. However, most LSH algorithms do not work well in sparse data environments. minLSH is able to calculate the similarity between sets, but does not consider the weights of the elements in the set. Although a considerable amount of work has sought to improve minLSH, this work increases the complexity [56]. simHash [44] showed good performance in similar text detection. LSH can project the feature vectors of similar items to equal hash values with a high probability [20], and this makes LSH widely used for nearest neighbour searches, fast high-dimensional information searches, and similarity connections [36,63]. Due to the inherent sparsity of big data, using LSH to construct a GSM to aggregate sparse MF on a big data platform is nontrivial work. Furthermore, the accuracy of the low-rank tracking of online learning for incremental big data is a key problem [27]. ùê∂‚Ñéùëíùëõ ùëíùë° ùëéùëô. proposed an online hash for incremental data [7]. However, there is a lack of an online LSH strategy for sparse and online data on parallel and distributed platforms. In this section, LSH for neighbouring points with closer projective hash values is presented in Section 3.1. The basic MF model and nonlinear MF with notations are introduced in Section 3.2, and the related symbols are listed in Table 1. DeÓÄõnition 3.1 (Graph Similarity Matrix (GSM)). We assume 2 sets asùêº = {ùêº, ¬∑ ¬∑ ¬∑ , ùêº, ¬∑ ¬∑ ¬∑ , ùêº}andùêΩ = {ùêΩ, ¬∑ ¬∑ ¬∑ , ùêΩ, ¬∑ ¬∑ ¬∑ , ùêΩ}. Given two variables{ùêΩ, ùêΩ} ‚àà ùêΩand a similarity functionS(ùëó||ùëó), the goal is to construct a weighted fully directed graphG, where each vertex represents a variable inùêΩ, and the weight of each edge represents the similarity of the output vertex to the input vertex calculated byS(ùëó||ùëó). The construction of GSMGshould consider the relationÓÄàÓÄâ between ùêΩ and ùêº. The value of Grelies on{ùëü|ùëñ ‚ààbŒ©}, {ùëü|ùëñ ‚ààbŒ©}. The neighbourhood similarity query for variable setùêΩrelies on the GSMG‚àà R[29] [67] [20]. The most important problem in the neighbourhood model is to ÓÄõnd a set of Top-ùêæsimilar variables. For this problem, the Top-ùêæ nearest neighbours query is emerging. DeÓÄõnition 3.2 (Top-ùêæNearest Neighbours). Given a set of variablesS, each variable as a vertex constitutes a fully directed graphG. The goal is to ÓÄõnd a subgraphSwhere each vertex hasKand onlyKout edges point to the vertices of its Top-K similar variables. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data By querying the GSM, the Topof the GSM is huge. If variable set overhead for the Topnearest neighbours for the variable set overall overhead is GSM using high-dimensional sparse big data is not advisable. In the context of high-dimensional sparse big data, the calculation costs of a GSM are squared. In this case, we need to reduce unnecessary calculations or ÓÄõnd an alternative method. LSH is a probabilistic projection method that projects two similar variables with a high probability to the same hash value while two dissimilar variables are projected to diÓÄùerent hash values with a high probability. We need to judge the similarity between the two variables and ÓÄõnd the Top-ùêæ nearest neighbours for each variable. ùëü(ùëñ, ùëó)th element in matrix R; Œ©The set ùëó of non-zero value in matrix R for variable ùêº; bŒ©The set ùëñ of non-zero value in matrix R for variable ùêΩ; U/ùë¢Left low-rank feature matrix ‚àà R/ ùëñth row; V/ùë£Right low-rank feature matrix ‚àà R/ ùëó th row; ùëèThe deviation between variable ùêº‚àà ùêº and ùúá; bùëèThe deviation between variable ùêΩ‚àà ùêΩ and ùúá; ùëèOverall baseline rating = ùúá + ùëè+bùëè; ùëõThe number of entries in variable set ùêº which have relations withVariables {ùëó, ùëó} in variable set ùêΩ ; ùúåPearson similarity of two variables { ùëó, ùëó} ‚àà ùêΩ ; ùëÜ‚àà ùêΩ ; WExplicit inÓÄûuence matrix ‚àà Rto represent the degree of explicitInÓÄûuence for variable set ùêΩ ; CImplicit inÓÄûuence matrix ‚àà Rto represent the degree of implicitInÓÄûuence for variable set ùêΩ ; ùë§/ùë§ùëóth Explicit inÓÄûuence vector ‚àà Rof W / the ùëòth element of ùë§; ùëê/ùëêùëóth Implicit inÓÄûuence ‚àà Rof C / the ùëòth element of ùëê; ùëÇùëÅ(2ùêæ +1) + ùëÅ (ùêæ ‚àí ùêæ‚àí1), and the spatial overhead isùëÇ (ùëÅ ùêæ). Thus, the construction of a DeÓÄõnition 3.3 (Locality Sensitive Hash (LSH)). The LSH function is a hash function that satisÓÄões the following two points: ‚Ä¢For any pointsùë•andùë¶inRthat are close to each other, there is a high probabilityùëÉthat they are mapped to the same hash value ùëÉ[‚Ñé(ùë•) = ‚Ñé(ùë¶)] ‚©æ ùëÉfor||ùë• ‚àí ùë¶||‚©Ω ùëÖ; and ‚Ä¢For any pointsùë•andùë¶inRthat are far apart, there is a low probabilityùëÉ< ùëÉthat they are mapped to the same hash value ùëÉ[‚Ñé(ùë•) = ‚Ñé(ùë¶)] ‚©Ω ùëÉfor||ùë• ‚àí ùë¶||‚©æ ùëêùëÖ= ùëÖ. The use of LSH has allowed us to reduce the complexity from ùëÇ (ùëÅ) to ùëÇ(ùëÅ ). As Fig. 1 shows, the construction of a GSM requiresùëÇ (ùëÅ)similarity calculations and consumesùëÇ (ùëÅ)space while the calculation and spatial consumption of LSH is ùëÇ(ùëÅ ). LSH can alleviate the problem of huge computational overhead. However, there are several problems when the LSH is applied to a system with a neighbourhood model: 1) How can a system with a neighbourhood model using LSH obtain the same overall accuracy as the original method? 2) How can the computational model for LSH be incorporated in a big data processing system? 3) How can the system with the LSH model accommodate online learning for incremental data? In big data analysis communities, representation learning can disentangle the explicit and implicit information behind the data, and the low-rank representation problem is presented as follows. DeÓÄõnition 3.4 (Representation Learning for Sparse Matrix [3]). Assume a sparse matrixR ‚àà Rpresents the relationship of 2 variable sets{ùêº, ùêΩ }. The valueùëürepresents the relation degree of the variables{ùêº}inùêºand{ùêΩ}in ùêΩ. Due to missing information, the representation learning task for variable{ùêº}trains the feature vectorùë¢relying on nonzero values {ùëü| ùëó ‚àà Œ©}, and the representation learning task for variable {ùêΩ} is to train the feature vector ùë£ relying on nonzero values {ùëü|ùëñ ‚ààbŒ©}. DeÓÄõnition 3.5 (Sparse Matrix Low-rank Approximation). Assume a sparse matrixR ‚àà Rand a divergence functionÓÄÄÓÄÅ DR‚à•bRthat evaluates the distance between two matrices. The purpose of the low-rank approximation is to ÓÄõnd an optimal low-rank matrixbR and then minimize the divergence. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data MF only involves low-rank feature matrices, and the feature vectors are used for cluster and social community detection [ is applied to this problem because it factorizes the sparse matrix into two low-rank feature matrices. In addition, MF model has two limitations: 1) this model is too shallow to capture more aÓÄüuent features, and 2) this model cannot capture dynamic features. There are 4 parts in Equation (1), and those parameters can combine the explicit and implicit information of the neighbourhood for nonlinear MF, which are introduced as follows[29] [67] [20]: 1‚óã {ùúá, ùëè ùêΩin set ùêΩ . Considering that diÓÄùerent variables ùêº diÓÄùerent variables supposeùúá variableùêΩ√ç ùúá = average relation of the known elements in relation of the known elements in ùêΩ is the number of variables variables{ùêΩ regularization parameter that adjusts the importance. By searching for the GSM, the TopofùêΩwith explicit relation with variable variable{ùêΩ the above explicit relations. Feature vectors ùëÜ( ùëó)of variable ùêΩ. The closer the basic predicted value by a scaling factor 3‚óã {ùëÅ (ùëñ), ùëÅ relation with the variable the variable vectorsùëê 9]. A sparse matrix has only a few elements that are valuable, and all other elements are zero. Sparse MF |{z}|{z}(1) ,bùëè, ùëè}: The baseline score is represented asùëè= ùúá + ùëè+bùëèfor the relation of variableùêº‚àà ùêºand variable ùêΩ‚àà ùêΩhave their own diÓÄùerent preferences for the entire variable setùêº. To simplify the description, is the overall relation between variable setùêºand variable setùêΩ;ùëèrepresents the deviation between variable ùúá, which indicates the preference of variableùêºto variable setùêΩ; andbùëèrepresents the deviation between ‚àà ùêΩandùúá, which indicates the preference of variableùêΩto variable setùêº. A simple case is presented as:√ç ùëü/|Œ©|(the average relation of the known elements),ùëè=ùëü/|Œ©| ‚àí ùúá(the diÓÄùerence between the , ùëÜ( ùëó), ùëÖ(ùëñ), ùëÖ(ùëñ;ùëó), ùë§}: Suppose thatùêΩandùêΩare any two variables inùêΩ, andùëõ= |bŒ©bŒ©| , ùêΩ} ‚àà ùêΩas a baseline. The( ùëó, ùëó)th element of GSM is deÓÄõned asùëÜ=ùúå, whereùúÜis the of the variableùêΩ‚àà ùêΩcan be obtained. To retain the generalizability,ùëÖ(ùëñ)is denoted as the variable subset } ‚àà ùëÖ(ùëñ;ùëó) = ùëÖ(ùëñ)ùëÜ( ùëó), variableùêº‚àà ùêºhas more explicit relations with variableùêΩ. We parameterize ùêΩ.ùë§is used to represent the information gain that variableùêΩ‚àà ùëÖ(ùëñ;ùëó)explicitly brings toùêΩ‚àà )is used as the coeÓÄúcient ofùë§. Combining all(ùëü‚àí ùëè)ùë§,ùêΩ‚àà ùëÖ(ùëñ;ùëó)and multiplying the resultÓÄåÓÄåÓÄåÓÄå (ùëñ;ùëó), ùëê}: To retain the generalizability,ùëÅ (ùëñ)is denoted as the variable subset ofùêΩwith an implicit√ë ùêº‚àà ùêºhas more implicit relations with variableùêΩ. We parameterize the above implicit relations. Feature ‚àà Rare used as the implicit factors for the Top-ùêænearest neighboursùëÜ( ùëó)of a variableùêΩ.ùëêis used to represent the information gain that variableùêΩ‚àà ùëÅ(ùëñ;ùëó)implicitly brings to variableùêΩ‚àà ùêΩ. Combining allùëê,ÓÄåÓÄåÓÄåÓÄå ùêΩ‚àà ùëÅ(ùëñ; ùëó ) and multiplying the result by a scaling factorÓÄåùëÅ(ùëñ; ùëó )ÓÄå, we obtainÓÄåùëÅ(ùëñ; ùëó )ÓÄå√çùëê. 4‚óã {ùë¢, ùë£}: Original MF model.ùë¢is the low-rank feature vector for variableùêº‚àà ùêº, andùë£is the low-rank feature vector for variable ùêΩ‚àà ùêΩ . With the neighbourhood consideration andùêønorm constraints for the parameters{U, V, ùúá, ùëè,bùëè, ùë§, ùëê}, the optimization objective is presented as: where {ùúÜ, ùúÜ, ùúÜ, ùúÜ, ùúÜ, ùëéùëõùëëùúÜ} are the corresponding regularization parameters. There are two improvements: 1) the neighbourhood inÓÄûuences are inherent in some big data applications [1] [68] [22], and 2) the Top-ùêænearest neighbourhood with explicit and implicit information can replace all queries of neighbourhood points [29] [67] [20]. Fig. 2 illustrates the structure of this work. First, we consider the interaction value of variableùêºin variable setùêº and variableùêΩin variable setùêΩand generate the interaction matrixRfrom this. Second, the original method, which is based on the GSM, can calculate the similarity of every two variablesùêΩandùêΩin variable setùêΩto generate a similarity graphG; and queryingGto obtain the subgraphScan hold the Top-ùêænearest neighbours of each variableùêΩ‚àà ùêΩ. The diÓÄùerence is that the simLSH method we proposed constructs a hash table throughùëùcoarse-grained hashings and ùëûÓÄõne-grained hashings. Then, we obtain the subgraphSthrough the hash table. Finally, we train the feature vectors using the updating rule (5). As Fig. 2 shows, this work should consider the following three parts: 1) Interaction matrixRof two variable sets {ùêº, ùêΩ }, which should consider the incremental data and add the coupling ability of the overall system. 2) The construction of a neighbourhood relationship should reduce the overall space and computational overhead and maintain the overall accuracy. 3) Training the representation feature vectors in a low computational and high accuracy way. The above objectives guide this section. In this section, LSH for sparse big data and CUDA parallelization are presented in Section 4.1; and then stochastic optimization strategy, CUDA parallelization and multiple GPUs for sparse big data are presented in Section 4.2, Finally, the online learning solution is presented in Section 4.3. The Top-ùêænearest neighbours, which relies on the construction of the GSM, is a key step in the nonlinear neighbourhood model. However, the GSM requires a huge amount of calculations, and the time complexity isùëÇ (ùëÅ)based on the Pearson similarity. A variety of LSH functions are not friendly to sparse data, Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data because the accuracy of most distance measures will be greatly reduced. This is caused by there being very few positions where the nonzero elements of each vector are the same. The Jaccard similarity is suitable for sparse data, and its representative algorithm is minHash[ and neglects the real value. In order to solve this problem, simLSH, which is inspired by simHash applied to text data, is proposed for sparse dig data projection [ value of the elements and maintains low computational complexity. simLSH can eÓÄùectively combine the number of interactions of variable sets the computational complexity. simLSH is comprised of the following two parts: 1) Coding for Sparse Big Data: simLSH randomly generates value. The hash value hash value diÓÄùerentùëü can be expressed as: ùêªshould also be aùê∫-bits{0,1}string. After the hash valueùêªfor variableùêΩ‚àà ùêΩis calculated, we obtain by accumulatingŒ¶(ùêª) ¬∑ Œ®(ùëü),ùëñ ‚àà Œ©.Œ®(ùëü)is a function such that there is a suitable interval between s, andŒ¶(ùêª)is a function that mapsùêªfrom{0,1}to{‚àí1, +1}. Finally,Œ•()maps the nonnegative value {1}and the negative value to{0}. Then, theùê∫-bit{0,1}stringùêªis obtained. The entire process of simLSH As Fig. 3 shows, variableùêΩhas three relation valuesùëü{3,4,5}with{ùëñ, ùëñ, ùëñ} ‚àà Œ©. Whenùê∫ =3,{ùêª, ùêª, ùêª}areÓÄàÓÄâ randomly assigned to{001,010,100}, respectively. It takesŒ®(ùëü) = ùëüby calculating(‚àí3‚àí4+5), (‚àí3+4‚àí5), (3‚àí4‚àí5); and then, theùê∫positions{‚àí2, ‚àí4, ‚àí6}ofùêªare obtained, respectively. Finally, we obtain theùê∫-bit{0,1}stringùêª {0, 0, 0} by mapping operations. 2)Coarse-grained and Fine-grained Hashing: LSH is an approximation method to estimate the GSM, but it will achieve accuracy losses when applied to sparse big data. In this case, simLSH is proposed to speed up the calculations and improve the accuracy. Since the maximum probability of two extremely dissimilar variables {ùêΩ, ùêΩ} with the same hash value is ùëÉ, the mapping of a hash function does not guarantee that the variables{ùêΩ, ùêΩ}with the same hash value are similar. In order to alleviate this situation, the multiple random mapping strategy is considered as follows. (1) Coarse-grained Hashing: Similar variables with the same hash values of all mappings are considered. Ifùëùrandom mappings are conducted, whereùëù ‚â™ ùëÅ, the probability of two dissimilar variables projected as similar pairs is reduced to at mostùëÉ. Furthermore, the probability of two similar variables projected as similar pairs is also reduced to at leastùëÉ. Under this condition, many similar variable pairs will be missed. (2) Fine-grained Hashing: In this strategy, as long as at least one of the two variables{ùêΩ, ùêΩ}projected as similar pairs is subjected to coarse-grained hashing, the similar variable pairs{ùêΩ, ùêΩ}are selected. Suppose thatùëûcoarse-grained hashings are conducted. The probability of two similar variables{ùêΩ, ùêΩ}projected as similar pairs is increase to at least 1‚àí (1‚àí ùëÉ). By increasing the values of Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data ùëùandùëû, the probability that two similar pairs of variables method can improve the probability, and its calculation amount is sizes ofùëù variable pairs, which can reduce the computational complexity to probability, the computational complexity is only ùëù √ó ùëû √ó ùëÅ , and ùëù √ó ùëû √ó ùëÅ is much smaller than ùëÅ Our goal is to ÓÄõnd the Topnearest neighbours for We use the coarse-grained and ÓÄõne-grained hashing of simLSH and select the ùêΩin the hash table of variable each thread block for simLSH (CULSH) manages a variable 1‚àí9: The calculation of simLSH with coarse-grained hashing and ÓÄõne-grained hashing. In lines 3-5, calculate the hash valueùêªfor variable Search the Top-ùêæ nearest neighbours {ùêΩ Input: Sparse matrix R of variable sets {ùêº, ùêΩ }, Random Hash values ùêª 4.2 Stochastic Optimization Strategy And CUDA Parallelization On GPUs And Multiple GPUs The basic optimization objective (2) involves 6 tangled parameters strategy of SGD in [ the optimization objective (2) is nonconvex, and alternative minimization is adopted [ andùëû. Before the model training, we only need to perform multiple simLSHs onùëÅvariables to ÓÄõnd similar : The Top-ùêæNearest Neighbors MatrixJ‚àà R. Each row represents the Top-ùêæNearest Neighbors of ‚àà ùêΩ . disentangle the involved parameters as follows: SGD is a powerful optimization strategy for large-scale optimization problems [17] [54]. Using SGD to solve the optimization problem (4) is presented as: where the parameters{ùõæ,ùõæ,ùõæ,ùõæ,ùõæ,ùõæ}are the corresponding learning rates andùëí= ùëü‚àíbùëü. The update rule (5) has parallel inherence. Then, the proposed CULSH-MF is comprised of the following three steps: 1)Basic Optimization Structure(CUSGD++): CUSGD++ only considers the basic two parameters{U, V}. Compared with cuSGD, CUSGD++ has the following two advantages: (1) Due to the higher usage of GPU registers in Stream Multiprocessors (SMs),ùë¢orùë£can be updated in the registers, avoiding the time overhead caused by a large number of memory accesses. The memory access model is illustrated in Fig. 4. SM{1,2}update{ùë¢, ùë¢}in the registers,ÓÄà respectively; and{ùë£, ùë£, ùë£, ùë£, ùë£, ùë£, ùë£}, {ùë£, ùë£, ùë£, ùë£, ùë£, ùë£,ÓÄâ ùë£}are returned to global memory after each update step. (2) Due to the disentanglement of the parameters in the update rule (5), the data access conÓÄûict is reduced, which ensures a high access speed. From the update rule (5), the update processes of{U, V}are symmetric. Algorithm 2 only describes the update process of{U}in the reg-ÓÄàÓÄâ isters as follows:(1)Lines 2‚àí3: Givenùëá ùêµthread blocks, feature vectorsùë¢|ùëñ ‚àà {1, ¬∑ ¬∑ ¬∑ , ùëÄ}are evenly assignedÓÄàÓÄâ to thread blocksùëá ùêµ|ùë°ùëè_ùëñùëëùë• ‚àà {1, ¬∑ ¬∑ ¬∑ ,ùëá ùêµ}. Each thread blockùëá ùêµreads its own feature vectorùë¢from the global memory into the registers.(2)Line 4: The feature vectorùë¢with all nonzero values{ùëü| ùëó ‚àà Œ©}in the thread blockùëá ùêµis updated.(3)Lines 5‚àí7: Use the warp shuÓÄüe instructions [34] to accelerate the dot product Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data ùë¢ùë£of two vectors ters that are faster than shared memory and does not involve thread synchronization. Furthermore, this technology aligns and merges memory to reduce the access time. The number of threads in a form is 32, and elementsÓÄà ùëá|ùë°_ùëñùëëùë• ‚àà { elements corresponding products the dot product rereading from global memory for the next update, and feature vectors Line 11: After all nonzero values will no longer be used. 2)Aggregated Model imbalance does not aÓÄùect the serial model. However, it obviously aÓÄùects the running speed of the parallel model. The most signiÓÄõcant impacts are the following two points: (1) discontinuous memory access, and (2) imbalanced load on each thread this section. In CULSH-MF, the adjustment takes the set√ê each variable expression, we use align and merge memory to reduce the overhead for GPU memory access, are used. 1, ¬∑ ¬∑ ¬∑ ,32}. A threadùëáin each thread blockùëá ùêµsequentially reads the correspondingÓÄàÓÄâ ùë¢, ùë£|ùëì%32= ùë°_ùëñùëëùë•, ùëì ‚àà {1, ¬∑ ¬∑ ¬∑ , ùêπ }in feature vectors{ùë¢, ùë£}, and the threadùëácalculates theÓÄàÓÄâ ùë¢ùë£=ùë¢ùë£.(4)Lines 8‚àí10: Feature vectorsùë¢are updated in the registers to avoid ùëá. In order to solve the above problems, an adjustment for the parameters{W, C}is proposed in ùëÅ(ùëñ;ùëó),ùëÖ(ùëñ;ùëó)ùëÅ(ùëñ;ùëó) = ‚àÖ. Thus, the number of the involved elements for{W, C}are equal andÓÄàÓÄâ ùêΩinvolves 2ùêæparameters{ùë§|ùëò ‚àà {1, ¬∑ ¬∑ ¬∑ , ùêæ}}, {ùëê|ùëò ‚àà {1, ¬∑ ¬∑ ¬∑ , ùêæ}}. For the convenience of the ùëòandùëòto represent the indexes ofùëóandùëóin theseùêæparameters, respectively, which means that√ç are represented asùë§andùëê, respectively. The computational process of(ùëü‚àíùëè)ùë§ ùëêinvolves the dot product and summation operations. Thus, thewarp shuÓÄüe instructions, which can Algorithm 2: CUSGD++ Input: Initialization of low-rank feature matrices {U, V}, interaction matrix R, learning rate {ùõæ,ùõæ}, regularization parameter {ùúÜ, ùúÜ}, and training epoches ùëíùëùùëú. Output: U. CULSH-MF also takes advantage of the register to reduce the memory access overhead and then increase the overall speed. Due to the limited space, we only introduce the update rule of{V,bùëè, W, C}in the registers. In Al-√çÓÄé gorithm 3, the update process is presented in detail as follows:(1)Line 1: Average valueùúá =ùëü|Œ©|as the basis value.(2)Lines 3‚àí7: Given TB thread blocks, parameters{ùë£,bùëè, ùë§, ùëê| ùëó ‚àà {1, ¬∑ ¬∑ ¬∑ , ùëÅ }}are evenly as-ÓÄàÓÄâ signed to thread blocksùëá ùêµ|ùë°ùëè_ùëñùëëùë• ‚àà {1, ¬∑ ¬∑ ¬∑ ,ùëá ùêµ}. Each thread blockùëá ùêµreads its own parameters {ùë£,bùëè, ùë§, ùëê}from the global memory into the registers. In addition, the reading of memory is also aligned and merged.(3)Lines 8: The parameters{ùë£,bùëè, ùë§, ùëê}with all nonzero values{ùëü|ùëñ ‚ààbŒ©}in thread blockùëá ùêµare updated.(3)Lines 9‚àí11: Use the warp shuÓÄüe instructions [34] to accelerate the dot productùë¢ùë£and summation√ç√çÓÄàÓÄâ {(ùëü‚àí ùëè)ùë§,ùëê}. Elementsùë¢, ùë£, ùë§, ùëê|ùëì ‚àà {1, ¬∑ ¬∑ ¬∑ , ùêπ }, ùëò, ùëò‚àà {1, ¬∑ ¬∑ ¬∑ , ùêæ}in parameters{ùë¢, ùë£, ùë§, ùëê| ùëó ‚àà {1, ¬∑ ¬∑ ¬∑ , ùëÅ }}are evenly assigned to thread blocksÓÄàùëá|ùë°_ùëñùëëùë• ‚àà {1, ¬∑ ¬∑ ¬∑ ,32}ÓÄâ. A threadÓÄà ùëáin each thread blockùëá ùêµsequentially reads the corresponding elementsùë¢, ùë£, ùë§, ùëê|ùëì%32=ÓÄâ ùëò%32= ùëò%32= ùë° _ùëñùëëùë•, ùëì ‚àà {1, ¬∑ ¬∑ ¬∑ , ùêπ }, ùëò, ùëò‚àà {1, ¬∑ ¬∑ ¬∑ , ùêæ}in parameters{ùë¢, ùë£, ùë§, ùëê}, and the threadùëáÓÄà calculates the corresponding calculationsùë¢ùë£, (ùëü‚àí ùëè)ùë§, ùëê|ùëì%32= ùëò%32= ùëò%32= ùë°_ùëñùëëùë•, ùëì ‚ààÓÄâ√ê√ë {1, ¬∑ ¬∑ ¬∑ , ùêπ }, ùëò, ùëò‚àà {1, ¬∑ ¬∑ ¬∑ , ùêæ}. Please note that sinceùëÜ( ùëó) = ùëÖ(ùëñ;ùëó)ùëÅ(ùëñ;ùëó)andùëÖ(ùëñ;ùëó)ùëÅ(ùëñ;ùëó) = ‚àÖ, the thread only calculates one of(ùëü‚àí ùëè)ùë§andùëê. This makes the load of each threadùëárelatively balanced√çÓÄÄ√ç during the update process. Then, the warp shuÓÄüe in threadùëáto obtain thebùëü= ùúá+ùëè+bùëè+ùë¢ùë£+ √ç(ùëü‚àí ùëè)ùë§+√çùëêÓÄÅ.(4)Lines 12‚àí18: Parameters{ùë£,bùëè, ùë§, ùëê}are updated in the registers to avoid rereading from global memory for the next update, and parameters{ùë¢, ùëè}are updated directly in global memory.(5)Lines 19‚àí22: After all nonzero values{ùëü|ùëñ ‚ààbŒ©}have been updated, the latest{ùë£,bùëè, ùë§, ùëê}are written to global memory because they will no longer be used. These operations are similar to CUSGD++. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data The algorithm has the following advantages: access to global memory and decreasing the time consumption; and each thread with CUSGD++, CULSH-MF can assemble more tangled parameters of the nonlinear MF model. The parameters optimize the memory access by allowing the computational overhead to be further reduced. The spatial overhead is ùëÇ (|Œ©| + ùëÄùêπ + ùëÅ ùêπ + and the Top-ùêæ GSM matrix J R{ùëùùëéùëüùëéùëöùëíùë°ùëíùëü }: parameter in register memory Input: Initialization for {U, V, ùúá, ùëè regularization parameters {ùúÜ 3)Multi-GPU Model be extended to multiple GPUs (MCUSGD++/MCULSH-MF). We use data parallelism to allow multiple GPUs to run our algorithms at the same time. To avoid data conÓÄûicts, each GPU-updated block cannot be on the same ùêΩ. After the update is completed, the updated parameters are not sent back to the CPU because another GPU needs these data directly. Transferring data directly in the GPUs avoids the extra time overhead of uploading to the CPU and ùëáis balanced, which can avoid idle threads and can improve the active rate of threads. Compared , ùëê}are taken as a whole, and the memory is merged and aligned. Then, the use ofwarp shuÓÄüecan further } ‚Üê G{ùë£} } ‚Üê G{ùë§} } ‚Üê G{ùëê} } ‚Üê R{ùë£} } ‚Üê R{ùë§} } ‚Üê R{ùëê} then allocates them to other GPUs. Each GPU is assigned some speciÓÄõc parameters, which are not needed by other GPUs. After all updates are completed, each GPU passes the parameters that are stored at that time back to the CPU.ÓÄàÓÄâ Assume that we haveùê∑GPUs. The sparse matrixRis divided intoùê∑ √ó ùê∑partsR|ùëë, ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}. Low-ÓÄàÓÄâ rank feature matrices{U, V}are divided into{U|ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}}, {V|ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}}, respectively. InÓÄûuenceÓÄàÓÄâ matrices{W, C}are divided into{W|ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}}, {C|ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}}, respectively. The parametersÓÄàÓÄâ R, V, W, C|ùëë‚àà {1, ¬∑ ¬∑ ¬∑ , ùê∑}are allocated to theùëëth GPU and do not require transmission. Parameter Vis allocated to theùëëth GPU at initialization and then transferred to another GPU after each update step. Fig. 5 depicts MCUSGD++ on three GPUs. MCULSH-MF is similar and is given in parentheses below. The sparse matrixR is divided into 3√ó3 blocks. The training process of all the parameters is divided into three parts: (1): GPUs{1,2,3}ÓÄàÓÄâÓÄàÓÄâ update{U, V, (W, C)}, {U, V, (W, C)}, {U, V, (W, C)}and then transmitU, U, Uto GPUs{3,1,2},ÓÄà respectively; (2): GPUs {1, 2, 3} update{U, V, (W, C)}, {U, V, (W, C)}, {U, V, (W,ÓÄâÓÄàÓÄâÓÄà C)}and transmitU, U, Uto GPUs {3, 1, 2}, respectively; and (3): GPUs {1, 2, 3} update{U, V,ÓÄâÓÄàÓÄâ (W, C)}, {U, V, (W, C)}, {U, V, (W, C)}and transmitU, U, Uto GPUs {3, 1, 2}, respectively. Big data analysis should consider the incremental data, and the corresponding model can be compatible with the incremental data. The amount of incremental data is much smaller than the amount of original data. Thus, the time overhead for retraining the overall data is not worthwhile. It is nontrivial to design an online model for incremental data. The variable sets{ùêº, ùêº,bùêº }and{ùêΩ, ùêΩ,bùêΩ }are denoted as the original variable set, new variable set, and overall variable set, respectively. In this work, we consider that the new variable setsùêºandùêΩenter the system and interact with variable sets ùêΩ and ùêº, respectively. Please note that this allows variable set ùêº to interact with variable set ùêΩ . For the original variableùêΩ‚àà ùêΩ, the Top-ùêænearest neighbours{ùêΩ, ¬∑ ¬∑ ¬∑ , ùêΩ} ‚àà ùêΩare kept. For the new variable ùêΩ‚àà ùêΩ, we search its Top-ùêænearest neighbours{bùêΩ, ¬∑ ¬∑ ¬∑ ,bùêΩ} ‚ààbùêΩ. The hash value of variable setùêΩdepends on ùêº, and the hash value of variable setùêΩdepends onbùêº. In order to keep them consistent, we update the hash value√ç of variableùêΩ‚àà ùêΩ; then, we save the intermediate variablesŒ®(ùëü)(2¬∑ ùêª‚àí1)of simLSH and updateùêª= Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data learning solution is described in Algorithm 4 as follows: (1) Lines 1 ùêΩ. Saving the intermediate variables makes the process only require a small amount of calculation. (2) Lines 4 Calculate hash value depend on neighbours in the overall variable set reduce memory access. (5) Lines 13 CULSH-MF is comprised of two parts: 1) Basic parallel optimization model depends on CUSGD++, which can utilize the GPU registers more and disentangle the involved parameters. CUSGD++ achieves the fastest speed compared to the state-of-the-art algorithms. 2) The Topreduce the time and memory overheads. Furthermore, it can improve the overall approximation accuracy. In order to demonstrate the eÓÄùectiveness of the proposed model, we present the experimental settings in Section 5.1. The speedup performance of CUSGD++ compared with the state-of-the-art algorithms is shown in Section 5.2. The accuracy, robustness, online learning and multiple GPUs of CULSH-MF are presented in Section 5.3. CULSH-MF is a nonlinear neighbourhood model for low-rank representation learning, and we compare CULSH-MF with the DL model in Section 5.4 to demonstrate the eÓÄùectiveness of CULSH-MF. Œ®(ùëü)Œ¶(ùêª) +√çŒ®(ùëü)Œ¶(ùêª). Furthermore, we obtainùêª= Œ•ÓÄÄ√çŒ®(ùëü) Œ¶(ùêª)ÓÄÅ. The online bùêº. (3) Lines 7‚àí9: Search the Top-ùêænearest neighbours{bùêΩ, ¬∑ ¬∑ ¬∑ ,bùêΩ}of variableùêΩ‚àà ùêΩ. The Top-ùêænearest |ùêº‚àà ùêº, ùêΩ‚àà ùêΩ }is used and{bùëè, ùë£, ùë§, ùëê}remains unchanged, but they can still be stored in registers to , ùëê} remains unchanged. , ùë¢,bùëè, ùë£, ùë§, ùëê}, new variable sets ùêº and ùêΩ , random Hash values ùêª. The experiments were run on an NVIDIA Tesla P100 GPU with CUDA version 10.0. The same software and hardware conditions can better reÓÄûect the superiority of the proposed algorithm. The experiments are conducted on 3 public datasets: NetÓÄûix, MovieLensand Yahoo! Music. For MovieLens and Yahoo! Music, data cleaning is conducted, and 0 values are changed from 0 to 0.5. This will make cuALS work properly, which is one of the shortcomings of cuALS. The speciÓÄõc situations of the datasets are shown in Table 2. The ratings in the Yahoo! Music dataset are relatively large, which aÓÄùects the training process. In the actual training process, we divided all the ratings in the Yahoo! Music dataset by 20, and then we multiply by 20 when verifying the results. In this way, the ratings of the three datasets are in the same interval, which facilitates the parameter selection. The accuracy is measured by the ùëÖùëÄùëÜùê∏ as: where Œì denotes the test sets. The number of threads in athread warpunder the CUDA system is 32. Therefore, we set the number of threads in the thread block to a multiple of 32. This is done to maximize the utilization of the warp. Then, in order to align access, we set the parameters {ùêπ, ùêæ } as multiples of 32. 5.2 CUSGD++ CUSGD++ is used to compare cuALS [54] and cuSGD [59] on the three datasets. The parameters of cuALS and cuSGD were set as described in their papers and optimized according to the hardware environment, and CUSGD++ uses the dynamic learning rate in [64] as where the parameters{ùõº, ùõΩ, ùë°,ùõæ}represent the initial learning rate, adjusting parameter of the learning rate, the number of current iterations, and the learning rate atùë°iterations, respectively. The learning rate and other parameters in CUSGD++ are listed in Table 3. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data The GPU experiments are conducted on three datasets. In order to ensure running fairness, we ensure that the GPU executes these algorithms independently, and there is no other work. Fig. 6 shows the relationship between the ùëÖùëÄùëÜùê∏and training time. In Table 4, the times it takes to achieve an acceptable MovieLens and Yahoo! Music, respectively) are presented. cuALS has an extremely fast descent speed, but the time of each iteration is very long because the matrix inversion calculation is performed twice for each iteration. Furthermore, because the number of same, the thread load imbalance further increases the time overhead. cuSGD has a slower descent speed but less time overhead per iteration due to using data parallelism without load balancing issues. cuSGD has an obvious ÓÄûaw in that it does not take full advantage of the hardware resources of the GPU. cuSGD stores data in global memory, which makes it take too much time to read and write data. Our proposed CUSGD++ is signiÓÄõcantly faster than the state-of-the-art algorithms on the GPU. CUSGD++ and cuSGD have the same number of iterations to obtain an acceptable the same gradient descent algorithm, the proposed CUSGD++ and cuSGD algorithms are basically the same in terms of descent speed. CUSGD++ makes full use of the GPU hardware. Therefore, the time overhead of each iteration is only approximately 1 our further work is to solve this problem. Simultaneously, we simply sort the index of the row or column for according to the number of This approach can reduce the time overhead on a single iteration and achieve speedups of NetÓÄûix, MovieLens and Yahoo! Music datasets, respectively. Fig. 6. RMSE vs time: the experimental results demonstrate that CUSGD++ converges faster than other approaches. Before introducing the experiment, we will introduce the selection of the relevant parameters. CULSH-MF still uses the dynamic learning rate in Equation (7). The initial learning rate and regularization parameters are shown in Table 5, andùõΩfor all three datasets is 0 into the following 5 parts: 1) The overall performance comparison, 2) The performance comparison for the various methods of Top-ùêænearest neighbourhood query, 3) The performance comparison of neighbourhood nonlinear MF with naive MF methods, 4) The performance comparison on a GPU and multiple GPUs, and 5) The robustness of CULSH-MF. We ÓÄõrst compare the serial algorithms, i.e., LSH-MF and GSM-based Top-ùêænearest neighbourhood MF [29]. To ensure the fairness of the comparison, the parameters used are the same[29]. The serial algorithms are conducted on an Intel Xeon E5-2620 CPU, and the CUDA parallelization algorithms are conducted on an NVIDIA Tesla P100 GPU. Parameters{ùêπ, ùêæ }are set as{32,32}, respectively. Table 6 presents the time overhead of the three algorithms on the MovieLens dataset (baseline RMSE 0.80). The experimental results show that the LSH-MF can achieve a 44.3ùëãspeedup compared to the GSM-based Top-ùêænearest neighbourhood MF. CULSH-MF can achieve a 196.22ùëãspeedup compared to the LSH-MF serial algorithm. These results demonstrate that the proposed algorithms are eÓÄúcient. The comparison baselines of the GSM and simLSH are set under the same experimental conditions. To make the experiment more rigorous, a randomized control group was added, and it randomly selectsùêævariables for each variable rather than the Top-ùêæ nearest neighbours query. Furthermore, we compared two other LSH algorithms, random projection (RP_cos) based on cosine distance and minHash based on Jaccard similarity. On sparse data, compared to the Euclidean distance, the LSH algorithms based on the cosine distance have less accuracy loss. In addition, minHash can approximately calculate the Jaccard similarity between sets or vectors. The above two LSH functions are simple and have low computational complexity, Furthermore, the more complex LSH functions are not suitable for high-dimensional sparse data. The baselineùëÖùëÄùëÜùê∏s are{0.92,0.80,22.0}for NetÓÄûix, MovieLens and Yahoo! Music, respectively. For the MovieLens and NetÓÄûix datasets,Œ®(ùëü) = ùëüis set to expand the gap between interaction values, and the Yahoo! Music dataset has more dense interaction values. Thus, Œ®(ùëü) = ùëü. We use a byte as a hash value (ùê∫ = 8) and set ùúÜas the commonly used 100. Fig. 7 shows that the random selection method performs worse than the GSM-based method, simLSH and other LSH algorithms on the three datasets. When the parameters{ùëù, ùëû}are set as{ùëù =3, ùëû =100}, simLSH is almost the same as that of the GSM. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data When the parameters and minHash are far from that of simLSH. The reason is that the datasets are very sparse, and the descent speed brought by minHash is not very impressive. Table 7 shows the optimal can achieve a better only in descent speed but also in accuracy. Table 7 (middle) shows the time overhead of GSM, simLSH and other LSH algorithms on the three datasets, and simLSH takes much less time than the GSM. The calculation time required for RP_cos is slightly larger than that of simLSH, and minHash requires considerable calculation time due to the high dimensionality of the datasets. Table 7 (bottom) shows the spatial overhead of GSM, simLSH and other LSH algorithms on the three datasets, and simLSH takes much less space than the GSM. Furthermore, simLSH can surpass the GSM since it can adjust the parameters to achieve a balance between accuracy and time and it can set appropriate parameters according to actual needs. Fig. 8 shows the inÓÄûuence of various values of ùëùwill reduce the probability of two dissimilar variables projecting to the same hash value to higher accuracy. Fig. 7. ùëÖùëÄùëÜùê∏ vs time: The comparison between GSM, simLSH (various ùëù and ùëû values) and other LSH algorithms. We should select the best parameters and ensure which parameters play a greater role. In order to ensure that the threads are fully utilized, the parameters on CULSH-MF. As the Fig. 9 shows, under the same accuracy than CUSGD++ without the neighbourhood model in terms of the with CUSGD++ to demonstrate to what degree the neighbourhood model can improve the accuracy. Fig. 10 shows that CULSH-MF with the parameters )of two similar variables projected to the same hash value will decrease. Choosing a suitableùëùwill achieve Fig. 9. ùëÖùëÄùëÜùê∏ vs influence of various value of {ùêπ, ùêæ }. Compared with ùêπ , increasing ùêæ can reduce RMSE more. Table 7. The optimalùëÖùëÄùëÜùê∏of various Top-ùêæmethods (Up), the time overhead of various Top-ùêæmethods (Seconds) (Middle) and the The neighbourhood model with a lowùêæcan greatly improve the descent speed, and it can reach the targetùëÖùëÄùëÜùê∏with only a few iterations. CUSGD++ has a shorter training time per iteration, but it requires more training periods. Thus, CULSH-MF can outperform CUSGD++ owing to the overall training time with the optimalùëÖùëÄùëÜùê∏. Another noteworthy Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data results is that CULSH-MF runs faster than CUSGD++ as the value of can achieve {2.67ùëã, 2.97ùëã, 1.36ùëã } speedups compared to CUSGD++ when ùêπ = {32, 64, 128}, respectively. Finally, we present the experimental results of the robustness of CULSH-MF and CUSGD++, the online learning and multiple GPU solutions of CULSH-MF. First, data inevitably have noise, and a robust model should suppress noise interference. The experiment is conducted on all datasets with noise rates of experimental results in Table 8 show that CULSH-MF has more robustness than CUSGD++, which means that the neighbourhood nonlinear model performs more robustly than the naive model. Second, we divide the training datasets of NetÓÄûix, MovieLens and Yahoo! Music into original set of the dataset are shown in Table 9. In the online experiments, the MovieLens, and Yahoo! Music datasets only increased by online CULSH-MF avoids the retraining process. Third, multiple GPUs can accommodate a larger data, and CULSH-MF is extended to MCULSH-MF. Due to the communication overhead between each GPU, MCULSH-MF cannot reach the linear speeds, and properly distributing communications can shorten the computation time. CULSH-MF can obtain {1.6ùëã, 2.4ùëã, 3.2ùëã } speedups on {2, 3, 4} GPUs, respectively, compared to CULSH-MF on a GPU. Our model also applies to recommendations for implicit feedback and has a very obvious time advantage. NCF works well but takes too much time, and CULSH-MF can achieve similar results with a lower time overhead. We change the loss function of CULSH-MF to the cross entropy loss function, and the update formula will also follow the corresponding change. This derivation is too simple and will not be repeated here. Because the time overheads to Table 10. Time comparison (Seconds) to obtain basic HR of various nonlinear MF methods train the deep learning models on large-scale datasets are unacceptable, three deep learning models, e.g., Generalized Matrix Factorization (GMF), the Multilayer Perceptron (MLP) and Neural Matrix Factorization (NeuMF), of [18] are just tested on two small datasets, e.g., MovieLens1m and Pinterest. 1) GMF is a deep learning model based on matrix factorization that extends classic matrix factorization. It ÓÄõrst performs one-hot encoding on the indexes in the sets{ùêº, ùêΩ } of the input layer, and the obtained embedding vectors are used as the latent factor vectors. Then, through the neural matrix decomposition layer, it calculated the matrix Hadamard product of factor vectorùêºand factor vectorùêΩ. Finally, a weight vector and the obtained vector are projected to the output layer by the dot product. 2) The MLP is used to learn the interaction between latent factor vectorùêºand latent factor vectorùêΩ, which can give the model greater ÓÄûexibility and nonlinearity. With the same conditions as GMF, the MLP uses the embedded vector of the one-hot encoding of indicesùêºandùêΩas the latent factor vector ofùêºandùêΩ. The diÓÄùerence is that MLP concatenates latent factor vectorùêº with latent factor vectorùêΩ. The model uses the standard MLP; and each layer contains a weight matrix, a deviation vector, and an activation function. 3) GMF uses linear kernels to model the interaction of potential factors while MLP uses nonlinear kernels to learn the interaction functions from data. To consider the above two factors at the same time, NeuMF integrates GMF and the MLP, embeds GMF and the MLP separately, and combines these two models by connecting their last hidden layers in series. This allows the fusion model to have greater ÓÄûexibility. The Hit Ratio (HR) is used to measure the accuracy of the nonlinear models. We use the same datasets and the same metrics. For the same baseline HR, we compare the time overheads of CULSH-MF and the three nonlinear models, i.e., GMF, the MLP and NeuMF. The experimental results are shown in Table 10. Table 10 shows that the time overhead of the CULSH-MF is only 0.01% that of the three nonlinear models, i.e., GMF, the MLP and NeuMF. Furthermore, the parameters of the CULSH-MF are much smaller than those of the three nonlinear models, i.e., GMF, the MLP and NeuMF. The research was partially funded by the National Key R&D Program of China (Grant No. 2020YFB2104000) and the Programs of National Natural Science Foundation of China (Grant Nos. 61860206011, 62172157). This work has been partly funded by the Swiss National Science Foundation NRP75 project (Grant No. 407540_167266) and the China Scholarship Council (CSC) (Grant No. CSC201906130109).