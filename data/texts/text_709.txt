<title>Local Citation Recommendation with Hierarchical-Attention Text Encoder and SciBERT-based Reranking</title> <title>1 Introduction</title> <title>arXiv:2112.01206v1  [cs.IR]  2 Dec 2021</title> for supporting information on a claim, or most commonly when we want to learn about some new topic, we perform literature discovery. Literature discovery remains challenging in today’s age of information overﬂow, largely arising from the exponential growth in both the publication record [14] and the underlying vocabulary [12]. Assistance to literature discovery can be provided with automatic citation recommendation, whereby a query text without citation serves as the input to a recommendation system and a paper worth citing as its output [8]. Citation recommendation can be dealt with either as a global retrieval problem [32,26,2] or as a local one [11,13,15]. In global citation recommendation, the query text includes the title and the abstract of a source paper [2]. In contrast, Fig. 1: Overview of our two-stage local citation recommendation pipeline. in local citation recommendation, the query consist of two sources of contexts [11,24]: 1) the text surrounding the citation placeholder with the information of the cited paper removed (the local context); and 2) the title and abstract of the citing paper as the global context. The aim of local citation recommendation is to ﬁnd the missing paper cited at the placeholder of the local context. In this paper we focus on local citation recommendation. It is important for a local citation recommendation system to maintain a balance between accuracy (e.g., recall of the target paper among the top K recommended papers) and speed in order to operate eﬃciently on a large database containing millions of scientiﬁc papers. The speed-accuracy tradeoﬀ can be ﬂexibly dealt with using a two-step prefetching-reranking strategy: 1) A fast prefetching model ﬁrst retrieves a set of candidate papers from the database; 2) a more sophisticated model then performs a ﬁne-grained analysis of scoring each candidate paper and reordering them to result in a ranked list of recommendations. In recent studies [24,3,5,21], (TF-IDF) [29] or BM25 [30] were generally used as the prefetching algorithm, which were neither ﬁne-tuned nor taken into consideration when evaluating the recommendation performance. In this paper, we propose a novel two-stage local citation recommendation system (Figure 1). In the prefetching stage, we make use of an embedding-based paper retrieval system, in which a siamese text encoder ﬁrst pre-computes a vector-based embedding for each paper in the database. The query text is then mapped into the same embedding space to retrieve the K nearest neighbors of the query vector. To enable our paper retrieval system to handle queries and papers of various lengths in a memory-eﬃcient way, we design a two-layer Hierarchical Attention-based text encoder (HAtten) that ﬁrst computes paragraph embeddings and then computes from the paragraph embeddings the query and document embeddings using a self-attention mechanism [33]. In the reranking step, we ﬁne-tune the SciBERT [1] to rerank the candidates retreived by the HAtten prefetching model. In addition, to cope with the scarceness of large-scale training datasets in many domains, we construct a novel dataset that we distilled from 1.7 million arXiv papers. The dataset consist of 3.2 million local citation sentences along with the title and the abstract of both the citing papers and the cited papers. Extensive experiments on the arXiv dataset as well as on previous datasets including ACL-200 [24], RefSeer [24,5], and FullTextPeerRead [15] show that our local citation recommendation system performs signiﬁcantly better on both prefetching and reranking than the baseline and requires fewer prefetched candidates in the reranking step thanks to higher recall of our prefetching system, which indicates that our system strikes a better speed-accuracy balance. In total, our main contributions are summarized as follows: 1) We propose a competitive retrieval system consisting of a hierarchical-attention text encoder and a ﬁne-tuned SciBERT reranker. 2) In evaluations of the whole pipeline, we experimentally demonstrate a well-balanced tradeoﬀ between speed and accuracy. 3) We release a large-scale scientiﬁc paper dataset for training and evaluation of production-level local citation recommendation systems. <title>2 Related Work</title> Local citation recommendation was previously addressed in He et al. [11] in which a non-parametric probabilistic model was proposed to model the relevance between the query and each candidate citation. In recent years, embedding-based approaches [18,9] have been proposed to more ﬂexibly capture the resemblance between the query and the target according to the cosine distance or the Euclidean distance between their embeddings. Jeong et al. [15] proposed a BERTGCN model in which they used Graph Convolutional Networks [17] (GCN) and BERT [4] to compute for each paper embeddings of the citation graph and the query context, which they fed into a feed-forward network to estimate relevance. The BERT-GCN model was evaluated on small datasets of only thousands of papers, partly due to the high cost of computating the GCN, which limited its scalability for recommending citations from large paper databases. Although recent studies [24,3,5,21] adopted the prefetching-reranking strategy to improve the scalability, the prefetch part (BM25 or TF-IDF) only served for creating datasets for training and evaluating the reranking model, since the target cited paper was added manually if it was not retrieved by the prefetch model to make the recall of the target among the candidate papers always equal to 1. Therefore, these recommendation systems were evaluated in an artiﬁcial situation with an ideal prefetching model that in reality does not exist. Supervised methods for citation recommendation rely on the availability of numerous labeled data for training. While it is simple to construct a dataset for global citation recommendation [2], it is more challenging to assemble a dataset for local citation recommendation because the latter requires parsing the full text of papers to extract the local contexts and ﬁnding citations that are also available in the dataset, which eliminates a large bulk of data. Therefore, existing datasets on local citation recommendation are usually limited in size. For example, the ACL-200 [24] and the FullTextPeerRead [15] contain only thousands of papers. One of the largest datasets is RefSeer used in Medi´c and Snajder [24], which contains 0.6 million papers in total, but this dataset is not up-to-date as it only contains papers prior to 2015. This motivates the creation of a novel dataset. <title>3 Proposed Dataset</title> Table 1: Statistics of the datasets for local citation recommendation. with existing datasets used in this paper. As the most recent contexts available in the arXiv dataset is from April 2020, we use the contexts from 1991 to 2019 as the training set, the contexts from January 2020 to February 2020 as the validating set, and the contexts from March 2020 to April 2020 as the test set. The sizes of the arXiv training, validating, and testing sets are comparable to RefSeer, one of the largest existing datasets, whereas our arXiv dataset contains a much larger number of papers, and there are more recently published papers available in the arXiv dataset. These features make the arXiv dataset a more challenging and up-to-date testbench. <title>4 Approach</title> Our two-stage telescope citation recommendation system is similar to that of Bhagavatula et al. [2], composed of a fast prefetching model and a slower reranking model. The prefetching model scores and ranks all papers in the database to fetch a rough initial subset of candidates. We designed a representation-focused ranking model [10] that computes a query embedding for each input query and ranks each candidate document according to the cosine similarity between the query embedding and the pre-computed document embedding. Fig. 2: The Hierarchical-Attention text encoder (HAtten) used in the prefetching step is composed of a paragraph encoder (a) and a document encoder (b). The core of the prefetching model is a light-weight text encoder that eﬃciently computes the embeddings of queries and candidate documents. As shown in Figure 2, the encoder processes each document or query in a two-level hierarchy, consisting of two components: a paragraph encoder and a document encoder. Paragraph Encoder For each paragraph p in the document, the paragraph encoder (Figure 2a) takes as input the token sequence p = [w , . . . , w ] composed of n tokens (words) to output the paragraph embedding e as a single vector. In order to incorporate positional information of the tokens, the paragraph encoder makes use of positional encoding. Contextual information is encoded with a single transformer encoder layer following the conﬁguration in Vaswani et al. [33], Figure 2a. To obtain a single ﬁxed-size embedding e from a variably sized paragraph, the paragraph encoder processes the output of the transformer encoder layer with a multi-head pooling layer [19] with trainable weights. Let ∈ R be the output of the transformer encoder layer for token w in a paragraph p . For each head j ∈ {1, . . . , n } in the multi-head pooling layer, we ﬁrst compute a value vector v ∈ R as well as an attention score ˆa ∈ R associated with that value vector: Prefetched document candidates The prefetched document candidates are found by identifying the K nearest document embeddings to the query embedding in terms of cosine similarity. The ranking is performed using a brute-force nearest neighbor search among all document embeddings as shown in Figure 1. Fig. 3: Structure of our SciBERT Reranker. The reranking model performs ﬁne-grained comparison between a query q (consisting of a local and a global context) and each prefetched document candidate (its title and the abstract). The relevance scores of the candidates constitutes the ﬁnal output of our model. We design a reranker based on SciBERT [1], which is a BERT model [4] trained on a large-scale corpus of scientiﬁc articles. The input of the SciBERT reranker has the following format: “[CLS] Sentence A [SEP] Sentence B”, where sentence A is the concatenation of the global context (title and abstract of the citing paper) and the local context of the query, and sentence B is the concatenation of the title and the abstract of the candidate paper to be scored, Figure 3. The SciBERT-encoded vector for the “[CLS]” token is then fed into a feed-forward network that outputs the relevance score s ∈ [0, 1] provided via a sigmoid function. We use a triplet loss both to train our HAtten text encoder for prefetching and to ﬁnetune the SciBERT reranker. The triplet loss is based on the similarity s(q, d) between the query q and a document d. For the prefetching step, s(q, d) is given by the cosine similarity between the query embedding v and the document embedding v , both computed with the HAtten encoder. For the reranking step, s(q, d) is given by the relevance score computed by the SciBERT reranker. In order to maximize the relevance score between the query q and the cited document d (the positive pair (q, d )) and to minimize the score between q and any non-cited document d (a negative pair (q, d )), we minimize the triplet loss: where the margin m > 0 sets the span over which the loss is sensitive to the similarity of negative pairs. For fast convergence during training, it is important to select eﬀective triplets for which L in Equation (3) is non-zero [31], which is particularly relevant for the prefetching model, since for each query there is only a single positive document but millions of negative documents (e.g., on the arXiv dataset). Therefore, we employ negative and positive mining strategies to train our HAtten encoder, described as follows. Negative mining Given a query q, we use HAtten’s current checkpoint to prefetch the top K candidates excluding the cited paper. The HAtten embedding of these prefetched non-cited candidates have high cosine similarity to the HAtten embeding of the query. To increase the similarity between the query and the cited paper while suppressing the similarity between the query and these non-cited candidates, we use the cited paper as the positive document and select the negative document from these K overly similar candidates. Positive mining Among the prefetched non-cited candidates, the documents with high textual similarity (e.g. measured by the Jaccard index [2]) to the query were considered relevant to the query, even if they were not cited. These textually relevant candidate documents should have a higher cosine similarity to the query than randomly selected documents. Therefore, in parallel with the negative mining strategy, we also select positive documents from the set of textually relevant candidates and select negative documents by random sampling from the entire dataset. The checkpoint of the HAtten model is updated every N training iterations, at which point the prefetched non-cited and the textually relevant candidates for negative and positive mining are updated as well. In contrast, when ﬁne-tuning SciBERT for reranking, the reranker only needs to rerank the top K prefetched candidates. This allows for a simpler triplet mining strategy, which is to select the cited paper as the positive document and randomly selecting a prefetched non-cited papers as the negative document. <title>5 Experiments</title> Implementation Details In the prefetching step, we used as word embeddings of the HAtten text encoder the pre-trained 200-dimensional GloVe embeddings [28], which were kept ﬁxed during training. There are 64 queries in a mini-batch, each of which was accompanied by 1 cited paper, 4 non-cited papers randomly sampled from the top K = 100 prefetched candidates, and 1 randomly sampled paper from the whole database, which allow us to do negative and positive mining with the mini-batch as described in Section 4.3. The HAtten’s checkpoint was updated every N = 5000 training iterations. In the reranking step, we initialized the SciBERT reranker with the pretrained model provided in Beltagy et al. [1]. The feed-forward network in Figure 3 consisting of a single linear layer was randomly initialized. Within a mini-batch there was 1 query, 1 cited paper (positive sample), and 62 documents (negative samples) randomly sampled from the top K = 2000 prefetched non-cited documents. In the triplet loss function the margin m was set to 0.1. We used the Adam optimizer [16] with β = 0.9 and β = 0.999. In the prefetching step, the learning rate was set to α = 1e and the weight decay to 1e , while in the reranking step these were set to 1e and to 1e for ﬁne-tuning SciBERT, respectively. The models were trained on eight NVIDIA GeForce RTX 2080 Ti 11GB GPUs and tested on two Quadro RTX 8000 GPUs. Evaluation Metrics We evaluated the recommendation performance using the Mean Reciprocal Rank (MRR) [34] and the Recall@K, consistent with previous work [24,7,15]. The MRR measures the reciprocal rank of the actually cited paper among the recommended candidates, averaged over multiple queries. The Recall@K evaluates the percentage of the cited paper appearing in the top K recommendations. <title>6 Results and Discussion</title> In this section, we ﬁrst present the evaluation results of our prefetching and reranking models separately and compare them with baselines. Then, we evaluate the performance of the entire prefetching-reranking pipeline, and analyze the inﬂuence of the number of prefetched candidates to be reranked on the overall recommendation performance. Table 2: Comparison of prefetching performance. “*” indicates statistical signiﬁcant rejection of the null hypothesis for p < 0.05 in a two-sided t-test [20]. In the prefetching step, we compare our HAtten with the following baselines: BM25, Sent2vec [27], and NNSelect [2]. BM25 was used as the prefetching method in previous works [24,5,21]. Sent2vec is an unsupervised text encoder which computes a text embedding by averaging the embeddings of all words in the text. Here we use the 600-dim word embedding pretrained on the Wikipedia corpus. Although NNSelect [2] computes text embeddings also by averaging, it is a supervised model in which the magnitude of the embedding of each word in the vocabulary is trained on the citation recommendation task. Since NNSelect was originally used for global citation recommendation on a diﬀerent dataset, we retrained NNSelect on each of our datasets using the same training conﬁguration as our HAtten model. We evaluated the prefetching performance using average prefetching time, MRR (computed on top 2000 candidates) and Recall@K (R@K for short), where K = 10, 100, 200, 500, 1000, 2000. Our HAtten model signiﬁcantly outperformed all baselines (including the strong baseline BM25, Table 2) on the ACL-200, RefSeer and the arXiv datasets, evaluated by MRR and Recall@K. We also observed the following phenomena. First, for larger K, such as K = 200, 500, 1000, 2000, the improvement of Recall@K with respect to the baselines is more signiﬁcant on all four datasets, where the increase is usually more than 0.1, which means that the theoretical upper bound of the ﬁnal reranking recall will be higher using our HAtten prefetching system. Second, the improvements of recall@K on large datasets such as RefSeer and arXiv are more prominent than on small datasets such as ACL-200 and FullTextPeerRead, which ﬁts well with the stronger need of a prefetching-reranking pipeline on large datasets due to the speed-accuracy tradeoﬀ. The advantage of our HAtten model is also reﬂected in the average prefetching time. As shown in Table 2, the HAtten model shows faster prefetching compared to BM25 on large datasets such as RefSeer and arXiv. This is because for HAtten, both text encoding and embedding-based nearest neighbor search can be accelerated by GPU computing, while BM25 beneﬁts little from GPU acceleration because it is not vector-based. Although other embedding-based baselines such as Sent2vec and NNSelect also exhibit fast prefetching, our HAtten prefetcher has advantages in terms of both speed and accuracy. In the reranking step, we compare our ﬁne-tuned SciBERT reranker with the following baselines: 1) a Neural Citation Network (NCN) with an encoder-decoder architecture [5,6]; 2) DualEnh [24], a neural network recommendation model that scores each candidate paper using both semantic information from the query and bibliographic information (such as author network); 3) DualCon [24], a variant of DualEnh that computes a semantic score using only the local context of the query; and 4) BERT-GCN [15] that scores each candidate paper using context embeddings and graph embeddings computed by BERT and GCN respectively. Furthermore, to analyze the inﬂuence on ranking performance of diverse pretraining corpuses for BERT, we compared our SciBERT reranker with a BERT reranker that was pretrained on a non-science speciﬁc corpus [4] and then ﬁnetuned on the reranking task. Both the SciBERT and the BERT rerankers were trained using the candidates prefetched by our HAtten Model. Table 3: Comparison of reranking performance on four datasets. For a fair performance comparison of our reranker with those of other works, we adopted the prefetching strategies from each of these works. On ACL-200 and RefSeer, we tested our SciBERT reranker on the test sets provided in Medi´c and Snajder [24]. For each query in the test set, we prefetched n (n = 2000 for ACL-200 and n = 2048 for RefSeer) candidates using BM25, and manually added the cited paper as candidate if it was not found by BM25. In other words, we constructed our test set using an “oracle-BM25” with Recall@n = 1. On the FullTextPeerRead dataset, we used our SciBERT reranker to rank all papers in the database without prefetching, in line with the setting in BERT-GCN [15]. On our newly proposed arXiv dataset, we fetched the top 2000 candidates for each query in the test set using the ‘oracle-BM25’ as introduced above. We evaluated the reranking performance using MRR and Recall of the top 10 reranked candidates. As shown in Table 3, the SciBERT reranker signiﬁcantly outperformed previous state-of-the-art models on the ACL-200, the RefSeer, and the FullTextPeerRead datasets. We ascribe this improvement to BERT’s ability of capturing the semantic relevance between the query text and the candidate text, which is inherited from the “next sentence prediction” pretraining task that aims to predict if two sentences are consecutive. The SciBERT reranker also performed signiﬁcantly better than its BERT counterpart, suggesting that large language models pretrained on scientiﬁc papers’ corpus are advantageous for citation reranking. 6.3 Performance of entire Recommendation Pipeline Table 4: The performance of the entire prefetching-reranking pipeline, measured in terms of Recall@10 of the ﬁnal reranked document list. We varied the number of prefetched candidates for reranking. The evaluation in Section 6.2 only reﬂects the reranking performance because the prefetched candidates are obtained by an oracle-BM25 that guarantees inclusion of the cited paper among the prefetched candidates, even though such an oracle prefetching model does not exist in reality. Evaluating recommendation systems in this context risks overestimating the performance of the reranking part and underestimating the importance of the prefetching step. To better understand the recommendation performance in real-world scenarios, we compared two pipelines: 1) a BM25-based pipeline with BM25 prefetching + SciBERT reranking, where the SciBERT was ﬁne-tuned on BM25-prefetched candidates, denoted as SciBERT ; 2) a HAtten-based pipeline with HAtten prefetching + SciBERT reranking. We evaluated recommendation performance by Recall@10 of the ﬁnal reranked document list and monitored the dependence of Recall@10 on the number of prefetched candidates for reranking. Fig. 4: The reranking time of the SciBERT reranker linearly increases with the number of reranked candidates K , tested on arXiv. In comparison, the prefetching time is invariant of K , as the prefetcher always scores and ranks all documents in the database to fetch the candidates to be reranked. As shown in Table 4, the HAtten-based pipeline achieves competitive performance, even when compared with the oracle prefetching model in Section 6.2. In particular, on the FullTextPeerRead dataset, using our HAtten-based pipeline, we only need to rerank 100 prefetched candidates to outperform the BERT-GCN model (Table 3) that reranked all 4.8k papers in the database. Compared to the BM25-based pipeline, our HAtten-based pipeline achieves signiﬁcantly higher Recall@10 for any given number of prefetched candidates. Our reranker needs to rerank only 200 to 500 candidates to match the recall score of the BM25-based pipeline when reranking 2000 candidates. For large datasets like RefSeer and arXiv, such improvements are even more pronounced. Our pipeline achieves a much higher throughput. For example, on the arXiv dataset, in order to achieve an overall recall@10=0.39, the BM25-based pipeline takes 0.7 s (Table 2) to prefetch 2000 candidates and it takes another 13.4 s (Figure 4) to rerank them, which in total amounts to 14.1 s. In contrast, the HAtten-based pipeline only takes 8 ms to prefetch 200 candidates and 1.4 s to rerank them, which amounts to 1.4 s. This results in a 90% reduction of overall recommendation time achieved by our pipeline. These ﬁndings provide clear evidence that a better-performing prefetching model is critical to a large-scale citation recommendation pipeline, as it allows the reranking model to rerank fewer candidates while maintaining recommendation performance, resulting in a better speed-accuracy tradeoﬀ. <title>7 Conclusion</title> The speed-accuracy tradeoﬀ is crucial for evaluating recommendation systems in real-world settings. While reranking models have attracted increasing attention for their ability to improve recall and MRR scores, in this paper we show that it is equally important to design an eﬃcient and accurate prefetching system. In that respect, our proposed HAtten-SciBERT recommendation pipeline signiﬁcantly outperforms previous pipelines. By releasing our large-scale arXiv-based dataset, we provide a new testbed for research on local citation recommendation in realworld scenarios. <title>References</title>