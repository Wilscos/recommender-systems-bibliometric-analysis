We consider the problem of query recommendation in closed loop interactive computing environments, like exploratory data analysis, and online information discovery/gathering. In such applications, a user (data analyst) starts oﬀ a session by issuing an initial query related to a topic of investigation/exploration to the data system and, then, exploring the topic in-depth by executing further queries. Query recommendation algorithms are employed to recommend these ‘future’ further queries based on previously issued queries to improve user experience (Baeza-Yates et al., 2004). Furthermore, in this context, query recommendation algorithms can be envisaged as a pillar component in resource-eﬃcient decision making methods in data management systems. In addition to providing recommendations for data exploration tasks, eﬀective query recommendation algorithms are expected to greatly improve the way current data systems work (including query processing, pre-fetching data, pre-analysing data, managing cached data). We consider the query recommendation problem in closed loop interactive learning settings like online information gathering and exploratory analytics. The problem can be naturally modelled using the Multi-Armed Bandits (MAB) framework with countably many arms. The standard MAB algorithms for countably many arms begin with selecting a random set of candidate arms and then applying standard MAB algorithms, e.g., UCB, on this candidate set downstream. We show that such a selection strategy often results in higher cumulative regret and to this end, we propose a selection strategy based on the maximum utility of the arms. We show that in tasks like online information gathering, where sequential query recommendations are employed, the sequences of queries are correlated and the number of potentially optimal queries can be reduced to a manageable size by selecting queries with maximum utility with respect to the currently executing query. Our experimental results using a recent real online literature discovery service log ﬁle demonstrate that the proposed arm selection strategy improves the cumulative regret substantially with respect to the state-of-the-art baseline algorithms. Our data model and source code are available at https://anonymous.4open.science/r/0e5ad6b7-ac02-4577-9212-c9d505d3dbdb/. Keywords: query recommendation, multi-armed bandits, arm selection, maximum utility be prepared for any immediate downstream tasks. For example, if the forecast next query requires loading (or transferring through the network) a considerably huge amount of data to the main memory or running a CPU intensive task, the system can prefetch the data, free up caches and memory/processes to accommodate the tasks to carry out in the immediate short-term future. These advantages of timely next-query forecasting will anticipate impact on the way data centers work and schedule tasks in terms of throughput and scalability in task scheduling. Hence, an eﬃcient query recommendation (forecasting) engine not only improves user engagement but also improves system performance and application-driven Quality of Experience. obtain in-depth knowledge about the data for follow-up tasks like, data trend explanation (Savva et al., 2020), report summarization (Marcel and Negre, 2011) and (ii) users gathering information by discovering scholarly articles to conduct literature review using online services (Krause and Guestrin, 2011). These tasks can be conceptualized as the system recommending queries and the user either accepting or rejecting these recommendations, thus forming a closed loop interactive environment. This learning environment can be viewed as a repeated game between an online algorithm and the user. At each trial t = 1, . . . , T , the algorithm chooses a query for recommendation from a very large query set A. Based on the utility of the recommendation, the user chooses either to execute the query or ignore it, thus resulting in a reward. Like in the standard recommendation settings, the rewards are in the form of ‘clicks’ for the correctly recommended queries. trade-oﬀ between exploration and exploitation over a set of items (queries in our context) with unknown reward distribution. We model query recommendations using the MAB framework. The standard MAB algorithms assume that the number of arms is ﬁxed and relatively smaller than the number of trials. In query recommendation, the total number of queries (arms) far exceeds the number of times users will use the recommendation platform. Hence, the number of queries for recommendation is inherently huge and assumed to be countably many. We do not consider the queries to be inﬁnite as there can be many implausible queries and query sequences. In addition, unlike in personalized recommendation settings, the queries can be about very general topics like COVID, MAB or SQL statements, and need not have a personalization component. Our goal is to recommend the next query to be executed based on past executed queries and the topic of investigation, i.e., currently executing query. have to choose from an extremely large number of arms, often much larger than the number of experimental trials or time horizon. The sheer volume of possible arms make it computationally impossible to try each of the arms even once. The standard way to deal with countably many arms is to either randomly select a candidate set containing a ﬁxed (k) but reduced number of arms (k  T  |A|) from the pool of arms, and run standard MAB algorithms on this reduced arm set (Wang et al., 2008; Kalvit and Zeevi, 2020; Zhu and Nowak, 2020; Bayati et al., 2020), or exploit the benign reward structure of the arms (Kleinberg et al., 2019; Magureanu et al., 2014). As we demonstrate in our experimental evaluation, the random selection strategy often results in the optimal or near-optimal arms Forecasting the correct next-query helps the system to proactively reserve resources and Typical use case scenarios include: (i) data scientists analyzing a large volume of data to The MAB framework is popular in personalized recommender systems to model the As opposed to the standard MAB settings, countably many armed bandit algorithms to be ignored in the selection process and, thus, hindering the performance of the bandit algorithm. The algorithms that exploit the reward structure of the arm contexts also fall short of expectations. As demonstrated in the experiments, the zooming based algorithm (Kleinberg et al., 2019) performs sub-optimally and the OSLB (Magureanu et al., 2014) is infeasible in our case as it requires one to solve an LP in each round. Hence, the key component in the algorithm design for countably many armed bandit problems is the arm selection strategy. To this end, we propose a strategy to select the candidate set of the most promising arms based on maximizing the utility of an arm with respect to the currently playing arm. of arm for selection’ with respect to the currently playing arm based on the notion of utility. Our assumption is similar to the assumption that the rewards are Lipschitz function of the arm contexts (Magureanu et al., 2014; Kleinberg et al., 2019). Furthermore, we model the likelihood, i.e., probability of a query to be preferred by the current arm as a function of pairwise similarity between them. Speciﬁcally, the currently executing query provides us with the context information of the topic the user is interested in, and we make use of the shared context information between diﬀerent queries, represented as real valued vectors, to choose the best next query to run. Then, we propose an eﬀective selection strategy based on utility maximization. For theoretically plausible utility functions, our selection strategy reduces to monotonic submodular maximization problem with cardinality constraint. For a very large set of arms, the submodular maximization problem with cardinality constraint can be solved in nearly linear time using a distributed greedy algorithm (Mirzasoleiman et al., 2016). Our candidate set selection strategy allows us to dynamically set the number of arms in the candidate set, following the design principles (Berry et al., 1997): (i) when the number of trials is large, the algorithm is allowed to sacriﬁce short term gain by eschewing arms with larger reward, if necessary, while exploring for arms with even larger reward that will expect to yield a long-term beneﬁt, (ii) when the number of trials is small, the algorithm is allowed to eschew new arm exploration in favor of selecting an arm that has a large immediate reward. We also elaborate on the case of setting the value of k in ‘anytime’ bandit settings. rithms for countably many armed bandit problems and query recommendations in Section 2, we describe our framework and elaborate on the candidate set selection algorithm in Section 3. In Section 4, we report the results of our experimental study on real world sequential query recommendation for online literature discovery settings and compare against strong baselines like zooming algorithm (Kleinberg et al., 2019). Section 5 concludes the paper. We present studies related to our problem in diﬀerent areas. Multi-Armed Bandits: We limit our discussion to Countably Many Armed Bandit (CMAB) algorithms. For standard MAB algorithms, we recommend the readers to refer to Given the currently executing query (currently playing arm), we deﬁne the ‘goodness The remainder of this paper is organized as follows: after a brief overview of the algo- Cesa-Bianchi and Lugosi (2006); Lattimore and Szepesv´ari (2020). As opposed to standard MAB settings, in CMAB settings, bandit algorithms have to choose from an extremely large number of arms, often much larger than the possible number of experimental trials (T ). The sheer volume of possible arms makes it impossible to try each of the arms even once. In stochastic bandit problems with countably many arms, the learner is restricted to ignore many arms without even trying them once, and dedicate the valuable exploration scheme only to a certain number of arms. That is, in addition to the exploration-exploitation trade-oﬀ, which is typical to sequential learning algorithms, we need also to deal with the arm discovery-exploitation trade-oﬀ within the exploration phase. That is, while exploring, the algorithm has to decide whether it should try a new arm or revisit an already played arm to get a better estimate of the expected reward. Recently, many algorithms have been proposed in the CMAB settings. These algorithms can be broadly classiﬁed into: (i) failure-based approaches and (ii) pre-selection based approaches. Such algorithms try diﬀerent arms until the number of trials is exhausted or reach a predeﬁned failure rate for the arms. Hence, the exploration phase lasts until the algorithms hit the end of time horizon. In Berry et al. (1997), the authors proposed k-failure, where an arm is played until it incurs k failures, and m − run, where 1-failure is used until m arms are played or m success is obtained, algorithms for Bernoulli arms. Asymptotically, failure-based algorithms yield lower cumulative regret; but for ﬁnite T < ∞, they tend to perform poorly. Additionally, failure-based algorithms require prior knowledge of T . Similarly, Herschkorn et al. (1996) proposed a non-recalling, bounded memory, failure-based algorithm for Bernoulli arms. Kalvit and Zeevi (2020) proposed an algorithm in a setting where the arms are partitioned into diﬀerent types, and the goal is to ﬁnd the arm from the superior type. Their algorithm can be considered as failure-based, as it terminates only when the superior type is identiﬁed with large conﬁdence. The failure-based approaches are better suited for pure-exploration settings. number can be either ﬁxed in advance (before the start of the experiment) or adapted as the trial progresses. Such approaches randomly choose k arms from the pool of arms and use standard MAB algorithms on this subset of k arms downstream. Indicatively, the pre-selection based algorithm in Wang et al. (2008) deals with selecting k randomly chosen arms for exploration and exploitation. The exact value of k in Wang et al. (2008) is deﬁned as a function of the current trial count. They also adopt the ‘goodness of arm’ assumption which states that: the probability of the mean reward of a newly explored arm diﬀering from the optimal arm is close to zero. Zhu and Nowak (2020) proposed an algorithm in the CMAB settings with multiple best arms. They also used k arms selected uniformly at random from the arms pool. In Bayati et al. (2020), authors proposed a subsampling (uniformly at random without replacement) based greedy algorithm in CMAB settings. The arm rewards of Lipschitz bandits Magureanu et al. (2014) are assumed to be smooth, but arms are sampled sequentially till every arm is tried at least a ﬁxed number of times. The zooming algorithm proposed by Kleinberg et al. (2019) reﬁnes the region for sampling the arms based on the arms similarity. In recent years, there is a surge of literature dealing with bandit algorithms in countably many, and, the closely related, inﬁnite arm settings. The reader is advised to refer to Kalvit and Zeevi (2020); Zhu and Nowak (2020); Bayati et al. In failure-based approaches, there is no hard limit on the number of arms to explore. In the pre-selection based approach, only a speciﬁc number of arms are explored. This (2020); Kleinberg et al. (2019); Lattimore and Szepesv´ari (2020) and references therein for a detailed coverage of diﬀerent algorithms. tured fashion and does not make use of the arms context eﬀectively. To the best of our knowledge, our proposed method is the ﬁrst non-random candidate arm selection strategy that makes use of the context information, which is evaluated in the context of sequential query recommendation systems. Query Recommendations: Query recommendation based on historical query logs is extensively studied in the Information Retrieval (IR) and Database communities (Baeza-Yates et al., 2004; He et al., 2009; Dimitriadou et al., 2014; He et al., 2009; Li et al., 2019; Dehghani et al., 2017; Rosset et al., 2020). Most of the earlier work on query recommendation made use of simple IR techniques like query similarity and query support (Baeza-Yates et al., 2004). Dimitriadou et al. (2014) proposed a query recommendation algorithm for interactive data exploration applications. The proposed algorithm works by building on-the-ﬂy decision tree classiﬁers from the user feedback obtained before the exploration starts. Other prominent approaches include building oﬄine probabilistic models using historical session data. In He et al. (2009), supervised N-gram based Markov model is trained on historical session data to predict the sequence of next queries. The Click Feedback-Aware Network algorithm proposed in Li et al. (2019), models the sequential queries using deep neural networks. Given a query, the proposed method predicts a ranked list of queries for recommendations. The model is trained on positive and negative query instances, constructed from the historical logs. Similar to Li et al. (2019), Dehghani et al. (2017) proposed a sequence-to-sequence Recurrent Neural Network model with query-aware attention mechanism. Jiang and Wang (2018) also used sequence-to-sequence recurrent neural networks with attention mechanisms to recommend next query. The main diﬀerence between the above two approaches is: Jiang and Wang (2018) used a ‘query reformulation inferencer’ to obtain homomorphic embedding of the queries. As in the case of CMAB, recent years evidenced a surge in the query recommendation literature attributed mainly to the success of sequence-to-sequence deep learning models. The reader is advised to refer to Wu et al. (2018); Li et al. (2019); Jiang and Wang (2018); Dehghani et al. (2017) and references therein for a detailed coverage of diﬀerent query recommendation algorithms. supervised learning setting. The user feedback is not dealt with in an online fashion. Instead, it is used to create supervised training examples. In our limited knowledge, our work is the ﬁrst attempt to formulate query recommendation as a pure online learning task using the MAB framework. Many information ﬁltering tasks, e.g., information discovery in the Web (Krause and Guestrin, 2011) or exploratory data analysis (Marcel and Negre, 2011), involve interactive data exploration through user issued queries, often chosen from the recommendation In all the above discussed work, candidate arm selection is not carried out in a struc- All the above work considers query recommendation from a pure supervised / weakly provided by a query recommendation engine. In such environments, conceptually, a (user) session starts with a user issuing a task or topic speciﬁc initial query to the data system. Then, the system responds with a ranked list of relevant results and a query recommendation to be potentially executed at the next step of the process. Depending on the usefulness (or utility) of the recommended query, a user may wish to either execute it or ignore it. For meaningful exploration, the recommended queries are anticipated to be related to the topics (tasks) corresponding to the currently executing query and, thus, expected to be semantically similar to the current one. Normally, the recommended queries are taken or derived from the historical query logs containing all queries executed in the past (Baeza-Yates et al., 2004). Hence, the potential queries for recommendation are considered as countably many. We envisage this problem as an optimal arm selection in stochastic multi-armed bandit settings with countably many arms (Kalvit and Zeevi, 2020) where queries correspond to arms. goal is to select an arm from the set A reward. The reward in our setting is the same as the reward in a typical recommendation problem using MAB. That is, rewards are in the form of clicks for the correctly recommended queries. If the cardinality of A standard stochastic multi-armed bandit algorithms like Upper Conﬁdence Bound (UCB) (Auer et al., 2002) or -greedy (Lattimore and Szepesv´ari, 2020). However, as pointed out earlier, in query recommendation settings, the number of arms is countably many and any standard multi-armed bandit algorithm has to choose from an extremely large number of arms. The sheer volume of possible arms makes it impossible to try each of the arms (even once), and consequently the algorithm is enforced to ignore many arms and dedicate the valuable query exploration only to a certain number of arms. set, from the pool of arms, and adopt standard MAB algorithms over this reduced arms set (Wang et al., 2008; Kalvit and Zeevi, 2020; Zhu and Nowak, 2020; Bayati et al., 2020). The random selection might result in the optimal arms to be ignored in the selection process, thus, hindering the performance of the recommendation engine. In our method, we make use of the shared context information between queries to select the candidate set. We assume that the queries are represented as d-dimensional real-valued vectors encoding the semantic content of the queries (Le and Mikolov, 2014; Mikolov et al., 2013). These vectors can be regarded as the contexts associated with the arms. Without ambiguity, we use the same notation a do not make use of context information when generating the candidate set. query log ﬁles of an online literature discovery service. We plot the sequences of queries executed in ﬁve random user-sessions in Figure 1. Speciﬁcally, we plot the 2-dimensional query context vectors (we used PCA to reduce the original dimension d = 128) for ﬁve random user-sessions in ﬁve diﬀerent colours (further details about the queries and sessions is given in Section 4 and the supplementary ﬁle). From the plot, one can observe that the queries issued within each session are contextually similar with respect to standard similarity measures like cos, and form relatively small loose clusters. Based on this observation on query similarity, we propose to make use of the shared context information between Let A be the set of countably many arms and a∈ A be the currently played arm. Our Standard CMAB algorithms randomly select a ﬁxed number of arms, called candidate We base our reasoning on using the context associated with an arm by analyzing a real the currently executing query and queries in A approach is the assumption that arms with similar context, with respect to a given arm, have similar utility. We formalize this assumption using the following deﬁnition of utility adopted from the information-theoretic interpretation of utility function in multi-agent systems. Speciﬁcally, one can conceive utility in terms of the preference probabilities for being at diﬀerent states as given in (Ortega and Braun, 2010). Deﬁnition 1 Goodness of Arm for Selection (Ortega and Braun, 2010, Deﬁnition 1) Given the currently playing arm a of the arm a a real-valued utility function util which is: (i) sub-additive util util score for the arm a playing arm. The utility function util deﬁned above assigns a scalar value to each possible arm such that arms with higher utility correspond to arms that are more preferred. However, the challenge in our setting is that the preference scores p(a Typical multi-armed bandit algorithms work by adaptive hypothesis testing. From an abstract point, such algorithms randomly pick two arms (assuming only two types of arms: optimal and non-optimal) and run on-the-ﬂy hypothesis testing for a predeﬁned duration to estimate statistical properties of the arms. As noted earlier, contextually similar arms are preferred and, thus, query similarity is correlated to its utility. Hence, instead of randomly selecting the candidate set, we argue that the pair of arms is chosen using a joint probability distribution deﬁned over the similarity of the arms. S(ε) = {a the arms for a given similarity threshold value ε > 0. Based on this similarity indices, a distribution is induced on the elements of S where J·K is the indicator function. Extending over the whole population of arms, we deﬁne g{a}, {a}+utilg{a}, {a}; and (ii) consistent, i.e., g{a}, {a}> g{a}, {a}⇔ The conditional probability, p(a|a) can be considered as the normalized preference Given two arms a, aand a similarity function evaluation oracle s= sim(a, a), let tive hypothesis testing is sampled according to the following joint distribution; note, with Derivation: By assumption, the candidate set of arms (a testing are selected among those that satisfy (a S(ε). Therefore, the probability of selecting the pair (a where the second line follows from the fact that S events, whereas the last line follows from the independence of arms. arm a similar/dissimilar to a example is given in the supplementary ﬁle. Example 1 Assume, we have four arms with contexts a the information gathering session by playing the arm a choose one of the arm from the remaining three arms (1,2,3) for recommendation. Assume that the similarity values are sim(a ε = 0.5, now estimating the diﬀerent quantities in the equation, we get Given the currently playing arm a, we posit that the candidate set of arms for adap- , s) ≥ εi, we denote the pairwise comparison: s≥ ε ∧ s≥ ε. p(a, a|a, ε) = p(a, a|S(ε)∪ S(ε)) = The rationale behind the above sampling scheme is that, given the currently playing , we independently draw two arms aand aand accept/reject them only if pairs are Now, estimating the joint probability for sampling the pairs, we get p(a, a|a, 0.5) =0.550 × 0.348 × 0.478 + 0.067 × 1 × 1.3750.550 + 0.067= 0.297 p(a, a|a, 0.5) =0.550 × 0.348 × 0.522 + 0.067 × 1 × 1.50.550 + 0.067= 0.324(3) p(a, a|a, 0.5) =0.550 × 0.478 × 0.522 + 0.067 × 1.375 × 1.50.550 + 0.067= 0.445 Now, let us try the same with ε = 0.6 tationally ineﬃcient. In Lemma 2, we show that arms similar to the currently playing arm are selected with the probability in (6). This will help us to select a candidate set based on marginal probabilities only. Lemma 2 Given the set of arms A independently drawn with probability Proof From our assumption, arm pairs {(a order to get the marginal probability, we sum over a and take all the arms with highest marginal probability according to (6), or randomly sample k arms with the estimated marginal probability. However, such an approach is not eﬀective in practice, i.e., users do not prefer to issue the most similar queries in data exploration tasks, as demonstrated in our experimental study in Section 4. Moreover, in the CMAB settings, there can be a huge number of similar arms for a ﬁxed threshold. So randomly picking k number of arms as per the marginal probability might lead the optimal or nearoptimal arms to be ignored as discussed in our problem statement. The number of arms for diﬀerent threshold values of marginal probability is given in Table 2. To this end, we propose to select a candidate arms set that maximizes the utility of the arms as given in Deﬁnition 1. Furthermore, we elaborate on an easy-to-implement distributed framework for ﬁnding a candidate set with varying numbers of candidate arms. Now, as before, let us estimate the joint probability, we get p(a, a|a, 0.6) =0.150 × 0.667 × 0.917 + 0.376 × 0.421 × 0.5790.150 + 0.376= 0.349 p(a, a|a, 0.6) =0.150 × 0.667 × 1 + 0.376 × 0.421 × 0.6320.150 + 0.376= 0.380(5) p(a, a|a, 0.6) =0.150 × 0.917 × 1 + 0.376 × 0.579 × 0.6320.150 + 0.376= 0.523 Estimating the joint probability for every pair of arms is time consuming and compu- A naive approach for generating candidate arms set will be to ﬁx a probability threshold To select arms based on utility, we need a function that realizes real valued utilities from the probabilistic preference scores obtained using the similarity between shared contextual information content. According to Theorem 3 (Ortega and Braun, 2010), the logarithm function is the only function that can express such a relationship between the preference probability scores and utility function. For completeness, we restate Theorem 3 by Ortega and Braun (2010). Theorem 3 ((Ortega and Braun, 2010, Theorem 1)) Given the arm set A with the probability space deﬁned as in (6), a function util is a utility function on the probability space, if and only if for all a c > 0 is an arbitrary constant. Deﬁnition 1 with our preference probability space deﬁned in (6). Given log as the utility function, c = 1 and currently executing arm a the optimal utility if it solves mization problem with cardinality matroid constraint. Though submodular maximization is NP-hard in general, a simple greedy heuristic due to Nemhauser et al. (1978) guarantees a solution with constant approximation factor equal to 1 − max-utility based candidate arm selection procedure is given in Algorithm 1. The input to the algorithm is A an initial set of candidate arms with ﬁxed cardinality. This set is constructed following the greedy heuristic. At each trial, the algorithm dynamically updates the candidate set based on the current trial count using Algorithm 2. At the initial stages of trials, we allow the candidate set to grow as the algorithm is allowed to sacriﬁce short term gain by exploring for arms with even larger reward that will expect to yield a long-term beneﬁt. As the trial progresses we keep the candidate set ﬁxed as the algorithm is allowed to eschew new arm exploration, by exploiting the unimodality of t contextual MAB algorithm using the candidate set and currently executing arm. The standard greedy algorithms for submodular maximization guarantee a near-optimal candidate selection, without room for further improvement using current computing environments. However, greedy algorithms do not scale well when applied to massive data and, thus, initialization of C will incur considerable computation time. The greedy algorithms work well for centralized submodular maximization problems; but this requires O(nk) value oracle calls to select k arms from n arms. The adaptive addition of new arms to the existing It can be easily veriﬁed that the log function satisﬁes all the properties given in the Since log is a concave function, the objective function in (7) is a submodular maxi- Algorithm 1: Max-utility based Countably Many-armed Contextual Bandits Input : A t = 1, . . . , T do Algorithm 3: Distributed Submodular Maximization Input : A that maximizes log C (Algorithm 2) can be achieved in linear time, as the number of arms to be added will be relatively very small. For large data, submodular maximization with cardinality constraint can be solved in nearly linear time using a distributed greedy algorithm (Mirzasoleiman et al., 2016). We use a faster version of the submodular maximization algorithm (Mirzasoleiman et al., 2016), which can be parallelized. The proposed algorithm adopted to our setting is then given in Algorithm 3. In line 2 & line 3 of the Algorithm 3, as mentioned earlier, g is the joint probability of all the arms in the input set conditioned on the currently playing arm. Due to the use of a greedy algorithm, we do not explicitly calculate it, but incrementally construct the set. If the total number of trials T is known beforehand, one can choose a value of k that minimizes the regret associated with the underlying bandit algorithm. For instance, in the UCB-∞ algorithm in Wang et al. (2008), for a distribution speciﬁc parameter β, one can choose k to be of the order of T of trials T is not known in advance, one can adaptively choose k depending on the current trial number. A MAB algorithm is anytime, if the regret bounds on the expected regret holds for all values of T (up to constant factors). We can make our algorithm anytime by bandit algorithm on arm set C using}; aas context; employing anytime bandit algorithm like UCB-∞ in line 3 of Algorithm 1. For example, in case of using UCB-∞, we can use k as shown in line 1 of Algorithm 2. We discuss the dataset, baselines, and provide in-depth analysis to verify the performance of the proposed arm selection scheme. Our source code is available at: https://anonymous. 4open.science/r/0e5ad6b7-ac02-4577-9212-c9d505d3dbdb/. Our experiments are conducted on recent large scale query logs from an online literature discovery service. The application works by accepting a user query and returning a list of the most relevant research articles to the query. The logs contained around 4.5 million queries grouped into 547740 user sessions. For our experimental study, as a pre-processing step, we removed user sessions with less than 4 queries and more than 50 queries. We also removed sessions containing non-English queries. The statistics of the ﬁnal data used in the experiment are given in Table 3. Details Of Queries: The log ﬁle contains the user issued search queries, in the form of free text, to a popular online literature discover service. For each user session, the log ﬁle contains the unique session id, timestamp and the query text. The query text covers diverse topics like ’knowledge graph embeddings’, ’cryptocurrencies’, ’COVID 19’ etc. It also contains non-science topics like politics, behavioural studies etc. service, with session ids anonymized. In Table 1, we list queries issued in three diﬀerent sessions. Though the queries can be broadly classiﬁed under very speciﬁc topics, it covers diverse aspects within the topic. For example, the queries in the ﬁrst session are related to detecting coding errors, and within the session the user explores diﬀerent aspects of the topic by exploring implementation details, details about speciﬁc tools and details about speciﬁc errors. et al., 2017) to extract the contexts from the queries. We train a bidirectional BERT model (Devlin et al., 2019) on queries to get a word level vector embeddings. We feed these embeddings to a sentence embedding algorithm (Reimers and Gurevych, 2019) to get the ﬁnal context vector representation for each query in the log. We set the dimension of the context vectors d = 128. We compare our max-utility selection strategy against a variant of the zooming algorithm (Kleinberg et al., 2019) and the random selection strategy that is used in standard CMAB algorithms (Wang et al., 2008; Kalvit and Zeevi, 2020; Zhu and Nowak, 2020; Bayati et al., 2020). In random selection, we randomly select k arms and pass to the downstream MAB algorithm whereas in max-utility we follow the Algorithm 1. In case of random and maxutility schemes, we experimented with four diﬀerent contextual bandit algorithms (LinUCB, Below, we give three examples of the queries issued during the literature discovery For the query context vector, we used a transformer-based deep neural network (Vaswani LinThompSamp, Random and Similar) as downstream MAB algorithms and for the zooming variant, we used LinUCB as the downstream MAB algorithm. Linear UCB (LinUCB) was proposed in the context of news recommendation (Li et al., 2010) where the algorithm sequentially selects news articles based on contextual information about the users and articles, while simultaneously adapting to the user-click feedback. LinUCB models the reward as a linear function of the context vector. Linear Thomson Sampling(LinThompSamp)was proposed as an extension to the Thomson sampling scheme to the stochastic contextual bandit settings (Agrawal and Goyal, Table 2: # arms for diﬀerent p(a|a)Table 3: Dataset Statistics 2013). The rewards are assumed to be a linear function of the context vectors. The algorithm itself is based on Bayesian ideas and assumes that the reward likelihood and mean parameter follow the Gaussian distribution. Most Similar strategy recommends one query from the top-ﬁve queries with highest conditional marginal probability with respect to the currently executing query from the candidate set. In Figure 1, we plot the context vectors associated with queries issued in ﬁve diﬀerent user sessions in ﬁve diﬀerent colors. Each data point represents a query. The queries in each session are contextually similar and form relatively loose clusters. Looking at the ﬁgure, one might expect that recommending the most similar query might be a useful strategy. We used this baseline strategy to demonstrate that the naive strategy of picking the queries with high marginal probability or similarity with respect to the currently executing query underperform compared to other algorithms. Random strategy randomly selects a query for recommendation from the candidate set. Zooming LinUCB is based on the zooming algorithm proposed by Kleinberg et al. (2019). It combines the upper conﬁdence bound technique with an adaptive reﬁnement step that selects a candidate set region. For each currently executing query, we select candidate arms that is equal or higher than the similarity threshold and run LinUCB on this arm set. We used the cumulative regret to compare the performance of the algorithms. Formally, we compare the quantity where r(a we used the ﬁrst query in each user session as the initial query and A to the candidate set selection algorithm. We used cosine as the similarity function between sessions in ﬁve diﬀerent colors.gorithms. ) is the reward obtained from the arm played at the tround. In the experiments, Figure 3: Per-round regret for diﬀerent k. arms. If the recommended query is the one of the next queries executed in the corresponding user session, the algorithm is rewarded with r(a) = 1, otherwise, r(a) = 0. The per-round regret of diﬀerent MAB algorithms against the random and max-utility selection strategies is shown in Figure 2. At round t, the per-round regret is deﬁned as seen from the plot, random selection strategy results in optimal or near-optimal arms to be ignored whereas max-utility based candidate selection always includes optimal or nearoptimal arms in the candidate set. Though the zooming algorithm with LinUCB performed as well as LinThompSamp with max-utility, it resulted in higher regret compared to LinUCB with max-utility. Our results are also inline with the regret guarantees proved for LinUCB and LinThompSamp. In the case of LinUCB, the per-round regret is of the order of whereas in the case of LinThompSamp it is of the order of in lower regret compared to LinThompSamp with max-utility. Even with superior regret guarantees of the LinUCB algorithm, the zooming algorithm failed to achieve the regret of LinUCB with max-utility. Looking at the plot in Figure 1, one might be tempted to use simple strategies like recommending similar queries (our Similar strategy), but from the performance comparison in Figure 2 it is very evident that even with the proper candidate selection strategy, it performs very badly in the longer runs. the trivial random recommendation strategy performs better than random recommendation with random arm selection strategy. Moreover, random recommendation with max-utility arm selection strategy results in lower regret than recommending similar queries from a randomly picked candidate set. Hyperparameter Analysis. We further investigated the eﬀect of the size of the candidate set and the similarity threshold (ε) on the algorithm performance. For this set of experiments, we used LinUCB as the downstream MAB algorithm. Our results are shown in Figure 3 & Figure 4. In Figure 3, we varied the value of k from 10 to 500 and compared the performance as a function of k. Here, we used ε = 0.5. Keeping the cardinality of the candidate set to small or high does not give any performance advantage. Though the average number of queries per session in our dataset is ∼7, setting k = 10 performed very Another interesting observation is that with the max-utility arm selection strategy, even poorly. So it is very important to choose the correct size for the candidate set. Precisely, k should be chosen large enough such that the candidate set contains all possible, relevant and diverse queries with respect to the currently running query. We also analyzed the performance of the max-utility candidate selection strategy for diﬀerent values of ε. Probability preference score for an arm is determined by the value of ε, thus, it is an hyperparameter of the selection strategy. The performance of the LinUCB algorithm for diﬀerent values of ε is plotted in Figure 4. Here, we used k = 250. For small and very large values of ε, the per-round regret is slightly higher than mid-range (0.4 - 0.6) values of ε. By keeping ε to small and large values, we make the preference probability score between the currently running query and the remaining queries to be high and low respectively. As a result, we notice the same trend as in Figure 3. When ε is small many irrelevant queries will have high preference probability scores. Similarly, when ε is large, many diverse but relevant queries will have low preference probability scores. We modelled the query recommendation problem in closed loop interactive learning settings like online information gathering using a MAB framework with countably many arms. The standard way to solve MAB problems with countably many arms is to select a small set of candidate arms and then apply standard MAB algorithms on this candidate set downstream. We showed that such a selection strategy often results in higher cumulative regret and proposed a selection strategy based on the maximum utility of the arms. Our experimental results using a real online literature gathering service log ﬁle demonstrated that the proposed arm selection strategy signiﬁcantly improves the cumulative regret compared to zooming algorithm and the commonly used random selection strategy for a variety of contextual multi-armed bandit algorithms.