We investigate the design of recommendation systems that can efﬁciently learn from sparse and delayed feedback. Deep Exploration can play an important role in such contexts, enabling a recommendation system to much more quickly assess a user’s needs and personalize service. We design an algorithm based on Thompson Sampling that carries out Deep Exploration. We demonstrate through simulations that the algorithm can substantially amplify the rate of positive feedback relative to common recommendation system designs in a scalable fashion. These results demonstrate promise that we hope will inspire engineering of production recommendation systems that leverage Deep Exploration. Recommendation systems (RS) play a critical role in helping users ﬁnd desired content within the ever-expanding universe of information that can be accessed via the Internet. Supervised learning procedures, such as collaborative ﬁltering(Schafer et al. 2007) and content-based ﬁltering(Blanda 2016) that form the cornerstone of RSs, are typically used to model the probability that each piece of content will immediately engage a given user. However, those methods are inadequate when optimizing for delayed user feedback (Joulani, Gyorgy, and Szepesv´ari 2013; Ktena et al. 2019). In particular, the value of a recommendation can become evident in later interactions with a user, rather than through immediate engagement. For example, a user might submit a positive rating about a news platform after a series of recommendations. Delays in relevant feedback are prevalent in practical RSs. One approach to developing a RS that accounts for delayed feedback is to maintain a model that predicts cumulative future value from interactions with each user, using reinforcement learning algorithms (Liu and Shih 2005; Shih and Liu 2008; Desirena et al. 2019; Iwata, Saito, and Yamada 2008; Ie et al. 2019; Theocharous, Thomas, and Ghavamzadeh 2015; Chen et al. 2019; Zheng et al. 2018). Reinforcement Learning algorithms can be used to select lifetime-value-maximizing recommendations. Maximizing lifetime value is especially important when making recommendations that can trigger informative feedback only after subsequent interactions with the user. Although the above work has demonstrated the potential of Reinforcement Learning in RSs, its capability of succeeding in RS problems with sparse and delayed feedback remains to be veriﬁed. A major challenge to existing RS algorithms arises when positive feedback is rare and tends to arise only after a series of suitable recommendations. To apply the methods mentioned above, the RS needs to gather enormous quantities of data in order to build a model to draw statistically signiﬁcant inferences due to the rarity in positive feedback. Furthermore, the required amount of data for ﬁtting a model grows proportionally to the ratio between uninformative and informative feedback (Ganganwar 2012). Strategic selection of recommendations geared to elicit informative feedback can dramatically reduce data requirements. Exploratory behavior that elicits informative feedback is important because it actively collects data that do not necessarily maximize the immediate or cumulative positive feedback, but help inform unknown attributes about the user and reduce the number of interactions required to learn about the user. Deep Exploration (Osband et al. 2019) refers to a strategic form of such exploratory behavior. It allows an agent to proactively seek information from unknown domains within the environment, which is beneﬁcial to subsequent decision making. This approach enables RSs to more quickly learn users’ preferences when their feedback is sparse and delayed. To the best of our knowledge, this paper is the ﬁrst to demonstrate the potential of Deep Exploration to improve RSs in a practical and scalable manner. We present an algorithm based on Thompson Sampling that carries out Deep Exploration. We also demonstrate that Deep Exploration substantially ampliﬁes the rate of positive feedback. In Section 2, we deﬁne the RS problem as one of sequential decision making problems and offer a generic simulation framework for modelling sparsity and delay in user feedback. In Section 3, we present results from case studies that compare common RS designs against our algorithm, and in doing so, demonstrate the value of Deep Exploration in dealing with sparsity and delays. In Section 4, we introduce several algorithms that are commonly adopted for RSs when delays and sparsity in user feedback exist. In Section 5, we design an algorithm based on Thompson Sampling. In Section 6, we study the effectiveness of Deep Exploration through highﬁdelity simulations of a toy environment and a Yahoo front page environment and also study the scalability of the algorithm. In this section, we formulate the RS problem in the context of a sequential decision problem where the agent makes observations, takes an action, makes some more observations in the resulting world, then takes a new action conditioned on the new observations. In the context of a RS, the agent in a RS would make a recommendation to a user as an action and receive a response from the user as an observation. For more details, see Figure 1. In the following subsections, we offer a formal deﬁnition for the generic RS problem and discuss the speciﬁc setting and simulation framework that concerns our work tackling sparsity and delay. General Interface The generic problem deﬁnition for the RS problem consists of 5 components, EnvironmentE, Action SpaceA, Reward R, History H, and the termination condition. Environment E: In a sequential decision problem, the environment is where the agent sends its chosen action and makes observations. Here, the environment is an interface that represents a user of a RS. Given the content presented to the user, the user would outputcontinueandsatisﬁed, both Boolean.continueindicates whether or not he/she would like to continue receiving recommendations andsatisﬁed indicates whether he/she is satisﬁed at timet. Hence the observation, O, at time step t for the agent is This setup can be easily extended to cases where the agent can make observations that contain more information.continue andsatisﬁedserve as an example of the basic information a RS would receive from the environment. Agent Actions A: At each time step, the agent has a set of units available to present to the user or decide to not deliver any units. At time stept, the agent has a set of available recommendation unitsA=a, a, a, . . . , a, and aindicates no-op. Herekis the cardinality of the available set of units at time stept. From the set of available units, the agent chooses a unit A∈ Aas the action. Interaction History H: The historyH= (A, O)indicates the history of interactions between the agent and the environment. Agent Reward R: At each time stept, the agent would receive rewardras a function ofO. Here the agent can use deﬁnitions forrin order to optimize for different objectives such as the length of a recommendation session and the amount of time that the user is satisﬁed. Termination: Each episode terminates ifcontinue= 0 or t ≥ T , where T is the maximum horizon.P The objective of the agent ismaxr, which is the return in an episode. An episode can be considered as a session where the interaction between the agent and the environment starts and terminates. When an episode terminates, the environment could change its representing user depending on the next upcoming user. Note that episodes do not need to end before the next episode starts and hence this general interface deﬁnition is also compatible with cases where multiple users simultaneously interacts with the agent. We could also alternatively track cumulative reward across all episodes (sum of per episode return) to evaluate the performance of the agent. Agent Knowledge and Environment Simulation Agent’s Knowledge.For the speciﬁc setting in this work, we assume that the agent has some prior knowledge about the environment (the user) and the actions available at each time step. For each environment that the agent sends actions to, the agent has access to the environment’s representing feature vectorφin the current episode. For each action unita, the agent also has access to a representing feature vectorφ. The agent can also access the representing feature vectorφfor historyH. We deﬁne reward for each time step r= satisﬁedto measure the amount of time that the user is satisﬁed with the agent’s recommendations. Generic Simulation Framework.We offer a generic framework to simulate delay and sparsity in RS problems. In the simulation framework, the environment keeps track of the user’s interest levely, which measures the user’s satisfaction level and is not visible to the agent. The initial user interest levely= 0. Given that the agent selectsaas action at time step t, the user interest level is updated with wheregis a user interest function that takes in the environment feature and the action feature as inputs to model user interest change after receiving a new recommendation.gis not observable to the agent andg(φ, φ) = 0is set as a deﬁnition, since ais a no-op. With user interestydeﬁned above, we can now simulate continueand satisﬁed. continueis simulated as and satisﬁedis simulated as wherebis the target interest level for the user to be satisﬁed and 1 is the indicator function. Note that the generic simulation framework above is an abstract and simpliﬁed model for the interaction between a user and the agent. The deﬁnition above abstracts delayed user feedback to a function that accumulates positive experience into delayed feedback where an observation would only happen after a sequence of actions. Delayed feedback similar to the deﬁnition is commonly observed in software rating, cumulative revenue and total user engagement time. In the following sections, we study a few case studies to understand the difference between classic exploration strategies and Deep Exploration, present a few algorithm candidates for this problem and introduce how to use Deep Exploration with Thompson Sampling for this problem. Exploration plays an important role in improving sample efﬁciency when learning user behavior that could be sparse and delayed. Among the current RS research that aims to address sparsity in user feedback, the commonly adopted exploration strategy are-Greedy (Kamishima and Akaho 2011; Theocharous, Thomas, and Ghavamzadeh 2015; Hu, Shi, and Liu 2017) and Upper Conﬁdence Bound (UCB) (Nguyen-Thanh et al. 2019; Nakamura 2015). Since UCB’s computational tractability remains to be a challenge in large scale systems and parametric approaches, we focus on discussing-Greedy as a baseline compared against an efﬁcient version of Deep Exploration that leverages Thompson Sampling.-Greedy keeps a point estimate for the value of each recommendation and provides a random recommendation with probability, which may or may not help learn more information about this recommendation. The main advantage of Thompson Sampling over-Greedy is that it effectively keeps a posterior distribution over the value of each possible recommendation and collects data that best help concentrate distributions over recommendations. Empirical results also show that Thompson Sampling outperforms-Greedy in cumulative positive feedback when only optimizing for immediate user feedback (Chapelle and Li 2011). To illustrate more clearly why Deep Exploration is more effective in RSs with sparse and delayed feedback, we provide two case studies for decentralized and centralized RSs, where they generate recommendations to a single user and a group of users respectively. Example 1 (Decentralized RS) Consider a content recommendation agent with an environment following the deﬁnition in the Problem Deﬁnition section. The environment’s underlying user does not change across episodes. The environment’s target interest levelbis 1.0. At each time step, the agent has two actions available for recommendation,A= (a, a)for allt.ais an action that moves user interestyback to 0 but does not terminate the episode.ais an action that adds1/Tto the user’s interest levely. The agent only knows that one of the two actions add1/Ttoybut does not know which. The maximum time horizon interacting with the user in a single episode isT. See Figure 2 for an abstract state transition diagram. The agent has prior knowledge that at least one ofaandaadd1/T to yat each time step. This example aims to illustrate the complexity of the RS problem when feedback is sparse and delayed. The agent, in this example, will not be able to observe any difference when applyingaandaunless it appliesaTtimes. The optimal strategy here is to applyT a’s in an episode and applyT a’s in an episode to ﬁnd out which action adds1/T at each time step based on the observation ofsatisﬁedin these two episodes. If the optimal strategy is not applied, the agent would not be able to receive any positive reward for the entire episode. In the following analysis, we demonstrate the difference between-Greedy and Deep Exploration with Thompson Sampling when tackling this problem. Since the agent could not observe any difference unless it appliesTrecommendations ofa, there is a unique series of recommendations that would lead to learning action optimality. If-Greedy is applied in this case, the number of episodes required to learn the user’s subscription preference in expectation isΘ(2)since there are2possible permutations of recommendation series in T time steps. When using Deep Exploration with Thompson Sampling, the method keeps a posterior distribution overaanda’s cumulative value. Before any interactions with the user, the agent’s prior belief is that with probability1/2,aadds1/T toyper time step and with probability1/2,aadds1/T toyper time step. Hence with probability1/2in the ﬁrst episode, Deep Exploration keeps usingaforTtimes because it assumesais the action that adds1/Ttoyper time step. Therefore after 2 episodes tryingaandaboth for T times in an episode, the agent learns the optimal policy. To understand how the above approach can help in a RS that serves a group of users, consider the following example. Example 2 (Centralized RS) Following the setup in Example 1, now the agent needs to simultaneously interact withMenvironments. For each environment’s underlying useri, eitheraadds1/Ttoyora adds1/Ttoy. Letpreferindicate whetheraoraadds 1/Ttoyfor useri.prefer= 1ifais the better action for useriandprefer= 2otherwise. The agent interacts withMenvironments simultaneously forKepisodes. The goal of the agent is to optimize cumulative reward across all Menvironments. Assume that there is a function approximator that can modelprefer∀i ∈ {1, 2, . . . , M}withM/2 preferobserved and K, M  2. Consider-Greedy ﬁrst. Since all sampling processes are executed randomly and with1/Θ(2)probability of learning preferfor useri, the expected number of user preferences learned afterMusers’ experiences isM/Θ(2). In expectation, there needs to beΘ(2/M)episodes from allM users for the agent to learn allprefer’s. SinceK, M  2 and there is not a reasonable model learned forprefer∀iafterKepisodes, in expectation, the total reward of the system is less than MK/2. Now consider Deep Exploration. The algorithm has probability1/2of learning useri’spreferafter a single episode interacting with the user. In expectation, after interacting withMusers, the system can perfectly modelprefer∀iand aggregate M(K − 1/2) total reward. From above, intelligent exploration is instrumental not only to single user preference learning, but also to inferring population preferences. Classic exploration strategies similar to-Greedy are not suited for problems with sparse and delayed consequences. Deep Exploration is much more competent with these types of problems compared to-Greedy. In this section, we provide a quick introduction for one of the state-of-the-art neural network architecture, Neural Collaborative Filtering, for RSs. In addition, we also offer two commonly adopted algorithms that could potentially leverage Neural Collaborative Filtering to tackle delay and sparsity in feedback for RSs using -Greedy. Neural Collaborative Filtering Architecture Neural Collaborative Filtering (NCF) (He et al. 2018a) is an architecture that combines the advantages of matrix factorization and neural networks. This approach accounts for the non-linearity of user item interaction that linear matrix factorization cannot model. NCF is commonly adopted as the state-of-the-art architecture for RSs (He et al. 2018b; Bai et al. 2017; Liu et al. 2018; Gao et al. 2019; Yu et al. 2019) and can be adopted for any learning algorithms that try to capture the interaction between the environment and the actions applied. To use the NCF architecture, in addition toφ, φand, φ, each environment and action pair will have two unique onehot representationsβandβrespectively, often referred to as sparse features. The one-hot representations are used to better uniquely represent each environment and each action as well as enable Matrix Factorization. Each environment and action one-hot representation will then be projected into its respective embedding space:p= f(β); p= f(β). After embedding generation, the embeddings then go through a matrix factorization layer and a multilayer perceptron (MLP) layer in parallel. In the matrix factorization layer, two embeddings go through element-wise productm = p p. In the MLP layer, the two embeddings are concatenated and then sent through an MLP and get output n. Once both layers are processed,mandnare then concatenated along withφ, φandφto make a ﬁnal prediction. See Figure 3a for an illustration for the entire architecture. Note that we made modiﬁcations on the architecture presented in (He et al. 2018a) to account for features that are not one-hot representation and features that represent the history of interactions, which are generally referred to as dense features. These features are included as a general practice of combining dense features and sparse features to capture descriptive information of the environment, the history and the action. In this work, we adopt this architecture for all algorithms that we introduce to match with the state-of-the-art performance. In the following sections, we abuse the notation to omitβandβfrom equations and assume that they are included in φand φbut used separately in NCF. NCF Life-Time Value Learning: NCF TD(1) For a sequential decision making problem, a common approach is to use Reinforcement Learning (RL) algorithms, among which TD(1) is the most widely adopted method for life-time value estimate in the RS ﬁeld. TD(1), temporal difference learning with trace decay factor of 1, refers to an approach that accumulates all future reward into account when modeling the current action’s value. TD(1) is also commonly referred to as life-time value learning in literature (Liu and Shih 2005; Shih and Liu 2008; Desirena et al. 2019; Iwata, Saito, and Yamada 2008). We denote the NCF architecture for TD(1) asQ, which is parameterized byθ. At time steptin episodel, the TD(1) agent would take an action with probability1 − and with probability, the agent would take a random action. After taking actionA, the agent would receive rewardrand make a new observationOfrom the environment. Thereafter, the agent storesφ, randφas a tuple in its buffer D for future learning. When the agent performs the learning step using the buffer to update the model, the target variable for each data sample is the cumulative reward starting from the sample’s time stamp to the end of the episode. More formally, with a data sampled ∼ D, its time stamptand its episodel, the update for the model parameter θ is only happen between episodes. NCF Deep Q Learning: NCF TD(0) A less common approach for RSs is TD(0), temporal difference learning with trace decay factor of 0. This approach is commonly adopted by Q-learning algorithms such as Deep Q Network (DQN) (Mnih et al. 2015). The main difference between TD(1) and TD(0) is that TD(1) aims to capture all future reward by taking an action in a state regardless of future actions applied whereas TD(0) aims to capture the per step value difference in taking a speciﬁc action in a state. We denote the NCF architecture for TD(0) asQ, which is parameterized byθ. At the beginning of each episode, the TD(0) agent would make a copy of the current NCF TD(0) model as the target modelQ. At time step t in episode l, the TD(0) agent would take an action with probability1 − and with probability, the agent would take a random action. After taking actionA, the agent would receive rewardrand make a new observationOfrom the environment. The agent then storesφ, r, φand the extractedφas well asAbased onOas a tuple in its buffer D for future learning. After each episode, the RS samples datad ∼ Dto learn from and improve its modelQfor its policy. Instead of optimizing against cumulative reward in an episode, the T D(0)approach optimizes the model against the sum of the immediate reward and the value of next state given by the target model. This is also known as the Bellman Update. More formally, withd’s time stampt, its episodeland algorithm learning rate α, the update for the model parameter θ ish Different from the TD(1) approach, TD(0) could update its model per time step where the algorithm would refresh the target model and make an update to θ per time step. (a) NCF Architecture for Problem Setting in Section Problem DeﬁnitionFigure 3: Deep Exploration with NCF Architecture In this section, we offer an efﬁcient implementation of NCF with Deep Exploration leveraging Thompson Sampling. At a high level, Thompson Sampling is a method that tackles the exploration vs exploitation dilemma where a decision maker needs to select an action to either explore to obtain new information or exploit to maximize immediate return. In a sequential decision problem with a discrete history space and a discrete action space, a Thompson Sampling agent starts with a prior distribution for each action-history pair. At time stept, the Thompson Sampling agent samples values from the posterior distributions of all available actions given the history up to time stept. The agent then picks the action which has the highest sampled value and makes a new observation from the environment and computes its the reward. The agent then updates the posterior distribution of the selected action-history pair with the reward and the new observation. WithKbeing the total number of episodes andTbeing the time horizon, Thompson Sampling theoreti-p cally could achieveO(KT log(T ))regret bound in a classic multi-armed bandit setting, which is near order-optimal (Russo et al. 2018). However, it is not scalable in a problem with a large and continuous history spaceHas well as action spaceAbecause Thompson Sampling need to keep track of a posterior distribution for each history and action pair. In this work, we leverage Ensemble Sampling (Lu and Van Roy 2017) to extend NCF TD(0) to adopt the concept of Deep Exploration with Thompson Sampling (NCFDE). Instead of only initializing one NCF architecture for modeling in NCF TD(0), a NCFDE agent initializes its model as an ensemble of NCF architectures as main models with cardinal- ityM,Q, Q, . . . , Qalong withMprior models Q, Q, . . . , Q. The prior models are initialized with Glorot Sampling and are never updated throughout the lifetime of the models (Osband, Aslanides, and Cassirer 2018; Glorot and Bengio 2010). Similar to NCF TD(0), NCFDE also needs to keep track of the ensemble’s corresponding target model for Bellman Update. For each of theMmain modelsQ, the NCFDE agent needs to keep track of its corresponding target modelQ. The prior models are added to their corresponding main models when performing updates to provide sufﬁcient regularization (Zhang et al. 2017; Bartlett, Foster, and Telgarsky 2017). For simplicity of notations, we denote Q= Q+ Qand Q= Q+ Q. 1: Initialize M NCF models Q. 2: Initialize M NCF prior models Q. 3: Initialize M replay buffers D. 4: Deﬁne Q= Q+ Q, ∀i ∈ {1, 2, . . . , M}. 7:Set target modelQ= Q,∀i ∈ {1, 2, . . . , M}. 20:Sampled ∼ Dwith episodeland time stept. At the beginning of each episode, a NCFDE agent samples a modelQfrom the ensemble. At time steptin episodel, the NCFDE RS selects action deterministically unlike-Greedy. After taking actionA, the agent would receive rewardrand make a new observation Ofrom the environment. The NCFDE agent keeps a bufferDfor each modelk in the ensemble. To make sure that the ensemble best approximates the distribution of the value for each historyaction pair, we adopt double-or-nothing bootstrapping (Osband et al. 2019) when updating each buffer. With probability 1/2,φ, r, φand the extractedφas well asAbased on Oare stored in Das a tuple for future learning. After each episode, for each model in the ensemble, the agent samples datad∼ Dto updateθ. Withd’s timestep t, episode l and algorithm learning rate α, θupdates withh See Algorithm 1 for the complete NCFDE implementation and Figure 3b for a visualization of the architecture. NCFDE offers an efﬁcient implementation of Deep Exploration for RSs. It leverages ensemble sampling to approximate Thompson Sampling and meanwhile scales to continuous history and action space because it does not require the agent to keep track of a posterior distribution for each history and action pair like Thompson Sampling. In the following sections, we will study empirically the effectiveness of NCFDE compared against NCF TD(1) and NCF TD(0). In this section, we run a few experiments to demonstrate the performance difference between the candidate algorithms we introduce in the previous sections. The algorithms we consider are: Supervised Learning (using NCF architecture), NCF TD(1), NCF TD(0), and NCFDE. NCF TD(0) takes 0.05as the value of. We consider a toy environment (proof of concept), a single-user Yahoo environment (Decentralized RS) and a multi-user Yahoo environment (Centralized RS). We use episodes needed to reach peak performance, per episode return and cumulative reward across episodes to evaluate the performance of the algorithms. We believe that the Yahoo simulator we provide in this work also suits as a comprehensive evaluation benchmark for future reinforcement learning algorithms in RSs. A summary of experiment results are presented in Table 1 and 2. Table 1 shows the time needed to hit peak performance and Table 2 shows the sum of total reward received across all episodes in an experiment for each algorithm. Computation time for each of the algorithm in the Yahoo Centralized environment is presented in Table 3, where prediction time is the time needed to compute the next action and training time is the time needed to perform a training step, both measured on a Macbook Pro with Intel-i9 2.9GHz and 32 GB of RAM. Code and data are provided in supplementary material. Note that to the best of authors’ knowledge, there is only one piece of literature that built a reinforcement learning simulator for RSs (Rohde et al. 2018). However, this simulator updates user state independently from recommendations given to the user and does not introduce sparsity in user feedback. Therefore the simulator does not provide an environment with delayed and sparse feedback and is not well suited for testing the capability of algorithms under delay and sparsity, which are commonly observed in real-world RSs. Toy Experiment We start with a toy experiment that constructs an environment with extreme sparsity and delay in feedback. The environ- Table 3: Prediction & Training Computation Time (Seconds) ment is set up such that the maximum horizonT = 7and b = 5. The toy experiment represents a decentralized RS environment where the environment does not changeφacross episodes. The environment has a repository ofTrecommendation units, ordered asa, a, . . . , a, and at each time step, the agent is presented with a set of unitsA= (no-op, a). These units are carefully designed such that Here, for any-greedy algorithms, the expected time to ﬁnd an optimal solution isO(2), where using Thompson Sampling, the expected time to ﬁnd an optimal solution is O(T ), where T is the time horizon (Osband et al. 2019). The feature extraction structure for all three algorithms is identical. Since all environments represent the same user,φ does not inﬂuence the prediction model and hence we set it to be empty.φis designed to be a vector with cardinalityT, each entry represents a time step as the following. φ[i] =is used at time step i Each algorithm uses a neural network with a single hidden layer of size 20 and output size 2 for the ﬁnal layer, whether or not to send unitaat time stept(hence we do not needφ here). NCF architecture is omitted in this experiment given the simplicity of feature setup. We run the algorithms with 2000 episodes for 10 times. Results are shown in Figure 4a. In the results, Deep Exploration ﬁnds the near-optimal policy for most experiments in 150 episodes while Supervised Learning, TD(0) and TD(1) are unable to ﬁnd a near-optimal policy. Real World Experiments To fully understand the effectiveness of the models in the real world, we use Yahoo!’s front page today user click log dataset to create a real recommendation system simulation (a) Per episode return for toy(b) Per episode return for decen-(c) Per episode return for cenRS experimenttralized RS experimenttralized RS experiment environment. This dataset provides labels of whether a user has clicked on a piece of content on Yahoo!’s front page. For the Yahoo! recommendation system simulation environment, we still keep the general simulation framework deﬁned in the Problem Deﬁnition section, but with a few modiﬁcations based on the dataset. We train a click-through rate (CTR) classiﬁcation model based on the dataset to generate the engagement intent from each user and unit pair and use the model asg(φ, φ). This model is not visible to the agent. gis also normalized such that its 80th percentile is equal to 0. Bothφandφfor each user and each unit are vectors with cardinality of 6 which is prepared by Yahoo! as features. In the experiments below, we set the cardinality of unit set at each time stepk= 5and the horizon of the problem T = 10. Given a pre-selected list of recommendation candidatesA, A, . . . , Aat the beginning of each episode byP the environment, we setb = 0.8maxg(φ, φ). Note that the list of recommendation candidates are reselected for every episode. NCF detailed architecture is listed in supplementary material. Decentralized (Single User) ExperimentIn this experiment, we sample a single user from Yahoo’s dataset and run all algorithms with the following feature extraction method. We refer to this single user experiment as the decentralized RS experiment. Note that since we still use the same user for all algorithms, we setφto be empty.φis a vector with cardinality 6T φ[6i : 6i + 5] =φif ais used at time step i. (13) where −1 is a vector of six −1’s and 0 is a vector of six 0’s. We run the experiment 20 times, each time with 2000 episodes. The result is shown in Figure 4b. We see that NCFDE signiﬁcantly outperforms Supervised Learning, NCF TD(0) and NCF TD(1). From Figure 4b, NCFDE uses approximately 150 episodes to reach its peak performance while NCF TD(0) takes approximately 900 episodes, NCF TD(1) takes about 500 episodes and Supervised Learning takes about 1100 episodes. NCFDE’s peak performance is at 2.5 cumulative reward per episode while NCF TD(0) at 1.6 per episode, NCF TD(1) at 0.5 per episode and Supervised learning at 0.3 per episode. From Table 2, we can see that cumulative reward across episodes for NCFDE is about 1.5 times that of NCF TD(0), 5 times that of NCF TD(1) and 11 times that of Supervised Learning. Overall, in the decentralized experiment, the NCFDE agent takes much less time to learn a near-optimal policy and achieves a much higher cumulative reward. Centralized (Multi-User) ExperimentIn this experiment, we ﬁrst sample 1,000 users and create an environment where in each episode, all 1,000 users simultaneously experience the simulated Yahoo frontpage. We refer to this experiment as the centralized RS experiment. We run the experiment for 10 times, each time with 80 episodes. The result of the experiment is shown in Figure 4c. We see that NCFDE signiﬁcantly outperforms Supervised Learning, NCF TD(0) and NCF TD(1). NCFDE uses approximately 50 episodes to reach its peak performance while Supervised Learning, NCF TD(0) and NCF TD(1) are all learning after 80 episodes. NCFDE’s peak performance is at 1500 cumulative reward per episode while NCF TD(0) at 700 and NCF TD(1) as well as Supervised Learning at less than 100. From Table 2, we can see that cumulative reward across episodes for NCFDE is about 5 times that of NCF TD(0), 150 times that of NCF TD(1) and 60 times that of Supervised Learning. To study the scalability of the algorithm, we present two set of statistics. Figure 4d and Table 2 shows the experiment result for 100,000 users in Yahoo! environment in a single episode where the model is trained after serving every 1,000 users. NCFDE greatly outperforms all others. Table 3 shows the computation time needed for prediction and training for each algorithm. NCFDE uses almost identical amount of time as the other algorithms in prediction and only uses about twice as much time as other algorithms where NCFDE requires parallel training and more memory to train. Overall, in the centralized experiment, the NCFDE agent also takes much less time to learn a near-optimal policy and achieves a much higher cumulative reward. In this paper, we studied the issue of sparse and delayed feedback in recommendation systems, and the potential for Deep Exploration to improve data efﬁciency in assessing a user’s needs and personalizing service. We designed an algorithm based on Thompson Sampling as an efﬁcient mechanism for carrying out Deep Exploration in recommendation systems. Our experiment results demonstrate promise, verifying that Deep Exploration improves data efﬁciency and ampliﬁes the rate of positive feedback in a scalable manner relative to common recommendation system designs, which explore via-Greedy. We hope that the results of this paper will inspire engineering of production systems that leverage Deep Exploration.