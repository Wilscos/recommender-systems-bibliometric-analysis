Recommender systems [Schafer, Konstan, and Riedl, 1999; Ricci, Rokach, and Shapira, 2011; Aggarwal, 2016] have become essential tools in most real-world applications of big data. These systems ease the users’ effort in selecting suitable items for their needs (e.g., commercial products, books, services) by ﬁnding common patterns of their preferences. In practice, the preferences of users and their opinions of items frequently change over time [Koren, 2009; Xiong et al., 2010]. It poses a challenge in designing recommender systems: How to incorporate temporal dynamics into recommendation results? Following the footsteps of sequential neural modeling, recommender systems researchers have designed a wide range of recurrent neural networks (RNNs) to solve the temporal dynamic challenge [Hidasi et al., 2016; Li et al., 2017; Zhu et al., 2017; Ma et al., 2020]. The main idea of these architectures is to capture the relationship between the current and the past interactions using recurrent units [Wang et al., 2019a]. However, applying RNNs to new types of learning models is often difﬁcult [Pascanu, Mikolov, and Bengio, 2013; Yu et al., 2019a]; For example, the graph neural network (GNN) is a promising model that has recently become the state-of-the-art for recommendations on non-temporal datasets [Wang et al., 2019b; He et al., 2020]. Applying the recurrent architecture to a GNN would require redesigning the whole neural network [Xu et al., 2019; Rossi et al., 2020]. Therefore, an interesting research question arises: Is there a more versatile alternative solution than RNNs to model temporal dynamics? Drawing inspirations from the meta-learning literature, which studies fast adaptation to new tasks [Vilalta and Drissi, 2002], we hypothesize that meta-learning methods could be useful to model temporal dynamics. Research on meta-learning for differentiable models demonstrated the advantages of a gradient-based method named MAML [Finn, Modern recommender systems are required to adapt to the change in user preferences and item popularity. Such a problem is known as the temporal dynamics problem, and it is one of the main challenges in recommender system modeling. Different from the popular recurrent modeling approach, we propose a new solution named LeapRec to the temporal dynamic problem by using trajectory-based meta-learning to model time dependencies. LeapRec characterizes temporal dynamics by two complement components named global time leap (GTL) and ordered time leap (OTL). By design, GTL learns long-term patterns by ﬁnding the shortest learning path across unordered temporal data. Cooperatively, OTL learns short-term patterns by considering the sequential nature of the temporal data. Our experimental results show that LeapRec consistently outperforms the state-of-the-art methods on several datasets and recommendation metrics. Furthermore, we provide an empirical study of the interaction between GTL and OTL, showing the effects of long- and short-term modeling. Abbeel, and Levine, 2017] over RNN-based models [Ravi and Larochelle, 2016; Santoro et al., 2016] in the few-shot learning problem. This technique leverages the gradient path of the optimization process to learn an initialization of parameters, which allows fast adaptation to unseen tasks. Figure 1: Left: Our proposed LeapRec consists of GTL and OTL. Right: A Neural Recommender. LeapRec enables any neural recommender to learn temporal dynamics by optimizing the gradient paths {γ In this paper, we study the application of meta-learning to model the temporal dynamics in recommender systems. The overall approach is to consider the recommendation problem on data from each time period as a separate task. This approach implies that each time step is modeled as a sequence of gradient steps (a gradient path) in the optimization process. However, a straightforward application of MAML poses two main issues. First, since the computational cost of MAML mainly depends on the number of gradient steps and the number of gradient steps grows linearly with the number of time steps, it becomes infeasible to keep track of long-term time dependency. Second, although we can reduce the computational cost using the ﬁrst-order approximation of MAML (FOMAML) [Finn, Abbeel, and Levine, 2017; Nichol, Achiam, and Schulman, 2018], this only considers the start and end points of the gradient paths. That is, FOMAML ignores the gradient trajectory recently, trajectory-aware meta-learning was proposed by Flennerhag et al. [2019a] to reduce the computational cost of MAML without sacriﬁcing performance by minimizing the total length of gradient paths across tasks. In the temporal recommendation context, we found that Leap is a promising approach because it considers the entire gradient trajectory while maintaining the computation at minimal cost. We propose LeapRec as a novel adaptation of trajectory-based meta-learning to recommender systems. Figure 1 illustrates an overview of our approach. In LeapRec, we divide temporal dynamics into two categories: global time leap (GTL) and ordered time leap (OTL). GTL ﬁnds common patterns that share across all time periods. Examples of these patterns in e-commerce are classic books, basic goods, and all-time best seller items. On the other hand, OTL captures the temporal dynamics from momentary social trends. Our experimental results show that our implementation of LeapRec recommendation model (Figure 2) outperforms the state-of-the-art (SOTA) methods on benchmark datasets by a large margin. Furthermore, LeapRec, when used as a plug-in method for other recommender systems, also signiﬁcantly improves their performances. The code and resources are available at https://github.com/nutcrtnk/LeapRec. We summarize our contributions as follows. • To the extent of our knowledge, LeapRec is the ﬁrst trajectory-based meta-learning approach to recommender • To fully utilize the trajectory-based approach to recommender systems, we propose two components of LeapRec systems. named Global and Ordered Time Leap. • Our empirical results show a clear improvement over existing approaches, and our empirical analyses explain Sequential history of user interactions provides valuable information about their preferences and dependencies between items. Naturally, RNN-based models [Hidasi et al., 2016; Li et al., 2017; Wu et al., 2017; Chairatanakul, Murata, and Liu, 2019] have become the most popular choice for modern recommender systems, owing to their effectiveness in handling sequential data. However, this approach is limited by the memory of RNNs: the amount of memory required to capture long-term dependencies grows linearly with the number of time steps. Since it is infeasible to design such a learning model, Chen et al. [2018]; Liu et al. [2018] proposed the use of memory networks [Weston, Chopra, and Bordes, 2015] to improve long-term dependencies modeling in recommender systems. In parallel with these developments in RNNs, self-attention (SA) models [Vaswani et al., 2017] have gained popularity due to their success in natural language processing. Their popularity inspires researchers to investigate the applications of self-attention to improve recommendation quality [Kang and McAuley, 2018; Sun et al., 2019]. These models have the advantage that they can partially adapt to recent data based on the change in input sequences. However, the dependencies between items can be outdated (e.g., combinations of fashion items), and the models cannot differentiate the outdated and new trends. In the real world, the preference of users and their opinions of items change over time. Temporal-aware recommendation models explicitly incorporate time into their recommendation results. TimeSVD++ [Koren, 2009] is a pioneer work, which includes a time-bias factor. Xiong et al. [2010] proposed tensor factorization with time as a dimension. Along with recent developments in the sequential recommendation, Time-LSTM [Zhu et al., 2017], MTAM [Ji et al., 2020], TiSASRec [Li, Wang, and McAuley, 2020], and TGSRec [Fan et al., 2021] incorporate time intervals between successive interactions into LSTM, Memory-Network, and SA; SLi-Rec [Yu et al., 2019b] and TASER [Ye et al., 2020] consider both time intervals and the time of the prediction. Each design has its own modiﬁcations that reﬁne a few speciﬁc components to be time-aware. Thus, it is difﬁcult to transfer the design of temporal components to other architectures. Another type of temporal-aware models is streaming or online recommendation [He et al., 2016; Chang et al., 2017; Wang et al., 2018b], where we can access only the most recent data but not the complete historical data. Training only on recent data inevitably shifts the model to be more biased toward such data than the past. However, this poses a problem in retaining past information [Wang et al., 2018a]. With the concept of meta-learning that aims to make learning faster, Vartak et al. [2017]; Lee et al. [2019]; Wei et al. [2020]; Wang, Ding, and Caverlee [2021] adopted meta-learning techniques to alleviate the cold-start problem, where a model has not yet gathered sufﬁcient information to draw any inferences for new users or new items. Liu et al. [2020] applied meta-learning for session-based recommendation [Hidasi et al., 2016]. Luo et al. [2020] considered an application of meta-learning to select a suitable model for each user. Most of these approaches are based on MAML, which has been shown to be effective for few-shot learning. Meta-learning also has been applied to improve recommendation in online learning scenario. For example, Zhang et al. [2020] proposed SML, a CNN-based meta-update method that uses MAML for meta-training; Xie et al. [2021] modiﬁed MAML for enhancing time adaptation of their recommender system. the dynamic behavior of LeapRec. Let U = {u be a ﬁnite sequence of timestamps. The cardinalities of these sets correspond to the number of users U = |U|, number of items I = |I|, and number of timestamps T = |T |. Given sets of interactions between users and items in each timestamps: D = {D to predict new interactions after t To model how likely a user u ∈ U would interact with an item i ∈ I, we deﬁne f parameterized by θ. Commonly, Bayesian personalized ranking (BPR) [Rendle et al., 2009] is a suitable choice for the optimization target. The core idea is to maximize the interaction likelihood by using the contrastive learning principle. The model f interactions. Formally, we learn the recommendation model f parameter θ. where ∼ denotes i.i.d. uniform samplings, Throughout this paper, we use BPR as the loss function. Gradient-based meta-learning aims to obtain an initialization of parameters, also named meta-parameters it allows fast adaptation across different learning tasks via the gradients [Finn, Abbeel, and Levine, 2017]. Flennerhag et al. [2019b] recently proposed a trajectory-based meta-learning method that minimizes the length of the learning process. The length of the learning process consists of (i) the distance between the loss landscape and (ii) the distance in the parameter space of an update, namely from θ where τ represents a task sampled from some distribution p(τ ), K is the number of update steps, L with respect to τ , ∆L from our equation because using the identity matrix should provide a good approximation [Flennerhag et al., 2019b]. Subsequently, we deﬁne the update of θ by adopting a simple gradient-based update, following MAML: θ θ− α∇L We adopt Leap to our recommendation models by considering data from each time step as a task. This section proposes two distinct components of LeapRec named Global Time Leap (GTL) and Ordered Time Leap (OTL) and γ to be meta-parameters and model parameters of GTL while denoting ¯ω and ω to be meta-parameters and model parameters of OTL. , u, . . . , u} be a ﬁnite set of users, I = {i, i, . . . , i} be a ﬁnite set of items, and T = (t, t, . . . , t) , D, . . .}. (θ), where α denotes the learning rate. Algorithm 1: Meta-optimization for ¯γ and ¯ω using Leap Require: α, β, η: learning rate parameters 4.1.1 Global Time Leap (GTL) GTL aims to ﬁnd common patterns across all time steps by minimizing their gradient paths starting from ¯γ (see Figure 1 for an illustration). We deﬁne a task as the recommendation objective under each timestamp with the following loss function: L Because the parameters of GTL will revert to ¯γ that stores the information across all timestamps before each update, this mechanism imitates the static or long-term preferences of users across all time steps. 4.1.2 Ordered Time Leap (OTL) While it is straightforward to use meta-learning in this context by assuming each time step is independent, encoding the sequential nature of temporal data is a non-trivial problem. Our idea is to capture the temporal information using the gradient trajectory that accumulates across the time sequence. We denote ω k and time step t accumulate the gradient along the trajectory as follows. Within the same time step t repeated until the last time step t The intuition behind this construction is that the obtained parameter ω sequence. Since the effect of catastrophic forgetting in neural networks can be mitigated with gradient trajectory regularization [Goodfellow et al., 2015; Chaudhry et al., 2018], the shortest gradient path helps to retain useful information from previous time steps. LeapRec combines GTL and OTL by taking the summation of their prediction scores: To train {¯γ, ¯ω}, we simultaneously update {γ, ω} and update {¯γ, ¯ω} at the end of an iteration using Leap, as shown in line 13 of Algorithm 1. In deployment, we use the latest model parameters {γ To demonstrate the application of LeapRec to a recommender system, we deﬁne a neural recommender described in Figure 2. The architecture consists of two main components: a graph neural network (GNN) [Scarselli et al., 2009; − α∇L(ω); then, carry the result to the next time step t: ω← ω. This procedure is Figure 2: Our neural recommender. Θ denotes a set of embedding E, GNN, and Self-Attention parameters. Kipf and Welling, 2016] and a self-attention mechanism (SA) [Vaswani et al., 2017]. The neural recommender learns an embedding matrix E ∈ R dimensional embedding of a user (e of dimensionalities denoted by d items based on their related information guided by an interaction graph. On the other hand, the SA component extracts information from the histories of users to enhance user embeddings for more accurate recommendations. Let G = (V, E) be a graph of user-item interactions. The node set V consists of all users and items in data D, and the edge set E consists of interactions between users and items. Then, we use the GNN to propagate the node information based on the graph structure. The propagation for any node, namely m, in the graph by the ` be written as: where N mation matrices of the ` ﬁnal outputs are stored in the embedding matrix E. We construct the sequence of items that a user, namely u, has been interacted with before timestamps t: S the embedding of user u to the sequence: x The attention score between inputs, namely from x denotes a set of the neighborhood of node m, σ denotes a non-linearity function, W, Ware transfor- |N||N|; or GAT [Veliˇckovi´c et al., 2018] proposed to use an attention mechanism to train a. The , . . . , i). Embedding lookup is performed on E for each item in the sequence: x= e. We also append where W in a sequence each feature vector to a two-layer fully-connected feed-forward network (FFN) with ReLU activation. For each SA and FFN, we use a residual connection, dropout, and layer normalization, following Vaswani et al. [2017]: Thus far, we have deﬁned the components of the neural recommender (Figure 2). To ﬁnd the compatibility score between a user u and an item i, we obtain the embedding e embedding e a concrete modeling for f Our neural recommender with LeapRec is a powerful model that uniﬁes multiple concepts of recommender systems; GNN signiﬁes the similarity between users and items beyond local interaction structures, which helps in learning common patterns; SA captures dependencies between items; LeapRec enhances both aforementioned to learn temporal dynamics. This is achievable because of the minimal computation cost of LeapRec. We conducted experiments to answer the following research questions: RQ1 How well does LeapRec perform compared with SOTA models? RQ2 How effective is LeapRec as a plug-ins method for GNN and SA models? RQ3 How important are GTL and OTL, and what are the changes in their learned representations over time? RQ4 How sensitive is LeapRec to hyperparameters, and can LeapRec converge? We used three publicly available datasets: Amazon [Ni, Li, and McAuley, 2019], Goodreads [Wang and Caverlee, 2019], and Yelp preprocessed Amazon data by Wang et al. [2020], where data across categories are mixed. The Goodreads dataset is mined from the book-reading social network Goodreads. User-item interactions here are ratings and reviews of books. Yelp is a crowd-sourced business reviewing website. We used the latest ofﬁcially provided data, updated on February 2021. For each dataset, we set a cutting time that only the data occurred before the time was used for training and the rest for validation/testing. The basic data statictics are presented in Table 1. To visualize the difference in the temporal dynamics between datasets, we plot the relative number of interactions in each group representing the top 100 items grouped by peak time on Amazon and Yelp. Figure 3 shows that popular items on Amazon changed more frequently compared with those on Yelp. , W, W∈ Rare the transformation matrices. Note that our SA cannot be aware of the positions , unlike those SA-based methods for the sequential recommendation. After each SA layer, we apply from the output of the GNN. The score between u and i is given by the dot product he, ei. This is Figure 3: Relative number of interactions of groups of popular items (top 100) under each timestamp. Each line represents a group of items that became popular at the same time. The popularity of items on Amazon changed more drastically than one on Yelp. We divided each dataset into training, validation, and test sets based on the timestamps of interactions. We kept the interactions within six months after the cutting time as the validation set, and the rest after that as the test set. For each interaction in the test set, we randomly sampled 99 negatives instead of ranking all items to reduce the computation cost in the evaluation, and subsequently, the positive will be ranked among them by a model. We measured the recommendation quality using commonly used metrics for Top-K recommendation including Hit Rate (HR@K), Normalized Discounted Cumulative Gain (NDCG@K), and Mean Reciprocal Rank (MRR). To answer RQ1, we extensively evaluated LeapRec by comparing it to various types of recommendation models: For all baselines, we conducted hyperparameter search on the dimensionality of embedding: {64, 128}, learning rate: {0.001, 0.0001}, and dropout: {0, 0.2, 0.5}. We also run hyperparameter search on the number of layers L: {1, 2, 3, 4} for GNN and {1, 2} for SA. We used the default values as provided for the rest. We used Adam [Kingma and Ba, 2014] as the optimizer for all models. For LeapRec, we used the same hyperparameter settings as SASRec for its SA, factorization (MF) [Koren, Bell, and Volinsky, 2009], deep neural network-based: NCF [He et al., 2017], variational autoencoder-based: BiVAE [Truong, Salah, and Lauw, 2021], and GNN-based: NGCF [Wang et al., 2019b] and LightGCN [He et al., 2020] methods. sequences, this includes RNN-based approaches: GRU4Rec [Hidasi et al., 2016] and NARM [Li et al., 2017], a Memory-Network-based approach: SUM [Lian et al., 2021], a SA-based model: SASRec [Kang and McAuley, 2018], and a meta-learning-based method: MetaTL [Wang, Ding, and Caverlee, 2021]. tions. This includes GNN-based: TGN [Rossi et al., 2020], RNN-based: SLi-Rec [Yu et al., 2019b], SA-based: TiSASRec [Li, Wang, and McAuley, 2020] and GNN with SA-based: HyperRec [Wang et al., 2020] SPMF [Wang et al., 2018b] and SML [Zhang et al., 2020]. Note that SML uses FOMAML to train a metaupdater. Table 2: Performance comparison of different models. The best performances are highlighted in bold. The second best performances are underlined. Improvement indicates relative improvement of LeapRec over the best performing baseline in each case. The symbols 2, k, À, and  denote static, sequential, temporal, and streaming models, respectively. We mark MAML approaches by ** for vanilla MAML and * for ﬁrst-order MAML (FOMAML). and for its GNN, we adopt two-layer GCN [Kipf and Welling, 2016] with dropout rate set at 0.2. We performed grid search on meta learning rates, the number of update steps K, and the time granularity on Amazon, then used them on other datasets {0, 40, 91, 121, 128} (see Table 4). We report the experimental results in Table 2. LeapRec consistently outperforms all the baselines across all datasets and metrics. Speciﬁcally, LeapRec achieves 8% to 18% relative improvements over the best performing baseline on Amazon dataset. This indicates that LeapRec can adapt fast enough to capture the dynamics between timestamps. Similarly, we observe that temporal models tend to outperform static and sequential models on the Amazon dataset. Conversely, on Yelp dataset, they achieve lower performances than those of sequential models. This observation demonstrates the inﬂexibility of the current temporal-aware SOTA recommendation models in modeling temporal dynamics. However, LeapRec performs well in both all datasets, indicating its ﬂexibility. Recently, GNN- and SA-based models have gained growing attention from researchers. It is interesting to determine the effect of LeapRec on these neural networks as a plug-ins method. In this experiment, we show that LeapRec can be applied to various types of models, making them temporal-aware. In the GNN-based category, we applied LeapRec on GCN, NGCF, and LightGCN, and compare the recommendation results between these GNNs with and without LeapRec. The main difference between these GNNs is based on feature transformation. LightGCN, a SOTA GNN for recommendation, is a simple GNN without feature transformation and nonlinearity. We also investigate the importance of Leap by introducing a FOMAML variant based on the modiﬁcation of Eq. 2. Figure 4: Performance of GNNs. FOMAML and Leap variants were equipped with GTL and OTL. The number over a bar indicates the relative improvement over its vanilla. Table 3: Performance (NDCG@5) of SASRec without and with LeapRec with different numbers of layers L We plot the results of the GNN experiments in Figure 4. The NDCG@5 scores signiﬁcantly increase up to 46.27% and 53.57% in relative gain among GCN and NGCF thanks to LeapRec. For LightGCN, we observe an improvement on the Amazon dataset and a slight decrease on Goodreads. We suppose that the feature transformation can accelerate the task-learning process [Lee and Choi, 2018; Flennerhag et al., 2019b]. In addition, the LeapRec variants signiﬁcantly outperform the FOMAML variants in all cases. This empirically conﬁrms the importance of trajectory-based metalearning in the recommendation context. The results of experiments on SA-based models are reported in Table 3. Even though SA models can already adapt to recent timestamps, LeapRec still improves SA with up to 12.26% gain over temporal-aware TiSASRec. This indicates the beneﬁt of LeapRec’s adaptability. We observe similar improvements to GNN and SA models on the Yelp dataset (omitted due to the page limit). We investigate the effects of GTL and OTL on the performance of the model and the best resource-allocation ratio between them. We run experiments of LeapRec with varying the dimensionalities of GTL and OTL while maintaining the ﬂoating-point operations per second (FLOPs) of the model. The results are reported in Table 4. We observe that the best conﬁguration depends on the datasets. In fast dynamic data such as Amazon, the model can perform well without GTL. Conversely, in slower dynamic data (Goodreads, Yelp), GTL becomes more important. To further investigate the difference, we monitored a shift in the ﬁnal representation of popular items in both GTL and OTL, as motivated by the observation in Figure 3. The shift of the representation of item i from time t is deﬁned as: s update steps. We plot the shift monitored on OTL and GTL in Figure 5. For OTL, we observe that the popular group at a certain time had a larger shift than less popular groups. This implies that OTL actively adjusted such a group more than others to adapt to the trends shown in Figure 3. As time went on, the popularity of such a group would decrease or become stable, and OTL also became less active accordingly. In addition, we observe that OTL was more active on Amazon than Yelp because of the difference in the magnitude of temporal dynamics. On the other hand, for GTL, its Table 4: LeapRec with different combinations of GTL and OTL dimensionalities with similar FLOPs in total. Figure 5: Shift in the item representations monitored on OTL and GTL of popular items under each timestamp. Each line represents a group of items that became popular at the same time (corresponding to the same group in Figure 3). activeness was almost constant across time. These demonstrate that GTL holds common and long-term patterns while OTL captures temporal information that varies frequently. We investigate the effect of the number of gradient-based update steps K and time granularity of timestamps on the performance of LeapRec. We conducted further experiments using LeapRec with varying numbers of both and plot the results in Figure 6. We observe that the performance of LeapRec tends to increase with the number of update steps. The performance on Goodreads continued to increase when K > 20, but reached a plateau on Amazon. This possibly can be contributed to the difference in the densities of the datasets. With respect to time granularity, LeapRec performs best with the ﬁnest-grained of one month. This implies its fast adaptability. Unsurprisingly, LeapRec behaves closer to a static model with coarser-grained time granularity. Finally, we analyze the convergence of LeapRec by monitoring its training loss. We plot the training loss across timestamps in Figure 7. We observe that the training loss continued to decrease on all timestamps, suggesting that LeapRec could converge. Figure 7: Training loss of LeapRec in each timestamp on Amazon dataset in different epochs. The horizontal axis in each time window is the gradient-based update steps. In this work, we proposed a novel method using trajectory-based meta-learning to solve the temporal dynamic challenge in modern recommender systems. The empirical results clearly show the advantages of our method compared with a wide range of SOTA methods. Our deep analyses into the temporal dynamics of benchmark datasets also validate our designs of GTL and OTL as they capture long-term and short-term information respectively. We believe the effectiveness of our proposal will initiate new discussions regarding trajectory-based recommendation. We would like to thank Noppayut Sriwatanasakdi and Nontawat Charoenphakdee for helpful discussion. This work was partially supported by JST CREST (Grant Number JPMJCR1687), JSPS Grant-in-Aid for Scientiﬁc Research (Grant Number 17H01785, 21K12042), and the New Energy and Industrial Technology Development Organization (Grant Number JPNP20006). Nuttapong Chairatanakul was supported by MEXT scholarship.