Alexandre Parmentier · Robin Cohen · Xueguang Ma · Gaurav Sahu · Queenie Chen Abstract In this paper, we present an approach for predicting trust links between peers in social media, one that is grounded in the artiﬁcial intelligence area of multiagent trust modeling. In particular, we propose a data-driven multi-faceted trust modeling which incorporates many distinct features for a comprehensive analysis. We focus on demonstrating how clustering of similar users enables a critical new functionality: supporting more personalized, and thus more accurate predictions for users. Illustrated in a trust-aware item recommendation task, we evaluate the proposed framework in the context of a large Yelp dataset. We then discuss how improving the detection of trusted relationships in social media can assist in supporting online users in their battle against the spread of misinformation and rumours, within a social networking environment which has recently exploded in popularity. We conclude with a reﬂection on a particularly vulnerable user base, older adults, in order to illustrate the value of reasoning about groups of users, looking to some future directions for integrating known preferences with insights gained through data analysis. Keywords Online social networks · Misinformation · Multiagent trust modeling · Trust link prediction · Multi-facted trust modeling · Personalization Online sources of information are increasingly relied upon by many. According to yearly studies by the Pew research center, the percentage of American adults using the internet has jumped from 52% in 2000 to 90% in 2019 [3]. In addition to the established institutions that have made the jump from paper and TV to the web, many new blogs, content aggregators, and social networks have become a vital source in the information diet: up to 62% of American adults rely on information shared through social media for their news [35]. A study conducted in the wake of the 2016 American election found that, among American voters, Facebook ranked as the third most relied upon source for news about the election (after Fox News and CNN), far outranking local TV and newspapers and many national stations [14]. It is clear that the power to inﬂuence and inform has shifted drastically away from traditional institutions and into the hands of individuals. While this democratization of information and inﬂuence may strike one as appealing, there are reasons to be concerned about this new paradigm. According to Facebook, throughout the 2016 American election thousands of ads designed to incite panic over gun rights and LGBTQrights were purchased by accounts believed to be funded by the Russian government, some of them speciﬁcally targeting voters in swing districts [42]. Also in 2016, a heavily armed man broke into a neighborhood pizza parlor during business hours and ﬁred shots after having become convinced by an online conspiracy popular on Twitter that the basement of the restaurant was used by the Clintons and other Washington elite to murder and rape children [11]. With an increasing number of individuals garnering attention from provocative posts, it becomes critical to assess whether the content that is shown to users can in fact be trusted. One way to address the existence of untrustworthy information online is to deploy message recommender systems. Rather than showing users a random sampling or chronologically ordered list of the content that has been added to the network since their last visit, artiﬁcial intelligence (AI) systems can be designed to reason about which messages should be shown to which users. The subﬁeld of multiagent trust modeling is especially relevant: the future trustworthiness of an agent can be predicted based on reported past experiences of peers with this agent. While this research has traditionally been applied in contexts such as selecting trusted sellers in e-marketplace environments [48], a few eﬀorts in recent times have focused on using the methods for reasoning about reputable content in online social networks [38]. One promising new direction has been to recognize that multiple features of the data may be relevant, and thus that a proper weighting of these diﬀerent contributing factors, when reasoning about trustworthiness, is important. This is the basic premise of the very novel pursuit known as multifaceted trust modeling [29,9]. In this paper, we ﬁrst of all expand the horizons of multi-faceted trust modeling in order to oﬀer a more comprehensive treatment of the diﬀerent features under consideration. We then introduce a very important new focus on supporting personalized solutions of trust modeling for users. We do this by adding an unsupervised clustering step before trust formulation models are ﬁt, and learning a distinct model for each cluster of users. This approach allows groups of similar users, who potentially express trust in similar ways, to have a model ﬁt for their community, rather than receiving trust predictions that have been smoothed out to apply well to the entire population of the network. As will be explained, we consider trust to be subjective and thus believe it critical to move beyond the standard view of current multiagent systems trust modeling which adopts, for all agents in the network, a “one size ﬁts all” approach to trust prediction; we reveal how recommendations that support personalization can lead to improved predictions. In order to demonstrate the value of our approach, we apply our methods in the context of recommending items to users, making use of a Yelp dataset of reviews which indicates user preferences. We demonstrate the value of integrating multi-faceted trust modeling which explicitly reasons about how to weight the different trust indicators, and of supporting personalized predictions of trust links when recommending content to users. Following our results, we reﬂect further on how to extend our current algorithms and implementation, and then explicitly discuss how the methods proposed in this paper can be used towards helping to moderate online social network content for users. We also illustrate the value of reasoning about certain classes of users such as older adults, to consider solutions which cater to the general needs and preferences of this user base. Trust modeling is a subﬁeld of study within the artiﬁcial intelligence research area of multiagent systems [44]. We consider trustworthiness as deﬁned in [6], namely the quality of being worthy of trust: in essence, the truster shows willingness to take risk based on a belief that the trustee is expected to exhibit reliable behaviour, drawing from an assessment of past experience. Multiagent trust modeling algorithms seek to predict the trustworthiness of another agent based upon ﬁrst hand experience and reports provided by other agents in the environment, sometimes referred to as advisors [48]. The beta reputation system (BRS) proposed by Jøsang and Ismail [23] estimates reputation of selling agents using a probabilistic model. The beta distributions are a family of statistical distribution functions that are characterized by two parameters α and β. The beta probability density function is deﬁned as follows: beta(p|α, β) =Γ (α + β)Γ (α)Γ (β)p(1 − p)(1) where Γ is the gamma function, p ∈ [0, 1] is a probability variable, and α, β > 0. This function shows the relative likelihood of the values for the parameter p, given the ﬁxed parameters α and β. Ratings from peers (used to estimate the reputation of a seller) are binary in this model (1 or 0, to represent that the advisor considers the seller to be satisfactory or dissatisfactory in a transaction). Individual ratings received are combined by simply accumulating the number of ratings supporting the conclusion that the seller has good reputation and the number of ratings supporting the conclusion that the seller has bad reputation. The prior distribution of the parameter p is assumed to be the uniform beta probability density function with α = 1 and β = 1. The posteriori distribution of p is the beta probability density function after observing α − 1 ratings of 1 and β − 1 ratings of 0. The reputation of the seller s can then be represented by the probability expectation value of the beta distribution, which is the most likely frequency value, used to predict whether the seller will act honestly in the future. The formalization of this is given as follows: T r(s) = E(p) =αα + β(2) Zhang and Cohen [48] suggest a personalized trust model (PTM) to determine whom to listen to among a network of buyers and sellers in thee-marketplace domain. In particular, they address whether a buyer, b, should purchase a product from a seller, s, based on a combination of global advice from other buyers (i.e., advisors, a), and b’s own local past experiences with s. The PTM global metric is further broken down to combine private and public trust estimates of advisors. The intuition is that b may have radically diﬀerent expectations or preferences regarding s’s product than a, and so b should have some notion of how much to trust a. To the extent that b relies on past common experiences to evaluate a’s trustworthiness, b uses a private trust metric to incorporate a’s recommendation. To the extent that b relies on a’s similarity to the global rating of various sellers (i.e., how fair are a’s ratings), b uses a public trust metric to incorporate a’s recommendation. In particular, b’s private reputation according to a, R(a, b), is modeled by the expectation of a beta distribution where α is the number of times a and b have agreed in the past about the reputation of other agents, and β corresponds to how many times they have disagreed. The public reputation of b, R(b), is again modeled by the expectation of a beta distribution, where α corresponds to the number of times b’s advice has agreed with majority opinion, and β the number of times it has not. The ﬁnal reputation of b fora is then a linear combination of the private and public reputation of b, weighted by a factor w which reﬂects how much comparable experience a has had with b (i.e. the number of agents commonly rated agents). Multi-faceted trust modeling (MFTM) is a ﬂexible and data driven approach to trust modeling. Inspired by work in the social sciences which have outlined the numerous variables which inﬂuence the formation of trust relationships [30], MFTM incorporates arbitrarily many indicators of trustworthiness into a single (optionally context-dependent) trustworthiness score. Operationalizing this core idea for trust and social tie prediction has been proposed by multiple researchers (e.g. [29,9,26,13,22]). As is evident in these works, there is little agreement over whether this technique should be called multi-dimensional, multi-faceted or composite trust modeling, and this confusion has likely led to some diﬃculty in coordinating eﬀorts in this research direction. We use the term “multi-faceted”, in keeping with the most recent works. The deﬁning feature of an MFTM is a customizable vector of trust indicators, where each indicator is a real number based on two agents: A “trust indicator” can be thought of as a piece of evidence for or against trusting an agent under a particular context. For example, ψ(a, a) = friendCount(a) may be relevant to assessing the reputation of ain a domain where only popular and reputable agents can accrue large numbers of friends. The indicators ψmust be computable given A and E (the set of agents and their attributes and the history of events). One important feature of MFTM is its ﬂexibility to tune its parameters to diﬀerent domains of use. The customizability of MFTM is highly attractive for application to social networks, as it is rare to ﬁnd explicit statements of trust encoded into the feature set of online environments. Instead, an arbitrary number of “imperfect” indicators of trustworthiness, such as popularity, friendship, reputation, interaction history, preference similarity and institutional credibility can be considered as each contributing to a ﬁnal tally of trustworthiness. Clearly, the underlying assumption of this model is that the existence of trustworthiness between two agents can be predicted based on a comparison of the attributes and behaviors of those agents. The consideration of multiple indicators of trust can be viewed as an emulation of the way in which humans consider multiple sources of evidence when deciding to trust or not [30]. For example, consider the problem of choosing an auto mechanic shortly after having moved to a new town. In this case, one has no interaction history with any nearby mechanics and must weigh available evidence in order to choose which mechanic to trust. In a simple case, one might only consider two pieces of evidence towards or against a mechanic: has any colleague recommended them (ψ), and have their prices been posted clearly online (ψ). In this case both indicators are binary, and it seems likely that the mechanic awho satisﬁes both indicators, Ψ(a,a) = h1, 1i, will be a good candidate to trust. (Note that we usefor the trustor andfor the trustee at times in our discussion below, for additional clarity). In order to predict trustworthiness, the relevance of each indicator can be learned using an oﬀ-the-shelf machine learning technique given A and E to train with. To do this, a trust link is chosen as a target of prediction : y (e.g. explicit statements of trust/friendship, high degrees of preference alignment). Then, given the set of existing implicit/explicit trust links, a machine learning model ﬁts a classiﬁerˆf to the function that determines how trust indicators are related to trust links f : Ψ (a, a) → y. For example, in the case where logistic regression is used, y will be binary and we have: where θ is the vector of weights learned through the logistic regression process and Tis trustworthiness under context c. (We believe that whether someone is trusted may truly vary according to the context; for the remainder of the paper, we drop the context variable c in the equations). We wish to emphasize that while logistic regression is an elegant and natural choice with some popularity in the literature (e.g. [9]), it is by no means the only choice. The ability to deﬁne custom indicators appropriate to whichever application domain one is pursuing oﬀers a tremendous amount of ﬂexibility. As we will show in Section 3, both highly generic as well as applicationspeciﬁc trust indicators can be deﬁned. Finally, we wish to emphasize how MFTM can be seen as a generalization of a number of existing trust modeling techniques. Primarily this is because many trust modeling techniques do in fact consider multiple sources of evidence, but they weigh or combine this evidence in a non-data-driven manner. For example, the beta reputation system can be conﬁgured so that old advice is considered less important than new advice. However, a method for specifying how much more important newer advice should be treated compared to older advice is not speciﬁed. A similar situation occurs in the Personalized Trust Model [48], where private and public reputation are weighed against each other. The weighting function chosen has a good statistical justiﬁcation, but ultimately does not specify how error bounds should be chosen, and thus how exactly to weigh personal and private reputation. MFTM can consider arbitrarily many sources of information, and learns the weights for them directly from data. For example, PTM could be roughly replicated by treating private and public reputation as trust indicators, and learning an appropriate function for combining them. Another example of how MFTM is data driven is that it does not specify which distributions should be used to model beliefs. For example, both the foundational trust modeling Beta Reputation System (BRS) [23] and PTM rely heavily on the beta distribution. By allowing arbitrary machine learning methods to combine many forms of trust evidence into prediction, MFTM loses Bayesian rigor, but gains a large degree of ﬂexibility and generalizability. Trust modeling in the context of recommender systems has been examined by several researchers, dating back to the seminal paper of O’Donovan and Smyth [31]. More recent work has examined such issues as addressing cold start recommendation using trust modeling [17] or examining how to speed up trust-aware recommendation through improvements from matrix factorization [15]. In this paper, trust-aware recommendation arises as a central element of the validation of our proposed framework. To explain: one of the recurring challenges in the development of trust models is ﬁnding grounds for the validation of the accuracy of the models [6]. Trust models aim to predict new trust links but independent agents may choose to follow or ignore these predictions. It is therefore diﬃcult to truly evaluate the eﬀectiveness of models without deploying a system on an active service and measuring the real eﬀects of trust link prediction. As this is expensive and requires the cooperation of an active social network service, many models validate their eﬀectiveness on data generated by an agent simulation instead (e.g. [23,48,36,4]). While this is a useful approach for contrasting the eﬀectiveness of various models and gives the researcher a large amount of control for simulating speciﬁc types of agent behavior, it clearly adds a layer of ambiguity between the reported eﬀectiveness of the model and its potential for real world application. In some cases, merely changing simulation parameters can defeat systems that had performed well on the simulations their creators had designed [24]. A rising trend in this ﬁeld is to validate models by applying their predictions to a recommendation task (e.g. [9,29]): that is, using the trust model to predict novel trust links,ˆΓ , in a multiagent system (MAS), then feeding those predicted links into a trust-aware item recommendation system. These trust-aware recommender systems incorporate both user-item rating behavior and user-user social/trust connections to better recommend items by leveraging the fact that social/trust connections exert inﬂuence on the preferences of agents (e.g. you are more likely to watch/enjoy a ﬁlm a trusted friend recommends). The logic of this two part process is that when a trust model is able to accurately predict trust links in the context of peer to peer item recommendation, then the resulting accuracy of the recommender system trained with those links will improve. For the validation of the model we present in Section 3, we implement a task-aware item recommendation task. As will be explained in more detail in Section 3.2.3 we introduce two distinct trust-aware recommendation systems, TrustMF [46] and MTR [29]. MTR belongs to a class of recommenders based on k-nearest neighbours [19]. This approach requires a good selection of the value of k and an appropriate distance metric to determine closeness. In section 3.2.3 we provide more insights into how these were chosen for our experimentation with MTR. TrustMF belongs to a class of systems known as latent factor models. To be clearer about how these systems operate, we provide below additional explanation. As will be seen, this method works well with data-driven trust recommendation, in seeking to leverage the most relevant factors of the users. Latent factor models for recommendation are a popular approach to collaborative ﬁltering based recommendation derived from matrix factorization technique called Singular Value Decomposition (SVD) [39]. Speciﬁcally, by applying an SVD technique, a m × n matrix R of rank ` can be decomposed into three matrices of rank k ≤ `: R = Q · S · V , where Q is m × k, S is k × k and V is k × n. While S has a number of interesting mathematical properties, in recommender system literature it is frequently ignored by substituting U = Q · S. SVD can be applied to recommender systems when R is the user-item matrix of review scores such that r is the rating user i gave to item j. Naturally, this matrix is sparse - in practice, the vast majority of the entries in R are unknown, as most users have only given feedback on a small number of items. While SVD cannot be applied directly to a sparse matrix like R, we can imagine that the deﬁned entries in R comprise a subset of the entries in the (unknown) dense matrix Rwhere every user has expressed an opinion on every item. By SVD, R, is guaranteed to have a minimal rank-k decomposition. This line of reasoning serves as inspiration for the following loss function [25]: where uis a length k vector corresponding to user i and vis a length k vector corresponding to item j, and κ is the set of indices (i, j) such that ris deﬁned in R. λ simply controls the strength of the regularization penalty. By optimizing Equation 6, one constructs matricesˆU andˆV , where the i’th row ofˆU is uand the j’th column ofˆV is v. Then,ˆR =ˆU ·ˆV is a matrix where the distance between deﬁned members of R and their corresponding entries inˆR has been minimized. At the same time, estimates for every undeﬁned entry in R are present inˆR. A user i can then be recommended items where ris undeﬁned (the user has not yet rated the item) but ˆris high (the user is predicted to rate the item highly). This approach is particularly amenable to the recommendation task, as it makes the optimization far more tractable. In particular, rather than grappling with the O(mn) user-item ratings directly, the O(k(m + n)) values inˆU andˆV are all that need to be optimized. This oﬀers considerable performance improvements when k << min(m, n) (in many applications there may be millions of users and items, but 10 ≤ k ≤ 100 factors are suﬃcient for good modeling of the system [25]). Koren et al. [25] describe the intuition behind this procedure in illuminating way. For the task of recommending movies, we can imagine that each movie can be measured on k dimensions. Each user will have some level of preference for various dimensions of a movie. Rather than explicitly deﬁning these k dimensions and laboriously categorizing each movie in this way, the SVD recommendation procedure infers factors directly from rating patterns. These so-called “latent factors” are essentially learned via error minimization over available data regarding users and movies. TrustMF is used to test the accuracy of our trust link prediction algorithm in Section 3. This system can be roughly characterized as combining the optimization described above with an optimization over a matrix of user-user trust links that shares a latent space with the user-item matrix. Conceptually, a user’s preference for items shares a space with that user’s preferences for trusting other users. Thus, the presence of trust links exerts an inﬂuence over the latent factors that are discovered, incorporating social trust into the recommendation process. The ability to incorporate social trust data makes this recommender system “trust-aware”. In this section we describe an experiment which explores the inﬂuence of personalization and context on a multi-faceted trust model. We aim to demonstrate the beneﬁt of learning trust formulation behaviours at Fig. 1: Example Yelp review with rating the level of clusters of users, rather than on the entire population of agents. We argue that this increase in resolution constitutes a form of personalization (albeit, performed at a group level rather than at an individual level). In addition, we explore the impact of considering diﬀering contexts of trust by testing the eﬀect of predicting two types of trust links. At the heart of our solution is an eﬀort to predict novel trust links in a social network by using machine learning methods to determine how to weight feature importance, and to approximate trust formulation procedures among groups of similar agents. Our approach makes use of the ﬂexibility of MFTM, which we demonstrate by combining features drawn from two existing proposals with our own novel features. Evaluation is performed by measuring the error rates on a recommendation task that incorporates trust information. This is performed on a data set collected from Yelp, a content rating site with social network features. On Yelp, users can indicate binary social trust towards other users (friends) and submit ratings for products, businesses or websites (taken together, and following the trend in recommender systems literature, these entities are called “items”) that they have experienced, indicating their satisfaction with that item. These ratings are integers in the range [1, 5], illustrated as stars, where higher numbers indicate a stronger recommendation. An example 5-star review for Schwartz’s Deli is presented in Figure 1. We used this data set particularly because it is amenable to validation of trust model eﬀectiveness via a downstream item recommendation task. Yelp was in fact used by [29], one of the central multi-faceted trust modeling papers which motivated our work. As will be seen in our experimentation below, trust links for the Yelp environment will be predicted both on the basis of friendship relations and through the discovery of similar rating behaviour. The rationale for testing the eﬀect of personalization is simple: we expect trust formulation procedures to vary from person to person, therefore learning approximations of trust formulation procedures may be more accurate on a more personalized scale. The inherent subjectivity of trust has important implication from a machine learning perspective. In particular, it implies that trust predictors trained on large data sets representing the behavior of many individuals are not necessarily more correct than those trained on smaller groups. While the predictor trained in the former case will likely have a higher accuracy across the broad population, it is essentially learning the “average” trust formulation procedure, potentially disadvantaging agent’s whose preferences are not aligned with the population at large. We note that learning distinct predictors based on the data associated with clusters of users was suggested by Fang et al. [9], but was not pursued. While our approach is useful for capturing the variance of trust formulation in smaller groups, it does not attempt to learn the preferences of individual agents. We will discuss possible avenues for truly individual personalization of trust modeling and other approaches in Section 5.2.1. In this work we test the eﬀect of personalization or trust link prediction on an item recommendation task. This is the case where agent aencourages agent ato invest resources into accessing or consuming item k based on their own experience with it. In particular, we test whether clustering agents and learning trust link predictors on the basis of clusters of similar agents, as opposed to learning a single trust link predictor for the entire population of agents, can increase the accuracy of trust-aware item recommendation systems. We also test the eﬀect of altering the type of trust link prediction by either a) attempting to predict the presence of an explicit friend/trust link or b) predicting positive correlation in item review scores (i.e. two agents having scores that are the same). Our ﬁnal analysis will report the recommendation accuracy of 7 conﬁgurations, where each conﬁguration Fig. 3: Reviews submitted and received counts uses an identical set of agents and recommendation procedures, but a diﬀerent procedure for predicting the trust links between agents. Each conﬁguration is given a name reﬂecting which (if any) type of clustering was performed, and which type of trust link was predicted. The complete list is presented in Table 1. The entire procedure can be described sequentially, as follows. Each step will be brieﬂy explained, noting its inputs and outputs, then will be more carefully considered in subsections below. When the ﬁrst step is skipped, no personalization is performed. When the second step is skipped, no trust link prediction is performed. The rationale of skipping certain steps is to compare and contrast the eﬀect applying these steps has on the ﬁnal accuracy of the recommendation task. The overview presented below assists readers in clearly understanding the interplay between the central processes of our approach: clustering, trust link prediction, and recommendation evaluation, before the details of our methodology are provided. – Clustering – Input: All agents A and an agent-agent similarity matrix, S. – Output: An assignment of every agent to a cluster, C – Description: Partition the agents into groups of highly similar agents. We used social circle overlap (Jaccard Similarity) and review score correlation (Pearson Correlation Coeﬃcient) as similarity measures. We developed two clustering methods for this step. – Trust Link Prediction – Input: Clusters of agents, C, and trust indicator function Ψ(a, b). – Output: A matrix of trust link predictions,ˆΓ – Description: For each cluster cof agents a logistic regression learns a distinct MFTM trust prediction function for that cluster. We experimented with predicting friendship links and positive review score correlation. Output a |A| × |A| matrix,ˆΓ , where,ˆΓ= 1 if the classiﬁer for the i’th agent’s cluster predicts a trust link between agents i and j and 0 otherwise. – Recommendation Evaluation – Input: Agent-item rating matrix R, trust link prediction matrix,ˆΓ . – Output: A agent-item matrix of predicted review scores,ˆR. – Description: Given reviews present in the original data set and the predictions from the previous step train a trust-aware recommender system to predict review scores. After training, we evaluate the correctness of the recommender on a reserved testing set using Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) metrics. We note that truly individual personalization is diﬃcult to test. This is because explicit elicitation of factors which inﬂuence trust on a personal level is rare on most services, and most agents have not participated in enough activity in order to accurately measure the patterns of their preferences implicit in their behavior. Therefore, we focus on clusters of similar agents rather than considering each agent distinctly. We posit that if personalization at this level of granularity is suﬃcient to increase the accuracy of our trust models, then we will have found evidence that some level of personalization is indeed useful for the trust modeling task, and will have motivated further research in the area. Clustering procedures generally rely on the deﬁnition of a distance or closeness (alternatively, similarity) metric between elements to be clustered [37]. In this work, we tested two separate similarity functions. Speciﬁcally, we tested clustering agents on the basis of preference similarity and social circle similarity, as deﬁned in Equations 7 and 8 respectively: socialSim(a, a) =|friends(a) ∩ friends(a)||friends(a) ∪ friends(a)|(8) where Ris the set of items which both agents a and ahave reviewed, ris the rating given by agent ato item k, ¯ris the average rating for item k, and friends(a) is the set of agents ahas entered into mutual friendship with. Put otherwise, we clustered agents on the basis of the Pearson Correlation Coeﬃcient of scores they had given in reviews to items and on the basis of the Jaccard Similarity of their friend groups. The choice of both metrics was motivated by a desire to extract metrics from our data which: 1. Are relatively generic (i.e. could likely be applied to similar data sets). 2. Could plausibly be argued to constitute a basis for determining which agents are similar enough that we might expect their trust formulation procedures to also be similar. Since our context of trust is based on recommending items, we argue that both criteria are met. For 1., we argue it is reasonable to assume that on any online service with an item review and recommendation component, it will be possible to calculate Equation 7. Similarly, it is reasonable to assume that Equation 8 will often be computable on these services, as it is widely believed that friend relationships are a useful tool for expressing preference alignment among agents in such domains [18]. For 2., we argue that socialSimdirectly satisﬁes this criteria by its deﬁnition, as socialSimmeasures the observed similarity in the output of a trust-like relationship formation procedure (friendship). For prefSim, we argue that if two agents a and b have demonstrated a strong preference for similar items, then it is reasonable to conclude that their procedures for choosing who to trust for new recommendations should be similar. Thus, it is reasonable to cluster them under this context. While we have argued that these similarity metrics are relevant for our goals, they do present challenges as metrics for clustering algorithms. Speciﬁcally: – Both metrics violate the triangle inequality. – Both metrics can sometimes be undeﬁned (when the denominator is 0). As many clustering algorithms are deﬁned over Euclidean spaces, these caveats represent signiﬁcant restrictions of possible approaches. It is somewhat helpful that the second caveat can be addressed by simply substituting default values in the case where division by zero would occur. Accordingly, we used the following metrics in our ﬁnal procedure: prefSim(a, a) = 1 if socialSim(a, a) = 0 if |friends(a) ∪ friends(a)| = 0; Note that 0 ≤ pref Sim(a, a) ≤ 2, where values below 1 indicate a negative correlation. Therefore, the most appropriate default value is 1. Similarly, 0 ≤ socialSim(a, a) ≤ 1, where values near 0 indicate very few common friends between aand a, thus, the most appropriate default value when neither agent has any friends is 0. In addition, in Equation 9 we have also substituted a default value when |R| < 4. This is because correlation tests produce noisy results with small data sets, making it prudent to choose a cutoﬀ point under which no correlation metrics are considered. Meanwhile, if this cutoﬀ is too high, then potentially useful data is ignored to avoid error. We chose the cutoﬀ at 4 arbitrarily. While this at least leaves the similarity functions well deﬁned, it also creates a situation where the vast number of pairs of agents have a default distance between them, as any two randomly picked agents in a large enough environment will be unlikely to have any interaction history. This is a potential issue as it may causes clusters to appear signiﬁcantly less cohesive than they actually are, e.g. in the case where agents a and b have “default” distance between them, but are both close to agent c. In this case, a and b should likely be in the same cluster as c, even if they don’t themselves appear to share any relationship. In addition to the challenges described above, our clustering task had the additional goal of ﬁnding relatively large clusters. This is because our “downstream” goal was to learn personalized classiﬁers for each cluster of agents. If clusters are too small, then the accuracy of classiﬁers will suﬀer. Given these goals and constraints, our ﬁrst attempt at a clustering was a simple, non-iterative greedy algorithm, shown in Algorithm 1. This algorithm takes as input the set of agents to be clustered, A, the similarity matrix between agents S (where S= sim(a, a) for some similarity function) and the desired size of clusters In the above, f reeAgents(A, C) returns the set of agents not yet assigned to a cluster in C (unassigned agents), pickCentroid(A, C, S) returns the unassigned agent with the greatest mean similarity to all other agents, and pickN ext(c, A, C, S) returns the unassigned agent with the greatest mean similarity to the agents in c. Roughly, Algorithm 1 partitions the set of agents into at least b|A|/ηc clusters of size η. It does this by picking the most central unassigned agent as the core of a new cluster c, then adding agents to that cluster in order of greatest mean similarity to agents already in cluster cuntil |c| = η. The process repeats for c, except only agents not already assigned to a cluster are considered. This continues until less than η agents remain unassigned, at which point all unassigned agents are added to a ﬁnal cluster of unspeciﬁed size. Clearly this algorithm is quite simple, but it is appropriate for the constraints outlined above. Firstly, all clusters of agents except for one will have a guaranteed minimum size η, allowing control over the minimum training data size for the downstream prediction task. More importantly, it handles the non-Euclidean nature of the data by using the mean distance of all points in a cluster as a similarity metric, rather than a geometric center. We improved this algorithm by transforming it into an iterative version listed below (Algorithm 2). In the above, greedilyP artition(A, C, S) assigns each agent to a cluster using the procedure outlined above in Algorithm 1. computeClusterSims(A, C, S) computes a new similarity matrix, S, between agents and clusters, where Sis the average similarity between agent i and all agents in the j’th cluster (other than themselves): S=1|c| − (a∈ c)(i 6= k)S where (cond) is the function which is equal to 1 when cond is true and 0 otherwise. assignT oN earestCluster(a, S computes a modiﬁcation of the current set of clusters C by moving agent ato the cluster cthat maximizes S, that is, the cluster for which they have the highest average similarity with other cluster members, (with ties broken randomly). This process repeats for a predetermined maximum number of iterations m. This process is much closer to classic k-means clustering, again with the modiﬁcation that distances between clusters and points must be calculated on the basis of mean distances rather than distances to the cluster’s geometric center. In addition, rather than picking random points to serve as initial cluster centers, the initial clusters are determined by a greedy partitioning method. These modiﬁcations result in an algorithm that, in our experiments, tended to produce relatively large and cohesive clusters. Both Algorithm 1 and 2 require a parameter used to control the number of clusters (η and k respectively). When performing our experiments, we determined values for these parameters by running the clustering step multiple times over a range of parameters with a relatively low maximum iteration setting, then choosing the best performing parameter to proceed with. Data set selection and ﬁltering: We pause to provide further details on the dataset used within our experimentation, so that our continued description of the clustering process can be deepened by discussing it within this context. Yelp is an ideal candidate to explore relationships between agents against a validation of recommended content. It is a product review site and social network of crowd-sourced reviews targeting brick-and-mortar businesses. In addition to writing reviews of services, users of the site can form mutual friendships and follow other users in order to receive the recommendations of these trusted users ﬁrst. Data describing users, reviews and businesses is made public by Yelp on a regular basis. The full data set from 2019 contained descriptions of 1,637,138 users, 192,609 businesses, and 6,685,900 reviews. We ﬁltered the data set both to reduce the massive amount of data and to narrow down the context of trust in focus. Speciﬁcally, we only considered users who had reviewed at least 20 businesses that were tagged as restaurants. This narrows the context of trust from “recommending businesses or services” to “recommending restaurants” and reduces the data set down to 30,721 , C)users and 4,432,064 reviews concerning 74,560 businesses. This ﬁltering procedure was inspired by [29]. Statistics from the ﬁltered data set are presented in the Table 2 and in the histograms in Figures 2 and 3. As can be seen, there is a relatively well spread out distribution of scores given to items, centered around 4 stars. This leads to a relatively diﬃcult prediction task, as predicting the median review score is only correct in 35% of cases. Counts of friends and reviews are plotted on a logarithmic scale, and show a “long tail” distribution that is common in online phenomenon [40]. Clustering methods used with the data set: Against the backdrop of the Yelp dataset, in Figures 4 and 5 we illustrate the performance of clustering techniques as the number of clusters (k) is altered. We measure cluster cohesiveness using two metrics, mean-intra cluster distance and silhouette score. In the below, dist(i, j) is the appropriate distance measure for the similarity function chosen, i.e. if sim(i, j) is high when i and j are similar, then dist(i, j) is low when i and j are similar. Mean intra cluster distance is deﬁned as follows: meanintra(C) =1|C|dist(i, j)|c|(11) That is, the average distance between all elements in a cluster and the other elements in that cluster, averaged over all clusters. Silhouette score s(j) for a single clustered point j which has been assigned to cluster cis deﬁned as follows: a(j) =1|c| − 1 b(j) = min1c s(j) =if|c| > 1(14) That is, a(j) is the average distance point j has to other points in its cluster. b(j) is the minimum distance from j to to any other point not in the same cluster as j. s(j) is the silhouette score for the point j. When a point exists which is not in the same cluster as j but is closer to j than the average point in j’s cluster, then the score is negative. When the closest point to j outside of its cluster is farther away than the average distance of point in j’s cluster, then the score is positive. The silhouette score for a set of clusters C is calculated by taking the average of a random sample of points from diﬀerent clusters. Both metrics capture a sense of the cohesiveness of a set of clusters, and can be used to judge the relative merits of diﬀerent clustering schemes and parameter settings. While both metrics are interesting, there are a few caveats to consider. First, for this data set and clustering algorithm, it is expected that intra-cluster distance will decrease as cluster count increases. This is shown to be true in the results reported. Thus, this metric is better suited for showing the diﬀerence in performance between clustering methods than it is for choosing a value of k. The silhouette metric does a better job of outlining the tradeoﬀ for k values, as it will punish a method for assigning two close points to separate clusters. Therefore, a value of k that maximizes the silhouette score over a range is a more appropriate guide to choosing a value of k. Noticeably, performance on both scores is low in an absolute sense. Silhouette score ranges from [−1, 1], where positive scores are good and negative bad. Our clustering algorithms achieve scores in the range of [−0.06, 0.02] - a tiny portion of the possible range near 0. Similarly, the scores for intra cluster dist should range from [0, 2], where small scores are good, and our algorithms ﬁnd scores in the [0.95, 0.99] range. Why is this the case? The answer is related to the sparsity of deﬁned links between agents when all |A|possible pairs of agents are considered. As most agents do not know most other agents, and there is no basis for determining the similarity (distance) between them, in the vast majority of cases sim(a, a) is equal to a default value for randomly picked i and j. Therefore, as both metrics take some kind of average of the distances between pairs of agents, the metrics will always be close to the default distance. Should these low absolute scores deter us from this method? We argue they should not. First, as we have brieﬂy argued above, the nature of this data implies that average measures of cluster cohesiveness will always be close to a default. Secondly, the trend lines show that appropriate choice of k and cluster methodology can eﬀect the sign and magnitude of silhouette scores in consistent ways - for example, when k goes above 60 in Figure 4 (b). We take this as an indication that positive results are not simply a coincidence. Our trust link prediction procedure was intended to combine what we perceived to be the best traits of the work of Fang et al. [9] and Mauro et al. [29]. Both works tested the eﬀects of predicting trust links using Multi Faceted Trust Modeling (MFTM) on an item recommendation task. In [29], a relatively large number of domain speciﬁc trust indicators are proposed for the Yelp data set; however, the importance of each trust indicator is not learned in a data driven way. Instead, they selectively enable and disable indicators for each performance test, combining their values by simply taking the average of the enabled indicators. In [9], a relatively small number of generic trust indicators are proposed for an Epinions data set. The importance of each indicator is learned via logistic regression and performance is tested under a number of diﬀerent sparsity conditions. We will compare our work more closely to the works of Fang et al. and Mauro et al. in Section 4. In our work, we combined indicators proposed in both work and adopted a data-driven indicator importance weighting procedure. Trust indicator list: To quickly restate the goals of MFTM, we wish to deﬁne a vector of trust indicators over every pair of agents, Ψ(a,a) then use machine learning to approximate the function f : Ψ(a,a) → y, where y is some type of trust link. In Table 3 we have listed all trust indicators that we calculated for Yelp data. When an indicator was proposed in the works of either Fang et al. [9] or Mauro et al. [29], we have indicated this in the last column (although some were also adjusted by us, as clariﬁed below). Some indicators are deﬁned speciﬁcally for pairs of agents (e.g. the similarity of rating behavior for two agents), while others are deﬁned on the basis of a single agent. In Yelp data, the only explicit trust link present is mutual friendship. Here, we describe some of these indicators in full detail, in order to illustrate some less obvious indicators. Complete descriptions of the indicators not described here are available in [9] and [29]. Benevolence: Already described in Equation 9. In [9], ¯r, the average rating given to item j was replaced with ¯r, the average score agent i gave to items. We made this replacement as a common rating behavior in the Yelp data set was for an agent to only submit 5 star reviews, causing frequent divisions by zero. By comparing to the global average rating of an item, this behavior is no longer an issue. Intuitively, when benevolence(a, a) is high, agents aand amay be inclined to trust each other’s reviews, as they have reviewed items similarly in the past. Competence: A threshold value  is used to determine how often the trustee’s ratings where “close enough” to the ratings of other agents who had also rated those items to be considered “correct”. competence(a) =P|I| Where Ris all the items the i’th agent has rated and Iis the set of all agents who have rated item j and ris the rating agent i gave to item j. Competence is high when an agent’s rating behavior is similar to the plurality of agents. Since ratings on Yelp use a 5star scale, we used the threshold value 0.5. Intuitively, when competence(a) is high, amay be trustworthy for agents who consider agreement with popular consensus to be important. Predictability: A threshold value θ is used to determine how often a trustee’s preferences are consistently higher, lower, or similar to the truster’s : predictability(a, a) =max(n, n, n) − min(n, n, n)|R| where n, n, and ncount how many times the trustee rated an item about the same as the truster, lower than the truster, and higher than the truster respectively. Accordingly, predictability is lowest when n= n= n, meaning the trustee rates items better, worse, and equivalent to the truster in equal amounts. This would mean there isn’t a justiﬁcation to expect that the trustee has a bias in any particular direction, relative to the truster. Similar to Competence, we used a threshold value of 0.5. Intuitively, predictability(a, a) may be important to adeciding whether or not to trust a, as it is useful to know whether a’s ratings have a consistent bias compared to a. Visibility: The relative popularity of agent, taking into consideration how much content the agent has produced and the popularity of the most popular agent. visibility(a) =appr(i)max(appr(j) × contr(i))(20) Where appr(i) is the number of public “appreciations” an agent has received from other agents (e.g. likes) and contr(i) is the number of contributions an agent has made (e.g. posts, reviews). Intuitively, when visibility(a) is high, amay be trustworthy to agents who consider consistent popularity important. Some of the indicators listed in Table 3 were developed by us. For example, we normalized a number of the indicators proposed by [29] by dividing by how many years the target user had been on the site. This is useful for giving newer users a chance to compete with older users on certain attributes (e.g. how many “fans” they’ve accrued). We also computed the Jaccard similarity between users with respect to the sets of items they had reviewed and the categories of items they had reviewed, reasoning that these indicators would help to outline the case when users has similar areas of interest. Finally, we checked to see if pairs of users were friends of friends, a potentially useful feature for integrating trust transitivity into reasoning. When computing these trust indicators, it appears necessary to consider all ordered pairs of agents in the environment, as trust is directed and can occur between any two agents. This presents a signiﬁcant computational bound on the number of agents that can be considered. Discussion of this issue, and our approach to minimizing this impact is presented in Appendix A. In brief, only pairs of agents where there is signiﬁcant evidence that the pair have an overlap in interests / social circle are actually considered as candidates for novel trust link prediction. Classiﬁcation process: The trust indicators listed in Table 3 were used to predict two types of trust links 1) whether the truster had explicitly expressed trust in the trustee (friendship), and 2) whether the truster and trustee had a positive correlation in review scores. Note, in the case where expressed trust was the target of prediction, review score correlation was considered as evidence (e.g. included in Ψ (a, a)) and vice versa, although the target of prediction was obviously not considered as evidence. Following the example set by Fang et al [9], we use logistic regression to learn functions that predict the presence of statistically likely trust links based on the vector of trust indicators computed between pairs of agents. We refer to these functions as “trust link classiﬁers”, as once learned, they classify each ordered pair of agents as either being linked by trust (the former should trust the latter) or not. We used the SAGA solver logistic regression classiﬁer included in the sklearn Python package [34] to learn these functions. In the case where no clustering was performed, a single classiﬁer was learned for all agents. When clustering was performed, a classiﬁer was learned for each cluster of agents. Thus, each cluster speciﬁc classiﬁer learns how the agents in the cluster form trust links in their role as trusters. This makes obvious a substantial tradeoﬀ to this approach to personalization: the more clusters are found (increasing cluster cohesiveness up to a point), the less data available to train machinelearning classiﬁers (decreasing prediction accuracy). We will discuss other potential approaches to personalization in Section 4. For our purposes, we only learned cluster speciﬁc classiﬁers for clusters that had at least 100 agents and at least 1000 positive outgoing trust links. When a cluster failed to meet these standards, it was assigned a generic classiﬁer, trained on examples from a random sample of users across all clusters. We implemented this strategy in order to avoid training wildly inaccurate classiﬁers. When training the clusterspeciﬁc classiﬁers, all available data relevant to each cluster was used for training. This is because we are not directly interested in how well each classiﬁer is able to ﬁt each cluster, only on whether this personalization process increases the accuracy of the downstream recommendation task. Therefore, it is not necessary to reserve a test/validation set for any trust link classiﬁer. Note, at this point, test sets of ratings data are already reserved for the recommendation task outlined in the next section. A common problem in link prediction generally is the large class imbalance between positive and negative examples. Put simply, the number of negative examples of trust links in a community of agents grows with O(|A|), while positive examples have much more conservative linear growth. This can be either because humans have an upper limit on how many others they will trust, or, like on Yelp, technical limitations are imposed on the number of allowed friends. Compounding the problem is that there are two kinds of negative examples, which are often diﬃcult to distinguish between. On the one hand, agents aand amay not be friends simply because they have never met. On the other hand, they may have interacted and prefer not to do so again in the future. Therefore, it is necessary to devise a strategy for training classiﬁers to deal with this imbalance and ambiguity. One popular method, which we have used here, is to construct balanced training sets by including a random negative link for every positive link. This method has the advantage of requiring no further tinkering to classiﬁers in order to accommodate a class imbalance. The ambiguity in negative links is ignored as best as possible by simply sampling negative links randomly. After training classiﬁers for each cluster, trust link prediction is performed by feeding the trust indicator vector Ψ(a, a) to the appropriate classiﬁer for the cluster of agent a. Ultimately a matrix of trust link predictionsˆΓ is produced, whereˆΓ= 1 if the classiﬁer for the i’th agent’s cluster predicts a trust link from ato awith probability greater than 0.5. It is noted once again that predictions were only made for pairs of agents that were considered to be in the same neighborhood, as described in Appendix A. Two trust-aware recommender systems were used to measure the accuracy of the trust links predicted in the previous step: TrustMF and MTR. Two systems were evaluated in order to reduce the risk (e.g. of using a a ﬂawed implementation that skews results). TrustMF leverages matrix factorization and gradient descent to optimize predictions of user-item ratings. We used the Librec implementation of TrustMF for our experiments [16]. MTR is a trust aware modiﬁcation of a similarity based KNN recommendation model proposed by Mauro et al. [29]. Under this system, the predicted rating for an agent i for an item j is: where ¯ris the mean score agent i has given in ratings, N(i) is the set of the top κ most inﬂuential agents on i who have also rated item j, and infis the inﬂuence agent k’s recommendation exerts on agent i: a linear combination of the similarity between k and i’s past rating behaviour and a trust metric. In our case, inf is the probability that k is trustworthy for i according to the predictions of the trust model in the previous step inf= β · σ(ik) + (1 − β) ·ˆΓ(22) where β is simply a parameter for controlling the weight of trust modeling on the recommendation process. When ˆΓwas undeﬁned (e.g. in the case where i and k are not in the same neighborhood, see Appendix A), a value of 0 was substituted. We modiﬁed an implementation of a KNN based recommender system distributed in the Surprise library [20] to test this method. Accuracy of recommendation was measured by Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE): where R andˆR are a set of real agent-item ratings and predicted agent-item ratings respectively, and ris the rating given by user i to item j. MAE simply captures the average unsigned error in predictions across all ratings, while RMSE is more heavily penalized for gratuitously erroneous ratings and less penalized for very nearly correct ratings. These measures can be analogized to the mean and variance of a distribution over prediction error. As measures of error, we prefer recommendations that minimize these measures. Thus for all of the following graphs lower values are better. The sensitivity of RMSE to outliers is a useful property for this application, as grossly inaccurate predictions can erode user trust in future recommendations. The latent factors model adopted by TrustMF was outlined already in Section 2.3.1. TrustMF has the following signiﬁcant hyperparameters: the regulation penalty λ, the weight given to ﬁtting the user-user trust matrix (as opposed to the user-item rating matrix), λ, and the number of dimensions of the latent space d. We kept the number of dimensions at the default of 10 and the regulation penalty at 0.01. In order to determine an appropriate setting for λ, we sampled 10000 users from the ﬁltered Yelp data set and plotted MAE and RMSE over the change in λ. Results of this tuning are presented in Figure 6. For readability we have only plotted the best performing experiments from each of the main groups(RealFriends as a baseline, FriendPrediction as a non-personalized (MFTM) example, and PrefCluster-PrefPredict as a personalized (PMFTM) example). Each data point is the average of three runs with diﬀerent random seeds, and an iteration limit of 200 epochs. For these preliminary tests, we set the number of clusters at 10. Recommendation accuracy changes little for the actual trust links in the data set (RealFriends) as the importance of trust for recommendation increases, but the MFTM and PMFTM lines form roughly convex curves, reaching a range of minimal values around 0.8 < λ< 0.125. For future experiments, we set λ= 0.11. Figure 6 also serves as some encouraging early results, showing both that the impact of predicting trust links reduces recommendation error and a personalized approach can reduce this error further, given the correct weighting of trust importance. MTR has two signiﬁcant hyper parameters: the maximum neighborhood size for a user (e.g. the maximum number of peer recommendations that will be taken into consideration), κ, and a social weighting parameter, the value of β in Equation 22. In their original work, Mauro et al. [29] set β at 0.1, but did not report on how modifying this variable eﬀects recommendation accuracy. We used all Yelp users from the ﬁltered data set and computed the recommendation accuracy as β changed using a set of predictions based on a single social classiﬁer (i.e. the FriendPrediction setup). Results are illustrated in Figure 7, showing a clear tradeoﬀ between only considering user-user similarity and incorporating trust. Similar results were seen for the PrefPredict experiments. Accordingly, future experiments were run with a value of β = 0.3. We ran experiments with κ set to 50, but also experimented later with modifying the value of κ (to simulate sparsity). As a reminder, this value is the maximum number of ratings that are considered when recommending to a user. Test sets were created by reserving 20% of each user’s reviews. For all ﬁgures in this section, except in the results shown in Figure 6, these reviews were excluded from every step of the process, that is, the clustering and link prediction steps did not have access to these reviews. Due to the computation time required to generate and evaluate many of these experiments, only the results reported in Table 4 were cross validated. In this case, 5-fold cross validation was used with respect to users, so each user had a distinct 20% of their reviews reserved as a validation set for each of the folds. This validation set was hidden from every step in the pipeline. This validation approach is similar to the ones used in [29] and [9], where results were reported based on the average across folds of a 10-fold cross validation and the average across a complete leave-one-out cross validation, respectively. In our original tests, we attempted to set the number of clusters, k, by evaluating the silhouette scores of clusters in a large range for each data set, then simply choosing k based on whichever cluster count had the highest silhouette score. Unfortunately this method was ﬁckle, as the silhouette evaluation is based on a random sample of the clusters, and a single outlier could achieve a minimal score even if other nearby values of k were not optimal. Further, it is basically a heuristic to use cluster cohesion to choose the number of clusters, when ultimately we are interested in improving the personalized trust links. Therefore, we iterated over a range of cluster values and repeated the entire experiment with each choice of cluster score, using the MTR recommender system. Results are illustrated in Figures 8 to 11. In general, results show that as the Fig. 8: Eﬀect of k on error for SocialClusterFriendPredict with MTR number of clusters searched for (k) increases, the error in the task follows a consistent trend of reduction. Note that when k = 1, the situation is equivalent to a non-personalized approach (as only searching for a single cluster is equivalent to doing no clustering), and as k increases the granularity of personalization increases. When predicting whether two users should be friends or not, MAE can be lowered by 0.003 points by adding personalization, while when predicting aligned preferences it is only lowered by 0.0005 points. These results are less impactful than the early results seen using the TrustMF classiﬁer in Figure 6. That said, the results indicate a consistent trend of improvement as clustering based personalization is applied: a fairly consistent line of decrease in error can be observed in all lines. We wished to verify that the results illustrated in these ﬁgures are indeed improving because our clus- Fig. 9: Eﬀect of k on error for PrefCluster-FriendPredict with MTR tering technique was ﬁnding groups of similar users, which allowed the prediction techniques to learn more personalized classiﬁers for these groups. For instance, it is conceivable that splitting users into groups and learning multiple classiﬁers is helpful regardless of the groups picked, as this procedure would be similar to bootstrap aggregating [1], which allows simple classiﬁers to model multiple weak correlations in data. To test this, we repeated the FriendPredict experiment, but clustered agents into k clusters randomly. The results illustrated in Figure 12 compare this random clustering to clustering by social circle overlap. This ﬁgure clearly shows that the reduction in error is largely due to the non-random clustering approach. The solid Fig. 10: Eﬀect of k on error for SocialClusterPrefPredict with MTR and dashed lines start oﬀ identically at k = 1 on the x-axis (no clustering) but as the numbers of clusters increases, the error decreases, for the case where social clusters are used. We take this as evidence that the clustering technique is improving accuracy because clustering genuinely enables more personalized predictions, not simply because the number of models being learned has increased. We also experimented with modifying the κ variable on MTR, eﬀectively simulating sparsity, as this variable controls how many peer advisers can be considered for a recommendation. Results illustrated in Figure 13 compare a accuracy on the SocialCluster-FriendPredict task, comparing the error rates for a single cluster (unclustered) and for 55 clusters (clustered). Overall, the gap in error rate is most dramatic when a larger κ value Fig. 11: Eﬀect of k on error for PrefCluster-PrefPredict with MTR is used, but the advantage for a clustered approach applies over the range of values. Finally, in Table 4 we present the results of a 5fold cross validation over user ratings for these tasks. Best results are bolded. There are a number of interesting results. First, the conceptually simple MTR system outperforms TrustMF across the board, despite the fact that the TrustMF system was allowed to run for a much greater period of time in order to reach convergence. This gap in performance is often dramatic, for example, in the best cases for each system, MTR has a MAE that is 5% lower than TrustMF, and a RMSE that is 1.6% lower. On the better performing MTR recommender system, the best results are achieved when predicting friendship links rather than predicting preference correlation. This makes, sense, as the MTR system already consid- RandomCluster-FriendPredict. ers observable user preference (see Equation 22), thus predicting new instances of aligned preferences is not likely to add much new information. The best performing task, by a small margin, is the SocialCluster-FriendPredict task, which basically reconﬁrms the ﬁndings presented earlier in this section (with the added certainty of being averaged across folds). Clustering did not have an appreciable eﬀect (at least not at the scale of 10) for the preference alignment prediction task. Note that, in the case where an improvement was seen, although the scale of the eﬀect appears small it is clear from the previous graphs that this is not merely due to statistical variance. For example, Figure 8, clearly shows that the decrease in error between FriendPrediction and SocialCluster-FriendPredict is due to the increasing the number of clusters. Fig. 13: Eﬀect of κ on accuracy for MTR. On TrustMF, results are more tightly grouped and there is quite little appreciable diﬀerence between experiments. As this system is more conceptually complex than MTR, it is diﬃcult to interpret exactly why this might be the case. Interestingly, the best performing task for MTR is the worst performing task on TrustMF. Clustering does not have a positive eﬀect in these experiments, and in the SocialCluster-FriendPredict task actually seems to harm the performance. Our early experiments with this recommender system (presented in Figure 6) suggested that there might be more interesting diﬀerences between approaches, but this was not the case in these ﬁnal results. We speculate that because these earlier results were computed using diﬀerent techniques to select Yelp users (randomly selecting 10000 users versus our ﬁnal strategy of selecting all 30000 users with more than 20 reviews submitted), the underlying distributions of ratings may have been diﬀerent enough to cause this change. In this work, we evaluated the eﬀect that personalization via clustering had on the accuracy of a trust link prediction task. We accomplished this by predicting novel trust links on a data set of Yelp users and measuring accuracy of these predicted trust links via an item recommendation task. Our results show that the option of predicting novel trust links results in better performance than using the explicitly stated trust links for the recommendation task. Further, our results show a small but consistent improvement in recommendation accuracy when clustering is used to determine groups of similar agents and distinct trust prediction models are learned for each group of agents with the MTR recommender system (e.g. Figures 8 to 11). We showed that this improvement was not simply the result of the fact that more classiﬁers were trained, as randomly splitting users into groups does not improve accuracy nearly as much as the clustering technique does (Figure 12). While our early results with TrustMF inspired conﬁdence, and we hoped to see more dramatic improvement in recommender accuracy from these experiments, the ﬁnal results show that, while consistent, the improvements in accuracy from the procedures outlined here are small. We will comment on ways these techniques could be improved, and potential avenues for future work, in Section 5.2.1. In addition to the experiments with personalization (which explored multiple approaches for clustering users), we produced a comprehensive MFTM solution, combining techniques from the literature with novel features. We also make clear the applicability of MFTM to social networks. We experimented with predicting two types of trust links: explicit friendship (the FriendPredict experiments) and implicitly stated preference alignment (the PrefPredict experiments) and evaluated the utility of the predicted links derived by our methods, using two distinct trust-aware recommender systems. We found that the preferred target of trust link prediction can vary with the desired use-case: it was not clearly preferable to predict friendship links or preference alignment links. On the MTR system, which already strongly considers user preference alignment, our experiments performed better when predicting friend links, while on the TrustMF system predicting preference alignment between users produces (slightly) better recommendation accuracy. In this section, we ﬁrst reﬂect on how our work compares to those of other researchers, with respect to both multi-faceted trust modeling and to personalizing solutions for trust modeling. We then discuss how our methods for predicting trust links using our particular personalized multi-faceted trust modeling can serve as a useful starting point for handling misinformation in social networking environments. Our work in Section 3 was heavily inspired by the works of Mauro et al. [29] and Fang et al. [9]. All works have a similar structure: they propose a multi-faceted trust model and test it on a recommendation task on a data set harnessed from a site with item rating component. We sought to extend these works by combining the best features from each of them while testing the eﬀects of a personalization step to increase the accuracy of trust prediction. In particular, Mauro’s work developed a large set of trust indicators on the Yelp data set, while Fang’s work proposed a smaller set of relatively generic indicators that could be used on the Epinion’s data set. In our work, we combined these indicators when testing on the Yelp data set, with the goal of achieving a more comprehensive model of user to user trust formulation. While Mauro’s work proposes a large number of trust indicators, it does not seek to weight the importance of those indicators in a data driven manner: they instead experiment by taking a non-weighted average of a subset of the indicators. Like in Fang’s work, we have used a logistic regression to ﬁnd weights for these indicators that ﬁt the data set, believing this method to be a more principled approach to the problem. In addition, we did some preliminary investigation of Epinions data in order to expand the environments examined under our approach. Conceptually, the approaches taken to personalize recommendations we undertook on the Yelp data would be easily transferable to this data set. See Section 5.2.1 for more details. Another work which has relevance is that of Gilbert and Karahalios [13]. While not an artiﬁcial intelligence paper, the authors present a multi-faceted statistical analysis of the factors which aﬀect tie strength between pairs of users in social media. They found that a set of 74 variables collected from a the Facebook accounts of participants could be used to predict, with high accuracy, the answers these participants gave to survey questions designed to model social tie strength with their friends on Facebook (e.g. “How comfortable would you feel asking this person for a loan?”). This work presents strong evidence for the notion that trust (i.e. as an aspect of a strong social tie) can be predicted between agents based on relatively simple data extracted from interaction history on social media. Given the subjective nature of trust, it is clear that accurate trust models need to incorporate some level of agent-speciﬁc personalization. It is feasible to model reputation or popularity without such personalization. However, given the sparsity of data in most networks, the cold start problem, and computational limitations, it is not typically feasible to give each agent a completely distinct model. Our approach to personalization in Section 3 was to determine clusters of similar users and learn trust link classiﬁers on the basis of these clusters. The Personalized Trust Model developed by Zhang and Cohen [48] (described in Section 2.1) can be seen as an extension of the Beta Reputation System [23] that computes both a private and public trust factors for integrating the advice of some other agent. The weighting of these factors is based on the overlap in the number of common items the truster and trustee have advised on (rated), thus giving higher weight to private trust when there is more basis for comparing the two agents directly with respect to past behaviour. Eﬀectively, this system implements personalization by using generic predictions under uncertainty about individual preferences. However, their formula for assigning weight to personal and private trust factors is basically a heuristic, as the settings for appropriate error and conﬁdence bounds are not derived in a data driven manner. Our work attempts to implement personalization in a data-driven way, by identifying clusters of similar users and learning their trust formulation procedures at a cluster level. The usefulness of stereotypes towards improved trust modeling is an approach examined by other researchers who may also derive beneﬁt from examining our datadriven methods. The StereoTrust Model developed by Liu et al. [28] implements personalization by allowing each agent to deﬁne its own grouping function for partitioning the set of other agents via stereotypes. For example, an agent may decide to stereotype based on stated interest, location, seniority, etc. This is intended to model the subjective assumptions humans apply in every day life. The agent then uses a trust estimation function (inspired by the Beta Reputation System [23]) to reason about their trust with respect to groups deﬁned by stereotypes, rather than with respect to individuals. The trust an agenta has in another agentb is then computed as a weighed average of the trusta has in all the groups thatb is a part of. This system implements personalization by allowing each agent to specify its own stereotypes, although in practice it’s not clear how this information would be elicited from real users. This approach relies on the notion that members of a group will act similarly, but by allowing individual agents to deﬁne groups arbitrarily, the usefulness of this notion is under a certain strain. Without the ability to statistically analyze large amounts of data from the environment, it is unclear how individual agents could be expected to create stereotypes that deﬁne groups which actually have some cohesiveness of behaviour. In their actual implementation, stereotypes were implemented based on rating similarity, so no agents actually had an opportunity to specify their stereotypes, and in practice this solution turns out to be a complex approach to reach the same end goal as, for instance, clustering users based on rating behaviour (as we have done). In order to improve the lives of users of social media, we have presented an approach for predicting trust links between peers within these networks. Our framework makes it possible to assess whether the content created online is a good candidate to display to a user or not (where options may include ﬂagging messages coming from sources that are not established to be well trusted). Our work aims to improve online experiences by supporting distinct presentation of content to diﬀering users, achieved by reasoning about relationships with peers and the concept of trust. Our concern with trustworthiness of content relates well to companion efforts devoted to detect digital misinformation [7,21,47]. There is a spectrum of possible outcomes when messages which are of questionable quality are shown to users, including special attention in contexts such as healthcare where the consequences may be more troubling [32]. Note that there will still be various options for actions to take, once trust modeling has provided some insights into messages of concern. The methods we present here are designed to be self-contained algorithms which can be provided to any party which has the data at hand, to reason about trustworthiness. It would be possible, for instance, to have platform owners ﬂag less trustworthy posts and individual users have agency to choose what kinds of information should be ﬁltered for them. Our algorithms would be able to indicate, for a particular user, whether the other users in the environment are predicted to be trusted (based on whether a trust link is predicted to exist between them). There is then an interesting dilemma about whether top-down control of the social network (e.g. dictated by government) or bottom-up management of the content (e.g. under the control of individual users) should be launched in order to take actions with respect to the messages of agents with questionable trustworthiness. While our model promotes a solution that is attuned to an individual’s preferences, in cases where these may be in conﬂict with interests of the public (for instance, promoting hate) a tension may exist in deciding where the control should lie. If a decision is made to consult reputable outside sources to determine acceptability, this could potentially be integrated into the trust models to discourage inappropriate behaviour. We do not propose an answer to this challenge of determining appropriate control but merely acknowledge this as a concern for anyone trying to address content recommendation in social media. As our work has drawn out the value of personalized solutions, the models that we have presented should be ﬂexible enough to support a variety of overall preferences with respect to ﬁnal outcomes. With respect to the array of concerns for this Special Issue, our work is best viewed as focused on computational approaches grounded in artiﬁcial intelligence methods which assist in the detection of misinformation and disinformation. We introduce novel perspectives on this particular agenda for improving online social networks, through techniques for personalizing the analysis and with highlighting of the potential provided by performing trust modeling. In this paper we considered the problem of improving the experience of users on social networks, particularly with respect to content overload and the propagation of untrustworthy information. We argued that a trust modeling approach could be appropriate for social networks and could be used to enhance message recommendation systems. We then outlined some of the issues involved in applying these models as they currently exist. The types of trust models that can be applied need to be highly ﬂexible, capable of capturing many diﬀerent kinds of data, and personalizable. We argued that a multi-faceted trust model was ideal for application to social networks. This is because the multi-faceted model can incorporate arbitrarily many signals from the agents and their environment into a data driven model of how trust is apportioned by agents in an environment. We argued that this ﬂexibility was a key feature, as it allows the model to adapt to many diﬀerent kinds of social networks. In Section 3, we designed a comprehensive MFTM and applied it to a large data set, including multiple new features and features proposed in previous works. We experimented with personalizing the predictions generated by a multi-faceted model, by clustering similar users and learning distinct models for each cluster of users. We argued that although this approach is not “truly individualized” personalization, a data driven model like MFTM imposes a tradeoﬀ between the number of users a model is learned for, as smaller numbers of users will have less data available to train classiﬁers with. We showed that this approach can lower error rates in a downstream trust aware recommendation task. In brief, our primary contributions with this work are to: – identify a critical challenge in applying trust models to social networks, namely to personalize trust prediction – develop a clustering based approach to personalization on a large dataset, applying predictions to a downstream recommendation task and showing consistent improvements in error rates – assemble a comprehensive array of multi-faceted trust indicators to incoporate into data-driven reasoning about trustworthiness as an advance to this method for multiagent trust modeling – outline the potential for our approach to trust link prediction (reasoning either about rating behaviour of users or social circles) to assist in eﬀorts to address misinformation in social networks, clarifying as well challenges which remain to be addressed When misinformation abounds in social media, being able to judge which sources are trusted is a critical step in assisting the users in these networks to navigate the waters. In the sections below we elaborate on ways to extend the models we have developed, and how to assist users in handling misinformation once our trust link prediction process has been run. We follow this with some suggested steps forward to assist some of the most vulnerable online users, older adults, illustrating how reasoning with clusters of users, the centrepiece of our proposed model, can be quite valuable in allowing unique experiences for this user base. 5.2.1 Expanding upon Personalized Multi-Faceted Trust Modeling There are a number of ways the project of personalizing multi-faceted trust predictions can be extended. Clustering: For example, we spent considerable time in this paper explaining the diﬃculties involved in clustering points that represent agents in a social network. While the approach we took was ultimately geometrically inspired, graph clustering algorithms could potentially oﬀer a better ﬁt to this type of data. This is an especially attractive option, as the sparsity of deﬁned similarities between agents when considered geometrically is a major issue for applying and accurately measuring performance of geometric clustering approaches. There is also merit in examining hierarchical clustering methods, as certain types of data may ﬁt this model well; however, parameters will need to be tuned to produce groupings with suﬃcient moderate sized clusters. Two other challenges related to the clustering aspect of this work are ﬁnding new methods of determining the optimal number of clusters (k), and considering other distance functions. In this work, we ran the entire experiment from beginning to end (cluster, predict, recommend) many times in order to measure the eﬀect of cluster count changes. Searching for new methods of determining k which are more computationally tractable than an exhaustive search, or ﬁnding heuristics that can guide this search, would be a useful and interesting research project. Second, we clustered agents in this work on the basis of social circle overlap (Jaccard similarity of trusted users) and preference similarity (Pearson Correlation Coeﬃcient observed in train set ratings). While we’ve argued that each of these are fairly natural metrics, it would be interesting to explore new metrics, including those based on implicit preferences (e.g. browsing behaviour), categories of interest (e.g. types of items enjoyed), and other biographical factors of the agents (e.g. geographic location, age). Each of these can plausibly be argued to be indicative of some facet of agent similarity, which in turn may be correlated to similarities in trust formulation procedures. Other clustering options that we could explore include seeing whether meaningful clusters exist ﬁrst using a statistic such as the Hopkins Statistic and delving further to determine the number of clusters in the dataset using a metric such as the Bayesian Information Criterion [19]. This then may help to direct the choices for clustering that are used. Our machine learning methods could also be broadened. We settled on logistic regression as the central method used but exploring further the use of other choices such as SVM [19] may help to determine whether additional robustness with performance could be achieved. Logistic regression is valuable as it admits a simple probabilistic interpretation, is quick to optimize and the weight vector learned is highly interpretable; however, there may be limitations of dealing with linearly separable data, making it more challenging to compute interesting feature combinations. Experimental set up: Our work does not consider dynamic changes in the network or agent preferences over time. For example, our method did not consider agents who had no preference data associated with them, that is, new agents joining the network. In practice, this could be handled by simply assigning generic predictions for agents who lacked suﬃcient preference data on which to cluster them. A periodic re-training of the models would also allow the system to account for changing preferences over time. This dynamic process of agents entering the network could be simulated for our experiments by leaving out a sample of users from the initial processing, then adding them after clusters have been created already. Another area for possible expansion is in our use of the personalized cluster classiﬁers. We did not learn a classiﬁer for a cluster when that cluster had less than 1000 positive examples of outgoing trust links and 100 agents in it. This step was taken to avoid learning very inaccurate classiﬁers, but some of the classiﬁers learned still ﬁt the data related to the cluster signiﬁcantly worse than a classiﬁer trained on larger sample of random agents. Therefore, it is worth exploring better ways of combining the “local” (cluster speciﬁc) predictions with the “global” predictions, similar to the procedure taken in the Personalized Trust Model [48]. Perhaps the weight given to a local trust model could be based on the diﬀerence in accuracy between the ﬁt of that model to the agents it represents and the accuracy a generic classiﬁer would achieve for those agents. This way, local irregularities could still be learned, but in cases where data is sparse, a little help from a generic classiﬁer can nudge predictions towards a more accurate ﬁnal outcome. This approach could also be taken to enable more “truly individual” personalization. For users with a large amount of activity (thousands of friends and other users to compare preferences with), a singleuser classiﬁer could be trained, and the results of this classiﬁer combined linearly with a cluster or global classiﬁer, allowing truly individualized personalization, and a gradual ramp up from generic to individual solutions as more data becomes available. In our work, we excluded users from experimentation who had fewer than 20 reviews. This ﬁltering procedure was inspired by Mauro et. al [29], but it imposes certain biases on the following evaluations. Under this procedure, only the opinions and activities of the most active users are taken into account (only about 2% of Yelp users have submitted at least 20 reviews). In our earlier experiments, we sampled users randomly, and the results from this time tended to show a more dramatic diﬀerence between personalized and non personalized approaches using TrustMF (e.g. in Figure 6). It would be valuable to experiment with diﬀerent procedures for sampling users from this data set. There is also merit in examining how our model operates in other social networking contexts. Epinions is a reasonable second case for us to explore, as it was also examined by [9]. We conducted a preliminary study of Epinions data sets and noticed that the chance of a randomly picked review score being 5 (the highest) is over 70%, while on Yelp the distribution is much more spread out, with the highest probability being only 35% on a score of 4. With this kind of bias in the data, we would expect even better score accuracy on the score prediction task when applying our methods. It is also interesting to note that Epinions users typically have fewer friends (trusted users) and that with Epinions users submitting ratings to written text (rating others’ reviews), there is vastly more feedback to examine. All of these diﬀerences may provide greater insights into the conditions under which our model has the most value. Expanding our study to other more elaborate datasets will also help to shed light on the scalability of our particular approach. There may be additional challenges when examining other social networks. While many recent projects in trust modeling focus on data from social networks with a signiﬁcant item rating component (as it is convenient to measure trust-aware recommendation accuracy on a set of reserved ratings as a proxy measure for the quality of novel predicted trust links), we acknowledge that many popular networks such as Twitter and Facebook lack a signiﬁcant item rating component. In cases like these, it would likely be necessary to engage in a user study (like the one in [13]) and survey actual users whether the predicted trust links appeal to them or not. This work would be useful, especially if a data set can be publicly released, as more data where preferences are explicitly indicated by users (rather than inferred) will be a boon to future trust modeling research. 5.2.2 Integrating with eﬀorts to address digital misinformation Integrating our proposed approaches directly into the larger eﬀort aimed at combating digital misinformation would be a rich area for future work. Some subtopics which would be especially valuable to explore include connecting to eﬀorts on detecting content which has been generated by bots [10]. Our methods may be able to provide more insights into bot detection algorithms or our algorithms may be able to adjust their predictions based on information revealed to us about suspected bot nodes within the network. It would also be useful to adjust our predictions of trust links and the use of these outcomes towards addressing misinformation, in view of the networking behaviour in the social media environment. Work such as that of Tong et al. [43] conducts an analysis of how rumours spread amongst the network’s peers. They suggest where to seed factual information in order to increase the odds of halting false information. Shao et al. [41] also proposes a way to limit attention to those nodes which are most critical for the ﬂow of information. Cho et al. [5] reﬂect both on stemming false informers and promoting true informers by examining more closely how beliefs of users are updated over time, considering various types of network centrality. What we are able to learn about trusted links, together with a study of the accompanying network relationships, may provide important insights for where to focus eﬀort aimed not just at identifying misinformation but also at stemming its tide. A persistent diﬃculty in applying trust models to social networks has been in ﬁnding appropriate data set and evaluation procedures. Most previous attempts to apply trust models to real social networks have relied on networks that included a signiﬁcant content rating component, such as Yelp, Epinions, and FilmTrust. These networks are attractive primarily because of the ease of harvesting objective test sets from the data extracted from them. How to tell if two agents should really trust each other? Simply check the correlation between the ratings they have given to content - if it’s positive, they should trust each other. One concern is the fact that some of these datasets, such as that of Epinions, represent dated information (with the site now defunct). The most popular networks (Facebook, Reddit and Twitter) may be overlooked because they lack a signiﬁcant content rating component. The issue of securing publically available datasets for some of these platforms arises at times as well. There may be value to making more of an eﬀort in the future to interrogate actual users of systems. We note the work of Gilbert and Karahalios [13], where 35 participants were recruited for the experiment. The authors had access to the data that the participants agreed to share with them - it was not necessary to convince Facebook to produce a data set for the researchers. After the statistical analysis, a qualitative analysis was performed to help contextualize the errors in the system by interviewing the participants. Cooperating more closely with the users of online social networks in this way will likely be impactful for future trust modeling research. 5.2.4 Considering vulnerable users: the case of older adults Another direction for future research that would be especially valuable to explore is making use of information about the needs and preferences of a cluster of users that are known independently before performing the data analysis proposed here to determine trust relationships (of value in assisting users in coping with misinformation). Below we sketch our current thoughts for how to integrate this prior information about the user base into the overall solution. In this approach, we could in fact have some preconceived notions of the user at hand due to what the user modeling community refers to as stereotypes [2] (what that entire class of users is likely to generally prefer). We have thought of this direction forward as part of our particular interest in oﬀering support for certain groups of users who are especially vulnerable online. One such community is that of older adults. It would be valuable to be able to carefully advise this demographic about misleading content, and personalized trust link prediction via clustering may thus be of use. We have begun to examine the special considerations of this user base when it comes to misinformation; we note that other research has already identiﬁed notable diﬀerences for older adults in social media [8,27, 45]. The framework presented in this paper could expand to integrate prior knowledge of its users in the following way. Suppose we had a group of older adult users. Per the algorithm of Section 3, the unsupervised learning method could assign them to the same cluster, suggesting the same weight of their trust indicators (if they have similar trust proﬁles). For Trust Link prediction, besides deﬁning trust indicators as we discuss in Section 3.3.3, we could also take the general preference of the older adults into consideration, allowing some ﬁner granularity to the reasoning. In other words, we could consider factors which generally inﬂuence the preferences of this particular user base. Ideally, predicting a trust link from this set of users to a certain peer should be predicated both on what the algorithm of Section 3 suggests from its datadriven analysis and also by the known prior preferences of the user base. A more detailed view of how to expand the overall process (our preliminary ideas for doing so) is as follows: – We have some priors about the needs of users who ﬁt certain stereotypes (e.g. older adults). We’d like to help those particular users. – We do the clustering based on the methods in Section 3. – We examine the clusters and observe that a large portion of users in some cluster embody one of our known stereotypes, e.g. in some cluster over half the users are older adults. – We combine data driven with stereotype based predictions for those clusters where large proportions of the users embody a particular stereotype, therefore applying our prior about the needs of a stereotype group to a cluster of users that seem to largely embody that stereotype. One way this process could be operationalized once clustering is performed as in Section 3, with clusters of older adults located: – Trust Link Prediction – Input: Trust indicators of older adults I(a, a), ..., I(a, a) and preference eﬀective function: f : I− > {0, 1}. – Output: T (a, a), the score of atrusting a’s recommendation. – Process: P redictor1 takes all indicators and makes a prediction. P redictor2 makes an independent prediction on indicators with f(I) = 1. Get T (a, a) by combining the two prediction results. – Information Recommendation – Input: Trust-link Prediction T (a, a) and information scores from a’s feedback, s. – Output: Recommendation scores for a, R(a, s) It would be interesting to delve further into these options for combining prior knowledge and trust link prediction, in order to provide richer recommendations to users. While an approach such as this could be used for any subgroup with known preferences, we feel it especially worthwhile to continue to learn more about the speciﬁc needs of older adults, and would explore our new direction with respect to this user base, as a ﬁrst step. Not applicable. Not applicable. The dataset used in this work can be downloaded from https://www.yelp.com/dataset (the 2019 full dataset). Algorithms as explained in the paper.