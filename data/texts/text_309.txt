 to be largely unexplored and under documented. Recommender systems (RS), which are currently unexplored for the enzyme-substrate interaction prediction problem, can be utilized to provide enzyme recommendations for substrates, and vice versa. The performance of Collaborative-Filtering (CF) recommender systems however hinges on the quality of embedding vectors of users and items (enzymes and substrates in our case). Importantly, enhancing CF embeddings with heterogeneous auxiliary data, specially relational data (e.g., hierarchical, pairwise, or groupings), remains a challenge. Results: We propose an innovative general RS framework, termed Boost-RS, that enhances RS performance by “boosting” embedding vectors through auxiliary data. Speciﬁcally, Boost-RS is trained and dynamically tuned on multiple relevant auxiliary learning tasks Boost-RS utilizes contrastive learning tasks to exploit relational data. To show the eﬃcacy of Boost-RS for the enzyme-substrate prediction interaction problem, we apply the Boost-RS framework to several baseline CF models. We show that each of our auxiliary tasks boosts learning of the embedding vectors, and that contrastive learning using Boost-RS outperforms attribute concatenation and multi-label learning. We also show that Boost-RS outperforms similarity-based models. Ablation studies and visualization of learned representations highlight the importance of using contrastive learning on some of the auxiliary data in boosting the embedding vectors. Availability and implementation: A Python implementation for Boost-RS is provided at https://github.com/HassounLab/Boost-RS Contact: Liping.liu@tufts.edu and Soha.Hassoun@tufts.edu Understanding the rich functionality of enzymes is fundamental in advancing biochemistry, molecular and synthetic biology and many other application domains. Enzymes were assumed speciﬁc, catalyzing a speciﬁc substrate; however, there is now wide consensus that enzymes are promiscuous, catalyzing many substrates, including substrates that the enzymes did not evolve to catalyze [Tawﬁk and S, 2010]. Our ability to analyze this inherent promiscuity has proved instrumental in guiding the direct evolution of novel proteins [Romero and Arnold, 2009], elucidating metabolism in natural and engineered organisms [Porokhin et al., 2021], and creating novel synthesis pathway to produce valuable therapeutics and commodity molecules [Bowie et al., 2020]. Despite progress in protein function annotation and modeling protein-ligand interactions (mostly focused on drug-ligand interactions), and manual and automated curation eﬀorts , there remains large gaps in our knowledge of enzyme capabilities. Computational tools that predict enzyme promiscuity on molecules can augment existing knowledge, guide biological and biomedical applications and reduce costly experimental eﬀorts. models, including molecular docking and molecular dynamic simulations, attempt to identify the most favorable binding mode of a ligand with a given target protein These methods require 3D models of both protein and molecule, and require signiﬁcant compute time, making these methods suitable for detailed analysis of a small number of interactions. Rulebased methods predict site of metabolism or products of enzymatic transformations on a query molecule. Most such methods, however, utilize hand-curated biotransformation rules (e.g., [Ridder and Wagener, 2008]), or applicable to only speciﬁc enzymes (e.g., [Tyzack and Kirchmair, 2019]), thus limiting their general applicability. Machine-Learning (ML) approaches have taken advantage of available enzymatic data and solve many important questions such the likelihood of enzymatic transformations between a compound pair, e.g. Support Vector Machines [Kotera et al., 2013], graph embedding [Jiang et al., 2021], identifying enzyme commission numbers that act on molecules, e.g. using hierarchical classiﬁcation of enzymes on molecules [Visani et al., 2020], and predicting the likelihood of a sequence catalyzing a reaction or quantifying the aﬃnity of sequences on substrates using Gaussian processes [Mellor et al., 2016]. Once trained, ML models provide quick evaluation and are suited for many bioengineering and biological applications that require the exploration of the vast interaction space. to recommend enzymes that are likely to act on speciﬁc substrates, and/or compounds that are suited as substrates for an enzyme. Recommender systems (RS) are heavily utilized in industrial applications. For example, more than 50% Motivation: Despite experimental and curation eﬀorts, the extent of enzyme promiscuity on substrates continues Computational approaches for predicting enzyme-substrate interactions target diﬀerent applications. Physics-based To expand the use of ML in predicting enzyme-substrate interactions, we investigate the use of recommender systems of all AI training cycles at Facebook are devoted to training deep learning recommendation models [Acun et al., 2021]. RS, however, were not used prior for predicting enzyme-substrate interactions. Previously, RS were used for predicting protein-drug interactions [Bagherian et al., 2021]. Many such techniques employ collaborative ﬁltering (CF) in the form of matrix factorization (MF), e.g., Multiple Similarities Collaborative Matrix Factorization (MSCMF) [Zheng et al., 2013], Probabilistic Matrix Factorization (PMF) [Mnih and Salakhutdinov, 2008], Neighborhood Regularized Logistic Matrix Factorization (NRLMF) [Liu et al., 2016]. auxiliary (side) data to learn improved embeddings. Many techniques enhance CF by using similarities among proteins and among drugs, e.g., MSCMF [Zheng et al., 2013] or neighborhood regularization, e.g., NRLMF [Liu et al., 2016], and REMAP [Lim et al., 2016], with the goal of minimizing distances between a protein (or a drug) and its nearest neighbors in the latent space. Other RS aim to integrate auxiliary data by fusing knowledge graphs (e.g., [Wang et al., 2021]), or integrating multi-source data (e.g., [Zhu et al., 2017], [Gao et al., 2018]). In practice, auxiliary data is complex and often exhibits multiple relational aspects: item labels may be hierarchical, and users may share a group label (zip code, building address, or profession). The common practice to concatenate auxiliary data with the learned embedding does not necessarily maximally exploit the relational aspect of the data. A general methodology for computing enhanced embeddings based on relational data and other complex heterogeneous auxiliary data therefore remains a challenge. vectors through auxiliary learning tasks. Boost-RS integrates the primary CF task with boosting tasks that aim to upgrade the embedding vectors based on available heterogeneous auxiliary data. The integration of user and item attributes addresses the interaction matrix sparsity issue and has already shown RS performance improvements [Sun et al., 2019, Bagherian et al., 2021]. To minimize negative transfer from the auxiliary tasks to the main task, the CF and the boosting tasks are dynamically weighted [Liu et al., 2019]. Each auxiliary task is designed to maximally utilize the available auxiliary data. Importantly, to learn from relational data, Boost-RS employs contrastive learning, which contrasts positive and negative samples to learn discriminative representations in a self-supervised manner. Contrastive learning is applied through triplet loss [Weinberger and Saul, 2009], where the model is trained to produce representations such that, for a given anchor example, a positive example is closer to the anchor than a negative example. Prior RS contrastive work [Liu et al., 2021] used perturbation to user (or item) preferences and contrasted the perturbed views to maximize learning mutual information between the two views. CF primary task of interaction prediction is accompanied by multi tasks that learn from heterogeneous auxiliary data. For enzymes, we exploit the Enzyme Commission (EC) hierarchical relationships and the enzyme functional orthologs. For substrates, we utilize the molecular ﬁngerprints and substrate-substrate biotransformation relationships due to functionally similar enzymes. We use contrastive learning on the enzyme functional orthologs and the biotransformation relationships, and formulate a hierarchical loss on the EC relationships. Our main contributions are: We apply Boost-RS to the enzyme-substrate interaction prediction task to recommend substrates to enzymes that are most likely to interact and vice versa. Our dataset is culled from biochemical reactions in the KEGG database [Kanehisa and Goto, 2000]. As most biochemical reactions are reversible, no distinction is made between substrates and products, and hence interacting molecules are referred to as compounds or substrates interchangeably. Reactions form a bipartite graph that can be captured as a binary interaction matrix between enzymes and substrates (Fig. 1A), where each row represents a compound, and each column represents an enzyme. A matrix entry is set to 1 if the compound and enzyme participate the same reaction, therefore representing a positive interaction instance. An entry is set to 0 in case there is no catalogued enzyme-substrate interaction. Compounds that are common to many enzymatic reactions, including cofactors such as ATP and NADH and metals, are excluded from the matrix. To ensure valid data splits, enzymes or compounds with a single entry in the interaction matrix are also excluded. In total, 17,627 enzyme-compound interactions are collected. They involve 4,768 enzymes and 6,397 compounds. The interaction matrix is therefore sparse with 0.06% positive entries. Four attributes are collected or derived from the KEGG database and are used for training auxiliary tasks: As the performance of CF hinges on learned embeddings of the users and items, prior RS techniques aimed to utilize We present in this paper a novel technique, Boost-RS, for enhancing the performance of RS by ”boosting” the embedding We demonstrate Boost-RS’s eﬀectiveness by applying it to the enzyme-substrate interaction prediction problem. The • Creating a ﬂexible and generalizable framework, Boost-RS, that enriches the embedding vectors for CF-based recommender systems using individual and relational heterogeneous auxiliary data. • Showing that using contrastive learning on relational data may outperform other techniques such as multi-labels learning and concatenation with learned embeddings. • Demonstrating the generality of the Boost-RS framework by showcasing its applicability to three recent neural network baseline CFs: Deep Matrix Factorization (DMF) [Xue et al., 2017], Neural Graph Collaborative Filtering (NGCF) [Wang et al., 2019], and Neural Matrix Factorization (NMF) [He et al., 2017]. • Showing that Boost-RS outperforms state-of-the-art similarity-based Graph Regularized Generalized Matrix Factorization (GRGMF) recommender systems [Zhang et al., 2020]. Figure 1: Boost-RS framework for enzyme-substrate recommendation prediction. A) Interaction matrix construction from enzymatic reactions. e.g., for E1, three positive interactions are added to the matrix. B) The Boost-RS framework that integrates the main task of interaction prediction with related auxiliary tasks. C) Collaborative ﬁltering models used as baselines. comprises four numbers, separated by dots, starting with a number that broadly represents the enzyme class, then the sub-class, sub-subclass, and a ﬁnal number that reﬂects the speciﬁcity of the enzyme towards a small group of substrates. As the EC numbers are hierarchical, the auxiliary learning task of EC prediction can be formulated as such. The EC numbers have 7, 94, and 164 distinct labels in the ﬁrst three respective ﬁelds of EC. We only consider the ﬁrst three ﬁelds of EC, as the fourth index typically denotes speciﬁc substrates and cofactors. [Kanehisa et al., 2016]. A particular KO designation, the letter K followed by 5 numerical digits, is assigned to a group of genes sharing similar functionality. KO numbers can therefore be considered as a ”group” attribute. We utilize 5575 sets of KO designations. the form of MACCS ﬁngerprints (FP) [Durant et al., 2002] are calculated using RDKIT. ignated as RClasses, shared by multiple substrate-product, or compound-compound (CC), pairs. CC pairs under the same RClass are transformed by enzymes with similar functionality (e.g., hydroxylation or methylation). CC relationships give rise to a compound-centric graph, akin to a social network. This graph is not a similarity network as CC pairs are not necessarily similar: a compound may undergo signiﬁcant molecular changes under some enzymatic transformations (e.g., transferases, ligases). The main task of the Boost-RS framework (Fig. 1B) is “recommending” compounds to enzymes (or enzymes to compound). Per the interaction matrix, an entry y N. As in a standard recommendation task, the main task function, f using learned enzyme and compound representations. We denote the representations for compound i and enzyme j as v and u The parameters for f The speciﬁcs of this loss are dependent on the underlying base CF model used as a RS (see Section 2.3.4). The embedding Enzyme Commission (EC) numbers. Each enzyme is associated with an EC number [Webb et al., 1992] that Functional Orthologs (KO) numbers. Another enzyme attribute is its KEGG functional orthology (KO) number Molecular ﬁngerprints (FP). Based on descriptions for molecules in the KEGG database, molecular attributes in Compound-compound biotransformations (CC). The KEGG database provides biotransformation patterns, des- , respectively. The probability of interaction, ˆy, and task loss, L, are then deﬁned as: In the simplest form, f(v, u) = vu. In more complex forms, f(v, u) is calculated using neural networks. vectors, v auxiliary data for compounds and enzymes into the embedding vectors via multi-task learning. Each auxiliary task calculates attribute probabilities with an auxiliary task function and updates representations and parameters based on the task loss. To address the diﬀering characteristic in relational attributes, auxiliary losses are deﬁned on individual, hierarchical, group, and pairwise attributes. While we describe the details relevant to the speciﬁc substrate and enzyme attributes, the framework easily accommodates other attributes with individual and relational attributes. is a binary vector, we use BCE to evaluate the prediction accuracy for each vector entry. The FP prediction function and the FP task loss are deﬁned as: where C is the set of compounds, and l The loss for each enzyme is based on the cross entropy loss on each of its ﬁeld set, F S two, and the ﬁrst three ﬁelds of the EC number, respectively. The prediction function and the cumulative task loss for the EC attribute is therefore: where E is the set of enzymes, and k is indicating which ﬁeld set, F S, in EC are we calculating the cross entropy loss on its distinct labels. And w each ﬁeld’s contribution to the loss. and CC prediction as f samples in the same set closer together in the embedding space and pushing away the representation of a sample outside the set. A function, d(·), measures the distance between a pair of representations. The CC loss function is deﬁned as: where i an anchor compound, P i, N margin between the positive P are derived from the KO group relationships deﬁned on the enzymes. The contrastive loss on CC reﬂect a pairwise relationship within the compound-centric relational graph, while the loss on KO reﬂects the group label distinction. The Boost-RS loss, L where L tasks, respectively. to balance the weights among the main task and auxiliary tasks. The abridged linear schedule [Belharbi et al., 2016] emphasizes auxiliary tasks in the early training epochs and shifts the focus to the main task in later epochs. We assign weights for the main and auxiliary tasks losses as follows: where t is the current training iteration, and T is time point where the focus shifts completely to the main task. and u, are critically important for the recommendation task. To boost performance, we inject task-relevant Using FP as an individual attribute, we denote the function for FP prediction as f. As each compound ﬁngerprint For EC prediction, we capitalize on the EC’s hierarchical structure, and denote the function for EC prediction as f. When using group attributes and pairwise relationship for the auxiliary tasks, we denote functions for KO prediction is a compound in a set of compounds that does not have an CC relationship with compound i, and γ is a positive The KO loss function is deﬁned similarly to the CC loss function, except that the anchor, positive and negative samples is the additive losses across the auxiliary tasks, αand αare weights for the main task and auxiliary The weights of task losses directly inﬂuence the performance of interaction prediction. We employ a dynamic strategy We use three neural-network recommender systems (Fig. 1C) for the interaction prediction task: DMF [Xue et al., 2017], NGCF [Wang et al., 2019], and NMF [He et al., 2017]. Each recommender system has its own characteristic and may be better suited for some applications. We apply Boost-RS to each of these models. networks are trained through f compound representations are computed using cosine similarity and outputed as the probability of the interaction, ˆy. Normalized cross entropy loss is used to compute the loss between y and ˆy. (GNN) are utilized to learn node representations. GNNs can account for diﬀerent order neighbors including ﬁrst order neighbors, second order neighbors, and so on. Node representations for enzyme (compound) nodes that are learned for each level of neighbors are then concatenated. The inner product of the enzyme and compound representations is then computed. To compute the loss, an additional optimization step (not shown in the ﬁgure) favors assigning higher predictions for known interactions than for unknown interactions. each learn independent representations. A function σ calculates the interaction probability based on the concatenation of the learned representations. We divide positive interactions into training, validation, and test sets at a ratio of 7:2:1. During training and validation, negative interactions are randomly sampled from the unknown interactions at a negative sampling ratio, which is a hyperparameter that varies across models. During testing, all unknown interactions are assumed negative. Positive interactions in the training set are excluded during sampling. 0.01%, evaluation metrics are selected to reﬂect the ability of RS to rank positive interactions ahead of negative ones. Average Precision (AP) computes the average precision after each predicted positive interaction in the ranked order list provided by RS. AP is utilized for model selection. To place lower emphasis on the exact ranking of known interactions, R-Precision computes the precision after all R positive interactions have been identiﬁed in the ranked order list. The overall performance at distinguishing between positive and negative interactions is reported using the area under the receiver operating characteristic curve (AUC). We additionally report the Mean AP (MAP) and the R-Precision across the enzymes and the substrates. As each enzyme and substrate had a varied number of positives, we also report the MAP for the top 3 items (MAP@3) and the precision on the top one item (Precision@1). range of hyperparameter search is speciﬁed as follows. The negative sampling ratio for training set is selected from {1, 5, 10, 15, 20, 25, 30}. The margin in the triplet loss, γ, is selected from {0.5, 1.0, 1.5}. The dimension of the two hidden layers of MLPs of f Adam optimizer [Kingma and Ba, 2015] with learning rates selected among {10 a rate selected from {0.0, 0.3, 0.5} and L2 norm at a weight selected from {10 dynamic weighting strategy, we allow a maximum of 3000 iterations, with T = 2000. During the ﬁrst 2000 iterations, the model shifts linearly from training the auxiliary tasks to the interaction prediction task. Training is stopped early if there is no improved MAP on the validation set in 500 consecutive iterations. We evaluate the performance gain (Table 1A) when implementing Boost-RS for the three baselines Boost-RS signiﬁcantly boosts the performance of every baseline across all metrics. NMF, which is the best performing baseline, gains 74%, 68%, and 10% on MAP, R-Precision, and AUC, respectively, when combined with Boost-RS. For this work, we use Boost-NMF as our ”Boost-RS” model, and use NMF as a baseline model for the rest of the experiments, unless noted otherwise. Boost-RS utilizes contrastive learning on KO and CC, hierarchical learning on EC, and individual attribute learning on FP. We create a compound (enzyme) binary multi-label vector for CC (KO), where each entry of the vector indicates the presence or absence of an RClass (KO) designation. The length of the vector is the number of distinct CCs (KOs). For CC, the multi-label vector has 3163 entries, where each compound is involved on average with 1.03 distinct CCs. For KO, the multi-label vector has a length of 5575 entries, where each enzyme has on average 1.37 distinct KO designations. for the multi-label loss, and using concatenation (NMF-Concat Multi-label), where we concatenate the outputs of GMF and MLP with the outputs of MLP layers representing the auxiliary data. The comparisons (Table 1B) utilize multi-label KO and CC data along with hierarchical EC and FP attributes. Boost-RS Multi-label outperforms NMF-Concat Multilabel as contrastive learning explicitly enforces negative pairs to have distinct representations, while multi-label training The inputs to DMF are the rows and columns of the interaction matrix. Two separate Multi-layer Perceptron (MLP) To compute f, NGCF ﬁrst applies graph neural network to the bipartite interaction graph. Graph Neural Network For NMF, fhas GMF and MLP working in parallel and are then followed with a scoring layer. GMF and MLP As the test data set is imbalanced, where the ratio of positive to the assumed negative interactions is less than For the three baseline recommender systems, we follow the authors’ guidelines on hyperparameter tuning. The We then compare multi-label performance using contrastive loss (Boost-RS Multi-label), where we use weighted BCE Table 1: Interaction-prediction performance evaluation. The best model (Boost-RS) is based on NMF and utilizes contrastive learning on KO and CC, hierarchical learning on EC, and individual attribute learning on FP. does not. Further, the sparsity of the KO and CC multi-label vectors (less than 0.1% of non-zero vector entries) may contribute to the limited improvements using concatenation. Importantly, a ﬂexible framework such as Boost-RS allows the judicious selection of the appropriate losses to boost the embeddings (last row of Table 1A). 2) using the various techniques (NMF, NMF-Concat Multi-label, Boost-RS Multi-label, and Boost-RS). Enzyme representations are shown to the left of each sub-panel. Compound representations are shown to the right of each sub-panel, where an edge is added between two compound representations if the two compounds are related via a CC. For the enzyme representations, each dot is colored with an EC class. Enzymes in sub-panels C and D form the most distinguishable clusters when compared to sub-panels A and B as both multi-label and contrastive learning perform well on KO (see next section). Across the sub-panels, compound representations initially show no evident pattern (sub-panel A), but display more deﬁned clusters with the progression towards sub-panel D. There is a node grouping on the left side of the D sub-panel for compounds that lack a CC relation as evident by the absence of any edges connecting these compounds. We characterize the contribution of each auxiliary task to the performance of Boost-RS independently of other tasks (Table 1C). We also contrast each such contribution against using the same data via concatenation with the baseline NMF model (Table 1D). For both Boost-RS and NMF-Concat, each auxiliary task contributes positively to predicting the overall interactions, indicating that these auxiliary tasks are relevant to the main task and provide additional information beyond what is captured within CF. For Boost-RS, CC contributes the most (50% improvement on the overall AP), while while KO, FP, and EC show more modest improvements (6%, 10%, 23% respectively). For NMF-Concat, CC improves the baseline NMF by 15% AP, while KO, FP and EC show limited improvements (2%, 3%, 4%, respectively). Boost-RS(CC) improves enzyme and compound MAPs signiﬁcantly (enzyme MAP by 55% and compound MAP by 48%). These results indicate that each auxiliary data improves the learning of compound and enzyme representations. task on select metrics other than overall AP, Precision1 for enzymes, and MAP for the compounds. Despite this performance variation and other experimental evaluations, learning KO using contrastive learning results in the best boosting performance (last row in Table 1A) as our model selection is based on AP. We attribute this varied performance to the characteristics of the KO auxiliary data. For KO, contrastive loss is applied on 779 paired relationships, where there are multiple pairwise relationships involving all pairs of enzymes under the same KO group label. In contrast, for CC, contrastive loss is applied to 7915 paired relationships derived from pairwise compound-compound transformations. We compare Boost-RS with a recent recommender system, Graph Regularized Generalized Matrix Factorization (GRGMF) [Zhang et al., 2020]. In addition to implementing matrix factorization, GRGMF learns latent node representations based on their neighborhood similarity. GRGMF therefore provides an alternative model for incorporating EC and FP auxiliary data. GRGMF takes as input a pairwise compound-similarity matrix and a pairwise enzyme-similarity matrix, where we use EC numbers and FP to obtain Jaccard similarity scores. CC and KO relationships cannot be readily integrated with GRGMF. We therefore evaluate Boost-RS when using only EC and FP as auxiliary data. Boost-RS outperforms GRGMF in most metrics, except for the AUC of GRGMF (Table 1E). The results show that the Boost-RS framework can eﬀectively capture the auxiliary data. We use t-SNE [Van der Maaten and Hinton, 2008] to visualize learned enzyme and compound representations (Fig. Boost-RS consistently improves performance on each tasks over NMF-concat, with the exception of the KO prediction Figure 2: Visualization using t-SNE for learned representation of enzymes and compounds, shown to the left and right of each sub-panel, respectively. A) Baseline NMF. B) Baseline with multi-label KO and CC concatenation (NMF-Concat Multilabel). C) Boost-RS with the auxiliary task of learning multi-label KO and CC (Boost-RS Multi-label). D) Boost-RS with triplet loss on KO and CC (Boost-RS). Our proposed framework, Boost-RS, oﬀers an elegant and generalizable model for boosting learned representations with heterogeneous auxiliary data for CF recommender systems. Dynamically training Boost-RS on multiple tasks allows evolving their relative weights along the learning epochs. The learning tasks are applicable to various individual and relational attributes. While intended as a general framework, we here demonstrated the utility of Boost-RS for enzymesubstrate interaction prediction task. We demonstrated Boost-RS on three CF baseline models. We identiﬁed four auxiliary data (molecular ﬁngerprints, enzyme commission numbers, functional orthologs and biotransformation relationships), and proved their relevance for enhancing interaction prediction. Importantly, we showed the ﬂexibility of Boost-RS framework through multi-task learning allows the integration of various auxiliary data modalities such as individual attributes, group attributes, pairwise relationship. Replacing relational attributes with their multi-label representations and using them concatenation or multi-task learning, cannot achieve the same performance boost as with Boost-RS. We compared BoostRS with similarity-based RS models and showed that Boost-RS outperforms GRGMF when utilizing the same data. Because of its demonstrated advantages, generality and elegance in integrating attributes with CF, the Boost-RS framework may prove beneﬁcial for a diverse set of application. Further, the use of the trained auxiliary machinery might prove useful in addressing the cold-start problem, common across recommender systems. This research is supported by NSF Award 1909536. Li-Ping Liu is supported by NSF Award 1850358 and NSF Award 1908617. The research is also supported by the NIGMS of the National Institutes of Health, Award R01GM132391. The content is solely the responsibility of the authors and does not necessarily represent the oﬃcial views of the NIH.