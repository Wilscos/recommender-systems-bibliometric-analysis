Compliments and concerns in reviews are valuable for understanding users’ shopping interests and their opinions with respect to speciﬁc aspects of certain items. Existing reviewbased recommenders favor large and complex language encoders that can only learn latent and uninterpretable text representations. They lack explicit user-attention and item-property modeling, which however could provide valuable information beyond the ability to rectightly coupled two-stage approach, including an Aspect-Sentiment Pair Extractor (ASPE) and an Attention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines Aspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as concrete aspect-level evidences. Extensive experiments on seven real-world Amazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs which enable APRE to deliver superior accuracy over the leading baselines. Reviews and ratings are valuable assets for the recommender systems of e-commerce websites since they immediately describe the users’ subjective feelings about the purchases. Learning user preferences from such feedback is straightforward and efﬁcacious. Previous research on review-based recommendation has been fruitful (Chin et al., 2018; Chen et al., 2018; Bauman et al., 2017; Liu et al., 2019). Cutting-edge natural language processing (NLP) techniques are applied to extract the latent user sentiments, item properties, and the complicated interactions between the two components. However, existing approaches have disadvantages bearing room for improvement. Firstly, they dismiss the phenomenon that users may hold different attentions toward various properties of the merchandise. An item property is the combination of an aspect of the item and the characteristic associated with it. Users may show strong attentions to certain properties but indifference to others. The attended advantageous or disadvantageous properties can dominate the attitude of users and consequently, decide their generosity in rating. Table 1 exempliﬁes the impact of the user attitude using three real reviews for a headset. Three aspects are covered: microphone quality, comfortableness, and sound quality. The microphone quality is controversial. R2 and R3 criticize it but R1 praises it. The sole disagreement between R1 and R2 is on microphone, which is the major concern of R2, results in the divergence of ratings (5 stars vs. 3 stars). However, R3 neglects that disadvantage and grades highly (5 stars) for its superior comfortableness indicated by the metaphor of “pillow”. Secondly, understanding user motivations in granular item properties provides valuable information beyond the ability to recommend items. It requires aspect-based NLP techniques to extract explicit and deﬁnitive aspects. However, existing aspect-based models mainly use latent or implicit aspects (Chin et al., 2018) whose real semantics are unjustiﬁable. Similar to Latent Dirichlet Allocation (LDA, Blei et al., 2003), the semantics of the derived aspects (topics) are mutually overlapped (Huang et al., 2020b). These models undermine the resultant aspect distinctiveness and lead to uninterpretable and sometimes counterintuitive results. The root of the problem is the lack of large review corpora with aspect and sentiment annotations. The existing ones are either too small or too domain-speciﬁc (Wang and Pan, 2018) to be applied to general use cases. Progress on sentiment term extraction (Dai and Song, 2019; Tian et al., 2020; Chen et al., 2020a) takes advantage of neural networks and linguistic knowledge and partially makes it possible to use unsupervised term annotation to tackle the lack-of-huge-corpus issue. In this paper, we seek to understand how reviews and ratings are affected by users’ perception Table 1: Example reviews of a headset with three aspects, namely microphone quality, comfort level, and sound quality, highlighted speciﬁcally. The extracted sentiments are on the right. R1 vs. R2: Different users react differently (microphone quality) to the same item due to distinct personal attentions and, consequently, give divergent ratings. R1 vs. R3: A user can still rate highly of an item due to special attention on particular aspects (comfort level) regardless of certain unsatisfactory or indifferent properties (microphone and sound qualities). of item properties in a ﬁne-grained way and discuss how to utilize these ﬁndings transparently and effectively in rating prediction. We propose a twostage recommender with anunsupervisedAspectSentiment Pair Extractor (ASPE) and an AttentionProperty-aware Rating Estimator (APRE). ASPE extracts(aspect, sentiment)pairs (ASpairs) from reviews. The pairs are fed into APRE as explicit user attention and item property carriers indicating both frequencies and sentiments of aspect mentions. APRE encodes the text by a contextualized encoder and processes implicit text features and the annotated AS-pairs by a dual-channel rating regressor. ASPE and APRE jointly extract explicit aspect-based attentions and properties and solve the rating prediction with a great performance. Aspect-level user attitude differs from user preference. The user attitudes produced by the interactions of user attentions and item properties are sophisticated and granular sentiments and rationales for interpretation (see Section 4.4 and A.3.5). Preferences, on the contrary, are coarse sentiments such as like, dislike, or neutral. Preference-based models may infer that R1 and R3 are written by headset lovers because of the high ratings. Instead, attitude-based methods further understand that it is the comfortableness that matters to R3 rather than the item being a headset. Aspect-level attitude modeling is more accurate, informative, and personalized than preference modeling. Note.Due to the page limits, some supportive materials, marked by “†”, are presented in theSupplementary Materials. We strongly recommend readers check out these materials. The source code of our work is available on GitHub at https://github.com/zyli93/ASPE-APRE. Our work is related to four lines of literature which are located in the overlap of ABSA and Recommender Systems. Aspect-based sentiment analysis (ABSA) (Xu et al., 2020; Wang et al., 2018) predicts sentiments toward aspects mentioned in the text. Natural language is modeled by graphs in (Zhang et al., 2019; Wang et al., 2020) such as Pointwise Mutual Information (PMI) graphs and dependency graphs. Phan and Ogunbona (2020) and Tang et al. (2020) utilize contextualized language encoding to capture the context of aspect terms. Chen et al. (2020b) focuses on the consistency of the emotion surrounding the aspects, and Du et al. (2020) equips pre-trained BERT with domain-awareness of sentiments. Our work is informed by these progress which utilize PMI, dependency tree, and BERT for syntax feature extraction and language encoding. Aspect and sentiment terms extraction is a presupposition of ABSA. However, manually annotating data for training, which requires the hard labor of experts, is only feasible on small datasets in particular domains such as Laptop and Restaurant (Pontiki et al., 2014, 2015) which are overused in ABSA. Recently, RINANTE (Dai and Song, 2019) and SDRN (Chen et al., 2020a) automatically extract both terms using rule-guided data augmentation and double-channel opinion-relation co-extraction, respectively. However, the supervised approaches are too domain-speciﬁc to generalize to out-ofdomain or open-domain corpora. Conducting domain adaptation from small labeled corpora to unlabeled open corpora only produces suboptimal results (Wang and Pan, 2018). SKEP (Tian et al., 2020) exploits an unsupervised PMI+seed strategy to coarsely label sentimentally polarized tokens as sentiment terms, showing that the unsupervised method is advantageous when annotated corpora are insufﬁcient in the domain-of-interest. Compared to the above models, our ASPE has two merits of being (1) unsupervised and hence free from expensive data labeling; (2) generalizable to different domains by combining three different labeling methods. Aspect-based recommendation is a relevant task with a major difference that speciﬁc terms indicating sentiments are not extracted. Only the aspects are needed (Hou et al., 2019; Guan et al., 2019; Huang et al., 2020a; Chin et al., 2018). Some disadvantages are summarized as follows. Firstly, the aspect extraction tools are usually outdated and inaccurate such as LDA (Hou et al., 2019), TF-IDF (Guan et al., 2019), and word embeddingbased similarity (Huang et al., 2020a). Second, the representation of sentiment is scalar-based which is coarser than embedding-based used in our work. Rating prediction is an important task in recommendation. Related approaches utilize text mining algorithms to build user and item representations and predict ratings (Kim et al., 2016; Zheng et al., 2017; Chen et al., 2018; Chin et al., 2018; Liu et al., 2019; Bauman et al., 2017). However, the text features learned are latent and unable to provide explicit hints for explaining user interests. Review-based rating prediction involves two major entities: users and items. A useruwrites a review rfor an itemtand rates a scores. LetR denote all reviews given byuandRdenote all reviews received byt. A rating regressor takes in a tuple of a review-and-rate event(u, t)and review sets Rand Rto estimate the rating score s. We combine three separate methods to label ASpairs without the need for supervision, namely PMIbased, neural network-based (NN-based), and language knowledge- or lexicon-based methods. The framework is visualized in Figure 1. 3.2.1 Sentiment Terms Extraction PMI-based methodPointwise Mutual Information (PMI) originates from Information Theory and is adapted into NLP (Zhang et al., 2019; Tian et al., 2020) to measure statistical word associations in corpora. It determines the sentiment polarities of words using a small number of carefully selected positive and negative seeds (sands) (Tian et al., 2020). It ﬁrst extracts candidate sentiment terms satisfying the part-of-speech patterns by Turney (2002) and then measures the polarity of each candidate term w by Pol(w) =) −). (1) Given a sliding window-based context samplerctx, the PMI(·, ·) between words is deﬁned by PMI(w, w) = logp(w, w)p(w)p(w) wherep(·), the probability estimated by token counts, is deﬁned byp(w, w) = andp(w) =. Afterward, we collect the top-qsentiment tokens with strong polarities, both positive and negative, as ST. NN-based methodAs discussed in Section 2, coextraction models (Dai and Song, 2019) can accurately label AS-pairs only in the training domain. For sentiment terms with consistent semantics in different domains such as good and great, NN methods can still provide a robust extraction recall. In this work, we take a pretrained SDRN (Chen et al., 2020a) as the NN-based method to generateST. The pretrained SDRN is considered an off-the-shelf tool similar to the pretrained BERT which isirrelevantto our rating prediction data. Therefore, we argue ASPE is unsupervised for open domain rating prediction. Knowledge-based methodPMI- and NN-based method depends on the seed selection. The accuracy of the NN-based method deteriorates when the applied domain is distant from the training data. As compensation, we integrate a sentiment lexiconSTsummarized by linguists since expert knowledge is widely used in unsupervised learning. Examples of linguistic lexicons include SentiWordNet (Baccianella et al., 2010) and Opinion Lexicon (Hu and Liu, 2004). The latter one is used in this work. Building sentiment term setThe three sentiment term subsets are joined to build an overall sentiment set used in AS-pair generation:ST = ST∪ST∪ST.The three sets compensate for the discrepancies of other methods and expand the coverage of terms shown in Table 10†. 3.2.2 Syntactic AS-pairs Extraction To extract AS-pairs, we ﬁrst label AS-pair candidates using dependency parsing and then ﬁlter out non-sentiment-carrying candidates using (ST). Dependency parsing extracts the syntactic relations between the words. Some nouns are considered potential aspects and are modiﬁed by adjectives with two types of dependency relations shown in Figure 2:amodandnsubj+acomp. The pairs of nouns and the modifying adjectives compose the AS-pair candidates. Similar techniques are widely used in unsupervised aspect extraction models (Tulkens and van Cranenburgh, 2020; Dai and Song, 2019). AS-pair candidates are noisy since not all adjectives in it bear sentiment inclination. STcomes into use toﬁlterout non-sentimentcarrying AS-pair candidates whose adjective is not inST. The left candidates form the AS-pair set. Admittedly, the dependency-based extraction for (noun, adj.) pairs is suboptimal and causes missing aspect or sentiment terms. An implicit module is designed to remedy this issue. Open domain AS-pair co-extraction is blocked by the lacking of public labeled data and is left for future work. We introduceItemTokas a special aspect token of thensubj+acomprule wherensubjis a pronoun of the item such as it and they. Infrequent aspect terms with less thancoccurrences are ignored to reduce sparsity. We use WordNet synsets (Miller, 1995) tomergethe synonym aspects. The aspect with the most synonyms is selected as the representative of that aspect set. Figure 2: Two dependency-based rules for AS-pair candidates extraction. Effective dependency relations and aspects and sentiments candidates are highlighted. DiscussionASPE is different from Aspect Extraction (AE) (Tulkens and van Cranenburgh, 2020; Luo et al., 2019; Wei et al., 2020; Ma et al., 2019; Angelidis and Lapata, 2018; Xu et al., 2018; Shu et al., 2017; He et al., 2017a) which extracts aspects only and infers sentiment polarities in {pos, neg, (neu)}. AS-pair co-extraction, however, offers more diversiﬁed emotional signals than the bipolar sentiment measurement of AE. APRE, depicted in Figure 3, predicts ratings given reviews and the corresponding AS-pairs. It ﬁrst encodes language into embeddings, then learns explicit and implicit features, and ﬁnally computes the score regression. One distinctive feature of APRE is that it explicitly models the aspect information by incorporating ad-dimensional aspect representationa∈ Rin each side of the substructures for review encoding. LetA= {a, . . . , a}denotes thekaspect embeddings for users andAfor items.kis decided by the number of unique aspects in the AS-pair set. Language encodingThe reviews are encoded into low-dimensional token embedding sequences by a ﬁxed pre-trained BERT (Devlin et al., 2019), a powerful transformer-based contextualized language encoder. For each reviewrinRorR, the resulting encodingH∈ Rconsists of(|r|+ 2) d-dimensional contextualized vectors: Figure 3: Pipeline of APRE including a user review encoder in the orange dashed box and an item review encoder in the top blue box, each containing an implicit channel (left) and an aspect-based explicit channel (right). Internal details of item encoder are identical to the counterpart of user encoder and hence omitted. H= {h, h, . . . , h, h}.[CLS]and [SEP]are two special tokens indicating starts and separators of sentences. We use a trainable linear transformation,h= Wh+ b, to adapt the BERT output representationHto our task asH transformed dimension of internal features. BERT encodes the token semantics based upon the context which resolves the polysemy of certain sentiment terms, e.g., “cheap” is positive for price but negative for quality. This step transforms the sentiment encoding to attention-property modeling. Explicit aspect-level attitude modelingFor aspectain thektotal aspects, we pull out all the contextualized representations of the sentiment words that modifya, and aggregate their representations to a single embedding of aspect a in r as h=h, w∈ ST ∩ r and wmodiﬁes a. An observation by Chen et al. (2020b) suggests that users tend to use semantically consistent words for the same aspect in reviews. Therefore, sum-pooling can nicely handle both sentiments and frequencies of term mentions. Aspects that are not mentioned byrwill haveh= 0. To completely picture useru’s attentions to all aspects, we aggregate all reviews fromu, i.e.R, using review-wise aggregation weighted byαgiven in the equation below.αindicates the signiﬁcance of each review’s contribution to the overall understanding of u’s attention to aspect a where[·; ·]denotes the concatenation of tensors. w∈ Ris a trainable weight. With the usefulness distribution ofα, we aggregate the hof r ∈ Rby weighted average pooling: Now we obtain the user attention representation for aspecta,g∈ R. We useG∈ Rto denote the matrix ofg. The item-tower architecture is omitted in Figure 3 since theitem property modelingshares the identical computing procedure. It generates the item property representations gofG. Mutual attention (Liu et al., 2019; Tay et al., 2018; Dong et al., 2020) is not utilized since the generation of user attention encodingsGis independent to the item properties and vice versa. Implicit review representationIt is acknowledged by existing works shown in Section 2 that implicit semantic modeling is critical because some emotions are conveyed without explicit sentiment word mentions. For example, “But this one feels like a pillow . . . ” in R3 of Table 1 does not contain any sentiment tokens but expresses a strong satisfaction of the comfortableness, which will be missed by the extractive annotation-based ASPE. In APRE, we combine a global featureh, a local context featureh∈ Rlearned by a convolutional neural network (CNN) of output channel sizenand kernel sizenwith max pooling, and two token-level features, average and max pooling ofHto build a comprehensive multi-granularity review representation v: v=h; h; MaxPool(H); AvgPool(H), h= MaxPool(ReLU(ConvNN_1D(H))). We apply review-wise aggregation without aspects for latent review embedding v whereβis the counterpart ofαin the implicit channel,w∈ Ris a trainable parameter, and d= 3d+ n. Using similar steps, we can also obtain vfor the item implicit embeddings. Rating regression and optimizationImplicit featuresvandvand explicit featuresGand Gcompose the input to the rating predictor to estimate the score sby ˆs= b+ b| {z }+ F([v; v])+ hγ, F([G; G])i. multi-layer fully-connected neural networks with ReLU activation and dropout to avoid overﬁtting. They model user attention and item property interactions in explicit and implicit channels, respectively.h·, ·idenotes inner-product.γ ∈ Rand {b, b} ∈ Rare trainable parameters. The optimization function of the trainable parameter setΘ with an Lregularization weighted by λ is J(Θ) =(s− ˆs)+ L-reg(λ). J(Θ)is optimized by back-propagation learning methods such as Adam (Kingma and Ba, 2014). DatasetsWe use seven datasets from Amazon Review Datasets (He and McAuley, 2016)including AutoMotive (AM), Digital Music (DM), Musical Instruments (MI), Pet Supplies (PS), Sport and Outdoors (SO), Toys and Games (TG), and Tools and Home improvement (TH). Their statistics are shown in Table 2. We use 8:1:1 as the train, validation, and test ratio for all experiments. Users and items with less than 5 reviews and reviews with less than 5 words are removed to reduce data sparsity. Baseline modelsThirteen baselines in traditional and deep learning categories are compared with the proposed framework. The pre-deep learning traditional approaches predict ratings solely based upon the entity IDs. Table 3 introduces their basic proﬁles which are extended in Section A.3.3†. Specially,AHN-Brefers to AHN using pretrained BERT as the input embedding encoder. It is included to test the impact of the input encoders. Evaluation metricWe use Mean Square Error (MSE) for performance evaluation. Given a test set R, the MSE is deﬁned by ReproducibilityWe provide instructions to reproduce AS-pair extraction of ASPE and rating prediction of baselines and APRE in Section A.3.1†. The source code of our models is publicly available on GitHub. 4.2 AS-pair Extraction of ASPE We present the extraction performance of unsupervised ASPE. The distributions of the frequencies of extracted AS-pairs in Figure 5 follow the trend of Zipf’s Law with a deviation common to natural languages (Li, 1992), meaning that ASPE performs consistently across domains. We show the qualitative results of term extraction separately. Sentiment termsGenerally, the AS-pair statistics given in Table 9†on different datasets are quantitatively consistent with the data statistics in Table 2†regardless of domain. Figure 4 is a Venn diagram showing the sources of the sentiment terms extracted by ASPE from AM. All three methods are efﬁcacious and contribute uniquely, which can also be veriﬁed by Table 10† in Section A.3.2†. Aspect termsTable 4 presents the most frequent aspect terms of all datasets. ItemTok is ranked top as users tend to describe overall feelings about items. Domain-speciﬁc terms (e.g., car in AM) and general terms (e.g., price, quality, and size) are intermingled illustrating the comprehensive coverage and the high accuracy of the result of ASPE. 4.3 Rating Prediction of APRE Comparisons with baselinesFor the task of review-based rating prediction, a percentage in- Table 2: The statistics of the seven real-world datasets. (W: Words; U: Users; T: iTems; R: Reviews.) Table 3: Basics of compared baselines. Models’ input is marked by “X”. “U” and “T” denote Users and iTems. D-CNN represents DeepCoNN. AHN-B denotes the variant of AHN with BERT embeddings. Figure 4: Sources of sen-Figure 5: Freq. rank vs. timent terms from AM.frequency of AS-pairs creaseabove 1%in performance is considered signiﬁcant (Chin et al., 2018; Tay et al., 2018). According to Table 5, our model outperforms all baseline models including the AHN-B on all datasets by a minimum of 1.337% on MI and a maximum of 4.061% on TG, which are signiﬁcant improvements. It demonstrates (1) the superior capability of our model to make accurate rating predictions in different domains (Ours vs. the rest); (2) the performance improvement is NOT because of the use of BERT (Ours vs. AHN-B). AHN-B underperforms Table 4: High frequency aspects of the corpora. the original word2vec-based AHNbecause the weights of word2vec vectors are trainable while the BERT embeddings are ﬁxed, which reduces the parameter capacity. Within baseline models, deep-learning-based models are generally stronger than entity ID-based traditional methods and recent ones tend to perform better. Ablation studyAblation studies answer the question of which channel, explicit or implicit, contributes to the superior performance and to what extent? We measure their contributions by rows of w/o EX and w/o IM in Table 5. w/o EX presents the best MSEs of an APRE variant without explicit features under the default settings. The impact of AS-pairs is nulliﬁed. w/o IM, in contrast, shows the best MSEs of an APRE variant only leveraging the explicit channel while removing the implicit one (without implicit). We observe that the optimal performances of the single-channel variants all fall behind those of the dual-channel model, which reﬂects positive contributions from both channels. w/o IM has lower MSEs than w/o EX on several datasets showing that the explicit channel can supply comparatively more performance improvement than the implicit channel. It also suggests that the costly latent review encoding can be less effective Table 5: MSE of baselines, our model (Ours for test and Val. for validation), and variants. The row of ∆ calculates the percentage improvements over the best baselines. All reported improvements over the best baselines are statistically signiﬁcant with p-value < 0.01. than the aspect-sentiment level user and item proﬁling, which is a useful ﬁnding. Hyper-parameter sensitivityA number of hyper-parameter settings are of interest, e.g., dropout, learning rate (LR), internal feature dimensions (d,d,n, andn), and regularization weightλof theL-reg inJ(Θ). We run each set of experiments on sensitivity search 10 times and report the average performances. We tune dropout rate in[0, 0.1, 0.2, 0.3, 0.4, 0.5]and LR in[0.0001, 0.0005, 0.001, 0.005, 0.01]with other hyper-parameters set to default, and report in Figure 6 the minimum MSEs and the epoch numbers (Ep.) on AM. For dropout, we ﬁnd the balance of its effects on avoiding overﬁtting and reducing active parameters at 0.2. Larger dropouts need more training epochs. For LR, we also target a balance between training instability of large LRs and overﬁtting concern of small LRs, thus 0.001 is selected. Larger LRs plateau earlier with fewer epochs while smaller LRs later with more. Figure 7†analyzes Table 6: Per epoch run time of APRE on the seven datasets. The run time of AM and MI, denoted by “”, is disproportional to their sizes since they can ﬁt into the GPU memory for acceleration. hyper-parameter sensitivities to changes on internal feature dimensions (d,d, andn), CNN kernel size n, and λ of L-reg weight. EfﬁciencyA brief run time analysis of APRE is given in Table 6. The model can run fast with all data in GPU memory such as AM and MI, which demonstrates the efﬁciency of our model and the room for improvement on the run time of datasets that cannot ﬁt in the GPU memory. The efﬁciency of ASPE is less critical since it only runs once for each dataset. Finally, we showcase an interpretation procedure of the rating estimation for an instance in AM: how does APRE predictu’s rating for a smart driving assistanttusing the output AS-pairs of ASPE? We select seven example aspect categories with all review snippets mentioning those categories. Each category is a set of similar aspect terms, e.g., {look, design} and {beep, sound}. Without loss of generality, we refer to the categories as aspects. Table 7 presents the aspects and review snippets given by uand received bytwith AS-pairs annotations. Three aspects, {battery, install, look}, are shared (yellow rows). Each side has two unique aspects never mentioned by the reviews of the other side: {materials, smell} ofu(green rows) and {price, sound} of t(blue rows). APRE measures the aspect-level contributions of user-attention and item-property interactions by the last term ofsprediction, i.e., hγ, F([G; G])i. The contribution on theith aspect is calculated by theith dimension ofγtimes theith value ofF([G; G])which is shown in Table 8. The top two rows summarize the attentions ofuand the properties oft. Inferred Impact states the interactional effects of user attentions and item properties based on our assumption that attended aspects bear stronger impacts to the ﬁnal prediction. On the overlapping aspects, the inferior property of battery produces the only negative score (-0.008) whereas the advantages on install and look create positive scores (0.019 and 0.015), which is consistent with the inferred impact. Other aspects, either unknown to user attentions or to item properties, contribute relatively less:t’s unappealing price accounts for the small score 0.009 and the mixture property of sound accounts for the 0.006. This case study demonstrates the usefulness of the numbers that add up toˆs. Although small in scale, they carry signiﬁcant information of valued or disliked aspects inu’s perception oft. This process of decomposition is a great way to interpret model prediction on an aspect-level granularity, which is a capacity that other baseline models do not enjoy. In Section A.3.5†, another case study indicates that a certain imperfect item property without user attentions only inconsiderably affects the rating although the aspect is mentioned by the user’s reviews. In this work, we propose a tightly coupled twostage review-based rating predictor, consisting of an Aspect-Sentiment Pair Extractor (ASPE) and an Attention-Property-aware Rating Estimator (APRE). ASPE extracts aspect-sentiment pairs (ASpairs) from reviews and APRE learns explicit user attentions and item properties as well as implicit sentence semantics to predict the rating. Extensive quantitative and qualitative experimental results demonstrate that ASPE accurately and comprehensively extracts AS-pairs without using domainspeciﬁc training data and APRE outperforms the state-of-the-art recommender frameworks and explains the prediction results taking advantage of the extracted AS-pairs. Several challenges are left open such as fully or weakly supervised open domain AS-pair extraction and end-to-end design for AS-pair extraction and rating prediction. We leave these problems for future work. Table 7: Examples of reviews given by uand received by twith Aspect-Sentiment pair mentions as well as other sentiment evidences on seven example aspects. Table 8: Attentions and properties summaries, inferred impacts, and the learned aspect-level contributions. We would like to thank the reviewers for their helpful comments. The work was partially supported by NSF DGE-1829071 and NSF IIS-2106859. This paper proposes a rating prediction model that has a great potential to be widely applied to recommender systems with reviews due to its high accuracy. In the meantime, it tries to relieve the unjustiﬁability issue for black-box neural networks by suggesting what aspects of an item a user may feel satisﬁed or dissatisﬁed with. The recommender system can better understand the rationale behind users’ reviews so that the merits of items can be carried forward while the defects can be ﬁxed. As far as we are concerned, this work is the ﬁrst work that takes care of both rating prediction and rationale understanding utilizing NLP techniques. We then address the generalizability and deployment issues. Reported experiments are conducted on different domains in English with distinct review styles and diverse user populations. We can observe that our model performs consistently which supports its generalizability. Ranging from smaller datasets to larger datasets, we have not noticed any potential deployment issues. Instead, we notice that stronger computational resources can greatly speed up the training and inference and scale up the problem size while keeping the major execution pipeline unchanged. In terms of the potential harms and misuses, we believe they and their consequences involve two perspectives: (1) the harm of generating inaccurate or suboptimal results from this recommender; (2) the risk of misuse (attack) of this model to reveal user identity. For point (1), the potential risk of suboptimal results has little impact on the major function of online shopping websites since recommenders are only in charge of suggestive content. For point (2), our model does not involve user and item ID modeling. Also, we aggregate the user reviews in the representation space so that user identity is hard to infer through reverse-engineering attacks. In all, we believe our model has little risk of causing dysfunction of online shopping platforms and leakages of user identities.