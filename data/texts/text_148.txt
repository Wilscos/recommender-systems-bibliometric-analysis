Motivated by suggested question generation in conversational news recommendation systems, we propose a model for generating question-answer pairs (QA pairs) with selfcontained, summary-centric questions and length-constrained, article-summarizing answers. We begin by collecting a new dataset of news articles with questions as titles and pairing them with summaries of varying length. This dataset is used to learn a QA pair generation model producing summaries as answers that balance brevity with sufﬁciency jointly with their corresponding questions. We then reinforce the QA pair generation process with a differentiable reward function to mitigate exposure bias, a common problem in natural language generation. Both automatic metrics and human evaluation demonstrate these QA pairs successfully capture the central gists of the articles and achieve high answer accuracy. Automatic generation of question-answer pairs (QA pairs) is a widely studied problem, primarily used to improve the performance of question answering systems via data augmentation (Alberti et al., 2019; Shakeri et al., 2020). However, question generation has also recently garnered interest in the context of conversational agents, where suggested questions (SQs) (i.e., You can also ask...) have emerged as a promising approach to drive multi-turn dialogues by educating customers about the agent capabilities and guiding users along dialogue trajectories with more engaging content (Yin et al., 2020; Nouri et al., 2020). As an example, consider a news chatbot engaged in a dialogue regarding COVID-19 vaccine developments producing the SQ {Q: How effective is the Pﬁzer-BioNTech vaccine?} paired with the answer {A: Pﬁzer/BioNTech vaccine is around91%effective at preventing COVID-19, according to updated trial data. Experts fear new variants of COVID-19 from South Africa and Brazil may be resistant to existing vaccines and treatment.} Firstly, SQs of this form mitigates the user burden regarding the necessity of both deep subject knowledge to ask good questions and awareness of the agent question answering capabilities to expect good answers. Secondly, the agent can look-ahead when selecting SQs to bias toward conﬁdently correct answers and content expected to lead to further follow-up questions and general system engagement. Targeting the SQ problem in news chatbot scenarios (e.g., (Laban et al., 2020)), this work examines QA pair generation corresponding to a news article summary paired with a self-contained question. Table 1 shows an example of the task. SQs based on these summary-centric QA pairs act as implicit article recommendations, complementing SQs focusing on passage-level extracted answers or factoid information. QA pairs generated for this purpose must satisfy several criteria including: (1) questions are self-contained (i.e., users need not read the corresponding articles nor require signiﬁcant additional domain knowledge to unambiguously understand the questions (Yin et al., 2020)), (2) questions are summary-centric (questions capture the gists of the corresponding articles), (3) answers correctly answer the questions, and (4) answers are brief but sufﬁcient such that users can conﬁdently trust the results. Additionally, to support different settings (e.g., screened device, mobile device, voice-only), we explore QA pair generation for varying application-speciﬁc answer length requirements. To satisfy these requirements, we ﬁrst collect a corpus of suitable QA pairs, accomplished by curating a set of news articles with well-formed questions as their titles and for which we can conﬁdently generate variable length summaries as an- Table 1: The suggested QA pair generation task. Given an article, we generate a self-contained and summarycentric question and a length-constrained answer. The question captures the gist of the article and can be understood without reading the corresponding article. swers. Observing that the summary generation→ question generation pipeline suffers from exposure bias (Ranzato et al., 2016), we propose a novel differential reward imitation learning (DRIL) training method that samples summary answers and reconstructs questions exclusively based on the hidden states of the answer decoder. Generated summaries are capable of directly reconstructing the questions, making them more likely the answers to the questions, and generate questions more closely related to the gists of the articles. We empirically validate the model with automated and human evaluations. In this paper, we study QA pair generation corresponding to variable length article-summarizing answers paired with self-contained and summarycentric questions. Our contributions include: (1) We collect a new QA dataset targeted for producing SQs in a news chatbot. (2) We propose a QA pair generation model where both questions and answers are well-formed, questions capture the central gists of articles, and answers are succinct while containing sufﬁcient supporting context. (3) We propose a novel differentiable reward imitation learning (DRIL) method which shows better performance over maximum likelihood estimation (MLE) and reinforcement learning (RL) for QA pair generation. (4) We perform extensive empirical evaluations to quantify DRIL-based QA pair generation improvements. Question-only Generation (QG). Both heristicbased (Heilman and Smith, 2010) and neural models (Du et al., 2017; Zhou et al., 2017; Sun et al., 2018) have been applied to QG. Usually, neural QG models are given contexts containing answers beforehand, contrasting with our goal of jointly generating QA pairs. Tuan et al. (2020); Song et al. (2018); Zhao et al. (2018) proposed to generate questions from long text and wider contexts, which is related to our method for QG using summaries. However, these wider contexts are only used to improve QG for the speciﬁed answer spans and do not attempt to capture the central gists of articles. Question and Answer Generation (QG+AG). QG+AG generates QA pairs jointly (Liu et al., 2020; Alberti et al., 2019; Du and Cardie, 2018, 2017; Subramanian et al., 2018; Wang et al., 2019; Krishna and Iyyer, 2019), frequently with two independent steps: identify question-worthy answer spans followed by generating answer-aware questions. Recent works train neural models to generate QA pairs (Shakeri et al., 2020; Lee et al., 2020) using QA datasets such as SQuAD (Rajpurkar et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) modulo the goal of generating self-contained questions paired with succinct but sufﬁcient articlesummarizing answers. Applications of QG and QG+AG. QG and QG+AG have been used for applications including data augmentation for QA systems (Alberti et al., 2019; Shakeri et al., 2020), information seeking in chatbots (Qi et al., 2020a; Laban et al., 2020), document understanding (Krishna and Iyyer, 2019), educational practice and assessment (Le et al., 2014), and online shopping (Yu et al., 2020). Training Mechanism for Sequence Prediction. Sequence prediction models are commonly trained with MLE. However, MLE can lead to degeneration (Holtzman et al., 2019) caused by exposure bias (Ranzato et al., 2016). Many algorithms (Yu et al., 2017; Lamb et al., 2016; Song et al., 2020; Welleck et al., 2019) have been proposed to mitigate exposure bias. Our DRIL method not only mitigates exposure bias, but also optimizes for a differentiable reward function that is aligned with the end goal. Please refer to Section 4.2 for comparison between DRIL and existing algorithms. While multiple QA datasets exist to train a QG or AG model, none speciﬁcally ﬁt the goal of this paper. QA pairs in SQuAD (Rajpurkar et al., 2018), NewsQA (Trischler et al., 2017), and Natural Questions (NQ) (Kwiatkowski et al., 2019) are not designed to capture the article gists, and a signiﬁcant number of questions in SQuAD and NewsQA are not self-contained. A key observation enabling this work is that many news articles have questions as their titles (e.g. How has the Biden administration helped student loan borrowers?) that can be used to train a SQ generation model since these questions usually correspond to the central gists of the news articles and are designed to be understood without reading the articles. However, two challenges remain: (1) clickbait titles need to be ﬁltered, and 2) these questions are not paired with summary-centric answers. Therefore, we developed the following data collection procedure to produce(SC)QA, our selfcontained summary-centric QA dataset. Starting with a curated URL list of news websites, we collected all articles between September2020 to March2021with a title that starts with a predeﬁned list of words (e.g., Where, What, How) and ends with a question mark. We then deﬁne a set of rules to ﬁlter out ill-formed and clickbait titles (details in Appendix A). Finally, we remove any questions that appear in the articles to ensure we don’t learn to copy the questions when present. In total, we collected39,460such question-article pairs. Given collected question-article pairs, we must pair them with suitable answers to produce QA pairs. From a preliminary study, we observed that∼ 70% of title questions can be answered by summaries of the corresponding articles. As a result, we set out to augment the question-article dataset with generated summaries as pseudo ground truth answers using following three-step procedure: Step 1(Deﬁne desired answer lengths): One of our goals is to generate well-formed answers that are succinct while containing sufﬁcient supporting context. Therefore, we generate summaries with varying brevity. Analyzing the average number of tokens for the ﬁrst1,2and3sentences of the CNN/DailyMail summaries (Hermann et al., 2015), we deﬁne three buckets of varying answer lengths: (0, 30], (30, 50] and (50, 72] BPE tokens. Step 2(Generate summary): For each article and desired length bucket, we use three SoTA summarization models (PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and CTRLSum (He et al., 2020)) ﬁne-tuned on CNN/DailyMail to generate three candidate summaries – enforcing summary length via control ofEOStoken generation. Unﬁnished sentences are removed and the length bucket is reassigned if needed. Step 3(Filter-out incorrect summary answers): Not all questions can be answered by the generated summaries since: (1) even the ground truth summary may not be a correct answer to the question and (2) summaries generated by SoTA models may not be good. To identify if a candidate summary answers the question, we train a QA pair classiﬁer using the4million question-snippet pairs MSMARCO dataset (Bajaj et al., 2016). For each article and length bucket, we select the candidate summary that has the highest score predicted by the trained classiﬁer. In total, we produce53,7464-Tuples of {Question, Article, Summary, Length Constraint}. For additional details and dataset statistics, please refer to Appendix A. In this section, we propose a family of QA pair generation models that are trained on the data collected in Section 3. LetDdenote a document (news article),Sdenote a summary,Qdenote a question, Ldenote a length bucket indicator (LB0,LB1or LB2), and<s>and</s>denote the specialBOS and SEP tokens respectively. 4.1 Base D→S→Q Model (D-S) Our base model is shown in Figure 1, consisting of two transformer-based encoder-decoder models (Vaswani et al., 2017) where one performs answer generation (AG) and the other question generation (QG). During training, the AG model encodes a concatenation of the length bucket indicator and Figure 1: Training of answer generation (AG) and question generation (QG) of the D-S model. L, D, S, Q denotes the length bucket indicator, document, summary, and question, respectively. Red dash arrows denote gradient ﬂow. the document, and decodes a length-constrained summary: whereθandθare the encoder and decoder parameters,cis the sequence of hidden states at the last encoder layer,Sis the ground truth summary, andSis the decoder input (S offset by one timestamp and prepended by aBOS token). The AG model is trained using MLE: L(θ, θ) = −log p(S|L+ D) where(n)represents then-th training instance. QG is also trained via MLE, mapping an input summary to a question: L(θ, θ) = −log p(Q|S) During inference, when decoding summary answers, we again control the generation ofEOSto fall into the range speciﬁed by the desired length bucket. We remove any unﬁnished sentences at the end unless after the truncation the answer is shorter than the minimum length of the length bucket. We use a pre-trained BART model (Lewis et al., 2020) to initializeθ,θ,θandθ. We name this base model D-S since the AG model takes the document (D) as input and the QG model takes the summary (S) as input. In Section 4.3 we will describe multiple variants of this model. When using MLE to train the base model, the decoder input at timesteptis the ground truth token at timestept − 1, sometimes called teacherforcing (Williams and Zipser, 1989) and known to suffer from exposure bias (Ranzato et al., 2016) due to the mismatch between training and inference. That is, during inference the decoder input is the predicted token instead of the ground truth token of the last timestep, causing errors from each timestamp to accumulate during generation. It has been shown that neural text generation models trained with MLE lead to generic and repetitive outputs (Welleck et al., 2019; Holtzman et al., 2019). Additionally, we usually want to optimize generation metrics (e.g., ROUGE) and human feedback directly instead of optimizing training data likelihood. To mitigate these concerns, we can sample decoder output during training and calculate the loss of the sampled output. Several works use RL to achieve this for text generation (Stiennon et al., 2020; Ziegler et al., 2019; Yu et al., 2017) and directly optimize for preferred metrics. However, RL is not sample efﬁcient and difﬁcult to tune in text generation tasks due to sparse rewards. For example, Hosking and Riedel (2019) have shown that applying RL to QG do not improve human evaluation metrics. Meanwhile, we observe that when generating a summary as the answer of a QA pair, we want to generate a summary that can better reconstruct the ground truth question without the article since: (1) a summary that can reconstruct a question is more likely to be able to answer that question and (2) a summary that better reconstructs the ground truth question leads to a generated question that is closer to the gist of the article. Moreover, the AG model is conditioned on the length bucket to control the levels of brevity, meaning that when the maximum allowed answer length is short, the question reconstruction will enforces the AG model to generate succinct but informative answers with respect to the question given the selected brevity level. We validate these assumptions in Section 5. We now propose the differentiable reward imita- Figure 2: Training of answer generation (AG) of the D-S-DRIL model. The input to the AG decoder is either Sor <s>. When the input is S, the AG decoder uses teacher-forcing to predict S, and the gradients back-propagate from Sto the AG decoder and AG encoder (the red dash arrow on the middle left), which is similar to the AG of the D-S model. However, when the input is <s>, the AG decoder samples a summary S, and the answer decoder hidden states are used to reconstruct the question Q. The gradients back-propagate from Qto the AG decoder and AG encoder (the red dash arrow on the top right). This reinforces the model to generate summaries that can reconstruct the questions. tion learning (DRIL) method for training the AG model as shown in Figure 2. During training, the AG model performs vanilla sampling to generate a summary: wherecis the sequence of hidden states at the last layer of the decoder, andSis the sampled summary. This differs from teacher-forcing since summaries are sampled in training. We then use another transformer-based decoder to reconstruct the question: noting that this decoder only depends on the hidden states of the AG decoder (notL + D). This forces the model to reconstruct the question only from the summary. The gradient can back-propagate from the question to the hidden states of the AG decoder cand AG encodercsuch that the question reconstruction loss will guide AG. To ensure generated summary ﬂuency, we also add the MLE loss from the base model. Overall, the AG model’s loss function is given by: −λ log p(Q|S, L+ D) In our experiments,λ = 0.3performs the best on the validation set. Finally, while we apply DRIL to the training of the AG model, the QG model remains the same as the base model. We do not use the question reconstruction decoderθas our QG model because its encoder inputcis a unidirectional representation and hence not preferred. We call this QA pair generation model D-S-DRIL. Connection with RL, Unlikelihood (Welleck et al., 2019), SeqGAN (Yu et al., 2017), and Professor-forcing (Lamb et al., 2016), etc.These methods mitigate exposure bias to some degree by calculating the loss from sampled sequences during training. Unlikelihood training penalizes the likelihood of undesired sampled sequences. SeqGAN and Professor-forcing both calculate the loss using a discriminator which learns to distinguish between the generated and ground truth sequences. They don’t optimize an extrinsic reward function. Caccia et al. (2019) show that Language GANs suffer from mode collapse and do not outperform MLE on the quality and diversity evaluation. SeqGAN uses RL optmization and thus suffers from aforementioned issues. Our DRIL method, on the other hand, learns to optimize a differentiable reward function that aligns with the end goal, and has lower gradient variance compared with RL. We empirically compare RL with DRIL in Section 5. Beyond this work, DRIL can be applied to other sequence prediction problems. For example, in step-by-step instruction following such as ALFRED tasks (Shridhar et al., 2020), DRIL can optimize the current step’s action trajectory such that it can reconstruct the nextKinstructions. The intuition is if the current step’s action trajectory is correct, then the agent should be able to follow the ground truth actions in the next steps to fulﬁll the tasks. From this perspective, DRIL is similar to SQIL (Reddy et al., 2020), which avoids drifting away from the demonstrations over long horizons by encouraging trajectories that return to demonstrated states when encountering unseen states. In conversational AI, Hosseini-Asl et al. (2020) proposed to ﬁne-tune a GPT-2 model to generate system responses turn-by-turn. DRIL can optimize response generation at each turn such that the response and dialogue context can reconstruct the nextKturns’ user and system response with a similar intuition: a correct system response will increase the likelihood of the ground truth in future turns. It avoids drifting away from demonstrations and mitigates exposure bias. In this section, we specify additional baseline QA pair generation models. Similar to the base DS model, these models are based on transformer encoder-decoder architectures. The differences between these models are the encoder and decoder inputs during training and inference as summarized in Table 2. Models are named by the encoder input of the AG and QG models joined with a ‘-’. D-D is similar to D-S except that QG takes the document (D) rather than the summary (S) as encoder input. QD-D generates question-conditioned answers, such that the AG model becomes a questionanswering model. D-SD is an extension of D-S and D-D such that the encoder of the QG model takes the concatenation of S and D. D-S-DRIL optimizes the AG model of D-S using DRIL. D-S-RL optimizes the AG model of D-S using RL, and the reward function is deﬁned as the negative question reconstruction loss calculated by the QG model of D-S. For further details, refer to Appendix B. We conduct experiments to answer 3 research questions: (1) How good are the QA pairs generated by each algorithm?, (2) Can DRIL outperform MLE and RL on QA pair generation?, and (3) Is our (SC)QAdataset preferable compared with existing public QA datasets for QA pair generation? For each generated QA pair, we are interested in evaluating the following 3 questions: (1) Does the length-constrained summary answer the question?, (2) Does the question capture the article gist?, (3) Is the question self-contained? We specify automated metrics and human evaluations to quantify the answers to these research questions. ROUGE-L (R-L) and BLEU. ROUGE-L and BLEU evaluate generated summaries/questions with respect to reference summaries/questions in the validation set. QA Pair Classiﬁer Scores (QACS). We need to measure how well the generated summaries answer the generated questions despite not having ground truth answers. Using the trained QA pair classiﬁer from Section 3, we propose QACS, which is the average of classiﬁer predicted scores on the generated QA pairs. The pseudo upper and lower bounds of QACS are0.359and0.046based on the average classiﬁer predicted scores of the positive and negative QA pairs in our human evaluation. 5.2 Human Evaluation We conduct human evaluation on Amazon Mechanical Turk. We designed 7 annotation tasks (ATs). Please refer to Appendix C for detailed human evaluation setup. Here we describe 4 ATs for which we are most concerned:AT-1shows a QA pair and asks Without referring to the answer or the article, are you able to understand the question? (Is the question self-contained?),AT-2follows AT-1 and asks Does the passage in the Answer text box answers the question?,AT-5shows the corresponding article and asks Does the question in the Question text box capture the gist of the Article?. For these three tasks, annotators select either TRUE or FALSE. AT-6shows an article and a list of questions generated by different models and asks Which Question shown above best captures the gist of the Article? 5.3 Baseline We evaluate D-S and its variants in Table 2. Beyond that, we evaluate the following baselines.QAGen 2S:This is the state-of-the-art model for QA pairs generation for improving QA systems. We train QAGen 2S on our dataset, which is similar to QD-D except that there is no length control on the answers.CTRLSum:We use a pretrained CTRLSum model to generate questiondependent summaries. Questions are generated by the QG model of QD-D.QA Transfer:We train a question-answering model on the NewsQA dataset to answer the generated questions. Questions are generated by the QG model of QD-D. This is to verify if a pre-trained question-answering model is sufﬁcient to answer the questions in our dataset. D-S-NewsQAandNatural Questions (D-S-NQ): These two models are similar to D-S, except that the QG models are trained on NewsQA and NQ, respectively. This is to verify if(SC)QAis better than other existing QA datasets for QG tasks. Refer to Appendix B for implementation details. Table 2: A summary of models (D-S and its variants) we proposed for QA pair generation. Q’ and S’ denote the question and answer generated during inference, respectively. QAGen 2S (Shakeri et al., 2020) is a state-of-the-art baseline. A full table that includes all the baselines in our experiments is shown at Appendix Table 15. Training and Validation set. We use the data described in Section 3 for training, taking the last 5,000out of the53,746examples as validation set. Test set. It is desirable to evaluate models on articles that do not use questions as titles. We sampled news articles between April1to April7,2021from the following news domains: washingtonpost.com, reuters.com, foxnews.com, cnn.com, cbsnews.com, nbcnews.com, nydailynews.com. We ﬁltered out articles that use questions as titles, and removed all questions in the articles. In total we collect7,698 test examples. Unlike validation set, there are no ground truth questions or answers in the test set. In this section we measure the quality of answers, particularly, whether they answer the corresponding questions. In Table 3, we show the ROUGE-L score of predicted summaries on the validation set, and QACS and AT-2 accuracy on the test set, resulting in the following observation. Models that generate questions based on answers have higher QACS and AT-2 accuracy than models that generate answers based on questions.Recall that during inference, D-S, D-D, D-S-DRIL and D-S-RL ﬁrst generate summaries as answers and then generate questions based on the answers (see Table 2). These algorithms perform much better than QD-D, CTRLSum, QAGen 2S and QA Transfer which ﬁrst generate questions and then generate answers to these questions. For example, D-S achieves51.2%,39.6%, and23.4% higher AT-2 accuracy than QAGen 2S in each of the3length buckets respectively. This observation is consistent in both QACS and AT-2 accuracy. Meanwhile, QD-D achieves the best ROUGE-L scores while the QACS and AT-2 accuracy are signiﬁcantly lower than D-S (e.g., AT-2 accuracy is 33.9%lower than D-S in length bucket 0). All these observations show that, to ensure the generated questions and answers match with each other, we should generate questions from answers rather than the opposite direction. This is especially true on our dataset, because the ground truth answers of our dataset are summaries, which are generated without conditioning on the questions (modulo examples generated by the CTRLSum in Section 3). 5.6.1 Results on (SC)QA Dataset In this section, we evaluate the quality of generated questions, particularly, whether the questions capture the gists of articles. From Section 5.5 we already observed that only D-D, D-S, D-S-DRIL, and D-S-RL can generate high quality answers. Therefore, here we only focus on these four models (refer to Appendix C and Section 5.7 for results on other models). The results are shown in Table 4. We report ROUGE-L/BLEU score of predicted questions on the validation set. Questions are predicted from predicted summaries instead of ground truth summaries, which is consistent with inference on the test set where we also don’t have ground truth summaries. We also report AT-5 accuracy on test set and make the following observations. DRIL and RL reinforce AG with question reconstruction loss and thus better reconstruct ground truth questions on validation set and better capture gists of articles on test set.Table 4 shows that D-S-DRIL achieves higher ROUGEL and BLEU score than D-S across all the length buckets. Note that D-S and D-S-DRIL have the same QG model so the only difference is the AG model, showing that D-S-DRIL is able to generate better summaries that can better reconstruct the ground truth questions. This aligns with our goal of designing the question reconstruction loss. Mean- Table 3: Evaluation of Answer Quality. Underline, bold, and bold represent the best results on ROUGE-L (R-L), QACS, and human evaluation, respectively. We report a 95% binomial proportion conﬁdence interval on human evaluation. D-S-DRIL generates higher quality answers than baselines in all three answer length bucket on test set. while, we assume that in our dataset the ground truth questions capture the gists of articles, this means that, by optimizing question reconstruction loss, D-S-DRIL can generate questions that better capture the gists of articles. This is validated by the results on AT-5 accuracy. D-S-DRIL has about6%and3%higher AT-5 accuracy than D-S on length bucket 0 and 1, respectively. D-S-DRIL has lower AT-5 accuracy than D-S on length bucket 2, likely because when the maximum allowed summary length is long, there is sufﬁcient information to reconstruct the questions even without the reconstruction loss. D-S-DRIL also shows better performance compared with D-S-RL, indicating the advantage of differentiable question reconstruction loss over the non-differentiable question reconstruction reward. AT-6 shows one article and a list of questions generated by D-D, D-S, D-S-DRIL, and D-S-RL. Annotators select the question that best captures the gist of the displayed article. Figure 3 shows the percentage of each model selected. We can see that questions generated by D-S-DRIL are preferred in length bucket 0 and 1, which is consistent with our results in Table 4. In this section, we evaluate if(SC)QAis better than existing publicly available QA datasets for QG. We compare with D-S-NewsQA and DS-NQ. NewsQA and NQ datasets are designed for question-answering but not QG speciﬁcally. Similar to(SC)QA, NewsQA is in news domain but without explicitly self-contained questions. For example, the question “what are they going to address?” in the NewsQA dataset is incomprehensible without reading the article due to lack of pronoun resolution. The human evaluation results are Figure 3: Proportion of most preferred AT-6 questions. (Which question best captures the gist of the article?) According to human evaluation, questions generated by D-S-DRIL best captures the gist of the article in answer length bucket 0 and 1. shown in Figure 4, leading to the following observation. QG models trained on NewsQA and Natural Questions cannot generate self-contained questions that capture gists of articles due to the limitations of the datasets, while QG models trained on (SC)QA can.We can see that the QG model trained on NewsQA achieves about50% lower AT-1 accuracy than the other two models, indicating that it cannot generate self-contained questions. Moreover, QG models trained on NewsQA and Natural Questions achieve73.55%and60.03% lower accuracy on AT-5 (averaged over3length buckets) compared with the QG model trained on(SC)QA, even though all models generate questions from summaries. We observe that DS-NewsQA tends to ask trivial questions such as the name of a person. D-S-NQ also fails to identify the focus of a summary. For example, in the summary “Michael Jordan has two brothers and Table 4: Evaluation of Question Quality. Bold, and bold represents the best results on ROUGE-L(R-L)/BLEU and AT-5 accuracy, respectively. We report a 95% binomial proportion conﬁdence interval on human evaluation. D-S-DRIL generates signiﬁcantly better questions in answer length bucket 0 and 1. Table 5: Joint accuracy on AT-1, 2 & 5. Bold represents our best model and underline represents best baseline. D-S-DRIL generates signiﬁcantly better QA pairs than the best performing baseline in all three answer length buckets according to the joint AT-1, 2 & 5 accuracy. two sisters. He grew up playing basketball and baseball against his older brother.”, D-S-NQ generates “Who is Michael Jordan’s brother playing against?”. However, the summary focus is Michael Jordan rather than his brother. We discuss such cases further in the qualitative analysis section. Figure 4: QG Human evaluation on different datasets. Our (SC)QA dataset performs better than NewsQA and Natural Questions on both AT-1 and AT-5 human evaluation in all three answer length buckets. We report the joint accuracy of {AT-1, AT-2, AT5}, deﬁned by the proportion of QA pairs that are answered TRUE for all three ATs and treat it as a metric for the overall QA pair quality, reporting results in Table 5 with the following observations. D-S-DRIL performs signiﬁcantly better than the best performing baselines.The best performing baselines are QA Transfer in length bucket 0 and QAGen 2S in length bucket 1 and 2. We observe that D-D, D-S, D-S-DRIL and D-S-RL all surpass them by a large margin. Particularly, D-SDRIL outperforms them by31.51%,34.82%and 22.92% in length bucket 0, 1 and 2, respectively. DRIL consistently outperforms RL and MLE. We can see from Table 5 that D-S-DRIL outperforms D-S and D-S-RL by3.22%and2.80%, respectively (averaged over3length buckets). The results are consistent on human annotations (AT-2 in Table 3, AT-5 in Table 4, AT-6 in Figure 3, and joint accuracy in Table 5), and automated metrics (QACS in Table 3 and ROUGE-L/BLEU scores in Table 4). This further shows the advantage of DRIL over MLE and RL, indicating that DRIL can efﬁciently reinforce AG to generate better QA pairs. We also conduct qualitative analyses on generated QA pairs. Please refer to Appendix D for details. This paper proposes a model for generating QA pairs with self-contained and summarycentric questions and length-constrained articlesummarizing answers. The target application is suggested questions for conversational news recommendation system. We collect a new dataset, (SC)QA, which contains news articles with questions as titles paired with summaries of varying length. We further propose differential reward imitation learning (DRIL) to efﬁciently mitigate exposure bias encountered with MLE. Empirically, it is shown that DRIL outperforms multiple alternative baseline neural architectures on automated and human evaluations. Regarding societal considerations, we consider three aspects. (1) Generating QA pairs that correspond to headlines and article summaries to power a news chatbot can provide users with a rapid glance of recent events. However, exposing users exclusively to article summaries may results in less informed users. Naturally, this can be mitigated by also developing experiences that lead to more in-depth examination of articles, but should be carefully considered. (2) Our(SC)QAdataset collection begins with articles (and potentially news providers) that use questions as article titles. Such articles may have stylistic elements that align with certain forms of journalism (e.g., tabloids) or audience manipulation (e.g., alarmism). Accordingly, the corresponding models may learn to generate similarly biased QA pairs which is certainly undesirable. Future work in this direction may include data cleaning to remove biased QA pairs and/or design de-biased models. (3) Factuality is also a potential issue. A news article itself may be fake news. Meanwhile, the AG model may generate a summary that is factually inconsistent with the corresponding news article. Future work may incorporate recent work in optimizing the factual correctness and considering multiple perspectives of the QA pairs.