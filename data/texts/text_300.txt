SLIMTRAIN - A STOCHASTIC APPROXIMATION METHOD FOR for approximating complex mappings, possessing universal approximation properties [15] and ﬂexible architectures composed of simple functions parameterized by weights. Numerous studies have shown that excellent performance can be obtained using state-of-the-art DNNs in numerous applications including mage processing, speech recognition, surrogate modeling, and dimensionality reduction [23, 45, 49]. However, getting such results in practice may be a computationally expensive and cumbersome task. The process of training DNNs, or ﬁnding the optimal weights, is rife with chal- 1. Introduction. Deep neural networks (DNNs) provide a powerful framework lenges, e.g., the optimization problem is non-convex, expressive networks require a very large number of weights, and, perhaps most critically, appropriate regularization is needed to ensure the trained network generalizes well to unseen data. Due to these challenges, it can be diﬃcult to train a network eﬃciently and to suﬃcient accuracy, especially for large, high-dimensional datasets and complex mappings and in the absence of experience on similar learning task. This is a particular challenge in scientiﬁc applications that often involve unique training data sets, which limits the use of standard architectures and established hyperparameters. recent survey [7]), the most popular approaches are stochastic approximation (SA) methods. SA methods are computationally appealing since only a small, randomlychosen sample (i.e., mini-batch) from the training data is needed at each iteration to update the DNN parameters. Also, SA methods tend to exhibit good generalization properties. The most extensively studied and utilized SA method is the stochastic gradient descent (SGD) method [43] and its many popular variants such as AdaGrad [18] and ADAM [29]. Despite the popularity of SGD variants, major disadvantages include slow convergence and, most notoriously, the need to select a suitable learning rate (step size). Stochastic Newton and stochastic quasi-Newton methods have been proposed to accelerate convergence of SA methods [6, 24, 9, 52, 12], but including curvature information in SA methods is not trivial. Contrary to deterministic methods, which are known to beneﬁt from the use of second-order information (consider, e.g., the natural step size of one and local quadratic convergence of Newton’s method), noisy curvature estimates in stochastic methods may have harmful eﬀects on the robustness of the iterations [12]. Furthermore, SA methods cannot achieve a convergence rate that is faster than sublinear [1], and additional care must be taken to handle nonlinear, nonconvex problems arising in DNN training. The performance and convergence properties of SA methods depend heavily on the properties of the objective function and on the choice of the learning rate. bility inherent in most common DNN architectures. We assume that the network, G, is parameterized by two blocks of weights, W and θ, and is of the form where F , also referred to as a feature extractor, is a parameterized, nonlinear function. The important observation here is that the DNN is nonlinear in θ and, crucially, is linear in W. Any DNN whose last layer does not contain a nonlinear activation function can be written in this form, so our deﬁnition includes many state-of-theart DNNs; see, e.g., [27, 41, 31, 30, 44] and following works like [46, 49, 34]. In a supervised learning framework, the goal is to ﬁnd a set of network weights, (W, θ), such that WF (y, θ) ≈ c for all input-target pairs (y, c) in a data space. Training the network means learning the network weights by minimizing an expected loss or discrepancy of the DNN approximation over all input-target pairs (y, c) in a training set, while generalizing well to unobserved input-target pairs. memory training method that exploits the separability of the DNN architecture to leverage recently-developed sampled Tikhonov methods for automatic regularization parameter tuning [34, 13].For the linear weights in a regression framework, we obtain a stochastic linear least-squares problem, and we use recent work on sampled limitedmemory methods to approximate the global curvature of the underlying least-squares problem. Such methods can be viewed as row-action or SA methods and can speed While the literature on eﬀective solvers for training DNNs is vast (see, e.g., the In this paper, we seek to simplify the training of DNNs by exploiting the separa- Main contributions. In this paper, we describe slimTrain, a sampled limitedup the initial convergence and to improve the accuracy of iterates [13]. As discussed above, applying a second-order SA method to the entire problem is not trivial and obtaining curvature information for the nonlinear weights is computationally expensive, particularly for deep networks. As our approach only incorporates curvature in the ﬁnal layer of the network, where we have a linear structure, its computational overhead is minimal. In doing so, we can not only improve initial convergence of DNN training, but also can select the regularization parameter automatically by exploiting connections between the learning rate of the linear weights and the regularization parameter for Tikhonov regularization [11]. Thus, slimTrain is an eﬃcient, practical method for training separable DNNs that is memory-eﬃcient (i.e., working only on mini-batches), exhibits faster initial convergence compared to standard SA approaches (e.g., ADAM), produces networks that generalize well, and incorporates automatic hyperparameter selection. architectures and review various approaches to train such networks, with special emphasis on variable projection. Notably, ae provide new theoretical analysis to support a VarPro stochastic approximation method. In section 3, we introduce our new slimTrain approach that incorporates sampled limited-memory Tikhonov (slimTik) methods within the nonlinear learning problem. Here, we describe cross-validationbased techniques to automatically and adaptively select the regularization parameter. Numerical results are provided in section 4, and conclusions follow in section 5. input features Y ⊆ R be the data space containing input-target pairs (y, c) ∈ D. We focus on separable DNN architectures that consist of two separate phases: a nonlinear feature extractor F : Y × R the stochastic optimization problem where L : R R are regularizers. Here, E denotes the expected value over a distribution of inputtarget pairs in D. squares loss function promotes data-ﬁtting and is well-suited for function approximation tasks whereas a cross-entropy loss function is preferred for classiﬁcation tasks where the network outputs are interpreted as a discrete probability distribution [28]. In this work, we focus on exploiting separability to improve DNN training for function approximation or data ﬁtting tasks such as PDE surrogate modeling [49, 55] and dimensionality reduction such as autoencoders [23]. Hence, we restrict our focus to a stochastic least-squares loss function with Tikhonov regularization where Φ : R operator, k·k for θ and W, respectively. The paper is organized as follows. In section 2, we describe separable DNN 2. Exploiting separability with variable projection. Given the space of . In general, the goal is to learn the network weights, (W, θ), by solving Choosing an appropriate loss function L is task-dependent. For example, a leastminΦ(W, θ) ≡ EkWF (y, θ) −ck+kLθk+kWk,(2.2) state-of-the-art, approach to solve (2.2) is stochastic optimization over both sets of weights (W, θ) simultaneously (i.e., joint estimation). While generic and straightforward, this fully-coupled approach can suﬀer from slow convergence (e.g., due to illconditioning) and does not attain potential beneﬁts that can be achieved by treating the separate blocks of weights diﬀerently (e.g., exploiting the structure of the arising subproblems). We seek computational methods for training DNNs that exploit separability, i.e., we treat the two parameter sets θ and W diﬀerently and exploit linearity in W. Three general approaches to numerically tackle the optimization problem (2.2) while taking advantage of the separability are as follows. ables θ and W is alternating optimization [3]. For (2.2), this corresponds to alternating between two stochastic optimization problems. Note for simplicity of presentation we assume that each of following optimization problems has a unique minimizer. Suppose we initialize θ and Notice that convergence of this approach can be slow when variables are tightly coupled [2, 53]. Furthermore, this approach is not practical in our settings, since minimization problem (2.3) and (2.4) are computationally expensive, particularly the non-convex, high-dimensional, often non-smooth optimization problem for θ. block coordinate descent. The general idea of a block coordinate descent approach for (2.2) is to approximate the alternating optimization of (2.3) and (2.4) via iterative update schemes (e.g., one iteration of an iterative optimization step) for each set of variables [53]. Note that under certain assumptions, a block coordinate descent method applied to two sets of parameters has been shown to converge [33, 42]. Although a block coordinate descent approach provides a computationally appealing alternative to the fully coupled and alternating directions approaches, this approach, like alternating directions, suﬀers from slow convergence when the blocks are tightly coupled. and block coordinate descent is to solve (2.3) with respect to W while performing an iterative update method for (2.4) with respect to θ. This can be seen as a stochastic approximation version of a variable projection approach [21]. Formally, we can write the iteration in terms of the reduced stochastic optimization problem where (2.6) Notice that (2.6) is a stochastic Tikhonov-regularized linear least-squares problem and, under mild assumptions, there exists a closed form solution, i.e., (2.7) 2.1. SA methods that exploit separability. A standard, and the current Alternating directions. One approach that exploits separability of the vari- Block coordinate descent. A practical alternative for alternating directions is Variable projection (VarPro). A compromise between alternating directions Here, µ the derivation can be found in Appendix A. cW(θ) in (2.6), VarPro uses an iterative scheme, typically an SGD variant, to update θ. The key is to ensure that the mini-batch gradients used to update θ are unbiased. To the best of our knowledge, we provide the ﬁrst theoretical analysis demonstrating that VarPro in an SA setting produces an unbiased estimate of the gradient. We note that the derivation, presented for stochastic Tikhonov-regularized least-squares problems, can be extended to any objective function which is convex with respect to the linear weights, such as when using a cross-entropy loss function. At the k-th training iteration, we select a mini-batch the training set, T the T A VarPro SA method applied to (2.5) considers the reduced functional at the k-th iteration, where regularized linear least-squares problem over the entire data space. to θ and compute the next iterate, Here, γ denotes an appropriate learning rate and p based on the current estimate of θ selection of p knowing information about the derivative of (2.8). Explicitly, we compute the derivative of Φ (2.11) Note that, contrary to VarPro derivations in deterministic settings [21, 14, 34], the ﬁrst term in (2.11) does not vanish. This is because conditions for Φ, the objective function for expected value minimization problem (2.6) but may not be optimal for Φ we observe that the term vanishes in expectation over all samples, that is, (2.12) (θ) = EF (y, θ) and Σ(θ) = E(F (y, θ) − µ)(F (y, θ) − µ). Details of 2.2. Theoretical justiﬁcation for VarPro in SA methods. After solving for In the context of the DNN training problem, let T ⊆ D be a ﬁnite training set. we seek to minimize the function cW(θ) is obtained from (2.6), i.e., the solution to the stochastic Tikhonov- To update the nonlinear weights, we select a “descent” direction pwith respect [DΦ(W, θ)]· DcW(θ)= [DE Φ(W, θ)]· DcW(θ) Because (2.11) is equal to the gradient of the full objective function Φ in expectation, we say the update for θ is unbiased. Since SA methods can handle unbiased noisy gradients, one could deﬁne a VarPro SGD approach using the following unbiased estimator for the gradient, where the derivative is (2.14) Note that D lelized over samples. VarPro approach is marred by the impracticality of computing each mini-batch update of θ, one would need to recompute propagating many samples through the network. Since a computation is costly, in terms of time and storage, we can only obtain an approximation of EcF (y, θ) a sample covariance matrix. The accuracy of the approximation, and hence the expected bias of the gradients for the nonlinear weights, will depend on the size of the sample. However, these quantities still depend on θ, and hence for any iterative process where θ is being updated, these values need to be recomputed at each iteration. imation (SAA) approach. In SAA methods, one ﬁrst approximates the expected loss using a (large and representative) sample. The resulting optimization problem is deterministic and a wide range of optimization methods with proven theoretical guarantees can be used. For example, inexact Newton methods may be utilized to obtain fast convergence [5, 36, 54]. Solving a deterministic SAA optimization problem with an eﬃcient solver guarantees the linear model ﬁts the sampled data optimally at each training iteration. Note that if an SAA approach were used to solve both (2.5) and (2.6) with the same (ﬁxed) sample set, then this would be equivalent to the variable projection SAA approach described in [34]. Indeed, there are various recent works [34, 40, 16] that exploit the separable structures (1.1) of neural networks in SAA settings in order to accelerate convergence. However, the disadvantage of SAA methods is that very large batch sizes are needed to obtain suﬃcient accuracy of the approximation and to prevent overﬁtting. Although parallel computing tools (e.g., GPU and distributed computing) and strategies such as repeated sampling may be used, the storage requirements for SAA methods remain prohibitively large. θ and W simultaneously) and the alternating minimization approach represent two extremes: the former is a tractable approach, but ignores the separable structure while the latter exploits separability, but is computationally intractable in the stochastic DΦ(W, θ) = D1|T|kWF (y, θ) −ck+kLθk 2.3. Challenges of VarPro in stochastic optimization. The appeal of a One way to approximatecW(θ) is to replace the vector µ(θ) and the matrix with sample mean approximations and the covariance matrix Σ(θ) with A practical strategy to approximatecW(θ) is to use a sample average approx- To summarize section 2, the widely-used, fully-coupled approach (optimizing over setting. Although a block coordinate descent approach decouples the parameters and replaces expensive optimization solves with iterative updates, a VarPro approach can mathematically eliminate the linear weights, thereby reducing the problem to a stochastic optimization problem in θ only. The resulting noisy gradient estimates for θ are unbiased when variants to update θ. However, computing and poor approximations may lead to a large bias in the gradients for θ. Hence, providing an eﬀective and eﬃcient way to approximate practical implementation of VarPro stochastic optimization. slimTrain as a tractable variant of VarPro in the SA setting, which adopts a sampled limited-memory Tikhonov scheme to approximate the linear weights and to estimate an eﬀective regularization parameter for the linear weights. The key idea is to approximate the linear weights using the output features obtained from recent mini-batches and nonlinear weight iterates. By storing the output features from the most recent iterates, slimTrain avoids additional forward and backward propagations through the neural network which, especially for deep networks, is computationally the most expensive part of training, and hence adds only a small computational overhead to the training. in section 2, approximating in the gradient for θ, see (2.12). This motivates us to use state-of-the-art iterative sampling approaches to solve stochastic, Tikhonov-regularized, linear least-squares problems. For exposition purposes, we ﬁrst reformulate (2.6) as (3.1) where w = vec(W) ∈ R A(y, θ) = F (y, θ) structure extends to a mini-batch T i = 1, . . . , |T where Henceforth, in this section, since θ is ﬁxed in (3.1), we use A tion purposes. Tikhonov (slimTik) methods are specialized iterative methods developed for solving 3. Sampled limited-memory DNN training with slimTrain. We present 3.1. Sampled Tikhonov methods to approximatecW(θ). As described Z(θ) =F (y, θ) ··· F (y, θ)∈ R, Introduced in [47, 13], sampled Tikhonov (sTik) and sampled limited-memory stochastic regularized linear least-squares problems. For an initial iterate w sTik iterate is given by where w ing previously computed output features, Λ + parameter estimate. The sTik iterates can also be expressed in update form as an SA method, with g the current mini-batch and B global curvature information of the least-squares problem. Note that contrary to standard SA methods, (3.3) does not require a learning rate nor a line search parameter. The learning rate can be interpreted as one, which is optimal for Newton’s method. to be set in advance, has been replaced with a new parameter estimate Λ which can be chosen adaptively at each iteration. Each Λ tion parameter at iteration k and can change at each iteration (Λ correspond to regularization parameters from previous iterations). In fact, the parameters λ and Λ all training samples), the sTik iterate is identical to the Tikhonov solution of (3.1) with λ = exemplify the convergence of sTik in Figure 1 when approximating Matlab’s peaks function [25]. Moreover, it has been shown that sTik iterates converge asymptotically to a Tikhonov solution and subsequently adaptive parameter selection methods were developed in [47]. of standard regularization parameters methods, such as the discrepancy principle (DP), unbiased predictive risk minimization (UPRE), and generalized cross validation (GCV) techniques can be utilized. Indeed, sampled regularization parameter selection methods sDP, sUPRE, and sGCV for sTik and slimTik and their connection to the overall regularization parameter λ can be found in [47]. In this work, we focus on regularization parameter selection via sGCV since this method does not require any further hyperparameters (e.g., noise level estimates for the mini-batch), and we have observed that sGCV provides favorable λ estimates. For details on the GCV function, see original works [22, 51] and books [26, 50]. The sGCV parameter at the k-th slimTik iterate can be computed as is the previously computed estimate, A, . . . , Aare matrices contain-P Importantly, the regularization parameter λ in (3.1), which is typically required PΛwhere k is the number of iterations required for one epoch. We Since (3.2) and (3.6) correspond to standard Tikhonov problems, extensions where model matrices, sTik may not be practical since each iteration requires either solving a least-squares problem (3.2) whose coeﬃcient matrix is growing at each iteration or updating matrix B the sampled limited-memory Tikhonov (slimTik) method was proposed in [47]. Let r ∈ N problems, it can be shown that for the case r = 0, the slimTik method is equivalent to the stochastic block Kaczmarz method. Furthermore, for linear least-squares problems with a ﬁxed regularization parameter, theoretical convergence results for slimTik with memory r = 0 were developed in [13]. We point out that limited memory methods like For some problems, e.g., inverse problems where Arepresent large-scale forward be a memory depth parameter. Then, the k-th slimTik iterate has the form We provide a few remarks about the slimTik method. For linear least-squares slimTik were initially developed to address problems where the size of w is massive, but this is not necessarily the case in DNN training where the number of weights in w may be modest. However, as we will see in subsection 3.2, a limited memory approach is suitable and can even be desirable in the context of solving nonlinear problems, where nonlinear parameters have direct impact on the model matrices A In this work, we are interested in incorporating extensions of slimTik with adaptive regularization parameter selection for nonlinear problems that exploit separability. separable structure of many DNNs and integrates the slimTik method for eﬃciently updating the linear parameters and for automatic regularization parameter tuning. We consider the slimTik update of W to serve as an approximation of the eliminated linear weights in VarPro SA from (2.6). Speciﬁcally, at the k-th iteration, with and Λ equivalent to the slimTik method for arg min iterative process and because of the dependence on previous θ algorithm is provided in Algorithm 3.1. Algorithm 3.1 slimTrain: sampled limited-memory training for separable DNNs for separable nonlinear inverse problems in [11], but there are some distinctions. First, 3.2. slimTrain. Our proposed SA algorithm, slimTrain takes advantage of the = mat(w(Λ)) where is computed using the sGCV method (c.f., (3.4)). Notice that this is not γ, regularization parameter α We note that an SA method that incorporates the slimTik method was considered the results in [11] use a ﬁxed regularization parameter, but here we allow for adaptive parameter choice, which has previously only been considered for linear problems. We note that updating regularization parameters in nonlinear problems (especially stochastic ones) is a challenging task, and currently there are no theoretical justiﬁcations. Second, all forward matrices were recomputed for each new set of nonlinear parameters in [11]. That is, for updated estimate θ Such an approach would be computationally demanding for DNN learning problems, since this would require revisiting previous mini-batches and re-computing the forward propagation matrix for new parameters θ we will show that these methods can perform well in practice. adds some computational costs compared to existing SA methods like ADAM; however, those are modest in many cases and the overhead in computational time can be reduced by an eﬃcient implementation. The additional costs stem from solving for the optimal linear weights in (3.6) and approximating the optimal regularization parameter using the sGCV function (3.4). The costs of these steps depend on the size of the nonlinear feature matrix, A memory matrix, M batches, and the number of linear weights. In the case when the linear weights are applied via dense matrix, we can exploit the Kronecker structure in our problem; see subsection 3.1 for details. The Kronecker structure results in solving n squares problems simultaneously where each problem is moderate in size (typically, on the order of 10 decomposition (SVD) to solve the least-squares problem. We also re-use the SVD factors for eﬃciently adapting the regularization parameter. For the peaks and surrogate modeling experiments (subsection 4.1 and subsection 4.2), we implement the Kronecker-structure framework in Matlab. The code is available in the Meganet.m repository on https://github.com/XtractOpen/Meganet.m. tantly, a convolution), eﬃcient iterative solvers, such as LSQR [38] that only require matrix-vector products and avoid forming the matrix explicitly, can be used to ﬁnd the optimal linear weights. Such methods were employed in [11] where the authors applied slimTik to massive, separable nonlinear inverse problems where the data matrix could not be represented all-at-once. Modiﬁcations of the sGCV function using stochastic trace estimators can then be used for estimating the regularization parameter eﬃciently; for more details, see [47]. input but only one output channel. Exploiting the separability between the diﬀerent channels and the small number of weights per channel, we form the nonlinear feature matrix, A based automatic regularization parameter selection as in the dense case. To be precise, the columns of A (on the order of 10 small because the number of weights parameterizing the linear operator, denoted |w|, 3.3. Eﬃcient implementation. Training separable DNNs with slimTrain In the case when the linear weights parameterize a linear operator (most impor- In subsection 4.3, the linear weights parameterize a convolution layer with several , explicitly in our implementation. This allows us to use the same SVDis small (on the order of 10 taking advantage of the structure of convolutional operators; each channel has its own linear weights and the samples share the same weights. For storage eﬃciency, we can form the smaller matrix A rule (3.3) to adjust the linear weights. We implement the convolutional operator framework in Pytorch [39]. The code is available on github at https://github.com/ elizabethnewman/slimTrain. DNNs using slimTrain with automatic regularization parameter selection. In this section, we ﬁrst provide a general discussion on numerical considerations of our proposed method in subsection 3.3. In subsection 4.1, we explore the relationship between various slimTrain hyperparameters (e.g., batch size, memory depth, regularization parameters) in a function approximation task. Our results show that automatic regularization parameter selection can mitigate poor hyperparameter selection. In subsection 4.2, we apply slimTrain to a PDE surrogate modeling task and show that it outperforms the state-of-the-art ADAM for the default hyperparameters. In subsection 4.3, we apply slimTrain to a dimensionality-reduction task in which the linear weights are applied via a convolution. Notably, we observe faster convergence and, particularly with limited training data, improved results compared to ADAM. function approximation task. We train a DNN to ﬁt the peaks function in Matlab, which is a mixture of two-dimensional Gaussians. We use a small residual neural network (ResNet) [27] with a width of w = 8 and a depth of d = 8 corresponding to a ﬁnal time of T = 5. Further details about the ResNet architecture can be found in Appendix B. The nonlinear feature extractor maps F : R 528 is the number of weights in θ. The ﬁnal linear layer introduces the weights W ∈ bias. Our training data consists of 2,000 points sampled uniformly on the domain [−3, 3] × [−3, 3]. We display the convergence of slimTrain for various combinations of hyperparameters in Figure 2. depth is apparent in Figure 2. In this scalar-function example, we seek 9 weights (i.e., W ∈ R the problem is underdetermined (or not suﬃciently overdetermined) and solving for W signiﬁcantly overﬁts the given batch at each iteration. This results in the slow, oscillatory convergence behavior, particularly with a batch size of |T ﬁrst column). When the memory depth and batch size are large enough (e.g., r = 100 in the |T the training loss converges faster and to a lower value (Figure 2, purple line in ﬁrst column). a proxy to the goal of DNN training: to generalize to unseen data. To illustrate the generalizability of DNNs trained with slimTrain, we display the DNN approximations in Figure 3 corresponding to a batch size of |T the convergence plots. cantly impacts the approximation quality of the network when training with a ﬁxed regularization parameter (Figure 3, second column set of ﬁgures). If the optimization problem over-regularizes the linear weights (λ = 10 4. Numerical results. We present a numerical study of training separable 4.1. Peaks. To explore the hyperparameters in slimTrain, we examine a scalar , where the number of columns equals the width of the ResNet plus an additive The interplay between number of output features, the batch size, and the memory ) to ﬁt (r + 1)|T| samples. With small memory depth and batch size, | = 1), the linear least-squares problem is suﬃciently overdetermined and Solving the optimization problem and decreasing the loss of the training data is Exempliﬁed in Figure 3, the choice of regularization parameter for W signiﬁis smoother than the true peaks function and does not ﬁt the extremes tightly (Figure 3, ﬁrst row). In the under-regularized case (λ = 10 depth (r = 0), W overﬁts the batches and the DNN approximation does not generalize well (e.g., we miss the small peaks) (Figure 3, third row). With a well-chosen regularization parameter (here, λ = 10 true peaks function, but tuning this regularization parameter can be costly (Figure 3, second row). In comparison, the DNN approximations when automatically choosing a regularization parameter using the sGCV method are good approximations and look similar, no matter the initial regularization parameter or memory depth (Figure 3, ﬁrst column set of ﬁgures). lem, as illustrated for the λ = 10 (Figure 4, ﬁrst column), the linear least-squares problem is underdetermined for memory depths r = 0 and r = 5 and is overdetermined when r = 10. To avoid overﬁtting in the underdetermined cases, larger regularization parameters are selected. In the The selected regularization parameters are related to the ill-posedness of the proboverdetermined case, overﬁtting is less likely and thus less regularization is needed. slimTrain decreases the training loss and generalizes well to unseen data. The choice of regularization parameter signiﬁcantly impacts the resulting network: too much regularization and the training stagnates; too little regularization and the training oscillates. Employing adaptive regularization parameter selection mitigates these extremes and simpliﬁes the costly a priori step of tuning the parameter. has been increasing interest in using DNNs as eﬃcient surrogate models for computationally expensive tasks arising in scientiﬁc applications. One common task is partial diﬀerential equation (PDE) surrogate modeling in which a DNN replaces expensive With an adequate choice of memory depth and batch size, training a DNN with 4.2. PDE surrogate modeling. Due to their approximation properties, there linear system solves [37, 4, 56, 49]. Here, we consider a parameterized PDE where u is the solution to a PDE deﬁned by A and parameterized by y (which could be discrete or continuous). In our case, the solution is measured at discrete points given by the linear operator P and the observations are contained in c. The goal is to train a DNN as a surrogate mapping from parameters y to observables c and avoid costly PDE solves. which models physical phenomena in many ﬁelds including climate modeling [48] and mathematical biology [17, 8]. As its name suggests, the CDR equation is composed of three terms: a diﬀusion term that encourages an even distribution of the solution u (e.g., chemical concentration), a convection (or advection) term that describes how the ﬂow (e.g., of the ﬂuid containing the chemical) moves the concentration, and a reaction term that captures external factors that aﬀect the concentration levels. In our example, the reaction term is a linear combination of 55 diﬀerent reaction functions and the parameters y ∈ R are measured at the same 6 spatial coordinates and 12 diﬀerent time points; for details, see [34]. We train a ResNet with a width of w = 16 and a depth of d = 8 In our experiment, we consider the convection diﬀusion reaction (CDR) equation corresponding to a ﬁnal time of T = 4; see Appendix B for further details. The linear weights in the ﬁnal, separable layer are stored as a matrix W ∈ R number of columns is the width of the ResNet plus an additive bias. The results of training the ResNet with slimTrain are displayed in Figure 5. The major takeaway is that slimTrain exploits the separable structure of the ResNet and, as a result, trains the network faster and ﬁts the observed data better (lower loss) than ADAM with the recommended learning rate (γ = 10 In Table 1, we examine if the performance of slimTrain and ADAM generalizes to unseen after 20 epochs; we choose 20 epochs to analyze early performance and because the training loss decreases more closes after 20 epochs in Figure 5. The training and validation losses are close for both slimTrain and ADAM, indicating that both training algorithms produce networks that generalize well. For ADAM’s suggested learning rate, γ = 10 orders of magnitude less than that of ADAM. When the learning rate is tuned to γ = 10 is achieved by slimTrain. Most signiﬁcantly, the performance of slimTrain is less sensitive to the choice learning rate. tween batch size, memory depth, and the number of output features. In this experiment, because W ∈ R 17 unknowns in each problem. Illustrated in Figure 6, when the memory depth is small (r = 0, 5), each least-squares problem is underdetermined or not suﬃciently overdetermined, and hence more regularization on W is needed to avoid overﬁtting. Because we use sGCV to automatically select the regularization parameter, the training with slimTrain achieves a comparable loss for all memory depths. In addition, the learning rate to update θ plays a role in the regularization parameters chosen. When the learning rate is large (γ = 10 change rapidly. As a result, larger regularization parameters are selected, even in the suﬃciently overdetermined case (r = 10), to avoid ﬁtting features that will change signiﬁcantly at the next iteration. a better accuracy than ADAM using the recommended learning rate (γ = 10 , the performance of ADAM improves, but the overall best performance As with the numerical experiment in subsection 4.1, there is a relationship be- In this surrogate modeling example, slimTrain converges faster to the same or exploiting the separability of the DNN architecture. Tuning the learning rate can improve the results for ADAM, but training with slimTrain produces comparable results and reaches a desirable loss in the same or fewer epochs. Using sGCV to select the regularization parameter on the weights W provides more robust training, adjusting automatically to the various hyperparameters (memory depth, learning rate) to produce consistent convergence. using two neural networks: an encoder that represents high-dimensional data in a lowdimensional space and a decoder that reconstructs the high-dimensional data from this encoding, illustrated in Figure 7. Training an autoencoder is an unsupervised learning problem that can be phrased as optimization problem (4.2) where the components of the objective function are the following: remainder of this section. The data consists of 60,000 training and 10,000 test gray-scale images of size 28 × 28 y P Rpy P R 4.3. Autoencoders. Autoencoders are a dimensionality-reduction technique minΦ(w, θ, θ) ≡ EkK(w)F(F(y, θ), θ) − yk • Encoder: F: Y × R→ Ris the encoding neural network that reduces the dimensionality of the input features nto an intrinsic dimension nwith n n. Typically, the true intrinsic dimension is not known and must be chosen manually. The weights are θ∈ R, the number of encoder weights is |θ|, and the regularization parameter is α≥ 0. • Decoder Feature Extractor: F: R× R→ Ris the decoder feature extractor. The weights are θ∈ R, the number of weights is |θ|, and the regularization parameter is α≥ 0. • Decoder Final Layer: K(·) : R→ Ris a linear operator, mapping w to a matrix K(w). For instance, K(w) could be a sparse convolution matrix which can be accessed via function calls. The learnable weights w have a regularization parameter λ ≥ 0. For notational simplicity, we let θ = (θ, θ) and α = α= αfor the In this experiment, we train a small autoencoder on the MNIST dataset [31]. (i.e., 784 input features). We implement convolutional neural networks for both the encoder and decoder with intrinsic dimension n Unlike the dense matrices in the previous experiments, the ﬁnal, separable layer is a (transposed) convolution. Because convolutions use few weights and the prediction is high-dimensional, the least-squares problem is always overdetermined for this application. Hence, we require only a moderate memory depth in our experiments and, motivated by our results in subsection 4.1 and subsection 4.2, we use a memory depth of r = 5 when training with slimTrain. ure 8. Here, we see that training with slimTrain converges faster than ADAM in the ﬁrst 10 epochs and to a comparable lowest loss after 50 epochs. Each training The convergence results comparing slimTrain and ADAM are presented in Figscheme forms an autoencoder that approximates the MNIST data accurately and generalizes well, even after the ﬁrst epoch. However, the absolute diﬀerence between the slimTrain approximation and the true test images after the ﬁrst epoch is noticeably less noisy than the ADAM-trained approximations after the ﬁrst epoch, particularly for a poor choice of regularization parameter on w (e.g., λ = 10 because we employ automatic regularization parameter selection, the performance of slimTrain was nearly identical with diﬀerent initial regularization parameters, Λ We display the case that produced slightly less oscillatory convergence. (α = 10 training method. The results in Figure 9 support our choice of a small regularization parameter on θ. It can be seen that smaller regularization parameters on θ produce better DNN approximations. When α is poorly-chosen (in this case, when α is large), slimTrain produces a considerably smaller loss than training with ADAM. Hence, training with slimTrain and sGCV can adjust to poor hyperparameter selection, even when those hyperparameters are not directly related to the regularization on w. that training with slimTrain oﬀers signiﬁcant performance beneﬁts in the limiteddata setting; see Figure 10. When only a few training samples were used, training with slimTrain produces a lower training and validation loss. In the small training data regime, the optimization problem is more ill-posed and there are fewer network weight updates per epoch. Hence, the automatic regularization selection and fast initial convergence of slimTrain produces a more eﬀective autoencoder. ple with a ﬁnal convolutional layer, slimTrain converges faster initially than ADAM to a good approximation and is less sensitive to the choice regularization on the nonlinear weights, θ. In the case of limited data, a common occurance for scientiﬁc Using a good choice of the regularization parameter on the nonlinear weights ) is partially responsible for the quality approximations obtained for each In addition to adjusting regularization parameters for the linear weights, we found Consistent with the results in our previous experiments, in the autoencoder examapplications, the training problem becomes more ill-posed. Here, slimTrain produces networks that ﬁt and generalize better than ADAM. By solving for good weights w and automatically choosing an appropriate regularization parameter at each iteration, slimTrain achieves more consistent training performance for many diﬀerent choices of hyperparameters. separability inherent in most commonly-used architectures whose output depends linearly on the weights of the ﬁnal layer. Our proposed algorithm, slimTrain, leverages this separable structure for function approximation tasks where the optimal weights of the ﬁnal layer can be obtained by solving a stochastic regularized linear least-squares problem. The main idea of slimTrain is to iteratively estimate the weights of the ﬁnal layer using the sampled limited-memory Tikhonov scheme slimTik [13], which is a state-of-the-art method to solve stochastic linear least-squares problems. By using slimTik to update the linear weights, slimTrain provides a reasonable approximation for the optimal linear weights and simultaneously estimates an eﬀective regularization parameter for the linear weights. The latter point is crucial – slimTrain does not require a diﬃcult-to-tune learning rate and automatically adapts the regularization parameter for the linear weights, which can simplify the training process. In our numerical experiments, slimTrain is less sensitive to the choice of hyperparameters, which can make it a good candidate to train DNNs for new datasets with limited experience and no clear hyperparameter selection guidelines. the variable projection [20, 35] (VarPro) scheme extended to the stochastic approxi- 5. Conclusions. We address the challenges of training DNNs by exploiting the From a theoretical perspective, slimTrain can be seen as an inexact version of mation (SA) setting. Using this viewpoint, we show in subsection 3.2 that we obtain unbiased gradient estimates for the nonlinear weights when the linear weights are estimated accurately. This motivates the design of slimTrain as a tractable alternative to VarPro SA, which is infeasible as it requires re-evaluation of the nonlinear feature extractor over many samples after every training step. The computational costs of slimTrain are limited as it re-uses features from the most recent batches and therefore adds little computational overhead; see subsection 3.3. In addition, slimTrain approximates the optimal linear weights obtained from VarPro, thereby reducing the bias introduced by the approximation when updating the nonlinear weights. mated hyperparameter selection, are demonstrated by the numerical experiments for both fully-connected and convolutional ﬁnal layers. In subsection 4.1, we explore the relationship of the slimTrain parameters, observing that memory depth and batch size play a crucial role in determining the ill-posedness of the least-squares problem to solve for the linear weights. The regularization parameter adapts to the least-squares problem accordingly – larger regularization parameters are selected when the problem is underdetermined. In subsection 4.2, we observe that slimTrain is less sensitive to the choice of learning rate, outperforming the recommended settings for ADAM. Again, the regularization parameters adapt to the learning rate – larger parameters are chosen when the nonlinear weights change more rapidly. In subsection 4.3, we show that slimTrain can be applied to a ﬁnal convolutional layer and outperforms ADAM in the limited-data regime, which is typical in scientiﬁc applications. Numerical Analysis in Data Science in 2020. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the National Science Foundation. From a numerical perspective, the beneﬁts of slimTrain, and speciﬁcally auto- Acknowledgments. This work was initiated as a part of the SAMSI Program on