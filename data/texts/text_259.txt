Traditional approaches to next-item and next basket recommendation typically extract users’ interests based on their past interactions and associated static contextual information (e.g. a user id or item category). However, extracted interests can be inaccurate and become obsolete. Dynamic attributes, such as user income changes, item price changes (etc.), change over time. Such dynamics can intrinsically reect the evolution of users’ interests. We argue that modeling such dynamic attributes can boost recommendation performance. However, properly integrating them into user interest models is challenging since attribute dynamics can be diverse such as time-interval aware, periodic patterns (etc.), and they represent users’ behaviors from dierent perspectives, which can happen asynchronously with interactions. Besides dynamic attributes, items in each basket contain complex interdependencies which might be benecial but nontrivial to eectively capture. To address these challenges, we propose a novel aware encoder to allow the model to capture various temporal patterns from dynamic attributes. To eectively learn useful item relationships, intra-basket attention module is proposed. Experimental results on three real-world datasets demonstrate that our method consistently outperforms the state-of-the-art. CCS Concepts: • Information systems → Recommender systems; • Computing methodologies → Neural networks. Additional Key Words and Phrases: Dynamic Attributes, Context Interaction Learning, Next Basket Recommendation ACM Reference Format: Yongjun Chen, Jia Li, Chenghao Liu, Chenxi Li, Markus Anderle, Julian McAuley, and Caiming Xiong. 2021. Modeling Dynamic Attributes for Next Basket Recommendation. In CARS: Workshop on Context-Aware Recommender Systems (RecSys ’21), October 2nd, 2021, Online, Worldwide. ACM, New York, NY, USA, 10 pages. https://doi.org/nn.nnnn/nnnnnnn.nnnnnnn Sequential recommendation systems have been successfully applied to various applications such as product recommendation, food recommendation, music recommendation, etc. There are two lines of sequential recommendation tasks based on dierent assumptions about users’ interaction behavior. Next item recommendation assumes that users interact with items sequentially, so that recommendations can be made by modeling the sequential semantics of the interaction history [ ttributes (namedAnDa). AnDa separately encodes dynamic attributes and basket item sequences. We design a periodic Next basket recommendation [16,28] assumes that users interact with multiple items during each round (i.e., a basket). The goal is to recommend a basket of items that a user is likely to interact with at the next time step. In the next basket recommendation task, in addition to the sequential patterns underlying historical interactions, items in each basket , and context of users or items often provides useful information. . Existing solutions for next-basket recommendation tasks train a sequential recommender based on the users’ interaction history [8,11,18,28], or with additional static contextual attributes [2] (product category, brand, etc.) to extract users’ interests. However, extracted interests can be inaccurate and obsolete. Dynamic attributes, which change over time, appear in many applications, and can provide more accurate descriptions of the shift of a user’s interests or changes in item properties. For example, in a bank product recommendation scenario (see Figure 1), products from the bank are recommended to customers. Given only a sequence of monthly records of a customer’s products, the next basket recommended to the user is deterministic. Instead, dynamic attributes such as the household income and the customer membership type , which are changing overtime, help a recommender to beter capture a user’s changing interests. Case 1 and 2 in Figure 1 illustrate that two customers with the same historical purchase behaviors but dierent household income and membership type sequences can have dierent interests. Although it is essential to model dynamic attributes, properly integrating dynamic attributes into sequential models is challenging. First, the temporal patterns underlying dynamic attributes can be diverse. There can be time-interval (i.e., two items purchased with dierent time-intervals has dierent impact on users’ future purchase behaviors.) and periodic patterns (i.e., seasons, weekday/weekend patterns, etc.). Second, dynamic attributes represent users’ behaviors from dierent perspectives, and can happen asynchronously with interactions. Directly concatenating basket items with dynamic attributes at each time step may not be stable to model diverse sequential patterns. Besides dynamic attributes, in users’ historical interactions, items in each basket contain complex interrelationships (item correlations, co-purchases, etc.). Existing solutions [3,11] either pre-dene static item correlations based on co-occurring items, or (vanilla) attention to extract item correlations based on the whole items the last basket. However, multiple item interrelationships can exist based on multiple subsets of items in a basket and together inuence next basket items. For example, in a grocery shopping, a customer bought apple, banana, TV and Speaker. The apple and banana are high correlated while TV and Speaker are also high correlated. The user may purchase some fruits again with TV accessories. Using multi-head attention allows the model to capture dierent item relationships under dierent subset of items in a historical basket. To address the above challenges, we propose a novelAttentivenetwork to model theDynamicattributes as well as users’ historical interacted items (AnDafor short). AnDa separately encodes and learns representations from dynamic attributes and interactions with basket items. To allow the model to capture time-interval aware and periodic patterns, we propose an input encoder containing a time-aware padding and periodic index embedding to encode dynamic attributes. To capture complex item relationships in each basket, an intra-basket attentive module is introduced. It is applied to each basket item to extract useful item relationships. We conduct experiments on three real-world datasets for next basket recommendation. Our experimental results demonstrate that our proposed method signicantly outperforms baseline approaches. 2 RELATED WORK 2.1 Next Basket Recommendation To capture sequential patterns at a basket level for next basket recommendation, DREAM [ basket using max and average pooling and learns the sequence representation through an RNN-based network. ANAM [2] improves upon DREAM by considering static item attributes using vanilla attention. Sets2Sets [ as a multiple baskets prediction and proposes a RNN based encoder-decoder method to improve the performance. To capturing item relationships at each basket, Beacon [ on the co-occurring items in the observed training baskets and then incorporates it into an RNN-based model. IIANN [ learns the correlation between the most recent basket items and the target item to summarize users’ short-term interests. In this work, we use multi-head self-attention within each basket so that complex item interrelationships (e.g., co-occurrences) can be captured. MITGNN [ across dierent users’. In comparison, our work focus on leverage dynamic attributes for providing more accurate user interests. 2.2 Feature Interaction Learning Factorization Machines (FMs) [ for recommendation [ feature interactions using DNN. NFM [ Wide&Deep [ interactions. Dierent from Wide&Deep, Deep&Cross [ and xDeepFM [ to capture high-order feature interactions implicitly, lack good explanation ability in general. To this end, AutoInt [ uses a self-attention mechanism to model high-order interactions with a more precise explanation of the interacted features. Inspired by this, we also apply multi-head self-attention to learn higher-order feature interactions and item interrelationships in each basket. 2.3 Aention Mechanisms With the success of Transformer networks in machine translation tasks [ is the rst work that uses a pure self-attention mechanism to model sequential recommendation and demonstrates better performance than RNN-based methods. TiSASRec [ interval between two adjacent interactions. BERT4Rec [ FDSA [29 vanilla attention to capture users’ interests. However, it does not incorporate dynamic user attributes and only models the sequential patterns at the item level instead of the basket level. Although this approach could be extended for 5] model uses a wide part to model second-order interactions and a deep part to model the higher-order 13] proposes a CIN module to take the outer product at a vector-wise level. These works, which use DNN ] further improves sequential recommendation by incorporating the usage of static item attributes and using 𝐴predicted relevance score for item 𝑖 at time 𝑡 modeling dynamic features, it uses vanilla attention to average out dierent attributes at each time step. As a result, it is not able to learn high-order feature interactions. Also, it loses the temporal aspects of individual features. For example, if the trend of a usage metric is going up or down in the past three months, FDSA will not capture such a trend due to its averaging operation. Instead, our time-level attention module can capture these temporal patterns. 3 PROPOSED APPROACH In this section, we rst dene the notation and formalize the next basket recommendation task with dynamic user attribute information. And then we present the proposed frameworkAnDain detail. The framework is illustrated in Figure 2 and the notation is summarized in Table 1. 3.1 Problem Statement In next basket recommender systems with dynamic attributes, historical basket interactions and dynamic attribute sequences are given, and the goal is to recommend the next basket’s items. Formally, we denotes𝑈,𝑉and𝐹a sets ofno users, items and user attributes respectively. For a user𝑢 ∈ 𝑈, a sequence of baskets𝐵=𝐵, 𝐵, · · · , 𝐵represents his or her item interactions sorted by time.𝑇is the maximum time steps of each sequence, and𝐵⊆ 𝑉is a set of itemsno that user𝑢interacted with at time step𝑡. A sequence𝐴=𝐴, 𝐴, · · · , 𝐴represents the value of dynamic user attributes of user𝑢ordered by time. Specically,𝐴∈ 𝐹are all the attribute values of𝑢at time step𝑡. The goal is to predict basket items that user 𝑢 will interact with at time step 𝑇 + 1 given 𝑇 historical baskets 𝐵and attributes 𝐴. 3.2 Time-Interval and Periodic Aware Input Encoder Embedding Lookup:For each basket of dynamic attributes𝐴, we model categorical and numerical features dierently. Categorical attributes𝐴⊆ 𝐴are represented by an|𝐹|-dimensional multi-hot vector denoted by𝑒∈ R. Numerical attributes are normalized into the range[−1,1]using min-max normalization, denoted as𝑠∈ R. Each basket of items𝐵is represented by a|𝑉 |-dimensional multi-hot representation, denoted by𝑒∈ R. After that, we apply a concatenation-based lookup function [2] to encode 𝑒and 𝑒: where𝑅 ∈ Rand𝑄 ∈ Rare learnable embedding matrices for categorical attributes and items. The “CONCAT-LOOKUP” function retrieves the corresponding embedding vectors and then concatenates them together to form matrices𝑠∈ R, and𝑠∈ R.𝐷is the embedding dimension of each item and categorical attribute. |𝑉|is the total number of items in𝐵. Since the number of items in each basket varies, we set the maximum number of items in the basket as the largest basket size in the training set|𝑉|, and add padding vectors for baskets smaller than Time-aware Padding Operation: steps’ information. If the sequence length is shorter than Otherwise, we truncate to the last the sequence length is 𝑆, and 𝑆 Periodic Index Embedding: periodic patterns. The index repeats over every capturing seasonal patterns when the time interval between each two baskets is one month. Positional embeddings [ that commonly used to identify item positions is also used in this paper. Formally, we concatenate the periodic and positional index embedding with periodic and positional embedding vectors. Then a basket sequence is represented as add positional and periodic index embeddings to 𝑆 3.3 Time Level Aention Module To capture temporal patterns from (MHSA) [24]. Formally, let 𝐿 where learned parameters (𝐶 = (|𝐹 as the padded basket item, categorical attribute, and numerical attribute sequences respectively. ℎis the number of sub-spaces,𝑊∈ R,𝑊∈ R,𝑊∈ Rand𝑊∈ Rare Following [10], we add causality mask to avoid future information leek. To enhance the representation learning of the self-attention block, residual connections [6], dropout [22], layer normalization [1], and two fully connected layers with ReLU activation functions are added to form the entire multi-head self-attention block (MHSAB) as follows: We stack multiple attention blocks to capture more complex feature interactions: where𝐿= 𝐿andℎis the number of heads at the𝑘attention block.𝐿is the output after stacking multiple timelevel attention layers. The extracted representation vector at time step𝑡can be denoted𝐿and contains information extracted from time 1 to time 𝑡 of the input sequence. 3.4 Intra-Basket and Intra-Aribute Self-Aention Modules Item correlations in each basket can reveal some useful information such as co-purchase relationships. The key problem is how to determine which items should be combined or are correlated. In this paper, we use multi-head self-attention to learn information such as items’ correlation relationships. Specially, given representations of all items in a basket 𝐿, a single-head self-attention module will rst compute the similarity matrix which is seen as item correlation scores, and then it updates the item representation𝑙by combining all relevant items using the similarity coecients (generated based on the similarity matrix). We use MHSAB to enhance the model’s capability of capturing complex item correlations, which is formed as follows: where 𝐿is the output of the time level attention module at time 𝑡, and ℎis the (𝑘 + 1)attention block. We stack Eq. 5𝑚times to capture more complex item relationships𝐿. Similarly, we can stack Eq. 5 on dynamic attribute (named Intra-Attribute Attention) to get higher level categorical attribute interactions 𝐿. 3.5 Model Training The encoded user representations are projected as:𝐿= FFNN ([𝐿, 𝐿, 𝐿]), whereFFNNis a feed forward network,𝐿∈ Ris the nal representation given dynamic attributes and basket items from time step 1 to 𝑡. We then adopt the binary cross-entropy loss as the objective for training the model dened as: where𝜎is the sigmoid function𝜎 (𝑥) =1/(1+𝑒).𝑄is the item embedding matrix which is shared for encoding basket items in input encoders. The target basket items for user𝑢are a shifted version of𝐵, denoted by𝐵, 𝐵, · · · , 𝐵. 4 EXPERIMENTS 4.1 Experimental Seing 4.1.1 Datasets. Table 2 summarizes the statistics of the datasets. EPR is a private dataset sampled from a leading enterprise cloud platform. The task is to recommend products to businesses. Examples of the dynamic attributes are behavior metrics on the website, sales information, and marketing activities of the business. SPR product recommendations for the Santander bank. Ta-Feng to create train, validation, and test sets by chronological order. The <train, validation, test> sets for EPR, SPR, and Ta-Feng datasets are <1 respectively. The interval between each two time steps is 1 month for EPR and SPR datasets, and 1 day for Ta-Feng dataset. 4.1.2 Baselines & Evaluation Metrics. We include three groups of baseline methods: PopRec considers no sequential patterns; FMC [ with (FDSA+) or without (DREAM [ methods on the whole item set without sampling. All the items are rst ranked and then evaluated by Hit Rate ( Normalized Discounted Cumulative Gain ( and NDCG with K=5. 4.1.3 Parameter Seings. We tune the embedding dimension 0.01}, and dropout from {0.0, 0.1, 0.2, 0.5}. For DREAM, we tune with RNN, GRU, and LSTM modules. AdamOptimizer is used to update the network with moment estimates layers from 16, and 30 in EPR, SPR, and Ta-Feng respectively. We report the hyper-parameter sensitivity study results in Figure 3. 4.2 Overall Performance Comparison Table 3 shows overall results compared with baseline approaches. We observe that, rst, our proposed approach consistently outperforms all baselines signicantly in terms of Hit Rate, NDCG, and MAP by 3.65% - 21.87%, 9.09% - 43.76%, and 2.32% - 24.53%, which demonstrate the eectiveness of our proposed method. Beside, The next basket recommenders (DREAM, Beacon, CTA, and Sets2Sets) outperform those for next item recommendation (FMC, FPMC). This indicates that learning the sequential patterns with the encoding of the intra-baseket information can better capture users’ dynamic interests. The FDSA+ method performs the best among baselines in the EPR and SPR datasets, while Sets2Sets performs the best in the Ta-Feng dataset. The main reason is that FDSA+ leverages attribute information where EPR and SPR have more attributes. We also report the models’ average inference time (milliseconds per sequence) on 400 sequence inputs in Table 3 (last row). The proposed method takes more time to generate recommendation lists than baseline methods, though is comparable with Set2Sets, DREAM, CTA, and FDSA+. Datasets/Information # Users # Items # Attributes Sparsity Avg. Baskets Avg. Basket Size 18] and FPMC [18] are Markov Chain-based sequential methods; and Neural Network based methods {1,2,4}and head number on each attention block from{1,2,4,6}. Maximum sequence lengths are set as 12, Table 3. Performance Comparison of dierent methods on next basket recommendation. Bold/underlined scores are the best/second best in each row. The last column shows AnDa’s relative improvement over the best baseline. 4.3 Ablation Study To understand the impact of dierent components in AnDa, we conduct a detailed ablation study using the SPR dataset in Table 4.AnDa(P)is AnDa without periodic index embedding. The results show that the periodic index can help capture users’ seasonal purchase patterns, and thus helps to improve performance.AnDa(B)is AnDa with basket information only. Without dynamic attributes,AnDa(B-)removes the intra-basket module from AnDa(B),AnDa(T) is AnDa without using intra-basket and intra-attribute modules on both items and attributes, andAnDa(I)is AnDa without applying the time level attention module. The performance degradation on the sub-models shows the benets of each component. 4.4 Aention Visualization We visualize the attention weights of time-level, intra-attribute, and intra-basket attentions on sampled sequences from the SPR dataset in Figure 3 (B) to gain more insights. (a) and (b) are attention weights from two dierent layers (layer 1 and 4) of time level basket attention, (c) and (d) are from two dierent heads of the rst intra-attribute layer, and (e) and (f) are from two dierent head of the rst intra-basket layer. From (a) and (b), we can see the attention varies over dierent layers. While the weights in layer 4 focus more on recent items, the weights in layer 1 attend more evenly to all previous histories. From (c) and (d), we observe that the attention weights vary over dierent heads, and the module captures meaningful feature interactions. For example, in (c), the position(11,1)(marked by a red square) corresponding to interacted feature value <"Foreigner index": NO, "Customer’s Country residence": ES> (the bank is based in Spain, so a customer who lives in Spain is not a foreigner). We can also observe that intra-basket attention can capture dierent item relationships under dierent heads comparing with (e) and (f). Fig. 3. (A): hyperparamter sensitivity study results of AnDa. (B): visualization of aention weights on dierent MHSA modules. 5 CONCLUSION In this paper, we propose a novel attentive network AnDa, which models dynamic attributes to better capture users’ dynamically changing interests and intentions. AnDa separately extracts temporal patterns from dynamic attributes and user historical interactions with a novel input encoder. AnDa also generates feature interactions and uncovers item interrelationships in each basket with proposed intra-attribute and intra-basket modules respectively. We evaluate AnDa on three real-world datasets and demonstrate the usefulness of modeling dynamic attributes for next basket recommendation.