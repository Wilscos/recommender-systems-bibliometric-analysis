We consider conceptualizing the online research process as a speciﬁc instance of the scientiﬁc method: posing a question, collecting information to answer the question, organizing the information, and communicating the answer, where the steps are cycled multiple times in the process. In interviews conducted by the author, we talked to ﬁve college students about their research process. We learned that they begin the research process with an outline of how they think about a topic. They then search Google with targeted queries to ﬁnd information to support this structure. Along the way, their outline evolves and prompts new searches. We learned that people are frustrated by the lack of integration between their outline and searches. I’ve organized my ideas in this outline, but how do I communicate that to Google? How can I search for new information in the context of research I’ve already done? We learned that people ﬁnd it difﬁcult to survey a topic at a high level. I have this idea of the ﬁeld that I’m trying to explore, but how can I map that ﬁeld into subtopics to understand what makes it up? I ﬁnd a number of articles through my searches, but it’s overwhelming to manually read and organize all of them. As an extension to our previous work [ tool clusters documents on a map based on their similarity (as seen in Figure 1), the goal being that similar documents show up on the map next to each other and can be grouped into hierarchical clusters [ function to search on the map (as seen in Figure 2), project results in nearby space, and group them into hierarchical clusters [ We asked them: given this organized knowledge space, how would you want to navigate it? How would you want to organize it yourself? What kind of information would you map out? How is this type of search different from current solutions? We learned that they would want to start typing on a map, visualize related topics, and continue searching through those topics, building a hierarchy of what they are thinking and continuing to search in the context of that hierarchy. They mentioned a side tab with a traditional outline. One could click on the outline to go to that place on the map. One could use the outline to maintain a context of where they are in the knowledge space. We talked about multiple people interacting with the same information in Projection. How do other people structure these topics I’m interested in? In the context of a large organization, who should I talk to about this topic? Who has spent a lot of time structuring this realm of information? We learned that they would want to explore types of information such as wikipedia, research articles, web articles, and personal documents. We found that Projection can be compared to traditional search in the following way: instead of a list of pages, Projection visualizes a knowledge space. Instead of searching with a single natural language query, Projection searches with a hierarchy of natural language. Instead of each search being independent, Projection searches come together into one navigable structure. Communication of dense information between humans and machines is relatively low bandwidth. Many modern search and recommender systems operate as machine learning black boxes, giving little insight as to how they represent information or why they take certain actions. We present Projection, a mixed-initiative interface that aims to increase the bandwidth of communication between humans and machines throughout the research process. The interface supports adding context to searches and visualizing information in multiple dimensions with techniques such as hierarchical clustering and spatial projections. Potential customers have shown interest in the application integrating their research outlining and search processes, enabling them to structure their searches in hierarchies, and helping them visualize related spaces of knowledge. 3]. We demoed the functionality to the same ﬁve interviewees to spur discussion about product development. Consider an entity graph, where the meaning of an entity depends on its relationship to and the meaning of entities connected to it. We apply this data model to Projection such that each piece of information is an entity that can be structured and searched for in the context of other entities. As an example, consider a hierarchy of entities mapping the sub topics of a ﬁeld. The parent-child connections of the hierarchy communicate the relationship between entities and can be used to ﬁnd more entities that ﬁt into the structure. Mixed-initiative interaction refers to a ﬂexible interaction strategy in which each agent (human or computer) contributes what it is best suited at the most appropriate time [ and the computer responds with contextually relevant entities projected into the structure. For example, after creating an entity with the language "investing", the machine might project clusters of entities about types of investments, famous investors, or investment strategies. As more structure is given to the entities by the human, the machine has more context to suggest relevant types of entities. React [5] is a JavaScript library Projection uses for building declarative and component-based interfaces. React automatically re renders components when their inputs change, saving developers from manually triggering updates. Redux [6 by Redux. React components select subsets of this state to render and emit actions to update state based on user interaction. When a component’s selected state changes, it is automatically re rendered. With this functionality, Redux serves as a powerful communication model between many declarative functions that need access to the same state. Mapbox [ developers can interface with to render components in 2D/3D space–the library then manages the entire map once it is rendered: zoom, drag, re renders, etc. Projection uses AWS Amplify [ Amplify can deploy a GraphQL server and multiple DynamoDB tables to fully support queries and mutations in a managed cloud environment. ] is a predictable state container for JavaScript applications. State is interacted with via a global store exposed 7] is a JavaScript library for building map applications in the browser. It exposes a powerful map API that On the backend we use Tensorﬂow’s Universal Sentence Encoder [ FAISS [10] for large scale vector operations such as similarity search. The entity graph data model can be seen in various environments of computationally deﬁning meaning: the meaning of a word is the meaning of words around it [ learned connection weights [ meaning of a YouTube video is the context in which it was watched [14]. People also consciously organize meaning with discrete connections. A hierarchical research outline is a tree of natural language, document citations form a document graph, and digital friends lists connect us to others online. Unconsciously, a perspective could be that our brains deﬁne meaning by relating information in multiple dimensions. Visual perception relates points of space based on position and color. Auditory perception detects changes in pressure of the environment across time. Our brain relates information across all of these dimensions unconsciously, and we use this high dimensional perception to act. Work done by Yang et al. [ meaning. Compared to self-attention, which fuses the computation of entity structure and meaning into one network, their work decouples the two, allowing the learned entity structure to transfer to new tasks where different features may be needed to model semantic meaning. This is interesting from our perspective since Projection aims to be a mixed-initiative interface that allows people to structure information in any format. Structuring information in different ways or structuring different types of information may be analogous to different tasks, warranting consideration of different semantic features. Structures created by everyone using Projection can also be valuable training data for computing entity meaning. Examples of mixed-initiative interfaces include recommender systems of popular internet applications. People interact with social connections online and Instagram recommends new social connections [ and YouTube learns from this context to suggest new videos [ history to auto complete relevant queries. Many existing mixed initiative interfaces are imbalanced–the machine learning model operates in a higher dimension than the human guiding it, leading to a lower bandwidth of communication between the model and human. It is difﬁcult to understand the model’s interpretation of meaning and guide its suggestion making. To increase the bandwidth of communication between the model and human, a mixed-initiative interface can be designed with multiple dimensions: searching based on a hierarchy of language, clustering search results to communicate topics, projecting entities on a 2D/3D space to communicate their relationships, or at the highest dimension with a brainmachine interface. Even a small increase in dimension of the human side of the interface can exponentially increase the bandwidth of communication, improving the shared meaning representation between model and human. We used AWS Amplify [ object queries and mutations. We deﬁne a single GraphQL schema with our application’s data model and leverage Amplify to manage the rest. Outside of Amplify, we split the Projection architecture into two components: a web interface to search, explore, and structure information and a backend to support stateful computation of data such as document similarities, clusters, and spatial entity projections. Our full architecture can be seen in Figure 3. The interface supports two core actions: humans creating/structuring entities and the model suggesting new entities/clusters in the context of that structure. People drive the structuring of information and the model responds with information that meshes with that structure. To track application state, we deﬁne the data model shown in Table 1. We use React [ build modular interface components that react independently to application state changes and mirror the data model. The Menu supports creating and selecting maps. The Map component reacts to a map being selected in the Menu component, queries the map’s entities, and renders multiple Entity components on the map. The Entity and Document components query and render model data. Additional Cluster components render sets of Entitys or Documents as hierarchical clusters. This modular component architecture on top of React and Redux allow us to easily test new components. A new component can be inserted anywhere in the interface model, Redux can be leveraged to query state of other components and communicate state updates, and we can let React handle efﬁcient re renders based on state changes. At the core of Projection’s backend is FAISS [ similarities. A web server sits on top of FAISS and exposes add and query operations. We organize Projection’s backend computation into a functional pipeline for each operation. add supports loading new language entities into FAISS by ﬁrst computing a vector representation of the language with the Universal Sentence Encoder [ following pipeline to construct a response: embed the text of each entity, search for documents/entities in context of the tree, cluster the resulting documents/entities, and project the response entities in space near the input entity tree. One strategy we consider to search in hierarchical context is as follows: ﬁnd neighbors of the root entity, ﬁnd neighbors of children within the parent level’s neighborhood space, and continue in a depth ﬁrst search fashion. This would 9], then loading it into the index. The query operation receives an entity tree as input and executes the allow human operators to search by "partitioning" the meaning space with their structured entity hierarchy. With the functional pipeline architecture, new strategies can easily be injected into the pipeline for testing. To cluster entities represented by dense vectors in 2D space we ﬁrst reduce the dimensionality of the vectors from 512 to 2 with T-SNE, then apply agglomerative clustering to ﬁnd hiearchical structure. Our goal for projecting entities is to move a set of entities in 2D space and scale their distances while maintaining directional relationships. Our initial multiplication transform is acceptable for testing, but future work should use a known mathematical operation for maximum precision. To create an initial MVP based on the architecture described above, we gathered web pages from our personal search history and loaded them into the backend. Web pages can be explored with hierarchical clusters on the map [ searched for by typing on the map [3]. To evaluate our product, we demoed the functionality to ﬁve college students as discussed in the introduction. All interviewees were excited by the potential of the product, but had a different purpose in mind than a traditional search engine. They would use Projection when they didn’t have a speciﬁc answer in mind. They would want start typing on the map and explore the different perspectives of their natural language query. They would connect their initial query to new information, and continue their search/exploration of the knowledge space. For Projection to be usable, the interviewees highlighted a few points of important functionality. First, it needs to contain the data that they are interested in exploring. For those we interviewed, this was Wikipedia entries, research articles, web articles, and personal documents. Filters should be available to narrow the information considered to a certain type. They need to be able to maintain context of where they are in the knowledge space. It was easy to get lost on the map because parent clusters split into child clusters on zoom, and weren’t visible for reference after. They mentioned being able to see multiple levels of parent-child clusters on the map at the same time in order to maintain depth context. They also mentioned a sidebar with a traditional bullet point outline. This outline could represent their mapped entity structure in a familiar format and highlight where they currently are in the knowledge space. The outline would be interactive: clicks take one to that location on the map, and the current location on the map is highlighted in the outline. Another piece of requested functionality was group search. I have this group of entities here and I want to ﬁnd more entities that ﬁt in the group. In the future, this can easily be implemented with similarity search and tested in our backend’s query pipeline. We present Projection, a mixed-initiative interface to the research process. Through hierarchical clustering, spatial projections, and an intuitive knowledge structure, Projection can increase the bandwidth of communication between humans and machines, a central issue in today’s interfaces to machine learning. The human and machine represent information as an entity graph, which can be used to communicate structure and meaning by both parties. Our scalable software architecture will allow rapid experimentation of new features and parallel development going forward. We have a strong roadmap of future work ahead based on our interview discussions. Likely the most important step going forward is expanding the types and scale of information accessible in Projection beyond our personal search history. Implementing a web scraper to collect web pages can automate this process, but customers should also be able to import their own documents or trigger a scrape of a website. This would allow potential customers to test the tool with information that is valuable to them, making their experience and feedback more authentic. Experimenting with the interface is another crucial step going forward. We will need to discover the right amount of information to display at one time, how to communicate where one is in their deﬁned structure and the overall knowledge space, and how to communicate the contents of varying types of entity clusters. To conduct real tests, this should be done after additional information is loaded into Projection so that potential customers can use the product for practical purposes. With the ability to conduct real tests, we will also be able experiment with various machine learning methods on the backend. Testing different search strategies and machine learning models will help us identify what solves customer problems the best. We will also continue abstracting our architecture with the goal of enabling the lowest cost of experimentation. The cost of experimentation can be broken into three parts: complexity of implementation, effort required to implement, and time required to implement. A set of functions that react independently to global state changes has proven to be a powerful architecture, one that we are looking to adopt across our entire software stack. Functions in the frontend or backend, across any number of physical machines, communicate via access-controlled namespaces of a global state store and react to queried state changes to produce additional state changes. We would like to thank Foaad Khosmood for advising our research.