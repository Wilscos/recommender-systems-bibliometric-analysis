Social recommendation based on social network has achieved great success in improving the performance of recommendation system. Since social network (user-user relations) and user-item interactions are both naturally represented as graph-structured data, Graph Neural Networks (GNNs) have thus been widely applied for social recommendation. Despite the superior performance of existing GNNs-based methods, there are still several severe limitations: (i) Few existing GNNs-based methods have considered a single heterogeneous global graph which takes into account user-user relations, user-item interactions and item-item similarities simultaneously. That may lead to a lack of complex semantic information and rich topological information when encoding users and items based on GNN. (ii) Furthermore, previous methods tend to overlook the reliability of the original user-user relations which may be noisy and incomplete. (iii) More importantly, the item-item connections established by a few existing methods merely using initial rating attributes or extra attributes (such as category) of items, may be inaccurate or sub-optimal with respect to social recommendation. In order to address these issues, we propose an end-to-end heterogeneous global graph learning framework, namely Graph Learning Augmented Heterogeneous Graph Neural Network (GL-HGNN) for social recommendation. GL-HGNN aims to learn a heterogeneous global graph that makes full use of user-user relations, user-item interactions and item-item similarities in a unied perspective. To this end, we design a Graph Learner (GL) method to learn and optimize user-user and item-item connections separately. Moreover, we employ a Heterogeneous Graph Neural Network (HGNN) to capture the high-order complex semantic relations from our learned heterogeneous global graph. To scale up the computation of graph learning, we further present the Anchor-based Graph Learner (AGL) to reduce computational complexity. Extensive experiments on four real-world datasets demonstrate the eectiveness of our model. • Information systems → Recommender system. social recommendation, graph learning, graph neural network ACM Reference Format: Yiming Zhang, Lingfei Wu, Qi Shen, Yitong Pang, Zhihua Wei, Fangli Xu, Ethan Chang, and Bo Long. 2018. Graph Learning Augmented Heterogeneous Graph Neural Network for Social Recommendation. In WXXXX, June 03–05, 2021, XXX, XX. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/1122445.1122456 Recent years have witnessed the rapid development of social recommendation, which leverages social network as side information to eectively alleviate the problem of data sparsity [17,27]. Conceptually, users’ preferences are often largely inuenced by people around them [3,7], including parents, friends, classmates, and so on. Therefore, a social recommendation system based on users’ social relationships usually signicantly improves the quality of recommendations. Recently, there are a surge of interests in graph neural networks (GNNs) [10,14,19,24,26,31], which have been proven to eectively learn node representations from graph-structured data. Since social network (user-user relations) and user-item interactions are both naturally represented as graph-structured data [5,28], GNNs have thus been widely employed to learn the representations of users and items, which has been shown to improve the performance of the recommendation system [11,15,29,33]. Furthermore, in order to enrich the types of the potential graphs and extract richer side information, a few existing works have considered the construction of extra item-item graph structure [6, 12, 30]. Despite the promising results current methods have achieved, there are still several severe limitations in their approaches. First, few existing GNNs-based methods have considered a single heterogeneous global graph which takes into account user-user relations, user-item interactions and item-item similarities simultaneously. As a result, these methods may fail to capture high-order crosssemantic information and limit the delivery of messages. Second, previous methods tend to overlook the reliability of the original user-user social graph that may be noisy and incomplete, partially because that the original connections often only record the social relationships between users but rather reect the similarity of users preferences. For example: Bob is Ketty’s husband, Jim is Ketty’s colleague, Bob and Jim both like sports, while Ketty likes to read. However, in the user-user graph, Bob and Jim are not directly connected, while they are connected to Ketty respectively. We can learn from this example that there may be conicts in interests between nearby neighbors, while distinct neighbors could have similar preferences. Such a topology will make the users preferences extracted from user-user graph deviate from the real situation, which may lead to the sub-optimal performance of downstream task. Third, previous methods do not fully exploit the relationships between items. Though a few existing methods attempt to construct the item-item graph, they only utilize the items’ initial rating attributes or extra attributes (such as category) in an ad-hoc fashion, which barely reects the optimized item-item graph structure with respect to downstream social recommendation. In order to address these issues, we propose an end-to-end heterogeneous global graph learning framework, namely Graph Learning Augmented Heterogeneous Graph Neural Network (GL-HGNN) for social recommendation. Our GL-HGNN aims to learn a heterogeneous global graph that makes full use of user-user relations, user-item interactions and item-item similarities in a unied perspective. In order to obtain and optimize heterogeneous global graph structure, we present a Global Graph Learning module. To this end, we rst establish item-item subgraph by calculating the similarity of the rating vectors. Then, our proposed Graph Learner (GL) method is employed to extract richer implicit relationships and lter out the explicit noisy edges in user-user relation subgraph and itemitem similarity subgraph. Specically, our GL method can learn the implicit connections between nodes by measuring the embeddings similarity between target nodes in each mini-batch and all nodes. We then combine the learned implicit graph with the initial graph to obtain the rened heterogeneous global with respect to downstream task. To capture the high-order complex semantic relations from different types of edges in the heterogeneous global graph, we also present a Heterogeneous Graph Neural Network (HGNN), to model the rened heterogeneous global graph. Technically, it is crucial to scale up the computation of user-user subgraph and item-item subgraph learning, especially when the number of users or items is very large. To this end, we further utilize anchor-based approximation technique [4] to design a scalable Graph Learner, namely Anchor-based Graph Learner (AGL). By selecting the anchor node set instead of all nodes to calculate the similarity to the target nodes, we can signicantly reduce computational complexity. In addition, we design a joint learning method and a hybrid loss which considers both graph learner loss and rating loss. Through multiple epochs of optimization, we can get more rened heterogeneous global graph structures with respect to social recommendation, and more reliable vector representations of users and items. To summarize, we highlight our main contributions as follows: •We construct a heterogeneous global graph with dierent semantic meta-paths for social recommendation. We propose a novel framework named GL-HGNN to learn the heterogeneous global graph of dierent relationships in a unied perspective, which can capture the complex semantic relations and rich topological information. •We propose the Global Graph Learning module to construct item-item connections and optimize both user-user and itemitem subgraph structures, so as to obtain the rened global graph with respect to the downstream social recommendation. In addition, we design an Anchor-based Graph Learner (AGL) method to scale up the proposed method, which can signicantly reduce computational complexity. •We conduct experiments on four real-world datasets. The experimental results demonstrate the eectiveness of our proposed model over state-of-the-art methods, and also verify that our scalable AGL module can reduce the computational costs. With the popularity of social platforms, social recommendation has become one of the hottest areas in recommendation research. Early research mainly employed matrix factorization-based methods for recommendation, such as SoRec [16], TrustFM [32] and TrustSVD [9]. Recently, deep learning-based methods have become the most successful methods in recommendation research. Plenty of recent works [7,18,21] have applied deep learning to social recommendation tasks and achieved promising performance. In recent years, a lot of works [2,5,29] transform user-user relations and user-item interactions to graph-structured data, and employ the graph neural network (GNN) to learn better user and item representations. In addition, to capture connections among items and enhance the performance of social recommendation, several eorts adopted the item attributes to construct item-item graph. For example, GraphRec+ [6] and DANSER [30] leveraged the item’s collaborative information to build item-item graph, while KCGN [12] utilized inter-dependent knowledge of items to construct graph. However, few of these methods extract high-order cross-semantic information by modeling a joint heterogeneous global graph, which includes three kinds of meta-paths: user-user, user-item and itemitem. As GNNs rely on the good quality of the original graph, graph structure learning method was proposed to alleviate this limitation. Figure 1: The overview of our model GL-HGNN. We rst establish the item-item subgraph structure. We employ the Graph Learner to update and optimize the graph structure. We utilize HGNN to model the global graph to extract complex crosssemantic information. The output embeddings are sent to the predictor for prediction. We design a hybrid loss including Graph Learner loss and rating loss for training. LDS [8] proposed to model each edge inside the adjacency matrix. IDGL [4] jointly and iteratively learned graph structure and graph embedding based on node features. HGSL [34] generated three kinds of graph structures to fuse an optimal heterogeneous graph. However, most of these models are applied to node classication or graph-level prediction tasks. To our best knowledge, we are the rst to adopt the graph structure learning to improve the quality of the heterogeneous global graph in social recommendation. In this paper, we dene𝑈 = {𝑢, 𝑢, ..., 𝑢}and𝑉 = {𝑣, 𝑣, ..., 𝑣} as the sets of users and items, separately. The user-user relations can be dened as𝐺= {𝑈, E}, in whichEis the set of edges, and(𝑢, 𝑢, 𝑟)inErepresents𝑢is related to𝑢. And the useritem interactions can be represented as the user-item graph with 𝐾kinds of edges𝐺= {𝑈,𝑉 , E}. The edge inEis dened as (𝑢, 𝑣, 𝑟), which indicates that the user𝑢rates the item𝑣as 𝑘. Let𝑈 (𝑢)denote the set of users related to user𝑢. In addition, 𝑉(𝑢)is dened as the set of items that the user𝑢rates𝑘to, while 𝑈(𝑣) as the set of users who give a rating 𝑘 to 𝑣. Problem Formulation. Letp,q∈ Rdenote initial embeddings of the target user𝑢and item𝑣. Given user-user relations and user-item interactions, the task is to predict the explicit score ˆ𝑟that user 𝑢will rate item 𝑣. Figure 1 provides the overall architecture of our model. We aim to construct a heterogeneous global graph, and extract cross-semantic relations and rich topological information from it. We rst build item-item subgraph𝐺= {𝑉, E}by similarity between items. Figure 2: The structure of proposed Global Graph Learning. The edge(𝑣, 𝑣, 𝑟)inEmeans items𝑣and𝑣are similar. We also dene𝑉 (𝑣)to denote the set of items similar to item𝑣. We combine these three graphs{𝐺, 𝐺, 𝐺}into a heterogeneous global graph𝐺, which contains two kinds of nodes, three kinds of meta-paths and𝐾 +2 kinds of edges, i.e. user-user relation edge, 𝐾kinds of rating edges, and item-item similarity edge. In order to get a better graph structure with respect to the downstream task, capture implicit connections and lter out possible noise, we design the Graph Learner to optimize user-user (u2u) and item-item (i2i) connections. Moreover, the rened global graph is passed as input to a heterogeneous graph neural network to distill high-order complex semantic information. We employ a rating predictor to predict the score that target user will rate the candidate item. We design a hybrid loss to train our model. Figure 2 shows the architecture of Global Graph Learning module, which constructs the heterogeneous global graph and optimizes the graph structures. It should be noted that there is usually no connection information between items in the raw data, but itemitem connections can enrich the graph structure and improve the receptive eld, which allows us to extract more information of both users and items. For this reason, we need to construct the item-item edges rst. Then, we employ Graph Learner (GL) to optimize the user-user and item-item subgraph by adding or removing the edges with the method of calculating the similarity of node embeddings [4]. We will introduce the details below. Item-item Connections Construction. We utilize the rating matrixR ∈ Rto calculate the cosine similarity between items following previous work [6]. For the rating matrixR, we take the𝑗th column vectoreas the item𝑣vector. The similarity calculation formula is denoted as follows: For each item, we choose the most similar𝐾items to create the edges. In this way, we construct the item-item connections. Graph Learner. Due to the noise or lack of possible information in the original graph structure, we propose to adapt Graph Learner to optimize the input u2u and i2i subgraphs’ topologies. For u2u Graph Learner, the input is the initial subgraph𝐺with node set{𝑢, 𝑢, . . . , 𝑢}and embedding set{p, p, . . . , p}. For the target node𝑢, we apply the multi perspective learning method to calculate the similarity between 𝑢and all nodes as follows: 𝑠𝑖𝑚(𝑢, 𝑢) =1𝐹𝑠𝑖𝑚(𝑢, 𝑢), 𝑛 = 1, 2, . . . , 𝑁(2) Where𝐹is the number of perspectives. For each perspective, we can choose one from three methods, which are called weighted cosine, attention, and add attention: Equation 3 is the principle of weighted cosine, andWis the weight of neural network. Equation 4 shows the calculation method of attention, whereWis a weight matrix. Equation 5 presents the principle of the add attention, wherewmaps the embedding of the node to 1 dimension and𝜎is the ReLU function. During the experiment, we mainly use the weighted cosine method, and the other two methods will be compared in the ablation study. For all the target user nodes in one batch, the initial adjacency matrix with all nodes isA∈ R, where𝐵is the number of target user nodes in the current batch. And we can obtain a new learned implicit adjacency matrixAwith similarity calculation. Though the initial graph may be noisy or missing information, it still contains rich valuable topological information. Therefore, we employ a weight value𝜆to combine the implicit matrix with the initial matrix: Each element in the rened matrix˜Arepresents the similarity of two nodes. In order to prevent information redundancy caused by too many edges, we set a truncation length𝐿. For each target node, we truncate the rst𝐿nodes with the highest similarity to establish new connections, and the remaining nodes are not connected to the target node. In this way, we can get the rened subgraph 𝐺. For the input i2i subgraph𝐺, we can apply the same method to get the rened subgraph 𝐺. Anchor-based Graph Learner. In the real world, the number of nodes is often very huge. For target nodes, if we calculate the similarity of all the nodes to them, the costs of computation are high. Inspired by [4], we proposed a scalable Anchor-based Graph Learner (AGL). Next, we take the item-item subgraph as an example. For target item nodes, we randomly select𝐻(𝐻≪ 𝑀) nodes as the anchor nodes set{𝑣, 𝑣, . . . , 𝑣}. We can get the initial adjacency matrix between target nodes and anchor nodes A∈ Rfrom the initial connections. We calculate the target-anchor similarity matrix A, as Equation 2. Then we use the weight value𝜆and the truncation length𝐿to calculate the rened item-item subgraph𝐺. Similarly, we can randomly select 𝐻(𝐻≪ 𝑁 )user nodes as the anchor nodes set, and employ AGL to get the rened user-user subgraph 𝐺. In this subsection, we discuss how to extract user and item latent features in a unied perspective, based on the rened global graph 𝐺= {𝐺, 𝐺, 𝐺}learned by Global Graph Learning. The global graph includes three kinds of semantic meta-paths: user-user relations, user-item interactions and item-item similarities. Inspired by [20], we employ a Heterogeneous Graph Neural Network (HGNN) to extract high-order information and fuse dierent semantic information. We employ𝑇-layer HGNN to model our rened heterogeneous global graph to distill cross-semantic information. For the target user𝑢and target item𝑣, the initial input embeddings of the rst layer arep= pandq= q. Letpandqdenote the representations of user𝑢and the item𝑣after the propagation of 𝑡-th layer. We next introduce the user node aggregation and item node aggregation in each layer of HGNN. User node aggregation. Generally, for each user node in the rened heterogeneous global graph, there exits one type of edges 𝑟connecting the user neighbors and𝐾types of edges𝑟, (𝑘 ∈ {1,2, . . . , 𝐾})connecting the item neighbors. For the user-user social semantic connections, we aggregate the features of user neighbors as follows: WhereWis a trainable transformation matrix,bis the bias vector, and 𝑐is the normalization coecient. Similarly, we perform user𝑢node aggregation based on𝐾types of user-item rating connections. Specically, for each type of edges 𝑟, we also aggregate neighbor items under the same rating level as follows where 𝑘 ∈ {1, 2, . . . , 𝐾 }. For user𝑢, we accumulate all messages propagated by dierent 𝐾 +1 types of edges[p, p, . . . , p]. Then, we aggregate the information of these 𝐾 + 1 embeddings: 𝜎is the ReLU function,pis the output embedding of user𝑢 in𝑡 +1-th HGNN layer. It is worth noting that, for the current layer, we integrate two kinds of meta-paths (user-user, user-item) information into the user’s features, while the item features already contain the item-item semantic information after𝑡layers aggregation. Therefore, the user’s features can also fuse item-item semantic information by the multi-layer HGNN. Item node aggregation. The target item𝑣also involves in two meta-paths: item-item similarity and user-item interactions including𝐾types of edges. Similarly, for the𝑡 +1-th layer, we propagate dierent mesages from𝐾 +1 types of edges and obtain𝐾 + 1 embeddings[q, q, . . . , q]of𝑣. Then we aggregate these embeddings into the output embedding q: After𝑇layers of HGNN, we can extract high-order and crosssemantic information from the rened heterogeneous global graph, which enables us to distill more latent features of users and items. The initial embeddings and output of each HGNN layer constitute the user𝑢embedding lists[p, p, . . . , p]and item𝑣embedding lists[q, q, . . . , q]. In this module, we design the shared attention mechanism to get the nal user and item latent embeddings. For the user𝑢, the nal embedding is dened as follows: WhereW,sandbare the shared trainable parameters,𝜎is the ReLU function. And the nal embeddingqof item𝑣can be calculated in the same way. In this paper, we focus on the rating prediction task in social recommendation, so we design the predictor based on multi layer perceptron (MLP) : Where [, ] is the concatenation operation. To better train our model, we design a special loss function, which contains two aspects of loss: (i) Graph Learner loss, (ii) rating loss. Graph Learner Loss. In our work, the updated graph structure plays an important role in rating prediction. In order to obtain the better graph topology with respect to the social recommendation task, we design the Graph Learner (GL) loss through graph regularization [1,4]. For the u2u GL, we can get the rened adjacency matrix˜A∈ R. Generally, graph regularization is often applicable for symmetric adjacency matrix. Since˜Ais not symmetric, we rst transform it to be symmetric as follows: Where∆ ∈ R(Δ=Í𝐴) is a diagonal matrix. As we all know, that values change smoothly among adjacent nodes is a widely applied assumption. Therefore, we utilizeˆAand initial user feature matrix P to design the smoothness loss as follows: Wheretr(·)indicates the trace of a matrix,L = D−ˆAis the graphÍ Laplacian, andD=ˆ𝐴denotes the degree matrix. However, only minimizing the smoothness loss will cause over smoothing, so we impose constraints[4] to control smoothness as follows: Where1indicates the vector in which elements are 1, andˆA indicates the Euclidean norm ofˆA. We then dene the overall Graph Learner loss of u2u GL as the sum of the previously dened losses: 𝛽 is a non-negative hyper-parameters. While for u2u AGL, we can convert the rened adjacency matrix ˜A∈ Rto the symmetric matrixˆAas Equation 17. And we can rewrite Equation 20 to dene the Anchor-based Graph Learner loss: We can also calculate i2i GL or AGL loss𝐿by the same method. Rating Loss. For the task of rating prediction, we adopt mean square error (MSE) loss function as: Where𝑟is the ground-truth value. For our model, we apply a hybrid loss to jointly learn the parameters: 𝛾,𝛾and𝜆are non-negative hyper-parameters.Θis the trainable parameters,Ω(·)denotes the L2 regularization. Through multiple epochs of optimization, we can iteratively learn an optimized global graph structure with respect to the social recommendation as well as reliable user and item features. GL-HGNN.As for GL-HGNN, the computational cost of the Graph Learner isO(𝐸 (𝑁 + 𝑀)𝐷)for𝑁user nodes,𝑀item nodes and𝐸 missing user-item rates to be predicted. The computational cost of HGNN isO(𝑇𝑋 (𝑀 + 𝑁 )𝐷), where𝑇denotes the number of layers and𝑋indicates the average neighbors of each node. The rating task costsO(𝐸𝑑𝐷)where𝑑is the hidden size, while the computational complexity of the hybrid loss isO(𝐸 (𝑁 + 𝑀)𝐷). The overall cost is aboutO((𝑇 𝑋 + 𝐸)(𝑁 + 𝑀)𝐷 + 𝐸𝑑𝐷). If we assume that𝐸 ≈ 𝑁 + 𝑀 and𝑇𝑋, 𝑑 ≪ 𝑁 + 𝑀, the overall time complexity isO((𝑁 + 𝑀)𝐷). AGL-HGNN.As for AGL-HGNN, the computational cost of the Anchor-based Graph Learner isO(𝐸 (𝐻+ 𝐻)𝐷), while computing node embeddings by HGNN costsO(𝑇𝑋(𝑀 +𝑁 )𝐷), where𝑋indicates the average neighbors of each node. The rating task also costs O(𝐸𝑑𝐷), and computing the hybrid loss costsO(𝐸 (𝐻+ 𝐻)𝐷). As 𝐻, 𝐻, 𝑑 ≪ (𝑁 + 𝑀), the overall time complexity of AGL-HGNN is O(𝑇𝑋(𝑁 + 𝑀)𝐷), which is linear to the number of user and item. Therefore, AGL-HGNN can signicantly reduce the computational complexity. In this section, we will detail the settings of our experiment and present the experimental results. To fully demonstrate the superiority of our model, we conduct experiments to verify the following four research questions (RQ): • (RQ1): Compared with the state-of-the-art models, does our model achieve better performance? • (RQ2): What are the impacts of key components on model performance? • (RQ3): How does the setting of hyper-parameters (such as the truncation length in Graph Learner) aect our model? • (RQ4): How can Global Graph Leaning module improve the performance of our model? 5.1.1 Datasets. We conduct experiments on several public social recommendation benchmark datasets Ciao[23], Epinions[22] and Flixster[13], which all contain rating information and social networks. The detailed statistics of dataset are given in Table 1. • Ciao: Ciao is drieved from a popular social networking ecommerce platform. We process two available versions of the Ciao datasets, separately called Ciao-5 and Ciao-28. Ciao-5 collects 5 categories of items and their corresponding users, while Ciao-28 contains all 28 categories of items (such as DVDS) and users. The rating range is[1,5]with the step size 1.. • Epinions: Epinions comes from a social based product review platform. The rating values contain ve discrete numbers, which are {1, 2, 3, 4, 5}. • Flixster: Flixster comes from a popular movie review website, where people can add others as friends to create the social network. The range of rating value is[0.5,5]with the step size 0.5. For each dataset, we select 20% as the test set, 10% as valid set and remaining 70% as training set. 5.1.2 Evaluation Metrics. In order to better evaluate the performance of models, we employ two widely used metrics, namely RMSE (root mean square error) and MAE (mean absolute error) [25]. The two metrics both indicate the error between the predicted value and the ground-truth, while RMSE is more sensitive to outliers. 5.1.3 Baselines. To evaluate the performance of our model, we select representative seven models, including classic and state-ofthe-art (SOTA) social recommendation models as follows: • SoRec[16]: It learns users’ feature vectors by decomposing the scoring matrix and the social relation matrix simultaneously. • TrustMF[32]: According to the direction of trust, this model maps users to the trusted space and the trustee space, by matrix factorization. • TrustSVD[9]: This is one matrix factorization-based model, aggregating friends embeddings into target users embeddings to learn explicit and implicit information. • DSCF[7]: This method proposes a deep learning-based framework, which captures the inuence of distant social relationships on target users. • GC-MC[2]: This model generates the implicit information between users and items in the form of information transfer in the bipartite interaction graph. However, it only models the links between users and item. In the experiment, we also join social network to make predictions. • GraphRec[5]: This method jointly captures the user-item interaction and opinion between users and items from useritem graph, and learns the heterogeneous social relationship between users from user-user graph. • DANSER[30]: This method constructs a large graph that contains user-user, item-item, and user-item sub-graphs. By modeling this large graph, it learns the dynamic and static attributes of users and items, and then fuses the dual attributes to predict users’ ratings on target items through one fusion strategy. • GraphRec+[6]: On the basis of Graphrec, Graphrec+ not only models user-item and user-user graphs, but adds itemitem graph to aggregate information between similar items. Table 2: Performance comparison of dierent models on the four datasets. The smaller the RMSE and MAE, the better the performance. 5.1.4 Parameters Seing. We implement our model based on Pytorch and DGL. We set the embedding dimension𝐷 =64, and the batch size as 128. For all trainable parameters, we initialize them with a Gaussian distribution with an average of 0 and a standard deviation of 0.01. We use mini-batch Adam optimizer to train the model parameters with initial learning rate of 0.001. In order to prevent over-tting, we add dropout layers with a probability value of 0.4 during training. In construction of item-item edges, we select top 20 items for each item to build connections, according to the similarity cosine values. For the Graph Learner, we search the weight𝜆of learned implicit graph structure in ity calculation in the Graph Learner, is tuned in the set of[1,2,3,4]. For the truncation length𝐿in Graph Learner, we obtain the optimal value in the range[20,40,60,80,100]through the grid search. In addition, we set the number of graph neural network layers in range of [1, 2, 3, 4]. In addition, we also apply Anchor-based Graph Learner module in our experiments. We dene the anchor rate 𝜏 = 𝐻/𝑁 = 𝐻/𝑀. We test the value of 𝜏 in the set [0.01, 0.02, 0.05, 0.1, 0.15, 0.2]. For all the baselines, in order to achieve the best performance of these models, we set the parameters strictly according to the papers. The experimental results of the baseline models and our models on four datasets are shown in Table 2. Based on the comparison in the table, we can summarize our ndings as follows: •Our model GL-HGNN comprehensively outperforms all the baseline models on the four datasets. The results indicate that our model is eective to the rating prediction task of the social recommendation. Dierent from the SOTA methods: GraphRec+ and DANSER, our approach models the heterogeneous global graph to capture high-order features and dierent semantic information. In addition, to obtain a better graph structure for social recommendation, GL-HGNN GL-HGNN-w/o GLs&i2i edges 0.9171 0.7077 1.0860 0.8369 employs Graph Learners to optimize initial u2u and i2i connections. Besides, compared with GL-HGNN, AGL-HGNN can achieve comparable results, even better ones sometimes. •Among all the baselines, the performance of deep learningbased methods is better than that of traditional methods, which shows that deep learning-based methods have a stronger learning ability for user relations and user-item interaction signals. Moreover, the GNN-based models achieve better results than other models without graph structure. It proves the eectiveness of GNN for social recommendation. Furthermore, GraphRec+ and DANSER achieve a better performance than other model without i2i subgraph construction. That suggests that adding extra i2i connections into the user-item graph can be helpful for social recommendation. In order to verify the eectiveness of some key modules, we conduct a series of ablation experiments on the Ciao-5 and Epinions datasets. The results are shown in Table 3. Firstly, we compare dierent calculation methods of nodes similarity in the Graph learner by replacing weighted cosine with attention and add attention. As can be seen in Table 3, it is clear that the weighted cosine method is the best one of three methods to capture similar attributes between nodes. Figure 3: Comparisons of dierent hyper-parameters w.r.t. the weight value 𝜆 perspectives 𝑃 and the truncation length 𝐿. Table 4: Performance with dierent number 𝑇 of HGNN layers. GL-HGNN-1 1.0501 0.8072 1.1276 0.8617 GL-HGNN-2 1.0343 0.7753 1.0912 0.8345 GL-HGNN-3 1.0320 0.7763 1.0709 0.8017 GL-HGNN-4 1.0398 0.7847 1.0846 0.8204 Besides, we explore to evaluate the eectiveness of the critical modules of GL-HGNN. We delete each module of GL-HGNN to observe the change of model performance, e.g., removing the u2u GL module and removing the i2i GL module. We can observe that the Graph Learner module is pivotal for the model performance by seeing "GL-HGNN-w/o GLs". These results demonstrate that a more suitable graph structure with respect to the downstream task plays an important role. In addition, without i2i connection information, the model performance declines to a certain extent, which shows that capturing implicit item relations from the user rating matrix is valuable for the rating prediction task. Global Graph LearningThe performance of the Global Graph Learning is mainly aected by four important parameters, i.e., the weight value𝜆of learnt implicit graph structure, the number of perspectives 𝑃, the truncation length 𝐿 and the anchor rate 𝜏. • Graph Leaner: For the rst three parameters, we adjust these parameters respectively for u2u and i2i Graph Learners on Epinions datasets. The results are shown in Figure Figure 4: Performance comparison and running time (seconds) with dierent anchor rates. 3 and we can see that: (i) For u2u and i2i Graph Learners, appropriate implicit graph weight values are required. If the weight is too large, a lot of noise may be introduced leading to sub-optimal performance. Too small weight value also hurt model performance since the learnt implicit information would become less. (ii) The increase of numbers of perspectives in GL does not necessarily lead to an increase in performance. On the contrary, too many perspectives may result in the over-tting. (iii) As shown in Figure 3c, the model performance reaches the best values when𝐿is 40. The performance change in the gure can indicate that too long or too short truncation will bring loss to the model eect. The most suitable truncation length should achieve the balance between eective information and irrelevant information in the graph learning. • Anchor-based Graph Leaner: For the anchor rate𝜏, we perform experiments on a single NVIDIA Tesla V100 GPU on Ciao-28 and Epinions datasets. We record the training time (seconds) of each epoch and RMSE evaluation results. As we can see from Figure 4, with the increase of the anchor rate, the performance of the model improves rst and then Figure 5: Visualization of an example for the case study from Ciao-5 data. Given the social connections among ve users and the corresponding user-item ratings, the prediction target is the rating of user 𝑢(white circular) on item 𝑣(green diamond). The coverage area represents the neighboring area of the target user or item. We obtain the updated graph through the Graph Learner, based on the initial graph. tends to be stable, while the training time is on the rise. It can be concluded that by controlling the anchor rate within a reasonable range, the model running time can be reduced without almost loss of model performance. Heterogeneous Graph Neural Network. Generally, the number𝑇of layers plays an important role for the GNN. We conduct the experiments on two datasets, and Table 4 presents the results of our model with dierent number of HGNN layers. From𝑇 =1 to 𝑇 =2, the model performance is greatly improved for both datasets, which shows the necessity of the high-order interconnection. For the Epinions dataset, from𝑇 =2 to𝑇 =3, the performance still increases quickly. Generally, appropriate increase in the number of layers will make information fusion deeper. However, when𝑇is too large, the performance will drop, probably because the model introduces too much noise or becomes over-smoothing. To show the eectiveness and rationality of Global Graph Learning module, we conduct a simple case study on several users from Ciao5 dataset. Specically, we make a comparison between GL-HGNN and the basic GNN-based methods (HGNN) without Global Graph Learning. Generally, we can build a graph using user-item ratings and user social relationships as the initial graph shown in the left part of Figure 5. However, we can nd that there is noise in this graph structure. Although there exist social connection between user𝑢 and𝑢, there are huge rating dierences between user𝑢and𝑢 on the same item set. Besides, despite𝑢and𝑢do not have the direct social connection, their rating histories are highly overlapped. It illustrates that they may be potential friends with the similar preferences. On the contrary, GL-HGNN propose to adopt the Global Graph Learning module to construct item-item connections and iteratively optimize the graph structure based on the initial graph. As shown in the the right part of Figure 5, the updated graph increases the potential relationship edge and reduces noise compared with the initial graph. We utilize the initial graph and updated graph to make scoring predictions through HGNN, respectively. Given the ground-truth rating 2, HGNN with the updated graph (GL-HGNN) predicts the result as 2.97, which is closer to the ground truth label compared to the value 3.44 generated by HGNN with the initial graph. The result demonstrates the validity and rationality of our proposed Global Graph Learning module. In this paper, we proposed a novel method GL-HGNN to learn the heterogeneous global graph with dierent relationships in a unied perspective for social recommendation. Our comparative experiments and ablation studies on four datasets illustrate that GL-HGNN can learn better graph structure with respect to social recommendation, and signicantly improve the performance of recommendation. In addition, to reduce the computational complexity, we propose the Anchor-based Graph Learner. In the future, we plan to introduce more nodes information (such as review information) for mapping multi-relation to multi-type edges in rened graph automatically.