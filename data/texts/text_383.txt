Recent advancements of sequential deep learning models such as Transformer and BERT have signicantly facilitated the sequential recommendation. However, according to our study, the distribution of item embeddings generated by these models tends to degenerate into an anisotropic shape, which may result in high semantic similarities among embeddings. In this paper, both empirical and theoretical investigations of this representation degeneration problem are rst provided, based on which a novel recommender model DuoRec is proposed to improve the item embeddings distribution. Specically, in light of the uniformity property of contrastive learning, a contrastive regularization is designed for DuoRec to reshape the distribution of sequence representations. Given the convention that the recommendation task is performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the regularization can be implicitly applied to the item embedding distribution. Existing contrastive learning methods mainly rely on data level augmentation for useritem interaction sequences through item cropping, masking, or reordering and can hardly provide semantically consistent augmentation samples. In DuoRec, a model-level augmentation is proposed based on Dropout to enable better semantic preserving. Furthermore, a novel sampling strategy is developed, where sequences having the same target item are chosen hard positive samples. Extensive experiments conducted on ve datasets demonstrate the superior performance of the proposed DuoRec model compared with baseline methods. Visualization results of the learned representations validate that DuoRec can largely alleviate the representation degeneration problem. • Information systems → Recommender systems. sequential recommendation, contrastive learning ACM Reference Format: Ruihong Qiu, Zi Huang, Hongzhi Yin*, and Zijian Wang. 2022. Contrastive Learning for Representation Degeneration Problem in Sequential Recommendation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM ’22), February 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3488560. 3498433 Figure 1: The item embedding matrix of the Amazon Clothing dataset is projected into 2D by SVD with colors indicating the frequency of items in the dataset. (a) Item embeddings learned by SASRec. Most of the rare items fall into a narrow cone, leading to high similarities among one another due to geometric properties. (b) Item embeddings learned by DuoRec. The distribution of item embeddings is more uniform in terms of both the magnitude and the frequency. (c) Normalized singular values of item embedding matrices. The fast decrease singular values of SASRec indicate the item embedding matrix is approximately in extreme lowrank. The slow decrease singular values of DuoRec reect that the item embeddings are more representative. Traditional recommender systems usually predict a user’s preference based on their historical records without considering the time factor [35,37–39] while the preference generally shifts as time goes on. Recent sequential recommendation methods exploit sequential patterns of the user’s interactions to capture the dynamic preference [3, 15, 19, 24–26, 31–34, 36, 41, 42, 51, 58]. During our study of these sequential models, a representation degeneration problem is observed in the item embeddings, whose distribution degenerates into a narrow cone and leads to an indiscriminate representation of the semantics. As shown in Figure 1(a), the item embeddings generated by SASRec [19] tend to be positive along the X-axis while distributing narrowly in the Y-axis direction. In this situation, most of the items are positively related to one another due to the geometric properties. This distribution is typically anisotropic [1,6,7,23,45], reected by which the singular values of the item embedding matrix quickly decrease to small values in the blue curve of Figure 1(c). There is a dominant dimension in the embedding matrix while other dimensions are ineective, which is approximately in extreme low rank. In contrast, item embeddings of the proposed DuoRec are distributed more uniformly around the origin point with the singular values decreasing more slowly as in Figure 1(b) and the orange curve in Figure 1(c). In this paper, we rst investigate the causes of the representation degeneration with theoretical analysis, motivated by which a novel sequential recommender model DuoRec is proposed to improve the distribution of item embeddings. Specically, inspired by the uniformity property of contrastive learning, a contrastive regularization is rst designed to enhance the uniformity of the sequence representation distribution. Given that the recommendation is generally performed by measuring the similarity between sequence representations and item embeddings in the same space via dot product, the contrastive regularization can implicitly inuence the item embeddings to distribute more uniformly. Existing contrastive learning methods generally generate positive samples using data-level augmentation, e.g., item cropping, masking, and reordering [51,52], which may cause semantically inconsistent samples. Considering that the input data itself is generally embedded into a dense vector, we propose a model-level augmentation method, which applies two dierent sets of Dropout masks to the sequence representation learning. Moreover, since there are a large number of semantically similar sequences representing similar user preferences, an extra positive sampling strategy is developed to generate hard and informative positive samples, where sequences with the same target item are considered semantically similar. The main contributions of this paper are summarized as follows: •The representation degeneration problem is identied and investigated in sequential recommender models with theoretical and empirical analysis. •To address the representation degeneration problem, a novel model DuoRec is proposed with a contrastive objective serving as the regularization over sequence representations. •A model-level augmentation for user sequences is designed based on Dropout. Furthermore, a positive sampling strategy is developed using the target item as the supervision signal. •Extensive experiments are conducted on ve benchmark datasets, which show the state-of-the-art performance of the proposed DuoRec model and the eectiveness of the contrastive regularization for sequential recommendation. In sequential recommendation, the problem setting is using historical interactions to infer user’s preference and recommend the next item. There is an item setVcontaining all items and|V|is the number of items. The historical interactions of a user are constructed as an ordered list𝑠 =[𝑣, 𝑣, . . . , 𝑣], where𝑣∈ V,0≤ 𝑖 ≤ 𝑡and 𝑡indicates the current time step as well as the length of𝑠. The recommendation task is to predict the next item at time step𝑡 +1, i.e.,𝑣for the user. In the following, bold lowercase and uppercase symbols are used to denote vectors and matrices respectively. To perform the next item prediction task in sequential recommendation, an interaction sequence is encoded into a xed-length vector by the model to conduct a retrieval over the item set. Given a sequence of items𝑠 =[𝑣, 𝑣, . . . , 𝑣], the model cal-Î culates the probability of this sequence as𝑝 (𝑠) =𝑝 (𝑣|𝑐), where𝑐is the context of an interaction at time step𝑛, containing all the previous interactions𝑣. Thelogprobability of the sequence can be represented with a 𝜃 parameterized model: where𝒉∈ Ris the𝑑-dimension vector of the context and𝒗, 𝒗∈ Rare the embeddings of item. Generally, the context is generated by sequential models such as GRU [5] and Transformer [44]. When using the cross-entropy loss to optimize the parameterized model above, the objective function can be abstracted as: According to [23,53], in a well trained sequential model, the dot product term can be approximately decomposed as: ⟨𝒉, 𝒗⟩≈ log 𝑝 (𝑣|𝑐) + 𝜆= PMI(𝑣, 𝑐) + log 𝑝 (𝑣) + 𝜆, (3) where𝑝 (·)is the true probability,PMI(𝑣, 𝑐) = logis the pointwise mutual information (PMI) between𝑣and𝑐, and𝜆 is a context-related term. PMI captures the co-occurrence statistics between the variables, which is usually considered as semantics between tokens and the context. To optimize the model with Equation (2), the gradient of the loss function with respect to an item embedding𝒗that does not appear in the input sequence is: This gradient means that for those items with lower frequency in the dataset, the gradient direction is almost determined by the context vector. This is also reected by Figure 1(a), where the yellow dots, representing items with a lower frequency, move in a similar direction within a narrow space. This is because most of the time, these item embeddings are trained with the gradient in Equation (4) as non-target items rather than being target items and trained with the gradient owing through the encoder. Such a distribution of embeddings is referred to as an anisotropic space [7, 23]. As Equation (3) indicates, the semantic information between the sequence embedding and the item embedding is captured by dot product based on the co-occurrence. However, according to Equation (4), it is impractical to expect the sequence embeddings can present a clear dierence in measuring the similarity between the rare items. Generally, the output layer of a recommender system is a dot product between the sequence representation and the item embeddings, which sets sequences and items representations in the same latent space. In the following sections, a contrastive regularization will be introduced to relocate the distribution of sequence representations around the origin point, which implicitly improves the distribution of item embeddings. Contrastive learning is a training scheme that pulls the positive pairs of samples closer and pushes the negative pairs of samples away [9,29]. Specically, a Noise Contrastive Estimation (NCE) objective is generally applied to train an encoder 𝑓 : where𝑥and𝑥are a pair of semantically close samples from the distribution𝑝, which serve as the positive pair here while𝑥and 𝑥are a pair of negative samples and𝑥is sampled randomly from 𝑝. 𝜏 is the temperature parameter. The NCE loss is intuitively performing the push and pull game according to Equation (5). The mathematical descriptions are formally dened as the alignment and the uniformity of the representations under the assumption that vectors are normalized [46]: where𝑝denotes the distribution of the positive pair of samples and 𝑝is the distribution of the independent samples. For Equation (6), minimizingℓis equivalent to encourage the learned representations of𝑥and𝑥from a positive pair distribution 𝑝to be close. For Equation (7), minimizingℓis equivalent to encourage the uniform distribution of the representations of all the data samples from the distribution 𝑝. In sequential recommendation, the main idea is to aggregate the historical interactions to prole the user’s preference. Similar to SASRec, the encoding module of DuoRec is a Transformer [44]. To leverage the encoding ability of Transformer, the items are rstly converted into embeddings. Then a multi-head self-attention module is applied to compute the user representation. 4.1.1 Embedding Layer. In DuoRec, there is an embedding matrix 𝑽 ∈ R, where𝑑is the dimension of the embedding. For the input sequence𝑠 = [𝑣, 𝑣, . . . , 𝑣], the embedding representations are 𝒔 = [𝒗, 𝒗, . . . , 𝒗], where 𝒗is the embedded vector. To preserve the time order of the sequence, a positional encoding matrix𝑷 ∈ Ris constructed, where𝑁indicates the maximum length of all the sequences. Formally, the item embedding and the positional encoding are added up as the input vector for the interaction at a time step 𝑡 of the Transformer: where𝒉∈ Ris the complete input vector of the interaction at𝑡 and 𝒑is the positional encoding of the time step 𝑡. 4.1.2 Self-aention Module. After obtaining the input sequences, the Transformer is applied to compute the updated representations of each item by the multi-head attention mechanism [44]. Assuming 𝑯=𝒉, 𝒉, . . . , 𝒉is the hidden representation of the sequence as both the input of an𝐿-layer multi-head Transformer encoder (Trm), the encoding procedure of the sequence can be dened as: where the last hidden vector𝒉in𝑯=𝒉, 𝒉, . . . , 𝒉is selected to be the user representation of this user sequence. The next item prediction task is framed as a classication task over the whole item set. Given the sequence representation𝒉and the item embedding matrix 𝑽 , the predictive score is calculated as: whereˆ𝒚 ∈ R. With the index of the ground truth item converted into a one-hot vector 𝒚, the cross-entropy loss is calculated as: To alleviate the representation degeneration problem, a contrastive regularization is developed by the exploitation of both the unsupervised and the supervised contrastive samples. 4.3.1 Unsupervised Augmentation. The unsupervised contrastive augmentation in DuoRec aims to provide a semantically meaningful augmentation for individual sequences in an unsupervised style. In the previous method such as CL4SRec [51], the augmentation methods include item cropping, masking, and reordering. Similar techniques in natural language processing are applied, e.g., word deletion, reordering, and substitution [28,49]. Although these methods provide augmentations that help to improve the performance of the corresponding models to some extent, the augmentations cannot provide a guarantee for high semantic similarity. Since the data-level augmentation is not perfectly t for discrete sequence, a model-level augmentation is proposed in this paper. In the computation of a sequence vector, there are Dropout modules in both the embedding layer and the Transformer encoder. Forward-passing an input sequence twice with dierent Dropout masks will generate two dierent vectors, which are semantically similar while having dierent features. Therefore, we choose a dierent Dropout mask for the unsupervised augmentation of the input sequence𝑠, which is rstly operated on the input embedding to the Transformer encoder in Equation (8) to obtain an𝒉. Afterward, the augmented input sequence embedding is fed into the Transformer encoder with the same weight but a dierent Dropout mask: where[−1]is mimicking the Python style of indexing the last element in the list and𝒉is the augmented sequence representation. 4.3.2 Supervised Positive Sampling. The supervised contrastive augmentation in DuoRec aims to incorporate the semantic information between semantically similar sequences into the contrastive regularization. The reason why semantic positives are required is that if only unsupervised contrastive learning is applied, the originally semantic similar samples will be categorized into the negative samples [20]. Therefore, the most important thing is to determine what samples are semantically similar. Semantic Similarity. In sequential recommendation, the goal is to predict users’ preferences. If two sequences represent the same user preference, it is natural to infer these two sequences that contain the same semantics. Therefore, given two dierent userhi sequences𝑠=𝑣, 𝑣, . . . , 𝑣and𝑠=𝑣, 𝑣, . . . , 𝑣, if the predictive objectives of𝑠and𝑠, i.e.,𝑣and𝑣, are the same item,𝑠and𝑠are considered semantically similar in DuoRec. Positive Sample. For the input sequence𝑠, there are sequences having the same target item in the dataset. A semantic similar sequence𝑠is randomly sampled from these sequences. With the input embedding 𝑯, the supervised augmentation is: where 𝒉is the augmented sequence representation. 4.3.3 Negative Sampling. To eectively construct the negative samples for an augmented pair of samples, all the other augmented samples in the same training batch are considered negative samples. Assuming that the training batch isBand the batch size is|B|, after the augmentation, there will be 2|B|hidden vectors,no 𝒉, 𝒉, 𝒉, 𝒉, . . . , 𝒉, 𝒉, where the subscript and superscript are overloaded to denote the index of samples in the batch and the augmentations for clarity. Therefore, for each positive pair in the batch, there are 2(|B | −1)negative pairs as the negative setS. For example, for the augmented pair of sequence representations𝒉and𝒉, the corresponding negative setS=no 𝒉, 𝒉, 𝒉, 𝒉, . . . , 𝒉, 𝒉. If there are sequences with the same target item, these sequences will be removed fromSas well. 4.3.4 Regularization Objective. Similar to Equation (5), the contrastive regularization for the batch B in DuoRec is dened as: which computes twice for the unsupervised and the supervised augmented representation respectively. Thus, the overall objective of DuoRec with 𝜆 scale weight is: In this section, the properties of the contrastive regularization of DuoRec and the connection with other methods will be described. 4.4.1 Solving Representation Degeneration Problem. To investigate how the contrastive regularization can solve the representation degeneration problem, the property of the contrastive regularization ℓin Equation (14) needs to be analyzed. According to Equation (6) and (7), the alignment and the uniformity ofℓare as follows: In the alignment term, it is meaningful to keep the alignment between the positive pairs of representations from two augmentations of the same input sequence. While in the uniformity term, the objective is to uniformly distribute the representations of the sequences. The alignment between semantic positive pairs is pulling the representations of semantically similar sequences together. While the uniformity term is pushing all the sequence representations to be uniformly distributed. Since the main learning objective of recommendation is performed by the dot product between the sequence representation and the item embeddings in Equation (10), it is meaningful to regularize the distribution of the sequence representation so that the distribution of item embeddings can be inuenced. In the representation degeneration problem, an essential drawback of the cone distribution is that there is a dominant axis of the embeddings. Based on the uniformity, this situation will be eased because the sequence representation will be distributed uniformly, which will guide the distribution of the item embeddings via the dot product in Equation (10). While for the other drawback that rare words tend to be far away from the origin point,ℓalleviates this phenomenon by adjusting the gradient for rare words. According to the analysis of the gradient in Equation (4) for rare words, the uni-direction of the gradient is because most of the rare words are mainly trained with the recommendation softmax loss. With the contrastive regularization, these rare words are exposed more often than before since there will be more positive and negative samplings, which are trained via the gradient ow through the encoder rather than directly on the embeddings. 4.4.2 Connection. Recent methods use the contrastive objective mainly for regularization. For example, CL4SRec [51] augments the input sequence in data-level with cropping, masking, and reordering. This is directly following the normal contrastive paradigm for computer vision to augment the samples in the input space. However, discrete sequences are hard to determine the semantic content and even harder to provide a semantically consistent augmentation. If the unsupervised Dropout augmentation of DuoRec is operated twice and only these unsupervised augmented samples are used for the contrastive regularization, it becomes the Unsupervised Contrastive Learning (UCL) variant in the following experiment. Table 1: Statistics of the datasets after preprocessing. Since the augmentation of UCL avoids the data-level augmentations, which cannot guarantee the augmented samples still contain similar semantics, the UCL can outperform CL4SRec consistently. Similarly, if only the supervised augmentation of DuoRec is used, then it becomes the Supervised Contrastive Learning (SCL) variant, which can provide a harder training objective. And SCL can outperform UCL for using more appropriate samples. This is also observed by recent natural language processing research [8]. In experiments, we will answer these research questions (RQ): • RQ1: How does the DuoRec perform compared with the state-of-the-art methods? (Section 5.2) • RQ2: How does the DuoRec perform compared with the existing contrastive training paradigms? (Section 5.3) • RQ3: How does contrastive regularization help with the training? (Section 5.4) • RQ4: How is the sensitivity of the hyper-parameters in the DuoRec model? (Section 5.5) 5.1.1 Dataset. The experiments are conducted over ve benchmark datasets with statistics after preprocessing shown in Table 1. • Amazon Beauty,ClothingandSports[27]. Following baselines [19,41,51,58], the widely used Amazon dataset is chosen in our experiments with three sub-categories. • MovieLens-1M[12]. Following [41], the popular movie recommendation dataset is used here, denoted as ML-1M. • Yelp, which is a widely used dataset for the business recommendation. Similar to [58], the transaction records after Jan. 1st, 2019 are used in our experiment. Following [19,41,51,58] for preprocessing, all interactions are considered as implicit feedback. Users or items appearing less than ve times are removed. The maximum length of a sequence is 50. 5.1.2 Metrics. For overall evaluation, top-𝐾Hit Ratio (HR@𝐾) and top-𝐾Normalized Discounted Cumulative Gain (NDCG@𝐾) are applied with𝐾chosen from{5,10}. We evaluate the ranking results over the whole item set for the fair comparison [22]. 5.1.3 Baselines. The following methods are used for comparison: • BPR-MF[37] is the rst method to use BPR loss to train a matrix factorization model. • GRU4Rec[15] applies GRU to model the user sequence. It is the rst recurrent model for sequential recommendation. • Caser[42] is a CNN-based method capturing high-order patterns by applying horizontal and vertical convolutional operations for sequential recommendation. • SASRec[19] is a single-directional self-attention model. It is a strong baseline in sequential recommendation. • BERT4Rec[41] uses a masked item training scheme similar to the masked language model sequential in NLP. The backbone is the bi-directional self-attention mechanism. • SRec[58] applied masked contrastive pre-training as well. The Mask Item Prediction (MIP) variant is used here. • CL4SRec[51] uses item cropping, masking, and reordering as augmentations for contrastive learning. It is the most recent and strong baseline for sequential recommendation. 5.1.4 Implementation. The embedding size is set to 64 with all linear mapping functions in DuoRec has the same hidden size. The numbers of layers and heads in the Transformer are set to 2. The Dropout [40] rate on the embedding matrix and the Transformer size is set to 256. We use the Adam [21] optimizer with the learning 0.001. 𝜆 in Equation (15) is chose from {0.1, 0.2, 0.3, 0.4, 0.5 }. In this experiment, we evaluate the overall performance to compare DuoRec with the baselines, which is presented in Table 2. According to the results, the rst observation is that the nonsequential result, BPR-MF, can hardly achieve a comparable result with other sequential methods. When it comes to the deep learning era, the rst representative method is GRU4Rec based on GRU, which can consistently outperform the non-sequential BPR-MF. It can be concluded that the incorporation of sequential information can improve performance. Similarly, Caser uses a convolutional module to aggregate the sequential tokens, which are stacked as a matrix. Caser generally has a similar performance to GRU4Rec. More recently, attention has become the strongest sequence encoder. SASRec is the rst method to apply uni-directional attention for sequence encoding. Compared with the previous deep learningbased models, SASRec can improve the performance by a large margin. This is achieved by the more representative sequential encoder. More recent methods generally inherit the attention-based encoder while introducing extra objectives. For example, BERT4Rec applies the masked item prediction objective to enforce the model to understand the semantics by lling in the masks. Although such a task can introduce a meaningful signal for the model, the performance is not consistent since the masked item prediction is not aligned well with the recommendation task. A similar situation happens toSRec, which also relies on the masked item prediction as the pre-training objective. The netuning stage gives out a more accurate prediction. For the most recent contrastive learningbased approach, CL4SRec, achieves a consistent improvement over the other baselines. The extra objective is the same as the normal contrastive learning norm to set two dierent views of the same sequence. For DuoRec, it can outperform all the baselines by a large Table 2: Overall performance. Bold scores represent the highest results of all methods. Underlined scores stand for the highest results from previous methods. The DuoRec achieves the state-of-the-art result among all baseline models. Figure 2: Performance of dierent contrastive objectives. margin. The incorporation of the supervised and unsupervised positive samples can improve the overall performance by regularizing the distribution of the sequence and the item representation. In this experiment, the ecacy of the unsupervised augmentation and the supervised positive sampling is evaluated. The variants are: CL4SRec, using cropping, masking, and reordering as augmentations to calculate NCE; UCL, the NCE uses unsupervised augmentations to optimize; SCL, the NCE uses supervised positive sampling to optimize; and UCL+SCL, trained with the addition of the UCL and the SCL losses. The result is shown in Figure 2. From the result, it is clear that adding a contrastive objective can generally improve the recommendation performance compared with the baseline SASRec. Compared with CL4SRec, UCL can outperform CL4SRec while both being unsupervised contrastive methods. It can be concluded that the model-level Dropout augmentation can provide a more semantically consistent unsupervised sample than the data-level augmentation. Furthermore, SCL relies on the target item to sample a semantically consistent supervised sample, which shows a large margin improvement over both the unsupervised methods. Interestingly, directly adding the UCL and the SCL losses will harm the performance. This could be due to the incompatible alignment of two contrastive losses. For DuoRec, both the unsupervised and supervised positive samples are exploited, which can yield the best performance compared with all other methods. To evaluate how the contrastive regularization aects the training, (1) the visualization of the learned embedding matrix and (2) the training losses, will be demonstrated to help understand how contrastive learning improves performance. The visualization is based on SVD decomposition, which will project the embedding matrix into 2D and give out the normalized singular values. The results are shown in Figure 3 and 4. The visualization of the training losses is decomposed into the alignment and the uniformity via Equation (6) and (7) as presented in Figure 5. 5.4.1 Visualization of Item Embedding. As discussed before, SASRec is trained without constraint on the embedding matrix, which yields a narrow cone in the latent space as in Figure 3(a) and 4(a). The resulted in singular values drastically decrease to very small values. Although CL4SRec has an extra contrastive loss with data-level augmentation, the embeddings improve in terms of the distribution magnitude while the rare items are still located on the same side Figure 5: Training loss with colors indicating the validation HR@5 (blue is better). The uniformity loss of sequence representations decreases during training of DuoRec as the HR@5 increases, which indicates a more uniform distribution. In contrast, the uniformity loss increases during training of CL4SRec, resulting in an anisotropic distribution. of the origin point. And the singular values of CL4SRec decrease slower than SASRec. This could be due to the data-level augmentation cannot consistently provide reasonable sequences. While for the UCL variant, it generates a similar embedding distribution as CL4SRec since they are both based on unsupervised contrastive learning. While for SCL, which only uses supervised contrastive learning, it is clear that the distribution of embeddings is more balanced that both the high- and low-frequency items are located around the origin point. The singular values are signicantly higher than the unsupervised methods. It can be concluded that the supervised positive samples are more semantically consistent with the input sequence. When adding both the unsupervised and the supervised positive samples, UCL+SCL has a similar situation as purely SCL for the Clothing dataset while dierent for the Sports dataset. This dierence is due to the combination of the unsupervised and supervised contrastive losses, which could lead the model in a dierent training direction. For the DuoRec, the embedding is Figure 6: Parameter sensitivity of Dropout ratio. distributed in a balanced style with the singular values decrease slowly. The proper combination of unsupervised and supervised contrastive learning improves the item embedding distribution. 5.4.2 Alignment and Uniformity. To investigate how contrastive learning takes eect during the training, the alignment loss term and the uniformity loss term are illustrated (both are better when smaller). The uniformity is calculated within the original output sequence representation, which is in the same range of samples for every method. Noting that since the choices of the positive sample are dierent across dierent methods, the alignment term is shown as a trend indicator without a proper meaning for comparison. From Figure 5(a) and 5(b), it is clear that as the training of DuoRec goes on, the uniformity loss decreases as the HR@5 increases. And the uniformity loss achieves a clearly lower value than CL4SRec, which reects the sequence representations are distributed more uniformly. For the CL4SRec, the uniformity loss increases during the training, which indicates a worse distribution space compared with DuoRec. Although the alignment loss is slightly increasing during the training of both methods, the drop of uniformity loss actually improves the recommendation performance. In this experiment, the parameter sensitivity of the Dropout ratio for the augmentation and the𝜆in Equation (15) are investigated. The results are presented in Figure 6 and 7 respectively. More results for reproducibility can be found in the appendices. The Dropout ratio mainly aects the unsupervised augmentation, which assumes that using dierent Dropout masks under the same weight can generate semantically similar samples. According to Figure 6, for ML-1M dataset, when increasing the Dropout ratio, the performance decreases, which could be due to the density for this dataset is higher with more training signals. When the augmentation is too dierent from the original input using a higher Dropout ratio, the model is guided to train in an inaccurate direction. While for the Beauty, Clothing, Sports and Yelp datasets, the eect of dierent Dropout ratios is not signicant. The𝜆in Equation (15) controls the scale of the contrastive regularization. According to Figure 7, the performance is consistent across dierent choices of𝜆. This is possibly because the contrastive regularization is well aligned with the recommendation task. The sequential recommendation task is mainly related to the sewhich rely on recurrent neural networks such as GRU [5] or attention structures [44] as the sequence encoder. GRU4Rec [15] is the very rst attempt to utilize the GRU network in sequential recommendation. Since the attention mechanism has shown a great ability, dierent related models are developed, e.g., SASRec [19], BERT4Rec [41] andSRec [58]. Recent graph-based methods, FGNN [32,34], GAG [36] and PosRec [31], achieve improved performance due to the graph modeling in sequence. In terms of the training method, most methods are based on the next-item prediction task [15,19,42]. These methods are naturally suitable for the next-item prediction problem. The other training scheme usually has extra training tasks [26,41,51,58]. The pre-training tasks mainly have the masked item prediction, the masked attribute or segment prediction inSRec [58] and the netuning step is the same as the next-item prediction. A recent work MMInfoRec [33] applies an item level contrastive learning for feature-based sequential recommendation. For BERT4Rec [41] and CL4SRec [51], the auxiliary task is added in the multi-tasking style. Contrastive learning has been widely used in various deep learning areas for its strong ability to help with the self-supervised problems, the early method such as CPC [14,43] and DIM [16], the encodings of dierent scales of the same image are fed into the contrastive learning as positive pairs. In the follow-up methods e.g., MoCo [13], SimCLR [2] and SimSiam [4], the dierent augmentations of the same image are considered as positive pairs for the contrastive learning. For the video representation learning, MemDPC [10] applies a similar strategy as CPC and DIM to encode the feature vectors of the segment and the video clip as positive pairs. For the COCLR [11] method, the positive samples of a video clip in RGB space are determined by the closeness of clips in the optical ow space and vice versa. In contrastive learning in the language modeling, ConSERT [52] introduces traditional augmentation methods such as cropping and reordering into the sentence augmentation as positive pairs. SimCSE [8] treats the same sentence with dierent Dropout [40] masks as positive pairs. Contrastive learning is used in recent recommendation methods. For the collaborative ltering methods, SGL [48] applies the NCE in node-level representation learning. SSL [54] proposes a siamese network to encode the items as pre-training with embedding-level augmentations. SEPT [55] applies the NCE for the socially-aware recommendation, which is based on node-level contrastive learning. For the contrastive learning in sequential recommendation, SRec [58] incorporates the contrasting mechanism between the prediction and the ground truth of the attribute-level, the item-level, and the segment-level together for training. The segment-level contrastive learning is also applied by Ma et al. [26] as a multitasking objective. CL4SRec [51] proposes three augmentations for the interaction sequence and applies a similar contrastive strategy as MoCo [13] and SimCLR [2] to set these augmentations of the same sequence as the positive pair in training. DHCN [50] and MHCN [56] are graph-based methods with contrastive learning on node-level representation. A more recent work MMInfoRec [33] has achieved a great improvement in sequential recommendation with side information using a contrastive objective in item level via Dropout augmentation in the item embedding. In this paper, the representation degeneration problem of the item embedding matrix in sequential recommendation is investigated. The empirical observation and the theoretical analysis are provided. To solve this problem, a DuoRec model is proposed, which contains a contrastive regularization with both the Dropout-based model-level augmentation and the supervised positive sampling to construct contrastive samples. The properties of this regularization term are analyzed towards the representation degeneration problem. Extensive experiments are conducted on ve benchmark datasets, which verify the superiority of DuoRec. The visualization is demonstrated to show how DuoRec solves this problem. The work is supported by Australian Research Council (CE200100025, DP190102353, DP190101985, FT210100624).