<title>An Improved and Optimized Practical Non-Blocking PageRank Algorithm for Massive Graphs</title> Abstract PageRank is a well-known algorithm whose robustness helps set a standard benchmark when processing graphs and analytical problems. The PageRank algorithm serves as a standard for many graph analytics and a foundation for extracting graph features and predicting user ratings in recommendation systems. The PageRank algorithm iterates continuously, updating the ranks of the pages till convergence is achieved. Nevertheless, the implementation of the PageRank algorithm on large-scale graphs that on shared memory architecture utilizing ﬁne-grained parallelism is a diﬃcult task at hand. The experimental study and analysis of the Parallel PageRank kernel on large graphs and shared memory architectures using diﬀerent programming models have been studied extensively. This paper presents the asynchronous execution of the PageRank algorithm to leverage the computations on massive graphs, especially on shared memory architectures. We evaluate the performance of our proposed non-blocking algorithms for PageRank computation on realworld and synthetic datasets using Posix Multithreaded Library on a 56 core Intel(R) Xeon processor. We observed that our asynchronous implementations achieve 10x to 30x speedup with respect to sequential runs and 5x to 10x improvements over synchronous variants. <title>arXiv:2109.09527v2  [cs.DC]  26 Sep 2021</title> modeled as graphs and solved with appropriate graph algorithms[1]. Most of the graphs are enormous and are scale to billions of nodes and edges while having uncommon and nuanced structures. Processing graphs and graph applications tends to be a performance issue, speciﬁcally in shared memory architectures. It is also essential to leverage the existence and interpretation of these large graphs by adding speciﬁc metrics for deriving useful analytics on many of these large graphs. PageRank is such a metric that can be used to determine the importance of nodes or pages in a web graph. Page et al. [2] devised this algorithm for Google Search Engine. The PageRank computation proceeds iteratively to estimate the signiﬁcance of a web page. In each iteration, we calculate the importance of a page by randomly selecting a page and picking a random link at uniform probability d to visit another page. This process continues by updating the rank of a particular page. Pages with more links are more likely to be visited, so they eventually have higher ranks. If the outgoing link is not available, then the process moves to a new page with probability (1-d) and restarts the process from this page. The algorithm’s fundamental premise is that a page’s rank is determined by its inbound link. Pages with more links are more likely to be visited, resulting in higher rankings. [2]. The rank pr of node u in Graph G is formally deﬁned as: 1 − d pr(v) pr(u) = + d ∗ (1) where, n = number of pages, q = outdegree deﬁning the number of hyperlinks on page v and d is the dampening parameter initialized to 0.85. Parallel implementations of PageRank algorithm have been extensively studied on various architectures. As PageRank algorithm iteratively progresses, multiple threads coordinate easily using synchronous mechanisms. Synchronization can be applied for both vertexcentric and edge-centric computations and on shared-memory and distributed memory architectures[3]. The barriersynchronization mechanism is more suitable for iterative algorithms such as the PageRank algorithm. However, synchronous computations utilize ThreadLevel Parallelism which leads to drawbacks in dealing with progress conditions in the occurrence of thread failures. On the other hand, in asynchronous computations, progress is guaranteed where threads do not have to wait for slower threads or failure threads. This criterion motivates us to apply asynchronous computations on shared memory architecture for vertex-centric, edge-centric, or graph-centric algorithm implementations. The algorithm implementations relied on processing and computing vertices, in a Vertex-centric model[3]. Edges are the key computational units in an Edge-centric model[4]. In a Graph-centric model, the computations are performed on sub graphs with implicit compiler optimizations. [5] In this paper, we present approximation techniques for our earlier proposed non-blocking methods to leverage the computation of PageRank algorithm on massive graphs, especially on shared memory architectures. Our main focus is on designing an asynchronous PageRank algorithm with no synchronous limitations that can be applied to vertex-based and edgebased representations. We examined that applying asynchronous computations using the No-Sync variant on the PageRank algorithm can speed up performance over synchronous methods. The Loop-Perforation is an approximate technique that skips some iterations of a loop to increase the computational speed-up. The primary idea of the loop perforation approximate technique is to reduce the amount of computation performed within each iteration as the algorithm makes progress[6][7]. Loop-Fusion is an optimization technique that unites two or more independent loops into a single loop and is applied only when data dependencies are preserved. Loop fusion technique when applied increases data locality and the level of parallelism and decreases the overhead of loop control. In this direction, we applied loop perforation and loop perforation approximate technique and enabled loop fusion optimization technique to compute the PageRank algorithm. • Developed vertex-centric Non-Blocking(No-Sync and Wait-free) PageRank algorithms and evaluated the implementations on real-world and synthetic datasets. [8] • Analysis of vertex-centric and edge-centric computations on PageRank Algorithm. • Applied and analysed approximation techniques on synchronous and asynchronous algorithms using both vertix-centric and edge-centric computations on PageRank Algorithm. • Testify the performance improvements of asynchronous variants with 5x to 10x speedup when compared with synchronous variants. This section presents a description for designing shared data objects using diﬀerent synchronization techniques [9] for designing shared data objects and algorithms proposed in this paper. Also, we discuss the primary motivation to design an asynchronous technique for the iterative PageRank algorithm. In a Shared Memory Multiprocessors or Multicore systems multiple processors or processing elements need to coordinate accesses using shared memory. Programming implementations on shared memory systems is challenging as multiple processes simultaneously access shared resources due to lack of coordination, resulting in unpredictable delays and performance bottlenecks. An eﬃcient synchronization mechanism is apt to deal with these issues in parallel computation by multiple processes. Two classes of synchronization approaches deal with multiple processes. a) According to the Blocking synchronization mechanism, a shared object can only be accessed by one thread at a time. It locks the other threads and only allows one thread at a time to access the shared object, avoiding conﬂicts. However, it causes waiting and deadlocks. b) The Non-Blocking synchronization mechanism is intended to prevent the issues associated with blocking synchronization. lock-free and Wait-free methods are used to avoid thread conﬂicts. The Wait-free technique guarantees that every thread is processed in a ﬁnite number of steps, whereas the lock-free method guarantees that at least some threads are processed in a ﬁnite number of steps. The Wait-free method is implemented using the Compare-and-Swap atomic primitive[9]. The research primarily pertains to the graph pre-processing step considering their structural properties and their basic representations [10][7] [11], load distributions on the threads [12], Etc. These algorithms mainly involve the use of the Barrier synchronization concept, but this has many drawbacks. The barrier synchronization applied on each thread at every iteration may cause indeﬁnite blockages without any progress, which stands as a signiﬁcant issue. Apart from this, parallel algorithmic designs pose many challenges concerning performance and memory. In the context of this, the motive of the algorithm proposed in this paper is to enable independent execution of threads avoiding barriers resulting in increased computational speeds. Concurrent thread executions are one solution to the parallel algorithm design issues and can help adequately utilize the multi-core architectures of the day. The iterative algorithms use computations of previous iterations for calculating values in the current iterations. When applied to large-scale graphs, Barrier synchronization consumes substantial time and memory resources to yield the desired solution. As is known, the scalability is limited in parallel shared systems due to memory latencies and synchronization problems. It is proposed to parallelize the PageRank algorithm on shared memory systems by improving synchronization aiming ﬁne-grained parallelism using optimistic concurrency control mechanisms and ultimately devise a mechanism to prove the correctness of a parallel algorithm is a challenging task. Non-blocking algorithms with lock-free and wait-free properties address these challenges on the PageRank algorithm using piece-wise concurrent programming, removing barrier constraints and iteration dependency from the iterative algorithms considering Graph optimizations as well. One assumption in this paper is that the system has a limited set of p threads running on the multiprocessors. The communication between these asynchronous threads happens using shared objects. Atomic primitives such as CAS(Compare-And-Swap) are used to overcome the hurdles encountered during thread communications. To implement a wait-free algorithm, we rely on the CPP vector template library to guarantee thread safety on the lock-free algorithm. This section presents an overview of the literature related to the PageRank computation from its origin to recent advances. Google’s ﬁrst algorithm proposed by Page et. al [2] is an iterative algorithm that would rank websites in their search engine results. This concept eventually became the PageRank algorithm we know today. The algorithm ranks web pages iteratively until their page rank value converges within a given threshold. These days, the PageRank algorithm is a key metric in determining the importance of a given web page. Due to this, its popularity has only grown over the past few decades. The research interest in the PageRank algorithm has been prominent in recent history. The parallel computation of the algorithm on shared memory architectures has been the subject of great discussion, with many proposing diﬀerent programming models for its computation [1],[13],[10],[11]. Berry et al.[13] proposed a parallel PageRank algorithm in their library known as the Multithreaded Graph Library (MTGL). The algorithm executes on Cray XMT a Multithreaded Scalable Architecture with 128 threads and uses the QThreads API to help process the threads and enable synchronization between them. Each thread computes the PageRank value of a node by accumulating the number of votes of its incoming edges. This implementation had performance bottlenecks due to a lack of optimizations with QThread API. GraphLab proposed by Low et al. in their paper [14] is a framework designed to achieve parallelism for Machine learning algorithms. It provides a shared-memory implementation that is eﬃcient but aims to further it to distributed systems and has been used on popular algorithms for testing. The framework has been implemented in C++ with the help of PThreads. The PageRank computations in this framework use synchronization locks and barriers in each iteration and compute the PageRank values using schedulers and assertively tuning needed parameters. The authors in their paper [15] proposed a new programming paradigm – GRACE to facilitate easy programmability along with synchronous and asynchronous executions. The model is built to capture the data dependencies through message passing with faster convergence. The experiments performed on the model show synchronous execution with performance as high as asynchronous executions. The framework provides an iterative synchronous programming approach for developers. A group of worker threads coordinates with a driver thread to calculate the PageRank of the scheduled vertices using Barriers. The authors of [16], discussed graph mining algorithms with a primary focus on the Page Rank Algorithm. The paper aims to develop a framework for designing scalable data-driven algorithms for graph mining algorithms through a case study on the PageRank algorithm. The paper investigates various implementations of the page rank algorithm in the purview of three design axes – work activation, data access pattern, and scheduling criteria to test and understand how various design choices aﬀect the performance. The results showed that considering data-driven designs, which are also scalable over iterative algorithms, improves performance. The results speciﬁcally showed that the data-driven, push-back algorithmic implementations had increased the performance by 28x. Shun and Blelloch proposed Ligra - a Lightweight Framework to Process Large Graphs [10] is built for shared memory architecture and encompasses both vertex-centric and edgecentric models. As the graphs are stored in the memory, this framework optimizes the computation on shared memory. This framework supports two data types: Graph(V, E), which stores the graph, and another vertexSubset, a subset of —V—. The framework has two functions VERTEXMAP( ) function is used for mapping over the vertices, and EDGEMAP( ) is used for mapping over the edges. Taking advantage of Frontier Based Computation, an active set of nodes and edges updates dynamically. Ligra uses Cilk Plus parallel codes to achieve parallelism. Paritosh Garg and Kishore Kothapally in their paper titled STIC-D: Algorithmic Techniques for Eﬃcient Parallel Pagerank Computation on real-world graphs[11], presents four techniques that optimize the large-scaled graphs enough to compute the PageRank algorithm. The ﬁrst method identiﬁes the Strongly Connected Components(SCCs), performing topological sorting on them. The PageRank is then computed on smaller subgraphs that are strongly connected and processed in a speciﬁc order. The second method uses the law that identical nodes have identical PageRank. If two nodes have incoming edges from the same set, then the two nodes would have the same PageRank. So, nodes can be classiﬁed, and PageRank is calculated on one vertex from each class. All the other vertices will have the same PageRank, thereby eliminating the redundant computation of similar nodes. If a set of nodes form a chain, each node has only one incoming edge, and one outgoing edge, the PageRank of a vertex with such a node is easy to compute. In the fourth method, if the PageRank does not change in the previous few iterations, we can mark it as a dead node and not include them in the successive iterations. However, the proposed preprocessing methods applied in this paper have not yet been parallelized and require performance enhancements. This paper [17] deals with providing a technique called Propagation Blocking with which the memory communication is reduced. Usually, graphs are sparse, and hence for processing to compute the PageRank, it would take a lot of memory communication. Propagations refer to transfers of vertex values. In the technique proposed, the propagations are stored in memory in a semi-sorted memory. Propagation blocking happens in two phases - binning and accumulate. Binning phase is like bucket sort, where the vertices are divided into diﬀerent continuous bins. A contribution is processed and is added to the corresponding bin, and the destination is appended to the contribution. In the second phase, accumulate, each bin is processed. Each contribution, destination pair is processed, and the contribution is added to the destination. This approach will reduce the memory communication as the contributions are binned; hence temporally close vertices have more probability of being in a bin. The cache misses will be less, corresponding to less memory communication. Hence his technique improves spatial locality on DRAM by limiting the bounds of memory communication. However, this technique requires an edge-centric representation as input and is bounded by barrier synchronization. Hamza Omar et al. in the paper [18], perform a study on the impact of input dependence for graph algorithms in the context of approximate computing. The authors justify that using perforation on the input graphs over the algorithms improves performance. Additionally, they proposed a predictor algorithm that helps in reducing the challenges in input dependencies of loop perforation for graphs and enables a satisfactory accuracy level. Experiments were tested using CPU and GPU architectures such as Nvidia, Intel CPU architectures- 8 core Xeon and 61-core Xeon Phi. The results have exhibited a 30% improvement of performance on using perforation in input graphs and this, when applied to the Nvidia architecture, showed an increase of 19% of power utilization. In [7], the authors aimed to design approximation techniques for computing, enabling good performance coupled with lesser loss of accuracy. The main techniques proposed are loop perforation, vertex/edge ordering, threshold scaling, and other heuristics such as data caching, graph coloring, Etc, which are implemented and tested on the two graph algorithms, i.e., PageRank and community detection. The paper shows the performance improvement of the PageRank algorithm by 83% and up to 450x for community detection with low inﬂuence on the accuracy of using the approximation techniques on the iterative techniques. The authors conclude that approximation techniques will provide good performance with lesser loss of correctness and optimality of solutions. These techniques led to a 7-10 times improvement in performance compared to the eﬃcient algorithms STICD [11]. Nevertheless, the approximate PageRank computation algorithm utilizes additional storage to save the sorted edge-list for computing the PageRank of the targeted vertex. Barriers synchronization technique is still used for the parallel implementation. Graphphi - a framework proposed by [19] with four major components - preprocessing, graph processing model, the MIMD/ SIMD aware scheduler, and extra optimizations. This framework can process edge and vertex-centric graphs, which means this is a hybrid of edge-centric and vertex-centric. This framework works eﬃciently with MIMD and SIMD with thread-level load balance. The implementation starts with preprocessing graphs into hierarchical blocks. Then the edges are processed, and then these are executed in a loadfree, cache-aware, load-balanced, and SIMD-eﬃcient manner. Another optimization layer is provided by push and pull execution and by taking advantage of the High Bandwidth Memory technique. Overall, the framework speed-up by 4X to 35X. An optimized shared-memory graph processing framework introduce by [12] increases cache and memory eﬃciency. This framework is called GPOP (Graph Processing Over Par- titions) framework, which promises to increase the eﬃciency by executing the graph algorithms at lower granularities called partitions. This framework is compared against Ligra, GraphMat, and Galois on diﬀerent graph algorithms using large datasets to check the efﬁciency. In comparing the frameworks, GPOP shows fewer cache misses than the other frameworks and increases the performance, which is almost 19x faster than Ligra, 9.3x GraphMat, and 3.6x – Galois, respectively. This section explains the parallel PageRank computation using Blocking and Non-Blocking algorithms on large-scale graphs. Implementation of iterative parallel graph algorithms takes into consideration the following factors, like convergence, performance. We rely on convergence factor at three diﬀerent levels for our algorithm: Node-level, Algorithm-level, and Thread-level. In node-level convergence, the termination of the PageRank algorithm depends on the convergence of each node independently. In algorithm-level convergence, the termination of the PageRank algorithm depends on all nodes from all partitions. In thread-level convergence, each partition terminates independently. In the below algorithms, Barriers, Barriers-Edge, Barriers-Helper fall under the algorithm-level convergence category, whereas No-Sync, No-Sync-Edge fall under thread-level convergence category. Barriers-Opt falls under a combination of node-level and algorithm-level convergence categories. No-Sync-Opt fall under a combination of node-level and thread-level convergence categories. The Barriers algorithm explained here is the baseline version discussed in paper[11]. Given a graph G = (V, E), vertices are divided into p equal-sized partitions. Each thread is responsible for the computation of one partition. We employed a static load allocation technique to assign nodes to partitions. Lines 4 - 9, to begin with, initializes all the variables and the arrays. This algorithm uses two arrays for storing PageRank values. The prev pr array holds the PageRank values from the previous iteration, and the pr array stores the current iteration PageRank values. The error variable helps us decide if the iteration should either continue or converge. This error value is the diﬀerence between the Previous PageRank and the PageRank for each vertex. The threshold is a constant value initialized to 10 which determines the termination condition of the algorithm. In this algorithm the computation is divided into two phases. Lines 12-18 are the ﬁrst phase of the algorithm that is responsible for PageRank computations. The algorithm computes the maximum absolute diﬀerence between the Previous PageRank and the PageRank values and saves the value in the thrErr array. Lines 20-22 are responsible for updating the shared variables. After computing the PageRank values in the current iteration, the algorithm proceeds to the next phase: to copy the values from the pr array to prev pr array and calculate the global error value. Barriers-Edge is the baseline algorithm proposed in [7] paper. In this approach, the author has developed a three-phase PageRank algorithm in which the second and third phases are similar to the Barriers Algorithm . In the second phase, instead of computing the contribution values of the incoming neighbors, we directly fetch the values from the ContributionList vector. In the ﬁrst phase, each node traverses through its outLinks and populates the contribution value to its respective outgoing neighbor. Similar to the Barrier Algorithm, each phase ends with barrier instruction. In our work, we proposed an asynchronous algorithm (No-Sync) for vertex-centric PageRank computations[8]. At a minimum, one thread must make progress by computing and updating the PageRank values.Multiple threads can access the same element in this approach, but only one thread will be responsible for writing to the memory. In this process, we can encounter read-write conﬂicts but not write-write conﬂicts. A thread can read the previous PageRank value (or the one computed in the current iteration) in such a scenario. C++ vector templates guarantee this thread-safety property. https://en.cppreference.com/w/cpp/container We modiﬁed Algorithm 1 to make it an asynchronous algorithm. The most notable change is to eliminate Barriers from the computation at the end of each phase. This change will allow threads to progress in the next iteration without waiting for other threads to complete their task. The subsequent change reduces the memory usage by eliminating the Previous PageRank array. Since we are eliminating the iteration level dependency with our ﬁrst change, we can apply our second change to Algorithm 1. Along with the PageRank computation, each thread will compute the error value locally. In the synchronous setting, each thread will update the local error value to the global value in the second phase of computation. A thread will update its local error value and partially computed error values from other threads and enter the next iteration in an asynchronous algorithm. This property allows us to have thread-level convergence irrespective of the mode of load allocation. Lemma 1 The algorithm eventually terminates in ﬁnite steps. Proof The fundamental concept behind this algorithm is that it will terminate when the threshold value is greater than the error value of all the threads. Therefore, it has to be proved that the error value of the threads decreases with each iteration. However, since the maximum error value of the vertices allocated to a thread is the error value of the thread, the statement can be rewritten as follows: it must be proven that in each iteration, the error value of each vertex decreases. The error value in the i iteration of a vertex u and PageRank in the baseline algorithm is given by Eq(2) and Eq(3) respectively. By the deﬁnition of our proposed No-Sync algorithm, threads simultaneously compute in diﬀerent iterations. The PageRank of any vertex can be part of any iteration at any given point of time, be in the ﬁrst or the n iteration. For the base case scenario, threads can be part of two consecutive iterations at a given point of time. Eq(2) can be modiﬁed to Eq(4) considering that the threads are present in i and (i1) iterations. Let S be a set of vertices where (v,u) ∈ E and PageRank of v is from i iteration. Error in Eq(3) can also be modiﬁed accordingly. err from the base algorithm is always expected to decrease in every iteration, so err also decreases with every iteration. Lemma 2 No-Sync algorithm fetches identical results to that of Sequential. Proof The PageRank of any given vertex is evaluated from the PageRank of all of its incoming vertices. Since the threads can determine PageRank simultaneously in a diﬀerent iteration, the PageRank of any given vertex is evaluated from the PageRank of incoming vertices that could be part of any iteration. Eq(4) can be modiﬁed for the threads to be present in 1 to i iteration. condition the Eq(7) can be modiﬁed as Eq(8) where S = {v|(v, u) ∈ E}. As per the deﬁned termination condition, the acquired error value from the modiﬁed PageRank values is less than the threshold. Therefore, the PageRank values from the NoSync algorithm are identical to Sequential, with the given error value less than the allocated threshold value. Eq(8) is exactly same as Eq(2) where |pr pr | ≤ threshold is satisﬁed only at the termination condition. This Lemma is also proved experimentally and the L1 norm of the PageRank values is less than 1/10th of the threshold for all the experiments. Likewise to how we developed an asynchronous algorithm for a 2-phased PageRank computational model, we also developed an asynchronous algorithm for the 3-phased PageRank computational model. The changes proposed in the previous algorithm are also applicable for this variant. In Algorithm 1, PageRank computations are happening in one single equation, whereas in Algorithm 2Barriers-Edge, we split the equation into two parts. Though we successfully developed asynchronous variants for both algorithms, this variant does not guarantee convergence for particular types of datasets. This variant resulted in better speedups when we tested it on our synthetic datasets; however, it did not converge with the given threshold for standard datasets. Since the asynchronicity is entirely random, we are still exploring the reasons behind the non-convergence of this variant. Many applications might not require the exact solution which can help reduce the overall computational cost. When using approximation techniques, we skip some portions of the computation to arrive at an approximate solution. This technique can signiﬁcantly improve the performance by a minimum compromise on accuracy. Barrier helper is a wait-free algorithm centered around solving thread delay and failure issues, thus ensuring an algorithm’s correctness. In each iteration in the barrier helper algorithm, threads may not enter the next iteration unless the PageRank value of each node is calculated for the given iteration. The main motive behind this Barrier-Helper is that threads completed with their task are re-assigned to other threads to help them compute the PageRank for that iteration, thus actively avoiding failure and delay scenarios. This process continues until the PageRank value has been calculated for each node in that iteration. Algorithm-6 explains the ﬂow for PageRank computation of vertices using the wait-free approach. “Please refer to our conference paper[8] for details”. struct ThreadCASObj{ int itrNum; int currNode; double thErr; }; struct GlobalCASObj{ int itrNum; double err; vector<bool>check; bool intermediate; }; struct PrCASObj{ int itrNum; double rank; }; The simulations were conducted on a 56 core Intel® Xeon® E5-2660 v4 processor that runs at 2.06 GHz core frequency. With this architecture, the two CPU sockets support 14 cores each, with each core supporting up to two logical threads. Also, every core has an L1-32K, L2 – 256K cache memory speciﬁc to the core, and L3 – 35840K cache memory. Implementations are coded in C/C++, and compilation was performed using g++ 7.5.0 and POSIX Multithreaded library. The experiment uses synthetic and real-world datasets for the simulations. There are three categories of real-world datasets taken from SNAP repository [20] and randomly generated synthetic datasets . All the datasets have been considered keeping the studies [10],[11],[21] in purview for providing a comparison. The datasets mentioned above are shown in Table 1. The initial experimentation was conducted on synthetic graphs generates randomly in the range 1∗10 to7∗10 using the RMAT graph library[22]. Further experiments were conducted on the standard datasets repository – Web-Graphs, Social-Networks, and Road-Networks. The formats of the graph dataset sizes are in Adjacency List [21] and are converted later to Compressed Sparse Row(CSR) format, and all the codes are tested with the given datasets. The section presents the speed-up obtained with the parallel variants of the PageRank algorithm. The algorithm’s speed-up is calculated using the ratio of Sequential execution time vs. Parallel execution time. The programs are executed with a ﬁxed number of threads(56) on diﬀerent classes of datasets in order to obtain the execution times. The proposed algorithms have shown signiﬁcant improvement at the hardware level by incorporating them alongside current graphs processing techniques. Figure 1 Shows the speedups obtained by parallel variants(blocking and non-blocking variants) on standard datasets using 56 threads. Barrier variants result in a maximum of 10x on standard datasets, whereas No-Sync variants (except for No-Sync-Edge) consistently produce greater than 10x speedup on almost all datasets. It is observed from the results that No-Sync, No-Sync-Identical, No-Sync-Opt and No-Sync-Opt-Identical, are giving better performance than the Barriers, Barriers-Identical and Barriers-Edge on all the datasets. We achieve substantial performance beneﬁts by removing the barriers and allowing partial computations on shared variables to eliminate iteration-level dependency and thread-level dependency. Thus it can be concluded that asynchronous variants outperform synchronous variants by a considerable magnitude. As each thread progresses independently and completes the given task, we achieve the lock-free property on the No-Sync variants. We conclude that the lock-free variants of the PageRank algorithm provide better performance improvements compared to the other variants. The notion behind the Wait-free variant is to display the sustainability of the current program execution and hence is not explicitly designed for performance. Since we are not using any compiler optimization ﬂags, the Barriers-Edge variant is not as performant. Figure 2 shows the speedups obtained by parallel variants on synthetic datasets. The insights noted in Figure 1 are also applicable here for synthetic datasets. Barrier variants result in a maximum of 5x speedup on synthetic datasets, whereas No-Sync variants (except for No-Sync-Edge) consistently produce greater than 10x speedup on almost all datasets. It is observed for Synthetic datasets that as size increases, No-Sync variants consistently outperform Barriers variants in terms of performance. Figure 3 and Figure 4 shows the speedups gained by the parallel version by varying thread count on randomly selected datasets (web-stanford a standard dataset and D70 a synthetic dataset). We apply the static load balancing technique to all parallel variants. With an increase in the number of threads, the speedup rate is signiﬁcantly less for barrier variants than the No-Sync variants since each thread has to wait for others in the barrier variants. This also leads No-Sync variants to have much better scalability in comparison to barrier variants. On the other hand, in No-Sync variants, as each thread progresses independently, we achieve a higher speedup with a higher thread value. These results suggest, our lock-free variant scales well with the increase in the number of threads. Figure 5 and Figure 6 show the speedup and L1-norm obtained by parallel variants on a randomly selected dataset (web-stanford a standard dataset and D70 a synthetic dataset) with a ﬁxed thread count(56). The summation of diﬀerences between PageRank of each node from sequential and parallel variants denotes L1-Norm. For most Barrier variants, the L1-norm is zero, which means the page rank values are equal to the sequential ones. No-Sync algorithms, except approximation algorithms on all datasets, is achieving a zero L1-norm. The value is high for No-Sync-Opt and No-Sync-Opt-Identical as we are performing the loop-perforation technique and skipping the computations when its PageRank value is less than 10 . The result of using the above approximation technique leads to an increase in speedup and L1-Norm. In Figure 7, we compare the number of iterations taken by each parallel variant. Ideally, we expect each variant to achieve convergence with the same number of iterations. In our case, as we are allowing threads to do partial updates on shared variables that depend on the convergence, No-Sync variants are taking a fewer number of iterations than barrier variants. Our lock-free variant not only gives better speedup but it also converges faster. Prior to this work, we knew about node-level convergence and algorithm-level convergence on the iterative algorithm, but to our knowledge, we are the ﬁrst ones to propose threadlevel convergence. Sleeping variants: We designed a case study to understand the importance of the Waitfree algorithm using predetermined steps of calling sleep function to threads during selected iterations. It was observed that every thread waited until the sleeping thread completed its task in the Barrier algorithm, while the task corresponding to the sleeping thread was resumed as soon as the thread woke up in the case of the No-sync approach. Nevertheless, the Wait-free (Barrier-helper) algorithm was vigorous to address both limitations. The Waitfree approach is designed so that threads do not have to wait for other threads; instead, they aid other threads in completing their tasks after completing their assigned work. Figure 8 displays the consistency of the Wait-free execution time even with an increase in sleep time; however, the election time of No-Sync and Barriers increases as sleep time increases. Failing variants: Wait-free algorithm handle thread failures, whereas other parallel algorithms fail to do so. Failures to the threads were added deterministically during the end of the initial iteration to study the eﬀect. As Figure 9 displays, with the increase in thread failures, the program execution time has increased.