Data Science Lab, University of Technology Sydney, Australia these authors contributed equally to this work Automated next-best action recommendation for each customer in a sequential, dynamic and interactive context has been widely needed in natural, social and business decision-making. Personalized next-best action recommendation must involve past, current and future customer demographics and circumstances (states) and behaviors, long-range sequential interactions between customers and decision-makers, multi-sequence interactions between states, behaviors and actions, and their reactions to their counterpart’s actions. No existing modeling theories and tools, including Markovian decision processes, user and behavior modeling, deep sequential modeling, and personalized sequential recommendation, can quantify such complex decision-making on a personal level. We take a data-driven approach to learn the next-best actions for personalized decisionmaking by a reinforced coupled recurrent neural network (CRN). CRN represents multiple coupled dynamic sequences of a customer’s historical and current states, responses to decision-makers’ actions, decision rewards to actions, and learns longterm multi-sequence interactions between parties (customer and decision-maker). Next-best actions are then recommended on each customer at a time point to change their state for an optimal decision-making objective. Our study demonstrates the potential of personalized deep learning of multi-sequence interactions and automated dynamic intervention for personalized decision-making in complex systems. In enterprise and complex problem-solving, automated and personalized decision-making is highly needed but rarely possible in practice. Personalized decision-making requires personalized next-best actions to be learned and used in a dynamic, sequential and interactive process and context, which is extremely demanding in both private and public sectors and natural and social systems. Examples are next-best treatments to-be-made by healthcare providers on patients, next-best trading strategies to-betaken by investors in a capital market, next-best interventions on cybersecurity attacks or climate change in real time, next-best communications between a bank and its clients, and any other services involving client-provider interactions. Personalized next-best action-taking sets up a high standard for long-term dependent, dynamic, sequential and interactive personalized and automated decision-making in sophisticated and constrained real-life environments. However, automated decision-making with personalized next-best action recommendation is extremely challenging: (1) the circumstances and response behaviors of each target client (interchangeable with customer) must be characterized and modeled when they evolve over time; (2) any decision actions taken by a decision-maker on the client at a time step takes place in a sequential and interactive context, where both client responses and decision actions interact and co-evolve under client-speciﬁc circumstances and decision-policy constraints, forming multiple interactive and coupled sequences; (3) often multiple decision choices are available at a time step, and the best decision action needs to ﬁt client states, expected decision goals and effect, and the underlying environment; (4) taking any decision actions will further affect the state, action and environment at the next time step, and the cumulative effect from all prior steps also evolves along the sequential action-response interactions, which form long-term dependencies between multiple sequences to affect the next-best action selection; and (5) while each next-best action is to achieve an expected local goal and effect on the client, the sequence of next-best actions should generate the optimal global goal and effect. complexities in the aforementioned personalized next-best action-taking in complex enterprises and systems. This domaindriven action selection collectively considers and balances the relationships between service policies, constraints, client circumstances, business procedures, risk indicators, decision rules, and intervention strategies. Although hand-crafted action rules may be effective for speciﬁc and static scenarios on a small scale, they are ad hoc and ineffective for wide and dynamic applications and for large-scale real-time decision-making. They lack a general and proactive capacity to tackle personalized, sequential and interactive decision-making and often result in issues such as a high false intervention rate, high missing rate, In practice, domain-driven action rules are often generated and tuned by a group of domain experts to address the and low cost-effectiveness. optimal recommendation of personalized next-best actions in the above complex decision-making settings. This, however, poses a signiﬁcant challenge to existing decision-support systems and modeling methods, including sequential decisionmaking existing theories or modeling methods capable of handling the aforementioned demand and challenges in an automated or semi-automated manner. Typical sequential decision-making methods decision processes (MDPs), i.e., the next state only depends on the current state and action historical states such as by weighing their impact on current states goes beyond Markovian states, responses and actions More recent work selectively represents historical interactions between clients and decision-makers using methods such as temporal logic-based models for next-best action recommendation, since they either treat states and actions homogeneously, i.e., ignoring the differences between states and actions, or ignore their complex interactions and couplings, by taking a predeﬁned action on a state without selecting the actions for the best ﬁt between clients, states, actions, and contexts. In addition, personalized recommendation and sequential recommender systems (including next-item and next-basket recommendation) have emerged recently to recommend particular or next products to users who may prefer in the next context. The existing methods do not involve comprehensive user-product couplings and heterogeneities (i.e., non-IIDness of users and items interactions, sequential actions and responses, or optimal decision effects, etc. In addition, intensive research has been done on group decision making and recommendation of recommending personalized next-best actions in the aforementioned complex decision-making settings. CRN integrates deep learning, reinforcement learning, behavior informatics and recommender systems to learn dynamic, sequential, interactive and personalized decision-making processes. First, CRN models client circumstances, states, behaviors, responses and decision-making actions by multi-dimensional sequential representations using recurrent neural networks. This captures and transforms the states and behaviors of clients and actions made by decision-makers and their evolution into computable vector representations. Second, CRN builds a coupled recurrent unit (CRU) to capture relevant historical behaviors and simultaneously learn the following sophisticated couplings and interactions between clients and decision-makers on the above learned sequential representations using two long-term memories and ﬁve control gates: (1) the long-term sequential dependencies between an action and its previous actions taken by a decision-maker, called action-action dependence, to reveal the inﬂuence and transition between a series of prior actions and the current action; (2) the long-term sequential dependencies between a response and its previous responses made by a client, called response-response dependence, to learn the inﬂuence and transition between previous responses and the current one; and (3) the long-term sequential dependencies between a current response and its corresponding previous actions, called action-response dependence, to model the inﬂuence and transition between previous sequential actions and the current response of a client. As a result, CRU captures, represents and memorizes a sequence of relevant interactions between a client and a decision-maker with their particular states and behaviors and their history. Third, CRN combines the represented behaviors with the client’s current state features and transforms them to a compact client state representation, which models client states and their transition. Lastly, CRN models the reward to candidate actions and learns the dependence between the current reward to actions and the next client state in a compact state representation to determine the next-best action tailored for the client to achieve the decision goal. actions on speciﬁc debtors for tailored, active and efﬁcient debt collection. CRN automatically recommends the next-best action tailored for each debtor at a particular time by incorporating the debtor’s current state and historical records, the government’s optional and constrained action sequences, and reward to actions speciﬁed by their debt collectors (domain experts) measuring the effectiveness of action on debt collection. In contrast to the related work that either assumes a Markovian property of sequential decision-making actions or has a limited computational capability in modeling complex contexts and interactions in personalized decision-making, our approach collectively involves and automatically learns sequences of decision actions, client behaviors and states, their interactions and transitions, the action-action, response-response and action-response dependencies, and the action effect (reward) on client responses in dynamic, sequential and interactive decision-making contexts at a client level. The advances in new-generation artiﬁcial intelligence and data science have made possible the automated selection and , sequential and personalized recommendation, and deep learning. To the best of our knowledge, there are no Here, we introduce a computational approach: a reinforced coupled recurrent network (CRN) to model the intrinsic nature The CRN model was tested in a major Australian government agency for debt collection to recommend next-best intervention Learning next-best actions Assume a next-best action selection process (illustrated in Fig 1) involves a client and their demographics and states, a decision-maker and their actions taken on the client under certain policy constraints, the response (behaviors) of the client to the actions, and the reward that measures the effectiveness of an action on the client to achieve business goals at a time point. For example, in government services such as social welfare and taxation, when a client incurs a debt (called a debtor, i.e., a government client who owes money to the government), the government may take a series of actions to recover the debt in full or fast. Although debt collection is a widely used yet sophisticated process, experienced debt collectors not only consider a debtor’s circumstances, the government’s service policies and constraints, business objectives, and the effect of particular actions, they also monitor a debtor’s responses to the implemented actions before a new action is taken. Some collectors may quantify the rewards for applied actions to indicate their effectiveness in intervention. At present, such action-based debt collection is mainly driven by business assumptions and rules, i.e., debt collection rules which we also call domain-driven action rules for complex systems and decision-making personalized debt collection using the collectors’ experience, understanding and belief of the debtors’ circumstances and possible responses and judgment in matching actions with client proﬁles. debt collection action must be carefully chosen and applied on a debtor at a particular time point by considering the client’s circumstances, the government’s policies and service objectives, the previous actions already taken on the client, the debtor’s responses, the potential response to an action, and the business impact of interventions (e.g., whether the debt will be collected faster, in a less costly manner etc.). The action selection process also needs to consider a debtor’s evolving circumstances, which further change during the sequential interactions with the government. Consequently, debt collection often involves a sequence of constrained candidate actions and the interactions with debtors in dynamic contexts sequentially and interactively. In summary, smart debt collection must be tailored for each debtor and debt case, dynamic in terms of catering for evolving debtor circumstances and business environmental settings (i.e., states), interactive between debtors and debt collectors with their iterative communications over the collection process, and sequential with both preceding and successive actions and states considered. Figure 1. Next-best action-based personalized decision-making in constrained, tailored, sequential and interactive dynamic processes with state-action-response-coupled sequences. time steps. At each time point i, the client is associated with his demographics d action a undertaken action takes effect with reward r diagram only illustrates an ideal scenario: one client who interacts with one decision-maker, and one action corresponds to one response at each time point. From bottom to top: a rectangle represents a client’s demographics, a light-blue ellipse represents a client’s response, a circle represents a client’s states, a light-red ellipse represents decision actions, and a rounded rectangle represents policies constraining decision actions. However, domain-driven action selection is often ad hoc, costly and unsuitable for complex enterprise decision-making. A on the client’s state sunder policy constraint p. The client responds to the action with his behavior o, and the We model the above debt collection problem illustrated in Fig 1 as personalized next-best action recommendation on each client in a dynamic, sequential, interactive and constrained decision-making process. This personalized next-best action recommendation involves client information, sequences of client and decision-maker behaviors, and interactions between clients and the decision-maker under certain contexts and constraints at each time point. where the decision-maker to the client before the current time (i ≤ t − 1 the interaction time period, in which at time step behaviors, prior decision-making actions taken on the client, and the client responses to the actions. Accordingly, comprehensive representation of client states, which will be further used to model the interactions with the decision-maker and quantify the effect of next-best action candidates. Further, after taking action effectiveness of a subset of satisfying policy constraints to achieve the topthe action associated with the highest reward is recommended, corresponding to the next-best action. sponds to the decision action that can lead to the highest reward per client state and to achieve the decision goal, which is learned by an action-value function response’s reward are the spaces of client descriptions, decision-making actions, and estimated rewards. Assuming real reward, the personalized next-best actions where parameters in the action-value function r where the action-value should be modeled as and client responses current state but ignores the client’s sequential behaviors in history. On the contrary, our action-value function captures the client circumstance client description dependencies between states, between actions, and between states and actions in the sequential state-action-response-coupled sequences (Fig 1), which sufﬁciently represent past-to-present interactions between a client and his decision-maker during sequential and interactive decision-making processes. at time step and deﬁne the objective function below to learn the action-value function capturing the long-term dependent interactions within the client group: where refers to the description of the i, and within function assuming the Markovian property between states, we model the long-term dependencies between client states, between decision actions, and between states and actions by jointly involving client circumstances, response behaviors to actions, and action constraints and rewards. In doing so, we capture the rich, personalized and evolving couplings and interactions in sequential, Without loss of generality, we assume a clientcover timetcan be described by a three-element tupleC=< D,A,O>, D= {d|i = 1,· ·· , n}refers to a set of client’s relatively stable informationd(e.g., the demographics of the client); refers to the size ofD;A= {a|i = 1, ·· · ,t − 1}refers to a sequence oft − 1past actions sequentially assigned by ); andO= {O|i = 1,· ·· ,t}refers to a sequence of client responses to the correspondingly assigned actions during ito actiona(i ≤ t) andnis the size of the response set.Cthus jointly captures the client’s circumstances, kactionsˆA= {a| j = 1, ·· · ,k}are selected as the next-best actions on the client from a candidate action setA By empowering reinforcement learningfor sequential and interactive decision-making, the next-best action correminimizeDiv(ˆR||R) −r(C,a) Div(·||·)is the divergence between the estimated reward spaceˆRand the actual reward spaceR, andθθθrefers to the The above action-value function differs from the typical reinforcement learning settings and Markovian decision processes We further adopt empirical error minimization to learn the action-value functionr(·,·)in Eq (1). For a group ofnclients t, we collect information about historical sequences of decision actions, responses, and rewards of each clientc, minimizel(r(C,a),r l(·, ·) : R × R → Rrefers to a loss function that measures the difference between the real and estimated rewards,C trefers to the maximal length of historical sequence of thej-th client. Our model also captures the client’s behaviors dynamic and interactive decision-making processes between individual clients in their group. After learning the action-value function optimizing the following objective function: demographics, debt amount and duration, historical debt collection actions applied by the government, and response behaviors, etc. to represent the debtor’s current description considerable by the government. We aim to optimize the objective function in Eq (3) to obtain the next-best intervention actions Modeling the process of personalized next-best action-oriented decision-making We model the personalized next-best action-oriented decision-making process as a personalized next-best action recommender, as shown in Fig 2. The next-best action recommender achieves the objective deﬁned in Eq tasks: (1) learning the action-value function, and (2) selecting the next-best actions. The ﬁrst task learns the action-value function and current state. Those actions with the top-k highest rewards are then recommended as the next-best actions. Figure 2. The framework for modeling the next-best action-oriented personalized decision-making. C representation describing client c at time t, s from the candidate action set actions. The recommender ﬁrst embeds a client’s demographics, behaviors and current state to a state vector s personalized representation module (Fig 3), then feeds of the action. The actions in the candidate set with the top-k highest rewards are then recommended as the next-best actions. prediction. The personalized client representation module represents each client behaviors and current state as a vector feeds r(·,·) next-best actions decision-making environments, where decision actions are constrained by related policies and/or environmental settings. This approach is also more efﬁcient that other approaches such as multi-class classiﬁcation-based action recommendation, because it does not need to estimate the probabilities of all possible actions (such estimation is often inefﬁcient and may generate meaningless results in practice). Personalized client representation by coupled recurrent networks We represent each client description client circumstances and the sequence of prior response behaviors to a sequence of corresponding past actions applied for decision-making up to time r(·,·), we further learn the personalized next-best actionsˆA= {a| j = 1, ·· · ,k}from the candidate action setAby For example, for the aforementioned debt collection, we model each debtor’s state at timetby involving the debtor’s = {a| j = 1, ·· · ,k} ⊆ Aon each debtor. r(·,·), which is then used in the second task to evaluate the actions in the candidate set based on a client’s behaviors Learning the action-value function is achieved by learning the personalized client representation and the action reward sto a selected actionaand evaluates the action reward (i.e., effectiveness) in terms of the learned action-value function . Those actions with the top-khighest rewards are selected from the candidate action set and recommended as the t. This transforms a client’s cumulative behaviors, current state and sensitivity to decision actions into a universal vector space. This personalized client representation of each client’s past and current situation forms a universal yet tailored foundation to further determine different decision-making tasks on the client level and makes it benchmarkable for different clients with the same state representation. We thus can make personalized next-best action recommendation in this client representation space for each client. makers in the past, different actions will be selected as the next-best ones to be taken on clients who share similar states to ﬁt their respective preferences and achieve the best possible reward for each client. Our method reveals the cumulative action effectiveness and the sensitivity of a client to actions by learning the complex interactions between a client’s responses and assigned actions. In addition, involving a client’s personal information at each time point further explains the ﬁtness between decision actions and client circumstances. For example, debtors with different demographics and family situations likely respond differently to the same debt collection action in a government debt recovery campaign. Our approach of integrating a client’s historical behavior sequence and their current personal information captures comprehensive factors affecting decision-making and is much more powerful than Markovian process models and other relevant methods. sequentially fed into the CRN. Initially, the client response’s hidden state is extracted by a fully connected network from the client’s relatively stable personal information. An embedding layer transforms actions described by categorical values (e.g., sending a message to a debtor) to numerical vectors. CRN embeds the client behaviors and personal information as a vector generated based on the client’s observable data and its characteristics by the deep network. We also extract domain-driven explicit features designed by domain experts to describe the explicit situations in the CRN and transform it to a vector Lastly, a client’s current state is represented by a vector through fully connected layers. in history and models client historical behaviors and interactions with the decision-maker using a coupled recurrent unit (CRU, Fig 4). Similar to the gated recurrent unit (GRU) outputs in CRU rather than one as in GRU, which correspond to actions and responses, respectively. Speciﬁcally, the historical sequences of actions and client’s responses are stored in the impact of historical response and action information on their current states respectively. Meanwhile, gates the impact of current states on updating the memory of historical information. In addition, CRU has an interaction gate capture the dependence between a decision action and a client response. With vector representation Since a client’s past behavior sequence reﬂects his personal responses and preferences to the actions taken by decision- We learn the personalized client representation using a coupled recurrent network (CRN, Fig 3). Given a client tuple ,A,O>, the decision actiona∈ Aand the set of client responsesO∈ Oat each prior time stepiare , which describes the hidden state of each client at timetin terms of a data-driven implicit feature sincesis purely CRN captures the complex couplings and interactions within and between the sequences of client states and decision actions at timetand vector representationaof decision actionaat timet − 1, the variables in CRU are calculated as follows: Figure 3. A reinforced coupled recurrent network to learn personalized client representation. Given a client c at current time t with the description C client, corresponding to the action, o corresponding to the client’s relatively stable personal information D FC refers to fully connected networks. where vectors with all elements as 1 and with a and dimension, dimension of response vector representation o, and n responses to the actions, and domain-driven factors considered in the decision-making process. For all clients, a personalized representation (see an example in Fig 5) is learned for each of them. The learned representations differ from or are similar to each other, corresponding to the similarity between their demographics and responses to actions. This provides a universal, comprehensive and benchmarkable representation to further conduct personalized decision-making. Reward prediction of next-best actions on client states We further measure the reward of each decision action on a client state using a reward prediction module (Fig 6), which is built on a residual network. The above learned client state representation vector orepresents the vector representation ofO,arepresents the vector representation ofa,arepresents the hidden state refers to the transformed domain-driven explicit features,sis the resultant state vector representation for the clientc, and σ (·)is the sigmoid function,tanh(·)is the hyperbolic tangent function,◦refers to the Hadamard product,1and1are Uare learnable matrices with an× ndimension,W,W,W,U,UandUare learnable matrices with an× n Uis a learnable matrix with an× ndimension, andIis a learnable matrix with an× ndimension,nis the As a result, each client is comprehensively represented in terms of his circumstances, past decision actions received, past Figure 4. A coupled recurrent unit (CRU) for modeling state-action-response-coupled long-term dependencies. a and two gates to control the impact of historical responses and actions on their current states. Gates current response and action states on updating the memory of their historical information respectively. to capture the dependence between a decision action and a client response. Figure 5. An example of representing three clients by the reinforced coupled recurrent network. Three debtors with different demographics and past response behaviors to the same decision actions are represented in three vectors. actions ais ﬁrst embedded through an action embedding layer (the same as the action embedding layer in the personalized client representation module) to input of the following three-layer residual network. The last layer of the residual network predicts the reward input action a decision-making actions. First, reward prediction is efﬁcient in processing a large number of states and actions since it learns a common reward prediction model for different clients. Given a client state representation, it efﬁciently predicts the reward values for different actions. Second, modeling complexity can be automatically controlled since the residual network structure is embedded with a potential bypass from low-level information to high-level information. When the input data involves hierarchical patterns, the high-level features will be learned for the ﬁnal prediction. For data with simple patterns, the low-level features will make a direct contribution to the ﬁnal prediction. This reduces over-ﬁtting in reward prediction and enables personalized client representation to be well learned to capture heterogeneous client behaviors, which are embedded in a common space for further decision-making tasks. candidate set toporefer to the representation vectors of the historical sequences of actions and client responses, respectively.randrare Athat satisfy decision-making policy constraints are input into the reward prediction module. The candidate action The residual network-based reward prediction module shows unique strengths in efﬁciently modeling large-scale sequential The next-best action recommendation module assesses the learned rewardr(·,·)associated with each action in the k(kis a hyperparameter to be determined by decision makers) highest rewards are recommended as the next-best actions Figure 6. Reward prediction for the next-best action on a client’s state. The reward (rating) of an action is predicted by residual networks corresponding to a client’s state. Strategies to learn from hierarchical imbalanced action-response interactions Real-life data often presents imbalanced distributions imbalanced and hierarchical across the attributes, attribute values, domain-driven rewards, and reward levels (Table 1). With respect to the actions, their frequency distribution is extremely imbalanced, which we call action imbalance. Some commonly taken actions may appear thousands of times more than other rarely taken actions. Regarding the client interactions, the counts of interactions between actions and clients are imbalanced, resulting in client interaction imbalance. For example, a small fraction of the client cohort may involve a large proportion of interactions. With regard to the reward of actions, most of the reward values given by domain experts to actions may be 0, leading to reward imbalance. Lastly, the action effectiveness is different, where a small number of actions are very strong and effective, thus they always generate a high reward, resulting in action effectiveness imbalance. to the personalized recommendation of next-best actions. Action imbalance makes the model sensitive to those actions with high frequency but insensitive to the rarely appearing actions. This is caused by the model parameters that are trained predominantly by samples with high-frequency actions in the training phase if the imbalance is not catered for. The client interaction imbalance also affects the training of CRN. Since the sequence lengths of past client behaviors and decision actions are both short in most cases, it is difﬁcult for CRN to effectively capture the long-term dependencies in those few but long historical sequences. Further, reward imbalance induces the reward prediction of the model to be hence the model cannot generate the next-best actions. In addition, the action effectiveness imbalance also results in the model consistently selecting those highly effective actions (which are usually tough actions) by prediction, which tends to recommend tough actions at all times for all clients. However, such recommendations mostly violate government service policies and constraints. In addition, the various imbalances are mixed with each other in the action-response interaction sequences, further increasing modeling difﬁculty. Consequently, the imbalanced distributions at different aspects bring signiﬁcant but different challenges to personalized next-best action modeling. hierarchical imbalances in action-response interactions. The key idea behind these strategies is to introduce explicit knowledge to regulate the implicit learning of multiple sequences and their dependencies in CRN (see the section on personalized client for the client. These hierarchical imbalanced distributions in actions, interactions, rewards and action effects bring a signiﬁcant challenge Accordingly, we propose several strategies to improve CRN training (Section ) and tackle the challenges brought by the representation). Speciﬁcally, the various imbalances are ﬁrst statistically quantiﬁed; then, the statistic information is used to sample the training data, weight the importance of samples, and adjust the effect on reward prediction loss. The respective strategies to tackle the imbalance at different aspects are as follows. The pilot settings and characteristics A backtesting of our personalized next-best action recommendation was conducted on ﬁve-year (2012-2017) debt collection data in a major Australian government agency. A subset of 5-year debt-related data from the government was used, which comprises 61,361 clients, 10 selected debt collection actions, and 66,126 client response-government action sequences in a total of 111,514 debt transactions. The data comprises attributes about client demographics and circumstances, the debt amount and duration at each time point associated with a debtor, a list of optional debt collection actions and their application policy constraints, a sequence of historical actions taken by the government on a debtor to recover the debt at each time point, the corresponding client response behavior to each debt collection action, and the time information associated with debt cases, responses and actions. collection experts rate the reward associated with each action on the debtor population (rather than individual debtors). Accordingly, we categorize all optional actions into two categories: (1) the low-reward action group where actions receive reward less than 0.5, and (2) the high-reward action group where actions receive reward larger than 0.5. The corresponding reward distribution of 10 selected debt collection actions (annotated for privacy consideration) is shown in Table 1, where the distribution of actions and their rewards over the ﬁve years is highly imbalanced. The most frequent action is Action 6 (A6) which appeared 62,263 times, while the least frequent action is Action 2 (A2) which only appeared 390 times. The length distribution of historical action sequences on each debtor is also imbalanced. Only 50% of clients had their action sequence length larger than 4. In addition, the domain-driven rewards given to these debt collection actions are also imbalanced, the mean reward of all actions is under 0.32, and the highest reward given to all actions equals 1. These show the need for handling the hierarchical imbalances with our strategies proposed in Section . resource constraints in the pilot, the government only selected a proportion of debtors from the entire debtor pool to apply the intervention actions recommended by our method. We calculated the average domain-driven reward given by the debt • Action imbalance: Setting the weight of client c with action aas w=exp(1/ f)∑exp(1/ f) wherefis the frequency of actiona, andmis the total number of actions. To reﬂect action imbalance in the loss function (Eq (3)), the loss value on the client is multiplied by wfor backward gradient propagation. •Client interaction imbalance: Sampling the training data with probabilities{p|i = 1,· ·· , n}for allnclients in each batch, pis the sampling probability of the i-th client and is calculated as where lis the length of historical information of the j-th client and nis the number of clients. • Reward imbalance: Setting the weight of the reward rto action aon client Cas w(r) = tanh(r The loss value (Eq(3)) of the client with rewardris multiplied withw, and only the top-klargest loss values in a batch are selected for backward gradient propagation. • Action effectiveness imbalance: Adjusting reward rin training samples as where t is the time duration (i.e., the current time step) when action ais assigned. In debt collection, those actions that likely bring about faster and more debt recovery are deemed as high reward. Debt We randomly split the data into training, validation and testing sets in proportions of 70%, 10%, 20% respectively. Due to collectors of the 10% highest predicted reward by our CRN model and reported it as our modeling performance. This result was agreed by the debt collectors to indicate how much percentage of debts can be deducted on average if the debt intervention was based on the next-best actions recommended by our model. For privacy reasons, we cannot report the government information or any details about the debtors and debt collectors in the pilot and cannot directly report the average debt deduction percentage incurred by our recommendations in comparison to that driven by the government’s rule-based action selection strategies. Instead, we report the reward lift and error reduction made by our model recommendations in comparison with the domain-driven debt collection rules. Table 1. The distribution of reward of 10 actions speciﬁed by debt collection experts. 10 actions were chosen by the government. Count(Reward lower than 0.5 by debt collectors. Count(Reward >= 0.5) refers to the number of action occurrences with reward larger than 0.5. Reward Mean refers to the mean of all rewards per action. Reward Std refers to the standard deviation of the rewards per action. High-reward Proportion refers to the percentage of the rewards per action. Baseline methods We test our CRN model against (1) domain-driven rules i.e. the debt collection rules deﬁned by the debt collection experts, (2) variants of three state-of-the-art deep models with modiﬁcations to cater for the next-best action recommendation: Google’s wide-and-deep (WD) model, LSTM and GRU-based RNNs, and (3) the combination of wide-and-deep model with RNN strategies. Speciﬁcally, the domain-driven rules were taken by the government, where debt collection actions were taken according to the government’s debt collection policies and constraints deﬁned by debt collection experts. Such domain-driven rule-based action-taking method reﬂects the best practice in the debt collection business and was taken as best practice, thus we treat it as the baseline to evaluate the effectiveness and business impact of our model recommendations. Second, the WD model was shown to achieve state-of-the-art results in recommendation decision process, and we revise it to learn decision rules based on the current state of a client. Third, the LSTM and GRU-based RNNs are shown to be effective in learning long-term dependencies. We embed historical client states into LSTM and GRU to transform a non-Markovian decision process to a Markovian decision process. They serve as the performance benchmark of the state-of-the-art Markovian decision process learning. Lastly, we combine the WD RNN with LSTM and GRU to take advantage of the two advanced deep modeling mechanisms: residual network (Res) and multiple layers (Multi), to form the best possible non-Markovian decision process learners: WD_LSTM, WD_GRU, WD_Res_LSTM, WD_Multi_LSTM, WD_Res_GRU, WD_Multi_GRU. They reﬂect the best possible performance we may achieve by hybridizing the state-of-the-art achievements in deep learning. following aspects: (1) Ability to reveal whether our model can effectively predict an accurate reward value; (2) Business impact to demonstrate whether the recommended next-best actions can lead to an estimated high reward for business in practice; and (3) Scalability to reﬂect whether CRN is scalable for handling a large amount of data. to form the initial states of CRU. This solves the cold-start problem in decision-making by assuming that clients with similar demographic features likely share similar behaviors. Our model uses the ReLU activation function has a batch-normalization layer after all non-linear layers. All multi-layer perceptron (MLP) networks in our model have three layers. We train the CRN model using the Adam algorithm We empirically evaluate the performance of the proposed personalized next-best action recommender CRN in terms of the In our experiments, CRN represents each client’s demographic features (e.g., client type, address, and industry sector, etc.) Recommendation of next-best actions for each client We applied the recommended next-best actions for ﬁve-year debt collection. As shown in Table 2, our reward prediction module achieves 2.1942 total average reward lift ( with 2.1089 ( improvement, respectively in recommending 10 next-best actions that satisfy the policy constraints for debt collection. By applying a hierarchical imbalanced training strategy (discussed in the Method section) on the CRN for reward prediction, our method achieves a reward lift of 2.5569 ( average and action average reward lift made by the WD model. Table 2. debt collection rules. A1 to A10 are 10 actions (Table 1) selected from ﬁve-year debt collection data by the government. CRN and CRN_IMB are our methods, WD, LSTM, WD_LSTM, WD_Res_LSTM, WD_Multi_LSTM, GRU, WD_GRU, WD_Res_GRU and WD_Multi_GRU are baseline deep models. ∆_IMB and ∆ refer to the improvement percentage made by CRN_IMB and CRN in comparison with the best competitors, respectively. faster, less costly and more debt collection. By comparing the domain-driven reward given by the debt collectors, we evaluate the precision of CRN-recommended actions in terms of calculating the percentage of domain-driven high-reward actions that CRN also predicts as high-reward ones. The results in Table 3 show that CRN results in 2.6465 (total_avg) and 3.2799 (action_avg) lift, which is 5.38% and 11.74% better than the best-performing WD model. CRN_IMB further shows that CRN improves action_avg to 3.3816, which is 15.20% better than the WD model. Our method largely improves the precision for those actions rarely applied in business (e.g., A2 which only appeared 100 times in ﬁve years), which are shown to be more effective for some debtors. Table 3. collection rules. best competitors, respectively. ﬁve-year debt collection, which measures the difference between the domain-driven reward given by debt collection experts and the reward predicted by CRN for each action in the 10 action candidates. CRN recommendations achieve the best overall Average reward lift of the reward made by 10 actions recommended by 11 deep models over that by the government In the pilot, those actions with an estimated reward larger than 0.5 were applied as an intervention with their debtors for Precision lift of the precision achieved by 10 actions recommended by 11 deep models over that by government debt We evaluate CRN effectiveness w.r.t. the mean squared error (MSE, Table 4) of recommendations of next-best actions for MSE results, i.e., total_avg at 0.0777 and action_avg of 0.0613, and CRN makes a 3.24% and 7.26% improvement over the best-performing WD model in terms of total_avg and action_avg, respectively. Table 4. The reward mean squared error (MSE) per action between the reward made by the domain-driven debt collection rules and that recommended by 10 deep models. A1 to A10 are 10 actions (Table 1) selected from ﬁve-year debt collection data by the government. CRN is our method, WD, LSTM, WD_LSTM, WD_Res_LSTM, WD_Multi_LSTM, GRU, WD_GRU, WD_Res_GRU and WD_Multi_GRU are baseline deep models. with the best-performing method. The lower MSE indicates the better CRN performance. action_avg shows the unweighted average MSE and total_avg is the weighted average MSE of each method. The weighted average is calculated in terms of the number of actions. Fig 7. In our test environment (Section ), CRN converges within 20 epochs, and the mean computational cost in each epoch is around 2 minutes in our testing environment. These empirical results show that CRN can be applied to large-scale interaction data and problems. Figure 7. CRN convergence w.r.t. loss value on the validation debt collection data. The X-axis refers to the number of epochs, and the Y-axis refers to the loss value of the CRN objective function (Eq (3)). Personalized decision-making reﬂects a deep understanding of each customer’s circumstances and precision interventions on the customer (client) for optimal objectives. This is challenging when dynamic, interactive and sequential decision-making processes are involved. In this work, personalized deep learning is proposed to learn and recommend next-best actions for each customer in the above context. We model the client-decision-maker interactions and their decision-making context related to client circumstances and behaviors and decision-maker actions and constraints. The proposed reinforced coupled recurrent network (CRN) provides a general neural multi-sequence interaction learning solution to formalize multi-party interactions with real-life evolving, long-term dependent states and behaviors of customers and intervention actions by decision-makers for automated personalized decision-making. The CRN incorporated with coupled recurrent units (CRU) effectively and efﬁciently We further test our CRN model to show it can efﬁciently model large-scale client-decision-maker interactions, as shown in models and recommends next-best actions for each client-oriented dynamic, personalized and sequential decision-making. CRU (1) reveals the complex long-term dependencies between client states, between decision actions, and between client responses and decision-maker interventions, and (2) involves and determines the client and decision-maker’s historical information relevant to their responses and actions. In this way, we are able to model the complex multi-sequence interactions and coupling relationships between customer states and behaviors and decision-maker’s actions and constraints for dynamic and personalized decision-making. This involves characterizing and coupling the roles, relationships and dynamics of clients and decision-makers in past, present and future decision-making processes. interactive and long-term dependencies, learning sequential historical information about a client’s circumstances and sequential behavior responses to decision actions, and capturing the dynamic sequential interactions between client responses and decision actions. Our method, thus, goes beyond the usual way of assuming such decision-making processes as Markovian or convertible to Markovian cost and a high rate of meaningless recommendations. Our method captures the above diverse multi-sequence-coupled and longterm dependencies while also controlling the computational cost. This explains why our method outperforms the wide-and-deep model, which utilizes the Markovian decision process. on a deep representation of individual-level decision-making processes over time. CRN embeds the CRU-captured historical information and the current client state as a compact representation to learn decision rules, i.e., the dependencies between the current reward and historical states and actions. All these happen in a personalized and optimal manner, i.e., resulting in recommending next-best actions for each client at each time point per the then context. historical information by assuming a non-Markovian process but overlooks the sequential and multi-party interactions between stakeholders and between their behaviors. More research is required to further explore hierarchical, heterogeneous, time-varying and role-dependent couplings and interactions between multi-parties, between their behavior sequences, and between customer preferences and decision-making expectations. Multi-party interaction processes and dynamics also involve other challenges to be modeled, e.g., the imbalance in action distribution which may follow a Beta rather than a normal process in some applications, and hierarchical dependencies from attribute values to objects (e.g., clients), and the heterogeneities between customers. the sequential decision processes decision-making to capture the dependencies between historical states and the current action. However, our method additionally captures the long-term interactions between actions and states and between actions and also incorporates the historical behaviors of clients into the current client states. mendation) and interactive recommendation and the attention mechanism next-best action recommendation since they do not involve decision processes, the impact evaluation of next actions, or dynamic environments, etc. action and recommend the next-best action in relation to a customer’s current states, future rewards to actions, the customer’s future responses, and decision objectives. We model the effect of each action on each client by considering a client’s current context, past long-term behaviors, and decision feedback (effectiveness) on past actions measured by domain-driven rewards. This creates a way to involve domain knowledge, historical experience, and client and action-speciﬁc circumstances into a real-life complex decision-making process and interaction learning. interactive decision-making processes is often associated with diverse computational challenges. They include hierarchical imbalanced data distributions, multi-party interactions, and sequential, evolving, long-term and multi-sequence couplings and dependencies. Our neural interaction learning method paves a computational way to effectively and efﬁciently make personalized recommendations on next-best actions for a large number of clients in enterprise decision-making. Multi-party interactions involve multiple coupled sequences, e.g. of each party’s states, behaviors and contexts. Personalized decision-making needs to not only model these coupled sequences and the couplings both within and between these sequences but also the couplings between parties, e.g., between a decision-maker and its clients. The automated learning of next-best actions to be taken on each customer at each time is essential for personalized and automated decision-making in any applications Our multi-sequence interaction learning method shows the potential of effectively modeling multi-aspect sequential, We also show the potential of personalized decision-making by selecting actions for each client at each time point based This study also goes beyond non-Markovian decision process-based decision-making modeling, which models The recent advancements in RNNs with long short-term memory (LSTM)and GRU has widely been applied to model In addition, the recent work on sequential recommendation (such as next-item, next-basket and next-songrecom- Further, interactive personalized decision-making needs to dynamically evaluate and optimize the reward of each decision Lastly, the pilot study on next-best actions for debt collection shows that modeling personalized, dynamic, sequential and involving customer services and communications. Learning personalized next-best actions has to further model the multi-party interactions for each customer and his decision-maker and learn heterogeneous dynamic multi-sequence couplings. These issues go beyond classic decision theories, Markovian decision process theories, and sequential modeling and recommendation. User modeling, sequential modeling, behavior informatics, recommender systems and personalized decision-making should be integrated to address the challenges and complexities in learning automated decision-making with personalized next-best action recommendation and in dynamic, interactive and evolving personalized decision-making processes.