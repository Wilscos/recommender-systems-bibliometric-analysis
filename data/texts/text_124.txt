<title>Recommendation Fairness: From Static to Dynamic</title> <title>Jun Wang Dell Zhang</title> <title>jun.wang@cs.ucl.ac.uk dell.z@ieee.org University College London ByteDance AI Lab London, UK London, UK based on the existing cell values. Such a mat rix completion p rob-</title> <title>ABSTRACT</title> <title>lem is often solved by matrix factorizati on [45, 69] algorithms. ReDriven by the need to capture users’ evolving interests and opticently, the nonlinear neural variants of matrix factorization [15, 34, mize their long-term experiences, more and mo re recommender 35] for recommendation have attract ed a lot of attention, though systems have started to model recommendation as a Markov desome researchers have their reservations [56, 70]. cision process and employ reinforcement learning to address the Today, modern recommender systems underpinning various web problem. Shouldn’t research on the fairness of recommender sysor mobile apps are expected to become more personalized and intems follow the same trend from static evaluation and one-shot interactive so as to bett er serve the users. Consequently, traditional tervention to dynamic monitoring and non-stop control? In this parecommendation techniques based on matrix completion which asper, we portr ay the recent developments in recommender systems sume users’ preferences being static and aim to maximize their imﬁrst and then discuss how fairness could be baked into the reinmediate satisfaction would no longer work well. forcement learning techniques for recommendation. Moreover, we Reinforcement learning (RL) [80] — an area of machine l earning argue that in order to make further progress in recommendation which is concerned with optimal decision making over time in a dyfairness, we may want to consider multi-agent (game-theoretic) opnamic environment — oﬀers a promising approach to tackling the timization, multi-objective (Pareto) optimizat io n, and simulationproblems of personalization and interactivity by capturing users’ based optimization, in the general framework of stochastic games. evolving interests and optimizing their long-term experiences [99]. Inspired by the great successes of reinforcement learning, partic-</title> <title>CCS CONCEPTS</title> <title>ularly when it is combined with de ep learning [33, 48] such as • Information systems → Recommender systems; • Computin AlphaGo [74], reinforcement learning based recommender sysing methodologies → Reinforcement learning. tems [46, 95, 96] have just starte d to gain popularity in the last couple of years.</title> <title>KEYWORDS</title> <title>recommender systems, reinforcement learning, machine learning</title> <title>2.1 Recommendation as an MDP</title> <title>fairness To apply reinforcement learning to recommender systems, we need to mod el recommendation as a Markov de cision process (MDP) [80].</title> <title>1 INTRODUCTION</title> <title>For example, in a video recommender system [11, 57], the state Fairness in recommendation is o ne of the most important aspects space S describes the users each accompanied with their contexof evaluating and providing socially responsible recommender systual status (e.g., the time when the recommendation is made and tems. In this short position paper, we brieﬂy review the research the query text entered by the user), and t he action space A conabout recommendation fairness over the last ﬁve years and present sists of all possible video items available for recommendation. The our opinions about where this area should go next. The central idea state representing each user evolves as the user interacts with the is that the paradigm of recommendation fairness will b e shifting recommender system; diﬀerent users will have diﬀerent states. Us-</title> <title>arXiv:2109.03150v3  [cs.IR]  1 Nov 2021</title> <title>from static evaluation and one-shot intervention to dynamic moning reinforcement learning, we seek a policy 𝜋 (𝑎|𝑠) which returns itoring and non-stop control. a distr ibution of video items 𝑎 ∈ A for each given user state 𝑠 ∈ S. The o bjective is that the learned policy 𝜋 can maximize the ex-</title> <title>2 THE RISE OF REINFORCEMENT LEARNING</title> <title>pected discounted cumulat ive reward over potentially inﬁnite time</title> <title>FOR RECOMMENDATION</title> <title>horizon, where the immediate reward for taking action 𝑎 at state 𝑠 The mainstream approach to building recommender systems is to (i.e., reco mmending item 𝑎 to user 𝑠) is deﬁned by a reward funcformulate recommendation as matrix completion [39], i.e., given a tion 𝑅 : S × A → R. matrix of users by items where the value at cell (𝑖, 𝑗 ) represents the 𝑖-th user’s rating of the 𝑗-th item, predict the missing cell values</title> <title>2.2 Reinforcement Learning Algorithms in</title> <title>Recommender Systems</title> represent the views of our employers. <title>Many diﬀerent reinforcement learning algorithms have been em-</title> Dell Zhang i s on leave from Birkbeck, University of London, and working fu ll-time for ByteDance AI Lab. <title>ployed by a variety of recommender systems in recent years. The simplest ones [5, 32] utilize contextual (multi-armed ) bandits [47,</title> <title>77] which solve the sp ecial case of one-step reinforcement learn-</title> <title>ing; some [100–102] use value-based methods such as the Deep Q</title> <title>jointly represented as the states in the MDP model of recommendaNetwork (DQN) [36, 63]; some [11, 5 7, 81] use policy-based methtion. Furthermore, a two-fold reward function is designed to comods such as the policy gradient algorithm REINFORCE [80, 8 4]; and bine accuracy and fairness. Also on the user-side of fairness but not some [12, 88] adapt actor-critic methods [44, 75]. A hot research particularly for recommender systems, Wen et al. [82] studied retopic is to develop oﬄine reinforcement learnin g [42, 49, 90] methinforcement learning under group fairness constraints. They show ods for interactive recommendation [87, 105] which can make efhow fairness constraints from the supervised learning setting such fective use of previously collected user-item interaction data withas demographic parity and equality of opp ortunity can be extended out expensive online data collection. to the MDP setting. The algor ithms develop ed by them t o solve the MDP problem can ensure that the learned policy does not favor the</title> <title>3 THE EVOLUTION OF RECOMMENDATION</title> <title>majority sub-popu lation over the minority sub-populat ion. Zhang</title> <title>FAIRNESS</title> <title>et al. [97] investigated speciﬁcally the dynamics of group qualiﬁcaSigniﬁcant progress has been made in the space of algorithmic fairtion rates [65, 83] under the more general partially-observed MDP ness [59, 71] for recommender systems [18, 28, 52] , as reﬂected by setting. Moreover, there also exist some studies of fairness-aware the FAccTRec workshop series since it ﬁrst took place in 2017 [20, (contextual) multi-armed bandits [5, 31, 32, 47, 77] — a simpliﬁed 21, 41]. form of reinforcement learning — where the fairness constraint is deﬁned as a minimum rate at which a task/resource is assigned to</title> <title>3.1 Static Recommendation Fairness</title> <title>a user [13, 14, 51, 66, 89]. It has been discovered and reporte d that various types of bias could Ge et al. [29] have made an attempt on dynamic reco mmendaexist in recommender systems, such as those with respect to user tion fairness o f not users but items. Their work focuses on the the demographics (gender, age, and race, etc.) [30, 92, 103], user activefairness to diﬀerent groups of items divided according to their deness [26], and item popularity [1–3, 98]. Hence, many diﬀerent metgrees of popularity which dynamically change during the recomrics of fairness [9] have been proposed in o rder to build fairnessmendation process: pop ular items could become unpopular after aware recommender systems. Su c h recommendation fairness meta while and vice versa. To achieve long-term fairness in terms of rics can be deﬁned at two levels: individual fairness and group item exposure, the MDP model of recommendation is augmented fairness. Generally speaking, the techniques for counteracting bias with a set of fairness constraints each of which is an auxiliary fairand promoting fairness in recommendation so far are largely in the ness cost function bounded by the corresponding limit. Such a conform of constrained optimization [93]: either maximizing utility strained M DP problem can be solved by performing constrained (which is for the most part the relevance of recommended items to policy optimization with an actor-critic architecture. users) subject to a set of fairness constraints [10, 30], or maximizWhen a recommender system starts t o utilize reinforcement learning fairness subject to a lower bound of utility [103], or jointly oping to optimize its users’ long-term engagement, there will be a risk timizing both for an overall satisfaction [27]. Tho se works mostly of t he unethical phenomenon “user tampering” [8, 24 ] whereby the make fairness adjustments to traditional matrix completion based recommender system t ries to actively manipulate its users’ preferrecommender systems. Their concept of recommendation fairness ences via its recommendations in order to gain maximum accuis static in the sense that the protected groups are ﬁxed during the mulated reward. For example, a news reco mmender system may recommendation process. be tempted to (politically) polarize its users with the early recommendations so that the users will become more engaged with the</title> <title>3.2 Dynamic Recommendation Fairness</title> <title>system’s later recommendations catering to such polarization. Obviously, fairness issues will arise if diﬀerent users are aﬀected b y Little research has been conducted to investigate the fairness of reuser tampering diﬀerently. How to ensure dynamic recommendainforcement learning based recommender systems where the protion fairness while addressing u ser tampering is still an open questected groups may change over time. In dynamic environments tion. where the distribution of population is shifting or the decisions being made have feedback eﬀects, counter-intuitive phenomena (like the Simpson’s paradox) could occur and biases could be iteratively</title> <title>3.3 Looking Ahead</title> <title>ampliﬁed. For example, imposing static fairness criteria myopically 3.3.1 Multi-Agent (Game-Theoretic) Optimization. Since recommender at every step may act ually exacerbate unfairness [16, 17, 54, 65, 83, systems are by their nature mult i-sided platforms or marketplaces [64, 97]. 76] involving at least the consumers (customers) of items as well Note that although there exist a few papers talking about fairas the producers (providers) of items [6, 79], there have been some ness in reinforcement learning in general, not all those fairness defworks on optimizing the static fairness for all stakeholders of the initions are related to the fair treatment of diﬀerent users or items recommender system [ 7, 58, 60, 67, 78]. The usual approach to (grouped by sensitive protected attributes) in recommendation. For multi-sided static fairness is to use a l inear interpolation o f all example, the “meritocratic” fairness of reinforcement learning [38] stakeholders’ fairness metrics [67, 78, 85] as the optimization obdoes not se em to be very relevant to our fairness concern for recjective or constraint. That is probably not sophisticated enough to ommender systems. handle the intricacies of dynamic fairness in reinforcement learnLiu et al. [55] have proposed a fairness-aware recommendation ing. framework based on reinforcement learning to dynamically balWe think that the principled approach to multi-sided dynamic ance recommendation accuracy and user fairness in the long run. fairness for recommender systems is to consider it as a multi-agent The constantly changing user preferences and fairness statuses are Recommendation Fairness: From Static to Dynamic FAccTRec ’21, September 25, 2021, Amsterdam, Netherlands</title> <title>REFERENCES</title> <title>reinforcement learning (MARL) [91] problem, as in [40, 94, 104].</title> [1] Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher. 2017. Con- <title>This implies adopting t he mathematical framework of stochastic</title> trolling Popularity Bias in Learning-to-Rank Recommendation. In Proceed- <title>games [50, 53, 72] which generalize MDPs to multiple interacting</title> <title>decision makers. Note that to maximize the total social welfare</title> <title>while ensuring each agent (player) get a fair share of opportu ni-</title> <title>ties, we may want to go beyond Nash equil ibrium and embrace the</title> Mobasher. 2019. The Unfair ness of Popularity Bias in Recommendation. <title>more general correlated equilibrium which is also computationally</title> <title>more eﬃcient. The advantages of correlated equilibrium over Nash</title> <title>equilibrium in terms of fairness could be illustrated b y the clas-</title> Fairness in Recommendation. In Fourteenth ACM Conference on Recommender Systems (RecSys ’20). Association for Computing Machinery, New York, NY, <title>sic “battle of the sexes” game [50, 72]. Furthermore, behavior game</title> <title>theory may be useful for analyzing multi-sided dynamic fairness</title> [4] Krisztian Balog, Filip Radlinski, and Shushan Arakelyan. 2019. Trans- <title>in real-life recommender systems. As revealed by the observation</title> parent, Scrutable and Explainable User Models for Personalized Recommendation. In Proceedings of the 42nd International ACM SIGIR Con- <title>of human playing the “u ltimatum bargaining” game, people have</title> ference on Research and Development in Information Retrieval (SIGIR’19). <title>the tendency to pursue fairness even when it contradicts with the</title> Association for Computing Ma chinery, New York, NY, USA, 265–274. <title>subgame perfect equilibrium that maximizes their monetary pay-</title> <title>oﬀs [50, 72].</title> in Recommender Systems. In Fourteenth ACM Conference on Recommender Systems (RecSys ’20). Association for Computing Machinery, New York, NY, USA, <title>3.3.2 Multi-Objective (Pareto) Optimization. It is probably also time</title> <title>to leave the constrained optimization [27, 93] approach to recom-</title> [6] Robin Burke, Himan Abdollahpouri, Edward C. Malthouse, KP Thai, and Yongfeng Zhang. 2019. Recommendation in Multistakeholder Environments. <title>mendation fairness behind and seek the Pareto optimization [12]</title> <title>of multiple objectives including the utility and the fairness. Even</title> <title>when fairness is the only goal of our concern, researchers have rig-</title> [7] Robin Burke, Na sim Sonboli, and Aldo Ordonez-Gauger. 2018. Balanced Neigh- <title>orously proved that there are inherent conﬂicts among some com-</title> borhoods for Multi-Sided Fairness in Recommendation. In Conference on Fair- <title>mon fairness metrics and it is often impossible to opt imize them</title> ness, Accountability and Transparency. PMLR, 202–214. [8] Micah Carroll, Dylan Hadﬁeld-Menell, Stuart Russell, a nd Anca Dragan. <title>simultaneously [25, 43]. A few early works have emerged [73, 86 ]</title> 2021. Estimating and Penalizing Preference Shift in Recommender Sys- <title>and more should follow the steps.</title> tems. In Fifteenth ACM Conference on Recommender Systems (RecSys ’21). Association for Computing Ma chinery, New York, NY, USA, 661–667. <title>3.3.3 Simulation Environment. To fully comprehend and tackle the</title> https://doi.org/10.1145/3460231.3478849 [9] Alessandro Castelnovo, Riccardo Crupi, Greta Greco, and Daniele Regoli. 2021. <title>complexities of dynamic fairness in recommendation, it is highly</title> The Zoo of Fairness Metrics in Machine Learning. arXiv:2106.00467 [cs, stat] <title>desirable to develop a simulation environment for such multi-agent</title> (June 2021). arXiv: 2106.00467 [cs, stat] <title>multi-objective recommender systems where a number of fairness</title> [10] L. Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth Vishnoi. 2019. Controlling Polarization in Personalization: An Algorithmic Framework. In Pro- <title>metrics could be continuously monitored and optimized. Although</title> ceedings of the Conference on Fairness, Accountability, and Transparency (FAT* <title>Google has released Fairness-Gym [17] for t he simulation of sim-</title> <title>ple dynamic fairness tasks (loan app lication [ 54], college admis-</title> <title>sion [37, 61], and attention allocation [22, 23]), a simulator dedi-</title> <title>cated to dynamic fairness in recommendation is not available, yet.</title> System. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (WSDM ’19). Association for Computing Machinery, <title>This would require mo re fundamental research on understanding</title> <title>user behavior and bu ilding user models in recommender systems [4,</title> [12] Xu Chen, Yali Du, Long Xia, and Jun Wang. 2021. Reinforcement Recommen- <title>19, 68]. It also looks promising to incorporate fairness metrics and</title> dation with User Multi-Aspect Preference. In Proceedings of the Web Conference 2021 (WWW ’21). Association for Computing Machinery, New York, NY, USA, <title>models into some newly emerging probabilistic simulators of multi-</title> <title>agent recommender systems such as RecSim NG [62].</title> [13] Yifang Chen, Alex Cuellar, Haipeng Luo, Ji gnesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis. 2020. Fair Contextual Multi-Armed Bandits: Theory and Experiments. In Conference on Unc ertainty in Artiﬁcial Intelligence. PMLR, 181– <title>4 CONCLUSION</title> 190. [14] Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, and Stefanos Niko- <title>The recent developments in the area of reco mmendation fairness</title> laidis. 2020. Multi-Armed Bandits with Fairness Constraints for Distribut- <title>exhibits a clear trend towards the dynamic view of fairness. Ac-</title> ing Resources to Human Teammates. arXiv:1907.00313 [cs] (Dec. 2020). arXiv:1907.00313 [cs] <title>cordingly, the underlying mat hematical framework of fair recom-</title> [15] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neur al Net- <title>mendation will probably move from matrix completion to Markov</title> works for YouTube Recommendations. In Proceedings of the 10th ACM Con- <title>decision process and then to stochastic games. Such new mo dels</title> <title>and algorithms for fairness may not only improve diﬀerent kinds</title> <title>of recommender systems but also have impacts upon the broader</title> <title>ﬁeld of Responsible AI.</title> Machine Learning. PMLR, 2185–2195. [17] Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and Yoni Halpern. 2020. Fairness Is Not Static: Deeper Un- <title>ACKNOWLEDGMENTS</title> derstanding of Long Term Fairness via Simulation Studies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* <title>We are grateful to Dr Hang Li, Director of AI Lab at ByteDance, for</title> ’20). Association for Computing Machinery, New York, NY, USA, 525–534. <title>his guidance on and support for this line of research. We thank the</title> <title>workshop organizers fo r providing this forum and the reviewers for their constructive feedback.</title> [75] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and [57] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Martin Riedmiller. 2014. Deterministic Policy Gra dient Algorithms. In InternaHong, and Ed H. Chi. 2020. O ﬀ-Policy Learning in Two-Stage Rectional Conference on Machine Learning. PMLR, 387–395. ommender Systems. In Proceedings of The Web Conference 2020 (WWW [76] Ashudeep Singh and Thorsten Joachims. 2018. Fairness of Exposure in Rank’20). Association for Computing Machinery, New York, NY, USA, 463–473. ings. In Proceedings of the 24th ACM SIGKDD International Conference on Knowlhttps://doi.org/10.1145/3366423.3380130 edge Discovery & D ata Mining (KDD ’18). Association for Computing Machin[58] Masoud Mansoury. 2021. Fairness- Aware Recommendation in Multi-Sided Platery, New York, NY, USA, 2219–2228. https://doi.org/10.1145/3219819.3220088 forms. In Proceedings of the 14th ACM International Conference on Web Search [77] Aleksandrs Slivkins. 2019. Introduction to Multi-Armed Bandits. Founand Data Mining (WSDM ’21). Association for Computing Machinery, New dations and Trends in Machine Learning 12, 1-2 (Nov. 2019), 1–286. York, NY, USA, 1117–1118. https://doi.org/10.1145/3437963.3441672 https://doi.org/10.1561/2200000068 [59] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and [78] Tom Sühr, Asia J. Biega, Meike Zehlike, Krishna P. G ummadi, and Abhijnan Aram Galstyan. 2021. A Survey on Bi as and Fairness in Machine Learning. ComChakraborty. 2019. Two-Sided Fairness for Repeated Matchings in Two-Sided put. Surveys 54, 6 (July 2021), 115:1–115:35. https://doi.org/10.1145/3457607 Markets: A Case Study of a Ride-Hailing Platform. In Proceedings of the 25th [60] Ris habh Mehrotra, James McInerney, Hugues Bouchard, Mounia Lalmas, ACM SIGKDD International Conference on Knowledge Discovery & Data Mining and Fernando Diaz. 2018. Towards a Fair Marketplace: Counterfactual (KDD ’19). Association for Computing Machinery, New York, NY, USA, 3082– Evaluation of the Trade-oﬀ between Relevance, Fairness &amp; Satisfac3092. https://doi.org/10.1145/3292500.3330793 tion in Recommendation Systems. In Proceedings of the 27th ACM Interna[79] Özge Sürer, Robin Burke, and Edward C. M althouse. 2018. Multitional Conference on Information and Knowledge Management (CIKM ’18). stakeholder Recommendation with Provider Constraints. In ProceedAssociation for Computing Machinery, New York, NY, USA, 2243–2251. ings of the 12th ACM C onference on Recommender Systems (RecSys ’18). https://doi.org/10.1145/3269206.3272027 Association for Computing Machinery, New York, NY, USA, 54–62. [61] Smitha Milli, John Miller, Anca D. Dragan, and Moritz Hardt. 2019. The Social https://doi.org/10.1145/3240323.3240350 Cost of Strategic Classi ﬁcation. In Proceedings of the Conference on Fairness, Ac[80] Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Incountability, and Transparency (FAT* ’19). Association for Computing Machintroduction (second ed.). MIT Press. ery, New York, NY, USA, 230–239. https://doi.org/10.1145/3287560.3287576 [81] Jun Wang, Lantao Yu, Weinan Zhang, Yu G ong, Yinghui Xu, Benyou Wang, [62] Martin Mladenov, Chih-Wei Hsu, Vihan Jain, Eugene Ie, Christopher Colby, Peng Zhang, and D e ll Zhang. 2017. IRGAN: A Minimax Game for Unifying Nicolas Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, and Craig Boutilier. Generative and Discriminative Information Retrieva l Models. In Proceedings of 2021. RecSim NG: Toward Principled Uncertainty Modeling for Recommender the 40th International ACM SIGIR Conference on Research and Development in Ecosystems. arXiv:2103.08057 [cs] (March 2021). arXiv:2103.08057 [cs] Information Retrieval (SIGIR ’17). Association for Computing Machinery, New [63] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel VeYork, NY, USA, 515–524. https://doi.org/10.1145/3077136.3080786 ness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, [82] Min Wen, Osbert Bastani, and Ufuk Topcu. 2021. Algorithms for Fairness in Seand Georg Ostrovski. 2015. Human-L evel Control through Deep Reinforcement quential Decision Maki ng. In International Conference on Artiﬁcial Intelligence Learning. Nature 518, 7540 (2015), 529. and Statistics. PMLR, 1144–1152. [64] Marco Morik, Ashudeep Singh, Jessica Hong, and Thorsten Joachims. 2020. [83] Joshua Williams and J. Zico Kolter. 2019. Dynamic Modeling and EquiControlling Fairness and Bias in Dynamic Learning-to-Rank. In Proceedings of libria in Fair Decision Making. arXiv:1911.06837 [cs, stat] (Nov. 2019). the 43rd International ACM SIGIR Conference on Research and Development in arXiv:1911.06837 [cs, stat] Information Retrieval (SIGIR ’20). Association for Computing Machinery, New [84] Ronald J. Williams. 1992. Simple Statistical Gr adient-Following Algorithms for York, NY, USA, 429–438. https://doi.org/10.1145/3397271.3401100 Connectionist Reinforcement Learning. Machine Learning 8, 3-4 (1992), 229– [65] Hussein Mouzannar, Mesrob I. Ohannessian, and Nathan Srebro. 2019. 256. From Fair Decision Making to Social Equality. In Proceedings of the [85] Yao Wu, Jian Cao, Gu andong Xu, and Yudong Tan. 2021. TFROM: Conference on Fairness, Accountability, and Transparency (FAT* ’19). A Two-Sided Fairness-Aware Recommendation Model for Both Customers Association for Computing Ma chinery, New York, NY, USA, 359–368. and Providers. In Proceedings of the 44th International ACM SIGIR Conhttps://doi.org/10.1145/3287560.3287599 ference on Research and Development in Information Retrieval (SIGIR ’21 ). [66] Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y. Narahari. 2020. AchievAssociation for Computing Machinery, New York, NY, USA, 1013–1022. ing Fairness in the Stochastic Multi-Armed Bandit Problem. Proceedings of https://doi.org/10.1145/3404835.3462882 the AAA I Conference on Artiﬁcial Intelligence 34, 04 (April 2020), 5379–5386. [86] Lin Xiao, Zhang Min, Zhang Yongfeng, Gu Zhaoquan, Liu Yiqun, and Ma Shaophttps://doi.org/10.1609/aaai.v34i04.5986 ing. 2017. Fairness-Aware Group Recommendation with Pareto-Eﬃciency. In [67] Gourab K Patro, Arpita Biswas, Niloy G anguly, Krishna P. Gummadi, and AbProceedings of the Eleventh ACM Conference on Recommender Systems (Rechijnan Chakraborty. 2020. FairRec: Two-Sided Fairness for Pers onalized RecSys ’17). Association for Computing Machinery, New York, NY, USA, 107–115. ommendations in Two-Sided Platforms. In Proceedings of The Web Conference https://doi.org/10.1145/3109859.3109887 2020 (WWW ’20). Association for Computing Machinery, New York , NY, USA, [87] Teng Xiao and Donglin Wang. 2021. A General Oﬄine Reinforcement Learning 1194–1204. https://doi.org/10.1145/3366423.3380196 Framework for Interactive Recommendation. In The T hir ty-Fifth AAAI Confer[68] Pearl Pu, Li Chen, a nd Rong Hu. 2011. A User-Centric Evaluation Framework ence on Artiﬁcial Intelligence, AAAI. for Recommender Systems. In Proceedings of the 5th ACM Conference on Recom[88] Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, a nd Joemon M. Jose. 2020. mender Systems (RecSys ’11). Association for Computing Machinery, New York, Self-Supervised Reinforcement Learning for Recommender Systems. In ProNY, USA, 157–164. https://doi.org/10.1145/2043932.2043962 ceedings of the 43rd International ACM SIGIR Conference on Research and Devel[69] Steﬀen Rendle. 2010. Factorization Machines. In 2010 IE EE International Conopment in Information Retrieval (SIGIR ’20). Association for Computing M achinference on Data Mining. 995–1000. https://doi.org/10.1109/ ICDM.2010.127 ery, New York, NY, USA, 931–940. https://doi.org/10.1145/3397271.3401147 [70] Steﬀen Rendle, Walid Krichene, Li Zhang, and John Anderson. 2020. [89] Huanle Xu, Yang Liu, Wing Cheong Lau, and Rui Li. 2020. Combinatorial MultiNeural Collaborative Filtering vs. Matrix Factorization Revisited. In Armed Bandits with Concave Rewards and Fairness Constrai nts. In TwentyFourteenth ACM Conference on Recommender Systems (RecSys ’20). AsNinth International Joint Conference on Artiﬁcial Intelligence, Vol. 3. 2554–2560. sociation for Computing Machinery, New York, NY, USA, 240–248. https://doi.org/10.24963/ijcai.2020/354 https://doi.org/10.1145/3383313.3412488 [90] Mengjiao Yang and Oﬁr Nachum. 2021. Representation Matters: Oﬄine Pre[71] Pedro Saleiro, Kit T. Rodolfa, and Rayid Ghani. 2020. Dealing with Bias and training for Sequential Decision Ma king. arXiv:2102.05815 [cs] (Feb. 2021). Fairness in Data Science Systems: A Practica l Hands-on Tutorial. In Proceedings arXiv:2102.05815 [cs] of t he 26th ACM SIGKDD International Conference on Knowledge Discovery & [91] Yaodong Yang and Jun Wang. 2021. An Overv iew of Multi-Agent ReinforceData Mining. Association for Computing Machinery, New York, NY, USA, 3513– ment Learning from Game Theoretical Perspective. arXiv:2011.00583 [cs] 3514. (March 2021). arXiv:2011.00583 [cs] [72] Yoav Shoham and Kevin Leyton-Brown. 2008. Multiagent Systems: Algorithmic, [92] Sirui Yao and Bert Huang. 2017. Beyond Parity: Fairness Objectives for CollabGame-Theoretic, and Logical Foundations. Cambridge University Press. orative Filtering. Advances in Neural Information Processing Systems 30 (2017). [73] Umer Siddique, Paul Weng, and Matthieu Zimmer. 2020. Lea rning Fair Poli[93] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez-Rodriguez, and Krishna P. cies in Multi-Objective (Deep) Rei nforcement Learning with Avera ge and DisGummadi. 2019. Fairness Constrai nts: A Flexible Approach for Fair Classiﬁcacounted Rewards. In International Conference on Machine Learning. PMLR, tion. Journal of Machine Learning Research 20, 75 (2019), 1–42. 8905–8915. [94] Chongjie Zhang and Julie A. Shah. 2014. Fairness in Multi-Agent Sequential [74] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Decision-Making. Advances in Neural Information Processing Systems 27 (2014). Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneer[95] Weinan Zhang, Xiangyu Zhao, Li Zhao, Dawei Yin, and Grace Hui Yang. shelvam, and Marc Lanctot. 2016. Mastering the Game of Go with Deep Neural 2021. DRL4IR: 2nd Workshop on Deep Reinforcement Learning for InforNetworks and Tree Sea rch. Nature 529, 7587 (2016), 484–489. mation Retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21 ). https://doi.org/10.1145/3219819.3219886 Association for Computing Machinery, New York, NY, USA, 2681–2684. [101] Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobi ng Liu, and Jiliang Tang. https://doi.org/10.1145/3404835.3462818 2020. Jointly Learning to Recommend and Advertise. In Proceedings of the 26th [96] Weinan Zhang, Xiangyu Zhao, Li Zhao, Dawei Yin, Grace Hui Yang, and ACM SIGKDD International Conference on Knowledge Discovery & Data Mining Alex Beutel. 2020. Deep Reinforcement Learning for Information Retrieval: (KDD ’20). Association for Computing Machinery, New York, NY, USA, 3319– Fundamentals and Advances. In Proceedings of the 43rd International ACM SI3327. https://doi.org/10.1145/3394486.3403384 GIR Conference on Research and Development in Information Retrieval (SIGIR [102] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing ’20). Association for Computing Machinery, New York, NY, USA, 2468–2471. Yuan, Xing Xie, and Zhenhui Li. 2018. DRN: A Deep Reinforcement Learnhttps://doi.org/10.1145/3397271.3401467 ing Framework for News Recommendation. In Proceedings of the 2018 World [97] Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan L iu, Hedvig Kjellstrom, Kun Zhang, Wide Web Conference (WWW ’18). International World Wide Web Conferand Cheng Zhang. 2020. How Do Fair Decisions Fare in Long-Term Qualiﬁcaences Steering Committee, Republic and Canton of Geneva, CHE, 167–176. tion?. In Advances in Neural Information Processing Systems , Vol. 33. Curran https://doi.org/10.1145/3178876.3185994 Associates, Inc., 18457–18469. [103] Ziwei Zhu, Xia Hu, and James Caverlee. 2018. Fairness-Aware Tensor[98] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Based Recommendation. In Proceedings of the 27th ACM International Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging PopuConference on Information and Knowledge Management (CIKM ’18). Aslarity Bias in Recommendation. In Proceedings of the 44th International ACM sociation for Computing Machinery, New York, NY, USA, 1153–1162. SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’21). Association for Computing Machinery, New York, N Y, USA, 11–20. https://doi.org/10.1145/3269206.3271795 [104] Matthieu Zimmer, Claire Glanois, Umer Siddique, a nd Paul Weng. 2021. Learnhttps://doi.org/10.1145/3404835.3462875 ing Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement [99] Xiangyu Zhao, Changsheng Gu, Haos henglun Zhang, Xiaobing Liu, Xiwang Learning. In International Conference on Machine Learning. PMLR, 12967– Yang, and Jiliang Tang. 2019. Deep Reinforcement Learning for Online 12978. Advertising in Recommender Systems. arXiv:1909.03602 [cs] (Sept. 2019). [105] Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ti ng Bai, Weidong Liu, JianarXiv:1909.03602 [cs] Yun N ie, and Dawei Yin. 2020. Pseudo Dyna-Q: A Reinforcement Learn[100] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and ing Framework for Interactive Recommendation. In Proceedings of the Dawei Yin. 2018. Recommendations with Negative Feedback via Pairwise 13th International Conference on Web Search and Data Mining (WSDM Deep Reinforcement Learning. In Proceedings of the 24th ACM SIGKDD In’20). Association for Computing Machinery, New York, NY, USA, 816–824. ternational Conference on Knowledge Discovery & Data Mining (KDD ’18). https://doi.org/10.1145/3336191.3371801 Association for Computing Machinery, New York, NY, USA, 1040–1048.