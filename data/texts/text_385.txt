<title>Optimizing Ranking Systems Online as Bandits</title> <title>arXiv:2110.05807v1  [cs.IR]  12 Oct 2021</title> <title>Chang Li</title> <title>Optimizing Ranking Systems Online as Bandits</title> <title>ACADEMISCH PROEFSCHRIFT ter verkrijging van de graad van doctor aan de Universiteit van Amsterdam op gezag van de Rector Magniﬁcus prof. dr. ir. K.I.J. Maex ten overstaan van een door het College voor Promoties ingestelde commissie, in het openbaar te verdedigen in de Agnietenkapel op donderdag 4 maart 2021, te 13.00 uur door</title> <title>Chang Li</title> <title>geboren te Hebei Promotiecommissie</title> The research was supported by the Netherlands Organisation for Scientiﬁc Research (NWO) under project number 612.001.551. With the aim to experience a different type of life and continue working on machine learning, I submitted my application for a PhD position at the University of Amsterdam. After four years and 9 months, I notice that it has been one of the best decisions I have made ever, not only because I received the offer from my supervisor Maarten de Rijke, but also because doing a Ph.D. is way more interesting, meaningful and, of course, challenging than what I expected. I used to think that doing a Ph.D. is nothing but producing some papers :P. After I joined the group, I realized that there are more things in Ph.D. life than papers. Thank you to all ILPSers for teaching me this. It has been a really interesting journey with all of you around. Special thanks to Maurits Bleeker for helping me translate the summary into a samenvatting. Maarten has been always trying to tell me that a Ph.D. thesis is more meaningful that a stack of papers, in several ways. Unfortunately, I did not fully get his points and understand how important it is until the beginning of my third year. One day, I was asked by one of my co-authors, Branislav Kventon, what will your thesis topic be? All of a sudden, Maarten’s words appeared, and I noticed that I should be thinking in a more structured way. Thank you Maarten, for the guidance and help during my study! A frustrating part of my Ph.D. has been that my papers got rejected eleven times in a row. Thank you to all my co-authors for helping me get through this. However, in my mind, a bigger challenge beyond this is how to make my research meaningful. Fortunately, I had Maarten as my supervisor, who supported me and helped to connect me to other researchers in the community. This is how I got to know Masrour Zoghi (who became my co-promoter) and Brano, two awesome collaborators. Thank you both for helping me with my research. Last but not least, I want to thank Ilya Markov for helping me think scientiﬁcally in the early stages of my research. I was fortunate enough to enjoy two internships. Thank you, Haoyun and Xinran at Bloomberg. That summer in New York was fantastic! Thank you, Hua at Apple. The internship in Cupertino was also amazingly special. I also want to thank Jeremy and Derek from the HR team at Apple for helping me relocate to the US during the COVID-19 pandemic. Feng, I owe you a big thank you. Because of you, I chose Europe. Without you, it would have been hard for me to reach this point. I really feel lucky and full of joy to have you as my girlfriend. And, of course, thank you for creating the cover of my thesis. 最后， 感谢来自父母的支持 。 读博这几年跟你们在一起的时 间也就一个多 月，感谢你们的理解，让我可以安心的在国外。 Although a bit sad, it is the time to conclude my student career. Thank you to all my classmates and teachers. <title>Contents</title> <title>Introduction</title> enough to capture shifts in user preferences. But, with interactive feedback, which is more timely than human annotated data, we can design a ranker that changes its ranking policy according to the change in user preferences, capturing the non-stationarity. In this thesis, we study the problems of online ranker optimization, and translate our ideas into new algorithms that work for novel Online Learning to Rank ( OLTR ) problems. More speciﬁcally, we formulate the problem of learning from noisy and biased feedback in ranking systems as different bandit problems [ 71 111 ], and propose new bandit algorithms to solve the proposed problems. Thus, the title of this thesis is Optimizing Ranking Systems Online as Bandits. <title>1.1 Research Outline and Questions</title> Through out the thesis, we want to answer the following question: how should we use implicit feedback to optimize ranking systems online? As implicit feedback is typically noisy, we face the challenge of exploration versus exploitation. Bandit algorithms are suitable for addressing this problem. In this thesis, we consider four different research directions in ranking system optimization and formulate each of them as a research question. The ﬁrst one is about ranker evaluation and the other three are related to OLTR. RQ1 How to conduct effective large-scale online ranker evaluation? Online ranker evaluation is one of the key challenges in information retrieval. While the user preferences for ranker over another can be inferred by using interleaving methods [ 130 ], the problem of how to effectively choose the ranker pair that generates the interleaved list without degrading the user experience too much is still challenging. On the one hand, if two rankers have not been compared enough, the inferred preference can be noisy and inaccurate. On the other hand, if two rankers are compared too many times, the interleaving process inevitably hurts user experience. This dilemma is known as the exploration versus exploitation dilemma. It is captured by the -armed dueling bandit problem [ 126 ], which is a variant of the -armed bandit problem, where feedback comes in the form of pairwise preferences. Today’s deployed search systems evaluate large numbers of rankers concurrently, and scaling effectively in the presence of numerous rankers is a critical aspect of nline ranker evaluation. We provide an answer to RQ1 in Chapter 2 by introducing the MergeDTS algorithm. MergeDTS uses a divide-and-conquer strategy to localize the comparisons carried out by the algorithm to small batches of rankers, and then employs Thompson Sampling ( TS to reduce the comparisons between suboptimal rankers inside these small batches. We conduct extensive experiments on three large-scale datasets to assess the performance of MergeDTS. The experimental results demonstrate that MergeDTS outperforms the state-of-the-art baselines. RQ2 How to achieve safe online learning to re-rank? and recommender systems [ 83 ]. However, this approach lacks exploration and thus is limited by the information contained in the ofﬂine training data. In the online setting, an algorithm can experiment with ranked lists and learn from feedback on them in a sequential fashion. Bandit algorithms are well-suited for this setting but they tend to learn user preferences from scratch (i.e., the “cold-start” problem), which results in a high initial cost of exploration. This poses an additional challenge of safe exploration in ranked lists. RQ2 is set up to address the safe exploration problem in OLTR . We address this question by introducing the BubbleRank algorithm in Chapter 3. BubbleRank can be viewed as a combination of ofﬂine and online Learning to Rank ( LTR ) algorithms. It uses the ofﬂine trained ranker, e.g., the production ranker, to obtain the initial ranked list, and then conducts safe online pairwise exploration to improve this list. The safety comes from the fact that BubbleRank explores the ranked lists by randomly exchanging items with their neighbors. Thus, during exploration, an item never moves too far from its original position. We analyze the performance as well as the safety of BubbleRank theoretically, and then conduct experiments on a large-scale click dataset to evaluate BubbleRank empirically. RQ3 How to conduct online learning to rank when users change their preferences constantly? Non-stationarity appears in many online applications such as web search and recommender systems [ 49 ]. User preferences are effected by different factors and may change constantly. An example is that user preferences are effected by abrupt incidents. Let us take the query “corona” as an example. Before January of 2020, the query was mainly related to a beer brand. But after the onset of the COVID-19 pandemic, the meaning of this word has shifted to a deadly virus. If the learning algorithm is not able to capture this change in user preferences, it would continue responding with beer-related items, which would hurt the user experience. We provide one solution to RQ3 in Chapter 4. Particularly, we consider abruptly changing environments where user preferences remain constant in certain time periods, named epochs, but change occurs abruptly at unknown moments called breakpoints. We introduce cascading non-stationary bandits, an online variant of the Cascade Model CM ) [ 27 ] with the goal of identifying the most attractive items in a non-stationary environment, and propose two algorithms: CascadeDUCB and CascadeSWUCB to solve this bandit problem. The performance of the proposed algorithms are analyzed, and two gap-dependent upper bounds on their -step regret are derived, respectively. We also establish a lower bound on the regret of the non-stationary OLTR problem, and show that both algorithms match the lower bound up to a logarithmic factor. At the end of the chapter, we evaluate the performance of the proposed algorithms on a real-world web search click dataset. RQ4 How to learn a ranker online considering both relevance and diversity? in decreasing order of relevance. However, this sorting process often makes the result list less diverse since items with the highest relevance generally come from the same topic. On the other hand, submodular functions are used for result diversiﬁcation [ 38 120 ], but this alone is not enough to guarantee items at the top of list to be relevant. In Chapter 5, we provide a solution to RQ4 by formulating the OLTR for relevance and diversity as the Cascade Hybrid Bandits ( CHB ). The CascadeHybrid algorithm is then proposed to solve CHB . We ﬁrst provide a gap free bound on the -step regret of CascadeHybrid , and then evaluate it on two real-world recommendation tasks: movie recommendation and music recommendation. <title>1.2 Main Contributions</title> We list the main contributions of the thesis in this section. The contributions of this thesis come in three groups: algorithmic contributions, theoretical contributions and empirical contributions. Algorithmic contributions We proposed ﬁve novel bandit algorithms to solve four online optimization problems in ranking systems: TS -based dueling bandit algorithm, MergeDTS, which solves the large-scale online ranker evaluation problem, Chapter 2. A safe online learning to re-rank algorithm, BubbleRank , which is inspired by bubble sort, Chapter 3. To the best of our knowledge, BubbleRank is the ﬁrst safe online learning to re-rank algorithm. The CascadeDUCB and CascadeSWUCB algorithms for solving the non-stationary OLTR problem, Chapter 4. To the best of our knowledge, the proposed CascadeDUCB and CascadeSWUCB are the ﬁrst bandit algorithms that solve the non-stationary OLTR problem. A hybrid algorithm, CascadeHybrid , for solving the OLTR problem, where both relevance ranking and result diversiﬁcation are critical to users, Chapter 5. To the best of our knowledge, CascadeHybrid is the ﬁrst bandit algorithm that considers both relevance and diversity in the ranking problem. Theoretical contributions We theoretically analyze the performance of all proposed algorithms. Our theoretical analyses do not aim to show superior performance of the algorithms, but to provide worst-case guarantees on their performance when deployed online. We provide: • A theoretical guarantee on the performance of MergeDTS, Chapter 2. • A theoretical analysis of BubbleRank and the safety of BubbleRank, Chapter 3. The ﬁrst theoretical analysis for the non-stationary OLTR problem, and an analysis of the proposed CascadeDUCB and CascadeSWUCB algorithms, Chapter 4. A theoretical analysis of CascadeHybrid for solving the OLTR for both relevance and diversity, Chapter 5. Evaluation contributions To complement our theoretical ﬁndings, we conduct extensive experiments to evaluate the proposed algorithms for the considered tasks: An empirical evaluation of MergeDTS, and the comparison against the state of the art in large-scale online ranker evaluation, Chapter 2. An empirical comparison of BubbleRank against baselines and a sanity check on the proven theoretical results, Chapter 3. An empirical evaluation of CascadeDUCB and CascadeSWUCB for the nonstationary OLTR task, Chapter 4. An empirical comparison of CascadeHybrid against baselines in OLTR for relevance and diversity, Chapter 5. <title>1.3 Thesis Overview</title> In this section, we provide a brief overview of the remaining chapters. In Chapters 2 to 5, we study four online ranking optimization tasks, formulate each of them as a bandit problem, and provide corresponding algorithms for solving each problem. The proposed algorithms are theoretically analyzed and empirically compared against the state of the art. More precisely, we organize them in the following order: Chapter 2 treats large-scale online ranker evaluation as a dueling bandits problem, and proposes the MergeDTS algorithm to solve it with theoretical guarantees on the performance. Experimental evaluation reveals that MergeDTS outperforms the state of the art baselines, e.g., MergeRUCB [ 130 ], DTS [ 115 ] and SelfSparring [109]. Chapter 3 studies the safe OLTR problem, and presents the BubbleRank algorithm. We analyze the performance of BubbleRank , and theoretically show that it is a safe algorithm. Then, we empirically compare BubbleRank with CascadeKL-UCB 65 ], BatchRank 132 ] and TopRank 72 ], and show that the performance of BubbleRank is comparable to those algorithms while only BubbleRank satisﬁes the theoretical safety constraint. Chapter 4 studies the non-stationary OLTR problem, where the user may change their preference over time. We formulate this problem as cascade non-stationary bandits and propose CascadeDUCB and CascadeSWUCB to solve the problem. Both algorithms have theoretical guarantees on their performance, and the experimental evaluation validates the theoretical ﬁndings. Chapter 5 presents the CascadeHybrid algorithm, which is designed to solve the OLTR problem for relevance and diversity. The proposed CascadeHybrid algorithm is theoretically sound and empirically outperforms CascadeLinUCB 133 and CascadeLSB [38]. In Chapter 6, we summarize all main ﬁndings of this thesis and provide several directions for future work. We have tried to keep the research chapters as close as possible to their published version (see below for the publication details). This implies that some research chapters have some overlap, especially concerning the related work and background material that they present. But we believe that this disadvantage is outweighed by the advantage that the research chapters are self-contained and by the fact that sticking close to the published versions of the research chapters prevents us from creating alternative versions of published work. Chapter 2 discusses online ranker evaluation. Chapters 3 to 5 consider the online learning to rank problem. It is recomended for the chapters to be read in order: Chapter 2, 3, 4 and 5. <title>1.4 Origins</title> The material in this thesis comes from the following publications: Chapter 2 is based on C. Li, I. Markov, M. de Rijke, and M. Zoghi. MergeDTS: A method for effective large-scale online ranker evaluation. ACM Transactions on Information Systems, 38(4):Article 40, August 2020 [79]. CL designed the algorithm, worked on the theory, and conducted the experiments. MZ helped with algorithm design and theory. All authors contributed to the writing. Chapter 3 is based on C. Li, B. Kveton, T. Lattimore, I. Markov, M. de Rijke, C. Szepesv ari, and M. Zoghi. BubbleRank: Safe online learning to re-rank via implicit click feedback. In UAI, July 2019 [77]. CL designed the algorithm, and conducted the experiments. BK worked on the theory. BK and MZ helped with the algorithm design. CL, TL, CS and MZ helped with the theory. All authors contributed to the writing. Chapter 4 is based on C. Li and M. de Rijke. Cascading non-stationary bandits: Online learning to rank in the non-stationary cascade model. In IJCAI, pages 2859–2865, August 2019 [74]. CL designed the algorithm, worked on the theory, and conducted the experiments. CL and MdR contributed to the writing. Chapter 5 is based on C. Li, H. Feng, and M. de Rijke. Cascading hybrid bandits: Online learning to rank for relevance and diversity. In RecSys, pages 33–42. ACM, September 2020 [78]. CL designed the algorithm, worked on the theory and conducted the experiments. CL, HF and MdR contributed to the writing. There are other publications that indirectly contributed to this thesis: C. Li and H. Ouyang. Federated unbiased learning to rank. In SIGIR. ACM, July 2021. Submitted [75]. C. Li and M. de Rijke. Incremental sparse bayesian ordinal regression. Neural Networks, 106:294–302, October 2018 [73]. B. Jiang, C. Li, M. de Rijke, X. Yao, and H. Chen. Probabilistic feature selection and classiﬁcation vector machine. ACM Transactions on Knowledge Discovery from Data, 13(2):Article 21, April 2019 [53]. C. Li, A. Grotov, I. Markov, and M. de Rijke. Online learning to rank with listlevel feedback for image ﬁltering. arXiv preprint arXiv:1812.04910, December 2018 [76]. <title>Effective Large-Scale Online Ranker Evaluation</title> This chapter is set up to address RQ1: RQ1 How to conduct effective large-scale online ranker evaluation? Particularly, we focus on solving the large-scale online ranker evaluation problem under the so-called Condorcet assumption, where there exists an optimal ranker that is preferred to all other rankers. We propose the MergeDTS algorithm, which ﬁrst utilizes a divide-and-conquer strategy that localizes the comparisons carried out by the algorithm to small batches of rankers, and then employs Thompson Sampling ( TS ) to reduce the comparisons between suboptimal rankers inside these small batches. The effectiveness (regret) and efﬁciency (time complexity) of MergeDTS are extensively evaluated using examples from the domain of online evaluation for web search. Our main ﬁnding is that for large-scale Condorcet ranker evaluation problems, MergeDTS outperforms the state-of-the-art dueling bandit algorithms. <title>2.1 Introduction</title> Online ranker evaluation concerns the task of determining the ranker with the best performance out of a ﬁnite set of rankers. It is an important challenge for information retrieval systems [ 48 88 ]. In the absence of an oracle judge who can tell the preferences between all rankers, the best ranker is usually inferred from user feedback on the result lists produced by the rankers [ 45 ]. Since user feedback is known to be noisy [ 32 55 56 90 ], how to infer ranker quality and when to stop evaluating a ranker are two important challenges in online ranker evaluation. The former challenge, i.e., how to infer the quality of a ranker, is normally addressed by interleaving methods [ 21 24 41 43 ]. Speciﬁcally, an interleaving method interleaves the result lists generated by two rankers for a given query and presents the interleaved list to the user. Then it infers the preferred ranker based on the user’s click feedback. As click feedback is noisy, the interleaved comparison of two rankers has to be repeated many times so as to arrive at a reliable outcome of the comparison. Although interleaving methods address the ﬁrst challenge of online ranker evaluation (how to infer the quality of a ranker), they give rise to another challenge, i.e., which rankers to compare and when to stop the comparisons. Without enough comparisons, we may mistakingly infer the wrong ranker preferences. But with too many comparisons we may degrade the user experience since we continue showing results from suboptimal rankers. Based on previous work [ 14 128 130 ], the challenge of choosing and comparing rankers can be formalized as a -armed dueling bandit problem [ 123 ], which is an important variant of the Multi-Armed Bandits ( MAB ) problem, where feedback is given in the form of pairwise preferences. In the -armed dueling bandit problem, a ranker is deﬁned as an arm and the best ranker is the arm that has the highest expectation to win the interleaving game against other candidates. A number of dueling bandit algorithms have been proposed; cf. [ 16 126 ] for an overview. However, the study of these algorithms has mostly been limited to small-scale dueling bandit problems, with the state-of-the-art being Double Thompson Sampling ( DTS ) [ 115 ]. By “small-scale” we mean that the number of arms being compared is small. But, in real-world online ranker evaluation problems, experiments involving hundreds or even thousands of rankers are commonplace [ 60 ]. Despite this fact, to the best of our knowledge, the only work that address this particular scalability issue is Merge Relative Upper Conﬁdence Bound ( MergeRUCB ) [ 130 ]. As we demonstrate in this chapter, the performance of MergeRUCB can be improved upon substantially. In this chapter, we propose and evaluate a novel algorithm, named Merge Double Thompson Sampling ( MergeDTS ). The main idea of MergeDTS is to combine the beneﬁts of MergeRUCB , which is the state-of-the-art algorithm for large-scale dueling bandit problems, and the beneﬁts of DTS , which is the state-of-the-art algorithm for small-scale problems, and attain improvements in terms of effectiveness (as measured in terms of regret) and efﬁciency (as measured in terms of time complexity). More speciﬁcally, what we borrow from MergeRUCB is the divide and conquer idea used to group rankers into small batches to avoid global comparisons. On the other hand, from DTS we import the idea of using Thompson Sampling ( TS ) [ 111 ], rather than using uniform randomness as in MergeRUCB, to choose the arms to be played. We analyze the performance of MergeDTS , and demonstrate that the soundness of MergeDTS can be guaranteed if the time step is known and the exploration parameter α > 0.5 (Theorem 2.1). Finally, we conduct extensive experiments to evaluate the performance of MergeDTS in the scenario of online ranker evaluation on three widely used real-world datasets: Microsoft, Yahoo! Learning to Rank, and ISTELLA [ 19 85 96 ]. We show that with tuned parameters MergeDTS outperforms MergeRUCB and DTS in large-scale online ranker evaluation under the Condorcet assumption, i.e., where there is a ranker preferred to all other rankers. Moreover, we demonstrate the potential of using MergeDTS beyond the Condorcet assumption, i.e., where there might be multiple best rankers. In summary, the main contributions of this chapter are as follows: (1) We propose a novel -armed dueling bandits algorithm for large-scale online ranker evaluation, called MergeDTS . We use the idea of divide and conquer together with Thompson sampling to reduce the number of comparisons of arms. (2) We analyze the performance of MergeDTS and theoretically demonstrate that the soundness of MergeDTS can be guaranteed in the case of known time horizon and parameter values in the theoretical regime. (3) We evaluate MergeDTS experimentally on the Microsoft, Yahoo! Learning to Rank and ISTELLA datasets, and show that, with the tuned parameters, MergeDTS outperforms baselines in most of the large-scale online ranker evaluation conﬁgurations. The rest of the chapter is organized as follows. In Section 2.2, we detail the deﬁnition of the dueling bandit problem. We discuss prior work in Section 2.3. MergeDTS is proposed in Section 2.4. Our experimental setup is detailed in Section 2.5 and the results are presented in Section 2.6. We conclude the chapter in Section 2.7. <title>2.2 Problem Setting</title> where r is the regret incurred by our choice of arms at time t. Let us translate the online ranker evaluation problem into the dueling bandit problem. The input, a ﬁnite set of arms, consists of a set of rankers, e.g., based on different ranking models or based on the same model but with different parameters [ 60 ]. The Condorcet winner is the ranker that is preferred, by the majority of users, over suboptimal rankers. More speciﬁcally, a result list from the Condorcet winner is expected to receive the highest number of clicks from users when compared to a list from a suboptimal ranker. The preference matrix records the users’ relative preferences for all rankers. Regret measures the user frustration incurred by showing the interleaved list from suboptimal rankers instead of the Condorcet winner. In the rest of the chapter, we use the term ranker to indicate the term arm in -armed dueling bandit problems since we focus on the online ranker evaluation task. <title>2.3 Related Work</title> MAB problem (a.k.a. utility-based dueling bandits), e.g., Self-Sparring, REX3 and Sparring T-INF [ 125 ], or have substantially suboptimal instance-dependent regret bounds as in the case of Sparring EXP3, which has a regret bound of the form O( KT ) as opposed to O(K log T ) . Indeed, as our experimental results below demonstrate, Sparring-type algorithms can perform poorly when the dueling bandit problem does not arise from a MAB problem. Below, we describe some of these algorithms to provide context for our work. Sparring [ ] uses two MAB algorithms, e.g., Upper Conﬁdence Bound ( UCB ), to choose rankers. At each step, Sparring asks each MAB algorithm to output a ranker to be compared. The two rankers are then compared and the MAB algorithm that proposed the winning ranker gets a reward of 1 and the other a reward of 0. Self-Sparring [ 109 ] improves upon Sparring by employing a single MAB algorithm, but at each step samples twice to choose rankers. More precisely, Sui et al. [109] use Thompson Sampling ( TS ) as the MAB algorithm. Self-Sparring assumes that the problem it solves arises from an MAB; it can perform poorly when there exists a cycle relation in rankers, i.e., if there are rankers and with > 0.5 > 0.5 and > 0.5 . As Self-Sparring does not estimate conﬁdence intervals of the comparison results, it does not eliminate rankers. Another extension of Sparring is REX3 29 ], which is designed for the adversarial setting. REX3 is inspired by the Exponential-weight algorithm for Exploration and Exploitation ( EXP3 ) [ 10 ], an algorithm for adversarial bandits, and has a regret bound of the form O( K ln (K)T ) . Note that the regret bound grows as the square-root of time-steps, but sublinearly in the number of rankers, which shows the potential for improvement in the case of large-scale problems. Relative Upper Conﬁdence Bound ( RUCB ) [ 127 ] extends UCB to dueling bandits using a matrix of optimistic estimates of the relative preference probabilities. At each step, RUCB chooses the ﬁrst ranker to be one that beats all other rankers based on the idea of optimism in the face of uncertainty. Then it chooses the second ranker to be the ranker that beats the ﬁrst ranker with the same idea of optimism in the face of uncertainty, which translates to pessimism for the ﬁrst ranker. The cumulative regret of RUCB after steps is upper bounded by an expression of the form O(K + K log T ) RMED1 61 ] extends an asymptotically optimal MAB algorithm, called Deterministic Minimum Empirical Divergence ( DMED ) [ 46 ], by ﬁrst proving an asymptotic lower bound on the cumulative regret of all dueling bandit algorithms, which has the order of Ω(K log T ) , and pulling each pair of rankers the minimum number of times prescribed by the lower bound. RMED1 outperforms RUCB and Sparring. Double Thompson Sampling ( DTS ) [ 115 ] improves upon RUCB by using TS to break ties when choosing the ﬁrst ranker. Speciﬁcally, it uses one TS to choose the ﬁrst ranker from a set of candidates that are pre-chosen by UCB. Then it uses another TS to choose the second ranker that performs the best compared to the ﬁrst one. The cumulative regret of DTS is upper bounded by O(K log T + K log log T ) . Note that the bound of DTS is higher than that of RUCB . We hypothesize that this is because the bound of DTS is rather loose. DTS outperforms other dueling bandits algorithms empirically and is the state-of-the-art in the case of small-scale dueling bandit problems [ 109 115 ]. As discussed in Section 2.6, for computational reasons DTS is not suitable for large-scale problems. The work that is the closest to ours is by Zoghi et al. [130] . They propose MergeRUCB , which is the state-of-the-art for large-scale dueling bandit problems. MergeRUCB partitions rankers into small batches and compares rankers within each batch. A ranker is eliminated from a batch once we realize that even according to the most optimistic estimate of the preference probabilities it loses to another ranker in the batch. Once enough rankers have been eliminated, MergeRUCB repartitions the remaining rankers and continues as before. Importantly, MergeRUCB does not require global pairwise comparisons between all pairs of rankers, and so it reduces the computational complexity and increases the time efﬁciency, as shown in Section 2.6.2. The cumulative regret of MergeRUCB can be upper bounded by O(K log T ) 130 ], i.e., with no quadratic dependence on the number of rankers. This upper bound has the same order as the lower bound proposed by Komiyama et al. [61] in terms of K log T , but it is not optimal in the sense that it has large constant coefﬁcients. As we demonstrate in our experiments, MergeRUCB can be improved by making use of TS to reduce the amount of randomness in the choice of rankers. More precisely, the cumulative regret of MergeRUCB is almost twice as large as that of MergeDTS in the large-scale setup shown in Section 2.6. A recent extension of dueling bandits is called multi-dueling bandits [ 14 102 109 ], where more than two rankers can be compared at each step. Multi-Dueling Bandits MDB ) is the ﬁrst proposed algorithm in this setting, which is speciﬁcally designed for online ranker evaluation. It maintains two UCB estimators for each pair of rankers, a looser conﬁdence bound and a tighter one. At each step, if there is more than one ranker that is valid for the tighter UCB estimators, MDB compares all the rankers that are valid for the looser UCB estimators. MDB is outperformed by Self-Sparring, the state-of-the-art multi-dueling bandit algorithm, signiﬁcantly [ 109 ]. In this chapter, we do not focus on the multi-dueling bandit setup. The reasons are two-fold. First, to the best of our knowledge, there are no theoretical results in the multi-dueling setting that allow for the presence of cyclical preference relationships among the rankers. Second, Saha and Gopalan [102] state that “(perhaps surprisingly) [. . . ] the ﬂexibility of playing sizesubsets does not really help to gather information faster than the corresponding dueling case ( k = 2 ), at least for the current subset-wise feedback choice model.” This statement demonstrates that there is no clear advantage to using multi-dueling comparisons over pairwise dueling comparisons at this moments. <title>2.4 Algorithm</title> In this section, we propose the MergeDTS algorithm, and explain the intuition behind it. Then, we provide theoretical guarantees bounding the regret of MergeDTS. <title>2.4.1 MergeDTS</title> Table 2.1: Notation used in this chapter. Algorithm 1 MergeDTS (Merge Double Thompson Sampling) Input: K rankers , a , . . . , a ; partition size ; exploration parameter α > 0.5 running time steps T ; probability of failure  = 1/T . Output: The Condorcet winner. {The comparison matrix} [a , . . . , a , . . . , [a , . . . , a {Disjoint batches of rankers, | {z } {z } element-wise and := 1} from B if u < 0.5 for any a ∈ B > 1 and |B | = 1 then with the next batch and decrement b {Phase I: Choose the ﬁrst candidate a = SampleTournament(W, B ) {See Algorithm 2} {Phase II: Choose the second candidate a = RelativeTournament(W, B , a ) {See Algorithm 3} {Phase III: Compare candidates and update batches} , a ) and increment w if a wins otherwise increment w {Phase IV: Update batch set} |B | ≤ then Pair the larger size batches with the smaller ones, making sure the size of every batch is in [0.5M, 1.5M]. , b = |B |. in any given batch is unlikely to be the Condorcet winner of the whole dueling bandit problem. As we will see again below, MergeDTS takes great care to avoid comparing suboptimal rankers against themselves because it results in added regret, but yields no extra information, since we know that each ranker is tied with itself. After the above elimination step, the algorithm proceeds in four phases: choosing the ﬁrst ranker (Phase I), choosing the second ranker based on the ﬁrst ranker (Phase II), comparing the two rankers and updating the statistics (Phase III), and repartitioning the rankers at the end of each stage (Phase IV). Of the four phases, Phase I and Phase II are the major reasons that lead to a boost in effectiveness of MergeDTS when compared to MergeRUCB. We will elaborate both phases in the remainder of this section. In Phase I, the method SampleTournament (Algorithm 2) chooses the ﬁrst candidate ranker: MergeDTS samples preference probabilities from the posterior distributions to estimate the true preference probabilities for all pairs of rankers in the batch (lines 1–3, the ﬁrst TS ). Based on these sampled probabilities, MergeDTS chooses the ﬁrst candidate so that it beats most of the other rankers according to the sampled preferences (line 5). Algorithm 2 SampleTournament Input: The comparison matrix W and the current batch B Output: The ﬁrst candidate a , a ∈ B and i < j do ∼ Beta(w + 1, w + 1) = 1 − θ 1(θ > 0.5) = arg max ∼ κ ; breaking ties randomly {First candidate} In Phase II, the method RelativeTournament (Algorithm 3) chooses the second candidate ranker: MergeDTS samples another set of preference probabilities from the posteriors of for all rankers in \{a (lines 1–2, the second TS ). Moreover, we set to be (line 3). This is done to avoid self-comparisons between suboptimal rankers for the reasons that were described above. Algorithm 3 RelativeTournament Input: The comparison matrix W, the current batch B and the ﬁrst candidate a Output: The second candidate a ∈ B and j 6= c do ∼ Beta(w + 1, w + 1) = 1 {Avoid self-comparison} = arg min ∼ φ ; breaking ties randomly {Second candidate} combined with the larger sized batches; we enforce that the number of rankers in the new batches is kept in the range of [0.5M, 1.5M]. <title>2.4.2 Theoretical guarantees</title> In this section, we state and prove a high probability upper bound on the regret accumulated by MergeDTS after steps, under the assumption that the dueling bandit problem contains a Condorcet winner. Since the theoretical analysis of MergeDTS is based on that of MergeRUCB , we start by listing two assumptions that we borrow from MergeRUCB in [130, Section 7]. Assumption 2.1. There is no repetition in rankers. All ranker pairs (a , a with i 6= j are distinguishable, i.e., 6= 0.5 , unless both of them are “uninformative” rankers that provide random ranked lists and cannot beat any other rankers. Assumption 2.2. The uninformative rankers are at most one third of the full set of rankers. These assumptions arise from the Yahoo! Learning to Rank Challenge dataset, where there are 181 out of 700 rankers that always provide random ranked lists. Assumption 2.1 ensures that each informative ranker is distinguishable from other rankers. Assumption 2.2 restricts the maximal percentage of uninformative rankers and thus ensures that the probability of triggering the merge condition (line 16 in Algorithm 1) is larger than Moreover, we emphasize that Assumptions 2.1 and 2.2 are milder than the assumptions made in Self-Sparring and DTS, where indistinguishability is simply not allowed. We now state our main theoretical result: Theorem 2.1. With the known time step , applying MergeDTS with α > 0.5 M ≥ 4 and  = 1/T to a K-armed Condorcet dueling bandit problem under Assumptions 2.1 and 2.2, with probability 1 −  the cumulative regret R(T ) after steps is bounded by: where := min , (2.4) is the minimal gap of two distinguishable rankers and C() = T −1 8αMK ln(T + C()) 8αMK ln(T + C()) · T + ≤ 1 + . (2.5) We note that the above expected bound holds only at time-step and so the horizonless version of MergeDTS does not possess an expected regret bound. The proof of Theorem 2.1 relies on the Lemma 3 in [ 130 ]. We repeat it here for the reader’s convenience. where = min is the minimal gap of two distinguishable rankers in batch B. <title>2.4.3 Discussion</title> The preﬁx “merge” in MergeDTS signiﬁes the fact that it uses a similar divide-andconquer strategy as merge sort. It partitions the -arm set into small batches of size The comparisons only happen between rankers in the same batch, which, in turn, avoids global pairwise comparisons and gets rid of the O(K dependence in the cumulative regret, which is the main limitation for using dueling bandits for large datasets. In contrast to sorting, MergeDTS needs a large number of comparisons before declaring a difference between rankers since the feedback is stochastic. The harder two rankers are to distinguish or in other words the closer is to 0.5 , the more comparisons are required. Moreover, if a batch only contains the uninformative rankers, the comparisons between those rankers will not stop, which incurs inﬁnite regret. MergeDTS reduces the number of comparisons between hardly distinguishable rankers as follows: (1) MergeDTS compares the best ranker in the batch to the worst to avoid comparisons between hardly distinguishable rankers; (2) When half of the rankers of the previous stage are eliminated, MergeDTS pairs larger batches to smaller ones that contain at least one informative ranker and enters the next stage. The second point is borrowed from the design of MergeRUCB. MergeDTS and MergeRUCB follow the same “merge” strategy. The difference between these two algorithms is in their strategy of choosing rankers, i.e., Algorithms 2 and 3. MergeDTS employs a sampling strategy to choose the ﬁrst ranker inside the batch and then uses another sampling strategy to choose the second ranker that is potentially beaten by the ﬁrst one. As stated above, this design comes from the fact that MergeDTS is carefully designed to reduce the comparisons between barely distinguishable rankers. In contrast to MergeDTS MergeRUCB randomly chooses the ﬁrst ranker and chooses the second ranker to be the one that is the most likely to beat the ﬁrst ranker, as discussed in Section 2.3. The uniformly random strategy inevitably increases the number of comparisons between those barely distinguishable rankers. In summary, the double sampling strategy used by MergeDTS is the major factor that leads to the superior performance of MergeDTS as demonstrated by our experiments. <title>2.5 Experimental Setup</title> In this chapter, we investigate the application of dueling bandits to the large-scale online ranker evaluation setup. Our experiments are designed to answer RQ1 , which we map into a set of six more reﬁned research questions. <title>2.5.1 Research questions</title> RQ1.1 In the large-scale online ranker evaluation task, does MergeDTS outperform the state-of-the-art large-scale dueling bandits algorithms in terms of cumulative regret, i.e., effectiveness? In the bandit literature [ 16 126 ], regret is a measure of the rate of convergence to the Condorcet winner in hindsight. Mapping this to the online ranker evaluation setting, RQ1.1 asks whether MergeDTS hurts the user experience less than baselines while it is being used for large-scale online ranker evaluation. RQ1.2 How do MergeDTS and the baselines scale computationally? What is the time complexity of MergeDTS ? Does MergeDTS require less running time than the baselines? RQ1.3 How do different levels of noise in the feedback signal affect cumulative regret of MergeDTS and the baselines? In particular, will we still observe the same results in RQ1.1 after a (simulated) user changes its behavior? How sensitive are MergeDTS and the baselines to noise? RQ1.4 How do MergeDTS and the baselines perform when the Condorcet dueling bandit problem contains cycles? Previous work has found that cyclical preference relations between rankers are abundant in online ranker comparisons [ 128 130 ]. Can MergeDTS and the baselines ﬁnd the Condorcet winner when the experimental setup features a large number of cyclical relations between rankers? RQ1.5 How does MergeDTS perform when the dueling bandit problem violates the Condorcet assumption? We focus on the Condorcet dueling bandit task in this chapter. Can MergeDTS be applied to dueling bandit tasks without the existence of a Condorcet winner? RQ1.6 What is the parameter sensitivity of MergeDTS? Can we improve the performance of MergeDTS by tuning its parameters, such as the exploration parameter α, the size of a batch M, and the probability of failure ? <title>2.5.2 Datasets</title> and not from their absolute quality. In other words, evaluating rankers with similar and possibly low performance is as hard as evaluating state-of-the-art rankers, e.g., LambdaMART [ 15 ]. Therefore, we stick to the standard setup of [ 41 130 ], treating each feature as a ranker and each ranker as an arm in the -armed dueling bandit problem. We leave experiments aimed at comparing different well trained learning to rank algorithms as future work. As a summary, the MSLR dataset contains 136 rankers, the Yahoo dataset contains 700 rankers and the ISTELLA dataset contains 220 rankers. Compared to the typical -armed dueling bandit setups, where is generally substantially smaller than 100 [5, 109, 115, 122], these are large numbers of rankers. Second, to answer RQ1.4 , we use a synthetic dataset, generated by Zoghi et al. [130] , which contains cycles (called the Cycle dataset in the rest of the chapter). The Cycle dataset has 20 rankers with one Condorcet winner, , and 19 suboptimal rankers, , . . . , a . The Condorcet winner beats the other 19 suboptimal rankers. And those 19 rankers have a cyclical preference relationship between them. More precisely, following Zoghi et al. [130] , the estimated probability of beating j = 2, . . . , 20 is set to = 0.51 , and the preference relationships between the suboptimal rankers are described as follows: visualize the 19 rankers , . . . , a sitting at a round table, then each ranker beats every ranker to its left with probability and loses to every ranker to its right with probability . In this way we obtain the Cycle dataset. Note that this is a difﬁcult setup for Self-Sparring, because Self-Sparring chooses rankers based on their Borda scores, and the Borda scores ( for each ranker 112 ]) are close to each other in the Cycle dataset. For example, the Borda score of the Condorcet winner is 10.19 , while the Borda score of a suboptimal ranker is 9.99 This makes it hard for Self-Sparring to distinguish between rankers. In order to be able to conduct a fair comparison, we generate the Cycle2 dataset, where each suboptimal ranker beats every ranker to its left with a probability of 0.51 and the Condorcet winner beats all others with probability 0.6 . Now, in the Cycle2 dataset, the Borda score of the Condorcet winner is 11.90 , while the Borda score of a suboptimal ranker is 9.9 . Thus, it is an easier setup for Self-Sparring. Furthermore, to answer RQ1.5 , we use the MSLR-non-Condorcet dataset from [ 115 ], which is a subset of the MSLR dataset that does not contain a Condorcet winner. This datasets has 32 rankers with two Copeland winners (instead of one), each of which beats the other 30 rankers. A Copeland winner is a ranker that beats the largest number of other rankers; every dueling bandit dataset contains at least one Copeland winner [ 129 ]. Finally, we use the MSLR dataset with the navigational conﬁguration (described in Section 2.5.4) to assess the parameter sensitivity of MergeDTS RQ1.6. <title>2.5.3 Evaluation methodology</title> bandit algorithm, we compare them by drawing a sample from a Bernoulli distribution with mean , i.e., means that beats and vice versa. This is a standard approach to evaluating dueling bandit algorithms [ 115 122 128 ]. Moreover, the proxy approach has been shown to have the same quality as interleaving in terms of evaluating dueling bandit algorithms [130]. In this chapter, we adopt the procedure described by Zoghi et al. [130] and obtain preference matrices for MSLR, Yahoo and ISTELLA datasets. Speciﬁcally, we use Probabilistic Interleave [ 41 ] to obtain preference matrices. The numbers of comparisons for every pair of rankers in MSLR, Yahoo and ISTELLA datasets are 400 000, 60 000 and 400 000. The reason for the discrepancy is pragmatic: the Yahoo dataset has roughly 27 times as many pairs of rankers to be compared. <title>2.5.4 Click simulation</title> As the interleaved comparisons mentioned above are carried out using click feedback, we follow Hofmann et al. [43] and simulate clicks using three conﬁgurations of a click model [ 23 ]: namely perfect, navigational and informational. The perfect conﬁguration simulates a user who checks every document and clicks on a document with a probability proportional to the query-document relevance. This conﬁguration is the easiest one for dueling bandit algorithms to ﬁnd the best ranker, because it contains very little noise. The navigational conﬁguration mimics a user who seeks speciﬁc information, i.e., who may be searching for the link of a website, and is likely to stop browsing results after ﬁnding a relevant document. The navigational conﬁguration contains more noise than the perfect conﬁguration and is harder for dueling bandit algorithms to ﬁnd the best ranker. Finally, the informational conﬁguration represents a user who wants to gather all available information for a query and may click on documents that are not relevant with high probability. In the informational conﬁguration the feedback contains more noise than in the perfect and navigational conﬁgurations, which makes it the most difﬁcult conﬁguration for dueling bandit algorithms to determine the best ranker, which, in turn, may result in the highest cumulative regret among the three conﬁgurations. To answer the research questions that concern large-scale dueling bandit problems, namely RQ1.1 RQ1.2 and RQ1.6 , we use the navigational conﬁguration, which represents a reasonable middle ground between the perfect and informational conﬁgurations [ 41 ]. The corresponding experimental setups are called MSLR-Navigational, Yahoo-Navigational and ISTELLA-Navigational. To answer RQ1.3 regarding the effect of feedback with different levels of noise, we use all three conﬁgurations on MSLR, Yahoo and ISTELLA datasets: namely MSLR-Perfect, MSLR-Navigational and MSLR-Informational; Yahoo-Perfect, Yahoo-Navigational and Yahoo-Informational; ISTELLA-Perfect, ISTELLA-Navigational and ISTELLA-Informational. Thus, we have nine large-scale setups in total. <title>2.5.5 Baselines</title> We compare MergeDTS to ﬁve state-of-the-art dueling bandit algorithms: MergeRUCB 130 ], DTS 115 ], RMED1 61 ], Self-Sparring [ 109 ], and REX3 [ 29 ]. Among these algorithms, MergeRUCB is designed for large-scale online ranker evaluation and is the state-of-the-art large-scale dueling bandit algorithm. DTS is the state-of-the-art small-scale dueling bandit algorithm. RMED1 is motivated by the lower bound of the Condorcet dueling bandit problem and matches the lower bound up to a factor of O(K , which indicates that RMED1 has low regret in small-scale problems but may have large regret when the number of rankers is large. Self-Sparring is a more recently proposed dueling bandit algorithm that is the state-of-the-art algorithm in the multi-dueling setup, with which multiple rankers can be compared in each step. REX3 is proposed for the adversarial dueling bandit problem but also performs well for the large-scale stochastic dueling bandit problem [ 29 ]. We do not include RUCB 127 ] and Sparring [ ] in our experiments since they have been outperformed by more than one of our baselines [29, 109, 130]. <title>2.5.6 Parameters</title> be too conservative. So, we only choose candidate values smaller than 4 726 908 . We use the log-scale of C() because the upper bound is logarithmic with C(). The sensitivity of parameters is analyzed by following the order of their importance to Theorem 2.1, i.e., and have a linear relation to the cumulative regret and has a logarithmic relation to the cumulative regret. We ﬁrst evaluate the sensitivity of with the default values of and . Then we use the best value of to test a range of values of (with default ). Finally, we analyze the impact of using the best values of α and M. We discover the practically optimal parameters for MergeDTS to be α = 0.8 M = 16 and C = 4 000 000 , in Section 2.6.6. We repeat the procedure for MergeRUCB and DTS , and use their optimal parameter values in our experiments, which are α = 0.8 M = 8 C = 400 000 for MergeRUCB and α = 0.8 for DTS . Then, we use these values to answer RQ1.1 to RQ1.5 . Self-Sparring does not have any parameters, so further analysis and tuning are not needed here. The shrewd readers may notice that the parameters are somewhat overtuned in the MSLR-Navigational setup, and MergeDTS with the tuned parameters does not enjoy the theoretical guarantees in Theorem 2.1. However, because of the existence of the gap between theory and practice, we want to answer the question whether we can improve the performance of MergeDTS as well as that of the baselines by tuning the parameters outside of their theoretical limits. We also want to emphasize that the parameters of MergeDTS and baselines are only tuned in the MSLR-Navigational setup, but MergeDTS is compared to baselines in nine setups. If, with the tuned parameters, MergeDTS outperforms baselines in other eight setups, we can also show the potential of improving MergeDTS in an empirical way. <title>2.5.7 Metrics</title> In our experiments, we assess the efﬁciency (time complexity) and effectiveness (cumulative regret) of MergeDTS and baselines. The metric for efﬁciency is the running time in days. We compute the running time from the start of the ﬁrst step to the end of the -th step, where T = 10 in our experiments. A commercial search engine may serve more than billion search requests per day [ 34 ], and each search request can be used to conduct one dueling bandit comparison. The total number of time steps, i.e. , considered in our experiments is about 1% of the one-week trafﬁc of a commercial search engine. We use cumulative regret in steps to measure the effectiveness of algorithms, which is computed as follows: Figure 2.1: Cumulative regret on large-scale online ranker evaluation: lower is better. Note that the scales of the y-axes are different. The shaded areas are standard error. The results are averaged over 50 independent runs. period to conduct our online ranker evaluation, and thus, the number of steps can be estimated beforehand. In our online ranker evaluation task, the -step cumulative regret is related to the drop in user satisfaction during the evaluation process, i.e. higher regret means larger degradation of user satisfaction, because the preference can be interpreted as the probability of the Condorcet winner being preferred to ranker i. Unless stated differently, the results in all experiments are averaged over 50 and 100 independent runs on large- and small-scale datasets respectively, where both numbers are either equal to or larger than the choices in previous studies [ 109 115 130 ]. In the effectiveness experiments, we also report the standard error of the average cumulative regret, which measures the differences between the average of samples and the expectation of the sample population. All experiments on the MSLR and Yahoo datasets are conducted on a server with Intel(R) Xeon(R) CPU E5-2650 2.00 GHz ( 32 Cores) and 64 Gigabyte. All experiments on the ISTELLA dataset are conducted on servers with Intel(R) Xeon(R) Gold 5118 CPU @ 2.30 GHz ( 48 Cores) and 256 Gigabyte. To be precise, an individual run of each algorithm is conducted on a single core with 1 Gigabyte. <title>2.6 Experimental Results</title> In this section, we analyze the results of our experiments. In Section 2.6.1, we compare the effectiveness (cumulative regret) of MergeDTS and the baselines in three large-scale online evaluation setups. In Section 2.6.2, we compare and analyze the efﬁciency (time complexity) of MergeDTS and the baselines. In Section 2.6.3, we study the impact of different levels of noise in the click feedback on the algorithms. In Section 2.6.4 and Section 2.6.5, we evaluate MergeDTS and the baselines in two alternative setups: the cyclic case and the non-Condorcet case, respectively. In Section 2.6.6, we analyze the parameter sensitivity of MergeDTS. <title>2.6.1 Large-scale experiments</title> To answer RQ1.1 , we compare MergeDTS to the large-scale state-of-the-art baseline, MergeRUCB , as well as the more recently proposed Self-Sparring, in three largescale online evaluation setups, namely MSLR-Navigational, Yahoo-Navigational and ISTELLA-Navigational. The results are reported in Fig. 2.1, which depicts the cumulative regret of each algorithm, averaged over 50 independent runs. As mentioned in Section 2.5, cumulative regret measures the rate of convergence to the Condorcet winner in hindsight and thus lower regret curves are better. DTS and RMED1 are not considered in the Yahoo-Navigational setup, and the cumulative regret of DTS is reported with in 10 steps on the ISTELLA dataset, because of the computational issues, which are further discussed in Section 2.6.2. Fig. 2.1 shows that MergeDTS outperforms the large-scale state-of-the-art MergeRUCB with large gaps. Regarding the comparison with Self-Sparring and DTS, we would like to point out the following facts: (1) MergeDTS outperforms DTS and Self-Sparring in MSLR-Navigational and ISTELLA-Navigational setups; (2) In the Yahoo-Navigational setup, MergeDTS has slightly higher cumulative regret than Self-Sparring, but MergeDTS converges to the Condorcet winner after three million steps while the cumulative regret of Self-Sparring is still growing after 100 million steps. Translating these facts into real-world scenarios, MergeDTS has higher regret compared to DTS and Self-Sparring in the early steps, but MergeDTS eventually outperforms DTS and Self-Sparring with longer experiments. As for REX3 , we see that it has a higher order of regret than other algorithms since REX3 is designed for the adversarial dueling bandits and the regret of REX3 is O( T ) In this chapter, we consider the stochastic dueling bandits, and the regret of the other algorithms is log (T ) MergeDTS outperforms the baselines in most setups, but we need to emphasize that the performance of MergeDTS here cannot be guaranteed by Theorem 2.1. This is because we use the parameter setup outside of the theoretical regime, as discussed in Section 2.5.6. <title>2.6.2 Computational scalability</title> Table 2.2: Average running time in days of each algorithm on large-scale problems for 10 steps averaged over 50 independent runs. The running time of DTS in the ISTELLA-Navigational setup is estimated based on the running time with 10 steps multiplied by 10 . The running time of DTS and RMED1 in the Yahoo-Navigational setup is estimated by multiplying the average running time at 10 steps by 10 . The experiments on the ISTELLA dataset are conducted on different computer clusters from those on the MSLR and Yahoo datasets. The speed of the former ones is about one time faster than the latter ones. Therefore the numbers may not be directly compared. on, MergeDTS does not perform any extra computations. The running time of SelfSparring is also low, but grows with the number of rankers, roughly linearly. This is because at each step Self-Sparring draws a sample from the posterior distribution of each ranker and its running time is Ω(T K) , where is the number of rankers. DTS is orders of magnitude slower than other algorithms and its running time grows roughly quadratically, because DTS requires a sample for each pair of rankers at each step and its running time is Ω(T K ). Large-scale commercial search systems process over a billion queries a day [ 34 ], and run hundreds of different experiments [ 60 ] concurrently, in each of which two rankers are compared. The running time for DTS and RMED1 that appears in Table 2.2 is far beyond the realm of what might be considered reasonable to process on even 20% of one day’s trafﬁc: note that one query in the search setting corresponds to one step for a dueling bandit algorithm, since each query could be used to compare two rankers by performing an interleaved comparison. Given the estimated running times listed in Table 2.2, we exclude DTS and RMED1 from our experiments on the large-scale datasets for practical reasons. <title>2.6.3 Impact of noise</title> the chosen parameters outperforms the baselines, and the gaps get larger as click feedback gets noisier. The results also show that Self-Sparring is severely affected by the level of noise. This is because Self-Sparring estimates the Borda score [ 112 ] of a ranker and, in our experiments, the noisier click feedback is, the closer the Borda scores are to each other, making it harder for Self-Sparring to identify the winner. Results on the Yahoo dataset disagree with results on the MSLR dataset. On the Yahoo dataset, MergeDTS is affected more severely by the level of noise than Self-Sparring. This is because of the existence of uninformative rankers as stated in Assumption 2.1. In noisier conﬁgurations, the gaps between uninformative and informative rankers are smaller, which results in the long time of comparisons for MergeDTS to eliminate the uninformative rankers. Comparing those uninformative rankers leads to high regret. In summary, the performance of MergeDTS is largely affected when the gaps between rankers are small, which is consistent with our theoretical ﬁndings. <title>2.6.4 Cycle experiment</title> Figure 2.3: Cumulative regret in cyclic and non-Condorcet setups. The results are averaged over 100 independent runs. in online evaluation [ 128 ]. Therefore, in this section we assess how dueling bandit algorithms behave when a dueling bandit problem contains cycles. In this section we conduct experiments for 10 million steps and repeat 100 times since Merge-style algorithms converge to the Condorcet ranker within less than 1 million steps and running longer only increases the gaps between MergeDTS and baselines. For the Cycle dataset (the ﬁrst plot in Fig. 2.3), the cumulative regret of Self-Sparring is an order of magnitude higher than that of MergeDTS , although it performs well in some cases (see the above experiments). As we discussed in Section 2.5.2, Self-Sparring chooses rankers based on their Borda scores and when the Borda scores of different arms become close to each other as in the Cycle dataset, Self-Sparring may perform poorly. Also, we notice that when the gaps in Borda scores of the Condorcet winner and other rankers are large, Self-Sparring performs well, as shown in the middle plot in Fig. 2.3. Other than Self-Sparring, we also notice that the other baselines performs quite differently on the two cyclic conﬁgurations. In the harder conﬁguration of the two, the Cycle dataset, only RMED1 and MergeRUCB outperform MergeDTS RMED1 excludes rankers from consideration based on relative preferences between two rankers. And, in the Cycle dataset, the preferences between suboptimal rankers are large. Thus, RMED1 can easily exclude a ranker based on its relative comparison to another suboptimal ranker. For the Cycle2 dataset, where the relative preferences between two rankers are small, RMED1 performs worse than MergeDTS. MergeRUCB also slightly outperforms MergeDTS on the Cycle dataset. This can be explained as follows. In the Cycle dataset, the preference gap between the Condorcet winner and suboptimal rankers is small (i.e., 0.01 ), while the gaps between suboptimal rankers are relatively large (i.e., 1.0 ). Under this setup, MergeDTS tends to use the Condorcet winner to eliminate suboptimal rankers in the ﬁnal stage. On the other hand, MergeRUCB eliminates a ranker by another ranker who beats it with the largest probability. So, MergeDTS requires more comparisons to eliminate suboptimal rankers than MergeRUCB. However, the gap between MergeRUCB and MergeDTS is small. <title>2.6.5 Beyond the Condorcet assumption</title> To answer RQ1.5 , we evaluate MergeDTS on the MSLR-non-Condorcet dataset that does not contain a Condorcet winner. Instead, the dataset contains two Copeland winners and this dueling bandit setup is called the Copeland dueling bandit [ 115 129 ]. The Copeland winner is selected by the Copeland score 1(p > 1/2) that measures the number of rankers beaten by ranker . The Copeland winner is deﬁned as = max . In the MSLR-non-Condorcet dataset, each Copeland winner beats 30 other rankers. Speciﬁcally, one of the Copeland winners beats the other one but is beaten by a suboptimal ranker. In the Copeland dueling bandit setup, regret is computed differently from the Condorcet dueling bandit setup. Given a pair of rankers (a , a ), regret at step t is computed as: Among the considered algorithms, only DTS can solve the Copeland dueling bandit problem and is the state-of-the-art Copeland dueling bandit algorithm. We conduct the experiment for 10 million steps with which DTS converges to the Copeland winners. And we run each algorithm 100 times independently. The results are shown in the last plot in Fig. 2.3. MergeDTS has the lowest cumulative regret. However, in our experiments, we ﬁnd that MergeDTS eliminates the two Copeland winners one time out of 100 individual repeats. In the other 99 repeats, we ﬁnd that MergeDTS eliminates one of the two existing winners, which may not be ideal in practice. Note that we evaluate MergeDTS in a relatively easy setup, where only two Copeland winners are considered. For more complicated setups, where more than two Copeland winners are considered or the Copeland winners are beaten by several suboptimal rankers, we speculate that MergeDTS can fail more frequently. In our experiments, we do not evaluate MergeDTS in the more complicated setups, because MergeDTS is designed for the Condorcet dueling bandits and is only guaranteed to work under the Condorcet assumption. The answer to RQ1.5 is that MergeDTS may perform well for some easy setups that go beyond the Condorcet assumption without any guarantees. <title>2.6.6 Parameter sensitivity</title> Figure 2.4: Effect of the parameters , and on the performance of MergeDTS in the MSLR-Navigational setup. The results are averaged over 100 independent runs. The shaded areas are ± standard error. in practice we do not want to eliminate the best ranker, we choose α = 0.8 ≈ 0.2621 in our experiments. The middle plot in Fig. 2.4 shows the effect of the batch size . The larger the batch size, the lower the regret. This can be explained as follows. The DTS -based strategy uses the full local knowledge in a batch to choose the best ranker. A larger batch size provides more knowledge to MergeDTS to make decisions, which leads to a better choice of rankers. But the time complexity of MergeDTS is O(TM , i.e., quadratic in the batch size. Thus, for realistic scenarios we cannot increase indeﬁnitely. We choose M = 16 as a tradeoff between effectiveness (cumulative regret) and efﬁciency (running time). The right plot in Fig. 2.4 shows the dependency of MergeDTS on . Similarly to the effect of , lower values of lead to lower regret, but also to a larger number of failures. C = 4 000 000 is the lowest value that does not lead to any failures, so we choose it in our experiments. In summary, the theoretical constraints on the parameters of MergeDTS are rather conservative. There is a range of values for the key parameters and , where the theoretical guarantees fail to hold, but where MergeDTS performs better than it would if we were to constrain ourselves only to values permitted by theory. <title>2.7 Conclusion</title> fall within the theoretical regime. Several interesting directions for future work arise from this chapter: (1) In our experiments, we have shown that there is a large gap between theory and practice. It will be interesting to study this gap and provide a tighter theoretical bound. (2) We only study dueling bandits in this chapter. We believe that it is interesting to study a generalization of MergeDTS , as well as the theoretical analysis presented here, to the case of online ranker evaluation tasks with a multi-dueling setup. (3) Since multi-dueling bandits also compare multiple rankers at each step based on relative feedback, it is an interesting direction to compare dueling bandits to multi-dueling bandits in the large-scale setup. (4) We suspect that the UCB -based elimination utilized in MergeDTS is too conservative, it might be that more recent minimum empirical divergence based techniques [ 62 ] may be leveraged to speed up the elimination of the rankers. (5) The feature rankers are chosen as arms in our experiments. A more interesting and realistic way of choosing arms is to use well trained learning to rank algorithms. <title>Safe Online Learning to Re-Rank</title> In this chapter, we answer the following research question: RQ2 How to achieve safe online learning to re-rank? Particularly, we study the problem of safe online learning to re-rank, where user feedback is used to improve the quality of displayed lists. We propose BubbleRank , a bandit algorithm for safe re-ranking that combines the strengths of both the ofﬂine and online settings. The algorithm starts with an initial base list and improves it online by gradually exchanging higher-ranked less attractive items for lower-ranked more attractive items. We prove an upper bound on the -step regret of BubbleRank that degrades gracefully with the quality of the initial base list. Our theoretical ﬁndings are supported by extensive experiments on a large-scale real-world click dataset. <title>3.1 Introduction</title> items in the descending order of relevance is optimal in several click models and proposed BatchRank for learning it. Then Lattimore et al. [72] built upon this work and proposed TopRank , which is the state-of-the-art online LTR algorithm. Second, these algorithms lack safety constraints and explore aggressively by placing potentially irrelevant items at high positions, which may signiﬁcantly degrade user experience [ 114 ]. A third and related problem is that the algorithms are not well suited for so-called warm start scnearios [ 113 ], where the ofﬂine-trained production ranker already generates a good list, which only needs to be safely improved. Warm-starting an online LTR algorithm is challenging since existing posterior sampling algorithms, such as Thompson sampling [ 111 ], require item-level priors while only list-level priors are available practically. We make the following contributions. First, motivated by the exploration scheme of Radlinski and Joachims [98] , we propose a bandit algorithm for online LTR that addresses all three issues mentioned above. The proposed algorithm gradually improves upon an initial base list by exchanging higher-ranked less attractive items for lower-ranked more attractive items. The algorithm resembles bubble sort [ 26 ], and therefore we call it BubbleRank . Second, we prove an upper bound on the -step regret of BubbleRank . The bound reﬂects the behavior of BubbleRank : worse initial base lists lead to a higher regret. Third, we deﬁne our safety constraint, which is based on incorrectly-ordered item pairs in the ranked list, and prove that BubbleRank never violates this constraint with a high probability. Finally, we evaluate BubbleRank extensively on a large-scale real-world click dataset. <title>3.2 Background</title> This section introduces our online learning problem. We ﬁrst review click models [ 23 and then introduce a stochastic click bandit [ 132 ], a learning to rank framework for multiple click models. The following notation is used in the rest of the chapter. We denote {1, . . . , n} by [n] . For any sets and , we denote by the set of all vectors whose entries are indexed by and take values from . We use boldface letters to denote random variables. <title>3.2.1 Click models</title> A click model is a model of how a user clicks on a list of documents. We refer to the documents as items and denote the universe of all items by D = [L] . The user is presented a ranked list, an ordered list of documents out of . We denote this list by R ∈ Π (D) , where (D) is the set of all -tuples with distinct items from . We denote by R(k) the item at position in ; and by (i) the position of item in if item i is in R. Many click models are parameterized by item-dependent attraction probabilities α ∈ [0, 1] , where α(i) is the attraction probability of item . We discuss the two most fundamental click models below. when α(1) ≥ ··· ≥ α(L) . Therefore, perhaps not surprisingly, the problem of learning the optimal list in both models can be viewed as the same problem, a stochastic click bandit [132]. <title>3.2.2 Stochastic click bandit</title> An instance of a stochastic click bandit [ 132 ] is a tuple (K, L, P , P , where K ≤ L is the number of positions, is the number of items, is a distribution over binary attraction vectors {0, 1} , and is a distribution over binary examination matrices {0, 1} The learning agent interacts with the stochastic click bandit as follows. At time , it chooses a list ∈ Π (D) , which depends on its history up to time , and then observes clicks ∈ {0, 1} on all positions in . A position is clicked if and only if it is examined and the item at that position is attractive. More speciﬁcally, for any k ∈ [K], R ∈ Π (D) and position k ∈ [K], <title>3.3 Online Learning to Re-Rank</title> Multi-stage ranking is widely used in production ranking systems [ 22 57 83 ], with the re-ranking stage at the very end [ 22 ]. In the re-ranking stage, a relatively small number of items, typically 10 20 , are re-ranked. One reason for re-ranking is that ofﬂine rankers are typically trained to minimize the average loss across a large number of queries. Therefore, they perform well on very frequent queries and poorly on infrequent queries. On moderately frequent queries, the so-called torso queries, their performance varies. As torso queries are sufﬁciently frequent, an online algorithm can be used to re-rank so as to optimize their value, such as the number of clicks [131]. We propose an online algorithm that addresses the above problem and adaptively re-ranks a list of items generated by a production ranker with the goal of placing more attractive items at higher positions. We study a non-contextual variant of the problem, where we re-rank a small number of items in a single query. Generalization across queries and items is an interesting direction for future work. We follow the setting in Section 3.2.2, except that D = [K] . Despite these simplifying assumptions, our learning problem remains a challenge. In particular, the attraction of items is only observed through clicks in Eq. (3.2), which are affected by other items in the list. <title>3.3.1 Algorithm</title> Our algorithm is presented in Algorithm 4. The algorithm gradually improves upon an initial base list by “bubbling up” more attractive items. Therefore, we refer to it as BubbleRank BubbleRank determines more attractive items by randomly exchanging neighboring items. If the lower-ranked item is found to be more attractive, the items are permanently exchanged and never randomly exchanged again. If the lower-ranked item is found to be less attractive, the items are never randomly exchanged again. We describe BubbleRank in detail below. BubbleRank maintains a base list at each time . From the viewpoint of BubbleRank , this is the best list at time . The list is initialized by the initial base list (line 2). At time BubbleRank permutes into a displayed list (lines 5– 9). Two kinds of permutations are employed. If is odd and so h = 0 , the items at positions and and , and so on, are randomly exchanged. If is even and so h = 1 , the items at positions and and , and so on are randomly exchanged. The items are exchanged only if BubbleRank is uncertain regarding which item is more attractive (line 8). The list is displayed and BubbleRank gets feedback (line 10). Then it updates its statistics (lines 11–18). For any exchanged items and , if item is clicked and item is not, the belief that is more attractive than (i, j) , increases; and the belief that is more attractive than (j, i) , decreases. The number of observations, (i, j) and (j, i) , increases. These statistics are updated only if one of the items is clicked (line 14), not both. i (line 22), the items are permanently exchanged in the next base list A notable property of BubbleRank is that it explores safely, since any item in the displayed list is at most one position away from its position in the base list Moreover, any base list improves upon the initial base list , because it is obtained by bubbling up more attractive items with a high conﬁdence. We make this notion of safety more precise in Section 3.4.2. <title>3.4 Theoretical Analysis</title> In this section, we provide theoretical guarantees on the performance of BubbleRank by bounding the n-step regret in Eq. (3.4). The content is organized as follows. In Section 3.4.1, we present our upper bound on the -step regret of BubbleRank , together with our assumptions. In Section 3.4.2, we prove that BubbleRank is safe. In Section 3.4.3, we discuss our theoretical results. The regret bound is proved in Section 3.4.4. Our technical lemmas are stated and proved in Section 3.7. <title>3.4.1 Regret bound</title> Before we present our result, we introduce our assumptions and complexity metrics. Assumption 3.1. For any lists R, R ∈ Π (D) and positions k, ` ∈ [K] such that k < `: A1. r(R, α, χ) ≤ r(R , α, χ), where R is deﬁned in Eq. (3.1); A2. {R(1), . . . , R(k − 1)} = {R (1), . . . , R (k − 1)} =⇒ χ(R, k) = χ(R , k); A3. χ(R, k) ≥ χ(R, `); A5. χ(R, k) ≥ χ(R , k). The above assumptions hold in the CM. In the PBM, they hold when the examination probability decreases with the position. Our assumptions can be interpreted as follows. Assumption A1 says that the list of items in the descending order of attraction probabilities is optimal. Assumption A2 says that the examination probability of any position depends only on the identities of higher-ranked items. Assumption A3 says that a higher position is at least as examined as a lower position. Assumption A4 says that a higher-ranked item is less attractive if and only if it increases the examination of a lower position. Assumption A5 says that any position is examined the least in the optimal list. be the minimum gap. Then the -step regret of BubbleRank can be bounded as follows. <title>3.4.2 Safety</title> be the set of incorrectly-ordered item pairs in list . Then our algorithm is safe in the following sense. Proof. Our claim follows from two observations. First, by the design of BubbleRank any displayed list contains at most K/2 item pairs that are ordered differently from its base list . Second, no base list contains more incorrectly-ordered item pairs than with a high probability. In particular, under event in Lemma 3.8, any change in the base list (line 23 of BubbleRank ) reduces the number of incorrectly-order item pairs by one. In Lemma 3.8, we prove that P(E) ≥ 1 − δ n. <title>3.4.3 Discussion</title> base list . This suggests that BubbleRank should have lower regret when initialized with a better list of items. We validate this dependence empirically in Section 3.5. In many domains, such lists exist and are produced by existing ranking policies. They only need to be safely improved. Second, the bound is O(χ , where and are the maximum and minimum examination probabilities, respectively. In Section 3.5.4, we show that this dependence can be observed in problems where most attractive items are placed at infrequently examined positions. This limitation is intrinsic to BubbleRank , because attractive lower-ranked items cannot be placed at higher positions unless they are observed to be attractive at lower, potentially infrequently examined, positions. The safety constraint of BubbleRank is stated in Lemma 3.1. For δ = n , as discussed above, BubbleRank becomes a rather safe algorithm, and is unlikely to display any list with more than K/2 incorrectly-ordered item pairs than the initial base list . More precisely, |V(R )| ≤ |V | + K/2 holds uniformly over time with probability of at least 1 − K /n . This safety feature of BubbleRank is conﬁrmed by our experiments in Section 3.5.3. The above discussion assumes that the time horizon is known. However, in practice, this is not always possible. We can extend BubbleRank to the setting of an unknown time horizon by using the so-called doubling trick [ 18 , Section 2.3]. Let be the estimated horizon. Then at time n + 1 is set to and is doubled. The statistics do not need to be reset. BubbleRank is computationally efﬁcient. The time complexity of BubbleRank is linear in the number of time steps and in each step O(K) operations are required. In this chapter, we focus on re-ranking. But BubbleRank can be extended to the full ranking problem as follows. Deﬁne (i, j) and (i, j) for all item pairs (i, j) . For even (odd) at odd (even) time steps, select a random item below position that has not been shown to be worse than the item at position , and swap these items with probability 0.5 . The item that is not displayed gets feedback . The rest of BubbleRank remains the same. This algorithm can be analyzed in the same way as BubbleRank. <title>3.4.4 Proof of Theorem 3.1</title> Now note that for any randomized (i, j) ∈ P at time t, (α(i) − α(j)) ≤ E (R (i)) − c (R (j)) = E [s (i, j) − s (i, j)] , [s (i, j) − s (i, j)] , where the extra factor of two is because BubbleRank randomizes any pair of items (i, j) ∈ P at least once in any two consecutive steps. Moreover, for any i < j on event E, Finally, let P = . Then, on event |P| ≤ K − 1 + 2 |V . This follows from the design of BubbleRank (Lemma 3.5) and completes the proof. <title>3.5 Experimental Results</title> We conduct four experiments to evaluate BubbleRank . In Section 3.5.1, we describe our experimental setup. In Section 3.5.2, we report the regret of compared algorithms, which measures the rate of convergence to the optimal list in hindsight. In Section 3.5.3, we validate the safety of BubbleRank . In Section 3.5.4, we validate the tightness of the regret bound in Theorem 3.1. We report the Normalized Discounted Cumulative Gain (NDCG) of compared algorithms, which measures the quality of displayed lists, in Section 3.5.5. Figure 3.1: The -step regret of BubbleRank (red), CascadeKL-UCB (green), BatchRank (blue), TopRank (orange), and Baseline (grey) in the CM, DCM, and PBM in up to million steps. Lower is better. The results are averaged over all 100 queries and 10 runs per query. The shaded regions represent standard errors of our estimates. Figure 3.2: The -step violation of the safety constraint of BubbleRank by CascadeKL-UCB (green), BatchRank (blue), and TopRank (orange) in the CM, DCM, and PBM in up to million steps. Lower is better. The shaded regions represent standard errors of our estimates. <title>3.5.1 Experimental setup</title> We evaluate BubbleRank on the Yandex click dataset. The dataset contains user search sessions from the log of the Yandex search engine. It is the largest publicly available dataset containing user clicks, with more than 30 million search sessions. Each session contains at least one search query together with 10 ranked items. We preprocess the dataset as in [ 132 ]. In particular, we randomly select 100 frequent search queries, and then learn the parameters of three click models using the PyClick package: CM and PBM, described in Section 3.2.1, as well as the dependent click model (DCM) [36]. The DCM is an extension of the CM [ 27 ] where each position is associated with an abandonment probability v(k) . When the user clicks on an item at position , the user stops scanning the list with probability v(k) . Therefore, the DCM can model multiple clicks. Following the work in [ 58 ], we incorporate abandonment into our deﬁnition of reward for DCM and deﬁne it as the number of abandonment clicks. The abandonment click is a click after which a user stops browsing the list, and each time step contains at most one abandonment click. So, the expected reward for DCM equals the probability of abandonment clicks, which is computed as follows: is the examination probability of position in list . A high reward means a user stops the search session because of clicking on an item with high attraction probability. The learned CM, DCM, and PBM are used to simulate user click feedback. We experiment with multiple click models to show the robustness of BubbleRank to multiple models of user feedback. For each query, we choose 10 items. The number of positions is equal to the number of items, K = L = 10 . The objective of our re-ranking problem is to place most attractive items in descending order of attractiveness at the highest positions, as in 132 ]. The performance of BubbleRank and our baselines is also measured only at the top 5 positions. BubbleRank is compared to three baselines Cascade- KL-UCB 65 ], BatchRank 132 ], and TopRank 72 ]. The former is near optimal in the CM [ 65 ], but can have linear regret in other click models. Note that linear regret arises when CascadeKL-UCB erroneously converges to a suboptimal ranked list. BatchRank and TopRank can learn the optimal list in a wide range of click models, including the CM, DCM, and PBM. However, they can perform poorly in early stages of learning because they randomly shufﬂes displayed lists to average out the position bias. All experiments are run for million steps, after which at least two algorithms converge to the optimal ranked list. In the Yandex dataset, each query is associated with many different ranked lists, due to the presence of various personalization features of the production ranker. We take the most frequent ranked list for each query as the initial base list in BubbleRank since we assume that the most frequent ranked list is what the production ranker would produce in the absence of any personalization. We also compare BubbleRank to a production baseline, called Baseline, where the initial list R is applied for n steps. <title>3.5.2 Results with regret</title> In the ﬁrst experiment, we compare BubbleRank to CascadeKL-UCB BatchRank , and TopRank in the CM, DCM, and PBM of all 100 queries. Among them, TopRank is the state-of-the-art online LTR algorithm in multiple click models. We evaluate these algorithms by their cumulative regret, which is deﬁned in Eq. (3.4), at the top positions. The regret, a measure of convergence, is a widely-used metric in the bandit literature [ 58 65 132 ]. In the CM and PBM, the regret is the cumulative loss in clicks when a sequence of learned lists is compared to the optimal list in hindsight. In the DCM, the regret is the cumulative loss in abandonment clicks. We also report the regret of Baseline. Our results are reported in Fig. 3.1. We observe that the regret of Baseline grows linearly with time , which means that it is not optimal on average. CascadeKL-UCB learns quickly in both the CM and DCM, but has linear regret in the PBM. This is expected since CascadeKL-UCB is designed for the CM, and the DCM is an extension of the CM. As for the PBM, which is beyond the modeling assumptions of CascadeKL-UCB , there is no guarantee on the performance of CascadeKL-UCB BubbleRank BatchRank , and TopRank can learn in all three click models. Compared to BatchRank and TopRank BubbleRank has a higher regret in million steps. However, in earlier steps, BubbleRank has a lower regret than BatchRank and TopRank as it takes advantage of the initial base list . In general, these results show that BubbleRank converges to the optimal list slower than BatchRank and TopRank . This is expected because BubbleRank is designed to be a safe algorithm, and only learns better lists by exchanging neighboring items in the base list. <title>3.5.3 Safety results</title> Figure 3.3: Regret of BubbleRank as a function of the number of incorrectly-ordered item pairs |V , and the minimal examination probability . In the bottom-right plot, the purple, red, green, orange, and blue colors represent equals 0.5 0.5 0.5 0.5 , and 0.5 , respectively. The shaded regions represent standard errors of our estimates. the initial base list , and may hurt user experience, more than 20% of search sessions in the ﬁrst 100 steps. Even worse, the violations of CascadeKL-UCB grow linearly with time in the PBM. The safety issues of BatchRank , and TopRank are more severe than that of CascadeKL-UCB. More precisely, the violations of BatchRank in the ﬁrst 100 steps are 83.01 ± 0.56 71.89 ± 0.92 , and 59.63 ± 0.98 in the CM, DCM, and PBM, respectively. And the violations of TopRank are 83.56 ±0.70 71.47 ±1.07 , and 57.00 ± 1.24 in the CM, DCM, and PBM, respectively. Note that the performance of TopRank is close to that of BatchRank in the ﬁrst 100 steps since they both require the ranked lists to be randomly shufﬂed during the initial stages. Thus, BatchRank , and TopRank would frequently hurt the user experience during the early stages of learning. To conclude, BubbleRank learns without violating its safety constraint, while CascadeKL-UCB BatchRank , and TopRank violate the constraint frequently. Together with results in Section 3.5.2, BubbleRank is a safe algorithm but, to satisfy the safety constraint, it compromises the performance and learns slower than BatchRank and TopRank . In Section 3.5.5, we compare BubbleRank to baselines in NDCG and show that BubbleRank converges to the optimal lists in hindsight. <title>3.5.4 Sanity check on regret bound</title> We prove an upper bound on the -step regret of BubbleRank in Theorem 3.1. In comparison to the upper bounds of CascadeKL-UCB 65 ] and BatchRank 132 ], we have two new problem-speciﬁc constants: |V and 1/χ . In this section, we show that these constants are intrinsic to the behavior of BubbleRank. We ﬁrst study how the number of incorrectly-ordered item pairs in the initial base list |V , impacts the regret of BubbleRank . We choose 10 random initial base lists in each of our 100 queries and plot the regret of BubbleRank as a function of |V Our results are shown in Fig. 3.3. We observe that the regret of BubbleRank is linear in |V in the CM, DCM, and PBM. This is the same dependence as in our regret bound (Theorem 3.1). We then study the impact of the minimum examination probability on the regret of BubbleRank . We experiment with a synthetic PBM with 10 items, which is parameterized by α = (0.9, 0.5, . . . , 0.5) and χ = (0.9, . . . , 0.9, 0.5 , 0.5 for i ≥ 1 The most attractive item is placed at the last position in = (2, . . . , K − 1, 1) Since this position is examined with probability 0.5 , we expect the regret to double when increases by one. We experiment with i ∈ [5] in Fig. 3.3 and observe this trend in million steps. This conﬁrms that the dependence on 1/χ in Theorem 3.1 is generally unavoidable. <title>3.5.5 Results with NDCG</title> . As in Section 3.5.3, we observe that BubbleRank is safe, since its NDCG@ is never much worse than that of Baseline. Figure 3.4: The per-step NDCG@ of BubbleRank (red), BatchRank (blue), TopRank (orange), and Baseline (grey) in the CM, DCM, and PBM in up to million steps. Higher is better. The shaded regions represent standard errors of our estimates. <title>3.6 Related Work</title> items with little prior knowledge. In contrast, BubbleRank starts from the production ranked list and learns under the safety constraint. Thus, BubbleRank gets rid of these drawbacks. Another related line of work are conservative bandits [ 59 117 ]. In conservative bandits, the learned policy is safe in the sense that its expected cumulative reward is at least 1 − α fraction of that of the baseline policy with high probability. This notion of safety is less stringent than that in our work (Section 3.4.2). In particular, our notion of safety is per-step, in the sense that any displayed list is only slightly worse than the initial base list with a high probability. We do not compare to conservative bandits in our experiments because existing algorithms for conservative bandits require the action space to be small. The actions in our problem are ranked lists, and their number is exponential in K. <title>3.7 Lemmas</title> (χ(R , k)α(k) − χ(R, k)α(R(k))) ≤ Kχ ∆(R) . ≤ χ (α(k) − α(R(k))) , α(k) − α(R(k)) = α(R(π(k))) − α(R(k)) Iα(R(i + 1)) − α(R(i)) > 0α(R(i + 1)) − α(R(i))). α(k) − α(R(k)) ≤ α(R(j)) − α(R(k)) Iα(R(i + 1)) − α(R(i)) > 0(α(R(i + 1)) − α(R(i))) . From the deﬁnition of ∆(R) , this quantity is bounded from above by ∆(R) . This concludes the proof. be the set of potentially randomized item pairs at time and = max ∆(R be the maximum attraction gap of any list , where ∆(R is deﬁned in Eq. (3.8). Then on event E in Lemma 3.8, holds at any time t ∈ [n]. holds on event by the design of BubbleRank . More speciﬁcally, (min {i, j}, max {i, j}) /∈ implies that α(R (k + 1)) − α(R (k)) ≤ 0. Second, suppose that the permutation at time is such that and could be exchanged, j and j could be exchanged, or both. Then α(R (k + 1)) − α(R (k)) ≤ I(min {i, i }, max {i, i }) ∈ P (α(min {i, i }) − α(max {i, i })) + α(j) − α(i) + holds on event by the design of BubbleRank . Therefore, for any position k ∈ [K −1] in both above cases, α(R (k + 1)) − α(R (k)) ≤ o min (`), (` + 1) , max (`), (` + 1) ∈ P o o min (`), (` + 1) − α max (`), (` + 1) Now we sum over all positions and note that each pair of (`) and (` + 1) appears on the right-hand side at most three times, in any list . This concludes our proof. Lemma 3.4. Let P be deﬁned as in Lemma 3.3. Then on event E in Lemma 3.8, holds at any time t ∈ [n]. Proof. A direct consequence of Lemmas 3.2 and 3.3. Lemma 3.5. Let be deﬁned as in Lemma 3.3, P = , and be deﬁned as in Eq. (3.6). Then on event E in Lemma 3.8, |P| ≤ K − 1 + 2 |V | . Proof. From the design of BubbleRank |P | = K − 1 . The set of randomized item pairs grows only if the base list in BubbleRank changes. When this happens, the number of incorrectly-ordered item pairs decreases by one, on event , and the set of randomized item pairs increases by at most two pairs. This event occurs at most |V times. This concludes our proof. Lemma 3.6. For any items i and j such that i < j, on event E in Lemma 3.8. Proof. To simplify notation, let = s (i, j) and = n (i, j) . The proof has two parts. First, suppose that ≤ 2 log(1/δ) holds at all times t ∈ [n] . Then from this assumption and on event E in Lemma 3.8, at any time t. Our claim follows from setting t = n. Now suppose that ≤ 2 log(1/δ) does not hold at all times t ∈ [n] . Let be the ﬁrst time when > 2 log(1/δ) . Then from the deﬁnition of and on event in Lemma 3.8, ≤ 3 log(1/δ) , Now note that = s for any t > τ , from the design of BubbleRank . This concludes our proof. For some = σ(R , c , . . . , R , c -measurable event , let (A) = P(A | F be the conditional probability of given history , c , . . . , R , c . Let the corresponding conditional expectation operator be E [·]. Note that is F -measurable. Lemma 3.7. Let i, j ∈ [K] be any items at consecutive positions in and z = c (R (i)) − c (R (j)) . Then, on the event that i and j are subject to randomization at time t, when α(i) > α(j), and E [−z | z 6= 0] ≤ 0 when α(i) < α(j). Proof. The ﬁrst claim is proved as follows. From the deﬁnition of expectation and z ∈ {−1, 0, 1}, [z] = χ α(i) − χ α(j) ≥ χ (α(i) − α(j)) , ≤ χ (α(i) + α(j)) , where the ﬁrst inequality is from inequalities (z = 1) ≤ χ α(i) and (z = −1) ≤ χ α(j), and the last inequality is from χ ≥ χ Finally, we chain all above inequalities and get our ﬁrst claim. The second claim follows from the observation that E [−z | z 6= 0] = −E [z | z 6= 0]. Let E = (E ∩ E ) and E be the complement of E. Then P(E) ≤ δ n. The above inequality holds for any (n (i, j)) , and therefore also in expectation over (n (i, j)) . From the deﬁnition of and the union bound, we have P(E ) ≤ K(K − 1). The claim that P(E ) ≤ K(K − 1) is proved similarly, except that we use 6= 0 ≤ 0. From the deﬁnition of E and the union bound, P(E) ≤ P(E ) + P(E ) ≤ δ n . This completes our proof. <title>3.8 Conclusions</title> BubbleRank to the general ranking setup in Section 3.4.3, we expect further experiments to validate this approach. Our general topic of interest are exploration schemes that are more conservative than those of existing online LTR methods. Existing methods are not very practical because they can explore highly irrelevant items at frequently examined positions. <title>Cascade Non-Stationary Bandits</title> This chapter provides an answer to RQ3: RQ3 How to conduct online learning to rank when users change their preferences constantly? We study the online learning to rank problem in a non-stationary environment where user preferences change abruptly at an unknown moment in time. We consider the problem of identifying the most attractive items and propose cascading non-stationary bandits, an online learning variant of the cascading model, where a user browses a ranked list from top to bottom and clicks on the ﬁrst attractive item. We propose two algorithms for solving this non-stationary problem: CascadeDUCB and CascadeSWUCB . We analyze their performance and derive gap-dependent upper bounds on the -step regret of these algorithms. We also establish a lower bound on the regret for cascading non-stationary bandits and show that both algorithms match the lower bound up to a logarithmic factor. Finally, we evaluate their performance on a real-world web search click dataset. <title>4.1 Introduction</title> the exploration vs. exploitation dilemma in an elegant way and aim to maximize user satisfaction in a stationary environment where users do not change their preferences over time. Moreover, they often come with regret bounds. Despite the success of the algorithms mentioned above in the stationary case, they may have linear regret in a non-stationary environment where users may change their preferences abruptly at any unknown moment in time. Non-stationarity widely exists in real-world application domains, such as search engines and recommender systems [ 49 92 116 119 ]. Particularly, we consider abruptly changing environments where user preferences remain constant in certain time periods, named epochs, but change occurs abruptly at unknown moments called breakpoints. The abrupt changes in user preferences give rise to a new challenge of balancing “remembering” and “forgetting” [ 11 ]: the more past observations an algorithm retains the higher the risk of making a biased estimator, while the fewer observations retained the higher stochastic error it has on the estimates of the user preferences. In this chapter, we propose cascading non-stationary bandits, an online variant of the Cascade Model ( CM ) [ 27 ] with the goal of identifying the most attractive items in a non-stationary environment. CM is a widely-used model of user click behavior [ 23 132 ]. In CM , a user browses the ranked list from top to bottom and clicks the ﬁrst attractive item. The items ranked above the ﬁrst clicked item are browsed but not attractive since they are not clicked. The items ranked below the ﬁrst clicked item are not browsed since the user stops browsing the ranked list after a click. Although CM is a simple model, it effectively explains user behavior [65]. Our key technical contributions in this chapter are: (1) We formalize a non-stationary Online Learning to Rank ( OLTR ) problem as cascading non-stationary bandits. (2) We propose two algorithms, CascadeDUCB and CascadeSWUCB , for solving it. They are motivated by discounted UCB ( DUCB ) and sliding window UCB ( SWUCB ), respectively [ 31 ]. CascadeDUCB balances “remembering” and “forgetting” by using a discounting factor of past observations, and CascadeSWUCB balances the two by using statistics inside a ﬁxed-size sliding window. (3) We derive gap-dependent upper bounds on the regret of the proposed algorithms. (4) We derive a lower bound on the regret of cascading non-stationary bandits. We show that the upper bounds match this lower bound up to a logarithmic factor. (5) We evaluate the performance of CascadeSWUCB and CascadeDUCB empirically on a real-world web search click dataset. <title>4.2 Background</title> <title>4.2.1 Cascade model</title> We refer readers to [ 23 ] for an introduction to click models. Brieﬂy, a click model models a user’s interaction behavior with the search system. The user is presented with a -item ranked list . Then the user browses the list and clicks items that potentially attract him or her. Many click models have been proposed and each models a certain aspect of interaction behavior. We can parameterize a click model by attraction probabilities α ∈ [0, 1] and a click model assumes: Assumption 4.1. The attraction probability α(a) only depends on item and is independent of other items. CM is a widely-used click model [ 27 132 ]. In the CM , a user browses the ranked list from the ﬁrst item R(1) to the last item R(K) , which is called the cascading assumption. After the user browses an item R(i) , he or she clicks on R(i) with attraction probability α(R(i)) , and then stops browsing the remaining items. Thus, the examination probability of item R(j) equals the probability of no click on the higher ranked items: (1 − α(R(i))) . The expected number of clicks equals the probability of clicking any item in the list: 1 − (1 − α(R(i))) . Note that the reward does not depend on the order in , and thus, in the CM , the goal of ranking is to ﬁnd the K most attractive items. The CM accepts at most one click in each search session. It cannot explain scenarios where a user may click multiple items. The CM has been extended in different ways to capture multi-click cases [ 20 36 ]. Nevertheless, CM is still the fundamental click model and ﬁts historical click data reasonably well. Thus, in this chapter, we focus on the CM and in the next section we introduce an online variant of CM , called cascading bandits. <title>4.2.2 Cascading bandits</title> observed. The reward at time t is deﬁned by the number of clicks: Under Assumption 4.1, the attraction indicators of each item in are independently distributed. Moreover, cascading bandits make another assumption. Assumption 4.2. The attraction indicators are distributed as: where P is a Bernoulli distribution with a mean of α(a). Under Assumption 4.1 and Assumption 4.2, the attraction indicator of item a at time t (a) is drawn independently from other items. Thus, the expectation of reward of the ranked list at time can be computed as E [r(R , A ] = r(R , α) . And the goal of the agent is to maximize the expected number of clicks in steps, which is equivalent to minimizing the n-step cumulative regret: Cascading bandits are designed for a stationary environment, where the attraction probability remains constant. However, in real-world applications, users change their preferences constantly [ 49 ], which is called a non-stationary environment, and learning algorithms proposed for cascading bandits, e.g., CascadeKL-UCB and CascadeUCB1 65 ], may have linear regret in this setting. In the next section, we propose cascading nonstationary bandits, the ﬁrst non-stationary variant of cascading bandits, and then propose two algorithms for solving this problem. <title>4.3 Cascading Non-Stationary Bandits</title> We ﬁrst deﬁne our non-stationary online learning setup, and then we propose two algorithms learning in this setup. <title>4.3.1 Problem setup</title> The learning problem we study is called cascading non-stationary bandits, a variant of CB . We deﬁne it by a tuple B = (D, P, K, Υ , where D = [L] and K ≤ L are the same as in CB bandits, P ∈ {0, 1} is a distribution over binary attractions and is the number of abrupt changes in up to step . We use (R (i)) to indicate the attraction probability distribution of item (i) at time . If = 0 , this setup is same as CB . The difference is that we consider a non-stationary learning setup in which > 0 and the non-stationarity in attraction probabilities characterizes our learning problem. In this chapter, we consider an abruptly changing environment, where the attraction probability remains constant within an epoch but can change at any unknown moment in time and the number of abrupt changes up to steps is . The learning agent interacts with cascading non-stationary bandits in the same way as with CB. Since the agent is in a non-stationary environment, we write for the mean of the attraction probabilities at time and we evaluate the agent by the expected cumulated regret expressed as: The goal of the agent it to minimizing the n-step regret. <title>4.3.2 Algorithms</title> one is inspired by SWUCB 31 ]. We summarize the pseudocode of both algorithms in Algorithm 5. CascadeDUCB and CascadeSWUCB learn in a similar pattern. They differ in the way they estimate the Upper Conﬁdence Bound ( UCB (R (i)) of the attraction probability of item (i) as time , as discussed later in this section. After estimating the UCBs (line 8), both algorithms construct by including the top most relevant items by UCB. Since the order of top items only affects the observation but does not affect the payoff of R , we construct R as follows: After receiving the user’s click feedback , both algorithms update their statistics (lines 13–24). We use (i) and (i) to indicate the number of items that have been observed and clicked up to t step, respectively. To tackle the challenge of non-stationarity, CascadeDUCB penalizes old observations with a discount factor γ ∈ (0, 1) . Speciﬁcally, each of the previous statistics is discounted by γ (lines 15–16). The UCB of item a is estimated as: is the conﬁdence interval, and t ∧ τ = min(t, τ). Initialization. In the initialization phase, we set all the statistics to and deﬁne := 1 for any (lines 3–4). Mapping back this to UCB, at the beginning, each item has the optimal assumption on the attraction probability with an optimal bonus on uncertainty. This is a common initialization strategy for UCB-type bandit algorithms [79]. <title>4.4 Analysis</title> In this section, we analyze the -step regret of CascadeDUCB and CascadeSWUCB We ﬁrst derive regret upper bounds on CascadeDUCB and CascadeSWUCB , respectively. Then we derive a regret lower bound on cascading non-stationary bandits. Finally, we discuss our theoretical results. <title>4.4.1 Regret upper bound</title> We refer to ⊆ [L] as the set of the most attractive items in set at time and as the complement of , i.e., ∀a ∈ D , ∀a : α (a) ≥ α (a and = D , D = ∅ . At time , we say an item is optimal if ∈ D and an item is suboptimal if a ∈ . The regret at time is caused by the case that includes at least one suboptimal and examined items. Let be the gap of attraction probability between a suboptimal item and an optimal at time = α (a ) − α (a) . Then we refer to as the smallest gap of between item and the -th most attractive item in all steps when is not the optimal items: = min (a ) − α (a). Theorem 4.1. Let  ∈ (1/2, 1) and γ ∈ (1/2, 1) , the expected -step regret of CascadeDUCB is bounded as: We outline the proof in 4 steps below; the full version is in Section 4.7.1. Proof. Our proof is adapted from the analysis in [65]. The novelty of the proof comes from the fact that, in a non-stationary environment, the discounted estimator (γ, a) is now a biased estimator of α (a) (Step 1, 2 and 4). Step 1. We bound the regret of the event that estimators of the attraction probabilities are biased by LΥ . This event happens during the steps following a breakpoint. Step 2. We bound the regret of the event that (a) falls outside of the conﬁdence interval around (γ, a) by ln (1 + 4 1 − 1/2)n(1 − γ) ln Step 3. We decompose the regret at time t based on [65, Theorem 1]. Step 4. For each item , we bound the number of times that item is chosen when a ∈ in steps and get the term . Finally, we sum up all the regret. The bound depends on step and the number of breakpoints . If they are known beforehand, we can choose by minimizing the right hand side of Eq. (4.10). Choosing γ = 1 − 1/4 (Υ /n) leads to R(n) = O( nΥ ln n) . When is independent of n, we have R(n) = O( nΥ ln n). Theorem 4.2. Let  ∈ (1/2, 1) . For any integer , the expected -step regret of CascadeSWUCB is bounded as: When τ goes to inﬁnity and n/τ goes to 0, We outline the proof in 4 steps below and the full version is in Section 4.7.2. Step 3. We decompose the regret at time t based on [65, Theorem 1]. Step 4. For each item , we bound the number of times that item is chosen when a ∈ in n steps and get the term e. Finally, we sum up all the regret. If we know and beforehand, we can choose the window size by minimizing the right hand side of Eq. (4.12). Choosing τ = 2 n ln(n)/Υ leads to R(n) = O( nΥ ln n). When Υ is independent of n, we have R(n) = O( nΥ ln n). <title>4.4.2 Regret lower bound</title> We consider a particular cascading non-stationary bandit and refer to it as (L, K, ∆, p, Υ) . We have a set of items D = [L] and K = positions. At any time t, the distribution of attraction probability of each item a ∈ D is parameterized by: p if a ∈ D (a) = (4.16) p − ∆ if a ∈ where is the set of optimal items at time is the set suboptimal items at time , and ∆ ∈ (0, p] is the gap between optimal items and suboptimal items. Thus, the attraction probabilities only take two values: for optimal items and p − ∆ for suboptimal items up to -step. is the number of breakpoints when the attraction probability of an item changes from to p − ∆ or other way around. Particularly, we consider a simple variant that the distribution of attraction probability of each item is piecewise constant and has two breakpoints. And we assume another constraint on the number of optimal items that |D | = K for all time steps t ∈ [n] . Then, the regret that any learning policy can achieve when interacting with is lower bounded by Theorem 4.3. Theorem 4.3. The n-step regret of any learning algorithm interacting with cascading non-stationary bandit B is lower bounded as follows: where (p − ∆||p) is the Kullback-Leibler (KL) divergence between two Bernoulli distributions with means p − ∆ and p. Proof. The proof is based on the analysis in [ 65 ]. We ﬁrst refer to as the optimal list at time that includes items. For any time step , any item a ∈ and any item ∈ D , we deﬁne the event that item is included in instead of item and item a is examined but not clicked at time step t by: period after a breakpoint. And we get: We sum up all the inequalities and obtain: <title>4.4.3 Discussion</title> We have shown that the -step regret upper bounds of CascadeDUCB and CascadeSWUCB have the order of O( n ln n) and O( n ln n) , respectively. They match the lower bound proposed in Theorem 4.3 up to a logarithmic factor. Speciﬁcally, the upper bound of CascadeDUCB matches the lower bound up to ln n . The upper bound of CascadeSWUCB matches the lower bound up to ln n , an improvement over CascadeDUCB, as conﬁrmed by experiments in Section 4.5. We have assumed that step is know beforehand. This may not always be the case. We can extend CascadeDUCB and CascadeSWUCB to the case where is unknown by using the doubling trick [ 31 ]. Namely, for t > n and any , such that ≤ t < 2 we reset γ = 1 − and τ = 2 ln(2 ). <title>4.5 Experimental Analysis</title> Figure 4.1: The n-step regret in up to 100 k steps. Lower is better. The results are averaged over all 100 queries and 10 runs per query. The shaded regions represent standard errors of our estimates. rank items based on all historical observations, i.e., they do not balance “remembering” and “forgetting.” Because of the use of a discounting factor and/or a sliding window, CascadeDUCB and CascadeSWUCB can detect breakpoints and show convergence. CascadeSWUCB outperforms CascadeDUCB with a small gap; this is consistent with our theoretical ﬁnding that CascadeSWUCB outperforms CascadeDUCB by a ln n factor. <title>4.6 Related Work</title> The idea of directly learning to rank from user feedback has been widely studied in a stationary environment. Ranked bandits [ 99 ] are among the earliest OLTR approaches. In ranked bandits, each position in the list is modeled as an individual underlying MAB s. The ranking task is then solved by asking each individual MAB to recommend an item to the attached position. Since the reward, e.g., click, of a lower position is affected by higher positions, the underlying MAB is typically adversarial, e.g., EXP3 ]. BatchRank is a recently proposed OLTR method [ 132 ]; it is an eliminationbased algorithm: once an item is found to be inferior to items, it will be removed from future consideration. BatchRank outperforms ranked bandits in the stationary environment. In our experiments, we include BatchRank and RankedEXP3 , the EXP3 based ranked bandit algorithm, as baselines. Several OLTR algorithms have been proposed in speciﬁc click models [ 58 65 70 ]. They can efﬁciently learn an optimal ranking given the click model they consider. Our work is related to cascading bandits and we compare our algorithms to CascadeKL-UCB an algorithm proposed for soling cascading bandits [ 65 ], in Section 4.5. Our work differs from cascading bandits in that we consider learning in a non-stationary environment. Non-stationary bandit problems have been widely studied [ 11 31 82 105 119 ]. However, previous work requires a small action space. In our setup, actions are (exponentially many) ranked lists. Thus, we do not consider them as baselines in our experiments. In adversarial bandits the reward realizations, in our case attraction indicators, are selected by an adversary. Adversarial bandits originate from game theory [ 12 ] and have been widely studied, cf. [ 18 ] for an overview. Within adversarial bandits, the performance of a policy is often measured by comparing to a static oracle which always chooses a single best arm that is obtained after seeing all the reward realizations up to step . This static oracle can perform poorly in a non-stationary case when the single best arm is suboptimal for a long time between two breakpoints. Thus, even if a policy performs closely to the static oracle, it can still perform sub-optimally in a non-stationary environment. Our work differs from adversarial bandits in that we compare to a dynamic oracle that can balance the dilemma of “remembering” and “forgetting” and chooses the per-step best action. <title>4.7 Proofs</title> set of the most attractive items in set at time and as the complement of i.e. ∀a ∈ D , ∀a ∈ : α (a ) ≥ α (a) and = D, D = ∅ . At time , we say an item is optimal if ∈ D and an item is suboptimal if a ∈ We denote = max r(R, α ) − r(R , A to be the regret at time of the learning algorithm. Let be the gap of attraction probability between a suboptimal item and an optimal at time = α (a ) − α (a) . Then we refer to as the smallest gap between item and the -th most attractive item in all time steps when a is not the optimal item: ∆ = min (K) − α (a). <title>4.7.1 Proof of Theorem 4.1</title> Let = {∃a ∈ D ∼ s.t. ∼ |α (a) − (γ, a)| > c (γ, t)} be the event that (a) falls out of the conﬁdence interval around (γ, a) at time t, and be the complement of M . We re-write the n-step regret of CascadeDUCB as follows: We then bound both terms above separately. We refer to T as the set of all time steps such that for t ∈ T , s ∈ [t − B(γ), t] and any item a ∈ D we have (a) = α (a) , where B(γ) = dlog ((1 − γ))e . In other words, is the set of time steps that do not follow too close after breakpoints. Since for any time step t /∈ T the estimators of attraction probabilities are biased, CascadeDUCB may recommend suboptimal items constantly. Thus, we get the following bound: Then, we show that for steps t ∈ T , the attraction probabilities are well estimated for all items with high probability. For an item , we consider the bias and variance of (γ, a) separately. We denote: (γ, a) = 1{a ∈ R , R (c ) = a}, N (γ, a) = 1{a ∈ R }, where the third inequality is due to the fact that 1 ∧ x ≤ and the last inequality is due to the deﬁnition of B(γ) . So, B(γ) time steps after a breakpoint, the “bias” is at most half as large as the conﬁdence bonus; and the second half of the conﬁdence interval is used for the variance. Second, we consider the variance. For a ∈ D and t ∈ T , let = {|α (a) − (γ, a)| > c (γ, t)} be the event that (a) falls out of the conﬁdence interval around (γ, a) at time . By using a Hoeffding-type inequality [ 31 , Theorem 4], for an item a ∈ D, t ∈ T , and any η > 0, we get: Thus, we get the following bound: ln N (γ) 1{M }r ≤ 2L (γ) ln (1 + η) By taking η = 4 1 − 1/2 such that 1 − = 1, and with t = (1 − γ) we get: We sum up and get the upper bound: Third, we upper bound the second term in Eq. (4.22). The regret is caused by recommending a suboptimal item to the user and the user examines but does not click the item. Since there are breakpoints, we refer to [t , . . . , t as the time step of a breakpoint that occurs. We consider the time step in the individual epoch that does not contain a breakpoint. For any epoch and any time t ∈ {t , t + 1, . . . , t − 1} , any item a ∈ and any item ∈ D , we deﬁne the event that item is included in instead of item a , and item a is examined but not clicked at time t by: = {∃1 ≤ k < c ∼ s.t. ∼ R (k) = a, R (k) = a }. Since the attraction probability remains constant in the epoch, we refer to as the optimal items and as the suboptimal items in epoch . By [ 65 , Theorem 1], the regret at time t is decomposed as: Then we have: 1{ }r 1{ }E [r ] ≤ 1{G (4.26) where the ﬁrst equality is due to the tower rule, and the inequality is due to Eq. (4.25). Now, for any suboptimal item in epoch , we upper bound 1{G . At time , event 1{ and event 1{a ∈ , a ∈ } happen when there exists an optimal item a ∈ D such that: (a) + 2c (γ, a) ≥ U (a) ≥ U(a ) ≥ α (a ), which implies that 2c (γ, a) ≥ α (a ) − α (a) . Taking the deﬁnition of the conﬁdence interval, we get: 16 ln N (γ) (γ, a) ≤ 16 ln N (γ) !# where the last inequality is due to [ 64 , Lemma 3]. Let = min be the smallest gap between the suboptimal item and an optimal item in all time steps. When (γ, a) > , CascadeDUCB will not select item a at time t. Thus we get: where the last inequality is based on [31, Lemma 1]. Finally, together with Eqs. (4.22) to (4.26) and (4.28), we get Theorem 4.1. <title>4.7.2 Proof of Theorem 4.2</title> Let = {∃a ∈ D : |α (a) − (τ, a)| > c (τ, t)} be the event that (a) falls out of the conﬁdence interval around (τ, a) at time t, and let be the complement of We re-write the n-step regret of CascadeSWUCB as follows: We then bound both terms in Eq. (4.29) separately. First, we refer to as the set of all time steps such that for t ∈ T s ∈ [t − τ, t] and any item a ∈ D we have (a) = α (a) . In other words, is the set of time steps that do not follow too close after breakpoints. Obviously, for any time step t /∈ T the estimators of attraction probabilities are biased and CascadeSWUCB may recommend suboptimal items constantly. Thus, we get the following bound: time steps after a breakpoint, the estimators of the attraction probabilities are not biased. 21], for an item a ∈ D, t ∈ T , and any η > 0, we get: We sum up and get the upper bound: Third, we upper bound the second term in Eq. (4.29). The regret is caused by recommending a suboptimal item to the user and the user examines but does not click the item. Since there are breakpoints, we refer to [t , . . . , t as the time steps of a breakpoint. We consider the time step in the individual epoch that does not contain a breakpoint. For any epoch and any time t ∈ {t , t + 1, . . . , t − 1} , any item a ∈ and any item ∈ D , we deﬁne the event that item is included in instead of item a and item a is examined but not clicked at time t by: = {∃1 ≤ k < c ∼ s.t. ∼ R (k) = a, R (k) = a }. Since the attraction probabilities remain constant in the epoch, we refer to as the optimal items and as the suboptimal items in epoch . By [ 65 , Theorem 1], the regret at time t is decomposed as: where the ﬁrst equality is due to the tower rule, and the inequality if due to Eq. (4.32). At time , event 1{ and event 1{a ∈ R , a ∈ happen when there exists an optimal item a ∈ D such that: (a) + 2c (τ, a) ≥ U (a) ≥ U(a ) ≥ α (a ), which implies that 2c (τ, a) ≥ α (a ) − α (a) . Taking the deﬁnition of the conﬁdence interval, we get: 4 ln N (τ) (τ, a) ≤ 4 ln N (γ) !# 4.8. Additional Experiments (τ, a) > , CascadeDUCB will not select item a at time t. Thus we get: where the last inequality is based on [30, Lemma 25]. Finally, together with Eqs. (4.29) to (4.31), (4.33) and (4.35), we get Theorem 4.2. <title>4.8 Additional Experiments</title> In this section, we compare CascadeDUCB CascadeSWUCB and baselines on single queries. We pick 20 queries and report the results in Fig. 4.2. The results exemplify that CascadeDUCB and CascadeSWUCB have sub-linear regret while other baselines have linear regret. <title>4.9 Conclusion</title> In this chapter, we have answered RQ3 by studying the Online Learning to Rank ( OLTR problem in a non-stationary environment where user preferences change abruptly. We focus on a widely-used user click behavior model Cascade Model ( CM ) and have proposed an online learning variant of it called cascading non-stationary bandtis. Two algorithms, CascadeDUCB and CascadeSWUCB , have been proposed for solving it. Our theoretical have shown that they have sub-linear regret. These theoretical ﬁndings have been conﬁrmed by our experiments on the Yandex click dataset. We open several future directions for non-stationary OLTR. First, we have only considered the CM setup. Although a CM is powerful in explanaining user behavior, it can only learn up to the ﬁrst click, which may ignore part of user feedback. Other click models that can handle multiple clicks such as DCM [ 36 ] and DBN [ 20 ] may be considered as part of future work. We believe that our analysis can be adapted to those cases easily. Second, we focus on UCB1 -based policy in building rankings. Another possibility is to use the family of softmax policies, which has original been designed for adversarial bandits [ 10 11 ]. Along this line, one may obtain upper bounds independent from the number of breakpoints. Figure 4.2: The n-step regret of CascadeDUCB (black), CascadeSWUCB (red), RankedEXP3 (cyan), CascadeKL-UCB (green) and BubbleRank (blue) on single queries in up to 100 k steps. Lower is better. The x-axis is step . The results are averaged over 10 runs per query. The shaded regions represent standard errors of our estimates. <title>Online Learning to Rank for Relevance and Diversity</title> This chapter is devoted to answering the following question: RQ4 How to learn a ranker online considering both relevance and diversity? We study an online learning setting that aims to recommend a ranked list with items that maximizes the ranking utility, i.e., a list whose items are relevant and whose topics are diverse. We formulate it as the cascade hybrid bandits ( CHB ) problem. CHB assumes a cascading user behavior, where a user browses the displayed list from top to bottom, clicks the ﬁrst attractive item, and stops browsing the rest. We propose a hybrid contextual bandit approach, called CascadeHybrid , for solving this problem. CascadeHybrid models item relevance and topical diversity using two independent functions and simultaneously learns those functions from user click feedback. We conduct experiments to evaluate CascadeHybrid on two real-world recommendation datasets: MovieLens and Yahoo music datasets. Our experimental results show that CascadeHybrid outperforms the baselines. In addition, we prove theoretical guarantees on the n-step performance demonstrating the soundness of CascadeHybrid. <title>5.1 Introduction</title> lower ranked items); (2) The learning algorithm should infer item relevance from user feedback and recommend lists containing relevant items (relevance ranking); (3) The recommended list should contain no redundant items and cover a broad range of topics (result diversiﬁcation). To address the position bias, a common approach is to make assumptions on the user’s click behavior and model the behavior using a click model [ 23 ]. The cascade model ( CM ) [ 27 ] is a simple but effective click model to explain user behavior. It makes the so-called cascade assumption, which assumes that a user browses the list from the ﬁrst ranked item to the last one and clicks on the ﬁrst attractive item and then stops browsing. The clicked item is considered to be positive, items before the click are treated as negative and items after the click will be ignored. Previous work has shown that the cascade assumption can explain the position bias effectively and several algorithms have been proposed under this assumption [38, 65, 74, 133]. In online LTR , the implicit signal that is inferred from user interactions is noisy [ 40 ]. If the learning algorithm only learns from these signals, it may reach a suboptimal solution where the optimal ranking is ignored simply because it is never exposed to users. This problem can be tackled by exploring new solutions, where the learning algorithm displays some potentially “good” rankings to users and obtains more signals. This behavior is called exploration. However, exploration may hurt the user experience. Thus, learning algorithms face an exploration vs. exploitation dilemma. Multi-armed bandit ( MAB ) [ 71 ] algorithms are commonly used to address this dilemma. Along this line, multiple algorithms have been proposed [ 42 77 80 ]. They all address the dilemma in elegant ways and aim at recommending the topmost relevant items to users. However, only recommending the most relevant items may result in a list with redundant items, which diminishes the utility of the list and decreases user satisfaction [ 120]. The submodular coverage model [ 91 ] can capture the pattern of diminishing utility and has been used in online LTR for diversiﬁed ranking. One assumption in this line of work is that items can be represented by a set of topics. The task, then, is to recommend a list of items that ensures a maximal coverage of topics. Yue and Guestrin [120] develop an online feature-based diverse LTR algorithm by optimizing submodular utility models [ 120 ]. Hiranandani et al. [38] improve online diverse LTR by bringing the cascading assumption into the objective function. However, we argue that not all features that are used in a LTR setting can be represented by topics [ 84 ]. Previous online diverse LTR algorithms tend to ignore the relevance of individual items and may recommend a diversiﬁed list with less relevant items. In this chapter, we address the aforementioned challenges and make four contributions: We focus on a novel online LTR setting that targets both relevance ranking (1) and result diversiﬁcation. We formulate it as a Cascade Hybrid Bandits ( CHB problem, where the goal is to select items from a large candidate set that maximize the utility of the ranked list (Section 5.3.1). (2) We propose CascadeHybrid , which utilizes a hybrid model, to solve this problem (Section 5.3.3). (3) We evaluate CascadeHybrid on two real-world recommendation datasets: MovieLens and Yahoo and show that CascadeHybrid outperforms state-of-the-art baselines (Section 5.4). (4) We theoretically analyze the performance of CascadeHybrid and provide guarantees on its proper behavior; moreover, we are the ﬁrst to show that the regret bounds on feature-based ranking algorithms with the cascade assumption are linear in K. The rest of the chapter is organized as follows. We recapitulate the background knowledge in Section 5.2. In Section 5.3, we formulate the learning problem and propose our CascadeHybrid algorithm that optimizes both item relevance and list diversity. Section 5.4 contains our empirical evaluations of CascadeHybrid , comparing it with several state-of-the-art baselines. An analysis of the upper bound on the -step performance of CascadeHybrid is presented in Section 5.5. In Section 5.6, we review related work. Conclusions are formulated in Section 5.7. <title>5.2 Background</title> In this section, we recapitulate the Cascade Model ( CM ), Cascading Bandits ( CB ), and the submodular coverage model. Throughout the chapter, we consider the ranking problem of candidate items and positions with K ≤ L . We denote {1, . . . , n} by [n] and for the collection of items we write D = [L] . A ranked list contains K ≤ L items and is denoted by R ∈ Π (D) , where (D) is the set of all permutations of distinct items from the collection . The item at the -th position of the list is denoted by R(k) and, if contains an item , the position of this item in is denoted by (i) . All vectors are column vectors. We use bold font to indicate a vector and bold font with a capital letter to indicate a matrix. We write to denote the d × d identity matrix and 0 the d × m zero matrix. <title>5.2.1 Cascade model</title> attraction probabilities α ∈ [0, 1] . The examination probability of item R(i) is if i = 1, otherwise 1 − (1 − α(R(j))). With the CM , we translate the implicit feedback to training labels as follows: Given a ranked list, items ranked below the clicked item are ignored since none of them are browsed. Items ranked above the clicked item are negative samples and the clicked item is the positive sample. If no item is clicked, we know that all items are browsed but not clicked. Thus, all of them are negative samples. The vanilla CM is only able to capture the ﬁrst click in a session, and there are various extensions of CM to model multi-click scenarios; cf. [ 23 ]. However, we still focus on the CM , because it has been shown in multiple publications that the CM achieves good performance in both online and ofﬂine setups [23, 65, 77]. <title>5.2.2 Cascading bandits</title> Cascading bandits ( CB ) are a type of online variant of the CM 65 ]. A CB is represented by a tuple (D, K, P ) , where is a binary distribution over {0, 1} . The learning agent interacts with the CB and learns from the feedback. At each step , the agent generates a ranked list ∈ Π (D) depending on observations in the previous t − 1 steps and shows it to the user. The user browses the list with cascading behavior and leaves click feedback. Since the CM accepts at most one click, we write ∈ [K + 1] as the click indicator, where indicates the position of the click and = K + 1 indicates no click. Let ∈ {0, 1} be the attraction indicator, where is drawn from and (R (i)) = 1 indicates that item (i) attracts the user at step . The number of clicks at step t is considered as the reward and computed as follows: where (·) is the Bernoulli distribution with mean α(i) . The expected number of clicks at step is computed as E [r(R , A )] = r(R , α) . The goal of the agent is to maximize the expected number of clicks in steps or minimize the expected -step regret: CB has several variants depending on assumptions on the attraction probability Brieﬂy, cascade linear bandits [ 133 ] assume that an item is represented by a feature vector ∈ R and that the attraction probability of an item to a user is a linear combination of features: α(a) ≈ z , where ∈ R is an unknown parameter. With this assumption, the attraction probability of an item is independent of other items in the list, and this assumption is used in relevance ranking problems. CascadeLinUCB has been proposed to solve this problem. For other problems, Hiranandani et al. [38] assume the attraction probability to be submodular, and propose CascadeLSB to solve result diversiﬁcation. <title>5.2.3 Submodular coverage model</title> Before we recapitulate the submodular function, we introduce two properties of a diversiﬁed ranking. Different from the relevance ranking, in a diversiﬁed ranking, the utility of an item depends on other items in the list. Suppose we focus on news recommendation. Items that we want to rank are news itms, and each news item covers a set of topics, e.g., weather, sports, politics, a celebrity, etc. We want to recommend a list that covers a broad range of topics. Intuitively, adding a news item to a list does not decrease the number of topics that are covered by the list, but adding a news item to a list that covers highly overlapping topics might not bring much extra beneﬁt to the list. The ﬁrst property can be thought of as a monotonicity property, and the second one is the notion of diminishing gain in the utility. They can be captured by the submodular function [120]. We introduce two properties of submodular functions. Let g(·) be a set function, which maps a set to a real value. We say that g(·) is monotone and submodular if given two item sets and , where B ⊆ A , and an item g(·) has the following two properties: monotonicity : g(A ∪ {a}) ≥ g(A); submodularity : g(B ∪ {a}) − g(B) ≥ g(A ∪ {a}) − g(A). The gain in topic coverage of adding an item a to A is: where (a | A) = g (A ∪ {a}) − g (A) . With this model, the attraction probability of the i-th item in a ranked list R is deﬁned as: where = ∆(R(i) | (R(1), . . . , R(i−1))) and is the unknown user preference to different topics [ 38 ]. In Eq. (5.6), the attraction probability of an item depends on the items ranked above it; α(R(i)) is small if R(i) covers similar topics as higher ranked items. CascadeLSB 38 ] has been proposed to solve cascading bandits with this type of attraction probability and aims at building diverse ranked lists. <title>5.3 Algorithm</title> In this section, we ﬁrst formulate our online learning to rank problem, and then propose CascadeHybrid to solve it. <title>5.3.1 Problem formulation</title> We study a variant of cascading bandits, where the attraction probability of an item in a ranked list depends on two aspects: item relevance and item novelty. Item relevance is independent of other items in the list. Novelty of an item depends on the topics covered by higher ranked items; a novel item brings a large gain in the topic coverage of the list, i.e., a large value in Eq. (5.6). Thus, given a ranked list , the attraction probability of item R(i) is deﬁned as follows: where = ∆(R(i) | (R(1), . . . , R(i − 1)) is the topic coverage gain discussed in Section 5.2.3, and ∈ R is the relevance feature, ∈ R and ∈ R are two unknown parameters that characterize the user preference. In other words, the attraction probability is a hybrid of a modular (linear) function parameterized by and a submodular function parameterized by θ Now, we deﬁne our learning problem, Cascade Hybrid Bandits ( CHB ), as a tuple (D, θ , β , K) . Here, D = [L] is the item candidate set and each item can be represented by a feature vector [x , z , where ∈ [0, 1] is the topic coverage of item discussed in Section 5.2.3. is the number of positions. The action space for the problem are all permutations of individual items from (D) . The reward of an action at step is the number of clicks, deﬁned in Eq. (5.1). Together with Eqs. (5.1), (5.2) and (5.7), the expectation of reward at step t is computed as follows: In the rest of the chapter, we write r(R ) = E [r(R , A )] for short. And the goal of the learning agent is to maximize the reward or, equivalently, to minimize the -step regret deﬁned as follow: The previously proposed CascadeLinUCB 133 ] cannot solve CHB since it only handles the linear part of the attraction probability. CascadeLSB 38 ] cannot solve CHB , either, because it only uses one submodular function and handles the submodular part of the attraction probability. Thus, we need to extend the previous models or, in other words, propose a new hybrid model that can handle both linear and submodular properties in the attraction probability. <title>5.3.2 Competing with a greedy benchmark</title> efﬁciently. Thus, we compete with a greedy benchmark that approximates the optimal ranking . The greedy benchmark chooses the items that have the highest attraction probability given the higher ranked items: for any positions k ∈ [K], where R(k) is the ranked list generated by the benchmark. This greedy benchmark has been used in previous literature [ 38 120 ]. As shown by Hiranandani et al. [38] , in the CM , the greedy benchmark is at least a -approximation of R . That is, r( R) ≥ ηr(R ), (5.12) where η = (1 − ) max{ , 1 − with = max + x . In the rest of the chapter, we focus on competing with this greedy benchmark. <title>5.3.3 CascadeHybrid</title> we recalculate the topic coverage gain of each item (line 7). The new gains are used to estimate the attraction probability of items. CascadeHybrid makes an optimistic estimate of the attraction probability of each item (lines 9–11) and chooses the one with the highest estimated attraction probability (line 12). This is known as the principle of optimism in the face of uncertainty [ ], and the estimator for an item is called the Upper Conﬁdence Bound (UCB): Finally, CascadeHybrid displays the ranked list to the user and collects click feedback (lines 15–23). Since CascadeHybrid only accepts one click, we use c ∈ [K + 1] to indicate the position of the click; = K + 1 indicates that no item in is clicked. Figure 5.1: -step regret on the MovieLens dataset with different . Results are averaged over 500 users with repeats per user. Lower regret means more clicks received by the algorithm during the online learning. Shaded areas are the standard errors of estimates. <title>5.3.4 Computational complexity</title> The main computational cost of Algorithm 6 is incurred by computing matrix inverses, which is cubic in the dimensions of the matrix. However, in practice, we can use the Woodbury matrix identity [ 33 ] to update and instead of and , which is square in the dimensions of the matrix. Thus, computing the UCB of each item is O(m + d . As CascadeHybrid greedily chooses items out of , the per-step computational complexity of CascadeHybrid is O(LK(m + d )). <title>5.4 Experiments</title> This section starts with the experimental setup, where we ﬁrst introduce the datasets, click simulator and baselines. After that we report our experimental results. <title>5.4.1 Experimental setup</title> evaluate the CascadeHybrid in a simulated interaction environment, where the simulator is built based on ofﬂine datasets. This is a commonly used evaluation setup in the literature [38, 65, 133]. Datasets. We evaluate CascadeHybrid on two datasets: MovieLens 20M [ 37 ] and Yahoo. The MovieLens dataset contains 20 M ratings on 27 k movies by 138 k users, with 20 genres. Each movie belongs to at least one genre. The Yahoo dataset contains over 700 M ratings of 136 k songs given by 1.8 M users and genre attributes of each song; we consider the top level attribute, which has 20 different genres; each song belongs to a single genre. All the ratings in the two datasets are on a -point scale. All movies and songs are considered as items and genres are considered as topics. Data preprocessing. We follow the data preprocessing approach in [ 38 81 133 ]. First, we extract the k most active users and the k most rated items. Let U = [1000] be the user set, and D = [1000] be the item set. Then, the ratings are mapped onto a binary scale: rating is converted to and others to . After this mapping, in the MovieLens dataset, about 7% of the user-item pairs get rating , and, in the Yahoo dataset, about 11% of user-item pairs get rating . Then, we use the matrix F ∈ {0, 1} to capture the converted ratings and G ∈ {0, 1} to record the items and topics, where is the number of topics and each entry = 1 indicates that item belongs to topic k. Click simulator. In our experiments, the click simulator follows the cascade assumption, and considers both item relevance and diversity of the list. To design such a simulator, we combine the simulators used in [ 81 ] and [ 38 ]. Because of the cascading assumption, we only need to deﬁne the way of computing attraction probabilities of items in a list. In particular, the relevance features are obtained by conducting singular-value decomposition on . We pick the 10 largest singular values and thus the dimension of relevance features is m = 10 . Then, we normalize each relevance feature by the transformation: , where kz is the L2 norm of . The user preference is computed by solving the least square on and then is normalized by the same transformation. Note that ∀a ∈ D : z ∈ [0, 1], since kz = 1 and kβ = 1. be attractive to the number of users who rate at least one item in topic to be attractive: . (5.15) 1{∃a ∈ D : F > 0} For some cases, we may have ∃a :∼ > 1 and thus > 1 . However, given the high sparsity in our datasets, we have ∈ [0, 1] for all items during our experiments. Finally, we combine the two parts and obtain the attraction probability used in our click simulator. To simulate different types of user preferences, we introduce a trade-off parameter λ ∈ [0, 1] , which is unknown to online algorithms, and compute the attraction probability of the ith item in R as follows: By changing the value of , we simulate different types of user preference: a larger value of means that the user prefers items to be relevant; a smaller value of means that the user prefers the topics in the ranked list to be diverse. Baselines. We compare CascadeHybrid to two online algorithms, each of which has two conﬁgurations. In total, we have four baselines, namely CascadeLinUCB and CascadeLinUCBFull 133 ], and CascadeLSB and CascadeLSBFull 38 ]. The ﬁrst two only consider relevance ranking. The differences are that CascadeLinUCB takes as the features, while CascadeLinUCBFull takes {x, z} as the features. CascadeLSB and CascadeLSBFull only consider the result diversiﬁcation, where CascadeLSB takes as features, while CascadeLSBFull takes {x, z} as features. CascadeLinUCBFull and CascadeLinUCB are expected to perform well when λ → 1 , and that CascadeLSB and CascadeLSBFull perform well when λ → 0 . For all baselines, we set the exploration parameter γ = 1 and the learning rate to . This parameter setup is used in [ 120 ], which leads to better empirical performance. We also set γ = 1 for CascadeHybrid. We report the cumulative regret, Eq. (5.9), within 50 k steps, called -step regret. The -step regret is commonly used to evaluate bandit algorithms [ 38 74 120 133 ]. In our setup, it measures the difference in number of received clicks between the oracle that knows the ideal and and the online algorithm, e.g., CascadeHybrid , in steps. The lower regret means the more clicks received by the algorithm. We conduct our experiments with 500 users from the test group and repeats per user. In total, the results are averaged over k repeats. We also include the standard errors of our estimates. To show the impact of different factors on the performance of online LTR algorithms, we choose λ ∈ {0.0, 0.1, . . . , 1.0} , the number of positions K ∈ {5, 10, 15, 20} , and the number of topics d ∈ {5, 10, 15, 20} . For the number of topics , we choose the topics with the maximum number of items. Figure 5.2: n-step regret on the MovieLens dataset with different topics d. <title>5.4.2 Experimental results</title> Figure 5.3: n-step regret on the MovieLens dataset with positions K. becomes relatively larger since = 1 . And given an item and a set the difference between ∆(a|S) and ∆(a|∅) is large. This behavior is also conﬁrmed by the fact that CascadeLinUCBFull outperforms CascadeLSBFull for large while they perform similarly for small . Finally, we study the impact of the number of positions on the regret. The results are displayed in Fig. 5.3, where we choose λ = 0.5 and d = 20. Again, we see that CascadeHybrid outperforms baselines with large gaps. Next, we report the results on the Yahoo dataset in Figs. 5.4 to 5.6. We follow the same setup as for the MovieLens dataset and observe a similar behavior. CascadeHybrid has slightly higher regret than the best performing baselines in three cases: CascadeLSB when λ = 0 and CascadeLinUCB when λ ∈ {0.9, 1} . Note that these are relatively extreme cases, where the particularly designed baselines can beneﬁt most. Meanwhile, CascadeLSB and CascadeLinUCB do not generalize well with different s. In all setups, CascadeHybrid has lower regret than CascadeLSBFull and CascadeLinUCBFull , which conﬁrms our hypothesis that the hybrid model has beneﬁt in capturing both relevance and diversity. Figure 5.4: -step regret on the Yahoo dataset with different . Results are averaged over 500 users with 2 repeats per user. <title>5.5 Analysis</title> <title>5.5.1 Performance guarantee</title> Since CascadeHybrid competes with the greedy benchmark, we focus on the -scaled expected n-step regret which is deﬁned as: (n) = E [ηr(R , α) − r(R , A )] , (5.18) where η = (1 − ) max{ , 1 − . This is a reasonable metric, since computing the optimal is computationally inefﬁcient. A similar scaled regret has previously been used in diversity problems [ 38 95 120 ]. For simplicity, we write = [θ , β . Then, we bound the η-scaled regret of CascadeHybrid as follows: Figure 5.5: n-step regret on the Yahoo dataset with different topics d. Combining Eqs. (5.19) and (5.20), we have (n) = O((m + d) Kn) , where the notation ignores logarithmic factors. Our bound has three characteristics: (i) Theorem 5.1 states a gap-free bound, where the factor is considered near optimal; (ii) This bound is linear in the number of features, which is a common dependence in learning bandit algorithms [ ]; and (iii) Our bound is O( K) lower than other bounds for linear bandit algorithms in CB 38 133 ]. We include a proof of Theorem 5.1 in Section 5.5.2. We use a similar strategy to decompose the regret as in [ 38 133 ], but we have a better analysis on how to sum up the regret of individual items. Thus, our bound depends on O( K) rather than O(K) . We believe that our analysis can be applied to both CascadeLinUCB and CascadeLSB , and then show that their regret is actually bounded by O( K) rather than O(K). <title>5.5.2 Proof of Theorem 5.1</title> Figure 5.6: n-step regret on the Yahoo dataset with different positions K. We deﬁne upper and lower conﬁdence bounds, and κ as: (R) = F [φ + s(R )] where l = |R| and [·] = max(0, min(1, ·)) . With the deﬁnitions in Eq. (5.23), f(R, κ) = r(R, α) is the reward of list R. E [ηr(R , α) − r(R , A )] = E [ηf(R , κ) − f(R , κ)] ≤ P (g )E [ηf(R , κ) − f(R , κ)] + P (¯g (5.24) ≤ P (g )E [ηf(R , u ) − f(R , κ)] + P (¯g ≤ P (g )E [f(R , u ) − f(R , κ)] + P (¯g ), (1 − κ(R )) − (1 − u (R )) (5.26) (1 − κ(R )) (u (R ) − κ(R )) (1 − u(R )) (1 − κ(R )) (u (R ) − κ(R )), ≤ 2γE s(R p(¯g ). For the ﬁrst term in Eq. (5.28), we have (5.29) s(R nK s(R nK2 log det(O ), where inequality (a) follows from the Cauchy-Schwarz inequality and (b) follows from Lemma 5 in [ 120 ]. Note that log det(O ) ≤ (m + d) log(K(1 + n/(m + d))) , which can be obtained by the determinant and trace inequality, and together with Eq. (5.29): This concludes the proof of Theorem 5.1. <title>5.6 Related Work</title> The literature on ofﬂine Learning to Rank ( LTR ) methods that account for position bias and diversity is too broad to review in detail. We refer readers to [ ] for an overview. In this section, we mainly review online LTR papers that are closely related to our work, i.e., stochastic click bandit models. Online LTR in a stochastic click models has been well-studied [ 58 65 74 77 81 99 106 120 132 133 ]. Previous work can be categorized into two groups: featurefree models and feature-rich models. Algorithms from the former group use a tabular representation on items and maintain an estimator for each item. They learn inefﬁciently and are limited to the problem with a small number of item candidates. In this chapter, we focus on the ranking problem with a large number of items. Thus, we do not consider feature-free model in the experiments. Feature-rich models learn efﬁciently in terms of the number of items. They are suitable for large-scale ranking problems. Among them, ranked bandits [ 99 106 ] are early approaches to online LTR . In ranked bandits, each position is model as a MAB and diversity of results is addressed in the sense that items ranked at lower positions are less likely to be clicked than those at higher positions, which is different from the topical diversity as we study. Also, ranked bandits do not consider the position bias and are suboptimal in the problem where a user browse different possition unevenly, e.g., CM 65 ]. LSBGreedy 120 ] and UCB 95 ] use submodular functions to solve the online diverse LTR problem. They assume that the user browses all displayed items and, thus, do not consider the position bias either. Our work is closely related to CascadeLinUCB 133 ] and CascadeLSB 38 ], the baselines, and can be viewed as a combination of both. CascadeLinUCB solves the relevance ranking in the CM and assumes the attraction probability is a linear combination of features. CascadeLSB is designed for result diversiﬁcation and assumes that the attraction probability is computed as a submodular function; see Eq. (5.6). In our CascadeHybrid , the attraction probability is a hybrid of both; see Eq. (5.7). Thus, CascadeHybrid handles both relevance ranking and result diversiﬁcation. RecurRank 81 is a recently proposed algorithm that aims at learning the optimal list in term of item relevance in most click models. However, to achieve this task, RecurRank requires a lot of randomly shufﬂed lists and is outperformed by CascadeLinUCB in the CM [81]. The hybrid of a linear function and a submodular function has been used in solving combinatorial semi-bandits. Perrault et al. [93] use a linear set function to model the expected reward of arm, and use the submodular function to compute the exploration bonus. This is different from our hybrid model, where both the linear and submodular functions are used to model the attraction probability and the conﬁdent bound is used as the exploration bonus. <title>5.7 Conclusion</title> In real world interactive systems, both relevance of individual items and topical diversity of result lists are critical factors in user satisfaction. This chapter has provide one solution to this problem, i.e., RQ4 . In order to better meet users’ information needs, we propose a novel online LTR algorithm that optimizes both factors in a hybrid fashion. We formulate the problem as Cascade Hybrid Bandits ( CHB ), where the attraction probability is a hybrid function that combines a function of relevance features and a submodular function of topic features. CascadeHybrid utilizes a hybrid model as a scoring function and the UCB policy for exploration. We provide a gap-free bound on the -scaled -step regret of CascadeHybrid , and conduct experiments on two real-world datasets. Our empirical study shows that CascadeHybrid outperforms two existing online LTR algorithms that exclusively consider either relevance ranking or result diversiﬁcation. In future work, we intend to conduct experiments on live systems, where feedback is obtained from multiple users so as to test whether CascadeHybrid can learn across users. Another direction is to adapt Thompson Sampling ( TS ) [ 111 ] to our hybrid model, since TS generally outperforms UCB-based algorithms [79, 133]. <title>Conclusions</title> We set up this thesis to study the online optimization of ranking systems as bandit problems. We consider two typical ranking tasks: online ranker evaluation and online learning to rank. A typical challenge in these tasks is that the feedback is implicit and partially-observed. Speciﬁcally, in the online evaluation task, the feedback is the relative comparison of two rankers; and in Online Learning to Rank ( OLTR ), the feedback is the click but due to the position bias, the examination of each item is unobserved. In this thesis, we have considered four research directions in the online ranker optimization and formulated each of them as a bandit problem and proposed the corresponding algorithm. In the rest of this chapter, we ﬁrst summarize the main results of this thesis and then outline some potential directions for future research. <title>6.1 Results</title> We ﬁrst revisit the research questions that are asked in Section 1.1. RQ1 How to conduct effective large-scale online ranker evaluation? The question has been answered in Chapter 2, where we introduce the Thompson Sampling ( TS ) based algorithm, called MergeDTS. Although in theory, the regret of MergeDTS has the same order as some baselines, i.e., MergeRUCB and Self-Sparring, MergeDTS outperforms those baselines with large gaps in our large-scale online ranker evaluation experiments, as shown in Section 2.6. Speciﬁcally, we have conducted experiments on three large-scale datasets, each of which has been combined with different types of click behavior. MergeDTS has lower regret than the baselines in out of conﬁgurations, and only has slightly higher regret than Self-Sparring in conﬁgurations. Meanwhile, the time efﬁciency of MergeDTS is better than that of Self-Sparring. To answer RQ1 , we conduct an effective large-scale online ranker evaluation by combining Thompson sampling and the divide-and-conquer idea in MergeRUCB. RQ2 How to achieve safe online learning to re-rank? comparison in dueling bandits. BubbleRank starts from the ranked list produced by a production ranker, and conducts pairwise comparisons between an item with its neighbors. To get rid of the position bias, the position of an item is randomly exchanged with its neighbors, during the comparisons. Thus, during the exploration, an item will not displayed too far from its original position. On the other hand, if one item is considered better than its upper neighbor with a high conﬁdence, the positions of two items are exchanged permanently. The safety feature of BubbleRank has been theoretically proved in Lemma 3.1. The -step regret of BubbleRank is O(K log(n)) where is the number of ranking positions and is the time horizon. This regret bound is only O(K) higher than the regret of TopRank , the state-of-the-art but unsafe baseline. We have conducted an empirical evaluation on a large-scale click dataset, the Yandex click dataset, and the empirical results have conﬁrmed the theoretical ﬁndings. RQ3 How to conduct online learning to rank when users change their preference constantly? Our answer to this question has been provided in Chapter 4. Firstly, the considered non-stationary OLTR problem has been formulated as a cascading non-stationary bandit problem, where we assume that users follow the cascading browsing behavior. Then, two algorithms, CascadeDUCB and CascadeSWUCB , have been proposed to solve this problem. To deal with the non-stationarity, the former algorithm uses the discount factor, and the latter uses a sliding window. The -step regret of CascadeDUCB and CascadeSWUCB is O( n ln(n)) and O( n ln(n)) , respectively, where is the time horizon. They both match the lower bound up to logarithmic factors. Finally, we have evaluated the proposed algorithms in a semi-synthetic experiment on the Yandex click dataset. The experimental results are consistent with our theoretical ﬁndings. RQ4 How to learn a ranker online considering both relevance and diversity? This question is answered in Chapter 5. As a follow up to Chapter 4, the cascade click model has also been used to interpret the click feedback. We have ﬁrst formulated the OLTR problem for relevance and diversity as a cascade hybrid bandit problem, and then introduced the CascadeHybrid algorithm to solve the proposed problem. The term hybrid comes from the fact that we use a linear function for relevance and a submodular function for diversity. The -step regret of the proposed CascadeHybrid is with the form of O( Kn), where O(·) ignores logarithmic terms, K is the number of positions, and is the time horizon. This bound is comparable to other linear bandit approaches for OLTR . In our experiments, we have evaluated CascadeHybrid on two tasks: movie recommendation on the MovieLens dataset and music recommendation on the Yahoo music dataset. The experimental results indicate that with the underlying hybrid model, CascadeHybrid learns a ranker that considers both item relevance and result diversiﬁcation. <title>6.2 Future Work</title> future work provided at the end of each of the research chapters, we believe there are two important future directions. Beyond the linear model The proposed algorithms in this thesis are based on linear or tabular models. Although they learn efﬁciently, the relatively low learning capacity limits the application scenarios of these algorithms. To model more complicated problems, it is appealing to consider non-linear models, e.g., tree-based models and deep models, which have a more powerful learning capacity. However, training non-linear models online with implicit feedback is not without difﬁculty. One of the challenges is exploration with non-linear models. Let us step back and take a look at the exploration strategies in this thesis, i.e., UCB and TS . They both require closed-form solutions of the underlying models to estimate the reward of each arm. This requirement can easily be satisﬁed when linear or tabular models are used. However, there are generally no closed-form solutions for nonlinear models. To use UCB and TS for non-linear models, additional approximation methods, e.g., the Laplacian approximation, are required [ 101 124 ], which can be computationally expensive. On the other hand, -greedy is one of the basic exploration strategies, and can be used for non-linear models. Although -greedy is far from optimal in Multi-Armed Bandits (the tabular case) and linear bandits (the linear case), for some non-linear bandit setups, -greedy is shown to be rather competitive [69, 101]. A more sophisticated policy is bootstrapped sampling or bootstrapping [ 67 69 101 ]. Bootstrapping is a statistical technique that estimates the distribution of random variables by conducting sampling with replacement. Thus, it can be used to estimate the variance of the predictions of non-linear models. However, there is no conclusive study to indicate which policy is the optimal one for the ranking task. We believe that it is an interesting direction to study how to conduct exploration for non-linear OLTR. Privacy This direction comes from the recently regulations on data protection, e.g., the General Data Protection Regulation ( GDPR by European Union and consumer data protection in the White House report [ 47 ]. The algorithms we have proposed in this thesis depend on the collection of user interaction data, which inevitably brings privacy risks. One way to provide protection w.r.t. user privacy is federated learning FL ) [ 13 63 75 ], where the learning happens on the local devices under the coordination of a central server, called federator. Importantly, federated learning is easily combined with randomization techniques from differential privacy [ 28 ], and provides theoretically sound approaches to protect user privacy. In federated learning, the private user data is not collected by any central server but the learned model gradients are. Brieﬂy, the federator maintains a global model that is trained by the participating clients, e.g., local mobile devices. The training goes in rounds. At the start of each round, the federator broadcasts the global model to local clients. Each client trains the model based on the local dataset, and then sends back the local gradient or the updated model to the federator. The federator updates the global model according to all local updates. This process continues until a certain termination criterion has been met, e.g., the loss stops dropping. Federated learning also works in an online fashion. That is, after each round, the new global model is updated based on user requests. However, in FL , the update at each round consists of several local updates, while in online learning, there is only one update per round. To combine the two learning paradigms, we have to deal with batched feedback or updates, which again brings new challenges to exploration. As federated learning to rank is a rather new but important topic, we expect to see more algorithms published in the future. <title>Bibliography</title> <title>Summary</title> People use interactive systems, such as search engines, as the main tool to obtain information. To satisfy the information needs, such systems usually provide a list of items that are selected out of a large candidate set and then sorted in the decreasing order of their usefulness. The result lists are generated by a ranking algorithm, called ranker, which takes the request of user and candidate items as the input and decides the order of candidate items. The quality of these systems depends on the underlying rankers. There are two main approaches to optimize the ranker in an interactive system: using data annotated by humans or using the interactive user feedback. The ﬁrst approach has been widely studied in history, also called ofﬂine learning to rank, and is the industry standard. However, the annotated data may not well represent information needs of users and are not timely. Thus, the ﬁrst approaches may lead to suboptimal rankers. The second approach optimizes rankers by using interactive feedback. This thesis considers the second approach, learning from the interactive feedback. The reasons are two-fold: (1) Everyday, millions of users interact with the interactive systems and generate a huge number of interactions, from which we can extract the information needs of users. (2) Learning from the interactive data have more potentials to assist in designing the online algorithms. Speciﬁcally, this thesis considers the task of learning from the user click feedback. The main contribution of this thesis is proposing a safe online learning to re-rank algorithm, named BubbleRank , which addresses one main disadvantage of online learning, i.e., the safety issue, by combining the advantages of both ofﬂine and online learning to rank algorithms. The thesis also proposes three other online algorithms, each of which solves unique online ranker optimization problems. All the proposed algorithms are theoretically sound and empirically effective. <title>Samenvatting</title> Mensen gebruiken interactiesystemen, zoals een zoekmachine, als hun belangrijkste hulpmiddel om informatie te verkrijgen. Om aan de informatiebehoefte te voldoen, bieden dergelijke zoekmachines meestal een lijst met items aan. Deze items zijn geselecteerd uit een grote set van kandidaten en zijn vervolgens gesorteerd in afnemende volgorde van relevantie. Deze resultatenlijst wordt gegenereerd door een rankingalgoritme, ranker genaamd, dat de informatiebehoefte van de gebruiker en de kandidaatitems als input neemt en vervolgens de volgorde van de kandidaat-items bepaalt. De kwaliteit van deze systemen is afhankelijk van de onderliggende rankers. Er zijn twee belangrijke benaderingen om de rankers in een interactiesysteem te optimaliseren: De eerste manier is door gebruik te maken van de gegevens die door mensen zijn geannoteerd. De tweede is door gebruik te maken van de interactie-feedback van gebruikers. De eerste benadering is uitgebreid bestudeerd, ook wel bekend als ofﬂine-learning-to-rank, en is de standaard binnen de industrie. Echter, geannoteerde gegevens geven mogelijk niet de juiste informatiebehoeften van de gebruikers weer en zijn niet actueel. De eerste benadering kan dus leiden tot sub-optimale rankers. De tweede benadering optimaliseert rankers door gebruik te maken van interactie-feedback. Dit proefschrift behandelt de tweede benadering. Namelijk, het leren van feedback van gebruikers-interactie. De redenen zijn tweeledig: (1) Elke dag hebben miljoenen gebruikers interactie met interactie-systemen waarmee zij een enorm aantal interacties genereren. Uit deze interactie-data van de gebruikers kan de informatiebehoeften van de gebruikers worden afgeleid. (2) Het leren van de interactie-gegevens biedt meer mogelijkheden bij het ontwerpen van de online-algoritmen. Dit proefschrift gaat speciﬁek in op de taak om te leren van feedback van gebruikers op basis van clicks. De belangrijkste bijdrage van dit proefschrift is het introduceren van een algoritme voor veilig online learning to re-rank, genaamd BubbleRank , dat een belangrijk nadeel van online leren aanpakt, namelijk het veiligheidsprobleem. Het doet dit door de voordelen van zowel ofﬂine als online learning to rank te combineren. Het proefschrift stelt ook drie andere online algoritmen voor, die elk een uniek optimalisatieprobleem voor online rankers oplossen. Alle voorgestelde algoritmen zijn theoretisch correct en empirisch effectief.