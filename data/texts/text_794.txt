An increasing awareness of biased patterns in natural language processing resources, like BERT, has motivated many metrics to quantify ‘bias’ and ‘fairness’. But comparing the results of different metrics and the works that evaluate with such metrics remains difﬁcult, if not outright impossible. We survey the existing literature on fairness metrics for pretrained language models and experimentally evaluate compatibility, including both biases in language models as in their downstream tasks. We do this by a mixture of traditional literature survey and correlation analysis, as well as by running empirical evaluations. We ﬁnd that many metrics are not compatible and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective. To improve future comparisons and fairness evaluations, we recommend avoiding embedding-based metrics and focusing on fairness evaluations in downstream tasks. With the popularization of word embeddings by works such as Word2vec (Mikolov et al., 2013), GLoVe (Pennington et al., 2014) and, more recently, contextualized variants such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), Natural Language Processing (NLP) has seen signiﬁcant growth and advancement. Word embeddings and language models have been adopted by many applications. With that in mind, probes have been made about the fairness of some of these models and if these models reﬂect or exacerbate biases and stereotypes that are captured in society. Word embeddings are generally trained on realworld data in such a manner that they model the statistical properties of the training data. Hence, they pick up on biases and stereotypes that are typically present in the data (Garrido-Mu˜noz et al., 2021). These biases and stereotypes can pose signiﬁcant challenges in downstream applications (Kurita et al., 2019), although this view has been questioned (Goldfarb-Tarrant et al., 2020). We will revisit this discussion later in this paper. Early works like Bolukbasi et al. (2016); Caliskan et al. (2017); Gonen and Goldberg (2019) widely explored fairness in non-contextualized language models. In non-contextualized embeddings, like Word2vec and GLoVe embeddings, models are trained to generate vectors that map directly to dictionary words and hence, are independent of the context in which the word is used. Contextualized word embeddings on the other hand take polysemy (words could have multiple meanings, e.g. ‘a stick’ vs ‘let’s stick to’) into consideration, as such, different embeddings are generated for a particular word depending on the context in which it appears. Owing to this distinction in both approaches, popular techniques for detecting and measuring bias in non-contextualized word embeddings, such as WEAT (Caliskan et al., 2017), do not apply naturally to contextualized variants. Many techniques have been proposed to measure bias in contextualized word embeddings, either as a standalone method (May et al., 2019; Bartl et al., 2020) or as an additional contribution to evaluating fairness interventions (Webster et al., 2020; Lauscher et al., 2021; Kurita et al., 2019). The challenge, however, is the difﬁculty in putting all these works into perspective and comparing their performances. This makes it difﬁcult for NLP practitioners to select an appropriate and reliable set of metrics to quantify bias in language models and NLP systems. These quantifying techniques also involve different choices for attribute and target words, commonly referred to as seed words, templates as context, and ﬁnally similarity methods. In this paper, we perform a combination of literature survey and experimental comparisons to compare fairness metrics for contextualized language models. Concretely, we aim to answer the following research questions: •Which fairness measures exist for contextualized language models like BERT? (Section 3) •How do these fairness measures translate beyond English? (§ 3.3) •What are the relationships between fairness measures, templates that these measures use, and embedding methods? (Section 4) • Which set of measures is recommended? Static word embeddings have typically been used with recurrent neural networks (RNN) and later, RNNs with an attention mechanism (Bahdanau et al., 2014). The transformer architecture (Vaswani et al., 2017) introduced a new paradigm relying only on attention, which proved faster and more accurate than RNNs and did not rely on static word embeddings. The transformer architecture consists of two stacks of attention layers, the encoder and the decoder, with each layer consisting of multiple parallel attention heads. BERT (Devlin et al., 2019) is based on the encoder stack and trained with a Masked Language Modeling (MLM) objective. Similarly, auto-regressive or Causal Language Models (CLM) like GPT (Radford et al., 2018) are inspired by the decoder stack and generate one token based on the previous input. In this survey, we focus mostly on the former, as MLM models are typically used for transfer learning to adapt to downstream tasks. BERT obtained state-of-the-art results for multiple NLP tasks by using transfer learning (see Figure 1). First, the model is pre-trained on large corpora using the MLM objective. The intuition behind this task is that learning to reconstruct missing words in a sentence helps with capturing interesting semantics—and because this relies on co-occurrences it unfortunately also captures stereotypes that are present in pre-training datasets, which we refer to as bias intrinsic to the language model. A tokenxin the input sequence x, . . . , xis replaced by a special[MASK]token and the training objective of the model with parametersθis to predict the original tokenx Figure 1: Illustration of the transfer learning paradigm where a language model is ﬁrst pre-trained on one dataset and afterwards ﬁnetuned on another dataset. Both stages can introduce biases. based on the positional-dependent contextx= x, . . . , x, x, . . . , x, following with1as indicator function. After training, the language model can inference the probability that a token occurs on the masked position, e.g. for BERT this givesP (x= ‘He’ | x= ‘[MASK]is a doctor.’) = 0.615. We will use this notation for MLMs throughout this paper. As a second step, this pre-trained model can be re-trained or ﬁnetuned on a new task, most commonly either sentence classiﬁcation, which uses the contextualized embeddings of the ﬁrst token x= [CLS], or token classiﬁcation, for which the embeddings of each respective token position are used. These embeddings are obtained from output states of the penultimate layer, after which a single linear layer is placed. This ﬁnetuning is typically done with different datasets that are labeled for the task at hand, which introduces a second source of bias referred to as extrinsic bias. Many models improved on the original BERT architecture and training setup. For example, RoBERTa (Liu et al., 2019) was trained on signiﬁcantly more data for a longer period and the authors removed a second pre-training objective, next sentence prediction. ALBERT (Lan et al., 2019) used parameter sharing between attention layers to obtain a smaller model without signiﬁcant performance degradation. Sanh et al. (2019) also created a smaller BERT variation, DistilBERT, by using knowledge distillation. Despite some differences, like tokenization and different pre-training setups, all these models can be used for MLM and ﬁnetuning. This gives us the opportunity to compare bias metrics across multiple models. 2.1 Fairness in word embeddings Fairness in machine learning has a long standing, with well-studied examples like recidivism risk prediction (Angwin and Larson, 2016). For a general introduction on fairness in machine learning, which focuses mostly on classiﬁcation tasks, we refer to Barocas et al. (2019). Currently, many NLP applications rely on transfer learning by ﬁnetuning pre-trained language models, as is visualized in Figure 1. This paradigm creates two types of bias: (i) one in the pre-trained resource, called intrinsic bias and (ii) bias in the ﬁne-tuning for a speciﬁc task, called extrinsic bias. We will mostly focus on the former, since evaluating extrinsic biases is highly dependent on the task and as such, it is challenging to draw general conclusions from such evaluation. Early methods for evaluating bias in noncontextualized embeddings like Word2vec, are WEAT (Caliskan et al., 2017) and a direct bias metric (Bolukbasi et al., 2016). The latter demonstrated that word embeddings contain a (linear) biased subspace, where for example ‘man’ and ‘woman’ can be projected on the same gender axis as ‘computer programmer’ and ‘homemaker’ (Bolukbasi et al., 2016). These analogies are calculated using cosine distance between vectors to deﬁne similarity and also to evaluate the authors’ proposed debiasing strategies. In addition, pairs of gendered words were also evaluated using Principal Component Analysis (PCA). This showed that most of the variance stemming from gender could be attributed to a single principal component (Bolukbasi et al., 2016). In parallel, the Word Embeddings Association Test (WEAT; Caliskan et al., 2017) was developed based on the Implicit Association Tests (IAT; Greenwald et al., 1998) from social sciences. WEAT measures associations between two sets of target wordsX , Y, e.g. male and female names, and another two sets of attribute wordsA, B, e.g. career and family-related words, s(X , Y, A, B) =s(x, A, B) −s(y, A, B) with a similarity measures(x, A, B)between a word embeddingxand word vectors of attributes a ∈ A, b ∈ B, deﬁned as s(x, A, B) = meancos (x, a) − meancos (x, b) . This method relies on a vector representation for each word, which can be obtained in different ways in contextualized models and we discuss in Section 3 and§4.3. Finally, it should also be noted that WEAT serves as an indicator of bias, not a predictor (Goldfarb-Tarrant et al., 2020). Discovery of correlations (DisCo).Webster et al. (2020) presented an intrinsic measure (DisCo) and an extrinsic measure, (STS-B, see§3.2). To quantify bias, Discovery of Correlations (DisCo) uses templates with two slots such asT =‘X likes to[MASK].’. We provide a complete list in§A.1. The ﬁrst slot, X, is ﬁlled with words based on a set of e.g. ﬁrst names or nouns related to professions. The second slot is ﬁlled in by the language model and the three top predictions are kept. If these predictions differ between genders, this is considered an indication of a biased association. The resulting score is the average number of predictions that differ between genders. Lauscher et al. (2021) slightly modiﬁed this method by ﬁltering predictions with P (x| T ) > 0.1 instead of the top-three items. log probability bias score (LPBS).This bias score presented by Kurita et al. (2019) is a templatebased method that is similar to DisCo,but also corrects for the prior probability of the target attribute, as the token ‘He’ commonly has a higher prior than ‘She’. The reasoning is that correction ensures that any measured difference between attributes can be attributed to the attribute and not to the prior of this token. LPBS uses the same WEAT-based stimuli tests as SEAT. Bartl et al. (2020) introduced an alternative dataset speciﬁcally for this evaluation method, called bias evaluation corpus with professions (BEC-Pro), with templates and seeds in both English and German. We will revisit the German results in § 3.3. Sentence embedding association test (SEAT). A limitation of WEAT (Caliskan et al., 2017) is that the method does not work directly on contextualized word embeddings. SEAT is an adaption of WEAT that works with contextualized embeddings (May et al., 2019). The main contribution Table 1: Overview of intrinsic measures of bias for language models. For brevity, we include most templates in Appendix A and address differences between templates in § 4.2. We also discuss the evaluation types (§ 3.1) and embedding types (§ 4.3). We also indicate if data and source code are both available ( available (fs), or if neither is publicly available (f). The repositories are linked in Appendix C. is that associations between target and attributes are tested with semantically bleached or purposely unbleached template sentences, e.g. ‘[He/she] is a[MASK].’. These templates are used to extract an embedding to measure the cosine distance between two sets of attributes, following the original WEAT measure. This embedding is obtained from the[CLS]token in BERT and the last token in GPT. SEAT implemented three tests from WEAT, namely test 1 (ﬂowers vs. insects), 3 (EuropeanAmerican vs. African-American names), and 6 (male vs. female names). In addition, the authors also made new tests for double binds (Stone and Lovejoy, 2004) and angry Black woman stereotypes. An approach inspired by SEAT was taken by Lauscher et al. (2021) using token embeddings from the ﬁrst four attention layers instead of the last layer, as a preliminary evaluation showed a performance increase using these embeddings (Vulic et al., 2020). Tan and Celis (2019) also adapted SEAT by considering the contextualized embedding of the token of interest, instead of the[CLS] token and introduced new tests on intersectionality. These approaches illustrate how different embedding methods can give vastly different results and, in the case of SEAT, also fail to reliably indicate stereotypes that are present in the model (Kurita et al., 2019). We will discuss the implications of these different choices of embeddings later in§4.3. SEAT relies on semantically bleached templates to obtain embeddings for the target attributes, which is deﬁned as context that does not contain important information about the bias (May et al., 2019). However, these templates are perhaps not as semantically bleached as expected (May et al., 2019; Tan and Celis, 2019), which we will investigate further in Section 4. Bias Score.Bordia and Bowman (2019) introduced a bias metric for language models based on LSTMs and word embeddings. Even though this method is not used on contextualized embeddings, we include it since it works in a similar way as other methods. The presented bias score is deﬁned bias(x) = logP (x| Female context words)P (x| Male context words). Context Association Test (CAT).Nadeem et al. (2021) created StereoSet, a dataset with stereotypes with regard to professions, gender, race, and religion. Based on this dataset, a score, CAT, is calculated that reﬂects (i) how often stereotypes are preferred over anti-stereotypes and (ii) how well the language model predicts meaningful instead of meaningless associations. One limitation is that the test set is not publicly available, although there is a leaderboard. Blodgett et al. (2021) calls attention to many ambiguities, assumptions, and data issues that are present in this dataset. CrowS-Pairs.CrowS-Pairs (Nangia et al., 2020) takes a similar approach as Nadeem et al. (2021) with the crowd-sourced StereoSet dataset, but the evaluation is based on pseudo-loglikelihood (Salazar et al., 2020) to calculate a perplexity-based metric of all tokens conditioned on the stereotypical tokens. All samples in the CrowS-Pairs dataset consist of pairs of sentences where one has been modiﬁed to contain either a stereotype or an anti-stereotype. The pseudo-loglikelihood is then calculated for all tokens in both sentences, excluding the tokens that differ. Nangia et al. (2020) evaluated this metric with stereotypes of nine different sensitive attributes and found that ALBERT and RoBERTa both had higher scores. This dataset also has data quality issues (Blodgett et al., 2021). All Unmaksed Likelihood (AUL).Kaneko and Bollegala (2021) modify the above CrowS-Pairs measure to consider multiple correct predictions, instead of only testing if the target tokens are predicted. In addition, the authors also argue against evaluations biases using[MASK]tokens, since these tokens are not used in downstream tasks. PCA-based methods.Both Basta et al. (2019); Zhao et al. (2019) analyzed gender subspaces in ELMo using a method that is very similar to Bolukbasi et al. (2016). They found evidence of systematic encoding of gender bias (Zhao et al., 2019), but less gender bias in comparison to noncontextualized word embeddings (Basta et al., 2019). This approach was then applied to BERTbased models (Sedoc and Ungar, 2019). These methods are less suited to obtain numerical bias scores because they rely on identifying a gender axis in the ﬁrst principal components. This is often done visually in practice (Sedoc et al., 2019). Causal methods.Vig et al. (2020) introduces a visual method inspired by causality to analyze which attention heads contribute to biased token predictions in GPT-2. Here, we discuss some extrinsic measures that have been adopted in the literature to measure bias. These extrinsic measures are used to measure how bias propagates in downstream tasks such as occupation prediction and coreference resolution. This typically involves ﬁne-tuning the pretrained language model on a downstream task and subsequently evaluating its performance with regard to sensitive attributes like gender and race. A number of benchmarks and techniques have been adopted and proposed by different authors to measure extrinsic bias. Like in other aspects of bias literature, the majority of these metrics focus on gender bias due to the relative availability of gender-related datasets and the relatively widespread concern for gender-related biases. These extrinsic measures range from generic performance metrics like accuracy score to task-speciﬁc tools like VADER (Hutto and Gilbert, 2014) for sentiment analysis. In this section, we will focus on extrinsic measures specifically developed to measure bias in NLP models. BiasInBiosBiasInBios is an English dataset developed by De-Arteaga et al. (2019) as an extrinsic benchmark for measuring bias in language models. It has been adopted as an extrinsic measure by works such as Webster et al. (2020) and Zhao et al. (2020). The task is to predict professions based on biographies of people. The standard metric used is the True Positive Rate difference between male and female proﬁles when predicting their occupations (Webster et al., 2020). WinoBiasThe WinoBias dataset was developed by Zhao et al. (2018) based on the Winograd format. Hirst et al. (1981) is another English dataset used to measure extrinsic bias. WinoBias has been widely used to measure gender bias in coreference resolution tasks and consists of 40 occupations. The usual approach is to ﬁrst train the language model on the OntoNotes dataset (Weischedel et al., 2013) for coreference resolution. The WinoBias dataset is then used to measure the discrepancy in performance between gender groups; the ability of the model to resolve coreferencing of gender pronouns in the context of pro-stereotypes and anti-stereotypes. A pro-stereotype setting is when, for instance, a male pronoun is linked to a maledominated job, whereas a female pronoun being linked to that same job will be an anti-stereotype example. E.g.Pro-stereotype:[The janitor] reprimanded the accountant because [he] got less allowance.Anti-stereotype:[The janitor] reprimanded the accountant because [she] got less allowance. A model is said to pass the WinoBias test if the resolution is done with the same level of accuracy for pro-stereotyped and anti-stereotyped settings. WinogenderWinogender (Rudinger et al., 2018), similar to (Zhao et al., 2018), is an English coreference resolution dataset based on the Winograd format. Although similar, there are nuances in both approaches. Firstly, winoBias focuses on revealing correlations and biases present in the real-world, whereas WinoBias focuses on analysing bias mitigation techniques (Rudinger et al., 2018). Secondly, Winogender includes a neutral gender whilst WinoBias only uses a binary (female-male) deﬁnition of gender. Thirdly, Winogender uses only one occupation in each instance, whereas WinoBias uses two for each instance. It is unclear how much these nuances contribute to differences in the scores of these measures. We will explore this issue in our future works. Many languages have some sort of grammatical gender, which can be problematic for the fairness evaluation metrics presented in§3.1 that focus mostly on gender stereotyping by measuring associations or observing gendered principal components. The assumption is that there should be no acceptable association between e.g. professions and gender. However, in gendered languages, these associations are usually expected, with different nouns for many professions for instance. We leave an in-depth comparison for future work, but provide a brief overview of some methods that address grammatical gender in languages beyond English. For Dutch, a Germanic language with a gender system, Delobelle et al. (2020); Ch´avez Mulsa and Spanakis (2020) evaluated RobBERT, a Dutch language model. Delobelle et al. (2020) did this visually with three templates (§A.5). Interestingly, the authors did not consider an association between a gendered pronoun and professions as an indicator of bias, since this is expected in a gendered language. However, they did consider a prior towards male pronouns as evidence, which is an opposite view to LPBS (Kurita et al., 2019), which corrects for this prior. For German, Bartl et al. (2020) evaluated BECPro, which is similar to LPBS (Kurita et al., 2019). The authors found that the scores for male and female professions were very similar, likely because of a gender system. In this section, our goal is to objectively investigate the consistency in the various techniques used by previous work in measuring bias. As earlier mentioned, aside from the choice of the fairness metric itself, three primary factors are important when measuring intrinsic bias in an embedding model: (i) choice of seed words, (ii) choice of template sentences and (iii) how representations for seed words are generated. Does the choice of template and technique for selecting embeddings to represent seed words matter in measuring bias? Are “semantically bleached” templates really semantically bleached, meaning they do not affect bias measurements? Are all these choices really unraveling bias? Can bias in embedding model be concealed by picking the “right” templates or representations? These are questions we seek to answer with a series of experimental analyses. We will measure correlations between various approaches to test the hypothesis that these templates and representations measure the same bias. Recent works investigating bias in language models have found issues with inconsistencies between seed words (Antoniak and Mimno, 2021), unvoiced assumptions and data quality issues in StereoSet and CrowS-Pairs templates (Blodgett et al., 2021), and issues with semantically bleached templates (Tan and Celis, 2019). Probing the effect of seed word choices has already been extensively discussed by Antoniak and Mimno (2021) where they show that the measure of bias in an embedding model can be heavily inﬂuenced by the choice of seed words. As such, we focus our investigation on the choice of templates and the choice of representation methods. We carry out our experiments by conducting correlation analysis of different choices for both representation methods (§4.3) and templates (§4.2) as used by previous works. To create a context and to help draw concise conclusions, we focus all our experiments on binary gender bias with respect to professions, which is a common setup. We start by compiling the sets of attribute words (professions) and target words (gendered words). Following Caliskan et al. (2017) and Zhao et al. (2018), we obtain a list of professions from the US bureau of labour, which is split in a set of female “stereotyped” professions (male and female attributes),P= {p, p, ..., p}and an equivalent male setP= {p, p, ..., p}. The full list of professions is provided in§B.1. Furthermore, we create a female set and a male set for attribute words and target words, which is also common practice (Caliskan et al., 2017; Figure 2: Correlation of template types when using the [CLS] (Figure 2a) and the token of interest (Figure 2b). May et al., 2019), letS= {s, s, ..., s} be the set of female words e.g. woman, and let S= {s, s, ..., s}be the set of male words (see § B.2). For each set of attributesPandPwe generate 20 subsets{a, ..., a}by randomly sampling 10 professions. We refer to the set of these subsets as A= {a, ..., a}andA= {a, ..., a}, for female and male professions respectively such thata= {x, ..., x}, wherex∈ Pand a∈ A. We expect that some subsets will show higher levels of bias than others and that given two “accurate” fairness metricsMandM, ifM ranks three subsets asa< a< a, indicating thatacontains less bias thanawhich in turn contains less bias thana,Mshould likewise rank the three subsets in the same order of fairness. If there is any deviation in this ranking, we can draw two conclusions, eitherMorMis inaccurate, or bothMandMare inaccurate. Caliskan et al. (2017); May et al. (2019); Lauscher et al. (2021); Tan and Celis (2019) used a similar approach to calculate distributional properties and perform statistical tests. Using this idea, we conduct a number of correlation analysis experiments with some of the popular fairness evaluation techniques in order to probe for (in)consistencies that ensue from using such techniques. We use Pearson correlation coefﬁcients to carry out these investigations. In addition to using subsets of attributes, we also investigate the correlation between fairness metrics in ﬁve language models, where the different language models replace the need for subsets. We assume that different language models have different levels of biases, because of different training setups on different datasets. In§4.4, we discuss these results and indeed observe different levels of bias for different models. This was also observed for metrics that were evaluated on multiple models, e.g. CrowS-Pairs (Nangia et al., 2020). The choice of template for creating contexts for seed words plays a very important role in measuring bias in contextual word embeddings. Many papers propose the use of “semantically bleached” sentence templates for context. The rationale behind this approach is that since semantically bleached sentences contain no semantic meaning, the embedding that will be generated by inserting a seed word into such a template will generally represent the seed word only. The challenge with this approach is that, although these templates may be linguistically informationless, from a computational perspective, there is no stipulated way of measuring the amount of semantic information a sentence or template carries, hence we do not know for certain if these semantically bleached sentence templates generate an ”informationless” embedding. May et al. (2019); Tan and Celis (2019) indicated that semantically bleached templates might still contain some semantics, at least related to the assessed bias. For this experimental section, our hypothesis is that if these templates are semantically bleached with regard to a gender bias, all these templates should give similar indications of this bias. We consider the SEAT templates (May et al., 2019), Table 2: Templates used in our evaluation of the compatibility between templates. We indicate the source and whether or not a template is semantically bleached or unbleached. The last columns provide the results of our experiment on relative entropy, where we measure the distance between all templates and template T divergence means a more similar template. listed in Table 2 (T− T). We additionally compare with the masked template of used by Kurita et al. (2019) for their SEAT implementation (T), and ﬁnally, we add 2 semantically unbleached templates from Tan and Celis (2019) (T− T) as control templates. For the ﬁrst experiment, we use the[CLS]embedding as sentence representation, similar to May et al. (2019) We expect that all the semantically bleached templates will have a high correlation with other bleached templates, since they carry no meaning aside from that of the inserted seed word, which has been the major justiﬁcation for the use of these templates in NLP fairness literature. We test our hypothesis by doing a correlation analysis as described in§4.1 and we additionally test how well templates are indeed semantically bleached. This concept is loosely deﬁned as providing no semantic information with regard to the bias (May et al., 2019), which we operationalize as two templates T 1, T 2having the same contextualized probability for a set of tokens on position x, following To quantify the distance between both distributions, we calculate relative entropy (Kullback and Leibler, 1951) between every template and templateT, which we expect to be lower for the semantically bleached templates compared to the unbleached templates. We perform this relative entropy experiment twice: (i) once with all tokens in the model’s vocabulary and (ii) once with a set of gendered tokens (see§B.2). Both sets aim to evaluate how the contextualized distributions of the masked tokent= P (x| T)differ, but we expect a lower divergence for the gendered subset. Figure 2a and Table 2 present our results for the correlation analysis and difference in distributions, where we make three observations. Firstly, the choice of “semantically bleached” template could signiﬁcantly vary the measure of bias. Although templatesTtoTare all “semantically bleached”, there are very weak and sometimes even negative correlations (e.g.T). The fact that we do not get (close to) perfect correlation among these templates conﬁrms the observation made by May et al. (2019) on the possible impact that “semantically bleached” templates could have on the fairness evaluation process. Secondly, semantically and syntactically similar templates do not necessarily have stronger correlations. Take “There is the .” (T) and “The is there.” (T) for example, these two templates contain the exact same words which are believed to carry no signiﬁcant information regarding gender. However, there is a minute negative correlation between these two. Thirdly, the distributional distances between Tand all other templates, as measured by the Kullback-Leiber divergence and shown in Table 2, highlight that the different templates are indeed not completely semantically bleached. However, this deﬁnition does have some merit, as the distance is signiﬁcantly less for all than bleached sentences the two unbleached sentences. Figure 3: Correlations between different representation methods. Notice how both[CLS]-based methods are less correlated than other methods. that semantically bleached templates need to be used cautiously, and any results stemming from the use of such templates cannot be objectively maintained so long as there does not exist a standardized and validated scheme of selecting such templates. How word representations or embeddings are selected could also be a source of inconsistency in evaluating contextualized language models. Since many techniques use templates, it is natural to use the entire sentence representation as the representation of the word in question, e.g. by mean-pooling over all tokens or using the[CLS]embedding in the case of BERT. Some of the techniques used to represent words in the existing literature are: [CLS]-templates:Seed words with semantically bleached templates where the[CLS]token embedding is used as the representation SEAT (May et al., 2019). [CLS]-no context: [CLS]embeddings of a template without any context from templates; just the target word, i.e. ‘[CLS]X[SEP]’ (May et al., 2019). Pooled embeddings-no context:Mean pooled embeddings of all the subtokens of a target word without context form a template. Pooled embeddings-templates: embeddings of all subtokens of a target word, but with semantically bleached templates. First embedding-templates:The embeddings of the ﬁrst subtoken of a target word in a semantically bleached context. (Tan and Celis, 2019; Kurita et al., 2019). Vulic et al. (2020):This approach averages the pooled embeddings of the ﬁrst four attention layers for the target token in a template without context, as used by Lauscher et al. (2021). Our ﬁrst goal is to investigate whether there are inconsistencies in results from the above-mentioned techniques. We carry out this investigation by conducting correlation analysis of bias scores produced by SEAT on scores from the subset of attribute words. The correlations between these embedding selection methods are visualized in Figure 3, where we see a weak correlation between techniques that select the[CLS]embedding as the representation of the seed word and the other techniques. The weak correlation among the[CLS]techniques themselves i.e. [CLS]-templates and [CLS]-no context conﬁrms the claim that semantically bleached contexts have signiﬁcant inﬂuence on the word representation. The general conclusion is that, using the[CLS]embedding as the representation of seed words may not be an accurate representation since it captures signiﬁcant information from the context which are evidently not as semantically bleached. Our second goal is to explore how other embedding selection methods withstand semantic inﬂuence from the context/templates. Based on the belief that template sentences are not semantically bleached, Tan and Celis (2019) propose using the contextual word representation of only the token of interest instead of[CLS]. Our hypothesis is that using this approach will not completely eliminate the problem posed by using[CLS]or pooling techniques, because the use of the attention mechanism means that context information will still be present. We investigate the effectiveness of Tan and Celis (2019)’s approach approach by replicating the experiment in Figure 2a for their approach. In Figure 2, the results on the correlations between template types show that using only the embeddings of the target word Figure 2b produces more stable results than using the[CLS]embedding as the representation Figure 2a. This indicates that using only the embeddings of the target word produces more stable results across templates and is more resilient to the context which may not be semantically bleached. This observation justiﬁes the approach of Tan and Celis (2019). Figure 4: Correlations between different intrinsic and extrinsic fairness measures. BiasInBios, an extrinsic fairness measure, is mostly negatively correlated. 4.4 Compatibility between metrics In this section, our goal is to (i) see if there is a general relationship between intrinsic and extrinsic bias measures and (ii) how individual bias metrics correlate with extrinsic bias. To do this, we use an extrinsic benchmark dataset, BiasinBios (DeArteaga et al., 2019) and we ﬁnetune and evaluate ﬁve popular language modelson this benchmark. We performed a correlation analysis between the results on BiasInBios and between a set of intrinsic fairness measures from Section 3; the results are presented in Figure 4. We observe that most correlations with the extrinsic BiasInBios measure are negative—which is expected since this measure gives a higher score if more bias is present—but still strongly correlated with some intrinsic measures, like a WEAT variant by Tan and Celis (2019). However, other measures, like CrowS-pairs (Nangia et al., 2020), correlate less with BiasInBios, which can be explained by the issues found by Blodgett et al. (2021). Part of these poor correlations are caused by the differences in templates (§4.2) and representations (§4.3) that we observed, but such differences remain worrisome. We make the source code and required data for our experiments available and also publish a package to bundle the discussed fairness metrics at We mostly compare one of the most frequently studied settings, namely binary gender biases with a special focus on professions. Although most methods should—at least in theory—be extendable to non-binary settings and also work for other biases, not every work considered these extensions explicitly. Future work could therefore contribute by testing and evaluating such extensions. With the availability of fairness metrics, we also risk that such metrics are used as proof or as insurance that the models are unbiased, although most metrics can only be considered indicators of bias at most (Goldfarb-Tarrant et al., 2020). Especially since we found major limitations when comparing different metrics, which demonstrates that current metrics have signiﬁcant limitations. We, therefore, urge practitioners to not rely on these metrics alone, but also to at least consider fairness in downstream tasks. Finally, we did not draw much attention to many other negative impacts of language models that practitioners should consider, e.g. high energy usage or not including all stakeholders when training a language model (Bender et al., 2021). In this paper, we presented an overview of fairness metrics for contextualized language models and we focused on which templates, embeddings and measures these metrics used. We evaluated how these metrics correlate with each other, as well as how parts of these metrics correlate. We found that many aspects of intrinsic fairness metrics are incompatible, for example when choosing different templates, embeddings, or even across metrics. A common motivation is that intrinsic biases can lead to stereotyping and undesirable patterns affecting downstream tasks, but the measures we have now do not correlate with unfair allocations in downstream tasks. Our advice is to use a mix of some intrinsic measures of fairness that don’t use embeddings directly and eliminate one source of variance, for example DisCo or LPBS, in addition to a measure like Tan and Celis (2019) that seems to correlate well with at least some notion of extrinsic bias. However, we also recommend to perform extrinsic fairness evaluations on downstream tasks, since this is where actual resource allocations happen and where intrinsic and extrinsic biases collude. We thank Luc De Raedt for his continued support and Jessa Bekker for her practical advice on writing a survey. Pieter Delobelle was supported by the Research Foundation - Flanders (FWO) under EOS No. 30992574 (VeriLearn). Both Pieter Delobelle and Ewoenam Kwaku Tokpo also received funding from the Flemish Government under the “Onderzoeksprogramma Artiﬁci¨ele Intelligentie (AI) Vlaanderen” programme.