Research in recommender system algorithms is largely driven by results from empirical studies. Newly proposed recommender algorithms are typically compared to established baseline algorithms on a set of recommender tasks. Findings from such studies inﬂuence both the direction of future research and the algorithms for practitioners to adopt, hence it is very important to make sure that the experimental results are reliable. Recent work has highlighted issues in recommender system evaluations (e.g., [6, 25]) where newly proposed algorithms often were unable to outperform older baselines hence leading to unreliable claims about the quality of diﬀerent algorithms. In this work, we carry out a study of the performance of iALS, a matrix factorization algorithm with quadratic loss, on top-n item recommendation benchmarks. We demonstrate that iALS is able to achieve much better performance than previously reported and is competitive with recently developed and often Matrix factorization learned by implicit alternating least squares (iALS) is a popular baseline in recommender system research publications. iALS is known to be one of the most computationally eﬃcient and scalable collaborative ﬁltering methods. However, recent studies suggest that its prediction quality is not competitive with the current state of the art, in particular autoencoders and other item-based collaborative ﬁltering methods. In this work, we revisit the iALS algorithm and present a bag of tricks that we found useful when applying iALS. We revisit four wellstudied benchmarks where iALS was reported to perform poorly and show that with proper tuning, iALS is highly competitive and outperforms any method on at least half of the comparisons. We hope that these high quality results together with iALS’s known scalability spark new interest in applying and further improving this decade old technique. more complex models. In addition to reinforcing the importance of tuning baseline models, we provide detailed guidance on tuning iALS, which we hope to help unlock the power of this algorithm in practice. iALS purpose of top-n item recommendation from implicit feedback. An example for such a recommendation task would be to ﬁnd the best movies (products, songs, ...) for a user given the past movies watched (products bought, songs listened, ...) by this user. iALS has been proposed over a decade ago and serves as one of the most commonly used baselines in the recommender system literature. While iALS is regarded as a simple and computationally eﬃcient algorithm, it is typically no longer considered a top performing method with respect to prediction quality (e.g., [18, 9]). In this work, we revisit four wellstudied item recommendation benchmarks where poor prediction quality was reported for iALS. The benchmarks that we pick have been proposed and studied by multiple research groups [18, 9, 13, 1, 6] and other researchers have used them for evaluating newly proposed algorithms [26, 14, 19, 27]. The poor iALS results have been established and reproduced by multiple groups [18, 13, 9, 6, 1] including a paper focused on reproducibility [1]. However, contrary to these results, we show that iALS can in fact generate high quality results on exactly the same benchmarks using exactly the same evaluation method. Our iALS numbers outperform not only the previously reported numbers for iALS but outperform the reported quality of any other recommender algorithm on at least half of the evaluation metrics. We attribute this surprising contradiction to the diﬃculty of evaluating baselines [25]. Instead of using oﬀ-the-shelf approaches for automated hyperparameter search, we achieve much better results by manual tuning. We give a detailed description on the role of the iALS hyperparameters, and how to tune them. We hope that these insights help both researchers and practitioners to obtain better results for this important algorithm in the future. As our experiments show, properly tuning an existing algorithm can have as much quality gain as inventing novel modeling techniques. Our empirical results also call for rethinking the eﬀectiveness of quadratic loss for ranking problems. It is striking that iALS achieves competitive or better performance than models learned with ranking losses (LambdaNet, WARP, softmax) which reﬂect the top-n recommendation task more closely. These observations suggest that further research eﬀorts are needed to deepen our understanding of loss functions for recommender systems. [12] is an algorithm for learning a matrix factorization model for the This section recaps the problem setting, model, loss and training algorithm for iALS [12]. The iALS algorithm targets the problem of learning an item recommender that is trained from implicit feedback. In this problem setting, items from a set I should be recommended to users u ∈ U. For learning such a recommender, a set of positive user-item pairs S ⊆ U ×I is given. For example, a pair (u, i) ∈ S could express that user u watched movie i, or customer u bought product i. A major diﬃculty of learning from implicit feedback is that the pairs in S are typically positive only and need to be contrasted with all the unobserved pairs (U ×I) \S. For example, the movies that haven’t been watched by a user or the products that haven’t been bought by a customer need to be considered when learning the preferences of a user. A recommender algorithm uses S to learn a scoring function ˆy : U × I → R that assigns a score ˆy(u, i) to each user-item pair (u, i). A common application of the scoring function is to return a ranked list of recommended items for a user u, e.g., sorting all items by ˆy(u, i) and recommending the k highest ranked ones to the user. iALS uses the matrix factorization model for scoring a user-item pair. Each user u is embedded into a d dimensional embedding vector w item i into a an embedding vector h pair is the dot product between their embedding vectors: Its scoring function is The model parameters of matrix factorization are the embedding matrices W and H. These model parameters are learned by minimizing the iALS loss. The iALS loss L(W, H) consists of three components: The ﬁrst component L much the predicted score diﬀers from the observed label, here 1. The second L(W, H) =(ˆy(u, i) − 1)(3) L(W, H) = αˆy(u, i)(4) R(W, H) = λ(kW k+ kHk) = λkwk+khk(5) component L predicted score diﬀers from 0. The third component R is an L2 regularizer that encourages small norms of the embedding vectors. Individually, it is easy to get a loss of 0 for each component, however, jointly they form a meaningful objective. The trade-oﬀ between the three components is controlled by the unobserved weight α trade-oﬀ is crucial for iALS and is explained in detail in Section 3.2. Hu et al. [12] propose to optimize the user embeddings, W , and the item embeddings, H by alternating minimization. This iALS algorithm alternates between optimizing W while ﬁxing H and optimizing H while ﬁxing W . Optimizing one side is equivalent to solving one linear regression problem per user if H is ﬁxed (or one problem per item if W is ﬁxed). A key discovery of the algorithm is that the suﬃcient statistics from the implicit loss L and can be precomputed. With this trick, the overall computational complexity of the iALS algorithm is O(d is independent of the number of all pairs |U||I| which makes iALS a highly scalable algorithm with respect to the number of users and items. A downside of the original iALS algorithm is its quadratic/cubic runtime dependency on the embedding dimension, d which becomes an issue for large embedding dimensions [21]. As a solution to reduce the runtime complexity, past work has proposed to apply coordinate descent variations, where a single element of the embedding vector is optimized at a time, to lower the runtime complexity to O(d|S|+d ical complexity, modern hardware is optimized for vector arithmetic, and scalar treatment has its drawbacks. A compromise between the full vector treatment of the original iALS algorithm and the scalar treatment of the follow-up work is to optimize a subvector of the embedding (a block) at a time [24]. We use this block solver algorithm with a block size of 128 for the experiments on ML20M and MSD; for the experiments on ML1M and Pinterest we use a standard iALS solver. For rating prediction, it has been observed that vanilla stochastic gradient descent (SGD) performs much better than vanilla ALS [30, 15]. Usually, the following update rules are used for training the model parameters of matrix factorization with SGD: where η ∈ R optimize the loss L but instead they optimize with Here, I(u) := {i : (u, i) ∈ S} denotes the set of items in the training set for user u, and U (i) := {i : (u, i) ∈ S} the set of users for item i. Eﬀectively, SGD applies heavier regularization to frequent items and users: a user u gets the regularization weight λ seems to be important empirically – as discovered by [30] for rating prediction. From a probabilistic perspective, this is surprising because the regularization weight λ corresponds to the precision of a Normal prior and typically, priors are not chosen data dependent. We can switch between the original ALS regularizer and the frequency-based regularizer by introducing the exponent ν: If ν = 0, the regularizer is equivalent to the vanilla ALS regularizer and with ν = 1 a frequency-based weighting as in SGD is introduced. Values between 0 and 1 trade-oﬀ the eﬀects of the two regularization schemes. The discussion about regularization so far was motivated by rating prediction and did not contain the loss term, L adding pseudo examples with weight α at the following regularization weighting for iALS: L(W, H) =(hw, hi − y)+ λkwk+ λkhk(9) R(W, H) = λ|I(u)|kwk+|U(i)|khk(12) In this section, we give guidelines on how to choose hyperparameter values. The hyperparameters of iALS are summarized search over these six hyperparameters is likely not eﬀective. The poor previously reported results (see Section 4) that employ automated techniques indicate this. Instead, the search should be guided by an understanding of the algorithm and the hyperparameters. We will describe some useful metrics to plot and then some guidelines on how to explore the search space. While exploring hyperparameters, it is useful to look at several metrics. Obviously, measuring the validation metric (e.g., Recall or NDCG on a holdout set) is useful and will drive the overall exploration. However, the validation metrics are often only loosely connected with the training process and might not reveal why a particular hyperparameter choice does not work. To spot some issues with learning related hyperparameters, we found it useful to plot the training loss L and its components L the hyperparameters are in the wrong region and can be useful in the beginning of the hyperparameter search. The metrics help to guide the hyperparameter search. It is important to understand the hyperparameters and reduce the search space early and then focus on the important hyperparameters. We also don’t recommend tuning a large set of hyperparameters jointly but to explore them iteratively. Number of Training Iterations It is advisable to measure the metrics during training after each iteration. This removes the number of iterations T from the search space – provided that T is large enough. A too large value of T is not a concern with respect to quality, but only with respect to runtime. iALS αUnobserved weight converges usually within a few iterations and a reasonable initial choice could be 16 iterations. Depending on the observed convergence curve, this value can be increased or decreased later. For speeding up exploration, we also found it useful to use a smaller value during initial exploration of the hyperparameter space, and then increase it for the ﬁnal search. For example, using 8 instead of 16 iterations during a broad search will cut the runtime in half. Standard Deviati on for Initialization Usually, the standard deviation for iALS is easy to set and we haven’t observed much sensitivity within a broad range. Instead of setting the hyperparameter σ, it helps to rescale it by the embedding dimension where σ sensitive to large changes in the embedding dimension. The intuition is that this keeps the variance of a random dot product constant, i.e., the variance of predictions at initialization is independent of d. We only observe some sensitivity if the value for the standard deviation is chosen orders of magnitude too small or too large. In this case, it takes a few extra steps for iALS to readjust the norms of the user and item embeddings. This can be spotted easily by plotting L, L immediately. Embedding Dimension The embedding dimension controls the capacity of the model. From our experience, a common reason for suboptimal results with iALS is that the embedding dimension is chosen too small. We usually observe that, with proper regularization, the larger the embedding dimension the better the quality. For example, for the Movielens 20M dataset, we found that 2000 dimensions provide the best results. It may seem that a 2000 dimensional embedding is too expressive for a dataset that has 73 ratings per user on average. And even worse it might lead to overﬁtting. However, empirically, larger dimensions are better and L2 regularization is very eﬀective at preventing overﬁtting. Moreover, other successful models such as VAE are also trained with large embedding dimensions and full rank models such as EASE are also eﬀective. The eﬀectiveness of large embedding dimensions for matrix factorization is also well studied in the rating prediction literature [16, 21, 30]. Computational resources are an important factor when choosing the embedding dimension. A good strategy is to ﬁrst get a rough estimate of good hyperparameter values using a mid-sized embedding dimension, such as d = 128, and then to perform a more reﬁned search using larger embedding dimensions, e.g., doubling the dimension during each reﬁnement of the other hyperparameters until the improvement plateaus. This way, the ﬁrst pass is suﬃciently fast and more time can be spent for the detailed search of the most important parameters: unobserved weight and regularization. is a small constant, such as 0.1. This makes the initialization less Figure 1: When using frequency-scaled regularization, ν, good regularization values λ are in diﬀerent regions for diﬀerent choices of ν (left). A shifted regularization scale λ The middle plot shows scaling to the reference ν Unobserved Weight and Regularization Both unobserved weight α the regularization λ are crucial for iALS carefully. It is advisable to search the unobserved weight together with the regularization because both of them control the trade-oﬀ between the three loss components, L we need both L dicting 1 (if α values of L with large embedding dimensions, we need some regularization, so again the values of R, L The scale of regularization values depends on ν (Eq. (13)) which sets the strength of frequency regularization – see Figure 1 left side for an example. Without frequency regularization, ν = 0, good regularization values are typically larger than 1, for frequency regularization ν = 1, good regularization values are usually smaller than 1. This is because the regularization value λ is scaled by (I(u) + α So, if ν is increased and λ is kept constant, then the regularization eﬀect gets stronger. Having two parameters that interact in this way can be complicated during hyperparameter search because whenever ν is changed, the region of good values for λ changes. Instead it can help to normalize the regularization values to a reference scale and search over the normalized parameter λ where ν for all ν to be in the same region as ν = 0, we would choose ν and Lshouldn’t diﬀer by several orders of magnitude. Similarly, λ = λ(|U(i)| + α|U|)+(|I(u)| + α|I|)P(|U(i)| + α|U|)+P(|I(u)| + α|I|) is the reference scale. For example, if we want the regularization values middle, where good regularization values λ for ν = 0. The right plot in Figure 1 shows a case where ν the reference and good regularization values for ν = 0 are shifted to the region of the frequency regularized version ν = 1. Which reference ν on the practitioner and the application. For example, if there is a comparison to SGD algorithms, choosing the reference as ν a practitioner is more familiar with common ALS algorithms, ν be better. Note that the discussed rescaling of λ does not introduce any new hyperparameters and is for convenience only. Also ν a reference, it does not need any tuning and (unlike ν) has no eﬀects on the solution; it just simpliﬁes the hyperparameter search for regularization values, λ. Unless stated otherwise, in the following we discuss λ of ν After the relationship of the parameters has been described, we want to give some practical advice on the search. When setting the unobserved weight one should consider the degree of matrix sparseness. It is advised that the overall magnitude of the unobserved loss term (L dominate that of the observed loss term (L ally, the unobserved weight is smaller than 1.0 and is decreasing for sparser matrices {1, 0.3, 0.1, 0.03, 0.01, 0.003}. For the regularization, a good starting point is λ∈ {0.1, 0.03, 0.01, 0.003, 0.001, 0.0003}. The regularization becomes especially important for larger embedding dimensions. If the hyperparameter search is performed on too small an embedding dimension, the regularization value found on the small dimension might not be a good one for larger dimensions. A large enough dimension (e.g., d = 128) can be used to get a rough estimate on which area a ﬁner search with a higher dimension should focus on. The suggested parameters are a starting point and need to be reﬁned based on the results. Some additional notes: • The training errors L for diﬀerent hyperparameter settings are not com- • Plotting curves for validation quality vs. log unobserved weight and val- • It also helps not to reﬁne exclusively around the best results, but to look . A good starting point is an exponential grid, for example α∈ parable and should not be used for selecting a model. idation quality vs. log regularization can help to get an idea which parts can be abandoned and where to reﬁne. In general, we would expect to see some ∪-shaped curve (for error) or ∩-shaped curve (for quality) for the parameters – if not, then the boundaries of the search may need to be expanded (see Figure 1 for an example). Also the change in quality between two neighboring hyperparameter values shouldn’t be too abrupt, otherwise more exploration is needed. Table 2: Benchmarks (dataset and evaluation protocol) used in our experiments. Our iALS hyperparameters were tuned on holdout sets. All of the experiments share ν = 1 and σ dimension of d = 192 for a fair comparison to the results from [9, 23]. • Measuring the noise in the validation metrics (i.e., if the same experiment • At some point, large embedding dimensions should be considered. Dou- Frequency Scaled Regularization In the experiments of Section 4, frequency scaled regularization was useful and ν = 1 worked the best. Interestingly, it becomes more important with larger embedding dimensions. For example, even though the quality plateaued with ν = 0, with ν = 1 further increasing the embedding dimension gave additional improvements. We revisit the performance of iALS on four well-studied benchmarks proposed by other authors. Two of them are item recommendation tasks and two are sampled item recommendation tasks. We use exactly the same evaluation protocol (i.e., same splits, metrics) as in the referenced papers. Table 2 summarizes the benchmarks. For all quality results, we repeated the experiment 10 times and report the mean. Our source code is available at https://github.com/ google-research/google-research/tree/master/ials/. In their work about variational autoencoders, Liang et al. [18] have established a set of benchmarks for item recommendation that have been followed by other at the overall behavior of the curves to get a better understanding of how hyperparameters interact on a particular data set. is repeated twice, how much do the numbers diﬀer) is also useful to avoid reading too much into a single experiment. bling the embedding dimensions until no improvement is observed is a good strategy. If no improvement is observed, a reﬁned search of regularization and unobserved weight might be needed. Then one can double the embedding dimension again. Table 3: Quality results on the ML20M benchmark sorted by Recall@20 scores. authors [26, 14, 27, 19] since then. The benchmarks include results for iALS that were produced in [18, 13]. We reinvestigate the results on the Movielens 20M (ML20M) and Million Song Data (MSD) benchmarks. We shortly recap the overall evaluation procedure and refer to [18] and their code The evaluation protocol removes all interactions from 10,000 users (ML20M) and 50,000 users (MSD) from the training set and puts them into a holdout set. At evaluation time, the recommender is given 80% of the interactions of each of the holdout users and is asked to generate recommendations for each user. Each ranked list of recommended items is compared to the remaining 20% of the withheld interactions, then ranking metrics are computed. The benchmark provides two versions of the holdout data: one for validation and one for testing. We use the validation set for hyperparameter tuning and report results for the testing set in Table 3 for ML20M and Table 4 for MSD. The evaluation setup is geared towards algorithms that make recommendations based on a set of items, like autoencoders or item-based CF, because the evaluation users are not available during training time. Matrix factorization with iALS summarizes the user information in a user embedding which is usually learned at training time – this is not possible for the evaluation users in this protocol. We follow the evaluation protocol strictly and do not train on any part of the evaluation users. Instead, at evaluation time, we create the user embedding using its closed form least squares expression. As discussed in [16], matrix factorization can be seen as a item-based CF method where the history embedding (=user embedding) is generated at inference time through the closed form projection. Note that when using a block solver [24], the user embedding does not have a closed form expression; instead, we perform updates for each block and repeat this 8 times. Table 4: Quality results on the MSD benchmark sorted by Recall@20 scores. Tables 3 and 4 summarize the benchmark results. The table highlights the previously reported numbers for iALS and our numbers. Previously, matrix factorization with iALS was found to perform poorly with a considerable gap to the state of the art. Both Mult-VAE [18] and the follow-up work RecVAE [26], H+Vamp (Gated) [14], RaCT [19] and EASE [27] outperform the previously reported iALS results on both datasets. However, we obtain considerably better results for iALS matrix factorization than the previously reported iALS numbers. On the MSD dataset (Table 4) only one method, EASE, outperforms our iALS results. On the ML20M dataset (Table 3), iALS has comparable performance to Mult-VAE. No method consistently outperforms our iALS results over both datasets and all measures, but iALS consistently outperforms CDAE, Mult-DAE, SLIM, WARP. For the remaining methods, iALS is tied with 3 wins out of 6 comparisons. It is interesting to note that most high quality results in Tables 3 and 4 have been obtained by the inventors of the corresponding methods which indicates that knowledge about an algorithm is useful for obtaining good results and that automated hyperparameter search alone might not be suﬃcient. Figure 2 breaks down the performance of iALS by embedding dimension. In this ﬁgure, we use the hyperparameters (see Table 2) that were tuned for the full dimension d = 2048 (ML20M) and d = 8192 (MSD) and apply them to smaller dimensions. For better comparison, we added the best numbers (independent of embedding dimension) previously reported for iALS, EASE and Mult-VAE as reference lines. As can be seen, the embedding dimension has a large inﬂuence on the quality of iALS. Nevertheless, hyperparameter tuning is even more important: even with a small dimension (d = 64 for ML20M and d = 256 for MSD) the previously reported results for iALS are already outperformed. A dimension of d = 1024 outperforms Mult-VAE on the MSD dataset, and a dimension of d = 512 outperforms EASE on ML20M. The gain of large embedding dimensions slowly levels oﬀ on ML20M around d = 1024 and MSD around d = 8192. The MSD benchmark is interesting because the Figure 2: Our iALS benchmark results with a varying embedding dimension, d. For comparison, the plots contain also the previous iALS results from [18], EASE [27] and Mult-VAE [18]. The capacity for these models is not varied in the plot and the numbers represent the best values reported in previous work. We vary the embedding dimension for our iALS results to get a better understanding of the importance of this hyperparameter. best performing methods, EASE and iALS are much simpler than the advanced autoencoder methods that perform the best on ML20M. However, both EASE and iALS have a very large model capacity in the item representation. EASE uses a dense item to item matrix with |I| found that a very large dimension d = 8192 is helpful. That might indicate that the MSD dataset has structure that is diﬀerent from ML20M, for example it might have a very high rank. Finally, the comparison so far ignored runtime. One training epoch of a multithreaded (single-machine) C++ implementation of iALS (using the block solver from [24]) took about 1 minute for d = 2048 on ML20M and 40 minutes for d = 8192 on MSD. And for smaller dimensions: 30 seconds for d = 1024 on ML20M and 1 minute 30 seconds for d = 1024 on MSD. While we ran the experiments for 16 epochs, i.e., the total runtime is 16 times higher, the results are almost converged much earlier. For example for MSD and d = 8192 the quality results after four epochs are Recall@20=0.305, Recall@50=0.412, NDCG@100=0.363. It is up to the application if it is worth spending more resources to train longer. Table 5: Quality results on the Movielens 1M and Pinterest benchmarks [9] sorted by HR@10 on ML1M. The previously reported numbers for iALS (and its eALS variation), and our iALS numbers are in bold. The best number is underlined. We also investigate the performance of iALS on the sampled item recommendation benchmarks from [9] which have been used in multiple publications, including [23, 18, 6, 1]. The benchmark contains two datasets, Pinterest [7] and an implicit version of Movielens 1M (ML1M) [8]. The evaluation protocol removes one item from each user’s history and holds it out for evaluation. At evaluation time, the recommender is asked to rank a set of 101 items, where 100 of them are random items and one is the holdout item. The evaluation metrics measure at which position the recommender places the withheld items. As discussed in [17], this evaluation task is less focused on the top items than the measured metrics indicate. We follow the same procedure as in [23] for hyperparameter tuning by holding out data from training for tuning purposes. Table 5 summarizes the results. The original comparison in [9] reports results for eALS [10] which is a coordinate descent variation of iALS. Follow up work [6] provided results for iALS which have been reproduced in [1]. The previously reported performance of eALS and iALS is poor and not competitive on both measures and both datasets. However, with well tuned hyperparameters, we could achieve a high quality on all metrics with iALS. The results are very close to the ones for a well tuned SGD optimized matrix factorization [23] – which is expected as both are matrix factorization models with a comparable loss. The well-tuned iALS is better than NCF on three metrics and tied on one. It is also better than Mult-DAE [18] on three metrics and worse on one. Furthermore it outperforms EASE [27] and SLIM [20] on all metrics. Note that we limited the embedding dimension for iALS to d = 192 to be comparable to the previously obtained results for NCF and SGD matrix factorization. We also produced results for smaller dimensions and d = 64 without further hyperparameter tuning works reasonably well: for ML1M HR@10=0.722 and NDCG@10=0.445 and for Pinterest HR@10=0.892, NDCG@10=0.573. This work reinvestigated matrix factorization with the iALS algorithm and discussed techniques to obtain high quality models. On four well-studied item recommendation benchmarks, where iALS was reported to perform poorly, we found that it can actually achieve very competitive results when the hyperparameters are well tuned. In particular, none of the recently proposed methods consistently outperforms iALS. On the contrary, iALS outperforms any method on at least half of the metrics and datasets: iALS improves over a neural autoencoder in 6 out of 10 comparisons and over EASE in 7 out of 10. These benchmarks focus on prediction quality but ignore other aspects such as training time, scalability to large item catalogs or serving of recommendations. iALS is known to excel in these dimensions: (i) It is a second order method with convergence in a few epochs. (ii) It uses the Gramian trick that solves the issue of |U | · |I| negative pairs. (iii) It is trivially parallelizable as it solves |I| independent problems. (iv) It learns an embedding for each item making the model size linear in the number of items. (v) It uses a dot product model that allows for eﬃcient querying of the top-n scoring items for a user. iALS also has some challenges: (i) Its loss is less aligned with ranking metrics but surprisingly, it achieves high ranking metrics on benchmarks, on par with models that optimize ranking losses such as lambdarank and softmax. Also EASE and SLIM that share the quadratic loss with iALS do not suﬀer from the quadratic loss on these benchmarks. (ii) In its simplest form discussed in this paper, iALS learns a matrix factorization model, making it less ﬂexible for richer problems with extra features. However, iALS has been extended for more complex models as well [11, 2, 22]. (iii) Finally, matrix factorization requires recomputing the user embedding whenever a user provides new feedback. Bag of item models like SLIM, EASE or autoencoders do not require retraining but can just make inference using the modiﬁed input. Nevertheless, the user embedding of iALS has a closed form expression and this can be seen as the inference step of iALS. Instead of passing the history through an encoder, in iALS a diﬀerent computation (the solve step) is performed. We hope that the encouraging benchmark results for iALS spark new interest in this old technique. Other models, such as autoencoders or SLIM, beneﬁted from a growing interest in item-based collaborative ﬁltering that resulted in improved versions such as Mult-VAE, RecVAE or EASE. The basic iALS model might have similar potential for improvements. Besides academia, iALS should be considered as a strong option for practical applications. iALS has very appealing properties in terms of runtime, scalability and low top-n querying costs, and as this study argues, it also performs well on benchmarks in terms of quality. Finally, this paper is another example for the diﬃculty of tuning machine learning models [25]. Converging to reliable numbers is a process that takes time and needs a community eﬀort. Over the long term, shared benchmarks like the ones established in the VAE paper [18] and adopted by other researchers [27, 14, 19, 26] are a way to make progress towards reliable numbers. Until then, it should be understood that both the benchmark results that we achieve with iALS and the ones from the other methods might be further improved in the future.