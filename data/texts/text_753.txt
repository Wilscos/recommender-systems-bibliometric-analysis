Deep Neural Networks (DNNs) are used in a wide variety of applications. However, as in any software application, DNN-based apps are aicted with bugs. Previous work observed that DNN bug x patterns are dierent from traditional bug x patterns. Furthermore, those buggy models are non-trivial to diagnose and x due to inexplicit errors with several options to x them. To support developers in locating and xing bugs, we propose DeepDiagnosis, a novel debugging approach that localizes the faults, reports error symptoms and suggests xes for DNN programs. In the rst phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sucient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models – 53 real-world from GitHub and Stack Overow, and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classication ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for x whereas DeepLocalize can only report faults that lead to numerical errors during training. DeepDiagnosis manifests the best capabilities of fault detection, bug localization, and symptoms identication when compared to other approaches. • Computing methodologies → Neural networks;• Software and its engineering → Software testing and debugging. deep neural networks, fault location, debugging, program analysis, deep learning bugs ACM Reference Format: Mohammad Wardat, Breno Dantas Cruz, Wei Le, and Hridesh Rajan. 2022. DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs. In Proceedings of The 44th International Conference on Software Engineering (ICSE 2022) . ACM, New York, NY, USA, 11 pages. Deep Neural Networks (DNNs) are becoming increasingly popular due to their successful applications in many areas, such as healthcare [23,30], transportation [38], and entertainment [17]. But, the intrinsic complexity of deep learning apps requires that developers build DNNs within their software systems to facilitate integration and development with other applications. The construction of such systems requires popular Deep Learning libraries [14, 28]. Despite the increasing popularity and many successes for using Deep Learning libraries and frameworks, DNN applications still suer from reliability issues [21,22,42]. These faults are harder to detect and debug when compared to traditional software systems, as the bugs are often obfuscated within the DNNs. Therefore, it is important and necessary to diagnose their faults, and provide actionable xes. To that end, software engineering research has recently focused on improving the reliability of DNN-based software. For instance, there have been studies on characterizing DNN bugs [21,22,42], on testing frameworks for deep learning [37], on debugging deep learning using dierential analysis [26], and xing DNNs [11,41,43]. There are also frameworks and tools for inspecting and detecting unexpected behavior in DNNs. However, they require that specialists verify the visualization, which is only available upon completing the training phase [3–5, 27, 34]. Due to the complexity of using existing frameworks to debug and localize faults in deep learning software, recent SE research has introduced techniques for automatically localizing bugs [39, 44]. DeepLocalize performs dynamic analysis during training to localize bugs by monitoring values produced at the intermediate nodes of the DNNs [39]. If there is a numerical error, then this approach traces that back to the faulty layer. DEBAR [44] is a static analysis tool that detects numerical errors in the DNNs. While both approaches have signicantly advanced the state of the art in debugging DNNs, they do not detect bugs that manifest as trends of values (e.g. vanishing gradient, exploding gradient, accuracy not increasing) and do not oer possible xes. We propose DeepDiagnosis (DD), an approach for localizing faults, reporting error symptoms, diagnosing problems, and providing suggestions to x structural bugs in DNNs. Our approach introduces three new symptoms of structural bugs and denes new rules to map fault location to its root cause in DNN programs. We implemented DD as a dynamic analysis tool and compared and contrasted it against state-of-the-art approaches. DD outperforms UMLAUT [33] and DeepLocalize [39] in terms of eciency and AUTOTRAINER in terms of performance [43]. For example, assume the unchanged weight symptom, which occurs when the weights in the network the output are not changing for several iterations. In that case, DD would identify the root cause as that the learning rate is too low or that the optimizer is incorrect and recommend a x. In summary, this paper makes the following contributions: •We study dierent types of symptoms and propose a dynamic analysis for detecting errors and recommending xes. •We introduced DeepDiagnosis (DD) the reference implementation of our approach. •We evaluated DD against SoTA. We found that DD is more ecient than UMLAUT [33] and DeepLocalize [39]. Also, DD has better performance than AUTOTRAINER [43]. •We provide a set of 444 models that practitioners can use to evaluate their fault localization approaches. •We make DD available, its source code, evaluation results, and the problem solutions for 444 buggy models at [6]. The rest of the paper is organized as follows: §2 describes the motivation of our approach. §3 describes our dynamic failure symptoms detection algorithm. §4 describes the evaluation of our approach compared with prior works. §5 discusses the threats to validity. §6 discusses related works, and §7 concludes and discusses future work. In this section, we motivate our work by providing an example to illustrate the complexity of localizing faults and reporting their symptoms in DNN programs. Consider the code snippet in Listing 1 from Stack Overow [2] with an example of a DNN. This model showed erratic behavior during training and returns bad results. At line 1, the developer constructed a sequential model and added a dense input layer at line 2 with the activation functionsreluspecied at line 3. Then the developer added a dropout layer at lines 4 and 7. Lines 5 and 8 are dense hidden layers with the activation functionsreluandsoftmax specied at lines 6 and 9, respectively. The developer then compiled the model at line 10 and trained it using thefit()function at line 11. When compiling, the developer must specify additional properties, such as loss function and optimizer. In this example, the developer used as lossbinary_crossentropyand optimizer RMSprop()at line 10. Finally, at line 11, the developer species the training data, batch_size, epoch, and validation_data. The developer noticed that the DNN program was providing bad accuracy and could not diagnose the problem nor x it (Stack Overow post [2]) while following the Keras MNIST example [14]. The main issue with the code in Listing 1 is that it handles a binary classication problem, and therefore it shouldnotuse the activation functionsoftmaxin line 9. As thesoftmaxworks for multi-class classications problems. Instead, it should usesigmoid, as it is the best suited for binary classication and will provide the best accuracy for the task. The current state-of-the-art for DNN fault localization is limited in terms of speed, accuracy, and eciency. Table 1 summarizes the analysis results from three tools (DeepLocalize [39], UMLUAT [33], AUTOTRAINER [43]) and our approach DeepDiagnosis to diagnose the DNN model in Listing 1. To apply UMLUAT for the above example, we made semantic changes that were validated by the authors [33]. After 104.65 seconds, the training was terminated, with UMLAUT not reporting any problems. To apply DeepLocalize, we followed the instructions in the GitHub repository [7]. DeepLocalize prints the following message after 2.14 seconds: “Layer 7: Numerical Error in delta Weights.” This message indicates that there is a numerical error in the backpropagation stage during training. Indicating fault location, but it does not help developers to x the problem. To apply AUTOTRAINER, we followed the instructions in the GitHub repository [8]. After performing the training phase, AUTOTRAINER did not solve the problem and took 495.83 seconds. Specically, AUTOTRAINER detects a Dying ReLU symptom, but it does not provide the fault location – whether if it is in line 3 or 6. AUTOTRAINER tries to automatically x the issue by trying dierent strategies (i.e., substituting activation functions, adding batch normalization layer, and substituting initializer), which, unfortunately, are unsuccessful. Our approach DeepDiagnosis correctly reports the fault location and its symptoms after 35.03 seconds. Also, it provides a suggestion to perform a x in the form of a message. Specically, DeepDiagnosis reports that the bug is located in the backpropagation stage of layer 7 at line 8. Also, it prints out a numerical message: “Error in delta Weights”, which indicates the type of the symptom. It also reports that the root cause is the activation function in layer 8 at line 9 (softmax). Finally, it answers the developer’s question – there is indeed a problem with the activation function in the last layer and not in the training dataset. In this section, we provide an overview of our approach for fault localization. We provide descriptions of failure symptoms and their root causes. Also, we describe the process of mapping symptoms to their root causes. Our approach monitors the key values during training, like weights and gradients. During training, it analyzes the recorded value to detect symptoms and determine whether a training problem exists. If a symptom is detected, our approach invokes a Decision Tree to diagnose/repair information based on a set of predetermined rules. Otherwise, the training will terminate with the trained model and reported the model is correct. Figure 1 shows an overview of our approach for fault localization, DeepDiagnosis, and for suggesting locations x. DD starts by receiving as input the initial model architecture with a training dataset and passing our callback method as a parameter to thefit()method (Figure 1 left component). This callback allows capturing and recording the key values (i.e., weight, gradient, etc.) during feed-forward and backward propagation stages (Figure 1 middle component). Then DD applies a dynamic detector during training to report dierent symptoms at dierent stages based on error conditions (see Section 3.2 for more details). If DD detects a symptom, it further analyzes the recorded key values to determine the input model’s probable location for the x (Figure 1 right component). Finally, DD reports the symptom type, which layers and stage the symptom was detected, and suggests a location x. Our goal is to detect failure symptoms as soon as possible during development. So that if the model is incorrect, developers would not have to wait until the end of the training to nd that model has low accuracy, thus wasting computational resources. To that end, we collected 8 types of failure symptoms and their root causes from previous work in the AI research community [19,20,29,35]. We provide more details of each of the symptoms and their root causes below. 3.2.1Symptom #1 Dead Node. The Dead Node symptom takes place when most of a neural network is inactive. For example, assume that most of the neurons of a DNN are using the ReLU activation function, which returns zero when receiving any negative input. If the majority of the neurons receive negative values (e.g., due to a high learning rate), they would become inactive and incapable of discriminating the input. The DNN would end up with poor performance [43]. To identify this symptom, we compute the percentage of inactive neurons per layer. If the majority of the neural network is inactive, then we call it Dead Node. Root Causes:This problem is likely to occur when [12]: (1) learning rate is too high/low. (2) there is a large negative bias. (3) improper weight or bias initialization. 3.2.2Symptom #2 Saturated Activation. The Saturated Activation symptom takes place when the input to the logistic activation function (e.g., tanh or sigmoid) reached either a very large or a very small value [19]. At the saturated point, the function results would equal zero or be close to zero and, thus, leading to no weight updates. Our experiments show [6] that the behavior of sigmoid and tanh have a minimum saturated point at x=-5 and a maximum saturated point at x=5. Previous work showed that the saturated function aects the network’s performance and makes the network dicult to train [19, 40]. Root Causes:This problem is likely to occur when [15]: (1) the input data are too large or too small; (2) improper weight or bias initialization; (3) learning rate is too high or too small. 3.2.3Symptom #3 Exploding Tensor:The Exploding Tensor symptom takes place when the tensors’ values become too large, leading to numerical errors in a feed-forward stage. For example, if the weight or output layer grows exponentially more than expected, becoming either innite or NaN (not a number). Eventually, this problem causes a numerical error, making it impossible for the model to learn. Root Causes:This problem is likely to occur when [16,24]:(1) the learning rate is too large; (2) there exist improper weight or bias initialization, or improper input data. 3.2.4Symptom #4 Accuracy Not Increasing&Symptom #5 Loss Not Decreasing. Both symptoms Accuracy Not Increasing and Loss Not Decreasing are very similar. The Accuracy Not Increasing symptom takes place when the accuracy of a target model is not increasing for N steps, but instead, it is decreasing or uctuating during training. While for the Loss Not Decreasing symptom, the loss metric is the one that is not decreasing for N steps but is uctuating. These behaviors indicate that the network will not achieve high performance. These symptoms are often caused by the incorrect selection of DNN hyperparameters [32], such as loss function, activation function for the last layer, learning rate, optimizer, or batch size. Root Causes:This problem is likely to occur when [16,24]: (1) there exist improper training data; (2) the number of layers is too This table shows procedures descriptions from [1, 10, 16, 32, 43]. T|F indicates that the procedure returns True| False respectively. This table is showing all the functionality of the procedures. T|F indicates the procedure return True Low| High respectively. We borrowed these methods from existing literature [1, 10, 16, 32, 43] large/small; and (3) the learning rate is very high/low; and (4) there exist incorrect activation functions. 3.2.5Symptom #6 Unchanged Weight. The Unchanged Weight symptom takes place when the DNN weights do not have a noticeable inuence on the output layers. This behavior leads to unchanging parameters and network stacks, which further prevents the model from learning [15, 39]. Root Causes:This problem is likely to occur when [15,39]: (1) learning rate is very low; (2) the optimizer is incorrect; (3) there exist incorrect weights initialization; and (4) there exists incorrect loss/activation at the last layer. 3.2.6Symptom #7 Exploding Gradient. This problem occurs during the back-propagation stage. In it, gradients are growing exponentially from the last layer to the input layer, which leads to non-nite values, either innite or NaN (not a number). This issue makes learning unstable and sometimes even impossible. Consequently, updating the weights becomes very hard, and the training model ends up with a high loss or very low accuracy values. Root Causes:This problem is likely to occur when [16,24]: (1) the learning rate is very high; (2) there is an improper weight or bias initialization; (3) there are improper data input; and (4) the batch size is very large. 3.2.7Symptom #8 Vanishing Gradient. The Vanishing Gradient problem occurs during the backward stage. When computing the gradient of the loss concerning weights using partial derivatives, the value of the gradient turns out to be so small or drops to zero. The problem causes major diculty if it reaches the input layer, which will prevent the weight from changing its value during training. Since the gradients control how much the network learns during training, the neural network will end up without contributing to the prediction task or leading to poor performance [36, 43]. Root Causes:This problem is likely to occur when [25]: (1) the network has too many layers; (2) the learning rate is low; (3) the hidden layers improperly used Tanh or Sigmoid; and (4) there exists the incorrect weight initialization problem. In Table 2 from Method S1 to S8, we describe the failure symptoms discussed in Section 3.2, using its name, input/output, and the detection procedure. Algorithm 1 shows an example of a dynamic analysis procedure, which DeepDiagnosis uses to detect failure symptoms during training (Table 2 Description). Also, the Algorithm 1 reports failure locations, such as in which layer and phases (i.e., feed-forward and backward propagation). In case a failure is detected, the algorithm will trigger the Mapping() procedure to identify the location in the original DNN source code. By doing so, it will localize the bug and determine the optimal x. At line 1, Algorithm 1 iterates over the training epochs, with the training dataset divided into batches. Line 3 shows the division of the training dataset into a mini-batch. On lines 2-28, the algorithm runs one batch of the training dataset before updating the internal model parameters. The neural network can be divided into two stages: First, the forward stage, in which the algorithm performs dynamic analysis and symptom detection, including Numerical Error, Dead node, Saturated Activation, and Out of Range, at lines 4-12. Second, the backward stage, in which the algorithm performs dynamic analysis to detect additional symptoms, such as Numerical Error, Vanishing Gradient, and Unchanged weight at lines 23-28. 3.3.1 Feed-forward stage. At lines 5 & 6 of the Algorithm 1, it computes the output of a feed-forward before and after applying the activation function. At line 7, it invokes the ExplodingTensor() procedure (S1 in Table 2) to determine if the output contains a numerical error obtained from the output value before/after the applying activation function, respectively. If there is an error, the algorithm reports the NS message as shown in Table 5. Next, it invokes the Mapping() procedure from the decision tree in Figure 2 by providing the symptom (NS), location, stage (FW), and layer (L). The decision tree returns the best actionable x for the model (see Section 3.4 for more details). At line 8, the Algorithm 1 invokes the UnchangeWeight() (S2 in Table 2) procedure to detect whether the output before/ after applying the activation function is no longer changing across steps. If the procedure indicates that the value does not change for N iterations, we follow [39] and set N=5. The UnchangeWeight() procedure can be applied either to the output before/after the activation function. The algorithm reports the message UCS, as shown in Table 5. At line 9, the Algorithm invokes the SaturatedActivation () procedure (S3 in Table 2) for the layer that has a logistic activation function (i.e., tanhorsigmoid) to determine if the layer is becoming saturated. This procedure takes two arguments, the value before applying the activation function (V_1) and the name of the activation function (V_2.name). If the procedure determines that the layer is saturated, the algorithm reports the message SAS as shown in Table 5. At line 10, the Algorithm 1 invokes the DeadNode() procedure (S4 in Table 2) to check the layers that use the Rectied Linear Unit (ReLU) activation function. The goal is to determine if the output after applying the activation function has dropped below a threshold [43]. This procedure is invoked only after applying the activation function. The algorithm reports the message DNS as shown in Table 5 when the error is detected. Similarly, at line 11, it invokes the OutofRange() procedure (S5 in Table 2) in the last layer. The goal is to determine if the developer has chosen the correct activation function. The algorithm reports the message ORS as shown in Table 5 if the error is detected. In lines 13 & 15 the algorithm interprets and validates how well the model is doing by computing the loss and accuracy metrics, Table 5: Abbreviation of Failure Symptoms respectively. Then it determines if there is any numerical error in those metrics at lines 14 & 16, respectively. The algorithm invokes LossNotDecreasing() and AccuracyNotIncreasing() (S6 & S7 in Table 2) to check if the loss or the accuracy has not changed for a long time. In both cases, the algorithm reports a message LNDS or ANIS as shown in Table 5. 3.3.2 Back propagation stage. During this stage, the Algorithm 1 computes the gradient of loss functionΔWeight for the weight by chain rules in each iteration. At line 24, the algorithm invokes Backward() to apply stochastic gradient descent, and this function returns the Weight andΔWeight in each iteration. At line 25, the algorithm invokes the VanishingGradient() procedure (S8 in Table 2) and passesΔWeight to check if the gradients become extremely small or close to being zero. In the same way, at line 26, the algorithm can determine if there is a numerical error in the Weight or the gradient weight in each layer by invoking the ExplodingTensor() procedure (S1 in Table 2). The backpropagation algorithm works if the Weight is updated using the gradient method and the loss value keeps reducing, to check if the backpropagation works eectively. In the backward propagation, we also invoke the UnchangeWeight() procedure (S2 in Table 2) to detect whether the weight orΔWeight is no longer changing across steps. If any procedure decides that there is an issue, then the algorithm will return a message to indicate the type of symptom as shown in Table 5, L represents a faulty layer number. Then the algorithm invokes Mapping() and passes the symptom, location, and layer to nd the best actionable change to x the issue in the model. Finally, if the algorithm did not detect any type of symptom, it will terminate after nishing the training at line 29 and print a message indicating that there is no issue in the model (CM). Algorithm 1: Failure Symptoms Detection input : Training data (input, label), DNN program output :Failure symptoms and locations (layers, iterations, epoch) Decision Tree: The main goal of this step is to mitigate manual eort and reduce the time for diagnosing and xing bugs. To that end, the Mapping() procedure in Algorithm 1 provides x suggestions based on the detected failure symptoms. Figure 2 shows a representation of the Decision Tree which the Mapping() procedure uses to provide a x recommendation. The Decision Tree consists of 24 rules, which corresponds to decision paths. Each rule provides a mapping from failure symptoms and detected locations to actionable changes. The tree denes a binary classication rule which maps instances in the format problem (Symptom, Location, Layer) into one of seven classes of changes (Table 4). The root node represents the problem, orange nodes the symptoms, blue nodes the locations, gray nodes the layer type, green nodes, the conditions, and red nodes the actionable changes. Table 3 shows the methodsData(),Weight()andLearn(), which are used to compute the conditions. Each Decision Tree instance maps a path from the root to one of the leaves. For example, assume that a developer wants to check the code in Listing 1. To that end, the developers can use the Algorithm 1 to verify the model. The algorithm invokes the Mapping() procedure (line 26) by passing the symptom NS, location, stage BW (backward), and layer (7). This procedure traverses the path under the NS node in the Decision Tree (Figure 2). Since the problem occurred in the BW stage, the algorithm takes the right path to satisfy the condition. Then, it veries the layer type (7). Since it nds an issue in the layer, the procedure returns the message MSG2 – Change the activation function (Table 4). Heuristics: We developed a set of heuristics based on the root causes (see Section 3.2). There are three main root causes: (1) Data Preparation; (2) Parameter Tuning; and (3) Model Architecture. For Data Preparation, the algorithm checks if the data is normalized (C1 - ImproperData() in Table 3). For Parameter Tuning, our approach checks if the hyperparameters (such as learning rate) were assigned correctly. Also, to check if the weights were initialized correctly, the algorithm invokes the WeightInitialization(). The TuneLearn() procedure veries whether the learning rate is very high or very low (C2, and C3 in Table 3, respectively). For model architecture, the algorithm searches for a relation between the location and the stage of the symptom. It performs this step to pinpoint which APIs are being misused in the model (e.g., loss, activation function). We collected the root causes for each symptom from previous work [19,20,29,35] (more details in Section 3.2). To arrive at a possible x for a given symptom, we choose the most frequent root cause. We follow this approach as our ndings show that changes in the order we check for the possible root causes do not aect the results, only on the total time to arrive at a solution. For example, assume that a model has the Dead Node symptom. In terms of frequency, improper data tends to happen more often than weight and learning rate. If the three root causes are correct, our approach checks the model architecture, which is the least common in this case. Thus arriving at an improper activation function as the root cause of this symptom. In the evaluation, we answer the following research questions: •RQ1 (Validation): Can our technique localize the faults and report the symptoms in Deep Learning programs eectively? •RQ2 (Comparison): How does our technique for fault localization compared to existing methodologies in terms of time and eectiveness? •RQ3 (Limitation): In which cases does our technique fail to localize the faults and report the symptoms? •RQ4 (Ablation): To what extent does each type of symptom we developed contribute to the overall performance of DeepDiagnosis? 4.1.1 Implementation. We implemented DeepDiagnosis on top of Keras 2.2.0 [14] and TensorFlow 2.1.0 [28]. Also, we implemented Algorithm 1 by overriding the method called (on_epoch_end(epoch, logs=None). For the Decision Tree in Figure 2, we implemented it as a decision rule consisting of a set of conditional statements. The overridden method invokes the decision tree once upon detecting a symptom. Then it passes the symptom type as a parameter for the decision tree. We conducted all the experiments on a computer with a 4 GHz Quad-Core Intel Core i7 processor and 32 GB 1867 MHz DDR3 GB of RAM running the 64-bit iMac version 4.11. 4.1.2 Benchmark. In total, we collected 548 models from prior work [7,33,43]. From these, we removed 104 RNN models, as our approach does not support them. The resulting 444 models are composed of 53, which are known tohave bugsfrom [7,33]. We refer to these 53 models SGS benchmark as they come from StackOverow,GitHub, andSchoop et al. [33]. Also, the 391 models from [43] are in the compiled *.h5 format. The remaining 391 models are divided into two sets. In particular, the rst with 188 correct models –withoutany known bugs – and the second with 203 buggy models – with bugs. Most machine learning developers share the source code or the trained weights of their models in *.h5 format. To allow others to improve the understanding of how a model operates and inspect it with new data, we implemented the Extractor tool [6]. It extracts the DNN source code from a *.h5 le. The Extractor follows three steps to generate the Keras source code: rst, it saves the model’s layer information to a JSON le. Then, it generates the Abstract Syntax Tree (AST) from the JSON le. Finally, it converts the AST to the source code. To build the ground truth for the SGS benchmark, we manually reviewed all models and their respective answers, from Stack Overow, and commits, from GitHub. We perform this verication process to determine the exact bug location and its root causes. For the remaining 391 models - 203buggymodels and 188not buggy models, we used our Extractor to generate the source code for each model before/after performing a x; we used the diib [18] module to generate the di le from the xed model. We use the di to locate the changes in the model, thus locating the root causes and the actual location of its corresponding x. We consider a model successfully repaired if its accuracy has improved after the x. 4.1.3 Results Representation. Table 6 shows the summarized evaluation results of the following approaches: UMLUAT [33], DeepLocalize [39], AUTOTRAINER [43], and our approach DeepDiagnosis. Please refer to the reproducibility repository [6] for the complete table. The rst column shows the source of the buggy model. The second column lists the model ID. The third column provided the Stack Overow post # and the model name from GitHub repositories, collected by Wardat et al. [39], and also the name of the model introduced by Schoop et al. [8] respectively. To compare our approach with the results generated from previous approaches, we reported the results in the following columns (from left to right): Time, Identify Bug (IB), Fault Localization (FL), Failure Symptom (FS), and Location Fix (LF), and Ablation (AB). Time, in seconds, reports how long each tool takes to report its results. The columns Identify Bug (IB) and Fault Localization (FL) show whether the approach successfully identies the bug and the fault location. Failure Symptom (FS) and Location Fix (LF) columns show whether the tool correctly reports a symptom and an actionable change (model repair x). Finally, the Ablation (AB) column shows which of the procedures listed in Table 2 detects the failure symptoms. Under each approach, the “Yes” and “No” status indicates whether it has successfully reported the target problem. Also, the “—” status denotes if the approach does not yet support the target problem. Lastly, we use method ID in Table 2 to indicate which procedure is used to detect the failure symptom. Table 7 summarizes the analysis results from four approaches using benchmarks collected from three dierent sources [33,39, 43]. The second column (Total) lists the total number of buggy models for each dataset. To compare our approach with previous approaches, we reported Time, in seconds, the average time for each tool takes to report its results for each dataset. The columns Identify Bug (IB) shows how many each approach successfully identies the bug from each dataset. Our approach is capable of handling eight types of symptoms with dierent types of datasets using dierent types of model architectures. Table 8 shows the number of symptoms detected from dierent types of datasets. Table 6 and 7 show the evaluation results for RQ1 and RQ2. DeepDiagnosis(DD) has correctly identied 46 out of 53 buggy models from the SGS benchmark. DD correctly reported the fault location for 34 models and the failure symptoms for 37 models. Also, DD correctly identied the actionable changes for 28 out 53 faulty models. Lastly, DD identied 138 out of the 203 buggy models from the AUTOTRAINER dataset, correctly reporting fault location, failure symptoms, and actionable changes. DeepLocalize(DL) [39] identied 45 out of the 53 models from the SGS benchmark and indicated fault locations for 26. It reported symptoms for only 23 models, but it cannot provide any suggestions to x these faults. Regarding the AUTOTRAINER dataset, DL identied 191 out of the 203 buggy models and correctly reported their fault location. However, DL did not provide any suggestions for xing those models. Lastly, DL can only detect bugs related to numerical errors. AUTOTRAINER(AT) [43] For the 53 models (SGS benchmark), AT identied 24 buggy models. Out of these, AT successfully reported symptoms for only 8. AT was only able to repair 16 models. DD can handle more varieties of semantically related errors than AT, as shown in Table 6. Please refer to [43] for AT’s evaluation results while analyzing its dataset. UMLUAT(UM) [33] identied 26 buggy models out of the 53 from the SGS benchmark and found the fault locations for 3. Also, UM reported the symptoms for 17 models and provided the location x for 15 out of 53. UM correctly identied models and reported possible x solutions to problems from 72 out of 203 buggy models of the AUTOTRAINER dataset. UM only supports classication problems, while DD supports additional types, such as regression and classication. Figure 3: Comparison between UMLUAT (UM) VS DeepLocalize (DL) VS AUTOTRAINER (AT) VS DeepDiagnosis (DD) in terms of seconds To evaluate the approaches’ overall performance, we collected their total execution time while analyzing the benchmarks. Figure 3 shows the results. UM, DL, AT, and DD require on average 46.16, 421.39, 771.56, and 103.74 seconds respectively for all the Stack Overow (SOF) benchmarks. For the GitHub (GH) benchmark, the four approaches require on average 46.16, 2613.60, 148.41, and 137.78 seconds respectively. For the Schoop et al.’s [33] benchmark, the four approaches take on average 193.52, 93.17, 3491.32, and 1020.80 seconds respectively. For the AUTOTRAINER dataset, the four approaches require on average 4159.25, 4157.36, 170156.70, and 74408.07 seconds respectively to complete their analysis. Lastly, the overall average time for UM, DL, AT, and DD, for all benchmarks is 2972.23, 8388.21, 106490.05, and 44914.17 seconds respectively. DD’ runtime overhead is mainly due to its online dynamic analysis. DD runs its dynamic analysis on the internal parameters of the neural networks, such as the changes of weights and gradients, during the training phase. DD is the most ecient for Stack Overow and Schoop et al.’s model, and is slower than UM on the GitHub models. The reason is that DD collects more information than UM during training and checks additional types of error conditions. DD is faster than AT on all benchmarks except for the Blob and Circle datasets. That is because AT checks the target model after nishing the training phase. DD requires additional time because it validates the model at the end of each epoch during training, and the number of epochs for these models are between 200 to 500. Out of 52 programs, our technique failed to identify faults in 6 and localize faults in 18. DD failed to report symptoms for 15 programs and failed to provide the location of x for 24 (Tables 2 and 6). In the following, we provide a few examples failed fault localization. Our technique does not yet support model withfit_generator() instead offit()function.fit_generator()is used for processing a large training dataset that is unable to load into the memory [13]. In the future, we plan to cover more APIs (such asfit_generator()). Both #47 (B3 (C10)), and #53 (B3 (C10)) programs are related to checking validation accuracy [33]. The model splits the train data into training and validation data, and then provide the validation data by passing validation_data=(x_val, y_val) into the t() method. The buggy model reported high accuracy for the validation dataset. There may exist an overlap between training data and validation data. But our approach would not indicate any symptom, as it does not support problems related to training and validation. Both #43 (A2 (C10)), and #49 (A2 (C10)) programs are related to the dropout rate in the Dropout layer [33]. The idea of the dropout is to remove certain percentage of neurons during iterations to prevent the overtting. The buggy model sets a high dropout rate = 0.8 which is more than the acceptable rate 50%. Our approach is not able to provide a correct suggestion to x the model. In our future work, we plan to investigate more hyperparameters such as the batch size, epoch, and dropout rate to handle the above models. DD supports deep learning models of various structures, including convolutional neural networks (CNNs) and fully connected layers. But, Recurrent Neural Networks (RNNs) are not supported by our current reference implementation. Developers can extend our DD to support RNNs and other architectures. UM only supports classication problems, in which the last layer is softmax. Otherwise, it reports false alarms. DL only supports numerical problems, and it does provide any suggestions on how to x a detected problem. AT supports classication problems and does Table 6: Comparing the Results from UMLAUT (UM), DeepLocalize (DL), AUTOTRAINER (AT) and DeepDiagnosis (DD) C-10: indicates to the model using CIFAR-10 dataset, and F-M: indicates to the model using Fashion-MNIST dataset. Table 8: The Symptoms Results from DeepDiagnosis not support problems in the model architecture (i.e., loss function, activation function at last layer, and some APIs (e.g.,t_generator())). In terms of eciency, AT takes longer to nd a x, as it tries all possible solutions until arriving at the correct one. In case it does not nd an improvement, it marks the problem as unsolvable. The "Ablation" column of Table 6 shows which procedure in Table 2 is used to report the symptom in each buggy model for SGS dataset. We found that ExplodingTensor () detects 23 buggy models, SaturatedActivation () detects 7, DeadNode () reports 5, OutofRange () detects 5, UnchangeWeight () nds 2, InvalidAccuracy () detects 2, AccuracyNotIncreasing (), and VanishingGradient () reports only one buggy model. Table 8 shows dataset names, and columns contain the number of symptoms, which were detected successfully by the corresponding procedure in Table 2. From Table 8, we found that ExplodingTensor () detects 73 buggy models, VanishingGradient () detects 32, UnchangeWeight () nds 25, AccuracyNotIncreasing () 22, DeadNode () reports 13, SaturatedActivation () detects 12, OutofRange () detects 5 , and InvalidAccuracy () reports only two buggy models. Although the incorrect DNN models related to parameters and structures often manifest as numerical errors during training, DD provided further reasoning and categories of causes using these procedures, which can help quickly x the bugs. Our study also found that data preparation is a frequently occurring issue and thus the ImproperData() procedure is frequently invoked. SGS benchmark does not have a very deep model that contains many layers. Thus we did not use VanishingGradient () detector very frequently. On the other hand, VanishingGradient () is invoked very frequently in AUTOTRAINER models, because this dataset has many layers using sigmoid and tanh as activation functions. However, when N layers use a Logistic activation function (like sigmoid or tanh), N small derivatives are multiplied together. Thus, the gradient decreases exponentially and propagates down to the input layer. We compared and contrasted three approaches [32,39,43] against our approach (DD). From Table 7, we found our approach detected more problems in the SGS dataset than AUTOTRAINER. Also, it detected fewer problems in AUTOTRAINER dataset than the AT approach. The reason is that our approach only reported the problem and solution if it detected one of 8 symptoms. On the other hand, AT inspects the model based on the training accuracy threshold [43]. For our evaluation, we used 188 normal models from [43]. 78 are MNIST, 35 are CIFAR-10, 36 are Circle, and 39 are Blob. UM reported the message: “<Warning: Possible overtting>” for 68 out 78 MNIST models. It reported the following message: “[<Error: Input data exceeds typical limits>]” for 35 out 35 CIFAR-10 models, because the training data is not in the range[-1, 1]. DL reported message: “MDL: Model Does not Learn” for 4 out 34 Circle models and 16 out 39 Blob models. For all MNIST and CIFAR-10 models, DL reported dierent messages. AT checks if a model has training accuracy less than or equal to the threshold of 60%. To make a fair comparison between the approaches, we changed the training accuracy threshold to 100%. AT reported dierent symptoms for 10 out of 36 Circle, 5 out of 39 Blob models, and 2 models with problems out of the 78 MNIST models. Our approach reported one saturated symptom for 36 Circle, which is not supported in AT, reported 8 symptoms - 6 “saturated activations” and 2 the “accuracy is not increasing.” For the MNIST model, our approach reported 37 symptoms - 35 “dead nodes” and one is a “numerical problem;” we investigated this model and found its accuracy is 20%. For CIFAR-10 models, DD reported 21 models with “dead node” out of 35 models. All detailed experiment results are publicly available [6]. DD signicantly outperformed the baselines UM, DL, and AT in the SGS dataset (Tables 6 and 7). In particular, identied 46 out of 53 buggy models, correctly performed fault localization in 34 models, and reported symptoms for 37 of those. DD also provided a location to x 28 out of 53 faulty models. Regarding total analysis time, DD outperformed DL and AT in benchmarks. As DD does not require the training phase to nish to detect bugs. Also, DD uses a Decision Tree (Figure 2) approach to reduce the search space when mapping symptoms to their root causes. Furthermore, DD is more comprehensive than prior work, as it supports several varieties and semantically related errors in classication and regression models. Also, DD supports 8 failure symptoms, while prior approaches support fewer (in Section 3). Finally, DD does not support some APIs (e.g., t_generator()) as we consider problems related to hyperparameters, for example, epoch, batch size, and dropout rate, as out of scope. External Threat:We have collected 53 real-world buggy DNN models from Stack Overow, GitHub and 496 models from prior work [33,39,43]. These models cover a variety of failure symptoms and location to perform xes; however, our dataset may not include all types of DNN APIs and their parameters. To mitigate the threat of behavior changes caused by the Extractor tool, used to extract the source code from the 496 models [43]. We have veried the accuracy of each model before and after their conversion. In terms of execution time, dierent hardware congurations may oer varying response times. We mitigated this threat by executing our experiments several times and calculated their averages. Internal Threat:When implementing Algorithm 1, Decision Tree (Figure 2), and Tables 2 and 3, we used the parameters dened by prior works [1,10,16,32,43]. These selected values may not work for some unseen examples. To mitigate this threat, we have validated these selected parameters against our benchmarks collected from a diverse set of sources [33,39,43]. For each of these benchmarks, our selected parameters work consistently well. Although we have carefully inspected our code, our implementation may still contain some errors. We manually constructed ground truths regarding fault location, failure symptoms, and location to x for all the buggy models based on the data from the previous research [33, 39, 43]. This process may have introduced errors. Fault localization in De ep Neural Networks:The recent increase in the popularity of deep learning apps has motivated researchers to adapt fault localization techniques to this context. With the intent of validating dierent parts of DL-based systems and discovering faulty behaviors. The goal of fault localization is to identify suspicious methods and statements, to isolate the root causes of program failures, and reduce the eort of xing the fault [31]. Wardat et al. [39] presented an automatic approach for fault localization called DeepLocalize. It performs dynamic analysis during training to determines if a target model contains any bugs. It identies the root causes by catching numerical errors during DNN training. While DeepLocalize focuses on identifying bugs and faults based on numerical errors, DeepDiagnosis aims to perform fault localization beyond that scope. Furthermore, our approach can report symptoms and provide actionable xes to a problem. DEBAR [44] is a static analysis approach that detects numerical bugs in DNNs. DEBAR uses two abstraction techniques to improve its precision and scalability. DeepDiagnosis uses dynamic analysis to localize faults and report symptoms of a model during training. In contrast, DEBAR uses a static analysis approach to detect numerical bugs with two abstraction techniques. Schoop et al. [33] proposed UMLUAT, a user interface tool to nd, understand and x deep learning bugs using heuristics. It enables users to check the structure of DNN programs and model behavior during training. Then, it provides readable error messages to assist users in understanding and xing bugs. Section §4 shows the comparison between UMLUAT [33] and DeepDiagnosis. DeepDiagnosis is more comprehensive, ecient, and eective than UMLAUT, which only supports classication models. DeepFault [11] is an approach that identies suspicious neurons of a DNN and then xes these errors by generating samples for retraining the model. DeepFault is inspired by spectrum-based fault localization. It counts the number of times a neuron was active/inactive when the network made a successful or failed decision. It then calculates a suspiciousness score such as the spectrum-based fault localization tool Tarantula. In contrast, DeepDiagnosis focuses on identifying faults and reporting dierent types of symptoms for structure bugs. Bug Repair in Deep Neural Networks:Zhang et al. [41] proposed Apricot, an approach for automatically repairing deep learning models. Apricot aims to x ill-trained weights without requiring additional training data or any articial parameters in the DNN. MODE [26] is a white-box approach that focuses on improving the model performance. It is an automated debugging technique inspired by state dierential analysis. MODE can determine whether a model has overtting or under-tting problems. Compared with MODE and Apricot, which focus on training bugs (e.g., insucient training data), DeepDiagnosis focuses on structure bugs (e.g., activation function misused). Zhang et al. [43] introduced AUTOTRAINER, an approach for xing classication problems. Zhang et al. dene ve symptoms, and provide a set of possible solutions to x each one. Once AUTOTRAINER detects a problem, it tries the candidate solutions, one by one, until it addresses the problem. If none of the solutions x the problem, it reports a failure message. The evaluation used six popular datasets and showed that AUTOTRAINER detects and repairs the models based on a specic threshold. AUTOTRAINER was able to improve the accuracy for all repairing models on average 47.08%. DeepDiagnosis analyzes the model’s source code during the training phase to localize the bug. DeepDiagnosis supports eight symptoms, while AUTOTRAINER supports ve. DeepDiagnosis does not perform automated xes, but it provides actionable recommendations that developers can follow. AUTOTRAINER tries all possible strategies in its search space to x a problem and outputs whether or not the x was successful. In contrast, DeepDiagnosis uses a decision tree to reduce the solution search space, thus saving time and computational resources. In summary, the goals of DeepDiagnosis and AUTOTRAINER are dierent; DeepDiagnosis focuses on fault localization while AUTOTRAINER on automatically repairing a model. This paper introduces a dynamic analysis approach called DeepDiagnosis that a non-expert can use to detect errors and receive useful messages for diagnosing and xing the DNN models. DeepDiagnosis provides a list of verication procedures to automatically detect 8 types of common symptoms. Our results show that DeepDiagnosis can successfully detect dierent types of symptoms and report actionable changes. It outperforms the state of the art tool such as UMLUAT and DeepLocalize, and it is faster than AUTOTRAINER for fault localization and provide suggestions to x the issue. In the future, we plan to expand this prototype to handle more types of models and failure symptoms, and also automatically x the bugs.