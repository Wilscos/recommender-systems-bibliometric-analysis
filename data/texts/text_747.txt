Many learning tasks require observing a sequence of images and making a decision. In a transportation problem of designing and planning for shipping boxes between nodes, we show how to treat the network of nodes and the ﬂows between them as images. These images have useful structural information that can be statistically summarized. Using image compression techniques, we reduce an image down to a set of numbers that contain interpretable geographic information that we call geographic signatures. Using geographic signatures, we learn network structure that can be utilized to recommend future network connectivity. We develop a Bayesian reinforcement algorithm that takes advantage of statistically summarized network information as priors and user-decisions to reinforce an agent’s probabilistic decision. Fig. 1: Mock topology and cost structure. In transportation and logistics companies, decisions have to be made frequently by operators on which warehouses or nodes need to be connected, i.e. the travelling salesperson problem (TSP). This problem has constraints and structure. We can abstract this down to a simpler scenario as in Figure 1. We want to ship a package fromA toFwith minimal cost, where cost is shown on the edges. NodesC,D, andEare relatively wellconnected toF, i.e. we are “positively rewarded” for takingC → FandD → F(cost is negative as in savings), whileE → Fis only $1. Cost can be the shipping cost of each box or the cost of ordering trucks. Unfortunately, in practice, such a topology and structure of costs becomes obsolete either because of stochasticity or evolution: new nodes are added (new nodeGeither within the graph or outside of it), new connections (A → F orB → Edirectly,A → G, etc.), and most importantly the cost structure is changing based on stochastic demand and pricing. This paper focuses on the decisions made by an operator choosing short-term network activity. In a short enough window, changes within a network are likely small, but identifying consistent connections and recommending new connections based on network activity is valuable. Airlines, for Springer Nature 2021 LTEX template example, change connection structure only every so often. Our goal is to deﬁne a mathematical representation of a transportation network that allows experimentation of new nodes or connections by an operator that is making short-term decisions on which connections should be opened. Ideally, to choose connections that will be active (have boxes shipping), we can forecast the number of boxes ﬂowing within all connections for a given network while respecting network or geographic constraints, such as node capacities or transit times. The traditional approach to network design and planning in operations is to consider global optimization techniques that solve for connectivity and ﬂows while constrained for asset capacities [6,9]. This global approach is useful for strategic or long-term decisions. Batch or campaign style planning is well-studied in industrial processes, but in transportation it could only be eﬀective provided standing decision mechanisms are aligned, such as simultaneous launch of buildings, etc. [3,7,8,11]. Overall, when we approach this problem by embedding unique node and time speciﬁc information, then the mathematical model does not generalize beyond that network state. If we can summarize network activity at some aggregate level while capturing network dependencies, then it would be possible to estimate the cost of new networks with added nodes and connections. For the TSP problem in general as represented by structural graphs, there are computationally expensive methods that leverage learned abstractions of historical graphs via deep neural networks to explore unseen instances of graph activity [2,10]. However, it would be diﬃcult to experiment with such a method if the user has geographical properties in mind, such as location of a connection relative to a highly active node. In this paper, we show that these ﬂows can be represented as images unique to each node and time point. Using compression techniques to reduce the dimension of the images, we can summarize the position and relative importance of each node at a particular time. We call these geography-speciﬁc compression summaries geographic signatures (Section 2). Using low-dimensional representations of graphs has been eﬀective in network analysis tasks [4,5,14]. We want to use the geographic signatures to capture historical network structure and infer relative importance between origin nodes and a destination (a rank), while also allowing experimentation with future network structure. Using geographic signatures along with live operator-feedback, we show that we can learn the relationship between historical network activity and node pair ﬂow/connection. Although rank features have been used in TSP problems [15], it has not been treated as a target that is updated and reinforced. Online learning approaches via lowdimensional representations have been looked at for social network learning [12]. Here, we develop methodology to extend the idea to operations bound by geography. In Section 3, we propose a framework for using the geographic signatures to train a probabilistic model that ranks potential future connections and learns based on actual user decisions. We conclude with future work. In this paper, we describe our work on the transportation problem of an e-commerce company, referred to here asXY Z. AtXY Z, inventory is stored within warehouses across the United States (Fulﬁllment Centers or FCs for short). From within the FC, a purchased item from the website is picked, packed and moved onto a truck to be moved through the transportation network. A package at an FC can move to a sortation node where a package will get sorted and put onto a truck with other packages going to a common delivery node. Alternatively, a package can also move directly to a delivery node without going through a sortation node. There are other variations possible, but the transportation of a package between the origin warehouse and a delivery node is what is known commonly in the industry as the ’middle mile.’ A central problem in middle-mile network design is predicting origin-destination (OD) ﬂows: How many packages will ﬂow from each origin warehouse to each Zip3/Zip5/facility/etc on a given date in the future? There are many possible approaches to this problem across the industry: optimization via mixed integer or linear programming or forecasting via statistical time series models are two major examples. However, in the optimization case, it is easy for the number of variables to blow up for a problem that has a large Springer Nature 2021 LTEX template network or a large horizon of time, so simpliﬁed problems are typically considered that preserve structure and process as much as computation allows. While in the statistical case, it can be difﬁcult to eﬀectively capture geography and other process constraints, but time and uncertainty can be modeled readily. We want to focus on capturing geography in a problem. In this paper, we introduce a mathematical construct for bringing in geographical information into modeling, by using a concept we call geosigs (abbreviation for “geographic signatures”). This concept’s construction draws from the ﬁeld of lossy image compression [1]. After motivating the problem in Section 2.1, we deﬁne the compression summary for this problem as geographic signatures (2.2). We demonstrate the value of geographic signatures by building a simple OD ﬂow prediction model for the United States and observe how the geographic infromation can be utilized for prediction and experimentation. Consider the situation in Figure 2, where we have two origins at equal distance from destinationZip, where Zip0 is some zip-code of customers. The question we are trying to answer is the following: “How much volume ﬂows from each FC to Zip0?” A purely distance-based model will assign equal volumes to both FCs, because they are equidistant from Zip0. However, the picture shows some potentially useful geographic information.F Cis surrounded by an ocean, with no population centers around it.F C, on the other hand, is surrounded by population centers. One would therefore expect that F Csends much of its volume to its nearby population centers, and has very little to send toZip, whileF Cfulﬁlls a disproportionately higher fraction ofZip’s demand. It is possible to capture this implicitly via capacity constraints, but our premise is that we should not need to go to force the constraint.We should enable our models to capture what is easily visible to the human eye: the diﬀerent geographic embeddings of the two FCs must result in diﬀerent predicted ﬂows toZip. The rest of this section describes how we propose doing this, and what our test results show. XY Zthat has been masked. All analyses are done using Python on an i7 2.5 Ghz machine with 16 GB RAM. We begin by providing the formal deﬁnition of how we compute compression summaries in this setting, followed by detailed discussion of each step. For notation’s sake, letZipNdenote the ﬁrstNdigits of a zip code, e.g. for 98109-4385, Zip3=981, Zip5=98109, and Zip9=98109-4385. Also, we abbreviate latitude (‘lat’) and longitude (‘lon’). LetGbe a collection of nodes, such as the collection of all Zip3s, or the collection of all FCs. Letxbe a single node, which may or may not belong toG. For every node, we are given the latitude, longitude, and a measuref. The measure may be just a constant 1 in the simplest case; or it may be something more appropriate for the context: e.g. ifGis the collection of all FCs, then fmay be the capacity of each FC, or ifGis the collection of all Zip3s, it may be the “purchasing power” of each Zip3. The compression summary,CS(·, ·) ofx ∈ G is deﬁned as: CS(x, g) = P rincipalComponents(F F T (P olar(x, G))). P olar(x, G): This is a matrix that represents (G, f), as viewed fromx, in polar coordinates, with some discretization. Consider a nodev, deﬁned by (lat(v), lon(v), f(v)). The polar representation of its location with respect toxis given byP olar(x, G) = (r, θ)(x, v), wherer=q (lat(v) − lat(x))+ (lon(v) − lon(x))andθ= tan, with appropriate manipulation of the sign ofθso that starting from East and rotating counter-clockwise,θgoes from 0 to 2π(in radians). Fig. 3: Bitmaps translating ﬂows from two FCs, in Los Angeles and Houston, to a collection of Zip3s for a particular day. We choose the step sizes ofrandθ. Forθ, suppose we can choose the four cardinal directions, steps of, or smaller steps of. Forr, choosing r= 1 on a map in latitude and longitude is similar to choosingr≈69 miles. We might chooseras the expected length of a truck route to reﬂect the actual segmentation of a transportation network. For any chosen set of step sizes, we sum thef values for all nodes inGthat fall into a common neighborhood ofrandθ. In general, applying polar coordinates with discretization yieldsP ∈ R, wheredandddenote the number of discrete points in the range of r and θ, respectively. We can visualize this polar matrix as a bitmap. Two such bitmaps are shown below for two FCs in theXY Ztransportation network (Figure 3), one in Los Angeles and the other in Houston (each FC can be treated as the top left corner). In each,θ is on the Y-axis, withY= 0 representing East, and as we go down the Y-axis, going to North, West, South, and back East. The X-axis represents distance. Here,Gis the set of all Zip3s, andfis the total volume at each Zip3. We see that a lot of the far distances (the right half of the images) are empty and this is mainly because an FC in LA may not typically be the origin of packages going into the eastern corners of the US. A little more than just the bottom half of the LA picture is also dark (zero measure), which is explained by the presence of an ocean Northwest, West, and Southwest of LA. Houston has some destinations West and close, but not far. F F T(P olar(x, G)): Next we apply a Fast Fourier Transform to the matrixP olar(x, G) yielding a matrixF ∈ C. FFT is also an invertible matrix transformation, from the real space to the complex space with the same dimensions. It decomposes the original image using simpler basis Fig. 4: For Los Angeles, the image after applying a mask onFand inverting onto the magnitude spectrum. functions. A consequence is information “concentration”: FFT (like other such transforms) forces most of the signal to be at matrix positions closest to (0,0) (as we have deﬁned it). P rincipalComponents(F F T(P olar(x, G))) : Finally, taking advantage of the information concentration done in the FFT step, we select the entries (i, j) ofFsuch thati+j ≤ mask, which we call the principal components of the matrix for ease of exposition. In one of the results displayed, we only selected 3 matrix (mask= 1) elements to summarize the position between each OD pair: (0,0), (0,1) and (1,0). This is where we accomplish compression: the entire picture is represented by 3 complex numbers, or equivalently, 6 real-valued numbers. In fact, the imaginary component of the (0,0) element of the FFT is always 0, so we’re really only using 5 numbers to represent the entire picture. These 5 numbers deﬁne the compression summary, or CS(x, G). In Figure 4, we show two pictures which are inverses of the LA picture displayed on the previous page, withmaskequal to 1 and 3 respectively. Observe that neither picture is exactly equal to the LA picture shown in FIgure 3, e.g. “lossy” image compression. Selecting more principal components (highermask) results in an inverted image that is close to the original: selecting a smaller value ofmaskresults in greater information loss. In that sense,maskmay be thought of as a “tuning parameter.” These summaries do not reside on the domain of the original image, which is why in the previous example we had to invert the image. In the original images on the real domain of ﬂows, each row/column pair correspond to an angle/distance historical ﬂow. Once we apply the mask on the coefﬁcients and transform, we get instead a smoothing of ﬂows on a magnitude spectrum. In signal processing, each complex coeﬃcientα= (r, c) can Springer Nature 2021 LTEX template be transformed such that the power,power, can be expressed aspower= (r+c). Here, that power is interpreted as instantaneous ﬂow or ﬂow intensity. When we use the ﬁrst three upper left corner coeﬃcients usingmask= 1, then we get the average ﬂow intensity at the origin (0,0), 1 degree away from the origin (1,0), and 1 turn or angular step from the origin (0,1). For the LA image, this might be useful for northern ﬂow activity, but it will not capture southern ﬂow activity. With a mask of 3, we increase the number of coeﬃcient and ﬂow intensity estimates to 6 (so 6 potential features for a regression model). However, we learn nothing about where ﬂows occur, only about the lack of ﬂows close to the FC, which is useful information for problems dependent on local network activity. Ideally, the level of discretization is fully utilized, since this is the most preserved level of information. In Figure 4, we observe how discretization can be interpreted on the magnitude spectrum (inverted image) as a smoothing of the observed ﬂows. We propose a summary of the magnitude spectrum that fully utilizes the discretization in a meaningful way for many network problems. The geographic signature,Geosig(·, ·) ofx ∈ G is deﬁned as: Geosig(x, g) = MagnitudeSpectrum(CS(x, G)). We use this transformation to summarize network structure and reduce dimensionality of the feature space. We propose to discretize the angles of the network into 4 angles: discretization follows the 4 directions: NE, NW, SW, and SE (in that order). Next, we propose distances of 100 miles for simplicity. Hence, we discretize each network ﬂow image into 4 rows and 17 columns. We propose using a semi-coarse mask (mask= 2 to be discussed) and summarizing the 4∗17 = 68 geosigs by the maximum smoothed ﬂow along each row or direction. Furthermore, each max ﬂow is paired with the position of the max, i.e. the number of 100 mile segments that it takes to get to that maximum ﬂow intensity. In this way, we preserve speciﬁc and interpretable information, namely, how far a Zip3 can be from a particular FC to be fulﬁlled. For other problems, alternatives include using the ﬁrst non-zero ﬂow intensity (requires a ﬁne mask) or using the ﬁrst ﬂow intensity above an interesting threshold (say 3000 packages). The geographic signatures or geosigs can be tailored to the problem. In what follows, we demonstrate the utility of the ﬁrst proposed summary of geosigs, the max ﬂow pairs. We now demonstrate the value of compression summaries and geosigs by building a simple OD ﬂow prediction model. The problem statement is the following: for a single selected date in the future, predict exactly how many packages will ﬂow from each FC to each Zip3. If items were in inventory at the closest facility to a customer always, then this would not be such a diﬃcult problem. Instead, we have to expect that many facilities will help to fulﬁll the demand from a particular destination, either because of rarely ordered items or items that can only be stored in special FCs. In this section, we estimate a linear regression model where the dependent variable is the number of packages in eachF C → Zip3 pair, and we use several independent variables explained next. Models considered are trained on a single day: this is day 221 in the year 2018, which is Thursday, August 9, 2018 (18,635 pairs). We test the models against day 249 of 2018, which is Thursday, September 5, 2018 (21,042 pairs). The ﬂow data used in these studies has a controlled amount of noise to retain a particular degree of the original signal, while aliasing the true values of the conﬁdential data. We evaluate the utility of the geosigs as explanatory and predictive variables using simple regression models evaluated via mean actual percentage error (MAPE) and adjustedR. At the scaleRis measured, we introduced at least 80% of the variation in the noisy data, so R> 0.10 will be desirable. The linear regression models use the following independent variables: •ln_lld: Natural logarithm of the Euclidean lat-lon distance between the FC and the Zip3. Included in all models. Model A includes 14 geosig variables (dDI_01, dDI_10, dDR_00, dDR_01, dDR_10, oOI_01, oOI_10, oOR_01, oOR_10, oDI_01, oDI_10, oDR_01, oDR_10, and oDR_00), deﬁned according to the following convention: •oDR_00: This computes the geosig of each origin FC, with respect to the matrix of all FCs, and extracts the real part at matrix location (0,0). •dDI_01: Geosig of each Zip3, with respect to the matrix of all Zip3s, imaginary component, matrix location (0,1). Model B uses magnitude or power of the ﬁrst coordinate (interpreted as average ﬂow intensity) of each domain, denoted as oO_00, oD_00, and dD_00. Model C and D use 4 max ﬂow distance pair values (8 values per OD pair), with the following convention: •oD_0r_summary_max: This computes the max intensity along the 0th direction (0, π/2) for F C → Zip3. •oO_0r_summary_max: This computes the max intensity along the 0th direction (0, π/2) for F C → F C. •dD_0r_summary_max: This computes the max intensity along the 0th direction (0, π/2) for Zip3 → Zip3. •oD_0r_summary_max_r: This computes the distance,r, at which the max intensity occurs along the 0th direction (0, π/2) for F C → Zip3. Model C uses only the max ﬂow (4 values per OD pair). These are all computed with the measure fbeing the total ﬂow on the training date: that is, for the matrixD, the measurefof each Zip3 is the total number of packages delivered to that Zip3, and likewise for FCs in O. We use three sets of geosigs: origin domain (o,O),origin-destination domain (o,D), and destination domain (d,D). For each domain set, we extract the real components at matrix indices (0,0), (0,1) and (1,0), and the imaginary components at (0,1) and (1,0). In Table 1, we present the results of ﬁtting linear models using geosigs. The best predictor of day 249 was Model B with 57% MAPE, although it had a lowR. Using the average ﬂow intensity along each domain resulted in a low variance, but higher bias estimator. We see that raw geosigs can explain about 19% of the variation in the training data (Model A), but summarizing the geosigs not only improves test MAPE (Model C, Model D), but also provides a more generalizable model (Model D with highest R). Overall, this shows the improvements we gain by tailoring geographic signatures for a ﬂow problem. By using geographic signatures, we reduced Table 1: AdjustedRvalues for linear models ﬁt on ﬂow data using features engineered using geographic summaries. the dimensionality of the network, preserved the signal relevant to the relationship between geography and OD ﬂow, and retained features that are interpretable. We next show how to use that interpretability to learn fundamental network properties that allow us to consider hypothetical networks. All of the measures that we are considering as belonging to a network or topology are intrinsically dependent on each other. This means even when we summarize the information into geographic signatures there is correlation among the features. We want to capture this collinearity to have an understanding of network eﬀects that generalizes to other within-network scenarios. We ﬁt a gradient boosted random forest to capture some of that dependence and demonstrate how the geosigs allow us to play out basic scenarios such as arc additions or new facilities. Using the features in Model D (distance and the max ﬂow pairs), a gradient boosted regression forest was trained (sklearnwith 1000 boosting iterations, otherwise default settings) and had a test MAPE of 63.6%. When observing the ﬂow tendencies of the network between the strongest pairs of FCs and destinations in the NE direction (’oD_0r_summary_max’), the pairs with an average ﬂow intensity less than 800 boxes (about 90% of arcs based on Figure 5) tended to have a similar overall ﬂow that was smaller than the ﬂow expected from pairs with an average ﬂow intensity greater than 800 boxes. These NE-directed max pairs tended to occur quite far (close to 60% occurred at more than 1500 miles) and those large distances tended to be associated with smaller ﬂows overall as can be seen in the PDP for the distance of the max pair, ’oD_0r_summary_max_r’. Overall, this suggests a network that has a particular relationship with the NE direction: it has FCs that serve it thick arcs from about 1500 miles, but Fig. 5: PDPs look at a variable of interest across a range. At each value, the model is evaluated for all observations of the other model inputs, and the output is then averaged. A negative (positive) value indicates that the feature value may be associated with smaller (greater) ﬂows compared to the rest of the range averaged across the other features. The ticks mark quantiles. for the most part NE directed arcs are not that thick. Using the partial dependence values directly, we can estimate the contribution of a particular type of activity to a network. In the example given, we can estimate the total change in ﬂow to a network relative to the activity of NE-moving packages. Under the network represented by the data sampled and transformed, we can expect 8931 fewer boxes moving NE than the average of the other directions. NW moving boxes are the minority averaging about 10,253 fewer boxes than the network average. The most active overall direction under this summary is the SW set of boxes at about 6491 above average. Suppose we place a hub in the middle between Los Angeles and New York at about 800 miles from Los Angeles, so that we can reduce the overall average arc length of the network, call itHUB. Using the PDPs, we estimate changes in ﬂow across the network. We wantHUBto play an important role for the SW region, so we want network information to reﬂect this new activity. We generate data such that all origins SW ofHUBwill have ’oD_0r_summary_max_r’ equal to the number of segments between the origin andHUB. ’oD_0r_summary_max’ might tend to be high around 800 boxes. Other than this behavior change in the network for the SW region,HUBis not going to impact any other origins. This implies that getting the average value for all other geosigs will suﬃce. In a small experiment, we replace 25% of the test data to reﬂect the SW region’s newHUB and randomly generate distance and two of the geosigs as Gaussian random variables centered at the means of interest. Partial dependence on generated data quantiﬁes how the network tends to be associated with that activity summary. Using the geosigs for the historical network, we can expect 17,382 fewer packages going NE with the addition ofHUB, but 12,226 more boxes to be moving SW. When one area of the network experiences more activity, the learned network associates that with a decrease in activity for other areas. This experiment highlights the dependence that can be captured using geographic summaries. Under this experiment, we would need to know speciﬁc arc cost information to estimate whether thisHUB would lead to overall reduction in network costs or other important metrics. In the next section, we consider the problem of network activity at a larger scale and utilize the geographic signatures to capture network structure. We deﬁne the short term problem of deciding arcs for the next time period based on last time period as a cascading bandit problem [13]. We train a probabilistic model, a cascade model, that governs the arcs that will be recommended to a business operator. This probabilistic model can be informed by our prior knowledge of the network as encoded by the geographic signatures. In Section 3.1, we rank arcs particular to a destination based on cost of a package on that arc derived from a regression model that takes as features geographic signatures among others. We rank by cost because we want to minimize the overall cost of the network. Using these ranks as priors, we propose a Thompson sampling algorithm to learn the probabilistic model that governs arc recommendations (Section 3.2). Using Bayesian techniques for reinforcement learning, we show in Section 3.3 that it does not take many iterations to learn important network structure under a consistent network topology. We also discuss user-driven agent-assisted experiments to summarize new topologies. Using historical arc costs (based on truck costs) and volumes, we want to predict future arc connections. To do this, we will rank the arcs based on cost and rank low cost arcs high. First, we must estimate cost. For demonstration, we propose a regression model for weekly data with the following data structure. LetF CandDdenote theith origin FC and thejth destination node.Dcan be a delivery node or a hub. We deﬁnecas the average historical cost per package andnas the number of packages or ﬂow on arcF C→ D at timet. Additional measures we consider in this demonstration include:F Clatitude and longitude, latandlon; the distance betweenF CandD, d; and a binary indicatordirect= 1 whenD is a delivery node (otherwise, 0). We need to be able to capture network topology of the United States. We want to know when a facility serves locally more than across the US or when there is a body of water or a mountain that explains why a facility does not serve a destination. However, we want to embed this information as simply as possible. Using ﬂow and location (f, lat, lon, lat, lon), we deﬁne geographic signatures as done in Section 2.2. In particular, we use the max ﬂow pair summaries introduced in Section 2.3, where g=geosigs(f, lat, lon, lat, lon)∈ R. Let x= [t, direct, f, d, lat, lon, g]. We aim to estimate the expected cost of an arc at time , whereˆfcan be estimated using random forest regression techniques. Using 52 weeks of historical data (spanning October 2018 to October 2019), we ﬁt dailyˆf to predictˆc(t>0). We pre-process the arcs to retain arcs with a minimal ﬂow and met other conﬁdential requirements. We consider 71 origin FCs and 232 destinations. For each D, letsorted= (F C, F C, . . .), where ˆc≤ ˆc≤ . . . ≤ ˆc. The position ofF Cinsorteddenotes the rank relative [0, 1, 2, ..., 70]. We see in Table 2 the rankings forDalongside distance for two weeks, week 45 and 52. Of note, the ranks do not obey a distance law. In Table 3, Table 2: Ranks forD. Rank 1 corresponds to the arc between an FC andDwith the lowest expected cost per package. Table 3: Actuals forD. Rank 1 corresponds to the arc between an FC andDwith the largest number of trucks (we show only top 10). We only show actuals for the FCs considered in this study. we show a glance of the top 10 week 52 actuals (see Table 4 for the number of connections per week). We see that the top 10 estimated ranks are similar to the top 10 actuals, exceptF C. New connections open as the peak season gets closer, so we expect peak-speciﬁc arcs to appear. Since the estimated ranks are time-dependent, then we expect some increase in ranks of infrequent arcs, but if a destination has a consistent set of 10 or more then we may not observe it in a top 10 rule. To take advantage of the recurring cycle of decisions and improve the chance we observe infrequent but necessary arcs, we propose a probabilistic model that updates based on user decisions using Thompson sampling as discussed next. Table 4: Number of actual connections between the study FCs andD. Mean=18.75, Standard Deviation=2.19. The cascading bandit model for this problem is deﬁned by (N, N, K, θ), whereNis the number of destinations,Nis the number of FCs, K≤ Nis the number of arcs recommended at timetforD, andθ ∈[0,1]is a vector of connection probabilities. In a cascading scheme at timet, an agent recommends an ordered list a∈ {1, . . . , N}. The user selectsk ≤ N, call these selected arcs a. For arcF C→ Ddenoted byarc, let α≡the number of times the arc was selected andβ≡the number of times the arc was not chosen, but was observed in the topK. Finally, to use the rankings as a weight for future decisions, letrankpct= 1−. In Algorithm 1, we describe the algorithm for using prior information to update the probability of selecting an arc,arcat timet. Note that the initial probabilities of connections are sampled independently (a false assumption for network connections). However, we estimated ranks based on costs that have accounted for dependencies at some high level. Assuming overall structure of a network does not change signiﬁcantly within short time intervals, then the geographic signatures presents a useful structured ranking. The updated connection probability reﬂects that information if it is reinforced by the user enough times. We show that this algorithm captures structure in a timely manner. Algorithm 1 ArcsT S(N, K, α, β) for t = 1, 2, . . . do # Sample probability of connection; for j = 1, 2, . . . , Ndo for i = 1, 2, . . . , Ndo end end a← arcs with K largest˜θ. end # Update posterior distribution hyperparameters; for t = 1, 2, . . . do # Sample probability of connection; for j = 1, 2, . . . , Ndo for i = 1, 2, . . . , Ndo end end end Using 8 weeks of data, we run Algorithm 1 to recommend 10 arcs. The probabilities can be initialized usingα= 0.1 andβ= 1.0. Choosing an arc increasesα. If an arc is not chosen but in the topKrule, thenβincreases. For the initial week (week 45), the sampled probabilities of connection are uninformative, so on a ﬁrst initialization we can use the ranks. In Figure 6, by the fourth week, FCs with rank percentiles greater 50% start to get recommended with probability greater 50%. FCs with rank percentiles below 50% tended to be neither selected nor viewed, so by the 5th week had low probabilities (no rewards). Table 5 shows the 10 arcs with largest sampled probability of connection toD.Fis recommended among the top 10. Although this is by probability, this case shows the usefulness of Bayesian reinforcement based on estimated ranking and operator decisions.F Cwas not in the top 10 actuals forD, but it was active with only 2 trucks. One way to decrease the role of chance in the topKrule (hereK= 10) is to introduce a topKrule, where the number recommended depends on thejth destination.Dalone averaged about 19 connections (sd=2.19) over the 8 Fig. 6: Rank percentile (solid) and connection probabilities (dashed) with Dacross 8 weeks for 3 random FCs (0, 42, 67). Table 5: Ranks forD. Rank 1 corresponds to the arc between an FC andDwith the largest sampled probability of connection. weeks. Other destinations play smaller or larger roles in terms of arc connectivity depending on time and network position. One approach to choosingKis to use the probabilities of connection themselves. We can treat arc connectivity probabilities as belonging to a Bernoulli randomP Assuming independence amongY, . . . , Y, the expected number of arcs,N, can be calcu-P lated asN=E(Y) =θ. In this way, we can use˜θto estimateθand say the agent wants to recommendK=Narcs. Under this formulation, we can also estimate the variability of the agent’s recommendation total onP destinationjasVar(Y) =θ(1− θ). For Dover the 8 weeks, the averageNwas 16p (sd=Var(Y)= 2.31). During week 52, the agent would have recommended 24 arcs compared to the actual 18. Overall, this shows the agent is already converging on network behavior forDas far as recommendations might be needed to. As a recommendation agent, we primarily want to reduce the search space for the next set of connections. We also want to be able to use the ranks and probabilities of connection to experiment with new networks. As discussed in Section 2.4, the geographic signatures can be used to summarize the fundamental properties of diﬀerent networks. As shown here, these geographic signatures can be used to rank arc connections within a network. These ranks in turn can inﬂuence a cascading model’s probability of connection to recommend a set of arcs. A simple framework for user-driven agent-assisted testing for the potential of a new network addition can be formulated as follows: 1.Fit a model that captures important network dependencies using historical arc costs/ﬂows data. 2.Predict cost/ﬂow on artiﬁcial data for new connections. 3.Use predictions to rank old and new connections. 4.Initialize new arcs using most similar existing arcs. 5.Run the agent and see how close new arcs are to be included in the setK. With a range of artiﬁcial data sets, uncertainty of new arc utility can be measured. There is more that can be done within Algorithm 1. For instance, a discount factor can be added so that connections that are guaranteed converge at some high probabiltity. Moreover, it would be important to also introduce penalties for inactivity. One simple idea is to create a rule dependent on decreasing rank: lowerβby 1 if an arc drops in rank by some threshold. Finally, the most important addition to such an algorithm would be to capture the dependencies amongst the probabilities of connection in a way that still allows user-driven experiments. In this paper, we discussed image transformations for learning recurring decisions. We introduced summarizing complex geographic network structures using geographic summaries or geosigs. Geosigs captured fundamental network properties that can be helpful in ranking the potential of new connections. For recurring connectios, we introduced a Bayesian reinforcement framework where an agent probabilistically determines connections based on ranks reﬂecting network value and user Springer Nature 2021 LTEX template decisions. There is more that can be considered in future work both in terms of method and application. The probabilities and rankings can be merged into one probabilistic model that captures network dependencies more explicitly. Furthermore, tendencies in the agent’s decisions can be used to track emerging networks in applications beyond transportation. On behalf of all authors, the corresponding author states that there is no conﬂict of interest.