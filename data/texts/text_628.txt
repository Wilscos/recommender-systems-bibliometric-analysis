Most of the existing recommender systems are based only on the rating data, and they ignore other sources of information that might increase the quality of recommendations, such as textual reviews, or user and item characteristics. Moreover, the majority of those systems are applicable only on small datasets (with thousands of observations) and are unable to handle large datasets (with millions of observations). We propose a recommender algorithm that combines a rating modelling technique (i.e., Latent Factor Model) with a topic modelling method based on textual reviews (i.e., Latent Dirichlet Allocation), and we extend the algorithm such that it allows adding extra user- and item-specic information to the system. We evaluate the performance of the algorithm using Amazon.com datasets with dierent sizes, corresponding to 23 product categories. After comparing the built model to four other models we found that combining textual reviews with ratings leads to better recommendations. Moreover, we found that adding extra user and item features to the model increases its prediction accuracy, which is especially true for medium and large datasets. • Information systems → Re commender systems; Personalization; Retrieval tasks and goals; e-commerce, LDA, Latent Factor Model, recommender systems, textual reviews ACM Reference Format: Tatev Karen Aslanyan and Flavius Frasincar. 2021. Utilizing Textual Reviews in Latent Factor Models for Recommender Systems. In The 36th ACM/SIGAPP Symposium on Applied Computing (SAC ’21), March 22–26, 2021, Virtual Event, Republic of Korea. ACM, New York, NY, USA, 10 pages. https://doi.org/10. 1145/3412841.3442065 Throughout the last decade, the importance of the Web as a medium for business and electronic transactions has increased drastically, forcing the IT to rapidly develop as well, making humans daily life much easier and more ecient. On its turn, this large development in IT has increased the popularity of online shopping and services. Making purchases online instead of buying products from physical shops, which can be very time-consuming, is one of the major consequences of IT development. However, this large increase in online sales has not only led to an increase in the number of customers but also an increase in the number of products and variety of these products. Therefore, when making purchase decisions, users are forced to process large amounts of information. According to [18] this information overload has a big impact on the human decision process and quality. Hence, it aects people’s online purchase experience signicantly. Therefore, to overcome this problem, one usually rely on suggestions from others, who have more experience on the topic [2]. This idea is used in the recommender systems aiming to employ various sources of information to recommend products to the users by inferring their interests. Besides solving the problem of information overload, the use of recommender systems also results in increased sales, customer satisfaction and loyalty [27], which explains increasing popularity of these systems. On the one hand, the information overload motivates the use of recommender systems in order to make the users’ online purchases more convenient. On the other hand, the increasing variety of ways that users can discover, evaluate, and review online products motivates companies and researchers to create even more revealing recommender algorithms, which will enable to sell more products. The Web enables users to provide their personal feedback about the product that they have purchased in the form of ratings and textual reviews. Assuming that the past interests and preferences are often good proxies of future choices, the previous interactions between items and users can be used for predicting which items might be interesting for a user in the future. Therefore, in order to correctly recommend the users their desired products, one should predict how the user will respond to a new product [1]. Recommender systems are usually categorized as: Collaborative Filtering systems based on ratings data [26], Content-Based systems based on content (often textual) data [21], and Hybrid systems that combine these two types of systems [7]. Most of the existing recommender systems are of the rst type (based only on ratings), and they ignore the enormous information incorporated in the users’ review texts [31]. Ignoring such an important source of information, that can potentially increase the accuracy of recommendations, seems not optimal. Moreover, adding extra user- and item-specic information, not included in the ratings or textual reviews, to the recommender system might also increase the quality of its recommendations [6, 12, 34]. Figure 1 presents the percentage of items having less than 10 ratings and more than 30 words in their review text per product category in the datasets of the largest e-commerce Amazaon.com [23]. We observe that for almost all product categories it holds that at least 80% of items have very few ratings (less than 10) while over 40% of items have long textual reviews (with more than 30 words per review). Therefore, textual reviews can be considered as a potential source of information that can used to complement the absent ratings to increase the prediction accuracy of the recommender system. In this paper, we propose a recommender algorithm that is based on product ratings as well as textual reviews of customers, and it allows adding extra user- and item-specic information in order to make product recommendations. We focus on Latent Factor Models (also called Matrix Factorization) for modeling the item features and user preferences in a shared topic space and Topic Models (more precisely Latent Dirichlet Allocation) for modelling review features. Although, there exist a large amount of literature regarding recommender systems that are based on a single type of data, such as ratings or textual reviews, there have been only few attempts of combining user-item ratings and textual reviews to uncover the latent rating and latent review dimensions [4,11,20,22,30]. [11] combined the predictions of the Latent Factor Model (LFM) with the predictions of the neighborhood model to generate more accurate recommendations. A similar approach was taken in case of the recommender system of ‘Bellkor’s Pragmatic Chaos’, the Netix Prize contest winner [17]. This system compares the watching and searching habits of similar users, and then recommends movies that share the characteristics with movies that are highly rated by the current user. Since then, LFM became the most popular Collaborative Filtering (CF) technique used for both rating and item recommendations [25]. [30] have developed an algorithm called Collaborative Topic Modeling, combining CF and probabilistic topic modeling, which recommends scientic papers to an online community of users. Authors found that the proposed recommender system, based on both contents of articles and users’ ratings, performs better than the recommender system based on standard Matrix Factorization (MF) methods. Among all the Hybrid recommender systems, one of the most known systems combining ratings with textual reviews for making recommendations is the Hidden Factors and Topics (HFT) algorithm proposed in [22]. HFT combines latent rating dimensions learned by LFM, with latent review topics learned by the topic modeling technique Latent Dirichlet Allocation (LDA), in order to make rating predictions. [22] stated that, the HFT algorithm results in highly interpretative textual labels for the hidden rating dimensions helping to ‘justify’ ratings with review text, and in increased prediction accuracy of the recommender system. Another example of a recommender algorithm that combines ratings with textual reviews has been introduced in [20], called Ratings Meet Reviews (RMR). The proposed method is a probabilistic generative model combining the topic modeling technique LDA with the MF method for ratings. The main dierence between HFT and RMR is the way the authors combine the two models. More specically, HFT uses the MF method to model the ratings, whereas RMR uses a mixture of Gaussian distributions. [20] found that RMR outperforms the standard MF based approach and results in similar prediction accuracy compared to HFT. The TopicMF algorithm introduced by [4] is also an example of a recommender algorithm combining ratings and reviews in order to make recommendations for the users. TopicMF uses biased MF for modeling the ratings and uses Non-negative Matrix Factorization (NMF) for modeling the latent topics in the textual reviews. The main dierence between this algorithm and the earlier mentioned recommender algorithms is that it uses NMF instead of LDA as the topic modeling approach. The nal example related to the model introduced in this study is the Rating-Boosted Latent Topics (RBLT) algorithm introduced by [29]. RBLT used LDA for extracting topics from the reviews like HFT and RMR and it also uses MF for modeling the ratings like HFT. The main dierence between RBLT and HFT is that HFT uses item-features in rating prediction and topic-distributions as a regularization for these item-features, whereas the RBLT includes the topic-distributions in the rating prediction procedure but not in the regularization term. [29] found that adding textual reviews to the CF system increases its prediction accuracy signicantly. One similarity that is shared by the previously surveyed papers is that they propose to use textual reviews as well as ratings to model item features and user preferences in a shared topic space and consequently bring them into LFM to generate recommendations. Our research will also be focused on utilizing recommender systems with the MF approach by using product ratings as well as textual reviews of customers. There have been also few attempts of building a recommender system that allows adding user- or item-specic characteristics, not present in the rating or review data [6], [12], [34]. [6] and [12] introduced CF recommender systems that also allow adding user- and item-specic features on the top of the ratings. As extra user and item information [12] used the browsing data. The CF system extension in [12] has been done by adding extra rows and columns to the user-item rating matrix. However, all these extended recommenders that allow adding user or item features to the system, are based only on ratings. To our knowledge, there are no studies of recommender systems combining ratings and textual reviews that also allow adding extra user or item information to the system. Another limitation of the existing literature is that most of the proposed recommender systems are modeled and implemented on a dataset consisting of very few product categories or a small number of observations. To address the previously identied limitations we propose a recommender algorithm called LDA-LFM, which combines the topicmodeling technique LDA with the rating-modeling method LFM and allows adding (latent) extra user- and item-specic features to make recommendations. LDA-LFM is a generalization of the HFT model proposed by [22], but it will use an alternative approach for model regularization and will allow adding extra user- and itemspecic features to the recommender system. These extra features will behave as additional factors in MF driving the ratings following the approach proposed in [12], while these extra features do not appear in the topic modelling method LDA. This system is applied on both small and large datasets, with or without a large number of product categories. In this section, we introduce all models and techniques used to build and evaluate the proposed LDA-LFM model. We describe the technical details and optimization approach of LFM and the topic modeling technique LDA used in this study. In CF recommender systems, the Latent Factor Model (LFM), also called Matrix Factorization, has become very popular especially after the earlier mentioned Netix Prize Contest [14,15]. Usually, the rating matrix contains lots of missing elements, thus suers from the sparsity problem. In order to overcome this problem, LFM uses the idea of dimensionality reduction to estimate and ll in all missing entries of the sparse user-item rating matrix. The goal of dimensionality reduction is to rotate the axis system such that the pairwise correlations between dimensions can be removed and a large sparse matrix can be decomposed into smaller and dense matrices. Accordingly, the reduced, rotated, and complete data matrix representation can be eciently estimated from a sparse data matrix. The key idea of the Matrix Factorization method is that any m x n sparse matrix R with rank k<min{m,n} can be approximated by rank-k matrices in the following way [28]: where P and Q are m x k and n x k matrices, respectively. So, the user-item sparse matrix R is approximately equal to the product of P and Q matrices, such that the vectors of R can be represented by the rows of matrix P and columns of matrix Q. Stated dierently, in LFM, sparse ratings matrix R is decomposed into the product of two low-rank rectangular matrices P, the user matrix, and Q, the item matrix, where both P and Q have the same rank k. Each row of matrix P and each column of matrix Q are referred aslatent factors. Let us dene by ptheuth row of user matrix P, the user factor representing the anity of user u towards the rating matrix R, and by qtheith row of item matrix Q, the item factor representing the anity ofith item towards the rating matrix R. Since, some users have a tendency to give higher ratings while other users are more prone to provide lower ratings, and that some products have a tendency to be highly rated compared to other products, baseline predictions (biases) should also be taken into account. [16] referred to biases as the observed variation in rating values due to the eects associated with either items or users independent of any interaction. Correspondingly, the estimate of each rating of theuth user about ith item, denoted by r, can be expressed as follows: where𝛼represents the global average of all ratings (an oset parameter), band brepresent the user and item biases, respectively. Accordingly, the error which arises in this estimation is dened as e= r-ˆ𝑟and in order to learn the latent factors pand q the following optimization problem should be solved, where we minimize the regularized squared error [17]: arg min1| T |(𝑒)+ 𝜆Ω(Θ) arg min1| T |(𝑟− (𝛼 + 𝑏+ 𝑏+ 𝑞𝑝))+ 𝜆Ω(Θ) Trepresents the corpus of all ratings (in the training set),Θ= {𝛼, b, b, p, q} is the parameter space of the model. The objective function in Equation 3 can be seen as quadratic loss function which quanties the loss of accuracy when an element of rating matrix R is approximated by low-rank factorization.∥.∥represents the squared Frobenius norm, also called the Lnorm. We use regularization to prevent model overtting, which is required especially when the dataset used for tting the model contains a large number of features, like it is in our case. The constant𝜆from Equation 3, which is often referred as the regularization constant, determines the level of regularization and controls how hard unnecessary features in the model are penalized. Determining the value of𝜆is a trade-o between prediction variance and bias. One popular way of determining the optimal value is Grid Search. One of the most popular ways of solving the optimization problem dened in Equation 3 is Stochastic Gradient Descent (SGD) [16,22,32,33]. Other typical methods which can be considered as possible alternatives to the SGD method, like the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) [35] or Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) [3], work very slowly when the model is tted on a large training dataset and performing it by one machine is sometimes intractable. SGD addresses these issues because it scales well with both big data and with the size of the model, therefore it is preferred in this analysis. However, even though the method itself is simple and fast, it is known as a “bad optimizer" because it is prone to nding local optimum instead of a global optimum. A popular technique designed to improve the performance of SGD method is the Adaptive Moment Estimation (Adam) introduced by [13]. Adam is the extended version of the SGD (with momentum). The main dierence compared to the SGD (with momentum), which uses a single learning rate for all parameter updates, is that Adam algorithm denes dierent learning rates for dierent parameters. The algorithm calculates the individual adaptive learning rates for each parameter based on the estimates of the rst two moments of the gradients. Each text review provided by a user, represented as a bag of words, contains valuable information, which can potentially increase the prediction accuracy of the recommender system. For this reason, the textual reviews should be modeled and analyzed. Latent Dirichlet Allocation (LDA) introduced by [5] is one of the most popular text mining methods in the context of recommender systems. Therefore, we will use LDA as a topic modeling technique in this analysis in order to uncover the hidden dimensions in the user review texts. There are three main entities dened in this method: words, documents, and corpora. The entitywordis dened as a basic unit of a discrete data from a vocabulary,𝑤where j∈{1,2,...,N}, which indicates the index of the word in document d. These words are represented in the form of a vector where thejth element of this vector takes value 1 and remaining all elements take value 0. The entitydocumentis a sequence of N words denoted by𝑑 ∈ Tsuch that Nrepresents the number of words in document d. Finally, the entitycorpusis dened as a collection of documents denoted by T= (d,d,...,d), where M is the number of all documents in the corpus. For simplicity we will use indices to identify documents. LDA makes a few important assumptions regarding the model. Firstly, it assumes that words carry strong semantic information and that documents discussing similar topics will use similar words. Therefore, latent topics are discovered by identifying a bag of words in a corpus that frequently occur together in a document. Secondly, LDA assumes that documents are probability distributions of latent topics and topics are probability distributions of words. So, every document consists of a certain amount of topics and each of these topics is a distribution of words. Therefore, the model assumes that there are in total K latent topics. Then, LDA assigns to each document d a K-dimensional topic distribution𝜃drawn from a Dirichlet distribution represented in the form of a stochastic vector, such that thekth entry of it,𝜃, represents the fraction of words in document d which discusskth topic. Stated dierently, the likelihood that words in document d will be about topic k is equal to𝜃. Furthermore, each topic k is a distribution of words represented by𝜙such that each word has a particular likelihood of being used in the topic k. Let us denote by zthe topic assigned to thejth word in document d. Then the LDA model is dened as follows: • 𝜃∼ DIR(𝛾) with d ∈ {1,...,M} • 𝜙∼ DIR(𝜈) with k ∈ {1,...,K} • z∼ Multinomial(𝜃) • w∼ Multinomial(𝜙) where𝛾represents the parameter of the Dirichlet distribution for document-topic distribution𝜃and𝜈is the parameter of the Dirichlet distribution for word-topic distribution𝜙. zrepresents the topic assigned to the jth word in document d. We assume that the total number of words in vocabulary is V. Moreover, we denote the likelihood function of zconditional on topic mixture of document d, 𝜃, p(z| 𝜃) as follows: Consequently, the probability ofjth word in document d, w, conditional on the chosen topic zdenoted by𝑝 (𝑤| 𝑧, 𝜈)is dened as follows: Furthermore, using the denition of the Dirichlet probability distribution, the conditional topic distribution is dened as follows: where𝜃>0 andΓ (·)represents the Gamma function. Consequently, the joint distribution of a topic𝜃, K topics z, and N words w is dened as follows: 𝑝 (𝜃, 𝑧, 𝑤 | 𝛾, 𝜈) = 𝑝(𝜃 | 𝛾 )𝑝 (𝑧| 𝜃)𝑝 (𝑤| 𝑧, 𝜈)(7) where N =ÍN. Using the properties of discrete and continuous random variables’ distributions, the marginal distribution of document d is dened as follows: 𝑝 (𝑤 | 𝛾, 𝜈) =𝑝 (𝜃 | 𝛾)𝑝 (𝑧| 𝜃)𝑝 (𝑤| 𝑧, 𝜈)𝑑𝜃 Consequently, using Equations 4, 5 and 8 the likelihood of a text corpusTconditional on the word distribution𝜙, topic distribution 𝜃and topic assignments z is dened as follows: 𝑝 (T | 𝛾, 𝜈, 𝑧) =𝑝 (𝜃| 𝛾)𝜃𝜙𝑑𝜃 This expression can also be rewritten in terms of the topic distribution 𝜃and word distribution 𝜙, in the following way: where parameters𝜃and𝜙should be estimated, which we denote byΦ, such thatΦ= {𝜃, 𝜙}. Then, the log-transformation of the conditional corpus probability p(T | 𝜃, 𝜙, z) is dened as follows: Typically, in order to estimate the LDA model parameters, Variational Bayesian (VB) methods or sampling approaches based on Markov Chain Monte Carlo (MCMC) sampling are being used [5,8]. Figure 2 visualizes the dependencies among the LDA model parameters. High𝛾indicates that it is likely that each document contains a mixture of most of the topics. Conversely, low𝛾indicates that each document contains only few of the topics. Furthermore, high𝜈indicates that each topic contains most of the words of that topic, whereas small𝜈means that each topic contains only small amount of words. The parameters𝛾and𝜈are at thecorpus level which are both assumed to be sampled once in the process of corpus generation. The random variable𝜃is the only variable at the document level, sampled once per document. Finally, the variables zand ware at the word level sampled once for each word per document. The model that we design, called Latent Dirichlet Allocation-Latent Factor Model (LDA-LFM), aims to combine two main core ideas of two methods discussed in Sections 3.1 and 3.2 to to uncover both hidden dimensions in ratings and textual reviews, respectively. As it was mentioned earlier, one of the three entities on which topic modeling is based on, is the document entity. Therefore, the concept of ‘document’ in the LDA-LFM model should be dened properly. There are dierent ways of dening this concept which should be based on the textual reviews. One can simply consider each text review of user u and item i as a document, denoted by d. On the other hand, one can dene a document as a set of all reviews corresponding to item i, denoted by d. Finally, the set of all reviews provided by a user u as a document, denoted by d. [22] found that the second denition, where the concept of a document is dened as the set of all reviews of item i (d), leads to the best model performance. The motivation behind this choice is that when users provide feedback about the products in terms of textual reviews, they discuss more often the characteristics of the product rather than discussing their personal preferences. Therefore, we will dene the concept of documents in LDA-LFM in the similar way as in [22]. The idea behind the LDA-LFM model is to nd the K-dimensional topic distribution𝜃of each item using textual reviews of item i which shows the extent to which each topic k is discussed across all the reviews for item i. Consequently, these topic distributions are used as item-factors in combination with user-factors in the Latent Factor Model to fully predict all user-item ratings. In Section 3.1 we stated that parameter qis the rating factor possessing the properties of item i that can be reviewed by users, whereas in Section 3.2 we stated that parameter𝜃is the topic distribution of words that appear in those reviews. Assuming that, if an item i has a certain property, then it will correspond to a particular topic discussed in that item’s textual review, such that qand𝜃are positively correlated, we need to dene the exact relation between these two parameters. However, qand𝜃cannot be considered as being equal since the topic distribution𝜃is a stochastic vector describing topic probabilities while latent item factor qcan take an arbitrary value inR. Stating that qis a stochastic vector like𝜃 would result in a loss of power in the proposed model and changing the structure of the topic distribution𝜃to make it more similar to qwill lead to the loss of probabilistic power in the model. In order to not encounter these problems, the transformation of qto𝜃Í should satisfy monotonicity, q∈ R, and𝜃= 1 assumptions. The following transformation satises all these criteria: where the parameter𝜅controls for the reaching of the highest possible value of the transformation, often called ‘pickiness’ parameter. Large value of𝜅indicates that users discuss only the most important topic, whereas small𝜅indicates that users discuss all topics equally. We dene the transformation, in such a way that, when𝜅 → ∞,𝜃→ 𝜄(unit vector with 1 for the largest value of𝑞), and, when𝜅 →0,𝜃converges to a uniform distribution. To make sure that the word distribution for topic k (𝜙) is a stochastic vector, the following transformation of𝜙is dened with an introduction of a new variable 𝜓 : where𝜓∈Ris used as a natural parameter for the topic distribution𝜙∈R, where V is the size of the vocabulary. Correspondingly,Í it holds that𝜙= 1. Then the objective function of the LDALFM model is dened as follows: − 𝜇𝑙 (T | 𝜃, 𝜙, 𝑧) whereΘ= {𝛼, b, b, p, q} andΦ= {𝜃, 𝜙} represent the set of parameters of the LFM and LDA model, respectively. The rst term of Equation 14 represents the prediction error corresponding to LFM, the second term represents the regularization of model parameters b, b, pand the third term represents the log-likelihood of the corpus of ratings and users from Equation 11. The parameter𝜇 ∈ 𝑅 trades-o the importance of these two eects. We observe that in the LDA-LFM model, the regularization of qis dierent compared to the standard Matrix Factorization case, the regularization term does not contain the norm of q. More specically, the third term of Equation 14 behaves as a regularization for q[22]. As it was mentioned earlier, the proposed recommender system should allow adding extra user- and item-specic features. A key aspect in adding extra features to the system is to better describe users and items, in order to better predict the preferences of those users for dierent items. Examples of user features are user demographics such as age, living area, gender, occupation, etc. [9]. If our goal is to build a movie recommender, then the genre, year of its release, name of the director, can all be interpreted as item characteristics, which can be added to system for making better recommendations. [6], [12] and [34] introduced CF recommender systems that allow adding user- and item-specic features on the top of ratings. We will follow the approach of [6] and [12], who proposed adding extra rows and columns to the user-item rating matrix representing the extra features added to LFM. Figure 3 visualizes an example of the Matrix Factorization model extended with three extra features. The main idea is to add the same amount of both extra user- and item-specic features. This assumption is necessary because LFM, which is used as rating modelling technique in the LDA-LFM model, requires matrix multiplication of two matrices with dimensions NUsers x K and K x NItems. This matrix multiplication is only possible when the number of columns in user-factor matrix P is equal to the number of rows of item-factor matrix Q. The extra features denoted by Kfrom Figure 3 do not appear in the LDA model and represent non-review factors that aect the review ratings. Our goal is to nd the solution to the optimization problem of Equation 14, which is: where the corpusTis given. The LDA-LFM model denes the following uterative stochastic optimization procedure of two steps: for i in Niter Sample 𝑧with 𝑝 (𝑧= 𝑘) = 𝜃𝜙 end for where Niter is the number of iterations, d≡drepresents the review or set of reviews (document) of item i by user u. In the rst step of this optimization procedure from Equation 16 we x the topic assignments for each word, i.e., the value of latent variable z and we solve the objective function with respect toΘ,Φ and𝜅. We use the Adam Optimizer for learning the rating related model parametersΘ= {𝛼, b, b, p, q}, but also the review related parametersΦ= {𝜃,𝜙}, and𝜅. As it was mentioned earlier,𝜃 ∈ Φ and q∈ Θare linked through Equation 12. So, we do not use the textual reviews in order to t the document-topic distribution𝜃 using the LDA approach. Instead, we determine𝜃using q. Since, we introduced a transformation of𝜙, to ensure that it is a stochastic vector, instead of learning𝜙we learn the parameter𝜓. Once we learn the𝜓, by using the transformation dened in Equation 13, the topic-word distribution𝜙can be determined. Moreover, using the same optimization approach, we also learn the parameter 𝜅. In the second step of this iterative procedure, using the updated parameter valuesΦ= {𝜃,𝜙} determined in the rst step by the Adam Optimization, we randomly assign a topic k to each word, with a probability that is proportional to the likelihood of the occurrence of that topic with that particular word [30]. That is, the topic assignment probability of assigning kth topic to a word w for user u, item i and in jth position p(z= k) is proportional to the product of topic probability for user u, item i (𝜃), and word probability used for that topic (𝜙). We assume that the terms zand zare equivalent (z≡z). We iterate through all documents and word positions, d and j, respectively, in order to update the corresponding topics assigned to those terms. Finally, we repeat these two steps for Niter times and report the prediction accuracy of the model corresponding to the last iteration. As a prediction accuracy measure we use the Mean Squared Error (MSE) determined as follows: whereTrepresents the corpus of all ratings in the test set,𝑟 represents the real rating from the test data for user u and item i, andˆ𝑟is the corresponding predicted rating. MSE can take only non-negative values. Moreover, a lower value of MSE is an indication of better performing model. It is worth to mention that the analysis is performed on a commodity machine with a Core i7 processor, 2.2 GHz frequency, and 252gb memory space using the programming language Python 3.7. In this research, we use a collection of datasets provided corresponding to the 23 product categories supplied by one of the largest e-commerce company in the world, Amazon.com. This data without duplicates was prepared by Julian McAuley. It consists of 142.8 million product reviews and a metadata for 9.4 million products, spanning a period of 18 years, from May 1996 to July 2014 [10,23]. The chosen dataset is of 5-core type, that is, the data set excludes all customers and products having less than 5 reviews. The review dataset includes feedbacks of Amazon customers in the form of ratings, textual reviews, and helpfulness score. Meanwhile, the metadata includes various characteristics of the product: price, brand, descriptions, category information, image features and links of ‘also viewed’ and ‘also bought’ products. The raw review data, after removing duplicates and excluding users or items with less than 5 reviews, consists of 42.13 million reviews. Table 1 presents the general overview of the datasets of all product categories. We observe that all datasets are highly sparse and contain a very large amount of missing ratings. For almost all datasets it holds that the average star rating is approximately equal to 4. Moreover, the average number of words per review is at least 18 and at most 67. Finally, the smallest dataset, Musical Instruments, consists of 0.5 million reviews and the largest dataset, Electronics, consists of approximately 8 million reviews. In order to correctly evaluate the chosen model, we split the data into three datasets: training, validation, and test sets. We t the model on the training data and nd a set of optimal model parameters (hyperparmeter tuning) using the validation set. Finally, we use the test set for predicting the ratings and calculating model accuracy measures using the optimal set of parameters from the hyperparmeter tuning. For data separation we use the common 80/20 splitting rule. In order to have enough observations to correctly t the model, we put 80% of all observations in the training set, while the remaining 20% we equally divided into the test and validation sets. However, splitting the data into training, test and validation sets, when some of the users and items appear only in the test set and not in the training set, will result in a loss of information about those users and items during the training of the model. Therefore, after randomly splitting the data into train and test set, we make sure that there is no user or item that is present in the test set but not present in the training set by removing these. For implementing the topic modeling technique LDA, the review data should be cleaned. Therefore, we perform a few Natural Language Processing (NLP) tasks on the textual reviews in review tuples by using the Natural Language Tool Kit (NLTK) library of the programming language Python. Firstly, we apply tokenization to all review texts, which are provided as a group of sentences, and transform them into a group of words. Secondly, we transform them to lower case words and remove from these tokenized reviews the common English stop words and one-letter words. Subsequently, all special characters, digits, punctuation and single or multiple spaces are removed. Next, we apply lemmatization to the processed review text, for removing inectional endings and holding the dictionary (base) form of a word only, known as the lemma of the word. Finally, we combine all those cleaned reviews corresponding to the same item and create a corpus of documents, where each document contains all reviews (represented in the form of a group of words) corresponding to one item. Dierent methods introduced earlier contain various parameters which should be initialized. We initialize the oset𝛼by averaging over all ratings in the training set. Vectors b, band matrices P, Q are initialized using the random normal distribution. The tting procedure of all models have been performed by the Adam Optimization with the learning rate 0.01. As initial value for𝜅we take the value 1, which will be updated by the Adam Optimization while tting the model. For each model we run 35 iterations based on [4] (with 20 iterations), [15] (with 20-35 iterations), [24] (with 30 iterations) while updating model parameters inΘ,Φ, and𝜅in each iteration. The prediction accuracy of the model is reported based on the last model corresponding to 35th iteration, assuming that the last model, after all the updates, is the best performing model. As a common practice, for the LDA model we set both parameters 𝛾and𝜈equal to 0.1. Following the approach of [22] we perform the analysis with the number of latent factors in the LFM model (K) and number of topics in the LDA model (L) equal to 5. LDA-LFM contains two regularization parameters,𝜆and𝜇. We tune this set of two parameters using the Grid-Search method, which ts the model for every specied combination of these two parameters and evaluates each of these models using validation set. As a result, the most accurate model specication, per product category, is then used in the main model prediction applied to the test dataset. Following the approach of [22], for𝜆we use values {0, 0.001, 0.01, 1, 10} as a possible values in the Grid-Search, while for regularization constant𝜇we use the values {1, 10, 100, 1000, 10,000}. As it was mentioned in Section 3.3, we set the number of documents in the LDA model equal to the number of items in the data, where each document represents all reviews of an item in the training set. We set the size of the vocabulary equal to 5000, by keeping most 5000 frequent words from the corpus of all documents built from the item reviews present in training set. In order to test for the performance of the proposed LDA-LFM model, we use four other models based on dierent algorithms. Then we compare the prediction accuracy of the LDA-LFM model with the performance of the following models: Oset Model:the predicted rating for all users is the same and is equal to the global average 𝛼. Baseline Rating Model:the predicted ratingˆ𝑟=𝛼+¯𝑟+¯𝑟 with𝛼representing the global average rating,¯𝑟the average dierence between user ratings and the global average𝛼,¯𝑟represents the average dierence between item ratings and the global average LFM:standard Latent Factor Model model corresponding to Equation 2. LDAFirst:in this model the user feedback in the form of ratings will be used as an input for the standard LFM, while the textual reviews will be used as an input for the LDA model described in Section 3.2. The key dierence between this method and the proposed method LDA-LFM is that, in LDAFirst the topic-distributions 𝜃are sampled from a Dirichlet distribution, where each document is treated as the set of all reviews corresponding to item i, and they are used to set the qvalues, which stays constant while modeling the ratings. Thus we do not learn the qparameter of LFM during the iterative optimization procedure and we only update the parameters b, band pusing the Adam Optimization. In the LDA-LFM model, we do not use the LDA method for determining the topic-distributions𝜃. After that we sample the word topics, we learnΦ= {𝜃,𝜙} using Adam Optimization as in Equation 16 (where qis dependent on𝜃by means of Equation 12). Since we start this method by the LDA model and use its output (𝜃) as an input for the LFM method (𝑞), we refer to this method as LDAFirst. Firstly, we perform the analysis for the case when no extra features are added to the LDA-LFM model, assuming that the number of topics in the LDA model is equal to the number of latent factors in the LFM model with K∈{5,10} [22]. Correspondingly, in order to analyse the impact of adding extra latent features (user- and item characteristics) on the performance of recommender system based on the proposed LDA-LFM model, such that the number of topics in LDA has value 5 and the number of latent factors with 4 dierent values of extra features K∈ {1, 2, 3, 4}. Table 2 presents the prediction per product category with the number of topics equal to 5. We observe that for the majority of supplied datasets it holds that the Oset and Baseline models perform the worst, with large MSE values, compared to the LFM, LDAFirst, and LDA-LFM models. Only for Video Games and Tools and Home Improvements datasets the Oset method performs better than the LFM and LDAFirst models. Moreover, we observe that compared to the Oset and Baseline models, standard LFM improves the recommender systems prediction accuracy for almost all datasets, except datasets Patio, Lawn and Garden, Video Games, and Tools and Home Improvement. This can be seen by the large dierence between the MSE values corresponding to LFM, and MSE values corresponding to the Oset and Baseline models. We also observe that the MSE’s corresponding to the LFM and LDAFirst models are very close to each other for the majority of datasets. This means that the LDAFirst model does not improve the prediction accuracy a lot compared to the standard LFM model. LDAFirst slightly outperforms LFM in case of the datasets Baby,Oce Products Grocery and Gourmet Food, Apps for Android, CDs and Vinyl. From Table 2 we observe that the LDA-LFM model outperforms all other models in almost all datasets, with its lowest MSE values. Last two columns of Table 2 present the percentage decrease in the MSE of the LDA-LFM model compared to the LFM and the LDAFirst models, respectively. Both improvement columns consist mostly of positive entries. We observe that the proposed LDA-LFM results in at least 0.24% (Health and Personal Care) and at most 14.12% (Kindle Store) improvement in prediction accuracy, compared to the standard LFM. From Imp.we observe that the proposed LDA-LFM results in at least 0.28% (Electronics) and at most 14.12% (Kindle Store) improvement, compared to the LDAFirst model. The improvement columns in Table 2 contain also few negative values which correspond solely to small datasets (Musical Instruments, Amazon Instant Video, Patio, Lawn and Garden). Table 2 also reports the average MSE over all datasets per model in case of K = 10. We observe that average MSE’s per model with K = 5 and K = 10 are similar. Table 3 presents the prediction results in terms of MSE, per product category with the number of topics equal to 5 and extra added features. We observe that for almost all datasets there is at least one LFM-LDA model with extra feature(s) with higher prediction accuracy (lower MSE value compared to the corresponding MSE value in the K= 0 model, i.e., without extra features). Moreover, for the datasets Musical Instruments, Patio, Lawn and Garden, Automotive, Toys and Games, Health and Personal Care, Sports and Outdoors, CDs and Vinyls, Home and Kitchen and Movies and TV all four models with dierent number of extra features are performing better compared to the model without extra added features. However, there are few datasets (Amazon Instant Video, Oce Products, and Beauty) for which it holds that adding extra features to the LDA-LFM model either does not change or worsens the performance of the model. We observe that all those datasets, for which adding extra features is not ecient, are either very small or medium size datasets in the set of all 23 Amazon datasets used in this study. The last row of Table 3 presents the number of cases in which adding a particular amount of extra features leads to an increase in the prediction accuracy of the model. We observe that in all four cases (adding 1, 2, 3, and 4 extra features), the number of datasets with better performance are close to each other. More specically about 15 out of 23 datasets (from which around 9 cases corresponds to a medium or a large dataset), adding extra features results in more accurate recommendations. Taking into account the current limitations in the existing literature, in this paper, we utilize the LFM model by using both ratings and textual reviews of customers, such that it is applicable on both small and large datasets and also allows adding user- and item-specic data to it. We have shown how one can combine review-based LDA for topic modeling, with rating-based LFM for rating predictions. From the prediction results where we compared the prediction accuracy of the proposed LDA-LFM model to the prediction accuracy’s of various baseline models applied on various datasets. We found that adding textual reviews to the recommender system leads to an increased prediction accuracy, which is especially true for medium and large datasets. Then we introduced an approach of adding extra latent features to the user-item rating matrix of the proposed LDA-LFM model, representing the user- and item-specic features not present in the review data. We found that for the majority of datasets (15 out of 23 datasets) it holds that, adding extra features to the proposed recommender system increases the quality of its recommendations, resulting in lower MSE, thus higher prediction accuracy. This indicates that again the improvements are better visible in medium and large datasets. As future work we would like to investigate whether using sentiment analysis improves the quality of recommendations. Correspondingly, one can extend our model so that it combines the rating modeling, topic extraction, and sentiment analysis techniques for making recommendations. For instance, sentiment analysis can be used to classify whether a review is negative or positive. [36] proposed a recommender system combining rating based CF system with sentiment analysis for making recommendations. [19] introduced the Joint Sentiment/Topic (JST) model which combines the topic modeling method LDA with sentiment analysis in order to detect a topic and a sentiment from text simultaneously. For future work, we aim to combine the JST model with our model for potentially better recommendations. In this way, we can exploit topics that carry sentiment and are possibly better proxies for ratings.