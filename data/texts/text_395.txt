Personalized outt recommendation has recently been in the spotlight with the rapid growth of the online fashion industry. However, recommending outts has two signicant challenges that should be addressed. The rst challenge is that outt recommendation often requires a complex and large model that utilizes visual information, incurring huge memory and time costs. One natural way to mitigate this problem is to compress such a cumbersome model with knowledge distillation (KD) techniques that leverage knowledge from a pretrained teacher model. However, it is hard to apply existing KD approaches in recommender systems (RS) to the outt recommendation because they require the ranking of all possible outts while the number of outts grows exponentially to the number of consisting clothing items. Therefore, we propose a new KD framework for outt recommendation, called False Negative Distillation (FND), which exploits false-negative information from the teacher model while not requiring the ranking of all candidates. The second challenge is that the explosive number of outt candidates amplifying the data sparsity problem, often leading to poor outt representation. To tackle this issue, inspired by the recent success of contrastive learning (CL), we introduce a CL framework for outt representation learning with two proposed data augmentation methods. Quantitative and qualitative experiments on outt recommendation datasets demonstrate the eectiveness and soundness of our proposed methods. Personalized Outt Recommendation, Knowledge Distillation, False Negative Distillation, Contrastive Learning Personalized outt recommendation is the task of determining the preference of a user to an input outt that consists of multiple clothing. It has recently attracted attention with the rapid growth of the online fashion industry, and several related studies [22,24,25] have been conducted. However, despite the success of existing works, outt recommendation has two signicant challenges that should be addressed. First, recommending outts often requires a complex and large model that involves the utilization of visual information (i.e., images) [24]. Such a large model incurs high latency and memory costs during the inference phase, making it dicult to apply to real-time services [14]. The second challenge is that outt recommendation inevitably suers from the data sparsity problem because the possible pool of outt data grows exponentially to the number of consisting clothing items [22]. The sparsity problem often leads to poor learning of outt representation, which hinders achieving satisfactory recommendation performance [40]. Figure 1: The user-outt space is a representation vector space of the trained teacher model. We dene the positive boundary as an average distance between a user and its positive outts. We treat negative outts inside the positive boundary as false-negative outts. To address the rst challenge stemming from a large model, one can employ knowledge distillation (KD) techniques that compress a model by transferring knowledge from a large teacher model to a small student model. Accordingly, one may try to apply existing studies [14,18,35] of KD available in recommender systems (RS) to the outt recommendation. However, existing methods leverage predicted ranking of all possible outts from the teacher model, so they are not applicable in outt recommendation tasks with explosively large pools. Therefore, we propose a novel KD framework named False Negative Distillation (FND) that does not require the ranking of all outt candidates. Similar to most outt recommendation studies [22,24,25], FND utilizes a ranking loss that pulls observed (positive) outts to a user while pushing unobserved (negative) outts. As illustrated in Fig. 1, FND claims that unobserved is not the same as true-negative and assumes that negative outts close enough to the user are false-negative outts. We show through various experiments that FND is eective, and the assumption is reasonable. The approach for the second challenge is to deal with the problem of poor outt representation. As learning enhanced representation of entities is one of the core components to achieve high performance in deep learning [2], numerous works [3,6,8,27] from diverse domains accomplished this with self-supervised learning (SSL) techniques. There have been several studies [26,41,44] utilizing SSL techniques in RS as well. Among them, more recent works [23,40,43] exploit contrastive learning (CL), especially SimCLR [3], which learns meaningful representations by pulling the dierently augmented view of the same data while pushing the others in the batch. Nevertheless, leveraging CL in outt recommendation is relatively unexplored. Hence, as illustrated in Fig. 2, we introduce Figure 2: Example of contrastive learning for outt recommendation. We randomly alter (erase/replace) one item from an outt to generate two dierent views. In the case of replace, it substitutes an item with a similar one using our pretrained autoencoder model. In this example, the rst and the second augmentations are erase and replace, respectively. The left outt erased the shoes and replaced the top. The right outt erased the bottom and replaced the bottom. an approach to make use of CL in outt recommendation, along with two proposed data augmentation methods (erase /replace). To demonstrate the eectiveness of our proposed methods, we conduct extensive experiments on outt recommendation datasets. We compare our approaches with state-of-the-art outt recommendation methods with quantitative performance evaluations. In addition, we study the impact of hyper-parameters and the model size with various experiments. We use visualization to show that the intuitive assumption of our FND illustrated in Fig. 1 is sound. We also experiment on the cold-start scenario where users have very few outts interacted, and the trained model does not have any knowledge of those users. To make appropriate recommendations to cold starters, we introduce two practical strategies that do not require additional training of the model. Our main contributions can be summarized as follows: •We propose a new knowledge distillation framework that can be utilized in outt recommendation tasks without requiring the ranking of all outt candidates in the system. •We propose two novel outt data augmentation methods to leverage contrastive learning in outt recommendation. •We introduce two practical strategies to deal with the coldstart problem. •We demonstrate the eectiveness and soundness of our approaches with comprehensive experiments on fashion outt recommendation datasets. Based on whether the individual preference is neglected or not, existing outt recommendation studies can be classied into two categories: non-personalized [4,10,20,32,34,37,38,42] and personalized [12,19,22,24,25] outt recommendation. Lu et al. [25] used pairwise scores, and they employed the weighted hashing technique to tackle the eciency problem. Lin et al. [22] utilized an attention mechanism to estimate the preference score, weighting items in an outt dierently. Lu et al. [24] exploited Set Transformer [17], the state-of-the-art model for set-input problems, to capture the high-order interactions among fashion items. They also disentangled each user into multiple anchors to accommodate the variety of preferences. Note that methods based on graph neural networks [4,19,32] or predicting distribution over whole clothing items [42] require the test items to be in the training set. Knowledge distillation is a model-agnostic compression strategy for generating ecient models. Since the early success of KD in image recognition [11,30], KD has been widely accepted in other elds. In recommendation tasks, several works [14,18,35] have employed KD. They rank all items with the teacher model and utilize the items of high rank when training the student model. Tang et al. [35] considered top-𝐾items as false-negatives and dierentiated their relative importance based on their rankings. Lee et al. [18] trained the student to mimic the predicted probabilities of the teacher on the sampled items of high rank. Kang et al. [14] achieved state-ofthe-art performance by transferring both the prediction and latent knowledge of the teacher. Contrastive learning is a framework for obtaining high-quality representations to boost the performance of downstream tasks and was rst introduced in computer vision [3]. CL enhances representations by maximizing agreement between two dierently augmented views of the same data. A few works [23,40,43] applied CL to RS, and they showed notable success. In sequential recommendation, Xie et al. [40] used CL by applying three augmentation methods (crop/mask/reorder) to user interaction history. Yao et al. [43] focused on large-scale item recommendations and employed a two-stage augmentation consisting of masking and dropout. Liu et al. [23] utilized CL for graph neural network based RS by randomly removing some edges. We recommend outts to users based on their preference score. To compute the preference score, we use user embeddings and vector representations of outts. Due to the set-like nature of fashion outts, the representation model requires two conditions. First, the outt representation should be invariant to the order of comprising fashion items. Second, the model should be able to process input outts of any size. To this end, we borrow the architecture from LPAE [24] model, which uses Set Transformer [17] module designed to address these set-input problems. As illustrated in Fig. 3, an outt𝑜with𝑛items is a tuple of fashion item images:𝑜= (𝑥, 𝑥, · · · , 𝑥) ∈ I.Let𝑓:I → Rbe a Figure 3: A brief architecture of computing the preference score of a user to an outt. First, images comprising the outt are transformed into item features by CNN with fully connected layers. After that, we get an outt representation using Set Transformer. Finally, the cosine similarity between a user embedding and the outt representation is the preference score of the user to the outt. Convolutional Neural Network (CNN) with fully connected layers that encodes𝑥into an item feature vectorx= 𝑓 (𝑥), where𝑑 is a feature dimension. Through Set Transformer𝑇:R→ R, we obtain an outt representationo= 𝑇 (X)from item features X= [xx· · · x].Then, we compute the preference score with user embedding u∈ Rfor each user 𝑢. 3.1.1 Set Transformer. Having beneted from the attention mechanism, Set Transformer can eectively reect high-order interactions among items in an outt. As Lee et al. [17] have proved, Set Transformer is an order-free module that always produces the same output regardless of the sequence order. Attention is a map that gives the weighted sum of value vectors V ∈ Rwith the weights being determined by each query vector of Q ∈ Rand key vectors K ∈ R: The multi-head attention utilizes multiple attentions through concatenation to bear more potential relationships: forℎattention maps, Following previous works [17,24], we use𝑑= 𝑑/ℎand𝑑= To apply the attention mechanism to a set, Set Attention Block (SAB) uses self-attention with residual terms: for item features X, where𝜎is any row-wise feed-forward layer, andLayerNorm(·)is Layer Normalization [1]. Multiple SABs can be stacked to encode higher-order interactions among the items: The nal outputs of the attention blocks are then aggregated by applying another multi-head attention on a learnable seed vector s ∈ Ras follows: The obtainedo∈ Ris a single compact vector representation of an outt𝑜, holding compatibility relationships among consisting fashion items. 3.1.2 Preference score prediction. Given a user𝑢and an outt𝑜, our model predicts the preference score of the user to the outt as follows: Large models generally show relatively higher recommendation performance compared to their smaller counterparts. However, employing a small-sized model is necessary to reduce latency and memory costs during the inference phase. Therefore, we propose a novel knowledge distillation framework named False Negative Distillation (FND) that transfers false-negative information extracted from a well-trained large teacher model to a small student model. As illustrated in Fig. 1, in the user-outt space of a trained teacher model, we assume that negative (i.e., unobserved) outts close enough to the user are false-negative outts. 3.2.1 Teacher model. Deep learning based recommendation models adopt learning to rank framework via deep metric learning in general. The goal is to maximize the ranking of positive outts given a predicted preference score. Many existing works [22,25,34], including LPAE [24], use triplet loss [31] or Bayesian personalized ranking (BPR) [29] as an optimization objective. However, they often suer from poor local optima, partially because the loss function employs only one negative outt in each update [33]. To address this problem, our model utilizes𝑁-pair loss [33]. Aided by the temperature-scaled cross-entropy,𝑁-pair loss can take multiple negative outts into account per positive outt. Let the batchB, size of𝑁, be a set of pairs(𝑢, 𝑜), indicating that the user𝑢prefers the outt𝑜. Each pair in the batch has a set of negative outts {𝑜}, sampled in the training step. Note that the negative set mainly contains randomly generated outts and even can include positive outts of other users. Our objective for the teacher model is as follows: where 𝜏> 0 is a temperature hyper-parameter. 3.2.2 Student model. Once the teacher model is trained, we optimize our student model with the help of additional false-negative information. The ordinary𝑁-pair loss can be interpreted as “pulling” positive outts to a user while “pushing” the negatives, similar to triplet loss with user-anchor. Whenever a given negative outt is determined as a false-negative, we wish to pull it rather than pushing it. Concretely, we determine the false-negativeness𝑑 based on the dierence between the preference score of a negative outt and the average score of the positives: with𝐽a set of indices of positive outts for a user 𝑢, where𝛼 >0 is a distillation scaling hyper-parameter andˆ𝑟denotes the predicted score from the teacher. The sign of𝑑determines whether the given negative outt is false-negative or not, and the magnitude presents how much the negative should be pushed or pulled. The student model is trained through our proposed FND loss Las follows: ℓ(𝑖, 𝑗) = − logexp𝑟/𝜏, (13) Since the teacher model is frozen when training the student, 𝑑stays constant for each𝑖and𝑗. Note that we use dierent validation sets when training the teacher model and the student model. Otherwise, the student model might learn information about the validation set via the teacher, which leads to overtting. By examining the gradient ofL, it can be shown that our objective function pulls negative outts to the user whenever they are determined as false-negatives by𝑑. Suppose𝑜is an element of the set of negative outts{𝑜}for(𝑢, 𝑜)in the training step. The gradient ofℓw.r.t the preference score of𝑢to𝑜is as follows: The sign of the gradient is the same as𝑑since𝜏>0 and 𝑝 (𝑟) >0 hold. Hence, negative outts closer than the average of positive outts are pulled toward the user rather than pushed. To obtain more enhanced outt representations, we propose a novel approach to leverage SimCLR [3] framework in outt recommendation. Specically, we suggest two data augmentation methods for outts. SimCLR learns representations by maximizing the agreement between dierently augmented views of the same outt while pushing the others in the batch. Given the batchB = {(𝑢, 𝑜)}, each outt𝑜is augmented twice to create two dierent views (𝑜, 𝑜), generating 2𝑁augmented outts{𝑜}in total. The agreement is measured by cosine similarity between each outt representation:𝑠= cos𝑔(o), 𝑔(o), where𝑔(·)is a non-linear projection layer. The objective of contrastive learning is as follows: L=12𝑁[ℓ(2𝑛 − 1, 2𝑛) + ℓ where 𝜏> 0 is a temperature hyper-parameter. To exploit the objective in Eq. 18, we must dene appropriate data augmentation methods which produce semantically similar outts with an input outt. As illustrated in Fig. 2, we suggest two augmentation methods suitable for outt recommendation: erase and replace. Both augmentations randomly alter comprising items from an outt while preserving the semantic context. We treat the augmentation set as a hyper-parameter and x them at the beginning of the training. Note that if two identical augmentations are applied, we alter dierent items from the input outt to obtain distinct views. 3.3.1 Erase. Randomly erasing components from the input is a common data augmentation method in diverse domains. In sequential recommendation, for example, Xie et al. [40] randomly crop items from user interaction history. In natural language processing, Wu et al. [39] erase or replace randomly selected words in a sentence. In computer vision, DeVries et al. [7] cut out contiguous sections of an input image, inspired by the object occlusion problem. When it comes to outt recommendation, a subset of an outt may imply or even determine the semantic information. Motivated by this, we randomly remove one item from the outt to generate an augmented view. 3.3.2 Replace. Our proposed model computes preference scores based solely on visual information (i.e., images), and this assumption is helpful if access to other metadata is limited. In this situation, the semantic information of an outt is derived from the appearance of consisting items. Accordingly, we claim that the visual similarity of consisting items leads to the semantic similarity of an outt. Based on the claim, we generate an augmented view by randomly replacing one item from the outt with a visually similar item from the same category. To this end, we train a CNN autoencoder model and retrieve similar items through their latent features. The proposed lossesLandLcan be used independently; hence we can take advantage of both methods. Therefore, our nal objective is to minimize the weighted sum of both losses as follows: where 𝜆 is a loss weight hyper-parameter. In application services with recommender systems, new users can join the service even after the model is trained and deployed. Such users, or cold starters, have relatively few interactions in general, and the deployed model does not have any prior knowledge of those users. In practice, ne-tuning the model for them is a timeconsuming process; thus, cold starters might starve for the recommendation until the next iteration of deployment. Therefore, it is necessary to have an alternative recommendation method that exploits the already deployed model with no additional training. In personalized outt recommendation, only a few works [24] handled the cold-start problem without ne-tuning the model. Here, we introduce two strategies to compute preference scores of cold starters, analogous to memory-based collaborative ltering. Given a cold starter𝑢, we dene a neighborhoodNfrom the set of non-cold usersUas follows: with a set of indices of positive outts 𝐽for 𝑢, where𝑠represents the asymmetric similarity from𝑢to𝑢.𝛿is a similarity threshold, and𝑖= argmax𝑠denotes the index of the most similar user, which ensures at least one neighbor for each 𝑢. To compute the preference score𝑟of the cold starter𝑢to a given outt𝑜, we aggregate the preference scores of neighbors to the outt. Here we use two aggregation strategies: Average and Weighted Average. 3.5.1 Average (avg). A basic aggregation strategy is simply averaging the preference scores: 3.5.2 Weighted Average (w-avg). We further utilize the similarity between the cold starter and its neighbors as aggregation weights using cross-entropy with temperature: where𝜏>0 is a temperature hyper-parameter, and note thatÍ 𝑠=1 holds. We considered other methods for deriving the aggregation weights𝑠from𝑠; however, the suggested method empirically showed the best and stable results, especially in terms of robustness to hyper-parameters. 4.1.1 Datasets. We use datasets collected from the Polyvore website: Polyvore-𝑈[25], where𝑈 ∈ {630,519,53,32}denotes the number of users. Polyvore-𝑈contains outts posted by users, each consisting of three categories: top, bottom, shoes. Outts in Polyvore{630, 53} have a xed number of items: one item for each category. Polyvore-{519, 32} include outts with a variable number of items (i.e., some outts may have two tops). We use Polyvore-{630, 519} for most of the experiments and Polyvore-{53, 32} for cold starter tasks. Statistics of the datasets are provided in Table 1. Following previous works [24,25], we dene user-posted outts as positive outts for each user and category-wise random mixtures of items as negative outts. We also discuss the results of hard negative outts (i.e., random samples of positive outts of other users) separately in Sec. 4.4. In the evaluation phase, we set the ratio between positive and negative outts to 1:10 for each user. We split training, validation, and test sets to 9:2:2, and we further split the validation set into two halves, one for the teacher model and the other for the student model. As [25] armed, there are no duplicate items between the training and the test sets for each user. 4.1.2 Evaluation metrics. We evaluate the ranking performance via Area Under the ROC curve (AUC) and Normalized Discounted Cumulative Gain (NDCG), similar to previous works [24,25]. For each user, we rank the test outts by the predicted preference score of the model. We report the performance averaged over all users. 4.1.3 Considered methods. We compare our methods with the following state-of-the-art non-personalized [10,34,37] and personalized [22,24,25] outt recommendation models. Type-Aware [37] projects pairs of items onto the type-specic subspaces. Compatibility is then measured in these subspaces and learned through the triplet loss. SCE-Net [34] learns conditional embeddings and their weights using an attention mechanism. Each conditional embedding is implicitly encouraged to encode dierent semantic subspaces via the triplet loss. Bi-LSTM [10] considers an outt as a sequence of items and uses a bidirectional LSTM to learn the compatibility. The model is trained by predicting the next and previous items in the sequence through cross-entropy loss. OuttNet [22] consists of two stages to capture both general compatibility and personal taste. The objective of both stages is to maximize the dierence between positive and negative scores, similar to BPR. FHN [25] uses pairwise scores to compute outt compatibility and personal preference simultaneously. We train FHN with BPR without the binarization step, following the previous work [24]. LPAE [24] includes two models LPAE-u and LPAE-g, which mainly handles the cold-start problem using multiple anchors for each user. Both models utilize BPR loss, and LPAE-g has additional general anchors to model non-personalized compatibility. For a more fair comparison, we apply temperature scaling when using BPR or cross-entropy loss. 4.1.4 Implementation details. Similar to the previous work [25], we use AlexNet [16] pretrained on ImageNet [5] as a backbone CNN. We dene two versions of AlexNet to experiment the knowledge distillation. One is AlexNet-large, which is the original AlexNet. Table 2: Performance comparison of dierent methods on Polyvore datasets. The other is AlexNet-small, a downsized version of AlexNet that all fully-connected layers are removed and with a global average pooling at the end [21]. Only the teacher model uses AlexNet-large, and the others use AlexNet-small. We set the feature dimension 𝑑 =128 for all methods. For simplicity, we set𝛿to zero, the median of the possible range of cosine similarity. We use𝜏= 𝜏=0.1, 𝜏=0.2,𝜆 =0.2, and the number of headsℎ =8. We set𝛼 to 1.25 for Polyvore-630 and 1.5 for Polyvore-519. When it comes to CL, the pair of augmentations are (erase, replace) for Polyvore630 and (erase, erase) for Polyvore-519. SGD with momentum [28] is used to train all methods, and the batch size is set to 32. For each method, we report the test performance with their optimal hyper-parameters searched via the validation set unless otherwise specied. As shown in Table 2, our proposed FND outperforms baseline methods under all datasets and metrics. Furthermore, the performance of FND-CL shows the eectiveness of the outt CL framework. Recall that LPAE-u adopts multiple anchors for representing each user, and LPAE-g further leverages non-personalized compatibility. Comparison with LPAE models shows that FND can eectively achieve improved performance without auxiliary parameters and structures. We evaluate the ranking performance of recommending to the cold starters. Concretely, we test a scenario where a model is trained in Polyvore-{630, 519}, and cold starters in Polyvore-{53, 32} desire recommendations. Following the previous work [24], we experiment on the circumstances that each cold starter has only 1 or only 5 interacted outts. We compare our methods with non-personalized and LPAE methods, which do not require additional training of the model. For LPAE methods, we use the anchor-search [24], which is known to be the most eective strategy in the cold-start case. In FND and FND-CL, we evaluate both avg and w-avg strategies. We conduct experiments 10 times and report the average results in Table 3: Comparison of dierent methods on cold starters. For our FND and FND-CL, w/o and w/ “(w)” represent the avg and w-avg strategies, respectively. We use AUC as an evaluation metric. Type-Aware [37] 72.79 72.79 77.44 77.44 Table 4: Comparison of dierent methods on hard negative outts. Table 3. The results show that our approaches consistently outperform baseline methods even though the primary purpose of LPAE methods is to deal with the cold-start problem. The w-avg strategy is more eective than the avg strategy in both FND and FND-CL, implying the importance of considering neighbors dierently based on similarity rather than treating them equally. We test a more challenging case where negative outts in evaluation are composed of positive outts of other users (i.e., hard negatives). We only compare with personalized methods since nonpersonalized methods cannot distinguish users. Following the previous work [25], we set half of the negative outts to hard negative outts when training the model. We report the results in Table 4. The results show that LPAE-g, which additionally considers nonpersonalized compatibility, performs poorly in this task compared to LPAE-u. We can see that our FND and FND-CL outperform the baseline methods. Note that the eectiveness of the CL framework is more apparent on hard negative outts than the results in Table 2. We believe that the functionality of CL to distinguish outts enables the model to capture more meaningful outt representations, especially in the hard negative setting. Figure 4: Comparison of dierent 𝛼 on Polyvore datasets. Figure 5: Comparison of dierent augmentation methods on Polyvore datasets. Augmentation pair XY represents that the rst and the second augmentations are X and Y. We also include the identity function as an augmentation method. I/E/R indicates identity/erase/replace augmentations. We evaluate the performance of FND under various𝛼(see Eq. 12). The results are reported in Fig. 4, and we also show the performance of𝑁-pair and BPR in the same gure. As mentioned in Sec. 3.2.1, 𝑁-pair overcomes the partial shortcoming of BPR by considering multiple negative outts in each update and thus clearly outperforms BPR. Dierent datasets tend to have dierent optimal𝛼, but given adequate value, FND can surpass the performance of a strong 𝑁 -pair. To test the performance of FND-CL for all possible augmentation methods, we put the identity function into the set of augmentations. The results are shown in Fig. 5, and we also report the performance of FND in the same gure. Note that we did not experiment on the (identity, identity) augmentation pair because two augmented views should be dierent. Regardless of which augmentation pair we use, FND-CL outperforms FND in almost all cases, and the optimal augmentation pair is dierent for each dataset. Concretely, replace and erase augmentation methods tend to be more eective at Polyvore-630 and Polyvore-519, respectively. Therefore, we can see that both erase and replace are meaningful augmentation methods and that the CL framework is eective in outt recommendation. Figure 6: Comparison of dierent model sizes on Polyvore datasets. We study the impact of model size on performance, especially in the case of the student model. We consider student models with three dierent sizes (i.e., XS, S, M) and the teacher model. XS uses AlexNet-small, and S and M use a downsized version of AlexNet that output dimensions of all fully-connected layers are reduced to 1/4 and 1/2, respectively. Fig. 6 shows the results. It is clear that the larger the size, the better the performance. Moreover, the fact that FND-CL outperforms FND and FND outperforms𝑁-pair is consistent regardless of the size of the model, supporting that our approaches are meaningful. The performance gap between FND and𝑁-pair appears to shrink with the increasing size of the student model. Such a tendency implies that the eectiveness of FND depends on the performance gap between the teacher and the student model, which implicitly emphasizes the importance of utilizing the superior teacher model. On the other hand, we can see that the eectiveness of the CL framework is hardly aected by the size of the model, as expected. Detailed information on inference eciency is measured for each model of dierent sizes and reported in Table 5. We conduct experiments using FND for the student model and𝑁-pair for the teacher model. Since FND aects only the training step, FND and 𝑁-pair share the same inference eciency. Note that if CL is added, only the number of parameters increases by about 0.03M. In all inference tests, we use PyTorch with CUDA from Tesla P100 SXM2 GPU and Xeon E5-2690 v4 CPU. From the results, we can see that as the size of the model increases, all metrics that indicate ineciency (i.e., Time, Memory, # Params) also increase. To study the impact of the batch size, we test the performance of FND-CL with dierent batch sizes. Note that we use a linear scaling of the learning rate when training with dierent batch sizes [9,15]. Fig. 7 shows the results of the experiment. The performance tends to be improved as the batch size increases and appears to converge when the batch size exceeds a certain threshold. We believe that the number of negative samples (i.e., outts and augmented views) proportional to the batch size is the primary factor of this tendency. Note that except for the number of negative samples, we do not use any other factors signicantly inuenced by the batch size, such as batch normalization [13]. Therefore, we can see from the results in Fig. 7 as well as Fig. 4 that it is important to exploit a Table 5: Model compactness and inference eciency. “Time” denotes model inference time for making recommendations to every user in each dataset, and we report the mean and standard deviation of 10 runs. “Memory” represents GPU memory usage. “Ratio” indicates the relative parameter size of the student model compared to the teacher model. Polyvore -630 Polyvore -519 Figure 7: Comparison of dierent batch sizes on Polyvore datasets. sucient number of negative samples per each update when using the ranking loss. We visualize the user-outt space of the teacher model to support the intuition of FND (see Fig. 1). The visualization uses𝑡-SNE [36] and shows three users and their positive and negative outts from the training set. We focus on the training phase since the approach of FND is to distill knowledge from the teacher model when training the student model. The results are shown in Fig. 8. Recall that negative outts are randomly generated, and thus a positive outt from the test set can appear as a negative sample by pure chance in the training step. With the help of the teacher model, the student model can treat such samples as false-negatives, denoted as a dashbordered rectangle in the gure. Moreover, other negative outts close to a user share a similar style with positive outts, showing the possibility of being false-negatives. Hence, we can conclude that the approach of FND that utilizes false-negative information from the teacher model is reasonable. In this paper, we study how to leverage knowledge distillation (KD) and contrastive learning (CL) framework for personalized outt Figure 8: The 𝑡-SNE visualization result of the user-outt space. The “X” symbol denotes a user embedding vector distinguished by dierent colors. Each rectangle is an outt representation vector. A rectangle with a black border indicates a negative outt. For rectangles with a colored border, they represent the positive outts of the user corresponding to each color. Among colored rectangles, a dashed border shows a false-negative outt found in the test set. recommendation. We propose a new KD framework named False Negative Distillation (FND) that does not require the ranking of all possible outts. We also propose two novel data augmentation methods to make use of the CL framework in outt recommendation. Quantitative experiments show that our FND and CL achieve notable success in outt recommendation tasks. In detail, FND outperforms the state-of-the-art methods under fair conditions and achieves improved performance than without using FND in the same model. The outt CL framework also contributes to the recommendation performance by allowing the model to obtain a more meaningful outt representation. We support the soundness of our FND by visualizing the user-outt space of the teacher model. One interesting future work is to apply a contrastive learning framework in a supervised manner by treating each user as a class.