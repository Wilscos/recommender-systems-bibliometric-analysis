With the explosive growth of data volume, recommendation systems (RS) have been a prevailing and powerful tool for helping people to alleviate information overload. As the most commonly used collaborative ﬁltering technique for RS, matrix factorization (MF) [ historical ratings by learning a shared latent space for their representations. To date, many state-of-the-art methods have been developed based on MF, such as Probabilistic Matrix Factorization, Deep Matrix Factorization and Neural Collaborative Filtering. However, data sparsity and intrinsic bias of observations can aﬀect the validity of latent presentations, thereby deteriorating the prediction accuracy. Recently, textual information has been proved to play a positive role in recommendation systems. However, most of the existing methods only focus on representation learning of textual information in ratings, while potential selection bias induced by the textual information is ignored. In this work, we propose a novel and general self-adaptive module, the Self-adaptive Attention Module (SAM), which adjusts the selection bias by capturing contextual information based on its representation. This module can be embedded into recommendation systems that contain learning components of contextual information. Experimental results on three real-world datasets demonstrate the eﬀectiveness of our proposal, and the state-of-the-art models with SAM signiﬁcantly outperform the original ones. To build more eﬀective MF models, one mainstream solution is to utilize auxiliary textual information, such as user proﬁle, item description, and reviews of users to items. Wang and Blei [ sion (CTR) that adopts the Latent Dirichlet Allocation (LDA) technique to improve the traditional collaborative ﬁltering under a probabilistic framework. Afterward, several variants of CTR were presented, which also employed LDA to discover valuable aspects from textual reviews [ proposed a novel MF recommendation based on topic modeling, which employs the non-negative matrix factorization to derive topics from textual reviews. It should be mentioned that the aforementioned models mainly adopted the bag-of-words model and ignored the contextual information of documents, e.g., the surrounding words and word orders. To address this issue, Kim et al. [ utilized deep learning (DL) models to capture the contextual understanding of textual information, and proposed a novel document context-aware recommendation model called Convolutional Matrix Factorization (ConvMF). Zhang et al. [7] developed a new hybrid model that jointly models content information as representations of eﬀectiveness and compactness, and leverages implicit user feedback to make accurate recommendations. Lu et al. [ tive recommendation model, which utilizes the attention-based recurrent neural networks to extract topical information from review documents. Although these models achieved outstanding performance, they ignored the natural selection bias problem caused by the potential distortion of available textual information, and thus cannot accurately reﬂect the target population. In this paper, we propose a novel and general self-adaptive module called the Self-adaptive Attention Module (SAM), which can self-adaptively learn attention by utilizing the representation of textual information to oﬀset the selection bias. Our module can be seamlessly integrated into any models that contain learning components of textual information. In particular, the integrated model eﬀectively takes advantage of the attention learned from the representation of textual information to enhance the prediction accuracy by altering the objective function of MF. We evaluate the eﬀectiveness of SAM on three real-world datasets. Experimental results demonstrate that the integrated models can signiﬁcantly outperform the state-of-the-art models. Moreover, extensive experiments verify that our proposal achieves performance improvement over a series of sparse scenarios. The main contributions of this paper are summarized as follows. To the best of our knowledge, we are the ﬁrst to propose a general module to alleviate the selection bias derived from utilizing available textual information in RS. The proposed module can be seamlessly integrated into any recommendation models, including learning components of the textual information. Experiments demonstrate the superiority of the integrated models with SAM and eﬀectiveness over a series of sparse scenarios. Matrix factorization generate the latent features matrices for users and items, and obtain the predicted rating using inner product of user matrix and item matrix. We denote observed rating matrix as the number of users and items, and Suppose the true hidden rating matrix feature matrices and item j can be predicted by ˆr To obtain the latent features function and corresponding predicted ratings. To avoid overﬁtting issue, term tend to be considered into loss function as follows: where I denotes indicator matrix satisfying I 0 otherwise, and k·k denotes the L In this part, we brieﬂy review the DL models for text representation. Feed-forward networks are among the simplest DL models to achieve text representation. For example, Iyyer et al. [ feeds an unweighted average of word vectors through multiple hidden layers. Le and Mikolov [ Vector, which learns vector representations for variable-length pieces of texts. These models essentially considered the text as a bad of words and learn a vector representation for each word while ignoring the word dependencies and text structures. To address this issue, methods based on Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) have been developed to capture a deeper understanding of textual information. Kalchbrenner et al. [ a CNN-based model for text representation. Afterward, Kim [ simpler CNN-based model which only has one layer of convolution and performs remarkably well. CNN-based models focus on patterns of words across space, whereas RNN-based ones can better capture time-level features of words. Tai et al. [ ory (LSTM) architecture to tree-structured network topologies, which achieves high performance for representing sentence meaning over a sequential LSTM. Considering huge success attention mechanism achieved in computer vision, it has been attracted increasingly researches in the text representation. Recently, there have been many works on incorporating attention module into various Lwhich represents the sum of square loss between the observed ratings 13] introduced a generalization of the standard Long Short-Term Mem- DL models such as [ alleviating the bias from the perspective of model structure. It has been well known that if the observations do not represent the distribution of the underlying data, selection bias will appear [15]. The reason may be that the research is adulterated with artiﬁcial selection criteria. There are many studies trying to correct selection bias. Heckman [ results from the usage of non-randomly selected samples. Smith and Elkan [ used Bayesian networks to formalize diﬀerent types of selection bias. Figure 1: The architecture of integration of context-aware RS and SAM module. The observed rating matrix recommendation models and Hardmard product ( and the bias-corrected weights matrix represent the observed ratings. In the true hidden ratings. The black and white areas in and observed positions, respectively. In indicating speciﬁc attention from SAM. The sketch of the integrated models is shown in Figure 1. Speciﬁcally, we decompose the observed matrix bias-corrected weights matrix internal parameters in the DL models of context-aware RS and SAM by P, respectively. The training of SAM is independent of indispensable to obtain of the selection process and adopt a bias-corrected risk function: where of auxiliary textual information, a naive modeling of the bias-corrected weights would be of item known in practice, most works provided several parametric ways to model it and plugged the estimators of propensity scores into diﬀerent rating estimation procedures respectively. However, these works suﬀer from the prior knowledge of speciﬁc selection model and usually requires additional parameter tunning process to avoid extreme propensities phenomena [ the textual information inﬂuences the target ratings, more importantly, can interact with the rating process in most context-aware RS. To include the textual information in the selection model without the requirement of speciﬁc modeling, we adopt a DL-based method to obtain the bias-corrected weightings motivated by [6]. More speciﬁcally, We aim to choose the desired DL-based bias-corrected weights {w} for a range of U ∈ R To allow our weight can be adaptively embedded into diﬀerent recommendation systems, we need to remove the speciﬁc rating information and one direct way is to control adopt a DL-based bias-corrected weights, we propose the following optimization: where respectively, and We adopt the objective function as matrix to evaluate all the bias-corrected weights simultaneously. Note that the bias-corrected rates are restricted to be greater than or equal to 1 due to the observation that weight taking the regularization terms into consideration, we can obtain the complete loss function by extending L wis the corresponding bias-corrected weights. With the consideration w=P(I= 1|x), wherexdenotes the context information j. This leads to an unbiased risk function due to the observationPP I= (I) andJrepresents indicator matrix and matrix with all entries 1 w(x), we adopt a CNN architecture to learnw(x) =cnn(P, x). By L = L+ λkuk+ λkv− dl(P, X)k,(5) where detailed implementations of which can be obtained by referring to the following section. We evaluate our proposal on two movie rating datasets from MovieLens [ the news reading-time dataset created by ourselves. For MovieLens datasets, users rate items on a scale of 1 to 5, which is similar to other homogeneous datasets. To verify the performance of our proposal over various recommendation scenarios, we build a new dataset that comes from Sogou News and consists of the reading-time of users on the news. To avoid problems caused by the scale of reading-time, we utilize the bi-scaling technique [ interaction matrix before training. The statistical description of datasets is shown in Table 1. Note that MovieLens does not contain the textual information, we extract the movie descriptions from IMDB and perform preprocessing on it as [ For Sougou News, we set the length of news to 500 words by truncating the longer news and padding the shorter news. In addition, we select the top 36000 words as a vocabulary and remove all non-vocabulary words from the news. We employ the classical MF and ConvMF, a representative model combining MF with DL models for textual information as baselines. Considering that ConvMF provides a framework for integration of MF and DL models, we design two new models RCNNMF that replaces “Conv” part with Recurrent Convolutional Neural Networks (RCNN) [ “Conv” part for baselines. We evaluate the performance improvement produced by the integration of these models and the SAM. dl(P, X) represents latent vectors of items via one speciﬁc DL model, Table 1: Statistical description on three real-world datasets Table 2: Overall test RMSE. Models with ’+’ in their names represent combinations with SAM. For each dataset, we randomly split observations into 80% and 20% as train/test sets. To make MF working on all users and items, we ensure at least one interaction existing on each user and item. We evaluate the performance of prediction using the Root Mean Square Error (RMSE) metric, which is computed byRMSE avoid the overﬁtting, we set the maximum iteration to 200 with early-stopping. Table 2 shows the performance of baselines and models with SAM. Considering the range of Sougou News, we scale the results by multiplying 10 mance display. We observe that models with SAM signiﬁcantly outperform the corresponding baselines on all datasets. Speciﬁcally, for two MovieLens datasets, the best performances are achieved by integrating ML and DL models. The improvements demonstrate the eﬀectiveness of SAM when textual information facilitates the rating prediction. For Sougou News, ML with SAM also obtains considerable improvement over the vanilla MF, which further proves the potency of SAM. Table 3 reveals that the comparison results of all methods under a series of missing scenarios. Overall, we observe that improvements caused by SAM get more signiﬁcant when the train set becomes more sparse. It implies that SAM has excellent potential in extremely sparse scenarios. Table 3: Test RMSE over various sparseness of training data on MoviesLens 100K dataset. Models with ’+’ in their names represent combinations with SAM. In this paper, we propose a general attention-based module SAM to alleviate selection bias derived from the utilization of textual information in recommendation systems, which can be seamlessly integrated into models containing learning components of the textual information. Empirical studies on three real-world datasets demonstrate the eﬀectiveness of SAM, and extensive experiments imply the great potential of SAM under extremely sparse scenarios.