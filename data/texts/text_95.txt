The Astronomy and Astrophysics Review manuscript No. (will be inserted by the editor) Paul Shah · Pablo Lemos · Ofer Lahav* Abstract Since the expansion of the universe was ﬁrst established by Edwin Hubble and Georges Lemaˆıtre about a century ago, the Hubble constant H which measures its rate has been of great interest to astronomers. Besides being interesting in its own right, few properties of the universe can be deduced without it. In the last decade a signiﬁcant gap has emerged between diﬀerent methods of measuring it, some anchored in the nearby universe, others at cosmological distances. The SH0ES team has found H locally, whereas the value found for the early universe by the Planck Collaboration is H microwave background. Is this gap a sign that the well-established ΛCDM cosmological model is somehow incomplete? Or are there unknown systematics? And more practically, how should humble astronomers pick between competing claims if they need to assume a value for a certain purpose? In this article, we review results and what changes to the cosmological model could be needed to accommodate them all. For astronomers in a hurry, we provide a buyer’s guide to the results, and make recommendations. Keywords Cosmology · Distance scale · Hubble constant · Cepheids · Supernovae · Cosmic background radiation In 1917, Einstein was the ﬁrst to combine the assumptions of homogeneity and isotropy with his new theory of general relativity, and produce a solution for the universe as a whole (Einstein 1917). Einstein imposed his belief in a static universe, and famously introduced the cosmological constant Λ, to make his equations compatible with this assumption. Friedmann (1922) derived a solution for an expanding (or contracting) universe, but his work remained largely unknown until after his death. Establishing expansion as an observational fact was very challenging with the technology of the day. George Lemaˆıtre published the ﬁrst estimate of the expansion rate in Lemaˆıtre (1927). Two years later, Edwin Hubble Mount Wilson telescope with Shapley’s, Humanson’s and Slipher’s redshifts z to a similar result (Hubble 1929). Hubble’s constant, as it later became known, is then the constant of proportionality between recession speed v and distance Surprisingly perhaps, it was not until 1958 that the ﬁrst recognisably modern Fig. 1: Hubble’s original diagram from Hubble (1929). Despite the typo on the labelling of the y axis, which should read km s easy to read oﬀ H the largest telescope in the world at the time. M31 is recognisable as the lowest black dot in the bottom left; Humanson had determined it is velocity as 220 km s value H several corrections to Hubble’s earlier results. Firstly, he noted the population of Cepheid variable stars was not as homogeneous as ﬁrst thought. This added both scatter and bias to distance estimates, compounded by the low numbers of Cepheids observed. Secondly, and more seriously, Hubble had mistaken (far brighter) HII regions as bright stars, and therefore his estimate of the distances to galaxies was too low. ' 75 km sMpcwas published (Sandage 1958). Sandage made present day debates. H consistent values. In particular, in 2018 the Planck collaboration used the Cosmic Microwave Background (CMB) temperature and polarization anisotropies and the ΛCDM cosmological model, to ﬁnd H (Planck Collaboration et al. 2020) whereas in 2021 the SH0ES collaboration (Riess et al. 2021) used Cepheids and supernovae to ﬁnd H 1.3 km s limits). We show the main modern results in Fig. 2. standard cosmological model is incomplete and new physics is required. All H results place some reliance on a background cosmology (for example to obtain peculiar velocity adjustments from a model), but the sensitivity is large when comparing results projected over large distances. Secondly, other cosmological parameters such as matter densities, curvature and large scale structure are often degenerate with H helps resolve their values too. Thirdly, knowing H trophysics, as distances in the universe are ∝ H h = H tances; it may be skipped by a reader familiar with cosmological models. In Sect. 2, we show how H problems that might generally arise. We also brieﬂy discuss the use of Bayesian methods as a tool to discriminate between competing observations and models. In Sect. 3, we discuss ways in which H is a rich literature on the subject, and it is diﬃcult to cover all papers. Our approach is to cite for each topic a seminal paper, and the recent most significant developments. In Sect. 4, we discuss the possibility that measurements are correct, and it is our understanding of cosmology that is wrong. In Sect. 5 we conclude, and in the spirit of our guide for consumers, we provide our buyer’s advice and recommendations. We aim to be impartial, and the views expressed here are solely our own. Busy readers could review Sects. 2, 4 and 5, and dip into Sect. 3 if more detail is needed. Hubble’s “constant” (Eq. 1) is not ﬁxed when we observe beyond our local cosmological neighbourhood; which is to say it is not ﬁxed in time. Therefore, we write H expansion has slowed down in the past, but the universe is now accelerating (since z ∼ 0.6) and has been dark energy dominated since z ∼ 0.3. The change in H(t) may be written using the phenomenological deceleration parameter q(t) Fast forward to today, and these historical developments seem to echo some Mpc(here, and for the rest of the review we quote 68% conﬁdence Why should this disagreement matter? Firstly, it may be a sign that the /100 km sMpcis ubiquitous in formulae. We organise our review as follows. The ﬁrst section deﬁnes Hand dis- For the remainder of this review, we adopt c = 1 throughout the text. Fig. 2: Summary of recent H a limited number of results in order to show those that are as independent from each other as possible, in the sense that they use diﬀerent photometric data, distance calibrators and so on. More comprehensive versions of this plot can be found for example in Di Valentino et al. (2021). Comments: 1) CCHP and Yuan et al. share a common distance to LMC as a calibrator. 2) TDCOSMO is a re-analysis of almost the same data as H0LiCOW, but with changes to the galactic potentials. 3) BOSS and DES share a prior constraint on baryon densities from BBN. 4) The results of Blakeslee et al. use new SBF observations, whereas Khetan et al. use archival SBF distances to calibrate SN Ia. The code use to generate this ﬁgure is publicly available at https://github.com/Pablo-Lemos/plot1d In our local universe q(t) is approximately constant, and hence some authors adopt q et al. (2019)). As redshift is a monotonically decreasing function of time, we can write H = H(z) and approximate As q been emitted when the Hubble constant was more than 1% diﬀerent to its current value. The purpose of phrasing cosmography in this way is to avoid explicit assumptions on the matter or energy content of the universe, and it is of course possible to use more general parametrisations to expand H(z). While this approximation is reasonable for z  0.1, if we wish to go further, or link phenomenological parameters to physical quantities, we need a cosmological model. on large scales, and has a space-time metric. Under those assumptions, the Friedmann–Robertson–Walker (FRW) metric is where a(t) is a scale factor deﬁning how physical distances evolve with cosmological time t, and (r, θ, φ) are comoving polar coordinates centered on ourselves. k is a curvature parameter, which here has units of inverse area as we wish to set a(t it is not in general possible to do both). Results from Planck (Planck Collaboration et al. 2020) for the CMB in isolation show a preference for mildly closed universe where k > 0, and allowing k to vary from zero lowers the CMB derived H tional evidence points to a ﬂat universe (for example galaxy survey data or gravitational lensing of the CMB – see for example Efstathiou and Gratton 2020). Our discussion would not be materially aﬀected by including spatial curvature, and for this review we will assume a ﬂat universe where k = 0. We return to the point in Sect. 4. where ˙a ≡ da/dt. The scale factor and redshift are related by By z emitter were stationary in comoving coordinates, which we take to be the frame in which the CMB has no dipole. Peculiar velocity is then the velocity with respect to this frame. We know the solar peculiar velocity relative to the ≡ q(t) as a constant (see for example Riess et al. 2016 or Freedman ' −0.55, light travelling to us from more than 100 Mpc away will have Cosmological models usually assume the universe is homogenous and isotropic , we mean the redshift that would be seen if both the observer and CMB from our observed dipole, but to estimate z in a peculiar velocity of 300 km s result in a 1% error in H that reason, astronomers seek large numbers of objects distributed across the sky, deep into the “Hubble ﬂow”, meaning their peculiar velocities are small compared to Hubble expansion and are assumed to average out. From here, we write z = z This can be generalised with an equation of state parameter p = wρ for dark energy where p is pressure, and the above corresponds to the cosmological constant w = −1. The ﬁctitious curvature density is Ω and we have assumed spatial ﬂatness Ω It is straightforward to expand H(a(z)) as a Taylor series in z and obtain Eq. (3) to ﬁrst order where from H type to another). This is how H the CMB. Hence, one way to reconcile the Hubble tension is to change the function H(z, Ω and we return to this later. Luminosity distance is deﬁned to recover the standard inverse square law ratio between the bolometric luminosity L and ﬂux F that would hold in a ﬂat, static universe: we also need the peculiar velocity of the emitter. For example, a 10% error The present day density fractions for matter, radiation and dark energy ≡ Ω(z = 0) are deﬁned in terms of the physical densities ρas So, in a narrow sense, a cosmological model is a function to derive H(z) and Ωor vice versa (provided nothing converts energy from one In a homogeneous and isotropic universe we ﬁnd where it is conventional to make the dependence on H H(z) ≡ H Angular diameter distance is the ratio between the physical size l of a distant object, and the small angle δθ it subtends on the sky: The Etherington relation (Etherington 1933) is a useful way to convert between the two z : where the jerk parameter j Eqns. 15 are now a reasonable approximation to Eq. (7) and Eq. (11) out to z ∼ 0.6. Taylor series in z, we see that v = cz as used by Hubble and Lemaˆıtre (and re-introducing c here for clarity) is only valid as a low-z approximation. To link ΛCDM to our local universe, we expand Eq. (7) to second order in H(z) = H[1 + (1 + q)z + (j− q)z2] , Setting Ω' 0, we then obtain q=(Ω− 2Ω) and j= Ω+ ' 1. Hubble’s law v = H d is implicit in Eq. (11). Expanding the integral as a The age of the universe t on other cosmological parameters. From the deﬁnition of H(t) = ˙a/a we can write In the special case of a ﬂat, radiation-free universe, where we set Ω 1, it can be written analytically as For a ﬂat universe with Ω 1980s), it was believed that the universe was Einstein–de Sitter (Ω and Ω is older than the oldest globular clusters. The existence of Λ > 0 makes the universe naturally older. The Planck estimate is t ΛCDM (Table 2 for Planck alone, 68% CL; Planck Collaboration et al. (2020)). This value for the age of the universe is comfortably larger than the age of any known astronomical object. A standard candle is any population of stars or events which – can be reliably identiﬁed – have the same characteristics wherever they are seen – have an established luminosity law specifying the absolute magnitude in Although the luminosity law is determined empirically by calibration (except in the case of gravitational waves – see Sect. 2.3), there is an advantage if there is also a solid understanding of the underlying physics of the standard candle as in that case the calibration can be cross-checked against a theoreticallyderived one. calibration of the absolute magnitude M, for example using parallax distances and apparent magnitudes m. m is deﬁned by where F range of the band X, and F = 0.0), which yields the product Ht=. Then, a low value of ≈ 50 km sMpcwas required in order to ensure that the universe terms of observable quantities. The nearby distance ladder starts with a choice of standard candle, and a is the energy ﬂux per unit area per second across the wavelength system being used (for example the Vega system). M is the apparent magnitude the object would have if it were at a distance of 10 parsecs. Distance is conveniently quoted as the distance modulus and then the luminosity distance (10) becomes which can then be substituted into Eq. (11), (15) or similar relations to obtain and in turn used as calibrators for the next rung. For example, the SH0ES team (Riess et al. 2019) calibrate Classical Cepheids (CC) using parallaxes, a maser distance to NGC4258, and detached eclipsing binaries (DEBs) in the LMC as their ﬁrst rung. Their next rung is Type Ia supernovae (SN Ia), calibrated using the 19 galaxies in which both Cepheids and SN Ia have been observed. We illustrate their construction in Fig. 3. Fig. 3: Our schematic illustration of the construction of distance ladders. Green circles represent the calibration of the distance ladder (either the base or overlap of each rung), and the bars are the rungs of the distance ladder. The size of dots or thickness of bars are in proportion to their contribution to the error budget of H to show the dependence on ΛCDM. Are there enough with good distances to accurately calibrate the absolute magnitude? Can we clearly identify them at large distances? Can they be observed out to a suﬃcient distance to reach the next rung? Are the objects observed at large distances of the same type as local ones used as calibrators? How to correct for extinction, reddening, metallicity eﬀects and crowded A given standard candle seen over a range of distances is termed a “rung”, The quality of the standard candle depends on a number of considerations. starﬁelds? Which band has the most reliable magnitudes? If data has been combined from diﬀerent telescopes, have the right adjustments been made to convert photometry? How is magnitude to be deﬁned for variable stars? Each rung depends on the previous one, and errors will propagate up the ladder. is non-linear. As the expectation E[d tional data will introduce systemic bias. Bias can also be introduced by sample selection unless care is taken as E[d sample but wish to know the expectation of the unselected one. Selection may be overt (for example by cutting outliers) or due to our telescope seeing only up to m < m can be (and usually are) corrected for, but require some assumptions and a careful analysis of the data and reduction pipeline. A standard ruler is a feature on the sky of a known physical size l, which enables us to calculate the angular diameter distance d their angular size. Parallax is an obvious example, and also the size of orbits of masers and detached eclipsing binaries can be determined from their positions, light curves and spectroscopy. In the early universe, acoustic pressure waves in the primordial charged particle and radiation plasma set a physical size called the sound horizon r tial seeds so Thomson scattering by charged particles was necessary to propagate the waves, and hence they are frozen-in by recombination. The sound horizon is then imprinted on the CMB as peaks in the power spectrum of temperature ﬂuctuations, and in the later spatial distribution of galaxies (known as baryon acoustic oscillations, or BAO for short) lated in a cosmological model, and depends both on the expansion rate H(z) (the waves are carried along by expanding spacetime) and the matter-energy content of the early universe (determining the sound speed). direction to the nearby distance ladder but on the same principle. It can use the sound horizon for a starting d relation (14) to calibrate the luminosity distances d example, Lemos et al. (2019) calibrate BAO at z ' 1 and SN Ia at z < 1 using the CMB sound horizon. They replace the standard ΛCDM formula for H(z) with a parametric form. H(z) is extrapolated to today to ﬁnd H A more subtle issue is that the conversion of observational data to H that generate them. The universe was not that dense at that time, The inverse distance ladder, as its name suggests, works in the opposite 68.42 ± 0.88 km s CMB value of Planck Collaboration et al. (2020) and Riess et al. (2019) is not caused by assuming the late-universe is ΛCDM. of ruler size or luminosity diﬀerences. Planck implies r ΛCDM; it would need to be ' 10 Mpc lower (Knox and Millea 2020) to bring consistency with Riess et al. (2019). Alternatively, Eqs. (21) and (11) show that Cepheids or SN Ia would need to be ' 0.2 mag brighter than thought to bring consistency with Planck. ward each other by the “guard rails” of SN Ia, do not meet! Hence, the H tension is sometimes characterised as “early” versus “late”, from which follows the question “Is ΛCDM right?”. This may be premature: in fact, few lateuniverse results in isolation are fully inconsistent with early universe ones, as we discuss later. We have seen distance ladders require calibration, whether they are nearby or inverse. However, there are some self-calibrating observations from which H may be calculated directly. mines other parameters in ΛCDM, and the value of H understood as just one part of the simultaneous inference of all cosmological parameters. A further example is maser emission systems, which occur in the nucleus of certain galaxies and are bright enough to be seen at cosmological distances. The emission spots appear to follow Keplerian orbits, and so with some disk modelling, the size of the orbit and hence the angular diameter distance can be deduced directly. pact objects and luminosity can be obtained from the shape of the waveform. That is to say, there is no need for an empirical calibration of their intrinsic luminosities, and instead the observational challenge is to determine the redshift of the source. Often referred to as standard sirens rather than standard candles, a gravitational wave event whose source galaxy has been identiﬁed (by locating the optical counterpart) is referred to as a “bright siren”, otherwise it is called a “dark siren”. Most gravitational wave events are dark, but progress can be made statistically with them given suﬃcient numbers. the lensing of the background source (a combination of both the longer path and time dilation) and the mass distribution of the lensing galaxies. The absolute time delay is not known, but if a rapidly varying source like a quasar can be seen in multiple images, the relative time delay between images allows the angular diameter distance of the lens to be calculated. In this case, the challenge is to obtain enough constraints on the mass distribution of both the We can express the diﬀerence between early and late universe Hin terms In summary, we see the two ways of constructing ladders, propagated to- One example is the CMB. The detailed shape of the power spectrum deter- Gravitational waves assume general relativity. The masses of merging com- For gravitational lenses, general relativity links the time delay caused by lensing galaxies and the general concentration of matter along the line of sight, using for example the velocity dispersions and surface brightness of the lensing galaxies, and imaging data. 2.4 What could cause the tension? By using the word “tension”, cosmologists mean the discrepancy in measurements of H means that if the values and errors are correct, this is very unlikely to be the result of chance. able telescopes of the day, and as a consequence observers have to be very careful to avoid bias in their derivations of H errors. Hubble believed he had one population of Cepheids, whereas we know today he had two, and had also confused nebulae with bright stars. Alternatively, many researchers interpret the tension in the spirit of the precession of the perihelion of Mercury: something is wrong with the (Newtonian) model and a new one is needed (general relativity). We can categorise explanations as follows: – Observational bias. An observational bias is an error in mean photom- – Astrophysical bias. An astrophysical bias occurs when the properties of Measuring the Hubble constant has always pushed the limits of the availetry that would be expected to increase with magnitude or distance. To give some examples, consider ﬁrst crowding. For distant stars, resolving them from their neighbours becomes harder, therefore their photometry will be progressively more blended with other stars the more distant they are. Blending increases the apparent magnitude, and changes the colour (see for example the discussion in Sect. 4.2 in Javanmardi et al. 2021). For very faint stars, the response of the detector may be non-linear (Riess et al. 2019), and needs to be corrected. Another issue is combining observations between ground and space telescopes, as in general fainter stars will be observed from space, but nearby ones more cheaply from the ground. Aside from atmospheric extinction, each instrument has diﬀerent passbands, detector response and resolution, meaning the magnitudes of the same star observed in each telescope will be diﬀerent. Photometry must be transformed to a common system (see for example Eqs. 10–12 in Riess et al. 2016), and if not done (or done incorrectly) some bias will likely have been introduced. Any parameters derived from photometry - such as photometric redshifts - would inherit the same propensity to bias. the object being studied are not fully resolved, and those properties diﬀer with distance. For example, consider Cepheids in the LMC and SN Ia hosts. The LMC is close so Cepheids with a full range of periods can be seen, whereas for distant galaxies only the brighter Cepheids with longer periods are seen. Additionally, the LMC is metal-poor compared to a typical spiral galaxy, the Cepheids there may be expected to be relatively metal-poor compared to those in SN Ia hosts. Hence, any curvature in the Leavitt – Statistical bias. The main causes of statistical bias will be selection ef- – Physics of ΛCDM. Before invoking new physics, could the explanation be – New Physics. If the expansion history of the universe were diﬀerent to 2.5 Is the tension signiﬁcant? The tension is often quoted as the number of standard deviations “mσ”. In particle physics, the meaning is clear: there are millions of collisions and probability is frequentist. There is no need to work in a Bayesian framework, with some prior assumption of a parameter to update with new data. The law of large numbers drives distributions to Gaussian normal shape, and we can law, or mis-calibrated metallicity dependency could bias distances (see for example Freedman and Madore 2010). A second example is the step-like link between SN Ia magnitudes and properties of the host galaxy (Smith et al. (2020) and references therein). This could indicate there are two distinct populations of SN Ia. If then the SN Ia in the 19 galaxies where both Cepheids and SN Ia were of mostly one type, whereas the rest a blend of both types, Hwould be biased by the calibration of SN Ia. fects and scatter as we discussed in the introduction. Statistical biases can be corrected by running random simulated observations through the same selection and analysis pipeline, but the simulations will themselves need some physical parameter choices, perhaps determined from previous surveys, or ﬁxed in advance to “reasonable” levels. In Bayesian analysis, residual dependence on the choice of prior is a feature of sparse observations. Statistical bias correction is subtle and diﬃcult, as we see later in the sections on parallax and SN Ia. found within ΛCDM? Cosmological formulae such as Eq. (7) are derived from a homogeneous, isotropic universe. Could corrections allowing for inhomogeneities be large enough to explain the tension? A speciﬁc example is the “Hubble Bubble” or local void proposal (for a recent example, see Shanks et al. 2019), in which we are by chance located in an under-dense region, and our local His diﬀerent from the “universal” one. Additionally, inhomogeneities mean we must correct redshifts for peculiar velocities, and further the propagation of light through overdense or underdense regions might bias our analysis (Kaiser and Peacock 2016). ΛCDM, Hinferred from the CMB or BAO might be brought into alignment with the local value. Alternatively, performing our analysis of a nonΛCDM universe using ΛCDM formulae may have confused us. For example, an extra particle species would increase the pre-recombination expansion speed, so reduce the size of the sound horizon: sound waves have less time to propagate before they are frozen in. To keep the same observed angular size of the CMB temperature ﬂuctuations, the value of Hcalculated from the CMB will increase (see Eqs. 11, 13, 14). However, as we see later ΛCDM makes many other successful predictions, such as the CMB spectrum itself or primordial element abundances, and is not lightly tampered with. translate mσ to a probability of occurrence by chance. At the 5σ level, it is unanimously agreed new physics has been detected. None of the above applies in cosmology! Fig. 2. But we are looking for more from our data analysis, and there are three ways in which Bayesian statistics are helpful, which we now brieﬂy survey. where D is the data and θ are the parameters of the model M of interest. For example M might represent ΛCDM with its associated parameters including any prior belief in the parameters of the model and P (D | θ, M) is called the likelihood, typically speciﬁed by the team analysing the data. The denominator is called the evidence. An extended cosmological model will have a smaller evidence if there exist large values of the parameter space with low likelihood, even if it agrees better with the data. Bayesian evidence then naturally embodies Ockham’s razor: a simpler model will have a larger evidence, unless the extended model has a signiﬁcantly better ﬁt to the data. the data itself may reveal issues. A Bayesian hierarchical analysis was used by Feeney et al. (2018) to test relaxing distributional and outlier assumptions embedded in the χ re-analysed SH0ES data using “hyperparameters”, which are weightings of datasets proposed as a measure of credibility by Lahav et al. (2000). An agnostic prior for the weights is set, and the hyperparameters are marginalised over. Both results are consistent with SH0ES. Bernal and Peacock (2018) extend the hyperparameter method by adding a free parameter shift in the mean of each dataset to account for unknown systematics, which they dub “BACCUS”. Using this to combine Planck, SH0ES and other datasets, produces a compromise. As shown in Fig. 4, the posterior middles the two with much larger error bars, which perhaps is unsurprising given the agnosticism of the method. Such types of analysis need to be taken with a grain of salt : data that is in tension should not be combined, and BACCUS is not a substitute for a critical analysis of why the tension has happened. However, if one demands a method to merge disparate results in a Bayesian framework, BACCUS is a way to achieve that. two posteriors for the same parameter barely overlap, a Bayesian analysis will seriously mislead with error bars that are too small, as shown by the yellow line in Fig. 4. We seek a statistic that is symmetric, (reasonably) independent of Nevertheless, that the tension is signiﬁcant should not be in doubt: see Bayes’ theorem states that the posterior probability distribution is and Msome extension of it with additional parameters. P (θ | M) is Secondly, Bayesian statistics can also help in re-analysis, in the hope that Thirdly, we may want to know how valid a combination of data sets is. If Fig. 4: An illustration of BACCUS applied to Planck, H0LiCOW and SH0ES data, whose posteriors are shown as thin grey lines. Conventional (orange) is the standard Bayesian combination assuming equal weighting, w/Rescaling (blue) is equivalent to hyper-parameters, w/Shifts (brown) adds an unknown systemic error oﬀset to each dataset, and w/Shifts+Rescaling (green) combines hyper-parameters and systemic oﬀsets. Figure from Bernal and Peacock (2018). prior assumptions and models, and straightforward to calculate and interpret. The R statistic compares the evidence of dataset D alone, but is dependent on the prior and so isn’t usually comparable between diﬀerent papers. Handley and Lemos (2019) deﬁne a new statistic called “suspiciousness” as where I is the information ratio log I = D information gain between prior and posterior. D and D This is independent of the prior and (being an integral) the choice of parameters. We can interpret log S  0 as the two data sets being in tension: loosely and Dare deﬁned similarly by replacing A → B, AB respectively. speaking, the evidence of combining them does not exceed the information of considering them separately. Therefore, suspiciousness ﬁts the criteria of simplicity and interpretation we outlined above. in our introduction: “All this debate is interesting, but which value for H should I use, and is it valid to use that in ΛCDM?”. We suspend judgement until after we have surveyed the data and potential new models. Parallax is both the oldest astrometric technique, and the easiest to understand. Hold out your thumb at arm’s length, relative to some ﬁxed point on the wall, and alternately close one eye and then the other. Relative to the ﬁxed wall, the apparent position of your thumb will change, and this is how our depth perception works: the smaller the change in position, the longer your arm must be. The same principle works with stellar distance, where now our “binoculars” correspond to the Earth’s position on opposite sides of its orbit. The change in a ﬁxed star’s position 2$ = θ the change in the Earth’s position by 2 a.u. over 6 months leads to the distance d = 1/$ parsecs Although the nearest star, Proxima Centauri, has a parallex of approximately 0.77 arcseconds, modern measurements target a remarkable 10 µas, which is the size of a thumbnail on the Moon as seen from Earth. name alludes to the ancient Greek astronomer Hipparchus, who measured the distance to the Moon). Launched by the European Space Agency in 1989, it measured the parallaxes of 100,000 stars at an accuracy of up 0.5 milliarcsecond (mas), the ﬁxed background frame now being extragalactic sources such as quasars. Although undoubtedly impressive, at 1,000 light years an error of 0.5 mas would still be a distance error of 15%. A further drawback is Cepheids, an important part of the distance ladder we discuss shortly, are relatively rare stars and only a handful are located in our neighbourhood of the Milky Way. are Gaia and the Fine Guidance Sensor/Wide Field Cameras (FGS/WFC3) aboard the Hubble Space Telescope (HST). Gaia was launched in 2013 and the mission goal is to measure over a billion stars (including 9,000 Cepheids and half a million reference quasars), both in our galaxy and satellites like the LMC. The mission-expectation precision is 7µas at m = 12, rising to 26µas at m = 20 (Gilmore 2018). Gaia does this by slowly scanning the sky with two telescopes set at relative angles of 106.5°, to make a one-dimensional measurement of the time and position of each star that slowly drifts across Finally, even in light of the tension, we cannot dodge the question posed Modern parallax measurements began with the satellite Hipparcos (the Moving on to the present day, our two best current sources of parallax the CCD. Up to 70 measurements will be made for each star, which allows the additional calculation of proper motions, and even small changes in position caused by the gravitational tug of planets orbiting the star. The HST operates on similar principles in “spatial scanning” mode; although it cannot survey like Gaia, when focused on nearby Cepheids its errors appear competitive (Riess et al. 2018a). the spacecraft. A variation in the angle between the two ﬁelds (which might be caused by thermal expansion) could cause spatial variations in apparent parallax, or if synchronous with the scan period even a ﬁxed systematic oﬀset. Indeed, such a variation has been inferred from the interim Data Release 2 (DR2) (Gaia Collaboration et al. 2018). The average of the quasar parallaxes in it is -29µas (negative parallaxes can happen when position measurement error is larger than the parallax), and there were indications this “zero point” may vary with stellar colour, luminosity and position on the sky (Arenou et al. 2018). Riess et al. (2018b) compared HST Cepheid parallaxes to Gaia, simultaneously solving for the Gaia zero point and Cepheid calibration. They found a diﬀerence between them 46 ± 15µas, with Gaia parallaxes again appearing too low. As the typical parallax of a Milky Way Cepheid is 400µas, this is very material to distance estimates. Breuval et al. (2020) creatively replaced DR2 Cepheid parallaxes with those of resolved bound binary companions (where available), which being dimmer are closer to the ideal magnitude range for Gaia. available, and has already by used by Riess et al. (2021) to revisit Gaia parallaxes. Gaia EDR3 is not intended to be the ﬁnal word, but indeed Cepheid zero points seem now to be reduced below 10µas. A calibration of Cepheids using 75 Gaia parallaxes only gives H lower, but consistent with their previous result based on HST parallaxes. into a discussion of parallax bias. The potential for bias occurs in any astrophysical observation, and is often the subject of lengthy analysis in H As it is most easily understood in the context of parallax, it is helpful to discuss it here. (Lutz and Kelker 1973; Hanson 1979). This is a summation of three quite diﬀerent eﬀects: non-linearity of the desired variable (distance) with respect to the observed variable (parallax), population bias (have we observed the object we intended to, or did we confuse it with something else?), and selection bias (our surveys are normally magnitude limited, so we will only “select” objects for which m < m allax measurement, such as might be caused by an instrumental point spread function. To be concrete, suppose the likelihood of measuring 150µas is the same as measuring 50µas when 100µas is the true value. If we average the distance, we will obtain Gaia’s high precision is dependent on a very stable mechanical structure of During the preparation of this review, Gaia Early Data Release 3 was made Before moving on to discuss alternative calibrations, we will ﬁrst digress Parallax bias is usually referred to as Lutz–Kelker–Hanson (LKH) bias To explain non-linear bias, imagine we have a symmetric error in our parrespect to the true distance of 10, 000 parsecs. In mathematical terms, because d = 1/$, then E[d] 6= 1/E[$]. at a given distance r from our position. Assuming a roughly constant spatial density of similar stars, there are more stars in the shell (r, r + ∆r) than there are in the shell (r −∆r, r) for some ﬁnite ∆r. Hence, there are more (further) stars whose parallaxes may be over-estimated to place them at r than (closer) stars whose parallaxes may be over-estimated. Taken to extremes, there are huge numbers of stars with eﬀective parallaxes of zero, waiting for their small but ﬁnite chance to “crowd in” to a given measured parallax. This will bias observed parallaxes too low. Note that if we were certain of our identiﬁcation of the star (as we would be for a Milky Way Cepheid close to us), we need not consider population bias: it would stand out from the crowd. Fig. 5: An illustration of Lutz–Kelker–Hanson population bias. Assume each parallax measurement can be in error by up to ±δ$. In the ﬁgure on the left, there is a constant spatial density of stars. Then, the region ($, $ − δ$) has a greater number of stars that can scatter to the observed parallax $ than the region ($, $ + δ$) and parallaxes are biased too low. Conversely in the ﬁgure on the right, the stellar density drops sharply beyond $ due to either the edge of the population or magnitude limitations. More stars are available to scatter out than in, and parallaxes are biased too high. Now the opposite bias would occur: we cannot see the further stars, so they can’t crowd in. But the same number of closer stars are available to crowd out, so our observed parallaxes will now be biased too high. This is the well- Population bias arises in parallax when we consider a broad survey of stars Conversely, suppose we were observing close to our magnitude limitations. known Malmquist bias (Malmquist 1922), and is a major issue for surveys as naturally we will try to see as far as we can! selection function, the population scatter and so forth. In modern surveys, this is normally done by constructing simulated catalogs with known physical parameters and some assumptions, and putting those catalogs through the same analysis pipeline as the real data to see what biases emerge. For example, Riess et al. (2018a) compute distance modulus biases of between 0.03 and 0.12 mag for MW Cepheids using a model for galactic stellar distributions. If working in a Bayesian framework, a posterior distribution for the distance may be derived using the method of Sch¨onrich et al. (2019). An alternative is to work directly with the parallaxes, instead converting Cepheid magnitudes to predicted parallaxes as done by Riess et al. (2018b). As the magnitudes are measured considerably more accurately than the parallaxes, bias corrections to the magnitude to parallax conversion are not necessary. To check the predictions of LKH bias, Oudmaijer et al. (1998) compared ground-based to Hipparcos parallaxes, ﬁnding a bias towards brighter magnitudes up to relative error ≈ 30%, and dimmer magnitudes for larger error when Malmquist bias is dominant, as one would expect from the discussion above. metrical distances. Imagine we had a star for which we knew the surface radiant ﬂux density J, and the physical size R. The stellar luminosity would straightforwardly follow as L = 4πR bright to be seen outside the Milky Way. For these “late-type” stars, just such an empirical relationship can be established for the surface brightness where φ is the stellar angular diameter, and V the visual band magnitude. This relationship has been calibrated by angular diameters obtained from optical interferometry of nearby stars, and is given by where V −K is the colour diﬀerence between magnitudes in the V and 2.2 µm near-infrared K band (Pietrzy´nski et al. 2019). The scatter is just 0.018 mag. Rearranging the deﬁnition of surface brightness and with φ = 2R/d it then follows that To deal with bias then involves modelling of the instrumental error, the We end our digression on biases here and move on to discuss other geo- Cool, stable, helium-burning giants (that is, red clump) are suﬃciently deﬁned as where R and φ have been converted to solar radii and milli–arcseconds respectively. The pre-factor is purely geometric. just that. If the stars are well separated enough to spectroscopically resolve each one surface brightnesses, colours, radial velocities, eclipse depths and shapes, and the orbital period. With this data, the radius (and other physical parameters such as mass, eccentricity and inclination of the orbital plane) of each star can be solved for. Such an alignment of the stars is of course rare, but suﬃcient numbers do exist! By painstakingly observing 20 systems in the LMC over more than 20 years (covering many eclipses) Pietrzy´nski et al. (2019) determine the stellar radii to 0.8% accuracy. Crowding can be easily spotted in the light curve and removed. They derive µ where the main contribution to the systemic error budget is the S above. date. As we shall see shortly when we discuss standard candles, this result has become key to many recent H the LMC, and the low error budget allows for a very accurate calibration. type” stars with hot atmospheres, for which a reliable surface brightness to colour relation has not been established. The hope is future 30m-class telescopes will have suﬃcient spectroscopic resolution to extend this to late-types in M31 and other local group galaxies (Beaton et al. 2019). Maser emission occurs when thermal collisions in warm gas in an accretion disk around the central black hole of a galaxy drive a population inversion of molecular energy levels. Such systems are rare: the disk must be “just right”, not too hot, and not too cold, and have suitable local molecular abundance. The Type 2 Seyfert galaxy NGC 4258 at a distance of 7.5 Mpc is just such a system. Isolated bright spots of 22.235 GHz maser emission (from a hyperﬁne transition of H (∼ 0.1 ly) long, with the overall shape of a warped line. After subtracting the overall system redshift, the spots on one side are blue-shifted by ∼ 1000 km s the other side is red-shifted by the same amount, and the spots in the middle are low velocity (Argon et al. 2008). If the spots are observed for long enough, their accelerations and proper motion can be obtained from the steady drift of their Doppler velocities and positions. The central spots show the lowest l.o.s. velocity and highest acceleration, and the outer spots having the highest velocity and lowest l.o.s. acceleration. But how can we know the radius of distant stars? Eclipsing binaries allow , but close enough to eclipse each other, we can obtain their individual This is the most accurate measurement of the distance to the LMC to Looking forwards, although DEBs have been found in M31 they are “earlyan orbital system with shared parameters, as it is anticipated that disk viscosity will have reduced the orbits to close to circular. The disk shape is then modelled (including such parameters as inclination, warp, residual eccentricity and so forth), ﬁtted and the orbital parameters of the spots derived. The outer dot velocities v show a Keplerian behaviour with radius: v the acceleration is then ˙v ∝ GM/r determined. The angular diameter distance is then where θ is the angular impact parameter. Humphreys et al. (2013) have observed NGC 4258 for 10 years at 6 monthly intervals using VLBI interferometry, which resolves the relative position of the maser spots to < 3 µas accuracy (even the tectonic drift of the ground radio telescopes must be corrected for). The Doppler shifts are measured to within an accuracy of 1 km s central spots accelerate by ∼ 10 km s provide additional information as d disk orbiting a 10 Mpc, an accuracy of 1.4%, competitive with the LMC distance error. The main statistical contribution to the error budget is the positional error of the ∼ 300 spots. NGC 6264 at 141 Mpc, and it is unlikely more will now be found at usable distances. We illustrate the data for UGC 3789 from Reid et al. (2013) in Figs. 6a and 6b. The Megamaser Cosmology project (Pesce et al. 2020) have used six of these galaxies to ﬁnd H of distance ladders. could be a large fraction of its redshift), NGC 4258 has become a key calibrator of the distance ladder, owing to its low error budget. Its particular usefulness is that, unlike the LMC or SMC, it is a fairly typical barred spiral galaxy, similar in morphology and environmental conditions (metallicity, star-formation rate and so forth) to the ones in which Cepheids and Type Ia supernovae are seen at greater distances. Using Cepheid and SN Ia data from Riess et al. (2016), Reid et al. (2019) ﬁnd H as the geometric calibrator. Having discussed calibrators, we can now talk about the “engine room” of distance ladders. Cepheids form two classes, but it is the younger, population I, classical Cepheids which are of interest. These are yellow bright giants and supergiants with masses 4-20 M The key assumption these observations support is that all the spots form Reid et al. (2019) derive the distance as d = 7.576±0.082(stat)±0.076(sys) Only 8 megamaser galaxies have been detected, with the furthest being Although it is too close to determine Hdirectly (as its peculiar motion Fig. 6: The UGC 3789 maser system, showing the warped disk, velocities and acceleration of maser spots. The top left and top right ﬁgures show the LOS velocities in km s in milliarcseconds; the GM/r curve shape is apparent. The top centre panel shows the LOS acceleration in km s drifts over 3 years of observations. In the bottom panel, we schematically show the warped disk and projected spots. UGC 3789 is at a distance of 46 Mpc so 1 mas ' 0.24 pc. The disk and maser spot data is available in Reid et al. (2013). period, between days and months, by around 1 magnitude. They are bright, up to 100,000 L Milky Way Cepheids had been observed and catalogued since the 18th century, it was ﬁrst discovered by Henrietta Swan Levitt in 1908 that there was linear relation between the logarithm of their oscillation period and absolute magnitudes, the Leavitt period-luminosity law (we use the term P -L law for Miras). She had been observing Cepheids in the SMC and LMC, using the Harvard College Observatory telescope, and decided to rank them in order of magnitude. As stars in the LMC will have roughly the same distance from the Earth, the Leavitt law was immediately clear from their apparent magnitudes. In fact, one could say modern extragalactic astrometry began with her groundbreaking discovery. equilibrium, and would be expected to oscillate about the equilbrium given any perturbation. However, something must drive the oscillation otherwise it would dissipate. For Cepheids, the driver is heat-trapping by an opaque layer of doubly-ionised He surrounding a He-burning core. The trapped heat increases pressure, which expands the star, cooling the ionised He to the point where it can recombine and therefore becomes transparent. As the radiation escapes, the core cools and re-contracts, which in turn releases gravitational energy into the He layer. The He heats, re-ionises and the cycle repeats, with period proportional to the energy released. The Cepheid population lies in an instability strip in the horizontal branch of the Hertzsprung–Russell diagram; the cool (red) edge of the population is thought to be due to the onset of convection in the He layer, and the hot (blue) edge by the He ionisation layer being too far into the atmosphere for pulsations to occur. They therefore form a well-deﬁned population. found in thermodynamic and dynamic arguments. The luminosity of a Cepheid will depend on its surface area via the Stefan-Boltzmann law L = 4πR which expressed in bolometric magnitudes is The stellar radius can be mapped to the period by writing an equation of motion for the He layer as where r denotes the radial position of the layer, p is the pressure and m the mass of the layer. For adiabatic expansion, it is then straightforward to solve for the period P and we ﬁnd P further details see Cox 1960). With ¯ρ ∝ R colour B − V , we obtain the Leavitt law as where P is the period in days, α, β are empirically calibrated from the Cepheid data, and for the zero-point γ we need a distance measurement. structed to be reddening-free, and have a reduced colour dependence (intrinsically redder Cepheids are fainter). Let’s see how this works in practice. Riess et al. (2019) deﬁne an NIR Wesenheit magnitude m 0.386(m constant is derived from a reddening law. They observed 70 LMC Cepheids with long periods, and after setting β = 0, they derive α = −3.26 with intrinsic scatter 0.075 mag. By comparison, the scatter using solely optical F555W magnitudes is 0.312 mag. After subtracting the DEB distance modulus (Pietrzy´nski That stars can pulsate is not so surprising; after all, a star is in local A straightforward understanding of the origin of the Leavitt law can be It is preferable to use so-called Wesenheit magnitudes, which are conet al. 2019), we obtain γ = −2.579. The formal error in the Cepheid sample mean is 0.0092 mag, equivalent to 0.42% in distance. we can ﬁnd Cepheids in, by applying this law to convert periods to absolute magnitudes, and comparing to the apparent magnitudes we observe. As is usual, though, things are not that simple! There are three principle objections: – Are Cepheids in the LMC the same as the ones in distant galax- – Can we obtain clean astrometry of very distant Cepheids? We We would now like to feel that we can deduce the distance to any galaxy ies? The LMC has a lower metallicity than a typical spiral galaxy, so can we expect the Cepheids found there to have the same brightness? Unfortunately, it is hard to determine the metallicity of a Cepheid from its colour alone, and studies on the eﬀects of metallicity are variable (Ripepi et al. 2020). One might try extending the Leavitt law to add a metallicity term κ [Fe/H]. Riess et al. (2019) have estimated Cepheid metallicity in the LMC based on optical spectra of nearby HII regions, and ﬁnd κ = −0.17 ±0.06, which is consistent with an earlier estimate from Freedman and Madore (2010) that LMC Cepheids are 0.05 mag dimmer than galactic Cepheids, and later results (Gieren et al. 2018; Breuval et al. 2021). seen in distant galaxies have longer periods than those used to calibrate the Leavitt law : they are brighter and more easily observed at distance. So, log P ∼ 1.5 for Cepheids in SN Ia host galaxies, whereas the average for the LMC is log P ∼ 0.8. If the Leavitt law is curved, a bias would be introduced. This can be dealt with by either introducing a separate calibration for short and long period Cepheids, or selecting only long period nearby Cepheids for calibration. want to observe Cepheids as far away as we can, to maximise the overlap with the SN Ia observations. But pushing the limits of resolution brings the risk of crowding, whereby the Cepheid photometry is blended with nearby dimmer, redder and cooler stars. Indeed, this seems to be the main cause of the increased scatter in residuals for distant galaxies. There are some ways to deal with crowding: Riess et al. (2016) (hereafter R16) add random Cepheids to images of the same galaxy and put them through the same analysis pipeline, to see how well input parameters are recovered. Figure 8 shows an example of this. Cepheids which are outliers in colour, indicative of high blending, may be discarded (removing the colour cut lowers H by 1.1 km sMpc). Another way to test for a crowding bias is to look for compression of the relative ﬂux diﬀerence from peak to trough (a more blended Cepheid will be more compressed) as is done in Riess et al. (2020). Using an optical Wesenheit magnitude, in which stars may be less crowded than the NIR one, reduces Hby 1.7km sMpc(R16), although the argument can be made this is due to higher metallicity eﬀects in the optical (Riess et al. 2019). As with metallicity, the matter of crowding continues to debated. Fig. 7: Illustration the Leavitt law in four galaxies used for the local distance ladder. The LMC and N4258 are the two main calibrators of Cepheid distances. For the two example SN Ia hosts N4536 and N1015, it is harder to observe the fainter, shorter-period Cepheids. The slope is ﬁxed at -3.26, corresponding to the best estimate global ﬁt in Riess et al. (2016), and magnitudes are m have inverted the normal decreasing magnitude axis used in the literature for presentation purposes. Fig. 8: Illustration of Cepheid photometry from SN Ia host galaxies from Riess et al (Riess et al. 2016). U9391 is the most distant with a distance modulus of 32.92 ±0.06. The association of Cepheids with spiral arms is clearly visible, and in the bottom left is an example crowding correction for point sources and background ﬂux. The scatter around the Leavitt law is ∼ 0.7 mag, thought to be due to residual crowding eﬀects. the eﬀect of some diﬀerent analytical choices such as breaks in the Leavitt law, diﬀerent assumed values for the deceleration parameter q of outlier rejection are shown to be ±0.7%. This may not cover all potential systematics. In a recent talk, Efstathiou (2020) re-examined the Leavitt laws of galaxies presented in R16. Calibrating each galaxy individually, the slopes of SN Ia host galaxies are generally shallower than M31 or the LMC, which should not be the case if Cepheids are a single population. A change in the slope will alter the zero-point, as would a change in the distance calibration. It is then noted that forcing the slope of the Leavitt law to −3.3 (the M31 value), in combination with using only NGC4258 as the anchor, lowers the R16 H appears to be tension between the relative magnitudes of Cepheids in the LMC and NGC4258, and the distance modulus implied by Masers and DEBs; calibrating with solely the LMC gives a higher H It is a feature of χ to the data with the lowest error, which in this case is the LMC value. But if two subsets of the data are in tension, it is uncertain that the one with the lowest χ show where any discrepancy may lie – it might be metallicity eﬀects, crowded Cepheid photometry, the NGC4258 distance, or some other systematic. Reanalyses of the results of R16 by various authors (Feeney et al. 2018; Cardona et al. 2017; Zhang et al. 2017; Follin and Knox 2018; Dhawan et al. 2018) use the same Cepheid photometric reduction data so are not independent as such. Systematic error analysis is provided in Section 4 and Table 8 of R16, where value to 70.3 ±1.8 km sMpc(Equation 4.2b). Additionally, there would have the least systematics. Such analyses do not show one value is preferable over another, nor can they already mentioned replacing LMC and NGC4258 Cepheids with Milky Way Cepheids in the section on parallax. In a recent paper, Riess et al. (2020) show crowding eﬀects can be detected as a light curve amplitude compression, and that their correction method has been robust. Finally, Javanmardi et al. (2021) fully re-derive the Cepheid periods and luminosities from the original HST imaging for NGC 5584 (which is face-on to the line of sight), intentionally using diﬀerent analytical choices, ﬁnding no systematic diﬀerence in the light curve parameters. check against Cepheids. We now turn to two possibilities, Miras and the Tip of the Red Giant Branch. Miras are variable stars that have reached the tip of the Asymptotic Giant Branch (AGB), comprising an inert C-O core, and a He-burning shell inside a H-burning shell. Their mass is 0.8–8 M lower end of this range. Their large size of ∼ 1 a.u. means they are actually brighter by 2–3 mag than Cepheids in the NIR. That they follow a P -L law was ﬁrst established in 1928 (Gerasimovic 1928), but being tricky to categorise and observe, were not extensively studied until the demand for cross-checks on Cepheids has brought renewed interest in them. the O-rich ones exhibiting somewhat less scatter than the C-rich (Feast 2004), but more than Cepheids. Miras have very long periods 90 < P < 3000 days and their light curves have many peaks of variable amplitude superimposed (see for example Fig. 3 in Yuan et al. 2017), so investment in observation time is needed to reliably determine P . The P -L law curves upwards at around P ∼ 400 days, where it seems likely the extra luminosity has been due to episodes of Hot-Bottom-Burning, so an extra quadratic term is required. Miras have high mass loss rates, and the surrounding dust cloud means Wesenheit magnitudes will be less reliable at subtracting reddening compared to Cepheids (because a standard reddening law is assumed in constructing them). Lastly, a size of 1 a.u. would mean their angular diameter is comparable to their parallax, and in addition the photocentre moves around the star, making parallax measurements challenging. are (a) more numerous, and therefore easier to ﬁnd in halos where there is less crowding (b) older, so can be found in all types of galaxies including SN Ia hosts with no Cepheids (c) 2 − 3 magnitudes brighter than Cepheids in the near infrared. Given imaginative observation strategies and careful population analysis, the issues above can be addressed, and modern studies now exist for Miras in the LMC (Yuan et al. 2017), NGC 4258 (Huang et al. 2018), and the SN Ia host NGC 1559 (Huang et al. 2020). Research has accelerated to close down these potential issues. We have All this said, it would be clearly preferable if we had some other candles to Miras form two distinct populations with O-rich and C-rich spectra, with So why bother with them? Their advantages versus Cepheids is that they 600 Miras in the LMC. They have sparse JHK-band observations from the LMC Near Infrared Synoptic Survey (LMCNISS, Macri et al. (2015)), which on their own wouldn’t be enough to establish the period, but using much denser Optical Gravitational Lens Experiment (OGLE) I-band observations (Szyma´nski et al. 2011), they are able to establish a regression rule for the relationship between passbands, and use OGLE to “ﬁll in” the light curves. The classiﬁcation of Miras into O-rich or C-rich is also obtained from OGLE. The reference magnitude is deﬁned as the median of maxima and minima of the ﬁtted light curves. The period is obtained by ﬁtting to a sum of sine functions, progressively adding harmonics if supported by a Bayesian Information Criterion The K-band for O-rich Miras shows the lowest scatter of 0.12 mag, with a −6.90 ±0.01, a using the DEB distance to the LMC given by Pietrzy´nski et al. (2013). By comparison, the m mag (Riess et al. 2019). over 10 months. Mira candidates were identiﬁed by their V and I amplitude variation, and it was possible to use LMC data to show that using this method contamination between O-rich and C-rich could be kept to a minimum. Their “Gold” sample size was 161 Miras, and ﬁtting Eq. (34) for apparent magnitudes gave a Adjusting the ground-based photometry of LMCNISS to F160W equivalent, the authors calculate the relative distance modulus between the LMC and NGC4258 of ∆µ = 10.95 ± 0.01(stat) ± 0.06(sys). This is consistent with the Cepheid value of ∆µ = 10.92 ± 0.02 (Riess et al. 2016) and the Maser-DEB value of ∆µ = 10.92 ±0.041 (Reid et al. 2019; Pietrzy´nski et al. 2019). Similar methods are used by Huang et al. (2020) for observations of NGC 1559, with the addition of a low period cut to deal with potential incompleteness bias of fainter Miras. Using the LMC DEB and NGC 4258 distances as calibrators, they obtain H Cepheids, but due to their larger error budgets, they are also consistent with Planck results. In the ﬁrst of the above, Yuan et al. (2017) establish a P -L curve using Huang et al. (2018) observed NGC4258 with the HST WFC in 12 epochs Fig. 9: A PL calibration for Miras in the LMC from Yuan et al. (2017). While non-linearity can straightforwardly be ﬁtted, contamination must be kept under control. As seen in the bottom three panels, Wesenheit magnitudes have more scatter compared to standard magnitudes (the opposite of applying them to Cepheids), and they also hinder separating O-rich from C-rich stars. Red giant branch stars are stars of mass ∼ 0.5 − 2.0M stage in their evolution, having moved oﬀ the main sequence towards lower temperatures and higher luminosities. The core of the star is degenerate He, and as He ash rains down from the H-burning shell surrounding it, it contracts and heats up. Since the temperature range for He fusion ignition is rather narrow, and the degenerate matter means there is a strong link between core mass and temperature, the core in eﬀect forms a standard candle inside the star, just prior to ignition. As soon as the helium ﬂash ignites, the star moves over the course of 1 My or so (almost instantaneously in stellar evolution terms) to the red clump at a higher temperature and somewhat lower luminosity. Therefore, there is a well-deﬁned edge in a colour-magnitude diagram that can be used to identify the tip of the red giant population just before the ﬂash (TRGB). The TRGB is then not one single star, but a statistical average for a suitably large population. Although red giants are fainter than Cepheids in the optical, in the K-band they are ∼ 1.6 magnitudes brighter. Stars at or near the TRGB are relatively common, and can be observed readily in uncrowded and dust-free galactic The TRGB oﬀers many advantages compared to other standard candles. halos. They are also abundant in the solar neighbourhood, meaning great numbers are available for calibration by parallax (by contrast, Cepheids with good parallaxes in Gaia DR3 will number in the hundreds). Suﬃcient numbers to resolve the tip can be seen in globular clusters such as ω Centuari, where well-formed colour-magnitude diagrams can resolve metallicity and extinction eﬀects, and an absolute magnitude calibration can be made either from Gaia DR3 parallaxes (Soltis et al. 2021) or DEB distances (Cerny et al. 2021). one does not need to revisit ﬁelds in order to determine periods. The James Webb Space Telescope is capable of observing red giants in the near IR at distances of ∼ 50Mpc, which is comparable to Cepheid distances obtainable from the HST. well understood. Empirical calibrations of absolute magnitude and residual dependence on metallicity, mass and age can be checked against the results of stellar codes. For example, the dependency on metallicity is somewhat complex: the presence of metals dims optical passbands by absorption in stellar atmospheres, and brightens the NIR. Serenelli et al. (2017) express the TRGB as a curve with linear and quadratic terms in colour. The authors ﬁnd an I-band median M a downward slope in the colour-magnitude diagram as metallicity increases. This median value is consistent with empirical calibrations. (noisy) stellar distribution in the colour-magnitude diagram, and techniques are borrowed from image processing to do this. The tip has a background of AGB stars, and the edge is sensitive to the distribution of them close to it. If the contamination is large or variable, there is a risk the mode can shift. It is therefore important to have a dense population of stars, and enough ﬁelds to test the robustness of the ﬁtting process. For example, Hoyt et al. (2021) bin data by separation from the galactic centre and ﬁt the tip separately for each bin. distances for 10 SN Ia host galaxies that also have Cepheid distances, using HST I-band imaging (Freedman et al. 2019). They calibrate the zero point from the TRGB of the LMC and SMC using the DEB distance, ﬁnding M −4.049 ± 0.022 (stat) ± 0.039 (sys), which is consistent with the theoretical value above. covering a range of distances from 7 Mpc to almost 20 Mpc. By expanding their sample to non-SN Ia host galaxies, the authors show the TRGB has a lower scatter versus their Hubble diagram than do Cepheids, by a factor of 1.4. Calibrating the Pantheon SN Ia sample on TRGB distances alone gives the TRGB distances to the SN Ia host galaxies being further than Cepheid distances. Like Miras, they are found in all galactic types. Observation is eﬃcient: It is also beneﬁcial that the modelling of late-stage stellar evolution is quite Fitting the TRGB is equivalent to ﬁnding the mode of the gradient of the The Carnegie-Chicago Hubble program (CCHP) have determined TRGB We show in Fig. 11 their comparison of TRGB and Cepheid distances, = 70.4 ± 1.4km sMpc, with the diﬀerence to SH0ES results due to Fig. 10: Fitting the TRGB edge from the colour-magnitude diagram of Freedman et al. (2019). The middle panel shows the horizontally-summed (between the blue lines) and smoothed number count, from which the edge is determined by the maximum (vertical) gradient, which is the peak in the right hand panel. The background AGB population is visible in the stars above the tip. result to a diﬀerent methodology to determine the LMC extinction (Freedman et al. (2019) determine the extinction applicable to the TRGB by comparison of LMC and SMC photometry in V IJHK bands, and using the lower SMC extinction as a reference point. Yuan et al. (2019) adopt the standard reddening law of Fitzpatrick (1999)). They also revise the blending corrections of the older, lower resolution photometry of the SMC. Conversely, a calibration of the TRGB in the outskirts of NGC 4258 to the maser distance by Jang et al. (2021) gives a value fully consistent with Freedman et al. (2019). In a followup paper, Madore and Freedman (2020) present an upgraded methodology in which stellar photometry is itself used to separate metallicity and extinction eﬀects (by exploiting their diﬀerent sensitivities in J, H and K-band magnitudes), conﬁrming their earlier results. Other results (Soltis et al. 2021; Reid et al. 2019; Capozzi and Raﬀelt 2020; Cerny et al. 2021 – see for example Table 3 in Blakeslee et al. 2021) cluster evenly around these two values. We also note the diﬀerence between SN Ia zero points (Fig. 6 of Freedman et al. 2019) of the TRGB vs Cepheids appears to increase with distance, and is particularily large for one galaxy, N4308. Currently, this result remains the subject of debate. Yuan et al. (2019) ﬁnd = −3.99, which gives H= 72.4 ± 2.0km sMpc. They attribute their Fig. 11: Comparison of TRGB and Cepheid distance moduli, from which it can be seen TRGB distances are consistently larger across a range of galaxies. The red dots are SN Ia host galaxies, the grey dots other galaxies, and the blue star is the LMC TRGB calibrator. Figure is from Freedman et al. (2019). was published by Freedman (2021). New and updated values for M the anchors of the LMC, SMC, NGC 4258 and galactic globular clusters are derived. Additionally, potential zero-point systematic problems with the direct application of Gaia EDR3 data to globular clusters (as done by Soltis et al. 2021) are discussed. Each anchor is consistent with each other (although the closeness of the agreement suggests the errors may be over-estimated), and the paper brings the TRGB method on par with Cepheids in terms of number of anchors used. More recently, an updated value of H= 69.8±0.6 (stat)±1.6 (sys)km sMpc very interesting: it is a late universe distance ladder that is in reduced 2σ tension with the Planck value of H cannot by itself fully resolve the Hubble tension, in combination with some other systematic – perhaps in the calibration of SN Ia (see next section) – it may oﬀer a resolution of it that does not involve new physics. of quantity of objects, but rather the accuracy of calibration and control of systematics. The TRGB seems very promising however. Once the calibration is agreed among the community, they oﬀer the tantalising prospect of either conﬁrming the H to a lower statistical signiﬁcance. It is hard to overstate the impact of Type Ia supernovae in cosmology. At M ∼ −19, they are both bright enough to be seen well into the Hubble ﬂow (the furthest to date is SN Wilson at z = 1.914), and have a standardisable luminosity. The mainstream view is that they originate from the accretion of material from a binary companion onto a carbon-oxygen white dwarf. When the white dwarf reaches the maximum mass that can be sustained by degeneracy pressure, the Chandrasekhar mass M detonation occurs destroying the white dwarf. Approximately ∼ 0.6M into heavier elements during the ﬁrst few seconds of the explosion, and the observed light curve, which peaks at ∼ 20 days and lasts for ∼ 60 days is powered by radioactive decay of Ia is a “standard bomb”, primed to explode when it reaches critical mass. accretion theory. Both the lack of H lines and computer modelling suggest the donor star should survive the explosion, but a search of the site of the nearby SN2011fe both pre- and post- explosion have revealed no evidence of a companion. This may support an alternative “double degenerate” theory that some, or all, SN Ia originate from the merger of two white dwarves (for a review, see Maoz et al. 2014). But if that is the case, why are their luminosities so uniform? Whether SN Ia have one or two types of progenitors has important implications for the Hubble constant, which we return to shortly. empirically standardisable by the Tripp estimator (Tripp and Branch 1999): where µ is the distance modulus, m is the absolute magnitude of a typical SN Ia. The parameter x is the “stretch” of the light curve, which is a dimensionless measure of how long the bright peak of the light curve lasts: longer duration SN Ia are brighter. c is a measure of colour: redder SN Ia are dimmer. ∆ The TRGB continues to attract attention because the CCHP result is What is also clear is that quality of a distance ladder is less a matter However, there is surprisingly little observational evidence to conﬁrm the Individual SN Ia luminosities can vary by up to a factor of 2, but they are the SN Ia, and ∆ to the LKH bias of parallax we described earlier. M ﬁnding SN Ia in galaxies for which there are Cepheid or TRGB luminosity distances, or in the inverse distance ladder approach by combination with angular diameter distances from baryon acoustic oscillations, or the CMB. scanning a reasonably sized patch of sky, supernova surveys can detect hundreds per year the sky. However, SN Ia close enough to calibrate their magnitudes are few: there are just 19 SN Ia in galaxies with Cepheid distances from HST observations, although more are expected soon from new cycles (see for example HST proposal 16198). multiple surveys in cosmological analyses. Scolnic et al. (2018) have compiled the Pantheon sample of 1,048 SN Ia from CfA, CSP, PS1, SDSS, SNLS and HST surveys in a uniform light curve calibration, representing almost 40 years of observational data. The sample divides into 180 mostly older Low-z (z < 0.1) SN Ia which are uniformly distributed on the sky, and 868 recent High-z SN Ia concentrated in the survey ﬁelds, notably the thin “Stripe 82” along the celestial equator. The sets have a small overlap at z ∼ 0.1, and crosschecks on their spectral and other parameters show they are likely to be from the same underlying population (that is, High-z SN Ia are not intrinsically diﬀerent to Low-z). However, the mean values of light curve parameters like stretch and colour do drift with z, possibly due to selection eﬀects or changes in the host galaxy properties at higher redshift. Large numbers of Type Ia supernova have also been found, or are in the process of being surveyed, by the Dark Energy Survey (Abbott et al. 2019), All-Sky Automated Survey for Supernovae (Holoien et al. 2018), Foundation Survey (Jones et al. 2019), and Zwicky Transient Facility (Yao et al. 2019). etry is corrected for the background sky obtained from imaging the site after the nova has faded (a bonus of SN Ia being transient), and the light curve is ﬁtted to a template, outputting the parameters x, c and m bias correction ∆ modelling of sources of scatter and contamination by mis-classiﬁed supernovae, and the selection function of survey. For example, more distant SN Ia may be preferentially targeted for spectroscopic conﬁrmation when their host galaxy is faint, so if their luminosities do depend on the galactic environment, this could bias the High-z population relative the to Low-z. The selection function of early Low-z surveys are not easy to model, and their biases can be up to 0.06 magnitudes. This could account for a somewhat elevated scatter of Pantheon residuals around z ∼ 0.1 in the Hubble diagram where the two sets join (Fig. 12). After ﬁtting α and β, the residual intrinsic scatter of the SN SN Ia are rare events: approximately 1 per galaxy per century, but by To maximise statistics and sky coverage, it is common to use SN Ia from The process of ﬁtting a SN Ia lightcurve is straightforward. ugriz photom- Ia population is σ standard candles. Fig. 12: Pantheon SN Ia distance moduli and residuals versus a best ﬁt ΛCDM cosmology. Figure is from Scolnic et al. (2018). mental correlations that aﬀect their luminosity (that is, what the nature of the ∆ generally accepted that SN Ia in galaxies with stellar mass M intrinsically brighter by ∆ and that the transition is sharp, rather than a gradual evolution (see Smith et al. 2020 and references therein). It has therefore become common to use a step-function for ∆ to lower H than the mass of the Hubble ﬂow set. The key point is then this: given that Cepheids are young, bright stars formed in active galaxies, is ∆ discriminating to ensure the 19 supernovae in galaxies with Cepheid distances are representative of the thousands in the Hubble ﬂow? a proxy for some other environmental condition. Metallicity does vary with galactic mass, but the step-like feature suggests a connection with star forma- We return now to the question of whether SN Ia have additional environterm in Tripp estimator is), and why this may matter for H. It is Host galaxy mass itself can’t matter to an individual SN Ia, so it must be tion: galaxies with mass below the threshold tend to be active, whereas those above are mostly passive. One hypothesis is that there is a prompt/bright SN Ia type that is continuously renewed by star formation, and an delayed/dimmer type originating from stars formed when the galaxy was young (Rigault et al. 2015, 2013; Maoz et al. 2014). But any link between this and SN Ia formed by accretion onto or mergers of white dwarfs is speculative. Also, with estimates of “global” properties being eﬀectively light-weighted, they are biased to galactic centres rather than the outskirts where most supernovae are seen. better proxy to be local speciﬁc star formation rates (that is, star formation normalised by local stellar mass or LsSFR for short), on the grounds that the younger SN Ia population will not have had enough time to disperse from their birth region. Because the pixel resolution of typical SN Ia surveys can resolve a 2 kpc aperture only out to z ∼ 0.1, studies concentrate on the Low-z sample. Star formation can be estimated from photometry, but is best tracked by H lines from ionised gas or UV imaging, when available. Factory sample (which has high resolution spectroscopy) and ﬁnd a signiﬁcant correlation between LsSFR and α, the stretch slope, and no correlation for β. This suggests there is indeed an intrinsic diﬀerence between SN Ia originating in active areas versus passive ones. The size of the LsSFR step is ∆ 0.163±0.029 mag, and including it in the Tripp luminosity estimator eliminates the need for a further global host mass step. The size of the step is interesting as it is signiﬁcantly larger than ∆ calibrators are in regions of high LsSFR, whereas in the rest of the Low-z sample the fraction is about a half. Hence, a potential bias to H much as 3%. In contrary results, Jones et al. (2018) ﬁnd a local stellar mass step the same size as ∆ found just 0.1km s to SN Ia found in spiral galaxies. In a recent paper, Brout and Scolnic (2021) argue the true factor is host galaxy extinction (correlated to star formation and hence mass), and present evidence of a distribution separated by colour into blue “clean” SN Ia with lower scatter than their red “dusty” counterparts. The host mass step is absent in the blue sample, and they suggest to use solely these in cosmological analyses to avoid bias (another possibility is to use nearinfrared photometry (Dhawan et al. 2018)). star formation rates, there is not yet a consensus on the cause, whether they constitute one or two populations, and the level of bias (if any) to H are numerous and therefore high relative precision beyond z > 0.1, and have been dubbed the “guard rails” of the Hubble diagram. However, their absolute accuracy in distance ladders is limited by the small numbers available to calibrate their luminosities, and some uncertainty about the underlying astrophysics. Work is underway to address both of these issues: for example, the Foundation and Zwicky Transient Facility surveys (Jones et al. 2019; Yao et al. 2019) are targeting a new high-quality Low-z set with local spectroscopy to If some SN Ia are associated with sites of star-formation, we can expect a Rigault et al. (2020) have examined 141 SN Ia from the Nearby Supernova In summary, while there is strong evidence SN Ia depend on local speciﬁc test environmental eﬀects or extensions to the Tripp estimator. New HST observation cycles aim to extend the number of Cepheid calibrators up from 19, and the JWST will access a greater volume, perhaps increasing the calibration set beyond a hundred for both TRGBs and Cepheids. This would make a 1% calibration from at least two independent sources achievable. Quasars are bright and can be variable on short timescales. Some are seen to be gravitationally lensed by a foreground galaxy, and each image will have a time delay caused by the extra path length and the gravitational time dilation of general relativity: where and ψ( The relevant quantity for H where D between the lens and the source respectively as speciﬁed by Equations (11, 14) – in the case of D of the lens and source. Then, if we have a system with multiple images, the relative time delay ∆t images scales with H weak dependence on other cosmological parameters. Acquiring the time delays months. Errors are typically ∼ 6%, although these are reducing over time. A typical lensing system may have z intermediate cosmological distances. to do so, we need an idea of the lensing potential ψ( tricky degeneracy to deal with, known as the mass sheet transformation. For our purposes, it is suﬃcient to understand that the same observed time delays may be produced by simultaneously scaling the line-of-sight surface density sheet”) and H (1985). proﬁles for the lensing galaxy with associated nuisance parameters to be marginalised over (b) a way to deal with the mass sheet degeneracy, through independent constraints on the mass distribution of the lensing galaxy and along the line-of-sight. ~θis the sky position of the image,~β the (unknown) source position, ~θ) the potential of the lensing galaxy integrated along the line of sight. , D, Dis the angular diameter distance to the lens, source, and requires extensive observations: relative delays range from weeks to a few Each single lens system may provide an independent estimate of H. But ~θ) of the lensing system by a constant factor Σ(~θ) → Σ(~θ) (the “mass- The ingredients for modelling are then this: (a) a range of plausible mass each system! Some other choices must be made, such as which of the galaxies along the line of sight will be modelled and which will be absorbed into the line of sight mass average (Chen et al. 2019). Galactic stellar velocity dispersions provide information on the lensing potential, but as a function of which galactic mass proﬁle is assumed, and with an anistropy nuisance parameter. Also, the accuracy of velocity dispersions at such distances are ∼ 10%. If there are visible background galaxies, they may also be lensed and the shape distortions provide extra information on the mass proﬁle. The line of sight mass may be estimated from the density of foreground galaxies, by searching in computer simulations of large scale structure for similar lines of sight, and calculating the line of sight density from them (lenses are associated with over-densities and therefore focusing is more likely). But a major advantage is that this modelling can be done blind to ﬁnal value of H on the part of the experimenter. 2020), and we show their results below. The main contributions to the errors are uncertainty in the time delay, and velocity dispersion of the lens. H0LiCOW use ﬂat priors of Ω any dependence on other cosmological datasets (although they do assume a ﬂat universe). The results are : of 2.4%. However, a key assumption in the combination is independence, and there are many modelling features in common between the lenses, in particular the speciﬁc mass proﬁles used. There is also an apparent drift in H with lensing distance, although the statistical signiﬁcance is not strong enough to say this is not just random chance. 2020). By treating the mass sheet degeneracy as a fully unknown parameter at the population level, their analysis reduces reliance on speciﬁc mass proﬁles. For same set of systems, they ﬁnd H value is consistent with H0LiCOW, but the wider conﬁdence intervals reﬂect the greater freedom allowed on lens mass distribution, combined with the small number of time-delay lenses. To re-narrow the uncertainty range, the paper This is a challenging, time-consuming and model dependent process for The H0LiCOW team have analysed six such systems so far (Wong et al. Their combined result is H= 73.3±1.8 km sMpc, a ﬁnal error budget A re-analysis has been undertaken by the TD Cosmo team (Birrer et al. investigates what eﬀect introducing a set of non-time delay lensing systems from the SDSS survey may have. The purpose of this larger set is to use their imaging to constrain the mass-proﬁle of lensings galaxy, on the assumption they are drawn from the same parent population as the H0LiCOW sample. TD Cosmo then ﬁnd H sample is selected as those systems capable of producing a clear time-delay signal, which may not have the same characteristics as those selected as having clean shear images. So the assumption they share the same mass proﬁle as each other may not be valid. SH0ES or Planck. More data is needed: upcoming surveys by the Vera Rubin Observatory, Euclid and Nancy Grace Roman telescope will result in several hundred new time delay systems being discovered (so improvements in analysis eﬃciency will certainly be needed!). Adaptive optics can provide spatially resolved velocity dispersions to improve the mass models. TD Cosmo estimate 200 extra systems will be needed to constrain H within reach in the next ﬁve years. The gravitational wave signal emitted by the merger of two compact objects can be used as a self-calibrating standard candle. There are now operational detectors at LIGO Hanford and LIGO Livingston in the USA, Virgo in Italy, and KAGRA in Japan, with a further planned LIGO India (Abbott et al. 2020). The detectors measure the strain amplitude of a gravitational wave by using laser interferometry to detect the miniscule changes in the length of perpendicular beams as a wave passes by. The purpose of the two sites in the USA is to ﬁlter out local seismic vibrations. The wave amplitude is related to the chirp mass M which is in turn derivable from the waveform calculated for a merger. A simpliﬁed form of the relevant equations are where f is the frequency, m and h(t) the measured dimensionless strain of the strongest harmonic (Abbott et al. 2016). The rest-frame chirp mass is redshifted by z of the angle between the sky position of the source and detector arms, and the inclination i between the binary orbital plane and line of sight (Arun It is therefore premature to claim lensed quasars are consistent with either et al. 2009). Note that as we are measuring amplitude rather than energy ﬂux, h(t) ∝ d it sets f, sion is ﬂoored by sensitivity of the strain measurement, currently around 5%. If the redshift and angles were known, d to the same precision. panying burst of light from matter outside the combined event horizon. For this reason, it is known as a “bright siren”. If the ﬂash can be observed, the host galaxy is identiﬁed and one can use its redshift in Eq. (41). The event GW170817 was just such a BNS merger (Collaboration and Collaboration 2017b). The gravitational wave was measured in Hanford and Livingston, which was enough to locate the sky position to 28 deg how). Given the search region, an optical counterpart was found in NGC 4993 at a distance of ∼ 40 Mpc mass in the detector frame as M = 1.197M consistent with a BNS merger. The main remaining uncertainty is then the inclination angle i. Using a ﬂat prior for cos i, Collaboration and Collaboration (2017a) obtain H this to H interaction of the ejecta with the ISM. called “dark sirens”. However, it is still possible to constrain H a probable redshift can be estimated. To do this, the relative amplitudes and time delay between detectors located around the globe are used to approximately determine the sky position (Soares-Santos et al. 2019). For example, if 3 detectors observe the wave with perfect accuracy, the two independent time delays and three measured amplitudes will in turn determine the two sky position angles, two polarisation amplitudes, and one phase lag between polarisations. Hence, simultaneous detections of an event are essential in narrowing the size of the region on the sky where the source is located. A reasonable prior for H volume. Given a suitably complete galaxy catalog with suﬃcient sky coverage (the best available currently being SDSS and DES), galaxies within this volume can be averaged over with a suitable weighting to determine a value for are many more black hole mergers than neutron star mergers, so the errors are competitive in aggregate. It is also straightforward to mix dark and bright sirens to produce a combined result (Palmese et al. 2020). will improve the errors relative to a single event by a factor of N In a loose sense, every wave cycle is a measurement of the chirp mass M as ˙f, although in practice the full waveform is ﬁtted. The relative preci- When a binary neutron star (BNS) system merges, there is an accom- For black hole mergers, no optical counterpart is generated, and these are will constrain the redshift range, and hence determine a localisation (Schutz 1986). Although each individual event is not very accurate, there As each event’s inclination angle is independent, averaging over N events Fig. 13: H which was localised to a region in the DES survey footprint containing ∼ 1, 800 possible host galaxies. The light and dark blue lines represent the dark approach for each siren, which can be compared to the gray dashed line for GW170814 which was localised to one galaxy by its electromagnetic counterpart. The estimate for this bright standard siren can be improved by the addition of the dark sirens, as shown in dark red line. The posteriors of Planck (Planck Collaboration et al. 2020) and SH0ES (Riess et al. 2019) are shown in purple boxes as a guide. Figure from Palmese et al. (2020). limiting factor is then detector calibration and sensitivity, which determines the event rates. Work is underway at the LIGO sites to improve this, using quantum engineering techniques such as squeezed light estimate a 5yr observing run by the upgraded LIGO, Virgo, KAGRA and LIGO India detectors will be enough to measure H Relativity correct, they are self-calibrating systems independent of the distance ladder and with minimal astrophysics input. Event rates and improvements in detector calibration are suﬃcient to converge to a ∼ 2% precision within the next decade. In summary, the appeal of gravitation waves is that if one believes General Baryon acoustic oscillations (BAO) are the relic of density ﬂuctuations propagating in the pre-recombination universe. The baryons in an initial localised seed of overdensity (such as would originate from inﬂation) are subject to radiation pressure and expand outwards at the speed of sound c dark matter behind. At recombination, temperatures have dropped enough so that protons and electrons can combine to neutral hydrogen and the Thomson scattering rate quickly drops below the Hubble expansion rate. At the drag redshift z the baryon overdensity shell is “frozen in” at a characteristic distance relative to the central dark matter overdensity. This distance in comoving coordinates is what is referred to as the sound horizon. As the universe continues to evolve, both the baryon shell and the central dark matter overdensity attract further gravitational infall of matter, forming galaxies. Thus, the sound horizon becomes visible as a characteristic (statistical) physical separation between galaxies. The sound speed c on the baryon-to-photon ratio with R = 3ρ dependence, and Planck data gives z given a cosmological model, r the evolution of structure from z compare observations of galaxy clustering in the late universe to r with data for r (Feldman et al. 1994). One assumes galaxies are a Poisson sample of the relative matter density ﬁeld 1 + δ(~r) and so where ¯n is the expected mean space density of galaxies. It is possible to work either with the two-point correlation function ξ(~r) ≡ hδ(~r alently the power spectrum It might seem preferable to use the power spectrum, as we are looking for a characteristic wavelength, and the modes of the power spectrum are independent if the density ﬁeld is Gaussian. Also, the galaxy survey footprint in real space becomes an easily understood point spread function in Fourier space. In practice though, because of issues with binning such as cut-oﬀs in Fourier space, most research papers calculate both the correlation function and power spectrum. Adjusting ¯n above can correct for bias introduced by edge eﬀects of angular and redshift cuts of the survey. The theory of measuring BAO was largely worked out by the mid 1990s termines d and White 2008) in a given redshift bin centred at z. With ∆θ and ∆z, the density peak separation transverse to and along the line of sight, we can solve for d cosmology seeded with density perturbations. This is matched to our real data with stretches α where d is the monopole term and  = α in ΛCDM (with small scale power suppressed) is then “tweaked” to compare to the real sky to justify ΛCDM. However, this method is essentially perturbative. If the real cosmology is far from the ﬁducial one, a bias might be introduced, but if it is close it should work well. stein et al. 2007; Anderson et al. 2012), and we describe it brieﬂy here as we will mention it in Sect. 4, when we discuss modiﬁed gravity. The observed positions of galaxies will be somewhat moved from their original positions “on” the sound horizon, due to their subsequent infall towards overdensities. This has the eﬀect of blurring the peak of the correlation function, and reduces precision. Reconstruction reverses the infall in the following way: (a) take a volume and smooth out small scale structure < 20 Mpc (b) embed it into a larger scale random structure to remove edge eﬀects (c) estimate the displacement ~q of each galaxy by the continuity equation ∇ · ~q = −δ galaxies by −~q plus an additional shift for redshift space distortions (e) do the same to the mock catalog. Two Degree Field survey (Cole et al. 2005), and the Sloan Digital Sky Survey (Eisenstein et al. 2005), followed by WiggleZ (Blake et al. 2011, 2012) and others. We discuss here results from Baryon Oscillation Spectroscopic Survey (BOSS) (Anderson et al. 2012). BOSS has spectroscopically surveyed 1.5 million luminous galaxies over 10,000 deg separations greater than 62 analyse the BAO signal in Data Release 12. The galaxies are divided into three bins each of redshift and cos θ. From a ﬁducial cosmology with h = 0.676, they ﬁnd α = 1.042±0.016 after reconstruction. Their result is summarised as a set of three values for d precision ∼ 2.5%. Expanding the power spectrum in spherical harmonics, the monopole deand H. We start with a mock galaxy catalog constructed in a ﬁducial There is the suggestion of circularity in this: a random catalog constructed A technique called reconstruction is used to sharpen the BAO signal (Eisenis the fractional galaxy overdensity and bthe galaxy bias (d) shift the Historically, the ﬁrst galaxy surveys to report a BAO detection were the Fig. 14: A beautiful illustration of the BAO peak in the redshift bin 0.4 < z < 0.6 of BOSS DR12. The left hand panel shows the power spectrum along and transverse to the line of sight, and the right hand is the two-point correlation function. The anisotropy visible in the graphs breaks the d Figure is from Alam et al. (2017). important parameter is b overdensity to the matter overdensity. Constructing a mock galaxy catalog by brute force of N-body simulations is prohibitively expensive. Instead, construction starts with a fast, approximate gravity solver for the matter density ﬁeld seeded with initial ﬂuctuations, and resolution down to halo size. Galaxies are inserted into the matter halos using the bias ratio, while preserving the two- and three- point correlation functions. BOSS assumed b but more sophisticated models exist. For example, bias tends to be higher for red galaxies (see for example Zehavi et al. 2011). Attention also needs to be paid to what types of galaxies are being counted: massive luminous galaxies of the type measured by BOSS reside in high density nodes of the cosmic web, whereas emission line galaxies are strung out along the ﬁlaments. For further details, see Kitaura et al. (2016). BOSS DR12 uses two independent mock catalogs to mitigate against biases introduced by simulation eﬀects. and the sound horizon r a cosmological model with constraints on its density parameters supplied by other datasets (for example SN Ia, CMB, or even the full shape of the matterpower spectrum), or we must supply a prior for r from the CMB, but see also below). We describe three independent results below. power spectrum and the JLA sample of SN Ia. They ﬁnd H 0.5 km s Collaboration et al. 2020), but is not independent of it. a WMAP-derived prior for r The construction of mock galaxy catalogs is a key part of this analysis. An BAO in isolation do not constrain H, but rather the combination of H(z) Alam et al. (2017) used ΛCDM calibrated to a combination of the Planck Mpc. This value is consistent with the Planck result (Planck Lemos et al. (2019) use a parametric formula for H(z), in conjuction with 67.9 ± 1.0 km s parametric H(z) allows for deviation from ΛCDM by separating early and late universe expansion histories. They determine Ω and numerous balloon experiments (Fixsen 2009). Setting Ω density Ω available way to get it is from primordial deuterium abundances. In Big Bang nucleosynthesis (BBN), deuterium is burned to create tainty due to the reaction rate of d(p, γ) physical baryon density so [D/H] decreases with Ω primordial [D/H] abundance measured from metal-poor damped Ly-α systems, which is 2.547 ± 0.033 × 10 the temperature, and the result corroborates the Planck value the analysis with galaxy data from the Dark Energy Survey (DES) Year 1 produces a consistent result (DES Collaboration 2018). has been used to derive a set of three correlation measures (referred to as “3x2pt”). These measure the angular correlation of galaxy positions with each other, with the tangential shear of their shapes caused by weak lensing, and the cross-correlation in the two components of their ellipticities also due to weak lensing. Combining this with Planck, SN Ia data, BAO and redshift space distortions, the DES Collaboration (2021) ﬁnd H when ﬁtting to the ΛCDM model. The eBOSS survey, the culmination of more than 20 years of survey work at the Apache Point Observatory, ﬁnd data plus the above additional probes to also ﬁt to ΛCDM (Alam et al. 2021). These seem to be the tightest constraints on H BAO can be combined with a H treating the sound horizon r values so inferred are in tension with the CMB, and we return to this point in Sect. 4.2. almost independent of astrophysical assumptions. They may be used in extended models beyond ΛCDM, albeit with the caveat they have been derived perturbatively against a ΛCDM background simulation. The two main data Addison et al. (2018) use data (almost) independent of CMB in ΛCDM. is derived from the CMB temperature as measured by COBE/FIRAS = 1 − Ω− Ω. To get the sound horizon, we now just need the baryon (see Eq. (42)). Without using the CMB power spectrum, the best = 66.98 ± 1.18 km sMpc. The only CMB data that has been used is These results are consistent with more recent ones. DES Y3 survey data = 68.19 ± 0.37 km sMpcwhen ﬁtting to a ΛCDM model using their We also note that the logic explained above to derive Hcan be reversed: In summary, BAO serve as a standard ruler in the late universe which are sets of BOSS and DES are soon to be joined by others. The Dark Energy Spectroscopic Instrument is operational, the ESA survey satellite Euclid is scheduled for launch in 2022 which is also when science operations on the Vera Rubin Observatory LSST in Chile commence. All of these surveys are complementary and will increase the precision and range of BAO measurements. Combined with deuterium abundances, BAO are an important corroboration of the Planck and WMAP results for H The CMB power spectrum carries the imprint of the same acoustic oscillations we described for BAO, sampled on the surface of last photon scattering. The Planck 2018 value of H et al. 2020) is notable for its precision, and is consistent with previous results such as Boomerang 64 ± 10 km s 70.0 ± 2.2 km s Cosmology Telescope (ACT) ﬁnds 67.9±1.5 km s There is, however, a moderate tension between Planck and the South Pole Telescope (SPT) result of 71.6 ± 2.0 km s diﬀerence is not yet clear (Henning et al. 2018; Handley and Lemos 2021). spectrum sampled on the two-dimensional sky of the early universe, and why is the Planck value so precise? determines cosmological parameters. The spectrum is created by primordial ﬂuctuations that evolve as a function of their scale and the sound speed, until recombination. Whilst this radiation is highly isotropic, we can obtain cosmological results from measurements of small anisotropies in both the temperature and the polarization of CMB photons. The former of these provides the largest amount of cosmological information, and can in fact measure the Hubble constant at high enough accuracy to be in tension with direct measurements without relying on polarization. These temperature anisotropies (∆T ≡ T (~n) − T The power spectrum of the coeﬃcients is commonly expressed as D 1)C is the estimator of the power spectrum (we have only one CMB, so averaging over m is a way to estimate the true “universe average” we want). The spectrum is shown in Fig. 16 and contains all the statistical information of the Two questions are immediately raised: how is Hderived from a power To answer the former, we need to explain how the CMB power spectrum /2π, where temperature anisotropies, as long as these are Gaussian (no evidence supporting non-Gaussianity in the CMB has been detected at the time of writing). This power spectrum has three main features: – Large-scale plateau at scales (` < 100) that are not aﬀected by post- – Acoustic oscillations which depend on the sound horizon size – Silk damping which is photon diﬀusion from hotter regions to colder ones, With all of this, how does the CMB constrain the Hubble parameter? Firstly, we can measure the acoustic scale length r As described in Eq. 50, this can be calculated as a function of Ω and Ω CMB. Ω eﬀect (Sachs and Wolfe 1967) – the redshift of CMB photons through wells or hills of gravitational potential during eras of non-matter domination – in the low multipoles of the power spectrum. Finally, the relative amplitudes of the acoustic peaks in the CMB serve to measure the ratio Ω computed the acoustic scale, we need a measurement of its comoving angular diameter distance θ peaks density Ω on the Hubble parameter. This is illustrated graphically in Fig. 15. in 9 frequencies, with angular resolution up to 5 arcmins (` ' 2000) and sensitivity 2 ×10 foreground eﬀects must be calculated and removed, and much of the work done by the Planck collaboration between successive data releases has been to recombination physics, and therefore reﬂect the primordial power spectrum. As noted above, we have just one “sampling” of the random seeds, so the amount of information we can extract from low multipoles is limited by cosmic variance ∝ (2` + 1). in ﬂat ΛCDM where a= Ω/Ω, Ωhdetermines cand both determine a= a(z), the scale factor at the surface of last scattering. These create the peaks in the spectrum. suppressing small scale power (Silk 1968). The mean free path of a photon is λ = 1/(σn) where nis the number density of charged particles and σp the Thomson cross-section, so photons can diﬀuse a length r=λ/H.p As the sound horizon r∝ 1/H, then r/r∝H/n. Silk damping creates the downward slope in the spectrum, so this is sensitive to n∝ Ω. h: The radiation density Ωis determined by the temperature of the hcan be estimated from the eﬀect of the integrated Sachs-Wolfe . In a ﬂat ΛCDM model, this provides a measurement of the matter , which combined with the measurement of Ωhgives an estimate Planck measured full-sky temperature T and polarisation E ﬂuctuations Fig. 15: A schematic of how the sound horizon determines H angular peak spacing θ given a model for the sound speed and expansion history there. The comoving distance χ recombination universe. The box gives the ﬂat ΛCDM formula for this. This ﬁgure is our version of Antony Lewis’ original at https://cosmologist.info develop the foreground model, and reduce the uncertainties it may introduce. Foreground eﬀects include : – Peculiar velocity of the earth-sun system induces a dipole which is – Galactic synchrotron and free-free emission caused by cosmic rays – Galactic dust emission whose contribution to the spectrum is also sub- – Point sources of microwave emission are masked. Sky masking introduces – Sunyaev–Zel’dovich (SZ) eﬀect where CMB photons interact with hot easily subtracted. The annual motion of the earth is actually helpful to calibrate the detector response. interacting with the magnetic ﬁeld and ISM of the MW. This has a nonthermal frequency spectrum (in eﬀect its colour is diﬀerent to the CMB). tracted by estimating its mapped “colour” diﬀerence to the CMB. some correlation between multipoles. intracluster gas and are Doppler-shifted (the kinetic SZ eﬀect) by bulk motion or up-scattered (the thermal SZ eﬀect) (Sunyaev and Zel’dovich 1972). The former eﬀect does not alter the black-body spectrum, but the latter does. The kinetic SZ eﬀect is sensitive to epoch of re-ionisation at z ∼ 8 (Ade et al. 2014a). The thermal SZ eﬀect may be exploited to produce a cluster map from which information on inhomogeneity can be extracted (Ade et al. 2014b). Modelling of intracluster gas predicts a spectral template (which peaks around ` ' 2000) to subtract. – Weak lensing of the CMB by matter. Weak lensing peaks between 1 < The latter two are perhaps better characterised as secondary anisotropies, as they have been used to derive cosmological information in their own right. The secondary anisotropies then provide a consistency check of certain parameters derived from the primary anisotropies, if the modelling is correct. We examine switching frequency bands, using alternative astrophysical models, and numerical simulations of parameter recovery from random CMB backgrounds (Ade et al. 2016; Planck Collaboration et al. 2020). The spectrum is then compared to one computed in a standard code such as CMBFAST (Seljak and Zaldarriaga 1996), CAMB (Lewis et al. 2000) or CLASS (Blas et al. 2011) and a posterior for each cosmological parameter is computed. In each check, no serious discrepancy with the main results was found. Although each C individually has ' 7% noise, there are 2,000 of them and the error is reduced further by the lensing signal. The binned spectrum and residuals is shown in Fig. 16. We now consider two questions regarding the data. Fig. 16 is an oscillating residual pattern lining up to the spectral peaks, indicating the Planck spectrum is slightly tilted versus theory (see Fig. 24 of Planck Collaboration et al. (2020) for an enlarged view). As each multipole is nearly independent, we can split the spectrum and run the analysis on each half. Addison et al. (2016) ﬁnd that A logical parameters with diﬀerent scales in the CMB. They ﬁx A compare H and H parameter restores concordance. is a “fudge factor” that resolves internal inconsistency in CMB data. It does not resolve the H sion with galaxy survey data such as the Dark Energy Survey and Kilo-Degree Survey. Could it be a foreground eﬀect? Its value is relatively stable for various band channels and sky masks, which argues that it is not. The Planck z < 2, and may be used as a probe of the distribution of foreground matter. The eﬀect increases as we move to smaller scales, particularily ` > 2000 (Zaldarriaga and Seljak 1998). There are two signals of lensing: (a) its nongaussianity, which can be measured by a four-point correlation function (b) a smoothing eﬀect on the peaks and troughs of the spectrum, both of which can be calculated as a function of Ω. However, there is more smoothing apparent in the spectrum than is predicted by the correlation function so a phenomenological parameter Ω= AΩhas been introduced to capture their relative amplitude. A' 1.2 ± 0.1 is seen in both Planck and South Polar Telescope (SPT) data (Bianchini et al. 2020), but appears to be absent in the Atacama Cosmology Telescope (ACT) data (Aiola et al. 2020). in greater depth below. Cross-checks on the eﬀectiveness of foreground modelling are done by Is Planck self-consistent? A feature which is (barely) visible by eye in We caution that Ais not a physical parameter connected to lensing; it Fig. 16: The binned Planck power spectrum for TT and residuals to the best ﬁt ΛCDM model. The vertical line delineates the diﬀerent methodology used to resolve the power spectrum at l < 30. Note that the ﬁgure shows the power spectrum of the temperature T, not of the anisotropies Θ. Figure from Planck Collaboration et al. (2020). Collaboration followed up on this curiosity (Planck Collaboration et al. 2020). They note a dip in the power spectrum for l < 30 pulls low-` H may account for the lower-resolution WMAP measurement being somewhat higher than Planck. The dip between 1420 ≤ ` ≤ 1480 mimics lensing but may be an unaccounted foreground eﬀect. Potential explanations such as negative curvature density Ω vincing evidence is found in favour of these models when BAO data is added to the ﬁt. A re-analysis of the Planck data using a diﬀerent foreground subtraction methodology (Efstathiou and Gratton 2019) found that A from 1.26 to 1.06 as the data was broadened from the temperature-only power spectrum to include polarisation and lensing data. Further, Planck data passes other consistency tests with the baseline ΛCDM model where A-type parameters may be introduced, such as the expected magnitude of the lensing correlation function and the Sachs-Wolfe eﬀect. Hence, the preferred explanation of the Planck Collaboration is that A be larger for full-sky Planck data is and is a roughly 2σ eﬀect. covers a 6% patch of the southern sky at a resolution 6× that of Planck, ﬁnding H does seem to vary over the sky, it is perhaps of concern that it seems to Is Planck consistent with other current CMB measurements? The SPT = 71.3 ± 2.1 km sMpc(Henning et al. 2018). SPT is limited to ` > 650 due its low sky coverage and atmospheric eﬀects and has greater noise, but when the results are compared on a like-for-like basis (“Planckin-patch” with SPT for 650 < ` < 1800), they are consistent with Planck. The analyses are independent and so argue against systematic errors (in this patch and multipole range) in either experiment. However, including higher SPT multipoles in the 150 GHz spectrum causes H 74 ± 3 km s the opposite direction to Planck data, this may be due to the l < 30 eﬀect in Planck noted above. SPT conﬁrm the Planck result of a greater lensing eﬀect than predicted, and also note a spectral tilt versus the best-ﬁt ΛCDM. However, the trend is not apparent in 143 GHz data, so it is not at all clear if this is a physical eﬀect, rather than merely chance or systematics. The ACT Data Release 4 derives cosmological parameters from a 6, 000deg the southern sky up to ` ∼ 4000, ﬁnding H (Aiola et al. 2020), however ACT appears to be in 2.6σ tension with Planck Handley and Lemos (2021); this appears to be caused by diﬀering physical baryon densities. current independent CMB data. The high precision for H Planck’s high resolution, lack of atmospheric interference, and full sky coverage. Checks have been performed on the codes, the methods used to remove foregrounds, and beam eﬀects. Internal inconsistencies hint at new physics or foreground eﬀects at small angular scales, manifesting either as a tilt or a smoothing of the spectrum, and non-physical parameters like A introduced to capture them. However after their introduction the results for Nevertheless, the persistence of these eﬀects across diﬀering sky patches, multipole ranges and experiments make them less likely to be a statistical ﬂuke. It is hoped next generation CMB experiments with resolution up to ` = 5000 will shed further light on this. Promising methods that we did not have space to discuss are Surface Brightness Fluctations (Tonry and Schneider 1988; Blakeslee et al. 2021; Khetan et al. 2021), Cosmic Chronometers (Jimenez and Loeb 2002; Moresco et al. 2018), the Tully–Fisher relation (Brent Tully and Fisher 1976; Kourkchi et al. 2020; Schombert et al. 2020), Type II supernovae (de Jaeger et al. 2020), HII galaxies (Terlevich and Davies 1981; Arenas et al. 2018), and Galaxy Parallax (Croft 2021). Quasars (Risaliti and Lusso 2019) and GRBs (Schaefer 2007) offer the prospect of extending the Hubble diagram up to z ∼ 5, further testing ΛCDM. However, despite improvements in characterising their intrinsic luminosity from observables (for example X-ray and UV ﬂux in the case of quasars), it remains challenging at present to regard them as standard candles. We refer the reader to the above for the latest work in these ﬁelds. In summary, Planck 2018 results for Hare consistent with previous and are not materially aﬀected, and remain in tension with late universe results. 4 Could new physics explain the tension? We have now reviewed the main recent results on H scientiﬁc scepticism we have identiﬁed potential sources of systematic error, we hope it is also clear how much eﬀort has been made to root out potential biases. Therefore, a possibility that should be taken seriously is that the tension is real, and a sign of new physics. Cepheids as “early versus late” tension. In this view, ΛCDM with its ﬂuids of radiation, standard model neutrinos, cold dark matter, baryons and dark energy is not the right model to derive H(z = 0) from the apparent H(z ∼ 1100). If late-time physics were changed, H be reconciled to the local one. Alternatively, a change to pre-recombination physics would alter our calculation of the sound horizon r would need to change distances to retrieve the same angular sizes of the CMB temperature ﬂuctuations. More radically, some authors have argued our local ﬁts the data very well across a huge span of the history of the universe. The CMB is not merely a “snapshot” of the universe, but carries the imprint of several epochs. The positions and heights of the peaks are sensitive to the content photon ratio close to recombination at z ∼ 1100, and CMB lensing by matter peaks between z ∼ 1–2. At earlier times, ΛCDM makes an accurate prediction of primordial element abundances, especially deuterium. At late times, galaxy surveys are consistent with the evolution of perturbations in a ΛCDM background to late universe. The shape of H(z) can be read oﬀ SN Ia luminosities between 0.01 < z < 1.4, and is also consistent with ΛCDM. amount, without disrupting ΛCDM’s other successful predictions. The model must be testable, compatible with particle physics results, and ideally not ﬁnetuned. Two serious problems must be avoided : by dint of extra parameters, most models increase the range of allowed H the local universe is convolved with the expanded likelihood, the posterior will – by construction – overlap with the prior. So to claim a resolution of the Hubble tension in this way is to use the same data twice: once to construct the posterior, and once to compare it. This can be avoided if the extra parameters have some preferred values – for example, a range that is predicted by microphysics (Vagnozzi 2020). A second problem with the use of H discussed by Efstathiou (2021), who points out the diﬀerence between it and the actual data analysis performed by the SH0ES team, which is a calibration Some authors have interpreted the disagreement between CMB results and is diﬀerent from an average over randomly placed observers. The essential problem for model builders is this: in other respects, ΛCDM between z ∼ 10–10, the high-` slope depends on the baryon-to- , and place tight limits on deviation from spatial ﬂatness in the mid What we want to have is a model capable of modifying Hby the right of the SN Ia ﬁducial absolute magnitude M away from its true source; it would instead be better to use M priors for this are given in Camarena and Marra (2021). Another prevalent issue is the selective use of datasets – in particular, BAO provide strong constraints on the late universe as we shall see shortly, and to omit them is again to risk a misleading analysis. that unfortunately we do not have space to introduce them all. Our preference here is to review selected models in broad classes that we hope will be informative, and readers are referred to extensive reviews by M¨ortsell and Dhawan (2018), Knox and Millea (2020), Di Valentino et al. (2021), and Vagnozzi (2020) if they would like more detail. In some respects, it is easier to propose changes to the late universe: one does not have to “negotiate” with the CMB power spectrum! It is straightforward to generalise the expansion history into any number of “cosmology-independent” forms, that do not rely on any speciﬁc form of matter-energy, or general relativity: all that is retained are the Copernican principles of homogeneity and isotropy, and the existence of a space-time metric (for example, see Eq. (15)). The latter is important, as it implies the Etherington relation: d which locks angular diameter distances against luminosity distances. Traditional models of large-scale modiﬁed gravity (normally referred to as MOND) would seem to have a low chance of solving the Hubble tension; the success of using reconstruction to sharpen the BAO peak argues that standard gravity works as we expect on these scales. Desmond et al. (2019) have proposed that small-scale modiﬁed gravity could distort the calibration of Cepheids in large galaxies like the Milky Way and NGC4258, however this appears to be disfavoured by the fact that using the LMC as a sole calibrator does not materially change H Clearly our universe is not completely uniform, so could we by chance live in an underdense region, measuring a local H in galaxy survey data by Keenan et al. (2013). They cite peculiar velocity outﬂows from 0 < z < 0.1 in the 6dFGS galaxy survey as evidence for a local underdensity, ﬁnding the universal (ie CMB) H by 1.8%. However, we would expect to see the eﬀect of a line of sight exiting to H. To use an Hprior is then to transform and compress the data A large number of creative proposals have been put forward, so much so Shanks et al. (2019) have investigated the “local hole” idea, initially noted our underdense region as a kink in the residuals of SN Ia luminosities ﬁtted to ΛCDM, or alternatively as an anistropy in H of the SN Ia survey used. While the sky distribution of low-z SN Ia versus Hubble ﬂow SN Ia are very diﬀerent, and there is an uptick in magnitude residuals on the boundary between the two (see for example Fig. 11 of Scolnic et al. 2018), the lack of evidence for such a kink places limits on any local under-density (Kenworthy et al. 2019). Also, large-scale structure simulations ﬁnd the probability for us to be in an underdensity of such magnitude is less than 1% (Odderskov et al. 2017; Macpherson et al. 2019) (although a diﬀerent likelihood of a void may be obtained in a revision of ΛCDM). That said, the observational tension between claims of local under-density based on galaxy surveys, and the lack evidence for it in SN Ia data, remains unexplained. velocities. Although heliocentric redshifts can be measured up to an accuracy of 10 (2019) have analysed the eﬀect of redshift biases on H for a set of standard candles between 0.01 < z < 0.15 causes a bias in H ∼ 3%. Systematics might arise in peculiar velocity estimates or corrections for residual stellar motion (for example, if observing on one side of a spiral galaxy). Rameez (2019) has pointed out diﬀerences between the redshifts for SN Ia in common between the JLA and Pantheon catalogs; there are 58 with diﬀerences > 0.0025, concentrated in an arc opposed to the CMB dipole. Nevertheless, this does not appear to be biasing any H In ΛCDM, dark energy is a property of the vacuum, with equation of state ρ = wP where w = −1. This distinguishes it from standard inﬂation models, where −1 < w < 0 is a time-varying function of the potential and kinetic energy of a scalar ﬁeld, but makes its microphysical origin rather obscure. This has caused some theorists to speculate with alternative models. sidered as a solution. The CMB has little to say about late-time dark energy: at early times, its physical density is much smaller than matter and radiation. The late time Integrated Sachs–Wolfe eﬀect does imprint on the spectrum, but only as a large scale secondary anisotropy once the universe has entered the dark-energy dominated phase at around z ∼ 0.3. (constant) value. However as what is required is a boost to late-time expansion to match the local universe H dark energy) to solve the tension. Phantom dark energy does not occur in standard single scalar ﬁeld models, but is possible in models with more complex ﬁeld conﬁgurations. If sustained, phantom dark energy leads to a “Big Rip” in a ﬁnite time, where all matter is pulled apart by accelerated expansion of the universe. Density ﬂuctuations also necessitate the adjustment of redshifts for peculiar , care is needed in the conversion to cosmological ones. Davis et al. In principle then, modifying dark energy in the late universe may be con- A simple modiﬁcation of ΛCDM would be to allow w to take on an arbitrary nomenon by allowing its equation of state to vary with time. This may be done writing w(a) = w nomological Ω(z). This could be motivated by the action of scalar ﬁelds, dark matter decaying to dark energy, or some other novel microscopic theory of dark energy (see for example Di Valentino et al. 2021). However, it can be shown that such models have a generic problem: the sound horizon. The sound horizon r (Eq. 42). But it is also imprinted on the late-time universe, as the peak in the angular correlation function of galaxy number densities, which constrains the product r vert the angular size to a physical size at a given redshift, SN Ia to constrain the late-time expansion history, and r CMB. Pantheon supernovae to obtain just such a late-time r modiﬁed late-time dark energy models applied to Planck data can resolve its (2020) investigated this in ΛCDM with the same result. appears that late-time modiﬁcations to ΛCDM will not be satisfactory as a solution to the Hubble tension. We have mentioned how the CMB is a set of rulers whose scale is ﬁxed at diﬀerent epochs. To recap, the observed angular scale of the sound horizon at a given redshift z is where c baryons, given by ω of photons approaches the Hubble radius c/H(z), hence z of all of the densities z sound horizon from (42) by ' 7%, as shown in Fig. 17. ﬁxed by data as follows. As discussed, we may omit dark energy ω numerator. In the late universe the radiation density is not signiﬁcant, and using the spatial ﬂatness implied by BAO, we may write Ω peaks represent modes that have crossed the horizon successively earlier in the universe when the ratio of matter to radiation was lower, and so will have had their growth suppressed by radiation pressure to a greater degree. Ω determined in this way from the CMB is consistent at a variety of redshifts Alternatively, dark energy may be considered a late-universe emergent pheis normally introduced via its deﬁnition from early universe physics H. All that is needed is a late-time distance measurement to con- Arendse et al. (2019) have used the H0LiCOW lens distances, BAO and tension with the late-universe, but not its rtension. Knox and Millea Hence, if all data including BAO are considered – as they should be – it (ω, z) is the sound speed which depends on the physical density of We may isolate the free parameters of this formula, from those that are Ωis determined by the heights of peaks in the CMB spectrum. Successive Fig. 17: ΛCDM tensions in the r epoch. There is no combination of values in standard, ﬂat ΛCDM that can explain observations. Figure is from Knox and Millea (2020). with values from the late universe: Lyman−α absorbtion lines of quasars, BAO, galaxy lensing and the SN Ia Hubble diagram (see Fig. 3). the relative heights of odd and even peaks: these are respectively compression and rarefaction modes, sensitive to the pressure to density ratio. Ω the CMB is consistent with the deuterium fraction predicted by BBN. Then, substituting for H(z) using (7), Eq. (51) becomes Hence we see that for ﬁxed ω are corroborated by non-CMB datasets – h only appears in the numerator. So to change H change ω non-dark matter component to the mix of the early universe. hand r≡ ris the sound horizon from the end of the baryon drag Further, Ωis determined by the Silk damping of high-` modes, and also by , allow conversion between energy types, or add a new non-baryonic, Extra relativistic species, such as additional neutrinos beyond the Standard Model ﬂavours, are well-motivated in extensions to the standard model in particle physics. There is a well-known constraint of 3 on the number of light neutral particles from the decay width of the Z-boson, but this bound may be avoided if the new particles do not couple to the Z (for example, if they are sterile neutrinos which only interact gravitationally). The number of species are parametrised by N where the fractions are due to the fermionic nature of neutrinos, and in the standard model N Eq. (52), would need to be increased to keep θ unchanged (Bernal et al. 2016). equality z the heights by also increasing Ω determined from SN Ia, BAO and lensing unless we accept dark matter decay between the CMB and then. Additionally, N to be < 4 at the 3σ level during BBN (Aver et al. 2015) (see also Fig. 39 in Planck Collaboration et al. 2020). neutrinos. The new species couples to itself, but not the standard model neutrinos (although it mixes with them). Free streaming neutrinos have a small but measurable impact on the spectrum as they travel faster than the sound speed cand drag some of photon-baryon ﬂuid with them, resulting in a sound horizon that would be larger than a neutrinoless universe. Self-interactions slow the speed of the neutrinos, and by tuning the strength of the interaction it is possible to arrange for the CMB spectral peak heights to be unchanged (with some small changes to other parameters such as the primordial power spectrum tilt n particle, their mixing may be suppressed at BBN, in which case they evade the He abundance constraints. Kreisch et al. (2020) have analysed such a model and found H the eﬀective coupling G the Fermi constant of weak interactions in the Standard Model is G 1.17 × 10 of muons and tau leptons, unless the self-interactions are restricted to the tau neutrino, or weakened to a level where they do not fully resolve the tension (Das and Ghosh 2021). So this proposal does seem highly ﬁne-tuned. However, it is accessible to testing by CMB S4 experiments, as it predicts a stronger decrease of the damping tail for modes ` > 2000. But if we increase ω, we have altered the redshift of matter-radiation One way in which these constraints can be avoided is via self-interacting The drawback is the self-interaction must be extremely strong. That is, Early dark energy (EDE) refers to a boost to the expansion of the prerecombination universe caused by a scalar ﬁeld potential V (φ) ∼ φ ner similar to (but milder than) inﬂation. This dark energy must be present at around z ∼ 10 sound horizon occurs. BBN constraints mean it must be absent earlier. The dark energy must also decay by the time of recombination, in order not to disrupt the damping tail of the CMB spectrum. EDE then provides a short-term boost to the expansion speed, which decreases the sound horizon by modifying the numerator of Eqn. (52). tuning again (though it should be said that the same point may be made about late dark energy – the “why now” problem)! The model shows a small residual oscillatory signal in the CMB spectrum versus a standard ΛCDM ﬁt, so if H is calculated in ΛCDM in an EDE universe, it is progressively biased lower for higher angular resolution (Knox and Millea 2020). Although this trend is already seen in WMAP and Planck, it is opposite to the trend in SPT. may also be testable before then: it is necessarily present around the time of matter-radiation equality, and therefore will leave an imprint in the presentday matter power spectrum P (k). The spectrum captures the strength of matter density ﬂuctuations of wavenumber k and decreases for k > k wavelength corresponding to matter-radiation equality, with a gradient weakly proportional to k greater precision of the Vera Rubin Observatory LSST and Euclid survey will constrain the presence of any additional components of the universe at z when EDE needs to be close to its maximum eﬀect. In Eq. (52), ω by the COBE instrument FIRAS in 1996 to be T 2009) and the equation of state for radiation energy T (t) = T Changes to ω degeneracy between the temperature of universe and H ment can then be done: what if T keeping other quantities ﬁxed? Ivanov et al. (2020) have considered this, opting to keep ﬁxed ω ponents. On combining Planck data with SH0ES, they ﬁnd the H resolved within in ΛCDM, but with T for radiation, it is a helpful recasting of the tension. Firstly, no balloon or other measurement of T Agrawal et al. (2019) have shown H∼ 72 km sMpcfor a fraction ∼ 5% of the energy content and the onset of its decay at redshift ∼ 5000, for a range of potentials. It may be said that this feels like ﬁne- EDE can then be tested by higher resolution in CMB S4 experiments. It tension While this is not new physics unless one proposes a new equation of state Secondly, the temperature of the universe may be estimated at earlier epochs via the Sunyaev–Zel’dovich eﬀect, providing an independent probe of T (z). Luzzi et al. (2015) use the Planck SZ cluster catalog to ﬁnd that deviations from T (z) = T Much eﬀort has been expended by all the teams involved to reduce photometric biases, environmental eﬀects, calibration error, lens mass modelling biases, CMB foreground eﬀects and so on. Nevertheless, there remain some areas where a degree of healthy scientiﬁc scepticism might be focused, or improvements might be forthcoming. In the spirit of our Buyer’s Guide, we oﬀer our view on these areas in Fig. 18. 2% for the highest precision results has calibrated the Cepheid luminosity zero point to 1.0% precision, and the values are consistent whether parallaxes, the DEB distance to the LMC, or the maser distance to NGC4258 are used. The calibration of SN Ia luminosity adds another 1.3% uncertainty, resulting in H 2021). For the early universe, the high resolution and sky coverage of Planck results in 0.75% accuracy (Planck Collaboration et al. 2020). This has been characterised as “early versus late” tension, but this would be to ignore the results from the CCHP calibration of TRGBs which are a late universe result intermediate between Planck and SH0ES (Freedman et al. 2019). Additionally, the size of the BAO sound horizon in the late universe when calibrated with early universe BBN constraints is consistent with the CMB value (Addison et al. 2018). 5.1 Systematics or beyond ΛCDM? Not too surprisingly there is at present no clear answer yet to the question “What is the true value of H cosmological model, and that is ΛCDM. It has been the best model around for the last 30 years, and no new model has yet presented a convincing case to replace it. Having said that, it is still disturbing that the two main ingredients in ΛCDM, dark matter and dark energy, are not understood. The H derived from the CMB is therefore part of a “package deal” which involves other cosmological parameters within the ΛCDM paradigm. Fortunately, the underlying physics of the CMB is well deﬁned – the CMB ﬂuctuation spectrum is a solution of Boltzmann’s equation. In contrast, the cosmic ladder measurements of H of the standard candles used is not fully understood. The last 20 years have seen the error bars on Hshrink from 8% to 1%– Fig. 18: A ‘traﬃc light’ colour coding corresponding to our view of the calculation of H corresponds to a solid current position with low uncertainty, yellow is where some improvements can be expected, and red is where caution or cross-checks may be needed. For more detail the reader may consult the relevant subsection in Sect. 3. This diagram was inspired by a similar plot by Wendy Freedman at the ESO H has uncertainties that may improve over the next few years. Green enjoy majority support. They either do not resolve other tensions related to can be seen as eﬀective theories of some unknown, and possibly more natural, microphysics. Given the lack of a compelling explanation from theory, and a greater understanding of how hard it is to “break” ΛCDM, opinion in the community seems to be shifting. At a conference called “Beyond ΛCDM” in Oslo in 2015, a poll suggested 69% of participants believed new physics the most likely explanation (although we suspect some bias in the voter views given the title of the conference!). However, at a conference in 2021 entitled “Hubble Tension Headache” held virtually by Southampton University, UK, over 50% of participants favoured the explanation that there were still systematics in the data. Here we comment on some frequently asked questions: – Are systematics still present? Regarding Cepheids, considerable recent – Is characterisation of “early versus late” helpful? The strongest Although many plausible extensions of ΛCDM can resolve the tension, none such as the sound horizon, or seem somewhat contrived. Thus, at best they Project have central values that are consistent with SH0ES, but due to their larger error bars are only in tension with Planck at the 1.5 ∼ 3.0σ level. Results from lensed quasars, TRGBs and surface brightness ﬂuctuations vary between papers, and cannot be regarded as convincingly supportive of SH0ES. Atacama Cosmology Telescope to within 1%, and is also consistent with the earlier WMAP satellite. However, it is in 2.1σ tension with the South Pole Telescope. Planck’s His also supported by BAO from the BOSS and DES galaxy surveys combined with an early universe prior for the baryon density. progress has been made in demonstrating crowding is under control and photometric reduction is free from bias. Nevertheless, potential weaknesses remain in the calibration and environmental corrections of SN Ia, and the calibration of TRGBs has been disputed. Re-analyses of lensed quasars show systematics are not yet under control. The nature of CMB analyses do not lend themselves easily to part-by-part decomposition or independent review, but a full re-analysis of Planck by Efstathiou and Gratton (2019) did not alter cosmological parameters. evidence in favour of this are the Cepheid results and Planck, as while other results are broadly consistent with this view they are individually more uncertain. However, if the calibration of TRGB were resolved in favour of the CCHP value, it would weaken the case for this view. It is however helpful to consider new cosmological models in terms of those modifying – Is there a convincing theoretical solution? Simply put, ΛCDM is very – Are TRGBs and gravitational waves oﬀering a way forwards? We would suggest the following in response to the question of “Which H should I use?”, depending on the desired application. – For cosmological inference: As the tension in the Hubble parameter the post- or pre-recombination universe, as at present those modifying the pre-recombination universe appear more viable. hard to break. Proposals need to clear three main hurdles: ﬁtting a wide range of cosmological data, consistency with other predictions of ΛCDM, and providing a convincing Bayesian argument that the model is preferable to ΛCDM. None yet do so. Nevertheless, there are strong hints what a solution might look like, and it is fair to point out that the innovation of Λ in ΛCDM itself solved a previous tension between a low matter density and ﬂat universe. It is likely that a consensus on TRGB calibration will be reached in the near-future, and TRGBs promise cleaner ﬁelds and better theoretical modelling than Cepheids. They are also ideal for JWST observations, which has four times the J band resolution of the HST/WFC3 H band. Miras are interesting, but are more expensive to observe and harder to calibrate. Gravitational waves oﬀer an inherently transparent and accurate way to measure H, once enough bright and dark siren data has been collected by the middle of this decade. If these two were to corroborate SH0ES, it would be strong evidence in favour of the breakdown of ΛCDM. Alternatively, if they agree with Planck, one would have to conclude SH0ES is the outlier. is between local direct estimates, and the best-ﬁtted ΛCDM model, the only two possible explanations are unknown systematic eﬀects in the local direct estimate, or that our universe is described by a model diﬀerent from ΛCDM. If it turns out that there are systematics in the local estimate, it is clear that we should use Planck’s value H= 67.4 km sMpc(Planck Collaboration et al. 2020). If the universe is not ΛCDM, it remains the case that ΛCDM is a great ﬁt to cosmological data with all other parameters consistent with their CMB-derived values. We therefore recommend the full package of Planck or Planck-like cosmological parameters should be used, so one can test the the ΛCDM model self consistently. A speciﬁc example is in choosing parameters for N-body simulations. It makes sense to select the Planck set of parameters (H, Ωand σ, etc.) so one can test the growth of structure with cosmic time given ΛCDM ﬁt to Planck. In contrast, the ΛCDM model with H= 73.2 km sMpcis a poor ﬁt to most existing cosmological observations (CMB and galaxy surveys). Another application of using Hand other parameters from Planck is when using them as priors for analyses of new data sets, in a Bayesian framework. Here the errors bars of parameters or the full Planck posterior of – For pedagogical purposes: For the purpose of teaching cosmology, pop- Looking to the future, the Zwicky Transient Facility and Foundation surveys of nearby SN Ia will be very helpful in reducing potential calibration issues. They will do this by resolving the underlying population characteristics, having cleaner selection functions, and providing more galaxies in which to calibrate the distant Hubble ﬂow sample. The early signs of Gaia Early Data Release 3 is that it provides a considerable reduction, but not elimination, of the bias apparent in Data Release 2. But it is not the last word, and the next release is scheduled for 2022. The key point of Gaia is that it addresses lingering concerns about how the low metallicity environment of the LMC, or the maser disk modelling and crowded ﬁelds of NGC4258 may aﬀect the calibration of standard candles. In particular, it will be useful to have accurate Gaia parallaxes to Milky Way globular clusters such as ωCen to provide additional calibrators of the TRGB. The JWST will greatly expand the range of TRGB observation, and provide continuity in the case of any further degradation of the ageing HST. The Extremely Large Telescope in Chile, scheduled for ﬁrst light in 2025, can extend the range of DEB distances to M31 allowing it to contribute its Cepheid and TRGB populations to calibrations. the probability distribution are important, as they propagate through the parameter marginalisation process. We also maintain that it is not correct to inﬂate Hconﬁdence intervals to “hedge bets”, unless one does that in a non-ΛCDM model and fully rederives the posteriors for all other parameters in a Bayesian fashion. (Riess et al. 2021) using Gaia parallaxes and Cepheids is a good choice, as it is consistent with independent measurements such as Megamasers and Miras and re-analyses of the data (albeit generally starting with the photometric parameters, and not the original imaging) have produced mostly consistent results. Therefore, the SH0ES value should be used for calculations that require an expansion rate or distances alone, but are local enough to have minimal cosmological model dependence. Having said that, the CCHP TRGB result of H= 69.8 ± 1.9 km sMpc(Freedman et al. 2019) is noteworthy for its consistency with Planck and the advantages TRGB observations may oﬀer relative to Cepheids. It is very much a case of “watch this space” for developments. ular talks, or back-of-the-envelope calculations it would make sense to use the intermediate round value of H= 70 km sMpc. Wherever possible we recommend indicating the dependence of your key results on H either in formulae, or as a method to adjust the key results. Anticipating a future resolution of the tension, this will be of great assistance to future researchers. have been no more observations of “bright sirens” like the neutron star merger GW170817 so far. But the commencement of operations at the VIRGO detector in Italy, and recently the KAGRA detector in Japan oﬀer hope that the more frequent event detections and better sky localization, combined with an improved instrumental calibration, will provide a 2% measurement of H within this decade. Many more time-delay lensing systems will be seen in future galaxy surveys, so there is a strong incentive to resolve remaining systematics in the modelling and speed up the analysis pipeline. the CMB to an order of magnitude higher than Planck, and will start taking data in the next two to three years. Close to 2030, two new CMB Stage 4 telescopes will be operational in Chile and the South Pole, which will further extend the spectral resolution. The depth of these surveys will be able to support or rule out many pre-combination modiﬁcations of ΛCDM. Surveys conducted by the Dark Energy Spectroscopic Instrument (DESI), Vera Rubin Observatory (previously named LSST), Euclid satellite and Nancy Grace Roman Space Telescope (previously named WFIRST), combined with theoretical progress in the quasi-linear regime of structure formation, will enable the matter-power spectrum to be compared with ΛCDM predictions with much greater depth and resolution. In the ﬁeld of gravitational waves, it is perhaps disappointing that there The Simons Telescope in Chile aims to map the polarisation spectrum of