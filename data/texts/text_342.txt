Esta obra est´a sujeta a una licencia de Reconocimiento - NoComercial - SinObraDerivada 3.0 Espa˜na de CreativeCommons. Fecha de entrega (mm/aaaa): October 8, 2021 This is for my aunt, Maria Giuliana Farris, may she rest in peace. She helped me get the computer, and courage, with which these words were typed. I would like to thank my advisor, Luis Esteve Elfau, for his guidance, trust, and for helping me prioritize my time in this work. Also, to thank the Universitat Oberta de Catalunya and its staﬀ for the opportunity. Finally I would like to thank my brother Pedro, my mother Lilian, and my beloved Natalia, for supporting me throughout the years. Recommender Systems have been the cornerstone of online retailers. Traditionally they were based on rules, relevance scores, ranking algorithms, and supervised learning algorithms, but now it is feasible to use reinforcement learning algorithms to generate meaningful recommendations. This work investigates and develops means to setup a reproducible testbed, and evaluate diﬀerent state of the art algorithms in a realistic environment. It entails a proposal, literature review, methodology, results, and comments. El Sistema de Recomendaciones es parte importante de un comercio electr´onico. Tradicionalmente estos sistemas se basaban en algoritmos de reglas, m´etricas de relevancia, clasiﬁcaciones y aprendizaje supervisado. Actualmente se pueden usar algoritmos de aprendizaje por refuerzo para crear recomendaciones m´as ´utiles. Este trabajo investiga y desarrolla maneras de producir un sistema de prueba de agentes, y comparar diferentes algoritmos de ´ultima generaci´on en un entorno de simulaci´on basado en datos reales. Se describe una propuesta, una revisi´on de la literatura, la metodolog´ıa de investigaci´on, los resultados obtenidos y comentarios. Keywords: deep reinforcement learning, recommender systems, e-commerce recommendations 2.4 Current Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1 Learning Environment Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2 How RecSim Simulates Recommender Systems . . . . . . . . . . . . . . . . . . . 22 3.3 Comparison Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.4 Evaluation Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 3.5 Evaluation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.6 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4.1 Preliminary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.2 Hyperparameter Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.3 Benchmark Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.4 Relevance Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 1.1 Adaptation of CRISP-DM focusing on investigation, rather than commercial 2.2 Example architecture of a Dueling Q-Network. Notice how value and advantage 3.1 Class diagram covering the most important parts of the implementation. . . . . 28 4.1 Comparison of an agent that recommends movies randomly (orange) and an 4.2 Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the 4.3 Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the 4.4 Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the 4.5 Comparison of training of the agents with the best hyper-parameters found. The A.1 Dueling DQN gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 are modeled separately. Only one neuron is used to estimate v(s), and to estimate A and Q we need as many neurons as there are actions available. The last layer is not a regular linear layer, but rather a calculation, as explained in eq. (3.1). . 15 agent that learns from user reviews (blue). . . . . . . . . . . . . . . . . . . . . . 30 REINFORCE agent. Notice how γ = 0.95 had the highest average, but also the highest variance. Therefore it would be wiser to choose 0.9 in this case. . . . . . 31 Actor-Critic agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Dueling DQN agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 horizontal line at return 32, symbolizing the performance of a random agent, was added to help compare them. The area around the lines is the 95% conﬁdence interval. The Dueling DQN agent was trained for 3 seeds, while the others were trained for 5. In this scenario, the maximum possible reward is 50, and it would mean that the user rated 5/5 for every single recommended movie. . . . . . . . . 33 A.2 Value estimator gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 A.3 Policy estimator gradient graph. . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 2.1 Example of a N × M Utility Matrix. Cell values represent utility or relevance, 3.1 Comparison of diﬀerent Recommender System environments. . . . . . . . . . . . 22 3.2 Comparison of recent studies that train RL agents using the MovieLens dataset. 4.1 Comparison of relevance metrics for an agent making random recommendations B.1 Actor-Critic and REINFORCE hyperparameter search results. . . . . . . . . . . 50 B.2 Dueling DQN hyperparameter search results. . . . . . . . . . . . . . . . . . . . . 51 for instance how many times the user at that row clicked on the product at that Note the diversity of metrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 and a Dueling DQN agent. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 Case-Based are knowledge-based systems that let users provide a sample item, and recom- Cold-Start Problem is a problem of recommendation systems that emerges when recom- Collaborative Filtering in recommender systems is the concept of using one user’s preferKnowledge-Based are recommendation systems that rely on user created constraints, and Supervised Learning are a class of machine learning algorithms that address regression and mend similar ones. 7, 35 mendations need to be made for a user who we don’t have enough data about. 7, 18, 33, ences to infer other’s. 6, 35 domain-speciﬁc product matching rules . 7, 8 classiﬁcation problems. 7, 10, 19, 20 A2C Advantage Actor-Critic Networks. 15, 33, 37 CTR Click-through Rate. 1, 17–19, 24 DDQN Double Deep Q-Networks. 14, 18, 19 DL Deep Learning. 10, 11, 13, 16, 20, 35 DP Dynamic Programming. 13 DQN Deep Q-Networks. 13, 15, 16, 35 DRL Deep Reinforcement Learning. 1, 2, 13, 16, 18 ERB Experience Replay Buﬀer. 14, 17, 19 i.i.d. independent and identically distributed. 12–14 LTV Life-Time Value. 15, 18 MDP Markov Decision Process. 2, 12, 13, 15–17 ML Machine Learning. 4, 7, 9–11, 19 NDCG Normalized Discounted Cumulative Gain. 9, 35 PER Prioritized Experience Replay. 25, 27 ReLU Rectiﬁer Linear Unit. 10, 25, 27 RL Reinforcement Learning. 1, 2, 5, 9, 11, 12, 15–19, 23–25, 35, 36 RS Recommender System. 1, 4, 15–17, 19, 22, 24, 33–37 TD Temporal-Diﬀerence. 13, 14, 25, 27 UCB Upper Conﬁdence Bound. 11, 15 Due to recent developments in large scale industrial production, improvements in logistics, and scalable technological infrastructure, e-commerce platforms are now able to oﬀer thousands of products to its users. One of the challenges that arise in this scenario is how to properly recommend products, given such large variety but little space for recommendations. A solution that addresses this problem is commonly known as a Recommender System (RS). Search engines like Apache Lucene rely on users typing or selecting ﬁlters that approximately match what they are looking for. They usually calculate relevance scores that estimate how well each document matches each search term, and order results based on it [see 1, chap. 3]. Some RS leverage features such as user-made evaluations, search history, demographic data, and geographic data. Then, they may apply a family of supervised learning ranking algorithms like Learning-To-Rank (LTR) [see 2, chap. 13]. Such systems may even estimate which products have the highest probability of being chosen by the end user. Alternatively, there are methods based on Reinforcement Learning (RL) that may take into account a user’s journey. They make recommendations not only based on search ﬁlters or user data, but also on a user’s interaction and behavior (i.e., what buttons were clicked and when). The usual goal of such agents is to optimize for metrics like Click-through Rate (CTR), conversion rate, or dropout rate. In other words, to increase the percentage of users that are clicking on products or buying them, or to decrease the percentage of users that leave without selecting recommendations. The proposal for this work is to leverage interaction data from large retailers, use them to generate a RL environment, and measure how diﬀerent Deep Reinforcement Learning (DRL) algorithms perform under these circumstances. There has been research on ways generate faithful user simulations from website interaction data, and how to use such simulations to create realistic learning environments. The goal is not to improve the simulations or environments, but rather perform a comprehensive evaluation on which methods work best. The relevance of this project can be understood from three points of view. There is the scientiﬁc relevance of applying the latest advancements in DRL to new problems and publish the results. There is the social beneﬁt of developing technologies that help humans ﬁnd what they need more easily - especially in the information era of overwhelming amounts of data. Lastly, there is the commercial relevance, since online shopping has been on the rise, and retailers often struggle to help consumers ﬁnd what they need. Personally, as many other online consumers, I have had trouble ﬁnding products online. More often than not I knew almost exactly what I was looking for but couldn’t ﬁnd relevant products in the recommendations. In many cases it was only after exhaustive searching through pages of recommendations that I found what I was looking for. From a professional point of view, I’m also very intrigued by this problem. In 2018 I started working as an engineer in an Elasticsearch partner ﬁrm. I’ve had the chance to study and learn how modern search engines estimate relevance and can scale to millions of users. In 2019 I’ve worked on improving marketing campaigns by predicting psychological proﬁles from users, based on their data. In 2020 I’ve had the opportunity to work on a solution based on RL that used interaction data to dynamically customize the user interface, with the goal of improving engagement. Another personal motivation for me was the book Persuasion Proﬁling by Kaptein [3]. It explains, among other things, why online stores fail to reach the conversion rates of their physical counterparts. The author describes this issue from a decision making standpoint, and discusses concepts that could be implemented to improve the performance of online retailers. I believe, that there is a lot of potential in modeling the recommendation problems as a Markov Decision Process (MDP), and solving it using DRL. The main goals of this work are to use an existing RL environment (containing an interaction dataset and a user simulator), apply state of the art DRL algorithms to it, thoroughly compare diﬀerent algorithms, and evaluate which ones work best under which conditions. In terms the environments, algorithms, and metrics, we are still evaluating which are more relevant in the literature. Currently the best potential environment we found was MARS-gym [4], which is based data from the ACM Trivago RecSys 2019 Challenge [5]. A few potential algorithms are: Deep Q-Learning (DQN), Double Deep Q-Learning (DDQN), Categorical DQN, Rainbow DQN, REINFORCE, and Advantage Actor-Critic (A2C). For the metrics we would like to compare standard RL metrics such as the cumulative mean reward, and more traditional RecSys metrics like catalog coverage [6]. This metric measures what percentage of the product catalog was displayed to a user during a session. The partial goals for the project are setting up the user simulations, setting up the environments, implementing each of the RL algorithms, and creating a testbed for reproducible comparison. If the main goals are reached in suﬃcient time, it would be interesting to evaluate how trained RL models perform after perturbations in the user simulator (to simulate seasonality in e-commerce websites). Another potential direction of investigation would be to evaluate if a model, pre-trained in a certain environment, performs well in a new but similar environment (to simulate adaptation to new demographics). Popular methodologies such as CRISP-DM [7] focus on the development of commercial and stable products. Therefore, it would be pragmatic to use a simpliﬁed version of them in an investigation eﬀort. We propose a reduced version of CRISP-DM, without the Business Understanding and Deploy steps. A sequential diagram of this methodology can be found in ﬁg. 1.1. The scope of the remaining steps would be: Figure 1.1: Adaptation of CRISP-DM focusing on investigation, rather than commercial application. 1. Data Understanding: to obtain and understand the Trivago tracking data 2. Data Preparation: to use the tracking data to generate a simulator, a RL environment, 3. Modeling: to implement several RL agents to be trained in the environment 4. Evaluation: to run several variations of each agent, measure, and compare them. and a testbed The planned schedule for the development of the thesis is represented in ﬁg. 1.2. It speciﬁes the order and dependencies of the multiple eﬀorts that need to take place for the work to be completed. The month of February is dedicated to ﬁnalizing this proposal, by deﬁning goals and reﬁning what we want to achieve. Then, during the ﬁrst three weeks of March (until the 20 will be in researching the state of the art of using Machine Learning (ML) in RS, and ﬁnding research related to the scope of this thesis. The most signiﬁcant challenges for me in this step are to get familiar with the recommender system literature, and to ﬁnd the best sources to review in the ﬁeld. Once the state of the art and related works are researched, we will concentrate on the implementation of the goals, until May 23 dataset, the user simulations, the environments, and the algorithms. It will also include the execution and measurement of all comparisons. I have experience implementing and evaluating DRL models and using Gym environments, but I lack experience with training recommender systems on oﬄine data, and specially in building user simulations from tracking data. I’m also not very familiar with the metrics commonly used in the RS literature. The next step of the project will be ﬁnalizing the writing of thesis, by appropriately documenting the methodology and ﬁndings. This step will also account for the preparation of presentations, and will ﬁnish on June 6 formal presentations, and defenses. For the purpose of reviewing the literature on applying RL to address the recommendation problem, and analyzing the state-of-the-art, the methodology described by Silyn-Roberts [8, chap. 4] was used. The literature review will ﬁrst cover a short historical overview of the ﬁelds of reinforcement learning and recommender systems. Secondly, based on some of the most inﬂuential textbooks oﬀ each ﬁeld, a review of standard techniques will be provided. Then we’ll explore a few recent review articles to have a panorama view of the ﬁeld. Finally, the key papers of the area will be analyzed, along with some more recent fringe papers. Recommender Systems grew in popularity during the 1990s. One of the ﬁrst such systems was GroupLens which was also responsible for publicly releasing large datasets that signiﬁcantly helped researchers in the area [9, chap. 1.2]. In more recent years, thanks to tech giants like Netﬂix and Amazon this ﬁeld has gained a lot of attention, and one of its hallmarks is the annual ACM RecSys challenge. The Reinforcement Learning ﬁeld as we know it came together in the 1980s, thanks to a combination of optimal control research (that started in the 1950s with Bellman) and psychological research of animal learning. A richly detailed history of the ﬁeld can be found in [10, chap. 1.7]. Artiﬁcial neural networks research started in the 1940s with cybernetics researchers attempting to generate computational models of the brain. Then in the 80s under the name connectionism there was the development of back-propagation, and multi-layer networks. Deep Learning research started in 2006 [11, chap. 1.2.1]. Most technical information described in this section was summarized from inﬂuential textbooks in the ﬁelds of Recommender Systems [9], Reinforcement Learning [10], and Deep Learning [11]. The problem that Recommender Systems solve can be generalized as in deﬁnition 2.3.1. Using this generic deﬁnition, we can explore all the types of recommender systems under a uniﬁed view. Deﬁnition 2.3.1 (The Recommender System Problem). To deliver a set of items, to a set of users, optimizing a set of goals. Some systems achieve this by predicting certain user-item metrics. Examples of metrics that could be predicted are click-throughs, user ratings, or relevance ranking. The most usual ones though are ratings of user-product tuples, and the top-k highest ranked items for a given user. Systems that use the latter are sometimes known as Top-N Recommender Systems. Common goals for such systems include accuracy, relevance, novelty, serendipity, and diversity. The predicted items could be movies in a streaming platform, books in an online store, ads in a marketing platform, or friend connections in a social network. The entities that receive predictions are usually users of an online system, but could also be businesses or even software agents. Metrics, items, and users can be modeled in diﬀerent ways. For instance, product ratings could be measured as a like or dislike (binary), a 5-star rating (numerical), a Likert scale (categorical), or even a button click (implicit). Items could be wine brands, and be modeled by the features that make them unique, for instance acidity, sweetness, bitterness, aroma, color, and aftertaste. Users could be modeled using demographic data, geographic data, their purchase history, their user interactions, and so on. One way of presenting these concepts is by using a Utility Matrix [see 12, chap. 9]. Some studies refer to the modeling of user-item goals as utility, and this matrix representation places users in rows, items in columns, and measurements for the cells. The table 2.1 contains an example of this concept. If the main features of a recommendation system are based on user-item interactions, it is known as a Collaborative Filtering system. For example, a collaborative ﬁltering wine recommendation system could recommend you a bottle because this particular brand has recently received high Table 2.1: Example of a N × M Utility Matrix. Cell values represent utility or relevance, for instance how many times the user at that row clicked on the product at that column. Note that in practice, this is often a very sparse matrix. ratings from other users with similar interests as you. Such systems are challenging because user-interaction data is usually sparse. The two main methods of collaborative ﬁltering are Memory-Based, and Model-Based, according to how predictions are made. Memory-Based methods make predictions based on the neighborhood of the current user. In other words, a distance metric is chosen (e.g., Euclidean distance, Manhattan distance), and computed pairwise between users. These methods are again subdivided into two categories, according to how predictions are made. User-Based Collaborative Filtering is a special case of memory-based methods that ﬁnd products that likeminded users chose in the past. Meanwhile Item-Based Collaborative Filtering ﬁnds items similar to what a particular user liked in the past. Model-Based methods, in contrast, use ML to predict item metrics for users. It has been shown that combinations of Memory-Based and Model-Based systems can provide very accurate results. It has also been shown that Supervised Learning can be generalized to eﬀectively tackle the recommendation problem. If the main features are modeled after product attributes, it is called a Content-Based system. In such systems user interaction is combined with item data into features, and the rating is predicted as the target of a Supervised Learning algorithm. Such systems often provide more obvious recommendations (this issue is known as low serendipity), and are not eﬀective in predicting items for newer users - this is known as the Cold-Start Problem. In contrast, they perform better when predicting ratings for new products (because many models are able to generalize knowledge based on product metadata). Systems that are modeled after user-speciﬁed constraints (e.g., search ﬁelds, ﬁlters, ranges) are called Knowledge-Based systems. They are usually applied when there’s limited domain knowledge, when user ratings tend to change over time, or due to seasonality. Such systems usually contain business-speciﬁc rules for determining product similarities, and they allow users to explicitly query what they’re looking for. These systems are labeled according to how users input requirements. Constraint-Based systems allow users to specify certain values or ranges for speciﬁc attributes of items (e.g., ﬁltering real state by square meter price). In Case-Based systems users provide sample items, or speciﬁcations, and it will ﬁnd similar items (e.g., ﬁltering real state by desired number of bedrooms). Depending on a Knowledge-Based system’s user interface, it can be categorized diﬀerently. In Conversational Systems, users are questioned about their needs via natural language dialogue (e.g., a chatbot), in Search-Based Systems users answer a preset number of questions (e.g., a quiz). In Navigation-Based Systems, also known as Critiquing-Based Recommender Systems, users iteratively request changes to an example product (e.g., a car dealership website that allows users to ﬁnd similar cars with lower carbon emission). Finally, a special case of Knowledge-Based systems are called Utility-Based, for when the relevance (i.e., item utility) formula is known a priori. Hybrid Recommender Systems are the ones that combine aspects of some of the diﬀerent techniques already mentioned. Other subcategories of recommender systems include: demographic recommender systems (when user demographic data is used), context-based or contextaware systems (when time, location, social data are used), time-sensitive systems (ratings evolve or depend on seasonality), location-based systems (user locations are used). Special types of recommender systems are social recommenders. These models recommend using social cues like social graph connections (i.e., using PageRank algorithms), social inﬂuence (also known as viral marketing or inﬂuence analysis), trustworthiness (by having users explicitly provide trust/distrust ratings), and social tagging (e.g., hashtags). Recommendation systems that model users in groups are known as Group Recommender Systems. Systems that allow each item attribute to have its own rating (for instance hotel booking websites, that rate hotels on cleanliness or hospitality) are known as Multi-Criteria Recommender Systems. The aspect of some recommendation systems to actively encourage users to rate less popular items is known as Active-Learning. There are some less popular deﬁnitions that are not present in the textbook, but are part of the more recent literature. Some researches refer to systems that directly receive user feedback on recommendations as Interactive Recommender Systems. Some researchers refer to systems that actively leverage user interaction to make predictions as Sequential Recommender Systems. Finally, some make a distinction between how soon users are expected to accept recommendations. In this sense, Long-Term Prediction systems are optimized to have users select items at any time in the future. Meanwhile Short-Term Prediction Systems expect users to select recommendations immediately. A diagram with the most important types of Recommender Systems can be found in ﬁg. 2.1 Before discussing evaluation metrics, it is important to make a distinction between how evaluations are made. Online Evaluation methods present real users with diﬀerent versions of recommender systems, and measure how each one performs. It is often the case that there are two versions and users are randomly split 50/50, and this setting is called A/B Testing. An automated generalization of this test setting can be implemented with RL, using Multi-Armed Bandit algorithms. Research using online methods is often limited, due to the commercial impact of testing unsuccessful recommenders, the lack of generalization of the domain of speciﬁc models, and commercial secrets. In contrast, Oﬄine Evaluation methods use historical datasets, often public, to train RS on recorded data. These are ideal for research because ﬁndings are comparable and reproducible. There are several goals that can be used to compare Recommender Systems, and the following deﬁnitions are purposefully generic, since many researchers decide to calculate these concepts diﬀerently, based on the goals of their work. Accuracy measures the percentage of the recommendations that would’ve been selected by a user. Coverage measures the percentage of items that were recommended, out of all the items available. Conﬁdence measures the average statistical conﬁdence or conﬁdence interval (which some ML models provide) in the recommendations. Novelty measures the percentage of the current recommendations that has never been recommended to this user before. Serendipity measures the percentage of the recommendations that would not have been selected by an obvious recommender (e.g., a simplistic model). Diversity measures the similarity between the items that were recommended. Robustness or Stability are measurements of how a recommendation system can be aﬀected by (i.e., how attack-resistant it is to) false reviews or fake ratings. Lastly, Scalability measures how an increased amount of items impact learning time, prediction time, and memory consumption. An example of a concrete goal often used in research is to maximize Normalized Discounted Cumulative Gain (NDCG). This metric averages how relevant each item in the recommendations is to the user (Cumulative Gain). The averages are weighed by how early each item appears on the list (Discounted Cumulative Gain), and normalized by how relevant each item is in general (i.e., the Ideal Relevance). Given a user u, a slate of products P = {p a relevance metric taken from user feedback, we can calculate the discounted cumulative gain as in eq. (2.1). Typically relevance is the expected rating of a user to an item [9, chap. 7.5.3]. Using groundtruth data (raw ratings) we can compute an ideal DCG metric, called IDCG, representing the maximum possible DCG. And ﬁnally, we have eq. (2.2): Which outputs a value in range (0, 1). Normally these metrics are reported as NDCG@N, where N is the number of products in the recommendation slate, or |P |. Machine Learning is a ﬁeld of Artiﬁcial Intelligence (like Natural Language Processing, or Computer Vision) that aims at building solutions to problems that involve pattern recognition, clustering, regression, classiﬁcation, and decision-making. Regression and classiﬁcation problems are commonly addressed by speciﬁc types of learning algorithms, and are referred to as Supervised Learning. Neural Networks are a class of the latter, and Deep Learning (DL) refers to some speciﬁc types of those networks (i.e., when there are multiple hidden layers). In Artiﬁcial Neural Networks (ANN), the main logic units are called neurons, and are characterized by an activation function that combines a set of weights (and often biases) and produces an output signal [11]. A perceptron is a neuron in which the output is 1 if the dot product of the weights is greater than 0, and 0 otherwise. This is often called a linear function, and can be represented as in eq. (2.3): where w is the weight vector, x is the input, and b is the bias vector. Other examples of activations include ReLU, the sigmoid function, and the softmax function. The training of such learning models requires adjusting the weights in order to minimize a speciﬁc cost function, which often is an aggregation of the loss that occurred during training. Examples of cost functions include the Mean Square Error (MSE) or the Cross-Entropy function. The logic that tells us how much to adjust the weights is called an optimizer, and is often based on gradient descent optimization. The degree to which the optimizer moves in the direction of the gradient is referred to as learning rate. Examples of optimizers include Momentum and Adam. The algorithm that optimizes all the neurons in a network accordingly is called backpropagation. When several layers of neurons are stacked together, the models are called Deep Learning. The layers can be fully connected or not, and architecture refers to how connections are setup, how layers are created, the activation functions chosen for each neuron, and so on. There are several utilities for these methods, and they are discussed in more detail in section 2.4.1. RL refers to a class of ML algorithms in which an agent interacts with an environment. The former tells the latter which action should be taken next, and receives a reward signal along with the next state. Commonly these interactions are measured by steps, and after some conditions are met the interaction episode ends (ﬁnite-horizon environments). There are RL problems though, in which episodes go on continuously. In such settings it is customary to discount rewards, so that agents can learn actions that are good in the long-term, but so that rewards in the distant future tend to zero. A classic example of a RL problem is the task of balancing a pole on a cart in two dimensions [10]. In this case, the environment would be a physics model, that would estimate the position, angle, and velocity of the pole. The agent would apply force to the cart, either left or right. For every step that the pendulum remains upright, the agent receives a +1 reward. If the pole gets too inclined, or the cart moves too far away from the starting position, the episode ends. By randomly exploring diﬀerent strategies, many RL algorithms are able to learn how to balance the pole appropriately. There is a notable class of algorithms, that seek to make optimal decisions without considering long-term agent-environment interactions (non-associative tasks), called Multi-Armed Bandits. Some examples of these include k-Armed Bandits, Upper Conﬁdence Bound (UCB) action selection, and Gradient Bandits. Bandit algorithms with environment context are called Contextual Bandits. Generally these algorithms start by randomly picking actions and observing the reward from each action. They estimate expectations of the reward, and greedily start choosing actions that maximize these expectations (this is called -greedy exploration). UCB methods have the advantage of modeling uncertainty in the reward expectation. Gradient Bandits go beyond and model the rewards as Boltzmann (soft-max ) distributions. The more advanced RL algorithms rely on the problems having certain attributes, speciﬁcally that they are Markov Decision Processes (MDP). They are extensions of Markov Chains, and therefore require the Markov Property (memorylessness). In other words, if we are at a particular state, the probabilities of transitioning to other states depend solely on information of the current state, not the past ones (i.e., a game of chess). Environments that don’t provide agents with the full state are a special type of MDP, called Partially Observable Markov Decision Processes (POMDP). Agents may learn policies, value functions, and models. Policies are a mapping between states and actions, that tells the agent which is the best action to take in each possible state. Policies can be deterministic or stochastic. Value functions predict the sum of all expected future rewards, if a particular policy is followed in the future (known as Return). Models can be estimations of the transition probability between all the states, or estimated reward for all state-action pairs. RL agents that don’t create models are referred to as model-free. Some agents attempt to learn all value functions, some attempt to learn an optimal policy (sometimes known as Control), and some attempt to do both. These last ones are called Actor-Critic methods. RL algorithms face several challenges that supervised models are not equipped to handle. The ﬁrst one is that data is not independent and identically distributed (i.i.d.). The second problem is the trade-oﬀ between exploring new action or state scenarios, and exploiting previously obtained knowledge. In some cases actions the agent takes will only payoﬀ later on, in other words, the expected reward can be delayed. Some problems also display the property of nonstationarity, which means that the transition matrix or the reward distribution may change over time. We can represent mathematically the value of being in a particular given state, using the Bellman equation. It calculates the immediate reward, plus the discounted reward of all future interactions. Given a state s, the value v deﬁned as in eq. (2.4): Here π(a|s) is the probability of this policy choosing an action a given state s, while p(s, r|s, a) is the probability of receiving reward r and transitioning to state s ing action a in state s. γ is the discount factor for future rewards, and v representation of the value of the following state s solvable numerically, albeit often unfeasible due to the high dimensionality of the state space. RL algorithms are more eﬃcient solutions to this problem. Traditional RL algorithms seek optimal policies by storing a map of the value of every known state-action pair. These solutions are called Tabular Solution methods. These are subdivided into Dynamic Programming (DP), Monte Carlo methods, and Temporal-Diﬀerence (TD) learning. DP methods are applied on environments that can be perfectly modeled, and they iteratively improve policies until reaching a mathematically optimal one. Examples of this collection of algorithms include Policy Iteration and Value Iteration. Monte Carlo methods don’t require environment modeling, and are able to learn optimal policies from experience, by simulating diﬀerent scenarios. An example of such a strategy is the On-policy MC control algorithm. TD learning is a combination of the two previous ones, in the sense that it also learns from experience like Monte Carlo methods, and that experiences aﬀect other related estimates, like in DP. Examples of these algorithms include SARSA and Q-Learning. Tabular methods are very limited, when applied to real world problems. The ﬁrst limitation is the need to model the state using discrete variables. A tabular solution would consider velocities of 14.999 and 15 to be two completely unrelated states. Another limitation is the unfeasibility of storing large numbers of states. A chess board for instance, has 10 conﬁgurations, which is much more than we can currently store. These problems can be solved with Approximate solutions, that attempt to approximate towards an optimal policy, although convergence is not guaranteed in most cases. These solutions use supervised learning algorithms, and although other models could be used, there are several beneﬁts of using DL. There’s the natural correlation between the data from RL environments, the changes in the value functions that happen when the policy changes, and the nonstationarity of many problems. A very straightforward example of such models is a neuron with linear activation, in which inputs are the state features, and the output models the value of that particular state. The cost function is the mean squared error between the observed and predicted values. This algorithm is called Monte Carlo Gradient Descent. If we model the value of choosing each action a in a particular state s, known as Q(s, a), as an output neuron this algorithm is called DQN. However, adding more neurons and layers introduces some challenges like the lack of i.i.d. data, the value function changing after every update of weights, and the lack of exploration strategies. Several techniques have been developed by the Deep Mind team at Google to make DRL useful under MDP problems. We will now discuss a number of the most important ones. To simulate the i.i.d. property researchers have developed the Experience Replay Buﬀer (ERB), which stores a large amount of episode experiences prior to learning, and the agent samples randomly from it in small batches. We can use the loss (TD-error) from certain experiences to learn which among them moves the network the most along the gradient. We can use this information, known as priority, to assign these experiences a higher probability of being sampled. When combining it with Importance Sampling (to avoid bias towards high priority experiences) we have a Prioritized Experience Replay, developed in 2016 by Schaul et al. [13]. To handle the problem of the Q-value functions changing after every training, the concept of a target network was introduced, which is a duplicate of the original network, but that gets updated every k iterations. This means decisions are made using the target network, but the original one gets updated during learning. If we obtain the Q-value from the target network when learning we have Double Deep Q-Networks (DDQN), developed in 2016 by Hasselt et al. [14]. To handle the exploration problem, the -greedy approach with a decaying exploration probability is often used. A more recent approach, called Noisy Nets, is to introduce random noise in the hidden layers, developed in 2019 by Fortunato et al. [15]. A recent approach to improve the estimations of the Q-value has been to split it into two diﬀerent estimators: the value v(s) of being in a state s and the advantage of choosing an action a in this state A(a, s). This setup is called a Dueling Q-Network, developed in 2016 by Wang et al. [16] and an illustration of this concept can be found in ﬁg. 2.2. The way Q is calculated from A and v is by adding them, and subtracting the average advantage (to avoid the identiﬁability problem). One of the limitations of DQN is that the action space needs to be necessarily discrete, since we estimate the value of each action using a neuron. A diﬀerent approach is to model the output as a probability distribution of the expected return, estimated separately for each action. This obviously also works on discrete action spaces, by using softmax distributions. Modeling this way means we will want to maximize the value outputs, instead of minimizing the cost. This 2014 algorithm is a policy gradient version of REINFORCE by Silver et al. [17]. There are advantages and disadvantages of modeling this way. On one hand it is simpler, since there’s no need for exploration, replay buﬀers, or target networks. On the other hand, the correlations between states of the same episode may introduce bias, the high variance of the reward signals can cause slow convergence. Figure 2.2: Example architecture of a Dueling Q-Network. Notice how value and advantage are modeled separately. Only one neuron is used to estimate v(s), and to estimate A and Q we need as many neurons as there are actions available. The last layer is not a regular linear layer, but rather a calculation, as explained in eq. (3.1). Approximate Actor-Critic methods are a combination of the previous two approaches. The name comes from the fact that this method uses two distinct neural networks, an actor that decides what action to take at each step, and a critic that evaluates the action taken by estimating Q. Advantage Actor-Critic Networks (A2C) are an example of this approach, but modeling the advantage of the state-action pair separately. They were developed in 2016 by Mnih et al. [18]. In the RecSys literature, there are two main ways in which RL is used. The ﬁrst one is to create an automated way to decide which recommender system (out of a pool of options) should be used with which user. The second way is to use item features to deliver personalized recommendations to each user. In both cases, most of the literature focuses on tabular methods such as Contextual Bandits, the -greedy Algorithm, or the Upper Conﬁdence Bound algorithm (UCB). Research in solving the RS problem using RL is very limited, and there is a lot of opportunity in this ﬁeld [see 9, chap. 13.3]. The RL literature refers to the second usage as Personalized Web Services [see 10, chap. 16.7]. In such setting agents use feedback directly (or via user clicks) to iteratively improve a recommendation policy. This is often referred to as Associative Reinforcement Learning, and it has had success in improving clickout-rates of online sellers, and has been shown to have better results when modeled as a MDP. One of the potentials of using RL in this scenario is the long term eﬀect (also known as Life-Time Value or LTV) of a policy, having the potential to convince a user to purchase something through a sort of recommendation funnel. Researchers testing RL models on commercial solutions often use oﬀ-policy evaluation to avoid ﬁnancial risks. Since the application of RL in the context of Recommender System is fairly new, there are not many review papers available. Nevertheless, two important review articles are worth discussing. The ﬁrst article from 2019 reviews the most important research in using DL to build RS, by Zhang et al. [19], and brieﬂy discusses some research that used DRL. It explains how several tech companies use DL to enhance recommendation quality. Some examples of such systems are recommendations in the form of YouTube videos, Google Play apps, and Yahoo news articles. All of these having served billions of user sessions over the years. Naturally, the amount of academic research around DL and RS grows steadily. There are several beneﬁts of using DL. Firstly the ability to model problems nonlinearly, which is a property that enables these models to recognize more complex patterns. Secondly there’s representational learning, which allows large number of features, as well as less traditional ones like images, audio, or text, to be used in the learning process. Finally there’s the ability to model signals sequentially, which in some cases allow Recurrent Neural Networks (RNN) to represent some temporal dynamics. Despite these beneﬁts, DL is not without some setbacks. These models often make the interpretability of recommendations very diﬃcult, require large amounts of data to be trained, and extensive hyperparameter tuning. It is important to look at the latest advances in DL because there are many important concepts of it that could be successfully applied in the ﬁeld of DRL. The concept of autoencoders, for instance, could be used for dimensionality reduction in ﬁnding latent variables in RL state vectors. Attention mechanisms could be used to improve recommendation interpretability. Convolutional layers could be used to introduce images to environments. In spite of all this potential, DL by itself has no means of optimizing recommendations in the long term, nor it has the ability to perform well on nonstationary scenarios (user behavior changes over time), nor the ability to handle mutating product catalogues. All of these are often requirements in recommender systems, and they’re where RL becomes very relevant. The second review article worth mentioning is an overview of RL (speciﬁcally MDP) techniques applied to RS [20]. These studies validate the usage of partially observable environments, oﬀ-policy training, nonlinear functions, the -greedy algorithm, SARSA, value iteration, and Deep Q-Networks. The most important reviewed works are discussed in section 2.4.2. We’ll review chronologically some the most cited journal articles and conference papers, in the ﬁeld of applying Reinforcement Learning to the RS problem. Then, we look at some interesting RL environments, and details about the most recent ACM RS conference. We start by analyzing the 2007 work of Mahmood and Ricci [21]. They showed how Conversational Recommender Systems could be modeled as MDP. Their system was based in users searching travel products, and relied on them specifying constraints. In this case, the RL agent suggests improvements to the constraints when too many or too little recommendations were returned. In their model, there is a very limited number of states an agent can be in, and a limited number of actions it can take. They show how an agent could improve recommendation policies by using the Policy Iteration algorithm, and evaluate it using simulations. In 2009 [22] they validate their model in an online setting, by comparing it with a ﬁxed recommendation policy. They found that users that received recommendations from the RL agent reached their goal faster, more often, and adopted more query improvement suggestions. Next we analyze the research from Yahoo! Labs in recommending news articles. In 2010 Li et al. [23] modeled a news recommender system as a contextual bandit problem that maximizes user clicks on articles. They show that it’s possible to reliably train RL models oﬄine with tracking data, recorded by recommending articles randomly. They deploy the trained model online, and show that it performs better than a multi-armed bandit agent without user context. In 2011 Li et al. [24] addressed some problems of training agents oﬄine, speciﬁcally the modeling bias often present in user simulations. It was accomplished by introducing a replay methodology (analogous to an Experience Replay Buﬀer), and research shows how training oﬄine by replaying dataset experiences can produce comparable results to online training. Results were measured in terms of CTR. Two more theoretical works from Yahoo! Labs helped prove regret bounds for a few models. In 2010 Kale et al. [25] showed how a multi-armed bandit model could be adapted to provide a set of actions per step. They analyze scenarios in which the actions need to be ordered, and scenarios that don’t require ordering. They provide theoretical a bound for the regret metric of such agents. In 2011 Chu et al. [26] provided a theoretical analysis of a variant of LinUCB. They prove a high-probability bound for the regret of their agent. There is a discussion about how to deal with scenarios when these linear models are not guaranteed to accurately estimate the expected reward of recommendations. It may be the case that in real online systems often it’s not possible to perfectly obtain the expected reward, especially in non-stationary settings. This is exactly the problem that Bouneﬀouf et al. [27] address in 2012. They develop an -greedy model in which the exploration probably  depends on the user’s situation. The precise deﬁnition of situation is the similarity between a user’s feature vector, and the closest vector in its history of features. They evaluate their model oﬄine, using a dataset from a commercial system, and measuring CTR. They compare it to a standard -greedy agent, and one with decreasing exploration probability, and show that their model performs best. Now we review important work from Adobe Research in the area of ad recommendation. In 2015 Thomas et al. [28] studied eﬀective ways to evaluate recommendation policies oﬄine. They discuss the motivation, and the high risks of using a poor recommendation strategy in live systems. Their evaluation technique accurately estimates a lower conﬁdence bound on the expected return of a given policy. Their methodology is evaluated using real recorded Adobe Marketing Cloud data, and training a user simulation from it. It’s shown how the conﬁdence interval for predictions narrows down when the volume of training data increases, as would be expected. Also in 2015, Theocharous et al. [29] investigate the beneﬁts of training RL agents to increase long-term user interactions (known as Life-Time Value or LTV), as opposed to maximizing CTR. They also provide means of evaluating such models oﬄine, to enable hyperparameter optimization, and to guarantee safe deploys of these policies. They use a Fitted-Q Iteration RL agent, that in turn uses a combination of -greedy exploration and a Random Forest classiﬁer. Even though this model handles highly dimensional states, it does not solve the problem of a large action space. The work that Choi et al. [30] did in 2018 was really important in addressing highly dimensional data, both in the state space and in the action space. They introduce a biclustering step before training, that drastically reduces the amount of states and actions of RL agents. Two added beneﬁts are that clustering actions allows some explainability of recommendations, and clustering users allows some generalization of recommendations, which eﬀectively handles the Cold-Start Problem. They test Q-Learning and SARSA oﬄine, on the MovieLens movie recommendation datasets. It would have been interesting to see comparisons of other dimensionality reduction techniques, and non-tabular agents. Lastly, we look at recent works that used DRL. Zheng et al. [31] in 2018 used a dueling DDQN to create a news recommendation system. With their model, they wanted to improve agents that optimized for short-term goals, that relied solely on CTR, and that had low Diversity. They evaluate that their model achieves these goals both with oﬄine training, and online. These works give us an idea on how a utility matrix can be adapted, and neural networks can be used to predict its metrics. By modeling the network input (the state) as a combination of features (user, item, interaction, context), the output can be, for instance, the probability of this user clicking on this item at some point in the future (in other words, the expected click-out rate). Chen et al. [32] in 2018 address the problem of high variance and biased expectation of rewards, typical of recommender systems. To improve the variance they introduce a modiﬁcation to the ERB, by stratifying stable user features (like age or gender). This means sampling experiences will draw similar amounts of experiences from these strata. To improve the expectation bias, they propose the concept of Approximate Regretted Reward, in which the reward of an agent learning online is penalized by the reward of an agent that learned with previously available oﬄine data. They apply these concepts to a DDQN model, and test it on a live search engine. Their model had over 20 thousand possible actions (user suggestions) and the states were modeled after user features. It’s shown to perform better (in terms of CTR and conversion rate) than a system based on a Supervised Learning algorithm, on an A/B test. Zhao et al. published two important works in 2018. The ﬁrst one [33] is about a model that explicitly requires users to vote if a recommendation was useful or not. The agent learns directly from these reward signals. They tested it with data from an online e-commerce platform. The second one [34] addresses some problems of learning with real-time user feedback, and recommending pages of products. They develop a page-wise Actor-Critic RL agent that is able to maximize clicks on a 2D grid of product recommendations. The researchers train and test the model oﬄine, but also perform online evaluations. With the recent advances in Reinforcement Learning and ML in general, certain tools and frameworks start to become standards. One of the most inﬂuential RL benchmark tools is the 2016 OpenAI Gym by Brockman et al. [35]. We will therefore explore research that was done in designing gym environments that speciﬁcally simulate the dynamics of recommendation systems. We start by reviewing Google Research’s 2019 RecSim by Ie et al. [36]. It consists of a gym environment in which it’s possible to ﬂexibly deﬁne models for users, documents (items), and user-choice interactions. User models refer to the features of a user (e.g., demographic data, context), and its probability distributions. The same modeling (sampling features from a probability distribution) is done for documents. The probability of a user selecting a recommendation is also sampled from a distribution, and so are the transitions that may occur in a user’s state when it selects a document (e.g., a user’s engagement may increase or decrease). The agent’s task is to optimally recommend documents to users. RecSim comes with several already prepared environments. A diﬀerent approach called PyRecGym was researched in 2019 by Shi et al. [37]. Also a set of gym environments, this framework uses some traditional RS datasets (e.g., MovieLens, Yelp) to create simulations of recommender systems. The tool divides datasets into initialization data used to train user simulators, and interaction data used to teach the RL agent. User and item features are combined, and recommendation selections by the simulated user happen only if they occurred in the initialization dataset. This is a naive way of modeling human-computer interactions, and this environment would have beneﬁted from a probability distribution when simulating the interactions. Another gym environment that is worth reviewing is the 2020 MARSGym by Santana et al. [38]. This environment is designed speciﬁcally for simulating marketplace dynamics, in which recommendation systems not only need to optimize click-throughs, but also maintain healthy levels of fairness (recommendation diversity). It bears similarities with PyRecGym in the sense that it learns from tracking data, and that user-interactions simulations are exact replicas of what happened in the historical data. The researchers go into detail about the problems and beneﬁts of oﬄine training, and how counterfactual estimators can be used to reduce the bias. They demonstrate the eﬀectiveness of the environment using data from Trivago. To conclude this review, it’s important to investigate not only in academic and commercial production, but also in competitions. In 2020 Jannach et al. [39] did an extensive exploration of algorithms used in competitions, and share their ﬁndings and hypothesis. From the top 5 winning teams of the ACM RecSys Conference from 2017 to 2019, only one used DL. All the others used extensive feature engineering and more traditional Supervised Learning algorithms. A similar situation happens in other related competitions. The authors argue how competition datasets are more massive than the ones used in academic research, and how the time available to train models in competitions is often short. DL-based models often require more training time, more expensive hardware, and more extensive hyperparameter optimization. Another important aspect is that competition datasets tend to be much more sparse, which causes overﬁtting in many DL models. Some of the authors of this paper were part of the winning team of the 2020 RecSys challenge, and their methodology is described in Schiﬀerer et al. [40]. The greatest contributions of this paper are their eﬀorts in improving the performance of preprocessing, feature selection, and model training on huge datasets. They are able to run all these steps in the GPU, and speed up computations 280 times, reducing processing time from several hours to a couple minutes. The authors describe how they compared several deep learning models and other supervised ones, but XGBoost was the one that performed best. In the previous chapter we proposed to evaluate three modern gym environments found in the literature: PyRecGym [37], RecSim (the interest evolution environment) [36], and MARS-Gym (the Trivago Rio environment) [4]. Unfortunately, it proved impossible to ﬁnd an open-source implementation of PyRecGym, therefore we will replace it in this comparison with one of Google’s ML Fairness Gym [41] recommender environments. ML Fairness Gym is based oﬀ RecSim, and we’ll be discussing speciﬁcally the experimental source code based on the 1M MovieLens dataset [42]. When comparing these environments there are many desirable traits we are looking for. The usage of real-world data, for instance, makes the environment more relevant to realistic problems. The availability of large amounts of data makes it possible to properly train deep neural networks. The documentation of the environment is also important, to enable us to understand the features of the states. The time it takes for a time-step to be simulated is also paramount, because smaller execution times allow for further experimentation. In table 3.1 these traits are displayed in comparison. MARS-gym only has one possible episode because its deﬁnition of an episode is a sequential iteration over its ground-truth dataset (exactly like a supervised learning training routine). Meanwhile on the others, episodes represent a user’s journey. The state of an environment’s documentation speciﬁcally relates to the existence of explanations regarding the RL variables (i.e., state/observations, actions, reward, and the done ﬂag). We benchmarked the time taken to simulate a time-step, averaged over 50 episodes, and taken using the same hardware (2.4GHz x 16 Intel i9). Diﬀerent machines will of course output diﬀerent simulation times. Since none of the studied environments are suitable for this investigation, our decision will be to use the ML Fairness environment, but extending it to incorporate MovieLens user features such as sex, age, occupation, and zip code. Another beneﬁt of this environment is the extensive usage of the MovieLens dataset in RS literature, which allows for comparison with other works. There’s also the possibility of implementing environments using other recommendation datasets (i.e., a Trivago-based environment). As previously discussed, the RecSim framework (which our environment is based on) contains 4 key abstractions of recommender systems, that need to be conﬁgured: • A User Model that deﬁnes ways in which users are generated. This logic speciﬁes which • A Document Model that deﬁnes ways in which documents are generated. Like the user • A User-Choice Model that simulates a user’s response to a particular document (i.e., • A User Transition Model that determines how user features are aﬀected after each user- Table 3.1: Comparison of diﬀerent Recommender System environments. features are part of the user, and how they are generated (sampled from a distribution, or read from a dataset). In our case, this includes user features from the MovieLens data (i.e., sex, age, occupation, zip code). model, this logic speciﬁes which features describe an item. In our case it includes features from the MovieLens dataset, merged with violence scores from the Genome project [43] (i.e., title, genres, year, violence score). a ﬁve-star rating to a movie). Like the user model, this logic speciﬁes which features describe an item. This can be generated for example by sampling from a distribution, or by reading a matrix factorization of a user-interaction dataset. User-Choice models may also contain features like timestamps, user interactions, and so on. In our case this data comes from a matrix factorization of the MovieLens ratings. document interaction. For instance, a user’s interest in a certain retail category may Additionally, it allows the conﬁguration of recommendation slates. A slate is a subset of all the available documents that is presented to the user, and the RL agent is responsible for deciding which documents are included in the slate. This type of strategy is often seen in e-commerce websites under sections that display items similar to the one currently being seen by the user (e.g., “You may also like”). During the execution of the episodes, documents and users can be sampled randomly, with or without resampling, or divided into pools (i.e., users may be divided into train, test, and validation pools). Scores are simulated using a matrix decomposition strategy, from the utility matrix. Given the utility-matrix U of shape M × N, where element U provided by a user M user-movie recommendation, we can use a non-negative Single-Value Decomposition (SVD) algorithm that replaces missing ratings with the average before training. This is a traditional recommendation strategy that has gained a lot of attention since the 2006 Netﬂix challenge [44]. The technique consists of estimating matrices (W, H) such that W ∗ H ≈ U. We can then estimate a user’s rating of an item by the dot product of a row of W (user embeddings) and a column of H (document embeddings). The larger the number of components the more accurate the factorization, and since the ML-Fairness project uses 55 components, we will use the same amount. In RL terms, the action space of this environment comprises of the 3883 movies that can be recommended at any moment. The observation space, just like in RecSim environments, is composed of the document’s (movies) features, the user’s response to the last seen item, and the user’s features. The movies in this environment have 19 binary features, that represent the movie’s category (e.g., drama, adventure). The features of the response are a simulated 5-star rating, and a violence score (extracted from the Genome dataset). The reward signal is in the range (0, 1) and relates to the user’s simulated rating of the recommended movie. The reward is a multi-objective model, and represent a trade-oﬀ between interest and violence. A user’s journey (history) H where O of the latest user-document interaction at time-step i. A decrease after interacting with it. In our case, we’ve conﬁgured users to slowly get addicted to certain movie categories; this is simulated by slowly increasing user aﬃnities (their embeddings obtained by the matrix factorization). contains the features of available documents, current features of the user, and features slate of movies at time-step i, and R The embeddings used to generate the reward are not observable by the agent, as this is partly what the RL agent tries to estimate. As described in section 3.2 the RL models will be maximizing the expected return (i.e., the simulated user ratings). Several other studies measure diﬀerent metrics, that may for instance help assess the health of a recommendation ecosystem, such as diversity. Modeling a return that balances these goals, performing a counterfactual evaluation, is a ﬁeld under active research. Martin Mladenov (personal communication, May 5 2021), one of the authors of RecSim, explained there are hard methodological challenges involved in counterfactually accurate simulations. Other studies measure the eﬃciency of a RS using traditional metrics like NDCG from eq. (2.2), but depending on how relevance is modeled, there are no guarantees that the highest expected reward will output a high NDCG. Some researchers, instead of the expected reward, simplify the simulations using clickthrough-rates or hit-rates. Arguably, simulating reviews provides more accurate simulations, given the higher cardinality of simulated user evaluation. Below is a brief description of recent studies that use RL and the MovieLens dataset, and table 3.2 summarizes them. Afkhamizadeh et al. [45], measured their agent using cumulative custom regret metric, but it proved hard to understand how it’s calculated and draw comparisons. Their best model achieved a cumulative regret of approximately 30 after 100 time-steps. The 2021 study by Huang et al. [46], measured not the expected return, but percentage of recommendation slates that contained a movie the user saw (i.e., Hit-Rate, or HR@N where N is the slate size). Their best results are approximately 45% HR@10 for the MovieLens1M dataset, after training for an unknown number of steps. Sanz-Cruzado et al. [47] in 2019 showed on the same dataset a cumulative recall of approximately 0.6 after 3 million time-steps, for greedy, k-NN with k = 10, and kNN-based bandits. Another 2019 study by Zou et al. [48] found NDCG@3 (from equation 2.2) of 75% using REINFORCE on the same dataset after training on 90% of the dataset (around 900k time-steps). Another study from 2020 that uses the same style of measurement (but they call it CTR) by Yu et al. [49] reported 35% in 30000 time-steps, using a Trust Region Policy Optimization (TRPO) algorithm. They experimented with Q-Learning but found TRPO to wield better results. The 2020 study by Liu et al. [50] also used a similar metric, although they called it Precision, and tested several models. One of the best performing models was DQN, and they found 65% Precision@20 after training on 80% of the user interactions (around 800k time-steps). Very similar previous works by the same authors [51] [52], on the same dataset, and using the same DQN model, had found 54% Precision@20. Table 3.2: Comparison of recent studies that train RL agents using the MovieLens dataset. Note the diversity of metrics. The evaluation of the performance of our RL agents will be done using three distinct agents: a Dueling DQN implementation that uses PER buﬀers and noisy networks, a REINFORCE implementation using discounted episode rewards as a baseline, and an Actor-Critic implementation that uses a value estimator as the critic. The source code will be written in Python, with tools such as Jupyter notebooks, pandas, and Pytorch. The code is made publicly available on the source-control management platform Github reproducibility. The Dueling DQN neural network is fully connected with one input neuron for each observation variable (there are 25), hidden noisy layers with linear neurons and ReLU activation, a noisy value-estimator subnetwork with ReLU activation, a noisy advantage-estimator subnetwork with ReLU activation, and the noisy output layer with one neuron for each movie (currently there are 3883). The hidden layers are model parameters. The network uses the Adam optimizer, and the learning rate η is a parameter. The noisy layers use gaussian noise that helps with exploration, and the starting noise σ is a parameter. The discount factor γ used to calculate the TD-error is also a parameter. The PER is also parametrized with the buﬀer size, the burn-in, the batch size, the priority importance α, the weight eﬀect β, the β annealing coeﬃcient, and the minimum allowable priority . The frequencies used to train the main network, and to update the target network, are also parameters of the model. The pseudo-code is demonstrated in algorithm 1. Algorithm 1: Dueling DQN with Prioritized Experience Buﬀer and Noisy Layers Let Q Let Q Let PER be the prioritized experience buﬀer of size N Let P be the priorities of the experiences of PER; Let F Let F Let c be the step count; Let η be the learning rate; for each Time-Step t, S end The REINFORCE model does not use noisy layers, uses tanh activation, and softmax output neurons. The hidden layers and learning rate η are parameters of this model. The discount be the main network with weights θ and noisy layers with noise σ; priority importance α, weight eﬀect β, annealing β, and minimum priority ; Let ready ← (|P ER| > N); Let Abe argmax Q(S) if ready else a random valid action; Let R, Sbe the environment’s reward and next state for A; if ready then c ← c + 1; if c mod F= 0 then end if c mod F= 0 then end end Store (S, S, A, R) in PER with priority max(P ); factor γ used to calculate the baseline is also a parameter. The network is trained at the end of each episode. Predictions are made sampling from the predicted multinomial probability distribution, meaning the policy is stochastic. The Actor-Critic agent includes the same parameters from the REINFORCE agent, namely the learning rate η includes the critic’s hidden layers and learning rate η and it uses ReLU activation. Both networks are trained at the end of each episode. Regarding the loss calculations and subsequent backpropagation, the Dueling DQN agent uses the mean of the squared TD-errors of the batch, multiplied by their PER weights. For more detailed representations of the network architectures and gradient calculations, please refer to the Appendix A. As in DDQN agents, the TD-error is calculated using the target network to estimate the expected Q-value of the next state. The feedforward logic that combines the estimated value and advantage is detailed in eq. (3.1). The subtraction of the average advantage is needed due to the identiﬁability problem (i.e., decomposing Q into V and Adv). While the critic error is the TD-error, the REINFORCE and Actor loss functions are the application of the Policy Gradient Theorem, as expected, and are described in eq. (3.2). Due to the large number of parameters in each agent, performing an exhaustive hyperparameter space search would not be eﬃcient. To that eﬀect, following the methodology from [10], we will test one parameter at a time, freezing the remaining ones. Each combination is tested 3 times for 500 episodes, and the average of the ﬁnal moving mean (100 episode window) of the Return will be used for comparison. The best value found for each parameter is chosen in the ﬁnal model. Once models have the best parameters found, we run the comparison for 5 diﬀerent seeds and 2000 episodes. The moving mean will be used to compare the agents, along with its 95% conﬁdence interval. A random recommendation baseline is included for comparison. After the best agent is chosen, we run a training with slates of size 10 to compute the NDCG metric. This runs again for 5 seeds, and the agent is trained for 500 episodes (25K time-steps). The source-code used to run the tests was developed using the git source control technology, and stored online and made available publicly on the Github website. The needed changes made to the MLFairness repository were made in a fork, available as a submodule. The repository is licensed under the MIT license, which allows the code to be used privately and commercially, but without liabilities or warranties. The repository contains wiki pages with detailed instructions on installation and execution. Additionally, the repository contains jupyter notebooks with example usage of the environment. The source code was designed using the Object Oriented Programming (OOP) paradigm. Using inheritance to create component abstractions allowed for quicker development with less need for duplication. A class diagram containing the main classes, attributes, and functions is available in ﬁg. 3.1. The main base classes created encapsulate common aspects of environments, neural networks, RL agents, and the experience buﬀers. The ReinforcementLearning abstract class deﬁnes what functions agents are expected Figure 3.1: Class diagram covering the most important parts of the implementation. to have. Namely, to predict the next action to be taken on a given state, the next k best actions (for slate environments), and to store a particular experience (i.e., a state, action, reward, done, next observed state) tuple. The BaseNetwork base class implements helper functions like saving and loading from a ﬁle, freezing speciﬁc parameters, running backward propagation of a loss, plotting the gradient graph, conﬁguring the hardware device (i.e., CPU or GPU). The ExperienceBuffer interface expects implementations for experiences to be stored, sampled, and for the buﬀer to tell if there are enough experiences to start making predictions. Additionally, a LearningStatistics module was developed to help collect diﬀerent metrics that agents may output while training, providing ways to retrieve, plot, and aggregate them on many levels (i.e., model, episode, time-step, environment). Finally, the Manager module coordinates the sending and receiving of actions and states. Managers help with training agents, hyperparameter search, executing episodes, and printing overviews of environments. The project was developed using modern best-practices, such as automatic code formatting and linting (using python packages black and ﬂake8 ) that help with the readability of the code. The project also uses a dependency-management tool to download, version, and update the several libraries needed to run the project (using the poetry package), which makes the project usable with just a few commands. Unit tests and coverage measurement were implemented to help provide a stable and reliable project (using pytest and coverage). Finally, it is possible to generate HTML documentation of all the modules (using pdoc). All these tools are easy to use, and described in the project’s documentation. Using the already discussed environment, we can run a comparison of a random agent and a Dueling DQN agent (as shown in ﬁg. 2.2), to see how they fare in terms of rewards obtained. For the comparison, agents learned for 500 episodes, and each episode contained 50 recommendations to a randomly selected user. This test was executed 20 times for each agent, and ﬁg. 4.1 shows the average moving reward of the previous 100 iterations, along with a conﬁdence interval of 95% conﬁdence. It is possible to see, at the last episode’s reward average, that the DQN agent had users rating around 3.8/5. Meanwhile, the random agent had ratings around 3.2/5. Figure 4.1: Comparison of an agent that recommends movies randomly (orange) and an agent that learns from user reviews (blue). This is already a signiﬁcant diﬀerence in terms of user satisfaction, and the plot indicates that further learning iterations would only increase the diﬀerence. Also, no hyperparameter optimization was done in this model. This preliminary result approximates the 2018 ﬁndings of Choi et al. [30], whose models (Q-Leaning and SARSA) approximate return 8 out of 10 after 200 episodes. When comparing the results of the executions, it was found that taking only the means was not enough to make a decision on the optimal parameters. The standard deviation also provided valuable data to choose the best hyperparameters. To that end, the following plots contain the 95% conﬁdence interval of the results of the executions, along with the means. The complete execution results can be found in Appendix B. The results for the hyperparameter search with the REINFORCE model can be found in ﬁg. 4.2. We found that the policy estimator network with two hidden layers of 128 neurons worked best. Additionally, we found highest returns with a discount factor γ of 90% and a learning rate η of 0.001. Figure 4.2: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the REINFORCE agent. Notice how γ = 0.95 had the highest average, but also the highest variance. Therefore it would be wiser to choose 0.9 in this case. Since the policy estimator is similar to the Actor network in the Actor-Critic agent, we focused its tests on the critic parameters. Results can be found in ﬁg. 4.3. The best critic network had hidden layers of 128 and 64 neurons, a γ of 0.99, and η Figure 4.3: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the Actor-Critic agent. The results from the Dueling DQN parameter search can be found in ﬁg. 4.4. It was found that the best network update frequency was 3, the target network sync frequency was 300, the priority importance was 0.4, the priority weight growth (β-annealing) was 0.01, the buﬀer size was 10000, the buﬀer burn-in was 1000, the batch size was 32, the gaussian noise σ was 0.017, and the best architecture was with two hidden layers of 512 neurons, and 128 neurons for the advantage and value sub-networks. During the tests we used a γ of 0.95, and η of 0.001. Figure 4.4: Average ﬁnal moving reward out of 100 episodes, for diﬀerent parameters of the Dueling DQN agent. The results of the benchmark for all the agents can be found in ﬁg. 4.5. The Dueling DQN agent was trained for 3 seeds, while the others were trained for 5. Inspecting the plot allows to identify that the DQN agent clearly outperformed the others. Moreover, it seems that the Actor-Critic agent hasn’t managed to get results better than random. There may be some extra challenges in modeling the recommendation choices as probability distributions. Given that those agents have stochastic policies, it may be that small diﬀerences in the output nodes cause sub-optimal movies to be sampled too often. It may also be that it needed more training episodes, due to the high variance of the reward signal. A combination of more sophisticated Actor-Critic agents (e.g., Soft Actor-Critic, A2C) could potentially bring powerful improvements as well. Figure 4.5: Comparison of training of the agents with the best hyper-parameters found. The horizontal line at return 32, symbolizing the performance of a random agent, was added to help compare them. The area around the lines is the 95% conﬁdence interval. The Dueling DQN agent was trained for 3 seeds, while the others were trained for 5. In this scenario, the maximum possible reward is 50, and it would mean that the user rated 5/5 for every single recommended movie. An interesting factor to analyze in this sort of setup is that the agent starts oﬀ without any previous knowledge of the environment. In the RS literature this is referred to as the ColdStart Problem. The ability of an agent to deliver as little “bad” recommendations as possible is very valuable in commercial situations, when the stakes are higher. From this perspective, and reviewing the results from ﬁg. 4.5, the REINFORCE agent seems to make less errors in the short term. In the long term, however, it fails to deliver predictions as good as the ones from the DQN agent. Using the best model found (the DQN agent), we can execute simulations with slates (i.e., when an agent recommends multiple movies at each time-step). This allows us to calculate more traditional RS relevance metrics. Results can be found in table 4.1. Table 4.1: Comparison of relevance metrics for an agent making random recommendations and a Dueling DQN agent. It is important to note that the results from other researchers mentioned before had diﬀerent ways to model relevance. While in their case a relevant movie is a movie that was reviewed by a user, in our case relevance is the simulated rating (from 1 to 5). Additionally, when calculating the ideal relevance, we used ratings of 5 (i.e., an ideal slate is one in which the chosen recommended movie receives a 5/5 review). Since our agent maximizes the simulated rating (part of our deﬁnition of reward), it was expected that the agent would perform better than random. If we measured other interesting RS metrics, like the diversity of recommended movie genres, it is possible that we would not see great results. This sort of investigation, optimizing for many objectives, is beyond the scope of this work. A comprehensive search was performed to ﬁnd proper RL environments that simulate realistic problems of a Recommender System. Having found a most suitable one, improvements were made to ﬁx the problems it had. Agents using diﬀerent learning strategies were tested in this novel environment, and the results were comparable to those found by other recent studies. The code was made available and open-source at Github The usage of RL allowed for an eﬀective build of a hybrid RS approach, that combines features of model-based Collaborative Filtering, and Case-Based systems. It is shown how even though the learning models start with zero information about the items and users, some are able to provide meaningful recommendations after around 200 simulated user sessions, thus eﬀectively handling the Cold-Start Problem. Three diﬀerent RL models were implemented, and some hyperparameter search was conducted to conﬁgure them as well as possible. It was found that the model based on estimating Q-values using DL worked better consistently. Particularly, the models that had stochastic policies did not perform as well. Results were compared based on the ﬁnal episode reward sum average, to demonstrate that models eﬀectively outperform random recommendation strategies. The NDCG@10 metric was computed for the DQN-based model, to demonstrate that it eﬀectively returns relevant recommendation slates. One of the biggest challenges of researching this topic was ﬁnding suitable learning environments. Open-source environments were scarce, not actively maintained, only executable on speciﬁc Operational Systems (i.e., Linux), only executable on speciﬁc Python versions (i.e., 3.6), and poorly documented. This represents a technical barrier, that prevents the ﬁeld from advancing faster. A great example of learning environments that work out-of-the-box on most operational systems and python versions is Open AI Gym, and it should serve as an inspiration for more RS environments. Additionally, more heterogeneous and larger datasets could be included, such as the MovieLens1B dataset or the ones from the ACM conferences. A second important branch of investigation is on the simulation of users. In other words, given a ground-truth dataset of user-journeys, ﬁnding a strategy that optimally generalizes how users behave in those particular conditions. While matrix factorization strategies are fast, practical, and relatively accurate, their performance might not scale as well as neural-network based approaches. In particular, it seems that Recurrent Neural Networks would be a powerful candidate in this particular problem. A third interesting, and maybe the hardest, problem suitable for future research is ﬁnding appropriate multi-objective rewards, that simultaneously keep users satisﬁed with the recommendations, but without compromising the user, nor the items being recommended. For instance, an optimized food recommender system could recommend only chocolates to a user if health factors are not considered in the reward. Alternatively, an optimized clothing recommender system could recommend only pieces from the three major stores, eﬀectively killing the competition. Encoding counterfactual elements in the reward signal could prevent such problems. A fourth, very interesting path of research is regarding the Reinforcement Learning agents. More sophisticated Actor-Critic methods seem to have been gaining traction recently on other applications, such as games and robotics. In particular, it would be interesting to understand if Advantage Actor-Critic Networks or Soft-Actor-Critic (SAC) agents could outperform the Dueling DQN model, both with deterministic and stochastic policies. A more thorough comparison would also be very interesting, checking if tabular methods would be feasible, and if their performance would be comparable. Lastly, there is a lot of room for research regarding the explainability of recommendations. Particularly in RS environments, the power of being able to explain why certain recommendations are being made is invaluable. While explainability in supervised learning problems has been explored extensively, there is still a gap in the Deep Reinforcement Learning application ﬁeld. Speciﬁcally, users could be interested in what is the expected relevance of a particular document, what factors were relevant in estimating it, and how each factor weighs in. [1] Otis Gospodnetic, Erik Hatcher, and Michael McCandless. Lucene in Action. Manning [2] Kim Falk. Practical Recommender Systems. Manning Publications, ﬁrst edition, January [3] Maurits Kaptein. Persuasion Proﬁling. Business Contact Publishers, 2015. [4] Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo, Bruno [5] RecSys Challenge ’19: Proceedings of the Workshop on ACM Recommender Systems [6] Mouzhi Ge, Carla Delgado, and Dietmar Jannach. Beyond accuracy: Evaluating recom- [7] Pete Chapman, Julian Clinton, Randy Kerber, Thomas Khabaza, Thomas Reinartz, Colin [8] Heather Silyn-Roberts. Writing For Science And Engineering. Elsevier, 2000. ISBN 978-0- [9] Charu C. Aggarwal. Recommender Systems. Springer International Publishing, Cham, Publications, 2nd edition, 2010. 2019. ISBN 978-1-61729-270-5. URL https://learning.oreilly.com/library/view/ practical-recommender-systems/9781617292705/. Accessed on 2021-02-27. Brand˜ao, Anderson Soares, Renan M. Oliveira, and Sandor Caetano. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces. arXiv:2010.07035 [cs, stat], September 2020. Challenge, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450376679. mender systems by coverage and serendipity. In Proceedings of the Fourth ACM Conference on Recommender Systems, pages 257–260, January 2010. doi: 10.1145/1864708.1864761. Shearer, and Rudiger Wirth. Crisp-dm 1.0 step-by-step data mining guide. Technical report, The CRISP-DM consortium, August 2000. 7506-4636-9. URL https://doi.org/10.1016/B978-0-7506-4636-9.X5000-9. Accessed on 2021-03-13. 2016. ISBN 978-3-319-29657-9 978-3-319-29659-3. doi: 10.1007/978-3-319-29659-3. URL http://link.springer.com/10.1007/978-3-319-29659-3. Accessed on 2021-03-13. [10] Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adap- [11] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. [12] Jure Leskovec, Anand Rajaraman, and Jeﬀ Ullman. Mining Of Massive Datasets. Cam- [13] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized Experience [14] Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with [15] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, [16] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Fre- [17] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried- [18] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillitive computation and machine learning. MIT Press, Cambridge, Mass, 2 edition, 2018. ISBN 978-0-262-19398-6. URL http://www.deeplearningbook.org. Accessed on 2021-03-03. bridge University Press, 3 edition, 2019. URL http://www.mmds.org. Accessed on 202103-03. Replay. arXiv:1511.05952 [cs], February 2016. URL http://arxiv.org/abs/1511.05952. arXiv: 1511.05952. Accessed on 2021-03-18. Double Q-Learning. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 30(1), March 2016. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/AAAI/article/ view/10295. Number: 1. Accessed on 2021-03-18. Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy Networks for Exploration. arXiv:1706.10295 [cs, stat], July 2019. URL http://arxiv.org/abs/1706.10295. arXiv: 1706.10295. Accessed on 2021-03-18. itas. Dueling Network Architectures for Deep Reinforcement Learning. In International Conference on Machine Learning, pages 1995–2003. PMLR, June 2016. URL http: //proceedings.mlr.press/v48/wangf16.html. ISSN: 1938-7228. Accessed on 2021-0318. miller. Deterministic Policy Gradient Algorithms. In International Conference on Machine Learning, pages 387–395. PMLR, January 2014. URL http://proceedings.mlr.press/ v32/silver14.html. ISSN: 1938-7228. Accessed on 2021-03-18. crap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous Methods for Deep Reinforcement Learning. In International Conference on Machine Learning, pages 1928– 1937. PMLR, June 2016. URL http://proceedings.mlr.press/v48/mniha16.html. ISSN: 1938-7228. Accessed on 2021-03-18. [19] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. Deep Learning Based Recommender Sys- [20] G. Gupta and R. Katarya. A Study of Recommender Systems Using Markov Decision [21] Tariq Mahmood and Francesco Ricci. Learning and adaptivity in interactive recommender [22] Tariq Mahmood and Francesco Ricci. Improving recommender systems with adaptive [23] Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach [24] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased oﬄine evaluation of [25] Satyen Kale, Lev Reyzin, and Robert E. Schapire. Non-stochastic bandit slate problems. tem: A Survey and New Perspectives. ACM Computing Surveys, 52(1):5:1–5:38, February 2019. ISSN 0360-0300. doi: 10.1145/3285029. URL https://doi.org/10.1145/3285029. Accessed on 2021-03-15. Process. In 2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS), pages 1279–1283, June 2018. doi: 10.1109/ICCONS.2018.8663161. systems. In Proceedings of the ninth international conference on Electronic commerce, ICEC ’07, pages 75–84, New York, NY, USA, August 2007. Association for Computing Machinery. ISBN 978-1-59593-700-1. doi: 10.1145/1282100.1282114. URL https://doi. org/10.1145/1282100.1282114. Accessed on 2021-03-13. conversational strategies. In Proceedings of the 20th ACM conference on Hypertext and hypermedia, HT ’09, pages 73–82, New York, NY, USA, June 2009. Association for Computing Machinery. ISBN 978-1-60558-486-7. doi: 10.1145/1557914.1557930. URL https://doi.org/10.1145/1557914.1557930. Accessed on 2021-03-14. to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, WWW ’10, pages 661–670, New York, NY, USA, April 2010. Association for Computing Machinery. ISBN 978-1-60558-799-8. doi: 10.1145/ 1772690.1772758. URL https://doi.org/10.1145/1772690.1772758. Accessed on 202103-13. contextual-bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM ’11, pages 297–306, New York, NY, USA, February 2011. Association for Computing Machinery. ISBN 978-1-4503-0493-1. doi: 10.1145/1935826.1935878. URL https://doi.org/10. 1145/1935826.1935878. Accessed on 2021-03-14. In Proceedings of the 23rd International Conference on Neural Information Processing Systems - Volume 1, NIPS’10, pages 1054–1062, Red Hook, NY, USA, December 2010. Curran Associates Inc. [26] Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual Bandits with Linear [27] Djallel Bouneﬀouf, Amel Bouzeghoub, and Alda Lopes Gan¸carski. A Contextual-Bandit [28] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-Conﬁdence [29] Georgios Theocharous, Philip S. Thomas, and Mohammad Ghavamzadeh. Ad Recommen- [30] Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha, and Sungroh [31] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, [32] Shi-Yong Chen, Yang Yu, Qing Da, Jun Tan, Hai-Kuan Huang, and Hai-Hong Tang. Payoﬀ Functions. In Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and Statistics, pages 208–214. JMLR Workshop and Conference Proceedings, June 2011. URL http://proceedings.mlr.press/v15/chu11a.html. ISSN: 1938-7228. Accessed on 2021-03-14. Algorithm for Mobile Context-Aware Recommender System. In Tingwen Huang, Zhigang Zeng, Chuandong Li, and Chi Sing Leung, editors, Neural Information Processing, Lecture Notes in Computer Science, pages 324–331, Berlin, Heidelberg, 2012. Springer. ISBN 9783-642-34487-9. doi: 10.1007/978-3-642-34487-9 40. Oﬀ-Policy Evaluation. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 29 (1), February 2015. ISSN 2374-3468. URL https://ojs.aaai.org/index.php/AAAI/ article/view/9541. Number: 1. Accessed on 2021-03-13. dation Systems for Life-Time Value Optimization. In Proceedings of the 24th International Conference on World Wide Web, WWW ’15 Companion, pages 1305–1310, New York, NY, USA, May 2015. Association for Computing Machinery. ISBN 978-1-4503-3473-0. doi: 10.1145/2740908.2741998. URL https://doi.org/10.1145/2740908.2741998. Accessed on 2021-03-13. Yoon. Reinforcement Learning based Recommender System using Biclustering Technique. January 2018. URL https://arxiv.org/abs/1801.05532v1. Accessed on 2021-03-16. and Zhenhui Li. DRN: A Deep Reinforcement Learning Framework for News Recommendation. In Proceedings of the 2018 World Wide Web Conference, WWW ’18, pages 167–176, Republic and Canton of Geneva, CHE, April 2018. International World Wide Web Conferences Steering Committee. ISBN 978-1-4503-5639-8. doi: 10.1145/3178876.3185994. URL https://doi.org/10.1145/3178876.3185994. Accessed on 2021-03-15. Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pages 1187–1196, New York, NY, [33] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. Rec- [34] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep [35] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie [36] Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui [37] Bichen Shi, Makbule Gulcin Ozsoy, Neil Hurley, Barry Smyth, Elias Z. Tragos, James [38] Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo, Bruno USA, July 2018. Association for Computing Machinery. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3220122. URL https://doi.org/10.1145/3219819.3220122. Accessed on 2021-03-15. ommendations with Negative Feedback via Pairwise Deep Reinforcement Learning. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD ’18, pages 1040–1048, New York, NY, USA, July 2018. Association for Computing Machinery. ISBN 978-1-4503-5552-0. doi: 10.1145/3219819.3219886. URL https://doi.org/10.1145/3219819.3219886. Accessed on 2021-03-15. reinforcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference on Recommender Systems, RecSys ’18, pages 95–103, New York, NY, USA, September 2018. Association for Computing Machinery. ISBN 978-1-4503-5901-6. doi: 10.1145/3240323.3240374. URL https://doi.org/10.1145/3240323.3240374. Accessed on 2021-03-09. Tang, and Wojciech Zaremba. Openai gym, 2016. Wu, and Craig Boutilier. RecSim: A Conﬁgurable Simulation Platform for Recommender Systems. arXiv:1909.04847 [cs, stat], September 2019. URL http://arxiv.org/abs/ 1909.04847. arXiv: 1909.04847. Accessed on 2021-03-17. Geraci, and Aonghus Lawlor. PyRecGym: a reinforcement learning gym for recommender systems. In Proceedings of the 13th ACM Conference on Recommender Systems, RecSys ’19, pages 491–495, New York, NY, USA, September 2019. Association for Computing Machinery. ISBN 978-1-4503-6243-6. doi: 10.1145/3298689.3346981. URL https://doi. org/10.1145/3298689.3346981. Accessed on 2021-03-17. Brand˜ao, Anderson Soares, Renan M. Oliveira, and Sandor Caetano. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces. arXiv:2010.07035 [cs, stat], September 2020. URL http://arxiv.org/abs/2010.07035. arXiv: 2010.07035. Accessed on 2021-03-17. [39] Dietmar Jannach, Gabriel de Souza P. Moreira, and Even Oldridge. Why Are Deep [40] Benedikt Schiﬀerer, Gilberto Titericz, Chris Deotte, Christof Henkel, Kazuki Onodera, [41] Alexander D’Amour, Hansa Srinivasan, James Atwood, Pallavi Baljekar, D. Sculley, and [42] F. Maxwell Harper and Joseph A. Konstan. The MovieLens Datasets: History and Con- [43] Jesse Vig, Shilad Sen, and John Riedl. The Tag Genome: Encoding Community Knowledge [44] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix Factorization Techniques for [45] Mostafa Afkhamizadeh, Alexei Avakov, and Reza Takapoui. Automated Recommendation Learning Models Not Consistently Winning Recommender Systems Competitions Yet? A Position Paper. In Proceedings of the Recommender Systems Challenge 2020, RecSysChallenge ’20, pages 44–49, New York, NY, USA, September 2020. Association for Computing Machinery. ISBN 978-1-4503-8835-1. doi: 10.1145/3415959.3416001. URL https://doi.org/10.1145/3415959.3416001. Accessed on 2021-03-17. Jiwei Liu, Bojan Tunguz, Even Oldridge, Gabriel De Souza Pereira Moreira, and Ahlenge ’20, pages 16–23, New York, NY, USA, September 2020. Association for Comhttps://doi.org/10.1145/3415959.3415996. Accessed on 2021-03-09. Yoni Halpern. Fairness is not static: Deeper understanding of long term fairness via simulation studies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAccT ’20, page 525–534, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369367. doi: 10.1145/3351095.3372878. URL https://doi.org/10.1145/3351095.3372878. Accessed on 2021-03-18. text. ACM Transactions on Interactive Intelligent Systems, 5(4):19:1–19:19, December 2015. ISSN 2160-6455. doi: 10.1145/2827872. URL https://doi.org/10.1145/2827872. Accessed on 2021-05-05. to Support Novel Interaction. ACM Transactions on Interactive Intelligent Systems, 2 (3):13:1–13:44, September 2012. ISSN 2160-6455. doi: 10.1145/2362394.2362395. URL https://doi.org/10.1145/2362394.2362395. Accessed on 2021-05-05. Recommender Systems. Computer, 42(8):30–37, August 2009. ISSN 1558-0814. doi: 10.1109/MC.2009.263. Conference Name: Computer. Systems. page 5, 2017. [46] Liwei Huang, Mingsheng Fu, Fan Li, Hong Qu, Yangjun Liu, and Wenyu Chen. A deep [47] Javier Sanz-Cruzado, Pablo Castells, and Esther L´opez. A simple multi-armed nearest- [48] Lixin Zou, Long Xia, Zhuoye Ding, Dawei Yin, Jiaxing Song, and Weidong Liu. Rein- [49] Yang Yu, Zhenhao Gu, Rong Tao, Jingtian Ge, and Kenglun Chang. Interactive Search [50] Feng Liu, Ruiming Tang, Huifeng Guo, Xutao Li, Yunming Ye, and Xiuqiang He. Top- [51] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng [52] Feng Liu, Huifeng Guo, Xutao Li, Ruiming Tang, Yunming Ye, and Xiuqiang He. End-toreinforcement learning based long-term recommender system. Knowledge-Based Systems, 213:106706, February 2021. ISSN 09507051. doi: 10.1016/j.knosys.2020.106706. URL https://linkinghub.elsevier.com/retrieve/pii/S0950705120308352. Accessed on 2021-03-09. neighbor bandit for interactive recommendation. In Proceedings of the 13th ACM Conference on Recommender Systems, pages 358–362, Copenhagen Denmark, September 2019. ACM. ISBN 978-1-4503-6243-6. doi: 10.1145/3298689.3347040. URL https: //dl.acm.org/doi/10.1145/3298689.3347040. Accessed on 2021-03-09. forcement Learning to Diversify Top-N Recommendation. In Guoliang Li, Jun Yang, Joao Gama, Juggapong Natwichai, and Yongxin Tong, editors, Database Systems for Advanced Applications, Lecture Notes in Computer Science, pages 104–120, Cham, 2019. Springer International Publishing. ISBN 978-3-030-18579-4. doi: 10.1007/978-3-030-18579-4 7. Based on Deep Reinforcement Learning. arXiv:2012.06052 [cs], December 2020. URL http://arxiv.org/abs/2012.06052. arXiv: 2012.06052. Accessed on 2021-02-22. aware reinforcement learning based recommendation. Neurocomputing, 417:255–269, December 2020. ISSN 0925-2312. doi: 10.1016/j.neucom.2020.07.057. URL https://www. sciencedirect.com/science/article/pii/S0925231220311656. Accessed on 2021-0309. Guo, Yuzhou Zhang, and Xiuqiang He. State representation modeling for deep reinforcement learning based recommendation. Knowledge-Based Systems, 205:106170, October 2020. ISSN 0950-7051. doi: 10.1016/j.knosys.2020.106170. URL https://www. sciencedirect.com/science/article/pii/S095070512030407X. Accessed on 2021-0309. End Deep Reinforcement Learning based Recommendation with Supervised Embedding. In Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM ’20, pages 384–392, New York, NY, USA, January 2020. Association for Computing Machinery. ISBN 978-1-4503-6822-3. doi: 10.1145/3336191.3371858. URL https://doi. org/10.1145/3336191.3371858. Accessed on 2021-03-09. ActorCritic critic hidden layers [256 128] 26.9030 20.4505 24.0356 Table B.1: Actor-Critic and REINFORCE hyperparameter search results.