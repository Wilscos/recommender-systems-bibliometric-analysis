The ever-prospering of Internet technologies has gradually shifted how people receive information. Online news applications have rapidly replaced traditional printed media, which collect content from multiple publishers and receive a considerable volume of news articles. Despite many advantages, these applications usually have to face the information overloading problem, which motivates the development of news recommendation systems. In the ﬁeld of news recommendation, a key observation is that people’s news-reading behaviors are not independent [7]. Previously interacted news has a substantial impact on the following reading choice. Along this line, a number of news recommendation models have been built [15], [16], [27], which capture people’s sequential reading patterns. The contents interacted previously usually have different impacts on choosing the next one to recommend in practice. Speciﬁcally, news articles are composed of several sentences, and different sentences of people’s previously interacted news have other impacts on their subsequent actions. Second, news articles have basic components called news elements, which are known as ﬁve W and one H (5W1H), i.e., who, when, where, what, why and how [18]. The 5W1H elements clearly describe the critical information of news explicitly. The six elements are the basic principle of news writing, generally followed by the world press. For example, for a news article that a user read before, the ﬁrst 4W elements are “Warriors, Cavaliers”, “June 1-9, 2018”, “Cleveland, Oakland” and “NBA ﬁnals”, respectively. Third, think of each news as a whole, news articles in people’s reading logs have different inﬂuences on their decisions about candidate news, i.e., to read or not. Continue the above example, and the user will read sports news primarily because of the NBA news rather than economic news in his reading logs. Besides, the current period and the time difference from the last clicked news signiﬁcantly impact the following news. For example, users may browse news about the stock market at around 10 am on weekdays, while constellations news at midnight. Finally, treating users’ news-reading behaviors as a sequence, since the order of this sequence or the interactions between the sequence hides much information. Therefore, given such various information of news reading sequences, it is desired to use them from sentence-, element, document- and sequence-level. Also, it is necessary to incorporate dynamism in news recommendations. Motivated by the above observations, in this paper, we propose D-HAN, a Dynamic news recommendation model based on Hierarchical Attention Network. The main building block of our multi-granularity model is a twolayer attention network. Speciﬁcally, the lower layer distinguishes sentence-level and element-level impacts. It automatically determines the attention weights between sentences and elements. The upper layer discriminates the various correlations in document-level. It determines the attention weights between history-candidate news pairs. To comprehensively model the dynamic characters, we further incorporate news clicked timestamps and relative time intervals in the document-level layer. The history summarization layer consists of several Transformer encoders [31] for efﬁciently learning sequence-level information from users’ history news-reading. The upper left dynamic negative sampling layer selects negative samples according to the output of document-level attention layer, and D-HAN model parameters dominate this process. The upper right prediction layer consists of a fully-connected network and takes the negative and positive samples as input. Contributions. In summary, the contributions can be concluded as follows: (1) We propose to simultaneously capture different granular information, i.e., sentence-, element-, document- and sequence-level information for news recommendation. (2) We propose recommending news dynamically by a time-aware document-level attention layer, which incorporates the absolute and relative time information. (3) We propose incorporating negative sampling into the training process to optimize the model. This study extends our earlier work [39] as follows. (1) On sentence- and element-level attention layers, we replace both the original additive attention and the cosine similarity with the scaled dot-product attention [31]. We replace the original additive attention and convolutional neural network (CNN) with a Transformer encoder on the document-level attention and history summarization layers. (2) We design a novel dynamic negative sampling layer such that the selected negative samples are more informative. (3) We replace the hard time-decaying factor with the absolute and relative time embedding on the time utilization method, which models the dynamic characteristics more comprehensively. (4) On the experiments, we compare the performance of each attention layer and add more experiments on the performance of the history summarization layer, the relative and absolute time information, and dynamic negative sampling. News recommendation has previously attracted much attention, aiming to provide personalized news articles for users. Traditional news recommendation methods can be divided into content-based, collaborative ﬁltering, and hybrid. Content-based methods recommend news solely based on content similarity [14], [24]. Collaborative ﬁltering methods utilize users’ feedback to news articles to make recommendation [5], suffering from serious cold-start problems. And hybrid methods combine the two strategies to achieve better recommendation performance [21], [23]. Recently, the neural recommendation has shown its superior performance. GRU4Rec [13] and its variants [19] and GRU4Rec++ [12] apply RNN to session-based recommendation. Caser uses horizontal and vertical convolution ﬁlters to capture sequential patterns [30]. Based on Caser, [37] further models the long-range dependence in the sequence. [40] proposes a reinforcement learning framework, aiming at online news recommendation. Attention mechanism has shown effective results in machine translation [2], image captioning [36] and so on. It has also shown surprising potential in the ﬁeld of recommendation. [32] enhances news recommendation with knowledge graphs, applying an attention network to get users’ representations. [22] designs a deep fusion model which leverages various levels of interaction by inception module and merges information from different channels by attention mechanism. Deep Interest Network [41] designs an attention unit for learning the representations of users adaptively. [35] improves factorization machines by discriminating the importance of feature interactions via an attention network. Attention Collaborative Filtering model [4] introduces an attention mechanism into collaborative ﬁltering to model item- and component-level implicit feedback in the multimedia recommendation. With the great success of Bert [6] in other NLP tasks, self-attention mechanism or Transformer architecture has gradually been used for news recommendations. [8] applies a Transformer architecture with multi-head self-attention to obtain news content representation from news titles and topics. BERT4Rec [29] concatenates all history news articles into a document as the input of Transformer encoder. NRMS [34] proposes a news encoder and a user encoder, where multi-head self-attention is used to learn representations. Dynamic recommendation considers the time-dependent effect in recommendation, which can help us to understand users’ behaviors. For instance, [38] observes that people tend to visit different locations at different time in a day and utilize the absolute time for location recommendation. With time gates, Time-LSTM [42] can not only model sequential information, but also capture well the time interval information. To model the dynamics of sequence recommendation, TiSASRec [20] replaces the position encoding of Transformer with the absolute position of items and the time intervals between any two items in a news sequence. Assume there is an online news platform offering news services to users. Once the platform receives a new piece of news, it estimates the click rate for each user based on the news articles the user had read earlier. Formally, let C= [c, c, ..., c] and A= [a, a, ..., a] denote the sequence of the most recent L pieces of news read by user i and the corresponding click time, where L is the number of news articles that we consider to estimate the click rate. Each piece of news cconsists of a sequence of sentences, i.e., [s, s, ..., s], where sis the k-th sentence in c and K is the maximum number of sentences we consider. Each piece of news cis also represented by a set of news elements, detailed in the following subsection. Given the news sequence Cand candidate news c, we aim to predict the click rate of cby user i. Extracting 5W1H elements is an intractable natural language processing (NLP) problem and only few works extract them from news contents [10], [33]. However, we can not employ them due to the language difference for [10] and the lack of speciﬁc news corpus for [33]. This part of NLP is not the focus of our model, so we deﬁne news elements that can be easily extracted by the named entity recognition and keywords extraction modules of NLP tools. Speciﬁcally, they are person, organization, time, location and keywords, corresponding to who, who, when, where and what elements of 5W1H, respectively. With news elements we deﬁne, each news can be summed up into a sentence, i.e., persons or organizations do something at a speciﬁc time and location. Formally, each piece of news cis represented by a set of elements, i.e., {e, e, e, e, e}, corresponding to the above elements we deﬁne, respectively. Figure 1 illustrates the architecture of our proposed DHAN model. It has four main components: the core hierarchical attention layer, i.e., sentence-level, element-level, and time-aware document-level attention layer, the history summarization layer, the dynamic negative sampling layer, and the prediction layer. Our multi-granularity model takes news sequence Cof user i, candidate news cand their corresponding click time A, Aas input. Sentence-level attention layer ﬁrst computes weights of interactions between sentences of cand cand gets the content vector v(c) of news cand v(c) of news c. Second, element-level attention layer computes weights of interactions between elements of cand cand gets the element vector l(c) and l(c). Then time-aware document-level attention layer takes content vector, element vector, user structural embedding u and click time embedding as input, and obtains candidatedependent representation. Next, these candidate-dependent representations are fed into dynamic negative sampling and history summarization layers simultaneously, based on which we obtain the negative samples S and sequence vector pfor user i, respectively. Finally, the prediction layer inputs the output of the other components to compute the click rate of cby user i. When predicting candidate news c, sentences of news c unequally affect user i’s choice, i.e., to read or not. Intuitively, sentences content-relevant to news chave more signiﬁcant impacts on reading. The sentence-level attention layer aims to discriminate various inﬂuences of sentences. We ﬁrst need content vectors of sentences of news c and content vector of candidate news c. There exist many sentence embedding methods. We adopt Paragraph Vector [17] due to its consideration of the ordering and semantics of the words in sentences. The content vector v(s) of sentence sis embedded in a d-dimensional space R, and the content vector v(c) ∈ Rof news cis stacked by v(s), and the content vector v(c) ∈ Rof candidate news cis calculated by averaging the sentences vectors. To model the interactions among news cand candidate news c, we concatenate u, v(c) and v(c) along the ﬁrst dimension as [uv(c)v(c)] ∈ R, and adopt scaled dot-product attention [31] to learn its representation. The self attention weights among cand care: where W∈ R, W∈ Rare the parameters of√ model, · denotes the dot product operation,d is a scale factor to prevent dot products growing large in magnitude, b∈ R. Note that we omit the bias for brevity here. These self attention weights are further normalized by softmax function β=. Here attention weight βcan be interpreted as the content relevance among sentence sand candidate news c, user embedding u. With these weights, we compute the content vector v(c) of news cwith respect to candidate news c as the dot product between βand linearly transformed [uv(c) v(c)] by model parameter W∈ R, that is, v(c) = β· [uv(c)v(c)]W∈ R. 4.2 Element-level Attention Layer Given a pair of news cand candidate news c, different elements play a different role in users’ decisions, and the goal of the element-level attention layer is to discriminate various impacts of other elements. With the named entity recognition and keywords extraction modules of NLP tools, we can extract elements we deﬁne, i.e., person, organization, time, location and keywords, for news cand candidate news c. Each element is extracted in the form of one or more words. In this paper, each word is embedded in a d-dimensional space Rby Word2vec [25] which is successful in capturing semantics relatedness. For instance, let lc= {le, le, le, le, le} ∈ R, and le∈ Ris the vector of element p of news c, which is obtained by averaging the vectors of words that represent e. We concatenate the corresponding element vector of news cand calong d-dimension as [lclc] ∈ R, and then to smooth their internal differences, we apply three onelayer feed-forward neural networks to transform them into d-dimension space: where W∈ R, W∈ Rand W∈ Rare model parameters, q, kand vrepresent different linear transformations of [lclc], which will be used to compute self-attended vector representation. To model the internal and external interactions among element vector of cand c, we take dot product between qand kto get the raw self-attention scores: whered is a scale factor, and the raw self-attention scores are normalized by softmax function to probabilities followed by a dropout operation on entire elements to attend to. Here self-attention scores γcan be interpreted as the relevance of elements under the inﬂuence of both historical news cand candidate news c. With these weights, we compute the element vector l(c) of news cas dot product between γand v: l(c) = γ· v∈ R. Given news-reading sequence Cof user i, news articles unequally inﬂuence whether he reads candidate news cor not, and different periods are also crucial for making decisions. Two pieces of news frequently co-clicked by people tend to be similar. To preserve the structural information, we learn a news id embedding in a d-dimensional space for each news according to its id, i.e., n∈ Rfor news c. News id embeddings are randomly initialized and automatically learned in the training phase. Similarly, two users with similar history news sequences tend to be similar. Users’ structural information partly reﬂects user preferences [4]. Therefore, we learn an embedding in a d-dimensional space for each user, i.e., u∈ Rfor user i. User embeddings are also randomly initialized and updated during training. For news c, we concatenate its sentence, element and structural embedding along this d-dimension to obtain representation x= [ v(c) l(c) n] ∈ R. For candidate news c, this operation leads to x= [ v(c) l(c) n] ∈ R. An observation is that users tend to read different news at different periods with a distinct time difference from the last clicked time. To model this dynamics of news recommendation, we propose to model news content information and timestamp information simultaneously. Speciﬁcally, we adopt the year, month, week, day, hour, minutes as the absolute time and the absolute time difference between news cand candidate news cas the relative time interval. To preserve the time information, we learn d-dimensional absolute time embedding and time interval embedding for news sequence C, i.e., a∈ Rfor absolute time and r∈ Rfor relative time interval. • Embed with r. Given a news sequence C, its representation xis concatenated with its corresponding relative time interval embedding rand candidate news representation xas z= [xrx] ∈ R, where x= {x, x, ..., x} ∈ R. The time-aware representation of candidate news is z= x. • Embed with a. History news representation and candidate news representation are concatenated with their corresponding absolute time embedding as z= [xa] ∈ R, z= [xa] ∈ R, respectively. • Embed with both rand a. First, to use the absolute timestamp and relative time interval, we concatenate x, aand ras z= [xar] ∈ R, Then, to embed time embedding into candidate news representation, we concatenate the representation xof candidate news c and the representation aof absolute time of candidate news cas z= [xa] ∈ R. Given the time-aware candidate news representation z and the time-aware news sequence representation z, concatenate them together and then perform a linear transformation to transform them into d-dimensional space. Specifically, take news cfor example, the time-aware representation zof news cis concatenated with zand user embedding uand then linearly transformed as t= [zzu]W∈ R, where W∈ R. With this operation, the document-level attention layer can consider content relevance, user preferences, and time dynamics simultaneously. To determine the representation of news cconcerning the candidate news cand user i, we borrow the idea of Transformer, considering its strong capability in modeling the correlations between the events in a sequence of user behaviors. The original Transformer is designed for the NLP applications, where the sequential signals are captured by the word indexes. However, continuous time information can be essential in user behavior modeling, as mentioned before. Thus, we revise the traditional Transformer by replacing the position encoding with a continuous time embedding. The variables are ﬁrstly input into the self-attention layer, and then a position-wise feed-forward layer is leveraged to process the output. At last, we use a fully connected layer and a residual connection layer to predict the ﬁnal results: where φ is the activation function, W∈ R, W∈ Rare model parameters, and dis the intermediate size of position-wise feed-forward layer. ‘Dropout’ is an approach used for alleviating the overﬁtting problems, ‘LN’ represents layer normalization, which normalizes an input vector by its mean and variance for stable training. 4.4 Dynamic Negative Sampling Layer Previous news recommendation models primarily leverage uniform negative sampling to optimize the users’ implicit feedback. However, as discussed by [3], uniform negative sampling is not optimal since it selects too random samples, which can be less discriminative to the positive ones. We adopt a dynamic negative sampling (DNS) method to train our model more effectively. Our general idea is to build more informative item pairs, which are critical for model optimization. In our method, if an item pair is hard to separate, we regard it to be more informative since the model can learn more by optimizing it. To evaluate how difﬁcult an item pair can be separated, we introduce a similarity function, that is: f (y, X) = W(X· y) + b, where y ∈ R and X ∈ Rare the representations of the positive and the whole candidate news, respectively. W ∈ Rand d ∈ Rare weighting parameters. Based on f , we select the most similar items in a greedy way: S = arg max f (y, X), where S is the index set of the selected negative items. In our model, we further introduce a loss to constraint the parameters in f: where Merge is a function projecting the embeddings of the selected negative samples into a vector. We would like to make the negative samples similar to the positive ones on different distance metrics by this loss function. 4.5 History Summarization Layer To summarize the users’ historical behaviors, we leverage Transformer to process the previously interacted news. Compared with other sequential models like convolutional neural network (CNN) and recurrent neural network (RNN), Transformer directly captures the correlations between any two steps of events in the sequence, which is effective in many other machine learning tasks. In our model, to obtain the representation of news sequence, we stack the representation of L news articles into a feature tensor E ∈ R. We adopt M Transformer layers. The input of the ﬁrst layer is E, and the previous layer’s output is fed into the next layer. Multiple Transformer layers are capable of modeling abstract sequential patterns, and the output pof the last layer is used for model inference. 4.6 Prediction Layer We concatenate the sequence vector p, the representation x of candidate news cand the user embedding u, and feed them into a fully-connected layer to estimate the click rate of user i click candidate news c: ˆy= φ([pxu]W+ b)W+ b, where W∈ R, b∈ R, W∈ Rand b∈ R are the parameters of the prediction layer. Besides, the negative samples set S output by dynamic negative sampling layer are combined with positive samples to compute loss. We adopt binary crossentropy loss function: L=ylog σ(ˆy)+(1−y) log (1−σ(ˆy)), (6) where yis the instance label, Drepresents the positive instance set. Therefore, the total loss of D-HAN is L = L+ αL, where α ∈ (0.0, 1.0] is a trade-off parameter. In this section, we conduct experiments to answer the following questions: • RQ1: Does our proposed D-HAN model outperform several state-of-the-art models? • RQ2: What are the impacts of each hierarchical attention layer on the performance of D-HAN model? • RQ3: Is relative time interval or absolute timestamp of news items improve model performance? • RQ4: Can the history summarization layer further improve the performance of hierarchical attention layers? • RQ5: Are the attention weights able to learn meaningful information in sentence-, element- and document-level? • RQ6: Can the dynamic negative sampling layer improve the recommendation performance? 5.1 Experimental Setup Datasets. We conduct experiments on three datasets: Adressa, Cert and Caing. Each log of all datasets contains user ID, news ID, reading timestamp and news contents. Adressa (http://reclab.idi.ntnu.no/dataset/) and Caing (http://www.dcjingsai.com/) are available online. • Adressa: This dataset is constructed by [9] for evaluating news recommendations. It contains reading logs of 10 weeks from Adresseavisen, a Norwegian news portal. In this paper, we ﬁlter the ﬁrst 15 days to do the experiments. • Cert: This dataset is provided by the Computer Emergency Response Technical Team of China, from March 2016 to April 2017, containing each user’s logs from various news portals, including people.com, cctv.com, et al. • Caing: This dataset is from Caing, a famous news portal in China. It contains complete reading logs of 10,000 users during March 2016. All the datasets are preprocessed to make sure that all users have at least 15 interactions. The statistical details of the three datasets are summarized in Table 1. Evaluation Protocols. To evaluate the performance of news recommendation, we adopt the leave-one-out strategy, which has been widely used in [4], [15], [16]. For each user, we use a sliding window of L + 1 (L historical news and 1 candidate news) length to slide over his interactions, and each window generates one instance. We hold out the latest instance for testing and utilize the remaining instances for training. For every user, we randomly sample 99 news articles during evaluation, which are not interacted by the user and rank the ground-truth news that the user has consumed among the 99 news. The performance of the ranked list is evaluated by Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). The HR@N measures whether the ground-truth news is ranked on the top-N list, and the NDCG@N accounts for the position of hit by assigning higher scores to hits at top ranks [11]. Baselines. We compare D-HAN with the following methods, which can be divided into two groups. The ﬁrst group only includes collaborative ﬁltering methods: • BPR [28]: This method optimizes the latent factor model with the pairwise ranking loss on implicit feedback data. • GRU4Rec [13]: This method applies recurrent neural network for session-based recommendation. • Caser [30]: This method utilizes horizontal and vertical convolution ﬁlters to simultaneously capture sequential patterns and model users’ general preferences. The second group contains recommendation methods that utilize content information. • GRU4Rec++ [12]: This method is an improved version of GRU4Rec, further considering news content information. • WE3CN [16]: This method applies 3D convolution neural network for news recommendation, utilizing content and sequential information simultaneously. • DNA [39]: This method is the preliminary work of this paper, where the same hierarchy architecture is adopted. Many methods based on deep learning for news recommendation have been proposed [22], [26], [32], [40]. However, they require external information not available in our datasets, i.e., knowledge graphs by [32], news category by [26], web browsing and searching records by [22] and online environment by [40]. Therefore, we do not include them as baselines. Moreover, in news recommendation, there exists a severe item cold-start problem for the collaborative ﬁltering methods of the ﬁrst group. In the testing phase, these models cannot use meaningful representations of unseen news articles. In this case, we use the average representation of the top-100 temporally-closest news articles instead. (https://polyglot.readthedocs.io/en/latest/index.html) [1] which is a natural language pipeline that supports massive multilingual applications. For the two Chinese datasets Cert and Caing, we choose NLPIR (http://ictclas.nlpir.org) (also known as ICTCLAS), a tool that integrates many functions such as word segmentation, part-of-speech tagging, and named entity recognition. For BPR, Caser, and GRU4Rec, user embedding and news embedding dimensions are set as 20. For GRU4Rec++, the dimension of the content vector is set as 64. For WE3CN, the dimension of the word vector is set as 64. Other parameters in the baselines are set as default. We implement our D-HAN model and other deep learning models with Pytorch. The number of news L of each sequence is set as 10. The number of sentences K for each news is set as 20. The dimension d is set as 64. The dimension of intermediate size dis set as 256. Merge(x, y) looks up the corresponding vector of the ﬁrst element in x from y. The loss trade-off parameter α is set to 1.0. The number of the history summarization layer is set as 2. Adam optimizer is applied for training, and the learning rate, batch size, weight decay, and dropout are ﬁxed to 10, 256, 10and 0.2, respectively. Each experiment is repeated three times, and we report the average results. 5.2 Performance Comparison (RQ1) The performance of our proposed D-HAN and two types of baselines on three datasets is shown in Table 2, in which we have the following observations: Our proposed D-HAN model achieves the best performance on all the datasets, signiﬁcantly outperform the best baseline. In speciﬁc, on the metrics of HR@10 and NDCG@10, D-HAN outperforms DNA by about 8.34% and 18.12% on Adressa dataset, 11.29% and 20.44% on Cert dataset and 4.85% and 2.92% on Caing dataset, respectively. We attribute the performance improvement to self-attention mechanism applied in each attention layer, the time information adopted in document-level attention layer, the Transformer encoder applied in modeling sequential information and the dynamic negative sampling method. We will detail the effectiveness of each component next sections. Content information improves the performance of news recommendation. For example, in most cases, the models of the second group are superior to the models of the ﬁrst group. More directly, GRU4Rec++ is merely added content information based on GRU4Rec and outperforms GRU4Rec on all datasets. However, Caser, a CF model, outperforms GRU4Rec++ on Cert dataset. This is probably because many adjacent actions do not have apparent dependency relationships in the Cert dataset. The RNN-based GRU4Rec++ cannot handle datasets with this characteristics, while CNNbased Caser can still capture sequential patterns with convolutional ﬁlters. Caser outperforms WE3CN on Adressa and Caing datasets, this is probably because, WE3CN represents each news article with the ﬁrst 50 words, which are not enough to express the contents of news articles very well. Sequential information improves the performance of news recommendation. BPR only utilizes users’ feedback information and performs the worst. In addition to users’ feedback information, GRU4Rec and Caser both utilize sequential information of users’ reading behaviors and perform better than BPR. This suggests the effectiveness of considering sequential information in news recommendation. 5.3 Impacts of Attention Layer (RQ2) A unique design of our model is leveraging attention mechanism to process the news information on the sentence-, element- and document-level. Comparing the previous DNA model, we have improved the attention computing methods. In this section, we study the effectiveness of the revised attention methods. From Table 3, we can observe: With sentence-level attention layer, D-HANimproves the performance by about 1.81%, 13.49% and 3.63% on HR@1 for different datasets, respectively. This result demonstrates that the scaled dot-product attention used in D-HANis more effective than the additive attention mechanism used in DNA. With element-level attention layer, D-HANimproves the performance by about 7.84%, 20.21% and 4.63% on HR@1 for all the three datasets, respectively, which indicates that there is much space to explore for element-level information, and it is important to make full use of the internal relationship between two elements in one piece of news and the external relationship between two elements in pairwise news. Note that D-HANis better than D-HAN, this is contrary to our intuition. Generally, we think that news text plays a greater role in news recommendation than news elements. Probably this is because that sentence-level vector instead of the token-level vector is used as the input of sentence-level attention component for a fair comparison with baselines, and this operation may result in the loss of much information. Exploring the representation of sentences by stacking the representation of tokens is left to be done in the future. With document-level attention layer, the performance of D-HANis better than D-HANand D-HAN. Speciﬁcally, for Cert dataset, D-HANgets improvements by 32.37%, 14.54%, 20.81% and 18.2% on HR@1, HR@5, NDCG@5 and NDCG@10, respectively. It demonstrates that the documentlevel information is pretty important for news sequence recommendation, and Transformer encoder adopted in document-level attention component is effective. D-HANwhich adopts sentence-, element- and document-level attention components simultaneously can achieve the best performance. Speciﬁcally, D-HANget the most performance improvements, 21.86%, 38.77% and 12.93% on HR@1 for all three datasets. 5.4 Inﬂuence of Time Embedding (RQ3) Another character of our model is the comprehensive time information modeling. In speciﬁc, we incorporate both absolute and relative time information in the document-level attention layer. We compare our ﬁnal model with its three variants: D-HANis a method without considering any continuous time information. D-HANis a method by removing the absolute time information in the ﬁnal model. D-HANis a method by removing the relative time information in the ﬁnal model. Relative time intervals can be computed by a) time difference between any two news items in a sequence, b) time difference between two adjacent news in a sequence, c) time difference between candidate news and each news in a sequence. [20] adopts the ﬁrst one to form a relative time interval matrix, and we adopt the last one since it is more effective in our experiments. The results are presented in Table 4. We can see: Without relative time information, D-HANdecreases the model performance by 0.44% and 0.73% on HR@1 for Adressa and Caing, respectively. This result demonstrates the effectiveness of the relative time information for the user behavior modeling. Without absolute time information, DHANcauses 5.7% and 0.86% drop in HR@1 for Adressa and Caing, respectively, which manifests that the absolute D-HANdenotes D-HAN with DNS. time information can also be useful in the news recommendation scenarios. Besides, D-HANis better than DHANin most cases, indicating that absolute time is more informative than relative time interval. On Adressa and Caing dataset, D-HAN performs best, indicating that relative time interval and absolute time can complement each other, and the combination of these two time information is more effective than using either one of them alone. 5.5 Impacts of History Summarization Layer (RQ4) In this section, we compare the history summarization layer performance by an ablation study. D-HANremoves the history summarization layer. Besides, we also exploit different sequential models for user history summarization. Speciﬁcally, we leverage CNN, LSTM, BiLSTM, single head Transformer and multi-head Transformer to process the previous user behaviors. The results are presented in Table 5, we can see: (a) Comparing the results of D-HANwith Transformer-based models, history summarization layer improves model performance for all three datasets. For Adressa dataset, setting CNN, LSTM or BiLSTM as history summarization layer can improve model performance and CNN performs the best on Cert dataset, while for Cert and Caing dataset, RNN-based models can not promote model performance. Such observation manifests that CNN and RNNbased summarization methods have their own advantages on some speciﬁc datasets, but it is encouraging to see that Transformer-based models can consistently achieve the best performance. We speculate that Transformer can directly model the correlations between any two steps in the news sequence, more sufﬁciently capturing the sequential patterns. (b) Comparing the results of SH with MHs indicates that more heads do not necessarily bring signiﬁcant performance improvements. This may be because the dimension in our experiment is small (d = 64), resulting in each head playing a small role [20]. In our experiments, we use a single-head Transformer for brevity. 5.6 Visualization (RQ5) Figure 2 presents the case studies of the sentence-, element, document- and time-aware document-level attention weights to show what each module learns for each level information. We randomly sample pairs of historical and candidate news from the testing set. The attention weights are extracted from the best performance epoch. Note that ﬁgure 2(c) and ﬁgure 2(d) use the same instance for fair comparison. Due to the space limitation, we only present the visualization on Caing dataset. Sentence-level attention weights: Figure 2(a) shows a typical self-attention weights of a history-candidate news pair, where the candidate news representation is concatenated as the last sentence of the history news, as introduced in Section 4.1. Note that in our experiments, 20 sentences are extracted from each news item, and insufﬁcient ones are padded to the left. The result shows that sentences tend to attend to the ﬁrst few sentences, and sentences in the latter part are less attended. Speciﬁcally, the upper left is much brighter (larger weights), and the bottom right is much darker (smaller weights). This is possibly because the former is the ground-truth sentences of the news while the latter is pads. In addition, the result also shows that the ﬁrst three sentences are signiﬁcantly attended, which is consistent with the text characteristics in the news scene, and the ﬁrst few sentences maintain the main content of the entire news. Besides, the last column and the last row have higher attention weights than the adjacent rows and columns, but the bottom right corner candidate news attention weight is lower. This is possibly because candidate news provides the reference information for historical news, but not for itself. Element-level attention weights: Figure 2(b) shows a heatmap of attention weights between two elements of one history-candidate news pair. Different from Figure 2(a), this pair is concatenated along the d dimension as detailed in Section 4.2. The x-axis and y-axis show the ﬁve elements of news we use, e.g., time, person, organization, location and keywords, respectively. As the ﬁgure shows, the person element plays the largest role, followed by the location element, demonstrating that these two elements are more important than others in this instance. Unlike the heatmap of sentence-level attention weights, which has ﬁxed and typical patterns, the heatmap of element-level attention weights is different for different instances. This may be due to that these ﬁve artiﬁcially selected elements are already essential elements for news sequence recommendation. Document-level attention weights: We ﬁrstly describe several possible cases demonstrating user behavior patterns: Case 1: Click news with the relevant or same topic. This kind of case often happens when the read news is newly released, such as major events or hot news, and users tend to browse more relevant news to obtain more relevant information. When the event gets out of date quickly, users will no longer be interested in it. But if this event lasts for a long time, users will be interested in this kind of news for a long time. For instance, a piece of news about major casualties on the highway, when a user reads this news for the ﬁrst time, his next step may tend to click relevant news. But if a piece of news is about the war between Azerbaijan and Armenia, since the war will not stop in a short time, users tend to track the progress of the war. Case 2: Click other news instead of hot news read by many users. For two pieces of news, one of them is clicked many times and the other is not received by many people. If the user skips the former and reads the latter, it means that the user is not interested in news relevant to the former. Case 3: Click news of a user’s long-term interests. Although recent behaviors greatly inﬂuence a user’s current news click behavior, her interest is often hidden in long-term historical records, it is a stable tendency. Case 4: Click news in which a user is interested during a speciﬁc time period. The time period has an impact on the users’ propensity, e.g., browsing news related to the stock market at around 10 am on weekdays, browsing news related to food cooking on weekends, and reading news related to constellations at midnight. Figure 2(c) shows a typical heatmap of document-level attention weights between news items in one news sequence, it is an example of case 1. We can clearly see that the upper left of the ﬁgure has lower weights, while the bottom, left and bottom left have higher weights. Speciﬁcally, the 9th news items have the largest weights followed by the 6th news, reﬂecting the former most inﬂuence user decision followed by the latter in this instance. In addition, to compare the inﬂuence of more distant and recent news on users’ decisions, we randomly sample 500 instances in the test set and check the heatmap of their document-level attention weights. We divide the inﬂuence into three categories: (a) recent news have larger inﬂuence, when the weights are concentrated in the 5th to 10th news. (b) distant news have larger inﬂuence, when the weights focus on the 1th to 5th news. (c) older news have the same inﬂuence as recent news, when the weights fall evenly or can not be clearly distinguished. The statistics show that these three categories account for 46.8%, 23.6% and 29.6%, respectively, indicating that user behavior patterns in news recommendation scenes are more affected by recent behavior. Time-aware document-level attention weights: For the above cases and results, we can see that the time interval and timestamp are pretty key information, that is, one user’s operations are different in different time interval scales or different periods. For example, if a user reads a newly released hot news, then in a short time, e.g., 30 minutes, he might be full of interest and curiosity about the news. But for a long time, e.g., two days, he might be no longer interested in this type of news, reﬂecting the timeliness of news scene. Figure 2(d) shows the heatmap of document-level attention weights of Figure 2(c) embeded with time information. Comparing Figure 2(d) with Figure 2(c), we can see that time-aware document-level attention weights are more sparse, and weights are dominated by the 5th news item, indicating relative time interval and absolute timestamp embedding do bring great inﬂuence on the weights. Too long sequence may cause historical information overload, i.e., distant information accounts for too high proportion for news recommendation, resulting in recommended news being limited to a certain range and lack diversity. To the opposite, too short sequence will lead to instant information overload, i.e., recent news account for too high proportion for news recommendation, resulting in recommended news being limited to the type of previous one. At present, however, the length of news sequence is set to a ﬁxed value based on experiments, e.g., 10 in our experiment. We have an idea that the long-term and short-term sequences can be processed separately by different models and then combined with different proportions dynamically to use historical and instant information. We left this idea in the future. 5.7 Impacts of DNS Layer (RQ6) In this section, we study the impacts of the DNS layer. For a fair comparison, we adopt DNS in the training phase, but uniform negative sampling in the test phase. Due to the space limitation, we only present the results on HR@1, NDCG@5 and NDCG@10 as shown in Table 6. We can observe that DNS improves the model performance in most cases. This results verify the effectiveness of our designed dynamic negative sampling method. Comparing with the uniform sampling, DNS can well discover more informative negative samples with higher discriminative abilities. Summary. From these tests, we ﬁnd the following. (1) Our approach D-HAN is effective for news recommendation. HR@N and NDCG@N scores of D-HAN are consistently higher than compared methods in all datasets. (2) Each component in D-HAN improves the performance. We provide insights into attention weights by experimentally demonstrating their reasonableness. (3) Dynamic strategies dealing with negative sampling are more effective. We propose to use sentence-, element- and document-level information simultaneously by incorporating them into a hierarchical attention network in the news recommendation scene and applying Transformer encoder to capture news sequential information. To model the dynamics of news recommendation, we propose a time-aware document-level attention layer to incorporate relative time interval and absolute timestamp. To generate more informative negative samples, we propose a dynamic negative sampling method that generates negative instances dynamically while training and then guide model optimization. This work is supported in part by National Key R&D Program of China 2018AAA0102301, NSFC 61925203&61832017, and Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098.