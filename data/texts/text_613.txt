Keywords Eye tracking · Data Quality · HoloLens 2 · Augmented Reality · AR · Spatial accuracy · Spatial precision · Temporal precision · Linearity · Crosstalk The growing ubiquity of dedicated eye trackers into augmented-reality (AR) devices promotes the application of eye tracking to improve interaction between the user and the device. Eye tracking can, for example, prolong the battery life of untethered devices without reducing the perceived quality of the simulated environment through foveated rendering. However, each potential application of eye tracking requires different levels of data quality to be effective. Eye movement biometrics [ the environment [ quality for a particular use case. The recently released HoloLens 2 includes a built-in eye tracker, and represents an accessible option for integrating eye tracking into research. In order to benchmark its performance, we evaluate the quality of the eye tracking data captured by the HoloLens 2. We introduce a new eye movement dataset collected with this device (n=30) and evaluate signal quality descriptors including spatial accuracy, spatial precision, temporal precision, linearity, and crosstalk. We also observe how these results respond to post hoc recalibration in order to determine whether the HoloLens 2 responds positively to post hoc recalibration strategies that improve data quality in other eye tracking systems. From this analysis, we identify discrepancies between the manufacturer-speciﬁed signal quality and our own results. We also present eye tracking signal quality metrics that, to our knowledge, have not yet been reported for the HoloLens 2. Department of Computer ScienceDepartment of Computer Science We present an analysis of the eye tracking signal quality of the HoloLens 2’s integrated eye tracker. Signal quality was measured from eye movement data captured during a random saccades task from a new eye movement dataset collected on 30 healthy adults. We characterize the eye tracking signal quality of the device in terms of spatial accuracy, spatial precision, temporal precision, linearity, and crosstalk. We are the ﬁrst to measure linearity and crosstalk in the HoloLens 2. Most notably, our evaluation of spatial accuracy reveals that the eye movement data in our dataset appears to be largely uncalibrated, possibly due to the experimental conditions in which we collected our data. We also observe that recalibrating the data using our own recalibration task produces notably better eye tracking signal quality. From these ﬁndings, we discuss the possible causes of poor signal quality in our experimental setup and make recommendations for future researchers interested in using eye tracking in the HoloLens 2 for research. 2]. It is therefore imperative to determine whether an eye tracker captures gaze data at a suitable Data quality metrics such as spatial accuracy and precision are commonly reported by the eye tracking manufacturer. These measurements are usually produced in carefully controlled settings and may not represent a signal quality that is achievable in a typical user’s environment. Discrepancies between the typical spatial accuracy reported by eye tracker manufacturers and spatial accuracy achieved in practice have been well-documented in the literature [ Multiple studies suggest that data quality depends heavily on the conditions in which the data were collected [ that there is no absolute data quality value that can be routinely achieved regardless of experimental condition [ Taken together, these ﬁndings underscore the importance of studying eye tracking signal quality under a variety of experimental conditions. Previous studies of eye tracking signal quality in the HoloLens 2 characterized eye tracking signal quality while participants performed a variety of tasks (sitting still, walking, etc.) [ a benchmark for the HoloLens 2’s eye tracking signal quality in an experimental setup similar to those commonly found in literature [3][8]. 33 subjects (15 female, 18 male, median age: 21, age range: 19-36) took place in this study. Ten subjects who normally wore glasses removed them for this study, and four subjects wore contact lenses while participating. None of the participants wore glasses during data collection. Three subjects were excluded from this analysis based on excessively noisy data or data loss; namely, subject data was excluded if more than 20% of their data consisted of invalid samples. We used the remaining 30 subjects for our data quality analysis. All subjects wore masks throughout the experiment, as data collection was conducted during the COVID-19 pandemic. We employed data from a random saccades task to compute the eye tracking quality descriptors reported herein. The stimulus shown consisted of two concentric circles displayed at a distance of 1500 mm. The outer circle was white with a diameter of 0.5 the center of the stimulus as it appeared in random positions uniformly sampled from a visual ﬁeld spanning horizontally and to a new location at least 3 To study the effects of manual calibration on the collected data, we also included a recalibration task that took place immediately before the random saccades task. During this task, subjects ﬁxated on a 1 in predetermined positions forming a 13-point grid spanning the same ﬁeld of view as the random saccades task (see Figure 1). The positions of these points were the same for all subjects, but were presented in random order. Each ﬁxation lasted between 1 and 1.5 s. We created our stimulus using Unity 2019.4.21 and captured gaze data with Microsoft’s Mixed Reality Toolkit (MRTK) plugin. We modiﬁed the MRTK by writing a custom event to write gaze data to a ﬁle as the HoloLens 2 reports a new gaze sample. Headset position tracking was disabled so that the stimulus was always displayed relative to the center of the headset. To reduce the number of visual distractions from the environment, subjects faced a non-reﬂective black canvas for the duration of the experiment. Subjects also put their heads on the chinrest during data collection to minimize head movements. We captured eye tracking data from the HoloLens 2’s integrated eye tracker. This eye tracker features a sampling rate of 30 Hz and a nominal spatial accuracy of 1.5 simultaneously. However, the manufacturer does not provide an API to extract raw monocular signal from the device. The two monocular signals are instead combined into a single gaze ray, which is then made available via the MRTK. The methods employed to compose this ray from the monocular signal are not publicly available. The eye tracking signal quality results described in this report were produced using the signal available through the MRTK. , and the inner circle was dark blue with a diameter of 0.25. Subjects were instructed to ﬁxate on ±10vertically. The stimulus remained at each position for between 1 and 1.5 seconds before jumping Figure 1: The HoloLens 2’s built-in gaze calibration protocol overlaid by our recalibration task. The ‘gems’ belong to the HoloLens 2’s calibration, and the white circles belong to our recalibration task. Each gaze sample is represented by a pair of gaze origin transformed raw gaze data into degrees of visual angle using MATLAB’s atan2d function: Eye tracking signal quality is typically measured during a stable ﬁxation on a target. Although there are a number of algorithms available for eye movement classiﬁcation (e.g. I-VT), many studies note that eye movement classiﬁcation algorithms differ signiﬁcantly in their outputs [ identiﬁed ﬁxations by ﬁrst removing saccade latency and empirically selecting the subset of samples that produced the most stable behavior across all subjects. First, we removed saccade latency from the signal to minimize the delay between the gaze signal and its corresponding target position (see Figure 2). Following an approach proposed by Lohr et al. [ latency for each recording by calculating the mean Euclidean distance between the measured gaze position and the target position. The gaze signal was then shifted by the number of samples that produced the lowest distance measure between the gaze and the target, up to 24 samples (approximately 800 ms). With the gaze and target signals temporally aligned, we identiﬁed ﬁxations by the start and end of each target step. Figure 2: Calculation and removal of saccade latency. The top panel illustrates a typical gaze signal. The bottom panel shows the data after the gaze position signal has been shifted to remove saccade latency. We then identiﬁed stable ﬁxation periods by the absence of error due to saccadic movement. The angular offset between the gaze signal and the target was calculated on a per-sample basis for the ﬁrst 30 samples (approximately 1000 ms) of each ﬁxation. We then calculated the mean angular error of each sample across all ﬁxations and empirically selected the largest contiguous window of samples with the lowest error (Figure 3), as a low error likely indicates that the eye is stable and ﬁxating on the target [ eliminate instability caused by non-ﬁxational movement that preceded it. The ﬁrst 466 ms of the remaining gaze signal was then used for all analysis herein. Figure 3: The average sample-by-sample error across all ﬁxations in the dataset. Samples with the lowest error (8 through 22) were empirically identiﬁed as the most likely to contain a stable ﬁxation on average. Because the HoloLens 2 does not provide feedback from its machine-supplied calibration, we tested the extent to which the captured signal could be recalibrated. We employed linear regression as a form of post-hoc data correction. In the Equations 2a and 2b, x and y represent the original signal and x’ and y’ represent the signal that was obtained as a result of recalibration. The corresponding A, B, and C coefﬁcients are obtained by regressing the gaze data on the stimulus position. We compute the recalibration coefﬁcients on the data obtained from the grid calibration task that preceded the random saccades task, as described in Section 2.1. This task resembled the built-in calibration protocol provided by the manufacturer, albeit with four more targets and smaller stimuli (see Figure 1). The results of the regression were then applied to the data obtained from the random saccades task. The quality of the recalibrated data was then compared to the quality observed in the original data. In each of the following subsections, we present eye tracking signal quality by introducing each metric, describing how it was measured, presenting results for the HoloLens 2, and brieﬂy discussing our results in the scope of existing literature. With the exception of temporal precision, each of the data quality metrics below are presented separately for uncorrected and corrected data. Spatial accuracy is measured as the distance degrees of visual angle [ (x, y) and a target position (x where H, V, and C respectively denote horizontal, vertical, and combined spatial accuracy. Because the spatial accuracies of other eye trackers typically present as right-skewed distributions [ median of the computed accuracy values across all ﬁxations in the dataset. We also include the mean spatial accuracy of our dataset to compare our results with existing analyses of the eye tracking signal quality for the HoloLens 2. Table 1 summarizes the spatial accuracy values measured across all ﬁxations. In the uncorrected data, accuracy is notably worse in the vertical direction. Correcting the data through the recalibration protocol described in Section 2.4 substantially improves average spatial accuracy, with error decreasing by 74% overall. Accuracy in the vertical dimension improves slightly more than in the horizontal dimension (74% and 67% respectively). The histograms in Figure 4 show the distribution of spatial accuracy by ﬁxation. While the uncorrected data has an unusually high frequency of ﬁxations with large error, the corrected data closely resembles the right-skewed distribution that is characteristic of spatial accuracy [5]. This corrective effect is seen most prominently in the vertical direction. Ours (Original) Ours (Recalibrated) Kapp et al. [7] Figure 4: Distribution of spatial error across all measured ﬁxations in the dataset. In a previous investigation of the HoloLens 2, Kapp et al. [ their stimulus in similar conditions to ours at a distance of 2 meters. Their observed spatial accuracy is lower than both our ﬁndings and the typical 1.5 should 1.5 We present spatial precision for the HoloLens 2 using data collected directly from our subjects. Following Holmqvist et al., we deﬁne spatial precision as the root mean square (RMS) of the intersample distances for a set of gaze samples [13]. By treating each ﬁxation as a set of n gaze samples, we compute spatial precision with the following equation: whereθis the Euclidean distance between samples. Although RMS conventionally uses angular distance as a measure of intersample distance [ precision when calculated during stable ﬁxation periods [ of computed precision values across all ﬁxations in the dataset due to the non-normal nature of the data (Figure 5). Figure 5: Distribution of spatial precision across all ﬁxations for uncorrected data for the combined (C) dimension. Recalibrated data has a similar distribution is therefore not depicted. † Calculated as the standard deviation of gaze samples for comparability. All other spatial precision values in this table to 3.0large to achieve their reported accuracy results–three to six times larger than the stimuli we present. Ours (Original) Ours (Recalibrated) Kapp et al. [7] Table 3.2 summarizes the spatial precision measured across all ﬁxations. Spatial precision values are relatively similar across all dimensions in both uncorrected and corrected data, with the majority of ﬁxations featuring spatial precision less than 0.1 between the original and recalibrated data across all dimensions. We also include the mean precision calculated using the method described in [ deviation between gaze samples and the average gaze position for each target, and report a mean spatial precision of 0.24at a distance of 2 meters. Our results are slightly lower, but still largely comparable their observations. The nominal sampling rate of the HoloLens 2’s integrated eye tracking is 30 Hz, but gaze samples are not always recorded in precise intervals of 33.3 ms. We evaluate temporal precision of the device by calculating the variability of inter-sample intervals (ISIs) between consecutive timestamps. Given n gaze samples captured by the HoloLens 2 that are each sampled at a timestamp timestamps of consecutive samples: After processing the data captured by the device, we were left with 89,721 gaze samples across all subjects. The mean difference between timestamps is 34.8 ms (SD 21.9 ms), which corresponds to approximately 1 sample. We also investigated the proportion of samples that were dropped by the HoloLens 2 eye tracker. Dropped samples are identiﬁed when ISIs exceed 49.95 ms (50% more than the ideal ISI of 33.3 ms). From the full set of 89,721 points, 4,088 samples across the entire dataset ﬁt this criteria (4.6%). As shown in Figure 6, the temporal precision of gaze data captured by the HoloLens 2 can vary signiﬁcantly between users. Three subjects, who were excluded this analysis, were excluded based on the presence of excessive data loss. Kapp et al. report a mean ISI of 33.3 ms [ that we consider dropped in our investigation. We should also note that Kapp et al.’s investigation captured gaze samples differently than our did. Where we capture gaze samples based on when the HoloLens 2 reports having gaze samples via a custom modiﬁcation of the MRTK, Kapp et al. checks for new gaze samples every 10 ms. Figure 6: The percentage of dropped samples in each subject’s gaze signal. The number of total dropped samples represents less than ﬁve percent of total samples in the dataset. . Recalibration had very little impact on average spatial precision, with by less than 10% difference Linearity measures the extent to which spatial accuracy changes based on the location of the target. A number of data quality investigations [ the target position in the ﬁeld of view. In order to describe linearity in a way that is easily comparable to another dataset collected in AR, we apply the approach used by Lohr et al. [3] for calculating linearity to our own dataset. We evaluate the linearity of our data by plotting the target position by the gaze position for each ﬁxation, ﬁtting a linear equation to the plotted points, and measuring the slope and 95% conﬁdence interval of the resulting line. This analysis is performed on all ﬁxations across the dataset. Figure 7 illustrates the horizontal linearity slope for uncorrected gaze data, and provides an example of where we would expect gaze samples to be located relative to target positions based on this analysis. Figure 7: Horizontal linearity results for the original gaze data, with an illustration of the expected accuracy based on the given slope. Gaze samples (blue) would ideally exhibit similar accuracy across space, but accuracy tends to be worse for the leftmost targets (vertical linearity not illustrated). We then identiﬁed data where our calculated linearity slopes differed signiﬁcantly from the ideal slope value of 1.0. Following [ include 1.0. Table 3 summarizes the linearity slope for the horizontal and vertical components of the data across calibration strategies. While the uncorrected data is signiﬁcantly different from ideal, the corrected data features much less error across the ﬁeld of view and a linearity slope that successfully approximates ideal conditions. This improvement is partially aided by the design of the grid calibration task, which captured and gaze data taken at evenly spaced intervals across the entire ﬁeld of view. Table 3: Linearity slopes by calibration, dimension, and session. Note that linearity is not calculated for combined gaze signals. 3], we designate linearity as signiﬁcantly different from ideal if its slope’s 95% conﬁdence intervals do not Crosstalk describes the extent to which the rotation of the eye in one dimension (e.g. horizontally or vertically) affects movement in the orthogonal dimension. We refer to horizontal crosstalk from vertical movements as “horizontal crosstalk,” and vertical crosstalk from horizontal movements as “vertical crosstalk.” Because Lohr et al. report the novel presence quadratic crosstalk in their dataset collected in virtual reality [ exhibiting linear, quadratic, and hybrid (i.e. both linear and quadratic components) crosstalk. For each ﬁxation, we plotted gaze offset against the target position of the stimulus in the orthogonal direction. We then ﬁt a regression model to this data, choosing the model that ﬁt the data most optimally from one of four possible options: linear, quadratic, hybrid, or intercept only. Figure 8 presents the results of crosstalk analysis for our data. We ﬁnd that our data does not exhibit signiﬁcant crosstalk in the horizontal direction in either original or recalibrated data. Our evaluation of vertical crosstalk reveals that the best-ﬁtting model varies by calibration method. Vertical crosstalk in the uncorrected data has both a linear and quadratic component, which is not maintained after the data is corrected via recalibration. The vertical crosstalk initially observed in our dataset may partially be an artifact of the poor spatial accuracy in the uncorrected data. Figure 8: Best-ﬁtting crosstalk models across all subjects in original (8A) and recalibrated (8B) data measured separately in the horizontal and vertical dimensions. The p-values shown are the signiﬁcance of each linear or quadratic component. Of all the eye tracking signal quality metrics we presented, only spatial accuracy is publicly benchmarked by the manufacturer. We ﬁnd that the spatial accuracy results from the uncorrected data are consistently worse than the typical spatial accuracy reported by the manufacturer (1.5 between 1.4 and 2.8for optimal performance [9]. The relatively poor spatial accuracy we observe contradicts our expectations, as we employed a stimulus size of 0.5 uncorrected data captured by the device also appears to be largely uncalibrated. The presence of masks during data collection may be partially responsible for this phenomenon; spurious corneal reﬂections introduced by masks may have degraded the quality of the captured signal. To our knowledge, we are the ﬁrst to evaluate the effect of recalibration on data in the HoloLens 2 when spatial accuracy does not meet the signal quality levels reported by the manufacturer. Recalibrating the data through linear regression produces remarkably better accuracy values than the original data. Most notably, the average spatial accuracy achieved by the recalibrated data better approximates the manufacturer-reported benchmark. These ﬁndings demonstrate that correcting data based on a recalibration task can drastically improve spatial accuracy, even when data is collected in non-ideal conditions. We recommend that future eye tracking researchers using the HoloLens 2 include a similar task to facilitate post hoc data correction, especially when tasks do not include explicit target positions (e.g. free-viewing tasks like reading and video watching). Although there are no manufacturer-supplied metrics for spatial precision, we can compare our results to an existing study on the HoloLens 2’s eye tracking. Kapp et al. report higher spatial accuracy and similar spatial precision values at a viewing distance of 2 meters, but captured eye movement data using signiﬁcantly larger targets in their analysis [7]. Our results represent a new data quality baseline for data quality for a stimulus size that is commonly used in eye tracking literature [3][12][14]. Our analysis of temporal precision reveals that the HoloLens 2 delivers gaze data at a steady rate which approximates manufacturer-reported values. However, inspecting the distribution of dropped samples across the dataset reveals that the device occasionally drops a signiﬁcantly higher proportion of samples for some subjects. We also report linearity and crosstalk, which has not been done in previous studies. Our results for linearity slope illustrate that spatial accuracy tends to deteriorate at the extremes of the ﬁeld of vision. However, recalibration brought linearity values closer to the ideal. Our analysis of crosstalk is based on a novel approach from [ possibility of crosstalk ﬁtting a quadratic curve. A subset of our data also exhibits partially quadratic crosstalk, which transforms into an intercept-only ﬁt after the data is corrected. Some studies [ movement in one direction per degree of movement in the orthogonal direction. Performing a similar analysis could contribute to a more robust understanding of how our dataset exhibits crosstalk across uncorrected and corrected data. The analysis presented in this report is limited by the nature of the gaze data available to us through the HoloLens 2’s eye tracking API. Although we know that the HoloLens 2 captures raw monocular data, it is currently not possible to extract it from the device. It is known that the eye tracking signal available through MRTK is pre-processed in an attempt to remove distinguishable information [ other way (e.g. smoothing). Without access to the raw eye movement signal, we are currently unable to assess the eye tracking signal quality before the data is processed by the device. The data captured during our investigation may also feature worse signal quality than expected due to the presence of masks during data collection. These ﬁndings have may signiﬁcant implications for future data collection efforts, but further investigation is needed to ascertain whether masks generally have a negative effect on data quality. Friedman et al. observe that spatial accuracy [ and suggest that this multimodality is caused in part by microsaccades and drift. Although the sampling rate of the HoloLens 2 is too low to adequately capture microsaccades, future investigations may reveal the extent to which multimodality within ﬁxations manifests in data captured by the device. We evaluated the HoloLens 2’s eye tracking signal quality using commonly used signal quality descriptors, including the ﬁrst analysis of linearity and crosstalk. Our investigation contributes to a growing body of literature that characterizes the nature of eye tracking data in AR environments. While the manufacturer reports a typical spatial accuracy of 1.5 we observed an average spatial accuracy of 6.2 metrics and our ﬁndings is consistent with the disparities found in other studies across eye tracking literature, which underscores the need for independent characterization of eye tracking signal quality. We also observed that recalibration the data improves spatial accuracy and (by extension) linearity, and changes the underlying nature of characteristics like crosstalk. Based on ﬁndings from this investigation, we recommend that future studies using the HoloLens 2 include a recalibration task that captures eye movement data from across the ﬁeld of view to facilitate post hoc data correction. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1840989.