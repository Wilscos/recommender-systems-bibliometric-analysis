University of Southern CaliforniaUniversity of Southern California Graph Neural Networks (GNNs) have shown great success in many applications such as recommendation systems, molecular property prediction, trac prediction, etc. Recently, CPU-FPGA heterogeneous platforms have been used to accelerate many applications by exploiting customizable data path and abundant user-controllable on-chip memory resources of FPGAs. Yet, accelerating and deploying GNN training on such platforms requires not only expertise in hardware design but also substantial development eorts. We propose HP-GNN, a novel framework that generates high throughput GNN training implementations on a given CPU-FPGA platform that can benet both application developers and machine learning researchers. HP-GNN takes GNN training algorithms, GNN models as the inputs, and automatically performs hardware mapping onto the target CPU-FPGA platform. HP-GNN consists of: (1) data layout and internal representation that reduce the memory trac and random memory accesses; (2) optimized hardware templates that support various GNN models; (3) a design space exploration engine for automatic hardware mapping; (4) high-level application programming interfaces (APIs) that allows users to specify GNN training with only a handful of lines of code. To evaluate HP-GNN, we experiment with two well-known sampling-based GNN training algorithms and two GNN models. For each training algorithm and model, HP-GNN generates implementation on a state-of-the-art CPU-FPGA platform. Compared with CPU-only and CPU-GPU platforms, experimental results show that the generated implementations achieve 55.67×and 2.17×speedup on the average, respectively. Compared with the state-of-the-art GNN training implementations, HP-GNN achieves up to 4.45×speedup. • Hardware → Recongurable logic applications; Hardware accelerators. Graph Neural Networks; FPGA Framework; Hardware Acceleration ACM Reference Format: Yi-Chien Lin, Bingyi Zhang, and Viktor Prasanna. 2022. HP-GNN: Generating High ThroughputGNNTraining Implementation on CPU-FPGA HeterogeneousPlatform. In Proceedings of the 2022 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA ’22), February 27-March 1, 2022, Virtual Event, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3490422.3502359 Recently, Graph Neural Networks (GNNs) have shown great success in many elds including recommendation systems [27,33], molecular property prediction [14], trac prediction [16], etc. Initially, GNN was trained using the full graph [17] as the input, directly. However, as graphs become larger, full-graph GNN training becomes inecient because the model is only updated once for each epoch. Moreover, huge amount of intermediate results needs to be stored in full-graph GNN training, which leads to high memory footprint [20]. To overcome the above issues, many sampling-based GNN training algorithms [2,8,14,29] have been proposed for training the GNN models. Sampling-based methods rst sample the full graph to produce mini-batches, and then take these mini-batches as input for training. The sampling-based mini-batch training methods demonstrate great advantages compared with full-graph training in terms of accuracy, generalization, and scalability for large-scale graphs [2,8,14,29]. Therefore, state-of-the-art GNN frameworks [11, 23] adopt sampling-based mini-batch training algorithms. Compared with the CPU-only platforms or CPU-GPU platforms, CPU-FPGA platforms are promising platforms to deploy GNN training since this can support customized memory access pattern [32] and data layout to reduce the substantial memory trac and random memory access in GNN training. However, deploying GNN training on CPU-FPGA heterogeneous platform is challenging due to notorious development eorts that requires hardware design expertise. Though there are many FPGA-based GNN accelerators [12,19,26,30,31], previous works either focus on full-graph GNN inference, or a specic GNN model, or a specic GNN training algorithm, and no general framework has been proposed. Motivated by the challenges, we propose HP-GNN, a framework for mapping GNN training on CPU-FPGA heterogeneous platform. We rst formulate an high-level abstraction to describe the computation of sampling-based mini-batch GNN training. Then we develop optimized hardware templates based on the GNN abstraction. In order to reduce development eort and eliminate the need for hardware expertise, HP-GNN provides easy-to-use software programming APIs that allow fast development without the need for hardware programming. To achieve high throughput and automate the accelerator generation process, we develop a general design space exploration engine that optimizes the hardware conguration based on the selected GNN training algorithm. In addition, we propose a data layout and internal representation that reduces the memory trac and random memory access. We summarize our contributions as follow: •We propose a general framework for mapping various sampling based mini-batch GNN training onto a CPU-FPGA platform. We demonstrate the applicability of HP-GNN to various sampling algorithms and GNN models. •We provide high-level and easy-to-use software programming interface that abstracts the hardware implementation details. •To enable hardware mapping and high throughput GNN training, the proposed framework consists of the following optimizations: –Data layout and internal representation that reduce the memory trac and random memory access caused by the irregular computation pattern of GNN. –Optimized hardware templates that support various widelyused GNN models. –General design space exploration algorithm that generates hardware design conguration to optimize the training throughput for various sampling methods and algorithmic parameters. •We evaluate our framework using two state-of-the-art GNN models: GraphSAGE [14], GCN [17], along with two commonly used sampling methods: neighbor sampling [14], and subgraph sampling [29]. Running on a Xilinx Alveo U250 board hosted by a 64-core AMD processor, the experimental results show that the accelerators produced by our framework can achieve 2.17×throughput on the average compared with a state-of-the-art GNN framework on CPU-GPU platform. Given an input graphG(V, E, 𝑿 ), a GNN model is specied by: • 𝐿: number of layers. • V: a set of target vertices to be inferred. • 𝑓: hidden dimension in layer 𝑙 (1 ⩽ 𝑙 ⩽ 𝐿). • A mechanism of how to construct: – V: the set of vertices in layer𝑙 (0⩽ 𝑙 ⩽ 𝐿).|V|denotes the number of vertices in layer 𝑙. Moreover, V= V. – 𝑨∈ R: adjacency matrix for feature aggregation in layer𝑙 (1⩽ 𝑙 ⩽ 𝐿).𝑨denes the inter-layer connectivity between Vand V. • 𝑾∈ R: weight matrix of layer𝑙 (1⩽ 𝑙 ⩽ 𝐿)that is used in update function to perform linear transformation of vertex features. • 𝑿∈ R: feature matrix of layer 𝑙 (1 ⩽ 𝑙 ⩽ 𝐿) •Aggregate() function that is used by each vertex to aggregate information from its neighbors. •Update() function including an one-layer multi-layer perception (MLP) and an activation function𝜎() that is used by each vertex to perform feature transformation. Figure 1 depicts the computation abstraction of a GNN layer. The computations using a GNN model can also be expressed using the aggregate-update paradigm [26], as shown in Algorithm 1.𝒉∈ R is the feature vector of𝑣 ∈ Bin layer𝑙, and𝒂∈ Ris the intermediate result of𝑣 ∈ B. There are several widely used GNN Algorithm 1 GNN Computation Abstraction models proposed in the literature: GCN: GCN is proposed in [17]. Given the input graphG(V, E, 𝑿 ), the GCN model is specied by: • V= V= ... = V= V= V • 𝑨= 𝑨= ... = 𝑨= 𝑫(𝑨 + 𝑰 )𝑫, where𝑨and𝑫are the adjacency matrix and the Laplacian matrix of the input graph. 𝑰 is the identity matrix. • 𝐿: number of layers; 𝑓: feature size in layer 𝑙 (1 ⩽ 𝑙 ⩽ 𝐿); The Aggregate() function and Update() function of GCN are expressed as: 𝒂= Sum(1· 𝒉: 𝑢 ∈ N (𝑣) ∪ {𝑣}) whereN (𝑣)denotes the neighbor set of𝑣inV,𝐷 (𝑣)denotes the degree of vertex 𝑣, and 𝒃denotes the bias of the update function. GraphSAGE: GraphSAGE is proposed in [14] for inductive representation learning on graphs. Starting from a set of target vertex V, GraphSAGE neighbor sampler recursively samples the neighbors to buildV, V, ..., V. The adjacency matrix𝑨denes the edge connections betweenVandV, and each edge has weight equal to one. The Aggregate() function and Update() function of GraphSAGE are expressed as: Note: In the rest of the paper, we use GCN and GraphSAGE to refer to their GNN-layer operators Aggregate(), Update(). To train a GNN model, the GNN training process consists of ve stages [2,8,14,29]: sampling, forward propagation, loss calculation, back propagation and weight update. In the sampling stage, a set of vertices and adjacency matrices are sampled from{V: 0⩽ 𝑙 ⩽ 𝐿} and{𝑨: 1⩽ 𝑙 ⩽ 𝐿}to form a mini-batch. We useBto denote the vertices sampled fromVin layer𝑙.𝑨denotes the sampled adjacency matrix, which describes inter-layer connections (edges) betweenBandBwithin the mini-batch. A mini-batch consists of target verticesB, sampled vertices for each layer{B: 0⩽ 𝑙 ⩽ 𝐿 −1}, and sampled adjacency matrices{𝑨: 1⩽ 𝑙 ⩽ 𝐿 −1}. In the forward propagation stage, the mini-batch is processed layer by layer. The output embeddings in the last layer{𝒉:𝑣∈ B}are compared with the ground truth for loss calculation. The calculated loss will be used as input for back propagation, which performs a similar computation as forward propagation but in the reverse direction. Finally, the gradients of𝑾in each layer are derived and be used to update the weight. We show the steps of GNN training in Algorithm 2. In Algorithm 2,N(𝑣)denotes neighbors of𝑣inB that are specied in𝑨. A GNN training algorithm is specied by a sampling algorithm (see Section 2.3) to construct the mini-batch that consists of {B: 0 ⩽ 𝑙 ⩽ 𝐿 − 1} and {𝑨: 1 ⩽ 𝑙 ⩽ 𝐿 − 1}. In HP-GNN, we exploit data parallelism within each mini-batch by aggregating and updating multiple vertices concurrently (shown in Line 5 of Algorithm 2). The computation order within the same mini-batch does not aect the nal results. Thus, training in our parallel framework leads to the same result and accuracy as training in serial fashion. An algorithm to sample a mini-batch is specied by: • A method to sample the vertices B(0 ⩽ 𝑙 ⩽ 𝐿) from V. •A method to construct the adjacency matrix𝑨(1⩽ 𝑙 ⩽ 𝐿) from 𝑨. By sampling the verticesB(0⩽ 𝑙 ⩽ 𝐿)and the adjacency matrix 𝑨(1⩽ 𝑙 ⩽ 𝐿), we construct the mini-batch as the input for each training iteration. Various sampling methods [2,10,14,27,29] are proposed to form a mini-batch from input graphs. These sampling methods [20] falls into three categories: neighbor sampling, layerwise sampling and subgraph sampling. We only introduce neighbor sampling and subgraph sampling, since layer-wise sampling [2] has the similar computation pattern with subgraph sampling [29]. Neighbor Sampling:Neighbor sampling [3,14,27] is a type of sampling strategy that recursively samples the neighbors from the target vertices. To perform neighbor sampling, users specify the size of target vertices|V|and the neighbor sample size𝑁𝑆for each vertex in layer𝑙. The sampler rst chooses a set of vertices as target verticesV. Then, the sampler samples𝑁𝑆neighbors for each vertex inVbased on a specic probability distribution (e.g., uniform distribution [14]). After the rst iteration of neighbor sampling, the set of 1-hop sampled neighborsBis obtained, where|B| = |V| × 𝑁 𝑆. Similarly, we obtain the 2-hop neighborsBby sampling the neighbors of the 1-hop neighborsB, where|B| = |V| × 𝑁 𝑆×𝑁 𝑆. By performing neighbor sampling recursively, we obtain𝐿-hop neighbors of the target vertices. After obtaining{B: 0⩽ 𝑙 ⩽ 𝐿 −1},𝑨(1⩽ 𝑙 ⩽ 𝐿)is constructed by: Subgraph Sampling:Subgraph sampling [8,29] is a strategy to sample a subgraph from the input graph and perform GNN information propagation within the subgraph. In the subgraph samplingbased method, users specify sampling budget𝑆𝐵which denotes how many vertices to be sampled for the subgraph. Then, the sampler samples𝑆𝐵of vertices or edges based on specic probability, and induce a subgraph based on the sampled vertices or edges. For subgraph sampling, the sampled vertices are identical for each layer, i.e.B= B= ... = B. The construction of𝑨(1⩽ 𝑙 ⩽ 𝐿)is the same as neighbor sampling. 2.4.1 Soware GNN Frameworks. Several software GNN frameworks [11,23,24,33] have been proposed in the literature. PyTorch Geometric (PyG) [11] is one of the most commonly-used frameworks for GNN deployment that uses PyTorch [22] as the backend. PyG users can describe various GNN models using PyG’s message passing API. Deep Graph Library (DGL) [23] adopts several parallelization strategies to achieve high performance and memory eciency for GNN computations. Moreover, DGL oers several major frameworks [1,4,22] as the backend, allowing users to port their model across frameworks. Aligraph [33] supports training on heterogeneous attributed graph (HAG), i.e. graphs that contain different types of vertices and edges. These software frameworks share similar features: They are built on existing frameworks [1,4,22], and abstract away the detailed implementations by providing graphoriented APIs so that users can describe GNN models easily. 2.4.2 Hardware GNN Acceleration. GraphACT [28] performs subgraph sampling based GNN training on CPU-FPGA heterogeneous platform. GraphACT exploits both task-level parallelism and data parallelism, and adopts a redundancy reduction technique to reduce the number of on-chip memory accesses. However, GraphACT optimizes the hardware design for inductive GNN models [14] using subgraph sampling, and does not support transductive model, such as GCN . As shown in [15], dierent sampling algorithms perform well in dierent applications, so there is no one-size-ts-all sampling algorithms. Rubik [6] decomposes GNN computations into graph-level and node-level computations, and proposes hierarchical task mapping strategies to exploit data reuse and parallelism in the two computation levels. However, Rubik is an ASIC accelerator that is hard to be optimized for various sampling algorithms. DeepBuring-GL [18] is a FPGA framework to accelerate GNN inference. DeepBuring-GL provides various templates to support dierent GNN computations and memory access patterns. DeepBuring-GL analyzes the GNN model with the input graph to identify performance bottleneck and choose appropriate hardware templates for kernel implementation to accelerate GNN inference. Though frameworks like DeepBuring-GL have been proposed, most of the hardware acceleration works still require signicant hardware expertise to make use of them. Moreover, no framework has been proposed to support various mini-batch GNN training on CPU-FPGA platform, which motivates us to conduct this work. In this paper, we build a framework to accelerate GNN training on CPU-FPGA platform. Our framework provides infrastructures to support various GNN models and training algorithms. We summarize the benets of using CPU-FPGA platform: while CPU can support various sampling algorithms, FPGA platform allows customizable data path and memory access pattern that can be exploited to optimize the GNN training throughput. Figure 3 depicts the mapping of graph data and various kernels onto the CPU-FPGA heterogeneous platform. Sampling is performed on the host CPU because CPU is exible to support various sampling algorithms; GNN operations including feature aggregation and feature update are performed on the proposed FPGA accelerator. Based on this task assignment, the structural information of input graph (V, E) is stored in the host memory for the host CPU to perform sampling. After sampling is done, the structural information of the mini-batch is generated and transferred to the FPGA local memory. The vertex features𝑿are stored in the FPGA local memory to be directly accessed by the FPGA accelerator, which can reduce the overhead of data movement. The state-of-the-art FPGA boards [13] have up to 260 GB memory; this can support medium size graph. Regarding very large graphs, we store the vertex features in host memory and transfer the vertex features of the mini-batch to the FPGA accelerator after sampling. Figure 2 demonstrates the framework overview. The generated design by the framework consists of two major components: (1) a host program that manages CPU-FPGA communication, kernel task scheduling and mini-batch sampling; (2) an accelerator design that runs on the FPGA. To generate the mini-batch GNN training implementation on CPU-FPGA platform, our framework takes the user program as the input and generates a high-level abstraction for mini-batch GNN training. In the input program, user species the following parameters: •GNN parameters: number of layers𝐿; hidden dimension of each layer:𝑓, (0⩽ 𝑙 ⩽ 𝐿). The hidden dimensions also dene the dimension of weight matrix 𝑾∈ R. •Specify an o-the-shelf GNN model, or provide user-dened functions (UDFs) for scatter( ), gather( ) and update( ) to build custom GNN computation layer. •Sampling algorithm and its parameters. For example, a neighbor sampler for a 2-layer GNN model can be dened as Sampler( ’NeighborSampler’, L=2, budgets=[10, 25]) through our high-level API described in Section 3.3. We provide several o-the-shelf samplers for users to choose from. The program parser extracts a GNN abstraction from user program, which serves as the intermediate representation for the software generator and hardware generator to generate the implementations on CPU-FPGA platform. The GNN abstraction consists of GNN model conguration (hidden dimensions𝑓, GNN operators, number of layers𝐿) and mini-batch conguration (number of vertices in each layer|B|, (0⩽ 𝑙 ⩽ 𝐿)and number of edges in each layer|E|, (1⩽ 𝑙 ⩽ 𝐿)). The mini-batch conguration is deduced from the sampling algorithm that implies number of vertices|B|, (0⩽ 𝑙 ⩽ 𝐿)in each layer and number of edges |E|, (1 ⩽ 𝑙 ⩽ 𝐿) in each layer. DSE Engine: the DSE engine takes the GNN abstraction and the platform metadata as input and generates the accelerator conguration that optimizes the GNN training throughput (Section 5). HLS Template/Hardware Template: In the framework, we provide optimized hardware templates written in high-level synthesis (HLS). The key computation operators of the templates (e.g. scatter(), gather()) are obtained from the GNN abstraction. We describe the details of the HLS template design in Section 4. Accelerator generator: Given the generated accelerator conguration and hardware templates, the accelerator generator generates the hardware accelerators for the target FPGA board. The accelerator generator uses the available synthesis tools such as Xilinx Vitis as the backend. Software generator: Given the input program, the software generator produces a runtime system that runs on the host processor. The runtime system performs the mini-batch sampling, CPU-FPGA communication management, and task scheduling. Table 1 summarizes our provided high-level APIs for user to program the mini-batch training in Python. Listing 1 is an example for developing the GNN training using our proposed framework. In the design phase, user species the mini-batch sampler, GNN model and parameters of platform. The framework automatically generates the optimized accelerator design and software design. In the runtime phase, user starts the GNN training on the target CPU-FPGA platform. Using our high-level APIs, a GNN training program only requires a few dozen lines of code. Use cases: Our framework can serve application developers, who utilize existing GNN models to build GNN applications. Our framework provides o-the-shelf GNN training implementations for some of the commonly-used GNN models (GCN [17], GraphSAGE [14], GIN [25]) which can be directly deployed on the CPU-FPGA platform. For machine learning (ML) researchers who develop novel GNN models, our framework allows them to customize their own GNN models. For both cases, our framework accelerates GNN training on CPU-FPGA platform without hardware expertise. We design hardware templates based on the computation abstraction of the GNN layer described in Section 2.1. The hardware templates describe a general architecture of the GNN layer as in Figure 5 and Figure 6. Then, the accelerator generator takes user-dened functions as input and integrates them into the hardware templates to generate the accelerator design. Aggregating feature vector from neighbors incurs irregular memory access and large memory trac. Figure 4 presents the data layout and internal representation used in our framework to reduce the memory trac as well as the number of random memory accesses. The data layout is produced by the sampler through renaming and sorting. Reducing Memory Trac (RMT): During aggregation stage, the feature vector of the source vertex is sent to its destination for aggregation. For the rst layer of aggregation, the input feature matrix𝑿 is stored in the memory. Thus, loading feature vectors incurs a large number of random memory accesses. Since the edges are represented in coordinate (COO) format sorted by source vertices in our framework, edges that share the same source vertex can reuse the feature vector that has been loaded, and thus reduce memory trac. The total number of memory trac can be reduced from𝑂 (|E|𝑓) to𝑂 (|B|𝑓), where|E|is usually larger than|B|. Fig 4 depicts Algorithm 3 Aggregation by Scatter-Gather Paradigm while not done do scatter phase: for each edge 𝑒 do end for gather phase: for each update 𝑢 do end for end while an example: <𝑣, 𝑣> loads the feature vector of𝑣from memory, and the loaded feature vector of 𝑣can be reused by (𝑣, 𝑣). Reducing Random Access (RRA): Since the edges are sorted by the source index in the rst layer, the destination vertex index of the edges are in a random order; thus, the hidden features are stored randomly as shown in the layer 1 and layer 2 of Figure 4. To reduce random access, our framework performs vertex renaming, which labels the vertices based on the order they are stored, this step also renames the vertices in each edge. Next, we sort the renamed edges by source vertices, and then accessing hidden features become sequential since the source vertex number follows the order it is stored. Aggregate Kernel: The aggregate kernel adopts the scatter-gather paradigm [5] as illustrated in Algorithm 3. Figure 5 depicts the detailed architecture of the aggregate kernel. Multiple processing elements (PEs) process multiple edges concurrently in each clock cycle. The vertex feature vectors are rst streamed to a feature duplicator. The feature duplicator broadcasts the loaded feature vector to all the Scatter PEs. The feature vector is stored in the PEs’ registers for data reuse. Then, Scatter PEs perform user-dened scatter() function, and stream the update𝑢to its destination via the routing network. The routing network is implemented as a buttery network [9]. After the Gather PEs receive the updates, Gather PEs perform user-dened gather function and obtain the intermediate results. The intermediate results are stored on-chip. Finally, when the aggregation is done, the results stored in the on-chip memory are written back to the FPGA local memory. Since the gather phase may incur reading and writing to the same destination vertices, read-after-write (RAW) data hazard may occur. The RAW Resolver addresses RAW data hazard by stalling. Update Kernel: The update kernel is a systolic array based design that performs block matrix multiplication. The input buer loads the aggregation results𝒂(see Algorithm 2) from the FPGA local memory. 𝒂will then be streamed into the MAC array. Each MAC module is followed by an element-wise operator𝜎. Typically, weight of each layer𝑾in GCN is small and frequently reused. Thus,𝑾 are stored on-chip in the Weight-Buer.𝑾will be broadcast to the multiply-accumulate (MAC) array during feature update. Finally, the result is stored into a result buer before written back to the FPGA local memory. Many modern FPGAs consists of multiple dies and number of interconnection wires across the dies is limited. Therefore, we implement multiple copy of the kernels that is distributed into multiple dies as shown in Figure 7. Multiple dies and multiple DDR channels are connected through an all-to-all interconnection which Figure 7: Architecture of the FPGA accelerator. is generated by vendor tool, such as Xilinx Vitis. The input feature matrix𝑿is equally partitioned into DDR channels. To utilize the multiple computation kernels for a single mini-batch, we perform task partitioning for the mini-batch training. In the forward propagation phase of layer 1, to infer the vertices inB= {𝑣, 𝑣, 𝑣, ...., 𝑣}. The workload for inferringBare equally partitioned into multiple kernels. Suppose there is 4 aggregate kernels and 4 corresponding update kernels. Aggregate kernel 1 aggregates {𝑣, 𝑣, ..., 𝑣}. Update kernel 1 updates{𝑣, 𝑣, ..., 𝑣}and write the results back to DDR. Similarly, aggregate kernel 2 aggregates {𝑣+ 1, 𝑣+ 2, ..., 𝑣}, and so on and so forth. The same partitioning scheme is applied to each layer. Our framework provides a design space exploration (DSE) engine for optimizing the GNN training throughput, given the conguration of mini-batch ({|B|: 0⩽ 𝑙 ⩽ 𝐿},{|E|: 1⩽ 𝑙 ⩽ 𝐿}), GNN hidden dimensions{𝑓: 0⩽ 𝑙 ⩽ 𝐿}, memory bandwidth𝛼, and hardware resources per die (DSPs, BRAMs, URAMs). To drive the optimization, we develop a performance model (Section 5.1) that models the training throughput on the CPU-FPGA platform, and the resource utilization model (Section 5.2) that is used to specify the resource constraints. Then, our DSE engine (Algorithm 4) performs parameter sweep in the design space to identify the hardware design parameters that optimizes the training throughput. We dene the throughput of mini-batch GNN training as Number of Vertices Traversed Per Second (NVTPS): The numerator indicates the total amount of vertices traversed in one mini-batch, and the denominator𝑡is the average execution time of one training iteration (see Algorithm 2). The modeling of the average execution time is based on the our task scheduling on the CPU-FPGA platform. We overlap sampling stage of the next batch with the execution of the current batch, so average execution time𝑡is estimated where𝑡consists of the execution time of forward propagation 𝑡, loss calculation𝑡, back propagation𝑡and weight update Modeling 𝑡: Loss calculation and weight update are executed on the host processor, which have optimized implementation in the software library. Forward propagation and backward propagation are executed on the FPGA platform, and their execution time depends on the hardware parameters and the mini-batch conguration ({|B|: 0⩽ 𝑙 ⩽ 𝐿},{|E|: 1⩽ 𝑙 ⩽ 𝐿}). We drive the approximate execution time of two propagation stages as: The total propagation time𝑡or𝑡is the sum of the execution time of each layer, and the execution time of each layer is decided by the task that takes longer to complete since aggregation stage and update stage are pipelined. The aggregation stage consists of two tasks: (1) loading vertex features or gradients, and (2) computation. Since the two tasks are pipelined, 𝑡can be modeled as: 𝑡=|B| × 𝑓× 𝑆𝐵𝑊 × 𝛼𝑡=|E| × 𝑓𝑛 × 16 × 𝑓 𝑟𝑒𝑞(8) We model the vertex feature loading time𝑡as. |B|indicates the number of vertices in each layer,𝑓is the feature length, and𝑆is the data size of each feature.𝛼is the eective bandwidth ratio. For the feature loading of the rst layer of neighbor sampling method,𝛼is estimated based on the memory burst transaction length𝑆[21] (for DDR4) because it incurs random memory access; for the rest of the layers,𝛼is near to 1 [21] since the memory accesses are sequential from DDR memory or on-chip memory (Section 4.1). The value of𝛼is obtained from the prior work [21] which performs a proling for the characteristics of the FPGA DDR memory. We model the compute time as (# of operations)/(# of PEs×kernel frequency).𝑛denotes the there are𝑛 Scatter PEs and𝑛Gather PEs instantiated in the aggregation kernel. |E|is the number of edges (i.e. non-zeros) in each layer. The size of|E|depends on the sampling method. We show the modeling of Efor various sampling methods in Table 2. For neighbor sampling, the number of edges|E|in each layer is decided by the size of target vertices|V|and sample size𝑁𝑆. For layer-wise and subgraph sampling, we formulate the number of edges in layer|E| as|B| × |B| × 𝜅 (|B|)where|B| × |B|corresponds to the case all the sampled vertices in layer𝑙and𝑙 −1 are connected, and 𝜅(|B|)is a pre-trained function that estimates the graph sparsity based on sample size |B|. Table 2: # of vertices and # of edges in each layer of various sampling methods Layer-wise 𝑆𝑆× 𝑆× 𝜅 (𝑆) Similar to𝑡, we model the𝑡as (# of operations)/(# of PEs×kernel frequency). The numerator is the complexity of matrix multiplication, and𝑚denotes how many parallel MACs are instantiated in the update kernel. Modeling 𝑡: the mini-batch sampling is performed on the host processor, which can potentially be a performance bottleneck. We exploit multi-threading to sample multiple mini-batches concurrently. In the design phase, we estimate𝑡under various number of threads and determine the minimum number of threads that satises 𝑡< 𝑡. We set the hardware constraints to form the solution space for our DSE Engine. Among the various hardware resources on the FPGA platform, DSPs and LUTs are used the most as we increase the parallelism of the hardware modules. Thus, we model the usage of LUTs and DSPs as our constraints: The coecients𝜆(1⩽ 𝑖 ⩽2)and𝜌(1⩽ 𝑖 ⩽3)are constants that indicate the resource consumption for each PE. In the case of DSPs, the utilization grows linearly as we instantiate more PEs; In the case of LUTs, an additional𝑛 log(𝑛)(see Section 4.2 for the denition of𝑛) term is introduced. The𝑛 log(𝑛)LUT overhead models the routing network in the aggregation kernel shown in Figure 5.𝑁and𝑁denote the available DSPs and LUTs on the FPGA platform. Many modern FPGAs consists of multiple dies [7], and the available resources may vary across dies. Thus, we perform DSE for each die to explore the optimal hardware conguration. We assume that each die is connected to one DDR channel (e.g. Xilinx Alveo U250) for simplicity in Algorithm 4; The DSE engine rst constructs the search space by deriving the maximum value of𝑛and𝑚separately based on Equations (10) and (11). Then, the engine performs an exhaustive search through all the possible congurations. For each conguration, the engine evaluates its throughput using Equation 4, and chooses the optimal design. We use our framework to generate GNN training implementations on a CPU-FPGA heterogeneous platform, and compare the training throughput with CPU-only platform and CPU-GPU platform. We list the information of each platform in Table 3. The CPU-only and CPU-GPU baseline are implemented using PyTorch-Geometric[11] and GraphSAINT[29]. Samplers, Models and Datasets: We generate mini-batch for GNN training using two sampling algorithms: (1) GraphSAGE neighbor sampler [14] for neighbor sampling (NS) and (2) GraphSAINT node sampler [29] for subgraph sampling (SS). For GraphSAGE neighbor sampler, we set the size of target vertices|V|as 1024, neighbor sampling size𝑁𝑆as 25 and 10 for 1-hop neighbors and 2-hop neighbors; for GraphSAINT-node sampler, we set the sampling budget𝑆𝐵as 2750. We measure the GNN training throughput of two-layer GCN model and two-layer GraphSAGE model on four medium-scale graph datasets (Flickr [29], Reddit [14], Yelp [29] and AmazonProducts [29]) that t in the FPGA local DDR memory. Details of the datasets and the GNN-layer dimensions are shown in Table 4. We implement the program parser, DSE engine, software and hardware generator in our framework using Python, and the accelerator templates are implemented using Xilinx HLS. The host program template is programmed in OpenCL. Users interface with our framework using our APIs programmed in Python. To serve application Table 4: Statistics of the Datasets and GNN-layer dimensions developers, our framework includes several commonly used GNN models that can be used o-the-shelf. For ML researchers, our APIs allow users to dene new models. In Listing 2, we provide some examples of user inputs to specify platforms, mini-batch samplers and GNN models via our APIs. Based on the given inputs, our framework generates the host program and synthesizable accelerator design. For example, based on the GNN model specied by the user, user decides the parameters in the host program template, and what aggregation function should be lled in the HLS template to generate the accelerator design. Based on the platform parameters, our DSE engine lls in the hardware congurations such as the unroll factor in our HLS template. In Listing 3, we provide an example that shows part of generated host program and synthesizable accelerator design. In Table 5 we show the resource utilization of the implementations generated by our design. The number𝑛which dentoes the number of Scatter PEs and Gather PEs are restricted to power of 2, and number of MACs𝑚is restricted to square of power of 2 due to the design of our accelerator. We evaluate the two optimizations of our data layout and internal representation described in Section 4.1 on a two-layer neighbor sampling GCN. The two optimizations are: (1) reducing memory trac (RMT) by reusing loaded vertex features in dierent edges that share the same source vertex; (2) reducing random access (RRA) by vertex renaming followed by edge sorting. We rst measure the throughput of the baseline implementation with no optimizations, and then incrementally apply the two optimizations. As shown in Table 6, both optimizations increase the GNN training throughput and can deliver up to 57% improvement in total. For evaluation, we use the throughput dened in Section 5 as metric, i.e. number of vertices traversed per second (NVTPS). To measure the throughput, we count the total number of vertices in each minibatch, and measure the average execution time of one training iteration. Table 7 shows the throughput comparison of GNN training across the three platforms. All three implementations use single precision oating point as data type. Table 7: Cross Platform Comparison (Throughput) Comparing with the CPU-only baseline, the CPU-GPU platform achieves 25.66×throughput on the average. This is because CPUGPU platform provides massive data parallelism with 5.27×peak performance and 14.5×memory bandwidth compared with the CPU-only platform (Table 3). Comparing with the CPU-only baseline and CPU-GPU baseline, the CPU-FPGA implementation generated by our framework achieves 55.67×and 2.17×throughput on the average respectively. Though CPU-GPU platform has higher memory bandwidth and peak performance than CPU-FPGA platform, the throughput is limited by the memory access overhead during aggregation stage. While our accelerator can access the onchip memory in one cycle (3.3 ns), CPU and GPU need to access the data in multi-level caches. Taking AMD Ryzen 3990 as an example, the L2 cache latency is 5 to 12 ns, and the L3 cache latency is around 32 ns. Moreover, as shown in Table 6, our data layout and internal representation also improves the training throughput by reducing the memory trac and reducing random memory accesses. We compare our results with two state-of-the-art GNN training implementations: GraphACT [28] and Rubik [6]. As shown in Table 8, our framework achieves up to 4.45×and 3.4×throughput respectively. Compared with GraphACT, the achieved speedup is due to (1) the vertex features are fetched directly from the FPGA local memory, (2) the proposed aggregate kernal has higher computation parallelism compared with Feature Aggregation Module in GraphACT. Compared with ASIC design Rubik, the obtained speedup is due to (1) larger on-chip memory of FPGA that can fully store the intermediate results under the setting of the experiments, (2) our proposed data layout optimizations that reduce the external memory trac and random memory accesses. We discuss the novelty of this work compared with previous work GraphACT [28] and the applicability of proposed optimizations to other platforms (e.g., CPU, GPU). Scaled from U200 to U250 using the number of DSPs. Comparison with GraphACT. In GraphACT [28], the redundancy reduction requires that all the edges have uniform weight value. Therefore, it can not support GCN [17]. In constrast, the proposed optimizations such as RMT and RRA do not have requirements on the weight, therefore, can support broader range of GNN models. Moreover, the Feature Aggregation Module in GraphACT has limited computation parallelism in feature-level that limits its performance for neighbor-sampling-based GNN training. In comparison, the proposed aggregate kernel adopts the scatter-gather paradigm with routing network, which can enable massive computation parallelism within feature aggregation. Optimizations on CPU/GPU platforms. In this work, we proposed a highly optimized aggregate kernel that adopts the scatter-gather paradigm to accelerate feature aggregation. While the scatter phase is optimized by our proposed data layout optimizaiton (Section 4.1), the performance of gather phase depends on the routing network of aggregate kernel (Section 4.2) that eciently routes the intermediate result from Scatter PEs to Gather PEs. On CPU/GPU platforms, the data layout optimization can potentially be adopted to optimize the scatter phase. However, the gather phase is hard to be optimized on CPU/GPU since the data communication among the computation cores is through a complex cache hierarchy. Since the scatter phase and gather phase need be optimized simultaneously, the proposed optimizations may lead to limited performance improvement on CPU/GPU platforms. In this paper, we proposed HP-GNN, a general framework to generate high-throughput GNN training implementation on a given CPUFPGA heterogeneous platform. Based on the high-level abstraction of GNN computation, we designed a host program template and hardware templates to support various GNN models. Our proposed data layout and internal representation improve the throughput of GNN training. The implementations generated by HP-GNN achieve 55.67×and 2.17×throughput compared with state-of-the-art CPUonly and CPU-GPU platforms. Compared with state-of-art accelerators, our framework achieves up to 4.45×throughput. In the future, we plan to extend our framework to multi-FPGA platforms by exploiting model parallelism. We would like to thank the anonymous reviewers for their comments which greatly improved the presentation. This work has been supported by the U.S. National Science Foundation under grant numbers OAC-1911229, CNS-2009057 and CCF-1919289. Equipment and support by Xilinx are greatly appreciated.