Abstract by constructing a penalty based on the data. If a list of penalties for a class of linear models is given, we provide comparisons based on sample size and number of non-zero parameters under a predictive stability criterion based on data perturbation. These comparisons provide recommendations for penalty selection in a variety of settings. If the preference is to construct a penalty customized for a given problem, then we propose a technique based on genetic algorithms, again using a predictive criterion. We ﬁnd that, in general, a custom penalty never performs worse than any commonly used penalties but that there are cases the custom penalty reduces to a recognizable penalty. Since penalty selection is mathematically equivalent to prior selection, our method also constructs priors. argue that predictive stability under perturbation is one of the few relevant properties that can be invoked when the true model is not known. Nevertheless, we study variable inclusion in simulations and, as part of our shrinkage selection strategy, we include oracle property considerations. In particular, we see that the oracle property typically holds for penalties that satisfy basic regularity conditions and therefore is not restrictive enough to play a direct role in penalty selection. In addition, our real data example also includes considerations merging from model mis-speciﬁcation. Keywords: prediction, penalized methods, shrinkage, oracle property, prior selection, genetic algorithm, evolutionary computation 1. Shrinkage and Prediction the sample size n, i.e., with p > n, are ill-posed and require some form of regularization to be solved. That is, some extra information must be added to ensure the existence of a reasonable solution. The earliest form of this is called Tikhonov regularization and was used initially for matrix inversion. In Statistics, ridge regression is probably the ﬁrst occurrence of Tikhonov regularization, see [1]. By the early 1990’s, L regularization was common in a neural networks context, see [2], not just to ensure that a solution existed but also to reduce variance. An important step forward was replacing the L see [3]. This shrinkage method is called the least absolute shrinkage and selection operator (LASSO). It provides a form of regularization that does variable selection as well as variance reduction while ensuring solutions exist. Another insight was the concept of an ‘oracle property’ (OP) ﬁrst proved for a penalty called the smoothly clipped absolute deviation (SCAD) in the context of linear models; see [4]. The OP meant that asymptotically, as n → ∞, the parameter estimates from the SCAD penalty behaved asymptotically as if the correct p were known, i.e., the parameter estimates either went to zero or were consistent, asymptotically normal, and eﬃcient according to whether the variables they represented were not or were in the true model. Choosing a shrinkage method can be done by selecting a penalty from a list of pre-speciﬁed penalties or The techniques and recommendations we oﬀer are intended for ﬁnite sample cases. In this context, we In the context of linear models, inference problems in which the number of parameters p is bigger than methods, representing individual priors and penalties, for the purposes of parameter inference. Here, we want to choose a penalty or prior from a class of penalties or priors. One way to do this is to ﬁx a list of shrinkage methods, ﬁnd a basis for comparing them, and choose the best. Alternatively, we can take a ‘build-your-own’ approach and construct a shrinkage method from part of the data and use it on the rest of the data. In this way we can choose the optimal shrinage method adaptively. Here, we provide both a comparison of ‘oﬀ the shelf’ methods and a build-your-own technique (based on genetic algorithms) under a predictive optimality criterion. The build-your-own shrinkage methods never perform worse that the oﬀ-the-shelf methods, but may return an oﬀ-the shelf method as optimal. We give an example of each case. matrix of design values, and  = ( have a data set D = {(y where L shrinkage arises from the fact that as λ → ∞, each β of (1.1)is replaced by a log-likelihood; in this case and if the likelihood is normal, L error. If L where w hand side of (1.2) may be replaced by a log-likelihood. Also, the dependence on w be more complicated; we have represented the adaptivity of the constraint to the data as multiplicative in the penalty term for the sake of convenience. We regard the SCAD and minimax concave (MCV) penalties ([5]) as adaptive because their penalty terms or constraints are data driven even though they only introduce one extra non-multiplicative parameter (and have the OP). This is not simply parameter counting. Note that the elastic net (EN), [6], introduces two parameters and is not adaptive while the adaptive EN (AEN) is adaptive and has the OP; see [7]. how to estimate the w to set ˆw extra parameter or from linear models when n is large enough) and choose γ by a cross-validation criterion. Another method, due to [10], sets ˆw estimator. This requires n > p unless some auxiliary technique is used to ﬁnd the SE. [10] argued that their method works well in cases where there is high collinearity. In practice, for both methods, γ = 1 is used to avoid extra computation. Here, we have exclusively used the Zou method since it does not require n > p and has a nice interpretation: As Roughly, shrinkage methods segregate into those that are nonadaptive, i.e., introduce exactly one ‘decay’ parameter and usually do not satisfy the OP, and those that introduce two or more decay parameters and often satisfy the OP. We will see that, contrary to initial impressions, the oracle property is not rare. Over the last two decades, numerous shrinkage methods have been proposed and studied as individual More formally, write a linear model (LM) of the form Y = Xβ + where Y = (Y, . . . , Y), X is an n×p and Lare loss functions, xis the i-th row of X, and λ is the decay parameter. The term By contrast an adaptive shrinkage method typically gives estimates of the form = (w, . . . , w)and the w’s are weights on the individual β’s. Again, the ﬁrst term on the right As written, expression (1.2) introduces p new parameters, the w’s. When n is not large, it is unclear = 1/|ˆβ|whereˆβis anyn-consistent estimator of β(e.g., from SCAD that only adds one Formally, the OP is the following. Write β = (β, β) where βrepresents the zero components of β.√ ˆβ corresponds to the mode of the posterior in which L This means that penalty selection is mathematically equivalent to prior selection. However, the equivalence is only mathematical because the class of reasonable penalties is a proper subset of the class of reasonable priors. In particular, most reasonable penalties are convex – our method in Sec. 5.2 assumes convecity, for instance – but priors do not have to be log-convex. uses n → ∞. This is partially ameliorated by results that give analogs of the OP where p increases much faster than n does; see [11]. However, the cost of amerlioration is often artiﬁcial conditions on the parameter space and/or design matrix. Moreover, many of the original examples given to verify that shrinkage methods were eﬀective actually had n > p and took p = 8, see [4], [8], and [9]. To the best of our knowledge no systematic comparison of shrinkage methods for diﬀerent relative sizes of n and p has been done. This paper seeks to ﬁll this gap and provide recommendations for when diﬀerent methods work well. ˆβ for a given shrinkage method we deﬁne the predictor for Y (x) at some new value x. Then we evaluate how well perturb the data using the technique of [12]. The idea is to add N (0, τ use part of the data to form a predictor and the rest of the data to evaluate the predictor. We do this by generating ‘instability’ curves, basically L curves with low values, that are smooth, and increase slowly with τ . provides an assessment that captures analogues of both variance (from the instability curves) and bias (from the accuracy measures). We regard instability as more important because even if a variable is incorrectly included its contribution may be small if its coeﬃcient is near zero and if it is incorrectly excluded the bias should show up in the instability curve. for four values of n when p = 100. Very roughly, for n << p we ﬁnd that ridge regression (RR) works comparatively better but that, as sparsity increased, EN was often best. As n increased but remained less than p, EN improved relative to RR and was often the best. When n > p, LM’s were usually best with little or no perturbation of the data but performance rapidly deteriorated as perturbations or sparsity increased. Also, a version of SCAD often performed best. For penalty selection in practice, however, these conclusions must be tempered by the more detailed analyses given in Sec. 3 where we include considerations from variable selection and the intuition developed in Sec. 4 to address model mis-speciﬁcation So, we needn’t limit ourselves to the speciﬁc shrinkage methods that have been studied. When n > p, we search a class of shrinkage methods that have the OP; when n < p we search a class of shrinkage methods that do not necessarily have the OP; see Subsec. 5.2. In either case, we generate priors (or penalties) that are optimal for a given data set, again in a predictive instability sense. In practice, this means we can use a technique such as genetic algorithms (GAs) to optimize a ﬁtness function corresponding to good prediction to ﬁnd optimal priors. We ﬁnd that GA optimization for use in prior selection beneﬁts from choosing point estimates for the β Sec. 2 carries over to this setting. That is, we ‘shrink’ to estimated values in the James-Stein sense to get shrinkage estimates in the penalized likelihood sense. then apply predictively. As seen in Sec. 5, using both GAs and shrinkage (when we can) together gives better predictive performance. By optimizing over the prior in this way we are eﬀectively optimizing over We note that (1.1) corresponds to a joint density ρ on the data and β. Indeed, exponentiating gives that Even though shrinkage methods were originally introduced as a way to solve the n < p problem, the OP Our comparison is in terms of predictive stability and accuracy of model selection. That is, after ﬁnding We also use more conventional accuracy measures for variable selection. We argue that pairing the two A ﬁrst contribution of this paper is the use of predictive stability to compare ten shrinkage methods A second contribution of this paper is to observe that the OP is actually relatively common; see Sec. 2. Thus, a third contribution of this paper is the use of GAs and part of the data to form a prior that we the penalty and hence choosing the optimal shrinkage method. This optimum may or may not coincide with an established shrinkage method but will still have the OP when n is large enough. Thus, we have chosen our shrinkage method to be predictively optimal for our data. In fact, we are de facto using GA’s to obtain an approxiamtion to the posterior based on the ﬁrst part of the data for use with the second part of the data. We verify in examples that this is predictively better than simply using all the data to make predictions. wide range of adaptive penalties. We present our comparisons of existing shrinkage methods in Sec. 3 and provide general recommendations on which to use in various settings. We compare our recommendations to the results for a benchmark data in Sec. 4. In Sec. 5, we present our GA optimization verifying that the theory in Sec. 2 holds and the result of the GA is optimal given the data. We summarize our overall ﬁndings and intuition in Sec. 6. 2. Theoretical Results diﬀerent penalty functions on diﬀerent parameters. This is diﬀerent from [11] who did not treat diﬀerent penalty functions on diﬀerent parameters or adaptivity. Their main result concerned the asymptotic equivalence of penalized methods and permitted p to increase with n. Like other papers that allow p to increase with n, some of the conditions appear artiﬁcial. For example, aside from truncations of the parameter space, one must assume that there is a sequence of explanatory variables such that if one of the early variables is correct and accidentally not included it can be reconstructed from later explanatory variables in the sequence, thereby sacriﬁcing identiﬁability. Our result, like many others, assumes either ﬁxed p or p increasing so slowly with n that the required convergences hold. 2.1. General Penalized Log Likelihood where x regression coeﬃcients, and  β ∈ Ω ⊂ R such that the six regularity conditions stated below are satisﬁed. Let L(β|x of the observations (x Below we state our six regularity conditions. The structure of this paper is as follows. Sec. 2 gives general conditions for the OP to hold for a Our proofs are motivated by the techniques in [4] and [9]. We assume an adaptive setting and allow = (x, . . . , x)is the p-dimensional covariate, β = (β, . . . , β)is the vector of associated = 0 for j > pfor some p≥ 0. Here, we regard the xs as deterministic. When needed we write Let (x, Y) for i = 1, . . . , n each have density ρ(Y|x, β) (with respect to a ﬁxed dominating measure) Here we write xto mean x, . . . , xfor ease of notation. Let inite uniformly in i, i.e. is bounded above and below. Also for some B, b > 0, we have BI≥ I(β|x) ≥ bI. Condition 4 There exists an increasing sequence of compact sets C = C Theorem 2.1. (Oracle Property) Assume Conditions 1–6 hold. Suppose and g(n) → ∞. Then, the estimator N(0, I 5. We can use the same function f parameter. Thus, we are also not restricted, at least theoretically, in our choice of the shrinkage methods except perhaps by requiring the OP. 2.2. Penalized Empirical Risk a more general setting because we allow a wider range for the function of the data as well as an arbitrary penalty function. notation for x We specify some similar but slightly diﬀerent conditions before getting into the proofs. These conditions are similar to those in Section 2.1, but they are diﬀerent and necessary for the cases we discuss in this section. all j, ` = 1, . . . , p, we have that ∀β ∈ Ω, ∀xand as η → 0, where B(β, η) is the ball centered at βwith radius η > 0 in Euclidean distance. where I(β|x) is positive deﬁnite. constants M = M∈ Rsuch that for all n, sup|f(β)| ≤ M. That is, the penalty term has a uniformly bounded ﬁrst derivative in j. we have f(0) = supf() = 0 uniformly in j. f(β)h + o(1) uniformly in j where supo(1) → 0. Our main result for penalized likelihoods is the following. =where→ 0 and h(n) → ∞, and suppose√nb→ ∞ in which b=where→ 0 (β|x)), where βis the component of β containing all the nonzero elements. For proof see Supplement A, Sec. 7.1. Therefore, we can construct an oracle procedure by choosing any f(β) that satisfy Conditions 4 and We now extend the OP results for penalized likelihoods to penalized empirical risks. This is arguably Consider the same regression scenario as Section 2.1 and a distance d(y− xβ). Let R(β|x) = d(y− xβ) be the empirical risk of the observations (x, Y), . . . , (x, Y) where xis shorthand We see that I is not necessarily the Fisher information matrix; however, if our empirical risk happens to correspond to a log-likelihood, then it is possible that I Condition 8 There exists an increasing sequence of compact sets C = C Condition 10 There exists uniform Taylor expandability for f Again the notion of uniform Taylor expandability in j here requires the error terms o go to zero at the same time. Lemma 1. Let d(u) be an even distance function with a unique minimum at 0. If u comes from some distribution with pdf f Theorem 2.2. (Oracle Property) Assume Conditions 7–10 and Lemma 3 hold. Suppose and is, for all j, ` = 1, . . . , p we have that ∀β ∈ Ω and as η → 0, and for some B, b > 0, we have BI≥ I(β|x) ≥ bI. In other words, I(β|x) is positive deﬁnite and we abbreviated it to I(β). constants M = M∈ R such that for all n, sup|f(β)| ≤ M. That is, the penalty term has a uniformly bounded ﬁrst derivative in j. we have f(0) = supf() = 0 uniformly in j. f(β)h + o(1) uniformly in j where supo(1) → 0. The following lemma identiﬁes the class of distance functions we consider. (u)f(u). Then g(u) is an odd function, and we haveg(u)du = 0, so Note that Lemma 1 is true for any even distance function d(u), and this is useful for us because we focus Our main result for penalized empirical risks is the following. satisﬁes a=such that→ 0 and h(n) → ∞, and suppose√nb→ ∞ where bsatisﬁes =such that→ 0 and g(n) → ∞. Then the estimatorˆβ = (ˆβ,ˆβ)satisﬁes P (ˆβ= 0) → 1 Previously it seemed oracle procedures were rare, isolated choices of priors. Now, even though we have not characterized the class of all oracle procedures, we can see that the conditions for a procedure to have the oracle property are quite general, allowing a large range of likelihoods, distances, and priors. given context. We propose choosing a ’best’ oracle procedure by optimizing a stability criterion over both a class of penalty functions and a class of distance functions. Allowing for f each parameter, as long as they are uniformly Taylor expandable and have similar properties, is powerful because we can choose variable-dependent penalties. This is a more general sense of adaptivity than each β merely having its own shrinkage parameter λ like |β Here we focus on comparing existing methods and then using genetic algorithms to ﬁnd optimal choices for the f 3. Computational Comparisons where the estimate known that many shrinkage methods (LASSO, EN, etc) zero-out coeﬃcients β as well as estimation. Here, we look only at the instability of predictive error and the accuracy of variable selection. data by D where we used τ generally increase with τ. Of course, we prefer instability curves that are small – less instability upon perturbation suggests a better predictor. However, if a instability curve decreases with τ then perturbation of Y is making the predictor more stable. We take this to mean the predictor is discredited for some reason. We suggest this behavior arises when the predictor has omitted or included terms incorrectly or has poorly chosen coeﬃcients. We prefer predictors with instability curves that are lower than the instability curves of competing predictors and smoothly increase slowly with τ. instability curve and a (large) number L for the number of iterations to be averaged. For each k = 1, . . . , K, let ` = 1, . . . , L. Instability curves for a given predictor can generically be formed by the following steps. 5. Let S( For proof see Supplement A, Sec. 7.2. Taken together, Theorems 2.1 and 2.2 give us insight into how large is the class of oracle procedures. Since there are obviously many oracle procedures, the key question becomes which one to choose in a |because it is data driven. We have not done this here, and we leave it for future work. (β)’s. Every shrinkage method for linear models generates a predictor of the form (1.3) ,i.e.,ˆY (x) = xˆβ Following [12] we add random N(0, τ) noise to the y’s in D and denote the partition of the perturbed (τ) = D(τ) ∪ D(τ). For any predictorˆY , we deﬁne its instability to be ˆYmeans we have formed a predictor using D(τ). In the computations we present in this section = .2k, k = 1, . . . , 8, to generate instability curves of the form (k, S(ˆY )), and looked for patterns. Intuitively, perturbing the Y ’s by adding normal noise should only increase S(ˆY ), i.e., in curves should Our basic computational procedure is as follows. Fix a number K of values of τ to form the points on the respectively. 6. For each predictor and k, ﬁnd the sample mean: 7. Plot S where the rows X or are IID ∼ t dim(Y ) = n, dim(X) = n ×p, and dim() = n. We use two choices for the distribution of  to represent light and heavy tails in the error, respectively. We are concerned mainly with the case p < n, but include cases p > n for completeness. the shrinkage methods had the OP, namely, LAD-LASSO (uses L 2007), ALASSO (Zou 2006), SCAD1 (L Zhang 2009), ASCAD1 (Dustin 2020: kY − Xβk β), SCAD2 (replace L OP, namely ridge regression (RR), LASSO and EN. First, for LASSO, RR, EN, ALASSO, AEN, we used the glmnet package (see [13]) in RStudio Ver. 1.2.5033. Second, for LAD-LASSO, SCAD1, and ASCAD1 we used the rqPen package in R, see [14]. The speciﬁc function used (cv.rq.pen() ) requires X to be nonsingular so we only implement these methods (as well as LM) in the n > p case. Third, for MCP and SCAD2 we used code that implemented a local linear approximation (LLA). code requires an explicit separation of D portion of the data is set aside for estimating λ. However, both glmnet and rqPen approximate λ internally and we use the default methods of the procedures, so the splitting of D programs. Below we list the choices we made for the explicit splitting. we do not have enough data to implement OLS, i.e. p > n, we used 1/| can be done, at least in principle, using any behind this choice comes from the assumptions on a sparsity levels,10%, 50%, and 90%, and four sample sizes n = 40, 75, 150, 500. For each n we let L = 100 for the instability computations. That is we average over 100 datasets to get a instability value for each perturbation level. We use a training data set to form the predictor and a testing data set to evaluate its performance, and for the various sample sizes we set (D (350, 150), respectively. For the LLA methods we allocate 10% of the D remaining 90% to D be trained with exactly the same data, and evaluated on a hold out set that has not been used in the training process. levels. The ﬁnal subsection summarizes our recommendations before we turn to a benchmark data example in Sec. 4. We compared predictors from ten diﬀerent shrinkage methods as well as a full linear model. Seven of Finding the instability curves for the ten shrinkage methods required three diﬀerent pieces of software. Because we use diﬀerent packages, we had to split the data diﬀerently for diﬀerent methods. The LLA As noted in Sec. 1, we followed [8] for the adaptive methods and chose ˆw= 1/|ˆβ| for p < n. When We focus here on a high dimensional setting of p relative to n. Thus, we set p = 100 and consider three The next four subsections present our simulation results for the four sample sizes and three sparsity 3.1. Sample Size n = 40 how good the variable selection, estimates, and thus predictions are for various sparsity levels where sparsity means the percentage of zero coeﬃcients in the model used to generate the data. predictor to see which variables were included correctly or incorrectly. These two assessments are roughly analogous to variance and bias. However, as noted in Sec. 1,we think of instability curves as more important because they reﬂect ‘variability’ and partially reﬂect bias: If a predictor includes a variable incorrectly it can still be downweighted by small parameter estimates and if the predictor fails to include a correct variable (that is important) the instability curve can increase to detect it. the tails, as sparsity increases the methods become more stable. We observe two clusters of methods. Namely, the non-adaptive methods in the lower cluster (L,EN,RR) and the adaptive methods in the upper cluster(AL,AEN,SCAD2, MCP). This is consistent with the fact that we have far fewer observations than predictors, so estimating the extra hyperparameters accurately is diﬃcult. Overall, this ﬁgure suggests that RR and EN perform best for sparsity levels 0.1 and 0.5 while EN is best for sparsity 0.9, although RR is not discredited, at least for light tails. that there is not enough information in the small sample relative to the number of predictors to set any coeﬃcients to zero. Thus RR performs well because it does not exclude any variables. EN, a mixture of LASSO and RR, also performs well because it may set fewer coeﬃcients to zero than other non-adaptive methods. curves decrease markedly as the perturbation increases suggesting they have chosen poor models. Also, RR is smallest at perturbation zero. models. Also, it is seen that there are some jumps in the curves. These are usually at the beginning when the perturbation changes from zero to a positive number. We suggest these jumps indicate a lack of stability under perturbation meaning that the shrinkage method they represent is unstable in terms of choosing a good predictor. 1 compares each of the methods for both light (Li) and heavy (H) tails based on the total percentage of coeﬃcients the methods sets to 0 on average (Tot; ideally equal to the sparsity level), what percentage of those set to 0 were truly 0 (Tr; ideally 100%) and what percentage of those set to 0 were truly non-zero (Fa; ideally zero). Note that we are focused on the sparsity in this setting, so we frame these tables in terms of the percentage of variables excluded by a method. Instead, if we wish to describe the variables included, we simply subtract each entry in the table from 1. low and medium sparsity are much too high which leads to high values in the Tr columns, trivially. The high values in the Fa columns indicate that all methods are typically excluding far too many terms. There are some very mild exceptions – SCAD2 or MCP – that perform noticeably better than other methods but even so their performance is poor. For high sparsity the methods do exclude fewer correct variables than for low sparsity. Table 1 agrees with Fig. 1 in that variable selection is generally worse for heavier tails. Curiously, the better performing methods in terms of variable selection are diﬀerent from the better performing methods in terms of instability curves. We attribute the diﬀerence to how well coeﬃcients are estimated rather than to how well variables are selected. Indeed, Table 1 shows that all methods usually selected far fewer variables than necessary, e.g., for L with .1 sparsity, ﬁve variables were included on average but 90 should have been. X ∼ MV N plots for these cases, howevver we give our recommendations in Subsec. 3.5. Our ﬁrst example uses the value n = 40 that is relatively small compared to p = 100. Here, we examine We use two assessments for this. First, we generate instability curves. Then we also look ‘inside’ the Fig. 1 shows that there is overall more instability with heavy tails, and, independent of heaviness of This is seen for both light and heavy tails. A possible explanation for the good performance of RR is It may seem odd to say RR is not discredited in the upper right panel of Fig. 1. However, all the other The curves increase after the perturbation is large enough that it outweighs the poor selection of the Next, we look at the predictors themselves, on average, to help us interpret the instability curves. Table From Table 1, it is seen that none of the methods performs well. The values in the Tot columns for We also considered two cases where the x’s have a nontrivial dependence structure. Speciﬁcally, we set analogous to those in Fig. 1 but for tridiagonal variance matrices, we found that for low and medium sparsity, EN and RR were typically best and close to each other. For high sparsity we got the same results as in the upper right panel of Fig. 1. When the variance matrix of X was Toeplitz, we found that EN is performing best for all low and medium sparsity levels. For high sparsity SCAD2 and MCP performed nearly the same as EN. Interestingly, SCAD2 and MCP were amongst the worst performing methods in the independence case but with the Toeplitz structure they were only slightly worse than EN. We suggest that the degree of dependence in the tridiagonal case is not much more than in the independence case. We also suggest that when there is high enough dependence e.g., the Toeplitz case, penalties that have some data dependence (but not too much) such as EN, SCAD2, and MCP are best able to generate predictions. Toeplitz were similar to the independence case across all methods and sparsity levels except for EN in the Toeplitz case where EN performed noticeably better than the other methods across all sparsity levels. In addition, L does well in the high sparsity case. We suggest that being able to choose what the penalty looks like as in EN gives some advantage over the other methods. We return to this point in Sec. 5. 3.2. Sample Size n = 75 tory variables, but n is much closer to p than in Subsec. 3.1. Examining the instability plots in Fig. 2, we see that as in Fig. 1, the methods become more stable as sparsity increases and less stable as the tails become heavier. (better) cluster with AEN, AL, MCP and SCAD2 as the worse (upper) cluster. EN outperforms all other methods with low and medium sparsity regardless of the heaviness of the tails. For high sparsity all methods are roughly comparable except for RR which performs poorly. That is the n = 40 and n = 75 cases are qualitatively similar apart from the light tail high sparsity case with n = 40 where model instability seems to dominate. Speciﬁcally, for n = 40 EN was the ‘best’ although there was good reason to prefer RR, but for n = 75 all methods but RR are essentially indistinguishable and RR performs poorly. predictive error may converge to σ, the SD of . sparsity. For high sparsity, all methods were very much improved. However, L and EN were the worst – the only methods not having the OP. The other methods, AL, AEN, SCAD2, and MCP, have the OP and perform roughly equally well. As a generality, the methods performed better for light tailed than for heavy As in the independence case, the methods performed generally better as sparsity increased. For ﬁgures In terms of variable selection as indicated in Table 1, we found that the results for the tridiagonal and Next we consider a second example where p > n. That is, we still have fewer observations than explana- Similar to the n = 40 case, we have the same clusters of methods, namely EN, RR, and L as the lower It is seen that the error decreases as both the sparsity and n increase. Accordingly, as both increase, the Table 2 shows that, as with Table 1, no method performs variable selection well for low to medium tailed distributions. As before, the better performing methods in the instability curves (EN, RR, L) tended to perform worse in terms of variable selection and we attribute this to better parameter estimation in the instability curves since the methods generally included more variables than necessary. That is, a variable may be included with an estimated coeﬃcient near zero. Namely, the instability curves show that EN performed best at all sparsity levels, roughly tying with most other methods for high sparsity. The key diﬀerence was that RR tended to perform poorly overall. When we switched to Toeplitz matrices, across all sparsity levels SCAD2 was best with MCP being a close second and virtually tying with SCAD2 with high sparsity. EN, which was overall best for n = 40, was noticeably worse than both SCAD2 and MCP across all sparsity levels. We suggest that since we have more data, the optimality of the methods that have optimality properties (other than the OP) is eﬀective. better. This was surprising and diﬃcult to interpret. In the Toeplitz case, EN, SCAD2 and MCP are noticeably better than the other methods for low and medium sparsity. For high sparsity, L also performs well. This is the same as the Toeplitz case with n = 40. Thus, overall, the results for dependence cases with n = 75 are very close to the corresponding results for n = 40. 3.3. Sample Size n = 150 Here we implement LM’s as well as shrinkage techniques. Note that the “penalty” associated with LM’s is identically zero and corresponds to a uniform prior. low sparsity, LM’s start out giving the best results in predictive instability. However, the instability curve for LM’s rises faster than for its competitors and arguably SCAD1 is equivalent. For medium sparsity, LM’s is roughly in the middle and SCAD1 is the best. For high sparsity, SCAD1 ties with the other top methods. LM’s perform worse as sparsity increases. As in earlier cases, all methods that have nontrivial penalties perform better for light tails than heavy tails and as sparsity increases. Also, as the sparsity level increases, the variability amongst the methods decreases. Note that RR tends to perform poorly since there is enough data that the sparsity has an eﬀect. generally performing better than the nonadaptive methods. For instance, AEN and AL are performing better for small perturbations than EN or L, respectively. When the perturbations are too high, it makes sense that the non-adaptive version of a penalty will perform better that their adaptive versions because they are less aﬀected by the noise; they use fewer estimators. One can argue that the perturbation level at which the curves for non-adaptive penalties and their adaptive versions cross represents the largest reasonable perturbation that should be considered for that penalty. Moreover, the OP is not a determining factor for performance: Some methods with the OP perform well and some do not. Some methods that do not have the OP perform better than other methods that do. variable selection. For medium sparsity they remain the best but MCP and SCAD2 are almost as good. For high sparsity, AL, AEN, ASCAD1, SCAD1, SCAD2, and MCP are essentially the same. Note that all methods have zero in the Fa column meaning they never exclude variables that are important For heavy tails, the results for low sparsity are the same but for medium sparsity more methods are nearly as good as the AL and AEN. For high sparsity, the only methods that are lagging in performance are L, EN, and L-L; the others are essentially equivalent. that the adaptive methods improved more. This is true for the instability curves as well. LM’s and RR are not included in Table 3 because they don’t do variable selection. although not necessarily in variable selection. In Table 3, SCAD1 never excludes a variable that should be included. Thus, its lesser performance in terms of variable selection may be ameliorated by its parameter estimation. When we included dependence via tridiagonal matrices, we found results similar to the independent case. In terms of variable selection, the tridiagonal case was similar to the independence case but slightly This is our ﬁrst case where n > p, making it qualitatively diﬀerent from the earlier two subsections. Fig. 3 parallels Figs. 2 and 1, but introduces four extra methods, LM’s, SCAD1, ASCAD1, and L-L. For A key diﬀerence between the n = 150 case and the n = 75, 40 cases is that the adaptive methods are For light tails and low sparsity, Table 3 shows that AL and AEN are the best methods in terms of Compared to Table 2 we see that all methods improved in variable selection, which is not surprising, but Comparing the conclusions from Fig 3 and Table 3, we see that SCAD1 performs best predictively tridiagonal case, the instability curves and the variable selection table are qualitatively the same as for the independence case. For the Toeplitz, LM’s, SCAD1, SCAD2, and MCP are the only methods that perform well in terms of instability curves. In the corresponding table, overall SCAD2 and MCP did best, but sometimes another method does best (but then does poorly in terms of instability). Overall, variable selection in this case is worse than in the independence case. This is virtually the opposite of Toeplitz in the n < p case where variable selection was improved. This case is more in line with intuition because intuition corresponds to n > p. 3.4. Sample Size n = 500 Fig. 4 and Table 4 have the same general properties as the earlier Figures and Tables, namely, as sparsity increases instability decreases. The methods improve as sparsity increases although the improvement is not as dramatic as in the smaller sample cases. In the heavy tailed cases, there is more variability. purposes it is easier to identify the methods that perform poorly rather than the ones that perform well. From the top row in Fig. 4, the only poor methods are RR, ASCAD1,L-L for low and medium sparsity. For high sparsity RR is clearly worst. From the bottom row in Fig. 4, RR is clearly the worst in all cases. For low sparsity LM and SCAD1 (and nearly SCAD2) are best. For medium sparsity only L-L (and RR) performs poorly. For high sparsity the worst performers are RR and LM’s. Note that L and EN still perform well even though they don’t have the OP. However, EN is a generalization of L and L has some consistency properties, see [16], so the good performance of L and EN is not surprising. Again, we considered two dependence cases with light tails, the tridiagonal and the Toeplitz. For the For completeness we also consider the case n = 500 to identify the limiting behavior of the methods. In fact, many of the methods at this point are indistinguishable via Fig. 4 or Table 4. So for descriptive Table 4 shows that at n = 500, most methods are performing variable selection quite well. In fact, almost all the methods used that have the OP are nearly perfect, on average, in performing variable selection. The two exceptions are LL and ASCAD1 which tend to include too many variables. That said, these two methods do not often exclude variables that are important, which allows them to perform comparably well predictively. Even the methods that look worse in terms of variable selection (L, EN) predict well because they retain all the important variables. The only methods this table suggest could be ruled out are in the low or medium sparsity case: L, EN, and L-L. have low entries in the Tot and Tr columns. (They also have near zero entries in the Fa column.) However, our comparison is predictive not based on variable selection and these methods perform generally well in a predictive stability sense. It is likely that the coeﬃcients of the incorrectly included variables are quite small. of the predictive error (σ = 1 ). For the heavier tailed methods, the smallest errors are below two. Since the error terms here are t lower limit. (This is seen in earlier cases but usually only for the highest sparsity.) 3.5. Summary Tables and Recommendations p > n, the instability graphs are more important that the tables because the variable selection is so poor for all methods that we must rely solely on choosing the method that predicts the best (or least bad). Thus, for n = 40 we just consider the ﬁgures in choosing the best methods. For n = 75, unless one speciﬁes a relative weighting among Tot, Tr, and Fa, there is usually no clearly preferred method. In these cases, we have used the instability curves to select from amongst the numerically best shrinkage methods. variable selection tables to make recommendations. That is, we choose the methods that are both selecting variables most appropriately and have the lowest instability generally. Again, there are cases where we have to weight Tot, Tr, and Fa. We continue to believe that Fa is relatively more important than Tot and Tr and that Tot is relatively more important than Tr. selection tables cannot be known. Accordingly, it is only the instability curve that is available. Thus, Note that in this case, the errors at zero of the light tailed methods approach the theoretical lower limit To specify our recommendations we assume the following, elaborated from Subec. 3.1. First, when Second, for larger sample sizes, and low to medium sparsity cases, we use both the instability curves and Two limitations of our recommendations are that in practice i) the level of sparsity and ii) the variable our recommendations are based primarily on the instability curves but taking into account the variable selection tables. Moreover, there are other limitations that a potential user should take into account e.g., our restriction to linear models. Subsec. 3.1). Recall that for n = 40 no method worked particularly well. However, the overall best performing of these poor methods was RR. in Subsec. 3.2). EN was typically the preferred method for low to medium sparsity. For high sparsity, all methods besides RR performed well. Interestingly, we found SCAD2 or MCP performed best when there is strong dependence as in the Toeplitz structure. low sparsity, LM’s were generally good. For medium sparsity, the conclusions did not follow a strong pattern. Several methods performed roughly equally well and tridiagonal was, as in the earlier cases, similar to the independence cases, except here for medium sparsity. With stronger dependence, the biggest change from the other cases was that SCAD1/SCAD2/MCP performed best like the medium sparsity case for tridiagonal. We also see that generally, as sparsity increases, LM’s do relatively worse. it is safe to assume the asymptotic properties have kicked in. Thus, we see several methods performing well regardless of whether they have the OP. Indeed, it is very hard to chose a single best method. There are, however, several methods that essentially never do well, namely, RR, LL, and ASCAD1. Further LM also does relatively poorly as sparsity increases. As such, we recommend not using these methods. Finally, we see from the two dependence cases that dependence has a very small eﬀect at most when the sample size is this large. Indeed, it is the low sparsity with heavy tails that stands out somewhat from the rest. except for high sparsity. Fortunately, at this sparsity level all shrinkage methods except for RR are essentially equivalent. LM’s are sometimes are not very good in the heavy tailed case but, again, this mainly occurs for high sparsity as shrinkage methods set zero coeﬃcients to zero faster than LM does. We begin our recommendations for small sample sizes with Table 5 (based on our computations from Recommendations for the next sample size, n = 75, are given in Table 6 (based on the computations Table 7 summarizes our recommendations for n = 150 (based on the computations in Subsec. 3.3). For Table 8 summarizes our recommendations for n = 500 (based on the computations in Subsec. 3.4) where We conclude with some general observations. In the n > p case, we observe LM’s often perform best and include only the relevant variables. This is generally true regardless of the sample size. That is, we are observing a sort of consistency under increasing sparsity that occurs even for sample sizes that are not asymptotic. uniquely so, regardless of the heaviness of the tails – as long regularity conditions e.g., conditions 2, 3 and 4 in Subsec. 2.1 on moments, are satisﬁed. 4. Corroboration on Real Data set Superconductivity presented in [17]. This data set has 81 explanatory variables of a physical or chemical nature to explain a response Y representing temperature measurements (in degrees K) for when a compound begins to exhibit superconductivity. Initial data analyses suggested the data were sparse, but it was unclear how sparse. [17] suggests a sparsity level of about 90% and our techniques here conﬁrm this in the sense that we ﬁnd, if a linear model is ﬁt, around 90% of the coeﬃcients will be zero and this will be nearly best possible from a predictive standpoint. In addition, [17] implicitly used light tails in the error term, , and did not comment on the distribution of the explanatory variables apart from eﬀectively taking them as independent and not requiring any special treatment to account for spread. Accordingly, we treated these as coming from a light tailed distribution. of them being important because the variable importance factors decreased suddenly at the eighth most important variable. This gives 7/81 < 10%, conﬁrming this case corresponds to the high sparsity setting. Histograms of the residuals from the full LM suggest this falls into the light tail case as well. Thus, we compare our computed results in this section to the recommendations for the light tailed high sparsity cases treated in Subsec. 3.5. LM as a ‘benchmark model’ and then improved on it by developing an XGBoosting model – a boosted, penalized tree model in which the penalty was carefully constructed to be appropriate for trees. LM’s for their interpretability, Also, when n << p, XGBoosting often does not perform well. So, it may sometimes be reasonable to use shrinkage techniques in mis-speciﬁed model situations with small sample sizes. and 500 data points at random so comparisons with our recommendations here would be fair. We note that many data sets are much smaller than Superconductivity so our example here is intended to be suggestive for them too. simulated data with the randomly chosen subsets of Superconductivity. We were able to generate instability curves but not the variable selection accuracy tables because the true model is unknown. The instability curves for Superconductivity are given in Fig. 5. in Subsec. 3.5. For n = 40, the upper left panel in Fig. 5 shows that RR and EN are the best shrinkage We observe that the higher the sparsity, the better the methods exclude variables that are not relevant That said, as sample size increases, methods with the oracle property do indeed emerge as best, if not As a test of our recommendations in Subsec 3.5, we used the same shrinkage methods on the data Furthermore, [17] identiﬁed 20 variables of potential importance. Of those 20, we identify only seven In fact, the full Superconductivity data set had n = 21263, so [17] was able to use a standard (unpenalized) Here, as is common in pratice, especially where a more justiﬁable methodology is infeasible, we have used Since Superconductivity is so much larger than the data sets used in our simulations, we drew 40, 75, 150, We repeated the analyses presented in Sec. 3 for the independent cases with light tails but replaced the For each sample size, we compare the best methods from Fig. 5 to the corresponding recommendations methods. This is the same as recommended in Table 5 for sparsity .9 and light, independent tails. For n = 75, the upper right panel in Fig. 5 shows EN is the best, followed by RR and L which are noticeably worse. Table 6 shows only that RR should not be used with light, independent tails. So, again we see agreement even if the recommendation was not speciﬁc. and RR. Table 7 indicates that RR and LM’s are to be avoided (for light independent tails). So,the good performance of RR disagrees our recommendations. Finally, for n = 500, the lower right panel in Fig. 5 shows that ﬁve methods form a cluster of the best of the 11 methods. The cluster of top methods is ALASSO, L, AEN, and RR. The recommendation from Table 8 is not to use RR or LM’s. Again, we have a disgreement on the use of RR. LM. Indeed, [17] ends up proposing a model based on trees. The agreement between our recommendations and the data analysis for small values of n probably means that the sample size is too small to detect the diﬀerence between the true model and a LM. However, when the sample size increases, the model misspeciﬁcation matters. RR normally performs well for non-sparse cases but here is performing well when the By contrast, for n = 150, the lower left panel in Fig. 5 shows that EN is best, closely followed by L We explain these ﬁndings by model mis-speciﬁcation. First, the true model is almost certainly not a true model is sparse. sparse non-linear model than a sparse linear model does. The analogy is to imagine representing a single true tree model with a single linear model. The linear model would have to have many terms to approximate a tree even one with relatively few nodes. That is, a large enough LM might provide a good approximation. LM’s, we could end up with diﬀerent recommendations and if the model class contained the true model for the Superconductivity data we would expect our recommendations to match the data analyes. 5. Optimizing Over the Shrinkage Method there are inﬁnitely many other penalties that could be used to get shrinkage methods with the OP. Recalling that penalties are special cases of priors, it is clear that the recommendations for choice of shrinkage method given in Sec. 3 are limited. Here we propose that, rather than choosing a shrinkage method from a list of options, one should ﬁnd a prior by optimizing a predictive optimality criterion using an adaptive search technique such as a genetic algorithm (GA) or, more exactly, ﬁnd a best posterior based on a portion of the data that can be used as a prior for remaining and forthcoming data. The main beneﬁt of shifting the location of the penalty is that it reduces prior-data conﬂict. That way, when we ﬁnd an optimal penalty in the next subsection, it will correspond to putting priors on the β that have more of their mass close to the true values of the parameters, thereby improving inference. If the location shift is not used in the penalty, our method below still can be used but is not as eﬀective. especially in small samples. It just makes sense that if the true value of a β prior centered at zero. Then we present our GA methodology and verify that our methodology seems to achieve optimality in simulations. This shows that a GA approach is a viable alternative to pre-selecting a shrinkage method. 5.1. Extending the Theory of Sec. 2 set requires the estimation of one extra parameter. Adaptive methods such as ALASSO, AEN, etc., are also viable. The idea is to use the the data is standard in shrinkage methods (in the James-Stein sense) where it is used to improve the risk performance of decisions. For penalized log likelihoods with location shifted penalties we have the following suﬃcient conditions for the OP to hold. Corollary 5.1. Redeﬁne the objective function in Subsec. 2.1 to be Then, under the same conditions as in Theorem 2.1, the estimator the OP, i.e., Proof. The proof of Cor. 5.1 follows directly from the proof of Theorem 2.1. Indeed, if a true β large n we have P ( unchanged. Then, since the penalty only has to be controlled in the last term of (7.7), to get the asymptotic normality it is enough for We conjecture this occurs because a non-sparse linear model may provide a better approximation to a As a ﬁnal point for this section, if we were to redo our simulations using a diﬀerent model class, i.e., not Although previous sections used existing well-studied shrinkage methods, the results of Sec. 2 show that We begin by extending the results of Sec. 2 to allow for data-dependent shifts in parameter locations. Letˆβbe a√n−consistent estimator of β. To take advantage of the fact that shrinkage methods can ˆβ’s to zero, it is natural to chooseˆβto be from a speciﬁc shrinkage method such as SCAD2 that only We state our extensions to Theorems 2.1 and 2.2 as corollaries since we assume the same hypotheses. √nλ→ 0, as guaranteed by the hypotheses. Corollary 5.2. Redeﬁne the objective function in Subsec. 2.2 to be Then, under the same conditions as Theorem 2.2, the estimator OP, i.e., Stein’ type shrinkage. So, we are introducing another p hyperparameters. For this reason we only recommend this ‘double shrinkage’ approach when n is not too much smaller than p and preferably n > p. For these cases, we continue to set ˆw from the OP on the intervals around β centered around zero when they should be and not centered around zero when they shouldn’t be. 5.2. Using GAs to Find a Shrinkage Method algorithm that tries to mimic evolution to optimize a ﬁtness function by using analogs of mutation, crossover, and selection. Here, we embed a GA in a two stage optimization to ﬁnd an optimal penalty/prior. We start with a ‘population’ of penalty functions and then use gradient descent on the Q from each of them to ﬁnd a corresponding set of from the current population that correspond to the top 20% (say) of ﬁtness values as our ‘elite’ set. Now we use mutation and crossover on the remaining penalty functions (i.e., non-elite members of the current population). The resulting penalty functions along with the ‘elite’ set then form the next population. Iterating this process gives a series of populations of penalty functions with a non-decreasing ‘best’ ﬁtness value and the function within a very large class, as long as the regularity conditions are met. Since our goal is to ﬁnd the optimal penalty and the optimal penalty may be very complicated, we satisfy ourselves with merely approximating it. Here we represent f notation, we set Obviously, we would get a better approximation to an optimal penalty if we used more terms but for present purposes sixth order polynomials turned out to be suﬃcient. Our initial population of penalty functions is generated from (5.1) by selecting M values of α = (α of size M over F iterations to a ﬁnal population A essentially all members to be the same. ([18] p. 75 states that the algorithm often stops when there is little diversity in the population, as we detected.) For penalized empirical risks with location shifted penalties we have the following analog to Theorem Cor. 5.2 follows from Theorem 2.2 the same as Cor. 5.1 follows from Theorem 2.1. The methods motivated by these corollaries continue to allow shrinkage via the w’s as well as ‘James- Our goal is to ﬁnd the penalty that leads to the best predictor. To this end, recall a GA is a computational First, we deﬁne our initial class of penalty functions. Cors. 5.1 and 5.2 imply we can use any penalty , . . . , α) for m = 1, . . . , M. The GA will update this initial population denoted A= {α, . . . , α} We start by showing how the typical iteration from Ato Aproceeds. Assume we have data D = D= , x)|i = 1, . . . , n} and dim(x) = p and the empirical risk In view of Cor. 5.2 we seek for each α described shortly. We will use two versions of (5.2) depending on the relative sizes of n and p. Speciﬁcally, if p ≥ n or not too much smaller than n, we set all w as noted in Subsec. 5.1. We make this choice because when n < p typically our asymptotic results do not apply. In the case that p ≥ n (5.2) reduces to writing D = D completed. Next we split the training data We use D begin by searching over a list of values Λ equally spaced from λ to λ the as given. (The ˆw Recall, the sub-gradient descent algorithm allows for us to have points of non-diﬀerentiability in the penalty (e.g., a corner as in L or SCAD2), and in cases where the penalty is diﬀerentiable, the sub-gradient is uniquely deﬁned by the gradient. Note that the objective function is constructed to be convex, so we do indeed have a minimum. We initialize the gradient descent algorithm at the LASSO solution for n > p and at the RR solution for n < p. We evaluate the ﬁtness for each α choice for exactly the same f -value because its possible to happen with probability zero, it is observed on a regular basis. This arises because diﬀerent but similar penalties may lead to the same solution and because computing only has limited precision. crossover and mutation to the bottom 80% of ﬁtness values to obtain a new generation of size M from the algorithm to go into the second iteration. Crossing means switching some entries of a genome α entries from another α until the population size M is achieved. Mutation means adding a perturbation to all members of α (here a random number between the user speciﬁed maximum and minimum values for each component in α). Mutation does not change the size of the population, only the speciﬁc genomes already in it. In this way we get a new population A and so on until A To solve (5.2) or (5.3), we randomly split the data to estimate the various parameters. We begin by = γλfor some 0 < γ < 1. (Here, n= #Dwith corresponding data indicated by and X.) For each ﬁxed αand each choice of λ ∈ Λ, we ﬁndˆβfrom Dand choose ˆλthat minimizes R(ˆβ|D). We ﬁndˆβfor each m in (5.2) by sub-gradient descent since α, λ =ˆλ, w= ˆwandˆβcan be taken ˆλandˆβand hence a single ﬁtness value. However, it is possible for diﬀerent α’s to give Next, by elitism we select oﬀ the top 20% of members of A. We ﬁll in the ‘missing’ 80% by applying to see that as we have set it up here, the GA is a Markov process. That is, the probabilistic behavior in moving from time t to time t + 1 depends only on the state at time t. Moreover, this Markov process is homogeneous in the sense that the transition from time step to time step is the same for any two adjacent time steps. Note that the Markov process is ‘discrete time’ and has a discrete population (leading to distinct crosses) but the mutation is continuous because of the uniform distribution. Thus, there is no transition matrix. Instead, there is a transition kernel, K(x, S), where x is a population member at time t and S is a set of possible states to which x may be transformed and K is independent of t. In fact, K(·, ·) can be partitioned into a K matrix since crossover is discrete. The mutation kernel includes the continuous mutation phase based on the uniform distribution. So, let x be any state at time t and suppose an optimum f process has state space E. Then, there will be elements of E arbitrarily close to f ﬁtness value within the t-th population and let d(x) = b(x) −f zero. Now, given that we have used elitism, Theorem 2 in [19] applies to give convergence of the GA to the global minimum of f within the class of priors that have the OP as in Cors. 5.1 and 5.2. Indeed, a pragmatic check on the behavior of a GA would be to run it with diﬀerent initial populations to see if the GA outputs approximately the same minimum. To ensure convergence of the GA one should set a large population as well as a large number of generations. We comment that in the sub-gradient descent phase of our procedure, we have limited ourselves to convex objective functions. For more general results we would have to ensure convergence of the gradient based optimization to ensure convergence of the GA-based optimization. We implemented our GA computations using genalg, see [20]. heavy dependence or non-asymptotic cases. This is due to the fact that asymptotically the OP methods are equivalent, and thus a GA can do no better. Further, in the smaller sample cases with high sparsity we observe most methods performing roughly the same. In low to medium sparsity cases, there is more variability between the methods and thus, we should be able to optimize to ﬁnd a penalty that in fact does perform better than the common shrinkage methods. We acknowledge that only using standard basis expansions may not allow us to approximate some penalties well. 5.3. Simulations performs relative to other shrinkage methods in a predictive setting. We simulate IID observations from where X ∼ M V N before. We assume 50% sparsity, so the dimension of both β and set β entries α be mathematically equivalent to it. The diﬀerence from actually estimating a hyperparameter comes from the fact we are only using D dependent on a proper subset of D penalty given the rest of the data. This posterior can be used to generate predictions for D compared with the predictions from the other shrinkage methods used in Sec. 3. 5.3.1. GA example n = 40 sample sizes (2, 30, 4) and #(D estimates of β and λ. Note for comparisons with other methods, those that use glmnet use all 36 observations To see that this is the typical behavior of this sort of GA, we use the framework of [19]. First, it is easy = {d(x) < } will have nonzero probability for  > 0 and hence K(x, B) will be bounded away from The behavior of the GA depends on M, the elements of A, the size of F , the choice of f, the data, etc. Our intuition tells us that this method will be beneﬁcial in low to medium sparsity cases, as well as Here we present two simulations, one for p > n and one for n > p to show how implementing the GA = 0. We consider n = 40 and n = 150 and we split the data as described in Sec. 5.2. The GA will ﬁnd an optimal penalty as deﬁned by an optimal vector α= (α, . . . , α). The Here we split the data so that #(D) = #(D∪ D∪ D) = 36 with corresponding in the training data to form the predictor and for the methods that we implemented with LLA we combined a fair comparison. RR. This is consistent with the methods that performed the best in the analogous cases in Subsec. 3.1. This suggests that when we have few data points relative to explanatory variables, we do not have enough information to obtain an informative prior (in terms of its location and variance) so we default to the prior that makes us retain all the explanatory variables. here, because we set w not averaged over many data sets to get the prediction errors reported in this table. However, we believe we have used a large enough population and large enough number of generations that our results are accurate. a penalty that is diﬀerent from an established method (although we argue this is the typical case). The guarantee is only that we will ﬁnd an optimal penalty for prediction and it is no surprise if there are settings where a well known technique is optimal. The novelty in our GA approach is that it can be used in any linear regression problem and, if properly implemented, will always give the best predictions. 5.3.2. GA example n = 150 n > p. Accordingly, we set with corresponding sample sizes of (9, 113, 13) respectively, and #(D in Subsec. 5.3.1, the methods implemented using glmnet and rqPen used all 135 observations in the training data to form the predictor. Also, as before, for the methods that we implemented with LLA we combined method is given in Table 10. We observe the penalty selected through GA achieves the best predictive error among all methods considered. As in Subsec. 5.3.1, we comment that because GA’s require a lot of computing time, we have not averaged over many data sets to get the prediction errors reported in this table. However, we believe we have used a large enough population and large enough number of generations that our results are accurate. values are zero, half are non-zero, and the penalty term has f parameter. This means that we allow diﬀerent penalties on diﬀerent parameters. The left hand panel shows a plot of the optimal penalty for one of the β penalties RR, LASSO, and SCAD. The right hand panel shows a plot of the optimal penalty for one of the β’s that is known to be non-zero, again compared with RR, LASSO, and SCAD. It is obvious that the GA method described in Subsec. 5.1 gives two sorts of f β= 0 to concentrate at zero and forces the f zero. This explains the improvement in prediction error seen in Table 10. not be genuinely optimal for prediction. That is so because we have not run the GA for many generations with a large population size so we cannot assume the GA has converged. In fact, in both cases here (n = 40, 150) we only ran a single generation of the GA and we only used a population size of M = 150. and Dto estimate λ. Thus, we ensured that each method used all the training data, providing Interestingly, but perhaps not surprisingly, we ﬁnd ˆα = (0, 1, 0, 0, 0, 0) which corresponds exactly to The predictive errors for #(D) are given in Table 9. Note that GA, RR,EN, and AEN are the same This example illustrates that by optimizing over the choice of penalties, we are not guaranteed to ﬁnd When splitting the data in this situation, we must keep more than 100 observations in Dto ensure and Dto estimate λ. Again, this ensured all methods were being treated fairly. After running the GA we found ˆα = (2, 0, 0, 20, 0, 0). The associated prediction error on Dfor each Since we found a new (and better) penalty, we have graphed it in Fig. 6. Recall that half the parameter To end this section, note that our simulations only show proof of concept; the priors we found here may However, because of the elitism operation, running the GA longer can never result in a worse predictor and our results show that it can be relatively easy to ﬁnd a penalty that is better for prediction than established penalties – even if they are not optimal within the class of all penalties with the OP. 6. Discussion that may be a bit unexpected. First, the OP is not rare; it is actually rather common. Its proof requires little more than what most would regard as regularity conditions. Second, for small n and large p shrinkage methods did not perform very well even if they have the OP and the true model has reasonable sparsity. Third, on the other hand, if optimal or near optimal penalties are used they give shrinkage methods that work noticeably better than the established ones. Our ﬁndings indicate that methods having the OP do not perform particularly well for n < p and for n > p the OP is no guarantee that they perform better than methods not having the OP. Fourth, our results also indicate that with increasing sparsity the performance of shrinkage methods improves. This intuition needs to be developed further because the obvious limit of perfect sparsity gives the trivial model. cases where sample sizes are ﬁnite. We still think it’s better to have the OP than not if only because it gives consistency, asymptotic normality and eﬃciency. This is especially the case with high sparsity and large n relative to p, but in these cases other methods often perform comparably. When n is small compared to p, the OP is not a useful property, and thus the adaptive methods that have the OP do not perform well. A possible explanation for this is a poor bias variance trade oﬀ when p > n. It does not seem to be a good idea to use methods that require estimating w the adaptive penalty gives better results that the nonadaptive version. This paper has assumed a predictive stability perspective and within that context shown several results So, even though the OP is important, it is not at all clear how important it is or when it is important in notion of instability of predictions as a criterion for selecting a penalty or prior. Comparing instability curves is a ﬁnite sample check for good predictive performance. Using this approach we can easily rule out unstable predictors. For instance, with high sparsity using a linear model by itself is often unstable. In general, quantifying the variability of variable selection when p is large is diﬃcult, so deﬁning instability in terms of the prediction errors seems reasonable. of variable selection, and thus prediction, as sparsity increases as well as when n increases. In fact, our simulations showed that regardless of n, as the sparsity increased, the methods seemed to perform roughly equally well. For instance, recall the n = 75 simulations in Subsec. 3.2 . At 90% sparsity, we observed what appeared to be asymptotic convergence of the method with the OP. Of course this situation is not asymptotic as n < p, but an increase in sparsity is associated with an increase in eﬃciency of the methods. out of penalty (or prior) selection by using a GA to ﬁnd an optimal penalty/prior for prediction. When n > p we use the GA approach to ﬁnd a predictively optimal penalty that has the OP. When n << p, we do not search for methods with the OP because we do not beneﬁt from the known asymptotic results. Thus, we search over the class of penalties that are non-adaptive and do not require estimation of many hyperparameters. In principle, as long as we let the GA run long enough to converge, this approach can never do worse that simply choosing a standard shrinkage method. In fact, we have examples where the GA approach does better than others; when the GA approach selects the best among standard methods, we can infer the standard method was the right choice. we are producing an approximation to a predictively optimal posterior given the training data that can then be used with the log-likelihood. Thus, the predictive improvement comes from the eﬃciency of the way the posterior uses the data with an optimal prior. That is, we are using the data to form a pseudo-posterior based on a de facto optimal prior only determined by the (pseudo-)posterior it forms. and other points of non-diﬀerentiability in the penalty with setting parameter values equal to zero in ﬁnite samples. Recall that minimizing is equivalent to minimizing Q = R typically decreases as λ increases. Denote the constraint region by Since D is closed and compact, the Krein-Milman theorem (see [21]) gives that D is the closed convex hull of its extreme points, i.e., D = CCH(D of regions of the form where the U that the optima occur at extreme points of D. When the extreme points of D are on the coordinate axes we will ﬁnd at least some of β then Q often attains its minimum over D on a ‘face ’of D and the exact point where the optima occur may lie at the intersections of some or all of the U well-established for the case of linear optimization with linear constraints. Indeed, if Q is minimized for at least one extreme point of D that lies on a coordinate axis then at least some β means that any locally convex penalty with a ‘corner’ on a coordinate axis will perform nontrivial variable selection if R is small enough. We conjecture a converse to this statement will hold, too. Since the OP really requires n → ∞ whereas n often must be taken as truly ﬁnite, we introduce the Our simulation studies show that as a generality, shrinkage methods tend to perform better in terms Since there are inﬁnitely many such choices for penalties that have the OP, we take the subjectivity Another way to look at the procedure in Sec. 5 is that when we ﬁnd a penalty/prior based on the data We close with another heuristic that seems to be borne out by our results. Namely, we associate corners 7. Supplement : Proofs from Sec. 2 7.1. Penalized Log-Likelihood Case, Sec. 2.1 Theorem 7.1. Suppose Conditions 1–6 are satisﬁed and suppose in which Proof. Step 1. We want to show for any  > 0 there exists a large constant C such that where α where f line joining β where p (˜β)αuis the Taylor expansion of the penalty term by Conditions 4 and 6 and where˜βis on the is the number of components in β. where I which implies that I RHS of (7.4) using Conditions 1 and 3 We multiply by and observe for C > B. Then with high probability C have that by choosing suﬃciently large C, the second term of (7.4) is larger in absolute value, with high probability than the ﬁrst term uniformly in ||u|| = C. is also larger than the third term. The fourth term of (7.4) also goes to zero because for p that β fourth term on the RHS of (7.4) goes to 0. Thus since the third term third and fourths term on the RHS of (7.4) both go to 0, we have that the second term on the RHS of (7.4) is larger than all of the other terms. Thus, (7.1) is true for suﬃciently large C. Now by Taylor expansion of the log likelihood at β, L(β) + αu) − L(β|x) = αuL(β|x) +n2(αu)(I(β|x)) (αu) We argue the second term on the RHS of (7.4) is dominant. To see this consider the ﬁrst term on the√ Also, by hypothesis√na→ 0, so the whole third term in (7.4) goes to zero and the second term of (7.4) = 0, so˜β→ 0 as n → 0.Thereforef(˜β)u→ 0 by Condition 5 and sincena→ 0, the Therefore, by choosing large enough C, Step 2. We now show the conclusion of Step 1 holds for any C> C. Let C> C. Then, It follows thatP(ˆβ− β)≤ αCand ∀j , (ˆβ− β)≤ αC. So, and we can absorb C and as n → ∞ we have Remark: The argument for Theorem 1 is true for diﬀerentiable f case of non-diﬀerentiable f diﬀers from f then the above argument holds. Lemma 2. Assume conditions 1- 6, and the result from Theorem 7.1 holds. If as n, g(n) → ∞, then with probability tending to 1, for any given β any constant C, Proof. Consider the objective function Q(β) = L(β|x where β and due to [22] and by the Law of Large Numbers for non-identically distributed random variables we have Then by Taylor expanding at β=β0we have lies betweenˆβ and β. Note that Note that due to Conditions 1–6, and the result in Theorem 1,ˆβ − β= O. So, we have because p is ﬁnite and using Slutsky’s Theorem. Since is determined by the sign of f which implies that we have a minimum at β as the λ Proof. First, we observe that under Conditions 1–6 and our assumptions on b ˆβ = ( in Theorem 1 that is a likelihood equation The requirement that bnot go to zero too fast, if it goes to zero at all, is consistent with the fact that ’s increase, the β’s must decrease to obtain an optimal solution. The proof of the OP for penalized log-likelihood Theorem 2.1 is as follows. Now to prove the asymptotic normality ofˆβ, consider. It can be shown that there exists aˆβ for j = 1, . . . , p where where the third to last line is due to Condition 2. Here bounded by Condition 4, so Theorem, Condition 3 and the fact that E Thus, 7.2. Penalized Empirical Risk Case, Sec. 2.2 Theorem 7.2. Suppose Conditions 7-10 and Lemma 1 are satisﬁed. If Proof. Step 1. We want to show for any  > 0 there exists a large constant C such that ˜β ∈< β,ˆβ>. Furthermore, To ﬁnish the proof, we examine the terms in (7.7). By supposition,na→ 0 and f(ˆβ) is uniformly√ =where→ 0 and h(n) → ∞ , then there exists a local minimizerˆβ of Q(β) such that || = O(n+ a). where f joining β penalty term by Condition 6. Thus, where p because by Condition 7 we have (˜β)αuis the Taylor expansion of the penalty term by Condition 10 and where˜βis on the line to β+ αu. Thus we have that the last two terms on the right are the Taylor expansion of the is the number of components in β. Now by Taylor expansion of the empirical risk at β, as η → 0 and where I sider the ﬁrst term on the RHS of (7.11). We multiply by rem, converges in distribution to a normal distribution since E[d with high probability C the second term of (7.4) is larger in absolute value, with high probability than the ﬁrst term uniformly in ||u|| = C. (7.11) is also larger than the third term. The fourth term of (7.11) also goes to zero because for p we know that β the third term of (7.11) is larger than the fourth term, which implies that the second term of (7.11) is larger than all of the other terms. Thus, (7.8) is true for suﬃciently large C. and we can absorb C and as n → ∞ we have Using (7.9) and (7.10) , Now we argue that the second term on the RHS of (7.11) dominates the others. To see this, con- √n. Thus we rewriteαR(β|x)u as¯Ru. Now by the Central Limit Theo- → 1. Thus, C= ||u||= uu >¯Ru as long as 0 <<¯R < B for C > B with high probability. Then Also, by hypothesis√na→ 1, so the whole third term in (7.11) goes to zero and the second term of Therefore, by choosing large enough C, Step 2. We now show the conclusion of Step 1 holds for any C> C. Let C> C. Then, It follows thatP(ˆβ− β)≤ αCand ∀j , (ˆβ− β)≤ αC. So, that are non-diﬀerentiable, particularly at the origin in the following remark. Remark. The argument for Theorem 1 is true for diﬀerentiable f case of non-diﬀerentiable f diﬀers from f then the above argument holds. Lemma 3. Assume conditions 7–10 hold. If and any constant C, Proof. Consider the objective function Q(β) = R(β|x and due to [22] and by the Law of Large Numbers for non-identically distributed random variables we have Again, we discuss the way you can get this result for penalty functions that have ﬁnitely many points → 0 and g(n) → ∞, then with probability tending to 1, for any given βsatisfying ||β−β|| = O(n) Then by Taylor expanding at β=β0we have where βlies betweenˆβ and β. Note that Also, due to Theorem 1,ˆβ − β = O, so we have because p is ﬁnite and using Slutsky’s Theorem. Since is determined by the sign of f which implies that we have a minimum at β Proof. First, we observe that the estimator in Theorem 3 that is a likelihood equations for j = 1, . . . , p The proof of the OP for penalized log-likelihood Theorem 2.2 is as follows. Now to prove the asymptotic normality piece, consider. It can be shown that there exists aˆβ where where the third to last line is due Condition 7. Here I Thus, using Lemma 1 and the linearity of the expectation operator we know E Thus, ˜β ∈< β,ˆβ>. Furthermore, To ﬁnish the proof, we examine the terms in (7.14). By supposition,na→ 0 and f(ˆβ) is bounded.√ nλf(ˆβ) → 0 for p< j ≤ p since λ< a. Then by the Central Limit Theorem, Condition 3 and