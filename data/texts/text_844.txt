Implicit feedback recommendation has attracted increasing attention in recent years as the convenience of data collection[1] and it can take full advantage of users’ interactions (such as view, click, purchase and so on) for recommendation. Prior eﬀorts have shown the importance of exploiting users’ behaviors for comprehending users’ interest[2]–[4]. Existing works beneﬁt from learning short-term preference by exploring users’ recent historical interactions. As integrating the short-term preference, recommender systems have fully considered the evolution of users’ preferences. However, in such an implicit feedback setting, a user’s exact preference level is hard to quantify through observing his behaviors directly[5]. More than this, there are even some unexpected behaviors in the historical interactions. For examples, as shown in ﬁgure 1, a businessman has bought a baby T-shirt for his son, and clicked a handicraft by accident or due to curiosity, as shown in red boxes. Consider another Manuscript received August 1, 2019. Manuscript revised August 1, 2019. School of Computer and Information Engineering, Tianjin Normal University, Tianjin 300387, China a) E-mail: gxuchenjie04@gmail.com b) E-mail: wxxjlf@sina.com c) E-mail: mcmxhd@163.com d) E-mail: sunhuazhi@tjnu.edu.cn DOI: 10.1587/trans.E0.??.1 , Lifen JIANG, Chunmei MA, and Huazhi SUN, scenario, someone has watched a bunch of movies, but not every movie was what he likes. In these two cases, we argue that these unexpected behaviors are irrelevant to the user’s preference. It will hurt the recommendation performance dramatically when we can not exploit these behaviors in a discriminating way. That means we need to eliminate the eﬀects of unexpected behaviors. However, existing works do not have clear information to guide the model to eliminate the eﬀects of unexpected behaviors and they achieve inferior performances. Fig. 1: Unexpected behaviors in the historical interactions, as shown in red boxes. In this work, we aim to ﬁll the research gap by developing a solution that eliminates the eﬀects of unexpected behaviors under the guidance of the target item. Towards this end, we propose a novel method, named Multi Preference Model (MPM), which can not only eliminate the eﬀects of unexpected behaviors but also retain the predictive information as much as possible. Speciﬁcally, we ﬁrst extract the users’ instant preferences from their recent historical interactions by a ﬁne-grained preferences module. And then we train an unexpected-behaviors detector to judge whether these instant preferences are biased by unexpected behaviors. This is done under the guidance of the target item, as the instant preferences should stay in line with the positive target items and keep a distance from the negative ones. We also integrate a general preference module, which is a neural latent factor model[6] based on collaborative ﬁltering. The integration of the general preference module in our model can ensure the recommendation will not deviate far away from the users’ real general preferences. Thereafter, an output module is performed to eliminate the eﬀects of unexpected behaviors and aggregate all information to obtain the ﬁnal recommendation. We conduct extensive experiments on two datasets about movie and e-retailing to verify our method, demonstrating signiﬁcant improvements in our Multi-Preferences Model over the state-of-the-art methods. MPM gets a massive improvement in HR@10 and NDCG@10, which relatively increased by 3.643% and 4.107% compare with AttRec model on average. The contributions of this work are threefold: 1. We highlight the importantance of eliminating the unexpected behaviors’ eﬀect in historical interactions, which is crucial for recommendations. 2. We propose an end-to-end neural network model which can eliminate the eﬀects of unexpected behaviors in historical interactions. 3. We conduct experiments on public datasets to demonstrate the eﬀectiveness of the proposed model, and it outperform the competitive baselines. As a personalized ﬁltering tool, recommender systems have been widely adopted to alleviate the information overload problem. In this section, we brieﬂy review related works from three perspectives: session-based recommendation, latent factor models and a new trend in recommendation. In the session-based setting, recommender systems have to rely only on the interactions of the user in the current sessions to provide accurate recommendations[7]. And, our work mainly integrates the recent historical interactions to the conventional latent factor model, meanwhile we eliminate the eﬀects of unexpected behaviors. In the session-based recommendation, Hidasi et al.[8] take the lead in exploring GRUs for the prediction of the next user’s action in a session. Li et al.[9] argue that both the user’s sequential behaviors and the main purpose in the current session should be considered in recommendations. Tuan et al.[10] describe a method that combines session clicks and content features to generate recommendations. Ruocco et al.[11] use a second RNN to learn from recent sessions, and predict the user’s interest in the current session. Wu et al.[12] incorporate diﬀerent kinds of user search behaviors such as clicks and views to learn the session representation, which can get a good result in recommendations. Session-based recommendation systems share many similarities with our work, they both can take the advantage of historical interactions for recommendations. Latent factor models get its name from mapping users and items to latent factor vectors. In the age of machine learning, latent factor models are ﬁrst proposed based on matrix factorizations to deliver accuracy superior to classical nearestneighbor techniques[13]. Latent factor models are canonical for capturing user’s general preference, as they are trained by considering the whole historical interactions. After entering the era of deep learning, latent factor model has been developed in diﬀerent ways[14]. Salakhutdinov et al.[15] ﬁrst use Restricted Boltzmann Machine to extract latent features of user preferences from the user-item preference matrix. In[16], Autoencoder is used in deep latent factor models to learn a non-linear representation of the user-item matrix. He et al.[6] replace the inner product with an MLP as the interaction function. They present a general framework named NCF and implement a GMF and NeuMF model under the NCF framework. Travis et al.[17] integrate the latent factor model and the neighborhood-based model by memory network and propose the collaborative memory network model. In our work, we integrate a latent factor model to capture the user’s general preference, which endows our model with particular robustness. From the literature, we can ﬁnd that session-based recommender systems produce recommendations base on shortterm preference and latent factor models mainly rely on longterm (general) preference. Recently, a surge of interest in combining long-term and short-term preference for recommendation has emerged. Liu et al.[4] propose a novel shortterm attention/memory priority model (STAMP), which is capable of capturing users’ general interests from the longterm memory of a session context, whilst taking into account users’ current interests from the short-term memory ofthe last-clicks. BINN learns historical preference and present motivation of the target users using two LSTM-based architecture by discriminatively exploiting user behaviors[2]. Zhang et al.[18] propose an AttRec model which learns the short-term intention by self-attention module and extract long-term intention in the traditional manner which measure the closeness between item 𝑖 and user 𝑢 by Euclidean distance. These works have achieved considerable success. However, as mentioned before, there are usually some unexpected behaviors in historical interactions, and these works do not have clear information to guide the model to learn to eliminate the eﬀects of unexpected behaviors. Towards this end, we propose a Multi-Preferences Model to address this problem. In this section, we will elaborate on the multi-preference model, as illustrated in ﬁgure 2. Here, we ﬁrst formally deﬁne the recommendation problem in implicit feedback datasets and then present MPM in detail. In the implicit feedback setting, the interactions between users and items such as purchase, browse, click, view or even mouse movements are informative for recommendations. However, users’ opinion can’t be deduced directly through observing these behaviors[5]. This brings great challenges to the recommendation problem. We assume there are 𝑀 users and 𝑁 items in the dataset and deﬁne the user-item interaction matrix 𝑌 = 𝑅as Fig. 2: The framework of Multi-Preferences Model. (1) Fine-grained preferences module learns the instant preferences from historical interactions, (2) unexpected-behaviors detector detect the unexpected behaviors, (3) general-preference module capture user’s general preference, (4) output module eliminate the eﬀects of unexpected behaviors and produce ﬁnal recommendation. equation 1. If exist interactions between user u and item i set 𝑦= 1, otherwise, 𝑦= 0 . 𝑦=1 if interactions (user u, item i) are observed;0 otherwise. Our task can be formulated as follows: Given a user 𝑢, a target item 𝑖, and the last 𝐾 interactions 𝐻= [𝑥, 𝑥, 𝑥, ..., 𝑥]. The holistic goal is to estimate the interaction by: Where 𝑓 denotes the underlying model with parameters Θ, and ˆ𝑦presents the predicted score for the interaction. We can explain ˆ𝑦as the likelihood of the user 𝑢 interact with the item 𝑖 based on the recent historical interactions Multi-preferences model takes the user’s last 𝐾 historical interactions as input and outputs a score indicating how possible the user will interact with the target item. As illustrated in ﬁgure 2, there are four key components in MPM: (1) ﬁnegrained preferences module learn the ﬁne-grained instant preferences from historical interactions, (2) unexpectedbehaviors detector validate whether the instant preferences are biased by unexpected behaviors, (3) general-preference module is a conventional neural latent factor model for capturing general preference,(4) at last, the output module is performed to eliminate the eﬀects of unexpected behaviors and fuse all information to produce a ﬁnal recommendation. Given the recent historical interactions, we need to extract useful knowledge for recommendations. Instead of learning a single representation of the historical interactions in an autoencoding manner like previous works[18], we output an instant preference in each step. This workﬂow is consistent with the conventional sequential modeling, which obey the autoregressive principle. Here, we implement the ﬁne-grained preference module based on the Temporal Convolutional Network. Temporal Convolutional Network (TCN) is a popular architecture. Studies have shown that TCN outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets[19]. More formally, an input of 1 − 𝐷 items sequence 𝐻∈ 𝑅and a ﬁlter 𝑓 :{0, ..., 𝑘 − 1}→ 𝑅, the dilated convolution operation 𝐹 on element 𝑥of the sequence is deﬁned as 𝐹 (𝑥) = (𝐻∗𝑓 )(𝑥) =𝑓 (𝑖).𝐻(3) Where 𝑑 is the dilation factor, 𝑘 is the ﬁlter size, and 𝑠 − 𝑑𝑖 accounts for the direction of the past. Dilated convolution is illustrated in ﬁgure 3. And zero padding of length (𝑘 − 1) is added to keep subsequent layers as the same length as previous ones. TCN also incorporates residual connection[20], weight normalization[21] and spatial dropout[22] to improve performance, as shown in ﬁgure 2 (a). In our model, the ﬁne-grained preferences module can be abstracted as equation 4 Where ℎcontains all the semantic information of the interactions before position 𝑖, that is ℎ= Ψ(𝑥, 𝑥, ..., 𝑥). ℎrepresents the user’s instant preference at this moment. However, some of these instant preferences always are biased as the existence of unexpected behaviors in historical interactions. For example, 𝑥is a gift that the user buys for his friend or he click the item by accident. As a consequence, some instant preferences will be destroyed by unexpected behaviors. Fig. 3: A dilated convolution with dilation factors 𝑑 = 1, 2, 4 and ﬁlter size 𝑘 = 3. To eliminate the eﬀects of unexpected behaviors, we ﬁrstly builds an unexpected-behaviors detector to judge whether an instant preference is biased by unexpected behaviors. More speciﬁcally, in the training phase, unexpected-behaviors detector incorporates the target items to judge whether an instant preference is biased, as the instant preferences should stay in line with the positive target items and keep a distance from the negative ones. Once the model trained, the detector has the ability to make a judgment by itself. The unexpected-behaviors detector is shown in ﬁgure 4, which consist of 𝐾 multi-layers perceptron (MLP). Every instant preference ℎwill be concatenated with a target item embedding 𝑒, and then send into an MLP. The MLP can model the interactions between instant preference and item latent features. Finally, it outputs a predictive vector 𝑝, which contains the information about whether the instant preference is distorted. Unexpected-behaviors detector is deﬁned as follows. Where 𝑊, 𝑏, and 𝑎represent the corresponding weight matrix, bias, and activation function in the 𝑥 −𝑡ℎ layer of the MLP respectively. All in all, unexpected-behaviors detector consumes the instant preferences and outputs a bunch of predictive vectors 𝑃 = [𝑝, 𝑝, ..., 𝑝], which indicate whether the instant preference is distorted. So far, we have described the unexpected-behaviors detector in detail and the predictive vectors 𝑃 are informative for recommendations. However, due to the complexity of the real world, we still need to introduce general preference to make sure recommendations go in the right direction. Here, we build a general preference module base on conventional neural latent factor model[6], which is proﬁcient in capturing the general preference by considering the whole user-item matrix. General preference module takes the user latent vector 𝑒and item latent vector 𝑒as input and outputs a predictive vector 𝑝, as illustrated in ﬁgure 5. As the user latent factor vector contains the user’s general preference and item latent factor vector contains the characteristics of the item 𝑖[13], the predictive vector 𝑝indicates whether the target item 𝑖 ﬁt the user’s general preference. The integration of the General Preference Module in our model can ensure the recommendation will not deviate from the general preferences of users. It can improve the robustness of the model by a large margin. The general preference module is deﬁned as follows, 𝑝= 𝑎(𝑊(𝑎(...𝑎(𝑊[𝑒, 𝑒]+ 𝑏)...)) + 𝑏) (9) Until now, we still haven’t eliminated the eﬀects of unexpected behaviors yet. In our model, the Output Module has two important tasks: (1) eliminates the eﬀects of unexpected behaviors base on the detection of unexpected-behaviors detector as much as possible, (2) integrates all the information to make a ﬁnal prediction. We build our Output Module atop MLP, it will automatically learn how to complete these two mentioned tasks. MLP is a powerful architecture, it is sufﬁcient to represent any function in theory[23]. The Output Module is illustrated in ﬁgure 6. Fig. 6: Output Module. It eliminates the eﬀects of unexpected behaviors and outputs the ﬁnal prediction. Output module consumes the predictive vectors 𝑃 = [𝑝, 𝑝, 𝑝, ..., 𝑝] to eliminate the eﬀects of unexpected behaviors and integrate 𝑝make the ﬁnal prediction ˆ𝑦, as deﬁned in equation 10. ˆ𝑦indicate the likelihood of user 𝑢 interacts with the item 𝑖 in the next time. Similar to the spirit in recent works[6], the recommendation problem in implicit feedback datasets can be regarded as a binary classiﬁcation problem, where an observed useritem interaction is assigned a target value 1, otherwise 0. We choose the binary cross-entropy as our loss function, and learn the parameters of MPM in a pointwise learning manner. The loss function is deﬁned as equation 11. 𝑙𝑜𝑠𝑠 = −𝑦𝑙𝑜𝑔 ˆ𝑦+ (1 − 𝑦)𝑙𝑜𝑔(1 − ˆ𝑦 Here 𝑦 denotes the positive sample set and 𝑦means the negative sample set. We elaborate on the implementation details in the section of Experimental Settings. In this section, we conduct experiments on two real-world datasets to evaluate our proposed method. We aim to answer the following research questions: 1. RQ1: Compared with the state-of-the-art methods in implicit feedback setting, how does our method perform? 2. RQ2: How do recent historical interactions aﬀect the performance of our method? 3. RQ3: Will a larger model size be helpful for improving performance? We consider two scenarios: movie recommendation and eretailing recommendation. Datasets[24], which are typically used as a benchmark dataset for the recommender systems. We chose to experiment with the ml-20m, ml-10m and ml-1m subsets of MovieLens. 2. For e-retailing domain: we use the Taobao User- Behavior Dataset[25], which is a behavior dataset provided by Alibaba for the study of implicit feedback recommendation problem. We sample three subsets from Taobao user-behavior data set, denoted as Taobao-1m, Taobao-10m, and Taobao-20m. Detail statistics of the datasets are presented in Table 1. Following previous eﬀorts[1], [6], [17], we evaluate the performance of our proposed model by the leave-one-out evaluation. For each dataset, we hold out the last one item that each user has interacted with and sample 99 items that unobserved interactions to form the test set, a validation set is also created like the test set and remaining data as a training set. For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items. Our datasets are formed as follows, 𝑈𝑒𝑟 𝑢, 𝐼𝑡𝑒𝑚 𝑖, 𝐻= (𝑥, 𝑥, 𝑥, ..., 𝑥), 𝑦(12) We rank the test item with the 99 negative items and then select the 𝑡𝑜𝑝 − 𝐾 items as a recommendation list. We evaluate our model by HR (Hit-Rate) and NDCG (Normalized Discounted Cumulative Gain) metrics, which are deﬁned as follows: 𝑁 𝐷𝐶𝐺@𝐾 =1𝑀1𝐼𝐷𝐶𝐺2− 1𝑙𝑜𝑔(𝑖 + 1)(15) 𝐼𝐷𝐶𝐺=2− 1𝑙𝑜𝑔(𝑖 + 1)(16) Here, In the HR metric, 𝑅is the rank generated by the model for item 𝑖. If a model ranks 𝑖 among the 𝑡𝑜𝑝 − 𝐾, the indicator function will return 1, otherwise 0. In the NDCG, 𝑝 is the rank position of 𝑖 in the recommendation list, and IDCG means the ideal discounted cumulative gain[26]. Intuitively, HR measures the presence of the positive item within the 𝑡𝑜 𝑝 − 𝐾 and NDCG measures the item’s position in the ranked list and penalizes the score for ranking the item lower in the list. We compare our proposed approach against the competitive baselines list below. • MF[27], a general matrix factorization model which is one of the most eﬀective techniques for capturing long-term preferences. • NeuMF[6] is a composite matrix factorization jointly coupled with a multi-layer perceptron model for item ranking. • AttRec[18] take both short-term and long-term intentions into consideration. We use an MLP network replace Euclidean distance to measure the closeness between item 𝑖 and user 𝑢. • MPM-attn, we also implemented a simple version of the MPM model in the conventional fashion[18], in which we learn a short-term preference from instant preferences by self-attention mechanism[28], and then combine the short-term preference and general preference for recommendations. For a fair comparison, we learn all models by binary crossentropy loss and optimize all models with Adaptive Moment Estimation–Adam[29]. We apply a grid search to ﬁnd out the best settings of hyperparameters. After a grid search was performed on the validation set, the hyperparameters were set as follows, the 𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔 − 𝑟𝑎𝑡𝑒 was set to 0.001, 𝑏𝑒𝑡𝑎was set to 0.9, 𝑏𝑒𝑡𝑎was set to 0.999, 𝑒𝑝𝑠𝑖𝑙𝑜𝑛 was set to 1e-8, 𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔 − 𝑠𝑖𝑧𝑒 was set to 32 and 𝑏𝑎𝑡𝑐ℎ − 𝑠𝑖𝑧𝑒 was set to 1024. We employed a four layers architecture [64, 128, 64, 32] for multi-layer perceptron. In the TCN module, the number of dilated convolution layers is set to 4, our kernel size is 3 and the dilation factors is [1, 2, 4, 8]. 4.3 Performance Comparison (RQ1) Table 2 reports our experimental results for HR and NDCG with cut oﬀ at 10 on the MovieLens and Taobao UserBehavior datasets, in which the number of historical interactions (history size) is set to 9. From the experimental results, we have the following ﬁndings: • MF gives poor performance in both datasets. This indicates that the conventional latent factor models, which rely heavily on the general preference, may fail to capture the slight variation in the preference and have not considered the evolution of users’ preferences. As a consequence, they achieve an inferior performances. • NeuMF outperform MF, which demonstrates that incorporating an MLP to model the interaction between users and items is beneﬁcial. NeuMF learn an arbitrary function from data by replacing the inner product with a neural architecture. • AttRec achieves better performance than NeuMF. It makes sense since by incorporating short-term intention learned from recent historical interactions as ﬁne adjustment. AttRec fully explore both long-term intent and short-term preference for recommendations. • It is worth mentioning that MPM-attn also achieves comparable performance to AttRec. MPM-attn is an alternative choice for integrating both long-term preference and short-term preference to make a recommendation, it extracts user’s short-term preference from the recent historical interactions by an self-attention mechanism[28]. • MPM substantially outperforms AttRec 𝑤.𝑟.𝑡 HR@10 and NDCG@10, achieving the best performance. By introducing the positive interactions which are in conformity with the user’s real preferences to train the model, after training, MPM has the ability to eliminate the eﬀect of unexpected behaviors. MPM gets a massive improvement in HR@10 and NDCG@10, which relatively increased by 3.643% and 4.107% on average. This demonstrate the importance of eliminating the effects of unexpected behaviors. It also illustrates the eﬀectiveness of proposed model. 4.4 Study of MPM (RQ2) For a comprehension of our method, we dive into an indepth model analysis. We start by exploring the inﬂuence of multi-preferences to investigate whether MPM can eliminate the eﬀects of unexpected behaviors. We then study how the history size aﬀects the performance. We only conduct experiments on the Taobao-20m dataset for saving space. To our knowledge, there are two methods to integrate the short-term preference and long-term preference for recommendations in the literature. But, actually, the main diﬀerence between them is the way of learning short-term preference. One learns a single representation of short-term preference from the recent historical interaction in an autoencoding manner directly[18], in which it doesn’t need to follow the principle of autoregression. The other extracts instant preferences by sequential model in an autoregression manner and then encode these instant preferences to obtain the short-term preference by self-attention mechanism, just like MPM-attn does. However, these works fail to eliminate the eﬀect of unexpected behaviors, as no information to guide model learning. To address this problem, we propose a MultiPreferences Model, it eliminates the eﬀect of unexpected behaviors through a multi-preferences way. We explore the inﬂuence of multi-preferences on Taobao-20m dataset, as illustrated in ﬁgure 7. From ﬁgure 7, we can see that MPM-attn achieved comparable performance to AttRec, even outperforms slightly. After eliminating the eﬀects of unexpected behaviors, the Multi-Preferences Model substantially outperforms the stateof-the-art method by a larger margin in both HR@10 and NDCG@10, which increased by 5.47% and 3.23%. This demonstrates that unexpected behaviors can hurt the recommendation performance dramatically and our model can eliminate the eﬀects of unexpected behaviors. The other important thing about our model is to decide how many historical interactions should be considered. We cannot simply assume that the longer the history, the more information MPM can capture, and the better performance we got. As users’ preferences evolve over time, too big a history size can only add the burden on the model and bring in some noise. Here, we study the eﬀect of history size on Taobao20m dataset, we search the history size on{5, 7, 9, 11, 13}, as illustrated in ﬁgure 8. From ﬁgure 8, we can’t observe signiﬁcant diﬀerences between the performances as history size growing from 5 to 13. However, looking from the other side, this can be saw as evidence of MPM can eliminate the eﬀects of unexpected behaviors eﬃciently, as the larger of history size, the more unexpected interactions it will encounter possibly, but the performance still keeps in a reasonable region. In addition, we can see a slight improvement on NDCG@10 metric. This indicates that we get a higher quality recommendation list from the model. But, in practices, we should tune the history size for diﬀerent dataset, this is a trade-oﬀ between performance and complexity. 4.5 Is Larger Model Size Helpful ? (RQ3) Considering the eﬃciency of model, it is curious to see whether employing a larger model size will be beneﬁcial to the recommendation task. Towards this end, we further investigated MPM with diﬀerent model size. To ensure the extraction of instant preferences, we keep the architecture of TCN unchanged. The other factors of model size are embedding size and the scale of the MLP module. We combine these two factors to determine the model size. When the embedding size is set to 16, the architecture of MLP is adopted to [32, 64, 32, 16], when the embedding size is 64, the MLP is set to [128, 256, 128, 64] and so on. We conduct experiments on ml-1m dataset to study the eﬀects of model size. The experimental result is shown as ﬁgure 9. Fig. 9: The performances of MPM 𝑤.𝑟.𝑡 diﬀerent model size. We can see that the larger the embedding size is, the better the performance of the model is. However, we cannot guarantee a larger model size will lead to better model performance, as overﬁtting could be a possible consequence. On the other hand, the improvement is not signiﬁcant enough in both HR@10 and NDCG@10, as the model size grows up. This may cause by the fact that when model size grows larger, the gain from increasing model size can’t trade oﬀ the complexity it brings in. As a consequence, we should tune the model size carefully for diﬀerent datasets. In this work, we explored the inﬂuence of unexpected behaviors, which is harmful to recommendations. We propose a multi-preferences model which can eliminate the eﬀect of unexpected behaviors. The proposed model extracts instant preferences from recent historical interactions and detect which instant preferences are biased by unexpected behaviors under the guidance of the target item. Then, the output module eliminates the eﬀects of unexpected behaviors based on the detection. We also integrate a conventional latent factor model to learn the general preferences for recommendations. Extensive experiments are performed to show the eﬀectiveness of our model. In the future, we will extend our work in three directions. First, we attempt to mimic the interactions between users and items via graph neural networks to capture general preferences of users and characteristics of the items, since precise user and item embedding learning is the key to building a successful recommender system. Second, we plan to design a dedicated loss function instead of the general binary cross-entropy loss to get the most out of our model’s performance. In the end, we should also try some eﬀect to bring in more auxiliary information for building a more robust recommender system.