Power iteration is a fundamental algorithm in data analysis. It extracts the eigenvector corresponding to the largest eigenvalue of a given matrix. Applications include ranking algorithms, recommendation systems, principal component analysis (PCA), among many others. In this paper, We introduce multiplication-avoiding power iteration (MAPI), which replaces the standard `-inner products that appear at the regular power iteration (RPI) with multiplication-free vector products which are Mercer-type kernel operations related with the `norm. Precisely, for an n × n matrix, MAPI requires n multiplications, while RPI needs nmultiplications per iteration. Therefore, MAPI provides a signiﬁcant reduction of the number of multiplication operations, which are known to be costly in terms of energy consumption. We provide applications of MAPI to PCAbased image reconstruction as well as to graph-based ranking algorithms. When compared to RPI, MAPI not only typically converges much faster, but also provides superior performance. Index Terms— Power iteration, multiplication-free algorithms, principal component analysis, PageRank algorithm. Let A be a diagonalizable matrix and b be some initial vector. Power iteration is described by the update equation or recurrence relation where k · k represents the Euclidean norm. It is well-known that Eq. (1) converges to the eigenvector of A corresponding to the dominant eigenvalue. Note that the normalization can be omitted, i.e., b ← Ab results in the same direction as the dominant eigenvector. A primary application of the power iteration Eq. (1) is conventional (or, L) principal component analysis (PCA): Suppose that we collect members of a zero-mean D-dimensional dataset {x, . . . , x} ⊂ Rto a D × N matrix X = [xx... x] ∈ R. Consider the corresponding sample covariance matrix The ﬁrst principal vector, say p, is then the dominant eigenvector of C. The eigenvector pcan be extracted via the power iteration in Eq. (1) applied with the substitution A = C. The ith principal vector can then be extracted via power iteration as the dominantP eigenvector of (I −pp)C. This work was supported in part by Army Research Lab (ARL) under Grant W911NF-21-2-0272, National Science Foundation (NSF) under Grants 1934915 and CCF-1814717, and by an award from the University of Illinois at Chicago Discovery Partners Institute Seed Funding Program. In the process of computing the PCA of C through Eq. (1), one effectively computes the Euclidean inner product of the candidate principal vector b with the dataset elements x, i = 1, . . . , N at each iteration. The fundamental underlying operation for each such Euclidean inner product is the multiplication of the components of b with the components of x. On the other hand, it is well-known that multiplication operation will overamplify the effects of outliers or noise in the dataset. To increase robustness, our idea is to replace the Euclidean inner products that appear in Eq. (1) with Multiplication Avoiding Vector Products (MAVPs), which were originally developed in in the context of neural networks [1–6]. Two of the MAVPs satisfy the Mercer-type kernel requirement [7, 8]. The resulting multiplication-avoiding power iteration (MAPI) becomes a power iteration in the Reproducing Kernel Hilbert Space (RKHS) deﬁned by the kernel. Energy efﬁciency and improved computational complexity provides another major motivation for utilizing MAPIs in lieu of ordinary power iteration. In fact, MAVPs rely only on minimum operations and sign changes, and avoid the energy consuming multiplication operation. Therefore, compared to an Euclidean inner product, an MAVP can be executed in an energy efﬁcient manner in many processors. The same beneﬁts transfer to MAPI, which utilize MAVPs. Kernel-PCA was introduced by Scholkopf et al. [8, 9]. It has been applied to many signal and image processing problems [10– 13]. Related work includes recursive `-PCA and PCA methods using the similarity measures related with the `-norm [14–19]. However all of the above mentioned methods are computationally costly in large covariance matrices because they either require the eigendecomposition of the covariance matrix or the solution of a complex optimization problem. To the best of our knowledge, this paper is the ﬁrst paper describing power iterations in the kernel domain. This is probably due to the fact that other kernel PCA methods require costly kernel computations compared to the regular vector dot product [13,20–23]. On the other hand, MAPI kernel operations are more energy efﬁcient than the dot product. In the context of PCA, MAPI can not only improve robustness but provide signiﬁcant improvements in computational complexity. MAPI provides similar beneﬁts in other applications in which the power iteration is used. Examples will be provided throughout the paper. The rest of the paper is organized as follows: In Section 2, we formally introduce the MAPI. In Section 3, we describe applications of MAPI and report corresponding results. Finally, in Section 4, we draw our main conclusions. In this section, we provide an overview of the MAVPs, and formally introduce the MAPI method that utilizes MAVPs. 2.1. Multiplication-Avoiding Vector Products (MAVPs) MAVPs were ﬁrst studied in [1, 24] to develop a robust region covariance matrix. They were used in computationally efﬁcient neural networks. In this work, we will utilize the MAVP as We deﬁne another related dot-product as follows: wx ,1 (sign(w) = sign(x)) min(|w|, |x|) (4) where 1(·) is the indicator function. We call Eq. (3) as min1 operation and Eq. (4) as min2 operation. In the following sessions, we will denote the two min-operations as ⊕ = {, }. Note that unlike an ordinary Euclidean inner product, Eq. (3) and (4) do not contain any multiplication operations. Energy efﬁciency of ⊕operation varies from processor to processor. For example, multiplicationbased regular dot-product operation consumes about 4 times more energy compared to the multiplication avoiding dot product operations deﬁned in Eq. (3) and (4) in compute-in-memory (CIM) implementation at 1 GHz operating frequency [6]. The MAVP in Eq. (3) and (4) can be extended to matrix multiplications as follows: Let W ∈ Rand X ∈ Rbe arbitrary matrices, then where wis the ith column of W for i = 1, 2, . . . , m and xis the jth column of X for j = 1, 2, . . . , p. In brief, the deﬁnition is similar to the matrix multiplication WX by only changing the element-wise product to element-wise min-operation. Recall from Section 1 that given a dataset X = [xx... x] ∈ R, the eigendecomposition of C =XXyields the ordinary PCA. Alternatively, one can construct the sample covariance matrix through MAVPs. Recall from Section 1 that given a dataset X = [xx... x] ∈ R, the eigendecomposition of C =XX yields the ordinary PCA. Alternatively, one can construct the sample covariance matrix through MAVPs. In specie, in [19], we considered the eigendecomposition of the “min-covariance matrix” We have shown in [19] that the resulting “Min-PCA” provides better resilience against impulsive noise than regular PCA in image reconstruction experiments. 2.2. Multiplication-Avoiding Power Iteration (MAPI) We are now ready to introduce the MAPI. We replace the standard products in Eq. (1) with a MAVP. To further reduce the computational complexity, we replace the normalization by `-norm by a normalization by `-norm. Our revisions yield the iteration We have observed that such a change of normalization does not effect the ﬁnal performance greatly. The ﬁnal MAPI algorithm is summarized in Algorithm 1. For applications to PCA, we can normalize the ﬁnal wby its l-norm for extraction of subsequent principal vectors. Input: A ∈ R, iteration times T . Output: Dominant-pseudo-eigenvector w∈ R. 1: Initialize was a random vector with ||w|| = 1; 2: for t = 0, 1, ..., T − 1 do 5: end for 6: (optional) w= w/||w||; 7: return w. Each step of the MAPI deﬁned in Eq. 8 corresponds to a transformation in the RKHS. As a result the convergence of MAPI depends on the matrix A. Since we normalize the iterations in (8) at each step, the iterates are bounded and they satisfy ||w||= 1. We have observed that the MAPI in Algorithm 1 converges in all experiments that we have tried and the resulting vector can be used in practical applications. Note that MAPI only requires N divisions per iteration as opposed to the RPI, which requires Nmultiplications and N divisions. Therefore, we expect MAPI to consume signiﬁcantly less energy compared to RPI in most processors. 3. APPLICATIONS AND NUMERICAL RESULTS 3.1. Image Reconstruction Example We consider image reconstruction example studied in [14]. Our experiment follows the same structure in [14, 19]. We use Regular Power Iteration (RPI) and MAPI to compare the image reconstruction results of the MAPI. In this experiment, the image size D ×D = 128 × 128, and the pixel values are in the range of [0, 1]. Suppose that we want to reconstruct a gray-scale image (Fig. 1a) from its N = 10 occluded versions. As Fig. 1b shows, the corrupted images are created by partitioning the original image into sixteen tiles of size 32 × 32 and replacing three arbitrarily selected tiles by 32×32 gray-scale noise patches. The noise patches are generated using the uniform distribution in the interval [0, 1]. The reconstruction algorithm is described in Algorithm 2. In particular, in Step 5 and Step 6, we obtain the ﬁrst two dominant generalized eigenvectors of the min-covariance matrix of images via MAPI. We then reconstruct the image using these two generalized eigenvectors in Line 8. In Table 1, we compare the peak signal-to-noise ratio (PSNR) performances provided by different algorithms. As Fig. 1d and 1e show, MAPI provides higher PSNRs than RPI, globally. The average PSNR of min2-PI is 1.62 dB higher than RPI. The MAPI based Algorithm 2 Image Reconstruction via MAPI Input: N corrupted versions of an image: I, I, ..., I∈ R, pixels are in range of [0, 1]. Output: Reconstructed imageˆI. 1: Reshape Iinto the column vector form v∈ Rfor i from 1 to N; 2: V = [vv... v] ∈ R; 3: Reduce mean: m = mean(V, 2) ∈ R, V = V−m; 4: Construct the min-covariance matrix of V: C = V ⊕ V∈ R; 5: Perform MAPI on C to get the dominant eigenvector 6: Perform MAPI on C − (w⊕ w)C to get the second dominant eigenvector w∈ R; 7: w = [ww] ∈ R 8: Reconstruct image: ˆv = (w ⊕ w)(v− m) + m; 9: Reshape ˆv back to the matrix formˆI; 10: returnˆI. reconstruction (24.8 dB) is also superior to Recursive L1-PCA (24.2 dB). MAPI is as robust as L1-PCA based restoration and its computational cost is much lower than the regular L1-PCA methods. Table 1: Image Reconstruction PSNRs (dB) In this section, we will compare the MAPI with the stochastic power iteration method on a synthetic dataset as in [25]. The synthetic dataset X ∈ Ris generated using the singular value decomposition. In detail, let a diagonal matrix Σ =√√ diag{1,0.9, ...,0.9} ∈ R, a random orthogonal projection matrix U ∈ Rand a random orthogonal matrix V ∈ R, then the dataset X = UΣVguarantees that the matrix C = XX has an eigen-gap of 0.1. MAPI method can be also implemented into the mini-batch power method with momentum algorithm (Algorithm 1 in [25]). We take the MAPI on A = XX with the same mini-batch strategy, as Algorithm 3 shows. The different between Algorithm 3 in this paper and Algorithm 1 in [25] is that we change all regular multiplication operations with our min1 operation Eq. (3) and normalize the iterates using the `-norm. At the end of iterations we normalize the ﬁnal vector using the `-norm of the vector. In this experiment, we cannot use min2-PI operation because the min2 operation Eq. (4) cannot return negative entries. The vector ucan have negative entries. In Fig. 2, we compute (1 − (wu/||w||)) after each iteration, where uis the dominant eigenvector obtained from eigendecomposition as in [25]. The MAPI iteration converges to a vector very close to the actual eigenvector because (1 − (wu/||w||)) is very close to zero as shown in Fig. 2a for t greater than 20. However, wobtained using the MAPI method is not exactly the same as the eigenvector vas we see from Fig. 2b whose vertical axis has a different range from Fig. 2a. This is expected because we perform the iterations in the Reproducing Kernel Hilbert Space (RKHS) domain. Nevertheless, based on our observation, ranks of the entries of wobtained from the min1-PI are the same as the ranks obtained from wof the RPI. After 100 iterations, w=[-0.1433, 0.4171, 0.1166, 0.3863, 0.1285, -0.1315, -0.4330, -0.2097, -0.5770, -0.2106] from the RPI and w=[-0.1649, 0.4166, -0.1371, 0.3887, 0.1480, 0.1508, -0.4306, -0.2359, -0.5362, -0.2370] from the min1-PI. When we order from the largest to the smallest, both vectors produce the same ranks {2, 4, 5, 3, 6, 1, 8, 10, 7, 9}. Therefore, considering that our min1-PI converges signiﬁcantly faster than the conventional power iteration and the min2-PI is identical to the min1-PI if all values are non-negative, our MAPI can be employed in the Google PageRank algorithm [26]. We will discuss it in Section 3.3. Moreover, if we want (1 −(wu/||w||)) to reach 0, we can optimize the vector via the MAPI ﬁrst and then switch to the RPI for further converging. Input: Data X ∈ R, iteration times T , batch size s, momentum parameter β. Output: Dominant-pseudo-eigenvector w∈ R. 1: Initialize was a random vector with ||w|| = 1; 2: for t = 0, 1, ..., T − 1 do distributed samples B = {˜A, ...,˜A}P 6: end for 7: (optional) w= w/||w||; 8: return w. PageRank algorithm uses the hyperlink structure of the web to view inlinks into a page as a recommendation of that page from the author of the inlinking page [27]. More speciﬁcally, it runs power iteration Fig. 1: Image reconstruction examples: (a) original images; (b) occluded images; (c) recursive ` using RPI results; (e) min1-PI results; (f) min2-PI results. MAPI is as robust as L1-PCA based restoration. Fig. 2: RPI versus MAPI on a synthetic dataset X ∈ R where the covariance matrix has eigen-gap ∆ = 0.1. algorithm on the Google Matrix where H is the network adjacent matrix, 1 is an all-ones matrix, and α = 0.85 is known as a damping factor. We ﬁrst run the PageRank algorithm with RPI and with our MAPI on a network graph as Fig. 3a, respectively. Then, we compute ||w−w|| after each iteration to compare the convergence. We apply one `-normalization after the ﬁnal iteration for easier comparison. After 20 iterations, from the RPI, w= [0.6335, 0.3452, 0.4399, 0.5348], and from the MAPI, w= [0.5675, 0.3486, 0.5201, 0.5347]. Consequently, though the weights are different, both index ranks are {1, 4, 3, 2}. We further try the PageRank algorithm with the RPI and with the MAPI on Gnutella peer-to-peer network datasets (Gnutella08 [28], Gnutella09 [29]). Gnutella08 contains 6,301 nodes with 20,777 edges, and Gnutella09 contains 8,114 nodes with 26,013 edges. The convergence curves are shown in Fig. 4. The MAPI still converges remarkably faster than the RPI on these large network datasets. However, because the size of the network is very large, the ranks of the two methods are not the same. After 10 iterations, from Gnutella08, the top-10 ranks of the indices from the RPI are {367, 249, 145, 264, 266, 123, 127, 122, 1317, 5}, while the MAPI {266, 123, 367, 127, 424, 249, 145, 264, 427, 251}. On the other hand, from Gnutella09, the top-10 ranks of the indices from the RPI are {351, 563, 822, 534, 565, 825, 1389, 1126, 356, 530}, while the MAPI returns {351, 822, 51, 1389, 563, 565, 530, 825, 356, 1074}. Therefore, there are 7 common top-10 ranks of indices {367, 249, 145, 265, 226, 123, 127} from the 6,301-node dataset Gnutella08 and 8 common top-10 ranks of indices {351, 563, 822, 565, 825, 1389, 356, 530} from the 8,114-node dataset Gnutella09. Therefore, if we implement the MAPI in the page-rank-based web search system, what links displayed on the ﬁrst page is very close to the conventional page-rank based system, but the search time can be reduced efﬁciently. Fig. 4: PageRank convergence curves on Gnutella. In this paper, we proposed two types of MAPI algorithms which replace the standard vector product operations in regular power iteration (RPI) with multiplication-free vector products performed in RKHS deﬁned by the kernel. The MAPI is energy efﬁcient because it reduces the number of multiplication operations signiﬁcantly, which are known to be costly in terms of energy consumption in many processors. The MAPI reduces the number of multiplications from n multiplications to n multiplications per iteration for an n×n matrix. The MAPI also produces as robust results as the `-PCA algorithm because MAPI kernels are related with the `-norm. Compared to the RPI, the min2-PI reaches an average PSNR 1.62 dB higher in our image reconstruction experiment. According to our graph-based ranking experiment, the MAPI is superior to the RPI in terms of convergence speed and energy efﬁciency. Though the ﬁnal ranking obtained from the MAPI is not always exactly identical to the ranking obtained from the RPI but they are very close to each other. [1] Hakan Tuna, Ibrahim Onaran, and A Enis Cetin. Image description using a multiplier-less operator. IEEE Signal Processing Letters, 16(9):751–753, 2009. [2] Arman Afrasiyabi, Baris Nasir, Ozan Yildiz, Fatos T Yarman Vural, and A Enis Cetin. An energy efﬁcient additive neural network. In 2017 25th Signal Processing and Communications Applications Conference (SIU), pages 1–4. IEEE, 2017. [3] Arman Afrasiyabi, Diaa Badawi, Baris Nasir, Ozan Yildi, Fatos T Yarman Vural, and A Enis C¸ etin. Non-euclidean vector product for neural networks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6862–6866. IEEE, 2018. [4] Hongyi Pan, Diaa Badawi, Xi Zhang, and Ahmet Enis Cetin. Additive neural network for forest ﬁre detection. Signal, Image and Video Processing, pages 1–8, 2019. [5] Tolga Ergen, Ali H Mirza, and Suleyman Serdar Kozat. Energy-efﬁcient lstm networks for online learning. IEEE transactions on neural networks and learning systems, 31(8):3114– 3126, 2019. [6] Shamma Nasrin, Diaa Badawi, Ahmet Enis Cetin, Wilfred Gomes, and Amit Ranjan Trivedi. Mf-net: Compute-inmemory sram for multibit precision inference using memoryimmersed data conversion and multiplication-free operators. IEEE Trans. Circuits and Systems, 2021. [7] Hongyi Pan, D Badawi, E Koyuncu, and AE Cetin. Robust principal component analysis using a novel kernel related with the l1-norm. In to appear in Proceedings of EUSIPCO - European Signal Processing Conference; a longer version is available at arxiv.org/abs/2105.11634, 2021. [8] B. Sch¨olkopf, A. Smola, and K. M¨uller. Kernel principal component analysis. In Artiﬁcial Neural Networks — ICANN’97, Lecture Notes in Computer Science, vol 1327, Heidelberg, Berlin, Germany, 1997. [9] Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Robert M¨uller. Nonlinear component analysis as a kernel eigenvalue problem. Neural computation, 10(5):1299–1319, 1998. [10] H Goldberg, Heesung Kwon, and Nasser M Nasrabadi. Kernel eigenspace separation transform for subspace anomaly detection in hyperspectral imagery. IEEE Geoscience and Remote Sensing Letters, 4(4):581–585, 2007. [11] Nasser M Nasrabadi. Hyperspectral target detection: An overview of current and future challenges. IEEE Signal Processing Magazine, 31(1):34–44, 2013. [12] Hien Van Nguyen, Vishal M Patel, Nasser M Nasrabadi, and Rama Chellappa. Kernel dictionary learning. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2021–2024. IEEE, 2012. [13] Heiko Hoffmann. Kernel pca for novelty detection. Pattern recognition, 40(3):863–874, 2007. [14] Panos P Markopoulos, George N Karystinos, and Dimitris A Pados. Optimal algorithms for l-subspace signal processing. IEEE Transactions on Signal Processing, 62(19):5046–5058, 2014. [15] P. P. Markopoulos, G. N. Karystinos, and D. A. Pados. Optimal algorithms for l1-subspace signal processing. IEEE Transactions on Signal Processing, 62(19):5046 – 5058, Jul. 2014. [16] Panos P Markopoulos, Sandipan Kundu, Shubham Chamadia, and Dimitris A Pados. Efﬁcient l1-norm principal-component analysis via bit ﬂipping. IEEE Transactions on Signal Processing, 65(16):4252–4264, 2017. [17] Panos P Markopoulos and Fauzia Ahmad. Indoor human motion classiﬁcation by l1-norm subspaces of micro-doppler signatures. In 2017 IEEE Radar Conference (RadarConf), pages 1807–1810. IEEE, 2017. [18] Panos P Markopoulos, Mayur Dhanaraj, and Andreas Savakis. Adaptive l1-norm principal-component analysis with online outlier rejection. IEEE Journal of Selected Topics in Signal Processing, 12(6):1131–1143, 2018. [19] Hongyi Pan, Diaa Badawi, Erdem Koyuncu, and A Enis Cetin. Robust principal component analysis using a novel kernel related with the l1-norm. arXiv preprint arXiv:2105.11634, 2021. Presented in EUSIPCO 2021. [20] Y Xiao, Huangang Wang, and Wenli Xu. Model selection of gaussian kernel pca for novelty detection. Chemometrics and Intelligent Laboratory Systems, 136:164–172, 2014. [21] Cheolmin Kim and Diego Klabjan. L1-norm kernel pca. arXiv preprint arXiv: 1709.10152, 2017. [22] Carolina Varon, Carlos Alzate, and Johan AK Suykens. Noise level estimation for model selection in kernel pca denoising. IEEE transactions on neural networks and learning systems, 26(11):2650–2663, 2015. [23] Samuele Battaglino and Erdem Koyuncu. A generalization of principal component analysis. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3607–3611. IEEE, 2020. [24] Cem Emre Akbas, Alican Bozkurt, Musa Tunc Arslan, Huseyin Aslanoglu, and A Enis Cetin. L1 norm based multiplication-free cosine similarity measures for big data analysis. In 2014 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM), pages 1– 5. IEEE, 2014. [25] Peng Xu, Bryan He, Christopher De Sa, Ioannis Mitliagkas, and Chris Re. Accelerated stochastic power iteration. In International Conference on Artiﬁcial Intelligence and Statistics, pages 58–67. PMLR, 2018. [26] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999. [27] Catherine Benincasa, Adena Calden, Emily Hanlon, Matthew Kindzerske, Kody Law, Eddery Lam, John Rhoades, Ishani Roy, Michael Satz, Eric Valentine, et al. Page rank algorithm. Department of Mathematics and Statics, University of Massachusetts, Amherst, Research, 2006. stanford.edu/data/p2p-Gnutella08.html Accessed: 2021-10-03. stanford.edu/data/p2p-Gnutella09.html Accessed: 2021-10-03.