Nanjing University of Posts andNanjing University of Posts and Sequential recommendation based on multi-interest framework models the user’s recent interaction sequence into multiple dierent interest vectors, since a single low-dimensional vector cannot fully represent the diversity of user interests. However, most existing models only intercept users’ recent interaction behaviors as training data, discarding a large amount of historical interaction sequences. This may raise two issues. On the one hand, data reecting multiple interests of users is missing; on the other hand, the co-occurrence between items in historical user-item interactions is not fully explored. To tackle the two issues, this paper proposes a novel sequential recommendation model called “GlobalInteraction AwareMulti-Interest Framework for SequentialRecommendation (GIMIRec)”. Specically, a global context extraction module is rstly proposed without introducing any external information, which calculates a weighted co-occurrence matrix based on the constrained co-occurrence number of each item pair and their time interval from the historical interaction sequences of all users and then obtains the global context embedding of each item by using a simplied graph convolution. Secondly, the time interval of each item pair in the recent interaction sequence of each user is captured and combined with the global context item embedding to get the personalized item embedding. Finally, a self-attention based multi-interest framework is applied to learn the diverse interests of users for sequential recommendation. Extensive experiments on the three real-world datasets of Amazon-Books, Taobao-Buy and Amazon-Hybrid show that the performance of GIMIRec on the Recall, NDCG and Hit Rate indicators is signicantly superior to that of the state-of-the-art methods. Moreover, the proposed global context extraction module can be easily transplanted to most sequential recommendation models. • Information systems → Recommender systems; Collaborative ltering. sequential recommendation, multi-interest framework, global context extraction ACM Reference Format: Jie Zhang, Ke-Jia Chen, and Jingqiang Chen. 2018. GIMIRec: Global Interaction Information Aware Multi-Interest Framework for Sequential Recommendation. In Woodstock ’18: ACM Symposium on Neural Gaze Detection, June 03–05, 2018, Woodstock, NY . ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445.1122456 The recommendation system predicts users’ preferences for items based on their historical behaviors. For example in e-commerce application, the users are often recommended some commodities based on their past purchase behaviors. Deep neural networks, especially graph neural networks (GNNs) [4,18], have now been applied on the recommendation systems, such as GRU4Rec [6], GGNN [15] and SR-GNN [25], achieving better performance than traditional collaborative ltering methods [7,16]. However, the graph-based methods are often with high complexity as the result of the large scale of users and items in real world [5]. The sequential recommendation has recently drawn more research attention as it starts with each user’s own interaction data, models the users’ preferences with only recent interaction behaviors and then gives personalized recommendation for each user. The main eort of sequential recommendation is to learn the representations of users and items. Since the interests of users are usually diverse and dicult to be represented by a single low-dimensional vector, the sequential recommendation methods based on multi-interest framework have emerged [1,2,13]. In these methods, the number of representation vectors for each user is increased from 1 toK, where each vector represents one interest of the user. Multiple interests can be captured by an improved capsule network [13] or a self-attention mechanism [1] and ultimately a more personalized recommendation based on K interests is provided. Most of the existing multi-interest frameworks for sequential recommendation use only users’ recent interaction sequence as it is believed that the user’s next interaction item is more closely related to his/her recent behaviors. However, we believe that the complete historical interactions of all users (referred to as the global context in this paper) can be better leveraged to reect the diversity of users’ interests and potential relations between items. An example shown in Figure 1 is to illustrate how the global context aects the prediction of the 10th interactive item for each user although this item did not appear in each user’s recent interactions. Since the popcorn and wine interaction sequence can be found in user A’s early historical sequence, the 10th interactive item for user B will be probably the wine. The cake could be recommended for user A instead of battery due to the co-occurrence of game console and cake in user C’s historical sequence. The example indicates that the user’s behaviors could be predicted with the reference of other users’ historical behaviors though dierent users may have dierent preferences. Figure 1: An example of users’ interaction sequences. To our knowledge, these have been some sequential recommendation models exploring users’ complete sequence to learn their preferences [17,20,27–29]. But they focus more on each user’s personalized behaviors but do not take the global context into consideration. Therefore, this paper proposes a global interaction information aware multi-interest framework for sequential recommendation (named GIMIRec) with the main contributions as below: •A new multi-interest sequential recommendation model is proposed, which is equipped with a global context extraction module to fully utilize the historical sequences of all users. This module can also provide a performance boost for the recommendation models based on representation learning. •A lightweight approach is designed to calculate the global context embedding of each item, based on the weighted co-occurrence matrix fusing the constrained co-occurrence number of item pairs and their time intervals in the global context. •Our model achieves the state-of-the-art performance on three real-world datasets without introducing any other supportive information (such as item attributes or user’s social information) other than interactions and timestamps. Sequential recommendation.Traditional recommendation methods, such as matrix decomposition and collaborative ltering [7,16] , represent interaction behaviors as a user-item matrix and explore their potential relation. With the development of deep neural networks, dierent recommendation methods combined with deep learning have emerged, such as the neural collaborative ltering method NGCF [22]. However, it is hard for the traditional recommendation system to manage huge but sparse interaction data. To solve this problem, sequential recommendation methods are proposed. The objective of sequential recommendation is to learn each user’s preferences from the historical interaction behaviors, and then to achieve recommendation. Typical methods include GRU4Rec [6] and SASRec [10] where the recurrent neural network and the attention mechanism are used respectively. Later, the time of the interaction occurrence is also taken into account. For example, a parallel time-aware mask network is proposed in MTIN [8], which extracts a variety of time information through multiple networks to enhance the short video recommendation eect. Ti-SASRec [14] is another method incorporating time interval of every two items in sequences to further improve the performance of SASRec [10]. Multi-interest framework for sequential recommendation. User preferences may contain a variety of interests. The existing sequential recommendation systems eventually model users’ preferences as a single low-dimensional vector while the users may have more than one single interest. MIND [13] was the rst to model each user withKvectors, where dierent vectors represent dierent interests. The extraction of multi-interests is similar to the clustering process and the behavior-to-interest (B2I) approach is proposed to obtainKinterests of users through a capsule network. Subsequently, Cen et al. [1] proposed a multi-interest extraction method based on self-attention mechanism (ComiRec-SA) to instead of capsule network. The latest method PIMI [2] introduced the time and periodic information in users’ recent interaction sequences into the model. The self-attention mechanism was used to capture users’ multi-interests and achieved the state-of-art performance. However, all the above multi-interest based sequential recommendation models, including PIMI, do not fully utilize the historical interactions of all users, thus lacking the global context of items. Graph neural networks.In recommendation systems, users’ historical interaction behaviors can be converted into a graph structure, where users and items are represented as nodes and their interactions are represented as edges. Graph neural networks (GNNs) [4,18] have been proved to better learn the representations of users and items. In non-sequential recommendation models, such as NGCF [22], LightGCN [5] and DGCF [23], simplied GNNs are used on the bipartite graph composed of users and items to generate their embeddings. In sequential recommendation models such as SR-GNN [25], GNNs are also widely used to represent user’s sequence and to capture user’s preferences. Before describing the proposed model in detail, the formal denitions of sequential recommendation and multi-interest framework are given as follows: Denition 1: Sequential Recommendation.Assuming that Urepresents a collection of all users andIrepresents a collection of all items. For a given user𝑢 ∈ U, his/her interaction sequenceSno is dened with an item sequenceI=𝑖, 𝑖, · · · , 𝑖and a times-no tamp sequence of each interaction occurrenceT=𝑡, 𝑡, · · · , 𝑡. The task of sequential recommendation is to predict the next interactive item 𝑖for the user 𝑢 based on S. Denition 2: Multi-Interest Framework.It denotes a framework designed for sequential recommendation. According to Denition 1, each user𝑢will be represented as a𝑑-dimensional embedding by learning from his/her interaction sequence. However, one single vector representation may not be sucient to describe the user’s multi-interest preferences. In the multi-interest framework,K 𝑑dimensional embeddings of each user will be learned for sequential recommendation. The main purpose of our model GIMIRec is to better capture the multiple interests of each user from the global context information, which is calculated from the historical sequences of all users. The overall model consists of the following modules. Global Context Extraction Module.The module is basically to collect the co-occurrence number and time interval for any item pair that appear one after another in each user’s historical interaction sequence for the calculation of the weighted co-occurrence matrix. Subsequently, a simplied graph convolution is used on the above matrix to obtain the global context embedding of each item. Recent Time Interval Representation Module.Though we believe the global context will better reect the diversity of user preferences, the user’s recent interactions directly reect user’s recent preferences. So this module rst identies the recent interaction sequence based on the predetermined length𝐿, and then encodes the time interval between any item pair within the sequence to obtain the time interval embedding for recent items. Aggregation Module.For each item in the user’s recent interaction sequence, the global context embedding and the time interval embedding are merged in this module to get the hybrid embedding. For each user, a graph neural network is used to learn the nal personalized embedding by aggregating the above hybrid embeddings in his/her recent interaction sequence. Multi-Interest Extraction Module.The module extractsK interest vectors from the personalized embedding of user’s recent sequence by the self-attention mechanism [21]. For model training, one interest vector that can best represent user’s current preference is selected from K interest vectors. Finally, the trained model is used to predict the user’s next𝑁 interactive items (i.e., Top-𝑁recommendation) in the test set according to user’s K interests. The architecture of GIMIRec is demonstrated in Figure 2, and each module will be detailed in the following subsections. The existing multi-interest frameworks for sequential recommendation extract the user’s interests only from𝐿items with which user recently interacted. Historical interactions earlier than𝐿 items are discarded. The Global Context Extraction (GCE) Module tries to learn the global context embedding of items from all historical user interactions. Figure 3 illustrates how the GCE module processes historical interaction sequences with timestamps to get the weighted cooccurrence matrixAby taking three users A, B and C as example. Figure 2: The architecture of the proposed model GIMIRec. On the right-hand side of the Figure 3, a simplied graph convolution is used onAto obtain the global context embedding of each item. Here, the weighted co-occurrence matrixAis dened as a combination of𝑘-hop (𝑘 = 1, 2, 3) item co-occurrence matrices. For eachS, the𝑘-hop item pair is dened as a directed <𝑖,𝑖> (𝑛 ∈ N). For example, user B in Figure 2 has four interacted items, so the 1-hop item pairs are <𝑖,𝑖>, <𝑖,𝑖> and <𝑖,𝑖>, the 2-hop item pairs are <𝑖, 𝑖>, <𝑖, 𝑖> and the 3-hop item pair is <𝑖, 𝑖>. The detailed steps of the GCE module are as follows. Firstly, the𝑘-hop item pairs are ltered out from the historicalno interaction sequencesI=𝑖, 𝑖, · · · , 𝑖and the correspondingno timestamp sequenceT=𝑡, 𝑡, · · · , 𝑡,𝑢 ∈ U. In order to better capture the relation between items, a time interval threshold𝐿 is set. The two items inSwith the time interval less than𝐿 can become a co-occurrence item pair. Given the large number of items in the sequences, we only consider 1-hop, 2-hop and 3-hop item pairs to reduce the amount of calculation. For convenience, triplet collectionsR(𝑘 = 1, 2, 3)(Eq. 1) are used to save the id of two items and their time interval for three types of hop. Since the user id is no longer functional here, the superscript𝑢is replaced with𝑗, representing one co-occurrence of the item pair <𝑖, 𝑖>. For dierent 𝑗, the time interval of <𝑖, 𝑖> could be dierent. Figure 3: An example of Global Context Extraction Module. As illustrated in Figure 2, there are three users interacted with 9 items in total, thus initially generating 141-hop item pairs, 11 2-hop item pairs and 83-hop item pairs. Assuming that the time threshold𝐿, the nal number of item pairs would be 9, 3 and 1 for 1-hop, 2-hop and 3-hop respectively shown in Figure 3. Secondly, three co-occurrence matricesA(𝑘 = 1, 2, 3)are calculated. Intuitively, the item pair will have closer relation if they appear more frequently. Therefore, the entry inAwill be higher with shorter time intervals and higher frequency. For each collectionR(𝑘 = 1, 2, 3)dened above, the time interval of an item pair <𝑖, 𝑖> is rst calculated by Eq. 2 where𝑎and𝑏are hyperparameters satisfying𝑎 + 𝑏 = 1and the time interval of <𝑖, 𝑖> is normalized to [0, 1]. Eq. 3 sums up all the time intervals of <𝑖, 𝑖>. Eq. 4 symmetrizes each matrix to facilitate the subsequent graph convolution, which usually performs better on an undirected graph [12]. Thridly, the three co-occurrence matricesAare linearly combined to get A(Eq. 5) where 𝛼, 𝛽 and 𝛾 are hyperparameters. Intuitively, the item pair will have closer relation if they are nearby in the sequence. Therefore,𝛼is supposed to be the largest and𝛾is supposed to be the smallest. The identity matrix𝐼is added into Ain order to facilitate the graph convolution. Finally, inspired by GCN [12], LightGCN [5] and GES [30], a simple and lightweight graph convolution operation is used to obtain the global context embedding of each item (Eq. 6-7). The Laplace matrixAis rst calculated by the obtainedAand its degree matrixDand then multiplied with the embedding lookup table of itemsEto get the global context embedding of items An example of the above calculations is illustrated in Figure 3. The GCE module only runs once in the training phase and can also be used as an independent module for any other sequential recommendation models. After getting the global context embedding of items, our model further processes each user’s recent item sequence and timestamp sequence to learn the user’s personalized interaction preferences. Firstly, the sequence length threshold𝐿is set to intercept each user’s most recent interactions, which does not include the target item for training. If there are less than𝐿items, the padding strategy [2] will be used. Eventually, each user𝑢has a xed-lengthno sequenceS=𝑖, 𝑖, · · · , 𝑖and the corresponding times-no tamp sequence T=𝑡, 𝑡, · · · , 𝑡. Secondly, the time interval matrix is generated. This module resets𝑡asmin(|𝑡− 𝑡|, 𝐿)by using the threshold𝐿(dened in GCE) to emphasize the user’s recent interests. The interaction time interval of every two items in the user’s recent interaction sequenceSis put into the time interval matrix𝑇∈ R, which is a symmetric matrix. Thirdly, the time interval matrix𝑇is embedded to a tensor 𝑇∈ R, where𝑑is the dimension of embedding. The time interval aware attention mechanism [2] is used to get attention scoresS∈ Rfor each time interval in the sequence, denoted as where𝑊is a trainable parameter matrix and⊤represents the transposition of the matrix. Finally, the time interval embedding of itemsEin the recent sequence for each user𝑢is calculated through the broadcast mechanism in TensorFlow, which combines the attention scoreSand the time interval embedding tensor 𝑇. This module aggregates the global context embeddings and the time interval embeddings of the user’s recent interactive items to get user’s personalized embedding. Firstly, the items in each user’s recent interaction sequence are picked out from the matrixEto form a global context embedding matrix Efor the user 𝑢: Secondly, the time interval embeddings and the global context embeddings of each user’s recent interactive items are combined by addition to get the hybrid embeddings: Thirdly, a GNN [21] is used to further extract the user’s personalized representation from the recent sequence. To construct a graph, the adjacent items in the sequence are rst connected and then a virtual central nodee∈ Ris created and linked with all item nodes to capture the relation between items under larger than 1-hop. The embedding ofeis set to the average value of all item’s hybrid embeddings: The multi-head attention mechanism is then used to aggregate the item nodesEand the central nodeein𝐿steps. The layer-0 aggregation is the information initialization, where𝑖represents the𝑖-𝑡ℎitem, and the superscript(0)represents layer-0. The neighbor node𝑞, the center nodeeand the global context representationeare concatenated.𝑞and𝑘 are then sent into the multi-head attention mechanism at layer-𝑙 (𝑙 = 1, · · · , 𝐿), 𝑞= 𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑𝐴𝑡𝑡 (𝐾 = 𝑘, 𝑄 = 𝑞,𝑉 = 𝑘 Similarly, the central node eis aggretated as: e= 𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑𝐴𝑡𝑡 (𝐾 = 𝑘, 𝑄 = e,𝑉 = 𝑘 After the total of𝐿iterations, the nal personalized embeddings of the items are stacked in sequential order into the matrix E, which will be used in the next module. This module extracts theKinterests from the nal representation of recent sequence. Research has shown that the self-attention mechanism achieves better performance than capsule network in extracting user’s multiinterests [1]. Therefore, our model also uses the self-attention mechanism with two trainable parameter matrices𝑊∈ Rand 𝑊∈ Rfor user’s K interest extraction: To train the whole model, the optimal interest𝑜of the obtained Kinterests of each user is selected based on the target item (i.e., the next interactive ground-truth item) (Eq. 22), where the𝑎𝑟𝑔𝑚𝑎𝑥operation in TensorFlow is used on the multiplication ofEand the embedding of the target iteme∈ To be more ecient, the sampled softmax method [19] is used to train the model, with the goal of minimizing the following loss L(𝜃): L(𝜃) =− logÍexp(𝑜e)(23) The optimizer used in GIMIRec is Adam [11]. After the training, GIMIRec achieves Top-𝑁recommendation for all users in the test set. Due to the large size of the item pool, a GPU-accelerated method Faiss [9] is used to search for𝑁items that are most relevant to user interests. The recommendation results are given based on the Eobtained from Eq. 21, whereeis the embedding of the predicted item,Eis the 𝑘-𝑡ℎ interest embedding of the user 𝑢. This section introduces the experimental setup, evaluation methods and the performance comparison of the proposed GIMIRec model against six baselines, including the state-of-the-art multi-interest recommendation model. Moreover, the ablation study is carried out to verify the eectiveness of each innovation in GIMIRec. Datasets.We conducted experiments on three real-world datasets: Amazon-Books, Amazon-Hybridand Taobao-Buy. The Amazon dataset is divided into a series of sub-datasets according to the product category, among which Amazon-Books is the largest sub-dataset. Considering users are not only interested in books, we mixed ve smaller datasets (“ Electronics ”, “ Clothing, Shoes and Jewelry ”, “ CDs and Vinyl ”, “ Tools and Home Improvement ” and “ Grocery and Gourmet Food ”) to form a new dataset AmazonHybrid. Taobao-Buy is the sub-dataset of the Taobao dataset ltering out all purchases. We discarded the users and items with less than 5 interactions and illegal timestamps. The length of the recent interaction sequence𝐿is set to 20 for Amazon-Books and Amazon-Hybrid, and 50 for Taobao-Buy. The statistics of the datasets are listed in Table 1. We maintained the dataset division in the model validation of ComiRec [1] and PIMI [2]. In detail, we divided each dataset into a training set, a validation set, and a test set at a ratio of 8:1:1. To evaluate the model, we predicted the most recent 20% of user behaviors in the validation and test sets and used the remaining 80% to infer user multi-interest embeddings. Baselines.Comparison methods include: (1) GRU4Rec [6] (2015), the rst sequential recommendation method using recurrent neural networks; (2) Youtube DNN [3] (2016), a deep neural network sequential recommendation model; (3) MIND [13] (2019), the rst sequential recommendation model proposing the multi-interest framework; (4) ComiRec-DR and (5) ComiRec-SA [1] (2020) use dynamic routing mechanism and self-attention mechanism respectively to extract user’s multiple interests; (6) PIMI [2] (2021), the upto-date sequential recommendation model based on multi-interest framework. It is noted that in above models based on multi-interest framework, MIND and ComiRec introduce extra information such as user age, item category etc., which is not adopted in PIMI and our model GIMIRec. Evaluation Metrics.We used three common Top-𝑁recommendation evaluation metrics, Hit Rate @𝑁, Recall @𝑁and NDCG @𝑁, to compare the performance of models. Hit Rate @𝑁represents the percentage that recommendation results contain at least one ground truth item in top𝑁position. Recall @𝑁indicates the proportion of the ground truth items in the recommendation results. NDCG (Normalized Discounted Cumulative Gain) @𝑁measures the ranking quality in the𝑁items by assigning high scores to high rankings [2]. In our experiments,𝑁is set to 20 and 50 respectively. Implementation Details.GIMIRec is implemented with Python 3.8 in TensorFlow 2.4, and a CUDA compatible GPU is used on the Ubuntu 20.04 operating system. The embedding dimension𝑑is set to 64 for all datasets. The batch size on Amazon-Book, AmazonHybrid and Taobao-Buy are set to 128,128 and 256, respectively, with a dropout rate 0.1 and learning rate 0.001. In sampled softmax loss calculation, the number of samples is set to 10. Other hyperparameter settings are shown in Table 2. Finally, we iterated the training up to 1 million rounds as the previous models did. Recommendation performance.The recommendation results of all methods on three datasets are shown in Tables 3 and 4, corresponding to the value of𝑁=20 and𝑁=50. To be fair, all models adopt the same parameter settings. The result shows that the GIMIRec model achieves the best performance on all evaluation indicators for all datasets. The performance of GIMIRec is signicantly better than MIND and ComiRec even though there is no extra information introduced. Compared to the most advanced PIMI method, GIMIRec has great advantages, especially for Amazon data-sets. For Amazon-Hybrid, the performance improvement is the highest. The possible reason is that the multiple interests of users can be better represented from a rich variety of items. For Taobao-Buy, the performance improvement is not as good as that for Amazon datasets due to the shorter time span of dataset and sparser user interactions. The result fully veries the eectiveness of integrating the global context information. Ablation study.We conduct a series of ablation studies to observe the impact of parameters settings in the GCE module on the results. GIMIRec is compared with three GIMIRec variants, GIMI-Rec(no Interval, Number and Threshold), GIMIRec (no Interval and Number) and GIMIRec(no Interval). Among them, GIMIRecchanges GIMIRec by setting a=0 and b=1 in Eq. 2, indicating that the interaction interval information is not used when calculating the global context. GIMIRecfurther changes GIMI-Recin Eq. 3 by setting𝑞= 1, indicating that neither time intervals nor the co-occurrence number of item pairs are used. The dierence of GIMIRecand GIMIRecis that the former does not set the time interval threshold𝐿. The experimental results in Table 5 show that the integration of the number of interactions and time intervals in the global context extraction module in GIMIRec is eective and can further improve the performance. The use of the time interval threshold𝐿is valid to lter nearby item pairs. Impact of the number of interests K.Dierent from the general sequential recommendation, the multi-interest framework learnsKvectors for each user. Therefore, we discussed the impact ofKon the performance. The experimental results are listed in Table 6. It shows that the recommendation result of modeling multiple vectors for user preferences is better than that of modeling only a single vector. Moreover, the optimalKvalue of the model relies on the dataset. The recommendation performance is the best whenKis 4 for two Amazon datasets. However, for Taobao-Buy, most of the recommendation results are the best whenKis 8. The possible reason is that Taobao-Buy has more product categories, which could reect more interests of users. Table 6: The impact of the number of interests K . The best in each column is bolded and the second best is underlined. Impact of hyperparameters 𝛼, 𝛽 and 𝛾.In the GCE module, we used three hyperparameters,𝛼,𝛽and𝛾, to set the co-occurrence weight of 1-hop, 2-hop and 3-hop respectively. Table 7 lists the recommendation results of GIMIRec on Amazon-Hybrid under dierent combinations of three parameters. It shows that the result with all three matrices is better than that of only 1-hop matrix. The recommendation result is optimal when the proportion of𝛼,𝛽and 𝛾is 5:2.5:1. The result is consistent with expectation, that is, the 1-hop relation is the most important. Impact of hyperparameters 𝑎 and 𝑏.The𝑎and𝑏in Eq. 2 determine how the GCE module uses the time interval of historical interactions. The value of𝑎(the weight of time interval) has been proved to be eective in above ablation experiments. In Table 8, we experimented dierent combinations of𝑎and𝑏in Amazon-Hybrid. Table 7: Impact of hyperparameters 𝛼, 𝛽 and 𝛾. The best in each column is bolded and the second best is underlined. 5:2.5:1 7.033 10.750 13.304 11.642 17.262 21.186 Table 8: The impact of hyperparameters 𝑎 and 𝑏. Table 9: The impact of the time threshold 𝐿. As seen, the best setting is𝑎=0.5,𝑏=0.5 and a too large value of𝑎 would cause the deterioration in model performance, because it may weaken the impact of co-occurrence number. Impact of the time interval threshold 𝐿.Time thresholds are set in both the GCE module and the recent time interval representation module. We take Amazon-Hybrid as an example to observe the impact of dierent𝐿on the results (see Table 9). It shows that for Amazon-Hybrid the model with𝐿set to 64 performs best, probably because a larger time interval threshold may cause user’s recent interests to not be well emphasized, while a smaller threshold may cause the loss of part of user’s interests. We also found that the optimal𝐿in Taobao-Buy is 7 (see the settings in Table 2). The results demonstrate that the𝐿value is closely related to the dataset. Impact of the number of GNN layers.The aggregation module uses a multi-head attention mechanism for aggregation and the number of GNN layers directly aects the performance. We tried 1 to 5 layers of aggregation on the Amazon-Hybrid dataset and found that the performance is stabilized after 3 layers. The detailed results are shown in Figure 4. Figure 4: The impact of the number of GNN layers 𝐿. Time complexity analysis of the GCE module.In GCE, we conducted a lightweight graph convolution instead of a complete GCN. The overall time complexity of the latter isO(|𝜉 |𝑑𝐾 +|I |𝑑𝐾) [24,26], where|I|represents the number of items,|𝜉 |represents the number of interactions (edges),𝑑is the embedding dimension and𝐾is the depth (i.e., the number of layers) of GCN. However, in the GCE module, only 1, 2 and 3-hop relations are calculated, the process of graph convolution is simplied and only one convolution layer is used. As a result, the time complexity of GCE is reduced to O(|𝜉 |𝑑). This paper proposes a novel multi-interest framework for sequential recommendation, which leverages the historical interactions of all users to extract global context information, so as to better learn users’ multiple interests and provide better personalized recommendations. Specically, the co-occurrence number of item pairs and their time intervals from all interaction sequences are rst used to learn the global context embedding of items based on a simplied graph convolution operation. Then, the global context embedding is fused with the recent time interval embedding to get the personalized embedding for each user. Finally, a multi-interest framework is used to learnKinterest vectors for each user to provide recommendations. Extensive experiments verify that the potential information contained in the historical sequences can greatly benet sequential recommendation. Since the proposed module does not use any external information and has lightweight calculations, it can be easily transplanted to any sequential recommendation model. In the future, we will further explore the multi-interest framework by focusing on the non-linear relationship between multiple interests of users and ne-grained representation of items.