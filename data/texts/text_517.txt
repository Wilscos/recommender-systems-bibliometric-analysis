Ravi Krishna, Aravind Kalaiah, Bichen Wu, Maxim Naumov, Dheevatsa Mudigere, Misha Smelyanskiy, Kurt {ravi.krishna, keutzer}@berkeley.edu, {aravindkalaiah, wbc, mnaumov, dheevatsa, msmelyan}@fb.com Abstract—Neural architecture search (NAS) methods aim to automatically ﬁnd the optimal deep neural network (DNN) architecture as measured by a given objective function, typically some combination of task accuracy and inference efﬁciency. For many areas, such as computer vision and natural language processing, this is a critical, yet still time consuming process. New NAS methods have recently made progress in improving the efﬁciency of this process. We implement an extensible and modular framework for Differentiable Neural Architecture Search (DNAS) [1] to help solve this problem. We include an overview of the major components of our codebase and how they interact, as well as a section on implementing extensions to it (including a sample), in order to help users adopt our framework for their applications across different categories of deep learning models. To assess the capabilities of our methodology and implementation, we apply DNAS to the problem of ads click-through rate (CTR) prediction, arguably the highest-value and most worked on AI problem at hyperscalers today. We develop and tailor novel search spaces to a Deep Learning Recommendation Model (DLRM) backbone for CTR prediction, and report state-of-the-art results on the Criteo Kaggle CTR prediction dataset. In recent years, deep learning based approaches have become the de-facto standard methods in a number of ﬁelds, such as computer vision (CV) [13], automated speech recognition (ASR) [16], and natural language processing (NLP) [15]. Building state-of-the-art deep neural networks (DNNs) often involves a signiﬁcant experimentation process to ﬁnd the best DNN architecture suited to a task, considering the varying accuracy requirements as well as inference time constraints involved. This has motivated the development of neural architecture search (NAS) techniques, which aim to automate the process of ﬁnding the optimal DNN architecture for a given task. NAS has been successfully applied to many DNN application areas, including CV [18], [19] and NLP [34]. NAS techniques work by searching for the best architecture in a large search space. This is typically judged by comparing their search efﬁciency to other NAS methods and to random search [24]. A common metric used to measure this efﬁciency is “GPU-hours” of search time, which allows us to compare the machine times used by NAS algorithms to arrive at their results.Many NAS approaches are based on reinforcement learning (RL) [14], [18]. They operate in an iterative fashion where an architecture is sampled, evaluated and the learning is fed back into the sampling process. However, RL-based approaches have a higher search cost, as measured by GPU-hours, than gradient-based NAS approaches which aim to perform the NAS process through the gradientdescent optimization of some parameterized computational graph. For example, [24], a gradient-based approach, showed at least 400× speedup over an RL-based approach [14] when optimizing architectures to run image classiﬁcation on mobile devices. We apply the DNAS framework designed in this work to the problem of ads click-through rate (CTR) prediction. CTR prediction is the task of estimating the probability that a given user will click on a given ad at a given time. We can then predict estimated CTRs over a subset of our ad inventory and show the ad which maximizes the predicted CTR (a more complex ranking function can also be used [3]). Section VI-A deﬁnes the task of CTR prediction formally. Ads CTR prediction, and CTR prediction for arbitrary content more generally, is one of the backbone technologies for the large-scale Internet services, allowing for personalized user experiences. Given that the accuracy of CTR prediction is critical to the revenue of companies offering such services (e.g. a given percentage increase in the AUC of CTR prediction will result in 5x that percentage increase in CTR [11]), there has justiﬁably been a large investment in both people and machines to improve the models which predict CTR [12]. At the same time, the large scale of these services, which reach billions of globally distributed users, means that the inference latency and efﬁciency of CTR prediction models is a non-trivial consideration. Hard SLAs [30] also mean that the problem naturally becomes one of ﬁnding the architecture which maximizes accuracy subject to some latency constraint. A scalable NAS framework would improve the development velocity of of ranking engineers. All of this motivates the application of DNAS to the problem. CTR prediction systems have, in the last decade, evolved from systems based on collaborative ﬁltering or large-scale logistic regression [10] to models incorporating techniques from DNNs such as neural-collaborative ﬁltering (NCF) [31], and more recently heavily deep-learning based models. These include the Deep & Cross Network [22], Deep Interest Evolution Network (DIEN) [21], Deep Learning Recommendation Model (DLRM) [20], Time-Based Sequence Model (TBSM) [25], and many others. Most of these systems share a few components, including their input feature types and some common layers, which we outline in more detail in sections II-A and VI-B. The contributions of this work are as follows. The rest of the paper follows this general structure, with the exception of related work (section II) and the conclusion (section X). 1) Novel DLRM CTR prediction search spaces to apply DNAS to CTR prediction. The problem statement and search spaces for our application of DNAS to CTR prediction is contained in section VI. 2) Experimental results on a CTR prediction dataset demonstrating the efﬁcacy of our approach. These results are presented in section VII. Our best result for embedding cardinality search compresses the total size of embedding tables 15.14× with a relative 0.0012 increase in loss, demonstrating the promise of our approach (see sections VI-C3 and VIII-C). Our approach discovered recommendation models that beat the state-of-the-art in terms of logloss with signiﬁcantly fewer parameters (0.4442 vs. 0.447 of [26]; see VIII-D). Moreover, our approach discovered this model using 52× less computational effort (see section VIII-D). 3) An efﬁcient and extensible DNAS framework, implemented and open-sourced in PyTorch. We implement the DNAS algorithm as it is described in section III and [17], [24] and in such a way that arbitrary supernets for arbitrary application areas can be used. The framework is described in detail in section IV, and section V is dedicated to helping others extend the framework to their application areas. CTR prediction systems have historically evolved to leverage the technologies, computational power, and data available at any given time. Early collaborative-ﬁltering based methods [32], as well as those based on factorization machines [33], have evolved into more complex deep learning based methods due to such methods’ ability to handle larger and more complex datasets. Most CTR prediction systems today rely on two kinds of input features: dense and sparse (or categorical). The former are simply real-valued inputs. The latter typically represents a category instance (eg., the item being ranked, an attribute of interaction). Such categorical features are passed through an embedding table, which maps each category to a vector of real values, with the resulting embedding vectors being passed to the rest of the model. Note that such sparse features can easily have millions or billions of categories. The metrics used to make decisions concerning the implementations of CTR prediction systems are typically those focused on service revenue and user experience, such as revenue per impression and actual CTR [7]. For our purposes, however, we consider the CTR prediction objective to be logloss, or binary cross-entropy loss, which can be deﬁned for a vector of predictions p ∈ (0, 1)and a vector of binary labels y ∈ {0, 1}as: It is important to note that, in contrast to other ﬁelds such as computer vision where tradeoffs of many percentage points of accuracy for efﬁciency gains can be leveraged [6], recommender systems have the strictest accuracy requirements. A logloss difference of just 0.001 is practically signiﬁcant [22]. This has inﬂuenced the direction of research in the ﬁeld, which has focused on improving accuracy through new architectural elements designed to leverage certain types of data. We now examine some of the architectural innovations in this area. The Deep & Cross Network [22] used a parallel deep network (consisting of fully connected or FC layers) and cross network (consisting of novel cross layers) to better model feature interactions. After concatenating the dense features and all embedding vectors into a single vector x, we pass this through both networks, concatenate their outputs, and pass that to a fully-connected (FC) layer. A sigmoid function is used to generate the ﬁnal click probability. The cross layer, as the name suggests, models cross features through use of an outer product: CrossLayer(x) = xxw + b + x where x, w, b ∈ Rand w and b are weight and bias parameters for the layer respectively. The Deep Learning Recommendation Model [20], which will be covered in more detail in section VI-B, directly leverages pairwise dot products as in a factorization machine [33], but then passes these through a multi-layer perceptron (MLP). It uses a “bottom MLP” to directly process dense feature inputs, and uses a “feature interactions layer” to take pairwise products between embeddings. This is then concatenated with the bottom MLP’s output and fed to a top MLP, which outputs a click probability. Note that the DLRM can also use a concatenation-based feature interactions layer. The Multi-channel user Interest Memory Network (MIMN) [23] focuses speciﬁcally on modeling user history information to improve predictive performance. It focuses on designing speciﬁc mechanisms to allow for the retention of user history which can then be used to inform the “user interest representation.” The authors also focus on the inference time efﬁciency of their model for user history sequences of up to a length of 150. B. Neural Architecture Search (NAS) We provide a brief introduction to NAS methods and their application to a few different areas. This is not meant to be a comprehensive summary. Rather, we select methods that we hope will help readers get the most from our work. In their work on NAS with reinforcement learning [18], Zoph et al. propose a controller-trainer reinforcement learning (RL) approach in which a controller samples architectures to be trained and is then updated based the results that these architectures achieve. The authors use a recurrent neural network (RNN) as the controller, which generates the different architectural parameters of the search candidates. As models are trained, the controller is updated via the REINFORCE [9] algorithm. One major drawback of this approach is that architectures have to be sampled in a sequential fashion, and that a large number of such candidates must be trained before we can sample high-performing architectures. This is the challenge that gradient-based NAS approaches attempt to remedy. In contrast to an RL-based methodology, Wu et al. [17], [24] attempt to complete the entire search process with the training of one supernetwork (covered in much more detail in section III). Each layer of the supernetwork contains multiple candidate operators with the ﬁnal output being a weighted average of the individual operators. The weights are deﬁned as a Gumbel Softmax distribution over a vector of architecture parameters. The supernet then trains both the operator weights and the architecture parameters. Once this is complete, highperforming architectures can be generated by simply sampling operators from the Gumbel Softmax distribution. As we train the supernet, we anneal the “temperature” of the Gumbel Softmax operation such that it becomes closer to hard sampling over time. In this way, the search is completed in one training pass, after which architectures can be generated. ProxylessNAS follows a similar direction of using a supernetwork to essentially conduct the search process [19]. Both of these approaches allow for directly incorporating latency information in the loss function of the search process by differentiating the expected latency of the network to be sampled from the supernet. NAS techniques can be applied to many different areas. AdaBERT [34] focuses on designing architectures speciﬁc to individual NLP tasks based on a general pre-trained BERT model. BERT [15] models are pre-trained on large datasets in a semi-supervised manner and ﬁne-tuned on smaller datasets in a supervised manner; AdaBERT leverages this to design more efﬁcient models during the ﬁne-tuning process. In recommendation systems, AutoCTR [27] aims to search over the key “building blocks” of a set of DLRM-type recommendation models. This work does not use a supernetwork-based NAS approach, but rather samples architectures. It is able to maintain efﬁciency by conducting the search process over subsampled versions of the datasets. This approach of using a proxy task is common in NAS [19]. In this section, we introduce the DNAS algorithm in detail, in preparation for the following section IV which focuses on our implementation of DNAS through our framework. We begin by presenting the structure and usage of the DNAS supernet, the core of the algorithm, then we clarify some points regarding search spaces and sampling, and ﬁnally we present the end-to-end DNAS pipeline. The DNAS supernet is what allows the algorithm to arrive at an optimal architecture distribution during a single training process, instead of through sequential sampling and architecture training. An illustration of a supernet is shown in ﬁgure 1. Each layer of the supernet consists of a set of operators. The input to the layer is fed to each operator, and the outputs of the operators are combined to generate the input to the next layer of the supernet. Formally, let us denote the ith operator at layer l of the supernet as f. Then, we can express the output of layer l in terms of its input x as: where the weights mare themselves determined by a Gumbel Softmax over a vector of parameters θ: m= GumbelSoftmax(θ|θ) =exp((θ+ g)/τ)Pexp((θ+ g)/τ) The noise gis drawn from a Gumbel(0, 1) distribution. By changing the “temperature” parameter τ , we can adjust how “hard” or “soft” the sampling performed by the Gumbel Softmax function is. As τ approaches 0, the Gumbel Softmax approaches a hard categorical sampling of each operator, with the probabilities of selection corresponding to those of a regular softmax function over θ. During the training of the supernet, we perform soft sampling during each forward pass, allowing us to select a different combination of operators for each sample in the input batch. We exponentially anneal τ to zero as the supernet training progresses, and we hard sample operators from the distribution at the end of training to result in optimal architectures. This method of sampling also allows us to incorporate operator latency, FLOP, or other hardware deployment cost information directly into the loss function. The hardware cost of the network is approximated as a sum of the costs for each layer, which is itself calculated using a weighted sum. Thus, the cost of an architecture a which might be soft or hardsampled from the supernet is: where Cost(f) is the measured (or calculated) hardware cost for the ith operator in supernet layer l [24]. The supernet itself is then trained with a loss function which is a combination of the loss for the task on which the supernet is trained as well as the hardware cost loss. In our framework we allow for this to be using either of the below forms: It would be easy for any users of our DNAS framework to implement different TotalLoss formulations if needed. The construction of the supernet is based on the search space it is designed to search over. In our case, the search space is the set of operators which can be chosen from at each layer in the network. While we may not search at all over some layers, we could also have different numbers of possible operators at each layer. We use the term search space group when we search over a set of search spaces. A search space group might be, for example, searching over embedding dimensions for different features. By contrast, a search space for embedding dimensions should specify precisely which dimensions can be chosen for which categorical features. In section IV, we discuss how our framework allows for searching over the search spaces within a search space group. Algorithm 1: The DNAS pipeline. Q← ∅ ; for epoch = 0, . . . , N do τ ← Texp(−η × epoch); Train G with respect to w for one epoch; if epoch > Nthen end end Sample architectures a ∼ P; Push a to Q; for a ∈ Qdo Train a on Xto convergence; Test a on X; end Output: Trained architectures Q. We also discuss the supernet training procedure before presenting the detailed DNAS pipeline in the following subsection III-C. The DNAS supernet is actually never trained as a single unit. Instead, we alternatively train 1) the weights of all the different candidate operators w, and 2) the architecture parameters θ. Consistent with this methodology, these two sets of parameters are trained on two distinct partitions of the original training dataset. This prevents us from over-ﬁtting the architecture parameters to the training dataset [17], [24]. It also allows the DNAS algorithm to identify which architectures are prone to over-ﬁtting and reduces the probability that they will be sampled, which is discussed further in section VIII-C in the context of our embedding cardinality search space group experiments. Our DNAS pipeline illustrated in algorithm 1 is identical to algorithm 3 of [8] except for slight modiﬁcations for clarity. We note a few relevant points from the algorithm. The training dataset Xis randomly split to generate Xand X (see section IV-E). We typically evaluate sampled architectures on a separate validation dataset because many DNAS pipelines may be run for a given dataset, although the test dataset X is written in the algorithm. We avoid training the architectural parameters in the early warmup epochs (N) so that we do not train the architecture parameters before the weights are trained enough to produce reasonable outputs [24]. Otherwise, we might end up with architectures that perform well very early in training but perform poorly by the end of training. In this section we brieﬂy highlight the importance of the various knobs which must be tuned in order for DNAS to achieve good results. These include traditional hyperparameters such as learning rate, weight decay, optimizer choice, etc. It also includes DNAS-speciﬁc choices such as the split X and Xof the dataset, the structure of the search space group, speciﬁc choice of search space, and the parameters to trade off between task loss and hardware latency. While DNAS itself is designed to be a one-pass algorithm in the sense that the search is completed in one single training procedure, different choices of hyperparameters can have a very signiﬁcant impact on the results achieved. We discuss this further in the context of our ads CTR prediction results, along with other insights regarding developing successful search spaces, in section IX. In this section, we provide a detailed look at the implementation of the DNAS algorithm in our framework. The framework is designed to be usable for any application which can be implemented in PyTorch. It is designed to allow for a high degree of tuning and customization, as well as to efﬁciently run on multi-GPU machines. We structure this section as follows: ﬁrst, we describe the functionality of the implementation; second, we summarize the major components and how they work together to achieve that functionality; and ﬁnally, we present each component of the implementation in detail. We extensively use code snippets from the implementation, which is open sourced and can be found at [1]. Our framework is designed to allow for large-scale “pushbutton” experimentation and tuning. It natively launches multiple search space training jobs in parallel and multiple sampled architecture training jobs in parallel. That essentially allows the user to specify a search space group and particular search space conﬁgurations in addition to the supernet training hyperparameters and sampled architecture training hyperparameters. When the DNAS pipeline completes, the user is able to review all of the results using their own scripts if desired. This is all in pursuit of the goal of allowing both neural net designers and application experts to test new ideas without having to manually tune architectures and hyperparameters. We illustrate the end-to-end DNAS pipeline as implemented by our framework in ﬁgure 2. This shows multiple DNAS supernets being trained in parallel after which multiple architectures are sampled from each one. Subsequently, each of those are trained with different hyperparameters and thus allowing us to ﬁnd the trained architectures that yield the best tradeoff between accuracy and latency (or other hardware cost parameters, such as the total parameter storage of embedding tables). We show an illustration of this multi-GPU DNAS training on an 8-GPU machine in ﬁgure 3. This ﬁgure illustrates the running of multiple supernet training jobs in parallel to tune the search process. A similar process occurs to train the 8 (in this case) sampled architectures. Below we summarize the major components of our implementation, noting the relevant ﬁles and classes, and brieﬂy describing each one’s functionality. The below sections provide much more detail on each component: the search manager is described in section IV-C, the supernet in section IV-D, the training scripts in section IV-E, training manager in section IV-F, the DNAS pipeline manager in section IV-G and the utility scripts in section IV-H. The search manager manages the entire DNAS search process for training a single supernet training from start to ﬁnish. This includes initializing the supernet, creating the optimizers for the weights and architecture parameters, alternatively training the weights and architecture parameters, and hard-sampling architectures from the supernet (at any time during training, as speciﬁed on input arguments). The search manager is implemented in nas_searchmanager.py as a single SearchManager class. The supernet is the construct which is trained during DNAS. As mentioned above, the training of the supernet is managed exclusively by the search manager and any code implemented by the user need not have any interaction with the supernet other than initializing it, passing it to the search manager, and reading the architectures sampled from it. We implement a base supernet class, SuperNet in nas_supernet.py, which provides the Gumbel Softmax sampling and weighted sum utilities needed for any supernet. The user then creates a supernet for their speciﬁc search space group (see section IV-D) as a subclass of SuperNet. In our experiments we used many such supernets, including those implemented in nas_mlp.py, nas_embedding_dim.py, dlrm_supernet.py, and nas_embedding_card.py. Training scripts are the front-end interface through which DNAS search jobs and sampled architecture training jobs are launched. The principal purpose of these is to take arguments from the user. For our speciﬁc application, we have one train_dnas.py script and one train_sampled.py script, whose functions are to take arguments for a single DNAS supernet training and a single sampled architecture training, respectively. The ﬁrst script results in sampled architectures which are then trained using the second script. The training manager handles the parallelization of multiple jobs on a multi-GPU machine and implements the functionality displayed in ﬁgure 3. It is designed to be agnostic to the type of job that it is managing. We can use the same script to manage multiple supernet training jobs and multiple sampled architecture training jobs. We implement the training manager in a single tuning.py script. Finally, the DNAS pipeline manager coordinates the entire process. This is mostly a convenience allowing the user to launch a single script and avoid manually launching the training manager ﬁrst for the DNAS supernet training process and then to train sampled architectures. The pipeline manager takes all arguments related to the supernets, the sampled architectures and data preprocessing. It is implemented in run_dnas_pipeline.py script. In addition to these core components, there will likely be additional utility scripts needed for an implementation. In our case, we implemented an additional script dnas_data_utils.py to perform data preprocessing and splitting operations. Depending on the user’s application, they may be able to use existing functions from PyTorch or other libraries and may not need such scripts. The search manager class is designed with two goals in mind. First, that it should allow the user the maximum possible customizability of the search process, such that the same search manager can be used for any DNAS application with minimal changes. Second, that it should manage the search process end-to-end, requiring minimal user interaction. To this end, we have implemented the SearchManager class such that there are only two user-facing methods. The ﬁrst is the constructor, SearchManager.__init__() which allows for the customization. The second is the SearchManager.train_dnas() method which runs the DNAS process and terminates after all architectures have been sampled. We show an example in listing 1 of how SearchManager can be used (without complete arguments to SearchManager.__init__() as there are far too many to include in the listing). Instead, we list and categorize these arguments in table I. We split the remainder of this subsection into two pieces: ﬁrst, we cover the architecture of the SearchManager class and how it works internally; and second, we describe some of the more important arguments to the SearchManager constructor. 1) Code architecture: SearchManager has two main functions: train_dnas() and sample_archs. The ﬁrst manages the entire search process end-to-end and calls the second at the end of each epoch of training in order to sample any architectures that should be at that time. train_dnas itself relies heavily on run_one_dnas_step() which, as the name indicates, runs one DNAS supernet training step. The main purpose of this function is to simplify the structure of train_dnas(); it optimizes either the operator weights or the architecture parameters as needed and computes the combination of the task loss and the hardware cost. The operation of train_dnas() is determined by several input arguments to SearchManager which are then fed to SearchManager.calc_epoch_training_params(). This function generates a series of conﬁgurations for each training epoch, including what should be trained (op. weights or arch. params), the Gumbel Softmax temperature to use during the epoch and how many architectures should be sampled when the epoch is complete. The SearchManager actually allows for fractional epochs so that we can alternate between training the weights and architecture parameters every 0.25 or 0.5 epochs instead of just every epoch. This is useful for our CTR prediction application because the number of epochs used is typically very low (e.g. 5). SearchManager is fairly simple, its usefulness lies in its customizability. To illustrate this, and also to provide some context for users of the code, we list the input arguments to the SearchManager constructor in table I. We organize these by their usage. There are two pieces to our implementation of the DNAS supernet in PyTorch (corresponding to the below two sections). The ﬁrst is the supernet base class, SuperNet, which provides the basic functionality needed for any other supernet. The second is the supernet for a particular application, for example EmbeddingDimSuperNet in our case to search over embedding dimensions. These speciﬁc supernets can also be combined into “super-supernets” which can include multiple supernets combined together; in our case, we use this to search over MLP sizes for both the top and bottom MLPs in a DLRM. The same DLRM supernet can also support embedding search spaces, though currently not at the same time as MLP search. 1) Supernet base class: The goal of the base class is to provide the key functionality that any other supernet will need to use. The functions included in SuperNet are thus soft_sample(), hard_sample(), and calculate_weighted_sum(). The soft_sample() function soft-samples from the Gumbel Softmax distribution using some temperature temp creating random architecture weights for each element in the batch as the DNAS algorithm requires. hard_sample() is called when an architecture needs to be sampled from the supernet and it hard-samples from the Gumbel Softmax distribution to generate a one-hot set of architecture weights for each supernet layer. This operation ﬁxes the operator of the sampled architecture at each layer. The calculate_weighted_sum() function is critical to the operation of any supernet. This function implements the weighted sum of operator outputs from equation 2 which generates the output of each supernet layer. It takes the following as inputs: weights, which should be the output of soft_sample() for one layer; mats, a list of Pytorch tensors of which the weighted sum should be taken; and n_mats, which should be equal to len(mats). Note that every tensor in mats should be of exactly the same size. 2) Speciﬁc supernets & combinations: When creating a supernet for a speciﬁc application, one must ﬁrst subclass SuperNet, and also set a few class variables in __init__: theta_parameters, mask_values, and num_mask_lists. We show an example of this with EmbeddingDimSuperNet which searches over the embedding dimensions (see listing 2). The forward() function of the supernet soft-samples with the speciﬁed temperature if the sampling type is soft sampling. We usually choose to store the hardware cost computed at each training iteration as curr_cost, as this allows for compatibility with “supersupernets’ which expect their sub-modules’ forward() functions to return only one tensor. We then later retrieve the curr_cost variables from each supernet to compute the overall cost. There are two Python training scripts that we use in our implementation. The ﬁrst one, train_dnas.py, takes in arguments via argparse and launches a single DNAS supernet training job by creating a SearchManager and calling train_dnas(). The second one, train_sampled.py, also similarly takes in arguments, but launches a single sampled architecture training job on its own (although it could use external objects or libraries if needed). Both scripts need to save a speciﬁc ﬁle when they have ﬁnished running, which may for example contain various information about the training job (e.g. training loss, validation loss, sampled architecture structure, sampled architecture latency). The training manager looks for this ﬁle and, if it is found for a particular job, concludes that the job has completed in which case it launches the next job in its queue. These two scripts are the jobs that the training manager allocates to GPUs. Typically the user would never need to directly launch a train_dnas.py or train_sampled.py job. Instead, they provide two conﬁguration ﬁles to the DNAS pipeline manager deﬁning what jobs should be launched to tune the search space, supernet hyperparameters and the sampled architecture hyperparameters. The pipeline manager launches the training manager for each step of the DNAS pipeline (supernet training and sampled architecture training) with the appropriate conﬁguration ﬁle and the training manager will launch the actual jobs. Below, we brieﬂy cover some of the arugments to train_dnas.py and train_sampled.py as well as their structure. 1) DNAS supernet training script: The DNAS supernet training script needs to take in all the arguments necessary to launch a single supernet training job. These include arguments which specify the search space, the hyperparameters (learning rate, weight decay, etc.) for supernet training, data preprocessing methodology, and so forth. Note the distinction mentioned in section III-B between a search space and a search space group. A different search space group would require a separate training script, whereas many search spaces can be specifed with arguments to a single script. 2) Sampled arch. training script: The sampled architecture training script needs to take in all the arguments necessary to launch a single sampled architecture training job. These include arguments which specify the hyperparameter (learning rate, weight decay, etc.) for the sampled architecture training, data preprocessing methodology (which may be materially different than the preprocessing methodology for the supernet training) and of course the architecture to be trained itself. Listing 2: EmbeddingDimSuperNet subclasses SuperNet to search over embedding dimensions. Below is EmbeddingDimSuperNet.__init__. → t h e n t a k i n g a w e i g h t e d sum o v e r t r u n c a t e d and t h e n ze r o − p a dd ed v e r s i o n s o f → t h e maximum d i me n s io n v e r s i o n o f an embed di ng v e c t o r . → c u rr d i m i n s e l f . d i m o p t i o n s ] ) , r e q u i r e s g r a d = F a l s e ) → n u m d i m opt i o ns ) , r e q u i r e s g r a d = T rue ) ] ) The training manager is in a script called tuning.py. It handles launching the jobs speciﬁed in a conﬁguration ﬁle so as to keep all its assigned GPUs occupied. The inputs to the script are 1) the path to the conﬁguration ﬁle itself and 2) an experiment ID which is used in all ﬁles related to the jobs that will be launched. It ensures that no ﬁles from prior experiments are overwritten. We provide an example conﬁguration ﬁle in listing 3. The ﬁrst line of listing 3 just tells the training manager what script to run and the version of python to use (e.g. users could specify their own virtual environment of python with different packages). Note that this codebase is designed to work with and has been tested only with Python 3. The second line of the conﬁguration ﬁle consists of the arguments to the script on the ﬁrst line. Three parameters must be replaced with special tokens: EXPERIMENT_ID, HOST_GPU_ID, and SAVE_METRICS_PARAM. This is because these parameters are replaced with their actual values on-the-ﬂy. For the experiment ID, this is provided when the DNAS pipeline is launched; for the latter two these values change as each job is launched on a different GPU and saves its metrics to a different path. Finally, the parameters which we want to tune are speciﬁed by curly brackets as weights_lr. The values separated by commas inside these brackets represent the different jobs that will be launched. Note that the training manager does a direct string replace of the curly brackets expression with each entry of it. For example, if we want to tune over adding an option, that can be speciﬁed inside this option as {--option_to_add, } . The next two lines are reasonable self-explanatory: they outline which GPUs the training manager owns and how many jobs per GPU to launch. The last line is needed for scripts Listing 3: Example conﬁguration ﬁle which can be input to tuning.py which will launch many train_dnas.py jobs; note that lines 2-9 would be on the same line of the ﬁle but are split here for readability. Also note that, for brevity, many arguments are skipped. that save output in each epoch (eg., train_sampled.py). It informs the training manager about the ﬁles to look for so that it can aggregate results after each epoch. The training manager is implemented in a fairly straightforward manner. It ﬁrst generates all possible jobs to be launched from the contents of the curly brackets expressions. It then puts all of these in a queue and starts launching them on the available GPUs. As soon as it detects the saved ﬁle from a particular job (indicating its completion), it launches the next job available in its queue. It can detect ﬁles indicating that a particular job has encountered a GPU out-of-memory (OOM) error and add the job to the back of the queue and relaunch it. This feature requires modiﬁcations to the script being run with tuning manager so that it catches the OOM exception (usually a RuntimeError) and saves the ﬁle indicating the error. The pipeline manager, run_dnas_pipeline.py, is a fairly simple script. It takes in all arguments for data preprocessing, as well as the training manager conﬁguration ﬁles for supernet and sampled architecture training and manages the launching of the training manager (tuning.py) for both kinds of training. It also updates a logﬁle with the status (e.g. started supernet training, started sampled architecture training, ﬁnished sampled architecture training). In our case, we implement a further script called run_kaggle_jobs.sh which ﬁlls in many of the arguments to the DNAS pipeline manager which are the same across many of our experiments and only takes as arguments the paths to the conﬁguration ﬁles, the search space group, and a few others. However, this additional script is not necessary for all implementations (see the sample extension described in section V). Utility scripts needed for a DNAS implementation are likely to be for data preprocessing. In our case, dnas_data_utils.py contains the function get_dnas_dataloaders which, given command line arguments, preprocesses data and returns Pytorch dataloaders for either DNAS supernet training or sampled architecture training. Note that the test dataloader is only used if speciﬁed in the arguments to train_sampled.py. Generally we would only set this --check_test_set_performance ﬂag once when we test the best architecture from sampled architecture tuning (loading it from a checkpoint). Other users may ﬁnd that the built-in dataloaders from PyTorch work with few enough modiﬁcations that dedicated utility scripts are not needed. We recommend implementing at least basic utility scripts. For example, this could be to handle splitting the data into weights and architecture parameter training. In this section we cover what it takes to extend the DNAS framework that we have open-sourced to different problems, which may be from any domain and using any backbone network, as long as the search space can be represented as a supernet of some type. This includes both a description of the required implementation components which have to be added for a new application, as well as a code sample which we provide to illustrate how this would actually be done. There are only a few implementation requirements which are needed to extend DNAS to another application problem. We enumerate and brieﬂy explain these below. 1) Supernet subclass corresponding to the search space group. This would be the analog of the EmbeddingDimSuperNet example shown in listing 2. Note that our framework allows this to consist of any valid PyTorch operators. It could be MLP-based, as our supernets are, or convolution based, LSTM-based, etc. 2) Sampled architecture module which is the PyTorch nn.Module that will be used to create and train sample architectures. It should take arguments that are generated when an architecture is sampled from the supernet. 3) DNAS training script to take in arguments and pass them to the search manager. As mentioned in section IV-E1, the main purpose of this script is to make running jobs easier via Python’s argparse library. 4) Sampled architecture training script to take in arguments and train a sampled architecture based on them. This script will require a training loop for the given application, which may include any advanced components of the architecture training process that are not present during the search process itself (e.g. changing the batch size during the course of training). 5) Conﬁguration ﬁles: one to search across different search spaces within a search space group and supernet training hyperparameters. Another ﬁle to search across sampled architecture training hyperparameters. Of course, multiple ﬁles are likely to be used as the search over the search space group is reﬁned and better hyperparameters are found. 6) Utility script(s) these are needed for any custom dataloaders or dataset implementations which would be used in the DNAS or sampled architecture training scripts. We implemented this in dnas_data_utils.py as mentioned in section IV-H. While adding a data utility script may not be required for all applications, it is recommended to use one if only to keep the codebase organized given the need for separate weight and architecture parameter training datasets. 7) Customizations to the SearchManager for different types of data, number of inputs to the supernet, different loss functions, etc. In addition to these minimum implementation components, one can make further changes to the framework to customize it for a particular application. One important area for tweaking is in the combination of the task loss and hardware cost into a single loss function, as different combination functions may suit different applications better. We provide a code sample in our repository (see folder sample_extension) which shows how our framework could be used to perform search over convolutional networks. In this example, we search over a 5-way classiﬁcation dataset of random 3 × 50 × 50 images. Note the following correspondence between the implementation requirements mentioned in the above section V-A and the custom ﬁles in the sample extension: 1) Supernet subclass: cnn_supernet.py. 2) Sampled CNN module: cnn_sampled.py 3) DNAS training script: train_dnas_cnn.py. 4) Sampled arch. training script: train_sampled_cnn.py. 5) Search space conﬁg. ﬁle: config_cnn_dnas_search; Sampled arch. conﬁg. ﬁle: config_cnn_sampled_search. 6) Data utility script: dnas_cnn_data_utils.py. 7) Customized search manager script: nas_searchmanager.py There are other ﬁles in the sample extension that are copied from the main DNAS implementation as these are dependencies required for any implementation (e.g. nas_supernet.py). We also copied utils.py. However, we deleted some functions from it that are unnecessary in the sample extension (e.g. to calculate latency for an MLP search supernet). In this section we present our methodology for applying the DNAS framework described in prior sections to the problem of ads click-through rate (CTR) prediction. We start by describing the motivation for and constraints of the problem, and then cover our backbone architecture and search spaces. As shown in ﬁgure 4, the process of determining the best ad to show to a user in a given context involves multiple machine learning or deep learning based models, with service level agreements (SLAs) enforced on the services that run these models. This forms the crux of the problem of efﬁcient CTR prediction: recommending the best quality ads while still meeting SLA constraints. These constraints are required to maintain a proper experience both for users of the Internet service as well as for advertisers (i.e. customers) bidding for inventory. SLAs are typically tens to hundreds of milliseconds [30]. Some concrete examples of this range can be found in [29]. Given the complexities and constraints of CTR prediction at the scale that is required for major Internet services, it is important to recognize the opportunity for NAS to make material contributions. As noted previously, very small improvements in CTR prediction accuracy can translate to very signiﬁcant revenue changes for Internet companies, and can equivalently signiﬁcantly improve user and customer experience. The importance of recommender systems in Internet services, as well as the strict latency constraints they face, motivates the need for a NAS algorithm to make improvements in the paretooptimal curve of accuracy vs. latency. Our backbone architecture, that is the architecture that we use as the starting point for all of our supernets, is the Deep Learning Recommendation Model (DLRM) [20]. We described this model brieﬂy in section II-A concerning related work in recommender systems. We will provide more detail in this section. Figure 5 provides an overview of the model structure. We provide further details on the main components of the model using the dimensions noted in ﬁgure 5. Their relevance to the search spaces that will be enumerated in section VI-C: 1) Embedding tables, which could be numerous, convert a multi-hot sparse feature vector (in our speciﬁc application one-hot) to a single dense vector of dimension d. Each embedding table has a certain number of rows and may be expressed as a matrix E ∈ R. The embedding lookup for a given sparse feature vector x operation is a vector sum Ex. Typically, x will have a small fraction of 1s and almost all 0s, corresponding to certain categories. For example, the location of the 1 may indicate for which user ads are currently being ranked. In practise, due to the sparsity of the matrixvector product, the operation is implemented as a memory lookup and vector sum. 2) The bottom MLP directly processes the dense feature inputs to the DLRM. These dense features are realvalued and can be represented by f∈ R, where nis the number of dense feature inputs to the model. The bottom MLP consists of sequential fully connected layers and ReLU activation functions. 3) The feature interactions layer combines information from the bottom MLP output and the embedding lookups, all of the same dimension d. This layer is designed to be general; however, we use the dot-product based layer in our application. Given nembedding tables (sparse features), the unbatched input to the feature interactions layer will be a matrix F ∈ R. The layer output is the ﬂattened (to a vector) upper triangular portion of F F, possibly including the self-interactions (i.e., diagonal elements). 4) The top MLP takes as input the concatenation of the bottom MLP output and the feature interactions layer output. Similar to the bottom MLP, it consists of sequential FC layers and activation functions. Different from the bottom MLP, the activation function for the last layer is a sigmoid, so that the output of that layer will be a click probability p∈ (0, 1). In this section we discuss the search spaces that we use in this work. We focus on search spaces covering the main components of the DLRM model. We exclude interactions layers from the search space which is considered in more detail in [27], [28]. Our MLP search space is designed to minimize the computational requirement (i.e. FLOPs and latency) needed to achieve a particular accuracy. Further, we believe that our search spaces which focus on embedding tables may be of particular interest for future work, as embedding tables speciﬁcally present signiﬁcant storage and memory bandwidth constraints. 1) MLP Search: The particular challenge posed by MLP search is that differing channel dimensions for the outputs of differing operators makes the weighted-sum approach infeasible without changes. One solution is that adopted by FBNetv2 [2] through operator sharing, truncation, and zero-padding. We use a novel solution with a more expansive search space that we term FC-of-FC. This solution fully represents every combination of FC operators and thus every possible MLP from the search space directly. s(s+1)/2 As the name suggests, FC-of-FC consists of a fullyconnected network of fully-connected layer operators, allowing for any combination of a ﬁxed number of FC sizes across a ﬁxed number of supernet layers to be sampled. Figure 6 illustrates how an FC-of-FC supernet is constructed using an example. As shown in ﬁgure 6, every FC-of-FC supernet has a ﬁxed input and output dimension. In between these two endpoints, we search for what sequence of FC sizes (and how many) would be optimal according to some combination of task loss and hardware or inference cost. The ﬁgure represents the supernet as a DAG, where nodes are tensors which are weighted sums of operator outputs and operators are represented by edges, which are labeled as input size → output size. For each possible output size, we have an operator corresponding to every possible input size. We also have special skip layer operators, denoted in black, which operate between nodes of the same size and skip application of the FC function. Then, the output of a given node is a Gumbel Softmax weighted sum over the outputs of the operators which point to that node. When we wish to hard-sample networks from this supernet, the process is somewhat more complex than usual. We hardsample an operator choice from each place where a weighted sum would be conducted, that is, the input to each node. However, this does not determine the network alone. Instead, we work backwards. So, the input to the node labeled 50 might be selected to be 16. Then, we select the edge which was hard-sampled as the input to this 16 node as the prior layer, and proceed until we reach the ﬁrst either 16 or 32 node, at which point the prior node must be 10 and the edge must be whichever connects 10 to the selected 16 or 32 node. The latency (or any cost) calculation methodology for this kind of FC-of-FC supernet is also different than would usually be done. We cannot just take a weighted sum because, unlike regular supernets, the operator selections at each layer are not independent. So, we have to multiply the selection probabilities starting from the output of the supernet and then multiply those adjusted probabilities by the latencies of each operator. This is very simple mathematically, but is a bit messy to implement, especially because the Gumbel Softmax operation randomizes selection probabilities within a single training batch. We implement this as a function in utils.py. In our experiments the MLP supernet is never used on its own, but rather as part of the DLRM. This is done by replacing the ﬁxed bottom and top MLPs with MLP supernets that ﬁnd the best conﬁgurations for these blocks. 2) Embedding dimension: Our embedding dimensions search space is speciﬁcally designed to reduce the parameter storage requirements for embedding tables. We do this by searching over a set of possible embedding dimensions for each table. Note that this would be infeasible with the DLRM setup without changes due to the dot-product based feature interactions layer. As shown in ﬁgure 7, we use a zero-padding approach similar to FBNetv2 [2] for embedding dimension search. This works by having a single embedding table of the maximum dimension and replacing elements in an embedding vector beyond each dimension in the search space with zeros. The weighted sum with Gumbel Softmax derived weights is taken over these vectors. After we have selected the dimensions for a sampled architecture, we maintain the same zero-padding up to the largest dimension of any embedding table in that network. While it would be possible to construct a supernet that required all embedding dimensions to be equal, we believe that setting dimensions speciﬁc to certain features allows for more parameter savings. As the embedding dimension search space is a reasonably straightforward application of the DNAS framework, we do not spend much more time on it. We do note, however, that the cost function we use for the embedding dimension search space is the number of parameters in all embedding tables in the network. 16à161616 32à32 3) Embedding cardinality: Our embedding cardinality search space is designed to search over the hash size that is used to reduce the cardinality of sparse features. For example, for a hash size of H = 100, we would take x mod H before looking up a given index x, which effectively reduces the number of categories in the feature to at most H. This hash size represents a tradeoff between the degree of personalization an embedding table affords and the amount of storage that it requires. The design of the search space is fairly simple. We store separate embedding tables (with the same dimension) for each possible hash size. Then we look up the vectors at index x mod Hfor each possible hash size Hin the search space and take a weighted sum over these vectors. Figure 8 provides an illustration of this. While it is possible to combine these different candidate tables into a single table and look up the different indices within that table, we felt that this would not allow for an effective search. Further, the storage overhead at search time is likely to be minimal because in our experiments we typically search over hash sizes decreasing by factors of 10 from the original cardinality of each feature. This means that the total storage requirement is less than or equal to= 1.11× that of the original feature. In this subsection we introduce the dataset we use in this work, Criteo Kaggle, as well as the preprocessing that we perform on this dataset. 1) Criteo Kaggle: The Criteo Kaggle datasetwas originally released by the advertising company Criteo as part of a Kaggle challenge. Because the test set labels from this challenge were never released to the public, the training set is itself split and used in many works as a benchmark for CTR prediction [20], [22], [27], [28]. We refer to this henceforth as the dataset. The dataset itself consists of 46M chronologically ordered click records from a 7-day period. Each record consists of 13 dense features and 26 sparse or categorical features as well as a binary click label, although some features may be missing from any given record. The categorical features themselves have a wide range of cardinalities. The minimum cardinality is just 3, while the maximum is 10131227, and the total number of categories across all sparse features is 33762577. Depending on the embedding dimension used, the total sparse feature storage can easily be in the gigabytes or even tens of gigabytes (and note that this dataset is orders of magnitude smaller than industry-scale datasets). Approximately 74.4% of records in the dataset have a 0 (i.e., no-click) label and the remaining ones have a 1 (i.e., click) label. In accordance with [22] and especially [20] as we directly leverage the DLRM Criteo Kaggle data processing code in our dnas_data_utils.py, we split the dataset as follows. The ﬁrst 6/7 are used as training data and the last 1/7 is split randomly (i.e. not chronologically) into 50/50 validation and test dataset. Note that the chronological separation of the training from the validation and test dataset is important because CTR prediction models face the same issue in practice: changing data distributions over time. The validation and test datasets are drawn from the same day so that they are i.i.d. 2) Preprocessing: The preprocessing we perform is reasonably straightforward and contains no novelty. We brieﬂy describe it below for completeness. We follow the preprocessing methodology used in [20] which in turn closely follows the preprocessing used in [22]. For dense features, missing features are replaced with -1, and the rest of the features are transformed by the function f(x) = ln(1 + x). We do not perform any subsampling of the data, either to increase the proportion of click labels, or to decrease the total number of records used during training time. Doing the latter especially would reduce the search time, and this is used in [27]. That being said, our train_dnas.py and dnas_data_utils.py script support both subsampling methodologies and the DLRM data processing itself supports [0:128] the ﬁrst. TABLE II: Backbone search architectures. TBS = To Be Searched, as in [24] This section presents the architectural parameters that we used for our experiments with all three search space groups mentioned in subsection VI-C. Table II presents the backbone architectures and the components that are part of the search process for each search space group. We now describe the precise search spaces, as well the tuning process that was used in conjunction with each. However, we do not cover all the parameters here. They may be found directly in the open-sourced code tuning conﬁguration ﬁles or as default parameters speciﬁed in the code. 1) MLP search conﬁguration: Both the bottom and top MLP layer size options were [128, 256, 512, 1024]. Both MLPs are limited to 5 layers in our search. This means, for example, that one possible bottom MLP conﬁguration of the maximum length might be [13, 128, 128, 1024, 1024, 32]. Because the MLP search space group incorporates layer skipping, a possible bottom MLP conﬁguration of the minimum length might be [13, 128, 32]. We tuned the temperature decay rate and weights LR arguments in train_dnas.py. The former was one of [0.1, 0.2] and the latter was sampled from [1.0, 1.5, 2.0]. We tuned only the LR when training sampled architectures. It was allowed to be one of [0.25, 0.5, 1.0, 2.0]. 2) Embedding dimension search conﬁguration: The dimension options for each sparse feature were [8, 16, 32, 64, 128]. We tuned the same hyperparameters in the same manner as we did for the MLP search space, for both supernet training and sampled architecture training. 3) Embedding cardinality search conﬁguration: The cardinality options were [1.0, 0.1, 0.01, 0.001], expressed as a factor of reduction from the original cardinality found in the dataset. This means that for a feature with an original cardinality of 10, we would search over candidate cardinalities [10000, 1000, 100, 10]. We tuned the same hyperparameters in the same manner as we did for the MLP search space for both supernet training and sampled architecture training. We now present the results from the experiments described previously in the section. Most of our analysis uses validation logloss as our task performance metric, with various efﬁciency metrics used corresponding to the search spaces groups tested. We also conduct test-set evaluations on the most accurate (overall) as well as most efﬁcient (selecting the tuning conﬁguraiton which performs best on the validation set) architectures for each search space group. This is a total of 3 search space groups × 2 architectures each = 6 test set results. Note this is a very small fraction of the total sampled architecture results: in total we have 3 seach space groups × 2 temperature decay options × 3 weights LR options × 4 sampled architectures × 4 sampled training LR options = 288 architectures, all of which have validation loss calculated at every one of 6 training epochs. We ﬁrst present statistics on the validation loss, calibration, and efﬁciency metrics for the three different search space groups. These are found in tables III, IV, and V for MLP search, embedding dimension search, and embedding cardinality search respectively. Note that these statistics are over each sampled architecture’s minimum validation loss epoch across the 6 epochs of training. For the purposes of analyzing and attempting to interpret the actual DNAS search process, we also provide heatmaps representing the weights parameterizing the Gumbel Softmax operations in supernets for the best-performing architecture (by loss) in each search space. Please see these in ﬁgures 9 and 10 for the embedding dimension and cardinality search spaces respectively. Note that these heatmaps do not depict the MLP search space which is discussed more in subsection TABLE IV: Statistics for embedding dimension search. Note that average dimension is a valid metric because all cardinalities are ﬁxed at 20K. Average dimension is directly proportional to total embedding storage. VIII-A. Finally, we report the test set results mentioned above in table in VI. The discussion and analysis of the results is presented in the following section VIII. We ﬁrst note that we did not provide a heatmap illustration of the architecture parameters for the MLP search space. This is because the architecture parameters for this search space, as is explained in section VI-C1, do not operate across an entire layer but rather weigh the inputs to an operator of a particular size. Thus, these architecture parameters are not directly interpretable. They also do not directly correspond to operator sampling probabilities due to the way in which FC-of-FC works (sampling probabilities are not independent across layers). From the statistical results in table III, we can see that the MLP search process has performed well, achieving a minimum validation log loss of 0.4467. We can also see a 3.3× range of FLOPs between the minimum and maximum, indicating that the search space itself would be useful in reducing actual computational requirements. Interestingly, the test set results show much less variation across FLOPs between the most accurate and most efﬁcient model. In fact, the most accurate model uses close to the minimum number of FLOPs found in TABLE V: Statistics for embedding cardinality search. TABLE VI: Test set results for selected architectures. Efﬁciency metric is total MLP FLOPs for MLP search, average dimension for embedding dimension search, and total # categories for embedding cardinality search. Note that for this table we put the actual calibration instead of its distance from 1, as there is no need to calculate statistics. any of the MLP architectures. The architectures achieve very different logloss with the most accurate achieving 0.4457 and the most efﬁcient achieving 0.4520. Due to the minimal difference in FLOPs, we believe that this is due to a difference in hyperparameters between the models and not due to any effect of the lower representational capacity of the more efﬁcient model. This may also point to a potential lack of sampling accuracy of FC operators, or insufﬁcient training of the architecture parameters, thus leading to operators being sampled suboptimally. We start by examining the results from table IV. We can see a signiﬁcant variation in the validation logloss: a range of [0.4470, 0.4532]. The calibration values also follow the logloss numbers, with worse calibration (farther from 1.0) seen for architectures with higher logloss. We can also see a signiﬁcant range of the average embedding dimension, from 67.07 to 89.54. Also, notably, the mean and median average dimensions are closer to the maximum dimensions than the minimum, suggesting that the optimization of dimensions is non-trivial. Interestingly, the average embedding dimension is higher for the architecture that achieves the logloss of 0.4532 than it is for the architecture that achieves the logoss of 0.4470. This suggests that rather than simple compression, what is required to both decrease loss and to decrease model size is compressing the correct features by the correct amount. On this point, we can look at the architecture parameters represented in ﬁgure 9. DNAS places an emphasis on using larger embedding dimensions for the features at indices 2, 11, 20, and to a lesser extent, 3, 6, 9, 10, 14, 23, and 25. The actual cardinalities of the features are [1460, 583, 10131227, 2202608, 305, 24, 12517, 633, 3, 93145, 5683, 8351593, 3194, 27, 14992, 5461306, 10, 5652, 2173, 4, 7046547, 18, 15, 286181, 105, 142572]. The average log base 10 cardinality of the features with indices listed above is 5.473, and the average for those not is 2.368 (the numbers are 6.925 and 3.124 respectively if we exclude the second set of indices). Needless to say, this difference of 100x clearly shows that DNAS has identiﬁed that high-cardinality features need a larger embedding dimension in order to result in embedding with the necessary representational ability. Looking at the test set results, we can also see that it is proper optimization of the dimensions for speciﬁc features that makes models perform well on the CTR prediction task. The best-performing model achieves a 0.4460 test logloss, but is only 7% larger in parameter storage than the most efﬁcient model, which actually achieves a very similar test logloss: 0.4463. Note that both of these logloss numbers are quite low themselves. Again, we start by examining the results from table V. One immediate observation is that the maximum loss is 20.55, which is an absurd value that can only indicate complete learning failure or divergence during training. As a result of such outlier values, the mean logloss is a useless indicator, as is the mean calibration. Also important to note is the degree of variation in the embedding cardinality: we can see a > 20× range in the number of categories, offering the largest opportunities for model compression of all of the search spaces groups used in this work. We also note that the minimum validation logloss is the best we have observed so far: 0.4451. Now, looking at ﬁgure 10, we can see that there are certain features for which DNAS has found that the original cardinality should be maintained. These fall at the indices 5, 6, 10, 12, 13, 14, 16, 17, 18, 19, 21, 22, and also to a lesser extent 24. The features listed have an average log base 10 cardinality of 2.424, and those not listed have an average log base 10 cardinality of 4.702. Clearly, DNAS has realized that it makes sense not to try to reduce the storage used by low-cardinality features, both because this saves little storage space, but also because it is likely to signiﬁcantly hurt task performance. We now examine the test set results of this search space. The architecture that achieves the lowest loss achieves a test set logloss of 0.4442, the lowest results achieved in al search spaces. It achieves a calibration of 0.9916, which is also the closest to 1 out of all search spaces. However, it contains 19M categories, resulting in a large model. What is even more impressive is that the architecture with the minimum number of categories is actually fairly close in logloss, with a test set result of 0.4454, which is the second best of all test set results across all search spaces. This architecture has 15.14× fewer categories (and thus uses the same factor less embedding table storage) than the architecture with the lowest loss, and yet it increases test loss by just 0.0012. While this would be signiﬁcant in a commercial context [22], this result shows that DNAS has great promise as a tool to help maximize the performance that can be achieved in a given storage budget for embedding tables. We also note our belief that part of the value of the embedding cardinality search space is that it allows DNAS to, at some level, remove feature information which contributes primarily to overﬁtting. Because the weights and architecture parameters training datasets are separate, DNAS has the ability to recognize features which are not generalizing well and reduce the probability that they are sampled with the same cardinality. Reducing the cardinality will reduce the overﬁtting, providing both a storage reduction and a loss improvement. We believe that this may also be part of why the architecture with 15.14× fewer categories than the best-performing one does not perform too much worse; that is, much of the trimmed categories may have been contributing more to overﬁtting than generalized performance. Our test set results of 0.4442 and 0.4454 for the lowest loss and most efﬁcient embedding cardinality search architectures respectively are signiﬁcantly in excess of the ∼0.447 (estimated from graphs by counting pixels) reported in ﬁgure 5 of [26] as their DLRM baseline. Our latter efﬁcient result also uses ∼12× fewer parameters than the 5.4 × 10reported for that baseline. As for search efﬁciency, we complete our DNAS search process in ∼0.28 GPU-days, which compares favorably with the ∼0.75 reported in [27], and especially so when we consider that they search over a dataset sub-sampled to 2M samples, which is 19.64× smaller than our search dataset (the entire training dataset). Were we to run our search over the same sub-sampled dataset, we would be 52.6× more efﬁcient, in GPU-days, than the result reported in [27]. In this section, we discuss some lessons we learned through conducting our experiments, as well as through valuable advice provided by our colleagues. We hope this might be useful to those who use our DNAS framework for their own applications, as well as other NAS researchers. After experimenting with different search space groups and search spaces, we believe that the search spaces that likely demonstrate the most promise are those focused on the sparse features of the DLRM. Speciﬁcally, we believe the embedding cardinality search space may provide the best opportunities for ﬁnding excellent accuracy / model size tradeoffs, and, if properly tuned, improved, and extended, may even provide value in commercial deployments. We also believe that much of the value of this DNAS (and tuning) framework may not be in the logloss gains, or model size reductions, but in the time it returns to industry practitioners and researchers, who can focus on model development, deployment, and so forth, instead of tuning the same model repeatedly. This improvement is impossible to quantify in a paper such as this, but we believe it is one of the key value propositions of the framework. For our own experiments, having the NAS and tuning infrastructure that we do now would have greatly accelerated our work, especially at the beginning of the project when we were less familiar with the speciﬁc architectural choices and hyperparameters that performed well for our dataset. One of the most important pieces of advice that we would like to pass on to readers is to test search spaces (and seaarch space groups) before running NAS or designing a NAS algorithm to search over them. This is important because, ultimately, NAS is not magic; it can only ﬁnd the best points within a search space. If the search space itself does not offer sufﬁcient architectural variability or is not correlated with task performance or efﬁciency, DNAS cannot do anything to change that. For this reason, we recommend that a random search be run ﬁrst, to assess the potential in any search space group. Another piece of advice we have to offer is more of a practical recommendation when it comes to actually using DNAS, NAS, or any large-scale experimentation and tuning frameworks daily. That is to plan experiments ahead of time, review results in a timely manner, and prepare new conﬁguration ﬁles (or however experiments are speciﬁed in a system) ahead of time. This will ensure that computational resources are fully utilized. If this is not done, it is very easy to fall behind in keeping track of experiments, resulting in unused machine time, as well as time inefﬁciently spent for people working on the project. This will also reduce the momentum of the project. In this paper, we introduced an efﬁcient and easily extensible Differentiable Neural Architecture Search (DNAS) framework, implemented in PyTorch and open-sourced. We described the design, structure, and functionality of this framework. This framework was then applied to one of the most commercially relevant applications of AI: ads click-through rate (CTR) prediction. Using the Deep Learning Recommendation Model (DLRM) [20] as our backbone, we developed novel search spaces, and showed experimental results on the Criteo Kaggle dataset. These results demonstrate the promise of both our search spaces, as well as the utility of our framework. In the future, we plan to extend this work to other deep recommender backbone architectures, resulting in new search spaces. We hope this work will spur further interest in the application of NAS to CTR prediction, and that the framework we have open-sourced will allow other researchers and industry practitioners to apply the DNAS algorithm to their AI problems. We would like to thank Bichen Wu for helping to initiate and support this project throughout its journey, as well as for providing valuable guidance and feedback on NAS and recommendation systems (see section IX-B). We also thank Kostadin Ilov of the ADEPT lab for help with regards to infrastructure and machines. Finally, we would like to thank Ruoxi Wang of [22] for providing helpful input regarding replication of the results from [22].