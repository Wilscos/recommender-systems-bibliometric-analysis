Due to the growing privacy concerns, decentralization emerges rapidly in personalized services, especially recommendation. Also, recent studies have shown that centralized models are vulnerable to poisoning attacks, compromising their integrity. In the context of recommender systems, a typical goal of such poisoning attacks is to promote the adversary’s target items by interfering with the training dataset and/or process. Hence, a common practice is to subsume recommender systems under the decentralized federated learning paradigm, which enables all user devices to collaboratively learn a global recommender while retaining all the sensitive data locally. Without exposing the full knowledge of the recommender and entire dataset to end-users, such federated recommendation is widely regarded ‘safe’ towards poisoning attacks. In this paper, we present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. As popular items are more likely to appear in the recommendation list, our innovatively designed attack model enables the target item to have the characteristics of popular items in the embedding space. Then, by uploading carefully crafted gradients via a small number of malicious users during the model update, we can eectively increase the exposure rate of a target (unpopular) item in the resulted federated recommender. Evaluations on two real-world datasets show that 1) our attack model signicantly boosts the exposure rate of the target item in a stealthy way, without harming the accuracy of the poisoned recommender; and 2) existing defenses are not eective enough, highlighting the need for new defenses against our local model poisoning attacks to federated recommender systems. • Information systems → Collaborative ltering. Federated Recommender System; Poisoning Attacks; Deep Learning ACM Reference Format: Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, and Lizhen Cui. 2021. PipAttack: Poisoning Federated Recommender Systems for Manipulating Item Promotion. In Proceedings of ACM Conference (Conference’17). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn The demand for recommender systems has increased more than ever before. Over the years, various recommendation algorithms have been proposed and proven successful in various applications. Collaborative ltering (CF), which infers users’ potential interests from users’ historical behavior data, lies at the core of modern recommender systems. Recently, enhancing CF with deep neural networks has been a widely adopted means for modelling the complex user-item interactions [20] and demonstrated state-of-the-art performance in a wide range of recommendation tasks. The conventional recommender systems centrally store users’ personal data to facilitate the centralized model training, which are, however, increasing the privacy risks [40]. Apart from privacy issues, another major risk emerges w.r.t. the correctness of the learned recommender in the presence of malicious users, where the trained model can be induced to deliver altered results as the adversary desires, e.g., promoting a target productsuch that it gets more exposures in the item lists recommended to users. This is termedpoisoning aack[12,25], which is usually driven by nancial incentives but incurs strong unfairness in a trained recommender. In light of both privacy and security issues, there has been a recent surge in decentralizing recommender systems, where federated learning [28,33,34] appears to be one of the most representative solutions. Specically, a federated recommender allows users’ personal devices to locally host their data for training, and the global recommendation model is trained in a multi-round fashion by submitting a subset of locally updated on-device models to the central server and performing aggregation (e.g., model averaging). Subsuming a recommender under the federated learning paradigm naturally protects user privacy as the data is no longer uploaded to a central server or cloud. Moreover, it also prevents a recommender from being poisoned. The rationale is, in recommendation, the predominant poisoning method for manipulating item Figure 1: Visualization on the popularity bias of a CF-based federated recommender. (a), (b) and (c) are the t-SNE projection of item embeddings at dierent training stages, where more details on popularity labels can be found in Section 4.1. (d) shows the convergence curve of a popularity classier using 𝐹 1 score. promotion is data poisoning, where the adversary manipulates malicious users to generate fake yet plausible user-item interactions (e.g., ratings), making the trained recommender biased towards the target item [12,13,25,36]. However, these aforementioned data poisoning attacks operate under the assumption that the adversary has prior knowledge about the entire training dataset. Apparently, such assumption is voided in federated recommendation as the user data is distributively held, thus restricting the adversary’s access to the entire dataset. Though the adversary can still conduct data poisoning by increasing the amount of fake interactions and/or malicious user accounts, it inevitably makes the attack more identiable from either the abnormal user behavioral footprints [23,38] or heavily impaired recommendation accuracy [25]. Thus, we aim to answer this challenging question: can we backdoor federated recommender systems via poisoning attacks? In this paper, we provide a positive answer to this question with a novel attack model that manipulates the target item’s exposure rate in this nontrivial decentralized setting. Meanwhile, our study may also shed some light on the security risks of federated recommenders. Unlike data poisoning, our attack approach is built upon the model poisoning scheme, which compromises the integrity of the model learning process by manipulating the gradients [4,5] of local models submitted by several malicious users. Given a federated recommender, we assume an adversary controls several malicious users/devices, each of which has the authority to alter the local model gradients used for updating the global model. Since the uploaded parameters directly aect the global model, it is more cost-eective for an adversary to poison the federated recommender from the model level, where a small group of malicious users will suce. Despite the ecacy of model poisoning in many federated learning applications, the majority of these attacks are only focused on perturbing the results in classication tasks (e.g., image classication and word prediction) [3,11]. However, designed for personalized ranking tasks, federated recommenders are optimized towards completely dierent learning objectives, leaving poisoning attacks for item promotion largely unexplored. In the meantime, the federated environment signicantly limits our adversary’s prior knowledge to only partial local resources (i.e., malicious users’ data and models) and the global model. Moreover, the poisoned global model should maintain high recommendation accuracy, so as to promote the target item stealthily while providing high-quality recommendations to benign users. To address these challenges, we take advantage of a common characteristics of recommendation models, namely the popularity bias. As pointed out by previous studies [1,39,41,42], data-driven recommenders, especially CF-based methods are prone to amplify the popularity by over-recommending popular items that are frequently visited. Intuitively, because such inherent bias still exists in federated recommenders, if we can disguise our target item as the popular items by uploading carefully crafted local gradients via malicious users, we can eectively trick the federated recommender to become biased towards our target item, thus boosting its exposure. Specically, we leverage the learnable item embeddings as an interface to facilitate model poisoning. To provide a proof-ofconcept, in Figure 1, we use t-SNE to visualize the item embeddings learned by a generic federated recommender (see Section 3.1 for model conguration). As the model progresses from its initial state towards nal convergence (Figure 1(a)-(c)), items from three dierent popularity groups gradually forms distinct clusters. To further reect the strong popularity bias, we train a simplistic popularity classier (see Section 3.4) that predicts an item’s popularity group given its learned embeddings, and show its testing performance in Figure 1(d). In short, the high prediction accuracy (𝐹1>0.8) again veries that the item embeddings in a well-trained federated recommender are highly discriminative w.r.t. their popularity. To this end, we proposepoisoningattackforitempromotion (PipAttack), a novel poisoning attack model targeted on recommender systems in the decentralized, federated setting. Apart from optimizing PipAttack under the explicit promotion constraint that straightforwardly pushes the recommender to raise the ranking score of the target item, we innovatively design two learning objectives, namely the popularity obfuscation constraint and distance constraint so as to achieve our attack goal via fewer model updates without imposing dramatic accuracy drops. On one hand, popularity obfuscation confuses the federated recommender by aligning the target item with popular ones in the embedding space, thus greatly beneting the target item’s exposure rate via the popularity bias. On the other hand, to avoid harming the usability of the federated recommender and being detected, our attack model bears a distance constraint to prevent the manipulated gradients uploaded by malicious users from deviating too far from the original ones. We summarize our main contributions as follows: •To the best of our knowledge, we present the rst systematic approach to poisoning federated recommender systems, where an adversary only has limited prior knowledge compared with existing centralized scenarios. Our study reveals the existence of recommenders’ security backdoors even in a decentralized environment. •We propose PipAttack, a novel attack model that induces the federated recommender to promote the target item by uploading carefully crafted local gradients through several malicious users. PipAttack assumes no access to the full training dataset and other benign users’ local information, and innovatively takes advantage of the inherent popularity bias to eectively boost the exposure rate of the target item. •Experiments on two real-world datasets demonstrate the advantageous performance of PipAttack even when defensive strategies are in place. Furthermore, compared with all baselines, PipAttack is more cost-eective as it can reach the attack goal with fewer model updates and malicious users. In this section, we rst revisit the fundamental settings of federated recommendation and then formally dene our research problem. LetVandUdenote the sets of𝑁items and𝑀users/devices, respectively. Each user𝑢∈ Uowns a local training datasetD consisting of implicit feedback tuples(𝑢, 𝑣, 𝑟), where𝑟=1 if 𝑢has visited item𝑣∈ V(i.e., a positive instance), and𝑟=0 if there is no interaction between them (i.e., a negative instance). Note that the negative instances are downsampled using a positiveto-negative ratio of 1 :𝑞for each𝑢due to the large amount of unobserved user-item interactions. Then, for every user, a federated recommender (FedRec) is trained to estimate the feedbackˆ𝑟∈ [0,1]between𝑢and all items, whereˆ𝑟is also interpreted as the ranking/similarity score for a user-item pair. With the ranking score computed, FedRec then recommends an item list for each user𝑢 by selecting𝐾top-ranked items w.r.t.ˆ𝑟. It can be represented as: 𝐹𝑒𝑑𝑅𝑒𝑐 (𝑢|Θ) = {𝑣|ˆ𝑟is top-𝐾 in {ˆ𝑟} where¯I(𝑖)denotes unrated items of𝑢andΘdenotes all the trainable parameters in FedRec. For notation simplicity, we directly use Θ to represent the recommendation model. Federated Learning Protocol.In FedRec, a central server coordinates individual user devices, of which each keepsDand a copy of the recommendation model locally. To train FedRec, the local model on the𝑖-th user device is optimized locally by minimizing the following loss function: L= −Í𝑟logˆ𝑟+ (1 − 𝑟) log(1 −ˆ𝑟), (2) where we treat the estimatedˆ𝑟∈ [0,1]as the probability of observing the interaction between𝑢and𝑣. Then, the above cross-entropy loss quanties the dierence betweenˆ𝑟and the binary ground truth𝑟. It is worth mentioning that, other popular distance-based loss functions (e.g., hinge loss and Bayesian personalized ranking loss [31]) are also applicable and behave similarly in FedRec. With the user-specic lossLcomputed, we can derive the gradients of𝑢’s local modelΘ, denoted by∇Θ. At iteration𝑡, a subset of usersUare randomly drawn. Each𝑢∈ Uthen downloads the latest global modelΘand updates its local gradients w.r.t.D, denoted by∇Θ. After the central server receives all local gradients submitted by|U|users, it aggregates the collected gradients to facilitate global model update. Specically, FedRec follows the commonly used gradient averaging [27] to obtain the updated model Θwith learning rate 𝜂: The training proceeds iteratively until convergence. Unlike centralized recommenders, FedRec collects only each user’s local model gradients instead of her/his own dataD, making existing poisoning attacks [9,25,36] fail due to their ill-posed assumptions on the access to the entire dataset. Furthermore, dierent local gradients are not shared across users, which further reduces the amount of prior knowledge that a malicious party can acquire. Following [11], we assume an adversary can compromise a small proportion of users in FedRec, which we term malicious users. The attack is scheduled to start at epoch𝑡, and all malicious users participate in the training of FedRec normally before that. From epoch 𝑡, the malicious users start poisoning the global model by replacing the real local gradients∇Θwith purposefully crafted gradients g∇Θ, so as to gradually guide the global model to recommend the target item more frequently. The task is formally dened as follows: Problem 1.Poisoning FedRec for Item Promotion.Given a federated recommender parameterized byΘ, our adversary aims to promote a target item𝑣∈ Vby altering the local gradients submitted by every compromised malicious user device𝑢∈ U at each update iteration𝑡, i.e.,𝑓:∇Θ↦→g∇Θ. Note that each malicious user will produce its unique deceptive gradientsg∇Θ. Hence, the deceptive gradients{g∇Θ}for all malicious users are parameters to be learned during the attack process, such that for each benign user𝑢∈ U\U, the probability that𝑣appears in 𝐹𝑒𝑑𝑅𝑒𝑐 (𝑢|Θ) is maximized. In this section, we introduce our proposed PipAttack in details. Federated learning is compatible with the majority of latent factor models. Without loss of generality, we adopt neural collaborative ltering (NCF) [20], a performant and widely adopted latent factor model, as the base recommender of FedRec. In short, NCF extends CF by leveraging an𝐿-layer feedforward network𝐹 𝐹 𝑁 (·)to model the complex user-item interactions and estimateˆ𝑟: whereu, v∈ Rare respectively user and item embeddings,⊕is the vector concatenation,h ∈ Rdenotes the projection weights that corresponds to the𝑑-dimensional output of the last network layer, and𝜎is the sigmoid function that recties the output to fall between 0 and 1. Meanwhile, it is worth noting that our attack method is generic and applicable to many other recommenders like feature-based [32] and graph-based [19] ones. In summary, the parametersΘto be learned in FedRec are: the projection vectorh, all weights and biases in the𝐿-layer FFN, as well as embeddingsuandvfor all users and items. Meanwhile, an important note is that, as user embeddings are regarded highly sensitive as they directly reect users’ personal interests, they are commonly disallowed to be shared across users in federated recommenders [8,30] for privacy reasons. In FedRec, this is achieved by withholdinguand its gradients on every user device, and the user embeddings are updated locally. Hence, the gradients ofuare excluded from both∇Θandg∇Θthat will be respectively submitted by benign and malicious users during the update of FedRec. To perform poisoning attacks on FedRec, traditional attack approaches designed for centralized recommenders are inapplicable. This is due mainly to the prior knowledge that needs to be accessible to the adversary, e.g., all user-item interactions [13,36] and even other benign users’ embeddings [12,25], which becomes an ill-posed assumption for federated settings as most of such information is retained at the user side. In short, FedRec substantially narrows down the prior knowledge and capability of an attack model, which is restricted to the following: I.The adversary can access the global modelΘat any iteration𝑡, excluding all benign users’ embeddings, i.e., {u}. II.The adversary can access and alter all malicious users’ local models and their gradients. III.The adversary knows the whole item set (not interactions) which is commonly available on any e-commerce platform, as well as side information that reects each item’s popularity. We will expand on this in Section 3.4. In what follows, we present the key components of PipAttack as shown in Fig 2 for learning all deceptive gradients{g∇Θ}, which induces FedRec to promote the target item 𝑣. Like many studies on poisoning attacks on recommender systems, our adversary’s goal is to manipulate the learned global model such that the generated recommendation results meet the adversary’s demand. Essentially, to give the target item𝑣more exposures (i.e., to be recommended to more users), we need to raise the ranking scoreˆ𝑟whenever a user𝑢is paired with𝑣. As a minimum requirement, we need to ensure all malicious users can receive𝑣 in their top-𝐾recommendations. With the adversary’s control over a group of malicious usersU, we can explicitly boost the ranking score of𝑣for every𝑢∈ Uvia the following objective function: which encourages a large similarity score between every malicious user and the target item. Theoretically, this mimics the eect of inserting fake interaction records (i.e., data poisoning) with the target item via malicious users, which can gradually drive the CFbased global recommender to predict positive feedback on𝑣for other benign users who are similar to 𝑢∈ U. Unfortunately, Eq.(5) requires the adversary to manipulate a relatively large number of malicious users in order to successfully and eciently poison FedRec. Otherwise, after aggregating the local gradients from sampled users, the large user base of a recommender system can easily dilute the impact of deceptive gradients uploaded by a small group ofU. In this regard, on top of explicit promotion, we propose a more cost-eective tactic in PipAttack from the popularity perspective. As discussed in Section 1, it is well acknowledged that CF-based recommenders are intrinsically biased towards popular items that are frequently visited [2], and FedRec is no exception. Compared with long-tail/unpopular items, popular items are more widely and frequently trained in a recommender, thus amplifying their likelihood of receiving a larger ranking score and gaining advantages in exposure rates. Hence, we make full use of the inherent popularity bias rooted in recommender systems, where we aim to learn deceptive gradients that can trick FedRec to ‘mistake’𝑣as a popular item. In a latent factor model like FedRec, this can be achieved by poisoning the item embeddings in the global recommender, so that embeddingvis semantically similar to popular items inV. Intuitively, if a popular item and𝑣are close to each other in the latent space, so will their ranking scores for the same user. Given the existence of popularity bias, vwill be promoted to more users’ recommendation lists. Availability of Popularity Information.Our popularity obfuscation strategy needs prior knowledge about items’ popularity. However, directly obtaining such information via user-item interaction frequencies is infeasible as it requires access to the entire dataset. Fortunately, though the user behavioral data is protected, the prosperity of online service platforms has brought a wide range of visible clues on the popularity of items. For example, hot tracks are always displayed on music applications (e.g., Spotify) without revealing identities of their listeners, and e-commerce sites (e.g., eBay) records the sales volume of each product while keeping the purchasers anonymized. Furthermore, as PipAttack is aware of the item set, item popularity can be easily looked up on a fully public platform (e.g., Yelp). Thus, popularity information can be fetched from various public sources to assist our poisoning attack, and the prerequisites of FedRec remain intact. Popularity Estimator.The most straightforward way to facilitate such popularity obfuscation is to incorporate a distance metric to penalizes anyg∇Θthat enlarges the distance between𝑣and a randomly sampled popular item𝑣. However, being a personalized model, whether an item can be recommended to𝑢is not only determined by its popularity, but also the user-item similarity. Therefore, such constraints will work the best only if the selected𝑣accounts for each user’s personal preference, which is infeasible due to the fact that all benign users withinU\Uare intransparent to the adversary. In PipAttack, we propose a novel popularity estimatorbased method that collectively engages all the popular items in the item set. Specically, we can assign each item a discrete label w.r.t. its popularity level obtained via public information. Then, suppose there are𝐶popularity classes, the popularity estimator 𝑓(·)is a deep neural network (DNN) that inputs a learned item embeddingvat iteration step𝑡, and computes a𝐶-dimensional probability distribution vectorˆyvia the nal softmax layer. Each elementˆy[𝑐] ∈ ˆy(𝑐 =1,2, ...,𝐶) represents the probability that𝑣 belongs to class𝑐. We train𝑓(·)with cross-entropy loss on all (v, y) pairs for all 𝑣 ≠ 𝑣, where y = {0, 1}is the one-hot label: Boosting Item Popularity.Once we obtain a well-trained𝑓(·), it is highly reective of the popularity characteristics encoded in each item embedding, where items at the same popularity level will have high semantic similarity in the embedding space. So, at the𝑡-th training iteration of FedRec, we aim to boost the predicted popularity of𝑣by making targeted updates on embeddingv∈ Θwith crafted deceptive gradientsg∇Θ. This is achieved by minimizing the negative log-likelihood of class𝑐(i.e., the highest popularity level) in the output of 𝑓(v): Note that𝑓(·)stays xed and will no longer be updated after the popularity obfuscation starts. Essentially, by optimizingL, PipAttack generates gradientsg∇Θthat enforces𝑣to approximate the characteristics of all items from the top popularity group in the embedding space, resulting in a boosted exposure rate. Generally, aggressively fabricated deceptive gradientsg∇Θmay help the adversary achieve the attack goal with fewer training iterations, but will also incur strong discrepancies with the real gradients∇Θ, leading to a higher chance to be detected by the central server [5] and harm the performance of the global model. Hence, the crafted gradients should maintain a certain level of similarity with the genuine ones. As such, we place a constraint on the distance between eachg∇Θfor𝑢∈ Uand all malicious users’ original local gradient ∇Θ: where|| · ||is p-norm distance and we adopt|| · ||in PipAttack. In this subsection, we dene the loss function of PipAttack for model training. Instead of training each component separately, we combine their losses and use joint learning to optimize the following objective function: where𝛼and𝛾are non-negative coecients to scale and balance the eect of each part. In this section, we rst outline the evaluation protocols for our PipAttacks and then conduct experiments on two real-world datasets to evaluate the performance of PipAttack. Particularly, we aim to answer the following research questions (RQs) via experiments: RQ1:Can PipAttack perform poisoning attacks eectively on federated recommender systems? RQ2:Does PipAttack harm the recommendation performance of the federated recommender signicantly? RQ3: How does PipAttack benet from each key component? RQ4: What is the impact of hyperparameters to PipAttack? RQ5:Can PipAttack bypass defensive strategies deployed at the server side? We adopt two popular public datasets for evaluation, namely MovieL ens-1M (ML) [16] and Amazon (AZ) [17]. ML contains 1 million ratings involving 6,039 users and 3,705 movies, while AZ contains 103,593 ratings involving 13,174 users and 5,970 cellphone-related products. Following [18,20,21], we have binarized the user feedback, where all ratings are converted to𝑟=1, and negative instances are sampled𝑞 =4 times the amount of positive ones. PipAttack needs to obtain the popularity labels of items. As described in Section 3.4, such information can be easily obtained in real-life scenarios (e.g., page views) without sacricing user identity in FedRec. However, as our datasets are mainly collected for pure recommendation research, there is no such side information available. Hence, we segment items’ popularity into three levels by sorting all items by the amount of interactions they receive, with intervals of the top 10% (high popularity), top 10% to 45% (medium popularity), and the last 55% (low popularity). Note that we only use the full dataset once to compensate for the unavailable side information about item popularity, and the interaction records of each user are locally stored throughout the training of FedRec and poisoning attack. Following the common setting for attacking federated models [5], we rst train FedRec without attacks (malicious users behave normally in that period) for several epochs, then PipAttack starts poisoning FedRec by activating all malicious users. To ensure fairness in evaluation, all tested methods are asked to attack the same pretrained FedRec model. We introduce our evaluation metrics below. Poisoning Attack Eectiveness.We use exposure rate at rank 𝐾(𝐸𝑅@𝐾) as our evaluation metric. Suppose each user receives 𝐾recommended items, then for the target item to be promoted, 𝐸𝑅@𝐾is the fraction of users whose𝐾recommended items include the target item. Correspondingly, larger𝐸𝑅@𝐾represents stronger poisoning eectiveness. Notably, from both datasets, we select the least popular item as the target item in our experiments as this is the hardest possible item to promote. For each user, all items excluding positive training examples are used for ranking. Recommendation Eectiveness.It is important that the poisoning attack does not harm the accuracy of FedRec. To evaluate the recommendation accuracy, we adopt the leave-one-out approach [21] to hold out ground truth items for evaluation. An item is held out for each user to build a validation set. Following [20], we employ hit ratio at rank𝐾(𝐻𝑅@𝐾) to quantify the fraction of observing the ground truth item in the top-𝐾recommendation lists. We compare PipAttack with ve poisoning attack methods, where the st three are model poisoning and the last two are data poisoning methods. Notably, many recent models are only designed to attack centralized recommenders, thus requiring prior knowledge Figure 3: Attack ((a)-(h)) and recommendation ((i)-(p)) results on ML and AZ. Note that the curves start from the rst ep och after the attack starts. that cannot be obtained in the federated setting. Hence, we choose the following baselines that do not hold assumptions on those inaccessible knowledge:P1[5]: This work aims to poison federated learning models by directing the model to misclassify the target input.P2[4]: This is a general approach for attacking distributed models and evading defense mechanisms.Explicit Boosting (EB): The adversary directly optimizes the explicit item promotion objectiveL.Popular Attacks (PA)[15]: It injects fake interactions with both target and popular items via manipulated malicious users to promote the target item.Random Attacks (RA)[15]. It poisons a recommender in a similar way to PA, but uses fake interactions with the target item and randomly selected items. In FedRec, we set the latent dimension𝑑, learning rate, local batch size to 64, 0.01 and 64, respectively. Each user is considered as an individual device, where 10% and 5% of the users (including both benign and malicious users) are randomly selected on ML and AZ at every iteration. Model parameters in FedRec are randomly initialized using Gaussian distribution (𝜇 =0, 𝜎 =1). The popularity estimator𝑓(·)is formulated as a 4-layer deep neural network with 32, 16, 8, and 3 hidden dimensions from bottom to top. In Pipattack, the local epoch and learning rate are respectively 30 and 0.01 on both datasets. We also examine the eect of dierent fractions𝜁of malicious users with𝜁 ∈ {10%,20%,30%,40%}on ML and𝜁 ∈ {5%,10%,20%,30%}on AZ. For the coecients in loss functionL, we set𝛼 =60 when𝜁 =10% and𝛼 =20 when 𝜁 ∈ {20%,30%,40%}on ML, and𝛼 =10 on AZ across all malicious user fractions. We apply 𝛾 = 0.0005 on both datasets. Figure 3 shows the performance of𝐸𝑅@5 w.r.t. to dierent malicious user fractions. It is clear that PipAttack is successful at inducing the recommender to recommend the target item. In fact, the recommender becomes highly condent in its recommendation results and recommend the target item to all the users after the 40th epoch, with just a small number of malicious users (i.e., 10% on ML and 5% on AZ). By increasing the fraction of fraudsters, the attack performance improves signicantly, as it is easier for the adversary to manipulate the parameters of the global model (a) Before Poisoning Attack (b) After Poisoning Attack (𝐸𝑅@5 = 1) Figure 5: Visualization of item embeddings before and after being attacked by PipAttack. (a) With Distance Constraint (b) Without Distance Constraint. Figure 6: Comparison of gradient distributions between benign and malicious users. via more malicious users. Additionally, PipAttack outperforms all baselines consistently in terms of𝐸𝑅@5, showing strong capability to promote the target items to all users. Furthermore, as the attack proceeds, our model can consistently reach 100% exposure rate in most scenarios, while there are obvious uctuations in the performance of other baseline methods. Finally, we observe that model poisoning methods (i.e., P1, P2 and Explicit Boosting) generally perform better than data poisoning methods (i.e., Random and Popular attack), indicating the superiority of model poisoning paradigm since the global recommender can be manipulated arbitrarily for mean even with limited number of compromised users. Recommendation accuracy plays a signicant role in the success of adversarial attacks. On one hand, it is an important property that the server can use to detect anomalous updates. On the other hand, only a recommender that is highly accurate will have a large user base for promoting the target item. Figure 3 shows the overall performance curve of FedRec after the attack starts. The rst observation is that, PipAttack, which is the most powerful for item promotion, still achieves competitive recommendation results on both datasets. It proves that PipAttack can avoid harming the usefulness of FedRec. Another observation is that there is an overall upward trend for all baseline models except random attack. Finally, compared with model poisoning attacks, data poisoning attacks, especially random attack, cause a larger decrease on recommendation accuracy, which further conrms the eectiveness of model poisoning methods. We conduct ablation analysis to better understand the performance gain from the major components proposed in PipAttack. We set 𝜁 =20% throughout this set of experiments, and we discuss the key components below. 4.7.1 Explicit Promotion Constraint. We rst remove explicit promotion constraintLand plot the new results in Figure 4. Apparently, it leads to severe performance drop on two datasets. As the main adversarial objective is to increase the ranking score for the target item, removingLdirectly limits PipAttack’s capability. In addition, the degraded PipAttack is more eective on the AZ than that on ML. A possible reason is that AZ is sparser and thus is easier to attack, despite the removal of explicit promotion constraint. 4.7.2 Popularity Obfuscation Constraint. We validate our hypothesis of leveraging the popularity bias in FedRec for poisoning attack. In Figure 4, without the popularity obfuscation constraintL, PipAttack suers from the obviously inferior performance on both two datasets. For instance,𝐸𝑅@5=100% is respectively achieved at 17th and 14th epochs on ML and AZ, which are far behind the full version of PipAttack. It conrms that by taking advantage of the popularity bias, PipAttack can eectively accomplish the attack goal with less attack attempts. To further verify the ecacy of popularity obfuscation in PipAttack, we visualize the item embeddings via t-SNE in Figure 5. Obviously, after the poisoning attack, the target item embedding moves from the cluster of unpopular items to the cluster of the most popular items. Hence, the use of popularity bias is proved benecial to promoting the target item. 4.7.3 Distance Constraint. As introduced in [5], the server can use weight update statistics to detect anomalous updates and even refuse the corresponding updates. To verify the contribution of the distance constraint in Eq.(9), we derive one variant of PipAttack by removing the distance constraintL. In Figure 6, on the ML dataset, the averaged gradients’ distributions for all benign updates and malicious updates are plotted. It is clear that the deceptive gradients’ distribution under the distance constraint is similar to that of benign users. In contrast, the gradients are much sparser and diverges far from benign users’ gradients. Furthermore, we calculate the KL-divergence to measure the dierence between two distributions in each gure. Specially, the result achieved with distance constraint (0.005) is much lower than the result achieved without it (0.177). Hence, PipAttack is stealthier, and is harder for the server to ag abnormal gradients. We answer RQ4 by investigating the performance uctuations of PipAttack with varied hyperparameters on both dataset. Specifically, we examine the values of𝛼in{1,10,20,40,60,80}on ML and{1,5,10,15,20,30}on AZ, respectively (𝜁 =20%). Note that we omit the eect of𝛾as it brings much less variation to the model performance. Figure 7 presents the results with varied𝛼. As can be inferred from the gure, all the best attack results are achieved with𝛼 =20 on ML and𝛼 =10 on AZ. Meanwhile, altering this coecient on the attack objective has less impact on the recommendation accuracy. Overall, setting𝛼 =20 on ML and𝛼 =10 on AZ is sucient for promoting target item, while ensuring the high-quality recommendation of FedRec. In general, the mean aggregation rule used in FedRec assumes that the central server works in a secure environment. To answer RQ5, we investigate how our dierent attack models perform in the presence of attack-resistant aggregation rules, namely Bulyan [14] and Trimmed Mean [35]. Note that though Krum [6] is also a widely used defensive method, it is not considered in our work due to the severe accuracy loss it causes. We benchmark all methods’ performance on the ML dataset, and Figure 8 shows the𝐸𝑅@5 results with 𝜁 =20%. The rst observation we can draw from the results is that, PipAttack consistently outperforms all baselines and maintains 𝐸𝑅@5=1, which further conrms the superiority of PipAttack. The main goal of those resilient aggregation methods is to ensure convergence of the global model under attacks, while our model’s distance constraint eectively ensures this. Our experiments reveal the limited robustness of existing defenses in federated recommendation, thus emphasising the need for more advanced defensive strategies on such poisoning attacks. A growing number of online platforms are deploying centralized recommender systems to increase user interaction and enrich shopping potential [29,37,40]. However, many studies show that attacking recommender systems can aect users’ decisions in order to have target products recommended more often than before [9,22,24]. In [10], a reinforcement learning (RL)-based model is designed to generate fake proles by copying the benign users’ proles in the source domain, while [36] rstly constructed a simulator recommender based on the full user-item interaction network, and then the results of the simulator system is used to design rewards function for policy training. [9,25] generate fake users through Generative Adversarial Networks (GAN) to achieve adversarial intend. [13] and [12] formulate the attack as an optimization problem by maximizing the hit ratio of the target item. However, many of these attack approaches fundamentally rely on the white-box model, in which the attacker requires the adversary to have full knowledge of the target model and dataset. Another variant of federated recommenders are devised to inhibit the availability of dataset [26,28,30,34]. For federated recommender systems, expecting complete access to the dataset and model is not realistic. [7,15] study the inuence of low-knowledge attack approaches to promote an item (e.g., random, popular attack), but the performance is unsatisfactory. Despite previous attack methods success with various adversarial objectives under federated setting such as misclassication [4,5] and increased error rate [11], the study of promoting an item for federated recommender system still remains largely unexplored. Therefore, we propose a novel framework that does not have full knowledge of the target model to attack under the federated setting to ll this gap. In this paper, we present a novel model poisoning attack framework for manipulating item promotion named PipAttack. We also prove that our attack model can work on the presence of defensive protocols. With three innovatively designed attack objectives, the attack model is able to perform ecient attacks against federated recommender systems and maintain high-quality recommendation results generated by the poisoned recommender. Furthermore, the distance constraint plays an essential role in disguising malicious users as benign users to avoid detection. The experimental results based on two real-world datasets demonstrate the superiority and practicality of PipAttack over peer methods even in the presence of defensive strategies deployed at the server side. This work is supported by Australian Research Council Future Fellowship (Grant No. FT210100624), Discovery Project (Grant No. DP190101985) and Discovery Early Career Research Award (Grant No. DE200101465).