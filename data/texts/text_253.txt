Knowledge Distillation (KD) offers a natural way to reduce the latency and memory/energy usage of massive pretrained models that have come to dominate Natural Language Processing (NLP) in recent years. While numerous sophisticated variants of KD algorithms have been proposed for NLP applications, the key factors underpinning the optimal distillation performance are often confounded and remain unclear. We aim to identify how different components in the KD pipeline affect the resulting performance and how much the optimal KD pipeline varies across different datasets/tasks, such as the data augmentation policy, the loss function, and the intermediate representation for transferring the knowledge between teacher and student. To tease apart their effects, we propose Distiller, a meta KD framework that systematically combines a broad range of techniques across different stages of the KD pipeline, which enables us to quantify each component’s contribution. Within Distiller, we unify commonly used objectives for distillation of intermediate representations under a universal mutual information (MI) objective and propose a class of MIα objective functions with better bias/variance trade-off for estimating the MI between the teacher and the student. On a diverse set of NLP datasets, the best Distiller conﬁgurations are identiﬁed via large-scale hyper-parameter optimization. Our experiments reveal the following: 1) the approach used to distill the intermediate representations is the most important factor in KD performance, 2) among different objectives for intermediate distillation, MI-α performs the best, and 3) data augmentation provides a large boost for small training datasets or small student networks. Moreover, we ﬁnd that different datasets/tasks prefer different KD algorithms, and thus propose a simple AutoDistiller algorithm that can recommend a good KD pipeline for a new dataset. Recent advancements in Natural Language Processing (NLP) such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ELECTRA (Clark et al., 2020) have demonstrated the effectiveness of Transformer models in generating transferable language representations. Pretraining over a massive unlabeled corpus and then ﬁne-tuning over labeled data for the task of interest has become the state-ofthe-art paradigm for solving diverse NLP problems ranging from sentence classiﬁcation to question answering (Raffel et al., 2019). Scaling up the size of these networks has led to rapid NLP improvements. However, these improvements have come at the expense of signiﬁcant increase of memory for their many parameters and compute to produce predictions (Brown et al., 2020; Kaplan et al., 2020). This prevents these models from being deployed on resource-constrained devices such as smart phones and browsers, or for latency-constrained applications such as click-through-rate prediction. There is great demand for models that are smaller in size, yet still retain similar accuracy as those having a large number of parameters. To reduce the model size while preserving accuracy, various model compression techniques have been proposed such as: pruning, quantization, and Knowledge Distillation (KD) (Gupta and Agrawal, 2020). Among these methods, task-aware KD is a popular and particularly promising approach for compressing Transformer-based models (Gupta and Agrawal, 2020). The general idea is to ﬁrst ﬁne-tune a large model (namely the teacher model) based on the task’s labeled data, and then train a separate network that has signiﬁcantly fewer parameters (namely the student model) than the original model to mimic the predictions of the original large model. A large number of task-aware KD algorithms have been proposed in the NLP regime, e.g., DistillBERT (Sanh et al., 2019), BERT-PKD (Sun et al., 2019), BERT-EMD (Li et al., 2020), TinyBERT (Jiao et al., 2020), DynaBERT (Hou et al., 2020), and AdaBERT (Chen et al., 2020), some of which can compress the teacher network by 10×without signiﬁcant accuracy loss on certain datasets. Innovations in KD for NLP generally involve improvements in one of the following aspects: 1) the loss function for gauging the discrepancy between student and teacher predictions (Kim et al., 2021), 2) the method for transferring intermediate network representations between teacher and student (Sun et al., 2019; Li et al., 2020; Yang et al., 2021), 3) the use of data augmentation during student training (Jiao et al., 2020), and 4) multiple stages of distillation (Chen et al., 2020; Mirzadeh et al., 2020). Many research proposals have simultaneously introduced new variations of more than one of these components, which confounds the impact of each component on the ﬁnal performance of the distillation algorithm. In addition, it is often unclear whether a proposed distillation pipeline will generalize to a new dataset or task, making automated KD challenging. For example, MixKD (Liang et al., 2021) has only been evaluated on classiﬁcation problems and it is unclear if the method will be effective for question answering. To understand the importance of different components in KD, we undertake a systematic study of KD algorithms in NLP. Our study is conducted using a meta-distillation pipeline we call Distiller that contains multiple conﬁgurable components. All candidate algorithms in the search space of Distiller work for two types of NLP tasks: text classiﬁcation and sentence tagging. Distiller uniﬁes existing techniques for knowledge transfer from intermediate layers of the teacher network to the student network (i.e. intermediate distillation) as special cases of maximizing (bounds on) the Mutual Information (MI) between teacher and student representations. Based on this uniﬁcation and recent progress in variational bounds of MI (Poole et al., 2019), we propose a new intermediate distillation objective called MI-αthat uses a scalarαto control the bias-variance trade-off of MI estimation. Including MI-αin the search space of Distiller, we run extensive hyper-parameter tuning algorithms to search for the best Distiller-conﬁguration choices over GLUE (Wang et al., 2019b) and SQuAD (Rajpurkar et al., 2016). This search helps us identify the best distillation pipelines and understand what impact different KD modules have on student performance in NLP. Using the observations of this large-scale study, we train a AutoDistiller model to predict the distillation ratio, which is deﬁned as the fraction of the teacher’s performance achieved by the student, based on KD pipeline choices and characteristics of a dataset. Leave-one-out cross validation evaluation of AutoDistiller demonstrates that it is able to reliably prioritize high-performing KD conﬁgurations in most folds, and is able to suggest good distillation pipelines on two new datasets. The main contributions of this work include: •The meta KD pipeline Distiller used to systematically study the impact of different components in KD, including the: 1) data augmentation policy, 2) loss function for transferring intermediate representations, 3) layer mapping strategies for intermediate representations, 4) loss function for transferring outputs, as well as what role the task/dataset play. •Uniﬁcation of existing objectives for distilling intermediate representations as instances of maximizing bounds of the mutual information between teacher and student representations. This leads us to propose the MI-αobjective that outperforms the existing objectives. •Using the results collected from our systematic Distiller study, we ﬁt a model that automatically predicts the best distillation strategy for a new dataset. On a never-seen dataset “BoolQ”(Wang et al., 2019a), predicted strategies achieve1.002distillation ratios (fraction of the student’s and the teacher’s performance) on average, outperforming random selected strategies with mean of0.960. To the best of our knowledge, this is the ﬁrst attempt towards automated KD in NLP. Knowledge Distillation.The general KD framework was popularized by Buciluˇa et al. (2006); Hinton et al. (2014), aiming to transfer knowledge from an accurate but cumbersome teacher model to a compact student model by matching the class probabilities produced by the teacher and the student. Focusing on AutoML settings with tabular data, Fakoor et al. (2020) proposed a general KD algorithm for different classical ML models and ensembles thereof. Also hoping to identify good choices in the KD pipeline like our work, Kim et al. (2021) compared Kullback-Leibler divergence and mean squared error objectives in KD for image classiﬁcation models, ﬁnding that mean squared error performs better. Recent KD research in the domain of NLP has investigated how to efﬁciently transfer knowledge from pretrained Transformer models. Sun et al. (2019) proposed BERT-PKD that transfers the knowledge from both the ﬁnal layer and intermediate layers of the teacher network. Jiao et al. (2020) proposed the TinyBERT model that ﬁrst distills the general knowledge of the teacher by minimizing the Masked Language Model (MLM) objective (Devlin et al., 2019), with subsequent task-speciﬁc distillation. Li et al. (2020) proposed a many-to-many layer mapping function leveraging the Earth Mover’s Distance to transfer intermediate knowledge. Our paper differs from the existing work in that we provide a systematic analysis of the different components in single-stage task-aware KD algorithms in NLP and propose the ﬁrst automated KD algorithm in this area. Mutual Information Estimation.Mutual Information (MI) measures the degree of statistical dependence between random variables. Given random variablesAandB, the MI between them, I(A, B), can be understood as how much knowing Awill reduce the uncertainty ofBor vice versa. For distributions that do not have analytical forms, maximizing MI directly is often intractable. To overcome this difﬁculty, recent work resorts to variational bounds (Donsker and Varadhan, 1975; Blei et al., 2017; Nguyen et al., 2010) and deep learning (Oord et al., 2018) to estimate MI. These works utilize ﬂexible parametric distributions or critics that are parameterized neural networks (NNs) to approximate unknown densities that appear in MI calcuations. Poole et al. (2019) provides a review of existing MI estimators and proposes novel bounds that trade-off bias and variance. Kong et al. (2020) uniﬁed language representation learning objective functions from the MI maximization perspective. Data Augmentation in NLP.Data Augmentation (DA) is an effective technique for improving the accuracy of text classiﬁcation models (Wei and Zou, 2019) and has also been shown to boost the performance of KD for NLP algorithms (Jiao et al., 2020) as well as KD in models for tabular data (Fakoor et al., 2020). Wei and Zou (2019) proposed the Easy Data Augmentation (EDA) technique that randomly replaces synonyms, inserts, swaps and deletes characters in the sentence. Jiao et al. (2020) proposed to utilize the pretrained BERT model and GloVe word embeddings (Pennington et al., 2014) to augment the input sentence via random wordlevel replacement. MixKD (Liang et al., 2021) adopts mixup (Zhang et al., 2018) and backtranslation (Edunov et al., 2018) in augmenting the text data to boost the performance of sentence classiﬁcation models. Unlike these papers, we propose a novel search space for DA policies that supports stacking elementary augmentation operations such as EDA, mixup, and backtranslation. Thus, our considered DA module is similar to AutoAugment (Cubuk et al., 2019), except it is used for KD in NLP with different elementary operators. Our study is structured around a conﬁgurable metadistillation pipeline called Distiller. Distiller contains four conﬁgurable components, namely: a data augmentation policya(·, ·), a layer mapping conﬁguration of intermediate distillation{m}, an intermediate distillation objectivel(·, ·), and a prediction layer distillation objectivel(·, ·). Assume the teacher networkfhasMlayers and the student networkfhasNlayers. Then for a given data/label pair(x, y)sampled from the datasetD, the student acquires knowledge from the teacher by minimizing the following objective: + βl(f(x), f(x)) + γl(y, f(x)) + βl(f(ˆx), f(ˆx)) + γl(ˆy, f(ˆx)), Herem∈ [0, 1]represents the layer mapping weight between thei-th teacher layer andj-th student layer,H, Hare thei-th and thej-th hidden states of the teacher and the student (i.e. their intermediate representations at layersiandj),β,β control the strength of distilling from class probabilities produced by the teacher, andγandγ control the strength of learning from ground truth data (x, y) and synthesized (augmented) data (ˆx, ˆy). In Appendix, we illustrated how previous model distillation algorithms (Jiao et al., 2020; Li et al., 2020; Liang et al., 2021) can be encompassed in the Distiller framework. Figure 1: Overview of the Distiller pipeline. All conﬁgurable components are colored. A key challenge is the limited data available to train students in KD. This can be mitigated via Data Augmentation (DA) to generate additional data samples. Unlike in supervised learning, where labels for synthetic augmented data may be unclear unless the augmentation is limited to truly benign perturbations, the labels for augmented data in KD are simply provided by the teacher which allows for more aggressive augmentation (Fakoor et al., 2020). Denoting the set of training samples of the down-stream task asD, the augmenter a(·, ·)will stretch the distribution fromE toE. We consider various elementary DA operations including: 1) MLM-based contextual augmentation (CA) , 2) random augmentation (RA), 3) backtranslation (BT) and 4) mixup. The search space of possible augmentations in Distiller is constructed by stacking these four elementary operations in an arbitrary order, as detailed in Algorithm 1. For contextual augmentation, we use the pretrained BERT model to do word level replacement by ﬁlling in randomly masked tokens. As in EDA (Wei and Zou, 2019), our random augmentation randomly swaps words in the sentence or replaces words with their synonyms. For backtranslation, we translate the sentence from one language (in this paper, English) to another language (in this paper, German) and then translate it back. Additionally, mixup can be used to synthesize augmented training samples. First proposed for image classiﬁcation (Zhang et al., 2018), mixup constructs a synthetic training example via the weighted average of two samples (including the labels) drawn at random from the training data. To use it in NLP, Guo et al. (2019); Liang et al. (2021) applied mixup on the word embeddings at each sentence position xwithλ ∈ [0, 1]as the mixing-ratio for a particular pair of examples x, x: ˆx= λx+ (1 − λ)x, ˆy= λy+ (1 − λ)y, Hereλis typically randomly drawn from a Uniform or Beta distribution for each pair,y, yare labels in one-hot vector format, and(ˆx, ˆy)denotes the new augmented sample. To apply mixup for sentence tagging tasks, in which each token has its own label, we propose calculating the weighted combination of the ground-truth target at each locationtas the new target: ˆx= λx+(1−λ)x, ˆy= λy+(1−λ)y, 3.2 Prediction Layer Distillation In traditional KD, the student network learns from the output logits of the teacher network, adopting these as soft labels for the student’s training data (Hinton et al., 2014). Here we penalize the discrepancy between the outputs of student vs. teacher via: wherel(·, ·)is the KD loss component whose search space in this work includes either: softmax Cross-Entropy (CE) or Mean Squared Error (MSE). 3.3 Intermediate Representation Distillation To ensure knowledge is sufﬁciently transferred, we can allow the student to learn from intermediate layers of the teacher rather than only the latter’s output predictions by minimizing discrepancies between selected layers from the teacher and the student. These high-dimensional intermediate layer representations constitute a much richer information-dense signal than is available in the low-dimensional predictions from the output layer. As teacher and student usually have different number of layers and hidden-state dimensionalities, it is not clear how to map teacher layers to student layers (m) and how to measure the discrepancy between their hidden states (l). Previous works proposed various discrepancy measures (or loss functions) for intermediate distillation, including: Cross-Entropy (CE), Mean Squared Error (MSE), L2 distance, Cosine Similarity (Cos), and Patient Knowledge Distillation (PKD) (Sun et al., 2019). For these objectives, we establish the following result (the proof is relegated to the Appendix). Theorem 3.1Minimizing MSE, L2, or PKD loss, and maximizing cosine similarity between two random variablesX,Yare equivalent to maximizing lower bounds of the mutual informationI(X; Y ). In our KD setting,XandYcorrespond to the hidden state representations of our student and teacher model (for random training examples), respectively. Inspired by this result, we can use any lower bounds of MI as an intermediate objective function in KD. In particular, we consider the multisample MI lower bound of Poole et al. (2019), which estimatesI(X; Y )given the samplex, y fromp(x, y)and anotherKadditional IID samples zthat are drawn from a distribution independent from X and Y : InI,f(·, ·)andq(·)are critic functions for approximating unknown densities andm(·, ·)is a Monte-Carlo estimate of the partition function that appears in MI calculations. Typically, the spacez and the samplex, yare from the same minibatch while training, that isK +1equals to the minibatch size.Ican ﬂexibly trade off bias and variance, since increasingα ∈ [0, 1]will reduce the variance of the estimator while increasing its bias. We propose to useIas an objective for intermediate distillation and call it MI-α. Our implementation leverages a Transformer encoder (Vaswani et al., 2017) to learnf(·, ·)andq(·). To our knowledge, this is the ﬁrst attempt to utilize complex NN architectures for critic functions in MI estimation; typically only shallow multilayer perceptrons (MLPs) are used (Tschannen et al., 2020). Our experiments (Table 4 in Appendix) reveal that Transformer produces a better critic function than MLP. Note that for intermediate distillation, objectives like MSE attempt to ensure the teacher and student representations take matching values, whereas objectives like MI (and tighter bounds thereof) merely attempt to ensure the information in the teacher representation is also captured in the student representation. The latter aim is conceptually better suited for KD, particularly in settings where the student’s architecture differs from the teacher (e.g. it is more compact), in which case forcing intermediate student representations to take the exact same values as teacher representations seems overly stringent and unnecessary for a good student (it may even be harmful for tiny student networks that lack the capacity to learn the same function composition used by the teacher). We emphasize that a high MI between student and teacher representations sufﬁces for the teacher’s prediction to be approximately recovered from the student’s intermediate representation (assuming the teacher uses deterministic output layers as is standard in today’s NLP models). Given that high MI sufﬁces for the student to match the teacher, we expect tighter MI bounds like MI-αcan outperform looser bounds like MSE that impose additional requirements on the student’s intermediate representations beyond just their information content. 3.3.1 Layer Mapping Strategy We investigate three intermediate layer mapping strategies: 1) Skip: the student learns from every bM/N clayer of the teacher, i.e.,m= 1when j = i × bM/Nc; 2) Last: the student learns from the lastklayers of the teacher, i.e.,m= 1when j = i + M − N; and 3) EMD: a many-to-many learned layer mapping strategy (Li et al., 2020) based on Earth Mover’s Distance. In the Distiller pipeline, the intermediate loss with EMD mapping can be denoted as:PP whereD= [d]is a distance matrix representing the cost of transferring the hidden states knowledge fromHtoH. AndW= [w]is the mapping ﬂow matrix which is learned by minimizing the cumulative cost required to transfer knowledge fromHtoH. In Distiller, the distance matrix is calculated via intermediate objective function: d= l(H, H). Our experiments indicate that the best distillation algorithm varies among datasets/tasks (see in particular the top-5 conﬁgurations listed in Table 9 in Appendix). This inspires us to train a prediction model that recommends a good KD pipeline given a dataset. To represent the distillation performance across datasets which are evaluated on different metrics, we deﬁne distillation ratio as the fraction of the teacher’s performance achieved by the student and use it as a general score for distillation performance. Then the prediction model can be trained to predict the distillation ratio based on features of the dataset/task as well as the features of each candidate distillation pipeline. Here we train our AutoDistiller performance prediction model via AutoGluon-Tabular, a simple AutoML tool for supervised learning (Erickson et al., 2020). To the best of our knowledge, our proposed method is the ﬁrst attempt towards automated KD in NLP. To study the importance of each component described in the previous section, we randomly sample Distiller conﬁgurations in the designed search space while ﬁxing the optimizer and other unrelated hyper-parameters. We apply each sampled distillation conﬁguration on a diverse set of NLP tasks and different teacher/student architectures. All experiments are evaluated on GLUE (Wang et al., 2019b) and SQuAD v1.1 (Rajpurkar et al., 2016) that contain classiﬁcation, regression, and sentence tagging tasks. Here, we view the question answering problem in SQuAD v1.1 as ﬁnding the correct answer span from the given context, which is essentially a sentence tagging task. We adopt the same metrics for these tasks as in the original papers (Wang et al., 2019b; Rajpurkar et al., 2016). Since Turc et al. (2019) ﬁnds initializing students with pretrained weights is better for distillation, we initialize student models with either weights obtained from taskagnostic distillation (Jiao et al., 2020) or pretrained from scratch (Turc et al., 2019). Three different pretrained modelsBERT(Devlin et al., 2019),RoBERTa(Liu et al., 2019) andELECTRA(Clark et al., 2020) are considered as teacher models in our experiments after task-speciﬁc ﬁne-tuning. As student models, we consider options likeTinyBERT, ELECTRA, as well as other models detailed in Table 6 in Appendix. Existing implementations of data augmentation in KD for NLP generally ﬁrst generate an augmented dataset that isKtimes larger than the original one and then apply the distillation algorithm over the augmented dataset. Such implementation separates the process of DA from KD, leading to a time/storage-consuming and inﬂexible KD pipeline. In Distiller, we instead apply DA dynamically during training. In addition, we use the teacher network to compute the soft labelˆyassigned to any augmented sample ˆx. To analyze the importance of different components in Distiller, we adopted fANOVA (Hutter et al., 2014), an algorithm for quantifying the importance of individual hyper-parameters as well as their interactions in determining down-stream performance. We use fANOVA to evaluate the importance of the four components in Distiller as well as their pairwise combinations: data augmentation, intermediate distillation objective, layer mapping strategy, and prediction layer distillation objective. Under the previously described experimental setup, we conducted a random search over Distiller conﬁgurations on each dataset and collected more than 1300 data points in total. Each collected data point contains a particular Distiller conﬁguration, the dataset/task, the teacher/student architectures, and the ﬁnal performance of the student model. Analyzing the data reveals three major ﬁndings: 1.Design of the intermediate distillation module is the most important choice among all factors studied in Distiller. 2.Among different loss functions for intermediate distillation, MI-α performs the best. 3.DA provides a large boost when the dataset or the student model is small. Additionally, we observe that the best distillation policy varies among datasets/tasks as shown in Table 9 in Appendix. Thus we train a meta-learning model, AutoDistiller, that can recommend a good distillation policy on any new NLP dataset based on which conﬁgurations tended to work well for similar datasets in our study. Table 1: Comparison of evaluation results on GLUE test set. BERT tuned BERTfrom (Devlin et al., 2019) and the teacher model trained by ourselves, respectively. BERT-EMD and MI-α are both initialized from TinyBERT, the difference is that BERT-EMD intermediate layer mapping strategy and MSE as intermediate loss, our MI-α model is trained with “Skip” as intermediate layer mapping strategy and MI-α as intermediate loss. Figure 2: As assessed via fANOVA, we report the individual importance of the four Distiller components in this ﬁgure and importance of interactions between any two of the four components in Figure 6 in Appendix. Four components are: lfor intermediate distillation objective, lfor prediction layer objective, a for data augmentation and m for layer mapping strategy. Average importance for each component (across tasks) is listed in the legend. Figure 2 illustrates that the objective function for intermediate distillationlhas the highest individual importance out of all components in Distiller, and the combination of the intermediate distillation objective and layer mapping strategy has the highest joint importance. Thus these are the components one should most critically focus on when selecting a particular KD pipeline. One hypothetical explanation is that the teacher can provide token-level supervision to the student via intermediate distillation, which can better guide the learning process of the student. Our ﬁnding is also consistent with the previous observations (Sun et al., 2019; Li et al., 2020). We submitted our MI-αmodel predictions to the ofﬁcial GLUE leaderboard to obtain test set results and also report the average scores over all tasks (the “AVG” column) as summarized in Table 1. The results show that the student model distilled Table 2: Ablation study of distillation performance on SQuAD v1.1 dev set. The ﬁrst line shows the performance of the BERTteacher. ELECTRA, TinyBERTand TinyBERTare three student networks. ELECTRA(FT) means to ﬁne-tune without KD. TinyBERTand TinyBERTare results obtained from Jiao et al. (2020). Models that end with “(MSE)” are trained with the MSE loss. “+ MI-α” means to distill the student with MI-α (α=0.9) as the intermediate loss function. “+ mixup” means to further apply the mixup augmentation. via the MI-αobjective function outperforms previous student models distilled via MSE or PKD loss. To further verify the effectiveness of MI-α, we compare how different choices oflaffect the distillation performance. In detail, we ﬁrst pick the top 5 best Distiller strategies according to the evaluation scores for every task and then count how many times each intermediate objective function appears in these strategies. Figure 3 shows that MI-αappears more frequently than all other objectives on both classiﬁcation and regression tasks. And the results for SQuAD v1.1 in Table 2 indicate the MI-αalso works well for sentence tagging. Figure 3: Intermediate objective functions used in the top-5 performing KD conﬁgurations on each dataset. The average count of each objective function is listed in the legend. Conﬁgurations are detailed in Appendix. Table 3: Student performance with(out) augmentation (augmenter initialized as CA+RA+mixup). We report the relative improvement for rows starting with “+ aug”. Data augmentation in KD provides the student additional opportunities to learn from the teacher, especially for datasets of limited size. Thus, our experiments investigate the effect of DA on four data-limited tasks: CoLA, MRPC, RTE and STS-B. We also study whether students with different architectures/sizes beneﬁt dissimilarly from DA. Table 3 demonstrates that DA generally provides a boost to student performance and is especially beneﬁcial for small models (BERTand BERT). Recall that we train our AutoDistiller performance prediction model on the previously collected experimental results via AutoGluon-Tabular. Once trained, AutoDistiller can recommend distillation pipelines for any down-stream dataset/task by ﬁxing the dataset/task features and searching for conﬁgurations that maximize the predicted distillation ratio. AutoDistiller operates on features that represent the dataset domain, the task type, and the task complexity, which are detailed in Appendix. Figure 4: Distillation ratio of AutoDistiller (AD) top-5 KD strategies vs. 5 randomly selected strategies, for a ﬁne-tuned BERTteacher and TinyBERTstudent. Higher ratio indicates better distillation performance. Mean and standard deviation of the four groups of ratios are listed in the legend. We evaluate the performance of AutoDistiller on the 8 GLUE datasets via a leave-one-dataset-out cross-validation protocol. Figure 5 in Appendix shows that AutoDistiller achieves positive Spearman’s correlation coefﬁcients for most datasets. Finally, we applied AutoDistiller on two unseen datasets “BoolQ” (Wang et al., 2019a) and “cloth” (Shi et al., 2021) not considered in our previous experiments. We compared the distillation ratio obtained by each of the top-N strategies suggested by AutoDistiller with the distillation ratio from each ofNrandomly selected strategies. The best strategy suggested by AutoDistiller achieves accuracy of74.2on “BoolQ” and70.1on “cloth”, close or superior to the teacher performance (73.4 on “BoolQ” and “71.2” on cloth). Figure 4 shows that AutoDistiller signiﬁcantly outperforms random search, indicating its promise for automated KD. We provided a systematic study of KD algorithms in NLP to understand the importance of different components in the KD pipeline for various NLP tasks. Using data collected from our study, we ﬁt a AutoDistiller to predict student performance under each KD pipeline based on dataset features, which is helpful for automatically selecting which KD pipeline to use for a new dataset. Here we uniﬁed the existing intermediate distillation objectives as maximizing the lower bounds of MI, leading to a new MI-αobjective based on tighter bounds, which performs better on many datasets.