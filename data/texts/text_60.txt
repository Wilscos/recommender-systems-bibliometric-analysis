Recommender systems typically operate on high-dimensional sparse user-item matrices. Matrix completion is a very challenging task to predict one’s interest based on millions of other users having each seen a small subset of thousands of items. We propose aGlobalLocal Kernel-based matrix completion framework, namedGLocalK, that aims to generalise and represent a high-dimensional sparse user-item matrix entry into a low dimensional space with a small number of important features. Our GLocal-K can be divided into two major stages. First, we pre-train an auto encoder with the local kernelised weight matrix, which transforms the data from one space into the feature space by using a 2d-RBF kernel. Then, the pre-trained auto encoder is ne-tuned with the rating matrix, produced by a convolution-based global kernel, which captures the characteristics of each item. We apply our GLocal-K model under the extreme low-resource setting, which includes only a user-item rating matrix, with no side information. Our model outperforms the state-of-the-art baselines on three collaborative ltering benchmarks: ML-100K, ML-1M, and Douban. • Information systems → Recommender systems;• Theory of computation → Kernel methods. Recommender Systems, Matrix Completion, Kernel Methods ACM Reference Format: Soyeon Caren Han, Taejun Lim, Siqu Long, Bernd Burgstaller, and Josiah Poon. 2021. GLocal-K: Global and Local Kernels for Recommender Systems. In Proceedings of the 30th ACM Int’l Conf. on Information and Knowledge josiah.poon@sydney.edu.au Management (CIKM ’21), November 1–5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.3482112 Collaborative ltering-based recommender systems focus on making a prediction about the interests of a user by collecting preferences from large number of other users. Matrix completion[2] is one of the most common formulation, where rows and columns of a matrix represent users and items, respectively. The prediction of users’ ratings in items corresponds to the completion of the missing entries in a high-dimensional user-item rating matrix. In practice, the matrix used for collaborative ltering is extremely sparse since it has ratings for only a limited number of user-item pairs. Traditional recommender systems focus on generalising sparsely observed matrix entries to a low dimensional feature space by using an autoencoder(AE)[11]. AEs would help the system better understand users and items by learning the non-linear user-item relationship eciently, and encoding complex abstractions into data representations. I-AutoRec[8] designed an item-based AE, which takes high-dimensional matrix entries, projects them into a lowdimensional latent hidden space, and then reconstructs the entries in the output space to predict missing ratings. SparseFC[6] employs an AE whose weight matrices were sparsied using nite support kernels. Inspired by this, GC-MC[1] proposed a graph-based AE framework for matrix completion, which produces latent features of user and item nodes through a form of message passing on the bipartite interaction graph. These latent user and item representations are used to reconstruct the rating links via a bilinear decoder. Such link prediction with a bipartite graph extends the model with structural and external side information. Recent studies [7,9,10] focused on utilising side information, such as opinion information or attributes of users. However, in most real-world settings (e.g., platforms and websites), there is no (or insucient) side information available about users. Instead of considering side information, we focus on improving the feature extraction performance for a high-dimensional useritem rating matrix into a low-dimensional latent feature space. In this research, we apply two types of kernels that have strong ability in feature extraction. The rst kernel, named “local kernel”, is known to give optimal separating surfaces by its ability to perform the data transformation from high-dimensional space, and widely used with support vector machines(SVMs). The second kernel, named “global kernel” is from convolutional neural network(CNN) architectures. The more kernel with deeper depth, the higher their feature extraction ability. Integrating these two kernels to have best of both worlds successfully extract the low-dimensional feature space. With this in mind, we propose aGlobal-Local Kernel-based matrix completion framework, calledGLocal-K, which includes two stages: 1) pre-training the auto-encoder using a local kernelised weight matrix, and 2) ne-tuning with the global kernel-based rating matrix. Note that our evaluation is under an extreme setting where no side information is available, like most real-world cases. The main research contributions are summarised as follows: (1) We introduce a global and local kernel-based auto encoder model, which mainly pays attention to extract the latent features of users and items. (2) We propose a new way to integrate pre-training and ne-tuning tasks for the recommender system. (3) Without using any extra information, ourGLocal-Kachieves the smallest RMSEs on three widely-used benchmarks, even beating models augmented by side information. Figure 1 depicts the architecture of our proposedGLocal-Kmodel, which applies two types of kernels in two stages respectively: pretraining (with the local kernelised weight matrix) and ne-tuning (with the global-kernel based matrix). Note that we pre-train our model to make dense connections denser and sparse connections sparser using a nite support kernel, and ne-tune with the rating matrix. This matrix is produced from a convolution kernel by reducing the data dimension and producing a less redundant but small number of important feature sets. In this research, we mainly focus on a matrix completion task, which is conducted on a rating matrix 𝑅 ∈ Rwith𝑚items and𝑛users. Each item𝑖 ∈ 𝐼 = {1,2, ..., 𝑚} is represented by a vector 𝑟= (𝑅, 𝑅, ..., 𝑅) ∈ R. Auto-Encoder Pre-training We rst deploy and train an item-based AE, inspired by [8], which takes each item vector𝑟as input, and outputs the reconstructed vector𝑟to predict the missing ratings. The model is represented as follows: where𝑊∈ Rand𝑊∈ Rare weight matrices,𝑏 ∈ R and𝑏∈ Rare bias vectors, and𝑓 (·)and𝑔(·)are non-linear activation functions. The AE deploys an auto-associative neural network with a singleℎ-dimensional hidden layer. In order to emphasise the dense and sparse connection, we reparameterise weight matrices in the AE with a radial-basis-function(RBF) kernel, which is known as Kernel Trick[3]. Local Kernelised Weight Matrix The weight matrices𝑊and𝑊in Eq. (1) are reparameterised by a 2d-RBF kernel, named local kernelised weight matrix. The RBF kernel can be dened as follows: where𝐾 (·)is a RBF kernel function, which computes the similarity between two sets of vectors𝑈,𝑉. Here,𝑢∈ 𝑈and𝑣∈ 𝑉. The kernel function can represent the output as a kernel matrixLK(see Figure 1), in which each element maps to 1 for identical vectors and approaches 0 for very distant vectors between𝑢and𝑣. Then, we compute a local kernelised weight matrix as follows: where𝑊is computed by the Hadamard-product of weight and kernel matrices to obtain a sparsied weight matrix. The distance between each vector of𝑈and𝑉determines the connection of neurons in neural networks, and the degree of sparsity is dynamically varied as vectors are being changed at each step of training. As a result, applying the kernel trick to weight matrices enables regularising weight matrices and learning generalisable representations. Global kernel-based Rating Matrix We ne-tune the pre-trained auto encoder with the rating matrix, produced by the global convolutional kernel. Prior to ne-tuning, we rstly describe how the global kernel is constructed and applied to build the global kernel-based rating matrix. The entire construction procedure can be dened as follows: As shown in Figure 1, the decoder output of the pre-trained model is the matrix that includes initial predicted ratings in the missing entries, and passed to pooling. With item-based average pooling, we summarise each item information in the rating matrix. Eq. (4) shows the reconstructed item vectorˆ𝑟from the decoder output matrix𝑅 is passed to pooling, and interpreted as item-based summarisation. Let𝑀 = {𝜇, 𝜇, ..., 𝜇} ∈ Rbe the pooling result, which plays a role as the weights of multiple kernels𝐾 = {𝑘, 𝑘, ..., 𝑘} ∈ R. In Eq. (5), these kernels are aggregated by using an inner product. The result can be dynamically determined by dierent weights and dierent rating matrices so that it can be regarded as the ratingdependent mechanism. Then, the aggregated kernel𝐺𝐾 ∈ Ris used as a global convolution kernel. We apply a global kernel-based convolution operation to the user-item rating matrix for global kernel-based feature extraction. In Eq. (6),ˆ𝑅is the global kernelbased rating matrix, which is used as input for ne-tuning, and⊗ denotes a convolution operation. Auto-Encoder Fine-tuning We then explore how the ne-tuning process works. The global kernel-based rating matrixˆ𝑅is used as input for ne-tuning. It takes weights of a pre-trained AE model and makes an adjustment of the model based on the global kernel-based rating matrix, as depicted in Figure 1. The reconstructed result from the ne-tuned AE corresponds to the nal predicted ratings for matrix completion in recommender system. Figure 1: The GLocal-K architecture for matrix completion. (1) We pre-train the AE with the local kernelised weight matrix. (2) Then, ne-tune the trained AE with the global kernel-based matrix. The ne-tuned AE produces the matrix completion result. We conduct experiments on three widely used matrix completion benchmark datasets: MovieLens-100K (ML-100K), MovieLens-1M (ML-1M) and Douban (density 0.0630 / 0.0447 / 0.0152). These datasets comprise of (100 k/1 m/136 k) ratings of (1,682 / 3,706 / 3,000) movies by (943 / 6,040 / 3,000) users on a scale of𝑟 ∈ {1,2,3,4,5}. For ML-100K, we use the canonical u1.base/u1.test train/test split. For ML-1M, we randomly split into 90:10 train/test sets. For Douban, we use the preprocessed subsets and splits provided by Monti et al. [5]. We compare the RMSE with the eleven recommendation baselines:(1) LLORMA[4]is a matrix factorization model using local low rank sub-matrices factorization.(2) I-AutoRec[8]is a autoencoder based model considering only the user or item embeddings in the encoder.(3) CF-NADE[13]replaces the role of the restricted Boltzmann machine (RBM) with the neural auto-regressive distribution estimator (NADE) for rating reconstruction.(4) GC-MC[1] is a graph-based AE framework that applies GNN on the bipartite interaction graph for rating link reconstruction. We consider GCMC with side information as(5) GC-MC+Extra.(6) GraphRec[7] is a matrix factorization utilizing graph-based features from the bipartite interaction graph. We consider GraphRec with side information as(7) GraphRec+Extra.(8) GRAEM[9]formulates a probabilistic generative model and uses expectation maximization to extend graph-regularised alternating least squares based on additional side information (SI) graphs.(9) SparseFC[6]is a neural network in which weight matrices are reparameterised in terms of low-dimensional vectors, interacting through nite support kernel functions. This is technically equivalent to the local kernel of GLocal-K.(10) IGMC[12]is similar to GCMC but applies a graphlevel GNN to the enclosing one-hot subgraph and maps a subgraph to the rating in an inductive manner.(11) MG-GAT[10]uses attention mechanism to dynamically aggregate neighbor information of each user (item) for learning latent user/item representations. We use two 500-dimensional hidden layers for AE and 5-dimensional vectors𝑢,𝑣for the RBF kernel. For ne-tuning, we use a single Table 1: RMSE test results on three benchmark datasets. The column Extra. represents whether the model utilises any side information. All RMSE results are from the respective papers cited in the rst column, and the best results are highlighted in bold. convolution layer with a 3x3 global convolution kernel. Inspired by [8], we train our model using the L-BFGS-B optimiser to minimise regularised squared errors, where𝐿regularisation is applied with dierent penalty parameters𝜆,𝜆for weight and kernel matrices respectively. Based on validation results, we choose the following settings for (ML-100K / ML-1M / Douban). (1) L-BFGS-B:𝑚𝑎𝑥𝑖𝑡𝑒𝑟 = (5 / 50 / 5),𝑚𝑎𝑥𝑖𝑡𝑒𝑟= (5 / 10 / 5), (2)𝐿regularisation:𝜆= (20 / 70 / 10),𝜆= (.006 / .018 / .022). We repeat each experiment ve times and report the average RMSE results. We rst evaluated our GLocal-K model on ML-100K (u1.base/u1.test split)/-1M datasets and compare with the baseline models. The RMSE test results are provided in Table 1. It can be easily observed from both GC-MC and GraphRec that incorporate side information improves the recommendation performance, e.g., the error rate of GC-MC+Extra. and GraphRec+Extra. reduce by 0.001 and 0.007 respectively on ML-100K via side information inclusion. Similar to GC-MC, IGMC also learns graph-structural relations from the bipartite user-item interaction graph derived from the rating matrix using GNN but outperforms GC-MC+Extra. by focusing on one-hot Figure 2: Performance comparison w.r.t. dierent sparsity levels on ML-100K and Douban datasets. Figure 3: Performance comparison w.r.t. the numb er of pretraining epochs on three benchmark datasets. sub-graphs with inductive matrix completion. GRAEM focuses on additional graph SI and MG-GAT uses auxiliary information to represent user-user and item-item graph relations. Dierent from those models above, the rst three models in the table use only the rating matrix structure and achieve better results on ML-1M. Our proposed GLocal-K also draws on the rating matrix structure and uses no extra information, outperforming all the baseline models above on three datasets, including those with additional side information, which illustrates the ecacy of combining the local-global kernels for recommendation tasks. Moreover, SparseFC also achieves higher accuracy than those baseline models on three datasets except for MG-GAT, showing the benets of proper kernel-approximations of the weight matrix. Our GLocal-K surpasses SparseFC, further illustrating the eectiveness of a global kernel that learns to rene and extract the relevant information from the sparse data matrix. We varied the training ratio from 0.2 to 1.0 and compared the RMSE test results with SparseFC on ML-100K and Douban in Figure 2. It can be seen that both models on the two datasets demonstrate a similar overall trend: the error rate increases as the training size decreases, which complies with conventional expectation. More specically, with training ratios of 0.4-1.0, GLocal-K outperforms SparseFC by a merely constant gap on both ML-100K and Douban. This illustrates the superior eectiveness of cooperation by local and global kernels of GLocal-K. In addition, when training size reduces from 0.4 to 0.2 on Douban, the error rate of SparseFC deviates from the previous curve and goes up dramatically while GLocal-K still rises at a stable rate as on ML-100K. This implies that the global kernel can deal with scarce data via feature extraction. We explored the optimal number of epochs for pre-training on ML-100K, ML-1M and Douban. The RMSE results for the three datasets using pre-training epochs from 0 (i.e., no pre-training) to 60 are provided in Figure 3. These three datasets represent similar Table 2: Performance comparison of RMSE test results of Global Kernel w.r.t. (1) dierent convolution kernel sizes, (2) dierent numbers of convolution layers and (3) dierent kernel aggregation mechanisms on three benchmark datasets. The best results are highlighted in bold. Agg. mechanism bowl-shaped curves. The RMSE rst keeps decreasing as the pretraining epoch increases from 0, indicating that pre-training benets GLocal-K to achieve better performance on all three datasets. Then the RMSE starts to go up again after reaching its optimum at 30 epochs for ML-100K and 20 epochs for both ML-1M and Douban. Referring to the dataset statistics, we surmise that having more item numbers with lower density may lead to less pre-training for optimal performance. To explore the eectiveness of the global kernel-based convolution with in-depth analysis, we rst tried multiple kernel sizes and convolution layers. The RMSE results on the three datasets are presented in Table 2. It can be seen from Table 2 that using 3x3 sized kernel achieves the best performance on all three datasets and the error rate goes up as the size increases to 5x5 or 7x7. It implies that focusing on more local features with smaller kernel size might be more eective for extracting generalizable patterns over the whole data matrix. Moreover, Table 2 shows an incremental performance degradation when the conv layer increases from 1 to 3, indicating a single convolution layer is enough and optimal for feature extraction. In addition, we also explored two variants of kernel aggregation mechanisms: (1) integrating multiple kernels based on the weights and (2) aggregating via pure element-wise average. As shown in Table 2, weight-based aggregation reduces RMSE by 0.004 and 0.009 on ML-100K and Douban while achieving similar performance on ML-1M. Overall, it can be seen that using feature-indicative weights to aggregate the kernels is more eective than purely applying element-wise averages. In this paper, we introducedGLocal-Kfor recommender systems, which takes full advantage of both a local kernel at the pre-training stage and a global kernel at the ne-tuning stage for capturing and rening the important characteristic features of the sparse rating matrix under an extremely low resource setting. We demonstrate RMSE on three benchmark datasets: MovieLens-100k/-1M and Douban, outperforming numerous baseline approaches. In particular, we highlighted the eectiveness of our global kernel for exerting scarce data by evaluating the cold-start recommendation. It is hoped that our Global-K gives some insight into the future integration of both kernels for high-dimensional sparse matrix completion with no side information.