research attention owing to the widespread existence of graph-structured data in the real world such as social networks, citation networks, and other user-item interaction systems [1]. Learning graph embedding is a powerful approach of graph representation learning, which maps the characteristics of nodes to a low-dimensional vector space so that the proximities of nodes in topological space can be well reserved [2]. It has shown a great success in graph representation learning from shallow graph embedding methods [3], [4], [5] to deep graph neural networks (GNNs) [6], [7], [8], [9]. Recently, representation learning on dynamic graph has attracted many research attention [10], [11], [12], and they mainly model temporal graphs either as a sequence of snapshots [13], [14], [15], [16], [17], [18], or as real events with timestamps [19], [20], [21], [22], [23], [24]. For existing works learning temporal interaction embedding from the sequence of interactions, a number of studies discretely update the embeddings of the interactive nodes once an interaction occurs [25], [26]. Other works learn the interactions by aggregating the temporal neighbor features to pass messages between nodes [19], [23], which model discrete dynamic of node representation in multiple propagation layers [27]. Although these discrete works have made substantial advances in learning temporal interaction embedding, they fail to capture the continuous dynamic evolution of nodes embedding trajectories thus losing temporal information for non-ignorable and inactive nodes with long-interval interactions (e.g., User A in Fig.1). Therefore, we aim to learn nodes embedding trajectories and capture the continuous dynamic of node representations. Learning dynamic node embedding trajectories is extremely challenging due to the complex non-linear dynamic in temporal interaction graphs (TIG). We conclude the challenges in three folds under the scenario of posting in Reddit website. First, latest interaction information may have a signiﬁcant impact on the current interaction, e.g., in Fig.1, the posts made by User A at twould be inﬂuential for t, since guidelines of holiday gathering under COVID-19 pandemics will be regularly implemented. Second, the node state is also affected by their neighbors’ attention over time. For instance, User A is expected to emulate their neighbors by posting “COVID-19 vaccine” at tin Fig.1. Third, inherent characteristics of node would fatally determine the state regardless of aforementioned two factors. For example, User A is a frontline healthcare personnel and she could inherently pay much attention to COVID-19 trainings on infection control. In summary, existing methods hold partial considerations for these three-fold factors. To cover the shortcomings of previous methods, we propose a Continuous representation learning method on Temporal Interaction Graphs (ConTIG). The proposed ConTIG contains two modules: the update module and transform module. When an interactive message comes, in the update module, inspired by [28], we deﬁne a neural ordinary differential equations (ODE) [29] with the three aforementioned factors affecting node states (i.e., latest interaction, neighbors’ feature and inherent characteristics), and incorporate them with the continuous-time encoding function to trace dynamic knowledge and learn node state trajectories. Then the node embeddings at the ending time of neural ODE is used to update embeddings of interacting nodes. In the transform module, a self-attention mechanism is introduced to awaken historical interactive information of current interacting nodes and convert them to generate future node representations. The results on temporal link prediction, temporal node recommendation and dynamic node classiﬁcation tasks show the effectiveness of our proposed model compared with a range of state-of-the-art baselines, especially for long-interval interactions prediction. The contributions of this work are summarized as follows: incorporate three-fold factors (i.e., latest interaction, neighbor features and inherent characteristics) with a neural ODE. This allows us to capture continuous dynamic of node embedding trajectories by continuously updating node embeddings in the update module. self-attention mechanism to predict future embeddings in the transform module. This makes ConTIG effective on long-interval interactions prediction. temporal node recommendation and dynamic node classiﬁcation tasks, and ConTIG compares favorably against state-of-the-art methods. The rest of this paper is organized as follows. In Section 2, we discuss some related work. Sections 3 and 4 describes the notations and our proposed model in detail. In Section 5, we conduct experiments on several datasets and compare them with state-of-the-art methods. In Section 6, the conclusion and our future work are presented. Early works for representation learning are mainly shallow models including graph factorization approaches [30], [31] and skip-gram models [3], [4], [5], which learn node representations by random walk objectives. With the success of deep learning, GNNs [7], [8], [9], [32] have gradually attracted great research interest. They are effective approaches to learn node representations by updating each node according to messages from neighboring nodes in graphs in each layer. GNNs essentially model discrete dynamics of node representations with multiple propagation layers [27]. However, all the above mentioned approaches are limited to learning node embeddings on static graph structure information, and the temporal behavior of interaction over time is generally ignored. One common way to handle temporal graphs is to decompose it into multiple static graphs snapshots by a regular time interval. Some works embed the graph convolution into the recurrent neural network (RNN) based models or attention mechanism [33], which learns to exploit the dynamic information in the graph evolution within a period of time [13], [14], [16], [17], [34], [35], [36]. Some other works are dynamic extensions of ideas applied in the static case inspired by methods such as PageRank [37], graph autoencoder [38] and the topic model [15] to capture both the temporal community dynamics and evolution of graph structure. However, learning embedding from the sequence of graph snapshots sampled from the temporal graph by a regular time interval may lose information by looking only at some snapshots of the graph over time. Therefore, recent works learn temporal graph embedding from the sequence of timed interactions. A number of studies learn the sequence of interactions by discretely updating the node embeddings once an interaction occurs by RNNs [24], [25], [39], memory networks [26], time point process [40], [41], transformer network [42], generative models [43], etc. Other works learn the interactions by aggregating the temporal neighbor features to pass messages between nodes [20], [23], [24], [44] or learning node representation from random walk objects [21] and temporal motifs [45], [46], [47]. TGAT [23] proposes the temporal graph attention layer to aggregate temporal-topological neighborhood features. TGN [24] makes a combination of updating operators and aggregating operators. However, these methods learn the discrete dynamic of node representations, and it is not beneﬁcial to learn the node state trajectories. Moreover, they ignore the inﬂuence of the inherent characteristics of the change on the node states, which is not beneﬁcial to capture the complex non-linear dynamic of node representations. Different from the aforementioned works, we assembled the latest interaction, neighbor features and inherent characteristics of the nodes into a neural ODE to learn node state trajectories and update node embeddings. As a result, our method captures the continuous dynamic of node representation. 2.3 Neural ODE Neural ordinary differential equations (ODE) [29] is an approach for modelling a continuous dynamics on hidden representation, where the dynamic is characterised through an ODE parameterised by a neural network (i.e. ODE Solver). Recently, there are some works devote to use neural ODE [29] combined with GNN to characterise the continuous dynamics of node representations [28], [48]. However, these methods are not built for temporal graphs, which are continuous-time GNNs learning the dynamic of node representation in static graph scenario to build ”deeper” networks. For temporal graphs, [49] learns the evolution of the temporal knowledge graph dynamics over time using ODEs as a tool. CG-ODE [50] proposed a latent ODE generative model that learns the coupled dynamics of nodes and edges with a GNN based ODE in a continuous manner. Different from them (i.e., [49], [50]), we stand at the perspective of node state modeling in temporal interaction graph, using a differential equation integrating the three-fold factors (i.e., latest interaction, neighbor features and inherent characteristics). This allows us to successfully capture continuous dynamic of node embedding trajectories. We summarize some notations and state the problem we want to solve in this section. Deﬁnition 1 (Temporal interaction graph). Temporal interaction graph is a graph with temporal and attributed interaction edges between nodes. Temporal interaction graph is deﬁned as a pair G = (V, E), where V represents vertices sets, and E is a set of the sequences of interactions with time label between two vertices. An interaction e is a tuple of form (u, i, t, f), where u and i ∈ V represent two vertices have an interaction at timestamp t, and frepresents the feature of interaction Deﬁnition 2 (Temporal interaction graph embedding). Given the temporal interaction graph G = (V, E), we aim to learn a mapping function: φ : V → Rto map node features into a low-dimensional latent space and generate node embeddings H ∈ Rto represent nodes, where d  |V| representing the dimension of node embeddings. Deﬁnition 3 (Interaction intervals). In the temporal interaction graph, each interaction is an edge connected by a source node and a target node, so we can use source nodes as the label of interaction to calculate the time interval ∆tbetween current interaction and its latest interaction, where the latest interaction is the edge where the source node last appeared. For example, the latest interaction of the current interaction, e= (u, i, t, f), is the interaction e= (u, r, τ, f) in which the node u at the last time appeared. As a result, the time interval ∆tbetween eand eis t−τ, where rand τrepresents the latest interactive node and timestamp of u, respectively. While, p and q represent the interaction id, and j represents that how many times u appears. Here, if the node appears for the ﬁrst time (i.e., j = 0), the time interval of the interaction is 0. In this section, we will describe our model in detail. The proposed model – ConTIG consists of two essential modules (i.e., update and transform module). We will brieﬂy introduce them in Section 4.1 and describe each module in detail in the rest of sections. 4.1 Overview of the ConTIG Fig.2 provides the framework of our proposed ConTIG. The encoder-decoder framework (Fig.2 left) consists of two modules: the update module and the transform module. In the update module (Fig.2 middle), a continuous inference block is used to update the embeddings of the interacting nodes in a continuous setting. In the transform module (Fig.2 right), a graph attention layer is applied to estimate the future embedding of nodes by capturing observed historical temporal interaction information. When an interaction message e= (u, i, t, f) comes, we ﬁrst employ a neural encoder to project the latest interaction message e= (u, r, τ, f) of input interacting node into a latent space, i.e., E = f (X). Afterwards, treating E as the initial value H(0) in continuous inference block, we utilize a neural ordinary differential equations (ODE) to learn the nodes’ continuously changing state trajectories at a certain time interval [0, ∆t], and the embeddings of nodes ¯His updated as¯Hby the node embeddings obtained in nodes’ state trajectory at ending time H(∆t). Then, selecting k observed historical neighbors of current nodes u, we introduce a self-attention mechanism on graphs to convert these interactions information to generate future node embeddingˆH. Finally, the decoder uses the future node embeddingsˆHfor downstream tasks (i.e., temporal link prediction, temporal node recommendation and dynamic node classiﬁcation). 4.2 Encoder-Decoder The proposed ConTIG adapts an encoder-decoder architecture (Fig.2 left). For each interaction, we ﬁrst project the features into a latent space to generate the initial hidden representation of nodes. After learning node embeddings at current timestamp, we ﬁnally design a decoder for the speciﬁc downstream tasks. Before the encoder, we initialize the latest interactive node rand timestamp τas 0 for all node v ∈ V (line 1 in Algorithm 1). In the encoder, to learn the temporal pattern of each interaction, we use a time encoding function in [51] to obtain a continuous functional mapping F: T → Rfrom time domain to the d-dimensional vector space, which projects the timestamps of interactions into the continuoustime space. For any given time t, we will generate its time embeddings as follows: F(t) =1√d[cos (ωt), sin (ωt) , where ω, ω, ..., ωare trainable parameters. To learn informative node representations Hat timestamp t, we concatenate the latest node embedding of the interactive Transform node¯Hand it latest interactive node¯H, the last interaction message feature fand the time embeddings of the time interval F(∆t) to represent new node features of u at timestamp t (line 3 in Algorithm 1). Here, as we focus on the interval between current interaction and its latest interaction, we use the time embedding F(∆t). Afterwards, we adopt a fully connected layer as an encoder (line 4 in Algorithm 1), and the hidden representations can be deﬁned as follows: where || is the concatenation operation, and f(·) is a linear projection as follows: where W and b are learnable parameters. For the decoder, two fully connected layers are designed for temporal link prediction and temporal node recommendation tasks, and three fully connected layers are designed for dynamic node classiﬁcation task. 4.3 Update: Continuous Inference Block In update module (Fig.2 middle), to capture the continuous dynamic of node representation, inspired by [28], we deﬁne a ODE with the latest interaction information, neighbor features and inherent characteristics of nodes, and use a neural solver to generate the node state trajectories at a certain time interval [0, ∆t]. In this way, our method can estimate the possible direction of embedding trajectory for an inactive node (line 5 and 11-16 in Algorithm 1. Continuous Graph Attention ()Ht We select the interactions between each node in temporal interaction graph u and its last interactive node r, and divide them into a set Eto generate an adjacency matrix Sdescribing their relationships. S∈ Ris deﬁned by Eas follows: As the degree of nodes can be very different, we typically normalize the adjacency matrix as DSD, whereP D= diagS∈ Ris the degree matrix of S. As such a normalized adjacency matrix always has an eigenvalue decomposition, and the eigenvalues are in the interval [−1, 1]. To get the positive eigenvalues and make graph learning algorithms stable in practice, we follow [7] and leverage the following regularized matrix for characterizing graph structures: where β ∈ (0, 1) is a hyperparameter, I∈ Ris the identity matrix, N = |V|, and eigenvalues of Aare in [0, β]. Afterwards, to capture the complex non-linear dynamic of node embeddings, we assume that there are three possible factors affecting the node states: 1) latest interaction information exhibiting the latest nodes states; 2) neighbors’ features affecting the change of node states; 3) the inherent characteristics of nodes determining the inﬂuence of the aforementioned two factors. Based on these considerations, we deﬁne an ODE in Eto solve the node embedding trajectories, which adaptively fuse them with a gated mechanism. Here, we treat the encoder Eas the initial value of ODE H(0), i.e., H(0) = E. Then we use a differential equation deﬁned as follows to learn the continuous node representations in the interval of interactions: where H(0) represents the latest interaction information of nodes, AH(t) represents the inﬂuence from neighbors, −H(t) represents the inherent characteristics of nodes,  denotes the element-wise product operation. z, z, and z represents three gates, respectively. They are computed as: where W, W, Wand b, b, bare trainable parameters, and σ(·) is the sigmoid activation function to normalize the output into [0, 1]. The gated fusion mechanism can adaptively fuse three factors according to their importance calculated by the initial value of ODE. In addition, the updating process starts at the timestamp of latest interaction of the node. To this end, following [24], when an interactive message comes, we use the latest interaction information to update node embeddings and save the current interaction message for the next time the node appears. Meanwhile, due to the integration of time encoding in H(0), the temporal behaviors of nodes could be captured. As a result, we can leverage the aforementioned ODE to learn the nodes’ state trajectories at the interval [0, ∆t], and the three main factors mentioned above jointly captures the continuous dynamic of node representations. Then the node representations are updated by ODE solver as follows: where g(t) =. Finally, we use the hidden state at the end time H(∆t) to update the previous embedding memory¯Has¯H (i.e.,¯H= H(∆t)) (line 5 in Algorithm 1). 4.4 Transform: Graph Attention Layer After capturing the continuous dynamic of node representations from latest interactions sets and updating the embeddings of nodes, a graph attention layer in transform module (Fig.2 right) is applied to convert the historical observed interaction features of nodes to generate future representations (Line 6 and 17-24 in Algorithm 1). In this module, for the current interaction e connected by u and v at time t, their temporal neighbors and the interaction information between them are token as input. We introduce a self-attention mechanism to distinguish different neighbors, and take account of the structural information with temporal information [23]. For node u at time t, we consider its neighbors N(u, t) = {i, ..., i}, where the interactions between u and i∈ N(u, t) occurred at time tprior to time t, and the sampling process of temporal neighbors of node i is the same as u. Then we take the node information with time t encoding Z=¯h||F(t) and the neighborhood information with the time delta t −t encoding Z=¯H||f||F(t − t) as the input of attention, where the time delta t − tis between current interaction e and the interaction of u and its neighbors i∈ N (u, t). In attention, three different linear projections is used to obtain the query Q, key Kand value V: where W, W, Ware the weight matrices. The attention weights αis given by: where the attention weight αreveals how node u attends to the features of node iwithin the topological structure deﬁned as N(u, t) after accounting for their interaction time with i. Therefore, the layer produces the time-aware representation of node u at time t,ˆh, which represents the future hidden representations generated by historical observed interactions as follows: To stabilize the learning process and improves performances, we extend the self-attention mechanism to be multihead ones [33]. Speciﬁcally, we concatenate M parallel attention mechanisms with different learnable projections: where Vrepresent the value with different projections in the mhead attention, and αrepresent the attention weight calculated by the query and key with different projections in the mhead attention. Finally, the future node embeddingsˆHis generated by calculatingˆhfor each node in interactions (line 6 in Algorithm 1). The latest interactive node rand rare updated as i and u, respectively. In addition, the latest interactive timestamps τand τare both updated as t (Line 7-8 Algorithm 1). 4.5 Binary Cross-Entropy Loss Function In this work, we adopt time-sensitive link prediction binary cross-entropy loss function to learn ConTIG’s parameters. The binary cross-entropy loss function is deﬁned as follows and our goal is to maximize this likelihood function: where the summation is over the observed edges on u and ithat interact at time t, Q is the number of negative samples and P (i) is the negative sampling distribution over the node space. Algorithm 1 Continuous Representation Learning on Temporal Interaction Graphs Input: Edge Stream e = (u, i, t, f) ∈ E in temporal interaction graph G = (V, E) Output: Embedding resultsˆhfor all nodes ¯h← x, r← 0, τ← 0, ∀v ∈ V ¯h,¯h← Continuous Inference Block (E, u, i) ˆh,ˆh← Graph Attention Layer (¯h,¯h, u, i, t) 4.6 Complexity Analysis The time complexity of our proposed method mainly consists of two portions. First, for the update module (i.e., continuous inference block), we consider that both adjacency matrices are stored as sparse matrices. And the runtime of the ODE solver depends on the length on the time interval (i.e., the end time of ODE solver) ∆t and the complexity of the dynamics. Then, the time complexity of the continuous inference block is O(∆t |E|). Second, for the transform module (i.e., graph attention layer), since the masked self-attention operation is parallelizable, as suggested by [33]. The per-batch time complexity of the graph attention layer with m heads can be expressed as O(mk), where k is the average neighborhood size. Since the batch is divided by edges, the time complexity of the graph attention layer is O(mk |E|), where m  |E| and k  |E|. Therefore, the time complexity of ConTIG is O((∆t + mk) |E|) ≈ O(C |E|), where C is a constant and is relative to the runtime of the ODE Solver. In this section, we will utilize four networks to compare the performance of our model with four static methods and ﬁve dynamic methods. We conduct three main tasks, link prediction, node recommendation and node classiﬁcation, to evaluate the inﬂuence of introducing temporal information and learn continuous dynamic of node embedding trajectories. 5.1 Experimental Setup Datasets We evaluate the performance of ConTIG on temporal link prediction, node recommendation and dynamic node classiﬁcation tasks with four public datasets, where three datasets are user-item networks selected by [25] and one dataset is e-mail network. The statistics of the four datasets are summarized in Table 1. between active users and pages they edit the contents with unique timestamps and the dynamic labels indicating whether a user is banned from editing. tween active users and the posts they submit on subreddits and the dynamic labels indicating if users are banned from posting. tween students and MOOC online course, e.g., viewing a video, submitting an answer, etc. at the University of California, and describes the interaction between users by sending private message at different timestamps. The dataset is without node label and edge features. Baselines To evaluate the performance of ConTIG, we compare our method with state-of-the-art graph embedding methods on both static and dynamic graphs. Static graph embedding methods: GAE and VGAE [52] learn latent representations by graph auto-encoder. GraphSAGE [8] and GAT [9] are GNNs that capture the dependence of graphs via message passing between the nodes of graphs. Dynamic graph embedding methods: CTDNE [21] deﬁnes the temporal random walk requiring the walks to obey the temporal order. JODIE [25] learns to generate embedding trajectories of all users and items from temporal interactions by update and project operations. DyRep [40] use deep recurrent architecture and attention mechanism to effectively model ﬁne-grained temporal dynamic. TGAT [23] introduces a self-attention mechanism and a functional time encoding technique to learn the time-feature interactions. TGN [24] is a generic inductive framework operating on continuous-time dynamic graphs [10] represented as a sequence of events. Parameter Settings For parameter settings, the dimension of both node representations and time representations are set as 172. The optimizer is Adam algorithm, learning rate is 0.0001, and dropout probability is 0.1. In continuous inference block, parameter β in adjacency matrix regularization is 0.95, and the end time of ODE is set as 1.0 instead of the real interval ∆t. In graph attention layer, the number of selected neighbors k is set as 15 and heads M in attention is set as 2, and the negative sampling distribution P (i) is uniform distribution. Settings for Baselines For static baselines, we adopt the same baseline training procedures as in [23]. We refer to the PyTorch geometric library for implementing the GAE and VGAE baselines, and develop off-the-shelf implementation for GraphSAGE and GAT by referencing their original implementations to accommodate the temporal setting and incorporate edges features. For dynamic baselines, we use the open-source implementations for CTDNE, and use the source code released by the authors to implementing TGAT and TGN, where we implement JODIE and DyRep in the version of TGN in PyTorch. Implementation All our experiments are implemented on a 32g-MEM Ubuntu 20.04 server with Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz and NVidia(R) 2080Ti GPUs. All code is implemented in Pytorch version 1.5.1. 5.2 Temporal Link Prediction The goal of the temporal link prediction task is to predict the probability that there exists a link between the two nodes given two nodes and a certain timestamp in the future. For this task, we evaluate our method on both the transductive and inductive setting. In the transductive task, we predict the links between nodes observed during training. In the inductive task, we predict the links between new nodes which have not been observed in the past. Our model is tuned on the validation set and we report the average precision (AP) on the test set. We divide the training, validation, and testing sets into a 70%-15%-15% split. After the division, the nodes that do not appear in the training set are considered as new nodes. The model is trained by temporal link prediction. The results comparison between our method and baseline methods in temporal link prediction tasks are shown in Table 2. We observe that: 1) static graph embedding methods including GAE, VGAE, GraphSAGE and GAT, perform worse than those baselines modeling temporal interaction information. Because most of the interactions in the real-world networks are time-stamped; 2) among the dynamic graph embedding works, both temporal neighbor information (i.e., CTDNE, TGAT) and latest interaction information (i.e., JODIE, DyRep) methods perform worse than fusing methods (i.e., TGN, ConTIG); and 3) ConTIG achieves the best prediction performance on the datasets Wikipedia, Reddit, and CollegeMsg for both transductive and inductive settings, and the second best performance on Mooc for the inductive task. This observation demonstrates the advantage of our proposed method compared to existing methods. In fact, by considering the inherent node properties and modeling the three important factors (i.e., latest interaction, neighbor features and inherent characteristics) in temporal interaction graphs, our method can capture the complex non-linear dynamics of node representations effectively, thereby achieving superb performance. 5.3 Temporal Node Recommendation The goal of temporal node recommendation task is to predict the top-K possible neighbors of node u at t given the node u and future timestamp t. This task is also used to evaluate the performance of temporal network embedding methods. For this task, we evaluate all methods on transductive setting, each method outputs the user u’s preference scores over all the items at time in test set. We sort scores in a descending order and record the rank of the paired node u and i. We evaluate the task on three user-item networks (i.e., Wikipedia, Reddit and Mooc), where CollegeMsg dataset is not included because it is not a user-item network. Our evaluation metric in this task is Recall@K, where K ∈ {5, 10, 15, 20}. The results comparison between our method and baseline methods in temporal node recommendation task is shown in Fig.3, showing that our model ConTIG perform better than all the baselines. Compared with the best competitors (i.e., TGN), the recommendation performance of ConTIG improves by 110.51% in terms of Recall@5 on Mooc. On Wikipedia and Reddit, the improvement is 3.25% and 13.13% with respect to Recall@5. These signiﬁcant improvements verify that the differential equation fused with three factors (i.e., latest interaction, neighbor features and inherent characteristics) proposed in ConTIG is capable of learning the trend of node state trajectories in the network. Additionally, the signiﬁcant improvement of ConTIG beneﬁts from the combination of continuous node state modeling in update module and dynamic sub-graph structure capturing in transform module on node embeddings, which is good for the down-stream node recommendation task. 5.4 Dynamic Node Classiﬁcation The goal of dynamic node classiﬁcation task is to predict the state label of user given the user, item and future timestamp. For this task, we evaluate our method on transductive setting, predicting the state labels of users who have been observed during training. We evaluate the task on three datasets with dynamic node labels (i.e., Wikipedia, Reddit and Mooc), where CollegeMsg dataset is not included because there is no node label. Speciﬁcally, we train a decoder after the the model trained by the temporal link prediction task. Our evaluation metric in this task is the area under the ROC curve (AUC). The results comparison between our method and baseline methods in dynamic node classiﬁcation tasks are shown in Table 3. Again, our algorithm achieves the best or comparable performance comparing with existing dynamic graph embedding approaches, demonstrating its effectiveness for the down-stream node classiﬁcation task. 5.5 Performance on Long-interval Interactions The goal of this task is to observe the link prediction performance of our method for interaction sets with different time intervals. To categorize the interactions, ﬁrst, we calculate the time interval ∆t between each interaction and its latest interaction as mentioned in Section 3, and describe the distribution of the time intervals of interactions. Then, we equally divide the interactions into ﬁve sets according to the four quantiles: 20%, 40%, 60%, 80%, where the quantiles are cut points dividing the interactions into continuous intervals with equal probabilities in terms of the intervals ∆t of interactions. For example, the interactions in 0 - 20% set are of shorter intervals, while the interactions in 80-100% set are of longer intervals. Finally, we calculate the AP score for each set. Our results of comparison between our method and dynamic graph baselines in temporal link prediction task on ﬁve interaction sets of Wikipedia and CollegeMsg are shown in Fig. 4. Comparing the AP results in each set, we ﬁnd that our method outperforms TGN, especially in long-interval interactions (e.g., the frequency interval 6080% and 80-100%), demonstrating the superiority of ConTIG in capturing continuous dynamics of node representations. And we argue that the long-interval link prediction is more beneﬁcial to practical applications, because quite a few users are always inactive on social networks, citation networks and other user-item interaction systems. 5.6 Ablation Study To further investigate the effect of each component in our model, we compare ConTIG with its variants as follows: form module (i.e. graph attention layer). module (i.e. continuous inference block). fusing three factors in update module. We replace it by directly adding the three factors in the update module. action factor in update module. bor features factor in update module. characteristics factor in update module. Table 4 shows the AP results of each model. ConTIG performs better than ConTIG w/o transform and ConTIG w/o update by a large margin, which demonstrates the effectiveness of the main modules of our model. Especially, the introduction of the continuous update module signiﬁcantly improves the results, indicating that the latest knowledge between two consecutive interactions is an essential feature to learn the node representations. By further modeling and aggregating the historical interaction information of nodes, ConTIG consistently improves the performance, showing the importance of the historical interaction information. Speciﬁcally, we investigate the effect of each component in update module. By removing the adaptive fusion approach, the performance of ConTIG w/o adaptive degrades obviously, pointing out that the adaptive fusion is a key factor to the success of the ODE solver in update module. It helps the network to focus on the most correlated factor to update the node state, and adaptively fuses three factors in a data-dependent way. Besides, ConTIG outperforms ConTIG w/o latest, ConTIG w/o neighbor and ConTIG w/o inherent by a small margin, which indicates that the three factors is of great importance for learning the change of node state in the update module. In particular, by comparing ConTIG with ConTIG w/o latest, ConTIG w/o neighbor and ConTIG w/o inherent, respectively, we observe that the neighbor features contributes most to the performance in all datasets, which indicates the importance of neighbors in the latest changing of node states. Simultaneously, for ConTIG w/o inherent, we observe that it has obvious effects on the datasets without edges features (i.e., Mooc and CollegeMsg), although it has tiny effects on the datasets with edge features (i.e., Wikipedia and Reddit). 5.7 Parameter Sensitivity There are several hyper-parameters in our proposed method. It is necessary to analyze how these parameters inﬂuence the performance of our model in temporal link prediction task on the above datasets. In detail, these parameters include the end time ∆t in continuous inference block, the number of neighbors k, heads M in graph attention layer, node embedding dimension d and time embedding dimension d. We choose different values for them and use the AP score to evaluate their performance. End time in continuous inference block (See Section 4.3). An important parameter is how long used to update the node states in the continuous inference block. In our experiment the ∆t ranges from 0.6 to 1.6 and the node and time embedding dimension is ﬁxed to 172. As shown in Fig.5a, as ∆t is larger, the AP score ﬁrst increases and then decreases, demonstrating that more time for learning node changes between two consecutive interactions could yield better performance. However, when the updating time is very long, it would make the current node over rely on the latest information, and forget the historical information, which hinders the performance. Number of neighbors (See Section 4.4). The inﬂuence of the number of neighbors will then be evaluated, which control the amount of historical interactive information is considered in the transform module. As shown in Fig.5b, in general, as the number of neighborhood k becomes larger, the model could achieve better performance because more historical information is considered. However, when the number of neighborhood is very large, it would introduce useless or overdue information into the learning process and thus degrade the performance. Heads in graph attention layer (See Section ??). Fig.5c show the AP results under different number of heads M in the multi-head attention. We observed that the AP score ﬁrst increases and then decreases or stabilize, and there is a relatively stable and good performance at two-head attention. It means that less-head attention will make the results unstable and offset, but attention with many head may pay attention to useless or unrelated information. Node embedding dimension. The inﬂuence of different node embedding dimensions 16; 64; 128; 172; 256 on our model is shown in Fig. 5d. We discover the same trend on two datasets that as the dimension rises from a small value, the performance of ConTIG will improve rapidly and it becomes relative stable while the size is large. The potential reason is that a higher dimension enables the model to learn more information in the latent space. However, it would make computational consumption become very large, so we choose a reasonable dimension with the best performance (i.e., 172). Time embedding dimension. The inﬂuence of different time embedding dimensions 16; 64; 128; 172; 256 on our model is shown in Fig. 5e. We observe the similar results with node embedding dimension, as dis larger, the AP score ﬁrst increases and then tends to be stable. Different from node embedding dimension, there is a ﬂuctuation in the high-dimension time embedding on Wikipedia and lowdimension time embedding on Mooc, which means high dimension and low dimensions may both hinder the performance, and shows the importance of choosing a reasonable time embedding dimension (i.e., 172). 5.8 Complexity Evaluation Here, we examine how the training time of ConTIG depends on the number of edges |E| which are used for training. We record the runtimes of ConTIG for training one epoch on the Wikipedia dataset using the best parameters in Section 5.7. Fig.6 shows that the entire runtime for one-epoch training is close to linear with |E|. This evaluation demonstrates our method’s advantage of being scalable to process long edge streams. In this paper, we present ConTIG, a novel representation learning method to capture the continuous dynamic of node embedding trajectories by identifying three-fold factors (i.e., latest interaction, neighbor features and inherent characteristics). ConTIG contains two modules: a update module to learn the node embedding trajectories and a transform module to generate the future representations according to the historical interaction information. Experiments results demonstrate that ConTIG achieves state-of-the-art performances, especially for long-interval interactions on temporal link prediction tasks. In the future, besides node state trajectory, we plan to pay more attention to community evolution, exploring the impact of community on individuals during the graph evolution. The research is supported by Natural Science Foundation of China (61872306), Xiamen Science and Technology Bureau (3502Z20193017) and Fundamental Research Funds for the Central Universities (20720200031).