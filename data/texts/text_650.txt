ZHIWEI LIU, University of Illinois at Chicago, USA LIANGWEI YANG, University of Illinois at Chicago, USA ZIWEI FAN, University of Illinois at Chicago, USA HAO PENG PHILIP S. YU, University of Illinois at Chicago, USA Recommender systems have become prosperous nowadays, designed to predict users’ potential interests in items by learning embeddings. Recent developments of the Graph Neural Networks (GNNs) also provide recommender systems with powerful backbones to learn embeddings from a user-item graph. However, only leveraging the user-item interactions suers from the cold-start issue due to the diculty in data collection. Hence, current endeavors propose fusing social information with user-item interactions to alleviate it, which is the social recommendation problem. Existing work employs GNNs to aggregate both social links and user-item interactions simultaneously. However, they all require centralized storage of the social links and item interactions of users, which leads to privacy concerns. Additionally, according to strict privacy protection under General Data Protection Regulation, centralized data storage may not be feasible in the future, urging a decentralized framework of social recommendation. As a result, we design a federated learning recommender system for the social recommendation task, which is rather challenging because of its heterogeneity, personalization, and privacy protection requirements. To this end, we devise a novel framework Firstly, FeSoG adopts relational attention and aggregation to handle heterogeneity. Secondly, FeSoG infers user embeddings using local data to retain personalization. Last but not least, the proposed model employs pseudolabeling techniques with item sampling to protect the privacy and enhance training. Extensive experiments on three real-world datasets justify the eectiveness of FeSoG in completing social recommendation and privacy protection. We are the rst work proposing a federated learning framework for social recommendation to the best of our knowledge. CCS Concepts: methodologies → Machine learning; Additional Key Words and Phrases: federated learning, recommender system; social recommendation, graph neural network ACM Reference Format: Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S. Yu. 2021. Federated Social Recommendation with Graph Neural Network. ACM Trans. Intell. Syst. Technol. 1, 1, Article 1 (January 2021), 25 pages. https: //doi.org/10.1145/3501815 • Information systems → Information retrieval;• Security and privacy;• Computing Fig. 1. Centralized learning (a) and federated learning (b) for social recommendation. The centralized learning trains the model with all the user privacy data, i.e., both social interactions and user-item interactions, available on the server. In contrast, federated learning locally stores user privacy data, only uploading and requesting non-sensitive data from the server. Dash lines between users denotes social interactions, while solid lines between users and items denotes user-item interactions. 1 INTRODUCTION The developments of Recommender Systems (RSs) [ well-designed recommender system is able to predict users’ potential interests in items. The core of it is to learn user/item embeddings [ prosperity of graph neural networks (GNNs) [ embeddings, which also motivates the community to design GNN-based RS models [ However, the cold-start issue [28, 66], which is associated with users having few records, impairs the performance of learned embeddings. To cope with this, one can leverage the social information of users [ we assume that users with social links also share similar item interests. Therefore, we could simultaneously aggregate social information and user-item interactions [ the cold-start issue. SocialGCN [ enhance user embedding by simulating how the recursive social diusion process inuences users. GraphRec [ user-item graph and item-item graph. Thus, it can comprehensively fuse the social links and item transactions. ConsisRec [ relation-level. It solves this problem by using a sampling-based attention mechanism. Though being eective in fusing the social and user-item information, they all require a centralized storage [5, 53, 62] of both the social networks and item transaction history of users. Existing centralized storage methods pose risks of leaking privacy-sensitive data. Additionally, due to the strict privacy protection under General Data Protection Regulation (GDPR) storage may not be the rst choice of online platforms in the future. Therefore, a new decentralized model training framework for social recommendation is necessary. According to previous 9] and GraphRec+ [10] propose to model three types of aggregations upon social graph, researches in federated learning [ only uploading the necessary gradients for updating the model on a server. As for the federated recommender systems [ only upload gradients to update user/item embeddings. However, there is few work discussing how to design a federated learning recommender system to complete the social recommendation task. We address the challenges of a federated social recommender system (FSRS) as follows: (1) Heterogeneity. The current federated recommender system stores user-item interactions locally. However, a FSRS requires both user-user and useritem interactions, as shown in Fig. 1. Therefore, we should store and fuse two types of relations simultaneously. (2) Personalization. Each client has special item interests and social connections, which leads to the non-iid distribution of the local data [ the personalized federated learning process [ Privacy Protection. Though user privacy data are stored locally, a federated recommender system yet demands collecting necessary gradients [ This uploading process may lead to information leakage of original data [ design a protection module before uploading any information. To this end, we devise a novel FSRS framework to address the challenges as mentioned above, which are to use a local graph neural network to learn node embeddings. To tackle the heterogeneity of local data, we employ a relation attention mechanism to distinguish user-user and user-item interactions, which characterize the importance of neighbors by assigning dierent attention weights with respect to their relations. Secondly, the local graph neural networks on each client are updated only based on the data on devices. As such, models on devices possess personalizing training. Last but not least, FeSoG employs pseudo-labelling technique for the on-device training of local models. This pseudo-labelling can protect the privacy data from leakage when uploading gradients and enhance the robustness of the training process. Extensive experiments on three datasets demonstrate the eectiveness of FeSoG. Compared with other baselines, FeSoG achieves up to 5.26% in RMSE and 5.46% in MAE across all three datasets. In ablation studies, FeSoG also demonstrates the necessity of each component developed in the federated learning framework. The contributions are summarized as follows: In the following sections, we rst introduce the related work in Sec. 2. Then, we present some preliminaries in Sec. 3, including both the denition and formulation. The detailed descriptions of our proposed FeSoG model are in Sec. 4. Experiments are discussed in Sec. 5. Finally, we conclude this paper and open up possible future work in Sec. 6. 2 RELATED WORK This section presents three relevant areas to this paper: GNN for recommendation, social recommendation, and federated learning for recommendation. FedratedSocial recommendation withGraph neural network (FeSoG). Firstly, we propose : We are the rst work proposing a federated learning framework to tackle the social recommendation problem to the best of our knowledge. privacy protection, by proposing a new model FeSoG. to verify the eectiveness of FeSoG. Detailed analysis and ablation study further prove the ecacy of our proposed components in FeSoG. 2.1 Graph Neural Network for Recommendation The recent developments of Graph Neural Networks (GNNs) [ to propose a GNN-based recommender system. The intuition of a GNN model is to aggregate neighbors to recursively learn node embeddings [ architecture to complete the user-item rating matrix. It uses the GCN as an encoder to train user/item embeddings, which are input to a fully connected neural network to predict the ratings. PinSAGE [ attributed item graph. It rst samples xed-size nodes from multi-hop neighbors and then uses aggregators to aggregate those sampled nodes to learn the embeddings for center nodes. NGCF [ is proposed later to explicitly model the collaborative signals upon user-item interaction graph by applying the GNN model. DGCF [ on the bipartite graph and solves it with cross-hop propagation layers. BasConv [ work that investigates using GNN to complete basket recommendation. These works prove the ecacy of using the GNN framework to learn embeddings in a recommender system. GNN-based models are advantageous as their aggregation can model high-order structural information crucial for learning user/item embeddings from interactions. This paper also adopts the GNN model to embed the local graphs. We employ the graph attention networks [48] as a backbone. 2.2 Social Recommendation The social recommendation aims to relieve the data sparsity and cold start problem by inducing information of social links between users [ generally categorized as social matrix factorization based methods and graph neural network based methods. Existing social matrix factorization approaches either jointly factorize the rating and social relationship matrices or regularize the user/item embeddings with constraints of social connections. SoRec [32] co-factorizes the user rating matrix and social link matrix. SocialMF [18] adds a regularization term to constrain the dierence between the user’s taste and his/her trusted friends’ average weighted taste. SoReg [ dierence in the user latent feature between two trusted users, which can prevent the counteraction of the latent feature of one’s trusted friends. HGMF [ factorization technique to learn the user-group feature in a social network for recommendation. Unlike matrix factorization methods, graph neural network methods infer node embeddings directly from graphs and demonstrate the eectiveness from recent social recommendation work [ GraphRec [ for recommendation. [ user’s interest from the social dimension. CUNE [ from each other. CUNE extracts semantic and reliable social information by graph embedding method. DiNet [ the social recommendation. ConsisRec [ recommendation and introduces a consistent neighbor sampling module in the GNN model. The above studies show the eectiveness of incorporating social information into the recommender system. 2.3 Federated Learning for Recommender System Google proposed Federated learning in 2016 [ machine learning models [ models. The fundamental of federated learning is to design a decentralized training framework, which distributes the data to clients rather than storing it in a server [ 63] proposes to use the GraphSAGE [15] backbone to learn item embeddings over an 9] and GraphRec+ [10] uses graph attention networks to learn user and item embeddings are sensitive information and probably cause identity information leakage if used for malicious purposes. Several recent works [ information protection while still preserving good enough personalization. Federated Collaborative Filtering (FCF) [ framework to learn the user/item embeddings for a recommender system. Both works develop the federated learning on the top of factorization [ federated learning, they propose that the user’s ratings should be stored locally. The user embeddings can be trained locally, and the server only retains the item embeddings. This training framework leads to protecting the privacy data, as there is no transfer of users’ interactions. Ribero et al. [ argues that the model updates sent to the server may contain sucient information to uncover raw data, which leaves privacy concerns. They propose to use the dierential privacy [ exposure of the data in a federated recommender system. FED-MVMF [ Matrix Factorization (MVMF) [ both feature matrices and interaction matrices. A-FRS [ system against the poisoning attacks of clients. It employs an item similarity model [ the user/item embeddings. FedGNN [ federated recommender system, which is also the most relevant work to our paper. However, FedGNN fails to solve social recommendations, and the clients’ models are not personalized [ present a comparison of a set of representative social recommender systems and federated learning methods in Table 1. Table 1. Comparison of representative models with respect to social information, multi-relation, graph neural network, rating protection, interaction protection and data storage TrustSVD [14] 3 PRELIMINARY In this section, we present the preliminaries and denitions of essential concepts. The glossary of necessary notations are summarized in Table 2. 3.1 Definitions The target in a social recommendation is to predict the users’ ratings to items, when given social interactions and user-item interactions. Denote the as the set of users and item, respectively. The social recommendation is to complete the ratings of users to items given both rating matrix R ∈ R an item federated learning scenario, the data of each user is stored locally. Hence, both the rating matrix and social connection matrix are not available. The data of each user are stored in the local client, which is dened as: Definition 1. data. Each client respectively. Definition 2. multiple clients in training a model. It does not exchange raw data from clients but only requests necessary messages for updating the model. In this paper, we assume clients and server to be honest-but-curious [ provide correct information and cannot tamper with the training process. The Federated Social Recommender System (FSRS) is to complete the rating matrix given its partially complete rating data and social data, which is dened as follows: Definition 3. social data as 𝑛, 𝑝 ∈ { 𝑐without access to the raw data in each client. Note that both the rating and social data are stored locally in the corresponding clients and never be uploaded to the server. Multiple clients collaboratively train an FSRS under the orchestration of the center server [20]. 3.2 Formulation Our FSRS is designed by formulating the data on clients as multiple local graphs, which is illustrated in Fig. 1. The local graph contains the rst order neighbors of the client user, including item ratings and social neighbors. We denote the local graph for client nodes and item nodes. are two type of edges in user-user edges denoting the social interactions. For each client predict the rating value of an unobserved item the attribute value of the local graph for the edge between the user problem can be formulated as follows: Definition 4. train a model to predict the attribute value for an unobserved edges data of any local graphs? We specify the social recommendation problem to be a link prediction problem. It indicates that we should learn graph embeddings from local graphs to preserve the structural information. Additionally, it is necessary to tackle the heterogeneity [16, 30, 60] of those local graphs. and the social connection matrixS ∈ {0,1}. We denote the user𝑛’s rating value to 𝑚asR. Similarly, the connection between an user𝑛and user𝑝is denoted asS. In a r= [𝑟, 𝑟, . . . , 𝑟]ands= [𝑠, 𝑠, . . . , 𝑠]of each client𝑐, respectively, where 1,2, . . . , 𝑁 }and𝑘 ∈ {1,2, . . . , 𝑀}, an FSRS can predict the unobserved rating data of the client = {𝑡, 𝑡, . . . , 𝑡}and social neighbors asU= {𝑢, 𝑢, . . . , 𝑢}. The FSRS can 4 PROPOSED FRAMEWORK In this section, we illustrate the FSRS framework of our proposed FeSoG. It has three crucial modules: embeddings layer, local graph neural networks, and gradient protector. The proposed framework is in Fig. 3. 4.1 Embeddings Node embeddings are crucial components in preserving graph structural information [ The FeSoG has embedding layers for user and item nodes. We denote the embeddings for users and items as 𝑑 ∈ N Then, they learn a local user/item embeddings and a local GNN model by using their interaction data. Those embeddings will be updated on the server by aggregating the gradients uploaded from clients. A client downloads the complete embedding tables and uses the user/item ids in interaction records to infer the corresponding embeddings. To be more specic, for a client items as the total number of item neighbors and user neighbors, respectively. Those embeddings are input the local GNN model to learn client user embedding and predict item scores. 4.2 Local Graph Neural Network The local GNN module is the major component in FeSoG to learn node embeddings and make a prediction. It consists of heterogeneous graph attention layers, relational graph aggregation layers, and rating prediction layers. 𝑁 ; 𝑀 total number of users; total number of items 𝑐client 𝑛, which is associated with user 𝑛 r; sthe local observed rating and social data of client 𝑐 ; Uthe local observed rated items; the social connected neighbors of client 𝑐 ); E(e) embedding for users, embedding for items ethe local inference embedding of user 𝑛 a; b; c attention layer vector for user-user interaction; for item-item interaction; for relation vector W; Wlinear mapping matrix for user-user interaction; for item-item interaction 𝛼, 𝛽attention weights for neighbor users; for neighbor items h; hhidden embeddings for neighbor users; hidden embeddings for neighbor items 𝛾;𝛾attention weight for aggregating social relation; for aggregating user-item relation v; vsocial relation vector; user-item relation vector L;˜Lloss for client 𝑢; protected loss for client 𝑢 ; gthe gradients for client 𝑛; the embedding gradients; the model gradients 𝛿; 𝜆 the parameters for LDP E∈ RandE∈ R, respectively, which are both maintained by the server. is the dimension size for embeddings. Clients request the embeddings tables from the server. T= {𝑡|}and social neighbors asU= {𝑢|}, its rated item embeddings are }and social neighbor embeddings are{e|}, wheree, e∈ Rand𝐾, 𝑃denote Fig. 2. (a): The calculation of the aention weights between two embeddings, which consists of linear mapping matrix (b): The local GNN learns the embedding of 𝑢 4.2.1 Relational Graph Aention. In general, we have no constraints on the local graph neural networks. There can be arbitrary GNN models, such as GCN [ etc. This paper focuses on proposing a new FSRS framework. We directly adopt the GAT layer for learning node embedding and leave other GNN layers for future investigation. The GAT layer is designed by employing the self-attention mechanism [ we aggregate neighbor embeddings of center node Specically, for a social pair (𝑢 where Attention is the attention layer. We dene the attention layer as a single-layer feed-forward neural network. It is parameterized with a weight vector which is formulated as: where operation of two vectors. An illustration of the attention weight is in Fig. 2(a). The attention weight should be calculated for all the neighbors of the center node distribution by employing the softmax function: where calculated as the attention weights for user neighbors. We should learn attention weights for user-item pair another attention parameters as in Eq. (4), as following: where attention layer. By employing the softmax to all the item neighbors, we derive the attention weights for item neighbors as: W, concatenation of two embeddings, aention layer𝑎, non-linear activation, and somax function. 𝑢, we should rst learn a weight for each neighbor by employing an attention layer. 𝑜is a scalar denoting the attention weight,W∈ Ris a linear mapping matrix, and adenotes the transpose of the attention layer parameter and∥denotes the concatenation 𝛼is the nal attention weights, exp denotes the exponential function. Note that𝛼is (𝑢, 𝑖)in a similar way, which employs another linear mapping as in Eq. (1) and W∈ Ris the mapping matrix andb ∈ Ris the weights for the user-item interaction where present the relational aggregation for both user neighbors and item neighbors. 4.2.2 Relational Graph Aggregation. Inferring the center user embeddings requires aggregating both neighbor user nodes and neighbor item nodes with their associated attention weights, which are formulated as follows: where embeddings for aggregating user neighbors and item neighbors, respectively. The general aggregation process is illustrated in Figure 2(b). Intuitively, we should aggregate hidden embeddings and the center node embedding to infer the embedding of interactions and center node embedding are not equally contributed to the learning process [ We should also handle the heterogeneity during the aggregation step. Therefore, we propose to use three relation vectors to preserve their semantics, i.e., semantics, user-item semantics and center node itself semantics respectively. To be more specic, we concatenate hidden embeddings with their relation vectors and employing the self-attention mechanism to learn the weights for aggregation: where neighbor embedding and center node itself embedding, respectively. for the attention layer. Given that, we infer the node embeddings of 𝑢 where such, local clients preserve their user embeddings, which tackles the personalization problem. Next, we will use the learned embeddings and downloaded item embeddings to make a prediction. 4.2.3 Prediction. To predict the local item ratings, we adopt the dot-product between the inferred user embedding and item embeddings. For a user respectively, the rating R where prediction by employing the Root Mean Squared Error (RMSE) between the predicted score 𝛽denotes the attention weights for items normalized over all neighboring items, next, we W∈ Ris the linear mapping weight matrix, andh, h∈ Rdenotes the hidden 𝛾,𝛾and𝛾are the attention weights for hidden user neighbors embedding, hidden item eis the local user embedding for prediction. The aggregation is illustrated in Fig. 3. As ·denotes the dot-product operation. We use the local user-item rating values to optimize the Fig. 3. The framework of FeSoG. For simplicity and without loss of generality, we present a two-client scenario. In each client, we use the local GAT layer to infer node embeddings and adopt the aention layer to aggregate social neighbors and item neighbors. Then, we sample a set of pseudo items bundled with local data to calculate the loss and gradients. Both the embedding gradients and model gradients are uploaded to the server for aggregation aer LDP operation. and the ground-truth rating score R where user is associated with a client. This loss function will be used to calculate the gradient for clients. Then, the gradients are collected from multiple clients to the server for further updates. However, directly uploading gradients leads the user-item interaction data to be vulnerable [ to design a privacy protection mechanism regarding the gradients, which will be introduced next. 4.3 Privacy Protection This section introduces two techniques to protect the local user-item interaction data when uploading the gradients: dynamic Local Dierential Privacy (LDP) and pseudo-item labelling. 4.3.1 Local Dierential Privacy. According to FedMF [ inferred if given the gradients of a user uploaded in two continuous steps. Though our case is more complicated than in FedMF, it is still problematic if directly optimizing the local data and uploading the gradients. Moreover, for embedding gradients, only items with ratings in a local client have non-zero gradients to upload to the server. FedMF [ gradients so that the server cannot inverse the encoding process. However, it requires generating Tdenotes the rated items of user𝑢andLis the local loss for user𝑢. Recall that each the public key and secret key and introducing additional computation for encryption. Additionally, to solve the zero-gradient for non-rated items, FedGNN [ items and add Gaussian noise with the same mean and variance as the ground-truth items to their gradients. Another technique used broadly is the Local Dierential Privacy (LDP) module [ It adopts clipping the local gradients based on their L module with zero-mean Laplacian noise to the unied gradients to achieve privacy protection. To be more specic, we instantiate the item embedding gradients, user embedding gradients and model gradients from client gradients as LDP is formulated as: where Laplacian is inappropriate when dealing with gradients at dierent magnitudes. The gradient magnitude of dierent parameters varies during training. Hence, we propose to add dynamic noise based on the gradient, which is formulated as follows: 4.3.2 Pseudo-Item Labelling. Based on existing work, we propose a new privacy protection module, which is advantageous as it can protect the training gradients and enhance the model with more robustness. In a local client, before calculating the training loss, we rst sample neighbor items, which are the pseudo-items in Fig. 3, denoting as we use the local model to predict the ratings for these pseudo items. The predicted ratings are rounded to be the pseudo ratings. Hence, the loss in Eq. (12) is changed to: Note that compared with Eq.(12), Eq.(15) is calculated from both the true interacted items and pseudo items. The ground-truth ratings for pseudo items are the rounded predicted score. The dierence between the predicted score and the rounded one contributes to the gradients for those pseudo items. Here, we assume that contain both ground-truth rating information and the pseudo item rating information, which prevents the data leakage problem. Additionally, the pseudo labels of items provide additional rating information, which can alleviate the cold-start issue of the data. Intuitively, this technique works as a data augmentation method [ dierence between rounded ratings with a predicted rating as the randomness, which enhances the robustness of the local model. 4.4 Optimization In this section, we rst present the optimization process of the FeSoG framework before we present the pseudo-code of the algorithm. 4.4.1 Gradient Collection for Optimization. The server in FeSoG collects the gradients uploaded from clients to update both the model parameters and embeddings, which collaboratively optimize the model. Recall that the gradients from client each round, the server builds a connection with a batch (e.g., 128) of clients, denoted as g= {g, g, g} =, whereΘdenotes all trainable parameters. Then, the ˜gis the randomized gradients, clip(𝑥, 𝛿)denotes limiting𝑥with the threshold𝛿, and (0, 𝜆)is the Laplacian noise with 0 mean and𝜆strength. However, a constant noise strength and Θindicating model parameters, item embedding and user embedding, separately. In sends the current model parameters local gradients from those clients as follows: where interactions and pseudo interactions. 𝑢, separately. Intuitively, aggregation, the server updates the parameter Θ with gradient descent as: where 𝜂 is the learning rate. This learning process is operated multiple rounds until convergence. 4.4.2 Algorithm. The pseudo-code of the algorithm of FeSoG is presented in Algorithm 1. The input are consist of the training hyper-parameters such as the embedding size Additionally, the client data should also be given, i.e. the clients local graphs target is to predict item ratings, we output the parameters is the loop operated on the server, which sends parameters to clients and collects their gradients for updating. The function ClientUpdate() is the operation on local devices. It downloads the parameters to infer the local user embeddings (line 8). Then, the pseudo items are sampled (line 10). Algorithm 1: FeSoG (Federated Social Recommendation with Graph Neural Network) Output : Model parameters and embeddings Θ; Local client embeddings {e Ris the total number interaction for calculating the gradients, including both the real }, which is sucient for clients to predict ratings. In the algorithm, the line 2 to the line 6 Total number of clients, items: 𝑁 , 𝑀,𝑇 The number of pseudo items: 𝑝 LDP parameter: 𝛿, 𝜆 Clients local graph: {G|} ˜L← Eq. (15); ˜g, 𝑝 + |T|; // return gradients and the number of interactions Pseudo-labelling and LDP are combined (line 10 to line 14) to protect the gradients from privacy leakage. This function returns the gradients and the number of interactions (line 15) for the server to collect. 5 EXPERIMENTS In this section, we conduct experiments to evaluate the eectiveness of FeSoG. We will answer the following Research Questions (RQs): • RQ1: Does FeSoG outperform existing methods in social recommendation? • RQ2: What is the impact of the hyper-parameters in FeSoG? • RQ3: Are those components in FeSoG necessary? 5.1 Experimental Setup 5.1.1 Datasets. In this paper, we adopt three commonly used social recommendation datasets to conduct experimental analyses, which are Ciao, Epinions [ Epinions on items and trust links between users as social relations. Each user can give an integer score in is built from online lm rating website and the trust relationship between users. The rating scale ranges from 1 to 8. Social relations are also the trust links between users in these datasets. Data statistics are shown in Table 3. In our FSRS scenario, each user is treated as a local client, and the user’s interactions are local privacy data on the device. The global graph information is transferred from user embeddings. 5.1.2 Baselines. We adopt three types of baselines for comparison: traditional Matrix Factorization (MF) based methods for social recommendation, recent GNN-based methods for social recommendation, and federated learning frameworks. MF-based and GNN-based methods are based on centralized learning, which is unable to protect user privacy. Federated learning methods are not able to handle the fusion of local social information and rating information. MF-based methods • SoRec [32]: It co-factorizes user-item rating matrix and user-user social matrix. • SocialMF [18]: Compared with SoReg, social matrix factorization also considers social trust [45–47] are crawled from shopping website. Both datasets contain user rating scores ,4,5}to rate an item, where 1 indicates least like while 5 represents most. Filmtrust[13] SoReg [33]: It develops a social regularization with social links to regularize on matrix factorization. propagation. • CUNE [64]: Collaborative user network embedding assumes users hold implicit social links 5.1.3 Evaluation Metrics. To evaluate the performance and compare, we adopt Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) to measure the model performance because they are the most commonly used metrics in social recommendation. Smaller values of both two metrics indicate better performance in the test data. The two metrics are calculated as follows: where tively. the evaluation is conducted on devices locally since the server has no access to the local privacy data. The lower MAE and RMSR both indicate better performance. 5.1.4 Experimental Setups. The ratings in each dataset are randomly split into training set (60%), validation set (20%), and test set (20%). Hyper-parameters are tuned based on the validation performance. Then, we report the nal performance on the test dataset. In all experiments, we initialize the parameters with standard Gaussian distribution. For the LDP technique used in FeSoG, the gradient clipping threshold is set to 0 parameters are tuned based on grid searching. The number of pseudo interacted items training round is searched in Training is stopped if RMSE on the validation set does not improve for 5 successive validations. 5.2 Overall Comparison (RQ1) In this section, we conduct the overall comparison of dierent models. The experimental results are shown in Table 4, which are categorized into three groups. We have the following observations: from each other, and it tries to extract semantic and reliable social information by graph embedding method. GCMC+SN [2]: GCMC is a graph neural network based method. User nodes are initialized as vectors learned by node2vec [12] from the social graph to obtain social information. The dense representation learned upon the social graph can include more information than the random initialized feature. GraphRec [9]: Graph recommendation uses graph neural network to learn user embedding and item embedding from their neighbors and uses several fully connected layers as the rating predictor. ConsisRec [61]: It is the state-of-the-art method in social recommendation. ConsisRec modies graph neural network to mitigate the inconsistency problems in social recommendation. FedMF [4]: It separates the matrix factorization computation to dierent users and uses an encryption method to avoid information leakage. FedGNN [53]: Federated graph neural network is the state-of-the-art federated recommendation method. It adopts local dierential privacy methods to protect user’s interaction with items. RandˆRare the true rating value and predicted rating value of user𝑛for item𝑡, respec𝑁is the total number of users for testing.Tdenotes the rated items for user𝑛. Again, ,50,100,500,1000}. Embedding size𝑑is tuned from{4,8,16,32,64}. User batch size in each Table 4. Experiment results compared with baseline methods. The best federated learning results are in bold, and the best results for non-federated learning methods are underlined. Improvement indicates the percent that FeSoG improves against the second-best federated learning result. FeSoG signicantly outperforms the State-Of-The-Art (SOTA) federated recommender systems in all datasets. Compared with FedGNN, FeSoG achieves on average 2.99% and 4.03% relative improvements on RMSE and MAE, respectively. Several advantages of FeSoG support its superiority: (1) social information helps the recommendation, which the improvement can demonstrate over FedMF; (2) the relational graph attention and aggregation can eectively integrate both user-item interactions and social information; (3) the local pseudo-item sampling technique enhances the performance. The GNN-based models perform better than those MF-based models. ConsisRec is the SOTA GNN model that employs relation attention and consistent neighbor aggregation, which leads to its best performance. Compared with SocialMF, which is the best MF-based method, ConsisRec achieves in average 3.47% and 5.27% relative improvements in RMSE and MAE, respectively. GNN-based models are better as they can directly model structure information and simultaneously aggregate social and user-item interactions. Between the two federated learning baselines, FedGNN also signicantly outperforms FedMF, which again supports the claims that GNN-based models are better than MF-based methods. FeSoG is also based on GNN aggregation. Its local GNN aggregation employs relation attention, which leads to its better performance against FedGNN. Federated learning impairs the performance compared with centralized learning. Even a simple GCMC+SN model is better than the FedGNN model. There are two reasons: On the one hand, to achieve privacy protection, the federated learning framework has no access to the local data, limiting its capacity to model the global structures. According to [9,27], the core of graph embedding is to aggregate high-order neighbors and select informative contexts. On the other hand, the local gradients are protected by adding random noise. Though it theoretically will not hurt the performance in expectation, it still prevents the server from receiving qualitative gradients from clients. We should nd a trade-o between performance and privacy protection. This observation also brings opportunities for federated learning research. 5.3 Sensitivity Analysis (RQ2) In this section, we emphasize on analyzing the impacts of those hyper-parameters involving in FeSoG and some other baselines. We include user batch size embedding size 𝛿 and 𝜆. 5.3.1 User Batch Size. In this section, we analyze the impact of user batch size. Intuitively, choosing a small user batch size increases the communication rounds for the server to train a model. However, it is unclear how it aects the prediction performance. We report the performance of FedGNN and FeSoG with respect to RMSE and MAE across three datasets, which is illustrated in Fig. 4 and Fig. 5, respectively. We have the following observations: 5.3.2 Number of Pseudo Items. Pseudo items are sampled to protect the gradients from privacy leakage. Sampling more pseudo items requires more computational cost as more ratings should be predicted. However, it is unclear how many samples should be selected to achieve satisfying results. Hence, we conduct experiments on three datasets to study the impacts of the number of sampled Fig. 4. RMSE performance with respect to user batch size on three datasets. Fig. 5. MAE performance with respect to user batch size on three datasets. FeSoG performs better than FedGNN. On all three datasets, the RMSE of FeSoG is consistently lower than FedGNN, which results from its powerful embedding ability of relational local GNN. The performance of FeSoG becomes better with the increase of user batch size across datasets. With a larger user batch size, the server can obtain a more accurate global information estimation, which leads to a better performance. However, in practice, aggregating more users at each training step would lead to more computational cost and more time to converge. pseudo items. We also compare FedGNN, which samples a set of negative items and assigns them with random gradients. The inuence of the number of pseudo items on three datasets with respect to RMSE and MAE is reported in Fig. 6 and Fig. 7. Besides the performance value, we also present the computational cost with respect to the number of sampled pseudo items. We have the following observations: Fig. 6. RMSE performance with respect to dierent pseudo item numbers on three datasets. Fig. 7. MAE performance with respect to dierent pseudo item numbers on three datasets. FeSoG yields better performance compared with FedGNN. On Ciao and Epinions datasets, the performance of FeSoG and FedGNN gets worse with the increase of pseudo items. However, the value of FeSoG is much lower than FedGNN. It suggests that our pseudo item sampling and the pseudo labeling techniques are robust. Therefore, we can conclude that the pseudo item sampling in FeSoG can protect privacy data and enhance the training process. If increasing the number of pseudo items, the error value all increases for both FedGNN and FeSoG on Ciao and Epinions datasets, which results from more noise caused by pseudo items. However, on the Filmtrust dataset, the error value of FedGNN rst increases and then drops. This is because FedGNN generates gradients of pseudo items from the same Gaussian distribution, and at the same time Filmtrust dataset only has 1957 items. So when sampling more than 500 pseudo items, the gradient of pseudo items from dierent users would counteract with each other to reduce noise impact. FeSoG generates gradients of pseudo items by user-specic labelling, which does not have this characteristic. We should nd a trade-o between pseudo-item sampling and model performance. As illustrated in Fig. 6 and Fig. 7, if increasing the number of pseudo items, the extra computational cost increases linearly and the server would be harder to infer the true interacted items. At the same time, prediction accuracy would become worse. So we should nd a suitable pseudo item number to balance privacy protection and model performance. For example, 100 pseudo 5.3.3 Embedding Size. This section studies the performance with respect to the embedding size. The values of RMSE and MAE on three datasets are reported in Fig. 8 and Fig. 9, respectively. For comparison, we also select two representative baselines, which are FedGNN and FedMF. We have the following observations: 5.3.4 Learning Rate. Learning rate aects the number of communication rounds for the convergence of a federated learning framework. Intuitively, fewer communication steps can decrease the number of communication times between the server and clients, which implies less risks of items for Ciao dataset would be an appropriate setting because the model performance does not deteriorate much when the number of pseudo items is lower than 100. Fig. 8. RMSE performance with respect to dierent embedding sizes 𝑑 on three datasets. Fig. 9. MAE performance with respect to dierent embedding sizes 𝑑 on three datasets. The proposed FeSoG consistently outperforms other federated recommender system methods across all embedding sizes. This observation suggests that FeSoG can learn informative structures from local graphs. Compared with FedMF, FedGNN demonstrates a much better performance, which justies the necessity to employ GNN for graph embedding. FeSoG has lower uctuations as embedding sizes change compared with the other two baselines. It indicates that FeSoG is more robust than FedGNN on dierent embedding sizes, demonstrating the eectiveness of using pseudo-item protection to enhance the training. Furthermore, suitable embedding sizes are crucial for dierent datasets to achieve satisfying performance. All federated learning methods follow the same trend and obtain the best performance in either𝑑 =16 or𝑑 =32. With a smaller embedding size (e.g.,𝑑 =4), the model has insucient representation ability. However, with large embedding sizes (e.g.,𝑑 =64), it may cause the overtting issue due to limited data. information leakage. We investigate the impact of learning rate with respect to the training loss of FeSoG and report the results in Fig. 10. The learning rather is selected from have the following observations: 5.3.5 Local Dierential Privacy parameters model performance and the LDP module. It contains two parameters: gradient clip threshold Laplace noise strength noise to gradients to protect the user’s privacy. The injected gradients contribute to the training because the server also aggregates them for updating. Therefore, we conduct experiments to study the model performance with respect to the variations of these two parameters, which is shown in Fig. 11 and Fig. 12 for RMSE and MAE, respectively. We have the following observations: 5.4 Ablation Study (RQ3) In this section, we conduct an ablation study to analyze those components in FeSoG to validate their eectiveness. We create three other variants of FeSoG: • sharing GAT layer: The training of FeSoG is smooth. The loss on three datasets all converges smoothly when increasing the number of communications steps. It suggests that the federated learning framework can eectively transfer informative gradients for the FeSoG to converge. Dierent datasets prefer dierent learning rates. For example, on the Epinions dataset, the best learning rate𝜂 =0.1. While both Ciao and Filmtrust datasets converge the fastest when the learning rate 𝜂 = 0.05. Learning rate has a critical inuence on dierent datasets. With a xed𝜆, FeSoG performs better when increasing𝛿. The reason is that a large𝛿tends to clip less gradients. Therefore, the aggregated gradient information would be more accurate to reect the true gradients. With xed𝛿, FeSoG performs worse when increasing𝜆. It is because a larger𝜆injects more substantial Laplace noise to the model gradient. Hence, the gradient learned from data would be overwhelmed by generated noises. Thus, a smaller 𝜆 is preferred. There is a trade-o in selecting optimal values. Although larger𝛿or smaller𝜆lead to better performance, it would increase the risk of privacy leakage. If𝛿is innitely large and𝜆is 0, the server can revert the interactions by checking uploaded gradients from clients. Therefore, we should choose an optimal pair to achieve acceptable performance with a small enough privacy budget. social neighbors and item neighbors. This variant shares the GAT layer for all neighbors. Table 5. Ablation study on FeSoG. The relative dierence represents the performance dierence between the corresponding variant and FeSoG. A positive dierence indicates worse performance, while negative ones indicate beer performance. • w/o relational vectors: • w/o pseudo items: Fig. 11. RMSE performance with respect to dierent 𝛿 and 𝜆 on three datasets. Fig. 12. MAE performance with respect to dierent 𝛿 and 𝜆 on three datasets. Hence, we only have one GAT layer to learn weight. Note that this variant also employs the relational vector during aggregation. during aggregation. As such, it directly aggregates all the neighbors with their associated attention weights. Since the attention weights are learned separately for user neighbors and item neighbors, we should normalize those weights for all neighbors. item is 0. It cannot protect the user privacy data from a protection perspective because. The performance comparison of these methods on three datasets is reported in Table 5. We also present the corresponding dierence between FeSoG and the variants in a group. We have the following observations: • We should employ dierent GAT layers for users neighbors and item neighbors. Compared 6 CONCLUSION AND FUTURE WORK In this paper, we propose a new federated learning framework, FeSoG, for social recommendation. It decentralizes the data storage compared with existing social recommender systems. Moreover, it comprehensively fuses the local user privacy data in clients and uses a server to train an FSRS collaboratively. We address three challenges in designing this model: the heterogeneity of the data, the personalization requirements of the local modeling, and privacy protection for communication. The components in FeSoG jointly tackle these challenges: the relational attention and aggregation of the local graph neural network distinguish social and item neighbors; the local user embedding inference preserves the personalizing information for clients; the pseudo-item labeling, as well as the dynamic LDP technique, protect the gradients from privacy data leakage. To verify the eectiveness of FeSoG, we conduct extensive experiments. The overall comparing experiments demonstrate that FeSoG signicantly outperforms SOTA federated learning framework in solving social recommendation problems. Detailed sensitivity analysis regarding the hyper-parameters further justies the ecacy of FeSoG infusing the social information and user-item interaction and preserving the user privacy data locally. Moreover, the ablation study by dropping the components in FeSoG demonstrates the necessity of our designing. Though being eective in solving the social recommendation problem, there are still several future directions. Firstly, we randomly sample pseudo items and predict their pseudo labels to protect gradients. However, as the items may have their relations, we may investigate employing an adaptive sampling of items rather than randomly. For example, we may train a local reinforcement learning model to explore the non-interacted item, which can decrease the noise. Secondly, we train the relational graph neural network only by leveraging local data. It is also possible to extend the local graph to be a high-order graph. However, this requires data transferring among clients. To protect privacy, we may design a peer-to-peer communication of clients preserving the decentralized storage characteristics of a federated recommender system. Finally, we can study the eciency of communication. Since it requires numerous communication rounds to train a federated Without pseudo items, we only use the true interacted items that have non-zero gradients. However, we also include this variant for a comprehensive study. with FeSoG, sharing GAT layer has worse performance. On the Filmtrust dataset, it has 2.57% and 0.53% relative dierence on RMSE and MAE, respectively. On the Epinions dataset, it has 0.43% and 0.21% relative dierence on RMSE and MAE, respectively. The worst performance of this variant is on Ciao, which has 5.8% and 6.2% relative dierence on RMSE and MAE, respectively. We should apply the relational vectors for aggregation. Compared with FeSoG, without relational vectors has worse performance. On the Epinions dataset, it has 0.42% and 0.40% relative dierence on RMSE and MAE, respectively. On the Filmtrust dataset, it has 2.09% and 3.7% relative dierence on RMSE and MAE, respectively. The worst performance of this variant is on Ciao, which has 12.58% and 13.96% relative dierence on RMSE and MAE, respectively. Sampling pseudo items always worsen performance. However, compared with FeSoG, without pseudo items is unable to protect the privacy because the server can easily infer the true interacted items by checking which item gradient is not 0. learning framework, it will be satisfying to decrease the time for communication or increase the bandwidth for communication with large-scale clients. ACKNOWLEDGMENT Hao Peng is supported by the National Key R&D Program of China through grant 2021YFB1714800, NSFC through grants 62002007 and U20B2053, S&T Program of Hebei through grant 21340301D, Fundamental Research Funds for the Central Universities. Philip S. Yu is partially supported by NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.